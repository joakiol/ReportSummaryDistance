Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 483?491,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsLearning to Link Entities with Knowledge BaseZhicheng Zheng, Fangtao Li, Minlie Huang, Xiaoyan ZhuState Key Laboratory of Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Science and Technology, Tsinghua University, Beijing 100084, China{zhengzc04,fangtao06}@gmail.com, {aihuang,zxy-dcs}@tsinghua.edu.cnAbstractThis paper address the problem of entity link-ing.
Specifically, given an entity mentioned inunstructured texts, the task is to link this entitywith an entry stored in the existing knowledgebase.
This is an important task for informa-tion extraction.
It can serve as a convenientgateway to encyclopedic information, and cangreatly improve the web users?
experience.Previous learning based solutions mainly fo-cus on classification framework.
However, it?smore suitable to consider it as a ranking prob-lem.
In this paper, we propose a learning torank algorithm for entity linking.
It effectivelyutilizes the relationship information amongthe candidates when ranking.
The experi-ment results on the TAC 20091 dataset demon-strate the effectiveness of our proposed frame-work.
The proposed method achieves 18.5%improvement in terms of accuracy over theclassification models for those entities whichhave corresponding entries in the KnowledgeBase.
The overall performance of the systemis also better than that of the state-of-the-artmethods.1 IntroductionThe entity linking task is to map a named-entitymentioned in a text to a corresponding entry storedin the existing Knowledge Base.
The KnowledgeBase can be considered as an encyclopedia for en-tities.
It contains definitional, descriptive or rele-vant information for each entity.
We can acquire theknowledge of entities by looking up the Knowledge1http://www.nist.gov/tac/Base.
Wikipedia is an online encyclopedia, and nowit becomes one of the largest repositories of encyclo-pedic knowledge.
In this paper, we use Wikipedia asour Knowledge Base.Entity linking can be used to automatically aug-ment text with links, which serve as a conve-nient gateway to encyclopedic information, and cangreatly improve user experience.
For example, Fig-ure 1 shows news from BBC.com.
When a user isinterested in ?Thierry Henry?, he can acquire moredetailed information by linking ?Thierry Henry?
tothe corresponding entry in the Knowledge Base.Figure 1: Entity linking exampleEntity linking is also useful for some informationextraction (IE) applications.
We can make use ofinformation stored in the Knowledge Base to assistthe IE problems.
For example, to answer the ques-tion ?When was the famous basketball player Jor-dan born?
?, if the Knowledge Base contains the en-483tity of basketball player Michael Jordan and his in-formation (such as infobox2 in Wikipedia), the cor-rect answer ?February 17, 1963?
can be easily re-trieved.Entity linking encounters the problem of entityambiguity.
One entity may refer to several entriesin the Knowledge Base.
For example, the entity?Michael Jordan?
can be linked to the basketballplayer or the professor in UC Berkeley.Previous solutions find that classification basedmethods are effective for this task (Milne and Wit-ten, 2008).
These methods consider each candidateentity independently, and estimate a probability thatthe candidate entry corresponds to the target entity.The candidate with the highest probability was cho-sen as the target entity.
In this way, it?s more likea ranking problem rather than a classification prob-lem.
Learning to rank methods take into account therelations between candidates, which is better thanconsidering them independently.
Learning to rankmethods are popular in document information re-trieval, but there are few studies on information ex-traction.
In this paper, we investigate the applicationof learning to rank methods to the entity linking task.And we compare several machine learning methodsfor this task.
We investigate the pairwise learning torank method, Ranking Perceptron (Shen and Joshi,2005), and the listwise method, ListNet (Cao et al,2007).
Two classification methods, SVM and Per-ceptron, are developed as our baselines.
In com-parison, learning to rank methods show significantimprovements over classification methods, and List-Net achieves the best result.
The best overall per-formance is also achieved with our proposed frame-work.This paper is organized as follows.
In the nextsection we will briefly review the related work.
Wepresent our framework for entity linking in section3.
We then describe in section 4 learning to rankmethods and features for entity linking.
A top1 can-didate validation module will be explained in section5.
Experiment results will be discussed in section 6.Finally, we conclude the paper and discusses the fu-ture work in section 7.2Infoboxes are tables with semi-structured information insome pages of Wikipedia2 Related WorkThere are a number of studies on named entity dis-ambiguation, which is quite relevant to entity link-ing.
Bagga and Baldwin (1998) used a Bag of Words(BOW) model to resolve ambiguities among people.Mann and Yarowsky (2003) improved the perfor-mance of personal names disambiguation by addingbiographic features.
Fleischman (2004) trained aMaximum Entropy model with Web Features, Over-lap Features, and some other features to judgewhether two names refer to the same individual.Pedersen (2005) developed features to represent thecontext of an ambiguous name with the statisticallysignificant bigrams.These methods determined to which entity a spe-cific name refer by measuring the similarity betweenthe context of the specific name and the context ofthe entities.
They measured similarity with a BOWmodel.
Since the BOW model describes the con-text as a term vector, the similarity is based on co-occurrences.
Although a term can be one word orone phrase, it can?t capture various semantic rela-tions.
For example, ?Michael Jordan now is the bossof Charlotte Bobcats?
and ?Michael Jordan retiredfrom NBA?.
The BOW model can?t describe the re-lationship between Charlotte Bobcats and NBA.
Ma-lin and Airoldi (2005) proposed an alternative sim-ilarity metric based on the probability of walkingfrom one ambiguous name to another in a randomwalk within the social network constructed from alldocuments.
Minkov (2006) considered extendedsimilarity metrics for documents and other objectsembedded in graphs, facilitated via a lazy graphwalk, and used it to disambiguate names in emaildocuments.
Bekkerman and McCallum (2005) dis-ambiguated web appearances of people based on thelink structure of Web pages.
These methods tried toadd background knowledge via social networks.
So-cial networks can capture the relatedness betweenterms, so the problem of a BOW model can besolved to some extent.
Xianpei and Jun (2009) pro-posed to use Wikipedia as the background knowl-edge for disambiguation.
By leveraging Wikipedia?ssemantic knowledge like social relatedness betweennamed entities and associative relatedness betweenconcepts, they can measure the similarity betweenentities more accurately.
Cucerzan (2007) and484Bunescu (2006) used Wikipedia?s category informa-tion in the disambiguation process.
Using differentbackground knowledge, researcher may find differ-ent efficient features for disambiguation.Hence researchers have proposed so many effi-cient features for disambiguation.
It is important tointegrate these features to improve the system per-formance.
Some researchers combine features bymanual rules or weights.
However, it is not conve-nient to directly use these rules or weights in anotherdata set.
Some researchers also try to use machinelearning methods to combine the features.
Milne andWitten (2008) used typical classifiers such as NaiveBayes, C4.5 and SVM to combine features.
Theytrained a two-class classifier to judge whether a can-didate is a correct target.
And then when they tryto do disambiguation for one query, each candidatewill be classified into the two classes: correct tar-get or incorrect target.
Finally the candidate answerwith the highest probability will be selected as thetarget if there are more than one candidates classi-fied as answers.
They achieve great performance inthis way with three efficient features.
The classifierbased methods can be easily used even the featureset changed.
However, as we proposed in Introduc-tion, it?s not the best way for such work.
We?ll detailthe learning to rank methods in the next section.3 Framework for Entity LinkingInput%a%queryOutput%the%final answer%Figure 2: The framework for entity linkingEntity linking is to align a named-entity men-tioned in a text to a corresponding entry stored inthe existing Knowledge Base.
We proposed a frame-work to solve the ?entity linking?
task.
As illustratedin Figure 2, when inputting a query which is an en-tity mentioned in a text, the system will return thetarget entry in Knowledge Base with four modules:1.
Query Processing.
First, we try to correct thespelling errors in the queries by using queryspelling correction supplied by Google.
Sec-ond, we expand the query in three ways: ex-panding acronym queries from the text wherethe entity is located, expanding queries with thecorresponding redirect pages of Wikipedia andexpanding queries by using the anchor text inthe pages from Wikipedia.2.
Candidates Generation.
With the queries gen-erated in the first step, the candidate genera-tion module retrieves the candidates from theKnowledge Base.
The candidate generationmodule also makes use of the disambiguationpages in Wikipedia.
If there is a disambigua-tion page corresponding to the query, the linkedentities listed in the disambiguation page areadded to the candidate set.3.
Candidates Ranking.
In the module, we rank allthe candidates with learning to rank methods.4.
Top1 Candidate Validation.
To deal with thosequeries without appropriate matching, we fi-nally add a validation module to judge whetherthe top one candidate is the target entry.The detail information of ranking module and val-idation module will be introduced in next two sec-tions.4 Learning to Rank CandidatesIn this section we first introduce the learning to rankmethods, and then describe the features for rankingmethods.4.1 Learning to rank methodsLearning to rank methods are popular in the area ofdocument retrieval.
There are mainly two types oflearning to rank methods: pairwise and listwise.
Thepairwise approach takes as instances object pairs ina ranking list for a query in learning.
In this way,it transforms the ranking problem to the classifica-tion problem.
Each pair from ranking list is labeledbased on the relative position or with the score of485ranking objects.
Then a classification model can betrained on the labeled data and then be used for rank-ing.
The pairwise approach has advantages in thatthe existing methodologies on classification can beapplied directly.
The listwise approach takes can-didate lists for a query as instances to train rankingmodels.
Then it trains a ranking function by min-imizing a listwise loss function defined on the pre-dicted list and the ground truth list.To describe the learning to rank methods, we firstintroduce some notations:?
Query set Q = {q(i)?i = 1 : m}.?
Each query q(i) is associated with a list of ob-jects(in document retrieval, the objects shouldbe documents), d(i) = {d(i)j ?j = 1 : n(i)}.?
Each object list has a labeled score list y(i) ={y(i)j ?j = 1 : n(i)} represents the relevance de-gree between the objects and the query.?
Features vectors x(i)j from each query-objectpair, j = 1 : n(i).?
Ranking function f, for each x(i)j it outputs ascore f(x(i)j ).
After the training phase, to rankthe objects, just use the ranking function f tooutput the score list of the objects, and rankthem with the score list.In the paper we will compare two different learn-ing to rank approaches: Ranking Perceptron for pair-wise and ListNet for listwise.
A detailed introduc-tion on Ranking Perceptron (Shen and Joshi, 2005)and ListNet (Cao et al, 2007) is given.4.1.1 Ranking PerceptronRanking Perceptron is a pairwise learning to rankmethod.
The score function f!
(x(i)j ) is defined as< !, x(i)j >.For each pair (x(i)j1 , x(i)j2 ), f!
(x(i)j1 ?
x(i)j2 ) is com-puted.
With a given margin function g(x(i)j1 , x(i)j2 ) anda positive rate  , if f!
(x(i)j1 ?
x(i)j2 ) ?
g(x(i)j1 , x(i)j2 ) ,an update is performed:!t+1 = !t + (x(i)j1 ?
x(i)j2 )g(x(i)j1 , x(i)j2 )After iterating enough times, we can use the func-tion f!
to rank candidates.4.1.2 ListNetListNet takes lists of objects as instances in learn-ing.
It uses a probabilistic method to calculate thelistwise loss function.ListNet transforms into probability distributionsboth the scores of the objects assigned by the ora-cle ranking function and the real score of the objectsgiven by human.Let  denote a permutation on the objects.
In List-Net alorithm, the probability of  with given scoresis defined as:Ps() =n?j=1exp(s(j))?nk=j exp(s(k))Then the top k probability of Gk(j1, j2, ..., jk) canbe calculated as:Ps(Gk(j1, j2, ..., jk)) =k?t=1exp(sjt)?ll=t exp(sjl)The ListNet uses a listwise loss function withCross Entropy as metric:L(y(i), z(i)) = ??
?g?GkPy(i)(g)log(Pz(i) (g))Denote as f!
the ranking function based onNeural Network model !.
The gradient ofL(y(i), z(i)(f!))
with respect to parameter !
can becalculated as:?!
= ?L(y(i), z(i)(f!))?
!= ???g?Gk?Pz(i)(f!)(g)?!Py(i)(g)Pz(i)(f!
)(g)In each iteration, the !
is updated with ?
??
!in a gradient descent way.
Here  is the learningrate.To train a learning to rank model, the manuallyevaluated score list for each query?s candidate list isrequired.
We assign 1 to the real target entity and 0to the others.4864.2 Features for RankingIn the section, we will introduce the features usedin the ranking module.
For convenience, we definesome symbols first:?
Q represents a query, which contains a namedentity mentioned in a text.
CSet represents thecandidate entries in Knowledge Base for thequery Q.
C represents a candidate in CSet.?
Q?s nameString represents the name string ofQ.
Q?s sourceText represents the source text ofQ.
Q?s querySet represents the queries whichare expansions of Q?s nameString.?
C?s title represents the title of correspondingWikipedia article of C. C?s titleExpand repre-sents the union set of the redirect set of C andthe anchor text set of C. C?s article representsthe Wikipedia article of C.?
C?s nameEntitySet represents the set of allnamed entities in C?s article labeled by Stan-ford NER (Finkel et al, 2005).
Q?s nameEnti-tySet represents the set of all named entities inQ?s sourceText.?
C?s countrySet represents the set of all coun-tries in C?s article, and we detect the countriesfrom text via a manual edited country list.
Q?scountrySet represents the set of all countriesin Q?s sourceText.
C?s countrySetInTitle rep-resents the set of countries exist in one of thestring s from C?s titleExpand.?
C?s citySetInTitle represents the set of all citiesexist in one of the string s from C?s titleExpand,and we detect the cities from text via a manualedited list of famous cities.
Q?s citySet repre-sents the set of all cities in Q?s sourceText.?
Q?s type represents the type of query Q.
It?s la-beled by Stanford NER.
C?s type is manuallylabeled already in the Knowledge Base.The features that used in the ranking module canbe divided into 3 groups: Surface, Context and Spe-cial.
Each of these feature groups will be detailednext.4.2.1 Surface FeaturesThe features in Surface group are used to measurethe similarity between the query string and candidateentity?s name string.?
StrSimSurface.
The feature value is the max-imum similarity between the Q?s nameStringand each string s in the set C?s titleExpand.
Thestring similarity is measured with edit distance.?
ExactEqualSurface.
The feature value is 1 ifthere is a string s in set C?s titleExpand same asthe Q?s nameString, or the Candidate C is ex-tracted from the disambiguation page.
In othercase, the feature value is set to 0.?
StartWithQuery.
The feature value is 1 if thereis a string s in set C?s titleExpand starting withthe Q?s nameString, and C?s ExactEqualSur-face is not 1.
In other case, the feature valueis set to 0.?
EndWithQuery.
The feature value is 1 if thereis a string s in set C?s titleExpand ending withthe Q?s nameString, and C?s ExactEqualSur-face is not 1.
In other case, the feature valueis set to 0.?
StartInQuery.
The feature value is 1 if there is astring s in set C?s titleExpand that s is the prefixof the Q?s nameString, and C?s ExactEqualSur-face is not 1.
In other case, the feature value isset to 0.?
EndInQuery.
The feature value is 1 if there is astring s in set C?s titleExpand that s is the post-fix of the Q?s nameString, and C?s ExactEqual-Surface is not 1.
In other case, the feature valueis set to 0.?
EqualWordNumSurface.
The feature value isthe maximum number of same words betweenthe Q?s nameString and each string s in the setC?s titleExpand.?
MissWordNumSurface.
The feature value isthe minimum number of different words be-tween the Q?s nameString and each string s inthe set C?s titleExpand.4874.2.2 Context FeaturesThe features in Context group are used to measurethe context relevance between query and the candi-date entity.
We mainly consider the TF-IDF similar-ity and named entity co-occurrence.?
TFSimContext.
The feature value is the TF-IDF similarity between the C?s article and Q?ssourceText.?
TFSimRankContext.
The feature value is theinverted rank of C?s TFSimContext in the CSet.?
AllWordsInSource.
The feature value is 1 if allwords in C?s title exist in Q?s sourceText, andin other case, the feature value is set to 0.?
NENumMatch.
The feature value is the num-ber of of same named entities between C?snameEntitySet and Q?s nameEntitySet.
Twonamed entities are judged to be the same if andonly if the two named entities?
strings are iden-tical.4.2.3 Special FeaturesBesides the features above, we also find that thefollowing features are quite significant in the entitylinking task: country names, city names and typesof queries and candidates.?
CountryInTextMatch.
The feature value is thenumber of same countries between C?s coun-trySet and Q?s countrySet.?
CountryInTextMiss.
The feature value is thenumber of countries that exist in Q?s country-Set but do not exist in C?s countrySet.?
CountryInTitleMatch.
The feature value is thenumber of same countries between C?s coun-trySetInTitle and Q?s countrySet.?
CountryInTitleMiss.
The feature value is thenumber of countries that exist in C?s country-SetInTitle but do not exist in Q?s countrySet.?
CityInTitleMatch.
The feature value is thenumber of same cities between C?s citySetInTi-tle and Q?s citySet.?
TypeMatch.
The feature value is 0 if C?s type isnot consistent with Q?s type, in other case, thefeature value is set to 1.When ranking the candidates in CSet, the fea-tures?
value was normalized into [0, 1] to avoid noisecaused by large Integer value or small double value.5 Top 1 Candidate ValidationTo deal with those queries without target entities inthe Knowledge Base, we supply a Top 1 candidatevalidation module.
In the module, a two-class classi-fier is used to judge whether the top one candidate isthe true target entity.
The top one candidate selectedfrom the ranking module can be divided into twoclasses: target and non-target, depending on whetherit?s the correct target link of the query.
Accordingto the performance of classification, SVM is chosenas the classifier (In practice, the libsvm package isused) and the SVM classifier is trained on the entiretraining set.Most of the features used in the validation mod-ule are the same as those in ranking module, such asStrSimSurface, EqualWordNumSurface, MissWord-NumSurface, TFSimContext, AllWordsInSource,NENumMatch and TypeMatch.
We also designsome other features, as follows:?
AllQueryWordsInWikiText.
The feature valueis one if Q?s textRetrievalSet contains C, andin other case the feature value is set to zero.The case that Q?s textRetrievalSet contains Cmeans the candidate C?s article contains the Q?snameString.?
CountryInTextPer.
The feature is the percent-age of countries from Q?s countrySet exist inC?s countrySet too.
The feature can be seen asa normalization of CountryInTextMatch/Missfeatures in ranking module.?
ScoreOfRank.
The feature value is the scoreof the candidate given by the ranking module.The ScoreOfRank takes many features in rank-ing module into consideration, so only fewerfeatures of ranking module are used in the clas-sifier.4886 Experiment and Analysis6.1 Experiment SettingAlgorithm Accuracy Improvementover SVMListNet 0.9045 +18.5%Ranking Perceptron 0.8842 +15.8%SVM 0.7636 -Perceptron 0.7546 -1.2%Table 1: Evaluation of different ranking algorithmEntity linking is initiated as a task in this year?sTAC-KBP3 track, so we use the data from this track.The entity linking task in the KBP track is to mapan entity mentioned in a news text to the Knowl-edge Base, which consist of articles from Wikipedia.The KBP track gives a sample query set which con-sists of 416 queries for developing.
The test set con-sists of 3904 queries.
2229 of these queries can?tbe mapped to Knowledge Base, for which the sys-tems should return NIL links.
The remaining 1675queries all can be aligned to Knowledge Base.
Wewill firstly analyze the ranking methods with thosenon-NIL queries, and then with an additional vali-dation module, we train and test with all queries in-cluding NIL queries.As in the entity linking task of KBP track, the ac-curacy is taken asaccuracy = #(correct answered queries)#(total queries)6.2 Evaluation of Machine Learning Methodsin rankingAs mentioned in the section of related work, learn-ing to rank methods in entity linking performs bet-ter than the classification methods.
To justify this,some experiments are designed to evaluate the per-formance of our ranking module when adopting dif-ferent algorithms.To evaluate the performance of the ranking mod-ule, we use all the queries which can be aligned to atarget entry in the Knowledge Base.
The training setcontains 285 valid queries and the test set contains1675.3http://apl.jhu.edu/ paulmac/kbp.htmlSet Features in SetSet1 Surface FeaturesSet2 Set1+TF-IDF FeaturesSet3 Set2+AllWordsInSourceSet4 Set3+NENumMatchSet5 Set4+CountryInTitle FeaturesSet6 Set5+CountryInText FeaturesSet7 Set6+CityInTitleMatchSet8 Set7+MatchTypeTable 2: Feature SetsThree algorithms are taken into comparison: List-Net, Ranking Perceptron, and classifier based meth-ods.
The classifier based methods are trained by di-viding the candidates into two classes: target andnon-target.
Then, the candidates are ranked accord-ing to their probability of being classified as target.two different classifiers are tested here, SVM andPerceptron.
!0.50.550.60.650.70.750.80.850.90.95Set1 Set2 Set3 Set4 Set5 Set6 Set7 Set8AccuracyFeature)SetListNetRanking!PerceptronFigure 3: Comparison of ListNet and Ranking PerceptronAs shown in Table 1, the two learning to rankmethods perform much better than the classificationbased methods.
The experiment results prove ourpoint that the learning to rank algorithms are moresuitable in this work.
And the ListNet shows slightimprovement over Ranking Perceptron, but since theimprovement is not so significant, maybe it dependson the feature set.
To confirm this, we compare thetwo algorithms with different features, as showedin Table 2.
In Figure 3, The ListNet outperformsRanking Perceptron with all feature sets except Set1,which indicates that the listwise approach is moresuitable than the pairwise approach.
The pairwiseapproach suffers from two problems: first, the ob-jective of learning is to minimize classification er-489Systems Accuracy of all queries Accuracy of non-NIL queries Accuracy of NIL queriesSystem1 0.8217 0.7654 0.8641System2 0.8033 0.7725 0.8241System3 0.7984 0.7063 0.8677ListNet+SVM 0.8494 0.79 0.8941Table 3: Evaluation of the overall performance, compared with KBP results (System 1-3 demonstrate the top threeranked systems)rors but not to minimize the errors in ranking; sec-ond, the number of pairs generated from list varieslargely from list to list, which will result in a modelbiased toward lists with more objects.
The issues arealso discussed in (Y.B.
Cao et al, 2006; Cao et al,2007).
And the listwise approach can fix the prob-lems well.As the feature sets are added incrementally, it canbe used for analyzing the importance of the featuresto the ranking task.
Although Surface Group onlytakes into consideration the candidate?s title and thequery?s name string, its accuracy is still higher than60%.
This is because many queries have quite smallnumber of candidates, the target entry can be pickedout with the surface features only.
The result showsthat after adding the TF-IDF similarity related fea-tures, the accuracy increases significantly to 84.5%.Although TF-IDF similarity is a simple way to mea-sure the contextual similarity, it performs well inpractice.
Another improvement is achieved whenadding the CountryInTitleMatch and CountryInTi-tleMiss features.
Since a number of queries in testset need to disambiguate candidates with differentcountries in their titles, the features about coun-try in the candidates?
title are quite useful to dealwith these queries.
But it doesn?t mean that thefeatures mentioned above are the most important.Because many features correlated with each otherquite closely, adding these features doesn?t lead toremarkable improvement.
The conclusion from theresults is that the Context Features significantly im-prove the ranking performance and the Special Fea-tures are also useful in the entity linking task.6.3 Overall Performance EvaluationWe are also interested in overall performance withthe additional validation module.
We use all the3904 queries as the test set, including both NILand non-NIL queries.
The top three results fromthe KBP track (McNamee and Dang, 2009) are se-lected as comparison.
The evaluation result in Table3 shows that our proposed framework outperformsthe best result in the KBP track, which demonstratesthe effectiveness of our methods.7 Conclusions and Future WorkThis paper demonstrates a framework of learning torank for linking entities with the Knowledge Base.Experimenting with different ranking algorithms, itshows that the learning to rank methods performmuch better than the classification methods in thisproblem.
ListNet achieves 18.5% improvement overSVM, and Ranking Perceptron gets 15.8% improve-ment over SVM.
We also observe that the listwiselearning to rank methods are more suitable for thisproblem than pairwise methods.
We also add a vali-dation module to deal with those entities which haveno corresponding entry in the Knowledge Base.
Wealso evaluate the proposed method on the whole dataset given by the KBP track, for which we add a bi-nary SVM classification module to validate the topone candidate.
The result of experiment shows theproposed strategy performs better than all the sys-tems participated in the entity linking task.In the future, we will try to develop more sophis-ticated features in entity linking and design a typicallearning to rank method for the entity linking task.AcknowledgmentsThis work was partly supported by the Chinese Nat-ural Science Foundation under grant No.60973104and No.
60803075, partly carried out with the aidof a grant from the International Development Re-search Center, Ottawa, Canada IRCI project fromthe International Development.490ReferencesBagga and Baldwin.
1998.
Entity-Based Cross-Document Coreferencing Using the Vector SpcaeModel.
in Proceedings of HLT/ACL.Gideon S. Mann and David Yarowsky.
2003.
Unsuper-vised Personal Name Disambiguation.
in Proceedingsof CONIL.Michael Ben Fleishman.
2004.
Multi-Document PersonName Resolution.
in Proceedings of ACL.Ted Pedersen, Amruta Purandare and Anagha Kulkarni.2005.
Name Discrimination by Clustering SimilarContexts.
in Proceedings of CICLing.B.Malin and E. Airoldi.
2005.
A Network AnalysisModel for Disambiguation of Names in Lists.
in Pro-ceedings of CMOT.Einat Minkov, William W. Cohen and Andrew Y. Ng.2006.
Contextual Search and Name Disambiguationin Email Using Graph.
in Proceedings of SIGIR.Ron Bekkerman and Andrew McCallum.
2005.
Disam-biguating Web Appearances of People in a Social Net-work.
in Proceedings of WWW.Xianpei Han and Jun Zhao.
2009.
Named Entity Disam-biguation by Leveraging Wikipedia Semantic Knowl-edge.
in Proceedings of CIKM.David Milne and Ian H. Witten.
2008.
Learning to Linkwith Wikipedia.
in Proceedings of CIKM.Herbrich, R., Graepel, T. and Obermayer K. 1999.
Sup-port vector learning for ordinal regression.
in Pro-ceedings of ICANN.Freund, Y., Iyer, R., Schapire, R. E. and Singer, Y.
1998.An efficient boosting algorithm for combining prefer-ences.
in Proceedings of ICML.Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds,M., Hamilton, N. and Hullender, G. 2005.
Learning torank using gradient descent.
in Proceedings of ICML.Cao, Y.
B., Xu, J., Liu, T. Y., Li, H., Huang, Y. L. andHon, H. W. 2006.
Adapting ranking SVM to documentretrieval.
in Proceedings of SIGIR.Cao, Z., Qin, T., Liu, T. Y., Tsai, M. F. and Li, H. 2007.Learning to rank: From pairwise approach to listwiseapproach.
in Proceedings of ICML.Qin, T., Zhang, X.-D., Tsai, M.-F., Wang, D.-S., Liu,T.Y., and Li, H. 2007.
Query-level loss functions forinformation retrieval.
in Proceedings of Informationprocessing and management.L.
Shen and A. Joshi.
2005.
Ranking and Reranking withPerceptron.
Machine Learning,60(1-3),pp.
73-96.Silviu Cucerzan.
2007.
Large-Scale Named Entity Dis-ambiguation Based on Wikipedia Data.
in Proceed-ings of EMNLP-CoNLL.Razvan Bunescu and Marius Pasca.
2006.
Using En-cyclopedic Knowledge for Named Entity Disambigua-tion.
in Proceedings of EACL.Paul McNamee and Hoa Dang.
2009.
Overviewof the TAC 2009 Knowledge Base Population Track(DRAFT).
in Proceedings of TAC.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating Non-local Informationinto Information Extraction Systems by Gibbs Sam-pling.
Proceedings of the 43nd Annual Meeting ofthe Association for Computational Linguistics (ACL2005), pp.
363-370.491
