Head Corner Parsing for Discontinuous ConstituencyGertjan van NoordLehrstuhl ffir ComputerlinguistikUniversit~t des SaarlandesIm Stadtwald 15D-6600 Saarbrficken 11, FRGvannoord@coli.uni-sb.deAbstractI describe ahead-driven parser for a class of gram-mars that handle discontinuous constituency by aricher notion of string combination than ordinaryconcatenation.
The parser is a generalization ofthe left-corner parser (Matsumoto et al, 1983)and can be used for grammars written in power-ful formalisms uch as non-concatenative versionsof HPSG (Pollard, 1984; Reape, 1989).1 In t roduct ionAlthough most formalisms in computational lin-guistics assume that phrases are built by stringconcatenation (eg.
as in PATR II, GPSG, LFGand most versions of Categorial Grammar), thisassumption is challenged in non-concatenativegrammatical formalisms.
In Pollard's dissertationseveral versions of 'qlead wrapping" are defined(Pollard, 1984).
In the analysis of the Australianfree word-order language Guugu Yimidhirr, MarkJohnson uses a 'combine' predicate in a DCG-likegrammar that corresponds to the union of words(Johnson, 1985).Mike Reape uses an operation called 'sequenceunion' to analyse Germanic semi-free word or-der constructions (l~ape, 1989; Reape, 1990a).Other examples include Tree Adjoining Gram-mars (Joshi et al, 1975; Vijay-Shankar andJoshi, 1988), and versions of Categorial Gram-mar (Dowry, 1990) and references cited there.Mot ivat ion.
There are several motivations fornon-concatenative grammars.
First, specializedstring combination operations allow elegant lin-guistic accounts of phenomena that are otherwisenotoriously hard.
Examples are the analyses ofDutch cross serial dependencies by head wrap-ping or sequence union (Reape, 1990a).Furthermore, in non-concatenative grammarsit is possible to relate (parts of) constituents hatbelong together semantically, but which are notadjacent.
Hence such grammars facilitate a sim-ple compositional semantics.
In CF-based gram-mars such phenomena usually are treated by com-plex 'threading' mechanisms.Non-concatenative grammatical formalismsmay also be attractive from a computationalpoint of view.
It is easier to define generationalgorithms if the semantics i built in a systemat-ically constrained way (van Noord, 1990b).
Thesemantic-head-driven generation strategy (vanNoord, 1989; Calder ef al., 1989; Shieber et al,1989; van Noord, 1990a; Shieber et al, 1990)faces problems in case semantic heads are 'dis-placed', and this displacement is analyzed us-ing threading.
However, in this paper I sketcha simple analysis of verb-second (an example ofa displacement of semantic heads) by an oper-ation similar to head wrapping which a head-driven generator processes without any problems(or extensions) at all.
Clearly, there are also somecomputational problems, because most 'standard'parsing strategies assume context-free concatena-tion of strings.
These problems are the subject ofthis paper.The task.
I will restrict the attention to aclass of constraint-based formalisms, in whichoperations on strings are defined that are morepowerful than concatenation, but which opera-tions are restricted to be nonerasing, and linear.The resulting class of systems can be character-ized as Linear Context-Free Rewriting Systems114(LCFRS), augmented with feature-structures (F-LCFRS).
For a discussion of the properties ofLCFRS without feature-structures, see (Vijay-Shanker et al, 1987).
Note though that theseproperties do not carry over to the current sys-tem, because of the augmention with featurestructures.As in LCFRS, the operations on strings in F-LCFRS can be characterized as follows.
First,derived structures will be mapped onto a set ofoccurances of words; i.e.
each derived structure'knows' which words it 'dominates'.
For example,each derived feature structure may contain an at-tribute 'phon' whose value is a list of atoms repre-senting the string it dominates.
I will write w(F)for the set of occurances of words that the derivedstructure F dominates.
Rules combine structuresD1 ...  Dn into a new structure M. Nonerasure re-quires that the union of w applied to each daugh-ter is a subset of w(M):}IU w(Di) C_ w(M)i= lLinearity requires that the difference of the car-dinalities of these sets is a constant factor; i.e.
arule may only introduce a fixed number of wordssyncategorematically:Iw(M)l- I U w(Oi)) = c,c a constanti=1CF-based formalisms clearly fulfill this require-ment, as do Head Grammars, grammars usingsequence union, and TAG's.
I assume in the re-mainder of this paper that I.Jin=l w(Di) = w(M),for all rules other than lexical entries (i.e.
allwords are introduced on a terminal).
Note thoughthat a simple generalization of the algorithm pre-sented below handles the general case (along thelines of Shieber et al (1989; 1990)by treatingrules that introduce xtra lexical material as non-chain-rules).Furthermore, I will assume that each rule has adesignated aughter, called the head.
AlthoughI will not impose any restrictions on the head, itwill turn out that the parsing strategy to be pro-posed will be very sensitive to the choice of heads,with the effect that F-LCFRS's in which the no-tion 'head' is defined in a systematic way (Pol-lard's Head Grammars, Reape's version of HPSG,Dowty's version of Categorial Grammar), may bemuch more efficiently parsed than other gram-mars.
The notion seed of a parse tree is definedrecursively in terms of the head.
The seed of atree will be the seed of its head.
The seed of aterminal will be that terminal itself.Other  approaches.
In (Proudian and Pollard,1985) a head-driven algorithm based on activechart parsing is described.
The details of the al-gorithm are unclear from the paper which makesa comparison with our approach ard; it is notclear whether the parser indeed allows for ex-ample the head-wrapping operations of Pollard(1984).
Reape presented two algorithms (Reape,1990b) which are generalizations of a shift-reduceparser, and the CKY algorithm, for the same classof grammars.
I present a head-driven bottom-upalgorithm for F-LCFR grammars.
The algorithmresembles the head-driven parser by Martin Kay(Kay, 1989), but is generalized in order to be usedfor this larger class of grammars.
The disadvan-tages Kay noted for his parser do not carry overto this generalized version, as redundant searchpaths for CF-based grammars turn out to be gen-uine parts of the search space for F-LCFR gram-mars.The advantage of my algorithm is that it bothemploys bottom-up and top-down filtering in astraightforward way.
The algorithm is closely re-lated to head-driven generators (van Noord, 1989;Calder et al, 1989; Shieber et al, 1989; van No-ord, 1990a; Shieber et ai., 1990).
The algorithmproceeds in a bottom-up, head-driven fashion.
Inmodern linguistic theories very much informationis defined in lexical entries, whereas rules are re-duced to very general (and very uninformative)schemata.
More information usually implies lesssearch space, hence it is sensible to parse bottom-up in order to obtain useful information as soonas possible.
Furthermore, in many linguistic the-ories a special daughter called the head deter-mines what kind of other daughters there may be.Therefore, it is also sensible to start with the headin order to know for what else you have to lookfor.
As the parser proceeds from head to head itis furthermore possible to use powerful top-downpredictions based on the usual head feature per-colations.
Finally note that proceding bottom-upsolves ome non-termination problems, because inlexicalized theories it is often the case that infor-mation in lexical entries limit the recursive appli-cation of rules (eg.
the size of the subcat list of115an entry determines the depth of the derivationtree of which this entry can be the seed).Before I present he parser in section 3, I willfirst present an example of a F-LCFR grammar,to obtain a flavor of the type of problems theparser handles reasonably well.2 A sample grammarIn this section I present a simple F-LCFR gram-mar for a (tiny) fragment of Dutch.
As a caveatI want to stress that the purpose of the currentsection is to provide an example of possible inputfor the parser to be defined in the next section,rather than to provide an account of phenomenathat is completely satisfactory from a linguisticpoint of view.Grammar rules are written as (pure) Prologclauses.
1 Heads select arguments using a sub-cat list.
Argument structures are specified lexi-cally and are percolated from head to head.
Syn-tactic features are shared between heads (henceI make the simplifying assumption that head -functor, which may have to be revised in orderto treat modification).
In this grammar I userevised versions of Pollard's head wrapping op-erations to analyse cross serial dependency andverb second constructions.
For a linguistic back-ground of these constructions and analyses, cf.Evers (1975), Koster (1975) and many others.Rules are defined asru le(Head,Mother ,Other)or  ~srule(Mother)(for lexical entries), where Head represents thedesignated head daughter, Mother the mothercategory and Other a list of the other daughters.Each category is a termx(Syn,Subcat,Phon,Sem,Rule)where Syn describes the part of speech, Subcat1 It should be stressed though that other unificationgrammar formalisms can be extended quite easily to en-code the same grammar.
I implemented the algorithm forseveral grammars written in a version of PATR I I  withoutbuilt-in string concate~aation.is a list of categories a category subcategorizesfor, Phon describes the string that is dominatedby this category, and Sere is the argument struc-ture associated with this category.
Rule indicateswhich rule (i.e.
version of the combine predicateeb to be defined below) should be applied; it gen-eralizes the 'Order' feature of UCG.
The value ofPhon is a term p(Left ,Head,R?ght)  where thefields in this term are difference lists of words.The first argument represents he string left of thehead, the second argument represents the headand the third argument represents he string rightof the head.
Hence, the string associated withsuch a term is the concatenation f the three ar-guments from left to right.
There is only one pa-rameterized, binary branching, rule in the gram-mar:rule(x(Syn,\[x(C,L,P2,S,R)\[L\],PI,Sem,_),x(Syn,L,P,Sem,_),\ [x(C,L ,P2,S,R) \ ] )  : -cb(R, PI,  P2, P).In this rule the first element of the subcategoriza-tion list of the head is selected as the (only) otherdaughter of the mother of the rule.
The syntac-tic and semantic features of the mother and thehead are shared.
Furthermore, the strings associ-ated with the two daughters of the rule are to becombined by the cb predicate.
For simple (left orright) concatenation this predicate is defined asfollows:cb(left, p(L4-L.H,R),p(L1-L2,L2-L3,L3-L4),p(L1-L,H,R)).cb( r ight ,  p(L,H,RI-R2),p(R2-R3,R3-R4,R4-R),p(L,H,RI-R)).Although this looks horrible for people not famil-iar with Prolog, the idea is really very simple.In the first case the string associated with theargument is appended to the left of the stringleft of the head; in the second case this string isappended to the right of the string right of thehead.
In a friendlier notation the examples maylook like:116p(A1.A2.A3-L,H, R)/ \p(L,H,R) p(A1,A2,A3)p(L, H. R-A1.A2.A3)/ \p(L,H,R) p(A1,A2,A3)Lexical entries for the intransitive verb 'slaapt'(sleeps) and the transitive verb 'kust' (kisses) aredefined as follows:rule( x(v,\[x(n, \[\] , _ ,A , le f t ) \ ] ,p(P-P, \ [s laapt lT \ ] -T ,R-R) ,s leep(A) ,_ ) ) .rule( x(v, \[x(n, \[\] ,_ ,B, left ) ,x(n, \[\] , _ ,A , le f t ) \ ] ,p (P-P, \[kust I T\]-T, R-R),kiss(A,B),_)).Proper nouns are defined as:rule( x(n, \[\] ,p(P-P, \ [pier \[T\]-T,R-R),pete,_)).and a top category is defined as follows (comple-mentizers that have selected all arguments, i.e.sentences):top(x(comp,\[\] .
.
.
.
.
.  ))
.Such a complementizer, g. 'dat' (that) is definedas :rule( x(comp, Ix(v, \[\] , _ ,A , r ight ) \ ] ,p(P-P, \[dat I T\]-T, R-R),that  (A), _) ).The choice of datastructure for the value ofPhon allows a simple definition of the verb raising(vr) version of the combine predicate that may beused for Dutch cross serial dependencies:cb(vr,  p(L1-L2,H,R3-R),p(L2-L,R1-R2,R2-R3),p(L1-L,H,R1-R)).Here the head and right string of the argumentare appended to the right, whereas the left stringof the argument is appended to the left.
Again,an illustration might help:p(L-AI , II, A2.A3.R)/ \p(L,li,X) p(A1,A2,A3)A raising verb, eg.
'ziet' (sees) is defined as:ru le (x (v , \ [x (n ,  \[\] , _ , In fSub j , le f t ) ,x ( in f , \ [x (  .
.
.
.
.
.
InfSubj ,_)\ ] ,_ ,B,vr) ,x(n, \[\] , _ ,A , le f t ) \ ] ,p(P-P, \ [z iot IT \ ] -T ,R-R) ,see(A,B),_)).In this entry 'ziet' selects - -  apart from its np-subject - -  two objects, a np and a VP (with cat-egory inf) .
The in f  still has an element in itssubcat list; this element is controlled by the np(this is performed by the sharing of InfSubj).
Toderive the subordinate phrase 'dat jan piet marieziet kussen' (that john sees pete kiss mary), themain verb 'ziet' first selects its rip-object 'piet'resulting in the string 'piet ziet'.
Then it selectsthe infinitival 'marie kussen'.
These two stringsare combined into 'piet marie ziet kussen' (usingthe vr version of the cb predicate).
The subjectis selected resulting in the string 'jan pier marieziet kussen'.
This string is selected by the com-plementizer, esulting in 'dat jan piet marie zietkussen'.
The argument structure will be instan-tiated as that  (sees (j elm, kiss  (pete,  mary))).In Dutch main clauses, there usually is no overtcomplementizer; instead the finite verb occupiesthe first position (in yes-no questions), or thesecond position (right after the topic; ordinarydeclarative sentences).
In the following analysisan empty complementizer selects an ordinary (fi-nite) v; the resulting string is formed by the fol-lowing definition of ?b:cb(v2, p(A-A,B-B,C-C),p(R1-R2,H,R2-R),p(A-A,H,RI-R)).which may be illustrated with:117p( \ [ \ ] ,  A2, A1.A3)/ \p(\[l, \[\], \[\]) p(A1,A2,A3)The finite complementizer is defined as:xatle(xCcomp, \[xCv, FI ,_,A,v2)\],p(B-B,C-C,D-D),that (A),_)).Note that this analysis captures the special rela-tionship between complementizers and (fronted)finite verbs in Dutch.
The sentence 'ziet jan pietmarie kussen' is derived as follows (where thehead of a string is represented in capitals):inversion: ZIET jan piet marie kussen/ \e left: jan piet marie ZIET kussen/ \raising: piet marie ZIET kussen JAN/ \left: piet ZIET left: marie KUSSEN/ \  / \ZIET PIET KUSSEN MARIE3 The  head corner  parserThis section describes the head-driven parsingalgorithm for the type of grammars describedabove.
The parser is a generalization of a left-corner parser.
Such a parser, which may be calleda 'head-corner' parser, ~proceeds in a bottom-upway.
Because the parser proceeds from head tohead it is easy to use powerful top-down pre-dictions based on the usual head feature perco-lations, and subcategorization requirements hatheads require from their arguments.In left-corner parsers (Matsumoto et aL, 1983)the first step of the algorithm is to select he left-2This name is due to Pete White.lock.most word of a phrase.
The parser then proceedsby proving that this word indeed can be the left-corner of the phrase.
It does so by selecting a rulewhose leftmost daughter unifies with the categoryof the word.
It then parses other daughters of therule recursively and then continues by connectingthe mother category of that rule upwards, recur-sively.
The left-corner algorithm can be general-ized to the class of grammars under considerationif we start with the seed of a phrase, instead of itsleftmost word.
Furthermore the connect predi-cate then connects maller categories upwards byunifying them with the head of a rule.
The firststep of the algorithm consists of the predictionstep: which lexical entry is the seed of the phrase?The first thing to note is that the words intro-duced by this lexical entry should be part of theinput string, because of the nonerasure require-ment (we use the string as a 'guide' (Dymetmanef al., 1990) as in a left-corner parser, but wechange the way in which lexical entries 'consumethe guide').
Furthermore in most linguistic theo-ries it is assumed that certain features are sharedbetween the mother and the head.
I assume thatthe predicate head/2 defines these feature perco-lations; for the grammar of the foregoing sectionthis predicate may be defined as:head(x(Syn .
.
.
.
.
Sent,_),x(ST-  .
.
.
.
.
Sn,_)).As we will proceed from head to head these fea-tures will also be shared between the seed andthe top-goal; hence we can use this definition torestrict lexical lookup by top-down prediction.
3The first step in the algorithm is defined as:parse(Cat,PO,P) "-predict_lex(Cat,SmallCat,PO,P1),connect(SmallCat,Cat,P1,P).pred ic t_ lex(Cat ,Smal lCat ,P0 ,P )  : -head(Cat,Sma11Cat),rule(SmallCat),string(SmallCat,Words),subset(Words,PO,P).Instead of taking the first word from the currentinput string, the parser may select a lexical en-3In the general case we need to compute the transitiveclosure of (restrictions of) pcesible mother-head relation-ships.
The predicate 'head may also be used to compilerules into the format adopted here (i.e.
using the defini-tion the compiler will identify the head of a rule).118try dominating a subset of the words occuring inthe input string, provided this lexical entry canbe the seed of the current goal.
The predicatesubset(L1,L2,L3) is true in case L1 is a subsetof L2 with complement L3.
4The second step of the algorithm, the connectpart, is identical to the connect part of the left-corner parser, but instead of selecting the left-most daughter of a rule the head-corner parserselects the head of a rule:connect(X,X,P,P).connect (Smal l ,B ig ,PO,P)  : -rule(Small,  Mid, Others),parse_rest(Others,PO,Pl),connect(Mid,Big,PI,P).parse_rest( \[\] ,P,P).parse_rest(\[HlT\],PO,P) : -parse(H,PO,P1),parse_rest(T,P1,P).The predicate ' s tar t_parse '  starts the parse pro-cess, and requires furthermore that the string as-sociated with the category that has been foundspans the input string in the right order.s tar t_parse  (St r ing ,  Cat) : -top(Cat),parse (Cat, Str ing ,  \[\] ),string(Cat, String).The definition of the predicate ' s t r ing '  dependson the way strings are encoded in the grammar.The predicate relates linguistic objects and thestring they dominate (as a list of words).
I assumethat each grammar provides a definition of thispredicate.
In the current grammar s t r ing /2  isdefined as follows:4In Prolog this predicate may he defined as follows:subset( \ [ \ ] ,P ,P) .subset( \ [H IT \ ] ,P0,P) : -selectchk(H, P0,Pl),subset(T, PI,P).select.chk (El, \[El IP\] ,P) :-!.select_chk (El, \[HIP0\], \[HIP\] ) :-select.chk (El, P0, P) .The cut in select.chkls necessary in case the same wordoccurs twice in the input  string; without it the parserwould not  be 'minima/' ;  this could be changed by index-ins words w.r.t, their position, hut  I will not assume thiscomplication here,s t r ing(x (  .
.
.
.
Phon .
.
.
.  )
,S t r ) : -copy_term(Phon,Phon2),s t r (Phon2,St r ) .str(p(P-P1,P1-P2,P2-\ [ \ ] ) ,P) .This predicate is complicated using the predi-cate copy_term/2 to prevent any side-effects tohappen in the category.
The parser thus needstwo grammar specific predicates: head/2 ands t r ing /2 .Example.
To parse the sentence 'dat janslaapt', the head corner parser will proceed asfollows.
The first call to 'parse' will look like:parse (x(colap, \[\] .
.
.
.
.
.  )
,\[dat, j an, s laapt\] ,  \[\] )The prediction step selects the lexical entry 'dat'.The next goal is to show that this lexical entry isthe seed of the top goal; furthermore the stringthat still has to be covered is now \ [ jan ,s laapt \ ] .Leaving details out the connect clause looks as :connect (x(comp, Ix(v, .
.
, r ight ) \ ] , .
.
),x(comp, 17,..  ), \[jan, slaapt\] ,  \[\] )The category of dat has to be matched withthe head of a rule.
Notice that dat subcatego-rises for a v with rule feature r ight .
Hence ther ight  version of the cb predicate applies, and thenext goal is to parse the v for which this comple-mentizer subcategorizes, with input 'jan, slaapt'.Lexical lookup selects the word s laapt  from thisstring.
The word s laapt  has to be shown to bethe head of this v node, by the connect predi-cate.
This time the le f t  combination rule appliesand the next goal consists in parsing a np (forwhich s laapt  subcategorizes) with input stringjan.
This goal succeeds with an empty outputstring.
Hence the argument of the rule has beenfound successfully and hence we need to connectthe mother of the rule up to the v node.
This suc-ceeds trivially, and therefore we now have foundthe v for which dat subcategorizes.
Hence thenext goal is to connect he complementizer withan empty subcat list up to the topgoal; again thissucceeds trivially.
Hence we obtain the instanti-ated version of the parse call:119parse(x(comp, \[\] ,p(P-P,  \[dat IT\]-T,\ [ jan ,s laapt  \ [q \ ] -q) ,that  (s leeps ( j  ohn) ) , _ ) ,\[dat, j an, slaapt\], O )and the predicate start_parse will succeed,yielding:Cat = x(comp, \[\] ,p(P-P,  \[dat \[T\]-T,\ [ jan,  s laapt  IQ\]-q),that (sleeps (john) ) ,  _)4 Discussion and ExtensionsSound and Complete .
The algorithm as it isdefined is sound (assuming the Prolog interpreteris sound), and complete in the usual Prolog sense.Clearly the parser may enter an infinite loop (incase non branching rules are defined that mayfeed themselves or in case a grammar makes aheavy use of empty categories).
However, in casethe parser does terminate one can be sure that ithas found all solutions.
Furthermore the parser isminimal in the sense that it will return one solu-tion for each possible derivation (of course if sev-eral derivations yield identical results the parserwill return this result as often as there are deriva-tions for it).Efficiency.
The parser turns out to be quite ef-ficient in practice.
There is one parameter thatinfluences efficiency quite dramatically.
If the no-tion 'syntactic head' implies that much syntac-tic information is shared between the head of aphrase and its mother, then the prediction stepin the algorithm will be much better at 'predict-ing' the head of the phrase.
If on the other handthe notion 'head' does not imply such feature per-colations, then the parser must predict the headrandomly from the input string as no top-downinformation is available.Improvements .
The efficiency of the parsercan be improved by common Prolog and parsingtechniques.
Firstly, it is possible to compile thegrammar rules, lexical entries and parser a bit fur-ther by (un)folding (eg.
the string predicate canbe applied to each lexical entry in a compilationstage).
Secondly it is possible to integrate well-formed and non-well-formed subgoal tables in theparser, following the technique described by Mat-sumoto et al (1983).
The usefulness of this tech-nique strongly depends on the actual grammarsthat are being used.
Finally, the current indexingof lexical entries is very bad indeed and can easilybe improved rastically.In some grammars the string operations thatare defined are not only monotonic with respectto the words they dominate, but also with respectto the order constraints that are defined betweenthese words ('order-monotonic').
For examplein Reape's sequence union operation the linearprecedence constraints that are defined betweenelements of a daughter are by definition part ofthe linear precedence constraints of the mother.Note though that the analysis of verb second inthe foregoing section uses a string operation thatdoes not satisfy this restriction.
For grammarsthat do satisfy this restriction it is possible to ex-tend the top-down prediction possibilities by theincorporation of an extra clause in the 'connect'predicate which will check that the phrase thathas been analysed up to that point can become asubstring of the top string.AcknowledgementsThis research was partly supported by SFB 314,Project N3 BiLD; and by the NBBI via the Eu-rotra project.I am grateful to Mike Reape for useful com-ments, and an anonymous reviewer of ACL, forpointing out the relevance of LCFRS.BibliographyJonathan Calder, Mike Reape, and Henk Zeevat.An algorithm for generation i  unification cat-egorial grammar.
In Fourth Conference of theEuropean Chapter of the Association for Com-putational Linguistics, pages 233-240, Manch-ester, 1989.David Dowty.
Towards a minimalist theory ofsyntactic structure.
In Proceedings of the Sym-posium on Discontinuous Constituency, ITKTilburg, 1990.Marc Dymetman, Pierre Isabelle, and FrancoisPerrault.
A symmetrical approach to parsing120and generation.
In Proceedings of the 13th In-ternational Conference on Computational Lin-guistics (COLING), Helsinki, 1990.Arnold Evers.
The Transformational Cycle inDutch and German.
PhD thesis, Rijksuniver-siteit Utrecht, 1975.Mark Johnson.
Parsing with discontinuousconstituents.
In 23th Annual Meeting ofthe Association for Computational Linguistics,Chicago, 1985.A.K.
Joshi, L.S.
Levy, and M. Takahashi.
Treeadjunct grammars.
Journal Computer SystemsScience, 10(1), 1975.Martin Kay.
ttead driven parsing.
In Proceedingsof Workshop on Parsing Technologies, Pitts-burgh, 1989.Jan Koster.
Dutch as an SOV language.
Linguis-tic Analysis, 1, 1975.Y.
Matsumoto, H. Tanaka, It.
Itirakawa,It.
Miyoshi, and H. Yasukawa.
BUP: a bottomup parser embedded in Prolog.
New Genera-tion Computing, 1(2), 1983.Carl Pollard.
Generalized Context-Free Gram-mars, Head Grammars, and Natural Language.PhD thesis, Stanford, 1984.C.
Proudian and C. Pollard.
Parsing head-drivenphrase structure grammar.
In P3th AnnualMeeting of the Association for ComputationalLinguistics, Chicago, 1985.Mike Reape.
A logical treatment .of semi-freeword order and bounded discontinuous con-stituency.
In Fourth Conference of the Euro-pean Chapter of the Association for Computa-tional Linguistics, UMIST Manchester, 1989.Mike Reape.
Getting things in order.
In Proceed-ings of the Symposium on Discontinuous Con-stituency, ITK Tilburg, 1990.Mike Reape.
Parsing bounded iscontinous con-stituents: Generalisations of the shift-reduceand CKY algorithms, 1990.
Paper presentedat the first CLIN meeting, October 26, OTSUtrecht.Stuart M. Shieber, Gertjan van Noord, Robert C.Moore, and Fernando C.N.
Pereira.
Asemantic-head-driven g eration algorithm forunification based formalisms.
In 27th AnnualMeeting of the Association for ComputationalLinguistics, Vancouver, 1989.Stuart M. Shieber, Gertjan van Noord, Robert C.Moore, and Fernando C.N.
Pereira.
Semantic-head-driven generation.
Computational Lin-guistics, 16(1), 1990.Gertjan van Noord.
BUG: A directed bottom-up generator for unification based formalisms.Working Papers in Natural Language Process-ing, Katholieke Universiteit Leuven, StichtingTaaltechnologie Utrecht, 4, 1989.Gertjan van Noord.
An overview of head-driven bottom-up generation.
In Robert Dale,Chris Mellish, and Michael Zoek, editors, Cur-rent Research in Natural Language Generation.Academic Press, 1990.Gertjan van Noord.
Reversible unifieation-basedmachine translation.
In Proceedings of the18th International Conference on Computa-tional Linguistics (COLING), Helsinki, 1990.K.
Vijay-Shankar and A. Joshi.
Feature struc-ture based tree adjoining grammar.
In Pro-ceedings of the 12th International Conferenceon Computational Linguistics (COLING), Bu-dapest, 1988.K.
Vijay-Shanker, David J. Weir, and Aravind K.Joshi.
Characterizing structural descriptionsproduced by various grammatical formalisms.In 25th Annual Meeting of the Association forComputational Linguistics, Stanford, 1987.121
