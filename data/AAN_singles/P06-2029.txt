Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 223?230,Sydney, July 2006. c?2006 Association for Computational LinguisticsThe Benefit of Stochastic PP Attachment to a Rule-Based ParserKilian A. Foth and Wolfgang MenzelDepartment of InformaticsHamburg UniversityD-22527 HamburgGermanyfoth|menzel@nats.informatik.uni-hamburg.deAbstractTo study PP attachment disambiguation asa benchmark for empirical methods in nat-ural language processing it has often beenreduced to a binary decision problem (be-tween verb or noun attachment) in a par-ticular syntactic configuration.
A parser,however, must solve the more general taskof deciding between more than two alter-natives in many different contexts.
Wecombine the attachment predictions madeby a simple model of lexical attractionwith a full-fledged parser of German to de-termine the actual benefit of the subtaskto parsing.
We show that the combinationof data-driven and rule-based componentscan reduce the number of all parsing errorsby 14% and raise the attachment accuracyfor dependency parsing of German to anunprecedented 92%.1 IntroductionMost NLP applications are either data-driven(classification tasks are solved by comparing pos-sible solutions to previous problems and their so-lutions) or rule-based (general rules are formu-lated which must be applicable to all cases thatmight be encountered).
Both methods face obvi-ous problems: The data-driven approach is at themercy of its training set and cannot easily avoidmistakes that result from biased or scarce data.
Onthe other hand, the rule-based approach dependsentirely on the ability of a computational linguistto anticipate every construction that might ever oc-cur.
These handicaps are part of the reason why,despite great advances, many tasks in computa-tional linguistics still cannot be performed nearlyas well by computers as by human informants.Applied to the subtask of syntax analysis, the di-chotomy manifests itself in the existence of learntand handwritten grammars of natural languages.A great many formalisms have been advanced thatfall into either of the two variants, but even thebest of them cannot be said to interpret arbitraryinput consistently in the same way that a humanreader would.
Because the handicaps of differ-ent methods are to some degree complementary,it seems likely that a combination of approachescould yield better results than either alone.
Wetherefore integrate a data-driven classifier for thespecial task of PP attachment into an existing rule-based parser and measure the effect that the addi-tional information has on the overall accuracy.2 MotivationPP attachment disambiguation has often beenstudied as a benchmark test for empirical meth-ods in natural language processing.
Prepositionsallow subordination to many different attachmentsites, and the choice between them is influencedby factors from many different linguistic levels,which are generally subject to preferential ratherthan rigorous regularities.
For this reason, PP at-tachment is a comparatively difficult subtask forrule-based syntax analysis and has often been at-tacked by statistical methods.Because probabilistic approaches solve PP at-tachment as a natural subtask of parsing anyhow,the obvious application of a PP attacher is to in-tegrate it into a rule-based system.
Perhaps sur-prisingly, so far this has rarely been done.
Onereason for this is that many rule-driven syntax an-alyzers provide no obvious way to integrate un-certain, statistical information into their decisions.Another is the traditional emphasis on PP attach-ment as a binary classification task; since (Hin-dle and Rooth, 1991), research has concentratedon resolving the ambiguity in the category pattern?V+N+P+N?, i.e.
predicting the PP attachment toeither the verb or the first noun.
It is often assumedthat the correct attachment is always among these223two options, so that all problem instances can besolved correctly despite the simplification.
Thistask is sufficient to measure the relative quality ofdifferent probability models, but it is quite differ-ent from what a parser must actually do: It is easierbecause the set of possible answers is pre-filteredso that only a binary decision remains, and thebaseline performance for pure guessing is already50%.
But it is harder because it does not pro-vide the predictor with all the information neededto solve many doubtful cases; (Hindle and Rooth,1991) found that human arbiters consistently reacha higher agreement when they are given the entiresentence rather than just the four words concerned.Instead of the accuracy of PP attachers in theisolated decision between two words, we investi-gate the problem of situated PP attachment.
In thistask, all nouns and verbs in a sentence are potentialattachment points for a preposition; the computermust find suitable attachments for one or moreprepositions in parallel, while building a globallycoherent syntax structure at the same time.3 MethodsStatistical PP attachment is based on the obser-vation that the identities of content words can beused to predict which prepositional phrases mod-ify which words, and achieve better-than-chanceaccuracy.
This is apparently because, as headsof their respective phrases, they are representativeenough that they can serve as a crude approxima-tion of the semantic structure that could be derivedfrom the phrases.
Consider the following example(the last sentence in our test set):Die Firmen mu?ssen noch die Bedenken der EU-Kommission gegen die Fusion ausra?umen.
(The compa-nies have yet to address the Commission?s concerns aboutthe merger.
)In this sentence, the preferred analysis will pairthe preposition ?gegen?
(against, about, versus)with the noun ?Bedenken?
(concerns), since theproposition is clearly that the concerns pertain tothe merger.
A syntax tree of this interpretation isshown in Figure 1.
Note that there are at leastthree different syntactically plausible attachmentsites for the preposition.
In fact, there are evenmore, since a parser can make no initial assump-tions about the global structure of the syntax treethat it will construct; for instance, the possibilitythat ?gegen?
attaches to the noun ?Firmen?
(compa-nies) cannot be ruled out when beginning to parse.3.1 WCDGFor the following experiments, we used the de-pendency parser of German described in (Foth etal., 2005).
This system is especially suited toour goals for several reasons.
Firstly, the parserachieves the highest published dependency-basedaccuracy on unrestricted written German input,but still has a comparatively high error rate forprepositions.
In particular, it mis-attaches thepreposition ?gegen?
in the example sentence.
Sec-ond, although rule-based in nature, it uses numer-ical penalties to arbitrate between different disam-biguation rules.
It is therefore easy to add anotherrule of varying strength, which depends on theoutput of an external statistical predictor, to guidethe parser when it has no other means of makingan attachment decision.
Finally, the parser andgrammar are freely available for use and modi-fication (http://nats-www.informatik.uni-hamburg.de/download).Weighted Constraint Dependency Grammar(Schro?der, 2002) models syntax structure as la-belled dependency trees as shown in the exam-ple.
A grammar in this formalism is written asa set of constraints that license well-formed par-tial syntax structures.
For instance, general projec-tivity rules ensure that the dependency tree corre-sponds to a properly nested syntax structure with-out crossing brackets1 .
Other constraints requirean auxiliary verb to be modified by a full verb, orprescribe morphosyntactical agreement between adeterminer and its regent (the word modified bythe determiner).
Although the Constraint Satisfac-tion Problem that this formalism defines is, in the-ory, infeasibly hard, it can nevertheless be solvedapproximatively with heuristic solution methods,and achieve competitive parsing accuracy.To allow the resolution of true ambiguity (theexistence of different structures neither of which isstrictly ungrammatical), weighted constraints canbe written that the solution should satisfy, if thisis possible.
The goal is then to build the struc-ture that violates as few constraints as possible,and preferentially violates weak rather than strongconstraints.
This allows preferences to be ex-pressed rather than hard rules.
For instance, agree-ment constraints could actually be declared as vio-lable, since typing errors, reformulations, etc.
can1Some constructions of German actually violate this prop-erty; exceptions in the projectivity constraints deal with thesecases.224AUXPNDETPPGMODDETOBJADETADVSSUBJDETdietheFirmencompaniesm?ssenhave tonochyetdietheBedenkenconcernsdertheEU-KommissionEuropean commissiongegenaboutdietheFusionmergerausr?umenaddress.Figure 1: Correct syntax analysis of the example sentence.and do actually lead to mis-inflected phrases.
Inthis way robustness against many types of errorcan be achieved while still preferring the correctvariant.
For more about the WCDG parser, see(Schro?der, 2002; Foth and Menzel, 2006) .The grammar of German available for thisparser relies heavily on weighted constraints bothto cope with many kinds of imperfect input andto resolve true ambiguities.
For the example sen-tence, it retrieves the desired dependencies ex-cept for constructing the implausible dependency?ausra?umen?+?gegen?
(address against).
Let usbriefly review the relevant constraints that causethis error:?
General structural, valence and agreementconstraints determine the macro structure ofthe sentence in the desired way.
For in-stance, the finite and the full verb must com-bine to form an auxiliary phrase, because thisis the only way of accounting for all wordswhile satisfying valence and category con-straints.
For the same reasons both deter-miners must be paired with their respectivenouns.
Also, the prepositional phrase itself iscorrectly predicted.?
General category constraints ensure that thepreposition can attach to nouns and verbs, butnot, say, to a determiner or to punctuation.?
A weak constraint on adjuncts says that ad-juncts are usually close to their regent.
Thepenalty of this constraint varies according tothe length of the dependency that it is appliedto, so that shorter dependencies are generallypreferred.?
A slightly stronger constraint prefers attach-ment of the preposition to the verb, sinceoverall verb attachment is more common thannoun attachment in German.
Therefore, theverb attachment leads to the globally best so-lution for this sentence.There are no lexicalized rules that capture theparticular plausibility of the phrase ?Bedenkengegen?
(concerns about).
A constraint that de-scribes this individual word pair would be trivialto write, but it is not feasible to model the generalphenomenon in this way; thousands of constraintswould be needed just to reflect the more impor-tant collocations in a language, and the exact setof collocating words is impossible to predict ac-curately.
Data-driven information would be muchmore suitable for curing this lexical blind spot.3.2 The Collocation MeasureThe usual way to retrieve the lexical preference ofa word such as ?Bedenken?
for ?gegen?
is to obtaina large corpus and assume that it is representativeof the entire language; in particular, that colloca-tions in this corpus are representative of colloca-tions that will be encountered in future input.
Theassumption is of course not entirely true, but it cannevertheless be preferable to rely on such uncer-tain knowledge rather than remain undecided, onthe reasonable assumption that it will lead to morecorrect than wrong decisions.
Note that the samereasoning applies to many of the violable con-straints in a WCDG: although they do not hold onall possible structures, they hold more often thanthey fail, and therefore can be useful for analysingunknown input.Different measures have been used to gauge thestrength of a lexical preference, but in general theefficacy of the statistical approach depends moreon the suitability of the training corpus than on de-tails of the collocation measure.
Since our focus225is not on finding the best extraction method, buton judging the benefit of statistical components toparsing, we employ a collocation measure relatedto the idea of mutual information: a collocationbetween a word w and a preposition p is judgedmore likely the more often it appears, and the lessoften its component words appear.
By normalizingagainst the total number t of utterances we derivea measure of Lexical Attraction for each possiblecollocation:LA(w, p) := fw+pt/(fwt ?fpt)For instance, if we assume that the word ?Be-denken?
occurs in one out of 2,000 sentences ofGerman and the word ?gegen?
occurs in one sen-tence out of 31 (these figures were taken fromthe unsupervised experiment described later), thenpure chance would make the two words co-occurin one sentence out of 62,000.
If the LA scoreis higher than 1, i. e. we observe a much higherfrequency of co-occurrences in a large corpus, wecan assume that the two events are not statisti-cally independent ?
in other words, that there is apositive correlation between the two words.
Con-versely, we would expect a much lower score forthe implausible collocation ?Bedenken?+?fu?r?, in-dicating a dispreference for this attachment.4 Experiments4.1 SourcesTo obtain the counts to base our estimates of at-traction on, we first turned to the dependency tree-bank that accompanies the WCDG parsing suite.This corpus contains some 59,000 sentences with1,000,000 words with complete syntactic annota-tions, 61% of which are drawn from online tech-nical newscasts, 33% from literature and 6% fromlaw texts.
We used the entire corpus except for thetest set as a source for counting PP attachments di-rectly.
All verbs, nouns and prepositions were firstreduced to their base forms in order to reduce theparameter space.
Compound nouns were reducedto their base nouns, so that ?EU-Kommission?
istreated the same as ?Kommission?, on the assump-tion that the compound exerts similar attractions asthe base noun.
In contrast, German verbs with pre-fixes usually differ markedly in their preferencesfrom the base verb.
Since forms of verbs such as?ausra?umen?
(address) can be split into two parts(w, p) fw+p fw LA?Firma?+?gegen?
72 76492 0.03?Bedenken?+?gegen?
1529 9618 4.96?Kommission?+?gegen?
223 52415 0.13?ausra?umen?+?gegen?
130 2342 1.73(where fp = 566068, t = 17657329)Table 1: Example calculation of lexical attraction.
(?NP ra?umte NP aus?
), such separated verbs werereassembled before stemming.Although the information retrieved from com-plete syntax trees is valuable, it is clearly insuf-ficient for estimating many valid collocations.
Inparticular, even for a comparatively strong collo-cation such as ?Bedenken?+?gegen?
we can expectonly very few instances.
(There are, in fact, 4such instances, well above chance level but stilla very small number.)
Therefore we used thearchived text from 18 volumes of the newspapertageszeitung as a second source.
This corpus con-tains about 295,000,000 words and should allowus to detect many more collocations.
In fact, wedo find 2338 instances of ?Bedenken?+?gegen?
inthe same sentence.Of course, since we have no syntactic annota-tions for this corpus (and it would be infeasible tocreate them even by fully automatic parsing), notall of these instances may indicate a syntactic de-pendency.
(Ratnaparkhi, 1998) solved this prob-lem by regarding only prepositions in syntacticallyunambiguous configurations.
Unfortunately, hispatterns cannot directly be applied to German sen-tences because of their freer word order.
As anapproximation it would be possible to count onlypairs of adjacent content words and prepositions.However, this would introduce systematic biasesinto the counts, because nouns do in fact very oftenoccur adjacently to prepositions that modify them,but many verbs do not.
For instance, the phrase?jmd.
anklagen wegen etw.?
(to sue s.o.
for s.th.
)gives rise to a strong collocation between the verb?anklagen?
and the preposition ?wegen?
; however,in the predominant sentence types of German, thetwo words are virtually never adjacent, because ei-ther the preposition kernel or the direct object mustintervene.
Therefore, we relax the adjacency con-dition for verb attachment and also count prepo-sitions that occur within a fixed distance of theirsuspected regent.Table 1 shows the detailed values when judg-ing the example sentence according to the un-parsed corpus.
The strong collocation that wewould expect for ?Bedenken?+?gegen?
is indeed226Value of i Recall for V for N overall1 96.2% 39.8% 65.2%2 96.2% 52.0% 71.9%5 88.8% 66.3% 76.4%8 80.0% 79.6% 79.8%10 67.5% 82.7% 75.8%Table 2: Influence of noun factor on solving isolated attach-ment decisions.observed, with a value of 4.96.
However, theverb attachment also has a score above 1, indicat-ing that ?gegen?+?ausra?umen?
(to address about)are also positively correlated.
This is almost cer-tainly a misleading figure, since those two wordsdo not form a plausible verb phrase; it is muchmore probable that the very strong, in fact id-iomatic, correlation ?Bedenken ausra?umen?
(to ad-dress concerns) causes many co-occurrences of allthree words.
Therefore our figures falsely suggestthat ?gegen?
would often attach to ?ausra?umen?,when it is in fact the direct object of that verb thatit is attracted to.
(Volk, 2002) already suggested that this count-ing method introduced a general bias toward verbattachment, and when comparing the results forvery frequent words (for which more reliable evi-dence is available from the treebank) we find thatverb attachments are in fact systematically over-estimated.
We therefore adopted his approach andartificially inflated all noun+preposition counts bya constant factor i.
To estimate an appropriatevalue for this factor, we extracted 178 instances ofthe standard verb+noun+preposition configurationfrom our corpus, of which 80 were verb attach-ments (V) and 98 were noun attachments (N).Table 2 shows the performance of the predictorfor this binary decision task.
Taken as it is, it re-trieves most verb attachments, but less than half ofthe noun attachments, while higher values of i canimprove the recall both for noun attachments andoverall.
The performance achieved falls somewhatshort of the highest figures reported previously forPP attachment for German (Volk, 2002); this isat least in part due to our simple model that ig-nores the kernel noun of the PP.
However, it couldwell be good enough to be integrated into a fullparser and provide a benefit to it.
Also, the syntac-tical configuration in this standard benchmark isnot the predominant one in complete German sen-tences; in fact fewer than 10% of all prepositionsoccur in this context.
The best performance on thetriple task is therefore not guaranteed to be the bestchoice for full parsing.
In our experiments, we1.00.81 3 5weightLAFigure 2: Mapping lexical attraction values to penaltiesused a value of i = 8, which seems to be suitedbest to our grammar.4.2 Integration MethodTo add our simple collocation model to the parser,it is sufficient to write a single variable-strengthconstraint that judges each PP dependency by howstrong the lexical attraction between the regent andthe dependent is.
The only question is how to mapour lexical attraction values to penalties for thisconstraint.
Their predicted relative order of plausi-bility should of course be reflected, so that depen-dencies with a high lexical attraction are preferredover those with lower lexical attraction.
At thesame time, the information should not be given toomuch weight compared to the existing grammarrules, since it is heuristic in nature and should cer-tainly not override important principles such as va-lence or agreement.
The penalties of WCDG con-straints range from 0.0 (hard constraint) through1.0 (a constraint with this penalty has no effectwhatsoever and is only useful for debugging).We chose an inverse mapping based on the log-arithm of lexical attraction (cf.
Figure 2):p(w, p) = max(1,min(0.8,1?
(2?log3(LA(w,p)))/50))?where ?
is a normalization constant that scalesthe highest occurring value of LA to 1.
For in-stance, this mapping will interpret a strong lex-ical attraction of 5 as the penalty 0.989 (almostperfect) and a lexical attraction of only 0.5 as thepenalty 0.95 (somewhat dispreferred).
The overallrange of PP attachment penalties is limited to theinterval [0.8 ?
1.0], which ensures that the judge-ment of the statistical module will usually comeinto play only when no other evidence is available;preliminary experiments showed that a strongerintegration of the component yields no additionaladvantage.
In any case, the exact figure dependsclosely on the valuation of the existing constraintsof the grammar and is of little importance as such.227Label occurred retrieved errors accuracyPP 1892 1285 607 67.9%ADV 1137 951 186 83.6%OBJA 775 675 100 87.1%APP 659 567 92 86.0%SUBJ 1338 1251 87 93.5%S 1098 1022 76 93.1%KON 481 406 75 84.4%REL 167 107 60 64.1%overall 17719 16073 1646 90.7Table 3: Performance of the original parser on the test set.Besides adding the new constraint ?PP attach-ment?
to the grammar, we also disabled severalof the existing constraints that apply to preposi-tions, since we assume that our lexicalized modelis superior to the unlexicalized assumptions thatthe grammar writers had made so far.
For instance,the constraint mentioned in Section 3 that glob-ally prefers verb attachment to noun attachmentis essentially a crude approximation of lexical at-traction, whose task is now taken over entirely bythe statistical predictor.
We also assume that lex-ical preference exerts a stronger influence on at-tachment than mere linear distance; therefore wechanged the distance constraint so that it exemptsprepositions from the normal distance penaltiesimposed on adjuncts.4.3 CorpusFor our parsing experiments, we used the first1,000 sentences of technical newscasts from thedependency treebank mentioned above.
This testset has an average sentence length of 17.7 words,and from previous experiments we estimate that itis comparable in difficulty to the NEGRA corpusto within 1% of accuracy.
Although online articlesand newspaper copy follow some different con-ventions, we assume the two text types are similarenough that collocations extracted from one canbe used to predict attachments in the other.For parsing we used the heuristic trans-formation-based search described in (Foth et al,2000).
Table 3 illustrates the structural accuracy2of the unmodified system for various subordina-tion types.
For instance, of the 1892 dependencyedges with the label ?PP?
in the gold standard,1285 are attached correctly by the parser, while607 receive an incorrect regent.
We see that PP at-tachment decisions are particularly prone to errors2Note that the WCDG parser always succeeds in assign-ing exactly one regent to each word, so that there is no dif-ference between precision and recall.
We refer to structuralaccuracy as the ratio of words which have been attached cor-rectly to all words.Method PP accuracy overall accuracybaseline 67.9% 90.7%supervised 79.4% 91.9%unsupervised 78.3% 91.9%backed-off 78.9% 92.2%Table 4: Structural accuracy of PP edges and all edges.both in absolute and in relative terms.4.4 ResultsWe trained the PP attachment predictor both withthe counts acquired from the dependency treebank(supervised) and those from the newspaper cor-pus (unsupervised).
We also tested a mode of op-eration that uses the more reliable data from thetreebank, but backs off to unsupervised counts ifthe hypothetical regent was seen fewer than 1,000times in training.Table 4 shows the results when parsing with theaugmented grammar.
Both the overall structuralaccuracy and the accuracy of PP edges are given;note that these figures result from the general sub-ordination task, therefore they correspond to Ta-ble 3 and not to Table 2.
As expected, lexical-ized preference information for prepositions yieldsa large benefit to full parsing: the attachment errorrate is decreased by 34% for prepositions, and by14% overall.
In this experiment, where much moreunsupervised training data was available, super-vised and unsupervised training achieved almostthe same level of performance (although many in-dividual sentences were parsed differently).A particular concern with corpus-based deci-sion methods is their applicability beyond thetraining corpus.
In our case, the majority of thematerial for supervised training was taken fromthe same newscast collection as the test set.
How-ever, comparable results are also achieved whenapplying the parser to the standard test set from theNEGRA corpus of German, as used by (Schiehlen,2004; Foth et al, 2005): adding the PP predic-tor trained on our dependency treebank raises theoverall attachment accuracy from 89.3% to 90.6%.This successful reuse indicates that lexical prefer-ence between prepositions and function words islargely independent of text type.5 Related Work(Hindle and Rooth, 1991) first proposed solvingthe prepositional attachment task with the help ofstatistical information, and also defined the preva-lent formulation as a binary decision problem withthree words involved.
(Ratnaparkhi et al, 1994)228extended the problem instances to quadruples byalso considering the kernel noun of the PP, andused maximum entropy models to estimate thepreferences.Both supervised and unsupervised training pro-cedures for PP attachment have been investigatedand compared in a number of studies, with su-pervised methods usually being slightly superior(Ratnaparkhi, 1998; Pantel and Lin, 2000), withthe notable exception of (Volk, 2002), who ob-tained a worse accuracy in the supervised case,obviously caused by the limited size of the avail-able treebank.
Combining both methods can leadto a further improvement (Volk, 2002; Kokkinakis,2000), a finding confirmed by our experiments.Supervised training methods already applied toPP attachment range from stochastic maximumlikelihood (Collins and Brooks, 1995) or maxi-mum entropy models (Ratnaparkhi et al, 1994)to the induction of transformation rules (Brill andResnik, 1994), decision trees (Stetina and Nagao,1997) and connectionist models (Sopena et al,1998).
The state-of-the-art is set by (Stetina andNagao, 1997) who generalize corpus observationsto semantically similar words as they can be de-rived from the WordNet hierarchy.The best result for German achieved so far isthe accuracy of 80.89% obtained by (Volk, 2002).Note, however, that our goal was not to optimizethe performance of PP attachment in isolation butto quantify the contribution it can make to the per-formance of a full parser for unrestricted text.The accuracy of PP attachment has rarely beenevaluated as a subtask of full parsing.
(Merlo et al,1997) evaluate the attachment of multiple preposi-tions in the same sentence for English; 85.3% ac-curacy is achieved for the first PP, 69.6% for thesecond and 43.6% for the third.
This is still ratherdifferent from our setup, where PP attachment isfully integrated into the parsing problem.
Closerto our evaluation scenario comes (Collins, 1999)who reports 82.3%/81.51% recall/precision on PPmodifications for his lexicalized stochastic parserof English.
However, no analysis has been carriedout to determine which model components con-tributed to this result.A more application-oriented view has beenadopted by (Schwartz et al, 2003), who devisedan unsupervised method to extract positive andnegative lexical evidence for attachment prefer-ences in English from a bilingual, aligned English-Japanese corpus.
They used this information to re-attach PPs in a machine translation system, report-ing an improvement in translation quality whentranslating into Japanese (where PP attachment isnot ambiguous and therefore matters) and a de-crease when translating into Spanish (where at-tachment ambiguities are close to the original onesand therefore need not be resolved).Parsing results for German have been publisheda number of times.
Combining treebank transfor-mation techniques with a suffix analysis, (Dubey,2005) trained a probabilistic parser and reached alabelled F-score of 76.3% on phrase structure an-notations for a subset of the sentences used here(with a maximum length of 40).
For dependencyparsing a labelled accuracy of 87.34% and an un-labelled one of 90.38% has been achieved by ap-plying the dependency parser described in (Mc-Donald et al, 2005) to German data.
This systemis based on a procedure for online large marginlearning and considers a huge number of locallyavailable features, which allows it to determinethe optimal attachment fully deterministically.
Us-ing a stochastic variant of Constraint DependencyGrammar (Wang and Harper, 2004) reached a92.4% labelled F-score on the Penn Treebank,which slightly outperforms (Collins, 1999) whoreports 92.0% on dependency structures automati-cally derived from phrase structure results.6 Conclusions and future workCorpus-based data has been shown to provide asignificant benefit when used to guide a rule-baseddependency parser of German, reducing the er-ror rate for situated PP attachment by one third.Prepositions still remain the largest source of at-tachment errors; many reasons can be trackeddown for individual errors, such as faulty POStagging, misinterpreted global sentence structure,genuinely ambiguous constructions, failure of theattraction heuristics, or simply lack of process-ing time.
However, considering that even humanarbiters often agree only on 90% of PP attach-ments, the results appear promising.
In particu-lar, many attachment errors that strongly disagreewith human intuition (such as in the example sen-tence) were in fact prevented.
Thus, the additionof a corpus-based knowledge source to the sys-tem yielded a much greater benefit than could havebeen achieved with the same effort by writing in-dividual constraints.229One obvious further task is to improve oursimple-minded model of lexical attraction.
For in-stance, some remaining errors suggest that takingthe kernel noun into account would yield a higherattachment precision; this will require a redesignof the extraction tools to keep the parameter spacemanageable.
Also, other subordination types than?PP?
may benefit from similar knowledge; e.g., inmany German sentences the roles of subject andobject are syntactically ambiguous and can onlybe understood correctly through world knowledge.This is another area in which synergy betweenlexical attraction estimates and general symbolicrules appears possible.ReferencesE.
Brill and P. Resnik.
1994.
A rule-based approach toprepositional phrase attachment disambiguation.
InProc.
15th Int.
Conf.
on Computational Linguistics,pages 1198 ?
1204, Kyoto, Japan.M.
Collins and J. Brooks.
1995.
Prepositional attach-ment through a backed-off model.
In Proc.
of the3rd Workshop on Very Large Corpora, pages 27?38,Somerset, New Jersey.M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Phd thesis, Universityof Pennsylvania, Philadephia, PA.A.
Dubey.
2005.
What to do when lexicalization fails:parsing German with suffix analysis and smoothing.In Proc.
43rd Annual Meeting of the ACL, Ann Ar-bor, MI.K.
Foth and W. Menzel.
2006.
Hybrid parsing: Us-ing probabilistic models as predictors for a symbolicparser.
In Proc.
21st Int.
Conf.
on ComputationalLinguistics, Coling-ACL-2006, Sydney.K.
Foth, W. Menzel, and I. Schro?der.
2000.
ATransformation-based Parsing Technique with Any-time Properties.
In 4th Int.
Workshop on ParsingTechnologies, IWPT-2000, pages 89 ?
100.K.
Foth, M. Daum, and W. Menzel.
2005.
Parsing un-restricted German text with defeasible constraints.In H. Christiansen, P. R. Skadhauge, and J. Vil-ladsen, editors, Constraint Solving and LanguageProcessing, volume 3438 of LNAI, pages 140?157.Springer-Verlag, Berlin.D.
Hindle and M. Rooth.
1991.
Structural Ambiguityand Lexical Relations.
In Meeting of the Associationfor Computational Linguistics, pages 229?236.D.
Kokkinakis.
2000.
Supervised pp-attachment dis-ambiguation for swedish; (combining unsupervisedsupervised training data).
Nordic Journal of Lin-guistics, 3.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.2005.
Non-projective dependency parsing usingspanning tree algorithms.
In Proc.
Human Lan-guage Technology Conference / Conference on Em-pirical Methods in Natural Language Processing,HLT/EMNLP-2005, Vancouver, B.C.P.
Merlo, M. Crocker, and C. Berthouzoz.
1997.
At-taching Multiple Prepositional Phrases: General-ized Backed-off Estimation.
In Proc.
2nd Conf.
onEmpirical Methods in NLP, pages 149?155, Provi-dence, R.I.P.
Pantel and D. Lin.
2000.
An unsupervised approachto prepositional phrase attachment using contextu-ally similar words.
In Proc.
38th Meeting of theACL, pages 101?108, Hong Kong.A.
Ratnaparkhi, J. Reynar, and S. Roukos.
1994.
AMaximum Entropy Model for Prepositional PhraseAttachment.
In Proc.
ARPA Workshop on HumanLanguage Technology, pages 250 ?255.A.
Ratnaparkhi.
1998.
Statistical models for unsu-pervised prepositional phrase attachment.
In Proc.17th Int.
Conf.
on Computational Linguistics, pages1079?1085, Montreal.M.
Schiehlen.
2004.
Annotation Strategies for Proba-bilistic Parsing in German.
In Proceedings of COL-ING 2004, pages 390?396, Geneva, Switzerland,Aug 23?Aug 27.
COLING.I.
Schro?der.
2002.
Natural Language Parsing withGraded Constraints.
Ph.D. thesis, Department of In-formatics, Hamburg University, Hamburg, Germany.L.
Schwartz, T. Aikawa, and C. Quirk.
2003.
Disam-biguation of english PP-attachment using multilin-gual aligned data.
In Machine Translation SummitIX, New Orleans, Louisiana, USA.J.
M. Sopena, A. LLoberas, and J. L. Moliner.
1998.A connectionist approach to prepositional phrase at-tachment for real world texts.
In Proc.
17th Int.Conf.
on Computational Linguistics, pages 1233?1237, Montreal.J.
Stetina and M. Nagao.
1997.
Corpus based PP at-tachment ambiguity resolution with a semantic dic-tionary.
In Jou Shou and Kenneth Church, editors,Proc.
5th Workshop on Very Large Corpora, pages66?80, Hong Kong.M.
Volk.
2002.
Combining Unsupervised and Super-vised Methods for PP Attachment Disambiguation.In Proc.
of COLING-2002, Taipeh.W.
Wang and M. P. Harper.
2004.
A statisticalconstraint dependency grammar (CDG) parser.
InProc.
ACL Workshop Incremental Parsing: BringingEngineering and Cognition Together, pages 42?49,Barcelona, Spain.230
