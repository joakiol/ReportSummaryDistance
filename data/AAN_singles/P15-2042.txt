Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 256?261,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAn Efficient Dynamic Oracle for Unrestricted Non-Projective ParsingCarlos G?omez-Rodr?
?guezDepartamento de Computaci?onUniversidade da Coru?naCampus de Elvi?na, s/n15071 A Coru?na, Spaincarlos.gomez@udc.esDaniel Fern?andez-Gonz?alezDepartamento de Inform?aticaUniversidade de VigoCampus As Lagoas, s/n32004 Ourense, Spaindanifg@uvigo.esAbstractWe define a dynamic oracle for the Cov-ington non-projective dependency parser.This is not only the first dynamic oraclethat supports arbitrary non-projectivity,but also considerably more efficient(O(n)) than the only existing oracle withrestricted non-projectivity support.
Ex-periments show that training with the dy-namic oracle significantly improves pars-ing accuracy over the static oracle baselineon a wide range of treebanks.1 IntroductionGreedy transition-based dependency parsers buildanalyses for sentences incrementally by followinga sequence of transitions defined by an automaton,using a scoring model to choose the best trans-ition to take at each state (Nivre, 2008).
Whilethis kind of parsers have become very popular,as they achieve competitive accuracy with espe-cially fast parsing times; their raw accuracy is stillbehind that of slower alternatives like transition-based parsers that use beam search (Zhang andNivre, 2011; Choi and McCallum, 2013).
For thisreason, a current research challenge is to improvethe accuracy of greedy transition-based parsers asmuch as possible without sacrificing efficiency.A relevant recent advance in this direction isthe introduction of dynamic oracles (Goldberg andNivre, 2012), an improvement in the training pro-cedure of greedy parsers that can boost their ac-curacy without any impact on parsing speed.
Anoracle is a training component that selects the besttransition(s) to take at a given configuration, us-ing knowledge about the gold tree.
Traditionally,transition-based parsers were trained to follow aso-called static oracle, which is only defined onthe configurations of a canonical computation thatgenerates the gold tree, returning the next trans-ition in said computation.
In contrast, dynamicoracles are non-deterministic (not limited to onesequence, but supporting all the possible computa-tions leading to the gold tree), and complete (alsodefined for configurations where the gold tree isunreachable, choosing the transition(s) that lead toa tree with minimum error).
This extra robustnessin training provides higher parsing accuracy.However, defining a usable dynamic oracle fora given parser is non-trivial in general, due tothe need of calculating the loss of each configura-tion, i.e., the minimum Hamming loss to the goldtree from a tree reachable from that configuration.While it is always easy to do this in exponentialtime by simulating all possible computations inthe algorithm to obtain all reachable trees, it isnot always clear how to achieve this calculationin polynomial time.
At the moment, this prob-lem has been solved for several projective pars-ers exploiting either arc-decomposability (Gold-berg and Nivre, 2013) or tabularization of compu-tations (Goldberg et al, 2014).
However, for pars-ers that can handle crossing arcs, the only knowndynamic oracle (G?omez-Rodr?
?guez et al, 2014)has been defined for a variant of the parser by At-tardi (2006) that supports a restricted set of non-projective trees.
To our knowledge, no dynamicoracles are known for any transition-based parserthat can handle unrestricted non-projectivity.In this paper, we define such an oracle forthe Covington non-projective parser (Covington,2001; Nivre, 2008), which can handle arbitrarynon-projective dependency trees.
As this al-gorithm is not arc-decomposable and its tabular-ization is NP-hard (Neuhaus and Br?oker, 1997),we do not use the existing techniques to definedynamic oracles, but a reasoning specific to thisparser.
It is worth noting that, apart from being thefirst dynamic oracle supporting unrestricted non-projectivity, our oracle is very efficient, solving theloss calculation in O(n).
In contrast, the restrictednon-projective oracle of G?omez-Rodr?
?guez et al256(2014) has O(n8) time complexity.The rest of the paper is organized as follows:after a quick outline of Covington?s parser inSect.
2, we present the oracle and prove its cor-rectness in Sect.
3.
Experiments are reported inSect.
4, and Sect.
5 contains concluding remarks.2 PreliminariesWe will define a dynamic oracle for the non-projective parser originally defined by Covington(2001), and implemented by Nivre (2008) underthe transition-based parsing framework.
For spacereasons, we only sketch the parser very briefly, andrefer to the above reference for more details.Parser configurations are of the form c =?
?1, ?2, B,A?, where ?1and ?2are lists of par-tially processed words,B is another list (called thebuffer) with currently unprocessed words, andA isthe set of dependencies built so far.
Suppose thatwe parse a string w1?
?
?wn, whose word occur-rences will be identified with their indices 1 ?
?
?nfor simplicity.
Then, the parser starts at an initialconfiguration cs(w1.
.
.
wn) = ?
[], [], [1 .
.
.
n], ?
?,and executes transitions chosen from those in Fig-ure 1 until a terminal configuration of the form{?
?1, ?2, [], A?
?
C} is reached, and the sen-tence?s parse tree is obtained from A.1The transition semantics is very simple, mirror-ing the double nested loop traversing word pairs inthe formulation by Covington (2001).
When thealgorithm is in a configuration ?
?1|i, ?2, j|B,A?,we will say that it is considering the focus wordsi and j, located at the end of the first list and at thebeginning of the buffer.
A decision is then madeabout whether these two words should be linkedwith a rightward arc i?
j (Right-Arc transition),a leftward arc i ?
j (Left-Arc transition) or notlinked (No-Arc transition).
The first two choiceswill be unavailable in configurations where thenewly-created arc would violate the single-headconstraint (a node cannot have more than one in-coming arc) or the acyclicity constraint (cyclesare not allowed).
In any of these three transitions,i is then moved to the second list to make i?1 andj the focus words for the next step.
Alternatively,we can choose to read a new word from the stringwith a Shift transition, so that the focus words in1The arcs in A form a forest, but we convert it to a tree bylinking any node without a head as a dependent of an artifi-cial node at position 0 that acts as a dummy root.
From nowon, when we refer to some dependency graph as a tree, weassume that this transformation is being implicitly made.the resulting configuration will be j and j + 1.The result is a parser that can generate any pos-sible dependency tree for the input, and runs inquadratic worst-case time.
Although in theory thiscomplexity can seem like a drawback compared tolinear-time transition-based parsers (e.g.
(Nivre,2003; G?omez-Rodr?
?guez and Nivre, 2013)), it hasbeen shown by Volokh and Neumann (2012) to ac-tually outperform linear algorithms in practice, asit allows for relevant optimizations in feature ex-traction that cannot be implemented in other pars-ers.
In fact, one of the fastest dependency parsersto date uses this algorithm (Volokh, 2013).3 The oracleAs sketched in Sect.
1, a dynamic oracle is a train-ing component that, given a configuration c anda gold tree tG, provides the set of transitions thatare applicable in c and lead to trees with minimumHamming loss with respect to tG.
The Hammingloss between a tree t and tG, written L(t, tG), isthe number of nodes that have a different head in tthan in tG.
Following Goldberg and Nivre (2013),we say that a set of arcs A is reachable from con-figuration c, written c  A, if there is some (pos-sibly empty) path of transitions from c to someconfiguration c?= ?
?1, ?2, B,A?
?, with A ?
A?.Then, we can define the loss of a configuration as`(c) = mint|c tL(t, tG),and the set of transitions that must be returned bya correct dynamic oracle is thenod(c, tG) = {?
| `(c)?
`(?
(c)) = 0},i.e., the transitions that do not increase configur-ation loss, and hence lead to the best parse (interms of loss) reachable from c. Therefore, imple-menting a dynamic oracle reduces to computingthe loss `(c) for each configuration c.Goldberg and Nivre (2013) show that the calcu-lation of the loss is easy for parsers that are arc-decomposable, i.e., those where for every config-uration c and arc setA that is tree-compatible (i.e.that can be a part of a well-formed parse2), c Ais entailed by c  (i ?
j) for every i ?
j ?
A.That is, if each arc in a tree-compatible set is indi-vidually reachable from configuration c, then that2In the cited paper, tree-compatibility required projectiv-ity, as the authors were dealing with projective parsers.
Inour case, since the parser is non-projective, tree-compatibilityonly consists of the single-head and acyclicity constraints.257Shift: ?
?1, ?2, j|B,A?
?
??1?
?2|j, [], B,A?No-Arc: ?
?1|i, ?2, B,A?
?
?
?1, i|?2, B,A?Left-Arc: ?
?1|i, ?2, j|B,A?
?
?
?1, i|?2, j|B,A ?
{j ?
i}?only if @k | k ?
i ?
A (single-head) and i ?
?j 6?
A (acyclicity).Right-Arc: ?
?1|i, ?2, j|B,A?
?
?
?1, i|?2, j|B,A ?
{i?
j}?only if @k | k ?
j ?
A (single-head) and j ?
?i 6?
A (acyclicity).Figure 1: Transitions of the Covington non-projective dependency parser.0 1 2 3 4Figure 2: An example of non-arc-decomposabilityof the Covington parser: graphical representationof configuration c = ?
[1, 2], [], [3, 4], A = {1 ?2}?.
The solid arc corresponds to the arc set A,and the circled indexes mark the focus words.
Thedashed arcs represent the gold tree tG.set of arcs is reachable from c. If this holds, thencomputing the loss of a configuration c reduces todetermining and counting the gold arcs that are notreachable from c, which is easy in most parsers.Unfortunately, the Covington parser is not arc-decomposable.
This can be seen in the example ofFigure 2: while any of the gold arcs 2?3, 3?4,4?1 can be reachable individually from the depic-ted configuration, they are not jointly reachable asthey form a cycle with the already-built arc 1?2.Thus, the configuration has only one individuallyunreachable arc (0?2), but its loss is 2.However, it is worth noting that non-arc-decomposability in the parser is exclusively dueto cycles.
If a set of individually reachable arcs donot form a cycle together with already-built arcs,then we can show that the set will be reachable.This idea is the basis for an expression to computeloss based on counting individually unreachablearcs, and then correcting for the effect of cycles:Theorem 1 Let c = ?
?1, ?2, B,A?
be a config-uration of the Covington parser, and tGthe set ofarcs of a gold tree.
We call I(c, tG) = {x ?
y ?tG| c  (x ?
y)} the set of individually reach-able arcs of tG; note that this set may overlap A.Conversely, we call U(c, tG) = tG\ I(c, tG) theset of individually unreachable arcs of tGfrom c.Finally, let nc(G) denote the number of cycles ina graph G.Then `(c) = |U(c, tG)|+ nc(A ?
I(c, tG)).2We now sketch the proof.
To prove Theorem 1,it is enough to show that (1) there is at least onetree reachable from c with exactly that Hammingloss to tG, and (2) there are no trees reachable fromcwith a smaller loss.
To this end, we will use someproperties of the graphA?I(c, tG).
First, we notethat no node in this graph has in-degree greaterthan 1.
In particular, each node except for thedummy root has exactly one head, either explicitor (if no head has been assigned inA or in the goldtree) the dummy root.
No node has more than onehead: a node cannot have two heads in A becausethe parser transitions enforce the single-head con-straint, it cannot have two heads in I(c, tG) be-cause tGmust satisfy this constraint as well, and itcannot have one head in A and another in I(c, tG)because the corresponding arc in I(c, tG) wouldbe unreachable due to the single-head constraint.This, in turn, implies that the graphA?I(c, tG)has no overlapping cycles, as overlapping cyclescan only appear in graphs with in-degree greaterthan 1.
This is the key property enabling us toexactly calculate loss using the number of cycles.To show (1), consider the graph A ?
I(c, tG).In each of its cycles, there is at least one arcthat belongs to I(c, tG), as A must satisfy theacyclicity constraint.
We arbitrarily choose onesuch arc from each cycle, and remove it fromthe graph.
Note that this results in removing ex-actly nc(A ?
I(c, tG)) arcs, as we have shownthat the cycles in A ?
I(c, tG) are disjoint.
Wecall the resulting graph B(c, tG).
As it has max-imum in-degree 1 and it is acyclic (because wehave broken all the cycles), B(c, tG) is a tree, mod-ulo our standard assumption that headless nodesare assumed to be linked to the dummy root.This tree B(c, tG) is reachable from c and hasloss `(c) = |U(c, tG)|+nc(A?I(c, tG)).
Reach-ability is shown by building a sequence of trans-258itions that will visit the pairs of words corres-ponding to remaining arcs in order, and inter-calating the corresponding Left-Arc or Right-Arctransitions, which cannot violate the acyclicity orsingle-head constraints.
The term U(c, tG) in theloss stems from the fact that A ?
I(c, tG) can-not contain arcs in U(c, tG), and the term nc(A ?I(c, tG)) from not including the nc(A ?
I(c, tG))arcs that we discarded to break cycles.Finally, from these observations, it is easy tosee that B(c, tG) has the best loss among reach-able trees, and thus prove (2): the arcs in U(c, tG)are always unreachable by definition, and for eachcycle in nc(A ?
I(c, tG)), the acyclicity con-straint forces us to miss at least one arc.
Asthe cycles are disjoint, this means that we neces-sarily miss at least nc(A ?
I(c, tG)) arcs, hence|U(c, tG)| + nc(A ?
I(c, tG)) is indeed the min-imum loss among reachable trees.
Thus, to calculate the loss of a configuration c,we only need to compute both of the terms in The-orem 1.
For the first term, note that if c has focuswords i and j (i.e., c = ?
?1|i, ?2, j|B,A?
), thenan arc x?
y is in U(c, tG) if it is not in A, and atleast one of the following holds:?
j > max(x, y), as in this case we have readtoo far in the string and will not be able to getx and y as focus words,?
j = max(x, y) ?
i < min(x, y), as in thiscase we have max(x, y) as the right focusword but the left focus word is to the left ofmin(x, y), and we cannot move it back,?
there is some z 6= 0, z 6= x such that z ?
y ?A, as in this case the single-head constraintprevents us from creating x?
y,?
x and y are on the same weakly connectedcomponent of A, as in this case the acyclicityconstraint will not let us create x?
y.All of these arcs can be trivially enumerated inO(n) time (in fact, they can be updated in O(1)if we start from the configuration that preceded c).The second term of the loss, nc(A?I(c, tG)), canbe computed by obtaining I(c, tG) as tG\U(c, tG)to then apply a standard cycle-finding algorithm(Tarjan, 1972) which, for a graph with maximumin-degree 1, runs in O(n) time.Algorithm 1 presents the resulting loss cal-culation algorithm in pseudocode form, whereCOUNTCYCLES is a function that counts the num-ber of cycles in the given graph in linear time asmentioned above.
Note that the for loop runs inAlgorithm 1 Computation of the loss of a config-uration.1: function LOSS(c = ?
?1|i, ?2, j|B,A?, tG)2: U ?
?
.
Variable U is for U(c, tG)3: for each x?
y ?
(tG\A) do4: left ?
min(x, y)5: right ?
max(x, y)6: if j > right ?7: (j = right ?
i < left)?8: (?z > 0, z 6= x : z ?
y ?
A)?9: WEAKLYCONNECTED(A, x, y) then10: U ?
u ?
{x?
y}11: I ?
tG\U .
Variable I is for I(c, tG)12: return |U |+ COUNTCYCLES(A ?
I )linear time: the condition on line 8 can be com-puted in constant time by recovering the head ofy.
The call to WEAKLYCONNECTED in line 9finds out whether the two given nodes are weaklyconnected in A, and can also be resolved inO(1), by querying the disjoint set data structurethat implementations of the Covington algorithmcommonly use for the parser?s acyclicity checks(Nivre, 2008).It is worth noting that the linear-time com-plexity can also be achieved by a standalone im-plementation of the loss calculation algorithm,without recurse to the parser?s auxiliary data struc-tures (although this is dubiously practical).
Todo so, we can implement WEAKLYCONNECTEDso that the first call computes the connected com-ponents of A in linear time (Hopcroft and Tarjan,1973) and subsequent calls use this information tofind out if two nodes are weakly connected in con-stant time.On the other hand, a more efficient implementa-tion than the one shown in Algorithm 1 (which wechose for clarity) can be achieved by more tightlycoupling the oracle to the parser, as the relevantsets of arcs associated with a configuration can beobtained incrementally from those of the previousconfiguration.4 ExperimentsTo evaluate the performance of our approach, weconduct experiments on both static and dynamicCovington non-projective oracles.
Concretely, wetrain an averaged perceptron model for 15 itera-tions on nine datasets from the CoNLL-X sharedtask (Buchholz and Marsi, 2006) and all data-259UnigramsL0w; L0p; L0wp; L0l; L0hw; L0hp; L0hl; L0l?w; L0l?p;L0l?l; L0r?w; L0r?p; L0r?l; L0h2w; L0h2p; L0h2l; L0lw;L0lp; L0ll; L0rw; L0rp; L0rl; L0wd; L0pd;L0wvr; L0pvr; L0wvl; L0pvl; L0wsl; L0psl; L0wsr;L0psr; L1w; L1p; L1wp; R0w; R0p; R0wp; R0l?w;R0l?p; R0l?l; R0lw; R0lp; R0ll; R0wd; R0pd; R0wvl;R0pvl;R0wsl; R0psl; R1w; R1p; R1wp; R2w; R2p;R2wp; CLw; CLp; CLwp; CRw; CRp; CRwp;PairsL0wp+R0wp; L0wp+R0w; L0w+R0wp; L0wp+R0p;L0p+R0wp; L0w+R0w; L0p+R0p;R0p+R1p;L0w+R0wd; L0p+R0pd;TriplesR0p+R1p+R2p; L0p+R0p+R1p; L0hp+L0p+R0p;L0p+L0l?p+R0p; L0p+L0r?p+R0p; L0p+R0p+R0l?p;L0p+L0l?p+L0lp; L0p+L0r?p+L0rp;L0p+L0hp+L0h2p; R0p+R0l?p+R0lp;Table 1: Feature templates.
L0and R0denotethe left and right focus words; L1, L2, .
.
.
are thewords to the left of L0and R1, R2, .
.
.
those to theright of R0.
Xihmeans the head of Xi, Xih2thegrandparent, Xiland Xil?the farthest and closestleft dependents, and Xirand Xir?the farthest andclosest right dependents, respectively.
CL andCR are the first and last words between L0andR0whose head is not in the interval [L0, R0].
Finally,w stands for word form; p for PoS tag; l for de-pendency label; d is the distance between L0andR0; vl, vrare the left/right valencies (number ofleft/right dependents); and sl, srthe left/right labelsets (dependency labels of left/right dependents).sets from the CoNLL-XI shared task (Nivre et al,2007).
We use the same feature templates for alllanguages, which result from adapting the featuresdescribed by Zhang and Nivre (2011) to the datastructures of the Covington non-projective parser,and are listed in detail in Table 1.Table 2 reports the accuracy obtained by theCovington non-projective parser with both or-acles.
As we can see, the dynamic oracle imple-mented in the Covington algorithm improves overthe accuracy of the static version on all datasetsexcept Japanese and Swedish, and most improve-ments are statistically significant at the .05 level.3In addition, the Covington dynamic oracleachieves a greater average improvement in ac-curacy than the Attardi dynamic oracle (G?omez-Rodr?
?guez et al, 2014) over their respective staticversions.
Concretely, the Attardi oracle accom-plishes an average improvement of 0.52 percent-3Note that the loss of accuracy in Japanese and Swedishis not statistically significant.s-Covington d-CovingtonLanguage UAS LAS UAS LASArabic 80.03 71.32 81.47?72.77?Basque 75.76 69.70 76.49?70.27?Catalan 88.66 83.92 89.28 84.26Chinese 83.94 79.59 84.68?80.16?Czech 77.38 71.21 78.58?72.59?English 84.64 83.72 86.14?84.96?Greek 79.33 72.65 80.52?73.67?Hungarian 77.70 74.32 78.22 74.61Italian 83.39 79.66 83.66 79.91Turkish 82.14 76.00 82.38 76.15Bulgarian 87.68 84.55 88.48?85.32?Danish 84.07 79.99 84.98?80.85?Dutch 80.28 77.55 81.17?78.54?German 86.12 83.93 87.47?85.15?Japanese 93.92 92.51 93.79 92.42Portuguese 85.70 82.78 86.23 83.27Slovene 75.31 68.97 76.76?70.35?Spanish 78.82 75.84 79.87?76.97?Swedish 86.78 81.29 86.66 81.21Average 82.72 78.39 83.52 79.13Table 2: Parsing accuracy (UAS and LAS, in-cluding punctuation) of Covington non-projectiveparser with static (s-Covington) and dynamic (d-Covington) oracles on CoNLL-XI (first block) andCoNLL-X (second block) datasets.
For each lan-guage, we run five experiments with the samesetup but different seeds and report the averagedaccuracy.
Best results for each language are shownin boldface.
Statistically significant improvements(?
= .05) (Yeh, 2000) are marked with?.age points in UAS and 0.71 in LAS, while our ap-proach achieves 0.80 in UAS and 0.74 in LAS.5 ConclusionWe have defined the first dynamic oracle fora transition-based parser supporting unrestrictednon-projectivity.
The oracle is very efficient, com-puting loss in O(n), compared to O(n8) for theonly previously known dynamic oracle with sup-port for a subset of non-projective trees (G?omez-Rodr?
?guez et al, 2014).Experiments on the treebanks from the CoNLL-X and CoNLL-XI shared tasks show that the dy-namic oracle significantly improves accuracy onmany languages over a static oracle baseline.AcknowledgmentsResearch partially funded by the Spanish Min-istry of Economy and Competitiveness/ERDF(grants FFI2014-51978-C2-1-R, FFI2014-51978-C2-2-R), Ministry of Education (FPU grant pro-gram) and Xunta de Galicia (grant R2014/034).260ReferencesGiuseppe Attardi.
2006.
Experiments with a multil-anguage non-projective dependency parser.
In Pro-ceedings of the 10th Conference on ComputationalNatural Language Learning (CoNLL-X), pages 166?170, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency parsing.In Proceedings of the 10th Conference on Computa-tional Natural Language Learning (CoNLL), pages149?164.Jinho D. Choi and Andrew McCallum.
2013.Transition-based dependency parsing with selec-tional branching.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 1052?1062, Sofia, Bulgaria.Michael A. Covington.
2001.
A fundamental al-gorithm for dependency parsing.
In Proceedings ofthe 39th Annual ACM Southeast Conference, pages95?102, New York, NY, USA.
ACM.Yoav Goldberg and Joakim Nivre.
2012.
A dynamicoracle for arc-eager dependency parsing.
In Pro-ceedings of COLING 2012, pages 959?976, Mum-bai, India, December.
Association for Computa-tional Linguistics.Yoav Goldberg and Joakim Nivre.
2013.
Trainingdeterministic parsers with non-deterministic oracles.Transactions of the Association for ComputationalLinguistics, 1:403?414.Yoav Goldberg, Francesco Sartorio, and Giorgio Satta.2014.
A tabular method for dynamic oracles intransition-based parsing.
Transactions of the Asso-ciation for Computational Linguistics, 2:119?130.Carlos G?omez-Rodr?
?guez and Joakim Nivre.
2013.Divisible transition systems and multiplanar de-pendency parsing.
Computational Linguistics,39(4):799?845.Carlos G?omez-Rodr?
?guez, Francesco Sartorio, andGiorgio Satta.
2014.
A polynomial-time dy-namic oracle for non-projective dependency pars-ing.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 917?927.
Association for Compu-tational Linguistics.John Hopcroft and Robert Endre Tarjan.
1973.
Al-gorithm 447: Efficient algorithms for graph manip-ulation.
Commun.
ACM, 16(6):372?378, June.Peter Neuhaus and Norbert Br?oker.
1997.
The com-plexity of recognition of linguistically adequate de-pendency grammars.
In Proceedings of the 35thAnnual Meeting of the Association for Computa-tional Linguistics (ACL) and the 8th Conference ofthe European Chapter of the Association for Com-putational Linguistics (EACL), pages 337?343.Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages915?932, June.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT 03), pages 149?160.
ACL/SIGPARSE.Joakim Nivre.
2008.
Algorithms for Deterministic In-cremental Dependency Parsing.
Computational Lin-guistics, 34(4):513?553.Robert Endre Tarjan.
1972.
Depth-first search and lin-ear graph algorithms.
SIAM J.
Comput., 1(2):146?160.Alexander Volokh and G?unter Neumann.
2012.
De-pendency parsing with efficient feature extraction.In Birte Glimm and Antonio Kr?uger, editors, KI,volume 7526 of Lecture Notes in Computer Science,pages 253?256.
Springer.Alexander Volokh.
2013.
Performance-Oriented De-pendency Parsing.
Doctoral dissertation, SaarlandUniversity, Saarbr?ucken, Germany.Alexander Yeh.
2000.
More accurate tests for the stat-istical significance of result differences.
In Proceed-ings of the 18th International Conference on Com-putational Linguistics (COLING), pages 947?953.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers - Volume 2, pages188?193.261
