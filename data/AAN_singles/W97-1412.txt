A Syndetic Approach to Referring Phenomena in Mult imodalInteractionGiorgio P.FacontiCNUCE Institute,National Research Council of Italy,56126 Pisa, ItalyG.
Facont ?@cnuce.
cnr.
?tMieke Mass inkCNUCE Institute,National Research Council of Italy,56126 Pisa, ItalyM.
Mass ink@guest .
cnuce,  cnr.
it1 Int roduct ionUser interfaces of many application systems have be-gun to include multiple devices which can be usedtogether to input single expressions.
Such interfaces(and even the whole application systems) are widelylabelled multi-modal, since they use different typesof communication channels to acquire information.These emerging devices and recognition systemspotentially allow users to express their intentionsmore naturally, in ways similar to those used by hu-mans to communicate with each other.
However,very few works have concentrated on the integrationand synergistic use of multimodal input capabilitieswithin the same system.
Most systems simply takealmost no account of how the different modes inter-act so that the interdependence of modalities con-tributing to information processing is not capitalizedupon.
Moreover, the close interaction and interde-pendency between input and output is still a largelyunexplored area.
For example, the capability of re-ferring directly to the content of a rich multimodalpresentation while formulating multimodal input re-quires the processing of a body of knowledge thatlargely extend the information content that can beconveyed by a simple pick operation.Underlying practical use of these new technologiesis the question of their suitability: are they appro-priate for the tasks users need to perform, and whatis their comparative ease of use?In order to build artifacts that are to be both use-ful and usable, the development of interactive sys-tems must address user-oriented requirements andaccommodate different perspectives in the (formal)design process.
Novel interaction techniques mayinterfere with the functional and task-oriented re-quirements that a system is intended to support.
Po-tential conflicts between these types of requirementscan be identified early in the design process throughthe use of appropriate specification techniques usingmathematical structures able to represent perceiv-able elements of the system and allowing for multi-disciplinary insight into the design problem.This work describes an approach to evaluating theusability of devices that accounts for the cognitiveresources needed to use a device to perform particu-lar tasks.
The framework draws its expressive powerfrom a technique called syndetic modelling that al-lows the description of both the device and cognitiveresources to be captured in a common representa-tion.
In this paper syndesis provides a foundation forexamining the interplay occurring betwen an oper-ator and a computer system when performing tasksinvolving deltic references made through speech andgestures.
It is the relationship between users andsystems, and the transformations that are necessaryto move from one to another, that provides novelinsight into usability.2 Syndet ic  Model l ingThe word syndesis comes from the ancient greek(aw = together and 5~o~ = to tie), meaning to bring,to connect, to compose together.
It conveys the ideaof being able to reason about complex systems as awhole while keeping the capability of isolating andreasoning about their basic components at the sametime.In our case, the syndetic model of an interactivesystem extends the formal model of its interface withthe model of the cognitive resources needed to inter-act with the devices.
Earlier work in this directionhas been using state based notations and was aim-ing at the exploration of this field at a high level ofabstraction (Barker and Buxton, 1987; Chan et al,1984).
In other approaches theoretical models origi-nating from psychology have been used in an indirectway, see for example (Card et al, 1990; Fitts, 1954).We deviates from those early approaches by usingcognitive models in a direct way within the designand specification process and find our justificationfor such an approach in that the factors that affect8~ G.P.
Faconti and M. Massinkusability depend on psychological nd social proper-ties of cognition and work, rather than on abstractmathematical models of programming semantics.Although in principle any cognitive theory mightbe adopted, we address one particular cognitivemodel, Phil Barnard's Interacting Cognitive Sub-systems or shortly ICS (Barnard and May, 1993;Barnard and May, 1994).
We formally model aspectsof this theory in such a way that it can be combinedwith a traditional system specification.
The formalmodel of the system provides few insights into theusability of its interface as well as the formal modelof the user derived from some psychological theorysupports general claims about the user's cognitiveprocesses but not about the effective use of cognitiveresources in a given context.
By combining both ofthem in a syndetic model we can reason about howcognitive resources are mapped onto the functional-ity of the system.Within this approach, we consider an abstractview of the flow of information between devices,users and system.
To facilitate precise descriptionand modelling at this level, we make use of a spec-ification notation in which the various components(device, system and user) are modelled as interac-tors.
The concept of an interactor has been de--scribed in detail elsewhere, for example (Duke andHarrison, 1993; Faconti and Paterno, 1990).
Briefly,an interactor is an object-like ntity with an internalstate, a presentation through which parts of the state(called percepts) can be perceived by a user, and ac-tions - either user or system initiated - that bringabout changes to the state.
Interactors have beendescribed using a number of formal notations includ-ing Z, LOTOS and MAL (Modal Action Logic), andit is the last of these that is used here.
Briefly, MAL(Ryan et al, 1991) is a typed first-order logic thatextends the predicate logic with an additional op-erator.
For any action 'A' and predicate 'P', thepredicate '\[A\] P' means that after the action A isperformed, P must hold.Interactors can describe the logical and physicalcomponents of an interactive system, but by them-selves give little direct insight into how a user mightor might not be able to use the system.
This is aproblem, as many of the developments in interac-tive systems that can benefit from use of abstractmodels also depend critically on human abilities toprocess information.
Syndetic models (Duke, 1995;Duke et al, 1995; Faconti and Duke, 1996) addressthis problem by expressing the behaviour of comput-ing and cognitive systems within a common frame-work that supports reasoning about the conjoint sys-tem.
Clearly, the 'computer' component of a synde-tic model is determined by the system being repre-sented, but for the cognitive side there is a range ofmodels to choose from, each emphasising differentaspects of human information processing.
The ap-proach that we have adopted for syndetic modellingis called Interacting Cognitive Subsystems, or ICS,and is summarised in Section 3.
Importantly, ICSoperates in terms of resources and information flowat a level of abstraction that is commensurate withthat used to describe interactors.3 In teract ive  Cogn i t ive  Subsystems( ICS)ICS is a comprehensive model of human informationprocessing that describes cognition in terms a collec-tion of sub-systems that operate on specific mentalcodes or representations.
Barnard and May identifytwo major aspects of ICS: a theory of representationand a theory of information flow.
Interestingly, thetwo kind of theories can be related respectively toabstract data types and state based specifications,and to process algebraic and data flow approachesin computer science.
The work on syndetic mod-elling has concentrated only on the capturing thetheory of information flow and on exploring prob-lems by reasoning about information flow.
The areaof the representation of the mental codes has not yetbeen explored from a formal perspective.
Recently,the ICS project at MRC Applied Psychology Unit inCambridge and at the Departments of Psychologyof University of Sheffield and Copenhagen has de-veloped a systematic treatment of visual structures(May et al, 1995; May et al, 1997) that will be partof our future research.3.1 In fo rmat ion  flow in ICSICS represents the human information processing asa highly parallel organization with a modular struc-ture composed of nine sub-systems.
Although spe-cialised to deal with specific codes, all subsystemshave a common architecture, shown in Figure 1.from store~l~ll~l~l~ll~l  storeI _i.p= of ~ | _1 ~11 trensform C to X.o,,o 'c,oYinput an'ay ~ JFigure I: Generic structure of an ICS subsystem.These subsystems can perform two kinds of oper-ation upon the representations that they receive atmIIA Syndetic Approach to Referring Phenomena in Multimodal Interaction 85an input array.
They can copy the representationdirectly into the image record, which acts as a mem-ory local to each subsystem, and they can transformthe information into another mental representationand pass it through a data network to other sub-systems.
The transformation processes within eachsubsystem are independent and can work in parallel.The representations that can be output by a sub-system are limited by the informational content ofthe representations that it operates upon; that is,a subsystem cannot produce output in every repre-sentation.
Moreover, any one transformation pro-cess can only operate upon a single coherent datastream at one time.
That is, it can only operateupon one representation, and can only produce oneoutput representation.If the incoming data is incomplete, a subsystemcan augment it by accessing the image record.
Co-herent data streams may be blended at the inputarray of a subsystem, with the result that a pro-cess can transform data derived from multiple inputsources in one step.
This balances the output limi-tation.The nine subsystems are further distinguished de-pending on their functionality as:Sensory subsystemsV IS  visual: hue, contour etc.AC  acoustic: pitch, rhythm etc.BS  body-state: proprioceptive feedbackMeaning subsystemsIMPL IC  implicational: holistic meaningPROP propositional: semantic relationsStructural subsystemsOBJ  object: mental imagery, etc.MPLmorphonolexical: lexical forms, etc.Effector subsystemsART articulatory: subvocal rehearsal, etc.L IM limb: motion of limbs, eyes, etcThe nine subsystems acts effectively as commu-nicating processes running in parallel as shown inFigure 2.The overall behaviour of the cognitive system isgoverned by a number of principles, most of whichare out of the scope of this paper.
Here, we will ad-dress only those configurations that are relevant tointeract with the system described in the previoussection.
Configurations are the way in which ICSresources are deployed at a point in time to per-form a cognitive task.
Complex configurations canbe constructed from elementary, partial ones, and ifan information flow can be constructed, then it isa legal configuration, subject to three constraints.The first one is that no process can appear morethan once in a configuration.
The second constraintis that the order of cyclical flows within the configu-ration is not important.
Finally, although any one ofthe sensors or effectors may be missing, if all sensorsor effectors or both are missing in a configurationthere must be a central flow.
In other terms, inputalone is meaningless and no output can be generatedwithout either input or central activity.Figure 2: Architecture of ICS.3.1.1 A formal  account  o f  ICSThe key observation underlying syndetic mod-elling is that the structures and principles embodiedwithin ICS can be formulated as an axiomatic modelin the same way as any other information processingsystem.
This means that the cognitive resources of auser can be expressed in the same framework as thebehaviour of computer-based interface, allowing themodels to be integrated irectly.
To begin this pro-cess, we define some sets to represent those conceptsof ICS that will be used here.
Here and elsewherein this document we will make use of the Z notation(Spivey, 1982) to define data types; much of this isbased on common mathematical  conventions for setsand relations, for example 'x '  for cartesian productand 'IP' for power set.\[sys\] : ICS subsystems\[repr\] : representationstr == sysxsysRepresentations consist of basic units of informa-tion organised into superordinate structures.
Coher-ence of units depends on several issues, including thetiming of data streams, that will not be addressedhere.
Instead, coherence is captured abstractly inthe form of an equivalence relation over representa-tions:_ ~ _ : repr ~ reprIn describing ICS it is also useful to discuss therepresentations that are being delivered as part ofa particular data stream.
We therefore introduce a86 G.P.
Faconti and M. Massinkfurther set, code, whose elements are representationsthat have been labelled by the subsystem in whichthey were generated.
Representations from or to theoutside world are tagged with '*'"code == repr?sysIn general we will write R, ys for the code (R, sys),and ':src-dst:' for the transformation (arc, dst).The state of the ICS interactor captures the datastreams involved in processing activities and theproperties of the streams such as stability and co-herence which define the quality of processing, orin other words, user competence at particular tasks.The sources of data for each transformation is repre-sented by a function 'sources' that takes each trans-formation 't' to the set of transformations fromwhich 't' is taking input.
In general only a sub-set of transformations are producing stable output,and this set is defined by the attribute 'stable'.
Thecodes that are available for processing at a subsys-tem are identified by a relation _~_, where 'c@s'means that code 'c' is available at subsystem 's'.interactor ICSattributessources : tr --~ IFtrstable : IF tr_@_ : code ~ sysAs not all representations are coherent, only cer-tain subsets of the data streams arriving at a sys-tem can be employed by a process to generate stableoutput.
The set 'coherent' contains those groups oftransformations whose output in the current statecan be blended.
If the inputs to a process are co-herent but unstable, the process can still generatea stable output by buffering the input flow via theimage record and thereby operating on an extendedrepresentation.
However, only one process in theconfiguration can be buffered at any time 1, and thisprocess is identified by the attribute 'buffered'.
Theconfiguration itself is defined to be those processeswhose output is stable and which are contributingto the current processing activity.coherent : IF IFtrbuffered : trconfig : IFtrFour actions are addressed in this model.
Thefirst two, 'engage' and 'disengage', allow a process tomodify the set of streams from which they are tak-ing information, by adding or removing a stream.
Aprocess can enter buffered mode via the 'buffer' ac-tion.
Lastly, the actual processing of information istThis is actually a simplification for the purposes ofthe paper.represented by 'trans', which allows representationsat one subsystem to be transferred by processing ac-tivity to another subsystem.actionsengage : tr x trdisengage : tr x trbuffertransThe principles of information processing embodiedby ICS are expressed as axioms over the model de-fined above.
Ax iom i concerns coherence, and statesthat a group of processes are coherent if and only ifthey have the same kind of output (in the code of thesystem 'dest') and that the representations producedby the processes and therefore available at 'dest' arethemselves coherent.axioms1 V trs : IF tr ?
trs E coherent3 dest : sys ?Vs, t : sys ?
:s-t: E trs ~ t = destAVs, t sys; p ,q : repr ?
:s-dest: E trs A ps@dest /A =~p~q:t-dest: E trs A qtOdest /The second axiom is that a transformation is sta-ble if and only if its sources are coherent, and eitherit is buffered or the sources are themselves stable.
Aconfiguration then consists of those processes thatare generating stable output that is used elsewherein the overall processing cycle.2 t E stable ~ sources(t) E coherent A(t = buffered V sources(t) C_ stable)3 t ?
config ~ (t ?
stable A 3 s ?
t ?
sources(s))A process will not engage an unstable stream (ax-iom 4).
If its own output is unstable, it will either en-gage a stable stream, disengage an unstable stream,or try to enter buffered mode (axiom 5).
The re-maining axioms (5-7) define the effects of the threeactions.4 per(engage(t, arc)) ~ arc ?
stable5 t ~ stable =~.3 s ?
s ?
stable ^ s ~ sources(t) ^obl(engage(t, s))V3 s s s ~ stable A s ?
sources(t) Aobl(disengage(t, s))vobl(buffer(t))The effects of the buffer, engage, and disengage ac-tions are straightforward and are given by axioms 6-A Syndetic Approach to Referring Phenomena in Multimodal Interaction 878067\[buffer(t)\] buffered : tsources(t) : S\[engage(t, s)\] sources(t) : S U {s~.8 sources(t) = S\[disengage(t, s)\] sources(t) : S - {s}The remaining two axioms define the effect of in-formation transfer.
Ax iom 9 is the 'forward' rule:if a representation is available at a subsystem, thenafter trans a suitable representation will be availableat any other subsystem for which the correspondingprocess is stable.
Conversely, if after trans some in-formation were to become available at a subsystem(dest), then there must exist some source systemsuch that the information is available at the source,and the corresponding transformation is stable.9 px~src A :src-dst: G stable ~ \[trans\] psrc@dst10 (3 p : repr; src, dst : sys * \[trans\] p,cGdst)3x  : sys ?
px@src A :src-dst: G stable3.2 The  s t ruc ture  of  menta l  representat ionsMost of the formal account of ICS given in the pre-vious section relies on an understanding of represen-tations and of their structure.In (May et al, 1997) the process of perception isdescribed as one of structuring the sensory informa-tion that we receive from objects in the environmentso that we can interact with them.
The details aboutthe structure of objects and their inter-relations arenot explicitly contained in the sensory information.It must be interpreted by combining this informa-tion with knowledge about the world, which we havelearnt through our experience of interacting with it.Computer displays are like the rest of the worldin this respect.
Consequently designing a computerdisplay is all about choosing the form of objects andarranging them so that they are perceived and dealtwith by the user of the computer.
Different arrange-ments of the same set of forms may lead to differentstructuring of objects' representations.
This may re-sult in different performances of a particular task bythe user.When we look at a visual scene, the features, col-ors and textures in the sensory information grouptogether to form objects.
If we look closely at anobject, we can see that it has also a structure andmay be composed by other objects.
We can see theworld at different scales, from a global level, down tomany levels of details.
For example, figure 3 can beseen as a computer display with objects in it.
Focus-ing the attention toward a particular object we maysee either a window or a cursor and so on.
This hier-archy can be represented as a structure diagram, asin figure 4, where the horizontal groupings are setsof objects at different levels of the visual structure.ChicagoFrankfurtLondonLos PztgelesNewYmlr,RomeTokio~L ~ t;:~,~iFigure 3: Objects within a computer display.CURSOR ) WINDOWSCROLLBAR~ LIST( - ) ,  )Figure 4: Information Structure.What we perceive at a given moment is limited bythe level at which we analyse the scene.
For exam-ple, a test made with figure 3 on a number of ourcolleagues revealed that the totality of them sees a'list of cities that can be scrolled'.
Clearly, this infor-mation is the result of an interpretation of the rawsensory data obtained from the eyes and enriched bya set of mental processes that convert he visual rep-resentation into an object one to which a semanticinformation is further added.What it is important to notice is that the attentionhas been focused on the 'list' node in the structure,that to reach that node one might have searchedthrough it, and that 'list' is related to 'scrollbar'.According to (May et al, 1997) we say that 'list'is the psychological subject being attended, 'scroll-bar'  (i.e.
objects in the same group of the psycho-logical subject) forms its predicate, and 'cities' (i.e.the sub-structure rooted at the psychological sub-ject) form its attribute.
The attention can be easily88 G.P.
Faconti and M. Massinkmoved towards one of the predicates of the subjectby swapping the subject-predicate relation.
Divert-ing the attention to a far object in the structurerequires much more cognitive load since it impliesthe traversal of larger parts of the structure.Clearly we are describing a 'static' situation wherethe persons were explicitly asked to perform onlya recognition task.
In dynamic (real case) situa-tions the same sensory information is interpreted toperform different tasks either in a sequence or inparallel.
For example, to move the cursor over anitem (i.e.
a city name) one must establish a rela-tion between the cursor and the item that requiresa reworking of the structure.
This can be describedas defining a ghost object to which both the cursorand the item are rooted.
The ghost is maintaineduntil the cursor-item relation is needed to performthe required task and hides the previous structurefor that period of time as shown in figure 5.
Duringthis period the objects in hiding cannot part of thepsychological subject.
Designing presentations lead-ing to stable structures over tasks greatly increasesthe ease of the interaction by reducing the cognitiveload necessary for the restructuring of structures.DISPLAY, ,s S ~?7 ~?
GHOSTea?
1 ?
/?
?
?
?
/ ITEMFigure 5: Ghost node within the information struc-ture.This reasoning leads to add a further axiom to theICS theory.
Two transformation processes withinthe same subsystem can act in parallel over the samerepresentation or over two representations such thatone is not a sub-structure of the other (they are d/s-joint).
Disjonction is captured abstractly in the formof a relation over code:_ ~ _ : code ~-~ code11 (3 p, q : repr; src, dl, d2 : sys ?
[trans] p,c~dl A q,c~d2 A dl ~ d2) ::?,p ~ q A px@src : qyOsrc Vpx@src @ qy@src3 .2 .1  Levels of  menta l  representat ionsIn the previous ection we have seen that sensoryinformation is interpreted in order to build struc-tured mental representations.
The interpretation re-quires the participation of several subsystems thatare deployed in a configuration.
The understand-ing of the structure in figure 4 is given by that thesensory information from eyes forms a visual repre-sentation made of colours and the likes that givesrise to the configuration represented in figure 6.:mpl-prop:Figure 6: Reading configuration.A mental process (VIS) transforms (:vis-obj:) itinto an object representation that involves the struc-turing of sensory data into objects, and the groupingtogether of those objects.
This new representationcan be interpreted by another mental process (OBJ)and transformed (:obj-prop:) to produce a more ab-stract representation at propositional level in whichobjects are identified and related.
At this point athird transformation (:prop-obj:) takes place at thepropositional subsystem (PROP)  that feeds back in-formation about object structure.
After this tran-formation the object structure that is perceived isa blend of information from propositional and vi-sual sources.
For this to take place, a number ofconditions must be met according to the formal ICStheory, such as:?
[:prop-obj:, :vis-obj:} E coherentPpropQobj A qvijobj ~ q ~, p?
[:prop-obj:, :vis-obj:, :obj-prop:} C_ stableThe configuration deployed so far doesn't justifythat the items in the list are recognized as cities.
Inorder to do this the objects' structure must be madeavailable to the morphonolexical system (MPL) as astructured representation of sound.
Consequently,A Syndetic Approach to Referring Phenomena in Multimodal Interaction 89the :obj-mph transformation operates in parallelwith the :obj-prop: one on the same code andproduces a morphonolexical representation that isequivalent to the one sent to the propositional sus-bsystem.
The morphonolexicai susbsystems trans-forms (:mpl-prop:) the representation i to propo-sitionai code that is blended to the one produceddirectly by the object susbsystem.
At the proposi-tional system the :prop-mph transformation is acti-vated in parallel with the :prop-obj: that feeds backsemantic information to the morphonolexical systemand enrich the object structure by blending with theobject source.
Again this requires that some addi-tional properties are satisfied in the ICS theory, suchas :{{:obj-prop:, :mpl-prop:},{: prop - mpl :, : obj - mpl :}} E coherentPobj~prop A qmpl~prop ~ q ~, pPobj~mpl A qp,op@mpl ~ q ~ p{:obj-mph, :mpl-prop:, prop-mph} C_stable4 The cognitive configuration fordeitic referenceThe configuration described in the previous sectioncan be defined as the reading one.
In fact, it might benoticed that once the object representation is trans-formed by :obj-mph and made available to the mor-phonolexical subsystem it is also ready to be spokenby the articulatory system after an :mpl-art: trans-formation.
This read aloud configuration is obtainedby adding the :mpl-art: and the :art-speech: trans-formations to the reading configuration so that{:mpl-art:, art-speech:} C_ stableA similar reasoning can be applied to the objectsubsystem in the sense that once the object structureis formed, the :obj-lim: tranformation can generatethe limb code equivalent to the object representa-tion so that (for example) the hand operates thecurrently selected psychological subject.
The newconfiguration is obtained by imposing that?
~:obj-lim:, lim-hand:} C_ stableTogether with the described configuration twofeedback loops exist involving the body-state subsys-tem which is a source of sensory information.
Thisinformation represents sensations that our body de-tects from tasting, touching and smelling as well asinformation from internal sensations such as the po-sition of our arms and legs and the state of our mus-cles.In our case the body state transforms two dis-joint representations from an interpretation of thehand position and muscle state, and of the state ofthe vocal muscles.
The information at this level ofrepresentation is important to co-ordinate our phys-ical actions because they enrich the limb and artic-ulatory representations by blending with those pro-duced by the object and the morphonolexical sub-systems.
Clearly, the followings must hold:{:bs-lim: bs-art:} C stablePbs@art A qbs~lim ~ p .~bs ?
q.
@bsI MPL I :mpl-art:PROP ~:ob j  mp!
:pmp-obj: : - : :o~,op: i_ ART.'bs-art:.'bx.lim.
"Figure 7: Speech and gestures configuration.The final configuration describing the cognitiveview of performing a deitic reference by speech andgestures is shown in figure 7.
In the following wewill refer to the configuration as deixis - Conf.5 Description of the system interfaceFrom the system perspective, the problem can nowbe formulated as the speficatlon of a presentationthat allows the speech and gesture configuration ofICS to be naturally deployed when making use ofdeixis.In principles, the devices we could use to imple-ment an interface supporting deixis range from tra-ditional tablets to data gloves, from cameras to videorecorders and players, from speakers to microphones,from fiat to head mounted isplays with stereoscopicviews, and many others.
Here we will comparetwo systems respectively built from a display and amouse, and a display equipped with a touch screen.The comparison can be easily extended to the caseof devices with similar caracteristics with respect othe addressed task such as a tablet instead of themouse, and a data-glove for the touch screen.5.1 D isp lay  and  mouse  based in ter faceThe most common and widespread graphical deviceis the 2D mouse, a physical device equipped with twotransducers able to measure the distance between acurrent position and a next point along two axesand with a number of buttons (usuaily from one to90 G.P.
Faconti and M. Massinkthree).
The buttons have little value for the pur-poses of this paper, and are disregarded.
The mousecan be described by a very simple interactor, wherethe type 'RelPos' represents re la t ive  positions, i.e.offsets.interactor Mouseattr ibutesmouse : RelPosact ions\[~\] operate : RelPosax ioms1 \[operate(a)\]  mouse  = a2 \[operate\] in \[Mouse\]The Mouse interactor describes the state space ofthe device as a coordinate defining the distance ofthe current position from the previous one along twocoordinate axis (RelPos == delta-xMouse x de l ta -yMouse).
The ~' \ ]  decoration of the 'operate' actionmeans that the device is sensed by the body-statesubsystem when it is used, and the notation \[...\]is used to refer to the perceivable aspect of an at-tribute, interactor or action.While the mouse can be used as a pure input de-vice, it is usually coupled with a cursor that providesthe feedback of the current position in the displayspace (DispPos).
The cursor is an object amongstthe others of type Obj in a display whose position isrelated to the mouse by a coordinate transformation.Consequently, we explicitly distinguish the cursor inspecifying a display interactor.interactor MDisplayMouseattr ibutesobjectscursorlocationtransform_ relate _act ionsrenderaxioms12: ~Obj: Obj: Obj --~ DispPos: RelPos --~ DispPos: DispPos ~ DispPoscursor E objectslocation(cursor) = P A mouse = 5\[render\]location(cursor) = P + transform(a) Amouse = (0, O)\[objects\] in \[Display\]o E objects =~(cursor relate olocation(cursor) = location(o) A\[cursor, o\] in \[Display\])Objects are located in the display.
The cursorlocation is computed by transforming the currentmouse movement at the next refresh of the screen(action 'render').
An object in the display is relatedto the cursor when it has the same position.
Thedecoration indicates that the objects in the displayare visually perceivable.5.2 Touch-screen based inter faceIf we plan to use a touch-screen display to build ourinterface, there exist only one device, namely thedisplay.
In contrast with the mouse-display pair, the\[~\] and ~ percepts apply to the same attributes.interactor TDisplayattr ibutes\[~\] objects : IPObjlocation : Obj ~ DispPosactions\[~\] operate : DispPosaxioms1 \[objects\] in ~Display\]6 Building the  Syndet ic  Mode lThe syndetic model of device interaction is createdby introducing both the user and system models intoa new interactor and then defining the axioms thatgovern the conjoint behaviour of the two agents.
Anew attribute (goals) is used to 'contextualise' thegeneric ICS model to the task of making a delticreference as set of pairs Obj x Operation.
Here, amore realistic approach might be to describe a classof desired or acceptable displays.
However, it wouldadd little to the analysis.interactor MDeixisMDisplay (alternatively TDisplay)ICSattributesgoals : II~(Obj x Operation)The configuration must be set to deixis-Conf andthe (goa/s) attribute is initialized.
For the goal tobe achieved we locate the buffer to the propositionalsubsystem to revive the :prop-obj: transformation.axioms1 deixis-Conf C config A buffered = :prop-obj:2 Dgoals = (item, read);((item, speak) II (item, locate))In initializing the goals we use the action prefix ';'notation to indicate sequentiality and 'll' to indicateparallel composition.7 AnalysisWe will examine the above specified model infor-maily, since there is not space to conduct a full for-A Syndetic Approach to Referring Phenomena in Multimodal Interaction 91mal analysis.
The interested reader may address thereferenced papers on syndesis for a more deep un-derstanding.
Here we will show directly the resultof the analysis and will make comments on it.To satisfy the first sub-goal (item, read), theobject subsystem receives coherent representationsfrom :prop-obj: and :vis-obj: that are in its sources.They must be also stable and coherent so that theirrepresentations are blended.
The enriched repre-sentation is tranformed by the object system intopropositional, morphonolexical and limb represen-tations.
Since the goal is to read, the psychologi-cal subject becomes an entry in the list.
The mor-phonolexicai system can operate on this representa-tion in order to find its related sound structure.
Sim-ilarly the propositional system revives it through itsbuffer to both morphonolexical and object systemsenriching their representations.In the case of the MDisplay system, which usesthe mouse, the information transmitted by the ob-ject system is of little use for the limb system.
Infact, the cursor is far from the psychological objectin the representation structure.
Consequently, theinformation from the body-state which 'feels' themouse troough the 'ooperate' action and the onefrom the object system cannot be blended leadingto buffering.
However the buffer is already allocatedand consequently the stream is disengaged leadingto a change of the configuration.In the case of the TDisplay model, which makes useof the touch screen, the same stream resulting fromthe :obj-lim: transformation is relevant to the limbsystem since it blends with the information arriv-ing from the body-state.
In is interesting to notethat in this second case the movement of pointingto an item starts before the same information is pro-cessed by the articulatory system for speaking.
Thisis confirmed by experiments in the field of cognitivepsychology.After one cycle of processing of the goal by all theinvolved subsystems, the propositional system re-moves the first part of the goal and starts satisfyingthe two parallel tasks of speaking and gesturing bysending representations again to the morphonolexi-cal and object subsystems.
At the morphonolexicallevel this representation blends naturally since allthe information was already available for speckingand it can be passed directly to the articulatory sub-system.
At the object level the new representationblend with the information stream from the visualsubsystem.In the case of the MDisplay system, the ghost nodeof figure 5 is built and sent to the propositional sys-tem for semantic checking.
Only after a further loopbetween the propositional and the object systemsthis information is sent to the limb system where itcan now be blended with the body-state informationto perform the pointing gesture.
However, at thistime the articulatory system has already directedthe speech of the referred word.
Consequently, inthis case the speech and locate actions cannot occurin parallel but are performed in a sequence.In the case of the TDisplay model, the limb systemhas already started to locate the item within thescreen so that the operation can continue in par-allel with the articulatory system and synchronizethrough the body state.The result is extremely interesting when relatedto previous works carried on the process of fusion ofinformation within multimedia systems.At University of Grenoble, CLIPS, they have de-velopped an original algorithm, known as the 'melt-ing pot', to support deixis within the Matis system.Matis is a Multimodal Airline Travel InformationSystem supporting several combinations of modali-ties to formulate queries against a flights data base.The melting pot algorithm is built around the in-trinsic uncertainty found in relating mouse eventsand spoken words, the authors have directly experi-mented in building the system.
The practical conse-quence is that the algorithm is noon-deterministic.Our woork clearly gives a motivation for this.In (Faconti et al, 1996) the fusion process is de-scribed at a high level of abstracion.
It defines a sys-tem architecture of fusion and a class of algorithmswhich the melting pot is one instance of.
The workis in line with the findings of this paper suggestingthat a non-deterministic fusion algorithm can be de-velopped based on exact temporal windows withinwhich pointing events may occur.
These temporalwindows are defined by the limb and articulary sub.-systems processes within ICS and can be capturedby the system speech recognizer.8 Conc lus ionsTraditional approaches to evaluating or comparinginput devices have focussed either on the logical be-haviour of the device, or ergonomic aspects of itsuse.
This paper has presented a framework that al-lows analysis of the cognitive ergonomics of inter-action, in terms of the mental resources needed toutilise a particular device for a specific task.
We haveused the model to present a systematic account ofthe diffences between mouse and touch screen.
Theexample was chosen for familarity, rather than fornovelty.
Hoowever, the approach is one that can beextended to rather more sophisticated and problem-atic techniques.92 G.P.
Faconti and M. MassinkOne argument raised against the wider use of syn-detic modelling for human factors evaluations is thelevel of formality involved.
This is a reasonable con-cern, and there are two responses.
The first is thatthe work on syndesis carried out so far has beenprimarily concerned with establishing its feasibilityas a model for explaining interaction, rather thanas a practical tool for industrial use.
We are nowbeginning to explore means by which the level offormality can be tamed, both by supporting devel-opment of formal models with software tools, or byencapsulating the technique within a tool to supportscenario-driven a alysis of interaction.The second response to concern about formalismis that the complexity of modern interfaces, and thesubtle demands that they place on users' cognitiveabilities, calls for an expressive and analytically pow-erful method for modelling and evaluation.
Such amethod needs to be able to span both the user andthe system, in order to capture the interplay betweenthe information available from the system, the ac-tions that can be taken, and the tasks and knowledgeof the user.
We are not advocating syndetic mod-els as a replacement for other design representations.There is an inherent trade-off between the power andgenerality of a notation (Blanford and Duke, 1998),and there are important issues, for example basedon social factors or domain requirements, for whichsyndetic models are either inappropriate or inade-quate.
Likewise however, syndesis brings consider-able analytical power and authority (in the form ofthe underlying cognitive theory) to bear on prob-lems whose complexity makes the use of less formaldesign techniques problematic.Re ferencesR.M.
Baecker and W. Buxton, editors.
1987.
Read-ings in human-computer interaction: A multidis-ciplinary approach.
Morgan-Kaufmann.P.J.
Barnard and J.
May.
1993.
Cognitive mod-elling for user requirements.
In P.F.
Byerley,P.J.
Bernard, and J.
May, editors, Computers,Communication a d Usability: Design Issues, Re-search and Methods for Integrated Services, NorthHolland Series in Telecommunication.
Elsevier.P.J.
Bernard and J.
May.
1994.
Interactionswith advanced graphical interfaces and the de-ployment of latent human knowledge.
In Euro-graphics Wor~hop on Design, Specification andVerification of Interactive Systems, pages 15-49.Springer.A.
Blandford and D.J.
Duke.
1996.
Integrating userand computer system concerns in the design ofinteractive systems.
IEEE Transactions on Soft-ware Engineering.S.K.
Card, J.D.
Mackinlay, and G.G.
Robertson.1990.
The design space of input devices.
In Proc.of CHI'gO.
ACM Press.S.K.
Card, J.D.
Mackinlay, and G.G.
Robertson.1990.
A semantics analysis of the design spaceof input devices.
Human- Computer Interaction.D.J.
Duke.
1995.
Reasoning about gestural inter-action.
Computer Graphics Forum, 14(3):55-66.Conference Issue: Proc.
Eurographics'95, Maas-tricht, The Netherlands.D.J.
Duke, P.J.
Bernard, D.A.
Dues, and J. May.1995.
Systematic development of the humaninterface.
In APSBC'95: Second Asia-PacificSoftware Engineering Conference, pages 313-321.IEEE Computer Society Press.D.J.
Duke and M.D.
Harrison.
1993.
Abstractinteraction objects.
Computer Graphics Forum,12(3):25-36.
Conference Issue: Proc.
Eurograph-ics'93.G.P.Faconti, M. Bordegoni, K.Kansy, T. Rist, P.Trahanias, and M.D.
Wilson.
1996.
FormalFramework and Necessary Properties of the Fu-sion of Input Modes in User Interfaces Interactingwith Computers, Vol.
8(2), pp.
134-161, ElsevierScience B.V.G.
Faconti and D. Duke.
1996.
Device Models.
InF.
Bodart, and J. Vanderdonckt, editors, Design,Specification and Verification of Interactive Sys-tems, pages 73-91.
Springer-Verlag.G.
Faconti and F. Paterno'.
1990.
An approach tothe formal specification of the components of aninteraction.
In C. Vandoni and D. Duce, editors,Eurographics 90, pages 481-494.
North-Holland.P.M.
Fitts.
1954.
The information capacity of thehuman motor system in controlling amplitude ofmovement.
Journal of Ezperimental Psychology,47:381-391.P.
Chan, J.D.
Foley, V.L.
Wallace.
1984.
Thehuman factors of computer graphics interactiontechniques.
Computer Graphics and Applications,4(11).J.
May, S. Scott, and P. Barnard.
1995.
Struc-turing Interfaces - a psychological guide.
Euro-graphics'95 Tutorial Notes.
European associationfor Computer Graphics, Geneva.J.
May, S. Scott, and P. Bernard.
1997.
Mod-elling multimodal interaction: A theory-basedtechniques for design, analysis and support.
IN-TERACT'97 Tutorial Notes.
Available also athttp://www.shef.ac.uk/~cljm/guide.htmlA Syndetic Approach to Referring Phenomena in Multimodal Interaction 93m\[\]munmmmM.
Ryan, J. Fiadeiro, and T. Maibaum.
1991.
Shar-ing actions and attributes in modal action logic.In T. Ito and A.R.
Meyer, editors, Theoretical As-pacts of Computer Software, volume 526 of Lec-ture Notes ia Computer Science, pages 569-593.Springer-Verlag.J.M.
Spivey.
1992.
The Z Notation: A ReferenceManual.
Prentice Hall International, second edi-tion.
