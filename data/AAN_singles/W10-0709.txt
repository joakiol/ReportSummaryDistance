Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 57?61,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsExploring Normalization Techniques for Human Judgments of MachineTranslation Adequacy Collected Using Amazon Mechanical TurkMichael Denkowski and Alon LavieLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15232, USA{mdenkows,alavie}@cs.cmu.eduAbstractThis paper discusses a machine translationevaluation task conducted using Amazon Me-chanical Turk.
We present a translation ade-quacy assessment task for untrained Arabic-speaking annotators and discuss several tech-niques for normalizing the resulting data.
Wepresent a novel 2-stage normalization tech-nique shown to have the best performance onthis task and further discuss the results of alltechniques and the usability of the resultingadequacy scores.1 IntroductionHuman judgments of translation quality play a vitalrole in the development of effective machine trans-lation (MT) systems.
Such judgments can be usedto measure system quality in evaluations (Callison-Burch et al, 2009) and to tune automatic metricssuch as METEOR (Banerjee and Lavie, 2005) whichact as stand-ins for human evaluators.
However, col-lecting reliable human judgments often requires sig-nificant time commitments from expert annotators,leading to a general scarcity of judgments and a sig-nificant time lag when seeking judgments for newtasks or languages.Amazon?s Mechanical Turk (MTurk) service fa-cilitates inexpensive collection of large amounts ofdata from users around the world.
However, Turk-ers are not trained to provide reliable annotations fornatural language processing (NLP) tasks, and someTurkers attempt to game the system by submittingrandom answers.
For these reasons, NLP tasks mustbe designed to be accessible to untrained users anddata normalization techniques must be employed toensure that the data collected is usable.This paper describes a MT evaluation task fortranslations of English into Arabic conducted us-ing MTurk and compares several data normaliza-tion techniques.
A novel 2-stage normalization tech-nique is demonstrated to produce the highest agree-ment between Turkers and experts while retainingenough judgments to provide a robust tuning set forautomatic evaluation metrics.2 Data SetOur data set consists of human adequacy judgmentsfor automatic translations of 1314 English sentencesinto Arabic.
The English source sentences and Ara-bic reference translations are taken from the Arabic-English sections of the NIST Open Machine Trans-lation Evaluation (Garofolo, 2001) data sets for 2002through 2005.
Selected sentences are between 10and 20 words in length on the Arabic side.
Arabicmachine translation (MT) hypotheses are obtainedby passing the English sentences through Google?sfree online translation service.2.1 Data CollectionHuman judgments of translation adequacy are col-lected for each of the 1314 Arabic MT output hy-potheses.
Given a translation hypothesis and thecorresponding reference translation, annotators areasked to assign an adequacy score according to thefollowing scale:4 ?
Hypothesis is completely meaning equivalentwith the reference translation.573 ?
Hypothesis captures more than half of meaningof the reference translation.2 ?
Hypothesis captures less than half of meaningof the reference translation.1 ?
Hypothesis captures no meaning of the refer-ence translation.Adequacy judgments are collected from untrainedArabic-speaking annotators using Amazon?s Me-chanical Turk (MTurk) service.
We create a humanintelligence task (HIT) type that presents Turkerswith a MT hypothesis/reference pair and asks foran adequacy judgment.
To make this task accessi-ble to non-experts, the traditional definitions of ad-equacy scores are replaced with the following: (4)excellent, (3) good, (2) bad, (1) very bad.
Each rat-ing is accompanied by an example from the data setwhich fits the corresponding criteria from the tradi-tional scale.
To make this task accessible to the Ara-bic speakers we would like to complete the HITs,the instructions are provided in Arabic as well as En-glish.To allow experimentation with various data nor-malization techniques, we collect judgments from10 unique Turkers for each of the translations.
Wealso ask an expert to provide ?gold standard?
judg-ments for 101 translations drawn uniformly from thedata.
These 101 translations are recombined with thedata and repeated such that every 6th translation hasa gold standard judgment, resulting in a total of 1455HITs.
We pay Turkers $0.01 per HIT and Ama-zon fees of $0.005 per HIT, leading to a total costof $218.25 for data collection and an effective costof $0.015 per judgment.
Despite requiring Arabicspeakers, our HITs are completed at a rate of 1000-3000 per day.
It should be noted that the vast ma-jority of Turkers working on our HITs are located inIndia, with fewer in Arabic-speaking countries suchas Egypt and Syria.3 Normalization TechniquesWe apply multiple normalization techniques to thedata set and evaluate their relative performance.Several techniques use the following measures:?
?
: For judgments (J = j1...jn) and gold stan-dard (G = g1...gn), we define average distance:?
(J,G) =?ni=1 |gi ?
ji|n?
K: For two annotators, Cohen?s kappa coeffi-cient (Smeeton, 1985) is defined:K =P (A)?
P (E)1?
P (E)where P (A) is the proportion of times that an-notators agree and P (E) is the proportion oftimes that agreement is expected by chance.3.1 Straight AverageThe baseline approach consists of keeping all judg-ments and taking the straight average on a per-translation basis without additional normalization.3.2 Removing Low-Agreement JudgesFollowing Callison-Burch et al (2009), we calcu-late pairwise inter-annotator agreement (P (A)) ofeach annotator with all others and remove judgmentsfrom annotators with P (A) below some threshold.We set this threshold such that the highest overallagreement can be achieved while retaining at leastone judgment for each translation.3.3 Removing Outlying JudgmentsFor a given translation and human judgments(j1...jn), we calculate the distance (?)
of each judg-ment from the mean (j?):?
(ji) = |ji ?
j?|We then remove outlying judgments with ?
(ji) ex-ceeding some threshold.
This threshold is also setsuch that the highest agreement is achieved whileretaining at least one judgment per translation.3.4 Weighted VotingFollowing Callison-Burch (2009), we treat evalua-tion as a weighted voting problem where each anno-tator?s contribution is weighted by agreement witheither a gold standard or with other annotators.
Forthis evaluation, we weigh contribution by P (A) withthe 101 gold standard judgments.583.5 Scaling JudgmentsTo account for the notion that some annotators judgetranslations more harshly than others, we apply per-annotator scaling to the adequacy judgments basedon annotators?
signed distance from gold standardjudgments.
For judgments (J = j1...jn) and goldstandard (G = g1...gn), an additive scaling factor iscalculated:?+(J,G) =?ni=1 gi ?
jinAdding this scaling factor to each judgment has theeffect of shifting the judgments?
center of mass tomatch that of the gold standard.3.6 2-Stage TechniqueWe combine judgment scaling with weighted vot-ing to produce a 2-stage normalization techniqueaddressing two types of divergence in Turker judg-ments from the gold standard.
Divergence can beeither consistent, where Turkers regularly assignhigher or lower scores than experts, or random,where Turkers guess blindly or do not understandthe task.Stage 1: Given a gold standard (G = g1...gn),consistent divergences are corrected by calculat-ing ?+(J,G) for each annotator?s judgments (J =ji...jn) and applying ?+(J,G) to each ji to produceadjusted judgment set J ?.
If ?
(J ?, G) < ?
(J,G),where ?
(J,G) is defined in Section 3, the annotatoris considered consistently divergent and J ?
is usedin place of J .
Inconsistently divergent annotators?judgments are unaffected by this stage.Stage 2: All annotators are considered in aweighted voting scenario.
In this case, annotatorcontribution is determined by a distance measuresimilar to the kappa coefficient.
For judgments (J =j1...jn) and gold standard (G = g1...gn), we define:K?
(J,G) =(max ???(J,G))?
E(?
)max ??
E(?
)where max ?
is the average maximum distance be-tween judgments and E(?)
is the expected distancebetween judgments.
Perfect agreement with the goldstandard produces K?
= 1 while chance agreementproduces K?
= 0.
Annotators with K?
?
0 are re-moved from the voting pool and final scores are cal-culated as the weighted averages of judgments fromall remaining annotators.Type ?
K?Uniform-a 1.02 0.184Uniform-b 1.317 -0.053Gaussian-2 1.069 0.145Gaussian-2.5 0.96 0.232Gaussian-3 1.228 0.018Table 2: Weights assigned to random data4 ResultsTable 1 outlines the performance of all normaliza-tion techniques.
To calculate P (A) and K with thegold standard, final adequacy scores are rounded tothe nearest whole number.
As shown in the table, re-moving low-agreement annotators or outlying judg-ments greatly improves Turker agreement and, inthe case of removing judgments, decreases distancefrom the gold standard.
However, these approachesremove a large portion of the judgments, leaving askewed data set.
When removing judgments, 1172of the 1314 translations receive a score of 3, makingtasks such as tuning automatic metrics infeasible.Weighing votes by agreement with the gold stan-dard retains most judgments, though neither Turkeragreement nor agreement with the gold standard im-proves.
The scaling approach retains all judgmentsand slightly improves correlation and ?, though Kdecreases.
As scaled judgments are not whole num-bers, Turker P (A) and K are not applicable.The 2-stage approach outperforms all other tech-niques when compared against the gold standard,being the only technique to significantly raise cor-relation.
Over 90% of the judgments are used, asshown in Figure 1.
Further, the distribution of fi-nal adequacy scores (shown in Figure 2) resemblesa normal distribution, allowing this data to be usedfor tuning automatic evaluation metrics.4.1 Resistance to RandomnessTo verify that our 2-stage technique handles prob-lematic data properly, we simulate user data from5 unreliable Turkers.
Turkers ?Uniform-a?
and?Uniform-b?
draw answers randomly from a uni-form distribution.
?Gaussian?
Turkers draw answersrandomly from Gaussian distributions with ?
= 1and ?
according to name.
Each ?Turker?
contributesone judgment for each translation.
As shown in Ta-59Gold Standard TurkerTechnique Retained Correlation ?
P (A) K P (A) KStraight Average 14550 0.078 0.988 0.356 0.142 0.484 0.312Remove Judges 6627 -0.152 1.002 0.347 0.129 0.664 0.552Remove Judgments 9250 0 0.891 0.356 0.142 0.944 0.925Weighted Voting 14021 0.152 0.968 0.356 0.142 0.484 0.312Scale Judgments 14550 0.24 0.89 0.317 0.089 N/A N/A2-Stage Technique 13621 0.487 0.836 0.366 0.155 N/A N/ATable 1: Performance of normalization techniques0 0.25 0.5 0.75 10500100015002000250030003500400045005000Vote WeightNumberof JudgmentsFigure 1: Distribution of weights for judgmentsble 2, only Gaussian-2.5 receives substantial weightwhile the others receive low or zero weight.
This fol-lows from the fact that the actual data follows a sim-ilar distribution, and thus the random Turkers havenegligible impact on the final distribution of scores.5 Conclusions and Future WorkWe have presented an Arabic MT evaluation taskconducted using Amazon MTurk and discussedseveral possibilities for normalizing the collecteddata.
Our 2-stage normalization technique has beenshown to provide the highest agreement betweenTurkers and experts while retaining enough judg-ments to avoid problems of data sparsity and appro-priately down-weighting random data.
As we cur-rently have a single set of expert judgments, our fu-ture work involves collecting additional judgmentsfrom multiple experts against which to further testour techniques.
We then plan to use normalized0 .
2 5717077017.77.1734Vote Wig hNVutmbVNihrigVfmVJdnFigure 2: Distribution of adequacy scores after 2-stagenormalizationTurker adequacy judgments to tune an Arabic ver-sion of the METEOR (Banerjee and Lavie, 2005) MTevaluation metric.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Proc.ACL WIEEMMTS.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of WMT09.
InProc.
WMT09.Chris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In Proc.
EMNLP09.John Garofolo.
2001.
NIST Open Machine TranslationEvaluation.
http://www.itl.nist.gov/iad/mig/tests/mt/.N.
C. Smeeton.
1985.
Early History of the Kappa Statis-tic.
In Biometrics, volume 41.60Figure3:ExampleHITasseenbyTurkers61
