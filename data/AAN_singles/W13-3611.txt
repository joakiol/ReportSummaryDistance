Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 82?87,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsIITB System for CoNLL 2013 Shared Task: A Hybrid Approach toGrammatical Error CorrectionAnoop Kunchukuttan Ritesh Shah Pushpak BhattacharyyaDepartment of Computer Science and Engineering, IIT Bombay{anoopk,ritesh,pb}@cse.iitb.ac.inAbstractWe describe our grammar correction sys-tem for the CoNLL-2013 shared task.Our system corrects three of the five er-ror types specified for the shared task -noun-number, determiner and subject-verbagreement errors.
For noun-number anddeterminer correction, we apply a classi-fication approach using rich lexical andsyntactic features.
For subject-verb agree-ment correction, we propose a new rule-based system which utilizes dependencyparse information and a set of conditionalrules to ensure agreement of the verbgroup with its subject.
Our system ob-tained an F-score of 11.03 on the officialtest set using the M2 evaluation method(the official evaluation method).1 IntroductionGrammatical Error Correction (GEC) is an inter-esting and challenging problem and the existingmethods that attempt to solve this problem takerecourse to deep linguistic and statistical analy-sis.
In general, GEC may partly assist in solv-ing natural language processing (NLP) tasks likeMachine Translation, Natural Language Genera-tion etc.
However, a more evident application ofGEC is in building automated grammar checkersthereby benefiting non-native speakers of a lan-guage.
The CoNLL-2013 shared task (Ng et al2013) looks at improving the current approachesfor GEC and for inviting novel perspectives to-wards solving the same.
The shared task makesthe NUCLE corpus (Dahlmeier et al 2013) avail-able in the public domain and participants havebeen asked to correct grammatical errors belong-ing to the following categories: noun-number,determiner, subject-verb agreement (SVA), verbform and preposition.
The key challenges are han-dling interaction between different error groupsand handling potential mistakes made by off-the-shelf NLP components run on erroneous text.For the shared task, we have addressed the fol-lowing problems: noun-number, determiner andsubject-verb agreement correction.
For noun-number and determiner correction, we use a clas-sification based approach to predict corrections- which is a widely used approach (Knight andChander, 1994; Rozovskaya and Roth, 2010).
Forsubject-verb agreement correction, we propose anew rule-based approach which applies a set ofconditional rules to correct the verb group to en-sure its agreement with its subject.
Our systemobtained a score of 11.03 on the official test setusing the M2 method.
Our SVA correction sys-tem performs very well with a F-score of 28.45 onthe official test set.Section 2 outlines our approach to solving thegrammar correction problem.
Sections 3, 4 and5 describe the details of the noun-number, deter-miner and SVA correction components of our sys-tem.
Section 6 explains our experimental setup.Section 7 discusses the results of the experimentsand Section 8 concludes the report.2 Problem FormulationIn this work, we focus on correction of threeerror categories related to nouns: noun-number,determiner and subject-verb agreement.
Thenumber of the noun, the choice of determiner andverb?s agreement in number with the subject areclearly inter-related.
Therefore, a coordinatedapproach is necessary to correct these errors.
Ifthese problems are solved independently of eachother, wrong corrections may be generated.
Thefollowing are some examples:Erroneous sentenceA good workmen does not blame his toolsGood correctionsA good workman does not blame his toolsGood workmen do not blame his tools82noun-numbersubject-verb agreement determinerFigure 1: Dependencies between the noun-number, determiner and subject-verb agreementerrorsBad correctionsA good workman do not blame his toolsGood workman does not blame his toolsThe choice of noun-number is determined bythe discourse and meaning of the text.
The choiceof determiner is partly determined by the noun-number, whereas the verb?s agreement dependscompletely on the number of its subject.
Fig-ure 1 shows the proposed dependencies betweenthe number of a noun, its determiner and num-ber agreement with the verb for which the nounis the subject.
Assuming these dependencies, wefirst correct the noun-number.
The corrections tothe determiner and the verb?s agreement with thesubject are done taking into consideration the cor-rected noun.
The noun-number and determiner arecorrected using a classification based approach,whereas the SVA errors are corrected using a rule-based system; these are described in the followingsections.3 Noun Number CorrectionThe major factors which determine the numberof the noun are: (i) the intended meaning of thetext, (ii) reference to the noun earlier in the dis-course, and (iii) stylistic considerations.
Gram-matical knowledge is insufficient for determiningthe noun-number, which requires a higher level ofnatural language processing.
For instance, con-sider the following examples:(1) I bought all the recommended books.
Thesebooks are costly.
(2) Books are the best friends of man.In Example (1), the choice of plural noun in thesecond sentence is determined by a reference tothe entity in the previous sentence.
Example (2) isa general statement about a class of entities, wherethe noun is generally a plural.
Such phenomenamake noun-number correction a difficult task.
Asinformation at semantic and discourse levels is dif-ficult to encode, we explored lexical and syntacticTokens, POS and chunk tags in?2 word-window around the nounIs the noun capitalized ?Is the noun an acronym ?Is the noun a named entity?Is the noun a mass noun, pluralia tantum?Does the noun group have an article/demonstrative/quantifier?What article/demonstrative/quantifier doesthe noun phrase have ?Are there words indicating plurality inthe context of the noun?The first two words of the sentenceand their POS tagsThe number of the verb for which this nounis the subjectGrammatical Number of majority of nounsin noun phrase conjunctionTable 1: Feature set for noun-number correctioninformation to obtain cues about the number of thenoun.
The following is a summary of the cues wehave investigated:Noun properties: Is the noun a mass noun, a plu-ralia tantum, a named entity or an acronym?Lexical context: The presence of a plurality indi-cating word in the context of the noun (e.g.
theancient scriptures such as the Vedas, Upanishads,etc.
)Syntactic constraints:?
Nouns linked by a conjunction agree witheach other (e.g.
The pens, pencils and books).?
Presence/value of the determiner in the noungroup.
However, this is only a secondary cue,since it is not possible to determine if it is thedeterminer or the noun-number that is incor-rect (e.g.
A books).?
Agreement with the verb of which the noun isthe subject.
This is also a secondary feature.Given that we are dealing with erroneous text,these cues could themselves be wrong.
The prob-lem of noun-number correction is one of mak-ing a prediction based on multiple cues in theface of such uncertainty.
We model the prob-lem as a binary classification problem, the taskbeing to predict if the observed noun-numberof every noun in the text needs correction (la-bels: requires correction/no correction).
Alterna-83tively, we could formulate the problem as a sin-gular/plural number prediction problem, whichwould not require annotated learner corpora text.However, we prefer the former approach since wecan learn corrections from learner corpora text (asopposed to native speaker text) and use knowledgeof the observed number for prediction.
Use of ob-served values has been shown to be beneficial forgrammar correction (Rozovskaya and Roth, 2010;Dahlmeier and Ng, 2011).If the model predicts requires correction, thenthe observed number is toggled to obtain the cor-rected noun-number.
In order to bias the systemtowards improved precision, we apply the correc-tion only if classifier?s confidence score for the re-quires correction prediction exceeds its score forthe no correction prediction by at least a thresholdvalue.
This threshold value is determined empiri-cally.
The feature set designed for the classifier isshown in Table 1.4 Determiner CorrectionDeterminers in English consist of articles, demon-stratives and quantifiers.
The choice of deter-miners, especially articles, depends on many fac-tors including lexical, syntactic, semantic and dis-course phenomena (Han et al 2006).
Therefore,the correct usage of determiners is difficult to mas-ter for second language learners, who may (i) in-sert a determiner where it is not required, (ii) omita required determiner, or (iii) use the wrong de-terminer.
We pose the determiner correction prob-lem as a classification problem, which is a wellexplored method (Han et al 2006; Dahlmeier andNg, 2011).
Every noun group is a training in-stance, with the determiner as the class label.
Ab-sence of a determiner is indicated by a specialclass label NO DET.
However, since the numberof determiners is large, a single multi-class classi-fier will result in ambiguity.
This ambiguity canbe reduced by utilizing of the fact that a partic-ular observed determiner is replaced by one of asmall subset of all possible determiners (which wecall its confusion set).
For instance, the confu-sion set for a is {a, an, the, NO DET}.
It is un-likely that a is replaced by any other determinerlike this, that, etc.
Rozovskaya and Roth (2010)have used this method for training preposition cor-rection systems, which we adopt for training a de-terminer correction system.
For each observed de-terminer, we build a classifier whose prediction isDescription Path1 Direct subjectverbnounnsubjverbnounnsubjpass2 Path through Wh-determinernounwh-determinerref verbrcmodnsubj3 Clausal subjectverbnouncsubjverbnouncsubjpass4 External subjectverb_1nounnsubjverb_2xsubjtoaux5 Path through copulaverbsubj_complementcopnounnsubj6 Subject in a different clauseverb_1verb_3 conjconjunctioncc nounnsubj verb_2conj7 Multiple subjectsnoun_1noun_2 conjnoun_3conj conjunctionccverbnsubjTable 2: Some rules from the singular-ize verb group rule-setlimited to the confusion set of the observed deter-miner.
The confusion sets were obtained from thetraining corpus.
The feature set is almost the sameas the one for noun-number correction.
The onlydifference is that context window features (token,POS and chunk tags) are taken around the deter-miner instead of the noun.5 Subject-Verb AgreementThe task in subject-verb agreement correction is tocorrect the verb group components so that it agreeswith its subject.
The correction could be madeeither to the verb inflection (He run ?
He runs)or to the auxiliary verbs in the verb group (Heare running ?
He is running).
We assume thatnoun-number and verb form errors (tense, aspect,modality) do not exist or have already been cor-rected.
We built a rule-based system for perform-ing SVA correction, whose major components are(i) a system for detecting the subject of a verb, and84(ii) a set of conditional rules to correct the verbgroup.We use a POS tagger, constituency parser anddependency parser for obtaining linguistic infor-mation (noun-number, noun/verb groups, depen-dency paths) required for SVA correction.
Our as-sumption is that these NLP tools are reasonablyrobust and do a good analysis when presented witherroneous text.
We have used the Stanford suite oftools for the shared task and found that it makesfew mistakes on the NUCLE corpus text.The following is our proposed algorithm forSVA correction:1.
Identify noun groups in a sentence and the in-formation associated with each noun group:(i) number of the head noun of the noungroup, (ii) associated noun groups, if thenoun group is part of a noun phrase conjunc-tion, and (iii) head and modifier in each noungroup pair related by the if relation.2.
Identify the verb groups in a sentence.3.
For every verb group, identify its subject asdescribed in Section 5.1.4.
If the verb group does not agree in numberwith its subject, correct each verb group byapplying the conditional rules described inSection 5.2.5.1 Identifying the subject of the verbWe utilize dependency relations (uncollapsed) ob-tained from the Stanford dependency parser toidentify the subject of a verb.
From analysis of de-pendency graphs of sentences in the NUCLE cor-pus, we identified different types of dependencypaths between a verb and its subject, which areshown in Table 2.
Given these possible depen-dency path types, we identify the subject of a verbusing the following procedure:?
First, check if the subject can be reached us-ing a direct dependency path (paths (1), (2),(3) and (4))?
If a direct relation is not found, then look fora subject via path (5)?
If the subject has not been found in the previ-ous step, then look for a subject via path (6)A verb can have multiple subjects, which can beidentified via dependency path (7).Rule Condition Action1 ?w ?
vg, pos tag(w) = MD Do nothing2 ?w ?
vg, pos tag(w) = TO Do nothing3 subject(vg) 6= I Replace are by is4 subject(vg) = I Replace are by am5 do, does /?
vg ?
subject(vg) 6= I Replace have by has6 do, does /?
vg ?
subject(vg) = I Replace has by haveTable 3: Some rules from the singular-ize verb group rule-setw is a word, vg is a verb group, POS tags are from the Penntagset5.2 Correcting the verb groupFor correcting the verb group, we have two sets ofconditional rules (singularize verb group and plu-ralize verb group).
The singularize verb grouprule-set is applied if the subject is singular,whereas the pluralize verb group rule-set is ap-plied if the subject is plural or if there are multi-ple subjects (path (7) in Table 2).
For verbs whichhave subjects related via dependency paths (3) and(4) no correction is done.The conditional rules utilize POS tags and lem-mas in the verb group to check if the verb groupneeds to be corrected and appropriate rules are ap-plied for each condition.
Some rules in the sin-gularize verb group rule-set are shown in Table 3.The rules for the pluralize verb group rule-set areanalogous.6 Experimental SetupOur training data came from the NUCLE corpusprovided for the shared task.
The corpus wassplit into three parts: training set (55151 sen-tences), threshold tuning set (1000 sentences) anddevelopment test set (1000 sentences).
In addi-tion, evaluation was done on the official test set(1381 sentences).
Maximum Entropy classifierswere trained for noun-number and determiner cor-rection systems.
In the training set, the numberof instances with no corrections far exceeds thenumber of instances with corrections.
Therefore,a balanced training set was created by includingall the instances with corrections and sampling?
instances with no corrections from the trainingset.
By trial and error, ?
was determined to be10000 for the noun-number and determiner cor-rection systems.
The confidence score thresholdwhich maximizes the F-score was calibrated onthe tuning set.
We determined threshold = 085TaskDevelopment test set Official test setP R F-1 P R F-1Noun Number 31.43 40 35.2 28.47 9.84 14.66Determiner 35.59 17.5 23.46 21.43 1.3 2.46SVA 16.67 23.42 19.78 29.57 27.42 28.45Integrated 29.59 17.24 21.79 28.18 4.99 11.03Table 4: M2 scores for IIT Bombay correction system: component-wise and integratedfor the noun-number and the determiner correctionsystems.The following tools were used in the devel-opment of the system for the shared task: (i)NLTK (MaxEntClassifier, Wordnet lemmatizer),(ii) Stanford tools - POS Tagger, Parser and NERand Python interface to the Stanford NER, (iii)Lingua::EN::Inflect module for noun and verb plu-ralization, and (iv) Wiktionary list of mass nouns,pluralia tantum.7 Results and DiscussionTable 4 shows the results on the test set (de-velopment and official) for each component ofthe correction system and the integrated system.The evaluation was done using the M2 method(Dahlmeier and Ng, 2012).
This involves comput-ing F1 measure between a set of proposed systemedits and a set of human-annotated gold-standardedits.
However, evaluation is complicated by thefact that there may be multiple edits which gen-erate the same correction.
The following exampleillustrates this behaviour:Source: I ate mangoHypothesis: I ate a mangoThe system edit is ?
a, whereas the gold stan-dard edit is mango?a mango.
Though both theedits result in the same corrected sentence, they donot match.
The M2 algorithm resolves this prob-lem by providing an efficient method to detect thesequence of phrase-level edits between a sourcesentence and a system hypothesis that achieves thehighest overlap with the gold-standard annotation.It is clear that the low recall of the noun-numberand determiner correction components have re-sulted in a low overall score for the system.
Thisunderscores the difficulty of the two problems.The feature sets seem to have been unable to cap-ture the patterns determining the noun-number anddeterminer.
Consider a few examples, where theevidence for correction look strong:1. products such as RFID tracking system havebecome real2.
With the installing of the surveillances forevery corner of SingaporeA cursory inspection of the corpus indicates thatin the absence of a determiner (example (1)), thenoun tends to be plural.
This pattern has not beencaptured by the correction system.
The coverageof the Wiktionary mass noun and pluralia tantumdictionaries is low, hence this feature has not hadthe desired impact (example(2)).The SVA correction component has a reason-ably good precision and recall - performing bestamongst all the correction components.
Sincemost errors affecting agreement (noun-number,verb form, etc.)
were not corrected, the SVAagreement component could not correct the agree-ment errors.
If these errors had been corrected, theaccuracy of the standalone SVA correction com-ponent would have been higher than that indicatedby the official score.
To verify this, we manuallyanalyzed the output from the SVA correction com-ponent and found that 58% of the missed correc-tions and 43% of the erroneous corrections wouldnot have occurred if some of the other related er-rors had been fixed.
If it is assumed that all theseerrors are corrected, the effective accuracy of SVAcorrection increases substantially as shown in Ta-ble 5.
A few errors in the gold standard for SVAagreement were also considered for computing theeffective scores.
The standalone SVA correctionmodule therefore has a good accuracy.A major reason for SVA errors (?18%) iswrong output from NLP modules like the POS tag-ger, chunker and parser.
The following are a fewexamples:?
The verb group is incorrectly identified ifthere is an adverb between the main and aux-iliary verbs.It [do not only restrict] their freedom in all86SVA ScoreDevelopment test set Official test setP R F-1 P R F-1Official 16.67 23.42 19.78 29.57 27.42 28.45Effective 51.02 55.55 53.18 65.32 66.94 66.12Table 5: M2 scores (original and modified) for SVA correctionaspects , but also causes leakage of personalinformation .?
Two adjacent verb groups are not distin-guished as separate chunks by the chunkerwhen the second verb group is non-finite in-volving an infinitive.The police arrested all of them before they[starts to harm] the poor victim.?
The dependency parser makes errors in iden-tifying the subject of a verb.
The noun prob-lems is not identified as the subject of is bythe dependency parser.Although rising of life expectancies is anchallenge to the entire human nation , thedetailed problems each country that will en-counter is different.Some phenomena have not been handled by ourrules.
Our system does not handle the case wherethe subject is a gerund phrase.
Consider the exam-ple,Collecting coupons from individuals are the firststep.The verb-number should be singular when agerund phrase is the subject.
In the absence ofrules to handle this case, coupons is identified asthe subject of are by the dependency parser andconsequently, no correction is done.Our rules do not handle interrogative sentencesand interrogative pronouns.
Hence the followingsentence is not corrected,People do not know who are tracking them.Table 6 provides an analysis of the error typedistribution for SVA errors on the official test set.8 ConclusionIn this paper, we presented a hybrid grammati-cal correction system which incorporates both ma-chine learning and rule-based components.
Weproposed a new rule-based method for subject-verb agreement correction.
As future work, weplan to explore richer features for noun-numberand determiner errors.Error types % distributionNoun-number errors 58.02 %Wrong tagging, chunking, parsing 18.52 %Wrong gold annotations 7.40%Rules not designed 6.1%Others 9.88 %Table 6: Causes for missed SVA corrections andtheir distribution in the official test setReferencesDaniel Dahlmeier and Hwee Tou Ng.
2011.
Grammat-ical error correction with alternating structure opti-mization.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies-Volume 1.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Betterevaluation for grammatical error correction.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerenglish: The NUS Corpus of Learner English.
In Toappear in Proceedings of the 8th Workshop on Inno-vative Use of NLP for Building Educational Appli-cations.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in english article usage bynon-native speakers.
Natural Language Engineer-ing.Kevin Knight and Ishwar Chander.
1994.
Automatedpostediting of documents.
In AAAI.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
The CoNLL-2013 Shared Task on Grammatical Error Correction.In To appear in Proceedings of the Seventeenth Con-ference on Computational Natural Language Learn-ing.Alla Rozovskaya and Dan Roth.
2010.
Generatingconfusion sets for context-sensitive error correction.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing.87
