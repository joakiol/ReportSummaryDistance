Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1013?1023,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsCollective Cross-Document Relation Extraction Without Labelled DataLimin Yao Sebastian Riedel Andrew McCallumUniversity of Massachusetts, Amherst{lmyao,riedel,mccallum}@cs.umass.eduAbstractWe present a novel approach to relation ex-traction that integrates information across doc-uments, performs global inference and re-quires no labelled text.
In particular, wetackle relation extraction and entity identifi-cation jointly.
We use distant supervision totrain a factor graph model for relation ex-traction based on an existing knowledge base(Freebase, derived in parts from Wikipedia).For inference we run an efficient Gibbs sam-pler that leads to linear time joint inference.We evaluate our approach both for an in-domain (Wikipedia) and a more realistic out-of-domain (New York Times Corpus) setting.For the in-domain setting, our joint modelleads to 4% higher precision than an isolatedlocal approach, but has no advantage over apipeline.
For the out-of-domain data, we ben-efit strongly from joint modelling, and observeimprovements in precision of 13% over thepipeline, and 15% over the isolated baseline.1 IntroductionRelation Extraction is the task of predicting seman-tic relations over entities expressed in structured orsemi-structured text.
This includes, for example,the extraction of employer-employee relations men-tioned in newswire, or protein-protein interactionsexpressed in biomedical papers.
It also includes theprediction of entity types such as country, citytownor person, if we consider entity types as unary rela-tions.A particularly attractive approach to relation ex-traction is based on distant supervision.1 Here in1Also called self training, or weak supervision.place of annotated text, only an existing knowl-edge base (KB) is needed to train a relation extrac-tor (Mintz et al, 2009; Bunescu and Mooney, 2007;Riedel et al, 2010).
The facts in the KB are heuris-tically aligned to an unlabelled training corpus, andthe resulting alignment is the basis for learning theextractor.Naturally, the predictions of a distantly supervisedrelation extractor will be less accurate than those ofa supervised one.
While facts of existing knowledgebases are inexpensive to come by, the heuristic align-ment to text will often lead to noisy patterns in learn-ing.
When applied to unseen text, these patterns willproduce noisy facts.
Indeed, we find that extractionprecision still leaves much room for improvement.This room is not as large as in previous work (Mintzet al, 2009) where target text and training KB areclosely related.
However, when we use the knowl-edge base Freebase (Bollacker et al, 2008) and theNew York Times corpus (Sandhaus, 2008), we ob-serve very low precision.
For example, the preci-sion of the top-ranked 50 nationality relationinstances is only 28%.On inspection, it turns out that many of the errorscan be easily identified: they amount to violationsof basic compatibility constraints between facts.
Inparticular, we observe unsatisfied selectional pref-erences of relations towards particular entity typesas types of their arguments.
An example is the factthat the first argument of nationality is alwaysa person while the second is a country.
A sim-ple way to address this is a pipeline: first predictentity types, and then condition on these when pre-dicting relations.
However, this neglects the fact thatrelations could as well be used to help entity typeprediction.1013While there is some existing work on enforcingsuch constraints in a joint fashion (Roth and Yih,2007; Kate and Mooney, 2010; Riedel et al, 2009),they are not directly applicable here.
The differenceis the amount of facts they take into account at thesame time.
They focus on single sentence extrac-tions, and only consider very few interacting facts.This allows them to work with exact optimizationtechniques such as (Integer) Linear Programs andstill remain efficient.2 However, when working ona sentence level they fail to exploit the redundancypresent in a corpus.
Moreover, the fewer facts theyconsider at the same time, the lower the chance thatsome of these will be incompatible, and that mod-elling compatibility will make a difference.In this work we present a novel approach thatperforms relation extraction across documents, en-forces selectional preferences, and needs no labelleddata.
It is based on an undirected graphical modelin which variables correspond to facts, and factorsbetween them measure compatibility.
In order toscale up, we run an efficient Gibbs-Sampler at in-ference time, and train our model using SampleR-ank (Wick et al, 2009).
In practice this leads to aruntime behaviour that is linear in the size of the cor-pus.
For example, 200,000 documents take less thanthree hours for training and testing.For evaluation we consider two scenarios.
Firstwe follow Mintz et al (2009), use Freebase assource of distant supervision, and employ Wikipediaas source of unlabelled text?we will call this anin-domain setting.
This scenario is somewhat arti-ficial in that Freebase itself is partially derived fromWikipedia, and in practice we cannot expect text andtraining knowledge base to be so close.
Hence wealso evaluate our approach on the New York Timescorpus (out-of-domain setting).For in-domain data we make the following find-ing.
When we compare to an isolated baseline thatmakes no use of entity types, our joint model im-proves average precision by 4%.
However, it doesnot outperform a pipelined system.
In the out-of-domain setting, our collective model substantiallyoutperforms both other approaches.
Compared tothe isolated baseline, we achieve a 15% increase in2The pyramid algorithm of Kate and Mooney (2010) mayscale well, but it is not clear how to apply their scheme to cross-document extraction.precision.
With respect to the pipeline approach, theincrease is 13%.In the following we will first give some back-ground information on relation extraction with dis-tant supervision.
Then we will present our graphi-cal model as well as the inference and learning tech-niques we apply.
After discussing related work, wepresent our empirical results and conclude.2 BackgroundIn this section we will introduce the terminology andconcepts we use throughout the paper.
We will alsogive a brief introduction to relation extraction, inparticular in the context of distant supervision.2.1 RelationsWe seek to extract facts about entities.
Example en-tities would be the company founder BILL GATES,the company MICROSOFT, and the country USA.A relation R is a set of tuples c over entities.
Wewill follow (Mintz et al, 2009) and call the termR (c1, .
.
.
cn) with c ?
R a relation instance.3 Itdenotes the membership of the tuple c in the re-lation R. For example, founded (BILL GATES,MICROSOFT) is a relation instance denoting thatBILL GATES and MICROSOFT are related in thefounded relation.In the following we will always consider some setof candidate tuples C that may or may not be re-lated.
We define Cn ?
C to be set of all n-ary tu-ples in C. Note that while our definition considersgeneral n-nary relations, in practice we will restrictus to unary and binary relations C1 and C2.Following previous work (Mintz et al, 2009; Ze-lenko et al, 2003; Culotta and Sorensen, 2004) wemake one more simplifying assumption: every can-didate tuple can be member of at most one relation.2.2 Entity TypesAn entity can be of one or several entity types.
Forexample, BILL GATES is a person, and a companyfounder.
Entity types correspond to the specialcase of relations with arity one, and will be treatedas such in the following.3Other commonly used terms are relational facts, groundfacts, ground atoms, and assertions.1014We care about entity types for two reasons.
First,they can be important for downstream applications:if consumers of our extracted facts know the typeof entities, they can find them more easily, visu-alize them more adequately, and perform opera-tions specific to these types (write emails to persons,book a hotel in a city, etc.).
Second, they are use-ful for extracting binary relations due to selectionalpreferences?see section 2.6.2.3 MentionsIn natural language text spans of tokens are used torefer to entities.
We call such spans entity mentions.Consider, for example, the following sentence snip-pet:(1) Political opponents of President Evo Moralesof Bolivia have in recent days stepped up...Here ?Evo Morales?
is an entity mention of pres-ident EVO MORALES, and ?Bolivia?
a mention ofthe country BOLIVIA he is the president of.People often express relations between entities innatural language texts by mentioning the participat-ing entities in specific syntactic and lexical patterns.We will refer to any tuple of mentions of entities(e1, .
.
.
en) in a sentence as candidate mention tu-ple.
If such a candidate expresses the relation R,then it is a relation mention of the relation instanceR (e1, .
.
.
, en).Consider again example 1.
Here the pair of en-tity mentions (?Evo Morales?, ?Bolivia?)
is a candi-date mention tuple.
In fact, in this case the candidateis indeed a relation mention of the relation instancenationality (EVO MORALES, BOLIVIA).2.4 Relation ExtractionWe define the task of relation extraction as follows.We are given a corpus of documents and a set oftarget relations.
Then we are asked to predict all re-lation instances I so that for each R (c) ?
I thereexists at least one relation mention in the given cor-pus.The above definition covers a range of existingapproaches by varying over what we define as tar-get corpus.
On one end, we have extractors thatprocess text on a per sentence basis (Zelenko et al,2003; Culotta and Sorensen, 2004).
On the otherend, we have methods that take relation mentionsfrom several documents and use these as input fea-tures (Mintz et al, 2009; Bunescu and Mooney,2007).There is a compelling reason for performing re-lation extraction within a larger scope that consid-ers mentions across documents: redundancy.
Oftenfacts are mentioned in several sentences and doc-uments.
Some of these mentions may be difficultto parse, or they use unseen patterns.
But the morementions we consider, the higher the probability thatone does parse, and fits a pattern we have seen in thetraining data.Note that for relation extraction that considersmore than a single mention we have to solve thecoreference problem in order to determine whichmentions refer to the same entity.
In the follow-ing we will assume that coreference clusters are pro-vided by a preprocessing step.2.5 Distant SupervisionIn relation extraction we often encounter a lack ofexplicitly annotated text, but an abundance of struc-tured data sources such as company databases or col-laborative knowledge bases like Freebase.
In orderto exploit this, many approaches use simple but ef-fective heuristics to align existing facts with unla-belled text.
This labelled text can then be used astraining material of a supervised learner.One heuristic is to assume that each candidatemention tuple of a training fact is indeed expressingthe corresponding relation (Bunescu and Mooney,2007).
Mintz et al (2009) refer to this as the dis-tant supervision assumption.Clearly, this heuristic can fail.
Let us againconsider the nationality relation between EVOMORALES and BOLIVIA.
In an 2007 article of theNew York Times we find this relation mention can-didate:(2) ...the troubles faced by Evo Morales inBolivia...This sentence does not directly express that EVOMORALES is a citizen of BOLIVIA, and hence vi-olates the distant supervision assumption.
The prob-lem with this observation is that at training timewe may learn a relatively large weight for thefeature ?<Entity1> in <Entity2>?
associated with1015nationality.
When testing our model we thenencounter a sentence such as(3) Arrest Warrant Issued for Richard Gere inIndia.that leads us to extract that RICHARD GERE is a cit-izen of INDIA.2.6 Global Consistency of FactsAs discussed above, distant supervision can lead tonoisy extractions.
However, such noise can often beeasily identified by testing how compatible the ex-tracted facts are to each other.
In this work we areconcerned with a particular type of compatibility:selectional preferences.Relations require, or prefer, their arguments to beof certain types.
For example, the nationalityrelation requires the first argument to be a person,and the second to be a country.
On inspection,we find that these preferences are often not satis-fied in a baseline distant supervision system akin toMintz et al (2009).
This often results from patternssuch as ?<Entity1> in <Entity2>?
that fire in manycases where <Entity2> is a location, but not acountry.3 ModelOur observations in the previous section suggestthat we should (a) explicitly model compatibil-ity between extracted facts, and (b) integrate ev-idence from several documents to exploit redun-dancy.
In this work we choose a Conditional Ran-dom Field (CRF) to achieve this.
CRFs are a naturalfit for this task: They allow us to capture correlationsin an explicit fashion, and to incorporate overlappinginput features from multiple documents.The hidden output variables of our model areY =(Yc)c?C .
That is, we have one variable Yc for eachcandidate tuple c ?
C .
This variable can take asvalue any relation in C with the same arity as c. Seeexample relation variables in figure 1.The observed input variablesX consists of a fam-ily of variables Xc =(X1c, .
.
.Xmc)m?M for eachcandidate tuple c. Here Xic stores relevant observa-tions we make for the i-th candidate mention tuple ofc in the corpus.
For example, X1BILL GATES,MICROSOFTin figure 1 would contain, among others, the pattern?
[M2] was founded by [M1]?.3.1 Factor TemplatesOur conditional probability distribution over vari-ables X and Y is defined using using a set T offactor templates.
Each template Tj ?
T definesa set of factors {(yi,xi)}, a set Kj of feature in-dices, parameters{?jk}k?Kjand feature functions{f jk}k?Kj.
Together they define the following con-ditional distribution:p (y|x) =1Zx?Tj?T?
(yi,xi)?TjePk?Kj?jkfjk(yi,xi)(4)In our case the set T consists of four templateswe will describe below.
We construct this graphicalmodel using FACTORIE (McCallum et al, 2009), aprobabilistic programming language that simplifiesthe construction process, as well as inference andlearning.3.1.1 Bias TemplateWe use a bias template TBias that prefers certainrelations a priori over others.
When the templateis unrolled, it creates one factor per variable Yc forcandidate tuple c ?
C. The template also consists ofone weight ?Biasr and feature function fBiasr for eachpossible relation r. fBiasr fires if the relation associ-ated with tuple c is r.3.1.2 Mention TemplateIn order to extract relations from text, we needto model the correlation between relation instancesand their mentions in text.
For this purpose we de-fine the template TMen that connects each relationinstance variable Yc with its observed mention vari-ables Xc.
Crucially, this template gathers mentionsfrom multiple documents, and enables us to exploitredundancy.The feature functions of this template are takenfrom Mintz et al (2009).
This includes features thatinspect the lexical content between entity mentionsin the same sentence, and the syntactic path betweenthem.
One example isfMen101 (yc,xc)def=????
?1 yc = founded ?
?i with"M2 was founded by M1" ?
xic0 otherwise.1016founderMicrosoft wasfounded by Bill Gates...personcompanynationalitycountryWith Microsoft chairmanBill Gates soon relinquishing...Bill Gates wasborn in the USA  in 19551nationofElevation Partners , wasfounded by Roger McNamee ...Roger McNamee, USAYrelZ1personR McNameecountryUSAworksforcomp.Microsoft1Z11Z1Roger McNamee, Microsoftfunctionality-factorner-relation-factorsrelation-mention factorsYtypelmention factorsElevation Partners , wasfounded by Roger McNamee ...Elevation Partners , wasfounded by Roger McNamee ...Figure 1: Factor Graph for joint relation mention prediction and relation type identification.fine the following conditional distribution:p (y|x) =1Zx?Tj?T?
(yi,xi)?TjePKjk=1?jkfjk(yi,xi) (3)In our case the set T consist of four templateswe will describe below.
Note that to construct thisgraphical model we use FACTORIE (McCallum etal., 2009), a probabilistic programming languagethat simplifies the construction process, as well asinference and learning.3.1.1 Bias TemplateWe use a bias template TBias that prefers certainrelations a priori over others.
When the template isunrolled, it creates one factor per variable Ycfor can-didate tuple c and one weight ?Biasr and feature func-tion fBiasr for each possible relation r. fBiasr fires ifthe relation associated with tuple c is r.3.1.2 Mention TemplateIn order to extract relations from text, we need tomodel the correlation between relation instances andtheir mentions in text.
For this purpose we definethe mention template TMen that connects each rela-tion instance variable Yc with its observed variablesmention variables XMc .The feature functions of this template are takenfrom (Mintz et al, 2009b) (with minor modifica-tions).
This includes features that inspect the lexicalcontext between entity mentions in the same sen-tence, and the syntactic path between these.
Oneexample isfMen101 (yc,xMc)def=????
?1 yc = founder?m1", director of "m2 ?
xMc0 otherwise.It tests whether for any of the mentions of the can-didate tuple the sequence ", director of " appears be-tween the mentions of the argument entites.Crucially, these templates function on a cross-document level.
They gather all mentions of the can-didate tuple c and extract features from all of these.3.1.3 Se ectional Preference TemplatesTo capture the correlations between entity typesand the relations the entities participate in, we in-troduce the joint template TJoint.
It connects a re-lation instance variable Ye1,...,ea to the entity typevariables Ye1 , .
.
.
, Yen .
To measure the compabil-ity between relation and entity variables, we useone feature f Jointr,t1...ta (and weight ?Jointr,t1...ta) for eachcombination of relation and entity types r, t1 .
.
.
ta.The feature fires when the variables are in thestate r, t1 .
.
.
ta.
After training we would expecta weight ?Jointfounder,person,company to be larger than?Jointfounder,person,country.We also add a template TPair that measures thecompability between Ye1,...,ea and each Yei in iso-lation.
Here we use features fPairi,r,t that fire if ei is1nationofElevation Partners , wasfounded by Roger McNamee ...Roger McNamee, USAYrelZ1personR McNameecountryUSAworksforcomp.Microsoft1Z11Z1Roger McNamee, Microsoftfunctionality-factorner-relation-factorsrelation-mention factorsYtypelmention factorsElevation Partners , wasfounded by Roger McNamee ...Elevation Partners , wasfounded by Roger McNamee ...Figure 1: Factor Graph for joint relation mention prediction and relation type identification.fine the following conditional distribution:p (y|x) =1Zx?Tj?T?
(yi,xi)?TjePKjk=1 ?jkfjk(yi,xi) (3)In our case the set T consist of four templateswe will describe below.
Note that to construct thisgraphical model we use FACTORIE (McCallum etal., 2009), a probabilistic programming languagethat simplifies the construction process, as well asinference and learning.3.1.1 Bias TemplateWe use a bias template TBias that prefers certainrelations a priori over others.
When the template isunrolled, it creates one factor per variable Ycfor can-didate tuple c and one weight ?Biasr and feature func-tion fBiasr for each possible relation r. fBiasr fires ifthe relation associated with tuple c is r.3.1.2 Mention TemplateIn order to extract relations from text, we need tomodel the correlation between relation instances andt ir mentions in text.
For this purpose we definethe mention template TMen that connects each rela-tion instance variable Yc with its observed variablesmention variables XMc .The feature functions of this template are takenfr m (Mintz et al, 2009b) (with inor modifica-tions).
This includes features that inspect the lexicalcontext between entity mentions in the same sen-tence, and the syntactic path between these.
Oneexample isfMen101 (yc,xMc)def=????
?1 yc = founder?m1", director of "m2 ?
xMc0 otherwise.It tests whether for any of the mentions of the can-didate tuple the sequence ", director of " appears be-tween the mentions of the argument entites.Crucially, these templates function on a cross-document level.
They gather all mentions of the can-didate tuple c and extract features from all of these.3.1.3 Selectional Preference TemplatesTo capture the correlations between entity typesand the relations the entities participate in, we in-troduce th j int tem late TJoint.
It connects a re-la ion instance variable Ye1,...,ea to the entity typevariables Ye1 , .
.
.
, Ye .
To measure the compabil-ity between relation and entity variables, we useone feature f Jointr,t1...ta (and weight ?Jointr,t1...ta) for eachcombination of relation and entity types r, t1 .
.
.
ta.The feature fires when the variables are in thestate r, t1 .
.
.
ta.
After training we would expecta weight ?Jointfounder,person,company to be larger than?Jointfounder,person,country.We also add a template TPair that measures thecompability between Ye1,...,ea and each Yei in iso-lation.
Here we use features fPairi,r,t that fire if ei is1nationofElevation Partners , wasfounded by Roger McNamee ...Roger McNamee, USAYrelZ1personR McNameecountryUSAworksforcomp.Microsoft1Z11Z1Roger McNamee, Microsoftfunctionality-factorner-relation-factorsrelation-mention factorsYtypelmention factorsElevation Partners , wasfounded by Roger McNamee ...Elevation Partners , wasfounded by Roger McNamee ...Figure 1: Factor Graph for joint relation mention prediction and relation type identification.fine the following conditional distribution:p (y|x) =1Zx?Tj?T?
(yi,xi)?TjePKjk=1 ?jkfjk(yi,xi) (3)In our case the set T consist of four templateswe will describe below.
Note that to construct thisgraphical model we use FACTORIE (McCallum etal., 2009), a probabilistic programming languagethat simplifies the construction process, as well asinference and learning.3.1.1 Bias TemplateWe use a bias template TBias that prefers certainrelations a priori over others.
When the template isunrolled, it creates one factor per variable Ycfor can-didate tuple c and one weight ?Biasr and feature func-tion fBiasr for each possible relation r. fBiasr fires ifthe relation associated with tuple c is r.3.1.2 Mention TemplateIn order to extract relations from text, we ne d tomodel the correlation between relation instances andtheir mentions in text.
For this purpose we definethe mention template TMen that connects each rela-tion instance variable Yc with its observed variablesmention variables XMc .The feature functions of this template are takenfrom (Mintz et al, 2009b) (with minor modifica-tions).
This includes features that inspect the lexicalcontext between entity mentions in the same sen-tence, and the syntactic path between these.
Oneexample isfMen101 (yc,xMc)def=????
?1 yc = founder?m1", director of "m2 ?
xMc0 otherwise.It tests whether for any of the mentions of the can-didate tuple the sequence ", director of " appears be-tween the mentions of the argument entites.Cruci lly, these templates function on a cross-document level.
Th y gather all mentions of the can-didate tupl c and extract features from all o these.3.1.3 Selectional Preference TemplatesTo capture the correlations between entity typesand the relations he entities participate in, we in-troduce the joint template TJoint.
It connects a re-lation instance variable Ye1,...,e to the entity typevariables Ye1 , .
.
.
, Yen .
To measure the compabil-ity between relation and entity variables, we useone feature f Jointr,t1...ta (and weight ?Jointr, 1...ta) for eachcombination of relation and entity types r, t1 .
.
.
ta.The featur fires when the variables are i thestate r, t1 .
.
.
ta.
After training we would xpecta weight ?Jointfounder,person,company to be larger than?Jointfounder,person,country.We also add a template TPair that measures thecompability between Ye1,...,ea and each Yei in iso-lation.
Here we use features fPairi,r,t tha fire if ei isg ( , ) ?
DKL ( || )g ( ) = log?1?
?i + ?ie?i??
?ie?i.
.
.
+ w?f?
(y , y , y ) + .
.
.> 0= maxy?
,y?
,y?f??y?
, y?
, y?
?< maxy?
,y?
,y?f??y?
, y?
, y??
?1 (y5,7,;x) = exp (.
.
.
+ w f (y;x) + .
.
.)?
(yi,j ;x) = exp?
?kwkfk (yi,j ;x)?p (y;x) =1Zx?1 (y;x) ?
.
.
.
?
?n (y;x)log E [?i]?
?i?i (y;x) = exp (?i?i (y;x))?i = E [?i]YYX1X2g ( , ) ?
DKL ( || )g ( ) = log?1?
?i + ?ie?i??
?ie?i.
.
.
+ w?f?
(y , y , y ) + .
.
.> 0= maxy?
,y?
,y?f??y?
, y?
, y?
?< maxy?
,y?
,y?f??y?
, y?
, y??
?1 (y5,7,;x) = exp (.
.
.
+ w f (y;x) + .
.
.)?
(yi,j ;x) = exp?
?kwkfk (yi,j ;x)?p (y;x) =1Zx?1 (y;x) ?
.
.
.
?
?n (y;x)log E [?i]?
?i?i (y;x) = exp (?i?i (y;x))?i = E [?i]YYYYYX1X2g ( , ) ?
DKL ( || )g ( ) = log?1?
?i + ?ie?i??
?ie?i.
.
.
+ w?f?
(y , y , y ) + .
.
.> 0= maxy?
,y?
,y?f??y?
, y?
, y?
?< maxy?
,y?
,y?f??y?
, y?
, y??
?1 (y5,7,;x) = exp (.
.
.
+ w f (y;x) + .
.
.)?
(yi,j ;x) = exp?
?kwkfk (yi,j ;x)?p (y;x) =1Zx?1 (y;x) ?
.
.
.
?
?n (y;x)log E [?i]?
?i?i (y;x) = exp (?i?i (y;x))?i = E [?i]YYYYYX1X2g ( , ) ?
DKL ( || )g ( ) = log?1?
?i + ?ie?i??
?ie?i.
.
.
+ w?f?
(y , y , y ) + .
.
.> 0= maxy?
,y?
,y?f??y?
, y?
, y?
?< maxy?
,y?
,y?f??y?
, y?
, y?
?1 (y5,7,;x) = exp (.
.
.
+ w f (y;x) + .
.
.)?
(yi,j ;x) = exp?
?kwkfk (yi,j ;x)?p (y;x) =1Zx?1 (y;x) ?
.
.
.
?
?n (y;x)log E [?i]?
?i?i (y;x) = exp (?i?i (y;x))?i = E [?i]YYYYYX1X2g ( , ) ?
DKL ( || )g ( ) = log?1?
?i + ?ie?i??
?ie?i.
.
.
+ w?f?
(y , y , y ) + .
.
.> 0= maxy?
,y?
,y?f??y?
, y?
, y?
?< maxy?
,y?
,y?f??y?
, y?
, y??
?1 (y5,7,;x) = exp (.
.
.
+ w f (y;x) + .
.
.)?
(yi,j ;x) = exp?
?kwkfk (yi,j ;x)?p (y;x) =1Zx?1 (y;x) ?
.
.
.
?
?n (y;x)log E [?i]?
?i?i (y;x) = exp (?i?i )?i = E [?i]YYYYYX1X2g ( , ) ?
DKL ( || )g ( ) = log?1?
?i + ?ie?i??
?ie?i.
.
.
+ w?f?
(y , y , y ) + .
.
.> 0= maxy?
,y?
,y?f??y?
, y?
, y?
?< maxy?
,y?
,y?f??y?
, y?
, y??
?1 (y5,7,; exp (.
.
.
+ w f (y;x) + .
.
.)?
(yi,j ;x) = exp?
?kwkfk (yi,j ;x)?p (y;x) =1Zx?1 (y;x) ?
.
.
.
?
?n (y;x)log E [?i]?
?i?i (y;x) = exp (?i?i (y;x))?i = E [?i]YYYYYX1X2g ( , ) ?
DKL ( || )g ( ) = log?1?
?i + ?ie?i??
?ie?i.
.
.
+ w?f?
(y , y , y ) + .
.
.> 0= maxy?
,y?
,y?f??y?
, y?
, y?
?< maxy?
,y?
,y?f??y?
, y?
, y??
?1 (y5,7,;x) = exp (.
.
.
+ w f (y;x) + .
.
.)?
(yi,j ;x) = exp?
?kwkfk (yi,j ;x)?p (y;x) =1Zx?1 (y;x) ?
.
.
.
?
?n (y;x)log E [?i]?
?i?i (y;x) = exp (?i?i (y;x))?i = E [?i]YYYYYX1X2Figure 1: Factor Graph of our model that captures selectional preferences and functionality constraints.
Forreadability we only label a subsets of equivalent variables and factors.
Note that the graph shows an exampleassignment to variables.It tests whether for any mentions of the candidatetuple the phrase "founded by" appears between thementions of the argument entities.3.1.3 S lectional Preference T mplatesTo capture the correlations between entity typesand relations the entities participate in, we introducethe template TJoint.
It connects a relation instancevariable Ye1,...,en to the individual entity type vari-ables Ye1 , .
.
.
, Yen .
To measure the compatibilitybetween relation and entity variables, we use onefeature f Jointr,t1...ta (and weight ?Jointr,t1...ta) for each com-bination of relation and entity types r, t1 .
.
.
ta.f Jointr,t1...ta fires when the factor variables are in thestate r, t1 .
.
.
ta.
For example, f Jointfounded,person,companyfires if Ye1 is in state person, Ye2 in state company,and Ye1,e2 in state founded.We also add a template TPair that measures thepairwise compatibility between the relation variableYe1,...,ea and each entity variable Yei in isolation.Here we use features fPairi,r,t that fire if ei is the i-th ar-gument of c, has the entity type t and the candidatetuple c is labelled as instance of relation r. For ex-ample, fPair1,founded,person fires if Ye1(argument i = 1)is in state person, and Ye1,e2 in state founded, re-gardless of the state of Ye2 .3.2 InferenceThere are two types of inference we have to perform:sampling from the posterior during training (see sec-tion 3.3), and finding the most likely configuration(aka MAP inference).
In both settings we employ aGibbs sampler (Geman and Geman, 1990) that ran-domly picks a variable Yc and samples its relationvalue conditioned on its Markov Blanket.
At testtime we decrease the temperature of our sampler inorder to find an approximation of the MAP solution.3.3 TrainingMost learning methods need to calculate the modelexpectations (Lafferty et al, 2001) or the MAP con-figuration (Collins, 2002) before making an updateto the parameters.
This step of inference is usuallythe bottleneck for learning, even when performedapproximately.SampleRank (Wick et al, 2009) is a rank-basedlearning framework that alleviates this problem byperforming parameter updates within MCMC infer-ence.
Every pair of consecutive samples in theMCMC chain is ranked according to the model andthe ground truth, and the parameters are updatedwhen the rankings disagree.
This update can fol-low different schemes, here we use MIRA (Cram-mer and Singer, 2003).
This allows the learner toacquire more supervision per instance, and has ledto efficient training for models in which inference1017is expensive and generally intractable (Singh et al,2009).4 Related WorkDistant Supervision Learning to extract relationsby using distant supervision has raised much interestin recent years.
Our work is inspired by Mintz et al(2009) who also use Freebase as distant supervisionsource.
We also heuristically align our knowledgebase to text by making the distant supervision as-sumption (Bunescu and Mooney, 2007; Mintz et al,2009).
However, in contrast to these previous ap-proaches, and other related distant supervision meth-ods (Craven and Kumlien, 1999; Weld et al, 2009;Hoffmann et al, 2010), we perform relation extrac-tion collectively with entity type prediction.Schoenmackers et al (2008) use entailment ruleson assertion extracted by TextRunner to increase re-call.
They also perform cross-document probabilis-tic inference based on Markov Networks.
However,they do not infer the types of entities and work in anopen IE setting.Selectional Preferences In the context of super-vised relation extraction, selectional preferenceshave been applied.
For example, Roth and Yih(2007) have used Linear Programming to enforceconsistency between entity types and extracted re-lations.
Kate and Mooney (2010) use a pyramidparsing scheme to achieve the same.
Riedel et al(2009) use Markov Logic to model interactions be-tween event-argument relations for biomedical eventextraction.
However, their work is (a) supervised,and (b) performs extraction on a per-sentence basis.Carlson et al (2010) also use selectional prefer-ences.
However, instead of exploiting them for train-ing a graphical model using distant supervision, theyuse selectional preferences to improve a bootstrap-ping process.
Here in each iteration of bootstrap-ping, extracted facts that violate compatibility con-straints will not be used to generate additional pat-terns in the next iteration.5 ExperimentsWe set up experiments to answer the following ques-tions: (i) Does the explicit modelling of selectionalpreferences improve accuracy?
(ii) Can we also per-form joint entity and relation extraction in a pipelineand achieve similar results?
(iii) How does ourcross-document approach scale?To answer these questions we carry out experi-ments on two data sets, Wikipedia and New YorkTimes articles, and use Freebase as distant supervi-sion source for both.5.1 Experimental SetupWe follow Mintz et al (2009) and perform two typesof evaluation: held-out and manual.
In both caseswe have a training and a test corpus of documents,and training and test sets of entities.
For held-outevaluation we split the set of entities in Freebase intotraining and test sets.
For manual evaluation we useall Freebase entities during training.
For testing weuse all entities that appear in the test document cor-pus.For both training and testing we then choose thecandidate tuples C that may or may not be relationinstances.
To pick the entities C1 we want to predictentity types for, we choose all entities that are men-tioned at least once in the train/test corpus.
To pickthe entity pairs C2 that we want to predict the rela-tions of, we choose those that appear at least oncetogether in a sentence.The set of candidates C will contain many tupleswhich are not related in any Freebase relations.
Forefficiency, we filter out a large fraction of these neg-ative candidates for training.
The number of neg-ative examples we keep is chosen to be about 10times the number of positive candidates.
This num-ber stems from trading-off the accuracy it leads toand the increased training time it requires.For both manual and held-out evaluation we rankextracted test relation instances in the MAP state ofthe network.
This state is found by sampling 20 iter-ations with a low temperature of 0.00001.
The rank-ing is done according to the log linear score that theassigned relation for a candidate tuple gets from thefactors in its Markov Blanket.
For optimal perfor-mance, the score is normalized by the number of re-lation mentions.For manual evaluation we pick the top ranked 50relation instances for the most frequent relations.We ask three annotators to inspect the mentions ofthese relation instances to decide whether they arecorrect.
Upon disagreement, we use majority vote.To summarize precisions across relations, we take1018their average, and their average weighted by the pro-portion of predicted instances for the given relation.5.1.1 Data preprocessingWe preprocess our textual data as follows:We first use the Stanford named entity recog-nizer (Finkel et al, 2005) to find entity mentions inthe corpus.
The NER tagger segments each docu-ment into sentences and classifies each token intofour categories: PERSON, ORGANIZATION, LO-CATION and NONE.
We treat consecutive tokenswhich share the same category as single entity men-tion.
Then we associate these mentions with Free-base entities.
This is achieved by performing astring match between entity mention phrases and thecanonical names of entities as present in Freebase.For each candidate tuple c with arity 2 and eachof its mention tuples iwe extract a set of featuresXicsimilar to those used in (Mintz et al, 2009): lexical,Part-Of-Speech (POS), named entity and syntacticfeatures, i.e.
features obtained from the dependencyparsing tree of a sentence.
We use the openNLP POStagger4 to obtain POS tags and employ the Malt-Parser (Nivre et al, 2004) for dependency parsing.For candidate tuples with arity 1 (entity types) weuse the following features: the entity?s word form,the POS sequence, the head of the entity in the de-pendency parse tree, the Stanford named entity tag,and the left and right words to the current entitymention phrase.5.1.2 ConfigurationsWe apply the following configurations of our fac-tor graphs.
As our baseline, and roughly equivalentto previous work (Mintz et al, 2009), we pick thetemplates TBias and TMen.
These describe a fully dis-connected graph, and we will refer to this configu-ration as isolated.
Next, we add the templates TJointand TPair to model selectional preferences, and referto this setting as joint.In addition, we evaluate howwell selectional pref-erences can be captured with a simple pipeline.
Forthis pipeline we first train an isolated system for en-tity type prediction.
Then we use the output of theentity type prediction system as input for the relationextraction system.4available at http://opennlp.sourceforge.net/5.1.3 Entity types and Relation typesFreebase contains many relation types and onlya subset of those relation types occur frequentlyin the corpus.
Since classes with very fewtraining instances are generally hard to learn,we restrict ourselves to the 54 most frequentlymentioned relations.
These include, for ex-ample, nationality, contains, foundedand place_of_birth.
Note that we con-vert two Freebase non-binary temporal relationsto binary relations: employment_tenure andplace_lived.
In both cases we simply disregardthe temporal information in the Freebase data.As our main focus is relation extraction, we re-strict ourselves to entity types compatible with ourselected relations.
To this end we inspect the Free-base schema information provided for each relation,and include those entity types that are declared asarguments of our relations.
This leads to 10 entitytypes including person, citytown, country,and company.Note that a Freebase entity can have several types.We pick one of these by choosing the most specificone that is a member of our entity type subset, orMISC if no such member exists.5.2 WikipediaIn our first set of experiments we train and test usingWikipedia as the text corpus.
This is a comparativelyeasy scenario because the facts in Freebase are partlyderived from Wikipedia, hence there is an increasedchance of properly aligning training facts and text.This is similar to the setting of Mintz et al (2009).5.2.1 Held Out EvaluationWe split 1,300,000 Wikipedia articles into train-ing and test sets.
Table 1 shows the statistics for thissplit.
The last row provides the number of negativerelation instances (candidates which are not relatedaccording to Freebase) associated with each data set.Figure 2 shows the precision-recall curves of re-lation extraction for held-out data of various config-urations.
We notice a slight advantage of the jointapproach in the low recall area.
Moreover, the jointmodel predicts more relation instances, as can beseen by its longer line in the graph.For higher recall, the joint model performsslightly worse.
On closer inspection, we find that1019Wikipedia NYTTrain Test Train Test#Documents 900K 400K 177K 39K#Entities 213K 137K 56K 27K#Positive 36K 24K 5K 2K#Negative 219K 590K 64K 94KTable 1: The statistics of held-out evaluation onWikipedia and New York Times.0.0 0.1 0.2 0.3 0.40.40.50.60.70.80.91.0RecallPrecisionjointpipeisolatedFigure 2: Precision-recall curves for various setupsin Wikipedia held-out setting.this observation is somewhat misleading.
Many ofthe predictions of the joint model are not in theheld-out test set derived from Freebase, but never-theless correct.
Hence, to understand if one systemreally outperforms another, we need to rely on man-ual evaluation.Note that the figure only considers binaryrelations?for entity types all configurations per-form similarly.5.2.2 Manual EvaluationAs mentioned above, held-out evaluation in thiscontext suffers from false negatives in Freebase.
Ta-ble 2 therefore shows the results of our manual eval-uation.
They are based on the average, and weightedaverage, of the precisions for the relation instancesof the most frequent relations.
We notice that hereIsolated Pipeline JointWikipedia 0.82 0.87 0.86Wikipedia (w) 0.95 0.94 0.95NYT 0.63 0.65 0.78NYT (w) 0.78 0.82 0.94Table 2: Average and weighted (w) average preci-sion over frequent relations for New York Times andWikipedia data, based on manual evaluation.all systems perform comparably for weighted aver-age precision.
For average precision we see an ad-vantage for both the pipeline and the joint modelover the isolated system.One reason for similar weighted average preci-sions is the fact that all approaches accurately pre-dict a large number of contains instances.
This isdue to very regular and simple patterns inWikipedia.For example, most articles on towns start with ?A isa municipality in the district of B in C, D.?
For thesesentences, the relative position of two location men-tions is a very good predictor of contains.
Whenused as a feature, it leads to high precision for allmodels.
And since contains instances are mostfrequent, and we take the weighted average, resultsare generally close to each other.To summarize: in this in-domain setting, mod-elling compatibility between entity types and rela-tions helps to improve average precision, but notweighted average precision.
This holds for both thejoint and the pipeline model.
However, we will seehow this changes substantially when moving to anout-of-domain scenario.5.3 New York TimesFor our second set of experiments we use NewYork Times data as training and test corpora.
Aswe argued before, this is expected to be the moredifficult?and more realistic?scenario.5.3.1 Held-out EvaluationWe choose all articles of the New York times dur-ing 2005 and 2006 as training corpus.
As test corpuswe use the first 6 months of 2007.Figure 3 shows precision-recall curves for our var-ious setups.
We see that jointly modelling entity10200.00 0.05 0.10 0.15 0.200.20.40.60.81.0RecallPrecisionjointpipeisolatedFigure 3: Precision-recall curves for various setupsin New York Times held-out setting.types and relations helps to improve precision.Due to the smaller overlap between Freebase andNYT data, figure 3 also has to be taken with morecaution.
The systems may predict correct relationinstances that just do not appear in Freebase.
Hencemanual evaluation is even more important.When evaluating entity precision we find that forboth models it is about 84%.
This raises the ques-tion why the joint entity type and relation extrac-tion model outperforms the pipeline on relations.We take a close look at the entities which partici-pate in relations and find that joint model performsbetter on most entity types, for example, countryand citytown.
We also look at the relation in-stances which are predicted by both systems and findthat the joint model does predict correct entity typeswhen the pipeline mis-predicts.
And exactly thesemis-predictions lead the pipeline astray.
Consider-ing binary relation instances where the pipeline failsbut the joint model does not, we observe an entityprecision of 76% for the pipeline and 86% for ourjoint approach.
The joint model fails to correctlypredict some entity types that the pipeline gets right,but these tend to appear in contexts where relationinstances are easy to extract without considering en-Relation Type Iso.
Pipe Jointcontains 0.92 0.98 0.96nationality 0.28 0.64 0.82plc_lived 0.88 0.70 0.96plc_of_birth 0.32 0.20 0.25works_for 0.96 0.98 0.98plc_of_death 0.24 0.40 0.42children 1.00 0.92 0.98founded 0.42 0.34 0.71Table 3: Precision at 50 for the most frequent rela-tions on New York Timestity types.55.3.2 Manual EvaluationManually evaluated precision for New YorkTimes data can be seen in table 2.
In contrast to theWiki setting, here modelling entity types and rela-tions jointly makes a substantial difference.
For av-erage precision, our joint model improves over theisolated baseline by 15%, and over the pipeline by13%.
Similar improvements can be observed forweighted average precision.Let us look at a break-down of precisions withrespect to different relations shown in table 3.
Wesee dramatic improvements for nationality andfounded when applying the joint model.
Note thatthe nationality relation takes a larger part inthe predicted relation instances of the joint modeland hence contributes significantly to the weightedaverage precision.5.4 ScalabilityWe propose to perform joint inference for large scaleinformation extraction.
An obvious concern in thisscenario is scalability.
In practice we find that infer-ence (and hence learning) in our model scales lin-early with the number of candidate tuples.
This canbe seen in figure 4a.
It is to be expected since thenumber of candidates equals the number of variablesthe sampler has to process in each iteration.The above observation also means that our ap-proach scales linearly with corpus size.
To illustrate5Note that our learned preferences are soft, and hence canbe violated in case of wrong entity type predictions.10211e+05 2e+05 3e+05 4e+05 5e+05200300400500600700800Number of Candidate TuplesTime per iteration (seconds)(a) CPU time0 5000 10000 15000 20000 250001e+052e+053e+054e+055e+056e+05Number of DocumentsNumber of Candidate Tuples(b) Candidate tuplesFigure 4: CPU time for one iteration per candidatetuple, and candidate tuples per document.this, figure 4b shows how the number of candidatesscales with the number of documents.
Again we ob-serve a linear behavior.
Since both are linear, we cansay that our joint approach is linear in the number ofdocuments.Total training and test times are moderate, too.For example, the held-out experiments with 200,000NYT documents finish within three hours.6 ConclusionThis paper presents a novel approach to extractingrelational facts from text.
Akin to previous work inrelation extraction with distant supervision, we re-quire no annotated text.
However, instead extract-ing facts in isolation, we model interactions betweenfacts in order to improve precision.
In particular, wecapture selectional preferences of relations.
Thesepreferences are modelled in a cross-document fash-ion using a large scale factor graph.
We show in-ference and learning can be efficiently performedin linear time by Gibbs Sampling and SampleRank.When applied to out-of-domain text, this approachleads to a 15% increase in precision over an isolatedbaseline, and a 13% improvement over a pipelinedsystem.A crucial aspect of our approach is its extensibil-ity.
Since it is exclusively framed in terms of anundirected graphical model, it is conceptually easyto extend it to other types of compatibilities, suchas functionality constraints.
It could also be ex-tended to tackle coreference resolution.
Eventuallywe seek to model the complete process of the au-tomatic construction of KB within this framework,and capture dependencies between extractions in ajoint and principled fashion.
As we have seen here,in particular when learning is less supervised andextractions are noisy, capturing such interactions isparamount.AcknowledgementsThis work was supported in part by the Center forIntelligent Information Retrieval, in part by TheCentral Intelligence Agency, the National Secu-rity Agency and National Science Foundation un-der NSF grant #IIS-0326249, and in part by UPennNSF medium IIS-0803847.
The University of Mas-sachusetts also gratefully acknowledges the supportof Defense Advanced Research Projects Agency(DARPA) Machine Reading Program under AirForce Research Laboratory (AFRL) prime contractno.
FA8750-09-C-0181.
Any opinions, findings,and conclusion or recommendations expressed inthis material are those of the author(s) and do notnecessarily reflect the view of the DARPA, AFRL,or the US government.ReferencesKurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a collabo-ratively created graph database for structuring humanknowledge.
In SIGMOD ?08: Proceedings of the 2008ACM SIGMOD international conference on Manage-ment of data, pages 1247?1250, New York, NY, USA.ACM.Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to extract relations from the web using min-imal supervision.
In Proceedings of the 45rd AnnualMeeting of the Association for Computational Linguis-tics (ACL ?07).Andrew Carlson, Justin Betteridge, Richard Wang, Es-tevam Hruschka, and Tom Mitchell.
2010.
Cou-pled semi-supervised learning for information extrac-tion.
In Third ACM International Conference on WebSearch and Data Mining (WSDM ?10).Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe Conference on Empirical methods in natural lan-guage processing (EMNLP ?02), volume 10, pages 1?8.1022Koby Crammer and Yoram Singer.
2003.
Ultraconserva-tive online algorithms for multiclass problems.
Jour-nal of Machine Learning Research, 3:951?991.M.
Craven and J. Kumlien.
1999.
Constructing biolog-ical knowledge-bases by extracting information fromtext sources.
In Proceedings of the Seventh Interna-tional Conference on Intelligent Systems for MolecularBiology, pages 77?86, Germany.Aron Culotta and Jeffery Sorensen.
2004.
Dependencytree kernels for relation extraction.
In 42nd AnnualMeeting of the Association for Computational Linguis-tics, Barcelona, Spain.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics (ACL?05), pages 363?370, June.S.
Geman and D. Geman.
1990.
Stochastic relaxation,gibbs distributions, and the bayesian restoration of im-ages.
pages 452?472.Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.2010.
Learning 5000 relational extractors.
In ACL.Rohit J. Kate and Raymond J. Mooney.
2010.
Joint en-tity and relation extraction using card-pyramid pars-ing.
In Proceedings of the 12th Conference on Com-putational Natural Language Learning (CoNLL?
10).John D. Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In International Conference on Machine Learn-ing (ICML).Andrew McCallum, Karl Schultz, and Sameer Singh.2009.
Factorie: Probabilistic programming via imper-atively defined factor graphs.
In Y. Bengio, D. Schuur-mans, J. Lafferty, C. K. I. Williams, and A. Culotta, ed-itors, Advances in Neural Information Processing Sys-tems 22, pages 1249?1257.Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP (ACL ?09),pages 1003?1011.
Association for Computational Lin-guistics.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-baseddependency parsing.
In Proceedings of CoNLL, pages49?56.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A markov logic approach tobio-molecular event extraction.
In Proceedings of theNatural Language Processing in Biomedicine NAACL2009 Workshop (BioNLP ?09), pages 41?49.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions withoutlabeled text.
In Proceedings of the European Confer-ence on Machine Learning and Knowledge Discoveryin Databases (ECML PKDD ?10).D.
Roth and W. Yih.
2007.
Global inference for en-tity and relation identification via a linear program-ming formulation.
In Lise Getoor and Ben Taskar, ed-itors, Introduction to Statistical Relational Learning.MIT Press.Evan Sandhaus, 2008.
The New York Times AnnotatedCorpus.
Linguistic Data Consortium, Philadelphia.Stefan Schoenmackers, Oren Etzioni, and Daniel S.Weld.
2008.
Scaling textual inference to the web.In EMNLP ?08: Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 79?88, Morristown, NJ, USA.
Association forComputational Linguistics.Sameer Singh, Karl Schultz, and Andrew McCallum.2009.
Bi-directional joint inference for entity res-olution and segmentation using imperatively-definedfactor graphs.
In European Conference on MachineLearning and Principles and Practice of KnowledgeDiscovery in Databases (ECML PKDD), pages 414?429.Daniel S. Weld, Raphael Hoffmann, and Fei Wu.
2009.Using wikipedia to bootstrap open information extrac-tion.
In ACM SIGMOD Record.Michael Wick, Khashayar Rohanimanesh, Aron Culotta,and Andrew McCallum.
2009.
Samplerank: Learningpreferences from atomic gradients.
In Neural Infor-mation Processing Systems (NIPS), Workshop on Ad-vances in Ranking.Dimitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relationextraction.
JMLR, 3(6):1083 ?
1106.1023
