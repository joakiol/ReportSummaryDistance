Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1074?1084,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Word Representations from Scarce and Noisy Data withEmbedding Sub-spacesRamon F. Astudillo, Silvio Amir, Wang Lin, M?ario Silva, Isabel TrancosoInstituto de Engenharia de Sistemas e Computadores Investigac?
?ao e Desenvolvimento (INESC-ID)Rua Alves Redol 9Lisbon, Portugal{ramon.astudillo, samir, wlin, mjs, isabel.trancoso}@inesc-id.ptAbstractWe investigate a technique to adapt unsu-pervised word embeddings to specific ap-plications, when only small and noisy la-beled datasets are available.
Current meth-ods use pre-trained embeddings to initial-ize model parameters, and then use the la-beled data to tailor them for the intendedtask.
However, this approach is prone tooverfitting when the training is performedwith scarce and noisy data.
To overcomethis issue, we use the supervised data tofind an embedding subspace that fits thetask complexity.
All the word representa-tions are adapted through a projection intothis task-specific subspace, even if they donot occur on the labeled dataset.
This ap-proach was recently used in the SemEval2015 Twitter sentiment analysis challenge,attaining state-of-the-art results.
Here weshow results improving those of the chal-lenge, as well as additional experiments ina Twitter Part-Of-Speech tagging task.1 IntroductionThe success of supervised systems largely dependson the amount and quality of the available train-ing data, oftentimes, even more than the particu-lar choice of learning algorithm (Banko and Brill,2001).
Labeled data is, however, expensive to ob-tain, while unlabeled data is widely available.
Inorder to exploit this fact, semi-supervised learn-ing methods can be used.
In particular, it is pos-sible to derive word representations by exploitingword co-occurrence patterns in large samples ofunlabeled text.
Based on this idea, several meth-ods have been recently proposed to efficiently es-timate word embeddings from raw text, leverag-ing neural language models (Huang et al, 2012;Mikolov et al, 2013; Pennington et al, 2014; Linget al, 2015).
These models work by maximizingthe probability that words within a given windowsize are predicted correctly.
The resulting embed-dings are low-dimensional dense vectors that en-code syntactic and semantic properties of words.Using these word representations, Turian et al(2010) were able to improve near state-of-the-artsystems for several tasks, by simply plugging inthe learned word representations as additional fea-tures.
However, because these features are esti-mated by minimizing the prediction errors madeon a generic, unsupervised, task they might besuboptimal for the intended purposes.Ideally, word features should be adapted to thespecific supervised task.
One of the reasons forthe success of deep learning models for languageproblems, is the use unsupervised word embed-dings to initialize the word projection layer.
Then,during training, the errors made in the predictionsare backpropagated to update the embeddings, sothat they better predict the supervised signal (Col-lobert et al, 2011; dos Santos and Gatti, 2014a).However, this strategy faces an additional chal-lenge in noisy domains, such as social media.The lexical variation caused by the typos, use ofslang and abbreviations leads to a great numberof singletons and out-of-vocabulary words.
Forthese words, the embeddings will be poorly re-estimated.
Even worse, words not present on thetraining set will never get their embeddings up-dated.In this paper, we describe a strategy to adapt un-supervised word embeddings when dealing withsmall and noisy labeled datasets.
The intuition be-hind our approach is the following.
For a giventask, only a subset of all the latent aspects capturedby the word embeddings will be useful.
Therefore,instead of updating the embeddings directly withthe available labeled data, we estimate a projec-tion of these embeddings into a low dimensionalsub-space.
This simple method brings two funda-1074mental advantages.
On the one hand, we obtainlow dimensional embeddings fitting the complex-ity of the target task.
On the other hand, we areable to learn new representations for all the words,even if they do not occur in the labeled dataset.To estimate the low dimensional sub-space, wepropose a simple non-linear model equivalent to aneural network with one single hidden layer.
Themodel is trained in supervised fashion on the la-beled dataset, learning jointly the sub-space pro-jection and a classifier for the target task.
Usingthis model, we built a system to participate in theSemEval 2015 Twitter sentiment analysis bench-mark (Rosenthal et al, 2015).
Our submission at-tained state-of-the-art results without hand-codedfeatures or linguistic resources (Astudillo et al,2015).
Here, we further investigate this approachand compare it against several state-of-the-art sys-tems for Twitter sentiment classification.
We alsoreport on additional experiments to assess the ad-equacy of this strategy in other natural languageproblems.
To this end, we apply the embeddingsub-space layer to Ling et al (2015) deep learningmodel for part-of-speech tagging.
Even thoughthe gains were not as significant as in the senti-ment polarity prediction task, the results suggestthat our method is indeed generalizable to otherproblems.The rest of the paper is organized as follows: therelated work is reviewed in Section 2.
Section 3,briefly describes the model used to pre-train theword embeddings.
In Section 4, we introduce theconcept of embedding sub-space, as well as thethe non-linear sub-space model for text classifica-tion.
Section 5, details the experiments performedwith the SemEval corpora.
Section 6 describes ad-ditional experiments applying the embedding sub-space method to a Part-of-Speech tagging modelfor Twitter data.
Finally, Section 7 draws the con-clusions.2 Related WorkNLP systems can benefit from a very large poolof unlabeled data.
While raw documents are usu-ally not annotated, they contain structure, whichcan be leveraged to learn word features.
Con-text is one strong indicator for word similarity,as related words tend to occur in similar con-texts (Firth, 1968).
Approaches that are based onthis concept include, Latent Semantic Analysis,where words are represented as rows in the low-rank approximation of a term co-occurrence ma-trix (Dumais et al, 1988), word clusters obtainedwith hierarchical clustering algorithms based onHidden Markov Models (Brown et al, 1992), andcontinuous word vectors learned with neural lan-guage models (Bengio et al, 2003).
The result-ing clusters and vectors, can then be used as moregeneralizable features in supervised tasks, as theyalso provide representations for words not presentin the labeled data (Bespalov et al, 2011; Owoputiet al, 2013; Chen and Manning, 2014).A great amount of work has been done on theproblem of learning better word representationsfrom unsupervised data.
However, not many stud-ies have discussed the best ways to use them insupervised tasks.
Typically, in these cases, wordrepresentations are directly used as features or toinitialize the parameters of more complex mod-els.
In some tasks, this approach is however proneto overfitting.
The work presented here aims toprovide a simple approach to overcome this lastscenario.
It is thus directly related to Labutovand Lipson (2013), where a method to learn task-specific representations from general pre-trainedembeddings was presented.
In this work, new fea-tures were estimated with a convex objective func-tion that combined the log-likelihood of the train-ing data, with regularization penalizing the Frobe-nius norm of the distortion matrix.
That is, the ma-trix of the differences between the original and thenew embeddings.
Even though the adapted em-beddings performed better than the purely unsu-pervised features, both were significantly outper-formed by a simple bag-of-words baseline.Most other approaches, simply rely on addi-tional training data to fine tune the embeddings fora given supervised task.
In Bansal et al (2014),better word embeddings for dependency parsingwere obtained by using a corpus created to cap-ture dependency context.
This technique requires,nevertheless, of a pre-existing dependency parseror, at least a parsed corpus.
For some other tasks,it is possible to collect weakly labeled corpora bymaking strong assumptions about the data.
In Goet al (2009) a corpus for Twitter sentiment anal-ysis was built by assuming that tweets with posi-tive emoticons imply positive sentiment, whereastweets with negative emoticons imply negativesentiment.
Using a similar corpus, Tang et al(2014b) induced sentiment specific word embed-dings, for the Twitter domain.
The embeddings1075were estimated with a neural network that mini-mized a linear combination of two loss functions,one penalized the errors made at predicting thecenter word within a sequence of words, while theother penalized mistakes made at deciding the sen-timent label.
Weakly labeled data has also beenused to refine unsupervised embeddings, by re-training them to predict the noisy labels before us-ing the actual task-specific supervised data (Sev-eryn and Moschitti, 2015).3 Unsupervised Structured Skip-GramWord EmbeddingsWord embeddings are generally trained by opti-mizing an objective function that can be measuredwithout annotations.
One popular approach is toestimate the embeddings by maximizing the prob-ability that the words within a given window sizeare predicted correctly.
Our previous work hascompared several such models, namely the skip-gram and CBOW architectures (Mikolov et al,2013), GloVe (Pennington et al, 2014), and thestructured skip-gram approach (Ling et al, 2015),suggesting that they all have comparable capabil-ities.
Thus, in this study we only use embeddingsderived with the structured skip-gram approach, amodification of the skip-gram architecture that hasbeen shown to outperform the original model insyntax based tasks such as, part-of-speech taggingand dependency parsing.Central to the structured skip-gram is a log lin-ear model of word prediction.
Let w = i denotethat a word at a given position of a sentence isthe i-th word on a vocabulary of size v, and letwp= j denote that the word p positions furtherin the sentence is the j-th word on the vocabu-lary.
The structured skip-gram models the follow-ing probability:p(wp= j|w = i) ?
exp(Cpj?E ?wi)(1)Here, wi?
{1, 0}v?1is a one-hot representa-tion of w = i.
That is, a vector of zeros of thesize of the vocabulary v with a 1 on the i-th entryof the vector.
The symbol ?
denotes internal prod-uct and exp() acts element-wise.
The log-linearmodel is parametrized by the following matrices:E ?
Re?v, is the embedding matrix, transform-ing the one-hot representation into a compact realvalued space of size e, Cpj?
Rv?eis a set of out-put matrices, one for each relative word position p,projecting the real-valued representation to a vec-tor with the size of the vocabulary v. By learn-ing a different matrix Cpfor each relative wordposition, the model captures word order informa-tion, unlike the original skip-gram approach thatuses only one output matrix.
Finally, a distributionover all possible words is attained by exponentiat-ing and normalizing over the v possible options.
Inpractice, negative sampling is used to avoid havingto normalize over the whole vocabulary (Goldbergand Levy, 2014).As most other neural network models, the struc-tured skip-gram is trained with gradient-basedmethods.
After a model has been trained, the lowdimensional embedding E ?
wi?
Re?1encapsu-lates the information about each word wiand itssurrounding contexts.
This embbeding can thusbe used as input to other learning algorithms tofurther enhance performance.4 Adapting Embeddings with Sub-spaceProjectionsAs detailed in the introduction and related work,word embeddings are a useful unsupervised tech-nique to attain initial model values or featuresprior to supervised training.
These models canbe then retrained using the available labeled data.However, even if the embeddings provide a com-pact real valued representation of each word in avocabulary, the total number of parameters in themodel can be rather high.
If, as it is often thecase, only a small amount of supervised data isavailable, this can lead to severe overfitting.
Evenif regularization is used to reduce the overfittingrisk, only a reduced subset of the words will actu-ally be present in the labeled dataset.
Words notseen during training will never get their embed-dings updated.
Furthermore, rare words will re-ceive very few updates, and thus their embeddingswill be poorly adapted for the intended task.
Wepropose a simple solution to avoid this problem.4.1 Embedding Sub-spaceLet E ?
Re?vdenote the original embeddingmatrix obtained, e.g.
with the structured skip-gram model described in Equation 1.
We definethe adapted embedding matrix as the factorizationS ?
E, where S ?
Rs?e, with s  e. We estimatethe parameters of the matrix S using the labeleddataset, while E is kept fixed.
In other words, wedetermine the optimal projection of the embedding1076matrix E into a sub-space of dimension s.The idea of embedding sub-space rests on twofundamental principles:1.
With dimensionality reduction of the embed-dings, the model can better fit the complexityof the task at hand or the amount of availabledata.2.
Using a projection, all the embeddings areindirectly updated, not only those of wordspresent in the labeled dataset.One question that arises from this approach, isif the estimated projection is also optimal for thewords not present in the labeled dataset.
We as-sume that the words on the labeled dataset are, tosome extent, representative of the words found inthe unlabeled corpus.
This is a reasonable assump-tion since both datasets can be seen as samplesdrawn from the same power-law distribution.
Ifthis holds, for every unknown word, there will besome other word sufficiently close it in the embed-ding space.
Consequently, the projection matrixS will also be approximately valid for those un-seen words.
It is often the case that a relativelysmall number of words of the labeled dataset arenot present on the unlabeled corpus.
These wordsare not represented in E. One way to deal with thiscase, is to simply set the embeddings of unknownwords to zero.
But in this case, the embeddingswill not be adapted during training.
Random ini-tializations of the embeddings seems to be help-ful for tasks that have a higher penalty for missingwords, although it remains unclear if better initial-ization strategies exist.4.2 Non-Linear Embedding Sub-space ModelThe concept of embedding sub-space can be ap-plied to log-linear classifiers or any deep learningarchitecture that uses embeddings.
We now de-scribe an application of this method for short textclassification tasks.
In what follows, we will referto this approach as Non-Linear Sub-space Embed-ding (NLSE) model.
The NLSE can be interpretedas a simple feed-forward neural network model(Rumelhart et al, 1985) with one single hiddenlayer utilizing the embedding sub-space approach,as depicted in Fig.
1.
Letm = [w1?
?
?wn] (2)denote a message of n words.
Each columnw ?
{0, 1}v?1of m represents a word in one-hot form, as described in Section 3.
Let y de-note a categorical random variable overK classes.The NLSE model, estimates thus the probability ofeach possible category y = k ?
K given a mes-sage m asp(y = k|m) ?
exp (Yk?
h ?
1) .
(3)Here, h ?
{0, 1}e?nare the activations of the hid-den layer for each word, given byh = ?
(S ?E ?m) (4)where ?
() is a sigmoid function acting on eachelement of the matrix.
The matrix Y ?
R3?smaps the embedding sub-space to the classifica-tion space and 1 ?
1n?1is a matrix of ones thatsums the scores for all words together, prior to nor-malization.
This is equivalent to a bag-of-wordsassumption.
Finally, the model computes a prob-ability distribution over the K classes, using thesoftmax function.Compared to a conventional feed-forward net-work employing embeddings for natural languageclassification tasks, two main differences arise.First, the input layer is factorized into two com-ponents, the embeddings attained in unsupervisedform, E, and the projection matrix S. Second, thesize of the sub-space, in which the embeddings areprojected, is much smaller than that of the origi-nal embeddings with typical reductions above oneorder of magnitude.
As usual in this kind of mod-els, all the parameters can be trained with gradientmethods, using the backpropagation update rule.5 NLSE for Twitter Sentiment AnalysisIn this section, we apply the NLSE model to themessage polarity classification task proposed bySemEval, for their well-known Twitter sentimentanalysis challenge (Nakov et al, 2013).
Given amessage, the goal is to decide whether it expressesa positive, negative, or neutral sentiment.
Mostof the top performing systems that participated inthis challenge, relied on linear classification mod-els and the bag-of-words assumption, representingmessages as sparse vectors of the size of the vo-cabulary.
In the case of social media, this approachis particularly inefficient, due to the large vocabu-laries necessary to account for all the lexical vari-ation found in this domain.
Thus, these models1077SoftmaxoverPolaritySubspaceWord Embeddingsppl r juz unrealiableSomeBowNegativeLogisticSigmoidFigure 1: Illustration of the NLSE model, appliedto sentiment polarity prediction.Positive Neutral NegativeDevelopment 3230 4109 1265Tweets 2015 1032 983 364Tweets 2014 982 669 202Tweets 2013 1572 1640 601Table 1: Number of examples per class in eachSemEval dataset.
The first row shows the trainingdata; the other rows are sets used for testing.need to be enriched with additional hand-craftedfeatures that try to capture more discriminative as-pects of the content, most of which require exter-nal tools (e.g., part-of-speech taggers and parsers)or linguistic resources (e.g., dictionaries and sen-timent lexicons) (Miura et al, 2014; Kiritchenkoet al, 2014).
With the embedding sub-space ap-proach, however, we are able to attain state-of-the-art performance while requiring only minimalprocessing of the data and few hyperparameters.To make our results comparable to other systemsfor this task, we adopted the guidelines from thebenchmark.
Our system was trained and tunedusing only the development data.
The evaluationwas performed on the test sets, shown in Table 1,and we report the results in terms of the averageF-measure for the positive and negative classes.5.1 Experimental SetupThe first step of our approach requires a corpus ofraw text for the unsupervised pre-training of theembedding matrix E. We resorted to the corpus of52 million tweets used in (Owoputi et al, 2013)and the tokenizer described in the same work.
Themessages were previously pre-processed as fol-lows: lower-casing, replacing Twitter user men-tions and URLs with special tokens and reducingany character repetition to at most 3 characters.Words occurring less than 40 times in the cor-pus were discarded, resulting in a vocabulary ofaround 210,000 types.
Then, a modified versionof the word2vec tool1was used to compute theword embeddings, as described in Section 3.
Thewindow size and negative sampling rate were setto 5 and 25 words, respectively, and embeddingsof 50, 200, 400 and 600 dimensions were built.Our system accepts as input a sentence rep-resented as a matrix, obtained by concatenatingthe one-hot vectors that represent each individualword.
Therefore, we first performed the afore-mentioned normalization and tokenization stepsand then, converted each tweet into this represen-tation.
The development set was split into 80%for parameter learning and 20% for model evalu-ation and selection, maintaining the original rela-tive class proportions in each set.
All the weightswere initialized uniformly at random, as proposedin (Glorot and Bengio, 2010).
The model wastrained with conventional Stochastic Gradient De-scent (Rumelhart et al, 1985) with a fixed learningrate of 0.01, and the weights were updated aftereach message was processed.
Variations of learn-ing rate to smaller values, e.g.
0.005, were con-sidered but did not lead to a clear pattern.
We ex-plored different configurations of the hyperparam-eters e (embedding size) and s (sub-space size).Model selection was done by early stopping, i.e.,we kept the configuration with best F-measure onthe evaluation set after 5-8 iterations.5.2 ResultsIn general, the NLSE model showed consistentand fast convergence towards the optimum in veryfew iterations.
Despite using class log-likelihoodas training criterion, it showed good performancein terms of the average F-measure for positiveand negative sentiments.
We found that all em-bedding sizes yield comparable performances, al-1https://github.com/wlin12/wang2vec1078Figure 2: Average F-measure on the SemEvaltest sets varying with embedding sub-space sizes.
Sub-space size 0 used to denote the baseline(log-linear model).though larger embeddings tend to perform better.Therefore, we only report results obtained with the600 dimensional vectors.
In Figure 2, we show thevariation of system performance with sub-spacesize s. The baseline is a log-linear using the em-beddings in E as features.
As it can be seen, theperformance is sharply improved when the em-bedding sub-spaces are used.
By choosing dif-ferent values of s, we can adjust the model to thecomplexity of the task and the amount of labeleddata available.
Given the small size of the train-ing set, the best results were attained with the useof smaller sub-spaces, in the range of 5-10 dimen-sions.Figure 3, presents the main results of the ex-perimental evaluation.
As baselines, we consid-ered two simple approaches: LOG-LINEAR, whichuses the unsupervised embeddings directly as fea-tures in a log-linear classifier, and LOG-LINEAR*,also using the unsupervised embeddings as fea-tures in a log-linear classifier, but updating the em-beddings with the training data.
These baselines,were compared against two variations of the non-linear sub-space embedding model: NLSE, wherewe only train the S and Y weights while the em-beddings are kept fixed, and NLSE*, where wealso update the embedding matrix during training.For these experiments, we set s = 10.
The re-sults in Figure 3a, show that our model largelyoutperforms the simpler baselines.
Furthermore,we observe that updating the embeddings alwaysleads to inferior results.
This suggests that pre-computed embeddings should be kept fixed, whenlittle labeled data is available to re-train them.Comparison with the state-of-the-artWe now compare the NLSE model with state-of-the-art systems, including the best submissions toprevious SemEval benchmarks.
We also includetwo other approaches that are related to the onehere proposed, where a neural network initializedwith pre-trained word embeddings is used to learnrelevant features.
Specifically, we compare thefollowing systems:?
NRC (Kiritchenko et al, 2014), a supportvector machine classifier with a large setof hand-crafted features, including word andcharacter n-grams, brown clusters, POS tags,morphological features, and a set of featuresbased on five sentiment lexicons.
Most of theperformance was due to the combination ofthese lexicons.
This was the top system inthe 2013 edition of SemEval.?
TEAMX (Miura et al, 2014), a logistic re-gression classifier using a similar set of fea-tures.
Additional features based on two dif-ferent POS taggers and a word sense dis-ambiguator were also included in the model.This approach attained the highest ranking inthe 2014 edition.?
CHARSCNN (dos Santos and Gatti, 2014b),a deep learning architecture with two con-volutional layers that exploit character-leveland word-level information.
The features areextracted by converting a sentence into a se-quence of word embeddings, and the individ-ual words into sequences of character embed-dings.
Convolution filters followed by maxpooling are applied to these sequences, toproduce fixed size vectors.
These vectors arethen combined and transfered to a set of non-linear activation functions, to generate morecomplex representations of the input.
Thepredictions, based on these learned featuresare computed with a softmax classifier.?
COOOOLLL (Tang et al, 2014a), a supportvector machine classifier that leverages thesentiment specific word embeddings, dis-cussed in Section 2.
The embeddings arealso processed with a convolution filter, butthe output of this operation is used to pro-duce three representations obtained with dif-ferent strategies, namely with max, min and1079(a) Comparison of two baselines with two variationsof the NLSE model(b) Performance of state-of-the-art systems for Twitter senti-ment predictionFigure 3: Average F-measure on the SemEval test setsaverage pooling.
The final feature vector isobtained by concatenating these representa-tions and Kiritchenko et al (2014) feature set.?
UNITN (Severyn and Moschitti, 2015), an-other deep convolutional neural network thatjointly learns internal representations and asoftmax classifier.
The network is trainedin three steps: (i) unsupervised pre-trainingof embeddings, (ii) refinement of the em-beddings using a weakly labeled corpus, and(iii) fine tuning the model with the labeleddata from SemEval.
It should be noted thatthe system was trained with a labeled corpus65% larger than ours2.
This system made thebest submission on the 2015 edition of thebenchmark.The results in Figure 3b, show that despite be-ing simpler and requiring less resources and la-beled data, the NLSE model is extremely compet-itive, even outperforming most other systems, inpredicting the sentiment polarity of Twitter mes-sages.6 Generalization to Other TasksWhile the embedding sub-space method workswell for the sentiment prediction task, we wouldlike to know its impact in other settings that areknown to benefit from unsupervised embeddings.Thus, we decided to replicate the part-of-speechtagging work in (Ling et al, 2015), where pre-training embeddings have been shown to improve2The UNITN system was trained with around 11,400 la-beled examples, whereas we used only 6,900.the quality of the results significantly.6.1 Sub-space Window ModelPart-of-speech tagging is a word labeling task,where each word is to be labeled with its syntacticfunction in the sentence.
More formally, given aninput sentence w1, .
.
.
, wnof n words, we wish topredict a sequence of labels y1, .
.
.
, yn, which arethe POS tags of each of the words.
This task isscored by the ratio between the number of correctlabels and the number of words to be labeled.We modified (Collobert et al, 2011) windowmodel, to include the sub-space matrix S. Theprobability of labeling the word wtwith the POStag k is given byp(y = k|mt+pt?p) ?
exp (Yk?
ht+ b) , (5)wheremt+pt?p= [wt?p?
?
?wt?
?
?wt+p] (6)denotes a context window of words around thet-th word, with a total span of 2p + 1 words.
htdenotes the activations of a hidden layer given byht= tanh?????
?H ??????
?S ?E ?wt+p?
?
?S ?E ?wt?
?
?S ?E ?wt?p????????????.
(7)Here tanh denotes the hyperbolic tangent, act-ing element-wise.
Aside from embedding E and1080sub-space S matrices, the model is parametrizedby the weights H ?
Rh?psand Y ?
Rv?has wellas a bias b ?
Rv?1.Note that if S is set to the identity matrix, thiswould be equivalent to the original Collobert et al(2011) model.TanhSoftmaxoverTagsSubspaceWord Embeddingsppl r juz unrealiableSomeWindowVerbFigure 4: Illustration of the window modelby (Collobert et al, 2011) using a sub-space layer.6.2 ExperimentsTests were performed in Gimpel et al (2011) Twit-ter POS dataset, which uses the universal POS tagset composed by 25 different labels (Petrov et al,2012).
The dataset contains 1000 annotated tweetsfor training, 327 tweets for tuning and 500 tweetsfor testing.
The number of word tokens in thesesets are 15000, 5000 and 7000, respectively.
Thereare 5000, 2000 and 3000 word types.Once again, we initialized the embeddingswith unsupervised pre-training using the struc-tured skip-gram approach.
As for the hyperpa-rameters of the model, we used embeddings withe = 50 dimensions, a hidden layer with h = 200dimensions and a context of p = 2 as used in (Linget al, 2015).
Training employed mini-batch gradi-ent descent, with mini batches of 100 sentencesand a momentum of 0.95.
The learning rate wasset to 0.2.
Finally, we used early stopping bychoosing the epoch with the highest accuracy inthe tuning set.
As for the sub-space layer size, wetried three different hyperparameterizations: 10,30 and 50 dimensions.6.3 ResultsFigure 5 displays the results.
Using the setup thatled to the best results in the sentiment predictiontask (FIX), that is, fixing E and updating S, leadsto lower accuracies than the baseline (TRAIN-ALL,s = 0).
We also see that different values of s donot have a very strong impact in the final results.Sentiment polarity prediction and POS taggingdiffer in multiple aspects and there may be morethan one reason for this poorer performance.
Oneparticularly relevant aspect, in our opinion, is theway words that have no pre-trained embedding aretreated.
In the case of sentiment prediction, thesewords were set to having and embedding of zero.This fits the use of the bag-of-words assumptionand the fact that only one label is produced permessage, as there are many other words to drawevidence from.
In the case of POS tagging a hy-pothesis must be drawn for each word, using ashorter context.
Thus, ignoring a word means thatcontext is used instead, which is a frequent causeof errors.One way around this problem would be to up-date the parameters of S and E, but this leads toresults similar to the experiment without the sub-space projections (TRAIN-ALL).
This is expectedas the sub-space layer was designed to work onfixed word embeddings, if these are updated itsbenefits are lost.
Thus, we solve this problemby fixing all the embeddings, except for the wordtypes not found in the pre-training corpus.
Thatis, instead of leaving the unknown words as thezero vector, we use the labeled data to learn a bet-ter representation.
Using this setup (TRAIN-OOV),we can obtain a small but consistent improvementover the baseline.
While these improvements arenot significant, as this task is not as prone to over-fitting as in sentiment analysis, this is a good checkof the validity of our method.7 ConclusionsWe presented a new approach to use unsupervisedword embeddings based on the idea of finding asub-space projection of the embeddings for a giventask.
This approach offers two main advantages.On the one hand, it allows to indirectly updateembeddings unseen during training.
On the other1081Figure 5: Results for the part-of-speech task onthe ARK POS dataset, for different strategies toupdate the embeddings and with variations of thesub-space size.
Sub-space size 0 used to denotethe baseline (window model).hand, it reduces the number of model parametersto fit the complexity of the task.
These propertiesmake this method particularly useful for the caseswhere only small amounts of noisy data are avail-able to train the model.Experiments on the SemEval challenge corporavalidated these ideas, showing that such a simpleapproach can attain state-of-the-art results compa-rable with the best systems of past SemEval edi-tions and often outperforming them in all datasets.It should be noted that this is attained while keep-ing the original embedding matrix E fixed andonly learning the projection S with the superviseddata.
Additional experiments on the Twitter POStagging task indicate however that, the techniqueis not always as effective as in the sentiment clas-sification task.
One possible explanation for thedifferent behavior is the use of embeddings of ze-ros for words without pre-trained embedding.
It isplausible that this has a stronger effect on the POStagging task.
Another aspect to be taken into ac-count is the fact that both tasks could have a differ-ent complexity which would explain why adaptingE in the POS taks yields better results.
Optimalityof the embeddings for each of the tasks might alsocome into play here.The implementation of the proposed methodand our Twitter Sentiment Analysis system hasbeen made publicly available3.3https://github.com/ramon-astudillo/NLSEAcknowledgmentsThis work was partially supported by Fundac?
?aopara a Ci?encia e Tecnologia (FCT), throughcontracts UID/CEC/50021/2013, EXCL/EEI-ESS/0257/2012 (DataStorm), research projectwith reference UTAP-EXPL/EEI-ESS/0031/2014,EU project SPEDIAL (FP7 611396), grantnumber SFRH/BPD/68428/2010 and Ph.D.scholarship SFRH/BD/89020/2012.ReferencesRamon F. Astudillo, Silvio Amir, Wang Ling, BrunoMartins, M?ario Silva, and Isabel Trancoso.
2015.Inesc-id: Sentiment analysis without hand-codedfeatures or liguistic resources using embedding sub-spaces.
In Proceedings of the 9th InternationalWorkshop on Semantic Evaluation, SemEval ?2015,Denver, Colorado, June.
Association for Computa-tional Linguistics.Michele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
In Proceedings of the 39th Annual Meetingon Association for Computational Linguistics, pages26?33.
Association for Computational Linguistics.Mohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
The Journal of Machine Learning Re-search, 3:1137?1155.Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-oufandeh.
2011.
Sentiment classification based onsupervised latent n-gram analysis.
In Proceedings ofthe 20th ACM international conference on Informa-tion and knowledge management, pages 375?382.ACM.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Comput.
Linguist., 18(4):467?479, Decem-ber.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 740?750.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.1082Cicero dos Santos and Maira Gatti.
2014a.
Deepconvolutional neural networks for sentiment analy-sis of short texts.
In Proceedings of COLING 2014,the 25th International Conference on ComputationalLinguistics: Technical Papers, pages 69?78, Dublin,Ireland, August.
Dublin City University and Associ-ation for Computational Linguistics.C?cero Nogueira dos Santos and Ma?ra Gatti.
2014b.Deep convolutional neural networks for sentimentanalysis of short texts.
In Proceedings of the 25th In-ternational Conference on Computational Linguis-tics (COLING), Dublin, Ireland.Susan T Dumais, George W Furnas, Thomas K Lan-dauer, Scott Deerwester, and Richard Harshman.1988.
Using latent semantic analysis to improve ac-cess to textual information.
In Proceedings of theSIGCHI conference on Human factors in computingsystems, pages 281?285.
ACM.John Rupert Firth.
1968.
Selected papers of JR Firth,1952-59.
Indiana University Press.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor twitter: annotation, features, and experiments.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers - Volume 2, HLT?11, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Xavier Glorot and Yoshua Bengio.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In International conference on artificialintelligence and statistics, pages 249?256.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, pages 1?12.Yoav Goldberg and Omer Levy.
2014. word2vecexplained: deriving mikolov et al?s negative-sampling word-embedding method.
arXiv preprintarXiv:1402.3722.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers - Volume 1, ACL ?12, pages 873?882, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Mo-hammad.
2014.
Sentiment analysis of short in-formal texts.
Journal of Artificial Intelligence Re-search, pages 723?762.Igor Labutov and Hod Lipson.
2013.
Re-embeddingwords.
In Proceedings of the 51st annual meeting ofthe ACL, pages 489?493.Wang Ling, Chris Dyer, Alan Black, and IsabelTrancoso.
2015.
Two/too simple adaptations ofword2vec for syntax problems.
In Proceedings ofthe 2015 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies.
Associationfor Computational Linguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013.
Distributed represen-tations of words and phrases and their composition-ality.
In 27th Annual Conference on Neural Infor-mation Processing Systems.Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, andTomoko Ohkuma.
2014.
Teamx: A sentiment ana-lyzer with enhanced lexicon mapping and weightingscheme for unbalanced data.
In Proceedings of the8th International Workshop on Semantic Evaluation(SemEval 2014), pages 628?632, Dublin, Ireland,August.
Association for Computational Linguistics.Preslav Nakov, Zornitsa Kozareva, Alan Ritter, SaraRosenthal, Veselin Stoyanov, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysis intwitter.Olutobi Owoputi, Chris Dyer, Kevin Gimpel, NathanSchneider, and Noah A. Smith.
2013.
Improvedpart-of-speech tagging for online conversational textwith word clusters.
In In Proceedings of NAACL.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors forword representation.
Proceedings of the EmpiricialMethods in Natural Language Processing (EMNLP2014), 12.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceedings ofthe Eight International Conference on Language Re-sources and Evaluation (LREC?12).
European Lan-guage Resources Association (ELRA), may.Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,Saif M Mohammad, Alan Ritter, and Veselin Stoy-anov.
2015.
Semeval-2015 task 10: Sentimentanalysis in twitter.
In Proceedings of the 9th In-ternational Workshop on Semantic Evaluation, Se-mEval ?2015, Denver, Colorado, June.
Associationfor Computational Linguistics.David E Rumelhart, Geoffrey E Hinton, and Ronald JWilliams.
1985.
Learning internal representationsby error propagation.
Technical report, DTIC Doc-ument.Aliaksei Severyn and Alessandro Moschitti.
2015.Unitn: Training deep convolutional neural networkfor twitter sentiment classification.
In Proceedingsof the 9th International Workshop on Semantic Eval-uation, SemEval ?2015, Denver, Colorado, June.
As-sociation for Computational Linguistics.1083Duyu Tang, Furu Wei, Bing Qin, Ting Liu, and MingZhou.
2014a.
Coooolll: A deep learning system fortwitter sentiment classification.
SemEval 2014, page208.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, TingLiu, and Bing Qin.
2014b.
Learning sentiment-specific word embedding for twitter sentiment clas-sification.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguis-tics, pages 1555?1565.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th annual meeting of the association for compu-tational linguistics, pages 384?394.
Association forComputational Linguistics.1084
