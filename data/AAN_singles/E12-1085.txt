Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 840?850,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsManaging Uncertainty in Semantic TaggingSilvie Cinkova?
and Martin Holub and Vincent Kr?
?z?Charles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied Linguistics{cinkova|holub}@ufal.mff.cuni.czvincent.kriz@gmail.comAbstractLow interannotator agreement (IAA) is awell-known issue in manual semantic tag-ging (sense tagging).
IAA correlates withthe granularity of word senses and theyboth correlate with the amount of informa-tion they give as well as with its reliability.We compare different approaches to seman-tic tagging in WordNet, FrameNet, Prop-Bank and OntoNotes with a small taggeddata sample based on the Corpus PatternAnalysis to present the reliable informationgain (RG), a measure used to optimize thesemantic granularity of a sense inventorywith respect to its reliability indicated bythe IAA in the given data set.
RG can alsobe used as feedback for lexicographers, andas a supporting component of automatic se-mantic classifiers, especially when dealingwith a very fine-grained set of semantic cat-egories.1 IntroductionThe term semantic tagging is used in two diver-gent areas:1) recognizing objects of semantic importance,such as entities, events and polarity, often tailoredto a restricted domain, or2) relating occurrences of words in a corpus to alexicon and selecting the most appropriate seman-tic categories (such as synsets, semantic frames,wordsenses, semantic patterns or framesets).We are concerned with the second case, whichseeks to make lexical semantics tractable for com-puters.
Lexical semantics, as opposed to proposi-tional semantics, focuses the meaning of lexicalitems.
The disciplines that focus lexical seman-tics are lexicology and lexicography rather thanlogic.
By semantic tagging we mean a process ofassigning semantic categories to target words ingiven contexts.
This process can be either manualor automatic.Traditionally, semantic tagging relies on thetacit assumption that various uses of polysemouswords can be sorted into discrete senses; under-standing or using an unfamiliar word be then likelooking it up in a dictionary.
When building a dic-tionary entry for a given word, the lexicographersorts a number of its occurrences into discretesenses present (or emerging) in his/her mental lex-icon, which is supposed to be shared by all speak-ers of the same language.
The assumed commonmental representation of a words meaning shouldmake it easy for other humans to assign randomoccurrences of the word to one of the pre-definedsenses (Fellbaum et al 1997).This assumption seems to be falsified by theinterannotator agreement (IAA, sometimes ITA)constantly reported much lower in semantic thanin morphological or syntactic annotation, as wellas by the general divergence of opinion on whichvalue of which IAA measure indicates a reliableannotation.
In some projects (e.g.
OntoNotes(Hovy et al 2006)), the percentage of agreementsbetween two annotators is used, but a numberof more complex measures are available (for acomprehensive survey see (Artstein and Poesio,2008)).
Consequently, using different measuresfor IAA makes the reported IAA values incompa-rable across different projects.Even skilled lexicographers have trouble se-lecting one discrete sense for a concordance (Kr-ishnamurthy and Nicholls, 2000), and, more tosay, when the tagging performance of lexicog-raphers and ordinary annotators (students) was840compared, the experiment showed that the men-tal representations of a word?s semantics differ foreach group (Fellbaum et al 1997), and cf.
(Jor-gensen, 1990).
Lexicographers are trained in con-sidering subtle differences among various uses ofa word, which ordinary language users do not re-flect.
Identifying a semantic difference betweenuses of a word and deciding whether a differenceis important enough to constitute a separate sensemeans presenting a word with a certain degreeof semantic granularity.
Intuitively, the finer thegranularity of a word entry is, the more oppor-tunities for interannotator disagreement there areand the lower IAA can be expected.
Brown et alproved this hypothesis experimentally (Brown etal., 2010).
Also, the annotators are less confidentin their decisions, when they have many optionsto choose from (Fellbaum et al(1998) reported adrop in subjective annotators confidence in wordswith 8+ senses).Despite all the known issues in semantic tag-ging, the major lexical resources (WordNet (Fell-baum, 1998), FrameNet (Ruppenhofer et al2010), PropBank (Palmer et al 2005) and theword-sense part of OntoNotes (Weischedel et al2011)) are still maintained and their annotationschemes are adopted for creating new manuallyannotated data (e.g.
MASC, the Manually An-notated Subcorpus (Ide et al 2008)).
More tosay, these resources are not only used in WSD andsemantic labeling, but also in research directionsthat in their turn do not rely on the idea of an in-ventory of discrete senses any more, e.g.
in dis-tributional semantics (Erk, 2010) and recognizingtextual entailment (e.g.
(Zanzotto et al 2009) and(Aharon et al 2010)).It is a remarkable fact that, to the best of ourknowledge, there is no measure that would relategranularity, reliability of the annotation (derivedfrom IAA) and the resulting information gain.Therefore it is impossible to say where the opti-mum for granularity and IAA lies.2 Approaches to semantic tagging2.1 Semantic tagging vs. morphological orsyntactic analysisManual semantic tagging is in many respects sim-ilar to morphological tagging and syntactic anal-ysis: human annotators are trained to sort cer-tain elements occurring in a running text ac-cording to a reference source.
There is, never-theless, a substantial difference: whereas mor-phologically or syntactically annotated data ex-ist separately from the reference (tagset, anno-tation guide, annotation scheme), a semanticallytagged resource can be regarded both as a cor-pus of texts disambiguated according to an at-tached inventory of semantic categories and asa lexicon with links to example concordancesfor each semantic category.
So, in semanti-cally tagged resources, the data and the referenceare intertwined.
Such double-faced semantic re-sources have also been called semantic concor-dances (Miller et al 1993a).
For instance, one ofthe earlier versions of WordNet, the largest lexi-cal resource for English, was used in the seman-tic concordance SemCor (Miller et al 1993b).More recent lexical resources have been built assemantic concordances from the very beginning(PropBank (Palmer et al 2005), OntoNotes wordsenses (Weischedel et al 2011)).In morphological or syntactic annotation, thetagset or inventory of constituents are given be-forehand and are supposed to hold for all to-kens/sentences contained in the corpus.
Prob-lematic and theory-dependent issues are few andmostly well-known in advance.
Therefore theycan be reflected by a few additional conventions inthe annotation manual (e.g.
where to draw the linebetween particles and prepositions or between ad-jectives and verbs in past participles (Santorini,1990) or where to attach a prepositional phrasefollowing a noun phrase and how to treat specific?financialspeak?
structures (Bies et al 1995)).Even in difficult cases, there are hardly more thantwo options of interpretation.
Data manually an-notated for morphology or surface syntax are reli-able enough to train syntactic parsers with an ac-curacy above 80 % (e.g.
(Zhang and Clark, 2011;McDonald et al 2006)).On the other hand, semantic tagging actuallyemploys a different tagset for each word lemma.Even within the same part of speech, individualwords require individual descriptions.
Possiblesimilarities among them come into relief ex postrather than that they could be imposed on the lex-icographers from the beginning.
When assign-ing senses to concordances, the annotator oftenhas to select among more than two relevant op-tions.
These two aspects make achieving goodIAA much harder than in morphology and syn-841tax tasks.
In addition, while a linguistically edu-cated annotator can have roughly the same idea ofparts of speech as the author of the tagset, thereis no chance that two humans (not even two pro-fessional lexicographers) would create identicalentries for e.g.
a polysemous verb.
Any humanevaluation of complete entries would be subjec-tive.
The maximum to be achieved is that the en-try reflects the corpus data in a reasonable gran-ular way on which annotators still can reach rea-sonable IAA.2.2 Major existing semantic resourcesThe granularity vs. IAA equilibrium is of greatconcern in creating lexical resources as well as inapplications dealing with semantic tasks.
WhenWordNet (Fellbaum, 1998) was created, both IAAand subjective confidence measurements servedas an informal feedback to lexicographers (Fell-baum et al (1998), p. 200).
In general, WordNethas been considered a resource too fine-grainedfor most annotations (and applications).
Nav-igli (2006) developed a method of reducing thegranularity of WordNet by mapping the synsetsto senses in a more coarse-grained dictionary.
Amanual, more coarse-grained grouping of Word-Net senses has been performed in OntoNotes(Weischedel et al 2011).
The OntoNotes 90 %solution (Hovy et al 2006) actually means sucha degree of granularity that enables a 90-%-IAA.OntoNotes is a reaction to the traditionally poorIAA in WordNet annotated corpora, caused by thehigh granularity of senses.
The quality of seman-tic concordances is maintained by numerous itera-tions between lexicographers and annotators.
Thecategories ?right???wrong?
have been, for the pur-pose of the annotated linguistic resource, definedby the IAA score, which is?in OntoNotes?calculated as the percentage of agreements be-tween two annotators.Two other, somewhat different, lexical re-sources have to be mentioned to complete the pic-ture: FrameNet (Ruppenhofer et al 2010) andPropBank (Palmer et al 2005).
While Word-Net and OntoNotes pair words and word senses ina way comparable to printed lexicons, FrameNetis primarily an inventory of semantic frames andPropBank focuses the argument structure of verbsand nouns (NomBank (Meyers et al 2008), a re-lated project capturing the argument structure ofnouns, was later integrated in OntoNotes).In FrameNet corpora, content words are associ-ated to particular semantic frames that they evoke(e.g.
charm would relate to the Aesthetics frame)and their collocates in relevant syntactic positions(arguments of verbs, head nouns of adjectives,etc.)
would be assigned the corresponding frame-element labels (e.g.
in their dazzling charm, theirwould be The Entity for which a particular grad-able Attribute is appropriate and under considera-tion and dazzling would be Degree).
Neither IAAnor granularity seem to be an issue in FrameNet.We have not succeeded in finding a report on IAAin the original FrameNet annotation, except onemeasurement in progress in the annotation of theManually Annotated Subcorpus of English (Ide etal., 2008).1PropBank is a valency (argument structure) lex-icon.
The current resource lists and labels ar-guments and obligatory modifiers typical of each(very coarse) word sense (called frameset).
Twocore criteria for distinguishing among framesetsare the semantic roles of the arguments alongwith the syntactic alternations that the verb canundergo with that particular argument set.
Tokeep low granularity, this lexicon?among otherthings?does usually not make special framesetsfor metaphoric uses.
The overall IAA measuredon verbs was 94 % (Palmer et al 2005).2.3 Semantic Pattern RecognitionFrom corpus-based lexicography to semanticpatternsThe modern, corpus-based lexicology of 1990s(Sinclair, 1991; Fillmore and Atkins, 1994) hashad a great impact on lexicography.
There is ageneral consensus that dictionary definitions needto be supported by corpus examples.
Cf.
Fell-baum (2001):?For polysemous words, dictionaries [.
.
. ]
donot say enough about the range of possible con-texts that differentiate the senses.
[.
.
. ]
On theother hand, texts or corpora [.
.
. ]
are not ex-plicit about the word?s meaning.
When we firstencounter a new word in a text, we can usuallyform only a vague idea of its meaning; checking adictionary will clarify the meaning.
But the morecontexts we encounter for a word, the harder it isto match them against only one dictionary sense.
?1Checked on the project web www.anc.org/MASC/Home2011-10-29.842The lexical description in modern Englishmonolingual dictionaries (Sinclair et al 1987;Rundell, 2002) explicitly emphasizes contextualclues, such as typical collocates and the syntac-tic surroundings of the given lexical item, ratherthan relying on very detailed definitions.
Inother words, the sense definitions are obtainedas syntactico-semantic abstractions of manuallyclustered corpus concordances in the moderncorpus-based lexicography: in classical dictionar-ies as well as in semantic concordances.Nevertheless, the word senses, even when ob-tained by a collective mind of lexicographers andannotators, are naturally hard-wired and tailoredto the annotated corpus.
They may be too fine-grained or too coarse-grained for automatic pro-cessing of different corpora (e.g.
a restricted-domain corpus).
Kilgarriff (1997, p. 115) shows(the handbag example) that there is no reason toexpect the same set of word senses to be relevantfor different tasks and that the corpus dictates theword senses and therefore ?word sense?
was notfound to be sufficiently well-defined to be a work-able basic unit of meaning (p. 116).
On the otherhand, even non-experts seem to agree reasonablywell when judging the similarity of use of a wordin different contexts (Rumshisky et al 2009).
Erket al(2009) showed promising annotation resultswith a scheme that allowed the annotators gradedjudgments of similarity between two words or be-tween a word and its definition.Verbs are the most challenging part of speech.We see two major causes: vagueness and coer-cion.
We neglect ambiguity, since it has proved tobe rare in our experience.CPA and PDEVOur current work focuses on English verbs.It has been inspired by the manual Corpus Pat-tern Analysis method (CPA) (Hanks, forthcom-ing) and its implementation, the Pattern Dictio-nary of English Verbs (PDEV) (Hanks and Puste-jovsky, 2005).
PDEV is a semantic concordancebuilt on yet a different principle than FrameNet,WordNet, PropBank or OntoNotes.
The man-ually extracted patterns of frequent and normalverb uses are, roughly speaking, intuitively sim-ilar uses of a verb that express?in a syntacti-cally similar form?a similar event in which sim-ilar participants (e.g.
humans, artifacts, institu-tions, other events) are involved.
Two patternscan be semantically so tightly related that theycould appear together under one sense in a tradi-tional dictionary.
The patterns are not senses butsyntactico-semantically characterized prototypes(see the example verb submit in Table 1).
Con-cordances that match these prototypes well arecalled norms in Hanks (forthcoming).
Concor-dances that match with a reservation (metaphor-ical uses, argument mismatch, etc.)
are called ex-ploitations.
The PDEV corpus annotation indi-cates the norm-exploitation status for each con-cordance.Compared to other semantic concordances, thegranularity of PDEV is high and thus discourag-ing in terms of expected IAA.
However, select-ing among patterns does not really mean disam-biguating a concordance but rather determining towhich pattern it is most similar?a task easier forhumans than WSD is.
This principle seems par-ticularly promising for verbs as words expressingevents, which resist the traditional word sense dis-ambiguation the most.A novel approach to semantic taggingWe present the semantic pattern recognition asa novel approach to semantic tagging, which isdifferent from the traditional word-sense assign-ment tasks.
We adopt the central idea of CPA thatwords do not have fixed senses but that regularpatterns can be identified in the corpus that ac-tivate different conversational implicatures fromthe meaning potential of the given verb.
Ourmethod draws on a hard-wired, fine-grained in-ventory of semantic categories manually extractedfrom corpus data.
This inventory represents themaximum semantic granularity that humans areable to recognize in normal and frequent uses of averb in a balanced corpus.
We thoroughly analyzethe interannotator agreement to find out which ofthe highly semantic categories are useful in thesense of information gain.
Our goal is a dynamicoptimization of semantic granularity with respectto given data and target application.Like Passonneau et al(2010), we are con-vinced that IAA is specific to each respectiveword and reflects its inherent semantic propertiesas well as the specificity of contexts the givenword occurs in, even within the same balancedcorpus.
We accept as a matter of fact that inter-annotator confusion is inevitable in semantic tag-ging.
However, the amount of uncertainty of the843No.
Pattern / Implicature1[[Human 1 | Institution 1] ?
[Human 1 | Institution 1 = Competitor]] submit [[Plan | Document| Speech Act | Proposition | {complaint | demand | request | claim | application | proposal| report | resignation | information | plea | petition | memorandum | budget | amendment |programme | .
.
.}]
?
[Artifact | Artwork | Service | Activity | {design | tender | bid | entry| dance | .
.
.}]]
(({to} Human 2 | Institution 2 = authority)?
({to} Human 2 | Institution 2 =referee)) ({for} {approval | discussion | arbitration | inspection | designation | assessment |funding | taxation | .
.
.
})[[Human 1 | Institution 1]] presents [[Plan | Document]] to [[Human 2 | Institution 2]] for {approval| discussion | arbitration | inspection | designation | assessment | taxation | .
.
.
}2[Human | Institution] submit [THAT-CL|QUOTE][[Human | Institution]] respectfully expresses {that [CLAUSE]} and invites listeners or readers toaccept that {that [CLAUSE]} is true}4[Human 1 | Institution 1] submit (Self) ({to} Human 2 | Institution 2)[[Human 1 | Institution 1]] acknowledges the superior force of [[Human 2 | Institution 2]] and puts[[Self]] in the power of [[Human 2 | Institution 2]]5[Human 1] submit (Self) [[{to} Eventuality = Unpleasant] ?
[{to} Rule]][[Human 1]] accepts [[Rule |Eventuality = Unpleasant]] without complaining6[passive][Human| Institution] submit [Anything] [{to} Eventuality][[Human 1|Institution 1]] exposes [[Anything]] to [[Eventuality]]Table 1: Example of patterns defined for the verb submit.?right?
tag differs a lot, and should be quantified.For that purpose we developed the reliable infor-mation gain measure presented in Section 3.2.CPA Verb Validation SampleThe original PDEV had never been tested withrespect to IAA.
Each entry had been based onconcordances annotated solely by the author ofthat particular entry.
The annotation instructionshad been transmitted only orally.
The data hadbeen evolving along with the method, which im-plied inconsistencies.
We put down an annotationmanual (a momentary snapshot of the theory) andtrained three annotators accordingly.
For practicalannotation we use the infrastructure developed atMasaryk University in Brno (Hora?k et al 2008),which was also used for the original PDEV de-velopment.
After initial IAA experiments withthe original PDEV, we decided to select 30 verbentries from PDEV along with the annotated con-cordances.
We made a new semantic concordancesample (Cinkova?
et al 2012) for the validation ofthe annotation scheme.
We refer to this new col-lection2 as VPS-30-En (Verb Pattern Sample, 30English verbs).We slightly revised some entries and updatedthe reference samples (usually 250 concordances2This new lexical resource, including the complete docu-mentation, is publicly available at http://ufal.mff.cuni.cz/spr.per verb).
The annotators were given the en-tries as well as the reference sample annotatedby the lexicographer and a test sample of 50 con-cordances for annotation.
We measured IAA, us-ing Fleiss?s kappa,3 and analyzed the interannota-tor confusion manually.
IAA varied from verb toverb, mostly reaching safely above 0.6.
When theIAA was low and the type of confusion indicated aproblem in the entry, the entry was revised.
Thenthe lexicographer revised the original referencesample along with the first 50-concordance sam-ple.
The annotators got back the revised entry, thenewly revised reference sample and an entirelynew 50-concordance annotation batch.
The fi-nal multiple 50-concordance sample went throughone more additional procedure, the adjudication:first, the lexicographer compared the three anno-tations and eliminated evident errors.
Then thelexicographer selected one value for each concor-dance to remain in the resulting one-value-per-concordance gold standard data and recorded itinto the gold standard set.
The adjudication pro-3Fleiss?s kappa (Fleiss, 1971) is a generalization ofScott?s pi statistic (Scott, 1955).
In contrast to Cohen?s kappa(Cohen, 1960), Fleiss?s kappa evaluates agreement betweenmultiple raters.
However, Fleiss?s kappa is not a generaliza-tion of Cohen?s kappa, which is a different, yet related, sta-tistical measure.
Sometimes, the terminology about kappasis confusing in the literature.
For a detailed explanation refere.g.
to (Artstein and Poesio, 2008).844tocol has been kept for further experiments.
Allvalues except the marked errors are regarded asequally acceptable for this type of experiments.In the end, we get for each verb:?
an entry, which is an inventory of semanticcategories (patterns)?
300+ manually annotated concordances (sin-gle values)?
out of which 50 are manually annotated andadjudicated concordances (multiple valueswithout evident errors).3 Tagging confusion analysis3.1 Formal model of tagging confusionTo formally describe the semantic tagging task,we assume a target word and a (randomly se-lected) corpus sample of its occurrences.
Thetagged sample is S = {s1, .
.
.
, sr}, where eachinstance si is an occurrence of the target wordwith its context, and r is the sample size.For multiple annotation we need a set of m an-notators A = {A1, .
.
.
, Am} who choose froma given set of semantic categories representedby a set of n semantic tags T = {t1, .
.
.
, tn}.Generally, if we admitted assigning more tags toone word occurrence, annotators could assign anysubset of T to an instance.
In our experiments,however, annotators were allowed to assign justone tag to each tagged instance.
Therefore eachannotator is described as a function that assigns asingle member set to each instance Ai(s) = {t},where s ?
S, t ?
T .
When a pair of annotatorstag an instance s, they produce a set of one or twodifferent tags {t, t?}
= Ai(s) ?Aj(s).Detailed information about interannotator(dis)agreement on a given sample S is rep-resented by a set of(m2)symmetric matricesCAkAlij = |{s ?
S | Ak(s) ?
Al(s) = {ti, tj}}|,for 1 ?
k < l ?
m, and i, j ?
{1, .
.
.
, n}.Note that each of those matrices can be easilycomputed as CAkAl = C + CT ?
InC, whereC is a conventional confusion matrix representingthe agreement between annotators Ak and Al,and In is a unit matrix.Definition: Aggregated Confusion Matrix (ACM)C?
=?1?k<l?mCAkAl .Properties: ACM is symmetric and for any i 6= jthe number C?ij says how many times a pair ofannotators disagreed on two tags ti and tj , whileC?ii is the frequency of agreements on ti; the sumin the i-th row?j C?ij is the total frequency ofassigned sets {t, t?}
that contain ti.An example of ACM is given in Table 2.
Thecorresponding confusion matrices are shown inTable 3.1 1.a 2 4 51 85 8 2 0 01.a 8 1 2 0 02 2 2 34 0 04 0 0 0 4 85 0 0 0 8 6Table 2: Aggregated Confusion Matrix.Our approach to exact tagging confusion analy-sis is based on probability and information theory.Assigning semantic tags by annotators is viewedas a random process.
We define (categorical) ran-dom variable T1 as the outcome of one annota-tor; its values are single member sets {t}, and wehave mr observations to compute their probabil-ities.
The probability that an annotator will useti is denoted by p1(ti) = Pr(T1 = {ti}) and ispractically computed as the relative frequency ofti among all mr assigned tags.
Formally,p1(ti) =1mrm?k=1r?j=1|Ak(sj) ?
{ti}|.The outcome of two annotators (they both tagthe same instance) is described by random vari-able T2; its values are single or double membersets {t, t?
}, and we have(m2)r observations tocompute their probabilities.
In contrast to p1, theprobability that ti will be used by a pair of anno-tators is denoted by p2(ti) = Pr(T2 ?
{ti}), andis computed as the relative frequency of assignedsets {t, t?}
containing ti among all(m2)r observa-tions:p2(ti) =1(m2)r?kC?ik.We also need the conditional probability that anannotator will use ti given that another annotatorhas used tj .
For convenience, we use the nota-tion p2(ti | tj) = Pr(T2 ?
{ti} | T2 ?
{tj}).845A1 vs. A2 A1 vs. A3 A2 vs. A31 1.a 2 4 5 1 1.a 2 4 5 1 1.a 2 4 51 29 1 1 0 0 1 29 2 0 0 0 1 27 2 0 0 01.a 0 1 0 0 0 1.a 1 0 0 0 0 1.a 2 0 1 0 02 0 1 11 0 0 2 0 0 12 0 0 2 1 0 11 0 04 0 0 0 2 0 4 0 0 0 1 1 4 0 0 0 1 45 0 0 0 3 1 5 0 0 0 0 4 5 0 0 0 0 1Table 3: Example of all confusion matrices for the target word submit and three annotators.Obviously, it can be computed asp2(ti | tj) =Pr(T2 = {ti, tj})Pr(T2 ?
{tj})=C?ij(m2)r ?
p2(tj)=C?ij?k C?jk.Definition: Confusion Probability Matrix (CPM)Cpji = p2(ti | tj) =C?ij?k C?jk.Properties: The sum in any row is 1.
The j-throw of CPM contains probabilities of assigning tigiven that another annotator has chosen tj for thesame instance.
Thus, the j-th row of CPM de-scribes expected tagging confusion related to thetag tj .An example is given in Table 3 (all confusionmatrices for three annotators), in Table 2 (thecorresponding ACM), and in Table 4 (the corre-sponding CPM).1 1.a 2 4 51 0.895 0.084 0.021 0.000 0.0001.a 0.727 0.091 0.182 0.000 0.0002 0.053 0.053 0.895 0.000 0.0004 0.000 0.000 0.000 0.333 0.6675 0.000 0.000 0.000 0.571 0.429Table 4: Example of Confusion Probability Matrix.3.2 Semantic granularity optimizationNow, having a detailed analysis of expected tag-ging confusion described in CPM, we are able tocompare usefulness of different semantic tags us-ing a measure of the information content associ-ated with them (in the information theory sense).Traditionally, the amount of self-information con-tained in a tag (as a probabilistic event) dependsonly on the probability of that tag, and would bedefined as I(tj) = ?
log p1(tj).
However, intu-itively one can say that a good measure of use-fulness of a particular tag should also take intoconsideration the expected tagging confusion re-lated to the tag.
Therefore, to exactly measureusefulness of the tag tj we propose to compareand measure similarity of the distribution p1(ti)and the distribution p2(ti | tj), i = 1, .
.
.
, n.How much information do we gain when an an-notator assigns the tag tj to an instance?
Whenthe tag tj has once been assigned to an instanceby an annotator, one would naturally expect thatanother annotator will probably tend to assign thesame tag tj to the same instance.
Formally, thingsmake good sense if p2(tj | tj) > p1(tj) and ifp2(ti | tj) < p1(ti) for any i different from j.If p2(tj | tj) = 100%, then there is full con-sensus about assigning tj among annotators; thenand only then the measure of usefulness of the tagtj should be maximal and should have the valueof ?
log p1(tj).
Otherwise, the value of useful-ness should be smaller.
This is our motivation todefine a quantity of reliable information gain ob-tained from semantic tags as follows:Definition: Reliable Gain (RG) from the tag tj isRG(tj) =?k?
(?1)?kjp2(tk|tj) logp2(tk|tj)p1(tk).Properties: RG is similar to the well knownKullback-Leibler divergence (or informationgain).
If p2(ti | tj) = p1(ti) for all i = 1, .
.
.
, n,then RG(tj) = 0.
If p2(tj | tj) = 100%, thenand only then RG(tj) = ?
log p1(tj), whichis the maximum.
If p2(ti | tj) < p1(ti) forall i different from j, the greater difference inprobabilities, the bigger (and positive) RG(tj).And vice versa, the inequality p2(ti | tj) > p1(ti)for all i different from j implies a negative valueof RG(tj).846Definition: Average Reliable Gain (ARG) fromthe tagset {t1, .
.
.
, tn} is computed as an expectedvalue of RG(tj):ARG =?jp1(tj)RG(tj)Properties: ARG has its maximum value if theCPM is a unit matrix, which is the case of theabsolute agreement among all annotators.
ThenARG has the value of the entropy of the p1 distri-bution: ARGmax = H(p1(t1), .
.
.
, p1(tn)).Merging tags with poor RGThe main motivation for developing the ARGvalue was the optimization of the tagset granular-ity.
We use a semi-greedy algorithm that searchesfor an ?optimal?
tagset.
The optimization processstarts with the fine-grained list of CPA semanticcategories and then the algorithm merges sometags in order to maximize the ARG value.
An ex-ample is given in Table 5.
Tables 6 and 7 showthe ACM and the CPM after merging.
The ex-amples relate to the verb submit already shown inTables 1, 2, 3 and 4.Original tagset Optimal mergeTag f RG Tag f RG1 90 +0.3001 + 1.a 96 +0.4251.a 6 ?0.0012 36 +0.447 2 36 +0.4734 8 ?0.0714 + 5 18 +0.3675 10 ?0.054Table 5: Frequency and Reliable Gain of tags.1 2 41 94 4 02 4 34 04 0 0 18Table 6: Aggregated Confusion Matrix after merging.1 2 41 0.959 0.041 0.0002 0.105 0.895 0.0004 0.000 0.000 1.000Table 7: Confusion Probability Matrix after merging.3.3 Classifier evaluation with respect toexpected tagging confusionAn automatic classifier is considered to be a func-tion c that?the same way as annotators?
assignstags to instances s ?
S, so that c(s) = {t},t ?
T .
The traditional way to evaluate the ac-curacy of an automatic classifier means to com-pare its output with the correct semantic tags ona Gold Standard (GS) dataset.
Within our formalframework, we can imagine that we have a ?gold?annotatorAg, so that the GS dataset is representedbyAg(s1), .
.
.
, Ag(sr).
Then the classic accuracyscore can be computed as 1r?ri=1 |Ag(si)?c(si)|.However, that approach does not take into con-sideration the fact that some semantic tags arequite confusing even for human annotators.
In ouropinion, automatic classifier should not be penal-ized for mistakes that would be made even by hu-mans.
So we propose a more complex evaluationscore using the knowledge of the expected taggingconfusion stored in CPM.Definition: Classifier evaluation Score with re-spect to tagging confusion is defined as the pro-portion Score(c) = S(c)/Smax, whereS(c) =?rr?i=1|Ag(si) ?
c(si)| ++1?
?rr?i=1p2(c(si) | Ag(si))Smax = ?+1?
?rr?i=1p2(Ag(si) | Ag(si)).?
= 1 ?
= 0.5 ?
= 0Verb Score Score Scorehalt 1 0.84 2 0.90 4 0.81submit 2 0.83 1 0.90 1 0.84ally 3 0.82 3 0.89 5 0.76cry 4 0.79 4 0.88 2 0.82arrive 5 0.74 5 0.85 3 0.81plough 6 0.70 6 0.81 6 0.72deny 7 0.62 7 0.74 7 0.66cool 8 0.58 8 0.69 8 0.53yield 9 0.55 9 0.67 9 0.52Table 8: Evaluation with different ?
values.Table 8 gives an illustration of the fact that us-ing different ?
values one can get different re-847sults when comparing tagging accuracy for dif-ferent words (a classifier based on bag-of-wordsapproach was used).
The same holds true for com-parison of different classifiers.3.4 Related workIn their extensive survey article Artstein and Poe-sio (2008) state that word sense tagging is oneof the hardest annotation tasks.
They assumethat making distinctions between semantic cate-gories must rely on a dictionary.
The problemis that annotators often cannot consistently makethe fine-grained distinctions proposed by trainedlexicographers, which is particularly serious forverbs, because verbs generally tend to be polyse-mous rather than homonymous.A few approaches have been suggested inthe literature that address the problem of thefine-grained semantic distinctions by (automatic)measuring sense distinguishability.
Diab (2004)computes sense perplexity using the entropy func-tion as a characteristic of training data.
She alsocompares the sense distributions to obtain sensedistributional correlation, which can serve as a?very good direct indicator of performance ra-tio?, especially together with sense context con-fusability (another indicator observed in the train-ing data).
Resnik and Yarowsky (1999) intro-duced the communicative/semantic distance be-tween the predicted sense and the ?correct?
sense.Then they use it for evaluation metric that pro-vides partial credit for incorrectly classified in-stances.
Cohn (2003) introduces the concept of(non-uniform) misclassification costs.
He makesuse of the communicative/semantic distance andproposes a metric for evaluating word sense dis-ambiguation performance using the Receiver Op-erating Characteristics curve that takes the mis-classification costs into account.
Bruce andWiebe (1998) analyze the agreement among hu-man judges for the purpose of formulating a re-fined and more reliable set of sense tags.
Theirmethod is based on statistical analysis of inter-annotator confusion matrices.
An extended studyis given in (Bruce and Wiebe, 1999).4 ConclusionThe usefulness of a semantic resource depends ontwo aspects:?
reliability of the annotation?
information gain from the annotation.In practice, each semantic resource emphasizesone aspect: OntoNotes, e.g., guarantees reliabil-ity, whereas the WordNet-annotated corpora seekto convey as much semantic nuance as possible.To the best of our knowledge, there has been noexact measure for the optimization, and the use-fulness of a given resource can only be assessedwhen it is finished and used in applications.
Wepropose the reliable information gain, a measurebased on information theory and on the analysis ofinterannotator confusion matrices for each wordentry, that can be continually applied during thecreation of a semantic resource, and that providesautomatic feedback about the granularity of theused tagset.
Moreover, the computed informationabout the amount of expected tagging confusionis also used in evaluation of automatic classifiers.AcknowledgmentsThis work has been supported by the Czech Sci-ence Foundation projects GK103/12/G084 andP406/2010/0875 and partly by the project Euro-MatrixPlus (FP7-ICT-2007-3-231720 of the EUand 7E09003+7E11051 of the Ministry of Edu-cation, Youth and Sports of the Czech Republic).We thank our friends from Masaryk Universityin Brno for providing the annotation infrastruc-ture and for their permanent technical support.We thank Patrick Hanks for his CPA method, forthe original PDEV development, and for numer-ous discussions about the semantics of Englishverbs.
We also thank three anonymous reviewersfor their valuable comments.848ReferencesRoni Ben Aharon, Idan Szpektor, and Ido Dagan.2010.
Generating entailment rules from FrameNet.In Proceedings of the ACL 2010 Conference ShortPapers., pages 241?246, Uppsala, Sweden.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596, December.Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-Intyre, Victoria Tredinnick, Grace Kim, Mary AnnMarcinkiewicz, and Britta Schasberger.
1995.Bracketing guidelines for treebank II style.
Tech-nical report, University of Pennsylvania.Susan Windisch Brown, Travis Rood, and MarthaPalmer.
2010.
Number or nuance: Which factorsrestrict reliable word sense annotation?
In LREC,pages 3237?3243.
European Language ResourcesAssociation (ELRA).Rebecca F. Bruce and Janyce M. Wiebe.
1998.
Word-sense distinguishability and inter-coder agreement.In Proceedings of the Third Conference on Em-pirical Methods in Natural Language Processing(EMNLP ?98), pages 53?60.
Granada, Spain, June.Rebecca F. Bruce and Janyce M. Wiebe.
1999.
Recog-nizing subjectivity: A case study of manual tagging.Natural Language Engineering, 5(2):187?205.Silvie Cinkova?, Martin Holub, Adam Rambousek, andLenka Smejkalova?.
2012.
A database of seman-tic clusters of verb usages.
In Proceedings of theLREC ?2012 International Conference on LanguageResources and Evaluation.
To appear.Jacob Cohen.
1960.
A coefficient of agreement fornominal scales.
Educational and PsychologicalMeasurement, 20(1):37?46.Trevor Cohn.
2003.
Performance metrics for wordsense disambiguation.
In Proceedings of the Aus-tralasian Language Technology Workshop 2003,pages 86?93, Melbourne, Australia, December.Mona T. Diab.
2004.
Relieving the data acquisitionbottleneck in word sense disambiguation.
In Pro-ceedings of the 42nd Annual Meeting of the ACL,pages 303?310.
Barcelona, Spain.
Association forComputational Linguistics.Katrin Erk, Diana McCarthy, and Nicholas Gaylord.2009.
Investigations on word senses and word us-ages.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 10?18, Suntec,Singapore, August.
Association for ComputationalLinguistics.Katrin Erk.
2010.
What is word meaning, really?
(And how can distributional models help us de-scribe it?).
In Proceedings of the 2010 Workshopon GEometrical Models of Natural Language Se-mantics, pages 17?26, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Christiane Fellbaum, Joachim Grabowski, and ShariLandes.
1997.
Analysis of a hand-tagging task.
InProceedings of the ACL/Siglex Workshop, Somer-set, NJ.Christiane Fellbaum, J. Grabowski, and S. Landes.1998.
Performance and confidence in a semanticannotation task.
In WordNet: An Electronic LexicalDatabase, pages 217?238.
Cambridge (Mass.
): TheMIT Press., Cambridge (Mass.
).Christiane Fellbaum, Martha Palmer, Hoa Trang Dang,Lauren Delfs, and Susanne Wolf.
2001.
Manualand automatic semantic annotation with WordNet.Christiane Fellbaum.
1998.
WordNet.
An ElectronicLexical Database.
MIT Press, Cambridge, MA.Charles J. Fillmore and B. T. S. Atkins.
1994.
Start-ing where the dictionaries stop: The challenge forcomputational lexicography.
In Computational Ap-proaches to the Lexicon, pages 349?393.
OxfordUniversity Press.Joseph L. Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 76:378?382.Patrick Hanks and James Pustejovsky.
2005.
A pat-tern dictionary for natural language processing.
Re-vue Francaise de linguistique applique, 10(2).Patrick Hanks.
forthcoming.
Lexical Analysis: Normsand Exploitations.
MIT Press.Ales?
Hora?k, Adam Rambousek, and Piek Vossen.2008.
A distributed database system for develop-ing ontological and lexical resources in harmony.In 9th International Conference on Intelligent TextProcessing and Computational Linguistics, pages1?15.
Berlin: Springer.Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw, and Ralph Weischedel.
2006.OntoNotes: the 90% solution.
In Proceedingsof the Human Language Technology Conferenceof the NAACL, Companion Volume: Short Papers,NAACL-Short ?06, pages 57?60, Stroudsburg, PA,USA.
Association for Computational Linguistics.Nancy Ide, Collin Baker, Christiane Fellbaum, CharlesFillmore, and Rebecca Passoneau.
2008.
MASC:The Manually Annotated Sub-Corpus of AmericanEnglish.
In Proceedings of the Sixth InternationalConference on Language Resources and Evaluation(LREC?08), pages 28?30.
European Language Re-sources Association (ELRA).Julia Jorgensen.
1990.
The psycholinguistic reality ofword senses.
Journal of Psycholinguistic Research,(19):167?190.Adam Kilgarriff.
1997.
?I don?t believe in wordsenses?.
Computers and the Humanities, 31(2):91?113.Ramesh Krishnamurthy and Diane Nicholls.
2000.Peeling an onion: The lexicographer?s experienceof manual sense tagging.
Computers and the Hu-manities, 34:85?97.849Ryan McDonald, Kevin Lerman, and FernandoPereira.
2006.
Multilingual dependency analysiswith a two-stage discriminative parser.
In Proceed-ings of the Tenth Conference on Computational Nat-ural Language Learning CoNLLX 06, pages 216?220.
Association for Computational Linguistics.Adam Meyers, Ruth Reeves, and Catherine Macleod.2008.
NomBank v 1.0.G.
A. Miller, C. Leacock, R. Tengi, and R. T. Bunker.1993a.
A semantic concordance.
In Proceedings ofARPA Workshop on Human Language Technology.G.
A. Miller, C. Leacock, R. Tengi, and R. T. Bunker.1993b.
A semantic concordance.
In Proceedings ofARPA Workshop on Human Language Technology.Roberto Navigli.
2006.
Meaningful clustering ofsenses helps boost word sense disambiguation per-formance.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the ACL, pages 105?112, Syd-ney, Australia.Martha Palmer, Dan Gildea, and Paul Kingsbury.2005.
The proposition bank: A corpus annotatedwith semantic roles.
Computational LinguisticsJournal, 31(1).Rebecca J. Passonneau, Ansaf Salleb-Aoussi, VikasBhardwaj, and Nancy Ide.
2010.
Word sense anno-tation of PolysemousWords by multiple annotators.In LREC Proceedings, pages 3244?3249, Valetta,Malta.Philip Resnik and David Yarowsky.
1999.
Distin-guishing systems and distinguishing senses: Newevaluation methods for word sense disambiguation.Natural Language Engineering, 5(2):113?133.Anna Rumshisky, M. Verhagen, and J. Moszkowicz.2009.
The holy grail of sense definition: Creatinga Sense-Disambiguated corpus from scratch.
Pisa,Italy.Michael Rundell.
2002.
Macmillan English Dictio-nary for advanced learners.
Macmillan Education.Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.Petruck, Christopher R. Johnson, and Jan Schef-fczyk.
2010.
FrameNet II: Extended Theory andPractice.
ICSI, University of Berkeley, September.Beatrice Santorini.
1990.
Part-of-Speech taggingguidelines for the penn treebank project.
Universityof Pennsylvania 3rd Revision 2nd Printing, (MS-CIS-90-47):33.William A. Scott.
1955.
Reliability of content analy-sis: The case of nominal scale coding.
Public Opin-ion Quarterly, 19(3):321?325.John Sinclair, Patrick Hanks, and et al1987.
CollinsCobuild English Dictionary for Advanced Learn-ers 4th edition published in 2003.
HarperCollinsPublishers 1987, 1995, 2001, 2003 and CollinsA?Z Thesaurus 1st edition first published in 1995.HarperCollins Publishers 1995.John Sinclair.
1991.
Corpus, Concordance, Colloca-tion.
Describing English Language.
Oxford Univer-sity Press.Ralph Weischedel, Martha Palmer, Mitchell Marcus,Eduard Hovy, Sameer Pradhan, Lance Ramshaw,Nianwen Xue, Ann Taylor, Jeff Kaufman, MichelleFranchini, Mohammed El-Bachouti, Robert Belvin,and Ann Houston.
2011.
OntoNotes release 4.0.Fabio Massimo Zanzotto, Marco Pennacchiotti, andAlessandro Moschitti.
2009.
A machine learningapproach to textual entailment recognition.
NaturalLanguage Engineering, 15(4):551?582.Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(November2009):105?151.850
