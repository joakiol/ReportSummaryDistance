Why Doesn't Natural Language Come Naturally?Richard  Schwar tzBBN Techno log iesCambr idge ,  MA 02138schwar tz@bbn.comAbst ractWe have seen great success over the past 15 yearsin speech recognition.
This success is due, largely,to the broad acceptance of Hidden Markov Models(HMMs) at the beginning of that period, which thenfacilitated rapid and steady improvements in speechrecognition that are still continuing today.
Althoughno one believes peech is produced by an HMM, themodel affords a rich framework in which to improvethe model in a rigorous and scientific manner.Could we create the same environment for a uni-form probabilistic paradigm in NL?
It requires ev-eral ingredients:?
A uniform notational system to express mean-ings,?
A statistical model that can represent the asso-ciations between meanings and words,* A training program that estimates parametersfrom annotated examples,?
An understanding program that finds the mostlikely meaning iven a word sequence, and?
A substantial corpus with meanings annotatedand aligned to the words.These problems are fundamental.
In speech recog-nition, we can all agree that the desired output isa sequence of orthographic words.
But in under-standing, we lack agreement as to the meaning ofmeaning.
And it gets harder from there, since thestructures we must look at are not sequences, butrather trees or more complex structures.
Still thegoal is a worthwhile one.We attempt to formulate several different lan-guage understanding problems as probabilistic pat-tern recognition problems.
In general, our goal is torely heavily on corpus based methods and learningtechniques rather than on human generated rules.At the same time, it is essential that we be able toincorporate our intuitions about the problem intothe model.
We choose probabilistic methods as ourpreferred form of learning technique because theyhave several desirable properties.
First, if we canaccurately estimate the posterior probability of ourdesired result, then we know a decision based on thisposterior probability will minimize the error rate.Second, we have a large inventory of techniques forestimation of robust probabilities from finite data.Third, in contrast o classical pattern recognitionproblems, language deals almost exclusively with se-quences (of sounds, phonemes, charaCters, words,sentences, etc.)
Our goal is not to recognize or un-derstand each of these independently, but rather tounderstand the sequence.
Probability theory pro-vides a convenient way to combine several pieces ofevidence in making a decision.We present several anguage problems for whichwe have developed probabilistic methods thatachieve accuracy comparable to that of the bestrule-based systems.
In each case we developed amodel that is (somewhat) appropriate for the prob-lem.
These problems include Topic Classification,Information Retrieval, Extracting Named Entities,and Extracting Relations.128
