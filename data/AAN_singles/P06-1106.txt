Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 841?848,Sydney, July 2006. c?2006 Association for Computational LinguisticsAnswer Extraction, Semantic Clustering, and Extractive Summarizationfor Clinical Question AnsweringDina Demner-Fushman1,3 and Jimmy Lin1,2,31Department of Computer Science2College of Information Studies3Institute for Advanced Computer StudiesUniversity of MarylandCollege Park, MD 20742, USAdemner@cs.umd.edu, jimmylin@umd.eduAbstractThis paper presents a hybrid approachto question answering in the clinicaldomain that combines techniques fromsummarization and information retrieval.We tackle a frequently-occurring class ofquestions that takes the form ?What isthe best drug treatment for X??
Startingfrom an initial set of MEDLINE citations,our system first identifies the drugs un-der study.
Abstracts are then clustered us-ing semantic classes from the UMLS on-tology.
Finally, a short extractive sum-mary is generated for each abstract to pop-ulate the clusters.
Two evaluations?amanual one focused on short answers andan automatic one focused on the support-ing abstracts?demonstrate that our sys-tem compares favorably to PubMed, thesearch system most widely used by physi-cians today.1 IntroductionComplex information needs can rarely be ad-dressed by single documents, but rather require theintegration of knowledge from multiple sources.This suggests that modern information retrievalsystems, which excel at producing ranked lists ofdocuments sorted by relevance, may not be suffi-cient to provide users with a good overview of the?information landscape?.Current question answering systems aspire toaddress this shortcoming by gathering relevant?facts?
from multiple documents in response toinformation needs.
The so-called ?definition?or ?other?
questions at recent TREC evalua-tions (Voorhees, 2005) serve as good examples:?good answers?
to these questions include inter-esting ?nuggets?
about a particular person, organi-zation, entity, or event.The importance of cross-document informationsynthesis has not escaped the attention of other re-searchers.
The last few years have seen a conver-gence between the question answering and sum-marization communities (Amigo?
et al, 2004), ashighlighted by the shift from generic to query-focused summaries in the 2005 DUC evalua-tion (Dang, 2005).
Despite a focus on documentranking, different techniques for organizing searchresults have been explored by information retrievalresearchers, as exemplified by techniques based onclustering (Hearst and Pedersen, 1996; Dumais etal., 2001; Lawrie and Croft, 2003).Our work, which is situated in the domain ofclinical medicine, lies at the intersection of ques-tion answering, information retrieval, and summa-rization.
We employ answer extraction to identifyshort answers, semantic clustering to group sim-ilar results, and extractive summarization to pro-duce supporting evidence.
This paper describeshow each of these capabilities contributes to an in-formation system tailored to the requirements ofphysicians.
Two separate evaluations demonstratethe effectiveness of our approach.2 Clinical Information NeedsAlthough the need to answer questions relatedto patient care has been well documented (Cov-ell et al, 1985; Gorman et al, 1994; Ely et al,1999), studies have shown that existing search sys-tems, e.g., PubMed, the U.S. National Library ofMedicine?s search engine, are often unable to sup-ply physicians with clinically-relevant answers ina timely manner (Gorman et al, 1994; Cham-bliss and Conley, 1996).
Clinical information841Disease: Chronic ProstatitisI anti-microbial1.
[temafloxacin] Treatment of chronic bacterial prostatitis with temafloxacin.
Temafloxacin 400 mg b.i.d.
adminis-tered orally for 28 days represents a safe and effective treatment for chronic bacterial prostatitis.2.
[ofloxacin] Ofloxacin in the management of complicated urinary tract infections, including prostatitis.
In chronicbacterial prostatitis, results to date suggest that ofloxacin may be more effective clinically and as effective micro-biologically as carbenicillin.3.
...I Alpha-adrenergic blocking agent1.
[terazosine] Terazosin therapy for chronic prostatitis/chronic pelvic pain syndrome: a randomized, placebo con-trolled trial.
CONCLUSIONS: Terazosin proved superior to placebo for patients with chronic prostatitis/chronicpelvic pain syndrome who had not received alpha-blockers previously.2.
...Table 1: System response to the question ?What is the best drug treatment for chronic prostatitis?
?systems for decision support represent a poten-tially high-impact application.
From a researchperspective, the clinical domain is attractive be-cause substantial knowledge has already been cod-ified in the Unified Medical Language System(UMLS) (Lindberg et al, 1993).
The 2004 versionof the UMLS Metathesaurus contains informationabout over 1 million biomedical concepts and 5million concept names.
This and related resourcesallow us to explore knowledge-based techniqueswith substantially less upfront investment.Naturally, physicians have a wide spectrum ofinformation needs, ranging from questions aboutthe selection of treatment options to questionsabout legal issues.
To make the retrieval problemmore tractable, we focus on a subset of therapyquestions taking the form ?What is the best drugtreatment for X?
?, where X can be any number ofdiseases.
We have chosen to tackle this class ofquestions because studies of physicians?
behaviorin natural settings have revealed that such ques-tions occur quite frequently (Ely et al, 1999).
Byleveraging the natural distribution of clinical in-formation needs, we can make the greatest impactwith the least effort.Our research follows the principles of evidence-based medicine (EBM) (Sackett et al, 2000),which provides a well-defined model to guide theprocess of clinical question answering.
EBM isa widely-accepted paradigm for medical practicethat involves the explicit use of current best ev-idence, i.e., high-quality patient-centered clinicalresearch reported in the primary medical literature,to make decisions about patient care.
As shownby previous work (Cogdill and Moore, 1997; DeGroote and Dorsch, 2003), citations from theMEDLINE database (maintained by the U.S. Na-tional Library of Medicine) serve as a good sourceof clinical evidence.
As a result of these findings,our work focuses on MEDLINE abstracts as thesource for answers.3 Question Answering ApproachConflicting desiderata shape the characteristics of?answers?
to clinical questions.
On the one hand,conciseness is paramount.
Physicians are alwaysunder time pressure when making decisions, andinformation overload is a serious concern.
Fur-thermore, we ultimately envision deploying ad-vanced retrieval systems in portable packages suchas PDAs to serve as tools in bedside interac-tions (Hauser et al, 2004).
The small form factorof such devices limits the amount of text that canbe displayed.
However, conciseness exists in ten-sion with completeness.
For physicians, the im-plications of making potentially life-altering deci-sions mean that all evidence must be carefully ex-amined in context.
For example, the efficacy of adrug is always framed in the context of a specificsample population, over a set duration, at somefixed dosage, etc.
A physician simply cannot rec-ommend a particular course of action without con-sidering all these factors.Our approach seeks to balance conciseness andcompleteness by providing hierarchical and inter-842active ?answers?
that support multiple levels ofdrill-down.
A partial example is shown in Fig-ure 1.
Top-level answers to ?What is the best drugtreatment for X??
consist of categories of drugsthat may be of interest to the physician.
Each cat-egory is associated with a cluster of abstracts fromMEDLINE about that particular treatment option.Drilling down into a cluster, the physician is pre-sented with extractive summaries of abstracts thatoutline the clinical findings.
To obtain more detail,the physician can pull up the complete abstracttext, and finally the electronic version of the en-tire article (if available).
In the example shown inFigure 1, the physician can see that two classes ofdrugs (anti-microbial and alpha-adrenergic block-ing agent) are relevant for the disease ?chronicprostatitis?.
Drilling down into the first cluster, thephysician can see summarized evidence for twospecific types of anti-microbials (temafloxacin andofloxacin) extracted from MEDLINE abstracts.Three major capabilities are required to producethe ?answers?
described above.
First, the systemmust accurately identify the drugs under study inan abstract.
Second, the system must group ab-stracts based on these substances in a meaningfulway.
Third, the system must generate short sum-maries of the clinical findings.
We describe a clin-ical question answering system that implementsexactly these capabilities (answer extraction, se-mantic clustering, and extractive summarization).4 System ImplementationOur work is primarily concerned with synthesiz-ing coherent answers from a set of search results?the actual source of these results is not important.For convenience, we employ MEDLINE citationsretrieved by the PubMed search engine (whichalso serves as a baseline for comparison).
Givenan initial set of citations, answer generation pro-ceeds in three phases, described below.4.1 Answer ExtractionGiven a set of abstracts, our system first identi-fies the drugs under study; these later become theshort answers.
In the parlance of evidence-basedmedicine, drugs fall into the category of ?interven-tions?, which encompasses everything from surgi-cal procedures to diagnostic tests.Our extractor for interventions relies onMetaMap (Aronson, 2001), a program that au-tomatically identifies entities corresponding toUMLS concepts.
UMLS has an extensive cov-erage of drugs, falling under the semantic typePHARMACOLOGICAL SUBSTANCE and a few oth-ers.
All such entities are identified as candidatesand each is scored based on a number of features:its position in the abstract, its frequency of occur-rence, etc.
A separate evaluation on a blind testset demonstrates that our extractor is able to accu-rately recognize the interventions in a MEDLINEabstract; see details in (Demner-Fushman and Lin,2005; Demner-Fushman and Lin, 2006 in press).4.2 Semantic ClusteringRetrieved MEDLINE citations are organized intosemantic clusters based on the main interventionsidentified in the abstract text.
We employed avariant of the hierarchical agglomerative cluster-ing algorithm (Zhao and Karypis, 2002) that uti-lizes semantic relationships within UMLS to com-pute similarities between interventions.Iteratively, we group abstracts whose interven-tions fall under a common ancestor, i.e., a hyper-nym.
The more generic ancestor concept (i.e., theclass of drugs) is then used as the cluster label.The process repeats until no new clusters can beformed.
In order to preserve granularity at thelevel of practical clinical interest, the tops of theUMLS hierarchy were truncated; for example, theMeSH category ?Chemical and Drugs?
is too gen-eral to be useful.
This process was manually per-formed during system development.
We decidedto allow an abstract to appear in multiple clustersif more than one intervention was identified, e.g.,if the abstract compared the efficacy of two treat-ments.
Once the clusters have been formed, allcitations are then sorted in the order of the origi-nal PubMed results, with the most abstract UMLSconcept as the cluster label.
Clusters themselvesare sorted in decreasing size under the assumptionthat more clinical research is devoted to more per-tinent types of drugs.Returning to the example in Figure 1, the ab-stracts about temafloxacin and ofloxacin wereclustered together because both drugs are hy-ponyms of anti-microbials within the UMLS on-tology.
As can be seen, this semantic resource pro-vides a powerful tool for organizing search results.4.3 Extractive SummarizationFor each MEDLINE citation, our system gener-ates a short extractive summary consisting of threeelements: the main intervention (which is usu-843ally more specific than the cluster label); the ti-tle of the abstract; and the top-scoring outcomesentence.
The ?outcome?, another term fromevidence-based medicine, asserts the clinical find-ings of a study, and is typically found towardsthe end of a MEDLINE abstract.
In our case,outcome sentences state the efficacy of a drug intreating a particular disease.
Previously, we havebuilt an outcome extractor capable of identifyingsuch sentences in MEDLINE abstracts using su-pervised machine learning techniques (Demner-Fushman and Lin, 2005; Demner-Fushman andLin, 2006 in press).
Evaluation on a blind held-out test set shows high classification accuracy.5 Evaluation MethodologyGiven that our work draws from QA, IR, and sum-marization, a proper evaluation that captures thesalient characteristics of our system proved to bequite challenging.
Overall, evaluation can be de-composed into two separate components: locatinga suitable resource to serve as ground truth andleveraging it to assess system responses.It is not difficult to find disease-specific pharma-cology resources.
We employed Clinical Evidence(CE), a periodic report created by the British Med-ical Journal (BMJ) Publishing Group that summa-rizes the best known drugs for a few dozen dis-eases.
Note that the existence of such secondarysources does not obviate the need for automatedsystems because they are perpetually falling out ofdate due to rapid advances in medicine.
Further-more, such reports are currently created by highly-experienced physicians, which is an expensive andtime-consuming process.For each disease, CE classifies drugs into one ofsix categories: beneficial, likely beneficial, trade-offs (i.e., may have adverse side effects), un-known, unlikely beneficial, and harmful.
Includedwith each entry is a list of references?citationsconsulted by the editors in compiling the resource.Although the completeness of the drugs enumer-ated in CE is questionable, it nevertheless can beviewed as ?authoritative?.5.1 Previous WorkHow can we leverage a resource such as CE to as-sess the responses generated by our system?
Asurvey of evaluation methodologies reveals short-comings in existing techniques.Answers to factoid questions are automaticallyscored using regular expression patterns (Lin,2005).
In our application, this is inadequatefor many reasons: there is rarely an exact stringmatch between system output and drugs men-tioned in CE, primarily due to synonymy (for ex-ample, alpha-adrenergic blocking agent and ?-blocker refer to the same class of drugs) and on-tological mismatch (for example, CE might men-tion beta-agonists, while a retrieved abstract dis-cusses formoterol, which is a specific represen-tative of beta-agonists).
Furthermore, while thisevaluation method can tell us if the drugs proposedby the system are ?good?, it cannot measure howwell the answer is supported by MEDLINE cita-tions; recall that answer justification is importantfor physicians.The nugget evaluation methodology (Voorhees,2005) developed for scoring answers to com-plex questions is not suitable for our task, sincethere is no coherent notion of an ?answer text?that the user reads end?to?end.
Furthermore, itis unclear what exactly a ?nugget?
in this casewould be.
For similar reasons, methodologies forsummarization evaluation are also of little help.Typically, system-generated summaries are eitherevaluated manually by humans (which is expen-sive and time-consuming) or automatically usinga metric such as ROUGE, which compares sys-tem output against a number of reference sum-maries.
The interactive nature of our answers vio-lates the assumption that systems?
responses arestatic text segments.
Furthermore, it is unclearwhat exactly should go into a reference summary,because physicians may want varying amounts ofdetail depending on familiarity with the diseaseand patient-specific factors.Evaluation methodologies from information re-trieval are also inappropriate.
User studies havepreviously been employed to examine the effectof categorized search results.
However, they oftenconflate the effectiveness of the interface with thatof the underlying algorithms.
For example, Du-mais et al (2001) found significant differences intask performance based on different ways of usingpurely presentational devices such as mouseovers,expandable lists, etc.
While interface design isclearly important, it is not the focus of our work.Clustering techniques have also been evaluatedin the same manner as text classification algo-rithms, in terms of precision, recall, etc.
basedon some ground truth (Zhao and Karypis, 2002).844This, however, assumes the existence of stable,invariant categories, which is not the case sinceour output clusters are query-specific.
Althoughit may be possible to manually create ?referenceclusters?, we lack sufficient resources to developsuch a data set.
Furthermore, it is unclear if suffi-cient interannotator agreement can be obtained tosupport meaningful evaluation.Ultimately, we devised two separate evaluationsto assess the quality of our system output basedon the techniques discussed above.
The first isa manual evaluation focused on the cluster labels(i.e., drug categories), based on a factoid QA eval-uation methodology.
The second is an automaticevaluation of the retrieved abstracts using ROUGE,drawing elements from summarization evaluation.Details of the evaluation setup and results are pre-ceded by a description of the test collection wecreated from CE.5.2 Test CollectionWe were able to mine the June 2004 edition ofClinical Evidence to create a test collection forsystem evaluation.
We randomly selected thirtydiseases, generating a development set of fivequestions and a test set of twenty-five questions.Some examples include: acute asthma, chronicprostatitis, community acquired pneumonia, anderectile dysfunction.
CE listed an average of 11.3interventions per disease; of those, 2.3 on averagewere marked as beneficial and 1.9 as likely benefi-cial.
On average, there were 48.4 references asso-ciated with each disease, representing the articlesconsulted during the compilation of CE itself.
Ofthose, 34.7 citations on average appeared in MED-LINE; we gathered all these abstracts, which serveas the reference summaries for our ROUGE-basedautomatic evaluation.Since the focus of our work is not on retrieval al-gorithms per se, we employed PubMed to fetch aninitial set of MEDLINE citations and performedanswer synthesis using those results.
The PubMedcitations also serve as a baseline, since it repre-sents a system commonly used by physicians.In order to obtain the best possible set of ci-tations, the first author (an experienced PubMedsearcher), manually formulated queries, takingadvantage of MeSH (Medical Subject Headings)terms when available.
MeSH terms are controlledvocabulary concepts assigned manually by trainedmedical indexers (based on the full text of the ar-ticles), and encode a substantial amount of knowl-edge about the contents of the citation.
PubMedallows searches on MeSH terms, which usuallyyield accurate results.
In addition, we limited re-trieved citations to those that have theMeSH head-ing ?drug therapy?
and those that describe a clin-ical trial (another metadata field).
Finally, we re-stricted the date range of the queries so that ab-stracts published after our version of CE were ex-cluded.
Although the query formulation processcurrently requires a human, we envision automat-ing this step using a template-based approach inthe future.6 System EvaluationWe adapted existing techniques to evaluate oursystem in two separate ways: a factoid-style man-ual evaluation focused on short answers and anautomatic evaluation with ROUGE using CE-citedabstracts as the reference summaries.
The setupand results for both are detailed below.6.1 Manual Evaluation of Short AnswersIn our manual evaluation, system outputs were as-sessed as if they were answers to factoid ques-tions.
We gathered three different sets of answers.For the baseline, we used the main interventionfrom each of the first three PubMed citations.
Forour test condition, we considered the three largestclusters, taking the main intervention from the firstabstract in each cluster.
This yields three drugsthat are at the same level of ontological granularityas those extracted from the unclustered PubMedcitations.
For our third condition, we assumed theexistence of an oracle which selects the three bestclusters (as determined by the first author, a med-ical doctor).
From each of these three clusters,we extracted the main intervention of the first ab-stracts.
This oracle condition represents an achiev-able upper bound with a human in the loop.
Physi-cians are highly-trained professionals that alreadyhave significant domain knowledge.
Faced with asmall number of choices, it is likely that they willbe able to select the most promising cluster, evenif they did not previously know it.This preparation yielded up to nine drug names,three from each experimental condition.
For short,we refer to these as PubMed, Cluster, and Oracle,respectively.
After blinding the source of the drugsand removing duplicates, each short answer waspresented to the first author for evaluation.
Since845Clinical Evidence PhysicianB LB T U UB H N Good Okay BadPubMed 0.200 0.213 0.160 0.053 0.000 0.013 0.360 0.600 0.227 0.173Cluster 0.387 0.173 0.173 0.027 0.000 0.000 0.240 0.827 0.133 0.040Oracle 0.400 0.200 0.133 0.093 0.013 0.000 0.160 0.893 0.093 0.013Table 2: Manual evaluation of short answers: distribution of system answers with respect to CE cat-egories (left side) and with respect to the assessor?s own expertise (right side).
(Key: B=beneficial,LB=likely beneficial, T=tradeoffs, U=unknown, UB=unlikely beneficial, H=harmful, N=not in CE)the assessor had no idea from which condition ananswer came, this process guarded against asses-sor bias.Each answer was evaluated in two differentways: first, with respect to the ground truth in CE,and second, using the assessor?s own medical ex-pertise.
In the first set of judgments, the asses-sor determined which of the six categories (ben-eficial, likely beneficial, tradeoffs, unknown, un-likely beneficial, harmful) the system answer be-longed to, based on the CE recommendations.
Aswe have discussed previously, a human (with suf-ficient domain knowledge) is required to performthis matching due to synonymy and differences inontological granularity.
However, note that the as-sessor only considered the drug name when mak-ing this categorization.
In the second set of judg-ments, the assessor separately determined if theshort answer was ?good?, ?okay?
(marginal), or?bad?
based both on CE and her own experience,taking into account the abstract title and the top-scoring outcome sentence (and if necessary, theentire abstract text).Results of this manual evaluation are presentedin Table 2, which shows the distribution of judg-ments for the three experimental conditions.
Forbaseline PubMed, 20% of the examined drugs fellin the beneficial category; the values are 39% forthe Cluster condition and 40% for the Oracle con-dition.
In terms of short answers, our systemreturns approximately twice as many beneficialdrugs as the baseline, a marked increase in answeraccuracy.
Note that a large fraction of the drugsevaluated were not found in CE at all, which pro-vides an estimate of its coverage.
In terms of theassessor?s own judgments, 60% of PubMed shortanswers were found to be ?good?, compared to83% and 89% for the Cluster and Oracle condi-tions, respectively.
From a factoid QA point ofview, we can conclude that our system outper-forms the PubMed baseline.6.2 Automatic Evaluation of AbstractsA major limitation of the factoid-based evaluationmethodology is that it does not measure the qual-ity of the abstracts from which the short answerswere extracted.
Since we lacked the necessaryresources to manually gather abstract-level judg-ments for evaluation, we sought an alternative.Fortunately, CE can be leveraged to assess the?goodness?
of abstracts automatically.
We assumethat references cited in CE are examples of highquality abstracts, since they were used in gener-ating the drug recommendations.
Following stan-dard assumptions made in summarization evalu-ation, we considered abstracts that are similar incontent with these ?reference abstracts?
to also be?good?
(i.e., relevant).
Similarity in content canbe quantified with ROUGE.Since physicians demand high precision, we as-sess the cumulative relevance after the first, sec-ond, and third abstract that the clinician is likelyto have examined (where the relevance for eachindividual abstract is given by its ROUGE-1 pre-cision score).
For the baseline PubMed condition,the examined abstracts simply correspond to thefirst three hits in the result set.
For our test system,we developed three different orderings.
The first,which we term cluster round-robin, selects the firstabstract from the top three clusters (by size).
Thesecond, which we term oracle cluster order, selectsthree abstracts from the best cluster, assuming theexistence of an oracle that informs the system.
Thethird, which we term oracle round-robin, selectsthe first abstract from each of the three best clus-ters (also determined by an oracle).Results of this evaluation are shown in Table 3.The columns show the cumulative relevance (i.e.,ROUGE score) after examining the first, second,and third abstract, under the different orderingconditions.
To determine statistical significance,we applied the Wilcoxon signed-rank test, the846Rank 1 Rank 2 Rank 3PubMed Ranked List 0.170 0.349 0.523Cluster Round-Robin 0.181 (+6.3%)?
0.356 (+2.1%)?
0.526 (+0.5%)?Oracle Cluster Order 0.206 (+21.5%)M 0.392 (+12.6%)M 0.597 (+14.0%)NOracle Round-Robin 0.206 (+21.5%)M 0.396 (+13.6%)M 0.586 (+11.9%)NTable 3: Cumulative relevance after examining the first, second, and third abstracts, according to differentorderings.
(?
denotes n.s., M denotes sig.
at 0.90, N denotes sig.
at 0.95)standard non-parametric test for applications ofthis type.
Due to the relatively small test set (only25 questions), the increase in cumulative relevanceexhibited by the cluster round-robin condition isnot statistically significant.
However, differencesfor the oracle conditions were significant.7 Discussion and Related WorkAccording to two separate evaluations, it appearsthat our system outperforms the PubMed baseline.However, our approach provides more advantagesover a linear result set that are not highlighted inthese evaluations.
Although difficult to quantify,categorized results provide an overview of the in-formation landscape that is difficult to acquire bysimply browsing a ranked list?user studies of cat-egorized search have affirmed its value (Hearstand Pedersen, 1996; Dumais et al, 2001).
Onemain advantage we see in our application is bet-ter ?redundancy management?.
With a ranked list,the physician may be forced to browse throughmultiple redundant abstracts that discuss the sameor similar drugs to get a sense of the differenttreatment options.
With our cluster-based ap-proach, however, potentially redundant informa-tion is grouped together, since interventions dis-cussed in a particular cluster are ontologically re-lated through UMLS.
The physician can examinedifferent clusters for a broad overview, or perusemultiple abstracts within a cluster for a more thor-ough review of the evidence.
Our cluster-basedsystem is able to support both types of behaviors.This work demonstrates the value of semanticresources in the question answering process, sinceour approach makes extensive use of the UMLSontology in all phases of answer synthesis.
Thecoverage of individual drugs, as well as the rela-tionship between different types of drugs withinUMLS enables both answer extraction and seman-tic clustering.
As detailed in (Demner-Fushmanand Lin, 2006 in press), UMLS-based features arealso critical in the identification of clinical out-comes, on which our extractive summaries arebased.
As a point of comparison, we also im-plemented a purely term-based approach to clus-tering PubMed citations.
The results are so inco-herent that a formal evaluation would prove to bemeaningless.
Semantic relations between drugs,as captured in UMLS, provide an effective methodfor organizing results?these relations cannot becaptured by keyword content alone.
Furthermore,term-based approaches suffer from the cluster la-beling problem: it is difficult to automatically gen-erate a short heading that describes cluster content.Nevertheless, there are a number of assump-tions behind our work that are worth pointingout.
First, we assume a high quality initial re-sult set.
Since the class of questions we examinetranslates naturally into accurate PubMed queriesthat can make full use of human-assigned MeSHterms, the overall quality of the initial citationscan be assured.
Related work in retrieval algo-rithms (Demner-Fushman and Lin, 2006 in press)shows that accurate relevance scoring of MED-LINE citations in response to more general clin-ical questions is possible.Second, our system does not actually performsemantic processing to determine the efficacy of adrug: it only recognizes ?topics?
and outcome sen-tences that state clinical findings.
Since the sys-tem by default orders the clusters based on size, itimplicitly equates ?most popular drug?
with ?bestdrug?.
Although this assumption is false, we haveobserved in practice that more-studied drugs aremore likely to be beneficial.In contrast with the genomics domain, whichhas received much attention from both the IR andNLP communities, retrieval systems for the clin-ical domain represent an underexplored area ofresearch.
Although individual components thatattempt to operationalize principles of evidence-based medicine do exist (Mendonc?a and Cimino,2001; Niu and Hirst, 2004), complete end?to?end clinical question answering systems are dif-847ficult to find.
Within the context of the PERSI-VAL project (McKeown et al, 2003), researchersat Columbia have developed a system that lever-ages patient records to rerank search results.
Sincethe focus is on personalized summaries, this workcan be viewed as complementary to our own.8 ConclusionThe primary contribution of this work is the de-velopment of a clinical question answering systemthat caters to the unique requirements of physi-cians, who demand both conciseness and com-pleteness.
These competing factors can be bal-anced in a system?s response by providing mul-tiple levels of drill-down that allow the informa-tion space to be viewed at different levels of gran-ularity.
We have chosen to implement these capa-bilities through answer extraction, semantic clus-tering, and extractive summarization.
Two sepa-rate evaluations demonstrate that our system out-performs the PubMed baseline, illustrating the ef-fectiveness of a hybrid approach that leverages se-mantic resources.9 AcknowledgmentsThis work was supported in part by the U.S. Na-tional Library of Medicine.
The second authorthanks Esther and Kiri for their loving support.ReferencesE.
Amigo?, J. Gonzalo, V. Peinado, A.
Pen?as, andF.
Verdejo.
2004.
An empirical study of informa-tion synthesis task.
In ACL 2004.A.
Aronson.
2001.
Effective mapping of biomedi-cal text to the UMLS Metathesaurus: The MetaMapprogram.
In AMIA 2001.M.
Chambliss and J. Conley.
1996.
Answering clinicalquestions.
The Journal of Family Practice, 43:140?144.K.
Cogdill and M. Moore.
1997.
First-year medi-cal students?
information needs and resource selec-tion: Responses to a clinical scenario.
Bulletin ofthe Medical Library Association, 85(1):51?54.D.
Covell, G. Uman, and P. Manning.
1985.
Informa-tion needs in office practice: Are they being met?Annals of Internal Medicine, 103(4):596?599.H.
Dang.
2005.
Overview of DUC 2005.
In DUC2005 Workshop at HLT/EMNLP 2005.S.
De Groote and J. Dorsch.
2003.
Measuring usepatterns of online journals and databases.
Journal ofthe Medical Library Association, 91(2):231?240.D.
Demner-Fushman and J. Lin.
2005.
Knowledge ex-traction for clinical question answering: Preliminaryresults.
In AAAI 2005 Workshop on QA in RestrictedDomains.D.
Demner-Fushman and J. Lin.
2006, in press.
An-swering clinical questions with knowledge-basedand statistical techniques.
Comp.
Ling.S.
Dumais, E. Cutrell, and H. Chen.
2001.
Optimizingsearch by showing results in context.
In CHI 2001.J.
Ely, J. Osheroff, M. Ebell, G. Bergus, B. Levy,M.
Chambliss, and E. Evans.
1999.
Analysis ofquestions asked by family doctors regarding patientcare.
BMJ, 319:358?361.P.
Gorman, J. Ash, and L. Wykoff.
1994.
Can pri-mary care physicians?
questions be answered usingthe medical journal literature?
Bulletin of the Medi-cal Library Association, 82(2):140?146, April.S.
Hauser, D. Demner-Fushman, G. Ford, andG.
Thoma.
2004.
PubMed on Tap: Discoveringdesign principles for online information delivery tohandheld computers.
In MEDINFO 2004.M.
Hearst and J. Pedersen.
1996.
Reexaming the clus-ter hypothesis: Scatter/gather on retrieval results.
InSIGIR 1996.D.
Lawrie and W. Croft.
2003.
Generating hierarchicalsummaries for Web searches.
In SIGIR 2003.J.
Lin.
2005.
Evaluation of resources for question an-swering evaluation.
In SIGIR 2005.D.
Lindberg, B. Humphreys, and A. McCray.
1993.The Unified Medical Language System.
Methods ofInformation in Medicine, 32(4):281?291.K.
McKeown, N. Elhadad, and V. Hatzivassiloglou.2003.
Leveraging a common representation for per-sonalized search and summarization in a medicaldigital library.
In JCDL 2003.E.
Mendonc?a and J. Cimino.
2001.
Building a knowl-edge base to support a digital library.
In MEDINFO2001.Y.
Niu and G. Hirst.
2004.
Analysis of semanticclasses in medical text for question answering.
InACL 2004 Workshop on QA in Restricted Domains.David Sackett, Sharon Straus, W. Richardson, WilliamRosenberg, and R. Haynes.
2000.
Evidence-Based Medicine: How to Practice and Teach EBM.Churchill Livingstone, second edition.E.
Voorhees.
2005.
Using question series to eval-uate question answering system effectiveness.
InHLT/EMNLP 2005.Y.
Zhao and G. Karypis.
2002.
Evaluation of hierar-chical clustering algorithms for document datasets.In CIKM 2002.848
