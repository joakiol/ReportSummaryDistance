Coling 2010: Poster Volume, pages 1336?1344,Beijing, August 2010Exploring the Use of Word Relation Featuresfor Sentiment ClassificationRui Xia and Chengqing ZongNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences{rxia, cqzong}@nlpr.ia.ac.cnAbstractWord relation features, which encoderelation information between words, aresupposed to be effective features forsentiment classification.
However, theuse of word relation features suffersfrom two issues.
One is the sparse-dataproblem and the lack of generalizationperformance; the other is the limitationof using word relations as additionalfeatures to unigrams.
To address the twoissues, we propose a generalized wordrelation feature extraction method andan ensemble model to efficiently inte-grate unigrams and different type ofword relation features.
Furthermore,aimed at reducing the computationcomplexity, we propose two fast featureselection methods that are specially de-signed for word relation features.
Arange of experiments are conducted toevaluate the effectiveness and efficiencyof our approaches.1 IntroductionThe task of text sentiment classification has be-come a hotspot in the field of natural languageprocessing in recent years (Pang and Lee, 2008).The dominating text representation method insentiment classification is known as the bag-of-words (BOW) model.
Although BOW is quitesimple and efficient, a great deal of the informa-tion from original text is discarded, word orderis disrupted and syntactic structures are broken.Therefore, more sophisticated features with adeeper understanding of the text are required forsentiment classification tasks.With the attempt to capture the word relationinformation behind the text, word relation (WR)features, such as higher-order n-grams and worddependency relations, have been employed intext representation for sentiment classification(Dave et al, 2003; Gamon, 2004; Joshi andPenstein-Ros?, 2009).However, in most of the literature, the per-formance of individual WR feature set was poor,even inferior to the traditional unigrams.
Forthis reason, WR features were commonly usedas additional features to supplement unigrams,to encode more word order and word relationinformation.
Even so, the performance of jointfeatures was still far from satisfactory (Dave etal., 2003; Gamon, 2004; Joshi and Penstein-Ros?, 2009).We speculate that the poor performance ispossibly due to the following two reasons: 1) inWR features, the data are sparse and the fea-tures lack generalization capability; 2) the useof joint features of unigrams and WR featureshas its limitation.On one hand, there were attempts at findingbetter generalized WR (GWR) features.
Gamon(2004) back off words in n-grams (and semanticrelations) to their respective POS tags (e.g.,great-movie to adjective-noun); Joshi and Ros?
(2009) propose a method by only backing offthe head word in dependency relation pairs to itsPOS tag (e.g., great-movie to great-noun),which are supposed to be more generalized thanword pairs.
Based on Joshi and Ros?
?s method,we back off the word in each word relation pairsto its corresponding POS cluster, making thefeature space smarter and more effective.On the other hand, we find that from uni-grams to WR features, relevance between fea-tures is reduced and the independence is in-1336creased.
Although the discriminative model(e.g., SVM) is proven to be more effective onunigrams (Pang et al, 2002) for its ability ofcapturing the complexity of more relevant fea-tures, WR features are more inclined to workbetter in the generative model (e.g., NB) sincethe feature independence assumption holds wellin this case.Based on this finding, we therefore intuitivelyseek, instead of jointly using unigrams andGWR features, to efficiently integrate them tosynthesize a more accurate classification proce-dure.
We use the ensemble model to fuse differ-ent types of features under distinct classificationmodels, with an attempt to overcome individualdrawbacks and benefit from each other?s merit,and finally to enhance the overall performance.Furthermore, feature reduction is another im-portant issue of using WR features.
Due to thehuge dimension of WR feature space, traditionalfeature selection methods in text classificationperform inefficiently.
However, to our knowl-edge, no related work has focused on featureselection specially designed for WR features.Taking this point into consideration, we pro-pose two fast feature selection methods (FMIand FIG) for GWR features with a theoreticalproof.
FMI and FIG regard the importance of aGWR feature as two component parts, and takethe sum of two scores as the final score.
FMIand FIG remain a close approximation to MIand IG, but speed up the computation by at most10 times.
Finally, we apply FMI and FIG to theensemble model, reducing the computationcomplexity to a great extent.The remainder of this paper is organized asfollows.
In Section 2, we introduce the approachto extracting GWR features.
In Section 3, wepresent the ensemble model for integrating dif-ferent types of features.
In Section 4, the fastfeature selection methods for WR features areproposed.
Experimental results are reported inSection 5.
Section 6 draws conclusions and out-lines directions for future work.2 Generalized Word Relation FeaturesA straightforward method for extracting WRfeatures is to simply map word pairs into thefeature vector.
However, due to the sparse-dataproblem and the lack of generalization ability,the performance of WR is discounted.
Considerthe following two pieces of text:1) Avatar is a great movie.
I definitely rec-ommend it.2) I definitely recommend this book.
It is great.We lay the emphasis on the following wordpairs: great-movie, great-it, it-recommend, andbook-recommend.
Although these features aregood indicators of sentiment, due to the sparse-data problem, they may not contribute as impor-tantly as we have expected in machine learningalgorithms.
Moreover, the effects of those fea-tures would be greatly reduced when they arenot captured in the test dataset (for example, anew feature great-song in the test set wouldnever benefit from great-movie and great-it).Joshi and Ros?
(2009) back off the head wordin each of the relation pairs to its POS tag.
Tak-ing great-movie for example, the back-off fea-ture will be great-noun.
With such a transforma-tion, original features like great-movie, great-book and other great-noun pairs are regarded asone feature, hence, the learning algorithmscould learn a weight for a more general featurethat has stronger evidence of association withthe class, and any new test sentence that con-tains an unseen noun in a similar relationshipwith the adjective great (e.g., great-song) willreceive some weight in favor of the class label.With the attempt to make a further generali-zation, we conduct a POS clustering.
Consider-ing the effect of different POS tags in both uni-grams and word relations, the POS tags arecategorized as shown in Table 1.POS-cluster Contained POS tagsJ JJ, JJS, JJRR RB, RBS, RBRV VB, VBZ, VBD, VBN, VBG, VBPN NN, NNS, NNP, NNPS, PRPO The other POS tagsTable 1: POS Clustering (the Penn Corpus Style)Since adjectives and adverbs have the highestcorrelation with sentiment, and some verbs andnouns are also strong indicators of sentiment,we therefore put them into separate clusters.
Allthe other tags are categorized to one cluster be-cause they contain a lot of noise rather than use-ful information.
In addition, we assign pronounsto POS-cluster N, aimed at capturing the gener-ality in WR features like great-movie and great-it, or book-recommend and it-recommend.1337Taking ?Avatar is a great movie?
for example,different types of WR features are presented inTable 2, where Uni denotes unigrams; WR-Biindicates traditional bigrams; WR-Dp indicatesword pairs of dependency relation; GWR-Biand GWR-Dp respectively denote generalizedbigrams and dependency relations.WR types WR featuresWR-Bi Avatar-is, is-a, a-great, great-movieWR-DpAvatar-is, a-movie, great-movie,movie-isGWR-BiAvatar-V, is-O, a-J, great-N,N-is, V-a, O-great, J-movieGWR-DpAvatar-V, a-N, great-N, movie-V,N-is, O-movie, J-movieTable 2: Different types of WR features3 An Ensemble Model for IntegratingWR Features3.1 Joint Features, Good Enough?Although the unigram feature space is simple,and the WR features are more sophisticated, thelatter was mostly used as extra features in addi-tion to the former, rather than to substitute it.Even so, in most of the literature, the improve-ments of joint features are still not as good aswe had expected.
For example, Dave et al(2003) try to extract a refined subset of WRpairs (adjective-noun, subject-verb, and verb-object pairs) as additional features to traditionalunigrams, but do not get significant improve-ments.
In the experiments of Joshi and Ros?
(2009), the improvements of unigrams togetherwith WR features (even generalized WRfeatures) are also not remarkable (sometimeseven worse) compared to simple unigrams.One possible explanation might be that dif-ferent types of features have distinct distribu-tions, and therefore would probably yield varyperformance on different machine learning al-gorithms.
For example, the generative model isoptimal if the distribution is well estimated;otherwise the performance will drop signifi-cantly (for instance, NB performs poorly unlessthe feature independence assumption holdswell).
While on the contrary, the discriminativemodel such as SVM is good at representing thecomplexity of relevant features.Let us review the results reported by Pangand Lee (2002) that compare different classifi-cation algorithms: SVM performs significantlybetter than NB on unigrams; while the outcomeis the opposite on bigrams.
It is possibly due tothat from unigrams to bigrams, the relevancebetween features is reduced (bigrams coversome relevance of unigram pairs), and the inde-pendence between features increases.Since GWR features are less relevant andmore independent in comparison, it is reason-able for us to infer that these features wouldwork better on NB than on SVM.
We thereforeintuitively seek to employ the ensemble modelfor sentiment classification tasks, with an at-tempt to efficiently integrate different types offeatures under distinct classification models.3.2 Model FormulationThe ensemble model (Kittler, 1998), whichcombines the outputs of several base classifiersto form an integrated output, has become aneffective classification method for many do-mains.For our ensemble task, we train six base clas-sifiers (the NB and SVM model respectively onthe Uni, GWR-Bi and GWR-Dp features).
Bymapping the probabilistic outputs (for C  classes)of D base classifiers into the meta-vector11 1 1?
[ , , , , , , ],C kj D DCo o o o ox !
!
!
!
(1)the weighted ensemble is formulized by1 1?
?
( ) ,D Dj j k kj k kk kO g o xX X q?
?x D j  (2)where  is the weight assigned to the -thssmization, we use descentdefined askX kbase cla ifier.3.3 Weight OptiInspired by linear regressionmethods to seek optimization according to cer-tain criteria.
We employ two criteria, namelythe perceptron criterion and the minimum clas-sification error (MCE) criterion.The perceptron cost function is1, ,11 N?
?max ( ) ( ) .ip j i y ij CiJ g gN?
?
?
??
??
x x!
(3)The minimization ofpJ is approximately equalsc1992)isfunction is given byto seek a minimum mi lassification rate.The MCE criterion (Juang and Katagiri,supposed to be more relevant to the classifica-tion error.
A short version of MCE criterion13381 11?
?
( ) ( ( ) max ( ))N Cmce ii jJ I y j g gNE ??
x x  (4) j kk jvwhere  is the sigmoid function.For both criteria, stochastic gradient descent.
SGD uses( )E <(SGD) is utilized for optimizationapproximate gradients estimated from subsets ofthe training data and updates the parameters inan online manner:( 1) ( ) ( )h hhJk k kX X I s   .
(5)Xsfunctions are respectivelyThe gradients of perceptron and MCE costp11?
?
( )NhihJx xNX qs s ?
(6) i iD s h D yqwherei, and1, ,?arg max ( )i jj Cs gx!MC ( ))y iJs E11?
?
?
?
( )(1 ( )i i i iNy i h D s h D yihl l x xNX q  q  s ?
x x  (7)where x?
andAs for perceptron criterion, we employ theaverage perceptron (AvgP) (Freund andScIn the past decade, feature selection (FS) studiesn.pose afast feature selection method that is speciallydesigned for GWR features.
In our method, there  (e.g., great-?
?
( ) ( ( ) max ( ))ij i y i k ih jl g gEv x x, ;?arg max ( )ii j iC j ys gvx!.1,jhapire, 1999), a variation of perceptron modelthat averages the weights of all iteration loops,to improve the generalization performance.4 Feature Selection for WR Featuresmainly focus on topical text classificatio(Yang and Pedersen, 1997) investigate five FSmetrics and reported that good FS methods(such as IG and CHI) can improve the categori-zation accuracy with an aggressive feature re-moval.
In sentiment classification tasks, tradi-tional FS methods were also proven to be effec-tive (Ng et al, 2006; Li et al, 2009).With regard to WR features, since the dimen-sion of feature space has sharply increased, theamount of computation is considerably largewhen employing traditional FS methods.4.1 Fast MI and Fast IGIn order to address this problem, we proimportance of a GWR featu wsmovie) is considered as two component parts:the non-back-off word w  (great) and the POSpairs s  (J-N).
We calculate the score of w  ands  respectively using existing FS methods, andtake the sum of them as the final score.
By as-suming the two parts are mutually independent,the im ortance of a relation feature can be takenseparately.
We now give a theoretical support.First, the mutual information between a rela-tion feature ws  and classkc  is defined asp( , )( , ) log .
( ) ( )kkkP ws cI ws cP ws P c(8)If w  and s  are independent, they are condi-tionally inde ndent.
Thus e have pe w( | )( , ) log( )( | ) ( | )log( ) ( )k kP w c P s cP w P sx( | ) ( | )log log( ) ( )( , ) ( , ).kkk kk kP ws cI ws cP wsP w c P s cP w P sI w c I s c(9)ula (9) indicates that under the assumtion that two component parts  andForm p-w s  of arelation feature  are mutuallythe mutual inwsformation of the relation featureindependent,( , )kI ws c  equals the sum of two componentparts ( , )kI w c  and ( , )kI s c .Since the aver  mutual information acrossall classes ( )ageI ws  is the probabilistic sum ofss, it can be written as: each cla.
( ) ( ) ( )I ws I w I sx   (10)d Peweighted average ofYang an dersen (1997) show that the in-formation gain ( )G t  is the( , )kI t c  and ( , )kI t ccan cons.
Therefore, with the saider the inforulat IG (FIG) respectively.
Now letof the independ-ence assumption.
In fact in a reltwmemation gain of reason, wea relation feature ( )G ws  as the sum of two com-ponent parts:( ) ( )G w G sx   (11)We refer to Form (10) and (11) as fast MI(FMI) and fasus look back at the rationality( )G wsation feature,o component parts are hardly independentsince they are ?related?.
Nonetheless, if we con-1339sider a GWR feature as a combination of thenon-back-off word and the POS pairs, the as-sumption will be easier to satisfy.
Taking great-movie (great-N) for example, compared to greatand N, great and J-N are more independent (J-Ncovers some relation information), therefore it ismore feasible to take ( ) (J-N)G great G  as anapproximation of ( -N)G great .Laying aside the assumption, we place em-phasis on the advantage of FIG (FMI) in com-putational efficiency.
A ensionof the unigrams feature spssuming the dimace is , and ignor-inescribed in section 3.2.
In--onensethe effective-lection., 2004) is used inment-level polaritpositive and 1,000NNg the data-sparse problem, the dimension ofthe GWR feature space is 2 5 Nq q  (backing offhead/modifier word to 5 POS-cluster).
Tradi-tional IG (MI) feature selection needs to calcu-late the score of all 10 Nq  features, while FIG(FMI) only needs to comp  words and25 POS pairs.
That is to say, FIG (FMI) canspeed up the computation of traditional IG (MI)by at most 10 times.4.2 Integration with the Ensemble ModelWe now present how FMI (FIG) is applied tothe ensemble model dute foreach of the six base-classifiers described in Section 3.2, feature selection is performed (traditional IG on unigrams, FIG on GWR features).Note that when performing FIG on individualGWR feature sets, the computation of non-back-off word ( )G w , is taken care of by havingalready computed IG on unigrams.
Thus, wely need to compute the score of 25 POS pairs.From this point of view, FIG (FMI) is quitesuitable for the mble model.5 ExperimentsWe first present the performance of system per-formance, and then demonstrateness of fast feature se5.1 Experimental SetupDatasets: The Cornell movie-review dataset 1introduced by (Pang and Leeour experiments.
It is a docudataset that contains 1,000ynegative processed reviews.1 http://www.cs.cornell.edu/people/pabo/movie-review-data/We also use the dataset2 introduced in (Joshiand Penstein-Ros?, 2009) for comparison.
It is asut and the E-product dataset isatl (McCallumts ared ?enko, 2004) is employed.Tae accuracy.gan-per-bset (200 sentences each for 11 differentproducts) of the product review dataset releasedby (Hu and Liu, 2004).
We will refer to it E-product dataset.The Movie dataset is a domain-specific docu-ment-level datasesentence-level and cross-domain.
We conductexperiments on both of them to evaluate ourapproach in a wide range of tasks.Classifier: We implement the NB classifierbased on a multinomial event modeand Nigam, 1998) with Laplace smoothing.
Thetool LIBSVM3 is chosen as the SVM classifier.Setting of kernel function is linear kernel, thepenalty parameter is set to one, and the Platt?sprobabilistic output for SVM is applied to ap-proximate the posterior probabilities.
Termpresence is used as the feature weighting.Implementation: The Movie dataset is evenlydivided into 5 folds, and all the experimenconducted with a 5-fold cross validation.
Fol-lowing the settings by Joshi and Ros?, an 11-fold cross validation is applied to E-productdataset, where each test fold contains all thesentences for one of the 11 products, and thesentences for the remaining 10 products areused for training.For ensemble learning, the stacking frame-work (D?eroski anking the Movie dataset for example, in eachloop of the 5-fold cross validation, the probabil-istic outputs of the test fold are considered astest samples for ensemble leaning; and an inner4-fold leave-one-out procedure is applied to thetraining data, where samples in each fold aretrained on the remaining three folds to obtainthe probabilistic outputs which serve as trainingsamples for ensemble learning.All the performance in the remaining tablesand figures is in terms of averag5.2 Results of Classification AccuracyThe results of classification accuracy are orized in three parts.
We first compare theformance of individual WR and GWR; secondlywe compare joint features and the ensemble2 http://www.cs.cmu.edu/~maheshj/datasets/acl09short.html3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/1340model; thirdly we compare different ensemblestrategies; finally we make a comparison withsome related work.5.2.1 WR vs. GWRTable 3 presents the refeature sets.
Four typessults of individual WRof WR features, includ-ing WR-Bi, WR-Dp, GWR-Bi and GWR-Dp,are examined under two classification modelson two datasets.
For each of the results, we re-port the best accuracy under feature selection.Model WR Feature Movie E-productWR-Bi 83.05 63.27GWR-Bi 85.55 65.17WR-Dp 82.15 65.14SVMGWR-Dp 83.40 67.09WR-Bi 84.60 66.86GWR-Bi 85.45 67.50WR-Dp 83.90 65.68NBGWR-Dp 83.65 67.41Table 3: Acc ) of I al WR eSetsformance of individua R and WR.
With theSVuracies (% ndividu  FeaturAt first, we place the emphasis on the per-l GWM model, the performance of GWR featuresis remarkable compared to traditional WR pairs.Specifically, on the Movie dataset, GWR-Bioutperforms WR-Bi by 2.50%, and GWR-Dpoutperforms WR-Dp by 1.35%; on the E-product dataset, the improvements are 1.90%and 1.95%.
Under the NB model, on the Moviedataset, GWR-Bi outperforms WR-Bi by 0.85%;on the E-product dataset, GWR-Bi outperformsWR-Bi by 0.64% and GWR-Dp outperformsWR-Dp by 1.73%.
One exception is GWR-Dpon the Movie dataset, but the decline is slight(0.25%).WR Feature Movie E-productWR-Bi 386k 21kGWR-Bi 152k 16kWR-Dp 455k 24kGWR-Dp 151k 16kTable ion of ual Fe Spacefe -agntins4: Dimen  Individ atureSecondly, we compare the dimensions of dif-rent feature space.
Table 4 presents the avere size of different types of feature spaces ontwo datasets.
On the Movie dataset, the size ofGWR feature space has been significantly re-duced (386k vs. 152k in Bi; 455k vs. 151k inDp).
On the E-product dataset, since the trainingset are made up by 10 different domains, dataare quite sparse, therefore, the extent of dimen-sion reduction is not as sound as that on Moviedataset, but still considerable (21k vs. 16k in Bi;24k vs. 16k in Dp).5.2.2 Joint Features vs. Ensemble ModelThe performance of individual feature sets, joifeature set and ensemble model is reportedTable 5.
Uni, GWR-Bi and GWR-Dp are usedas individual features sets in the ensemblemodel, and Joint Features denote the union ofthree individual sets.
For feature selection, IG isused in Joint Features, and FIG is used in theensemble model.
The reported results are interms of the best accuracy under feature selec-tion.Feature and Model Movie E-productSVM 85.20 67.77UniNB 84.10 66.18SVM 85.55 65.17GWR-BiNB 85.45 67.50SVM 83.40 67.09GWR-DpNB 83.65 67.41SVM 86.10 66.55Joint FeaturesNB 85.20 67.64AvgP 88.60 70.14EMnsemble ModelCE 88.55 70.18Table 5: Accuracies  Co t FeJoint Feature nsem odel-vidual emon-strels on different featurese(%) of mponen atures,s and E ble MTo begin with, we observe the results of indifeature sets.
Although we have dated that GWR features are more effectivethan WR, it is a pity that they do not show sig-nificant superiority (sometimes even worse)compared to unigrams.
That is to say, althoughGWR features encode more generalized wordrelation information than WR features, the roleof unigrams still can not be replaced.
This is inaccordance with that, WR (GWR) features areused as additional features to assist unigrams inmost of the literature.Secondly, we focus on the performance oftwo classification modts.
SVM seems to work better than NB onunigrams (more than 1%); while on GWR-Biand GWR-Dp feature sets, NB tends to be over-all effective.
This has confirmed our speculationthat WR features perform better under NB thanunder SVM (since independence between fea-tures increases) and strengthened the confidence1341of our motivation to ensemble different types offeatures under distinct classification models.Finally, we make a comparison of Joint Fea-tures and Ensemble model.
Observing the re-suhe result of Joint Features is evenwifferentlts on the Movie dataset, Joint Features ex-ceed individual feature sets, but the improve-ments are not remarkable (less than 1 percent-age compared to the best individual score).While the results of the ensemble model, as wehave expected, are fairly good.
AvgP and MCErespectively get the scores of 0.886 and 0.8855,robustly higher than that of Joint Features(0.8610 and 0.8520 respectively under SVMand NB).On the E-product dataset, it is quite surpris-ing that torse than some of the individual features sets.This also confirms that Joint Features are some-times not so effective at exploring differenttypes of features.
With regard to the ensemblemodel, AvgP gets an accuracy of 0.7014 andMCE achieves the best score (0.7018), consis-tently superior to the results of Joint Features.5.2.3 Different Ensemble StrategiesWe also examine the performance of dstrategies.
In Table 6, three ensemble strategiesare compared, where  ?
(Uni & Bi & Dp ) @SVM?
denotes ensemble of three kinds of fea-ture sets with the fixed SVM classifier,  ?Uni @(NB & SVM)?
denotes ensemble of two classi-fiers on fixed unigram features, and ?
(Uni & Bi& Dp ) @ (NB & SVM)?
denotes ensemble ofboth classifiers and feature sets.Ensemble Strategy Movie E-productAveP 86.60 69.50 (Uni & Bi & Dp )E@ SVM MC 86.60 69.59AveP 87.75 68.95 Uni@ ( M) NB & SV MCE 87.80 69.14AveP 88.60 70.14 (Uni &  Dp )@ (NB & SVM)Bi &MCE 88.55 70.18Table 6: Accuracies  Di EnsStrategies.fensemble of either s or classifiers isroramsoviee-lint result (0.679) on joint features ofunfor GWRof MI and-se(%) of fferent embleSeen from Table 5 and 6, the performance ofeature setbustly better than any individual classifier, aswell as the joint features on both datasets.
Withregard to ensemble of both feature sets and clas-sification algorithms, it is the most effectivecompared to the above two ensemble strategies.This is in accordance with our motivation de-scribed in Section 3.1.5.2.4 Comparison with Related WorkWe take the performance of SVM on unigas the baseline for comparison.
On the Mdataset, Pang and Lee (2004) and Ng et al(2006) reported the baseline accuracy of 0.871.But our baseline is 2 percentages lower (0.852).It is mainly due to that: 1) 0.871 was obtainedby a 10-fold cross validation, and our result isget by 5-fold cross validation; 2) the result ofthe tool LibSVM is inferior of SVMlight by al-most 1-2 percentages, since the penalty parame-ter in LibSVM is fixed, while in SVMlight, thevalue is automatically adapted; 3) the baselinein Ng et al (2006) is obtained with length nor-malization which play a role in performance.Ng et al reported the state of art best per-formance (0.905), which outperforms the base (0.871) by 3.4%.
Our best result of ensem-ble model (0.886) gets a comparable improve-ment (3.40%) compared to our obtained base-line (0.852).On the E-product dataset, Joshi and Ros?
re-ported the besigrams and their proposed GWR features.This is in accordance with our result of JointFeatures (0.6655 by SVM and 0.6764 by NB).The superiority of our ensemble result is quitesignificant (0.7014 by AvgP and 0.7018 byMCE).5.3 Results of Feature SelectionIn this part, we examine FMI and FIGfeature selection.
The performanceIG are also presented for comparison.
The re-sults on the Movie and E-product datasets aredisplayed in Figures 1 and 2 respectively.
Dueto space limit, we only report the results ofGWR-Bi features for Movie and GWR-Dp fea-tures for E-product.
In each of the figures, theresults under NB and SVM are both presented.At first, we observe the results of feature se-lection for GWR-Bi features on the Movie datat.
At first glance, IG and FIG have roughly thesame performance.
IG-based methods areshown to be quite effective in GWR feature re-duction.
For example under the NB model, top2.5% (4000) GWR-Bi features ranked by IGand FIG achieve accuracies of 0.849 and 0.8421342respectively, even better than the score with allfeatures (0.8415).0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 150,0000.60.650.70.750.80.85Movie: Bi-wpc @ SVMFeature numberAccuracyIGFIGMIFMI0 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000 150,0000.60.650.70.750.80.85Movie: Bi-wpc @ NBFeature numberAccuracyIGFIGMIFMIFigure 1: Feature Selection for GWR-Bi Features onthe Movie Dataset0 2000 4000 6000 8000 10000 12000 14000 160000.450.50.550.60.650.7E-product: Dp-wpc @ SVMFeature numberAccuracyIGFIGMIFMI0 2000 4000 6000 8000 10000 12000 14000 160000.450.50.550.60.650.7E-product: Dp-wpc @ NBFeature numberAccuracy WR features for sentiment classification.
Wehave proposed a GWR feature extraction ap-proach and an ensemble model to efficientlyintegrate different types of features.
Moreover,we have proposed two fast feature selectionmethods (FMI and FIG) for GWR features.Individual GWR features outperform trIGFIGMIFMIFigure 2: Feature Selection for GWR-Dp features onWe then ob  finer granu-laer-fosize of E-promparisons are made ac-cohe compuble model, when per-o6 Conclusions and Future Workadi-tioproved to be a good solution for se-lecting GWR features.
It is also worthy notingstuthe E-product datasetserve IG vs.
FIG in arity.
When the selected features are few (lessthan 5%), IG performs significantly better thanFIG, while the latter gradually approaches theformer when the feature number increases: as itcomes to 10-15%, their performance is quiteclose.
From then on, FIG is consistently compa-rable to IG, even sometimes slightly better.With regard to MI and FMI, although the prmance compared to IG and FIG is rather poor(the reason has been intensively studied byYang and Pedersen, 1997).
Our focus is theability of FMI for approximating MI.
From thispoint of view, FMI is by contrast effective, es-pecially with more than 1/3 features.Compared to the Movie dataset, theoduct dataset is much smaller, and the dataare much sparser.
Nevertheless, IG and FIG arestill effective.
On one hand, top 1.25% (2000)features ranked by IG yield a result better than(or comparable to) that with all features.
On theother hand, FIG is still competent to be a goodapproximation to IG.All of the above crding to accuracies, and we now pay attentionto computational efficiency.
Taking the Moviedataset for example, IG needs to compute scoresof information gain for all 152k  features, whileFIG only needs to comput 5 5k q  scores,saving more than 70% of t tationalload; on the E-product dataset, although the dataare sparse, the rate of computation reduction isstill significant (62.5%).Note that in the enseme 42f rming FIG for individual GWR feature set,part of its inherent complexity is already takencare of by having already computed IG on Unifeature set, and we only need to compute thescores for 25 POS pairs.
From this perspective,FIG is even more attractive in the ensemblemodel.The focus of this paper is exploring the use ofnal WR features significantly, but they stillcan not totally substitute unigrams.
The ensem-ble model is quite effective at integrating uni-grams and different types of WR feature, andthe performance is significantly better than jointfeatures.FIG isthat FIG is a general feature selection methodfor bigram features, even outside the scope ofsentiment classification and text classification.In the future, we plan to make an in-depthdy about why individual WR features areinferior to unigrams, and how to make the jointfeatures more effective.
We also plan to extendthe use of GWR features to the task of transferlearning, which we think is a promising direc-tion for future work.1343AcknowledgmentWe thank Yufeng Chen, Shoushan Li, Ping Jianand the anonymous reviewers for valuable com-ments and helpful suggestions.
The researchwork has been partially funded by the NaturalScience Foundation of China under Grant No.60975053, 90820303 and 60736014, the Na-tional Key Technology R&D Program underGrant No.
2006BAH03B02, the Hi-Tech Re-search and Development Program (?863?
Pro-gram) of China under Grant No.2006AA010108-4, and also supported by theChina-Singapore Institute of Digital Media(CSIDM) project under grant No.
CSIDM-200804.ReferencesKushal Dave, Steve Lawrence and David M. Pen-nock, 2003.
Mining the Peanut Gallery: OpinionExtraction and Semantic Classification of ProductReviews.
In Proceedings of the internationalWorld Wide Web Conference (WWW), pages519-528.Sa?o D?eroski and Bernard ?enko, 2004.
Is combin-ing classifiers with stacking better than selectingthe best one?
Machine Learning, 54 (3).
pages255-273.Yoav Freund and Robert E. Schapire, 1999.
Largemargin classification using the perceptron algo-rithm.
Machine Learning, 37 (3).
pages 277-296.Michael Gamon, 2004.
Sentiment classification oncustomer feedback data: noisy data, large featurevectors, and the role of linguistic analysis.
In Pro-ceedings of the International Conference on Com-putational Linguistics (COLING).
pages 841-847.Minqing Hu and Bing Liu, 2004.
Mining and sum-marizing customer reviews.
In Proceedings of theACM SIGKDD Conference on Knowledge Dis-covery and Data Mining (KDD), pages 168-177.Mahesh Joshi and Carolyn Penstein-Ros?, 2009.Generalizing dependency features for opinionmining.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the Association forComputational Linguistics (ACL), pages 313-316.Biing-Hwang Juang and Shigeru Katagiri, 1992.Discriminative learning for minimum error classi-fication.
IEEE Transactions on Signal Processing,40 (12).
pages 3043-3054.J Kittler, 1998.
Combining classifiers: A theoreticalframework.
Pattern Analysis and Applications, 1(1).
pages 18-27.Shoushan Li, Rui Xia, Chengqing Zong and Chu-Ren Huang, 2009.
A framework of feature selec-tion methods for text categorization.
In Proceed-ings of the Joint Conference of the 47th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 692-700.Andrew McCallum and Kamal Nigam, 1998.
A com-parison of event models for naive bayes text clas-sification.
In Proceedings of the AAAI workshopon learning for text categorization.Vincent Ng, Sajib Dasgupta and S. M. Niaz Arifin,2006.
Examining the Role of Linguistic Knowl-edge Sources in the Automatic Identification andClassification of Reviews.
In Proceedings of theCOLING/ACL, pages 611-618.Bo Pang and Lillian Lee, 2004.
A Sentimental Edu-cation: Sentiment Analysis Using SubjectivitySummarization Based on Minimum Cuts.
In Pro-ceedings of the Association for ComputationalLinguistics (ACL), pages 271-278.Bo Pang and Lillian Lee, 2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2 (1-2).
pages 1-135.Bo Pang, Lillian Lee and Shivakumar Vaithyanathan,2002.
Thumbs up?
Sentiment Classification usingMachine Learning Techniques.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 79-86.Yiming Yang and Jan O. Pedersen, 1997.
A com-parative study on feature selection in text catego-rization.
In Proceedings of the 14th InternationalConference on Machine Learning (ICML), pages412-420.1344
