Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 96?102,Baltimore, Maryland, 26-27 July 2014.c?2014 Association for Computational LinguisticsA Unified Framework for Grammar Error CorrectionLongkai Zhang Houfeng WangKey Laboratory of Computational Linguistics (Peking University) Ministry of Education, Chinazhlongk@qq.com, wanghf@pku.edu.cnAbstractIn this paper we describe the PKU systemfor the CoNLL-2014 grammar error cor-rection shared task.
We propose a unifiedframework for correcting all types of er-rors.
We use unlabeled news texts insteadof large amount of human annotated textsas training data.
Based on these data, atri-gram language model is used to cor-rect the replacement errors while two extraclassification models are trained to correcterrors related to determiners and preposi-tions.
Our system achieves 25.32% in f0.5on the original test data and 29.10% on therevised test data.1 IntroductionThe task of grammar error correction is diffi-cult yet important.
An automatic grammar errorcorrection system can help second language(L2)learners improve the quality of their writing.
Pre-vious shared tasks for grammar error correction,such as the HOO shared task of 2012 (HOO-2012)and the CoNLL-2013 shared task(CoNLL-2013),focus on limited types of errors.
For example,HOO-2012 only considers errors related to de-terminers and prepositions.
CoNLL-2013 furtherconsiders errors that are related to noun number,verb form and subject-object agreement.
In theCoNLL-2014 shared task, all systems should con-sider all the 28 kinds of errors, including errorssuch as spelling errors which cannot be correctedusing a single classifier.Most of the top-ranked systems in the CoNLL-2013 shared task(Ng et al., 2013) train individ-ual classifiers or language models for each kindof errors independently.
Although later systemssuch as Wu and Ng (2013); Rozovskaya and Roth(2013) use Integer Linear Programming (ILP) todecode a global optimized result, the input scoresfor ILP still come from the individual classifica-tion confidence of each kind of errors.
It is hardto adapt these methods directly into the CoNLL-2014 shared task.
It will be both time-consumingand impossible to train individual classifiers for allthe 28 kinds of errors.Besides the classifier and language modelbased methods, some systems(Dahlmeier and Ng,2012a; Yoshimoto et al., 2013; Yuan and Felice,2013) also use the machine translation approach.Because there are a limited amount of trainingdata, this kind of approaches often need to useother corpora of L2 learners, such as the Cam-bridge Learner Corpus.
Because these corpora usedifferent annotation criteria, the correction sys-tems should figure out ways to map the error typesfrom one corpus to another.
Even with these ad-ditions and transformations, there are still too fewtraining data available to train a good translationmodel.In contrast, we think the grammar error correc-tion system should 1) correct most kinds of er-rors in a unified framework and 2) use as muchunlabeled data as possible instead of using largeamount of human annotated data.
To be specific,our system do not need to train individual clas-sifiers for each kind of errors, nor do we needto use manually corrected texts.
Following theobservation that a correction can either replace awrong word or delete/insert a word, our systemis divided into two parts.
Firstly, we use a Lan-guage Model(LM) to correct errors with respect tothe wrongly used words.
The LM only uses thestatistics from a large corpus.
All errors related towrongly used words can be examined in this uni-fied model instead of designing individual systemsfor each kind of errors.
Secondly, we train extraclassifiers for determiner errors and preposition er-rors.
We further consider these two kinds of errorsbecause many of the deletion and insertion errorsbelongs to determiner or preposition errors.
The96training data of the two classification models alsocome from a large unlabeled news corpus there-fore no human annotation is needed.Although we try to use a unified framework toget better performance in the grammar error cor-rection task, there are still a small portion of errorswe do not consider.
The insertion and deletion ofwords are not considered if the word is neither adeterminer nor a preposition.
Our system is alsoincapable of replacing a word sequence into an-other word sequence.
We do not consider thesekinds of errors because we find some of them arehard to generate correction candidates without fur-ther understanding of the context, and are not easyto be corrected even by human beings.The paper is structured as follows.
Section 1gives the introduction.
In section 2 we describethe task.
In section 3 we describe our algorithm.Experiments are described in section 4.
We alsogive a detailed analysis of the results in section 4.In section 5 related works are introduced, and thepaper is concluded in the last section.2 Task DescriptionThe CoNLL-2014 shared task focuses on correct-ing all errors that are commonly made by L2 learn-ers of English.
The training data released bythe task organizers come from the NUCLE cor-pus(Dahlmeier et al., 2013).
This corpus containsessays written by L2 learners of English.
Theseessays are then corrected by English teachers.
De-tails of the CoNLL-2014 shared task can be foundin Ng et al.
(2014).3 System Overview3.1 OverviewIt is time-consuming to train individual models foreach kind of errors.
We believe a better way is tocorrect errors in a unified framework.
We assumethat each word in the sentence may be involved insome kinds of errors.
We generate a list of cor-rection candidates for each word.
Then a Lan-guage Model (LM) is used to find the most proba-ble word sequences based on the original sentenceand the correction candidates for each word.
Anillustrative example is shown in figure 1.Because the LM is designed for the replace-ment errors rather than insertion and deletion er-rors, we train two extra classifiers for determinersand prepositions.
The determiner model and thepreposition model can improve the performance inour experiment.3.2 Correction Candidate GenerationThe correction candidate generation phase aims togenerate a list of correction candidates for eachword in the original sentence.
We generate cor-rection candidates based on the following rules:1.
Words with the same stem2.
Similar words based on edit distanceThe first rule includes the words with thesame stem as candidates.
These candidatescan be used later to correct the errors re-lated to word form.
For example, candidatesfor the word ?time?
in the original sentence?This is a timely rain indeed.?
may include?timed?,?time?,?timed?,?times?,?timings?,?timely?,?timees?
and ?timing?, which all have the stem?time?.
The correct candidate ?timely?
is alsoincluded in the candidate list and can be detectedthrough further processing.The candidate generated by the second rule aremainly used for spelling correction.
For exam-ple, a such candidate for ?beleive?
may be ?belive?or ?believe?.
To generate meaningful candidateswhile guarantee accuracy, we require that the can-didate and the original word should have the sameinitial character.
By examining the training datawe experimentally find that very few L2 learn-ers make spelling errors on the initial characters.For example, they may spell ?believe?
as ?belive?.However, very few of them may spell ?believe?
as?pelieve?
or ?delieve?.In our system, we generate 10 candidates foreach word.
To keep the decoding of the best wordsequence controllable, we do not generate candi-dates for every word in the original sentence.
Weonly generate the edit distance based candidatesfor the following words:1.
Words that never appear in the English giga-word corpus12.
Words that appear in the gigaword corpus butwith frequency below a threshold (we use 10in the experiment)Besides, we do not generate candidates for thewords whose POS tags are ?NNP?
or ?NNPS?.1http://catalog.ldc.upenn.edu/LDC2003T0597Figure 1: Correction of the original sentence ?Thera is no spaces for Tom?.
We use red nodes to representthe original words in the sentence, and use blue nodes below each word to represent the candidate list ofeach word.
We use arrows to show the final corrected word sequence with the highest probability.These words are proper nouns.
The correctionof this kind of words should depend on morecontextual information.
For the stemming toolswe use the snowball stemmer2.
To generatecandidates based on edit distance, we use theorg.apache.lucene.search.spell.SpellChecker inLucene3.
Note that unlike other context basedspell checkers such as the one in Microsoft Office,the SpellChecker class in Lucene is actually nota spell checker.
For an input word w, it can onlysuggest words that are similar to w given a pre-defined dictionary.
We build the dictionary usingall words collected from the English Gigawordcorpus.3.3 Language Model for Candidate SelectionAfter given each word a list of candidates, we cannow find the word sequence which is most likely tobe the correct sentence.
The model we use is thelanguage model.
The probability P (s) of a sen-tence s = w0w1...wn?1is calculated as:P (s) =n?1?i=0P (wi|w0, ..., wi?1) (1)The transition probability P (wi|w0, ..., wi?1)is calculated based on language model.
Inour system we use a tri-gram languagemodel trained on the gigaword corpus.2http://snowball.tartarus.org/3https://lucene.apache.org/Therefore, P (wi|w0, ..., wi?1) is reduced toP (wi|wi?2, wi?1).
We do not use a fixed smooth-ing method.
We just set the probability of anunseen string to be a positive decimal which isvery close to zero.The decoding of the word sequence that max-imize p(s) can be tackled through dynamicprogramming using Viterbi algorithm(Forney Jr,1973).
One useful trick is that to multiplyp(wi|wi?2, wi?1) with a coefficient (4 in our sys-tem) if wi?2, wi?1and wiare all words in the orig-inal sentence.
This is because most of the originalword sequences are correct.
If the system needs tomake a correction, the corrected sequence shouldhave a much higher score than the original one.We do not generate candidates for determin-ers and prepositions.
Firstly, they are all frequentwords that are excluded by the rules we men-tioned in this section.
Secondly, the determinerand preposition errors are the main kinds of errorsmade by L2 learners.
Some of the errors are re-lated to the wrong deletions or insertions.
There-fore we choose to take special care of determinersand prepositions to correct all their replacement,deletion and insertion errors instead of generatingcandidates for them in this stage.3.4 Determiner CorrectionAfter using LM, the spelling errors as well as ordi-nary word form errors such as noun numbers, verb98forms are supposed to be corrected.
As we men-tioned in the introduction, we should now handlethe deletion and insertion errors.
We choose to usespecial models for determiner and prepositions be-cause many of the deletion and insertion errors arerelated to determiner errors or preposition errors.Also, these two kinds of errors have been consid-ered in HOO-2012 and CoNLL2013.
Thereforeit?s easier to make meaningful comparison withprevious works.
We use Maximum Entropy (ME)classifiers to correct the determiner and preposi-tion errors.
In this section we consider the deter-miner errors.
The preposition errors will be con-sidered in the next section.
For both of the twoparts, we use the open source tool MaxEnt4as theimplementation of ME.We consider the determiner correction task as amulti-class classification task.
The input instancesfor classification are the space between words.
Weconsider whether the space should keep empty, orinsert ?a?
or ?the?.
Therefore, 3 labels are con-sidered to indicate ?a?, ?the?
and ?NULL?.
We use??NULL?
to denote that the correct space does notneed an article.
We leave the clarification between?a?
and ?an?
as a post-process by manually de-signed rules.
We do not consider other determinerssuch as ?this?
or ??these?
because further informa-tion such as the coreference resolution results isneeded.Instead of considering all spaces in a sen-tence, some previous works(AEHAN et al., 2006;Rozovskaya and Roth, 2010; Rozovskaya et al.,2013) only consider spaces at the beginning ofnoun phrases.
Compared to these methods, oursystem do not need a POS tagger or a phrase chun-ker (which is sometimes not accurate enough) tofilter the positions.
All the operations are done onthe word level.
We list the features we use in ta-ble 1.
Note that for 3-grams and 4-grams we donot use all combinations of characters because itwill generate more sparse features while the per-formance is not improved.Because there are limited amount of trainingdata, we choose to use the English Gigaword cor-pus to generate training instances instead of us-ing the training data of CoNLL-2014.
Because thetexts in the Gigaword corpus are all news texts,most of them are well written by native speakersand are proofread by the editors.
Therefore they4http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html1-gram w?3, w?2, w?1, w1, w2, w32-gram all combinations of wiwjwherei, j ?
{?3,?2,?1, 1, 2, 3}3-gram w?3w?2w?1,w?2w?1w1,w?1w1w2, w1w2w34-gram w?3w?2w?1w1,w?2w?1w1w2,w?1w1w2w3Table 1: The features used in our system.
For agiven blank(space), wimeans the next ith wordand w?imeans the previous ith word.
For theexample of ?I do not play balls .
?, if the currentconsidered instance is the space between ?play?and ?balls?, then w?2means ?not?
and w1means?balls?.can serve as implicit gold annotations.
We gener-ate the training instances from the sentences in theGigaword corpus with the following rules:1. for each space between words, we treat it asan instance with label ?NULL?, which meansno article is needed.
We use the 3 words be-fore the space as w?3, w?2, w?1and the 3words after the space as w1, w2, w3to gener-ate features.
We name this kind of instances?Space Instance?
to indicate we operate ona space.
This kind of training instances canconvey the information that in this context noarticle is needed.2.
for each word that is an article, we assume itas an instance, with the label ?a?
or ?the?
de-pending on itself.
We use the 3 words beforeit as w?3, w?2, w?1and the 3 words afteris as w1, w2, w3.
In this case we do not usethe article itself as the context.
We name thiskind of instances ?article Instance?
to indicatewe operate on an article.
This kind of train-ing instances can convey the information thatin this context a particular article should beadded.The testing instance are also generated follow-ing the previously mentioned rules.
The decodingprocess is as follows.
If an instance is a ?spaceinstance?
and is predicted as ?a?
or ?the?, we thenadd ?a?
or ?the?
in this space.
If an instance is an?article instance?, the situation is a bit complex.
Ifit is predicted as another article, we replace it withthe predicted one.
If it is predicted as ?NULL?, weshould delete the article to make it a space.99To guarantee a certain level of precision, we re-quire the decoding should only be based on confi-dent predictions.
We use the probability calculatedby the classifier as the confidence score and re-quire the probability of the considered predictionsshould exceed a threshold.3.5 Preposition CorrectionThe preposition model is similar to the articlemodel.
We use the same set of features as in ta-ble 1.
The training and testing instance generationis similar except now we consider prepositions in-stead of articles.
The decoding phase is also iden-tical to the determiner model.3.6 Post ProcessingThe post processing in our system is listed as fol-lows:1.
Distinguish between ?a?
and ?an?.
We userule based method for this issue.2.
Splitting words.
If a word is not in the dic-tionary but one of its splitting results has ahigh frequency, we will split the word intotwo words.
For example, ?dailylife?
is anout of vocabulary word and the splitting re-sult ?daily life?
is common in English.
Thenwe split ?dailylife?
into ?daily life?.3.
We capitalize the first character of each sen-tence.4 Experiment and AnalysisWe experiment on the CoNLL-2014 test data.
Weevaluate our system based on the M2 scorer whichis provided by the organizers.
Details of the M2scorer can be found in Dahlmeier and Ng (2012b).We tune the additional parameters like all thethresholds on the CoNLL-2014 official trainingdata.
We use all the text in the Gigaword corpus totrain the language model.
We use 2.5 million sen-tences in the Gigaword corpus to train the extratwo classifier.Results of our system are shown in table 2.
LMrefers to using language model alone.
LM+detrefers to using a determiner classifier after usinga language model.
LM+prep refers to using apreposition classifier after using a language model.LM+det+preposition refers to using a prepositionclassifier after LM+det, which is the method usedin our final system.Model P R F0.5LM 29.89% 10.04% 21.42%LM+det 32.23% 13.64% 25.33%LM+prep 29.73% 10.04% 21.35%LM+det+prep(all) 32.21% 13.65% 25.32%Table 2: The experimental results of our system inthe CoNLL-2014 shared task.
The threshold fordeterminer model and preposition model is 0.99and 0.99.
Parameters are tuned on the CoNLL-2014 training data.Model P R F0.5LM+det+prep(all) 36.64% 15.96% 29.10%Table 3: The experimental results of our systemin the CoNLL-2014 shared task on the revised an-notations.
The threshold for determiner model andpreposition model is 0.99 and 0.99.
Parameters aretuned on the CoNLL-2014 training data.From the results we can see that the main con-tribution comes from the LM model and deter-miner model.
The preposition model can correctpart of the errors while introduce new errors.
Thepreposition model may harm the overall perfor-mance.
But considering the fact that the grammarerror correction systems are always used for rec-ommending errors, we still keep the prepositionmodel in real applications and suggest the errorspredicted by the preposition model.One limitation of our system is that we onlyuse a tri-gram based language model as well as upto 4-gram features for limited instances.
Previousworks(Rozovskaya et al., 2013; Kao et al., 2013)have shown that other resources like the Google 5-gram statistics can help improve performance.
Forthe determiner and preposition models, we exper-iment on different size of training data, from nearzero to the upper bound of our server?s memorylimit (about 72GB).
We find that under this lim-itation, the performance is still improving whenadding more training instances.
We believe theperformance can be further improved.Scores based on the revised annotations isshown in table 3.For the convenience of future meaningful com-parison, we report the result of our system on theCoNLL-2013 data set in table 4.
We tune the ad-ditional parameters like all the thresholds on theCoNLL-2013 official training data.
Note that inCoNLL-2013 the scorer considers F1 score in-100Model P R F1CoNLL13 1st 23.49% 46.45% 31.20 %CoNLL13 2nd 26.35% 23.80% 25.01 %LM 18.92% 14.55% 16.45%LM+det 23.76% 36.15% 28.67%LM+prep 18.89% 14.55% 16.44%LM+det+prep 23.74% 36.15% 28.66%Table 4: The experimental results of our systemon the CoNLL-2013 shared task data.
The thresh-old for determiner model and preposition modelis 0.75 and 0.99.
Parameters are tuned on theCoNLL-2013 training data.
CoNLL13 1st is Ro-zovskaya et al.
(2013) and the 2nd is Kao et al.
(2013)stead of F0.5.
Therefore some of the thresholds aredifferent with the ones in the CoNLL-2014 sys-tem.
Because the CoNLL-2013 shared task onlyconsiders 5 types of errors, it will be much easierto design components specially for each kind oferrors.
Therefore our system is a bit less accuratethan the best system.
In this system, we restrict thecandidates to be either noun or verb, and omit thespell checking model.
We also omit some post-processings like deciding whether a word shouldbe split into two words, because these kinds of er-rors are not included.5 ConclusionIn this paper we describe the PKU system forthe CoNLL-2014 grammar error correction sharedtask.
We propose a unified framework for correct-ing all types of errors.
A tri-gram language modelis used to correct the replacement errors while twoextra classification models are trained to correcterrors related to determiners and prepositions.
Oursystem achieves 25.32% in f0.5on the original testdata and 29.10% on the revised test data.AcknowledgmentsThis research was partly supported by Na-tional Natural Science Foundation of China(No.61370117, No.61333018), National HighTechnology Research and Development Programof China (863 Program) (No.2012AA011101)and Major National Social Science Fund ofChina(No.12&ZD227).ReferencesAEHAN, N., Chodorow, M., and LEACOCK,C.
L. (2006).
Detecting errors in english arti-cle usage by non-native speakers.Dahlmeier, D. and Ng, H. T. (2012a).
A beam-search decoder for grammatical error correc-tion.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural Lan-guage Processing and Computational NaturalLanguage Learning, pages 568?578.
Associa-tion for Computational Linguistics.Dahlmeier, D. and Ng, H. T. (2012b).
Better eval-uation for grammatical error correction.
In Pro-ceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics: Human Language Tech-nologies, pages 568?572.
Association for Com-putational Linguistics.Dahlmeier, D., Ng, H. T., and Wu, S. M. (2013).Building a large annotated corpus of learner en-glish: The nus corpus of learner english.
In Pro-ceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applica-tions, pages 22?31.Forney Jr, G. D. (1973).
The viterbi algorithm.Proceedings of the IEEE, 61(3):268?278.Kao, T.-h., Chang, Y.-w., Chiu, H.-w., Yen, T.-H.,Boisson, J., Wu, J.-c., and Chang, J. S. (2013).Conll-2013 shared task: Grammatical error cor-rection nthu system description.
In Proceed-ings of the Seventeenth Conference on Compu-tational Natural Language Learning: SharedTask, pages 20?25, Sofia, Bulgaria.
Associationfor Computational Linguistics.Ng, H. T., Wu, S. M., Briscoe, T., Hadiwinoto,C., Susanto, R. H., and Bryant, C. (2014).
Theconll-2014 shared task on grammatical errorcorrection.
In Proceedings of the EighteenthConference on Computational Natural Lan-guage Learning: Shared Task (CoNLL-2014Shared Task), pages 1?12, Baltimore, Mary-land, USA.
Association for Computational Lin-guistics.Ng, H. T., Wu, S. M., Wu, Y., Hadiwinoto, C., andTetreault, J.
(2013).
The conll-2013 shared taskon grammatical error correction.
In Proceed-ings of the Seventeenth Conference on Compu-tational Natural Language Learning: Shared101Task, pages 1?12, Sofia, Bulgaria.
Associationfor Computational Linguistics.Rozovskaya, A., Chang, K.-W., Sammons, M.,and Roth, D. (2013).
The university of illi-nois system in the conll-2013 shared task.In Proceedings of the Seventeenth Conferenceon Computational Natural Language Learning:Shared Task, pages 13?19, Sofia, Bulgaria.
As-sociation for Computational Linguistics.Rozovskaya, A. and Roth, D. (2010).
Train-ing paradigms for correcting errors in grammarand usage.
In Human language technologies:The 2010 annual conference of the north amer-ican chapter of the association for computa-tional linguistics, pages 154?162.
Associationfor Computational Linguistics.Rozovskaya, A. and Roth, D. (2013).
Joint learn-ing and inference for grammatical error correc-tion.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Pro-cessing, pages 791?802, Seattle, Washington,USA.
Association for Computational Linguis-tics.Wu, Y. and Ng, H. T. (2013).
Grammatical errorcorrection using integer linear programming.
InProceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics (Vol-ume 1: Long Papers), pages 1456?1465, Sofia,Bulgaria.
Association for Computational Lin-guistics.Yoshimoto, I., Kose, T., Mitsuzawa, K., Sak-aguchi, K., Mizumoto, T., Hayashibe, Y., Ko-machi, M., and Matsumoto, Y.
(2013).
Naist at2013 conll grammatical error correction sharedtask.
CoNLL-2013, 26.Yuan, Z. and Felice, M. (2013).
Constrained gram-matical error correction using statistical ma-chine translation.
CoNLL-2013, page 52.102
