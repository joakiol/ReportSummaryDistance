Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1146?1154,Beijing, August 2010?Got You!?
: Automatic Vandalism Detection in Wikipediawith Web-based Shallow Syntactic-Semantic ModelingWilliam Yang Wang and Kathleen R. McKeownDepartment of Computer ScienceColumbia Universityyw2347@columbia.edu kathy@cs.columbia.eduAbstractDiscriminating vandalism edits fromnon-vandalism edits in Wikipedia is achallenging task, as ill-intentioned editscan include a variety of content and beexpressed in many different forms andstyles.
Previous studies are limited torule-based methods and learning basedon lexical features, lacking in linguisticanalysis.
In this paper, we propose anovel Web-based shallow syntactic-semantic modeling method, which utiliz-es Web search results as resource andtrains topic-specific n-tag and syntacticn-gram language models to detect van-dalism.
By combining basic task-specificand lexical features, we have achievedhigh F-measures using logistic boostingand logistic model trees classifiers, sur-passing the results reported by majorWikipedia vandalism detection systems.1 IntroductionOnline open collaboration systems are becominga major means of information sharing on theWeb.
With millions of articles from millions ofresources edited by millions of people, Wikipe-dia is a pioneer in the fast growing, online know-ledge collaboration era.
Anyone who has Inter-net access can visit, edit and delete Wikipediaarticles without authentication.A primary threat to this convenience, however,is vandalism, which has become one of Wikipe-dia?s biggest concerns (Geiger, 2010).
To date,automatic countermeasures mainly involve rule-based approaches and these are not very effec-tive.
Therefore, Wikipedia volunteers have tospend a large amount of time identifying vanda-lized articles manually, rather than spendingtime contributing content to the articles.
Hence,there is a need for more effective approaches toautomatic vandalism detection.In contrast to spam detection tasks, where afull spam message, which is typically 4K Bytes(Rigoutsos and Huynh, 2004), can be sampledand analyzed (Itakura and Clarke, 2009), Wiki-pedia vandals typically change only a smallnumber of words or sentences in the targetedarticle.
In our preliminary corpus (Potthast et al,2007), we find the average size of 201 vanda-lized texts to be only 1K Byte.
This leaves veryfew clues for vandalism modeling.
The questionwe address in this paper is: given such limitedinformation, how can we better understand andmodel Wikipedia vandalism?Our proposed approach establishes a novelclassification framework, aiming at capturingvandalism through an emphasis on shallow syn-tactic and semantic modeling.
In contrast to pre-vious work, we recognize the significance ofnatural language modeling techniques for Wiki-pedia vandalism detection and utilize Websearch results to construct our shallow syntacticand semantic models.
We first construct a base-line model that captures task-specific clues andlexical features that have been used in earlierwork (Potthast et al, 2008; Smets et al, 2008)augmenting these with shallow syntactic andsemantic features.
Our main contributions are:?
Improvement over previous modeling me-thods with three novel lexical features?
Using Web search results as training datafor syntactic and semantic modeling?
Building topic-specific n-tag syntax modelsand syntactic n-gram models for shallowsyntactic and semantic modeling11462 Related WorkSo far, the primary method for automaticvandalism detection in Wikipedia relies on rule-based bots.
In recent years, however, with therise of statistical machine learning, researchershave begun to treat Wikipedia vandalismdetection task as a classification task.
To the bestof our knowledge, we are among the first toconsider the shallow syntactic and semanticmodeling using Natural Language Processing(NLP) techniques, utilizing the Web as corpus todetect vandalism.ClueBot (Carter, 2007) is one of the most ac-tive bots fighting vandalism in Wikipedia.
Itkeeps track of the IP of blocked users and usessimple regular expressions to keep Wikipediavandalism free.
A distinct advantage of rule-based bots is that they have very high precision.However they suffer from fixed-size knowledgebases and use only rigid rules.
Therefore, theiraverage recall is not very high and they can beeasily fooled by unseen vandalism patterns.
Ac-cording to Smets et al, (2008) and Potthast et al,(2008), rule-based bots have a perfect precisionof 1 and a recall of around 0.3.The Wikipedia vandalism detection researchcommunity began to concentrate on the machinelearning approaches in the past two years.
Smetset al (2008) wrapped all the content in diff textinto a bag of words, disregarding grammar andword order.
They used Na?ve Bayes as theclassification algorithm.
Compared to rule-basedmethods, they show an average precision of 0.59but are able to reach a recall of 0.37.
Thoughthey are among the first to try machine learningapproaches, the features in their study are themost straightforward set of features.
Clearly,there is still room for improvement.More recently, Itakura and Clarke (2009) haveproposed a novel method using Dynamic Mar-kov Compression (DMC).
They model their ap-proach after the successful use of DMC in Weband Mail Spam detection (Bratko et al, 2006).The reported average precision is 0.75 and ave-rage recall is 0.73.To the best of our knowledge, Potthast et al,(2008) report the best result so far for Wikipediavandalism detection.
They craft a feature set thatconsists of interesting task-specific features.
Forexample, they monitor the number of previouslysubmitted edits from the same author or IP,which is a good feature to model author contri-bution.
Their other contributions are the use of alogistic regression classifier, as well as the useof lexical features.
They successfully demon-strate the use of lexical features like vulgarismfrequency.
Using all features, they reach an av-erage precision of 0.83 and recall of 0.77.In addition to previous work on vandalism de-tection, there is also earlier work using the webfor modeling.
Biadsy et al (2008) extract pat-terns in Wikipedia to generate biographies au-tomatically.
In their experiment, they show thatwhen using Wikipedia as the only resource forextracting named entities and corresponding col-locational patterns, although the precision is typ-ically high, recall can be very low.
For that rea-son, they choose to use Google to retrieve train-ing data from the Web.
In our approach, insteadof using Wikipedia edits and historical revisions,we also select the Web as a resource to train ourshallow syntactic and semantic models.3 Analysis of  Types of VandalismIn order to better understand the characteristicsof vandalism cases in Wikipedia, we manuallyanalyzed 201 vandalism edits in the training setof our preliminary corpus.
In order to concen-trate on textual vandalism detection, we did nottake into account the cases where vandals hackthe image, audio or other multimedia resourcescontained in the Wikipedia edit.We found three main types of vandalism,which are shown in Table 1 along with corres-ponding examples.
These examples contain boththe title of the edit and a snippet of the diff-edcontent of vandalism, which is the textual differ-ence between the old revision and the new revi-sion, derived through the standard diff algorithm(Heckel, 1978).?
Lexically ill-formedThis is the most common type of vandal-ism in Wikipedia.
Like other online van-dalism acts, many vandalism cases inWikipedia involve ill-intentioned or ill-formed words such as vulgarisms, invalidletter sequences, punctuation misuse andWeb slang.
An interesting observation isthat vandals almost never add emoticonsin Wikipedia.
For the first example in1147Table 1: Vandalism Types and ExamplesTable 1, vulgarism and punctuation mi-suse are observed.?
Syntactically ill-formedMost vandalism cases that are lexicallyill-intentioned tend to be syntactically ill-formed as well.
It is not easy to capturethese cases by solely relying on lexicalknowledge or rule-based dictionaries andit is also very expensive to update dictio-naries and rules manually.
Therefore, wethink that is crucial to incorporate moresyntactic cues in the feature set in order toimprove performance.
Moreover, there arealso some cases where an edit could belexically well-intentioned, yet syntactical-ly ill-formed.
The first example of syntac-tic ill-formed in Table 1 is of this kind.Table 2: Feature Sets and CorrespondingFeatures of Our Vandalism Detection System?
Lexically and syntactically wellformed, but semantically ill-intentionedThis is the trickiest type of vandalism toidentify.
Vandals of this kind might havegood knowledge of the rule-based vandal-ism detecting bots.
Usually, this type ofvandalism involves off-topic comments,inserted biased opinions, unconfirmed in-formation and lobbying using very subjec-tive comments.
However, a common cha-racteristic of all vandalism in this categoryis that it is free of both lexical and syntac-tic errors.
Consider the first example ofsemantic vandalism in Table 1 with edittitle ?Global Warming?
: while the firstsentence for that edit seems to be fairlynormal (the author tries to claim anotherexplanation of the global warming effect),the second sentence makes a sudden tran-sition from the previous topic to mentiona basketball star and makes a ridiculousconclusion in the last sentence.In this work, we realize the importance of in-corporating NLP techniques to tackle all theabove types of vandalism, and our focus is onthe syntactically ill-formed and semantically ill-intentioned types that could not be detected byrule-based systems and straightforward lexicalfeatures.VandalismTypesExamplesLexicallyill-formedEdit Title:  IPodshit!!!!!!!!!!!!!!!!!!!!!
!Syntacticallyill-formedEdit Title: Rock musicDOWN WITH SOCIETYMADDISON STREET RIOTFOREVER.Edit Title: Vietnam WarCrabinarah sucks dont buy itLexically +syntacticallywell-formed,semanticallyill-intentionedEdit Title: Global WarmingAnother popular theory in-volving global warming isthe concept that globalwarming is not caused bygreenhouse gases.
The theoryis that Carlos Boozer is theone preventing the infraredheat from escaping the at-mosphere.
Therefore, theGolden State Warriors willwin next season.Edit Title: Harry PotterHarry Potter is a teenageboy who likes to smokecrack with his buds.
Theyalso run an illegal smugglingbusiness to their headmasterdumbledore.
He is dumb!FeatureSetsFeaturesTask-specificNumber of Revisions;Revisions Size Ratio;Lexical Vulgarism; Web Slang;Punctuation Misuse;Comment Cue Words;Syntactic Normalized Topic-specific N-tagLog Likelihood and PerplexitySemantic Normalized Topic-specificSyntactic N-gram LogLikelihood and Perplexity11484 Our SystemWe propose a shallow syntactic-semantic fo-cused classification approach for vandalism de-tection (Table 2).
In contrast to previous work,our approach concentrates on the aspect of usingnatural language techniques to model vandalism.Our shallow syntactic and semantic modelingapproaches extend the traditional n-gram lan-guage modeling method with topic-specific n-tag (Collins et al, 2005) syntax models and top-ic-specific syntactic n-gram semantic models.Moreover, in the Wikipedia vandalism detectiontask, since we do not have a sufficient amount oftraining data to model the topic of each edit, wepropose the idea of using the Web as corpus byretrieving search engine results to learn our top-ic-specific n-tag syntax and syntactic n-gramsemantic models.
The difference between oursyntactic and semantic modeling is that n-tagsyntax models only model the order of sentenceconstituents, disregarding the correspondingwords.
Conversely, for our syntactic n-grammodels, we do keep track of words together withtheir POS tags and model both the word andsyntactic compositions as a sequence.
The detailof our shallow syntactic-semantic modeling me-thod will be described in subsection 4.4.We use our shallow syntactic-semantic modelto augment our base model, which builds on ear-ly work.
For example, when building one of ourtask-specific features, we extract the name of theauthor of this revision to query Wikipedia aboutthe historical behavior of this author.
This kindof task-specific global feature tends to be veryinformative and thus forms the basis of our sys-tem.
For lexical level features, we count vulgar-ism frequencies and also introduce three newlexical features: Web slang, punctuation misuseand comment cue words, all of which will bedescribed in detail in 4.2 and 4.3.4.1 Problem RepresentationThe vandalism detection task can be formu-lated as the following problem.
Let?s assume wehave a vandalism corpus C, which contains a setof Wikipedia edits S. A Wikipedia edit is de-noted as ei.
In our case, we have S = {e1, e2?,en}.Each edit e has two consecutive revisions (an oldrevision Rold and a new revision Rnew) that areunique in the entire data set.
We write that e ={Rold, Rnew}.
With the use of the standard diffalgorithm, we can produce a text Rdiff, showingthe difference between these two revisions, sothat e = {Rold, Rnew, Rdiff }.
Our task is: given S,to extract features from edit e ?S and train alogistic boosting classifier.
On receiving an edite from the test set, the classifier needs to decidewhether this e is a vandalism edit or a non-vandalism edit.
e?
{1,0}.4.2 Basic Task-specific and Lexical Fea-turesTask-specific features are domain-dependent andare therefore unique in this Wikipedia vandalismdetection task.
In this work, we pick two task-specific features and one lexical feature thatproved effective in previous studies.?
Number of RevisionsThis is a very simple but effective featurethat is used by many studies (Wilkinsonand Huberman, 2007; Adler et al, 2008;Stein and Hess, 2007).
By extracting theauthor name for the new revision Rnew, wecan easily query Wikipedia and count howmany revisions the author has modified inthe history.?
Revision Size RatioRevision size ratio measures the size ofthe new revision versus the size of the oldrevision in an edit.
This measure is an in-dication of how much information isgained or lost in the new revision Rnew,compared to the old revision Rold, and canbe expressed as:RevRatio(?)
=Count (w)w  ?
R  newCount (w)w  ?
R  oldwhere W represents any word token of arevision.?
Vulgarism FrequencyRevision size ratio measures the size ofthe new revision versus the Vulgarismfrequency was first introduced by Potthastet al (2008).
However, note that not allvulgarism words should be considered asvandalism and sometime even the Wiki-pedia edit?s title and content themselvescontain vulgarism words.1149For each diff text in an edit e, we countthe total number of appearances of vulgar-ism words v where v is in our vulgarismdictionary1.VulFreq ?
=  Count(?)?
?Rdiff4.3 Novel Lexical FeaturesIn addition to previous lexical features, we pro-pose three novel lexical features in this paper:Web slang frequency, punctuation misuse, andcomment cue words frequency.?
Web Slang and Punctuation MisuseSince Wikipedia is an open Web applica-tion, vandalism also contains a fairamount of Web slang, such as, ?haha?,?LOL?
and ?OMG?.
We use the same me-thod as above to calculate Web slang fre-quency, using a Web slang dictionary2.
Invandalism edits, many vandalism edits al-1 http://www.noswearing.com/dictionary2 http://www.noslang.com/dictionary/fullso contain punctuation misuse, for exam-ple, ?!!!?
and ?????.
However, we havenot observed a significant amount of emo-ticons in the vandalism edits.
Based onthis, we only keep track of Web slang fre-quency and the occurrence of punctuationmisuse.?
Comment Cue WordsUpon committing each new revision inWikipedia, the author is required to entersome comments describing the change.Well-intentioned Wikipedia contributorsconsistently use these comments to ex-plain the motivation for their changes.
Forexample, common non-vandalism editsmay contain cue words and phrases like?edit revised, page changed, item cleanedup, link repaired or delinked?.
In contrast,vandals almost never take their time toadd these kinds of comments.
We canmeasure this phenomenon by counting thefrequency of comment cue words.11504.4 Topic-specific N-tag Syntax Models andSyntactic N-grams for Shallow Syntac-tic and Semantic ModelingIn Figure 1, we present the overview of our ap-proach, which uses Web-trained topic-specifictraining for both: (1) n-tag syntax models forshallow syntactic modeling and (2) syntactic n-gram models for shallow semantic modeling.For each Wikipedia edit, we consider its titleas an approximate semantic representation, usingit as a query to build topic-specific models.
Inaddition, we also use the title information tomodel the syntax of this topic.Given Rdiff, we produce the syntactic versionof the diff-ed text using a probabilistic POS tag-ger (Toutanova and Manning, 2000; Toutanovaet al, 2003).
The edit title is extracted from thecorpus (either Rnew or Rold) and is used to querymultiple Web search engines in order to collectthe n-tag and n-gram training data from the top-kresults.
Before we start training language models,we tag the top-k results using the POS tagger.Note that when modeling n-tag syntax models, itis necessary to remove all the words.
With thePOS-only sequences, we train topic-specific n-tag models to describe the syntax of normal texton the same topic associated with this edit.
Withthe original tagged sequences, we train syntacticn-gram models to represent the semantics of thenormal text of this edit.After completing the training stage, we sendthe test segment (i.e.
the diff-ed text sequence) toboth the learned n-tag syntax models and thelearned syntactic n-gram models.
For the n-tagsyntax model, we submit the POS tag-only ver-sion of the segment.
For the syntactic n-grammodel, we submit a version of the segmentwhere each original word is associated with itsPOS-tag.
In both cases we compute the log-likelihood and the perplexity of the segment.Finally, we normalize the log likelihood andperplexity scores by dividing them by the lengthof Rdiff, as this length varies substantially fromone edit to another.
3 We expect an edit that haslow log likelihood probability and perplexity tobe vandalism, and it is very likely to be unre-lated to the syntax and semantic of the normaltext of this Wikipedia edit.
In the end, the nor-malized log probability and perplexity scoreswill be incorporated into our back-end classifierwith all task-specific and lexical features.Web as Corpus: In this work, we leverageWeb search results to train the syntax and se-mantic models.
This is based on the assumptionthat the Web itself is a large corpus and Websearch results can be a good training set to ap-proximate the semantics and syntax of the query.Topic-specific Modeling: We introduce atopic-specific modeling method that treats everyedit in Wikipedia as a unique topic.
We thinkthat the title of each Wikipedia edit is an approx-imation of the topic of the edit, so we extract thetitle of each edit and use it as keywords to re-trieve training data for our shallow syntactic andsemantic modeling.Topic-specific N-tag and Syntactic N-gram:In our novel approach, we tag all the top-k queryresults and diff text with a probabilistic POS tag-ger in both the training and test set of the vandal-ism corpus.
Figure 2(a) is an example of a POS-tagged sequence in a top-k query result.For shallow syntactic modeling, we use an n-tag modeling method (Collins et al, 2005).
Giv-en a tagged sequence, we remove all the wordsand only keep track of its POS tags: tagi-2 tagi-13 Although we have experimented with using thelength of Rdiff as a potential feature, it does not appearto be a good indicator of vandalism.
(a)Rock/NNP and/CC roll/NN -LRB-/-LRB-also/RB spelled/VBD Rock/NNP 'n'/CCRoll/NNP(b)NNP CC NN -LRB- RB VBD NNP CCNNP(c)Rock/NNP !/.
!/.
!/.
and/CC roll/VByou/PRP !/.
!/.
!/.
(d)NNP .
.
.
CC VB PRP .
.
.Figure 2.
Topic-specific N-tag and SyntacticN-gram modeling for the edit ?Rock andRoll?
in Wikipedia (a) The Web-derivedPOS tagged sequence (b) The Web-derivedPOS tag-only sequence (c) A POS taggedvandalism diff text Rdiff (d) A POS tag-onlyvandalism Rdiff1151tagi.
This is similar to n-gram language modeling,but instead, we model the syntax using POS tags,rather than its words.
In this example, we canuse the system in Figure 2 (b) to train an n-tagsyntactic model and use the one in Figure 2 (d)to test.
As we see, for this test segment, it be-longs to the vandalism class and has very differ-ent syntax from the n-tag model.
Therefore, thenormalized log likelihood outcome from the n-tag model is very low.In order to model semantics, we use an im-proved version of the n-gram language modelingmethod.
Instead of only counting wordi-2 wordi-1wordi, we model composite tag/word feature, e.g.tagi-2wordi-2 tagi-1wordi-1 tagiwordi.
This syntacticn-gram modeling method has been successfullyapplied to the task of automatic speech recogni-tion (Collins et al, 2005).
In the example in Fig-ure 2, the vandalism diff text will probably scorelow, because although it shares an overlap bi-gram ?and roll?
with the phrase ?rock and roll?in training text, once we apply the shallow syn-tactic n-gram modeling method, the POS tagbigram ?and/CC roll/VB?
in diff text will be dis-tinguished from the ?and/CC roll/NN?
or?and/CC roll/NNP?
in the training data.5 ExperimentsTo evaluate the effectiveness of our approach,we first run experiments on a preliminary corpusthat is also used by previous studies and com-pare the results.
Then, we conduct a second ex-periment on a larger corpus and analyze in detailthe features of our system.5.1 Experiment SetupIn our experiments, we use a Wikipedia vandal-ism detection corpus (Potthast et al, 2007) as apreliminary corpus.
The preliminary corpus con-tains 940 human-assessed edits from which 301edits are classified as vandalism.
We split thecorpus and keep a held-out 100 edits for eachclass in testing and use the rest for training.
Inthe second experiment, we adopt a larger corpus(Potthast et al, 2010) that contains 15,000 editswith 944 marked as vandalism.
The split is 300edits for each class in held-out testing and therest used for training.
In the description of thesecond corpus, each edit has been reviewed by atleast 3 and up to 15 annotators.
If more than 2/3of the annotators agree on a given edit, then theedit is tagged as one of our target classes.
Only11 cases are reported where annotators fail toform a majority inter-labeler agreement and inthose cases, the class is decided by corpus au-thors arbitrarily.In our implementation, the Yahoo!
4  searchengine and Bing5 search engine are the sourcefor collecting top-k results for topic-specific n-gram training data, because Google has a dailyquery limit.
We retrieve top-100 results fromYahoo!, and combine them with the top-50 re-sults from Bing.For POS tagging, we use the Stanford POSTagger (Toutanova and Manning, 2000; Touta-nova et al, 2003) with its attached wsj3t0-18-bidirectional model trained from the Wall StreetJournal corpus.
For both shallow syntactic andsemantic modeling, we train topic-specific tri-gram language models on each edit using theSRILM toolkit (Stolcke, 2002).In this classification task, we used two logisticclassification methods that haven?t been usedbefore in vandalism detection.
Logistic modeltrees (Landwehr et al, 2005) combine tree in-duction with linear modeling.
The idea is to usethe logistic regression to select attributes andbuild logistic regression at the leaves by incre-mentally refining those constructed at higherlevels in the tree.
The second method we used,logistic boosting (Friedman et al, 2000), im-proves logistic regression with boosting.
Itworks by applying the classification algorithm toreweighted versions of the data and then taking aweighted majority vote of the sequence of clas-sifiers thus produced.5.2 Preliminary ExperimentIn the preliminary experiment, we tried logisticboosting classifiers and logistic model trees asclassifiers with 10-fold cross validation.
Therule-based method, ClueBot, is our baseline.We also implemented another baseline system,using the bag of words (BoW) and Naive Bayesmethod (Smets et al, 2008) and the same toolkit(McCallum, 1996) that Smets et al used.
Then,we compare our result with Potthast et al (2008),who used the same corpus as us.4 http://www.yahoo.com5 http://www.bing.com1152Table 3: Preliminary Experiment Results; Theacronyms: BoW: Bag of Words, LMT: LogisticModel Trees, LB: Logistic Boosting, Task-specific + Lexical: features in section 4.1 and 4.2As we can see in Table 3, the ClueBot has aF-score (F1) of 0.43.
The BoW + Na?ve Bayesapproach improved the result and reached an F1of 0.75.
Compared to these results, the system ofPotthast et al (2008) is still better and has a F1of 0.80.For the results of our system, LMT gives us a0.89 F1 and LogitBoost (LB) gives a 0.95 F1.
Asignificant F1 improvement of 15% wasachieved in comparison to the previous study(Potthast et al, 2008).
Another finding is that wefind our shallow syntactic-semantic modelingmethod improves 2-4% over our task-specificand lexical features.5.3 Results and AnalysisIn the second experiment, a notable differencefrom the preliminary evaluation is that we havean unbalanced data problem.
So, we use randomdown-sampling method to resample the majorityclass into balanced classes in the training stage.Then, we also use the two classifiers with 10-fold cross validation.The F1 result reported by our BoW + Na?veBayes baseline is 0.68.
Next, we test our task-specific and lexical features that specified in sec-tion 4.1 and 4.2.
The best result is a F1 of 0.82,using logistic boosting.
Finally, with our topic-specific shallow syntactic and semantic model-Table 4: Second Experiment Resultsing features, we have a precision of 0.86, a recallof 0.85 and F1 of 0.85.Though we are surprised to see the overall F1for the second experiment are not as high as thefirst one, we do see that the topic-specific shal-low syntactic and semantic modeling methodsplay an important role in improving the result.Looking back at the related work we men-tioned in section 2, though we use newer datasets, our overall results still seem to surpass ma-jor vandalism detection systems.6 Conclusion and Future WorksWe have described a practical classificationframework for detecting Wikipedia vandalismusing NLP techniques and shown that it outper-forms rule-based methods and other major ma-chine learning approaches that are previouslyapplied in the task.In future work, we would like to investigatedeeper syntactic and semantic cues to vandalism.We hope to improve our models using shallowparsing and full parse trees.
We may also trylexical chaining to model the internal semanticlinks within each edit.AcknowledgementsThe authors are grateful to Julia Hirschberg,Yves Petinot, Fadi Biadsy, Mukund Jha, Wei-yun Ma, and the anonymous reviewers for usefulfeedback.
We thank Potthast et al for the Wiki-pedia vandalism detection corpora.Systems Recall Precision F1ClueBot 0.27 1 0.43BoW +Na?ve Bayes0.75 0.74 0.75Potthastet.
al., 20080.77 0.83 0.80Task-specific+Lexical(LMT)0.87 0.87 0.87Task-specific+Lexical (LB)0.92 0.91 0.91Our System(LMT)0.89 0.89 0.89Our System(LB)0.95 0.95 0.95Features Recall Precision F1BoW +Na?ve Bayes0.68 0.68 0.68Task-specific(LMT)0.81 0.80 0.80Task-specific+Lexical(LMT)0.81 0.81 0.81Our System(LMT)0.84 0.83 0.83Task-specific(LB)0.81 0.80 0.80Task-specific +Lexical (LB)0.83 0.82 0.82Our System(LB)0.86 0.85 0.851153ReferencesAdler, B. Thomas, Luca de Alfaro, Ian Pye andVishwanath Raman.
2008.
Measuring Author Con-tributions to the Wikipedia.
In Proc.
of the ACM2008 International Symposium on Wikis.Biadsy, Fadi, Julia Hirschberg, and Elena Filatova.2008.
An Unsupervised Approach to BiographyProduction using Wikipedia.
In Proc.
of the 46thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 807?815.Bratko, Andrej,  Gordon V. Cormack, Bogdan Filip-ic,Thomas R. Lynam and Blaz Zupan.
2006.
SpamFiltering Using Statistical Data CompressionModels.
Journal of Machine Learning Research,pages 7:2673-2698.Collins, Michael, Brian Roark and Murat Saraclar.2005.
Discriminative Syntactic Language Model-ing for Speech Recognition.
In Proc.
of the 43rdAnnual Meeting of the Association for Computa-tional Linguistics.
pages 507?514.Friedman, Jerome, Trevor Hastie and Robert Tibshi-rani.
2000.
Additive Logistic Regression: a Statis-tical View of Boosting.
Annals of Statistics 28(2),pages 337-407.Geiger, R. Stuart.
2010.
The Work of Sustaining Or-der in Wikipedia: The Banning of a Vandal.
InProc.
of the 2010 ACM Conference on ComputerSupported Cooperative Work, pages 117-126.Heckel, Paul.
1978.
A Technique for Isolating Differ-ences Between Files.
Communications of the ACM,pages 264?268Itakura, Kelly Y. and Charles L. A. Clarke.
2009.Using Dynamic Markov Compression to DetectVandalism in the Wikipedia.
In Proc.
of the 32ndInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, pages822-823.Landwehr, Niels, Mark Hall and Eibe Frank.
2005.Logistic Model Trees.
Machine Learning, 59(1-2),pages 161?205.McCallum, Andrew.
1996.
Bow: a Toolkit for Statis-tical Language Modeling, Text Retrieval, Classifi-cation and Clustering.Potthast, Martin, Benno Stein, and Robert Gerling.2008.
Automatic Vandalism Detection in Wikipe-dia.
In Proc.
of the 30th European Conference onInformation Retrieval, Lecture Notes in ComputerScience, pages 663-668.Potthast, Martin and Robert Gerling.
2007.
WikipediaVandalism Corpus WEBIS-VC07-11.
Web Tech-nology & Information Systems Group, BauhausUniversity Weimar.Potthast, Martin, Benno Stein and Teresa Holfeld.2010.
PAN Wikipedia Vandalism Training CorpusPAN-WVC-10.
Web Technology & InformationSystems Group, Bauhaus University Weimar.Rigoutsos, Isidore and Tien Huynh.
2004.
Chung-Kwei: a pattern-discovery-based system for the au-tomatic identification of unsolicited e-mail mes-sages (SPAM).
In Proc.
of the First Conference onE-mail and Anti-Spam.Smets, Koen, Bart Goethals and Brigitte Verdonk.2008.
Automatic Vandalism Detection in Wikipe-dia: Towards a Machine Learning Approach InProc.
of AAAI '08, Workshop on Wikipedia andArtificial Intelligence, pages 43-48.Stein, Klaus and Claudia Hess.
2007.
Does It MatterWho Contributes: a Study on Featured Articles inthe German Wikipedia.
In Proc.
of the ACM 18thConference on Hypertext and Hypermedia, pages171?174.Stolcke, Andreas.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proc.
of the Interna-tional Conference on Spoken LanguageProcessing, volume 2, pages 901?904.Toutanova, Kristina and Christopher D. Manning.2000.
Enriching the Knowledge Sources Used in aMaximum Entropy Part-of-Speech Tagger.
In Proc.of the Joint SIGDAT Conference on Empirical Me-thods in Natural Language Processing and VeryLarge Corpora, pages 63-70.Toutanova, Kristina, Dan Klein, Christopher Man-ning, and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic DependencyNetwork.
In Proceedings of Human LanguageTechnology Conference and the North AmericanChapter of the Association of Computational Lin-guistics Series, pages 252-259.Wilkinson,Dennis and Bernardo Huberman.
2007.Cooperation and Quality in Wikipedia.
In Proc.
ofthe ACM 2007 International Symposium on Wikis,pages 157?164.1154
