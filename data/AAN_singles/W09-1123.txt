Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 183?191,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsImproving Text Classification by a Sense Spectrum Approach to TermExpansionPeter WittekDepartment of Computer ScienceNational University of SingaporeComputing 1, Law LinkSingapore 117590wittek@comp.nus.edu.sgSa?ndor Dara?nyiSwedish School of Libraryand Information ScienceGo?teborg University &University of Bora?sAlle?gatan 150190 Bora?s, Swedensandor.daranyi@hb.seChew Lim TanDepartment of Computer ScienceNational University of SingaporeComputing 1, Law LinkSingapore 117590tancl@comp.nus.edu.sgAbstractExperimenting with different mathematicalobjects for text representation is an importantstep of building text classification models.
Inorder to be efficient, such objects of a for-mal model, like vectors, have to reasonably re-produce language-related phenomena such asword meaning inherent in index terms.
We in-troduce an algorithm for sense-based seman-tic ordering of index terms which approxi-mates Cruse?s description of a sense spectrum.Following semantic ordering, text classifica-tion by support vector machines can benefitfrom semantic smoothing kernels that regardsemantic relations among index terms whilecomputing document similarity.
Adding ex-pansion terms to the vector representation canalso improve effectiveness.
This paper pro-poses a new kernel which discounts less im-portant expansion terms based on lexical re-latedness.1 IntroductionGenerally, building an automated text classificationsystem consists of two key subtasks.
The first taskis text representation which converts the content ofdocuments into compact format so that they can befurther processed by the text classifiers.
Anothertask is to learn the model of a text classifier whichis used to classify the unlabeled documents.
Thispaper proposes a substantially new model for textrepresentation to improve effectiveness of text clas-sification by semantic ordering.Our motivation for the research presented herecame from (Dorrer et al, 2001) who demonstratedthe viability of database searching by visible lightusing a quantum algorithm, albeit on meaninglessitems.
The question was, what kind of documentrepresentation would be necessary to extend theirin-principle results to include semantics, one thathas been leading us to test both periodic and non-periodic functions for this purpose.
Since represen-tation and retrieval by colors was implied in theirmethod, we speculated that the following compo-nents could be useful in a rephrased model: (a)a metaphorically presented spectral expression oflexical semantic phenomena, (b) a ranked one-dimensional condensate of multidimensional sensestructure, and (c) representation of documents andqueries by functions in L2 space with a similaritymeasure.
Our anticipation was that by matchingthese components, a new model could demonstratenew capacities in general, and contribute to comput-ing meaning by waves in particular.Semantic ordering (component b) is an approxi-mation of what (Cruse, 1986) referred to as a sensespectrum, i.e.
a series of points - called local sensesand constituting lexical units -, in a one-dimensionalsemantic continuum (component a).
Apart from dif-ferentiating between the conceptual content of thesame word in terms of its senses in word pairs, i.e.their semantic relatedness, it also compresses theresult in spectral form.
The scalar values of thisspectrum have the double potential of being a con-densed measure for semantic weighting, and, ten-tatively, they can play the role of mass in experi-ments where gravity is called in as a metaphor fortext categorization and information retrieval (Paij-mans, 1997; Shi et al, 2005; Wittek et al, 2009).183This paper addresses text categorization by meansof non-periodical functions only.In support of Cruse?s point, recently it has beendemonstrated by measurements that sense classifi-cation errors made by their maximum entropy basedword sense disambiguation system were partlyremedied once instead of a fine-grained view, a morecoarse-grained view of senses was adopted (Palmeret al, 2006).
Improvement of sense classification ac-curacy linked with ?zooming out?
in terms of obser-vation granularity indicates, in our eyes, the ?fluid?,perhaps spectral nature of sense inasmuch as it isimpossible to precisely distinguish between the bor-derlines and some fuzziness is implied both in thephenomenon and its perception.
This ?fluidity oflanguage?, as Palmer et al call it, is in accordwith the theory of shared semantic representations inpsycholinguistics (Rodd et al, 2002), according towhich related senses share a portion of their mean-ing representation in the mental lexicon; it also sup-ports an earlier observation of two of the present au-thors based on the same methodology as outlined inthis paper, namely that using continuous functionsfor information retrieval leads to content representa-tion without exact term or document locations, onewhich is regional in its nature and subject to a math-ematical uncertainty principle (Wittek and Dara?nyi,2007).We approach our problem in three steps: (1)whether distributional semantics alone is enough forthe representation of word meaning, (2) whether se-mantic relatedness between word pairs can be ex-pressed in an ordered form while preserving lexicalfield structure, and if (3) the uniqueness of entries insuch an order can be expressed by functions ratherthan scalars such as distance.
As we will show, thisline of thought leads to performance improvementin text classification by using kernel-based featureweighting.Since the early days of the vector space model,it has been debated whether it is a proper carrierof meaning of texts (Raghavan and Wong, 1986),arguing if distributional similarity is an adequateproxy for lexical semantic relatedness (Budanitskyand Hirst, 2006).
We argue for the need to enrichdistributional semantics-based text representation byother components because with the statistical, i.e.devoid of word semantics approaches there is gen-erally no way to improve both precision and recallat the same time, increasing one is done at the ex-pense of the other.
For example, casting a wider netof search terms to improve recall of relevant itemswill also bring in an even greater proportion of ir-relevant items, lowering precision.
In the mean-time, practical approaches have been proliferating,especially with developments in kernel methods inthe last decade (Joachims, 1998; Cristianini et al,2002).
Some researchers suggested a more generalmathematical framework to accommodate the needsthat the vector space model cannot satisfy (van Rijs-bergen, 2004).
This paper explores the opportunitiesof this representation in the domain of text classifi-cation by introducing it as a new nonlinear semantickernel.Another aspect of the same problem is term ex-pansion for document classification and retrieval.By automatically selecting expansion terms for atext classification system to expand a document vec-tor by adding terms that are related to the termsalready in the document, performance can be im-proved (Hu et al, 2008).
Such new terms can ei-ther be statistically related to the original terms orchosen from lexical resources such as thesauri, con-trolled vocabularies, ontologies and the like.However, in doing so the fundamental questionoften overlooked is whether the expansion terms ex-tracted are equally related to the document and areuseful for text classification.
In what follows wepropose a form of term expansion with decreasingimportance of those terms that are less related, ascontrasted with rigid term expansion.
This can becarried out by a combination of semantic orderingand using function space for classification.This paper is organized as follows.
Section 2overviews text classification by support vector ma-chines, expanding on traditional text similarity mea-sures (Section 2.1), semantic smoothing kernels(Section 2.2), term expansion strategies (Section2.3), and finally introduces our semantic kernels inthe L2 space (Section 2.4).
Section 3 discusses ex-perimental results and Section 4 concludes the pa-per.1842 Text Classification with Support VectorMachinesText categorization is the task of assigning unlabeleddocuments into predefined categories.
Given a col-lection of {d1, d2, .
.
.
, dN} documents, and a C ={c1, c2, ..., c|C|} set of predefined categories, thetask is, for each document dj (j ?
{1, 2, .
.
.
, N}),to assign a decision to file dj under ci or a deci-sion not to file dj under ci (ci ?
C) by virtue ofa function ?, where the function ?
is also referredto as the classifier, or model, or hypothesis, or rule.Supervised text classification is a machine learningtechnique for creating the function ?
from trainingdata.
The training data consist of pairs of input doc-uments, and desired outputs (i.e., classes).Support vector machines have been found themost effective by several authors (Joachims, 1998).The proposed semantic text classification method isgrounded in the kernel methods underlying supportvector machines.A support vector machine is a kind of supervisedlearning algorithm.
In its simplest, linear form, asupport vector machine is a hyperplane that sepa-rates a set of positive examples from a set of nega-tive examples with maximum margin (Shawe-Taylorand Cristianini, 2004).
The strength of kernel meth-ods is that they allow a mapping ?(.)
of x to a higherdimensional space.
In the dual formulation of themathematical programming problem, only the ker-nel matrix K(xi,xj) = ?(xi)??
(xi) is needed inthe calculations.2.1 Traditional Text Similarity MeasureIntuitively, if a text fragment of two documents ad-dress similar topics, it is highly possible that theyshare lots of substantive terms.
After having re-moved the stopwords and stemmed the rest, thestemmed terms construct a vector representation foreach text document.
Let aj be a document vector inthe vector space model, that is, aj = ?Mk=1 akjek,where M is the number of index terms, akj is someweighting (e.g., term frequency), and ek is a basisvector of the M -dimensional Euclidean space.
Thisrepresentation is also referred to as the bag-of-words(BOW) model.Given this representation, semantic relatedness ofa pair of text fragments is computed as the cosinesimilarity of their corresponding term vectors whichis defined as:S(ai,aj) = aiaj|ai||a|j .
(1)2.2 Linear Semantic KernelsOne enrichment strategy is to use a semanticsmoothing kernel while calculating the similaritybetween two documents.
Any linear kernel for textsis characterized by K(ai,aj) = a?iS?Saj , whereS is an appropriately shaped matrix commonly re-ferred to as semantic smoothing matrix (Siolas andd?Alche?
Buc, 2000; Shawe-Taylor and Cristianini,2004; Basili et al, 2005; Mavroeidis et al, 2005;Bloehdorn et al, 2006).
The presence of S changesthe orthogonality of the vector space model, as thismapping should introduce term dependence.
A re-cent attempt tried to manually construct S with thehelp of a lexical resource (Siolas and d?Alche?
Buc,2000).
The entries in the symmetric matrix S ex-press the semantic similarity between the terms i andj.
Entries in this matrix are inversely proportionalto the length of the WordNet hierarchy path linkingthe two terms.
The performance, measured over the20NewsGroups corpus, showed an improvement of2 % over the the basic vector space method.
More-over, the semantic matrix S is almost fully dense,hence computational issues arise.
In a similar con-struction, (Bloehdorn et al, 2006) defined the ma-trix entries as weights of superconcepts of the twoterms in the WordNet hierarchy.
Focusing on specialsubcategories of Reuters-21578 and on the TRECQuestion Answering Dataset, they showed consis-tent improvement over the baseline.
As (Mavroei-dis et al, 2005) pointed out, polysemy will remaina problem in semantic smoothing kernels.
A morecomplex way of calculating the semantic similarityas the matrix entries was also proposed (Basili et al,2005).
For a more general discussion on semanticsimilarity see Section 2.4.1.An early attempt to overcome the untenable or-thogonality assumption of the vector space modelwas proposed under the name of generalized vec-tor space model (Wong et al, 1985).
The articlewhich proposed the model did not provide empiri-cal results, and since then the model has been re-garded of large theoretical importance with less im-pact on actual applications.
The model takes a distri-185butional approach, focusing on term co-occurrences.The underlying assumption is that term correlationsare captured by the co-occurrence information.
Thatis, two terms are semantically related if they co-occur often in the same documents.
By eliminat-ing orthogonality, documents can be seen as similareven if they do not share any terms.
The term co-occurrence matrix is AA?, hence the model takes A?as the semantic similarity matrix S. A major draw-back of the generalized vector space model is that itreplaces the orthogonality assumption with anotherquestionable assumption.
The computational needsare tremendous too, if the dimensions of A are con-sidered.
Moreover, the co-occurrence matrix is notsparse anymore.Latent semantic indexing (or latent semantic anal-ysis) was another attempt to bring more linguis-tic and psychological aspects to language process-ing via a kernel.
Conceptually, latent semantic in-dexing is similar to the generalized vector spacemodel, it measures semantic information throughco-occurrence analysis in the corpus.
From the al-gorithmic perspective it is an enormous problem thattextual data have a large number of relevant fea-tures.
This results in huge computational needs andthe classification models may overfit the data.
Thenumber of features can be reduced by multivariatefeature extraction methods.
In latent semantic in-dexing, the dimension of the vector space is reducedby singular value decomposition (Deerwester et al,1990).Using rank reduction, terms that occur togethervery often in the same documents are merged intoa single dimension of the feature space.
The di-mensions of the reduced space correspond to theaxes of greatest variance.
For latent semantic in-dexing, by dual representation the kernel matrix isK = V ?2kV ?, where ?k is a diagonal matrix con-taining the k largest singular values of the singu-lar value decomposition of the vector space, and Vholds the right singular vectors of the decomposi-tion.
The new kernel matrix can be obtained directlyfrom K by applying an eigenvalue decompositionof K (Cristianini et al, 2002).
The computationalcomplexity of performing an eigenvalue decompo-sition on the kernel matrix is a major drawback oflatent semantic indexing.2.3 Text Representation Enrichment Strategiesby Term ExpansionIn order to eliminate the bottleneck of the traditionalBOW representation, previous approaches in termexpansion enriched this convention by external lexi-cal resources such as WordNet.As a first step, these methods generate new fea-tures for each document in the dataset.
These newfeatures can be synonyms or homonyms of docu-ment terms as in (Hotho et al, 2003; Rodriguezand Hidalgo, 1997), or expanded features for terms,sentences and documents as in (Gabrilovich andMarkovitch, 2005), or term context information forword sense disambiguation such as topic signatures(Agirre and De Lacalle, 2003; Agirre et al, 2004).Then, the generated new features replace the oldones or are appended to the document representa-tion, and construct a new vector representation a?ifor each text document.
The similarity measure ofdocument pairs is defined as:S(a?i, a?j) = a?ia?j|a?i||a?j | .
(2)2.4 Our FrameworkThe basic assumption of our framework is that termscan be arranged in an order such that consecutiveterms are semantically related.
Hence each term ac-quires a unique position, and this position ties theterm to its semantically related neighbors.
However,given a BOW representation with a cosine similaritymeasure, this position would not improve classifica-tion performance.
Therefore we suggest to associatea mathematical function with each term, thus map-ping terms and documents to theL2 space, and usingthe inner product of this space to express similar-ity.
The choice of function will determine to whichextent neighboring terms, i.e., the enriching terms,are considered in calculating the similarity betweentwo documents.
This section first introduces an al-gorithm that produces the aforementioned semanticorder, then the semantic kernels in the L2 space arediscussed.2.4.1 An Algorithm for a Semantic Ordering ofTermsThe proposed kernels assume that there is a se-mantic order between terms.
Let V denote a set of186terms {t1, t2, .
.
.
, tn} and let d(ti, tj) denote the se-mantic distance between the terms ti and tj .
Theinitial order of the terms is not relevant, though it isassumed to be alphabetic.
Let G = (V,E) denotea weighted undirected graph, where the weights inthe set E are defined by the distances between theterms.Various lexical resource-based (Budanitsky andHirst, 2006) and distributional measures (Moham-mad and Hirst, 2005) have been proposed to mea-sure semantic relatedness and distance betweenterms.
Terms can be corpus- or genre-specific.
Man-ually constructed general-purpose lexical resourcesinclude many usages that are infrequent in a partic-ular corpus or genre of documents.
For example,one of the 8 senses of company in WordNet is avisitor/visitant, which is a hyponym of person (Lin,1998).
This sense of the term is practically neverused in newspaper articles, hence distributional at-tributes should be taken into consideration.
Com-posite measures that combine the advantages of bothapproaches have also been developed (Resnik, 1995;Jiang and Conrath, 1997).
This paper relies on theJiang-Conrath composite measure (Jiang and Con-rath, 1997), which has been shown to be superiorto other measures (Budanitsky and Hirst, 2006), andwe also found that this measure works the best forthe purpose.
The Jiang-Conrath metric measuresthe distance between two senses by using the hier-archy of WordNet.
By denoting the lowest super-ordinate of two senses s1 and s2 in the hierarchywith LSuper(s1,s2), the metric is calculated as fol-lows:d(s1, s2) = IC(s1)+IC(s2)?2IC(LSuper(s1, s2)),where IC(s) is the information content of a senses based on a corpus.
Distance between two termsis calculated according to the following equation:d(t1, t2) = maxs1?sen(t1),s2?sen(t2) d(s1, s2), wheret1 and t2 are two terms, and sen(ti) is the set ofsenses of ti.
The distance between two terms isusually defined as the minimum of the sense dis-tances.
We chose maximum because it ensures thatonly closely related terms will be placed to adjacentpositions by the algorithm below.Finding a semantic ordering of terms can be trans-lated to a graph problem: a minimum-weight Hamil-tonian path G?
of G gives the ordering by readingthe nodes from one of the paths to the other.
G isa complete graph, therefore such a path always ex-ists, but finding it is an NP-complete problem.
Thefollowing greedy algorithm is similar to the nearestneighbor heuristic for the solution of the travelingsalesman problem.
It creates a graph G?
= (V ?, E?
),where V ?
= V and E?
?
E. This G?
graph is aspanning tree of G in which the maximum degree ofa node is two, that is, the minimum spanning tree isa path between two nodes.Step 1 Find the term at the highest stage of the hi-erarchy in a lexical resource.ts = argminti?V depth(ti).This seed term is the first element of V ?, V ?
={ts}.
Remove it from the set V :V := V \{ts}.Using WordNet, this seed term is entity, if thevocabulary of the text collection contains it.Step 2 Let tl denote the leftmost term of the order-ing and tr the rightmost one.
Find the next twoelements of the ordering:t?l = argminti?V d(ti, tl),t?r = argminti?V \{t?l}d(ti, tr).Step 3 If d(tl, t?l) < d(tr, t?r) then add t?l to V ?,E?
:= E?
?
{e(tl, t?l)}, and V := V \{t?l}.Else add t?r to V ?, E?
:= E?
?
{e(tr, t?r)} andV := V \{t?r}.Step 4 Repeat from Step 2 until V = ?.The above algorithm can be thought of as a modi-fied Prim?s algorithm, but it does not find the optimalminimum-weight spanning tree.2.4.2 Semantic Kernels in the L2 SpaceThe L2 space shares resemblance with a realvector space.
Real-valued vectors are replaced bysquare-integrable functions, and the dot product isreplaced by the following inner product: (fi, fj) =?fifjdx, for some fi, fj in the given L2 space.Lately, Hoenkamp has also pointed out that theL2 space can be used for information retrieval when187he introduced a Haar basis for the document space(Hoenkamp, 2003).
He utilized a signal processingframework within the context of latent semantic in-dexing.
In order to apply an L2 representation fortext classification, the problem is approached from adifferent angle than by Hoenkamp, taking discount-ing expansion terms as our point of departure.Assigning a function w(x ?
k) to the term in thekth position in a semantic order, a document j canbe expressed as follows:fj(x) =M?k=1akjw(x?
k), (3)where x is in [1,M ], and it is the variable of inte-gration in calculating the inner product of the L2;x can be regarded as a ?dummy?
variable carryingno meaning in itself.
The above formula will be re-ferred to as a document function.
In the experiments,the function exp(?bx2) was used as w(x), with b asa free parameter reflecting the width of the functionexpressing how many neighboring expansion termsare considered.The inner product of theL2[1,M ] space is appliedto express similarity between two documents in sim-ilar vein as the dot product does in a real-valued vec-tor space:(fi, fj) =?
[1,M ]fi(x)fj(x)dx, (4)where fi and fj are the representations of the docu-ments in the L2 space (fi, fj ?
L2([1,M ])).0.511.52brand brandname trade nameFigure 1: Two documents with matching term brandname.
Dotted line: Document-1.
Dashed line:Document-2.
Solid line: Their product.With the above formula, a matching term in twodocuments will be counted to its full term frequencyor tfidf score, while semantically related terms willbe counted less and less according their semanticsimilarity to the matching term.
Assuming that theterms brand, brand name, and trade name followeach other in the semantic order, consider the fol-lowing example.
The first document has the termbrand name, and so does the second document.
InFigure 1, it can be seen brand name is counted thesame way as it would be in a BOW model with itsfull term frequency score, brand and trade name arecounted to a lesser extent, while other related termsare considered even less.0.511.52brand brandname trade nameFigure 2: Two documents with no matching term butwith related terms brand and trade name.
Dotted line:Document-1.
Dashed line: Document-2.
Solid line:Their product.Now if the two documents do not share the exactterm, only related terms occur, for instance, tradename and brand, respectively, then the term brandname, placed between trade name and brand in thes semantic order, will be considered only to someextent for the calculation of similarity (see Figure2).3 Experimental ResultsThe most widely used benchmark corpus is theReuters-21578 collection.
For benchmarking pur-poses, the ModApte split was adopted.
9603 doc-uments were used as the training set and 3299 as thetest set in the experiments.
Only those ninety textcategories which had at least one positive examplein the training set were included in the benchmark.Another benchmark data corpus we used was the 20188Newsgroups corpus, which is a collection of approx-imate 20,000 newsgroup documents nearly evenlydivided among 20 discussion groups and each doc-ument is labeled as one of the 20 categories corre-sponding to the name of the newsgroup that the doc-ument was posted to.In preparing the index terms, we restricted the vo-cabulary to the terms of WordNet 3.0 in order to beable to calculate the similarity score between anytwo terms.
Stop words were removed in advance.Multiple word expressions were used to fully utilizeWordNet.
We used the built-in stemmer of WordNet,which is able to distinguish between different parts-of-speeches if the form of the word is unambiguous.For example, {accommodates, accommodated, ac-commodation} was stemmed to {accommodate, ac-commodate, accommodation}.
We used term fre-quency as term weighting.Prior to the semantic ordering, terms were as-sumed to be in alphabetic order.
Measuring theJiang-Conrath distance between adjacent terms, theaverage distance was 1.68 on the Reuters corpus.Note that the Jiang-Conrath distance was normal-ized to the interval [0, 2].
There were few terms withzero or little distance between them.
This is due toterms which are related and start with the same wordor stem.
For example, account, account executive,account for, accountable.The same average distance after reordering theterms with the proposed algorithm and the Jiang-Conrath distance was 0.56 on the same corpus.About one third of the terms had very little distancebetween each other.
Nevertheless, over 10 % of thetotal terms still had the maximum distance.
This isdue to the non-optimal nature of the proposed term-ordering algorithm.
These terms add noise to theclassification.
The noisy terms occur typically at thetwo sides of the scale, that is, the leftmost terms andthe rightmost terms.
While it is easy to find closeterms in the beginning, as the algorithm proceeds,fewer terms remain in the pool to be chosen.
For in-stance, brand, brand name, trade name, label are inthe 33rd, 34th, 35th and 36th position on the left sidecounting from the seed respectively, while windy,widespread, willingly, whatsoever, worried, worth-while close the left side, apparently sharing little incommon.
The noise can be reduced by the appropri-ate choice of the parameter b in exp(?bx2), so thatKernel Reuters Reuters 20News 20NewsMicro Macro Micro MacroLinear 0.900 0.826 0.801 0.791Poly 0.903 0.824 0.796 0.788L2 0.911 0.835 0.813 0.799Table 1: Micro- and macro-average F1 resultsthe impact of adjacent but distantly related terms canbe minimized.Table 1 shows the results on the two benchmarkcorpora with the baseline linear kernel.
Precisionand recall with regard to a class ck, the F1 scoreshown is their average.
For all the kernels, the resultswith the best parameter settings are shown.
Polyno-mial kernels were benchmarked between degrees 2and 5.
L2 kernels were benchmarked with width bbetween 1 and 8, the performance peaking at 4 inall cases.
The model is able to outperform the base-line kernels, and the differences in micro-averagedresults are statistically significant.
In all cases of theL2 kernel, the increase of F1 was due the increase inboth precision and recall.4 ConclusionsInformation systems are in great need of automatedintelligent tools, but existing algorithms and meth-ods cannot be pushed much further.
Most tech-niques in current use are impaired by the semanti-cally poor but widespread representation of informa-tion and knowledge.
For this reason, we propose anew formalism that combines Cruse?s idea about asense spectrum, approximated by semantic ordering,and its calculation by functions.The suggested model combines term expansionwith the semantic relations and semantic relatednessused in semantic smoothing kernels.
This slightlyunusual approach needs to transform the real vectorrepresentation to the L2 space, and the experimen-tal results show that this new representation can im-prove text classification effectiveness.Our new model also blends insights from differ-ent approaches to lexical semantics theory at its dif-ferent levels.
First, during the semantic orderingof terms the distributional hypothesis meets hand-crafted lexical resources of word meaning that relateto term occurrences as if they were their referents,189a component external to term context.
While high-quality lexical resources enable such an ordering inthemselves, the procedure can benefit from data de-rived from the specific corpora in study ?
seman-tic relatedness measures such as the Jiang-Conrathsimilarity operate this way.
Secondly, once the or-dering is done and a sense spectrum is constructed,weights expressing statistical relationships betweenterms and documents are borrowed from the vectorspace model to form the basis for constructing hypo-thetical signals of content, documents as continuousfunctions.5 Future researchFigure 3: A hypothetical spectrum of terms.As we have shown, a spectral interpretation ofsense granularity can lead to improved text catego-rization results by utilizing L2 space for informa-tion representation.
Whether non-periodic functionsother than the variant tested in this paper can be ap-plied to the same end needs to be explored.Turning back to the use of the spectrum of visi-ble light for representing meaning, this raises newresearch questions.
On the one hand, translat-ing one-dimensional semantic ordering into colorsis straightforward.
Consider the following map-ping.
Assume that a language has a finite N num-ber of terms, so the 1-dimensional result is an or-dered list o1, o2, .
.
.
, oN .
Calculate the following:?
= ?N?1i=1 d(oi, oi+1), where ?
is the sum ofdistances between consecutive words.
Further letF : [0,?]
?
[400, 700] be the following map-ping: F (x) = 400 + x300?
.
The visible spectrumis between 400 and 700 nm, F maps the cumulativedistances of terms from [0,?]
to the visible spec-trum congruently, i.e.
without distorting the dis-tances.
With this bijective (one-to-one) mapping,each term is assigned a physical wavelength and fre-quency.
Figure 3 shows an example of such a termspectrum.On the other hand, we have only begun to test theapplicability of periodic functions in L2 space (Wit-tek and Dara?nyi, 2007), hence a well-establishedlink to semantic computing by waves is missing forthe time being.
A possible compromise between thenon-periodic vs. periodic approaches can be to ap-ply wavelets instead of waves, a direction where ourongoing research shows promising results.
Thesewill be reported elsewhere.
In a broader frame ofthought, we are also working on the optical equiv-alents of the vector space model and the general-ized vector space model as a first step toward codingmore semantics in mathematical objects, and puttingthem to work in novel computing environments.6 AcknowledgmentsThe authors are grateful to Martha Palmer (Univer-sity of Colorado Boulder) for her inspiring sugges-tions and advice on sense granularity.ReferencesE.
Agirre and O.L.
De Lacalle.
2003.
Clustering Word-Net word senses.
In Proceedings of RANLP-03, 4thInternational Conference on Recent Advances in Nat-ural Language Processing, pages 121?130.E.
Agirre, E. Alfonseca, and O.L.
de Lacalle.
2004.
Ap-proximating hierarchy-based similarity for WordNetnominal synsets using topic signatures.
In Proceed-ings of GWC-04, 2nd Global WordNet Conference,pages 15?22.R.
Basili, M. Cammisa, and A. Moschitti.
2005.
Effec-tive use of WordNet semantics via kernel-based learn-ing.
In Proceedings of CoNLL-05, 9th Conference onComputational Natural Language Learning, pages 1?8.S.
Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti.2006.
Semantic kernels for text classification based ontopological measures of feature similarity.
Proceed-ings of ICDM-06, 6th IEEE International Conferenceon Data Mining.A.
Budanitsky and G. Hirst.
2006.
Evaluating WordNet-based measures of lexical semantic relatedness.
Com-putational Linguistics, 32(1):13?47.190N.
Cristianini, J. Shawe-Taylor, and H. Lodhi.
2002.
La-tent semantic kernels.
Journal of Intelligent Informa-tion Systems, 18(2):127?152.D.A.
Cruse.
1986.
Lexical semantics.S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Landauer,and R. Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society for Informa-tion Science, 41(6):391?407.C.
Dorrer, P. Londero, M. Anderson, S. Wallentowitz, andIA Walmsley.
2001.
Computing with interference:all-optical single-query 50-element database search.In Proceedings of QELS-01, Quantum Electronics andLaser Science Conference, pages 149?150.E.
Gabrilovich and S. Markovitch.
2005.
Feature gen-eration for text categorization using world knowledge.In Proceedings of IJCAI-05, 19th International JointConference on Artificial Intelligence, volume 19.E.
Hoenkamp.
2003.
Unitary operators on the documentspace.
Journal of the American Society for Informa-tion Science and Technology, 54(4):314?320.A.
Hotho, S. Staab, and G. Stumme.
2003.
WordNetimproves text document clustering.
In Proceedings ofSIGIR-03, 26th ACM International Conference on Re-search and Development in Information Retrieval.J.
Hu, L. Fang, Y. Cao, H.J.
Zeng, H. Li, Q. Yang, andZ.
Chen.
2008.
Enhancing text clustering by lever-aging Wikipedia semantics.
In Proceedings of SIGIR-08, 31st ACM International Conference on Researchand Development in Information Retrieval, pages 179?186.J.J.
Jiang and D.W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProceedings of the International Conference on Re-search in Computational Linguistics, pages 19?33.T.
Joachims.
1998.
Text categorization with support vec-tor machines: Learning with many relevant features.In Proceedings of ECML-98, 10th European Confer-ence on Machine Learning, pages 137?142.D.
Lin.
1998.
Automatic retrieval and clustering of simi-lar words.
In Proceedings of COLING-ACL Workshopon Usage of WordNet in Natural Language ProcessingSystems, volume 98, pages 768?773.D.
Mavroeidis, G. Tsatsaronis, M. Vazirgiannis,M.
Theobald, and G. Weikum.
2005.
Word sensedisambiguation for exploiting hierarchical thesauriin text classification.
Proceedings of PKDD-05,9th European Conference on the Principles of DataMining and Knowledge Discovery, pages 181?192.S.
Mohammad and G. Hirst.
2005.
Distributional mea-sures as proxies for semantic relatedness.H.
Paijmans.
1997.
Gravity wells of meaning: detectinginformation-rich passages in scientific texts.
Journalof Documentation, 53(5):520?536.M.
Palmer, H.T.
Dang, and C. Fellbaum.
2006.
Mak-ing fine-grained and coarse-grained sense distinctions,both manually and automatically.
Natural LanguageEngineering, 13(02):137?163.V.V.
Raghavan and S.K.M.
Wong.
1986.
A critical anal-ysis of vector space model for information retrieval.Journal of the American Society for Information Sci-ence, 37(5):279?287.P.
Resnik.
1995.
Using information content to evaluatesemantic similarity in a taxonomy.
In Proceedings ofIJCAI-95, 14th International Joint Conference on Ar-tificial Intelligence, volume 1, pages 448?453.J.
Rodd, G. Gaskell, and W. Marslen-Wilson.
2002.Making sense of semantic ambiguity: Semantic com-petition in lexical access.
Journal of Memory and Lan-guage, 46(2):245?266.M.D.E.B.
Rodriguez and J.M.G.
Hidalgo.
1997.
UsingWordNet to complement training information in textcategorisation.
In Procedings of RANLP-97, 2nd In-ternational Conference on Recent Advances in NaturalLanguage Processing.J.
Shawe-Taylor and N. Cristianini.
2004.
Kernel Meth-ods for Pattern Analysis.S.
Shi, J.R. Wen, Q. Yu, R. Song, and W.Y.
Ma.
2005.Gravitation-based model for information retrieval.
InProceedings of SIGIR-05, 28th ACM InternationalConference on Research and Development in Informa-tion Retrieval, pages 488?495.
ACM New York, NY,USA.G.
Siolas and F. d?Alche?
Buc.
2000.
Support vector ma-chines based on a semantic kernel for text categoriza-tion.
In Proceedings of IJCNN-00, IEEE InternationalJoint Conference on Neural Networks.C.
J. van Rijsbergen.
2004.
The Geometry of InformationRetrieval.P.
Wittek and S. Dara?nyi.
2007.
Representing wordsemantics for IR by continuous functions.
In S. Do-minich and F. Kiss, editors, Proceedings of ICTIR-07,1st International Conference of the Theory of Informa-tion Retrieval, pages 149?155.P.
Wittek, C.L.
Tan, and S. Dara?nyi.
2009.
An or-dering of terms based on semantic relatedness.
InH.
Bunt, editor, Proceedings of IWCS-09, 8th Inter-national Conference on Computational Semantics.S.K.M.
Wong, W. Ziarko, and P.C.N.
Wong.
1985.
Gen-eralized vector space model in information retrieval.In Proceedings of SIGIR-85, 8th ACM InternationalConference on Research and Development in Informa-tion Retrieval, pages 18?25.191
