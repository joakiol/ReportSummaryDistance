Towards semi-automatic methods for improving WordNetNervo VerdezotoUniversity of Trento &LOA-ISTC-CNR, Trentonverdezoto@gmail.comLaure VieuIRIT-CNRS, Toulouse &LOA-ISTC-CNR, Trentovieu@irit.frAbstractWordNet is extensively used as a major lexical resource in NLP.
However, its quality is far fromperfect, and this alters the results of applications using it.
We propose here to complement previousefforts for ?cleaning up?
the top-level of its taxonomy with semi-automatic methods based on thedetection of errors at the lower levels.
The methods we propose test the coherence of two sources ofknowledge, exploiting ontological principles and semantic constraints.1 IntroductionWordNet (Princeton WordNet (Fellbaum, 1998), henceforth WN) is a lexical resource widely used ina host of applications in which language or linguistic concepts play a role.
For instance, it is a cen-tral resource for the quantification of semantic relatedness (Budanitsky and Hirst, 2006), in turn oftenexploited in applications.
The quality of this resource therefore is very important for NLP as a whole,and beyond, in several AI applications.
Neel and Garzon (2010) show that the quality of a knowledgeresource like WN affects the performance in recognizing textual entailment (RTE) and word-sense dis-ambiguation (WSD) tasks.
They observe that the new version of WN induced improvements in recentRTE challenges, but conclude that WN currently is not rich enough to resolve such a task.
What is more,its quality may be too low to even be useful at all.
Bentivogli et al (2009) discuss the results1 of 20?ablation tests?
on systems submitted to the main RTE-5 task in which WN (alone) was ablated: 11 ofthese tests demonstrated that the use of this resource has a positive impact (up to 4%) on the performanceof the systems but 9 showed a negative (up to 2% improvement when ablated) or null impact.In the area of automatic recognition of part-whole relations, Girju and Badulescu (2006) proposeda learning method relying on WN?s taxonomy.
Analyzing the classification rules obtained, we couldsee that WN taxonomical errors lead to absurd rules, which can explain wrong recognition results.
Forinstance, the authors obtain pairs such as ?shape, physical phenomenon?
and ?atmospheric phenomenon,communication?
as positive constraints for part-whole recognition, while sentences like a curved shapeis part of the electromagnetic radiation or rain is part of this document would make no sense.Some semantic problems of WN are well-known: confusion between concepts and individuals (inprinciple solved since WN 2.1), heterogeneous levels of generality, inappropriate use of multiple in-heritance, confounding and missing senses, and unclear glosses (Kaplan and Schubert, 2001; Gangemiet al, 2003; Clark et al, 2006).
Nevertheless, the number of applications where WN is used as an on-tology has been increasing.
In fact, apart from the synonymy relation on which synsets are defined, thehyponymy/hypernymy relation is WN?s semantic relation most exploited in applications; it generatesWN?s taxonomy, which can be seen as a lightweight ontology, something it was never designed for,though.
Several works tried to address these shortcomings.
Gangemi et al (2003) proposed a manualrestructuring through the alignment of WN?s taxonomy and the foundational ontology DOLCE2, but thisrestructuring just focused on the upper levels of the taxonomy.
Applying formal ontology principles1http://www.aclweb.org/aclwiki/index.php?title=RTE5_-_Ablation_Tests2See (Masolo et al, 2003) and http://www.loa-cnr.it/DOLCE.html275(Guarino, 1998) and the OntoClean methodology (Guarino and Welty, 2004) have also been suggestedfor manually ?cleaning up?
the whole resource.
This however is extremely demanding, because thephilosophical principles involved require a deep analysis of each concept, and as a result, is unlikely tobe achieved in a near future.
Clark et al (2006) also gave some general suggestions as design criteria fora new WN-like knowledge base and recommended that WN should be cleaned up to make it logicallycorrect, but did not provide any practical method for doing so.
Two other more extensive works relyon manual interventions, either the mapping of each synset in WN to a particular concept in the SUMOontology (Pease and Fellbaum, 2009), or the tagging of each synset in WN with ?features?
from theTop Concept Ontology (Alvez et al, 2008) to substitute or contrast the original WN taxonomy.
Suchapproaches are clearly very costly, as each synset needs to be examined.
In addition, the ontologicalvalue of these additional resources themselves remains to be proven.
The method used in (Alvez et al,2008) has though helped pointing out a large number of errors in WN 1.6.Our purpose in this paper is to show that automatic methods to spot errors, especially in the lowerlevels of WN?s taxonomy, can be developed.
Spotting errors can then efficiently direct the manualcorrection task.
Such methods could be used to complement a manual top-level restructuring and couldbe seen as an alternative to fully manual approaches, which are very demanding and in principle requirevalidation between experts.
Here, we explore methods based on internal coherence checks within WN,or on checking the coherence between WN and annotated corpora such as those of Semeval-2007 Task 4(Girju et al, 2007).The paper is structured as follows: Section 2 presents the data used and the methodology; Section 3discusses the results; Section 4 concludes, exploring how the method could be extended and applied.2 MethodologyTo spot errors inWN, our basic idea is to contrast two sources of knowledge and automatically check theircoherence.
Here, we contrast part-whole data with WN taxonomy structure, on the basis of constraintsstemming from the semantics of the part-whole relations and ontological principles.
The part-whole dataused is taken either from the meronymy/holonymy relations of WN or from available annotated corpora.An incoherence between two sources of knowledge may be caused by an error in either one (or both).Contrasting part-whole data with the taxonomy will indeed help detecting errors in the taxonomy ?themost numerous?
but errors are also found in the part-whole data itself (see Section 3.3).2.1 Extracting the DatasetWe started extracting WN taxonomy from the hypernym relations in the current version of WN (3.0), anetwork of 117,798 nouns grouped in 82,155 synsets.
We also extracted WN meronymy relations, i.e.,22,187 synset pairs, split into 12,293 ?member?, 9,097 ?part?
and 797 ?substance?, to constitute the firstpart-whole dataset.
In order to replicate our methodology, we also extracted 89 part-whole relation wordpairs annotated with WN senses from the SemEval-2007 Task 4 datasets (Girju et al, 2007).
We kept thepositive examples from the training and test datasets,3 excluding redundant pairs, and correcting a coupleof errors.
This data is also annotated with the meronymy sub-relations inspired from the classification ofWinston et al (1987), but five subtypes instead of WN?s three, although ?member-collection?
can safelybe assumed to correspond to WN?s ?member?
meronymy.
We will call this sub-relation Member, be itfrom WN or from SemEval.We also tried to get similar datasets from the SemEval-2010 Task 8 but, not being annotated withWN senses, they are useless for our purposes.
Figure 1 illustrates a WN-extracted meronymy pair fromour corpus4, encoded in our own xml format.
Synsets are presented with the standard WN sense keys foreach word, the recommended reference for stability from one WN release to another.53http://nlp.cs.swarthmore.edu/semeval/tasks/task04/data.shtml4Available at http://www.loa-cnr.it/corpus/corpus.tar.gz5A sense key combines a lemma field and several codes like the synset type and the lexicographer id.
See http://276<pair relationOrder=?
(e1, e2)?
comment=?meronym part?
source=?WordNet-3.0?><e1 synset=?head%1:06:04?
isInstance=?No?><hypernym>{obverse%1:06:00}.
.
.{surface%1:06:00}.
.
.
{artifact%1:03:00 }.
.
.
{physical object%1:03:00}{entity%1:03:00}</hypernym></e1><e2 synset=?coin%1:21:02?
isInstance=?No?><hypernym>.
.
.
{metal money%1:21:00}{currency%1:21:00}.
.
.
{quantity%1:03:00}{abstract entity%1:03:00}{entity%1:03:00}</hypernym></e2></pair>Figure 1: Example pair from the annotated dataset2.2 The Tests2.2.1 Ontological constraintsThe semantics of the part-whole relation on which the meronymy/holonymy relations are founded in-volves ontological constraints: in short, the part and the whole should be of a similar nature.
Studiesin Mereology show that part-whole relations occur on all sub-domains of reality, concrete or abstract(Simons, 1987; Casati and Varzi, 1999).
As a few cognitively oriented works explicitly state, the partand the whole should nevertheless belong to the same subdomain (Masolo et al, 2003; Vieu and Aur-nague, 2007).
Other work, e.g., the influential (Winston et al, 1987), more or less implicitly exploit thishomogeneity constraint.
Our tests examine and compare the nature of the part and the whole in attestedexamples of meronymy, looking for incoherences.
Here we use only a few basic ontological distinctions,namely, the distinction between:?
endurants (ED) or physical entities (like a dog, a table, a cave, smoke),?
perdurants (PD) or eventualities (like a lecture, a sleep, a downpour), and?
abstract entities (AB?
like a number, the content of a text, or a time).These are only three of the four topmost distinctions in DOLCE (Masolo et al, 2003), that is, we actuallygroup qualities (Q, the fourth top-level category) into abstract entities here.Tests 1?3 are directly aimed at detecting ontological heterogeneity in meronymy pairs that mix thethree categories ED, PD and AB, as just explained.
The tests are queries on our corpus to extract andcount meronymy pairs (pairs of synsets of the form ?e1,e2?
where e1 is the part and e2 is the whole)that involve an ontological heterogeneity.
Test 1 focuses on pairs mixing endurants and abstract entities(pairs of type ?ED,AB?
or ?AB,ED?
), Test 2 on endurants and perdurants (?ED,PD?
or ?PD,ED?)
and Test3 on perdurants and abstract entities (?PD,AB?
or ?AB,PD?
).However, WN 3.0?s top-level is not as simple as DOLCE?s, so to recover the three basic categorieswe had to group several classes from different WN branches.
In particular perdurants are found bothunder physical entity%1:03:00 (process%1:03:00) and under abstraction%1:03:00 (event%1:03:00 andstate%1:03:00).
The map we first established was then as follows:?
ED = physical entity%1:03:00 \ process%1:03:00;?
PD = process%1:03:00 ?
event%1:03:00 ?
state%1:03:00;?
AB = abstraction%1:03:00 \ (event%1:03:00 ?
state%1:03:00).Since all groups in WordNet are under abstraction%1:03:00 irrespective of the nature of the members,it was obvious from the start that most ?member?
meronymy pairs would be caught by Tests 1 or 3.
Thisis the reason why groups were actually removed from AB so the final map posited:?
AB = abstraction%1:03:00 \ (event%1:03:00 ?
state%1:03:00 ?
group%1:03:00).wordnet.princeton.edu/wordnet/documentation/2772.2.2 Semantic constraintsTwo more tests were designed to check basic semantic constraints involved in meronymy relations.Test 0 is related to the problem of confusion between classes and individuals evoked above andchecks for meronymy pairs between an individual and a class.
Meronymy in WN applies to pairs ofclasses and to pairs of individuals, but mixed pairs are also found, either between a class and an individualor between an individual and a class.
The semantics of WN meronymy is not precisely described inFellbaum (1998), but observing the data, the following appears to fit the semantics of ?is a meronym of?between two classes A and B: the disjunction of the formulas ?for all/most instances a of A, there is aninstance b ofB such that P (a, b)?
and ?for all/most instances b ofB, there is an instance a ofA such thatP (a, b)?, where P is the individual-level part-whole relation.
On this basis, a meronymy between a classA and an individual b would simply mean: ?for all/most instances a of A, P (a, b)?, while a meronymybetween an individual a and a class B would mean: ?for all/most instances b of B, P (a, b)?.
The formercan make sense, cf.
?sura%1:10:00, koran%1:10:00?
(all suras are part of the Koran).
However, the latterwould imply that all (most) instances of the class would share a same part, i.e., they would overlap.
Thatthe instances of a given class all overlap is of course not logically impossible, but it is highly unlikely forlexical classes.
The purpose of Test 0 is to check for such cases, expected to reveal confusion betweenindividuals and classes, that is, errors remaining after the introduction of the distinction in WN 2.1.6Test 4 is dedicated to the large number of Member pairs in WN and SemEval data, somehow disre-garded by the removal of groups from AB above.
The semantics of this special case of meronymy clearlyindicates that the whole denotes some kind of group, e.g., a collection or an organization, and that the partis a member of this group (Winston et al, 1987; Vieu and Aurnague, 2007).
Group concepts in WN arehyponyms of group%1:03:00.
A last coherence check, done by Test 4, thus extracts the Member pairs inwhich the whole is not considered a group because it is not an hyponym (or instance) of group%1:03:00.3 Results, Analysis and DiscussionTable 1: Number of pairs extracted by the testsError Category Test WordNet SemEval0 349 1.57% 0 0%Semantic4 550 4.47% 7 7.87%1 163 1.62% 2 2.78%Ontological2 45 0.45% 2 2.78%3 108 1.07% 0 0%The number of pairs extracted by our queries are summarized on Table1.
The error rates are quitelow, ranging from 0 to 7.87% depending on the data set of meronymy pairs (WN or SemEval).
Thehighest error rate is provided by Test 4: 550 (4.47%) of the 12,293 WN Member pairs and 7 (7.87%)of 19 Member pairs in SemEval dataset were identified as semantic errors because the whole is not agroup in WN taxonomy.
Test 0 has the lowest rate, just 349 (1.57%) of 22,187 WN meronymy pairsare suspected of confusing classes and individuals.
More important than the error rate is that the testsachieved maximal precision.
After manual inspection of all the suspect pairs extracted, it turns out all thepairs indeed suffered from some sort of error or another.
Of course, the few tests proposed here cannotaim at spotting all the taxonomy errors in WN, i.e., recall surely is low, but their precision is a proof ofthe effectiveness of the method proposed, which can be extended by further tests to uncover more errors.For Tests 1?3, since the three categories ED, PD and AB are large and diverse, the analysis of theerrors started with looking for regularities among the taxonomic chains of hypernyms of the synsets in6Another, very simple and superficial test could be to check synsets for names with capital letters.
This of course doesn?trely on ontological knowledge.278the pairs.
In particular, we looked for taxonomic generalizations of sets of pairs to divide the results inmeaningful small sets.
These sets were manually examined in order to check the intended meaning of themeronymy relations and determine the possible problems, either in the taxonomy or in the meronymy;for this we used all the information provided by WordNet as synset, synonymy, taxonomy, and glosses.For Tests 0 and 4, similar regularities could be observed.
Several regularities denote a few systematicerrors relatively easily solved using standard ontological analysis, described in the Sections 3.1?3.5.3.1 Confusion between class and groupSeveral individual collections e.g., new testament%1:10:00 , organizations e.g., palestine liberationorganization%1:14:00, and genera e.g., genus australopithecus%1:05:00 are considered as classes inWN instead of groups (errors extracted with Test 0).
The first example, new testament%1:10:00, isglossed as ?the collection of books ...?, but is not considered as an instance of group, it is a subclass ofdocument%1:10:00.7 The latter two are seen as subclasses instead of instances of group; this wouldmean that all instances of palestine liberation organization%1:14:00 (whatever these could be) andall instances of genus australopithecus%1:05:00 (which makes more sense) actually are groups.
Butif there are instances of the genus Australopithecus at all, these are individual hominids, not groups.In fact, the hesitation of the lexicographer is visible here, since lucy%1:05:00 is both a Member ofgenus australopithecus%1:05:00 and an instance of australopithecus afarensis%1:05:00, a subclass ofhominid%1:05:00 (not of group).
To show further the confusion here, australopithecus afarensis%1:05:00 itself also is a Member of genus australopithecus%1:05:00, which, with the semantics of Memberbetween classes, would mean that instances of australopithecus afarensis%1:05:00 are members of in-stances of genus australopithecus%1:05:00, which is clearly not adequate.Despite this confusion, dealing with collections, organizations and groups as individuals poses noreal problem.
The Member meronymy is adequately used elsewhere in WN to relate individuals (e.g.,balthazar%1:18:00, an instance of sage%1:18:00, is a Member of magi%1:14:00, an instance of col-lection%1:14:00).
Dealing with biological genera is arguably more complex, as one can see them bothas classes whose instances are the individual organisms, and as individuals which are instances of theclass genus%1:14:00.
A first-order solution to this dilemma, which applies more generally to sociallydefined concepts, proposes to consider concepts (and genera) as individuals, and to introduce anothersort of instance relation for them (Masolo et al, 2004).
Beyond genera, related problems occur with theclassification of biological orders, divisions, phylums, and families, most of which are correctly consid-ered as groups (e.g., chordata%1:05:00), except for a few, pointed out by Test 4 (e.g., amniota%1:05:00,arenaviridae%1:05:00).
All these though should be group individuals, not group classes as now in WN.3.2 Confusion between class and individual which is a specific instance of the classTest 0 also points at a few errors where a class is confused with a specific instance of this class.This error corresponds to a missing sense of the word, used with a specific sense.
Examples includethe individual-class pairs ?great divide%1:15:00, continental divide%1:15:00?,8 ?saturn%1:17:00, so-lar system%1:17:00?, ?renaissance%1:28:00, history%1:28:00?, in which the continental divide at stakeis not any one but that of North America, the solar system, ours, and the history, the history of mankind.Sometimes the gloss itself makes it clear that the lexicographer wanted to do two things at a time; cf.
forcontinental divide%1:15:00: ?the watershed of a continent (especially the watershed of North Americaformed by a series of mountain ridges extending from Alaska to Mexico)?.7This particular error doesn?t show again with Test 4 because the meronyms of new testament%1:10:00 are ?part?meronyms, not Member meronyms.8WN has chosen a restrictive sense for the Great Divide, making it a proper part of the Continental Divide.
In otherinterpretations these two names are synonyms.2793.3 Confusion between meronymy and other relationsThe meronymy relation itself can be wrong, that is, it is confused with other relations, especially ?is lo-cated in?
?balkan wars%1:04:00, balkan peninsula%1:15:00?
(Test 2), ?nessie%1:18:00, loch ness%1:17:00?
(Test 1); ?participates in?
?feminist%1:18:00, feminist movement%1:04:00?, ?air%1:27:00,wind%1:19:00?
(Test 2); ?is a quality of?
?personality%1:07:00, person%1:03:00?, ?regulation time%1:28:00, athletic game%1:04:00?
(Test 3); or still other dependence relations such as in ?operatingsystem%1:10:00, platform%1:06:03?
(Test 1).
Diseases and other conditions regularly give rise to aconfusion with ?participates in?
or its inverse, as with ?cancer cell%1:08:00, malignancy%1:26:00?,?knock-knee%1:26:00, leg%1:08:01?, and ?acardia%1:26:00, monster%1:05:00?
(Test 2).3.4 Confusion between property (AB) and an entity (ED or PD) having that propertyA regular confusion occurs between an entity and a property of that entity, for instance a shape, a quantityor measure, or a location.
Similarly, confusions occur between a relation and an ED or PD being anargument of that relation.
Examples are extracted mostly with Tests 1 and 3, but a few examples are alsofound with Tests 2 and 4, when several problems co-occurred.
Such confusions lead to wrong taxonomicpositions: coin%1:21:02, haymow%1:23:00 and tear%1:08:01 are attached under quantity%1:03:00(AB), while the intuition as well as the glosses make it clear that a coin is a flat metal piece and ahaymow a mass of hay, that is, concrete physical entities under ED; similarly, corolla%1:20:00 andmothball%1:06:00 are attached under shape%1:03:00 (AB), while there are clearly ED.Regularities group together some cases, e.g., many hyponyms of helping%1:13:00 (drumstick, fillet,sangria...) are spotted because helping%1:13:00 is under small indefinite quantity%1:23:00 (AB).
Itturns out that small indefinite quantity%1:23:00 and its direct hypernym indefinite quantity%1:23:00cover more physical entities of a certain quantity rather than quantities themselves.
The tests revealsimilar errors at higher levels in the hierarchy: possession%1:03:00 ?anything owned or possessed?
isattached under relation%1:03:00 ?an abstraction belonging to or characteristic of two entities or partstogether?
(AB), that is, the object possessed is confused with the relation of possession.
Test 1 points atthis error 16 times (e.g., credit card%1:21:00 and hacienda%1:21:00, clearly not abstracts, are spottedthis way).
Another important mid-level error of this kind is that part%1:24:00, while glossed ?somethingdetermined in relation to something that includes it?, is attached under relation%1:03:00 (AB) as well.As a result, all its hyponyms, for instance, news item%1:10:00, and notably, substance%1:03:00 ?thereal physical matter of which a person or thing consists?
and all its hyponyms (e.g., dust%1:27:00,beverage%1:13:00) are considered abstract entities.93.5 Confusion between two senses of a wordAll the tests yield errors denoting missing senses of some words in WN.
Test 4 shows that Member issystematically used between a national of a country and that individual country, e.g.
?ethiopian%1:18:00,ethiopia%1:15:00?, thus referring to the sense of country as ?people of that nation?.
But while the wordcountry has both the ?location?
and the ?people?
senses (among others) in WN, individual countries donot have multiple senses and are all instances of country%1:15:00, the ?location?
sense.Similarly, hyponyms of natural phenomenon%1:19:00 (PD) are often confused with the object (ED)involved, i.e., the participant to the process, revealing missing senses (examples extracted with Test 2).Precipitation has (among others) two senses, precipitation%1:23:00 ?the quantity of water falling toearth?
(a quantity, AB), and precipitation%1:19:00 ?the falling to earth of any form of water?
(a naturalphenomenon, PD).
The actual water fallen (ED), is missing, as revealed by the pair ?ice crystal%1:19:00,precipitation%1:19:00?
(from Test 2).Other errors of this kind are more sporadic, as with ?golf hole%1:06:00, golf course%1:06:00?
(golfhole has only a ?playing period?
sense, its ?location?
sense is missing, from Test 1), and ?coma%1:17:00,9substance%1:03:00 acquires though a physical entity character through multiple inheritance, since it also has matter andphysical entity as hypernyms.
It not not obvious why multiple inheritance has been used here.280comet%1:17:00?
(coma has only a ?process?
sense, its ?physical entity?
sense is missing, from Test 2).3.6 Polysemy in WordNetThe last two types of error, 3.4 and 3.5, point at polysemy issues, as well as the few cases of 3.2.
Thereare two strategies to address polysemy in WN.
The main one is the distinction of several synsets for thedifferent senses of a word, but there is also the use of multiple inheritance that gives several facets to asingle synset.
The literature onWN doesn?t make it clear why and when to use multiple inheritance ratherthan multiple synsets, and it appears that lexicographers have not been methodical is its use.
Some casesof ?dot objects?
(Pustejovsky, 1995) have been accounted this way.
For instance, letter%1:10:00 inheritsboth its abstract content from its hypernym text%1:10:00 (AB) and its physical aspect from its hypernymdocument%1:06:00 (ED).
However, the polysemy of book, the classical similar case, is not accounted forin this way: book%1:10:00 only is ED.
And while document has two separate senses, document%1:10:00(AB) and document%1:06:00 (ED), there is no separate abstract sense for book.
Test 1 points at thisproblem with the pair ?book of psalms%1:10:01, book of common prayer%1:10:00?, where the part isa sub-class (rather than an instance, but this is an additional problem pointed by Test 0) of book%1:10:00(ED), while the whole is an instance of sacred text%1:10:00, a communication%1:03:00 (AB).As far as polysemy standardly accounted with multiple senses goes, our tests point at a need for amore principled use there as well.
In particular, the polysemy accounted for at a given level is oftennot reproduced at lower levels, as just observed for document and book.
We also have seen above thatthe polysemy of the word country is not ?inherited?
by individual countries.
Similarly the polysemyof precipitation has no repercussion on that of rain, which has a sense rain%1:19:00 under precipita-tion%1:19:00, and none under precipitation%1:23:00 (on the other hand, the material sense of rain,rain%1:27:00 ?drops of fresh water that fall?, an ED, lacks for precipitation).A few pairs extracted with Test 4 show the hesitation of the lexicographer between the classifica-tion of a collection as a group, and a classification that accounts for the nature of the collection ele-ments.
For instance constellation%1:17:00 and archipelago%1:17:00 have members but are ED, whilegalaxy%1:14:00 is a group.
This could be properly addressed by splitting the group category, erro-neously situated among abstract entities anyway, into different group categories (e.g., one for each ofED, PD and AB), or exploit multiple inheritance if compatible with its regimentation.3.7 Difficult ontological issuesAlthough all the pairs retrieved by our tests point at (one or several) errors, in a few cases, these are notsolved easily.
In particular, difficult ontological issues are faced with fictional entities.
WN classifiesmost of these under psychological feature%1:03:00 (AB).
However, these fictional entities often showvery similar properties to those of concrete entities.
As a result, some of them are classified as ED orPD, e.g., acheron%1:17:00 is an instance of river%1:17:00 (ED), while being somehow recognized asfictional since it is a meronym of hades%1:09:00, a subclass (here again, not an instance, an additionalproblem) of psychological feature%1:03:00 (AB), something pointed out by Test 1.
Others have concreteparts, e.g.
we find the pair ?wing%1:05:00, angel%1:18:00?
among the cases of ?ED,AB?, i.e.
Test 1results.
Angel wings (and feathers, etc.)
are of course of a different nature than bird wings, and hellishrivers are not real rivers, but how to distinguish them without duplicating most concrete concepts underpsychological feature%1:03:00 (AB) is unclear.10Another regular anomaly is found with roles and relations, e.g., with pairs like ?customer%1:18:00,business relation%1:24:00?, an ?ED,AB?
case (Test 1).
A straightforward analysis saying that meronymyhas been confused with participation (cf.
3.3) would overlook the fact that the customer role is definedby the business relation itself, i.e., that the dependence is even tighter.
Since currently in WN, cus-tomer%1:18:00 simply is a sub-class of person%1:03:00 (ED), in any case the classical issues related to10Although the ontological nature of fictional entities is discussed in metaphysics (see, e.g., (Thomasson, 1999)), how to dealwith their ?concrete?
aspects is not a central issue.281the representation of roles are not addressed, and a more general solution should be looked for, perhapsalong the lines of (Masolo et al, 2004).3.8 Small errorsFinally, our tests identify a few isolated WN errors, which can be seen as small slips, such as for in-stance a wrong sense selected in the meronymy, e.g., ?seat%1:06:01, seating area%1:06:00?
whereseat%1:15:01 (the area, not the chair) should have been selected,11 or a wrong taxonomical attachment,that is, a wrong sense selected for an hypernym, e.g., infrastructure%1:06:01 is an hyponym of struc-ture%1:07:00, a property, instead of structure%1:06:00, an artifact (from the pair ?infrastructure%1:06:01, system%1:06:00?
extracted with Test 1).3.9 Types of solutionsAs can be observed, tests do not all point at a unique type of problem, nor suggest a unique type ofsolution.
Basically, there are five kinds of formal issues underlying the types of errors analyzed above,each calling for different modifications of WN:?
a synset is considered as a class but should be an individual (3.1): need to change its direct hyper-nym link into an instance-of link, possibly changing as well the attachment point in the taxonomy;?
a synset is not attached to the right place in the taxonomy (3.4, 3.8): need to move it in thetaxonomy;?
a synset mixes two senses (3.2, 3.5): need to introduce a missing sense, either attached elsewherein the taxonomy or as instance of the synset at hand;?
the meronymy relation is confused with another one (3.3): need to remove it (or change it foranother sort of relation when this is introduced in WN);?
the meronomy relation is established between the wrong synsets (3.8): need to change one of thetwo synsets related by another sense of a same word.In some cases, the problems should be addressed through more general cures, at a higher level in thetaxonomy (3.4) or by imposing more systematic modeling choices (3.6, 3.7).4 Looking forwardWe showed in this paper that automatic methods can be developed to spot errors in WN, especially inthe hyperonymy relations in the lower levels of the taxonomy.
The query system based on ontologicalprinciples and semantic constraints we proposed was very effective, as all the items retrieved did pointto one or more errors.
With such generic tests though, a manual analysis of the extracted examples bylexicographers, domain or ontological experts is necessary to decide on how the error should be solved.However, this same analysis showed many regularities pointing at standard ontological errors, whichsuggested that the tests can be much refined to limit the variety of issues caught by a single test and thatsimple repair guidelines can be written.This work can therefore be developed in several directions.
On the one hand, the same tests can beexploited further by expanding the meronymy datasets, for instance if some annotated corpus similarto the SemEval2007 datasets becomes available.
The range of tests can be extended as well.
For in-stance, one can make further coherence tests exploiting meronymy data, refining or complementing theTests 0?4 presented here.
The class of abstract entities AB groups a variety of concepts, so incompatiblecombinations of subclasses are certainly present in ?AB,AB?
pairs (e.g., across relation%1:03:00, psycho-logical feature%1:03:00, or measure%1:03:00), suggesting new tests.
Without considering to removegroups from abstract entities, cases of incoherence involving groups could also be addressed by checking11This is extracted with Test 1, because an additional problem appears with seating area%1:06:00 (or rather with its directhypernym room%1:23:00), which is under spatial relation%1:07:00 (AB) rather than area and location (ED).
This shows thatthe error in the meronomy relation would in principle require finer-grained tests to be found.282the compatibility of the ontological categories of their members.
Among the class of physical entitiesED, we disregarded the presence of location entities, so new tests could also examine incompatible com-binations of subclasses of ED.
Finally, we could check whether the ?substance?
meronym relation indeedinvolves substances, in a similar way as Test 4 for groups.
Additional tests can be considered using otherknowledge sources than meronymy data.
Within WN, we could exploit the semantics of tagged glosses(cf.
Princeton WordNet Gloss Corpus) in order to check the coherence with the taxonomy.
And sinceWN is more than a network of nouns, others relations can be exploited, for instance between nouns andverbs.
Similarly, SemEval datasets deal with other relations than the one exploited here: from other sub-types of meronymy (e.g., ?place-area?
), to any of the semantic relations analyzed in the literature (e.g.,?instrument-agency?).
In particular, relations involving thematic roles are quite easily associated withontological constraints and so can constitute the basis for further tests.On the other hand, methods aiming at improving the quality of WN can be concretely built on thebasis of these tests.
A semi-automatic tool for ?cleaning-up?
WN could be fully developed, which couldcontribute to the next, improved, version of WN.
The analysis of regular errors made inWN could simplylead to guidelines to help lexicographers avoid classical ontological mistakes.
Such guidelines could beused for the extension of Princeton WN, e.g., for new domains.
They could be used also during the cre-ation of new WordNets for other languages, suggesting at the same time to abandon the common practiceof simply importing the taxonomy of Princeton WN, importing also its errors.
These two ideas couldbe combined in creating a tool to assist the development of WordNets by automatically checking errorsand pointing out them in the development phase.
This could well complement the TMEO methodology,based on ontological distinctions, used during the creation of the Sensocomune computational lexicon(Oltramari et al, 2010).AcknowledgementsWe wish to thank Alessandro Oltramari for his contribution to the initial stages of this work, LaurentPre?vot for fruitful discussions on this topic and comments on a previous draft, Emanuele Pianta andthree anonymous reviewers for their comments.
This work has been supported by the LOA-ISTC-CNRand the ILIKS joint European laboratory.ReferencesAlvez, J., J. Atserias, J. Carrera, S. Climent, E. Laparra, A. Oliver, and G. Rigau (2008).
Complete andconsistent annotation of WordNet using the Top Concept Ontology.
In Proceedings of LREC2008, pp.1529?1534.Bentivogli, L., I. Dagan, H. T. Dang, D. Giampiccolo, and B. Magnini (2009).
The Fifth PASCALRecognizing Textual Entailment Challenge.
In Proceedings of TAC 2009 Workshop, Gaithersburg,Maryland, USA.Budanitsky, A. and G. Hirst (2006).
Evaluating WordNet-based Measures of Lexical Semantic Related-ness.
Computational Linguistics 32(1), 13?47.Casati, R. and A. Varzi (1999).
Parts and Places - The Structures of Spatial Representation.
Cambridge,MA: MIT Press.Clark, P., P. Harrison, T. Jenkins, J. Thompson, and R. Wojcik (2006).
From WordNet to a KnowlegeBase.
In C. Baral (Ed.
), Formalizing and Compiling Background Knowledge and Its Applications toKnowledge Representation and Question Answering.
Papers from the 2006 AAAI Spring Symposium,pp.
10?15.
AAAI Press.Fellbaum, C.
(Ed.)
(1998).
WordNet.
An Electronic Lexical Database.
Cambridge (MA): MIT Press.283Gangemi, A., N. Guarino, C. Masolo, and A. Oltramari (2003).
Sweetening WordNet with DOLCE.
AIMagazine 24(3), 13?24.Girju, R. and A. Badulescu (2006).
Automatic Discovery of Part-Whole Relations.
ComputationalLinguistics 32(1), 83?135.Girju, R., V. Nastase, and P. Turney (2007).
SemEval-2007 Task 04: Classification of Semantic Rela-tions between Nominals.
In Proceedings of the 4th International Workshop on Semantic Evaluations(SemEval-2007), pp.
13?18.
Association for Computational Linguistics.Guarino, N. (1998).
Some ontological principles for designing upper level lexical resources.
In A. Rubio,N.
Gallardo, R. Castro, and A. Tejada (Eds.
), First International Conference on Language Resourcesand Evaluation, pp.
527?534.
European Language Resources Association.Guarino, N. and C.Welty (2004).
An overview of OntoClean.
In S. Staab and R. Studer (Eds.
), Handbookon Ontologies, pp.
151?159.
Springer-Verlag.Kaplan, A. N. and L. K. Schubert (2001).
Measuring and Improving the Quality of World KnowledgeExtracted From WordNet.
Technical Report 751, University of Rochester.Masolo, C., S. Borgo, A. Gangemi, N. Guarino, and A. Oltramari (2003).
The WonderWeb libraryof foundational ontologies and the DOLCE ontology.
WonderWeb (EU IST project 2001-33052)deliverable D18, LOA-ISTC-CNR.Masolo, C., L. Vieu, E. Bottazzi, C. Catenacci, R. Ferrario, A. Gangemi, and N. Guarino (2004).
Socialroles and their descriptions.
In D. Dubois and C. Welty (Eds.
), Proceedings of the 9th Int.
Conf.
onPrinciples of Knowledge Representation and Reasoning (KR 2004), pp.
267?277.
Menlo Park (CA):AAAI Press.
Whistler June, 2-5, 2004.Neel, A. and M. Garzon (2010).
Semantic Methods for Textual Entailment: How Much World Knowl-edge is Enough?
In Proceedings of FLAIRS 2010, pp.
253?258.Oltramari, A., G. Vetere, M. Lenzerini, A. Gangemi, and N. Guarino (2010).
Senso comune.
In N. Calzo-lari, K. Choukri, B. Maegaard, J. Mariani, J. Odijk, S. Piperidis, M. Rosner, and D. Tapias (Eds.
), Pro-ceedings of the Seventh conference on International Language Resources and Evaluation (LREC?10),Valletta, Malta, pp.
3873?3877.
European Language Resources Association (ELRA).Pease, A. and C. Fellbaum (2009).
Formal ontology as interlingua: the SUMO and WordNet linkingproject and Global WordNet.
In C.-R. Huang, N. Calzolari, A. Gangemi, A. Lenci, A. Oltramari, andL.
Pre?vot (Eds.
), Ontology and the Lexicon.
A Natural Language Processing Perspective, pp.
31?45.Cambridge University Press.Pustejovsky, J.
(1995).
The generative lexicon.
Cambridge (MA): MIT Press.Simons, P. (1987).
Parts - A study in ontology.
Oxford: Clarendon Press.Thomasson, A.
(1999).
Fiction and Metaphysics.
Cambridge University Press.Vieu, L. and M. Aurnague (2007).
Part-of relations, functionality and dependence.
In M. Aurnague,M.
Hickmann, and L. Vieu (Eds.
), The Categorization of Spatial Entities in Language and Cognition,pp.
307?336.
Amsterdam: John Benjamins.Winston, M., R. Chaffin, and D. Herrmann (1987).
A taxonomy of part-whole relations.
CognitiveScience 11(4), 417?444.284
