Proceedings of the SIGDIAL 2014 Conference, pages 199?207,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsReducing Sparsity Improves the Recognition of Implicit DiscourseRelationsJunyi Jessy LiUniversity of Pennsylvanialjunyi@seas.upenn.eduAni NenkovaUniversity of Pennsylvanianenkova@seas.upenn.eduAbstractThe earliest work on automatic detec-tion of implicit discourse relations reliedon lexical features.
More recently, re-searchers have demonstrated that syntacticfeatures are superior to lexical features forthe task.
In this paper we re-examine thetwo classes of state of the art representa-tions: syntactic production rules and wordpair features.
In particular, we focus on theneed to reduce sparsity in instance repre-sentation, demonstrating that different rep-resentation choices even for the same classof features may exacerbate sparsity issuesand reduce performance.
We present re-sults that clearly reveal that lexicalizationof the syntactic features is necessary forgood performance.
We introduce a novel,less sparse, syntactic representation whichleads to improvement in discourse rela-tion recognition.
Finally, we demonstratethat classifiers trained on different repre-sentations, especially lexical ones, behaverather differently and thus could likely becombined in future systems.1 IntroductionImplicit discourse relations hold between adjacentsentences in the same paragraph, and are not sig-naled by any of the common explicit discourseconnectives such as because, however, meanwhile,etc.
Consider the two examples below, drawn fromthe Penn Discourse Treebank (PDTB) (Prasad etal., 2008), of a causal and a contrast relation, re-spectively.
The italic and bold fonts mark the ar-guments of the relation, i.e the portions of the textconnected by the discourse relation.Ex1: Mrs Yeargin is lying.
[Implicit = BECAUSE] Theyfound students in an advanced class a year earlier whosaid she gave them similar help.Ex2: Back downtown, the execs squeezed in a few meetings atthe hotel before boarding the buses again.
[Implicit = BUT]This time, it was for dinner and dancing - a block away.The task is undisputedly hard, partly because itis hard to come up with intuitive feature represen-tations for the problem.
Lexical and syntactic fea-tures form the basis of the most successful stud-ies on supervised prediction of implicit discourserelations in the PDTB.
Lexical features were thefocus of the earliest work in discourse recogni-tion, when cross product of words (word pairs)in the two spans connected via a discourse re-lation was studied.
Later, grammatical produc-tions were found to be more effective.
Featuresof other classes such as verbs, inquirer tags, posi-tions were also studied, but they only marginallyimprove upon syntactic features.In this study, we compare the most commonlyused lexical and syntactic features.
We show thatrepresentations that minimize sparsity issues aresuperior to their sparse counterparts, i.e.
the bet-ter representations are those for which informativefeatures occur in larger portions of the data.
Notsurprisingly, lexical features are more sparse (oc-curring in fewer instances in the dataset) than syn-tactic features; the superiority of syntactic repre-sentations may thus be partially explained by thisproperty.More surprising findings come from a closer ex-amination of instance representation approachesin prior work.
We first discuss how choices inprior work have in fact exacerbated the sparsityproblem of lexical features.
Then, we introducea new syntactically informed feature class, whichis less sparse than prior lexical and syntactic fea-tures, and improves significantly the classificationof implicit discourse relations.Given these findings, we address the questionif any lexical information at all should be pre-served in discourse parsers.
We find that purelysyntactic representations show lower recognition199for most relations, indicating that lexical features,albeit sparse, are necessary for the task.
Lexicalfeatures also account for a high percentage of themost predictive features.We further quantify the agreement of predic-tions produced from classifiers using different in-stance representations.
We find that our novel syn-tactic representation is better for implicit discourserelation prediction than prior syntactic feature be-cause it has higher overall accuracy and makescorrect predictions for instances for which the al-ternative representations are also correct.
Differ-ent representation of lexical features however ap-pear complementary to each other, with markedlyhigher fraction of instances recognized correctlyby only one of the models.Our work advances the state of the art in implicitdiscourse recognition by clarifying the extent towhich sparsity issues influence predictions, by in-troducing a strong syntactic representation and bydocumenting the need for further more complexintegration of lexical information.2 The Penn Discourse TreebankThe Penn Discourse Treebank (PDTB) (Prasad etal., 2008) contains annotations for five types ofdiscourse relations over the Penn Treebank corpus(Marcus et al., 1993).
Explicit relations are thosesignaled by a discourse connective that occurs inthe text, such as ?because?, ?however?, ?for ex-ample?.
Implicit relations are annotated betweenadjacent sentences in the same paragraph.
Thereare no discourse connectives between the two sen-tences, and the annotators were asked to insert aconnective while marking their senses.
Some pairsof sentences do not contain one of the explicit dis-course connectives, but the insertion of a connec-tive provides redundant information into the text.For example, they may contain phrases such as?the consequence of the act?.
These are markedAlternative Lexicalizations (AltLex).
Entity rela-tions (EntRel) are adjacent sentences that are onlyrelated via the same entity or topic.
Finally, sen-tences where no discourse relations were identi-fied were marked NoRel.
In this work, we considerAltLex to be part of the Implicit relations, and En-tRel to be part of NoRel.All connectives, either explicit or implicitly in-serted, are associated with two arguments of theminimal span of text conveying the semantic con-tent between which the relation holds.
This is il-lustrated in the following example where the twoarguments are marked in bold and italic:Ex: They stopped delivering junk mail.
[Implicit = SO] Nowthousands of mailers go straight into the trash.Relation senses in the PDTB are drawn froma 3-level hierarchy.
The top level relations areComparison (arg1 and arg2 holds a contrast rela-tion), Contingency (arg1 and arg2 are causally re-lated), Expansion (arg2 further describes arg1) andTemporal (arg1 and arg2 are temporally related).Some of the largest second-tier relations are underExpansion, which include Conjunction (arg2 pro-vides new information to arg1), Instantiation (arg2exemplifies arg1) and Restatement (arg2 semanti-cally repeats arg1).In our experiments we use the four top level re-lations as well as the above three subclasses of Ex-pansion.
All of these subclasses occur with fre-quencies similar to those of the Contingency andComparison classes, with thousands of examplesin the PDTB.1We show the distribution of theclasses below:Temporal 1038 Comparison 2550Contingency 4532 Instantiation 1483Restatement 3271 Conjunction 3646EntRel/NoRel 54643 Experimental settingsIn our experiments we use only lexical and syntac-tic features.
This choice is motivated by the factthat lexical features have been used most widelyfor the task and that recent work has demon-strated that syntactic features are the single besttype of representation.
Adding additional featuresonly minimally improves performance (Lin et al.,2009).
By zeroing in only on these classes of fea-tures we are able to discuss more clearly the im-pact that different instance representation have onsparsity and classifier performance.We use gold-standard parses from the originalPenn Treebank for syntax features.To ensure that our conclusions are based onanalysis of the most common relations, we trainbinary SVM classifiers2for the seven relations de-scribed above.
We adopt the standard practice in1All other sub-classes of implicit relations are too smallfor general practical applications.
For example the Alterna-tive class and Concession class have only 185 and 228 oc-currences, respectively, in the 16,224 implicit relation anno-tations of the PDTB.2We use SVMLight (Joachims, 1999) with linear kernel.200prior work and downsampled the negative class sothe number of positive and negative samples areequal in the training set.3Our training set consists of PDTB sections 2-19.
The testing set consists of sections 20-24.
Likemost studies, we do not include sections 0-1 in thetraining set.
We expanded the test set (sections 23or 23-24) used in previous work (Lin et al., 2014;Park and Cardie, 2012) to ensure the number ofexamples of the smaller relations, particularly ofTemporal or Instantiation, are suitable for carryingout reliable tests for statistical significance.Some of the discourse relations are much largerthan others, so we report our results in term of F-measure for each relation and average unweightedaccuracy.
Significance tests over F scores werecarried out using a paired t-test.
To do this, thetest set is randomly partitioned into ten groups.
Ineach group, the relation distribution was kept asclose as possible to the overall test set.4 Sparsity and pure lexicalrepresentationsBy far the most common features used for rep-resenting implicit discourse relations are lexical(Sporleder and Lascarides, 2008; Pitler et al.,2009; Lin et al., 2009; Hernault et al., 2010;Park and Cardie, 2012).
Early studies have sug-gested that lexical features, word pairs (cross-product of the words in the first and second ar-gument) in particular, will be powerful predictorsof discourse relations (Marcu and Echihabi, 2002;Blair-Goldensohn et al., 2007).
The intuition be-hind word pairs was that semantic relations be-tween the lexical items, such as drought?famine,child?adult, may in turn signal causal or contrastdiscourse relations.
Later it has been shown thatword pair features do not appear to capture suchsemantic relationship between words (Pitler et al.,2009) and that syntactic features lead to higher ac-curacies (Lin et al., 2009; Zhou et al., 2010; Parkand Cardie, 2012).
Recently, Biran and McKeown(2013) aggregated word pair features with explicitconnectives and reported improvements over theoriginal word pairs as features.In this section, we show that the representationof lexical features play a direct role in feature spar-sity and ultimately affects prediction performance.The first two studies that specifically addressed3We also did not include features that occurred less than5 times in the training set.# Features Avg.
F Avg.
Accuracyword-pairs 92128 29.46 57.22binary-lexical 12116 31.79 60.42Table 1: F-scores and average accuracies of pairedand binary representations of words.the problem of predicting implicit discourse re-lations in the PDTB made use of very differentinstance representations.
Pitler et al.
(2009) rep-resent instances of discourse relations in a vec-tor space defined by word pairs, i.e.
the cross-product of the words that appear in the two argu-ments of the relation.
There, features are of theform (w1, w2) where w1?
arg1 and w2?
arg2.If there are N words in the entire vocabulary, thesize of each instance would be N ?N .In contrast, Lin et al.
(2009) represent instancesby tracking the occurrences of grammatical pro-ductions in the syntactic parse of argument spans.There are three indicator features associated witheach production: whether the production appearsin arg1, in arg2, and in both arguments.
For agrammar with N production rules, the size of thevector representing an instance will be 3N .
Forconvenience we call this ?binary representation?,in contrast to the word-pair features in which thecross product of words constitute the representa-tion.
Note that the cross-product approach hasbeen extended to a wide variety of features (Pitleret al., 2009; Park and Cardie, 2012).
In the ex-periments that follow we will demonstrate that bi-nary representations lead to less sparse featuresand higher prediction accuracy.Lin et al.
(2009) found that their syntactic fea-tures are more powerful than the word pair fea-tures.
Here we show that the advantage comes notonly from the inclusion of syntactic informationbut also from the less sparse instance representa-tion they used for syntactic features.
In Table 1we show the number of features for each repre-sentation and the average F score and accuracy forword pairs and words with binary representation(binary-lexical).
The results for each relation areshown in Table 8 and discussed in Section 7.Using binary representation for lexical informa-tion outperforms word pairs.
Thus, the differencein how lexical information is represented accountsfor a considerable portion of the improvement re-ported in Lin et al.
(2009).
Most notably, for theInstantiation class, we see a 7.7% increase in F-score.
On average, the less sparse representation201translates into 2.34% absolute improvement in F-score and 3.2% absolute improvement in accuracy.From this point on we adopt the binary represen-tation for the features discussed.5 Sparsity and syntactic featuresGrammatical production rules were first used fordiscourse relation representation in Lin et al.(2009).
They were identified as the most suitablerepresentation, that lead to highest performance ina couple of independent studies (Lin et al., 2009;Park and Cardie, 2012).
The comparison repre-sentations covered a number of semantic classesrelated to sentiment, polarity and verb informationand dependency representations of syntax.Production rules correspond to tree chunks inthe constituency parse of a sentence, i.e.
a nodein the syntactic parse tree with all of its children,which in turn correspond to grammar rules ap-plied in the derivation of the tree, such as S?NPVP.
This syntactic representation subsumes lexi-cal representations because of the production ruleswith part-of-speech on the left-hand side and a lex-ical item on the right-hand side.We propose that the sparsity of production rulescan be reduced even further by introducing a newrepresentation of the parse tree.
Specifically, in-stead of having full production rules where a sin-gle feature records the parent and all its children,all (parent,child) pairs in the constituency parsetree are used.
For example, the rule S?NP VPwill now become two features, S?NP and S?VP.Note that the leaves of the tree, i.e.
the part-of-speech?word features are not changed.
For easeof reference we call this new representation ?pro-duction sticks?.
In this section we show that Fscores and accuracies for implicit discourse rela-tion prediction based on production sticks is sig-nificantly higher than using full production rules.First, Table 2 illustrates the contrast in sparsityamong the lexical, production rule and stick repre-sentations.
The table gives the rate of occurrenceof each feature class, which is defined as the av-erage fraction of features with non-zero values inthe representation of instances in the entire train-ing set.
Specifically, let N be the total number offeatures, mibe the number of features triggered ininstance i, then the rate of occurrence ismiN.The table clearly shows that the number of fea-tures in the three representations is comparable,but they vary notably in their rate of occurrence.# Features Rate of Occurrencesticks 14,165 0.00623prodrules 16,173 0.00374binary-lexical 12,116 0.00276word-pairs 92,128 0.00113Table 2: Number of features and rate of occur-rence for binary lexical representation, productionrules and sticks.Avg.
F Avg.
Accuracysticks 34.73 64.89prodrules 33.69 63.55binary-lexical 31.79 60.42word-pairs 29.46 57.22Table 3: F-scores and average accuracies of pro-duction rules and production sticks.Sticks have almost twice the rate of occurrence ofthat of full production rules.
Both syntactic rep-resentations have much larger rate of occurrencethan lexical features, and the rate of occurrence ofword pairs is more than twice smaller than that ofthe binary lexical representation.Next, in Table 3, we give binary classifica-tion prediction results based on both full rulesand sticks.
The first two rows of Table 3 com-pare full production rules (prodrules) with produc-tion sticks (sticks) using the binary representation.They both outperform the binary lexical represen-tation.
Again our results confirm that the betterperformance of production rule features is partlybecause they are less sparse than lexical represen-tations, with an average of 1.04% F-score increase.Individually the F scores of 6 of the 7 relations areimproved as shown in Table 8.6 How important are lexical features?Production rules or sticks include lexical itemswith their part-of-speech tags.
These are the sub-set of features that contribute most to sparsity is-sues.
In this section we test if these lexical fea-tures contribute to the performance or if they canbe removed without noticeable degradation due toits intrinsic sparsity.
It turns out that it is not ad-visable to remove the lexical features entirely, asperformance decreases substantially if we do so.6.1 Classification without lexical itemsWe start our exploration of the influence of lexicalitems on the accuracy of prediction by inspectingthe performances of the classifiers with productionrules and sticks, but without the lexical items andtheir parts of speech.
Table 4 lists the average F202Avg.
F Avg.
Accuracyprodrules 33.69 63.55sticks 34.73 64.89prodrules-nolex 32.30 62.03sticks-nolex 33.86 63.99Table 4: F-scores and average accuracies of pro-duction rules and sticks, with (rows 1-2) and with-out (rows 3-4) lexical items.# Features Rate of Occurrenceprodrules 16,173 0.00374sticks 14,165 0.00623prodrules-nolex 3470 0.00902sticks-nolex 922 0.0619Table 5: Number of features and rate of occur-rence for production rules and sticks, with (rows1-2) and without (rows 3-4) lexical items.scores and accuracies.
Table 8 provides detailedresults for individual relations.
Here prodrules-nolex and sticks-nolex denote full production ruleswithout lexical items, and production sticks with-out lexical items, respectively.
In all but two re-lations, lexical items contribute to better classifierperformance.When lexical items are not included in the rep-resentation, the number of features is reduced tofewer than 30% of that in the original full produc-tion rules.
At the same time however, includingthe lexical items in the representation improvesperformance even more than introducing the lesssparse production stick representation.
Productionsticks with lexical information also perform bet-ter than the same representation without the POS-word sticks.The number of features and their rates of occur-rences are listed in Table 5.
It again confirms thatthe less sparse stick representation leads to betterclassifier performance.
Not surprisingly, purelysyntactic features (without the lexical items) aremuch less sparse than syntax features with lexicalitems present.
However the classifier performanceis worse without the lexical features.
This contrasthighlights the importance of a reasonable tradeoffbetween attempts to reduce sparsity and the needto preserve lexical features.6.2 Feature selectionSo far our discussion was based on the behaviorof models trained on a complete set of relativelyfrequent syntactic and lexical features (occurringmore than five times in the training data).
Featureselection is a way to reasonably prune out the setRelation %-nonlex %-allfeatsTemporal 25.56 10.95Comparison 25.40 15.51Contingency 20.12 25.05Conjunction 21.15 19.20Instantiation 25.08 16.16Restatement 22.16 17.35Expansion 18.36 18.66Table 6: Non-lexical features selected using fea-ture selection.
%-nonlex records the percentage ofnon-lexical features among all features selected;%-allfeats records the percentage of selected non-lexical features among all non-lexical features.and reduce sparsity issues in the model.
In factfeature selection has been used in the majority ofprior work (Pitler et al., 2009; Lin et al., 2009;Park and Cardie, 2012).Here we perform feature selection and exam-ine the proportion of syntactic and lexical featuresamong the most informative features.
We use the?2test of independence, computed on the follow-ing contingency table for each feature Fiand foreach relation Rj:Fi?Rj|Fi?
?Rj?Fi?Rj|?Fi?
?RjEach cell in the above table records the num-ber of training instances in which Fiand Rjarepresent or absent.
We set our level of confidenceto p < 0.1.Table 6 lists the proportions of non-lexical itemsamong the most informative features selected (col-umn 2).
It also lists the percentage of selected non-lexical items among all the 922 purely syntacticfeatures from production rule and production stickrepresentations (column 3).
For all relations, atmost about a quarter of the most informative fea-tures are non-lexical and they only take up 10%-25% of all possible non-lexical features.
The pre-diction results using only these features are eitherhigher than or comparable to that without featureselection (sticks-?2in Table 8).
These numberssuggest that lexical terms play a significant role aspart of the syntactic representations.In Table 8 we record the F scores and accura-cies for each relation under each feature represen-tation.
The representations are sorted according todescending F scores for each relation.
Notice that?2feature selection on sticks is the best represen-tation for the three smallest relations: Compari-son, Instantiation and Temporal.203This finding led us to look into the selected lex-ical features for these three classes.
We found thatthese most prominent features in fact capture somesemantic information.
We list the top ten most pre-dictive lexical features for these three relations be-low, with examples.
Somewhat disturbingly, manyof them are style or domain specific to the WallStreet Journal that PDTB was built on.Comparison a1a2 NN share a1a2 NNS cents a1a2 CC ora1a2 CD million a1a2 QP $ a1a2 NP $ a2 RB n?ta1a2 NN % a2 JJ year a2 IN ofFor Comparison (contrast), the top lexical fea-tures are words that occur in both argument 1 andargument 2.
Contrast within the financial domain,such as ?share?, ?cents?
and numbers between ar-guments are captured by these features.
Considerthe following example:Ex.
Analyst estimate the value of the BellSouth proposal atabout $115 to $125 a share.
[Implicit=AND] They valueMcCaw?s bid at $112 to $118 a share .Here the contrast clearly happens with the valueestimation for two different parties.Instantiation a2 SINV ?
a2 SINV , a2 SINV ?
a2 SINV .a1 DT some a2 S a2 VBZ says a1 NP , a2 NP , a1 DT aFor Instantiation (arg2 gives an example ofarg1), besides words such as ?some?
or ?a?
thatsometimes mark a set of events, many attributionfeatures are selected.
it turns out many Instanti-ation instances in the PDTB involve argument 2being an inverted declarative sentence that signalsa quote as illustrate by the following example:Ex.
Unease is widespread among exchange members.
[Im-plicit=FOR EXAMPLE] ?
I can?t think of any reason tojoin Lloyd?s now, ?
says Keith Whitten, a British business-man and a Lloyd?s member since 1979.Temporal a1 VBD plunged a2 VBZ is a2 RB latera1 VBD was a2 VBD responded a1a2 PRP hea1 WRB when a1 PRP he a1 VBZ is a2 VBP areFor Temporal, verbs like plunge and respondedare selected.
Words such as plunged are quite do-main specific to stock markets, but words such aslater and responded are likely more general indi-cators of the relation.The presence of pronouns was also a predictivefeature.
Consider the following example:Ex.
A Yale law school graduate , he began his career in cor-porate law and then put in years at Metromedia Inc. and theWilliam Morris talent agency.
[Implicit=THEN] In 1976, hejoined CBS Sports to head business affairs and, five yearslater, became its president.Overall, it is fairly easy to see that certain se-mantic information was captured by these fea-tures, such as similar structures in a pair of sen-tences holding a contrast relation, the use of verbsin a Temporal relation.
However, it is rather unset-tling to also see that some of these characteristicsare largely style or domain specific.
For exam-ple, for an Instantiation in an educational scenariowhere the tutor provides an example for a concept,it is highly unlikely that attribution features will behelpful.
Therefore, part of the question of findinga general class of features that carry over to otherstyles or domains of text still remain unanswered.7 Per-relation evaluationTable 8 lists the F-scores and accuracies of eachrepresentation mentioned in this work for predict-ing individual relation classes.
For each relation,the representations are ordered by decreasing F-score.
We tested the results for statistical signifi-cance of the change in F-score.
We compare allthe representations with the best and the worserepresentations for the relation.
A ?Y?
marks asignificance level of p ?
0.05 for the comparisonwith the best or worst representation, a ?T?
marksa significance level of p ?
0.1, which means atendency towards significance.For all relations, production sticks, either withor without feature selection, is the top represen-tation.
Sticks without lexical items also under-perform those including the lexical items for 6 ofthe 7 relations.
Notably, production rules withoutlexical items are among the three worst represen-tations, outperforming only the pure lexical fea-tures in some cases.
This is a strong indicationthat being both a sparse syntactic representationand lacking lexical information, these features arenot favored in this task.
Pure lexical features givethe worst or second to worst F scores, significantlyworse than the alternatives in most of the cases.In Table 7 we list the binary classification re-sults from prior work: feature selected word pairs(Pitler et al., 2009), aggregated word pairs (Biranand McKeown, 2013), production rules only (Parkand Cardie, 2012), and the best combination pos-sible from a variety of features (Park and Cardie,2012), all of which include production rules.
Weaim to compare the relative gains in performancewith different representations.
Note that the abso-lute results from prior work are not exactly com-parable to ours for two reasons ?
the training204Sys.
Pitler et al.
Biran-McKeownFeat.
wordpair-implicit aggregated wpComp.
20.96 (42.55) 24.38 (61.72)Cont.
43.79 (61.92) 44.03 (66.78)Expa.
63.84 (60.28) 66.48 (60.93)Temp.
16.21 (61.98) 19.54 (68.09)Sys.
Park-Cardie Park-CardieFeat.
prodrules best combinationComp.
30.04 (75.84) 31.32 (74.66)Cont.
47.80 (71.90) 49.82 (72.09)Expa.
77.64 (69.60) 79.22 (69.14)Temp.
20.96 (63.36) 26.57 (79.32)Table 7: F-score (accuracy) of prior systems.
Notethat the absolute numbers are not exactly compa-rable with ours because of the important reasonsexplained in this section.and testing sets are different; how Expansion, En-tRel/NoRel and AltLex relations are treated differ-ently in each work.
The only meaningful indicatorhere is the absolute size of improvement.
The tableshows that our introduction of production sticksled to improvements comparable to those reportedin prior work.The aggregated word pair is a less sparse ver-sion of the word pair features, where each pairis converted into weights associated with an ex-plicit connective.
Just as the less sparse binarylexical representation presented previously, the ag-gregated word pairs also gave better performance.None of the three lexical features, however, sur-passes raw production rules, which again echoesour finding that binary lexical features are not bet-ter than the full production rules.
Finally, wenote that a combination of features gives better F-scores.8 Discussion: are the featurescomplementary?So far we have discussed how different represen-tations for lexical and syntactic features can af-fect the classifier performances.
We focused onthe dilemma of how to reduce sparsity while stillpreserving the useful lexical features.
An impor-tant question remains as whether these representa-tions are complementary, that is, how different isthe classifier behaving under different feature setsand if it makes sense to combine the features.We compare the classifier output on the test datawith two methods in Table 9: the Q-statistic andthe percentage of data which the two classifiersdisagree (Kuncheva and Whitaker, 2003).sig- sig-Representation F (A) best worstComparisonsticks-?227.78 (62.83) N/A Yprodrules 27.65 (59.5) - Ysticks 27.50 (60.73) - Ysticks-nolex 27.01 (59.63) - Yprodrules-nolex 26.40 (58.47) T Ybinary-lexical 24.73 (58.32) Y -word-pairs 22.68 (45.03) Y N/AConjunctionsticks 27.55 (63.82) N/A Tsticks-?227.53 (64.06) - Tprodrules 27.02 (63.91) - -sticks-nolex 26.56 (61.03) T -binary-lexical 25.90 (61.77) Y -prodrules-nolex 25.20 (62.83) T N/Aword-pairs 25.18 (74.51) T -Contingencysticks 48.90 (67.49) N/A Ysticks-?248.55 (67.76) - Ysticks-nolex 48.08 (67.69) - Yprodrules 47.14 (65.61) T Yprodrules-nolex 45.79 (63.99) Y Ybinary-lexical 44.17 (62.68) Y Yword-pairs 40.57 (50.53) Y N/AExpansionsticks 56.48 (61.75) N/A Ysticks-?256.30 (62.26) - Ysticks-nolex 55.43 (60.56) - Yprodrules 55.42 (61.05) - Ybinary-lexical 54.20 (59.26) Y -word-pairs 53.65 (56.64) Y -prodrules-nolex 53.53 (58.79) Y N/AInstantiationsticks-?230.34 (74.54) N/A Ysticks 29.93 (73.80) - Yprodrules 29.59 (72.20) - Ysticks-nolex 28.22 (72.66) Y Yprodrules-nolex 27.83 (70.72) Y Ybinary-lexical 27.29 (70.05) Y Yword-pairs 20.22 (51.00) Y N/ARestatementsticks 35.74 (61.45) N/A Ysticks-?234.93 (61.42) - Ysticks-nolex 34.62 (61.08) T Yprodrules 33.52 (58.54) T Yprodrules-nolex 32.05 (56.84) Y -binary-lexical 31.27 (57.41) Y Tword-pairs 29.81 (47.42) Y N/ATemporalsticks-?217.97 (66.67) N/A Ysticks-nolex 17.08 (65.27) T Ysticks 17.04 (65.22) T Yprodrules 15.51 (64.04) Y -prodrules-nolex 15.29 (62.56) Y -binary-lexical 14.97 (61.92) Y -word-pairs 14.10 (75.38) Y N/ATable 8: F-score (accuracy) of each relation foreach feature representation.
The representationsin each relation are sorted in descending order.The column ?sig-best?
marks the significance testresult against the best representation, the col-umn ?sig-worst?
marks the significance test re-sult against the worst representation.
?Y?
denotesp ?
0.05, ?T?
denotes p ?
0.1.205Q-statistic is a measure of agreement betweentwo systems s1and s2formulated as follows:Qs1,s2=N11N00?N01N10N11N00+N01N10Where N denotes the number of instances, a sub-script 1 on the left means s1is correct, and a sub-script 1 on the right means s2is correct.There are several rather surprising findings.Most notably, word pairs and binary lexical repre-sentations give very different classification resultsin each relation.
Their predictions disagree on atleast 25% of the data.
This finding drastically con-trast the fact that they are both lexical features andthat they both make use of the argument annota-tions in the PDTB.
A comparison of the percent-ages and their differences in F scores or accuracieseasily shows that it is not the case that binary lex-ical models correctly predict instances word pairsmade mistakes on, but that they are disagreeing inboth ways.
Thus, given the previous discussionthat lexical items are useful, it is possible the mostsuitable representation would combine both viewsof lexical distribution.Even more surprisingly, the difference in classi-fier behavior is not as big when we compare lex-ical and syntactic representations.
The disagree-ment of production sticks with and without lexi-cal features are the smallest, even though, as wehave shown previously, the majority of productionsticks are lexical features with part-of-speech tags.If we compare binary lexical features with produc-tion sticks, the disagreement becomes bigger, butstill not as big as word pairs vs. binary lexical.Besides the differences in classification, the big-ger picture of improving implicit discourse rela-tion classification is finding a set of feature repre-sentations that are able to complement each otherto improve the classification.
A direct conclusionhere is that one should not limit the focus on fea-tures in different categories (for example, lexicalor syntax), but also features in the same categoryrepresented differently (for example, word pairs orbinary lexical).9 ConclusionIn this work we study implicit discourse relationclassification from the perspective of the interplaybetween lexical and syntactic feature representa-tion.
We are particularly interested in the trade-off between reducing sparsity and preserving lex-ical features.
We first emphasize the importantRel.
Q-stat Disagreementword-pairs vs. binary-lexicalComparison 0.65 33.55Conjunction 0.71 28.47Contingency 0.81 26.35Expansion 0.69 29.38Instantiation 0.75 31.33Restatement 0.76 28.42Temporal 0.25 25.34binary-lexical vs. sticksComparison 0.78 25.49Conjunction 0.78 24.67Contingency 0.86 20.68Expansion 0.80 24.28Instantiation 0.83 20.75Restatement 0.76 26.72Temporal 0.86 20.61sticks vs. prodrulesComparison 0.88 19.77Conjunction 0.89 18.43Contingency 0.94 14.00Expansion 0.88 19.18Instantiation 0.90 16.34Restatement 0.89 18.88Temporal 0.90 17.94sticks vs. sticks-nolexComparison 0.94 14.61Conjunction 0.92 16.63Contingency 0.97 10.16Expansion 0.91 17.35Instantiation 0.97 9.51Restatement 0.97 11.26Temporal 0.98 8.42Table 9: Q statistic and disagreement of differentclasses of representationsrole of sparsity for traditional word-pair represen-tations and how a less sparse representation couldimprove performance.
Then we proposed a lesssparse feature representation for production rules,the best feature category so far, that further im-proves classification.
We study the role of lexicalfeatures and show the contrast between the spar-sity problem they brought along and their domi-nant presence in the highly ranked features.
Also,lexical features included in syntactic features thatare most informative to the classifiers are found tobe style or domain specific in certain relations.
Fi-nally, we compare the representations in terms ofclassifier disagreement and showed that within thesame feature category different feature representa-tion can also be complementary with each other.ReferencesOr Biran and Kathleen McKeown.
2013.
Aggregatedword pair features for implicit discourse relation dis-ambiguation.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (ACL): Short Papers, pages 69?73.206Sasha Blair-Goldensohn, Kathleen McKeown, andOwen Rambow.
2007.
Building and refiningrhetorical-semantic relation models.
In Human Lan-guage Technologies: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL), pages 428?435.Hugo Hernault, Danushka Bollegala, and MitsuruIshizuka.
2010.
A semi-supervised approach to im-prove classification of infrequent discourse relationsusing feature vector extension.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 399?409.Thorsten Joachims.
1999.
Making large-scale supportvector machine learning practical.
In Advances inkernel methods, pages 169?184.Ludmila I. Kuncheva and Christopher J. Whitaker.2003.
Measures of diversity in classifier ensemblesand their relationship with the ensemble accuracy.Machine Learning, 51(2):181?207, May.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the PennDiscourse Treebank.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 343?351.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
APDTB-styled end-to-end discourse parser.
NaturalLanguage Engineering, 20:151?184, 4.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse re-lations.
In Proceedings of the 40th Annual Meet-ing on Association for Computational Linguistics(ACL), pages 368?375.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: The Penn Treebank.
Com-putational Linguistics - Special issue on using largecorpora, 19(2):313?330.Joonsuk Park and Claire Cardie.
2012.
Improving im-plicit discourse relation recognition through featureset optimization.
In Proceedings of the 13th AnnualMeeting of the Special Interest Group on Discourseand Dialogue (SIGDIAL), pages 108?112.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.Automatic sense prediction for implicit discourse re-lations in text.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on NaturalLanguage Processing of the AFNLP (ACL-IJCNLP),pages 683?691.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse TreeBank 2.0.In Proceedings of the Sixth International Conferenceon Language Resources and Evaluation (LREC).Caroline Sporleder and Alex Lascarides.
2008.
Usingautomatically labelled examples to classify rhetori-cal relations: An assessment.
Natural Language En-gineering, 14(3):369?416, July.Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, JianSu, and Chew Lim Tan.
2010.
Predicting discourseconnectives for implicit discourse relation recogni-tion.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (COLING),pages 1507?1514.207
