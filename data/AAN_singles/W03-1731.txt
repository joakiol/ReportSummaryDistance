Chunking-based Chinese Word TokenizationGuoDong ZHOUInstitute for Infocomm Research21 Heng Mui Keng TerraceSingapore, 119613zhougd@i2r.a-star.edu.sg)()(),(log)(log)|(log1111111 nnnnnnnGPTPGTPTPGTP ?+=  AbstractThis paper introduces a Chinese wordtokenization system through HMM-basedchunking.
Experiments show that such asystem can well deal with the unknown wordproblem in Chinese word tokenization.The second term in (2-1) is the mutualinformation between T  and .
In order tosimplify the computation of this term, we assumemutual information independence (2-2):n1nG1?== nininn GtMIGTMI1111 ),(),(  or?==?ninnnnGPTPGTP11111 log)()(),(log ?
niniGPtPGtP11)()(),(1  IntroductionWord Tokenization is regarded as one of majorbottlenecks in Chinese Language Processing.Normally, word tokenization is implementedthrough word segmentation in Chinese LanguageProcessing literature.
This is also affected in thetitle of this competition.That is, an individual tag is only dependent on thetoken sequence G  and independent on other tagsin the tag sequence T .
This assumption isreasonable because the dependence among the tagsin the tag sequence T  has already been capturedby the first term in equation (2-1).
Applying it toequation (2-1), we have (2-3):n1n1n1There exists two major problems in Chineseword segmentation: ambiguity and unknown worddetection.
While ngarm modeling and/or wordco-ocurrence has been successfully applied to dealwith ambiguity problem, unknown word detectionhas become major bottleneck in word tokenization.This paper proposes a HMM-based chunkingscheme to cope with unkown words in Chineseword tokenization.
The unknown word detection isre-casted as chunking several words(single-character word or multi-character word)together to form a new word.?
?==+?=nininiinnnGtPtPTPGTP111111)|(log)(log)(log)|(logFrom equation (2-3), we can see that:?
The first term can be computed by applyingchain rules.
In ngram modeling, each tag isassumed to be probabilistically dependent onthe N-1 previous tags.2 HMM-based Chunking2.1  HMM?
The second term is the summation of logprobabilities of all the individual tags.Given an input sequence , the goalof Chunking is to find a stochastic optimal tagsequence  that maximizes (Zhou andSu 2000) (2-1)nn gggG L211 =nn tttT L211 = ?
The third term corresponds to the ?lexical?
component (dictionary) of the tagger.We will not discuss either the first or the secondterm further in this paper because ngram modelinghas been well studied in the literature.
We will focuson the third term .
?=nini GtP11 )|(log2.2 Chinese Word TokenizationGiven the previous HMM, for Chinese wordtokenization, we have (Zhou and Su 2002):?
;  is the wordsequence;  is the wordformation pattern sequence and  is the wordformation pattern of .
Here  consists of:>=< iii wpg ,nP1nn wwwW L211 =nppp L21iw p=ipio The percentage of w  occurring as a wholeword (round to 10%)io The percentage of w  occurring at thebeginning of other words (round to 10%)io The percentage of  occurring at the end ofother words (round to 10%)iwo The length of  iwo The occurring frequence feature, which is setto max(log(Frequence), 9 ).?
tag : Here, a word is regarded as a chunk(called "Word-Chunk") and the tags are used tobracket and differentiate various types ofWord-chunks.
Chinese word tokenization can beregarded as a bracketing process whiledifferentiation of different word types can helpthe bracketing process.
For convenience, herethe tag used in Chinese word tokenization iscalled ?Word-chunk tag?.
The Word-chunk tagis structural and consists of three parts:itito Boundary category (B): it is a set of fourvalues: 0,1,2,3, where 0 means that currentword is a whole entity and 1/2/3 means thatcurrent word is at the beginning/in themiddle/at the end of a word.o Word category (W): used to denote the classof the word.
In our system, word is classifiedinto two types: pure Chinese word  type andmixed word type (for example, includingEnglish characters/Chinesedigits/Chinesenumbers).o Word Formation Pattern(P): Because ofthe limited number of boundary and wordcategories, the word formation pattern isadded into the structural chunk tag torepresent more accurate models.3 Context-dependent LexiconsThe major problem with Chunking-based Chineseword tokenization is how to effectivelyapproximate .
This can be done byadding lexical entries with more contextualinformation into the lexicon ?
.
In the following,we will discuss five context-dependent lexiconswhich consider different contextual information.
)/( 1ni GtP3.1 Context of current word formation patternand current wordHere, we assume:??????
?=iiiiiiiiini wpptPwpwptPGtP)/()/()/( 1where?
= },{ Cwpwp iiii ?
+  and  is aword formation pattern and word pair existing in thetraining data C .
},{ Cpp ii ?
ii wp3.2 Context of previous word formation patternand current word formation patternHere, we assume :???????=???iiiiiiiiinippptPpppptPGtP1111)/()/()/(where?
= },{ 11 Cpppp iiii ???
+ },{ Cpp ii ?
andis a pair of previous word formation patternand current word formation pattern existing in thetraining data C .ip1ip ?3.3 Context of previous word formation pattern,previous word and current word formationpatternHere, we assume :???????=??????iiiiiiiiiiiinipwpptPpwppwptPGtP1111111)/()/()/(where?
= +},{ 1111 Cpwppwp iiiiii ?????
},{ Cpp ii ?
,where  is a triple pattern existing in thetraining corpus.iii pwp 11 ?
?3.4 Context of previous word formation pattern,current word formation pattern and currentwordHere, we assume :???????=???iiiiiiiiiiiiniwppptPwppwpptPGtP1111)/()/()/(where?
= +},{ 11 Cwppwpp iiiiii ???
},{ Cpp ii ?
,where  is a triple pattern.
ii wp1?ip3.5 Context of previous word formation pattern,previous word, current word formation patternand current wordHere, the context of previous word formationpattern, previous word, current word formationpattern and current word is used as a lexical entry todetermine the current structural chunk tag and ?
=+},{ 1111 Cwpwpwpwp iiiiiiii ?????
},{ Cpp ii ?
,where  is a pattern existing in thetraining corpus.
Due to memory limitation, onlylexical entries which occurs at least 3 times are kept.iiii wpwp 11 ?
?4  Error-Driven LearningIn order to reduce the size of lexicon effectively, anerror-driven learning approach is adopted toexamine the effectiveness of lexical entries andmake it possible to further improve the chunkingaccuracy by merging all the abovecontext-dependent lexicons in a single lexicon.For a new lexical entry e , the effectivenessis measured by the reduction in error whichresults from adding the lexical entry to the lexicon :.
Here,is the chunking error number of thelexical entry e  for the old lexiconi( ie)( ieF?
)( ieF?
(Error eF?
))( ErroriError FeF ??+??
?=)ii ?
andis the chunking error number of thelexical entry e  for the new lexicon)ii(Error eF ??+??
?+?where ??
?ie ( ?
?iis the list of new lexicalentries added to the old lexicon ).
If ,we define the lexical entry e  as positive forlexicon?
0( >)ie?F?
.
Otherwise, the lexical entry e  isnegative for lexiconi?
.5 ImplementationIn training process, only the words occurs at least 5times are kept in the training corpus and in the wordtable while those less-freqently occurred words areseparated into short words (most of such shortwords are single-character words) to simulate thechunking.
That is, those less-frequently words areregarded as chunked from several short words.In word tokenization process, theChunking-based Chinese word tokenization can beimplemented as follows:1) Given an input sentence, a lattice of word andword formation pattern pair is generated byskimming the sentence from left-to-right,looking up the word table to determine all thepossible words, and determining the wordformation pattern for each possible word.2) Viterbi algorithm is applied to decode thelattice to find the most possible tag sequence.3) In this way, the given sentence is chunked intowords with word category informationdiscarded.6 Experimental ResultsTable 1 shows the performance of ourchunking-based Chinese word tokenization in thecompetition.PK (closed,official)CTB (closed,unofficial)Precision 94.5 90.7Recall 93.6 89.6F 94.0 90.1OOV 6.9 18.1Recall on OOV 76.3 75.2Recall on In-Voc 94.9 92.7Speed on P1.8G 420 KB/min 390 KB/minThe most important advantage ofchunking-based Chinese word segmentation is theability to cope with the unknown words.
Table 1shows that about 75% of the unknown words can bedetected correctly using the chunking approach onthe PK and CTB corpus.7 ConclusionThis paper proposes a HMM-based chunkingscheme to cope with the unkown words in Chineseword tokenization.
In the meantime, error-drivenlearning is applied to effectively incorporatevarious context-dependent information.Experiments show that such a system can well dealwith the unknown word problem in Chinese wordtokenization.ReferencesRabiner L. 1989.
A Tutorial on Hidden MarkovModels and Selected Applications in SpeechRecognition.
IEEE 77(2), pages257-285.Viterbi A.J.
1967.
Error Bounds for ConvolutionalCodes and an Asymptotically Optimum DecodingAlgorithm.
IEEE Transactions on InformationTheory, IT 13(2), 260-269.Zhou GuoDong and Su Jian.
2000.
Error-drivenHMM-based Chunk Tagger withContext-dependent Lexicon.
Proceedings of theJoint Conference on Empirical Methods onNatural Language Processing and Very LargeCorpus (EMNLP/ VLC'2000).
Hong Kong, 7-8Oct.
