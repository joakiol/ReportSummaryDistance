TOOLS AND METHODS FORCOMPUTATIONAL LEX ICOLOGYRoy J. ByrdNicoletta Calzolar i*Mart in  S. Chodorow**Judith L. K lavansMary  S. NeffOmneya A. R izk***IBM T. J. Watson Research CenterYorktown Heights, New York 10598This paper presents a set of tools and methods for acquiring, manipulating, and analyzing machine-readable dictionaries.
We give several detailed examples of the use of these tools and methods forparticular analyses.
A novel aspect of our work is that it allows the combined processing of multiplemachine-readable dictionaries.
Our examples describe analyses of data from Webster's SeventhCollegiate Dictionary, the Longman Dictionary of Contemporary English, the Collins bilingualdictionaries, the Collins Thesaurus, and the Zingarelli Italian dictionary.
We describe existing facilitiesand results they have produced as well as planned enhancements o those facilities, particularly in thearea of managing associations involving the senses of polysemous words.
We show how theseenhancements expand the ways in which we can exploit machine-readable dictionaries in the construc-tion of large lexicons for natural language processing systems.1.
INTRODUCTION.It has become clear that the construction of computersystems that process natural language requires thecreation of large computerized lexicons with extensiveand accurate syntactic and semantic information aboutwords (see Byrd(1986a), Ingria(1986)).
It is also clearthat it will be impossible to build these lexicons in thenumber and sizes required with only the manual labor ofindividual computer scientists, linguists, or lexicogra-phers.
There are too many systems requiring too muchinformation about too many words for the manualapproach to succeed.Fortunately, researchers are beginning to realize thatthe wealth of information in published ictionaries canbe tapped by semi-automatic and fully-automatic meth-* Department of Linguistics, University of Pisa and Institute ofComputational Linguistics of CNR, Pisa.
** Department ofPsychology, Hunter College of CUNY.
***Courant Institute of Mathematical Sciences, NYU.ods, in order to help build the computerized lexicons werequire.
Work reported in Alshawi(1985), Calzolari(1983,1984a,b), Lesk(1986), Michiels(1982), etc., de-scribe various attempts to decipher and extract infor-mation in machine-readable v rsions of published dic-tionaries (henceforth: MRDs).The present paper is intended to contribute to thatliterature by describing the tools and methods used bythe Lexical Systems project at IBM Research.
Individ-ual tools and their use in that project have been de-scribed elsewhere (Chodorow, et a1.
(1985), Byrd andChodorow(1985), Byrd, et al(1986b), Chodorow andRavin(1987), Neff and Byrd(1987)).
This paper differsfrom previous descriptions of our own and others'work, however, by presenting a framework and a set ofgeneral tools and methods that can be re-applied for avariety of lexicological and lexicographical applica-tions.
Furthermore, it addresses problems involved incoherently combining information from differentMRDs.
In particular, we show how we can transferinformation from one dictionary to another - -  evenCopyright 1987 by the Association for Computational Linguistics.
Permission tocopy without fee all or part of this material isgranted providedthat the copies are not made for direct commercial dvantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, or to republish, requires afee and/or specific permission.0362-613 X/87/030219-240503.00Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 219Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Metltodsacross languages - -  and correctly associate that infor-mation with the word senses to which it applies.The plan of this paper roughly parallels the logicalsequence of problems that face any project in compu-tational lexicology.
Section 2 on acquisition and storagedescribes the initial stages of capturing and organizingdictionary data.
Section 3 outlines the tools and meth-ods we use to exploit he contents of machine readabledictionaries.
Applications of the tools and methods aredescribed in the final two sections which discuss appliedcomputational lexicography for monolingual nd bilin-gual dictionaries.2.
ACQUISITION AND STORAGE OFMACHINE-READABLE DICTIONARIES.2.1, NORMALIZING MACHINE-READABLE DICTIONARIES.Once machine readable dictionary tapes are acquiredfrom the publishers, they usually have to be cleaned upand/or normalized.
We have encountered three basictypes of publisher tapes: one type, represented byLongman and Webster's Seventh, has some informationsegmented into separate records labelled with an iden-tifying code and other information implicit in fontchange codes.
A second type, represented by Collins,consists of unsegmented text interspersed with font-change codes.
A third type will be the new computer-ized OED (Tompa (1985)), where each type of informa-tion is specifically delimited at the beginning and end,and where there are any number of nesting levels.
Muchof the work described here was done with data of thesecond type; however, it is applicable also to the firsttype, and of course to the third.In the simplest sense, cleaning up usually means theremoval of typesetting codes and other formatting in-formation, or possibly removal of errors (usually doneby hand).
However, since the typesetting codes (e.g.begin-italic, begin-roman) are clues to the kind of infor-mation that follows the codes (for example, part-of-speech information appears in italics in the Collinsdictionaries), they cannot simply be washed out.
In-stead, the raw data undergoes a process of normaliza-tion, the goal of which is to identify and label all of theinformation i  the dictionary tapes in such a way that itcan be easily accessed without detailed knowledge ofthe formatting conventions.
This is our rationale for thecreation of lexical data bases.We have carried out this normalization process intwo stages.
The goal of the first stage is to segment theinformation on the tapes into separate ntries, storingthe results in a direct access file, where the key is thedictionary headword and the data is simply the body ofthe entry.
Dictionary data is stored in a DictionaryAccess Method (DAM) file, from which it can beconveniently retrieved, by headword, for subsequentprocessing.
DAM is completely described in Byrd, etal.(1986b).
For this discussion, we note that DAMallows the efficient storage and retrieval of entries forwords in alphabetical order and in random order byheadword.
Having the dictionary in a DAM file allowsits entries to be accessed through the WordSmith on-line dictionary system, to be described later.The goal of the second stage of the normalizationprocess is to segment and identify the data in the entry,creating a second DAM file in lexical data base format.Both stages involve some rather idiosyncratic proce-dures that are tied to the various ways different publish-ers code the information on the tapes, particularly sincethe tapes were not prepared with any consideration oflater computational use.
However, as will be apparentlater, our second-stage normalization is a more generalapproach that we envision using with a variety ofmachine-readable dictionary resources.
In what fol-lows, we discuss the normalization of the Collins bilin-gual dictionaries.The first stage of normalization, picking out theheadwords and the associated information, was mostlystraightforward: the headwords are preceded by aunique font code.
Exceptions to this straightforwardprocess were driven by the principle that the end resultof the process should be a direct access dictionaryusable by both people and other systems.
This involvesdefinition of a file keyword which differs from that of thedictionary headword in two important ways: (1) eachkeyword must be unique, and (2) there should be akeyword for each spelling of a word, with the importantexception of inflected forms predictable by morpholog-ical rules, as explained below.
The first constraintassures that dictionary lookup for a word will get al ofthe information associated with that word.
The secondassures that a lookup operation will find a word where itfirst expects to find it, and not inside of some otherentry.Three examples will suffice.
In adherence to the firstconstraint, we combined the entries for the noun quack 1and the verb quack 2 in the English-Italian Collins dic-tionary.
In adherence to the second constraint, wecreated new entries for alternate spellings of words: theGerman-English headord Abtak(e)lung was made intotwo entries, Abtaklung and Abtakelung, the associatedinformation was stored with just one of them, andcross-references to the other spelling were stored withboth.
Since the notation used for alternate spellings ofthis sort does not indicate which one is the morecommon usage, we stored the entry arbitrarily with theshorter form.
A third example: where the German-English dictionary stores a number of compound wordswith the first element of the compound (e.g.
Abend-: has-essen, -friede(n) -gesellschaft, etc.
), we created fullentries for all of them (e.g.
Abendessen, Abendfriede,Abendgesellschaft, e c).
In the process of reconstitutingthe compounds, however, we retained information onthe location of the break to support possible later workon German oun compounds and to conform to anotherprinciple: "do not throw away anything.
"An exception to the constraint that there should beseparate ntries for each spelling of a word, noted220 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsbefore, is that we do not require creation of separateentries for those words which can be derived bymorphological rules.
For example, from GermanLehrer(inJ), which is shorthand for Lehrer and Lehrerinj\[emale\], we created only an entry for Lehrer, leavingthe (in 79 part for later segmentation.
Identification ofthese exceptions i possible using heuristics and doesnot depend on having a system of morphological rulesfor the language of the dictionary.
These heuristics aredifferent for each dictionary.The program which accomplished stage one segmen-tation was written in a variant of PL/I using recursion tohandle the interaction of morphological variants, alter-nate spellings, and reconstruction of compounds.
Theresults have been used directly for several WordSmithapplications.Once we had the dictionary entries segmented intoDAM files, they were ready for the second stage ofnormalization: identifying and labelling the differenttypes of data in the entries in a standard and consistentway.
As we set out to create data bases from the first setof DAM files, it quickly became apparent that the usualdata base architecture, characterized bya fixed numberof fields for each entry and a fixed amount of space foreach, would never work for dictionary entries, becausethere might be one value for some fields (like spelling),but multiple values for others (like part-of-speech,translation, etc.
), or no value at all for still others (e.g.alternate spelling).
Moreover, some information, liketranslation, is associated with values for other informa-tion, like part-of-speech.
These facts lead to the obser-vation that dictionary entries are best represented asshallow hierarchies of attribute-value pairs.
We willreturn to this observation i the section on "lexicai databases".We have developed a parser, the goal of which is toconvert each dictionary entry into a tree structurewhere at each node there is an attribute name and avalue, which can be a simple value, a list of attribute-value pairs, or an entire subtree.
Figure 1 contains theresult of applying the parser to the entry for alert in theCollins English-Italian dictionary.
The attributes in thefigure should be familiar to users of bilingual dictionar-ies; they include hyphenation code ("hyph') ,  undeco-ded pronunciations ("pron"), homograph numbers("hnum"), part-of-speech ("pos"), translations ("spel"under "xlat"), collocations and examples in the sourcelanguage ("gloss" under "xmp"), and translations ofcollocations and examples ("tran" under "xmp").Figure 2 shows how the above-mentioned combina-tion of the multiple entries for quack and other wordsbrings about the creation of a "superhom" node.
Thesuperhom ("superscript homograph") node serves toseparate the multiple entries and preserve the originalsense numbers.
This is important because cross refer-ences in other entries may contain superhomographnumbers as well as sense numbers.
Even in entrieswhere there is only one sense, homograph, or superho-alert\[entry =\[hyph=O\]\[pron=>u71<>ull<>ulS<l>u26<>u17<t>u72<\]\[hom=\[hnum=l\]\[pos=adj\]\[sens =\[xlat=\[note=acute, wide-awake\]\[spel=sveglio(a)\]\]\[xlat=\[note=mind\]\[spel=pronto(a), agile, vivace\]\]\[xlat=\[note=expression\]\[spel=intelligente\]\]\[xlat =\[note=guard\]\[spel=vigile\]\]\]\]\[hom =\[hnum=2\]\[pos=n\]\[sens =\[xlat =\[spel=allarme\]\[gnd=m\]\]\[xmp=\[gloss=to be on the ~\]\[expl=person\]\[tran=stare all'erta\]\]\[xlat=\[note=troops\]\[spel=essere in stato di allarme\]\]\]\]\[hom=\[hnum=3\]\[pos=vt\]\[sens =\[xmp =\[gloss=to ~ sb (to sth)\]tran=avvisare qn (di qc)tran=avvertire qn (di qc)\[xmp =\[gloss=to ~ sb to the dangers of sth\]\[tran=mettere qn in guardia contro qc\]\]\]\]\]Figure 1.
Normalized Collins English-Italian entry for alert.mograph, these attributes are retained to make accesslogic consistent with other entries.The design of the dictionary entry parser began witha familiar model for parsers of English sentences: a setof rules supported by a parsing engine.
Two suchmodels were available to us: a bottom-up, all-pathsstrategy offered by PLNLP (Langendoen and Barnett(toappear)), and a top-down depth-first approach offeredby logic grammars written in Prolog (McCord(1987)).We have versions in PLNLP and VM/Prolog, both ofwhich are basically adequate.
The version which sup-ported the work reported here is the Prolog version;however, we are concerned about future problems withthe top-down approach when we have to process inputthat is corrupt.
Usually for lexicographical processing,one should not have to worry about large amounts ofill-formed input, but the complex history of some of ourtapes forces us to consider possible recovery strategiesfor text with missing or corrupt font-codes.
At thatpoint, it may be necessary to use the bottom-up ap-proach and some recovery strategy analogous to parse-fitting (cf.
Jensen, et a1.
(1983)).Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 221Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsquack\[entry =\[superhom=\[num=l\]\[hyph=O\]\[pron=>u71<kw>u43<k>u72<\]\[hom=\[hnum=l\]\[pos=n\]\[ s ens=\[xlat =\[spel=qua qua\]\[gnd=m inv\]\]\]\]\[hom=\[hnum=2\]\[pos=vi\]\[sens =\[xlat =\[spel=fare qua qua\]\]\]\]\]\[superhom =\[num=2\]\[hyph=O\]\[pron=>u71<kw>u43<k>u72<\]\[hom=\[pos=n\]\[sens=\[xlat =\[note=pej\]\[spel=ciarlatano/a\]\]\[.xlat=\[note=fam: doctor\]\[spel=dottore/essa\]\]\]\]\]\]Figure 2.
Normalized Collins English-Italian entry forquack.Grammars for dictionary entries and grammars fornatural language sentences differ in three importantways: (1) entries can be huge: some are longer than 5000bytes; (2) there is a different definition of a token, and(3) a dictionary grammar can be to a large extentdeterministic.
A consequence of (1) is that it takes alarge amount of storage to parse an entry.
The Prologversion now runs on a 4-megabyte virtual machineunder VM/CMS.
The motivation for (2) comes from thefact that the entries consist of text interspersed withfont-change codes, which are not required to be delim-ited with blanks.
Whereas for ordinary language, thetokens are strings between blanks, for dictionary entriesthey are the strings of characters between a defined setof delimiters, usually font-change codes, but occasion-ally also semicolons, periods, and complex characterstrings.
The delimiters are tokens as well.
In view of (3),one might well ask why anyone would use a powerfultool like PLNLP or a logic grammar to accomplishsomething that does not need such powerful support.The answer is that we now have a dictionary parsingengine, a formalism for rules, and an easily modifiableset of rules, all achieved in a shorter development timethan would have been accomplished with a conven-ti0nal programming language.
Furthermore, stage twoof the acquisition of new dictionaries now requires onlywriting, testing, and running a new set of grammarrules.The parsing engine and support code are written inVM/Pr01og.
The rules, which may be different for eachdictionary, are written using a formalism in the spirit ofthe "definite clause grammars" (DCG's) of Pereira andWarren (1980) and "modular logic grammars" (MLG's)of McCord (1986).
A rule compiler translates the rulesinto Prolog code; this compiler owes much to thegroundwork laid by McCord.
Significant o McCord'swork and necessary for ours here is the distinctionbetween two different kinds of rules, one which pro-duces a structural node (strong non-terminal), and onewhich produces an intermediate list of nodes (weaknon-terminal).
The dictionary entry rule compiler sportsseveral new features required by dictionary entry pars-ing.
These include an 'ignore' operator, which causestokens in the input to be ignored by the automaticstructure builder, an 'insert' operator which causes newelements to be inserted, and the efficient implementa-tion of rules consisting entirely of optional elements.This generalized rule-driven parsing engine will be usedto help develop entry grammars for all our dictionaryresources; a small tool kit for this is already available,and other tools will be added as developmentprogresses.We illustrate the rule formalism with the followingtwo rules from the grammar for the Collins Italian-English dictionary:hom(nonum) == >(pronunc(pos(morph(aux(xref(v)(nilnil ) :reflex \[nil ) :nil ) :nil ) :nil ) :gloss) :(senslist(*) I nil).senslist(num) ==> sens(num) : (senslist(num) \[ nil).The first rule says that an unnumbered homographconsists of a number of optional elements, of which atleast one must be present: pronunciation; choice ofpart-of-speech or verb-reflexive or none; morphologicalinformation; selection of perfect ense auxiliary; a cer-tain kind of cross reference (marked with 'v'); a gloss;or a list of senses.
The hom constituent is declaredelsewhere as a strong nonterminal; a node is created forit, with daughter nodes as defined to the right of thearrow.Prolog backtracking conventions apply; the first suc-cessful parse of an entry is used, even if there might bemore than one possibility.
In the case of most optionalconstituents, the existence of the constituent is triedbefore its absence.
In the case of the gloss in the firstrule, however, the correct result is obtained if theabsence of the gloss is tried first.222 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and MethodsThe second rule says that a list of numbered sensesconsists of a numbered sense and a numbered sense list(or nothing).
The recursive property of this rule allowsthe assembling of any number of numbered senses.
Thesenslist constituent is declared as a weak non-terminal,so that all the sub-constituents defined by the rule willappear as sister nodes at the same level, immediatelydominated not by senslist, but by its parent, hom.The present version of the grammar has been run onparts of the Collins Italian-English and English-Italiandictionaries with a success rate of about 95%.
Some ofthe entries were quite large.
The high success rate ispartly due to the consistency and discipline in theformatting of the Collins Italian dictionaries.
The rulesare still being improved.
There are several reasons forthe remaining 5% - -  and, indeed, also for some of the adhoc devices we have used to successfully parse someentries.
Sometimes the primary clues to parsing, thefont change codes, can be missing because two discretedata segments are contiguous and appear in the samefont.
Alternatively, the font changes can actually occurin the middle of a coherent data segment.
For example,the translation for English queen bee is given as "ape fregina inv".
Here, " f "  is the feminine gender markingfor ape, and "inv" (i.e., "invariable") indicates that theword regina remains invariable in the plural form.In this case, there is grammatical information abouteach of the pieces of the collocation.
In other cases, thefirst piece of grammatical information attaches to thefirst word, and the second to the entire collocation.
Totake a second example, the locution "x or y" is oftenused for phrases in all languages.
The problem is thatboth "x"  and "y"  can be gapped, or otherwise abbre-viated, by deletion of words identical to those found onthe other side of the "or".
An example is in thetranslation given for Italian si sta facendo buio: "it isgrowing o getting dark" (shorthand for "it is growingdark o it is getting dark").For cases like these, we would ideally like to identifyand extract all the information immediately.
It would benice to separate the grammatical information from aperegina, in the first case, and to reconstitute he transla-tions it is growing dark and it is getting dark, in thesecond.
However, we recognize that solving all suchproblems is a never-ending process and would indefi-nitely delay the time that any information is availablefor lexicological analysis.
Consequently, we make apractical distinction between the acquisition and analy-sis phases of our work: information that is not easilydecipherable using our entry parsing tools is left in itsundeciphered form in the initial lexical data base that wecreate.
Thus, for our examples, we should have thefollowing attributes.\[xmp =\[gloss= queen bee\]\[tran= ape .f.
regina .inv.\]\]\[xlat =\[spel= it is growing .o.
getting dark\]?
?
?
\]Later work can be devoted to the detailed analysis ofsuch complex attributes.
The results of such analysiswill be used to create more refined versions of the initiallexical data bases.2.2.
LEXICAL DATA BASES AND THE LEXICAL QUERYLANGUAGE.The lexical data base (LDB) facility is designed to storedictionary information and to provide fast, flexible, andconvenient access to it.
LDBs differ from DAM files inthat the entries are structured into hierarchies of at-tribute-value pairs, such as those that result from theentry parsing described in the previous section.
Thelexical query language (LQL) allows users to query,modify, and format those entries.
A prototypeLDB/LQL system has been built as a testing round forvarious concepts and algorithms and is currently in use,as described in sections 4and 5.
The final version of thefacility is still under development.Dictionary entries are typically organized as shallowhierarchies of attribute-value pairs with a variable num-ber of copies of certain nodes at each level.
Thehierarchical nature of entries stems from the fact thatvalues can be entire subtrees.
Thus, after processing bythe dictionary entry parser described in the previoussection, the entry for quack from the Collins English-Italian dictionary can be stored in an LDB as shown inFigure 3.
In this hierarchy, the "hdw" terminal nodecontains the headword of the entry.
Dictionary workersare familiar with the notion of hierarchically structuredentries of this type.It is important o distinguish between the type ofhierarchies found in dictionary entries and those, forexample, that characterize the syntactic structure ofEnglish sentences.
Both types of hierarchy are, inprinciple, of unbounded size.
Dictionary entries achievetheir unboundedness by iteration: a bilingual dictionaryentry may have any number of homographs, whichmay, in turn, have an arbitrary number of senses, and soforth.
Syntax trees achieve their unboundedness bothby iteration (e.g., coordination) and by recursion (e.g.,self-embedding).
The observation that dictionary entryhierarchies may only iterate is useful for defining lexicaldata bases.
All entries in an LDB can be characterizedby a "design" which is stored once for the data base.The design is a single hierarchy which Serves as a"grammar" for any instance of an entry by naming thepossible parent, siblings, and children of each node typethat may occur.
Actual LDBs are created by storinghierarchically formatted entries in a DAM file with thedesign stored as control information.
Each entry isstored with its headword as key.
Alternate access pathscan be established by building indexes on other at-tributes.Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 223Roy J. Byrd, Nicoletta Caizolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsentry+-hdw quackI+-superhom+-hum I+-hyph 0.I+-pron >u71<kw>u43<k>u72<I+-hem+-hnum I+-pos nI+-sensI+-xlat+-spel qua qua+-gnd m inV+-hom+-hnum 2+-pos ViI+-sensI+-xlat+-spel fare qua qua+-superhom+-hum 2+-hyph 0.I+-pron >u71<kw>u43<k >u72<I+-hem+-pos nI+-sensI+-xlatI +-note PsiI +-spel c ia r la tano /aI+-xlat+-note faro: doctor+-spel dottore/essaFigure 3.
LDB representation f rthe Collins English-Italianentry for quack.LQL allows the user to specify conditions on theattributes of LDB entries.
Only those entries whichsatisfy all conditions become part of the query answer.Further, the user specifies which attributes of the suc-cessful entries remain in the answer and what theiroutput format will be.
The query is stated as entries inthe nodes of a two-dimensional representation f anLDB's design.
The query syntax is reminiscent of thesyntax of the Query-by-Example (QBE) data base querylanguage (Zloof(1974)).
Example-elements, denoted bya leading underscore, are used to relate values ofattributes in a query tree to conditions in a "conditionbox", to display positions in an "output box", and toattribute values in other dictionary entry trees.Part (a) of Figure 4 shows a query which will list allwords which are both nouns and verbs in Englishtogether with their translations in the Collins English-Italian dictionary.
The condition on the noun part-of-speech attribute is simple (it must be "n"  for this database) and is entered irectly in the tree.
The conditionon the verb part of speech is more complex, and theexample-element _vpos is used to relate those attributevalues to the condition box, where they are tested forequality with either "v i"  or "vt" .
The example-ele-ments _word, _ntran, and _vtran are used to map an-swers from the hierarchy into the output box wheretheir eventual output format is schematically repre-sented.
Part (b) shows sample entries from the answerto this query when applied to the Collins English-Italiandictionary.
Such a query might be useful, for example,in a study of the relation between homography inEnglish and derivational morphology in Italian.
(a) Queryentry+-hdw wordI+-superhomI+-hom+-pos nI+-sensI+-xlat+-spel _ntran+-hem+-pos _vposI+-sensI+-xlat+-spel v t ran......... OUTPUT ........ +WORD: wordNOUNS VERBSntran vtran+ ......
CONDITIONS ....... +I I\] vpos  = v i  I _vpos  = v t \ [I II I+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+(b) Answer:WORD: forceNOUNS VERBSforza forzarecostringereWORD: quackNOUNS VERBSqua qua fare qua quaWORD: screamNOUNS VERBSgrido gridarestrillo urlareurloFigure 4.
An LQL query.LQL conditions pecify tests to be performed on thevalues of attributes in the data tree to which nodes in thequery tree may be mapped during query processing.224 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and MethodsTerminal nodes may be tested using a variety of stringand arithmetic operations.
The current prototype imple-mentation i cludes the built-in functions of the REXXprogramming language (IBM (1984)).
Non-terminalnodes may only be tested for equality or inequality withother nodes of the same type.
All nodes may haveaggregate functions (e.g., count, maximum, minimum,etc.)
applied to them and the results may either betested in conditions or be output as part of the queryanswer.
Nodes may also be tested for a null value (i.e.,non-occurrence in a particular entry).As in the query tree itself, the output box maycontain both constants and example-elements.
The con-stants define boilerplate material used to label thevariable instantiations of the example-elements, a  inFigure 4.
Such labelling is perhaps more useful whendictionary entries are presented one at a time, as in theWordSmith on-line dictionary system (Neff andByrd(1987)).A realistic data base system must provide facilitiesfor creating and maintaining data bases.
LDB creation isusually a bulk operation and has been discussed in theprevious ection.
LDB maintenance, onthe other hand,can benefit from the flexibility provided by combining apowerful query processor with the capability to insert,delete, and update parts of the LDB entries.
LQL offersthis flexibility by providing the operators " i ."
(for"insert"), "d . "
(for "delete), and "u ."
(for "update").These are also familiar Query-by-Example concepts andare described in a relational context in IBM(1978), andin the context of LDBs in Neff, et a1.(1988).
LDBmodification will not be discussed further in this paper.We envision that LQL will be available in multipleenvironments.
Naturally as a stand-alone processor, itcan be used to specify batch query and modificationoperations against entire lexical data bases.
This pro-cessor and the flexibility with which it can produceresults tailored to a specific problem make LQL avaluable tool for lexicological research.
We also plan touse LQL as an entry formatting and filtering mechanismfor the WordSmith on-line dictionary system.3.
TOOLS AND METHODOLOGIES.3.1.
TOOLS.In this subsection, we give an alphabetical listing of themajor computational tools, other than the dictionaryentry parser and LDB/LQL, at our disposal for doinglexicological analysis of our MRDs.
The tools aredescribed in more detail in the sections of the paperwhere their use in a certain application is presented.Further detail is available in the references.
Minor toolsused for individual applications are not shown here.The tools are in the form of a variety of programswritten by different members of the Lexical Systemsproject.
The programming languages used includeIBM's EXEC2, REXX, PL/I, and VM/Prolog lan-guages.
What makes them coherent are the fact that weuse the Dictionary Access Method and its utilities as acommon interface to the data and that we share acommon set of methods and goals when we apply them.DAM.
The Dictionary Access Method (Byrd, etal.
(1986b)) provides random and sequential access todictionary entries associated with words which serve assearch keys.
The sequential ccess can be modified toobserve "correct" alphabetical orders in any of severallanguages.
An associated set of utilities makes themanagement of DAM files (including creation, updating,and compression) especially convenient.DICTUTIL.
DICTUTIL is a menu-driven i terface tothe prototype implementation f the lexical data basesystem.
It allows the analyst to create LDBs usingoutput from the parsing of Collins bilingual dictionaryentries and to execute simple queries against thoseLDBs.
The output from a DICTUTIL query is stored asan index to the original LDB (i.e., as a set of value-headword pairs).Filtering.
Filtering, described in Chodorow, et a1.
(1985),is a method for expanding a set of words each of whichhas a given property.
It uses the hypernym relationship,produced by Head Finding, or the synonym relationshipto find new words that are related only to words alreadyin the set.
Such new words are added to the set and theprocess repeats until convergence.
See section 3.2,below.Head Finding.
Head finding, also described in Chodo-row, et a1.
(1985) and in Calzolari(1984a), is a method forautomatically discovering the hypernym (or genus term)for a word by analyzing its dictionary definition.
It isbased on the observation that, at least for nouns andverbs, the syntactic head(s) of a definition is(are) usu-ally the hypernym(s) of the word being defined.
Seesection 3.2, below.Matrix Building.
The Matrix Building program takesordered pairs of words that bear a given relationship toone another (e.g., X is a synonym of Y) and constructsa matrix in which each X word is represented by a row,each Y word is represented by a column, and therelationship between them is indicated in the cell that isformed by the XY intersection.
Synonym matrices areused for clustering synonyms into senses.
Translationmatrices (where X is a translation of Y) are used fortransferring and analyzing sense information using bi-lingual dictionaries.
See sections 5.1 and 5.2, below.Sprouting.
Sprouting, described in Chodorow, eta1.
(1985), is a method for exploring the hypernym,hyponym, and synonym relations.
It finds words relatedto some initial seed by computing the transitive closureover the relation (i.e., "sprouting a tree") beginningfrom the seed.
See 4.1, below.TUPLES.
TUPLES (Byrd(1986b)) is a text analysissystem for finding frequent words and phrases in ordi-nary text.
It uses UDICT to provide lemma normaliza-tion and base normalization as it compares trings fromComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 225Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsthe text.
TUPLES is used in the analysis of dictionarydefinitions for locating and counting defining formulaeand presenting them in a key-word-in-context format.UDICT.
UDICT (Byrd(1986a)) is a computerized lexi-con system.
It supplies a rich variety of lexical infor-mation to natural language processing applications.
Ofparticular interest for lexicography is the fact thatUDICT performs morphological nalysis of words hav-ing inflectional and derivational affixes as well as forcompound words and multi-word entries.
It also has afacility for producing correctly inflected form of Englishwords.
UDICT is a component of TUPLES and Word-Smith.WordSmith.
WordSmith is an on-line dictionary system,described in Neff and Byrd(1987), which allows flexibleaccess to dictionaries tored as DAM files and lexicaldata bases.
Although not explicitly mentioned in theremainder of the paper, it is an important reference toolfor all of our research.
Among its capabilities, Word-Smith provides access to:?
definitions, synonyms, and etymologies from Web-ster's Seventh (Merriam(1963)),?
pronunciations from Webster's Seventh and rhymesbased on them,?
definitions and grammatical information fromLDOCE (Longman(1978)),?
synonyms from the Collins Thesaurus (Collins(1984)),?
entries from the Collins bilingual dictionaries forEnglish/Italian, English/French, English/Spanish, andEnglish/German (Collins(1971, 1978, 1980, 1981)).3.2 .
METHODOLOGIES.At the beginning of our work, we dealt with words asundifferentiated wholes.
That is, we only stored lexicalinformation for a word when it applied to all of theword's senses for a given part of speech.
Even thoughthis practice made us somewhat uncomfortable, it al-lowed us to make significant progress in dictionaryanalysis.
The primary reason, as shown in Figure 5, isthat most words in the dictionaries we analyze aremonosemous; that is, they have just one sense.
Of theapproximately 60,000 headwords in Webster's Seventhhaving fewer than ten senses, almost wo-thirds of them(over 38,000) are monosemous.
Further analysis, notgiven here, shows that, even among words that havemultiple parts-of-speech, each part-of-speech is alsotypically monosemous.
This further increases the pro-portion of the dictionary entries that we can handle withour earliest techniques.
Analysis of Zingarelli(1971)reveals that the facts for Italian are analogous to thosefor English: over all parts-of-speech, 61.7 percent of theItalian entries were monosemous; 38.3 percent werepolysemous.
Further details of the Italian study arepresented in Calzolari(1980).These statistics explain why MRD-based methodol-ogies such as head-finding, sprouting, and filtering areas productive as they are, even though they don'tsenses/word percent # words64.1,%24.1%6.9,%2 6%1 2%0 6,%0 3%0 2%0 0%3844614415411715576993472031074359934 Total wordsFigure 5.
Number of senses per word in Webster's Seventh.handle polysemy correctly.
They are described in sec-tion 3.2.1.
Despite these encouraging statistics, how-ever, it is ultimately necessary to be able to dealcorrectly with the multiple senses of polysemous words.The need is especially acute because the most fre-quently used words tend to also be the most polyse-mous.
Two specific problems associated with polysemyin MRDs are the mapping problem and the addendaproblem.
Section 3.3 describes these problems andoffers solutions for them.3.2.1.
HEAD FINDING, SPROUTING, AND FILTERING.One goal of our research is to extract semantic andsyntactic information from standard ictionary defini-tions for use in constructing lexicons for natural an-guage processing systems.
Dictionaries are rich sourcesof detailed information, but in order to use the informa-tion for natural anguage processing, it must be orga-nized systematically.Amsler(1980) demonstrates that additional structurecan be imposed upon a dictionary by making certainassumptions about the ways in which definitions areconstructed.
Foremost among these assumptions is thatdefinitions consist of one or more "genus" terms, whichidentify superordinate categories of the defined word,and "differentia" which distinguish this instance of thesuperordinate categories from other instances.
By man-ually extracting and disambiguating genus terms for apocket dictionary, Amsler demonstrated the feasibilityof generating semantic hierarchies.It has been our goal to automate the genus extractionand disambiguation processes so that hierarchies can begenerated from full-sized dictionaries, such as Web-ster's Seventh New Collegiate Dictionary.
These effortsare described in detail in Chodorow, et al (1985).
Theybegin with Head Finding.In the definition of car as "a vehicle moving onwheels", the word vehicle serves as the genus term,while "moving on wheels" differentiates cars fromsome other types of vehicles.
Taken as a group, all ofthe word/genus pairs contained in a normal dictionaryfor words of a given part-of-speech form whatAmsler(1980) calls a "tangled hierarchy".
In this hier-archy, each word constitutes a node whose subordinate226 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsnodes are words for which it serves as a genus term.The words at those subordinate nodes are called theword's "hyponyms" .
Similarly, the words at the supe-rordinate nodes for a given word are the genus terms forthe various sense definitions of that word.
These arecalled the given word's "hypernyms" .
Because wordsare polysemous any word may have multiple hy-pernyms; hence the hierarchy is "tangled".Our automated approach to finding the genus termsin definitions is based on the observation that the genusterm for verb and noun definitions is typically, thoughnot always, the syntactic head of the defining phrase.This reduces the task to that of finding the heads of verbphrases and noun phrases.
The syntax of the verbphrase used in verb definitions makes it possible toalways locate its head with a simple heuristic: the headis the single verb following the word to, or if there is aconjunction of verbs following to, then they are allheads.Head finding is much more complex for noun defini-tions because of their greater variety, but we have takenadvantage of the fact that dictionary definitions arewritten in a special and predictable style to develop aheuristic procedure for finding the head.
The procedurecan be described briefly as follows.
First the substringwhich must contain the head is found.
This substring isbounded on the left by a word which obligatorilyappears in prenominal position: a, an, the, its, two,three .
.
.
.
.
twelve, first, second, and so forth.
It isbounded on the right by a word or sequence that canonly appear in postnominal position, such as a relativepronoun, a preposition, or a present participle followinga noun.Once the substring is isolated, the search for the headbegins.
Typically, but not always, it is the rightmostnoun in the substring.
If however, the substring con-tains a conjunction, each conjunct is processed sepa-rately, and multiple heads may result.
If the word foundbelongs to a small class of "empty heads" (words likeone, any, kind, class, manner, family, race, group,complex, etc.)
and is followed by of, then the stringfollowing of is reprocessed in an effort to locate addi-tional heads.
The accuracy of head-finding for nounswas approximately 98 percent, based on a randomsample of the output.The word defined and the head of its definition arethe raw materials for two semi-automatic processeswhich make explicit an important part of the lexicalorganization and featural information that are heldimplicit in dictionaries.
The first of these is sprouting.
'Sprouting, which derives its name from the action ofgrowing a semantic tree from a specified root, uses theresults of head-finding organized into a "hyponymindex", in which each word that was used as a genusterm is associated with all of its hyponyms.
Thus,vehicle would have an entry in the index which reads (inpart):vehicle: ambulance .
.
.
bicycle .
.
.
car .
.
.tanker .
.
.For a given part-of-speech, the hyponym index needs tobe built only once.When invoking the SPROUT program, the user se-lects a root from which a semantic tree is to be grown.The system then computes the transitive closure overthe hyponym index, beginning at the chosen root.
Ineffect, for each new word (including the root), all of itshyponyms are added to the tree.
This operation isapplied recursively, until no further new words arefound.The interactiveness of the sprouting process resultsfrom the fact that the user is consulted for each newword.
If the user decides that the word does not belongto the tree being grown, it can be pruned.
These pruningdecisions result in the disambiguation of the tree,though clearly not by means of an automatic process.The output of a sprouting session, then, is a disam-biguated tree extracted from the tangled hierarchy rep-resented by the hyponym index.
The words it containsall have at least one sense which bears the property forwhich the root was originally selected.
It is important tonote that any serious use of sprouting to locate all wordsbearing a particular semantic feature must involve thecareful selection and use of several roots, because of thevariety of genus terms employed by the Webster'slexicographers.
This will be quite obvious in the casestudies cited below.Filtering, like sprouting, results in lists of wordsbearing a certain property (e.g., \[+human\]).
Unlikesprouting, however, filtering only picks up words all ofwhose senses have the property.
It is based on ahypernym index (the inversion of the hyponym index),in which each word is listed with its hypernyms, as inthe example given here:vehicle: agent equipment means mediumThe filtering process begins with a "seed filter" consist-ing of an initial set of words all of whose senses bearsome required property.
The seed filter may be obtainedin any manner that is convenient.
Several approaches tosetting up a seed filter are discussed in the case studiesthat follow.
Given the filter, the system uses it toevaluate all of the words in the hypernym index.
Anywords, all of whose hypernyms are already in the filter,become candidates for inclusion in the filter during thenext pass.
The user is consulted for each candidate, andmay accept or reject it.
Finally, all accepted words areadded to the filter, and the process is repeated until itconverges.Like sprouting, filtering produces a list of wordshaving some desired property.
In the case of filtering,however, the resulting words have the property in all oftheir senses.3.3.
MULTI-DICTIONARY LEXICOLOGY.When lexicographers create dictionaries, they identifyvarying numbers of senses for words based on suchComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 227Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsthings as dictionary size, editorial policy, availablecitations, and intended audience.
Computer scientists whocreate or add to a dictionary for supporting natural lan-guage processing applications have other criteria whichguide the sense distinctions to be made.
It is not to beexpected that different MRDs will agree on the numbersor orders of senses for the words they contain.
Thedisagreement, however, raises problems for computa-tional lexicology, two of which are discussed in thissection.3.3.1.
THE MAPPING PROBLEM.When processing multiple MRDs, one encounters whatwe have termed the "mapping problem".
The problemis to map the senses given for a word in one dictionaryonto the senses given for the same word in anotherdictionary.
We see a mapping as a symmetric binaryrelation between (word senses in) two dictionaries forthe same language.A solution to the mapping problem is necessary inorder for us to combine information in different pub-lished dictionaries.
For example we might want to relategrammatical information from the Longman Dictionaryof Contemporary English with definitions from Web-ster's Seventh.
Having correct mappings will also allowus to transfer information from one dictionary to an-other when we are building new computerized dictio-naries such as UDICT.
We note that some computer-ized dictionaries will have artificially created sets ofsenses - -  possibly motivated by discriminations re-quired by the application being served.
Our mappingstrategy must be capable of handling these also.Figure 6 contains a general picture of the mappingrelation among 3 different dictionaries (Webster's Sev-enth, Collins English-Italian, and Italian monolingual)together with evidence for establishing a specific mapping.In the figure, the English word E 1 has three senses inWebster's Seventh, two known to refer to humans andW7DictionariesE1\[ 1I l : \ [+Hu\ ]  II 2:\[+Hu\] II 3 : \ [+ In l  It \]one to an instrument.
The Collins English-Italian dictio-nary has two senses for El; with two Italian words I1 and12 given as translations ofthe respective senses.
Finally amonolingual Italian dictionary contains the informationthat I1 is human and that 12 is an instrument.
Usingtechniques and representations to be described below, wecan establish the mapping shown between Webster'sSeventh and Collins English-Italian; the first two senses inWebster's map to the first sense in Collins and the third inWebster's maps to the second in Collins.Solving the mapping problem involves two kinds ofactivities.
First, a representation must be chosen forstoring the mappings as they emerge from variousprocedures intended to find them.
This decision is merelyone of choosing permanent file structure for the binaryrelations.
(Note that we reject he alternative of discover-ing mappings dynamically when they are needed because(1) no single procedure can be devised to find them all and(2) most procedures for finding any mappings would betoo expensive for dynamic application.)
In the secondactivity, we must devise and run procedures for obtainingthe detailed mappings.
In general, multiple procedureswill be required in order to fully cover the words in the twodictionaries to be mapped.Any file system that supports random access ofinformation keyed by words will be suitable for storingthe mappings.
DAM will be used for the mappingrelations built at IBM Research.
In the DAM files,headwords from the two dictionaries being mapped willserve as keys.The information stored for each headword can be aset of binary mappings.
For example, we give here themapping for the word E1 in Webster's Seventh andCollins English-Italian shown in Figure 6:1,2:1;3:2This can be read as: Webster's enses 1 and 2 maponto Collins's sense 1; Webster's ense 3 maps ontoCollins E-I Ital monolingE1i II II \[+Hu\]2 12 12 \[+In\]MappingW7 Coll.
E-II IIE l .1  E l .1  II" IIE1.2 E1.1 II 1IE1.3 E1.2 II IFigure 6.
Mapping from Webster's Seventh to Collins English-Italian.228 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and MethodsCollins's sense 2.
Here the binary mappings are sepa-rated by semicolons.
Each mapping consists of two listsof senses separated by colons.
The leftmost list is forWebster's Seventh, the rightmost for Collins English-Italian.
Since the mappings are symmetric, the assign-ment of left or right to a particular dictionary is arbi-trary.
Each list, in turn, is a comma-delimited set ofsense numbers.
A list may be null if one dictionaryexcludes a sense contained in the other.
(It is importantto note, however, that the success of the mappingstrategy depends on the MRDs involved having fairlycomplete coverage of the language.
Arbitrary omissionof word senses will, in general, lead to incorrectresults.
)With respect o the number of non-null word sensesin the two entries for a given word, there are fourpossible types of mappings, as shown in the table inFigure 7.good as having a single translation.
Likewise, if a singletranslation is polysemous in Italian but all senses bearthe relevant feature, then the observations still hold.Further generalizations should yield a useful procedurefor instantiating (part of) the desired mapping.Many such procedures, possibly augmented by somemanual labor, will be required to build an entire mapping.In particular, we expect hat for highly polysemous andcommon words (such as run, go, turn, thing, make, be,etc., in English) the mappings will have to be done byhand.
Once obtained, however, the mapping will be per-manent and can be used for a variety of purposes, some ofwhich will be discussed in sections 4 and 5 of this paper.3.3.2.
THE ADDENDA PROBLEM.Sense mapping is not the only aspect of work withcomputerized dictionaries where the ability to identifyType # senses # senseson left on rightentry in mapping file1 1 I:I1 >1 i:I,2(,...)>i 1 1,2(,...): i>i >i variedFigure 7.
Types of mappings.We can imagine very simple strategies for populatingthe mapping file for mappings of types 1,2 and 3.
Wesimply build the mapping entry shown in the fourthcolumn of the figure.
Given our previous observationthat most entries in a typical dictionary are monose-mous, these strategies will account for a significantportion of the mapping file entries.Mappings of type 4 will require more involved strat-egies.
As an example of one such strategy, consider theexample in Figure 6.
If I1 is \[+human\] and is the onlytranslation given for the first sense of E1 in CollinsEnglish-Italian, then the first two senses of E1 in Web-ster's may be mapped to the first sense of E1 in Collins.Likewise, if I2 is \[+instr\] and is the only translation givenfor the second sense of E1 in Collins, then we may assumethat the third sense of E1 in Webster's (which is \[+instr\])can be mapped to the second sense of E1 in Collins.
Aprocedure based on these observations will yield themapping shown before, namely:1,2:1;3:2Note that the procedure will include generalizations ofsome aspects of the observations made above.
Forexample, if Collins gives multiple translations for asingle sense of a word, and if all are \[+human\] that is asword senses is critical.
Activities which develop dictio-nary information--either by dictionary and text analysisor by transferring the information from other dictionar-ies-must  ultimately have word senses as targets forthat information.
The information to be associated withthe target dictionary is generally represented either asfeatures, or as relations (i.e., attribute-value pairs).The dictionary to which the information is to beattached may either be a published dictionary, likeWebster's Seventh, or a dictionary intended for com-puter use, such as UDICT.
In the former case, thesenses are those established by the lexicographers.
Inthe latter case, the senses are those which meet theneeds of the application(s) which the dictionary serves.We will also need to decide whether the new infor-mation should actually be added to the existing file forthe target dictionary or, rather, whether it should bekept in a separate "addendum" to the base file, leavingthe base file unchanged.
Ignoring, for the moment, thefirst possibility, we propose the following strategy foradding new information as an addendum to an existingbase dictionary.The information must be addressable by word andsense number, where the sense numbers are those givenin the base dictionary.
However, the need to accessComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 229Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsinformation by sense number, independently of theword, will never arise.
Consequently, a DAM file inwhich headwords from the base file are keys, and inwhich sense numbers are used to organize the datarecords, is a suitable implementation.
I  particular, thefile can be organized as an LDB whose entries have thefollowing structure:entry+-hdwI+-superhomI+-homI+-sens+-snum?
.
.
(new information).
.
.To take a concrete example, suppose that analysistechniques described in this paper have been used todetermine that the first sense of the noun mother inWebster's Seventh is \[+animate\] and takes \[+animate\]nouns as typical objects (the mother of my puppies isgrammatical but *the mother of my buggy is not).Further analysis will show that the third sense of mother("maternal tenderness or affection") is \[+abstract\] and\[+mass\].
The LDB entry for mother in the addenda filefor Webster's Seventh will have the following structure:entry+-hdw motherI+-superhomI+-homI+-sensI +-snum 1I + -an imate  yes\[ +- typ ica lob j  animateI+-sens+-snum 3+-abstract yes+-mass yesWith join operations in LQL, we will have a flexiblemeans of querying, formatting, and otherwise combin-ing the new information with that from the base dictio-nary.4.
APPLIED COMPUTATIONAL LEXICOLOGY:MONOLINGUAL DICTIONARIES.As we pursue our goal of building computerized dictio-naries containing extensive lexical information, we useour data bases, tools, and methods to perform a varietyof analyses.
This section and section 5 present samplesof those analyses.
The variety will be evident in thedifferent types of results ought and methods used.
It isalso reflected in the fact that some of these analyseshave been completed, whereas others are still in prog-ress.4.1.
EXTRACTING SYNTACTIC AND SEMANTICINFORMATION.We have established a list of semantic and syntacticinformation which we intend to obtain for the purposeof building dictionaries uitable for natural languageprocessing.
For nouns, we have chosen informationwhich is primarily motivated by selectional restrictions,such as \[+human\]/\[+nonhuman\], \[+male\]/\[+female\],\[+abstract\]/\[+concrete\].
Also for nouns, we are identi-fying attributes uch as "made_of" and "set_of", andfeatures uch as \[+unit\] (explained below)?
For verbs,we are currently extracting information such as\[+thatcomp\] (takes a that complementizer), \[+active\]/\[+stative\], \[+reflexive\], \[+reciprocal\] nd \[selects for asubject of X type\].
Note that some of this informationcan be represented in terms of binary features, whereasother information should be represented as attribute-value pairs.There appears to be independent consensus in thecomputational literature for choosing information suchas this.
For example, Ingria (1986) refers to a set oflexical features necessary for the construction of largelexicons both for text analysis and generation systems.McCord(1987) refers to the features \[+human\]/\[+nonhuman\], [+male\]/\[+female\], and \[+animal\]/\[+nonanimal\].
For McCord, semantic onstraints canbe exercised by operations on hierarchies of featureswith implicational relations.
Dahlgren andMcDowell(1986) present a system for reasoning aboutnatural language texts.
Among the primitive concepts intheir system are ones we too have identified, such asPROCESS, ACTIVITY, MOTION, ANIMATE, IN-ANIMATE, INDIVIDUAL, COLLECTIVE, and soon.We believe that some of the same lexical informationmay be required for different languages.
So far we havepreliminarily examined Italian and English; the informa-tion we have identified for each language independentlyturns out to be of mutual interest and utility.
Thissuggests to us that as we examine other languages, wemight find a core set of syntactic and semantic attributesof universal linguistic interest which should be repre-sented in the computational lexicon across languages.4.1.1.
\[+MALE\] AND \[+FEMALE\] NOUNS.Marking nouns with a feature reflecting semantic genderin English is motivated by both syntactic and semanticfacts.
Syntactically, processes like pronominalizationare dependent on gender in English.
Semantically,certain verbs, such as impregnate, select for \[+male\] or\[+female\] arguments.
The criteria for defining \[+male\]and \[+female\] are relatively straightforward.
We chosethe option of two features because this is not a binary230 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsdistinction, e.g.
happiness i both \[-male\] and \[-female\],whereas author could be either \[+male\] or \[+female\].We used three methodologies to extract nouns thatare candidates for marking with the gender features.The initial list was extracted from Longman using thesemantic odes which indicate "male human",  "femalehuman",  "male animal", and "female animal".
Wethen ran sprouting and filtering to enlarge the lists.
Thepreliminary lists for the \[+male\] and \[+female\] featuresusing sprouting and filtering were processed by BarbaraAnn Kipfer (see Kipfer(1985)).
The lists so obtainedconsisted of 732 entries for the \[+male\] feature and 519for the \[+female\] feature.In addition to filtering, we searched entries contain-ing the key words man, boy, husband, son, and male forthe candidate \[+male\] words, and woman, girl, wife,daughter, and female for potential \[+female\] words.The simple key-word search technique turned up manyitems, particularly from the domain of botany, that werenot necessarily \[+male\] or \[+female\].
This is not unex-pected, and indeed confirms the motivation for devel-oping an intelligent system such as filtering, rather thana less clever system such as the simple key-wordsearch.
However,  the key-word search provided us withitems that filtering did not catch.
In particular, multi-word entries, such as man Friday or office girl were notpicked up by filtering, but were picked up by thekey-word search.
Also problematic were noun com-pounds within a definition, such as:AVIATRIX (n) a woman aviator - -  called alsoaviatressCHURCHWOMAN (n) a woman adherent of achurchThe heads of these definitions are aviator or adherent,rather than woman.
Yet the words being defined belongto taxonomies other than the ones given by the genusterm of their definitions.
We need to explore othersyntactic ues (such as noun compounding) to member-ship in multiple taxonomies.Finally, we extracted a list of nouns ending in thegender-marked suffixes in English, such as -ess, -ette, or-ix, for \[+female\] and -man for \[+male\].
The number ofendings for \[+female\] is greater since this is the markedcase in English, as seen in cases like president/womanpresident/ *man president or lawyer/ woman lawyer/*man lawyer.
This method accounts for about 15% ofthe words on the final list.Results.
The lists we have for the features \[+male\]and \[+female\] consist of nouns which have the featurein all of its senses.
In the original unprocessed lists fromsprouting and filtering, there were 519 \[+female\] nouns,and 732 \[+male\].
For \[+female\], there were 230 entrieseliminated from the filtered list, 85 added from the keyword search, and 43 added from the backwards earchfor morphologically feminine nouns, giving a final totalof 417 nouns marked \[+female\].
For \[+male\], therewere 540 entries eliminated from the filtered list, and 36added from the key word search, giving a final total of228 nouns marked \[+male\].
Notice that many moremale nouns had to be eliminated.
The reason for this isthat the male case is unmarked in English, and oftenserves as the generic term referring to both the femaleand male version of a type.
For example, author can beused for men or women, whereas authoress alwaysrefers to women.The issue of sense discrimination plays a importantrole in marking words in the lexicon.
Many nouns onour lists were likely to be either inherently \[+male\] orinherently \[+female\].
These words are of two types.The first includes words which have several senses notall of which possess the required feature.
For example,among the definitions for king in Webster's Seventh are:king (n)DEFINITIONS:la (often attrib) a male monarch of a majorterritorial un i t .
.
.2 (cap) GOD, CHRIST3 .
.
.
a chief among competitors4 the principal piece in a set of chessmen5 a playing card .
.
.6 a checker that has been crownedThe last three definitions are clearly inanimate, somarking a chess piece, card, or checker as \[+male\] isincorrect.
However,  in a certain sense, the meaning ofking is intuitively male.
Since we have at the presenttime no way to distinguish senses, we cannot markcertain of the senses \[+male\] while leaving the othersenses neutral.
A particular application, however, mighthave criteria for ignoring certain word senses.
Forexample, in a context dealing with chess, king mightalways bear the features \[-human\] and \[-male\].
Mostother applications might ignore the \[-human\] sense ofking.The other words which are likely to bear a particularfeature are those which are culturally or traditionallyassociated with a particular attribute.
Such words areprostitute (a male prostitute is usually specified assuch), major-general, Pope and beautician.
This infor-mation is usually not explicitly stated in a definition.
Atthe moment, we have no mechanism to assign a feature"likely to be X" ,  but this is in our future plans.Many of the words that appeared on our initial listswere adjectives that are used to refer to nouns which arelikely to be either \[+male\] or \[+female\], such as petite,husky, and platinum-blonde.
If these adjectives weremarked as likely to select for nouns of a particular type,then this marking could in turn be used to identify nounsof that type.
For example, if an adjective like petitewere marked with a feature \[likely to select for a femalenoun\], then texts could be searched to pick out candi-date \[+female\] nouns based on the presence of\[+female\] adjectives.
This could be the basis for an-other methodology for identifying nouns possessing acertain semantic or syntactic feature.
So far our lexicalComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 231Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsinformation is confined to inherent features for nounsbut a logical next step is to also mark selectionalfeatures for adjectives.4.1.2.
\[+UNIT\] NOUNS.We have marked nouns such as dollar, peso, mile, andacre with the feature \[+unit\].
The motivation for thisarose from the following syntactic facts:Verb Agreement.
It is necessary to "break the rules"of verb agreement when using unit nouns.
For example,(1) Three dollars is too much to pay.
(2)*Three dollars are too much to pay.
(3) Four acres is too large for us.
(4) *Four acres are too large for us.Agreement within a Noun Phrase.
Usually a pluralnumber marker requires a plural noun within the samenoun phrase, such as "three dollars" or " four acres.
"However,  within a noun compound, this rule does notapply, even when the noun is clearly plural:(5) This is a three dollar blouse.
(6)*This is a three dollars blouse.
(7) We have a new dish washer.
(8)*We have a new dishes washer.There appear to be some differences between unitnouns and other count nouns.
It seems to be that unitnouns obligatorily require a verb inflected for singlenumber, whereas other count nouns optionally permit asingular or plural verb as shown in (9)-(10).
(9) Four books is too much to read in one night.
(10) Four books are too much to read in one night.The methods we used to identify dictionary entrieswhich should be marked with the \[+unit\] feature aresimilar to those used for \[+male\] and \[+female\].
Weobtained two lists, one from sprouting and filtering, andanother from the key-word search program.Results.
The number of unit nouns on the originalfiltered Longman list was 470.
The key-word searchtechnique turned up 527 more candidate words.
Sub-tracting the 153 words common to both lists, thereremained 844 words to be checked.
Half of these passedthe criteria in all senses, so a total of 422 words aremarked with the \[+unit\] feature.
In addition, a subset ofunit nouns (n=128) were marked with a feature\[+currency\],  of which 26 have irregular plural forms,yielding a total of 154 tokens marked.
This was anunexpected but useful result particularly for judging thegrammaticality of constructions like: Rice is four dollarsa pound.4.1.3.
ACTIVE AND STATIVE VERBS.Markowitz et al (1986) present linguistic motivationsfor extracting an "act ive"  and "stat ive" feature forverbs, and suggest methods for using MRD's as sourcesfor finding active/stative information for verbs.
Wedecided to try to carry out their suggestions using ouranalysis tools and our morphological analyzer on ourdictionary resources.
Our attempt was both a successand a failure.
The success arises from the design and useof our tools.
We achieved clear results, although theresults were not what we had expected.
The disappoint-ment arises from the fact that the active/stative distinc-tion, it turns out, is not a lexical property at all, butrather a property of sentences.
Our tools are gearedtowards lexical properties and are not designed toextract sentence or discourse properties.
It is unclearhow or whether to represent such attributes as lexicalfeatures.Linguistic Motivations.
The standard division be-tween Stative (or Status) verbs and Process (or active ordynamic) verbs revolves around aspectual constraints.The classic test is the ability to take progressive andimperative aspect as shown in the following examples:(I 1) The boy classifies the butterflies.
(12) The boy is classifying the butterflies.
(13) Classify the butterflies!
(14) The boy resembles his father.
(15) *The boy is resembling his father.
(16) *Resemble your father!The verb classify is a process or dynamic verb.
Theprogressive aspect is said to be possible with theseverbs because they indicate a condition which canchange over time.
Similarly, the imperative is possiblebecause one can order someone to do something inorder to change the situation or condition.
Other exam-ples of dynamic verbs are ask, grow, hurt, arrive, andjump.
Stative verbs, on the other hand, do not indicatea condition which can be changed by an action.
Asillustrated in (14)-(16), someone ither resembles ome-one else, or he doesn't.
Also, it is meaningless to ordersomeone to resemble someone lse.
"Resemblance"  isa condition, and not a changeable state.
Process verbsare said to outnumber stative verbs, not only by typebut also by token.
Quirk and Greenbaum(1972) identifyfive types of process verbs, but only two types of stativeverbs.Many stative verbs can be used as process verbs withcertain changes in meaning.
These meaning shifts mightbe reflected in a dictionary by different sense numberssince the fundamental reason for sense division issemantic.
Consider the stative use of the perceptionverb hear:(18) I hear the music on the radio.
(19) * I am hearing the music on the radio.
(20) The judge is hearing your case first.As shown in (19) the progressive is ungrammatical whenthe verb hear is used with a stative meaning, whereas(20) is fully grammatical since the interpretation of hearis as a process.
The problem of identifying both anactive and a stative sense of a verb is related to thesense discrimination problem since a shift from active tostative sense has syntactic implications.Methodology and Tools.
First, we obtained the list ofnoun hyponyms for "ac t "  and "s tate"  from our hy-ponym file.
We then analyzed these nouns using amodified version of the morphological analyzer con-232 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsstrained to give the outermost verb.
The expected resultwas that the outermost verb bases of these nouns wouldexhibit the active and stative properties depending onthe source hypernym.
We give some examples of defi-nitions which have the genus act or state.
Observe thatresemblance which is derived from the stative verbresemble has state as one of as its genus term; abbre-viation has act; and abatement has both act and stateRESEMBLANCE (n) the quality or state ofresembling : .
.
.ABBREVIAT ION (n) the act or result ofabbreviating : .
.
.ABATEMENT (n) the act or process of abating :the state of being abated .
.
.The results of morphological analysis on these samplenouns yield resemble, abbreviate, and abate as theirverb bases.Our processing of Webster's Seventh showed thatthere were 2185 words that had the noun state as ahypernym, for example, abatement, abidance, ability,and abnormality.
The number of nouns having act forthe hypernym was just slightly fewer at 2115, forexample, abatement, abbreviation, aberration, andabidance.
Not all have verbs bases, e.g.
aberration, andabnormality.
Observe that since many nouns have bothhypernyms, the corresponding verbs end up on bothlists, e.g.
for abatement.
The results are shown inFigure 8.Number(21) total state nouns 2185verb bases 646(22) total act nouns 2115verb bases 1570(23) verbs in state only 265verbs in act only 1189verbs in both 381Figure 8.
Results of processing "act" and "state"hyponyms.as input each verb to be tested.
Test sentences areconstructed and the user is asked to respond whether asentence is acceptable, unacceptable, or questionable.There is also opportunity for comments.
The results areplaced in a matrix containing the collected judgementsfor a list of verbs.Before running our lists of derived verbs through thesentence frame tool, we decided to test it with a verbswhich have been said to be undeniably stative andactive.
We took seventy stative verbs and forty actionverbs from Joos (1968) and Quirk and Greenbaum(1972).To illustrate, given the sample list of three verbs in(24), frame tests were automatically generated for eachverb in turn, as shown in (25)-(27).
The list in (24) hasthree verbs: one said to be stative (resemble), one active(throw), and one process (feel).
Results are tabulated foreach response, and given in (28).
(24) resemblethrowfeel(25) "RESEMBLE"TEST 1: He is resembling his mother today.TEST 2: She is resembling his mother more andmore (each day).TEST 3: Resemble your mother now!
(26) "THROW"TEST I: He is throwing the ball today.TEST 2: She is throwing the ball more and more(each day).TEST 3: Throw the ball now!
(27) "FEEL"TEST 1: He is feeling happy today.TEST 2: She is feeling happier more and more(each day).TEST 3: Feel happy now!The table given in (28) shows the results of applying theframe tests to the verbs given in (24): The expectedresult if the distinction were as it is said to be is given in(29).Once we derived the verbs, we were then ready to testthe original hypothesis.
I f  the hypothesis is correct,verbs underlying nouns which have state as a hypernymare likely to be stative.
Similarly, verbs underlyingnouns which have act as a hypernym are likely to beactive.We constructed the following syntactic tests fromJoos (1968) and Quirk and Greenbaum (1972) which aresaid to characterize stativity: (1) the simple presentprogressive, (2) the present progressive with the adver-bial modifier "more  and more each day"  in order to testa continual or process reading, and (3) the imperative.We built a sentence frame generating tool which takes(28) Results of Testing Verbs for Stativity:TEST 1 2 3resemble .......... - + -throw ...... .
.
.
.
.
.
.
.
+ - +feel .. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ + +(29) Ideal PatternTEST 1 2 3active verb ....... + + +stative verb .......Notice that the patterns in (28) do not conform to theexpected patterns in (29).A more complete set of responses using a randomComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 233Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodssubset of the verbs taken from Joos (1968) and Quirkand Greenbaum (1972), again failed to show any clearpatterns or any clear distinctions among sets of verbswhich are supposedly active and stative.Despite the fact that textbooks et out explicit dis-tinctions for verb types, there appear to be no signifi-cant clear-cut core set of verbs in each category.Rather, there appear to be situations in which a givenverb has a stative reading and other situations in whichthe same verb has an active reading.
What is worse,these "situations" can be either semantically or syntac-tically conditioned.Choice of Auxiliary Verb in Italian.
We next exploredthe possibility of a correspondence between verbswhich tend to be active in English and those whichselect for the auxiliary avere in Italian, and similarly forthe stative verbs in English and those requiring essere.Grammar books often state that in Romance languages,the choice of auxiliary verb depends on whether theverb is stative or active.
We performed two tests of thishypothesis.
The first was to extract some verbs from theCollins English-Italian dictionary which were labelledas taking essere "to be".
Verbs are not marked foravere "to have" since it is the unmarked case.
We thenpassed these verbs through a frame test, identical to theone described above (except in Italian, of course.
)Results showed that less than 10% of verbs with auxil-iary essere "to be" pass the test for stativity.
We alsotook the translations of the core set of \[+active\] and\[+stative\] verbs from the sample verbs given inJoos(1968) and in Quirk and Greenbaum(1972).
Wethenexamined the corresponding Italian verbs taken fromthe Collins English-Italian dictionary to determinewhich took essere "to be" and avere "to have".Results showed that almost all of them took the auxil-iary avere "to have".
Thus, we were unable to correlateactivity and stativity with translation of verbs takingessere "to be" and avere "to have".Conclusion.
The active-stative distinction appears tobe an aspectual feature related to discourse.
It is not alexical feature, although certain verbs occur more nat-urally whereas others are more tightly constrained.
Thismeans that the properties active and stative are quali-tatively different from the inherent features \[+human\]or \[+male\] for nouns, or from the selectional features\[+thatcomp\] (takes a that complementizer), or \[requiresa human subject\] for verbs.
For example, a verb whichindicates a true state, such as "resemble", is morelikely to occur in a sentence where stative aspect ispresent, but not necessarily.
This being the case, then itis strictly speaking not the job of the lexicographeralone to identify such features.
Rather, it is the job ofthe syntactician and semanticist to identify such con-texts, along with the lexicographer who should identifyverbs that are potentially active or stative in a givencontext.4.2.
SYNONYMY AND THE PROBLEM OF SENSEDISCRIMINATION.As described above, sense mapping between dictionar-ies is one of the most important and challenging prob-lems facing computational lexicography.
A relatedproblem is sense disambiguation within a single dictio-nary.
Webster 's  Seventh Collegiate Dict ionary (W7)defines car as a kind of 'vehicle' but does not indicatethat it is sense 4 of vehicle which is intended and notsenses 1, 2, or 3.
This lack of an explicit indication ofthe hypernym's intended sense limits the usefulness ofthe hypernym relation as we have currently extracted itbecause hypernymy is actually a relation between wordsenses, not a relation between words.
As a conse-quence, our sprouts from hypernyms require humanintervention toinsure that the proper sense of each nodeis followed.
Without automatic sense disambiguation,taxonomic sprouting is thus relegated to the status of asemi-automatic (rather than a fully automatic) process-ing tool.It has been suggested (Lesk(1986)) that sense disam-biguation of hypernyms might be achieved by compar-ing the differentia (the words which appear with thehypernym, e.g., "moving on wheels" in the definitionof car as a "vehicle moving on wheels") to the wordsthat appear in the definitions of the hypernym's senses.The sense definition which most closely matches thedifferentia would then be selected as the intended senseof the hypernym.
The measure of closeness might besomething as simple as the number of content wordsfound in the intersection of words from the differentiaand the sense definition.
However, to the best of ourknowledge, this has not yet been carried out automati-cally on a large scale.
Our disambiguation of synonymsenses has followed a somewhat different course basedon the assumption that synonymy is a symmetric rela-tion between word senses.Our procedures for sense disambiguation have beenused to process The Collins Thesaurus (CT), which isstored as a DAM file with 16,700 keyed records con-taining a total of 278,000 synonym tokens.
Each key isa headword or a boldface run-on word from a mainentry.
The record associated with each key holds thesynonym tokens organized by part-of-speech of the keyand, within each part-of-speech, by sense number.Many of the entries on the original tape and the printedversion of CT are not marked for part-of-speech, so itwas necessary to use UDICT to analyze the parts-of-speech of the synonyms listed under each sense num-ber.
From this it could then be inferred under whichpart-of-speech t e sense number belonged (Chodorowand Ravin (1987)).In a dictionary-style thesaurus uch as CT, an entryA may have word B listed as a synonym of its nth sense,and entry B may have word A listed as a synonym of itsmth sense.
Based on the assumption that synonyms aresymmetric, we can mark B in entry A as the mth sense234 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsof B, and A in entry B as the nth sense of A.
An exampleof this type of one-to-one mapping in CT is given below.dense (adj) 1 .
.
.
.
condensed .
.
.
solid .
.
.
.2 .
.
.
.
dull .
.
.
s tup id  .
.
.dull (adj) 1. dense .
.
.
.
stupid .
.
.
.2 .
.
.
.
callous .
.
.
unsympathetic .
.
.
.7. drab .
.
.
muted .
.
.
.Here, sense 1 of dull is synonymous with sense 2 ofdense.
37% of the 278,000 synonym tokens show thistype of symmetry.
Of course, there are also mappings ofthe one-to-many variety, but they account for only .5%of the tokens.
We have automatically marked the sensesof all synonyms in one-to-one and one-to-many rela-tions.The third type of mapping, many-to-many, accountsfor just .5% of the total, but it poses a problem for thestrategy outlined above.
This can best be seen byconsidering an example.
Senses 1 and 2 of institution listestablishment as a synonym, and senses 1 and 2 ofestablishment list institution.
Is sense 1 of institutionsynonymous with sense 1 of establishment or withsense 2?
The distribution of the terms institution andestablishment alone cannot answer the question, butthere may an automatic solution based on the fact thatsynonyms tend to cluster into groups that representseparate senses.
(This phenomenon will be describedlater in this section.)
Consider again the case of denseand dull.
Evidence for linking sense 2 of dense withsense 1 of dull comes from the symmetric distribution ofthe two words in the entries.
There is however anotherpiece of evidence for linking sense 2 of dense with sense1 of dull, and that is the co-occurrence of the wordstupid in both synonym lists.
Thus, taking the intersec-tions of synonym lists might form the basis for anautomatic approach to disambiguating the many-to-many mappings.
This is similar in some ways to Lesk'ssuggestion for disambiguating hypernyms by comparingwords that appear with the hypernym to words thatappear in the hypernym's ense definitions.Of course, not all tokens are symmetrically mapped;62% appear in asymmetric relations.
There appear to bemany reasons for the asymmetry.
20% of the itemsoffered as synonyms are phrases or rare words thatsimply do not appear as main entries in the thesaurus.The other 42% which do appear as main entries but donot 'point back' are the focus of current analysis.
Someof the asymmetries appear to be mere oversights on thepart of the lexicographers.
For example, assembly hasthrong listed as a synonym of one of its senses, butthrong does not list assembly as a synonym, although itdoes give assemblage, congregation, multitude, andother related words.
A second type of asymmetryresults when a central sense of one word is synonymouswith a very peripheral sense of another.
One sense ofsay lists add, as in "He  added that he would like to seethe demonstration."
The entry for add does not how-ever contain this peripheral sense and deals only withthe arithmetic add.
This omission is more serious thansimply omitting a word, for here an entire sense seemsto have been left out.
A third situation that gives rise toasymmetric links is when the relation between the twowords is not really synonymy at all but rather hy-pernymy.
For example, book lists manual as a syn-onym, but manual does not list book; instead specialtypes of books such as handbook are given.Once asymmetries have been discovered, it is possi-ble to supply main entries where they are missing (as inthe case of rare words or phrases), to insert synonymsinto already existing entries, and even to create missingsenses of entries.
These enhancements to the thesaurusshould make it more valuable for computational re-search and also as a resource for human use.Other enhancements require a fundamental reorgani-zation of information about synonyms.
In a dictionary-style thesaurus, synonymy is treated as a relationbetween pairs of word senses.
Another approach is toconsider it a relation among words that share a commonsense.
It can thus be viewed as a many-to-one mappingbetween various spellings and a single meaning.
Thewords dense, dull, and stupid all point to one anotherbecause they map to the same sense, although CT, as adictionary-style thesaurus, has no direct way of indicat-ing this fact.The many-to-one structure can be seen more explic-itly in the traditional Roget's style of organization wherea plan of classification clusters words into " ideas" andan alphabetical index of words is used to point to theideas.
In some dictionaries, such as W7, synonymparagraphs list synonyms or nearly synonymous wordsand describe the subtle, often connotative, differencesbetween them.
Main entries for the words point to theparagraphs in much the same way that alphabeticallyindexed words point to ideas in the traditional thesau-rus.Warnesson (1985) describes how words in a dictio-nary-style thesaurus can be automatically organizedinto synonym clusters, each representing a word-sense,and how associations between clusters can be mea-sured.
The goal of restructuring the thesaurus in thisway is not to reproduce an already existing, externallyimposed organizational scheme, such as Roget's, butrather to uncover the implicit structure of the thesaurusbased on the actual interconnections among its words.Although we have not yet completed this work, we havedeveloped the tools required for its first three stages.The first stage in the process is the disambiguation ofsenses, described above.
Next, a set of related words isobtained by sprouting from a selected root node in thesynonym index.
Because the synonym senses havebeen disambiguated, no human intervention is requiredin the sprouting.
The third stage is the production of asquare matrix in which the rows and columns arelabelled with the words obtained from sprouting, andComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 235Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsthe cells contain either a "1"  or a "0"  to indicate thepresence or absence of a synonym relationship betweenthe row word and the column word.
Finally, an algo-rithm (such as the one reported in Marcotorchino(1986))is applied to the matrix to produce the clusters.There are several advantages to organizing syn-onyms into clusters, as suggested by Warnesson (1985).
(1) Space can be saved by having all members of thecluster point to a single location which represents thesense they share, rather than listing in the entry for eachmember all the other members.
(2) The word having thelargest number of connections within the cluster and thefewest connections outside it might be offered to theuser as the synonym which is most representative of thesense.
(3) It is possible to compute the strength ofassociation between clusters based on connections be-tween member and nonmember words.
Strong associa-tions should exist between closely related senses, andweak associations between more distant ones.
In thisway, it might be possible to generate a hierarchicalstructure of senses from the bottom up, i.e., based onthe actual patterns of synonym connections found in thethesaurus.
(4) An on-line thesaurus in which sense-disambiguated words have been structured into clustersprovides the best of both forms of organization.5.
APPLIED COMPUTATIONAL LEXICOLOGY:BILINGUAL DICTIONARIES.5.1.
TRANSFER OF LEXICAL INFORMATION THROUGHBILINGUAL DICTIONARIES.The goal of the experiments described in this section isto take information available from a monolingual dictio-nary for a given language and to use a bilingual dictio-nary as a transfer device to enable the same informationto be transferred toa monolingual dictionary in a secondlanguage.
The clearest and most successful attempt atthis is reported in the first section below on the semanticfeature \[+human\].
Other more speculative attemptswith synonyms and with the syntactic feature\[+thatcomp\] resulted in partial successes and are re-ported in later sections.5.1.1.
\[+HUMAN\] NOUNS.Using sprouting and filtering, we extracted a set ofEnglish nouns with the feature \[+Human\] in all senses.Next, we obtained the set of Italian translations forthose nouns from the Collins English-Italian dictionary.Finally, we re-translated the Italian nouns back intoEnglish, using the Collins Italian-English dictionary.Our results were very positive.
Of the 321 originalEnglish words, 296 Italian words were obtained in theforward (E-I) direction; 63 multi-word translations werealso found.
Re-translating these 296 Italian words in thebackward (I-E) direction yielded 373 English words plus45 English multi-word translations.
Examining the twolists of English words showed that out of the 321original, 157 were not among the final re-translations.Furthermore, out of the 373 re-translations, 167 werenew words not in the original.An analysis of the words obtained gave two clearresults.
First, only six out of the 296 Italian wordsobtained in the forward (E-I) direction did not have thefeature \[+ Human\] in all of their senses.
This feature cantherefore be very well transferred (given our constrainton the polysemy of the source data) to the othermonolingual dictionary.
Second, well over 90% of the167 new English re-translations were also \[+Human\].This demonstrates the effectiveness of this procedure inthe task of enlarging the set of words having a givenfeature.The procedures used for transferring the \[+human\]feature from English to Italian can be viewed as in-stances of a special kind of sprouting called "bilingualsprouting".
In this case, the binary relation throughwhich we propagate features is the translation relation,rather than synonymy, hypernymy, or hyponymy.
Aswith other forms of sprouting, we can view the transla-tion relation as transitive, and therefore we are able topropagate lexical information ot only from one lan-guage to another, but also back to the original anguage.Of course, again as with normal sprouting, we must takecare to control for the effects of polysemy.
Theseconsiderations lead to the definition of two types ofbilingual sprouting: unconstrained and constrained.Unconstrained bi l ingual sprouting.
This form of bilin-gual sprouting assumes that if a word has a particularfeature in all of its senses, then any translations of thatword that are monosemous will also bear that feature.This, of course, excepts grammatical features whichonly apply in one of the two languages involved.
So, forexample, we could not propagate grammatical genderfrom Italian to English.A reasonable procedure that implements this kind ofsprouting would begin with a list of words in the firstlanguage (LI) bearing the feature to be propagated.
Foreach word, assign the feature to any of its monosemoustranslations, given by the bilingual dictionary.
Thisaccomplishes the transfer of the feature to the secondlanguage (L2).
We can continue by inspecting thebilingual dictionary for the other direction (from L2 toL1).
Since the words in L2 are all monosemous, anytranslations from them back into L1 are also likelycandidates for the feature.Despite the care taken by this procedure to deal onlywith monosemous words in L2, polysemy in L1 and inthe bilingual dictionary causes the procedure to yieldlists which must be manually checked.
However, thelabor involved is much less than would be required to dothe work entirely by hand.
As reported above, when theprocedure was used to propagate the \[+Human\] featurefrom English to Italian, we obtained a list of Italianwords which were correctly marked in much more than95 percent of the cases.
Furthermore, when we reflectedthe feature back onto English (with the Italian-Englishdictionary) we obtained a new set of English words noton our original ist.
More than 90 percent of those wereindeed \[+Human\] words.236 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and MethodsConstrained bilingual sprouting.
This type of sprout-ing allows the assignment of features to word senses inL1, rather than to an entire word.
It is based on theobservation that if a single sense of a word in L1 has amonosemous translation i  L2, then both words can beassigned any transferable f atures that either of thembears.
This assumes that the lexicographers who cre-ated the dictionaries being used had comparable ideasabout what constitutes a word sense.
If we can maintainthis assumption, then a sprouting procedure based onour observation should run automatically and yieldquite reliable results.
However, we still imagine that amanual check of the results for validity would be inorder.A further observation is that if the word sense hasmultiple monosemous translations into L2, then each ofthe L2 words can receive the union of their transferablefeatures.
A possible algorithm based on this observationwould use three dictionaries.
First, a monolingual dic-tionary for L1 would contain features to be transferred.Second, a bilingual dictionary from L1 to L2 wouldserve as the conduit for the lexical information and thepointer to words in L2.
Third, a monolingual dictionaryfor L2 would be the target for the information to bepropagated.
The algorithm would scan the words on theL1 side of the bilingual dictionary.
For any word sense(in the monolingual dictionary for L1) with multiplemonosemous translations into L2, we can assign theunion of the transferable f atures (from the L1 wordsense and all L2 translations) toto all of the words in theL2 monolingual dictionary.The two flavors of constrained bilingual sprouting(one for transferring information from L1 to L2, and theother for reflecting it back to L2) would both bestrengthened if we could drop the requirement that thewords in L2 be monosemous, and map to word sensesinstead.
In the section 5.2.1, we discuss an enhance-ment to bilingual dictionaries which will have preciselythis effect.Two important points must be made about bilingualsprouting.
First, it is only feasible and useful when thelexical information available and desired in the twolanguages' monolingual dictionaries are the same.
Asreported above, for Italian and English we have estab-lished lists of features and attributes of interest for bothlanguages, and we are confident hat these lists matchthe requirements of prospective users of the monolin-gual dictionaries.
Second, constrained bilingual sprout-ing obviously requires the prior establishment of thebinary mapping relationships between both monolingualdictionaries and the respective sides of the bilingualdictionary.5.1.2.
SYNONYMS OF "SAY" .We have also made a preliminary attempt to transfer athesaurus entry from the Collins Thesaurus (CT) intoItalian by means of the English-Italian and Italian-English bilingual dictionaries.
The goal of this effort isto determine the feasibility of using a monolingualEnglish thesaurus and bilingual dictionaries to generatea monolingual Italian thesaurus.
The entry selected forthe test was the verb say, which has six senses listed inCT.First, the synonyms of each sense of say and theirsynonyms were obtained through sprouting to form asynonym set.
Next, each of these synonyms was lookedup in the English-Italian dictionary to obtain Italiantranslations.
Of course, many of the translations wereinappropriate.
As an example, one translation of add iscalcolare ("to calculate").
All of the Italian translationswere then looked up in the Italian-English dictionary toobtain their English translations.
These were comparedto the original set of English synonyms.
An Italian wordwas deleted if none of its English translations wasamong those in the original synonym set, except ofcourse for the translation that was responsible for itsbeing included in the first place.
Calcolare was deletedbecause none of its English translations was found inthe original group of English synonyms.
In this way,inappropriate Italian words were eliminated from thetranslation set.A rectangular matrix was constructed with the col-umns representing English words and the rows repre-senting Italian words.
A "1"  was placed at the inter-section of a row and a column if the English word andthe Italian word were listed as translations in thebilingual dictionary; otherwise the intersection held a"0".
An algorithm for clustering binary rectangularmatrices (Marcotorchino(1986)) was used to rearrangethe rows and columns.We were particularly interested in seeing if the clus-ters showed the structure of the original CT entry forsay with its six senses.
The results were somewhatmixed.
For very circumscribed senses, such as thesense of say related to a performance (deliver, orate,perform; recite, etc.
), the translations did indeed showthe appropriate clustering, but for the more generalsenses, the structure, if any, was much harder todiscern.
Although this technique seems to hold somepromise, it is too early to say if it is feasible to use anEnglish thesaurus and bilingual dictionaries to automat-ically produce an Italian thesaurus.5.1.3.
\ [+THATCOMP\]  VERBS.We started with a list of English verbs which werehand-marked with a feature reflecting selection for athat complementizer.
We constructed a matrix of gram-maticality judgments for these verbs with the sentenceframe generating program.
The four test frame condi-tions were: (1) VERB obj that, (2) VERB ~b that, (3)VERB obj infinitival complement, and (4) VERB ~b.infinitival complement.
We then took the translations ofa test set of these verbs.
The same syntactic tests wereapplied to the Italian verbs.
An example of the com-bined matrix is given below.
Each column refers to eachsyntactic test frame.Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 237Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and MethodsTEST 1 2 3 4acknowledge ........... + +ammettere ........... + - + -add .. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+addizionare ..........admit .. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+riconoscere .......... + - + -The results we present should be viewed as prelimi-nary because we took only a small set of some tenEnglish verbs and some twenty-five Italian verbs.
Thefirst observation is that some of the translations do notbelong to the same semantic field as the word in L1.
Anexample is add and addizionare.
Addizionare means to" to  add" in the mathematical sense, but does not mean"to add" in the sense of "comment" .
When we havelexical information applied to senses, disambiguatedreferences in bilingual dictionaries (see section 5.2.1)and sense mappings (see section 5.2.2) such problemsshould disappear.Second, there are the cases of a single entry in L1with multiple translations in L2.
An example of that isanticipate:TEST l 2 3 4anticipate .... .
.
.
.
.
.
.
.
.
.
.
+aspettarsi ............ + - + -precedere ............pregustare ...........prevedere ............ +-  + -prevenire ............. - +There is a possibility of having a that clause for at leastone of the translations in each group of translations.Further, it appears that the pattern could serve as a clueto pick which of the many translations is the one thatcorresponds emantically to the English word sensetaking that.
This applies to the first two test frames, i.e.columns one and two in the table.
For example in thecase of anticipate, aspettarsi and prevedere are the twotranslations with the meaning closest o anticipate whenanticipate is used in the sense taking a that clause.
Ourconclusion is that this process is not feasible completelyautomatically, but that such a procedure could reducemuch searching and guessing.
Further, we believe wecould profit by utilizing information on that clausesfound in example sentences in the bilingual dictionary.This could provide an additional mechanism to elimi-nate non-corresponding translations?5.2.
CROSS-REFERENCES AND SENSE DISAMBIGUATIONIN BILINGUAL DICTIONARIES.Since the Collins bilingual dictionaries play such animportant role in our research, both as a conduit forlexical information transfer and as reference materialfor researchers, it is desirable to make them as accurateand useful as possible.
This section discusses twomethods of enhancing the contents of those bilingualdictionaries.
The first method supplies ense numbers tothe cross-references between the two sides of the dic-tionary.
The second supplies "missing information" inthe form of cross-references which should be there butare not.5.2.1.
DISAMBIGUATING REFERENCES IN COLLINSBILINGUAL DICTIONARIES.The English-Italian side of Collins assigns sense num-bers to the English words.
Similarly, the Italian-Englishside assigns sense numbers to the Italian words.
Adesirable nhancement to the entries in either side is toassign sense numbers to the translations o that theyaccurately point to the other side.
Pictorially, we wantto instantiate sense references represented by the ar-rows in Figure 9.
This result is essentially equivalent tothat discussed in section 4.2 for disambiguating thecross-references in the Collins Thesaurus (CT).In terms of LDBs, we are proposing that the struc-ture of the current translation attribute "x lat"  be en-hanced by the addition of a new "senseno"  (sub)at-tribute identifying the sense number on the other side ofthe dictionary for the word given as the value of the"spel"  (sub)attribute.
The resulting "x lat"  attributewill look like:I II I I l "  I IIE  I I  I I I  IE If I I I t II IFigure 9.
Disambiguating cross-references in a bilingualdictionary.I+-xlatI+-spel+-senseno"~-  .
, ?As with the mapping files, several procedures will berequired in order to determine the correct values forthese new "senseno"  attributes.
Some of these proce-dures will be trivial, as when the words involved aremonosemous.
Recall that, in the case of sense mappingwith Webster's Seventh, the majority of words aremonosemous.
We expect to find the same situation inCollins.
A version of the procedure given in Section 4.2should supply many values.
Specifically, if any sense ofan English word, E l ,  gives a particular Italian word, I1 ,as its translation, and if one sense of I1 gives E1 as itstranslation, then we can mark the II translation of E l(on the English-Italian side) with the sense number of I 1(on the Italian-English side) that had the reciprocal238 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodsreference.
Further, more difficult, and possibly evenmanual procedures may be required to obtain the rest.In any event, once the sense links have been estab-lished, they will remain permanently valid, as is the casefor the inter-dictionary mapping files.An immediate benefit hat would emerge from disam-biguating the references in a bilingual dictionary wouldbe that we can increase the power of constrainedbilingual sprouting.
We would then be able to mapdirectly onto word senses of polysemous words in thesecond language and would not be restricted to usingonly monosemous words.
The reader is invited to referback to the discussion of bilingual sprouting, in section5.1.1, for details.5.2.2.
SYMMETRIZING COLLINS BILINGUAL DICTIONARIES.Inspection of the Collins English-Italian and Italian-English dictionaries reveals that there are many asym-metries between the two sides.
An example of anasymmetry would be a case where an English wordgives an Italian word as its translation, but that Italianword does not, in turn, give the original English word asits translation.
In order to provide improved access tobilingual information by humans and by our analysisprocedures, it seems feasible to produce a symmetrizedbilingual dictionary.
Essentially, this would involveautomatically ocating all asymmetries and adding addi-tional entries and/or translations tomake them symmet-ric.
It should not be difficult to accomplish this task,given LDBs for the two dictionaries.Despite its initial appeal, we suspect that the creationof a fully symmetrized bilingual dictionary may not betotally desirable.
It may be that some of the asymme-tries exist because of valid lexicographic principles, andthat violation of those principles will lead to an unde-sirable result.
Through inspection of the dictionary wehave pinpointed four candidate principles which couldwarrant he use of asymmetric references.1.
Lemma forms of words which are only (or mostly)used in derived forms should not be given astranslations ofwords in the other language.
Thus,although allege appears in the English-Italian dic-tionary, it is never given as the translation of anItalian word.
This may be because allege is almostalways used in English in its past participial form:alleged.
In the corpus analyzed in Kucera andFrancis (1967), alleged occurs ten times whileallege occurs only once.2.
Rare words or words referring to highly specificconcepts may be translated with more generalwords, however a general word should not betranslated with a specific or rare one.
Thus, En-glish moment is translated as Italian importanza inits sense of "importance".
However, since the"importance" meaning of English moment isquite specific, it is not given as the translation ofItalian importanza.
This case seems imilar to theexample of say and add given in the discussion ofasymmetry in CT.
In the case of bilingual dictio-naries, perhaps the principle is to avoid offeringsuch rare word senses to the non-native speaker,who would find them difficult o use correctly.3.
In apparent violation of the preceding principle,we sometimes find a general word translated withwhat are essentially hyponyms in the other lan-guage.
Thus, for book, Collins English-Italiangives quaderno "(notebook)", bustina "(ofmatches)", and blocchetto "(of tickets)".
TheItalian-English side of Collins only gives specifictranslations for these words; none is translated asbook.
This seems entirely equivalent to the case ofbook and manual, covered in the discussion ofCT.
The principle here may be to avoid giving astranslations either hypernyms or generic wordswhich acquire a particular meaning only in aspecific context.
In the case of "book", the hy-pernym would be completely redundant with re-spect o the specific translations that are given andwould thus add no new information.4.
Some translations are not symmetric because thetranslation given does not appear as an entry onthe other side of the dictionary.
For example,Collins lists to parse as one of the English trans-lations for Italian analizzare.
However the En-glish-Italian side of Collins does not contain theword parse.
Similarly, for Italian codifica, Collinsgives codification and for Italian proclamare itgives promulgate; neither of these English wordsappears as a head word on the English-Italian sideof the dictionary.
These omissions may merelyreflect an oversight on the part of the lexicogra-phers.We assume that principles whose only purpose is tosave space can be safely violated by the creation of themissing symmetric links.
Our approach to sorting outthe question of whether symmetrizing is a good thing ingeneral is to generate candidate links and to see if theyviolate these or other principles and whether we like theresult.
We offer the preceding list of types of asymmetrynot as hypotheses tobe verified, but merely as things tolook for during such an analysis.
In the process, we maydiscover the real principles which favor the creation ofasymmetric links, if any exist.
The best outcome may bethat we can build a partially symmetrized dictionarycontaining all desirable symmetry and no undesirableasymmetry.
A further benefit is that we can provide atool which allows the lexicographers to inspect theasymmetries in their dictionaries before committingthem to print.6.
CONCLUSION.We have outlined a research program intended to pro-vide a comprehensive s t of tools and methods for usingmachine readable dictionaries to produce computerizeddictionaries suitable for use in natural language process~ing systems.
We have presented operational tools andanalysis results which we have obtained with them.
AComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 239Roy J. Byrd, Nicoletta Calzolari, Martin S. Chodorow, Judith L. Klavans, Mary S. Neff, Omneya A. Rizk Tools and Methodswide variety of techniques from computer science,linguistics, and lexicography need to be combined inorder to succeed at building the dictionaries we envi-sion.
The project described here provides a coherentframework and computational basis for proceeding.ACKNOWLEDGEMENTS.We are grateful to Yael Ravin and Howard Sachar fortheir major and cont inuing contr ibut ions to the analysisof the Coll ins Thesaurus.
Barbara Kipfer performedsignificant early analyses of semantic features, usingsprouting and filtering.
Gustaf  Neumann and SevedAndersson added important  new funct ions to the Dic-t ionary Access Method,  and Neumann used the newDAM to build WordSmith  funct ions for Webster 's  Sev-enth and Longman.
We thank the group working on theItal ian Machine Readable Dict ionary at the Inst itute forComputat ional  Linguist ics in Pisa for ideas conveyed inthe present work.
We also thank Robert  Ingria forsuggesting the use of direct nominal izat ions in the studyof active and stative verbs.REFERENCES.Alshawi, H. 1985 Processing Dictionary Definitions with PhrasalPattern Hierarchies.
Unpublished paper.
University of CambridgeComputer Laboratory, Cambridge, England.Amsler, R. A.
1980 The Structure of the Merriam-Webster PocketDictionary.
Doctoral Dissertation, TR-164, University of Texas,Austin.Byrd, R. J.
1983 Word Formation in Natural Language ProcessingSystems.
Proceedings of lJCAI-VIlI:704-706.Byrd, R. J.
1986a Dictionary Systems for Office Practice.
IBMResearch Report RC 11872, T.J. Watson Research Center, York-town Heights, New York.Byrd, R. J.
1986b The TUPLES Text Analysis System.
IBM ResearchReport, T.J. Watson Research Center, Yorktown Heights, NewYork.Byrd, R. J. and M. S. Chodorow.
1985 Using an On-line Dictionary toFind Rhyming Words and Pronunciations for Unknown Words.Proceedings of the Association for Computational Linguistics:277-283.Byrd, R. J., J. L. Klavans, M. Aronoff, and F. Anshen.
1986aComputer Methods for Morphological Analysis.
Proceedings ofthe Association for Computational Linguistics: 120-127.Byrd, R. J., G. Neumann, and K. S. B. Andersson.
1986b.
DAM-- ADictionary Access Method.
IBM Research Report, T.J. WatsonResearch Center, Yorktown Heights, New York.Calzolari, N. 1980 Polisemia e Omografia nel Dizionario Macchinadell'Italiano.
In: Studi di Lessicografia ltaliana.
Accademia dellaCrusca, Firenze: Vol.
1I:283-313.Calzolari, N. 1983 Semantic Links and the Dictionary.
In: Burton, S.K.
and D.D.Short.
eds., Proceedings of the Sixth InternationalConference on Computers and the Humanities.
Computer SciencePress, Rockville, Maryland: 47-50.Calzolari, N. 1984a Detecting Patterns in a Lexical Data Base.Proceedings of COLING84.
170-173.Calzolari, N. 1984b Machine-readable Dictionaries, Lexical DataBases, and the Lexical System.
Proceedings of COLING 84.
460.Chodorow, M. S., R. J. Byrd, and G. E. Heidorn.
1985 ExtractingSemantic Hierarchies from a Large On-line Dictionary.
Proceed-ings of the Association for Computational Linguistics: 299-304.Chodorow, M. S. and Y. Ravin.
1987 (in preparation) Analyzing theCollins Thesaurus.
IBM Research Report, T.J. Watson ResearchCenter, Yorktown Heights, New York.Collins.
1971.
Collins Spanish Dictionary: Spanish-English.
English-Spanish.
Collins Publishers, Glasgow.Collins.
1978 Collins Robert French Dictionary: French-English.English-French.
Collins Publishers, Glasgow.Collins.
1980 Collins German Dictionary: German-English.
English-German.
Collins Publishers, Glasgow.Collins.
1980 Collins Sansoni Italian Dictionary: Italian-English.English-Italian.
Collins Publishers, Glasgow.Collins.
1984 The New Collins Thesaurus.
Collins Publishers, Glasgow.Dahlgren, K., and J. McDowell.
1986 Kind Types in KnowledgeRepresentation.
Proceedings of COLING86.
Bonn, Germany.Heidorn, G. E., K. Jensen, L. A. Miller, R. J. Byrd, and M. S.Chodorow.
1982 The EPISTLE Text-Critiquing System.
IBMSystems Journal 21:305-326.IBM.
1978 Query-by-Example: Terminal Users Guide.
IBM form no.SH20-2078.Ingria.
R. J. P. 1986 Structuring The Lexicon.
Tutorial Session Paperpresented at the 24th Annual Meeting of the Association forComputational Linguistics.
Columbia University, New York,New York.Jensen, K., G.E.
Heidorn, L.A. Miller, and Y. Ravin.
1983 ParseFitting and Prose Fixing: Getting aHold on lll-formedness.
AJCL9(3-4): 123-36.Joos, M. 1968 The English Verb: Form and Meanings.
University ofWisconsin Press, Madison, Wisconsin.Kipfer, B.
A.
1985 Marking Semantic Features to Enhance a Dictio-nary for Natural Language Processing Applications.
Lexical Sys-tems Project Report, T.J. Watson Research Center, YorktownHeights, New York.Kucera, H. and W. N. Francis.
1967 Computational Analysis ofPresent-Day American English.
Brown University Press, Provi-dence, Rhode Island.Langendoen, D. T., and H. M. Barnett.
1987 (to appear) PLNLP: ALinguist's Introduction.
IBM Research Report.
T.J. Watson Re-search Center, Yorktown Heights, New York.Lesk, M. 1986 Automatic Sense Disambiguation Using Machine-readable Dictionaries: How to Tell a Pine Cone from an Ice CreamCone.
Proceedings of 1986 SIGDOC Conference.Longman.
1978 Longman Dictionary of Contemporary English.Longman Group, London.Marcotorchino, F. 1986 Maximal Association Theory.
In:Classification as a Tool for Research.
W. Gaul and M.
Schader,eds., North Holland.Markowitz, J., T. Ahlswede, M. Evans.
1986 Semantically SignificantPatterns in Dictionary Definitions.
Proceedings of the AnnualMeeting of the Association for Computational Linguistics: 112-119.McCord, M. C. 1987 Natural Language Processing in Prolog.
In:Walker.
A., McCord, M., Sowa.
J., and Wilson, W., KnowledgeSystems and Prolog.
Addison-Wesley, Waltham, Massachusetts.Merriam.
1963 Webster's Seventh New Collegiate Dictionary G. & C.Merriam, Springfield, Massachusetts.Michiels, A.
1982 Exploiting a Large Dictionary Data Base.
PhD.Dissertation, University of Liege, Liege, Holland.Neff, M. S. and R. J. Byrd.
1987 WordSmith Users Guide.
IBMResearch Report.
T.J. Watson Research Center, YorktownHeights, New York.Neff, M. S., R. J. Byrd, and O.
A. Rizk.
1988 Creating and QueryingLexical Data Bases.
Proceedings of the Association for Compu-tational Linguistics Second Applied Conference on Natural Lan-guage Processing: 84-92.Pereira, F. and D. Warren.
1980 Definite Clause Grammars for LanguageAnalysis - -  a Survey of the Formalism and a Comparison withAugmented Transition Networks.
Artificial Intelligence 13:231-178.Quirk, R., S. Greenbaum, G. Leech, and J. Svartvik.
1972 AGrammar of Contemporary English.
Longman, London.Tompa, F. 1986 (unpublished) Database Design for a Dictionary of theFuture.
University of Waterloo, Waterloo, Canada.Warnesson, I.
1985 Optimization of Semantic Relations by DataAggregation Techniques.
Journal of Applied Stochastic Modelsand Data Analysis, 1(2).240 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
