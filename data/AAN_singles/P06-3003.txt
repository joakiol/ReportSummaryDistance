Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 13?18,Sydney, July 2006. c?2006 Association for Computational LinguisticsSub-sentential Alignment Using Substring Co-Occurrence CountsFabien CromieresGETA-CLIPS-IMAGBP53 38041 Grenoble Cedex 9Francefabien.cromieres@gmail.comAbstractIn this paper, we will present an efficientmethod to compute the co-occurrencecounts of any pair of substring in a paral-lel corpus, and an algorithm that makeuse of these counts to create sub-sentential alignments on such a corpus.This algorithm has the advantage of be-ing as general as possible regarding thesegmentation of text.1 IntroductionAn interesting and important problem in theStatistical Machine Translation (SMT) domain isthe creation of sub-sentential alignment in a par-allel corpus (a bilingual corpus already aligned atthe sentence level).
These alignments can later beused to, for example, train SMT systems or ex-tract bilingual lexicons.Many algorithms have already been proposedfor sub-sentential alignment.
Some of them focuson word-to-word alignment ((Brown,97) or(Melamed,97)).
Others allow the generation ofphrase-level alignments, such as (Och et al,1999), (Marcu and Wong, 2002) or (Zhang, Vo-gel, Waibel, 2003).
However, with the exceptionof Marcu and Wong, these phrase-level align-ment algorithms still place their analyses at theword level; whether by first creating a word-to-word alignment or by computing correlation co-efficients between pairs of individual words.This is, in our opinion, a limitation of these al-gorithms; mainly because it makes them relyheavily on our capacity to segment a sentence inwords.
And defining what a word is is not aseasy as it might seem.
In peculiar, in manyAsians writings systems (Japanese, Chinese orThai, for example), there is not a special symbolto delimit words (such as the blank in most non-Asian writing systems).
Current systems usuallywork around this problem by using a segmenta-tion tool to pre-process the data.
There are how-ever two major disadvantages:- These tools usually need a lot of linguisticknowledge, such as lexical dictionaries andhand-crafted segmentation rules.
So using themsomehow reduces the ?purity?
and universalityof the statistical approach.- These tools are not perfect.
They tend to bevery dependent on the domain of the text theyare used with.
Besides, they cannot take advan-tage of the fact that there exist a translation of thesentence in another language.
(Xu, Zens and Ney,2004) have overcome partof these objections by using multiple segmenta-tions of a Chinese sentence and letting a SMTsystem choose the best one, as well as creating asegmentation lexicon dictionary by consideringevery Chinese character to be a word in itself andthen creating a phrase alignment.
However, it isprobable that this technique would meet muchmore difficulties with Thai, for example (whosecharacters, unlike Chinese, bear no specific sense)or even Japanese (which use both ideograms andphonetic characters).Besides, even for more ?computer-friendly?languages, relying too much on typographicwords may not be the best way to create analignment.
For example, the translation of a setphrase may contain no word that is a translationof the individual words of this set phrase.
Andone could consider languages such as German,which tend to merge words that are in relation ina single typographic word.
For such languages, itcould be a good thing to be able to create align-ment at an even more basic level than the typo-graphic words.These thoughts are the main motivations forthe development of the alignment algorithm wewill expose in this paper.
Its main advantage isthat it can be applied whatever is the smallest13unit of text we want to consider: typographicword or single character.
And even when work-ing at the character level, it can use larger se-quence of characters to create correct alignments.The problem of the segmentation and of thealignment will be resolved simultaneously: a sen-tence and its translation will mutually induce asegmentation on one another.
Another advantageof this algorithm is that it is purely statistical: itwill not require any information other than theparallel corpus we want to align.It should be noted here that the phrase-leveljoint-probability model presented in (Marcu andWong) can pretend to have the same qualities.However, it was only applied to word-segmentedtexts by its authors.
Making use of the EM train-ing, it is also much more complex than our ap-proach.Before describing our algorithm, we will ex-plain in detail a method for extracting the co-occurrence counts of any substring in a parallelcorpus.
Such co-occurrence counts are importantto our method, but difficult to compute or storein the case of big corpora.2 Co-Occurrence counting algorithm2.1 Notation and definitionsIn the subsequent parts of this paper, a sub-string will denote indifferently a sequence ofcharacters or a sequence of words (or actually asequence of any typographic unit we might wantto consider).
The terms ?elements?
will be usedinstead of ?word?
or ?characters?
to denote thefundamental typographic unit we chose for agiven language.In general, the number of co-occurrences oftwo substrings S1 and S2 in a parallel corpus isthe number of times they have appeared on theopposite sides of a bi-sentence in this corpus.
Itwill be noted N(S1,S2).
In the cases where S1 andS2 appears several times in a single bi-sentence(n1 and n2 times respectively), we might count 1,n1*n2 or min(n1,n2) co-occurrences.
We will alsonote N(S1) the number of occurrences of S1 in thecorpus.2.2 The Storage ProblemCounting word co-occurrences over a parallelcorpus and storing them in a data structure suchas a Hash table is a trivial task.
But storing theco-occurrences counts of every pair of substringpresents much more technical difficulties.
Basi-cally, the problem is that the number of values tobe stored is much greater when we consider sub-strings.
For two sentences with N1 and N2 wordsrespectively, there are N1*N2 words that co-occur;but the number of substrings that co-occur isroughly proportional to (N1*N2)^2.
Of course,most substrings in a pair of sentences are notunique in the corpus, which reduces the numberof values to be stored.
Still, in most cases, it re-mains impractical.
For example, the Japanese-English BTEC corpus has more than 11 millionunique English (word-) substrings and more than8 million unique Japanese (character-) substrings.So there are potentially 88,000 billion co-occurrence values to be stored.
Again, most ofthese substrings do not co-occur in the corpus, sothat non-zero co-occurrences values are only afraction of this figure.
However, a rough estima-tion we performed showed that there still wouldbe close to a billion values to store.With a bigger corpus such as the EuropeanParliament Corpus (more than 600,000 sentencesper languages)  we have more than 698 millionsunique English (word-) substrings and 875 mil-lions unique French (word-) substrings.
Andthings get much worse if we want to try to workwith characters substrings.To handle this problem, we decided not to tryand store the co-occurrences count beforehand,but rather to compute them ?on-the-fly?, whenthey are needed.
For that we will need a way tocompute co-occurrences very efficiently.
Wewill show how to do it with the data structureknown as Suffix Array.2.3 Suffix ArraysSuffix Arrays are a data structure allowing for(among other things) the efficient computation ofthe number of occurrences of any substringwithin a text.
They have been introduced byMamber and Myers (1993) in a bioinformaticscontext.
(Callison-Burch, Bannard and Scroeder,2005) used them (in a way similar to us) to com-pute and store phrase translation probabilitiesover very large corpora.Basically, a Suffix Array is a very simple datastructure: it is the sorted list of all the suffixes ofa text.
A suffix is a substring going from onestarting position in the text to its end.
So a text ofT elements has T suffixes.An important point to understand is that wewon?t have to store the actual suffixes in memory.We can describe any suffix by its starting posi-tion in the text.
Hence, every suffix occupies aconstant space in memory.
Actually, a commonimplementation is to represent a suffix by amemory pointer on the full text.
So, on a ma-14chine with 32-bit pointers, the Suffix Array of atext of T elements occupy 4*T bytes.
The timecomplexity of the Suffix Array construction isO(T*log(T)) if we build the array of the suffixesand then sort it.We will now describe the property of the Suf-fix Array that interest us.
Let S be a substring.Let pf be the position (in the Suffix Array) of thefirst suffix beginning with substring S and pl bethe position of the last such suffix.
Then everysuffix in the Array between positions pf and plcorresponds to an occurrence of S. And everyoccurrence of S in the text corresponds to a suf-fix between pf and pl.pf and pl can be retrieved in O(|S|*log T) witha dichotomy search.
Beside, N(S)=pl-pf+1; sowe can compute N(S) in O(|S|*log T).
We willnow see how to compute N(S1,S2) for two sub-strings S1 and S2 in a parallel corpus.2.4 Computing Co-Occurrences using Suf-fix ArrayA Suffix Array can be created not only fromone text, but also from a sequence of texts.
In thepresent case, we will consider the sequence ofsentences formed by one side of a parallel corpus.The Suffix Array is then the sorted list of all thesuffixes of all the sentences in the sequence.
Suf-fixes may be represented as a pair of integer (in-dex of the sentence, position in the sentence) oragain as a pointer (an example using integer pairsis shown on Figure 1).We can implement the Suffix Array so that,from a suffix, we can determine the index of thesentence to which it belongs (the computationalcost of this is marginal in practical cases and willbe neglected).
We can now compute pf and pl fora substring S such as previously, and retrieve thesentence indexes corresponding to every suffixbetween positions pf and pl in the Suffix Array,This allow us to create an ?occurrence vector?
: amapping between sentence indexes and the num-ber of occurrences of S in those sentences.
Thisoperation takes O(pl-pf), that is O(N(S)).
(Figure1.
shows an occurrence vector for the substring?red car?
)We can now efficiently compute the co-occurrence counts of two substrings S1 and S2 ina parallel corpus.We compute beforehand the two Suffix Arraysfor the 2 sides of the parallel corpus.
We canthen compute two occurrence vectors V1 and V2for S1 and S2 in O(N(S1)+|S1|*log(T1)) andO(N(S2)+|S2|*log(T2)) respectively.With a good implementation, we can use thesetwo vectors to obtain N(S1,S2) inO(min(size(V1),size(V2))), that isO(min(N(S1),N(S2)).Hence we can compute NbCoOcc(S1,S2) forany substring pair (S1,S2) inO(N(S2)+|S2|*log(T2)+N(S1)+|S1|*log(N1))).
Thisis much better than a naive approach that takesO(T1*T2) by going through the whole corpus.Besides, some simple optimizations will substan-tially improve the average performances.2.5 Some Important OptimizationsThere are two ways to improve performanceswhen using the previous method for co-occurrences computing.Firstly, we won?t compute co-occurrences forany substrings at random.
Typically, in the algo-rithm described in the following part, we com-pute N(S1,S2) for every substring pairs in a givenbi-sentence.
So we will compute the occurrencevector of a substring only once per sentence.Secondly, the time taken to retrieve the co-occurrence count of two substrings S1 and S2 ismore or less proportional to their frequency inthe corpus.
This is a problem for the average per-formance: the most frequent substrings will bethe one that take longer to compute.
This sug-gests that by caching the occurrence vectors ofthe most frequent substrings (as well as their co-occurrence counts), we might expect a good im-provement in performance.
(We will see in thenext sub-section that caching the 200 most fre-A small monolingual corpusindex sentence1 The red car is here2 I saw a blue car3 I saw a red carOccurrence Vector of?red car?index nbOcc1 12 03 1Suffix ArrayArrayindexSuffix Position Suffix0 2,3 a blue car1 3,4 a red car2 2,4 blue car3 2,6 car4 3,5 car5 1,3 car is here6 1,5 here7 2,1 I saw a blue car8 1,1 I saw a red car9 1,4 is here10 1,2 red car is here11 3,5 red car12 2,2 saw a blue car13 3,3 saw a red car14 1,1 The red car is hereFigure 1.
A small corpus, the corresponding suf-fix array, and an occurrence vector15quent substrings is sufficient to multiply the av-erage speed by a factor of 50)2.6 Practical Evaluation of the Perform-ancesWe will now test the computational practicalityof our method.
For this evaluation, we will con-sider the English-Japanese BTEC corpus(170,000 bi-sentences, 12MB), and the English-French Europarl corpus (688,000 bi-sentences,180 MB).
We also want to apply our algorithm towestern languages at the character level.
How-ever, working at a character level multiply thesize of the suffix array by about 5, and increasethe size of the cached vectors as well.
So, be-cause of memory limitations, we extracted asmaller corpus from the Europarl one (100,000bi-sentences, 20MB) for experimenting on char-acters substrings.The base elements we will choose for our sub-strings will be: word/characters for the BTEC,word/word for the bigger EuroParl, andword/characters for the smaller EuroParl.
Wecomputed the co-occurrence counts of every sub-strings pair in a bi-sentence for the 100 first bi-sentences of every corpus, on a 2.5GHz x86computer.
We give the average figures for dif-ferent corpora and caching strategies.These results are good enough and show thatthe algorithm we are going to introduce is notcomputationally impracticable.
The cache allowsan interesting trade-off between the perform-ances and the used memory.
We note that theproportional speedup depends on the corpus used.We did not investigate this point, but the differ-ent sizes of corpora (inducing different averageoccurrence vectors sizes), and the differences inthe frequency distribution of words and charac-ters are probably the main factors.3 Sub-sentential alignment3.1 The General PrincipleGiven two substrings S1 and S2, we can usetheir occurrence and co-occurrence counts tocompute a correlation coefficient (such as thechi-square statistic, the point-wise mutual infor-mation or the Dice coefficient).The basic principle of our sub-sentential align-ment algorithm will simply be to compute a cor-relation coefficient between every substring in abi-sentence, and align the substrings with thehighest correlation.
This idea needs, however, tobe refined.First, we have to take care of the indirect asso-ciation problem.
The problem, which was de-scribed in (Melamed, 1997) in a word-to-wordalignment context, is as follows: if e1 is the trans-lation of f1 and f2 has a strong monolingual asso-ciation with f1, e1 and f2 will also have a strongcorrelation.
Melamed assumed that indirect asso-ciations are weaker than direct ones, and pro-vided a Competitive Linking Algorithm that doesnot allow for a word already aligned to be linkedto another one.
We will make the same assump-tion and apply the same solution.
So our algo-rithm will align the substring pairs with the high-est correlation first, and will forbid the subse-quent alignment of substrings having a part incommon with an already aligned substring.
Aside-effect of this procedure is that we will beconstrained to produce a single segmentation onboth sentences and a single alignment betweenthe components of this segmentation.
Accordingto the application, this might be what we arelooking for or not.
But it must be noted that,most of the time, alignments with variousgranularities are possible, and we will only beable to find one of them.
We will discuss the is-sue of the granularity of the alignment in part 3.3.Besides, our approach implicitly considers thatthe translation of a substring is a substring (thereare no discontinuities).
This is of course not thecase in general (for example, the English word?not?
is usually translated in French by?ne?pas?).
However, there is most of the time agranularity of alignment at which there is no dis-continuity in the alignment components.Also, it is frequent that a word or a sequenceof words in a sentence has no equivalent in theopposite sentence.
That is why it will not bemandatory for our algorithm to align every ele-ment of the sentences at all cost.
If, at any point,the substrings that are yet to be linked have cor-relation coefficients below a certain threshold,the algorithm will not go further.So, the algorithm can be described as follow:1- Compute a correlation coefficient for all thesubstrings pairs in e and f  and mark all the ele-ments in e and f as free.Corpus Cache(cachedsubstrg )AllocatedMemory(MB)CoOcccomputed(per sec.
)bisentencesprocessed (persec.
)BTEC 0 22  7k 1.2BTEC  200 120 490k 85EuroParl 0 270  3k 0.4EuroParl 400 700 18k 1.2SmallEuroParl0 100 4k 0.04SmallEuroParl400 300 30k 0.3162- Among the substrings which contain onlyfree element, find the pair with the highest corre-lation.
If this correlation is not above a certainthreshold, end the algorithm.
Else, output a linkbetween the substrings of the pair.3- Mark all the elements belonging to thelinked pair as non-free.4- Go back to 2It should be noted that correlation coefficientsare only meaningful data is sufficiently available;but many substrings will appear only a couple oftimes in the corpus.
That is why, in our experi-ments we have set to zero the correlation coeffi-cient of substring pairs that co-occur less than 5times (this might be a bit conservative, but theBTEC corpus we used being very redundant, itwas not too much of a restriction).3.2 Giving a preference to bigger align-ments.A problem that arose in applying the previousalgorithm is a tendency to link incomplete sub-strings.
Typically, this happen when a substringS1 can be translated by two substrings S2 and S2?,S2 and S2?
having themselves a common sub-string.
S1 will then be linked to the common partof S2 and S2?.
For example, the English word?museum?
has two Japanese equivalents: ??
?and ???.
In the BTEC corpus, the commonpart (?)
will have a stronger association with?museum?, and so will be linked instead of thecorrect substring (???
or ???
).To prevent this problem, we have tried tomodify the correlation coefficients so that theyslightly penalize shorter alignment.
Precisely, fora substring pair (S1,S2), we define its area as?length of S1?
*?length of S2?.
We then multiplythe Dice coefficient by area(S1,S2) and the chi-square coefficient by log(area(S1,S2)+1).
Theseformulas are very empiric, but they created aconsiderable improvement in our experimentalresults.Linking the bigger parts of the sentences firsthas another interesting effect: bigger substringspresent less ambiguity, and so linking them firstmay prevent further ambiguities to arise.
For ex-ample, with the bi-sentence ?the cat on thewall?/?le chat sur le mur?.
Each ?the?
in theEnglish sentence will have the same correlationwith each ?le?
in the French sentence, and so thealgorithm cannot determine which ?the?
corre-spond to which ?le?.
But if, for example ?thecat?
has been previously linked to ?le chat?,there is no more ambiguity.We mentioned previously the issue of thegranularity of alignments.
These ?alignment sizepenalties?
could also be used to tune the granu-larity of the alignment produced.3.3 Experiments and EvaluationsAlthough we made some tests to confirm thatcomputation time did not prevent our algorithmto work with bigger corpus such as the EuroParlcorpus, we have until now limited deeper ex-periments to the Japanese-English BTEC Corpus.That is why we will only present results forthis corpus.
For comparison, we re-implementedthe ISA (Integrated Segmentation Alignment)algorithm described in (Zhang, Vogel andWaibel, 2003).
This algorithm is interesting be-cause it is somehow similar to our own approach,in that it can be seen as a generalization ofMelamed?s Competitive Linking Algorithm.
It isalso fairly easy to implement.
A comparison withthe joint probability model of Marcu and Wong(which can also work at the phrase/substringlevel) would have also been very interesting, butthe difficulty of implementing and adapting thealgorithm made us delay the experiment.After trying different settings, we chose to usechi-square statistic as the correlation coefficientfor the ISA algorithm, and the dice coefficientfor our own algorithm.
ISA settings as well asthe ?alignment size penalties?
of our algorithmwere also tuned to give the best results possiblewith our test set.
For our algorithm, we consid-ered word-substrings for English and characterssubstrings for Japanese.
For the ISA algorithm,we pre-segmented the Japanese corpus, but alsotried to apply it directly to Japanese by consider-ing characters as words.Estimating the quality of an alignment is not aneasy thing.
We tried to compute a precision and arecall score in the following manner.
Precisionwas such that:Nb of correct linksPrecision= Nb of outputted linksCorrect link are counted by manual inspectionof the results.
Appreciating what is a correct linkis subjective; especially here, where we considermany-words-to-many-characters links.
Overall,the evaluation was pretty indulgent, but tried tobe consistent, so that the comparison would notbe biased.Computing recall is more difficult: for a givenbi-sentence, multiple alignments with differentgranularities are possible.
As we are only tryingto output one of these alignments, we cannot de-fine easily a ?gold standard?.
What we did was to17count a missed link for every element that wasnot linked correctly and could have been.
Wethen compute a recall measure such that:Nb of correct links                 .
Recall= Nb of correct links+ Nb of missed linksThese measures are not perfect and inducesome biases in the evaluation (they tend to favoralgorithms aligning bigger part of the sentence,for example), but we think they still give a goodsummary of the results we have obtained so far.As can be seen in the following table, our al-gorithm performed quite well.
We are far fromthe results obtained with a pre-segmentation, butconsidering the simplicity of this algorithm, wethink these results are encouraging and justifyour initial ideas.
There is still a lot of room forimprovement: introducing a n-gram languagemodel, using multiple iterations to re-estimatethe correlation of the substrings...That is why we are pretty confident that wecan hope to compete in the end with algorithmsusing pre-segmentation.Also, although we did not make any thoroughevaluation, we also applied the algorithm to asubset of the Europarl corpus (cf.
2.6), wherecharacters where considered the base unit forFrench.
The alignments were mostly satisfying(seemingly better than with the BTEC).
Buthardly any sub-word alignments were produced.Some variations on the ideas of the algorithm,however, allowed us to get interesting (if infre-quent) results.
For example, in the pair (?I wouldlike?/ ?Je voudrais?
), ?would?
was aligned with?rais?
and ?voud?
with ?like?.4 Conclusion and future workIn this paper we presented both a method foraccessing the co-occurrences count for any sub-string pair in a parallel corpus and an algorithmtaking advantage of this method to create sub-sentential alignments in such a corpus.We showed our co-occurrence countingmethod performs well with corpus commonlyused in Statistical Machine Translation research,and so we think it can be a useful tool for thestatistical processing of parallel corpora.Our phrase level alignment algorithm gave en-couraging results, especially considering thereare many possibilities for further improvement.In the future, we will try to improve the algo-rithm as well as perform more extensive evalua-tions on different language pairs.ReferencesRalph Brown.
1997.
Automated Dictionary Extractionfor Knowledge-Free Example Based Translation,Proceedings of the 7th International Conference onTheoretical and Methodological Issues in MachineTranslation, pp.
111-118, Santa-Fe, July 1997.Chris Callison-Burch, Colin Bannard and Josh Scroe-der.
2005.
Scaling Phrase-Based Statistical Ma-chine Translation to Larger Corpora and LongerPhrases, Proceedings of 43rd Conference of the As-sociation for Computational Linguistics (ACL 05),Ann Arbor, USA, 2005.Philipp Koehn.
2003.
Europarl: A Multilingual Cor-pus for Evaluation of Machine Translation,Draft,Unpublished.Manber and Myers.
1993.
Suffix Array: A NewMethod for On-Line String Searches, SIAM Jour-nal on Computing, 22(5):935-948.Daniel Marcu, William Wong.
2002.
A Phrase-Based,Joint Probability Model for Statistical MachineTranslation, Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing , Philadelphia, USA, July 2002.Dan Melamed.
1997.
A Word-to-Word Model ofTranslational Equivalence, Proceedings of 35thConference of the Association for ComputationalLinguistics (ACL 97), Madrid, Spain, 1997.Franz Joseph Och, Christophe Tillmann, HermannNey.
1999.
Improved Alignment Models for Statis-tical Machine Translation.
Proceedings of the jointconference of Empirical Methods in Natural Lan-guage Processing and Very Large Corpora, pp 20-28, University Of Maryland,Jia Xu, Richard Zens., Hermann Ney.
2004.
Do WeNeed Chinese Word Segmentation for StatisticalMachine Translation?, Proceedings of the 3rdSIGHAN Workshop on Chinese Language Learn-ing, Barcelona, Spain, pp.
122-128 , July 2004Ying Zhang, Stephan Vogel and Alex Waibel.
2003.Integrated Phrase Segmentation and Alignment al-gorithm for Statistical Machine Translation, Pro-ceedings of International Conference on NaturalLanguage Processing and Knowledge Engineering,Beijing,China., October 2003Precision RecallOur algorithm(w/o segmentation)78% 70%ISA(w/o segmentation)55% 55%ISA + segmentation 98% 95%18
