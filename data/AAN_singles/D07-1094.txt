Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
897?905, Prague, June 2007. c?2007 Association for Computational LinguisticsLearning Structured Models for Phone RecognitionSlav Petrov Adam Pauls Dan KleinComputer Science Department, EECS DivisonUniversity of California at BerkeleyBerkeley, CA, 94720, USA{petrov,adpauls,klein}@cs.berkeley.eduAbstractWe present a maximally streamlined approach tolearning HMM-based acoustic models for automaticspeech recognition.
In our approach, an initial mono-phone HMM is iteratively refined using a split-mergeEM procedure which makes no assumptions aboutsubphone structure or context-dependent structure,and which uses only a single Gaussian per HMMstate.
Despite the much simplified training process,our acoustic model achieves state-of-the-art resultson phone classification (where it outperforms almostall other methods) and competitive performance onphone recognition (where it outperforms standard CDtriphone / subphone / GMM approaches).
We alsopresent an analysis of what is and is not learned byour system.1 IntroductionContinuous density hiddenMarkov models (HMMs)underlie most automatic speech recognition (ASR)systems in some form.
While the basic algorithmsfor HMM learning and inference are quite general,acoustic models of speech standardly employ richspeech-specific structures to improve performance.For example, it is well known that a monophoneHMM with one state per phone is too coarse anapproximation to the true articulatory and acousticprocess.
The HMM state space is therefore refinedin several ways.
To model phone-internal dynam-ics, phones are split into beginning, middle, and endsubphones (Jelinek, 1976).
To model cross-phonecoarticulation, the states of the HMM are refinedby splitting the phones into context-dependent tri-phones.
These states are then re-clustered (Odell,1995) and the parameters of their observation dis-tributions are tied back together (Young and Wood-land, 1994).
Finally, to model complex emissiondensities, states emit mixtures of multivariate Gaus-sians.
This standard structure is shown schemati-cally in Figure 1.
While this rich structure is pho-netically well-motivated and empirically success-ful, so much structural bias may be unnecessary, oreven harmful.
For example in the domain of syn-tactic parsing with probabilistic context-free gram-mars (PCFGs), a surprising recent result is that au-tomatically induced grammar refinements can out-perform sophisticated methods which exploit sub-stantial manually articulated structure (Petrov et al,2006).In this paper, we consider a much more automatic,data-driven approach to learning HMM structure foracoustic modeling, analagous to the approach takenby Petrov et al (2006) for learning PCFGs.
We startwith a minimal monophone HMM in which there isa single state for each (context-independent) phone.Moreover, the emission model for each state is a sin-gle multivariate Gaussian (over the standard MFCCacoustic features).
We then iteratively refine thisminimal HMM through state splitting, adding com-plexity as needed.
States in the refined HMMs arealways substates of the original HMM and are there-fore each identified with a unique base phone.
Statesare split, estimated, and (perhaps) merged, based ona likelihood criterion.
Our model never allows ex-plicit Gaussian mixtures, though substates may de-velop similar distributions and thereby emulate suchmixtures.In principle, discarding the traditional structurecan either help or hurt the model.
Incorrect priorsplits can needlessly fragment training data and in-correct prior tying can limit the model?s expressiv-ity.
On the other hand, correct assumptions canincrease the efficiency of the learner.
Empirically,897Startbegin endEndmid begin endmidd7= c(#-d-ae)begin endmidae3= c(d-ae-d) d13= c(ae-d-#)Starta dEnda d a dd ae db c b c b cFigure 1: Comparison of the standard model to our model (hereshown with k = 4 subphones per phone) for the word dad.The dependence of subphones across phones in our model isnot shown, while the context clustering in the standard model isshown only schematically.we show that our automatic approach outperformsclassic systems on the task of phone recognition onthe TIMIT data set.
In particular, it outperformsstandard state-tied triphone models like Young andWoodland (1994), achieving a phone error rate of26.4% versus 27.7%.
In addition, our approachgives state-of-the-art performance on the task ofphone classification on the TIMIT data set, suggest-ing that our learned structure is particularly effec-tive at modeling phone-internal structure.
Indeed,our error rate of 21.4% is outperformed only by therecent structured margin approach of Sha and Saul(2006).
It remains to be seen whether these posi-tive results on acoustic modeling will facilitate betterword recognition rates in a large vocabulary speechrecognition system.We also consider the structures learned by themodel.
Subphone structure is learned, similar to,but richer than, standard begin-middle-end struc-tures.
Cross-phone coarticulation is also learned,with classic phonological classes often emergingnaturally.Many aspects of this work are intended to sim-plify rather than further articulate the acoustic pro-cess.
It should therefore be clear that the basic tech-niques of splitting, merging, and learning using EMare not in themselves new for ASR.
Nor is the basiclatent induction method new (Matsuzaki et al, 2005;Petrov et al, 2006).
What is novel in this paper is (1)the construction of an automatic system for acous-tic modeling, with substantially streamlined struc-ture, (2) the investigation of variational inference forsuch a task, (3) the analysis of the kinds of struc-tures learned by such a system, and (4) the empiricaldemonstration that such a system is not only com-petitive with the traditional approach, but can indeedoutperform even very recent work on some prelimi-nary measures.2 LearningIn the following, we propose a greatly simplifiedmodel that does not impose any manually specifiedstructural constraints.
Instead of specifying struc-ture a priori, we use the Expectation-Maximization(EM) algorithm for HMMs (Baum-Welch) to auto-matically induce the structure in a way that maxi-mizes data likelihood.In general, our training data consists of setsof acoustic observation sequences and phone leveltranscriptions r which specify a sequence of phonesfrom a set of phones Y , but does not label eachtime frame with a phone.
We refer to an observa-tion sequence as x = x1, .
.
.
, xT where xi ?
R39are standard MFCC features (Davis and Mermel-stein, 1980).
We wish to induce an HMM over aset of states S for which we also have a functionpi : S ?
Y that maps every state in S to a phonein Y .
Note that in the usual formulation of the EMalgorithm for HMMs, one is interested in learningHMM parameters ?
that maximize the likelihood ofthe observations P(x|?
); in contrast, we aim to max-imize the joint probability of our observations andphone transcriptions P(x, r|?)
or observations andphone sequences P(x,y|?)
(see below).
We now de-scribe this relatively straightforward modification ofthe EM algorithm.2.1 The Hand-Aligned CaseFor clarity of exposition we first consider a simpli-fied scenario in which we are given hand-alignedphone labels y = y1, .
.
.
, yT for each time t, as isthe case for the TIMIT dataset.
Our procedure doesnot require such extensive annotation of the trainingdata and in fact gives better performance when theexact transition point between phones are not pre-specified but learned.We define forward and backward probabilities(Rabiner, 1989) in the following way: the forwardprobability is the probability of observing the se-quence x1, .
.
.
, xt with transcription y1, .
.
.
, yt and898nextpreviousthdhptbgdxwrlszshfclvclmnnglrer0(a)nextpreviousthdhptbgdxwrlszshfclvclmnnglrer0 1(b)nextpreviousthdhptbgdxwrlszshfclvclmnnglrer0321(c)nextpreviousthdhptbgdxwrlszshfclvclmnnglrer16034725(d)Figure 2: Iterative refinement of the /ih/ phone with 1, 2, 4, 8 substates.ending in state s at time t:?t(s) = P(x1, .
.
.
, xt, y1, .
.
.
yt, st = s|?
),and the backward probability is the probability ofobserving the sequence xt+1, .
.
.
, xT with transcrip-tion yt+1, .
.
.
, yT , given that we start in state s attime t:?t(s) = P(xt+1, .
.
.
, xT , yt+1, .
.
.
, yT |st = s, ?
),where ?
are the model parameters.
As usual, weparameterize our HMMs with ass?
, the probabilityof transitioning from state s to s?, and bs(x) ?N (?s,?s), the probability emitting the observationx when in state s.These probabilities can be computed using thestandard forward and backward recursions (Rabiner,1989), except that at each time t, we only con-sider states st for which pi(st) = yt, because wehave hand-aligned labels for the observations.
Thesequantities also allow us to compute the posteriorcounts necessary for the E-step of the EM algorithm.2.2 SplittingOne way of inducing arbitrary structural annota-tions would be to split each HMM state in intom substates, and re-estimate the parameters for thesplit HMM using EM.
This approach has two ma-jor drawbacks: for larger m it is likely to convergeto poor local optima, and it allocates substates uni-formly across all states, regardless of how much an-notation is required for good performance.To avoid these problems, we apply a hierarchicalparameter estimation strategy similar in spirit to thework of Sankar (1998) and Ueda et al (2000), buthere applied to HMMs rather than to GMMs.
Be-ginning with the baseline model, where each statecorresponds to one phone, we repeatedly split andre-train the HMM.
This strategy ensures that eachsplit HMM is initialized ?close?
to some reasonablemaximum.Concretely, each state s in the HMM is split intwo new states s1, s2 with pi(s1) = pi(s2) = pi(s).We initialize EM with the parameters of the previ-ous HMM, splitting every previous state s in twoand adding a small amount of randomness  ?
1%to its transition and emission probabilities to breaksymmetry:as1s?
?
ass?
+ ,bs1(o) ?
N (?s + ,?s),and similarly for s2.
The incoming transitions aresplit evenly.We then apply the EM algorithm described aboveto re-estimate these parameters before performingsubsequent split operations.2.3 MergingSince adding substates divides HMM statistics intomany bins, the HMM parameters are effectively es-timated from less data, which can lead to overfitting.Therefore, it would be to our advantage to split sub-899states only where needed, rather than splitting themall.We realize this goal by merging back those splitss ?
s1s2 for which, if the split were reversed, theloss in data likelihood would be smallest.
We ap-proximate the loss in data likelihood for a merges1s2 ?
swith the following likelihood ratio (Petrovet al, 2006):?
(s1 s2 ?
s) =?sequences?tPt(x,y)P(x,y).Here P(x,y) is the joint likelihood of an emissionsequence x and associated state sequence y. Thisquantity can be recovered from the forward andbackward probabilities usingP(x,y) =?s:pi(s)=yt?t(s) ?
?t(s).Pt(x,y) is an approximation to the same joint like-lihood where states s1 and s2 are merged.
We ap-proximate the true loss by only considering mergingstates s1 and s2 at time t, a value which can be ef-ficiently computed from the forward and backwardprobabilities.
The forward score for the merged states at time t is just the sum of the two split scores:?
?t(s) = ?t(s1) + ?t(s2),while the backward score is a weighted sum of thesplit scores:?
?t(s) = p1?t(s1) + p2?t(s2),where p1 and p2 are the relative (posterior) frequen-cies of the states s1 and s2.Thus, the likelihood after merging s1 and s2 attime t can be computed from these merged forwardand backward scores as:P t(x,y) = ?
?t(s) ?
?
?t(s) +?s??t(s?)
?
?t(s?
)where the second sum is over the other substates ofxt, i.e.
{s?
: pi(s?)
= xt, s?
/?
{s1, s2}}.
Thisexpression is an approximation because it neglectsinteractions between instances of the same states atmultiple places in the same sequence.
In particular,since phones frequently occur with multiple consec-utive repetitions, this criterion may vastly overesti-mate the actual likelihood loss.
As such, we also im-plemented the exact criterion, that is, for each split,we formed a new HMM with s1 and s2 merged andcalculated the total data likelihood.
This methodis much more computationally expensive, requiringa full forward-backward pass through the data foreach potential merge, and was not found to producenoticeably better performance.
Therefore, all exper-iments use the approximate criterion.2.4 The Automatically-Aligned CaseIt is straightforward to generalize the hand-alignedcase to the case where the phone transcription isknown, but no frame level labeling is available.
Themain difference is that the phone boundaries are notknown in advance, which means that there is nowadditional uncertainty over the phone states.
Theforward and backward recursions must thus be ex-panded to consider all state sequences that yield thegiven phone transcription.
We can accomplish thiswith standard Baum-Welch training.3 InferenceAn HMM over refined subphone states s ?
S nat-urally gives posterior distributions P(s|x) over se-quences of states s. We would ideally like to ex-tract the transcription r of underlying phones whichis most probable according to this posterior1.
Thetranscription is two stages removed from s. First,it collapses the distinctions between states s whichcorrespond to the same phone y = pi(s).
Second,it collapses the distinctions between where phonetransitions exactly occur.
Viterbi state sequences caneasily be extracted using the basic Viterbi algorithm.On the other hand, finding the best phone sequenceor transcription is intractable.As a compromise, we extract the phone sequence(not transcription) which has highest probability ina variational approximation to the true distribution(Jordan et al, 1999).
Let the true posterior distri-bution over phone sequences be P(y|x).
We forman approximation Q(y) ?
P(y|x), where Q is anapproximation specific to the sequence x and factor-1Remember that by ?transcription?
we mean a sequence ofphones with duplicates removed.900izes as:Q(y) =?tq(t, xt, yt+1).We would like to fit the values q, one for each timestep and state-state pair, so as to make Q as close toP as possible:minqKL(P(y|x)||Q(y)).The solution can be found analytically using La-grange multipliers:q(t, y, y?)
=P(Yt = y, Yt+1 = y?|x)P(Yt = y|x).where we have made the position-specific randomvariables Yt explicit for clarity.
This approximationdepends only on our ability to calculate posteriorsover phones or phone-phone pairs at individual po-sitions t, which is easy to obtain from the state pos-teriors, for example:P(Yt = y,Yt+1 = y?|x) =?s:pi(s)=y?s?:pi(s?)=y??t(s)ass?bs?(xt)?t+1(s?
)P(x)Finding the Viterbi phone sequence in the approxi-mate distribution Q, can be done with the Forward-Backward algorithm over the lattice of q values.4 ExperimentsWe tested our model on the TIMIT database, usingthe standard setups for phone recognition and phoneclassification.
We partitioned the TIMIT data intotraining, development, and (core) test sets accordingto standard practice (Lee and Hon, 1989; Gunawar-dana et al, 2005; Sha and Saul, 2006).
In particu-lar, we excluded all sa sentences and mapped the 61phonetic labels in TIMIT down to 48 classes beforetraining our HMMs.
At evaluation, these 48 classeswere further mapped down to 39 classes, again inthe standard way.MFCC coefficients were extracted from theTIMIT source as in Sha and Saul (2006), includ-ing delta and delta-delta components.
For all experi-ments, our system and all baselines we implementedused full covariance when parameterizing emission0.240.260.28 0.30.320.340.360.38 0.40.42  0200400600800 1000 1200 1400 1600 1800 2000Phone Recognition ErrorNumber of Statessplit onlysplit and mergesplit and merge, automatic alignmentFigure 3: Phone recognition error for models of increasing sizemodels.2 All Gaussians were endowed with weakinverse Wishart priors with zero mean and identitycovariance.34.1 Phone RecognitionIn the task of phone recognition, we fit an HMMwhose output, with subsequent states collapsed, cor-responds to the training transcriptions.
In the TIMITdata set, each frame is manually phone-annotated, sothe only uncertainty in the basic setup is the identityof the (sub)states at each frame.We therefore began with a single state for eachphone, in a fully connected HMM (except for spe-cial treatment of dedicated start and end states).
Weincrementally trained our model as described in Sec-tion 2, with up to 6 split-merge rounds.
We foundthat reversing 25% of the splits yielded good overallperformance while maintaining compactness of themodel.We decoded using the variational decoder de-scribed in Section 3.
The output was then scoredagainst the reference phone transcription using thestandard string edit distance.During both training and decoding, we used ?flat-tened?
emission probabilities by exponentiating tosome 0 < ?
< 1.
We found the best setting for ?to be 0.2, as determined by tuning on the develop-ment set.
This flattening compensates for the non-2Most of our findings also hold for diagonal covarianceGaussians, albeit the final error rates are 2-3% higher.3Following previous work with PCFGs (Petrov et al, 2006),we experimented with smoothing the substates towards eachother to prevent overfitting, but we were unable to achieve anyperformance gains.901Method Error RateState-Tied Triphone HMM27.7%1(Young and Woodland, 1994)Gender Dependent Triphone HMM27.1%1(Lamel and Gauvain, 1993)This Paper 26.4%Bayesian Triphone HMM25.6%(Ming and Smith, 1998)Heterogeneous classifiers24.4%(Halberstadt and Glass, 1998)Table 1: Phone recognition error rates on the TIMIT core testfrom Glass (2003).1These results are on a slightly easier test set.independence of the frames, partially due to over-lapping source samples and partially due to otherunmodeled correlations.Figure 3 shows the recognition error as the modelgrows in size.
In addition to the basic setup de-scribed so far (split and merge), we also show amodel in which merging was not performed (splitonly).
As can be seen, the merging phase not onlydecreases the number of HMM states at each round,but also improves phone recognition error at eachround.We also compared our hierarchical split onlymodel with a model where we directly split all statesinto 2k substates, so that these models had the samenumber of states as a a hierarchical model after ksplit and merge cycles.
While for small k, the dif-ference was negligible, we found that the error in-creased by 1% absolute for k = 5.
This trend is tobe expected, as the possible interactions between thesubstates grows with the number of substates.Also shown in Figure 3, and perhaps unsurprising,is that the error rate can be further reduced by allow-ing the phone boundaries to drift from the manualalignments provided in the TIMIT training data.
Thesplit and merge, automatic alignment line shows theresult of allowing the EM fitting phase to repositioneach phone boundary, giving absolute improvementsof up to 0.6%.We investigated how much improvement in accu-racy one can gain by computing the variational ap-proximation introduced in Section 3 versus extract-ing the Viterbi state sequence and projecting that se-quence to its phone transcription.
The gap varies,Method Error RateGMM Baseline (Sha and Saul, 2006) 26.0%HMM Baseline (Gunawardana et al, 2005) 25.1%SVM (Clarkson and Moreno, 1999) 22.4%Hidden CRF (Gunawardana et al, 2005) 21.7%This Paper 21.4%Large Margin GMM (Sha and Saul, 2006) 21.1%Table 2: Phone classification error rates on the TIMIT core test.but on a model with roughly 1000 states (5 split-merge rounds), the variational decoder decreases er-ror from 26.5% to 25.6%.
The gain in accuracycomes at a cost in time: we must run a (possiblypruned) Forward-Backward pass over the full statespace S, then another over the smaller phone spaceY .
In our experiments, the cost of variational decod-ing was a factor of about 3, which may or may notjustify a relative error reduction of around 4%.The performance of our best model (split andmerge, automatic alignment, and variational decod-ing) on the test set is 26.4%.
A comparison of ourperformance with other methods in the literature isshown in Table 1.
Despite our structural simplic-ity, we outperform state-tied triphone systems likeYoung andWoodland (1994), a standard baseline forthis task, by nearly 2% absolute.
However, we fallshort of the best current systems.4.2 Phone ClassificationPhone classification is the fairly constrained task ofclassifying in isolation a sequence of frames whichis known to span exactly one phone.
In order toquantify how much of our gains over the triphonebaseline stem from modeling context-dependenciesand how much from modeling the inner structure ofthe phones, we fit separate HMM models for eachphone, using the same split and merge procedure asabove (though in this case only manual alignmentsare reasonable because we test on manual segmen-tations).
For each test frame sequence, we com-pute the likelihood of the sequence from the forwardprobabilities of each individual phone HMM.
Thephone giving highest likelihood to the input was se-lected.
The error rate is a simple fraction of testphones classified correctly.Table 2 shows a comparison of our performancewith that of some other methods in the literature.A minimal comparison is to a GMM with the samenumber of mixtures per phone as our model?s maxi-902iy ix eh ae ax uw uh aa ey ay oy aw ow er el r w y m n ng dx jh ch z s zh hh v f dh th b p d t g k siliy ix eh ae ax uw uh aa ey ay oy aw ow er el r w y m n ng dx jh ch z s zh hh v f dh th b p d t g k siliyixehaeaxuwuhaaeyayoyawowerelrwymnngdxjhchzszhhhvfdhthbpdtgksiliyixehaeaxuwuhaaeyayoyawowerelrwymnngdxjhchzszhhhvfdhthbpdtgksilHypothesisReferencevowels/semivowelsnasals/flapsstrong fricativesweak fricativesstopsFigure 4: Phone confusion matrix.
76% of the substitutions fallwithin the shown classes.mum substates per phone.
While these models havethe same number of total Gaussians, in our modelthe Gaussians are correlated temporally, while inthe GMM they are independent.
Enforcing begin-middle-end HMM structure (see HMM Baseline) in-creases accuracy somewhat, but our more generalmodel clearly makes better use of the available pa-rameters than those baselines.Indeed, our best model achieves a surpris-ing performance of 21.4%, greatly outperform-ing other generative methods and achieving perfor-mance competitive with state-of-the-art discrimina-tive methods.
Only the recent structured margin ap-proach of Sha and Saul (2006) gives a better perfor-mance than our model.
The strength of our systemon the classification task suggests that perhaps it ismodeling phone-internal structure more effectivelythan cross-phone context.5 AnalysisWhile the overall phone recognition and classifi-cation numbers suggest that our system is broadlycomparable to and perhaps in certain ways superiorto classical approaches, it is illuminating to investi-gate what is and is not learned by the model.Figure 4 gives a confusion matrix over the substi-tution errors made by our model.
The majority of thenextpreviousehowaoaaeyiyixvfkmowaoaaeyiyihaeixzfs1435620pFigure 5: Phone contexts and subphone structure.
The /l/ phoneafter 3 split-merge iterations is shown.confusions are within natural classes.
Some partic-ularly frequent and reasonable confusions arise be-tween the consonantal /r/ and the vocalic /er/ (thesame confusion arises between /l/ and /el/, but thestandard evaluation already collapses this distinc-tion), the reduced vowels /ax/ and /ix/, the voicedand voiceless alveolar sibilants /z/ and /s/, and thevoiced and voiceless stop pairs.
Other vocalic con-fusions are generally between vowels and their cor-responding reduced forms.
Overall, 76% of the sub-stitutions are within the broad classes shown in thefigure.We can also examine the substructure learned forthe various phones.
Figure 2 shows the evolutionof the phone /ih/ from a single state to 8 substatesduring split/merge (no merges were chosen for thisphone), using hand-alignment of phones to frames.These figures were simplified from the completestate transition matrices as follows: (1) adjacentphones?
substates are collapsed, (2) adjacent phonesare selected based on frequency and inbound prob-ability (and forced to be the same across figures),(3) infrequent arcs are suppressed.
In the first split,(b), a sonorant / non-sonorant distinction is learnedover adjacent phones, along with a state chain whichcaptures basic duration (a self-looping state givesan exponential model of duration; the sum of twosuch states is more expressive).
Note that the nat-903ural classes interact with the chain in a way whichallows duration to depend on context.
In further re-finements, more structure is added, including a two-track path in (d) where one track captures the distincteffects on higher formants of r-coloring and nasal-ization.
Figure 5 shows the corresponding diagramfor /l/, where some merging has also occurred.
Dif-ferent natural classes emerge in this case, with, forexample, preceding states partitioned into front/highvowels vs. rounded vowels vs. other vowels vs. con-sonants.
Following states show a front/back dis-tinction and a consonant distinction, and the phone/m/ is treated specially, largely because the /lm/ se-quence tends to shorten the /l/ substantially.
Noteagain how context, internal structure, and durationare simultaneously modeled.
Of course, it shouldbe emphasized that post hoc analysis of such struc-ture is a simplification and prone to seeing what oneexpects; we present these examples to illustrate thebroad kinds of patterns which are detected.As a final illustration of the nature of the learnedmodels, Table 3 shows the number of substates allo-cated to each phone by the split/merge process (themaximum is 32 for this stage) for the case of hand-aligned (left) as well as automatically-aligned (right)phone boundaries.
Interestingly, in the hand-alignedcase, the vowels absorb most of the complexity sincemany consonantal cues are heavily evidenced onadjacent vowels.
However, in the automatically-aligned case, many vowel frames with substantialconsontant coloring are re-allocated to those adja-cent consonants, giving more complex consonants,but comparatively less complex vowels.6 ConclusionsWe have presented a minimalist, automatic approachfor building an accurate acoustic model for phoneticclassification and recognition.
Our model does notrequire any a priori phonetic bias or manual spec-ification of structure, but rather induces the struc-ture in an automatic and streamlined fashion.
Start-ing from a minimal monophone HMM, we auto-matically learn models that achieve highly compet-itive performance.
On the TIMIT phone recogni-tion task our model clearly outperforms standardstate-tied triphone models like Young and Wood-land (1994).
For phone classification, our modelVowelsaa 31 32ae 32 17ah 31 8ao 32 23aw 18 6ax 18 3ay 32 28eh 32 16el 6 4en 4 3er 32 31ey 32 30ih 32 11ix 31 16iy 31 32ow 26 10oy 4 4uh 5 2uw 21 8Consonantsb 2 32ch 13 30d 2 14dh 6 31dx 2 3f 32 32g 2 15hh 3 5jh 3 16k 30 32l 25 32m 25 25n 29 32ng 3 4p 5 24r 32 32s 32 32sh 30 32t 24 32th 8 11v 23 11w 10 21y 3 7z 31 32zh 2 2Otherepi 2 4sil 32 32vcl 29 30cl 31 32Table 3: Number of substates allocated per phone.
The leftcolumn gives the number of substates allocated when trainingon manually aligned training sequences, while the right columngives the number allocated when we automatically determinephone boundaries.achieves performance competitive with the state-of-the-art discriminative methods (Sha and Saul, 2006),despite being generative in nature.
This result to-gether with our analysis of the context-dependenciesand substructures that are being learned, suggeststhat our model is particularly well suited for mod-eling phone-internal structure.
It does, of courseremain to be seen if and how these benefits can bescaled to larger systems.ReferencesP.
Clarkson and P. Moreno.
1999.
On the use of Sup-port Vector Machines for phonetic classification.
InICASSP ?99.S.
B. Davis and P. Mermelstein.
1980.
Comparisonof parametric representation for monosyllabic wordrecognition in continuously spoken sentences.
IEEETransactions on Acoustics, Speech, and Signal Pro-cessing, 28(4).J.
Glass.
2003.
A probabilistic framework for segment-based speech recognition.
Computer Speech and Lan-guage, 17(2).A.
Gunawardana, M. Mahajan, A. Acero, and J. Platt.2005.
Hidden Conditional Random Fields for phonerecognition.
In Eurospeech ?05.A.
K. Halberstadt and J. R. Glass.
1998.
Hetero-geneous measurements and multiple classifiers forspeech recognition.
In ICSLP ?98.F.
Jelinek.
1976.
Continuous speech recognition by sta-tistical methods.
Proceedings of the IEEE.904M.
I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.Saul.
1999.
An introduction to variational methodsfor graphical models.
Learning in Graphical Models.L.
Lamel and J. Gauvain.
1993.
Cross-lingual experi-ments with phone recognition.
In ICASSP ?93.K.
F. Lee and H. W. Hon.
1989.
Speaker-independentphone recognition using Hidden Markov Models.IEEE Transactions on Acoustics, Speech, and SignalProcessing, 37(11).T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with latent annotations.
In ACL ?05.J.
Ming and F.J. Smith.
1998.
Improved phone recogni-tion using Bayesian triphone models.
In ICASSP ?98.J.
J. Odell.
1995.
The Use of Context in Large Vocab-ulary Speech Recognition.
Ph.D. thesis, University ofCambridge.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In COLING-ACL ?06.L.
Rabiner.
1989.
A Tutorial on hidden Markov mod-els and selected applications in speech recognition.
InIEEE.A.
Sankar.
1998.
Experiments with a Gaussian merging-splitting algorithm for HMM training for speechrecognition.
In DARPA Speech Recognition Workshop?98.F.
Sha and L. K. Saul.
2006.
Large margin Gaussian mix-ture modeling for phonetic classification and recogni-tion.
In ICASSP ?06.N.
Ueda, R. Nakano, Z. Ghahramani, and G. E. Hinton.2000.
Split andMerge EM algorithm for mixture mod-els.
Neural Computation, 12(9).S.
J.
Young and P. C. Woodland.
1994.
State clusteringin HMM-based continuous speech recognition.
Com-puter Speech and Language, 8(4).905
