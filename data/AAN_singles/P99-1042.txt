Deep Read: A Reading Comprehension SystemLynette Hirschman ?
Marc Light ?
Eric Breck ?
John D. BurgerThe MITRE Corporation202 Burlington RoadBedford, MA USA 01730{ l ynette, light, ebreck, john } @ mitre.orgAbstractThis paper describes initial work on Deep Read,an automated reading comprehension system thataccepts arbitrary text input (a story) and answersquestions about it.
We have acquired a corpus of 60development and 60 test stories of 3 rd to 6 th gradematerial; each story is followed by short-answerquestions (an answer key was also provided).
Weused these to construct and evaluate a baseline systemthat uses pattern matching (bag-of-words) techniquesaugmented with additional automated linguisticprocessing (stemming, name identification, semanticclass identification, and pronoun resolution).
Thissimple system retrieves the sentence containing theanswer 30-40% of the time.1 IntroductionThis paper describes our initial workexploring reading comprehension tests as aresearch problem and an evaluation method forlanguage understanding systems.
Such tests cantake the form of standardized multiple-choicediagnostic reading skill tests, as well as fill-in-the-blank and short-answer tests.
Typically, suchtests ask the student o read a story or article andto demonstrate her/his understanding of thatarticle by answering questions about it.
For anexample, see Figure 1.Reading comprehension tests are interestingbecause they constitute "found" test material:these tests are created in order to evaluatechildren's reading skills, and therefore, testmaterials, scoring algorithms, and humanperformance measures already exist.Furthermore, human performance measuresprovide a more intuitive way of assessing thecapabilities of a given system than currentmeasures of precision, recall, F-measure,operating curves, etc.
In addition, readingcomprehension tests are written to test a range ofskill levels.
With proper choice of test material,it should be possible to challenge systems tosuccessively higher levels of performance.For these reasons, reading comprehensiontests offer an interesting alternative to the kinds ofspecial-purpose, carefully constructed evaluationsthat have driven much recent research in languageunderstanding.
Moreover, the current state-of-the-art in computer-based language understandingmakes this project a good choice: it is beyondcurrent systems' capabilities, but tractable.
OurLibrary of Congress Has Books for Everyone(WASHINGTON, D.C., 1964) - It was 150 yearsago this year that our nation's biggest library burnedto the ground.
Copies of all the wriuen books of thetime were kept in the Library of Congress.
But theywere destroyed by fire in 1814 during a war with theBritish.That fire didn't stop book lovers.
The next year,they began to rebuild the library.
By giving it 6,457of his books, Thomas Jefferson helped get it started.The first libraries in the United States could beused by members only.
But the Library of Congresswas built for all the people.
From the start, it was ournational library.Today, the Library of Congress is one of thelargest libraries in the world.
People can find a copyof just about every book and magazine printed.Libraries have been with us since people firstlearned to write.
One of the oldest o be found datesback to about 800 years B.C.
The books werewritten on tablets made from clay.
The people whotook care of the books were called "men of thewritten tablets."1.
Who gave books to the new library?2.
What is the name of our national library?3.
When did this library burn down?4.
Where can this library be found?5.
Why were some early people called "men of thewritten tablets"?Figure 1: Sample Remedia TM ReadingComprehension Story and Questions325simple bag-of-words approach picked anappropriate sentence 30--40% of the time withonly a few months work, much of it devoted toinfrastructure.
We believe that by addingadditional linguistic and world knowledgesources to the system, it can quickly achieveprimary-school-level performance, and within afew years, "graduate" to real-world applications.Reading comprehension tests can serve as atestbed, providing an impetus for research in anumber of areas:?
Machine learning of lexical information,including subcategorization frames, semanticrelations between words, and pragmaticimport of particular words.?
Robust and efficient use of world knowledge(e.g., temporal or spatial relations).?
Rhetorical structure, e.g., causal relationshipsbetween propositions in the text, particularlyimportant for answering why and howquestions.?
Collaborative learning, which combines ahuman user and the reading comprehensioncomputer system as a team.
If the system canquery the human, this may make it possibleto circumvent knowledge acquisitionbottlenecks for lexical and world knowledge.In addition, research into collaboration mightlead to insights about intelligent tutoring.Finally, reading comprehension evaluatessystems' abilities to answer ad hoc, domain-independent questions; this ability supports factretrieval, as opposed to document retrieval, whichcould augment future search engines - seeKupiec (1993) for an example of such work.There has been previous work on storyunderstanding that focuses on inferentialprocessing, common sense reasoning, and worldknowledge required for in-depth understanding ofstories.
These efforts concern themselves withspecific aspects of knowledge representation,inference techniques, or question types - seeLehnert (1983) or Schubert (to appear).
Incontrast, our research is concerned with buildingsystems that can answer ad hoc questions aboutarbitrary documents from varied domains.We report here on our initial pilot study todetermine the feasibility of this task.
Wepurchased a small (hard copy) corpus ofdevelopment and test materials (about 60 storiesin each) consisting of remedial reading materialsfor grades 3-6; these materials are simulated newsstories, followed by short-answer "5W" questions:who, what, when, where, and why questions, l Wedeveloped a simple, modular, baseline system thatuses pattern matching (bag-of-words) techniquesand limited linguistic processing to select thesentence from the text that best answers the query.We used our development corpus to exploreseveral alternative evaluation techniques, and thenevaluated on the test set, which was kept blind.2 EvaluationWe had three goals in choosing evaluationmetrics for our system.
First, the evaluationshould be automatic.
Second, it should maintaincomparability with human benchmarks.
Third, itshould require little or no effort to prepare newanswer keys.
We used three metrics, P&R,HumSent, and AutSent, which satisfy theseconstraints o varying degrees.P&R was the precision and recall on stemmedcontent words 2, comparing the system's responseat the word level to the answer key provided bythe test's publisher.
HumSent and AutSentcompared the sentence chosen by the system to alist of acceptable answer sentences, coring onepoint for a response on the list, and zero pointsotherwise.
In all cases, the score for a set ofquestions was the average of the scores for eachquestion.For P&R, the answer key from the publisherwas used unmodified.
The answer key forHumSent was compiled by a human annotator,I These materials consisted of levels 2-5 of "The 5W's" written by Linda Miller, which can be purchasedfrom Remedia Publications, 10135 E. Via Linda#D124, Scottsdale, AZ 85258.z Precision and recall are defined as follows:p = #ofmatchinscontent words# content words in answer keyR = #ofmatchingcontent words# content words in system responseRepeated words in the answer key match or failtogether.
All words are stemmed and stop words areremoved.
At present, the stop-word list consists offorms of be, have, and do, personal and possessivepronouns, the conjunctions and, or, the prepositions to,in, at, of, the articles a and the, and the relative anddemonstrative pronouns this, that, and which.326Query: What is the name of our national library?Story extract:1.
But the Library of Congress was built for allthe people.2.
From the start, it was our national library.Answer key: Library of CongressFigure 2: Extract from storywho examined the texts and chose the sentence(s)that best answered the question, even where thesentence also contained additional (unnecessary)information.
For AutSent, an automated routinereplaced the human annotator, examining thetexts and choosing the sentences, this time basedon which one had the highest recall comparedagainst he published answer key.For P&R we note that in Figure 2, there aretwo content words in the answer key (library andcongress) and sentence 1 matches both of them,for 2/2 = 100% recall.
There are seven contentwords in sentence 1, so it scores 2/7 = 29%precision.
Sentence 2 scores 1/2=50% recall and1/6=17% precision.
The human preparing the listof acceptable sentences for HumSent has aproblem.
Sentence 2 responds to the question,but requires pronoun coreference to give the fullanswer (the antecedent of it).
Sentence 1contains the words of the answer, but thesentence as a whole doesn't really answer thequestion.
In this and other difficult cases, wehave chosen to list no answers for the humanmetric, in which case the system receives zeropoints for the question.
This occurs 11% of thetime in our test corpus.
The question is stillcounted, meaning that the system receives apenalty in these cases.
Thus the highest score asystem could achieve for HumSent is 89%.Given that our current system can only respondwith sentences from the text, this penalty isappropriate.
The automated routine for preparingthe answer key in AutSent selects as the answerkey the sentence(s) with the highest recall (heresentence 1).
Thus only sentence 1 would becounted as a correct answer.We have implemented all three metrics.HumSent and AutSent are comparable withhuman benchmarks, since they provide a binaryscore, as would a teacher for a student's answer.In contrast, the precision and recall scores ofP&R lack such a straightforward comparability.However, word recall from P&R (calledAnsWdRecall in Figure 3) closely mimics thescores of HumSent and AutSent.
The correlationcoefficient for AnsWdRecall  to HumSent in ourtest set is 98%, and from HumSent to AutSent isalso 98%.
With respect o ease of answer keypreparation, P&R and AutSent are clearlysuperior, since they use the publisher-providedanswer key.
HumSent requires human annotationfor each question.
We found this annotation to beof moderate difficulty.
Finally, we note thatprecision, as well as recall, will be useful toevaluate systems that can return clauses orphrases, possibly constructed, rather than wholesentence xtracts as answers.Since most national standardized tests featurea large multiple-choice component, manyavailable benchmarks are multiple-choice xams.Also, although our short-answer metrics do notimpose a penalty for incorrect answers, multiple-choice exams, such as the Scholastic AptitudeTests, do.
In real-world applications, it might beimportant that the system be able to assign aconfidence level to its answers.
Penalizingincorrect answers wouldhelp guide developmentin that regard.
While we were initially concernedthat adapting the system to multiple-choicequestions would endanger the goal of real-worldapplicability, we have experimented with minorchanges to handle the multiple choice format.Initial experiments indicate that we can useessentially the same system architecture for bothshort-answer and multiple choice tests.3 System ArchitectureThe process of taking short-answer readingcomprehension tests can be broken down into thefollowing subtasks:Extraction of information content of thequestion.?
Extraction of information content of thedocument.?
Searching for the information requested in thequestion against information in document.A crucial component of all three of thesesubtasks is the representation of information intext.
Because our goal in designing our systemwas to explore the difficulty of various readingcomprehension exams and to measure baseline327performance, we tried to keep this initialimplementation as simple as possible.3.1 Bag-of -Words  ApproachOur system represents the informationcontent of a sentence (both question and textsentences) as the set of words in the sentence.The word sets are considered to have no structureor order and contain unique elements.
Forexample, the representation for (la) is the set in(lb).la (Sentence): By giving it 6,457 of hisbooks, Thomas Jefferson helped get it started.lb (Bag): {6,457 books by get giving helpedhis it Jefferson of started Thomas}Extraction of information content from text,both in documents and questions, then consists oftokenizing words and determining sentenceboundary punctuation.
For English written text,both of these tasks are relatively easy althoughnot trivial--see Palmer and Hearst (1997).The search subtask consists of finding thebest match between the word set representing thequestion and the sets representing sentences inthe document.
Our system measures the matchby size of the intersection of the two word sets.For example, the question in (2a) would receivean intersection score of 1 because of the mutualset element books.2a (Question): Who gave books to the newlibrary?2b (Bag): {books gave library new the towho}Because match size does not produce acomplete ordering on the sentences of thedocument, we additionally prefer sentences thatfirst match on longer words, and second, occurearlier in the document.3.2 Normalizations and Extensions of theWord SetsIn this section, we describe xtensions to theextraction approach described above.
In the nextsection we will discuss the performance benefitsof these extensions.The most straightforward extension is toremove function or stop words, such as the, of, a,etc.
from the word sets, reasoning that they offerlittle semantic information and only muddle thesignal from the more contentful words.Similarly, one can use stemming to removeinflectional affixes from the words: suchnormalization might increase the signal fromcontentful words.
For example, the intersectionbetween (lb) and (2b) would include give ifinflection were removed from gave and giving.We used a stemmer described by Abney (1997).A different ype of extension is suggested bythe fact that who questions are likely to beanswered with words that denote people ororganizations.
Similarly, when and wherequestions are answered with words denotingtemporal and locational words, respectively.
Byusing name taggers to identify person, location,and temporal information, we can add semanticclass symbols to the question word sets markingthe type of the question and then addcorresponding class symbols to the word setswhose sentences contain phrases denoting theproper type of entity.For example, due to the name ThomasJefferson, the word set in (lb) would be extendedby :PERSON, as would the word set (2b) becauseit is a who question.
This would increase thematching score by one.
The system makes use ofthe Alembic automated named entity system(Vilain and Day 1996) for finding named entities.In a similar vein, we also created a simplecommon noun classification module usingWordNet (Miller 1990).
It works by looking upall nouns of the text and adding person or locationclasses if any of a noun's senses is subsumed bythe appropriate WordNet class.
We also created afiltering module that ranks sentences higher if theycontain the appropriate class identifier, eventhough they may have fewer matching words, e.g.,if the bag representation f a sentence does notcontain :PERSON, it is ranked lower as an answerto a who question than sentences which do contain:PERSON.Finally, the system contains an extensionwhich substitutes the referent of personal pronounsfor the pronoun in the bag representation.
Forexample, if the system were to choose the sentenceHe gave books to the library, the answer eturnedand scored would be Thomas Jefferson gave booksto the library, if He were resolved to ThomasJefferson.
The current system uses a verysimplistic pronoun resolution system which3280.5  ... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0 .450 .40 .350 .30 .250 .2)( Ans Wd Reca l l  /-~-Hurn  Sent Ace \]_- -o - .Aut  Sent Acci i i i t = i i i i ~ iff + d -" " P d " " ~," . "
~"Figure 3: Effect of Linguistic Modules on System Performancematches he, him, his, she and her to the nearestprior person named entity.4 Experimental ResultsOur modular architecture and automatedscoring metrics have allowed us to explore theeffect of various linguistic sources of informationon overall system performance.
We report hereon three sets of findings: the value added fromthe various linguistic modules, the question-specific results, and an assessment of thedifficulty of the reading comprehension task.4.1 Effectiveness of Linguistic ModulesWe were able to measure the effect of variouslinguistic techniques, both singly and incombination with each other, as shown inFigure 3 and Table 1.
The individual modulesare indicated as follows: Name is the Alembicnamed tagger described above.
NameHum ishand-tagged named entity.
Stem is Abney'sautomatic stemming algorithm.
Filt is thefiltering module.
Pro is automatic name andpersonal pronoun coreference.
ProHum is hand-tagged, full reference resolution.
Sem is theWordNet-based common noun semanticclassification.We computed significance using the non-parametric significance test described by Noreen(1989).
The following performanceimprovements of the AnsWdRecall metric werestatistically significant results at a confidence l velof 95%: Base vs. NameStem, NameStem vs.FiltNameHumStem, and FiltNameHumStem vs.Fi l tProHumNameHumStem.
The other adjacentperformance differences in Figure 3 aresuggestive, but not statistically significant.Removing stop words seemed to hurt overallperformance slightly--it is not shown here.Stemming, on the other hand, produced a small butfairly consistent improvement.
We comparedthese results to perfect stemming, which madelittle difference, leading us to conclude that ourautomated stemming module worked well enough.Name identification provided consistent gains.The Alembic name tagger was developed fornewswire text and used here with nomodifications.
We created hand-tagged namedentity data, which allowed us to measure theperformance of Alembic: the accuracy (F-measure) was 76.5; see Chinchor and Sundheim(1993) for a description of the standard MUCscoring metric.
This also allowed us to simulateperfect tagging, and we were able to determinehow much we might gain by improving the nametagging by tuning it to this domain.
As the resultsindicate, there would be little gain from improvedname tagging.
However, some modules thatseemed to have little effect with automatic nametagging provided small gains with perfect nametagging, specifically WordNet common nounsemantics and automatic pronoun resolution.329When used in combination with the filteringmodule, these also seemed to help.Similarly, the hand-tagged referenceresolution data allowed us to evaluate automaticcoreference resolution.
The latter was acombination of name coreference, as determinedby Alembic, and a heuristic resolution of personalpronouns to the most recent prior named person.Using the MUC coreference scoring algorithm(see Vilain et al 1995), this had a precision of77% and a recall of 18%.
3 The use of full, hand-tagged reference resolution caused a substantialincrease of the AnsWdRecall metric.
This wasbecause the system substitutes the antecedent forall referring expressions, improving the word-based measure.
This did not, however, providean increase in the sentence-based measures.Finally, we plan to do similar human labelingexperiments for semantic lass identification, todetermine the potential effect of this knowledgesource.4.2 Quest ion-Speci f ic  AnalysisOur results reveal that different question-types behave very differently, as shown inFigure 4.
Why questions are by far the hardest(performance around 20%) because they requireunderstanding of rhetorical structure and becauseanswers tend to be whole clauses (often occurringas stand-alone sentences) rather than phrasesembedded in a context that matches the queryclosely.
On the other hand, who and whenqueries benefit from reliable person, name, andtime extraction.
Who questions eem to benefitmost dramatically from perfect name taggingcombined with filtering and pronoun resolution.What questions how relatively little benefit fromthe various linguistic techniques, probablybecause there are many types of what question,most of which are not answered by a person, timeor place.
Finally, where question results are quitevariable, perhaps because location expressionsoften do not include specific place names.3 The low recall is attributable to the fact that theheuristic asigned antecedents only for names andpronouns, and completely ignored definite nounphrases and plural pronous.4.3 Task DifficultyThese results indicate that the sample tests arean appropriate and challenging task.
The simpletechniques described above provide a system thatfinds the correct answer sentence almost 40% ofthe time.
This is much better than chance, whichwould yield an average score of about 4-5% forthe sentence metrics, given an average documentlength of 20 sentences.
Simple linguistictechniques enhance the baseline system score fromthe low 30% range to almost 40% in all threemetrics.
However, capturing the remaining 60%will clearly require more sophisticated syntactic,semantic, and world knowledge sources.5 Future DirectionsOur pilot study has shown that readingcomprehension is an appropriate task, providing areasonable starting level: it is tractable but nottrivial.
Our next steps include:?
Application of these techniques to astandardized multiple-choice readingcomprehension test.
This will require someminor changes in strategy.
For example, inpreliminary experiments, our system chose theanswer that had the highest sentence matchingscore when composed with the question.
Thisgave us a score of 45% on a small multiple-choice test set.
Such tests require us to dealwith a wider variety of question types, e.g.,What is this story about?
This will alsoprovide an opportunity to look at rejectionmeasures, since many tests penalize forrandom guessing.?
Moving from whole sentence retrieval towardsanswer phrase retrieval.
This will allow us toimprove answer word precision, whichprovides a good measure of how muchextraneous material we are still returning.?
Adding new linguistic knowledge sources.We need to perform further hand annotationexperiments o determine the effectiveness ofsemantic class identification and lexicalsemantics.?
Encoding more semantic information in ourrepresentation for both question and documentsentences.
This information could be derivedfrom syntactic analysis, including nounchunks, verb chunks, and clause groupings.330Parameters Ans Wd Acc Hum Sent Acc Hum Right Aut Sent Acc Aut RightBase 0.29 0.28 84 0.28 85Stem 0.29 0.29 86 0.28 84Name 0.33 0.31 92 0.31 93NameStem 0.33 0.32 97 !0.31 92NameHum 0.33 0.32 96 0.32 95NameHumStem 0.34 0.33 98 0.31 94FiltProNameStem 0.34 0.33 98 0.32 95ProNameStem 0.34 0.33 100 0.32 95ProNameHumStem 0.35 0.34 102 0.33 98FiltNameHumStem 0.37 0.35 104 0.34 103FiltSernNameHumStem 0.37 0.35 104 !0.34 103FiltProNameHumStem 0.38 0.36 107 0.35 106FiltProHumNameHumStem 0.42 0.36 109 0.35 105Table 1: Evaluations (3 Metrics) from Combinations of Linguistic Modules#Q300300'300300300300300300300300300;300300?
who- .X- .what- -e - -where~&- -  when--It--why0.60.50.40,30.20.1* / / / / / / / / / /Figure 4: AnsWdRecall Performance by Query Type331Cooperation with educational testing andcontent providers.
We hope to work togetherwith one or more major publishers.
This willprovide the research community with a richercollection of training and test material, whilealso providing educational testing groupswith novel ways of checking andbenchmarking their tests.6 ConclusionWe have argued that taking readingcomprehension exams is a useful task fordeveloping and evaluating natural languageunderstanding systems.
Reading comprehensionuses found material and provides human-comparable valuations which can be computedautomatically with a minimum of humanannotation.
Crucially, the reading comprehensiontask is neither too easy nor too hard, as theperformance of our pilot system demonstrates.Finally, reading comprehension is a task that issufficiently close to information extractionapplications such as ad hoc question answering,fact verification, situation tracking, and documentsummarization, that improvements onthe readingcomprehension evaluations will result inimproved systems for these applications.7 AcknowledgementsWe gratefully acknowledge the contributionof Lisa Ferro, who prepared much of the hand-tagged ata used in these experiments.ReferencesAbney, Steven (1997).
The SCOL manual version0.lb.
Manuscript.Chinchor, Nancy and Beth Sundheim (1993).
"MUC-5 Evaluation Metrics," Proc.
Fifth MessageUnderstanding Conference (MUC-5).
MorganKaufman Publishers.Kupiec, Julian (1993).
"MURAX: A RobustLinguistic Approach for Question Answering Usingan On-Line Encyclopedia," Proceedings of the 16thIntl.
ACM SIGIR Conf on Research andDevelopment in Information Retrieval (SIGIR-93).pp.
181-190, Pittsburgh, PA.Lehnert, Wendy, Michael Dyer, Peter Johnson, C.J.Yang, and Steve Harley (1983) "BORIS--anExperiment in In-Depth Understanding ofNarratives", Artificial Intelligence, vol.
20, no.
1.Miller, George (1990).
"WordNet: an On-line lexicaldatabase."
International Journal of Lexicography.Noreen, Eric (1989).
Computer Intensive methods forTesting Hypotheses.
John Wiley & Sons.Palmer, David and Marti A. Hearst (1997).
"AdaptiveMultilingual Sentence Boundary Disambiguation.
"Computational Linguistics, vol.
23, no.
2, pp.
241-268.Schubert, Lenhart and Chung Hee Hwang (to appear).
"Episodic Logic Meets Little Red Riding Hood: AComprehensive, Natural Representation forLanguage Understanding", in L. Iwanska and S.C.Shapiro (eds.
), Natural Language Processing andKnowledge Representation: Language for Knowledgeand Knowledge for Language, MIT/AAAI Press.Vilain, Marc and David Day (1996).
"Finite-StateParsing by Rule Sequences."
InternationalConference on Computational Linguistics (COLING-96).
Copenhagen, Denmark, August.
TheInternational Committee on ComputationalLinguistics.Vilain, Marc, John Burger, John Aberdeen, DennisConnolly, Lynette Hirschman (1995).
"A Model-Theoretic Coreference Scoring Scheme."
Proc.
SixthMessage Understanding Conference (MUC-6).Morgan Kaufman Publishers.332
