Proceedings of the 10th Conference on Parsing Technologies, pages 36?38,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsThe Impact of Deep Linguistic Processing on Parsing TechnologyTimothy BaldwinUniversity of Melbournetim@csse.unimelb.edu.auMark DrasMacquarie Universitymadras@ics.mq.edu.auJulia HockenmaierUniversity of Pennsylvaniajuliahr@cis.upenn.eduTracy Holloway KingPARCthking@parc.comGertjan van NoordUniversity of Groningenvannoord@let.rug.nlAbstractAs the organizers of the ACL 2007 DeepLinguistic Processing workshop (Baldwin etal., 2007), we were asked to discuss our per-spectives on the role of current trends indeep linguistic processing for parsing tech-nology.
We are particularly interested inthe ways in which efficient, broad coverageparsing systems for linguistically expressivegrammars can be built and integrated intoapplications which require richer syntacticstructures than shallow approaches can pro-vide.
This often requires hybrid technolo-gies which use shallow or statistical methodsfor pre- or post-processing, to extend cover-age, or to disambiguate the output.1 IntroductionOur talk will provide a view on the relevance of deeplinguistic processing for parsing technologies fromthe perspective of the organizers of the ACL 2007Workshop on Deep Linguistic Processing (Baldwinet al, 2007).
The workshop was conceived with thebroader aim of bringing together the different com-putational linguistic sub-communities which modellanguage predominantly by way of theoretical syn-tax, either in the form of a particular theory (e.g.CCG, HPSG, LFG, TAG, the Prague School) or amore general framework which draws on theoreticaland descriptive linguistics.
These ?deep linguisticprocessing?
approaches differ from shallower meth-ods in that they yield richer, more expressive, struc-tural representations which capture long-distancedependencies or the underlying predicate-argumentstructure directly.Aspects of this research have often had their ownseparate fora, such as the ACL 2005 workshop ondeep lexical acquisition (Baldwin et al, 2005), aswell as the TAG+ (Kallmeyer and Becker, 2006),Alpino (van der Beek et al, 2005), ParGram (Buttet al, 2002) and DELPH-IN (Oepen et al, 2002)projects and meetings.
However, the fundamentalapproaches to building a linguistically-founded sys-tem and many of the techniques used to engineerefficient systems are common across these projectsand independent of the specific grammar formal-ism chosen.
As such, we felt the need for a com-mon meeting in which experiences could be sharedamong a wider community, similar to the role playedby recent meetings on grammar engineering (Wint-ner, 2006; Bender and King, 2007).2 The promise of deep parsingDeep linguistic processing has traditionally beenconcerned with grammar development (for use inboth parsing and generation).
However, the linguis-tic precision and complexity of the grammars meantthat they had to be manually developed and main-tained, and were computationally expensive to run.In recent years, machine learning approacheshave fundamentally altered the field of natural lan-guage processing.
The availability of large, manu-ally annotated, treebanks (which typically take yearsof prior linguistic groundwork to produce) enabledthe rapid creation of robust, wide-coverage parsers.However, the standard evaluation metrics for whichsuch parsers have been optimized generally ignore36much of the rich linguistic information in the orig-inal treebanks.
It is therefore perhaps only naturalthat deep processing methods, which often requiresubstantial amounts of manual labor, have receivedconsiderably less attention during this period.But even if further work is required for deepprocessing techniques to fully mature, we believethat applications that require natural language under-standing or inference, among others, will ultimatelyneed detailed syntactic representations (capturing,e.g., bounded and unbounded long-range dependen-cies) from which semantic interpretations can eas-ily be built.
There is already some evidence thatour current deep techniques can, in some cases, out-perform shallow approaches.
There has been workdemonstrating this in question answering, targetedinformation extraction and the recent textual entail-ment recognition task, and perhaps most notably inmachine translation: in this latter field, after a periodof little use of linguistic knowledge, deeper tech-niques are beginning to lead to better performance,e.g.
by redefining phrases by syntactic ?treelets?rather than contiguous word sequences, or by explic-itly including a syntactic component in the probabil-ity model, or by syntactic preprocessing of the data.3 Closing the divideIn the past few years, the divide between ?deep?,rule-based, methods and ?shallow?, statistical, ap-proaches, has begun to close from both sides.
Re-cent advances in using the same treebanks that haveadvanced shallow techniques to extract more expres-sive grammars or to train statistical disambiguatorsfor them, and in developing framework-specific tree-banks, have made it possible to obtain similar cov-erage, robustness, and disambiguation accuracy forparsers that use richer structural representations.
Aswitnessed by many of the papers in our workshop(Baldwin et al, 2007), a large proportion of currentdeep systems have statistical components to them,e.g., as pre- or post-processing to control ambigu-ity, as means of acquiring and extending lexical re-sources, or even use machine learning techniquesto acquire deep grammars automatically.
From theother side of the divide, many of the purely statisticalapproaches are using progressively richer linguisticfeatures and are taking advantage of these more ex-pressive features to tackle problems that were tradi-tionally thought to require deep systems, such as therecovery of traces or semantic roles.4 The continued need for research on deepprocessingAlthough statistical techniques are becoming com-monplace even for systems built around hand-written grammars, there is still a need for furtherlinguistic research and manual grammar develop-ment.
For example, supervised machine-learningapproaches rely on large amounts of manually anno-tated data.
Where such data are available, develop-ers of deep parsers and grammars can exploit themto determine frequency of certain constructions, tobootstrap gold standards for their systems, and toprovide training data for the statistical componentsof their systems such as parse disambiguators.
Butfor the majority of the world?s languages, and evenfor many languages with large numbers of speakers,such corpora are unavailable.
Under these circum-stances, manual grammar development is unavoid-able, and recent progress has allowed the underlyingsystems to become increasingly better engineered,allowing for more rapid development of any givengrammar, as well as for overlay grammars that adaptto particular domains and applications and for port-ing of grammars from one language to another.Despite recent work on (mostly dependencygrammar-based) multilingual parsing, it is still thecase that most research on statistical parsing is doneon English, a fixed word-order language where sim-ple context-free approximations are often sufficient.It is unclear whether our current models and al-gorithms carry over to morphologically richer lan-guages with more flexible word order, and it is possi-ble that the more complex structural representationsallowed by expressive formalisms will cease to re-main a luxury.Further research is required on all aspects ofdeep linguistic processing, including novel linguis-tic analyses and implementations for different lan-guages, formal comparisons of different frame-works, efficient parse and learning algorithms, betterstatistical models, innovative uses of existing dataresources, and new evaluation tools and methodolo-gies.
We were fortunate to receive so many high-37quality submissions on all of these topics for ourworkshop.5 Conclusion and outlookDeep linguistic processing brings together a range ofperspectives.
It covers current approaches to gram-mar development and issues of theoretical linguis-tic and algorithmic properties, as well as the appli-cation of deep linguistic techniques to large-scaleapplications such as question answering and dialogsystems.
Having industrial-scale, efficient parsersand generators opens up new application domainsfor natural language processing, as well as inter-esting new ways in which to approach existing ap-plications, e.g., by combining statistical and deepprocessing techniques in a triage process to pro-cess massive data quickly and accurately at a finelevel of detail.
Notably, several of the papers ad-dressed the relationship of deep linguistic process-ing to topical statistical approaches, in particular inthe area of parsing.
There is an increasing inter-est in deep linguistic processing, an interest whichis buoyed by the realization that new, often hybrid,techniques combined with highly engineered parsersand generators and state-of-the-art machines opensthe way towards practical, real-world application ofthis research.
We look forward to further opportu-nities for the different computational linguistic sub-communities who took part in this workshop, andothers, to continue to come together in the future.ReferencesTimothy Baldwin, Anna Korhonen, and Aline Villavicen-cio, editors.
2005.
Proceedings of the ACL-SIGLEXWorkshop on Deep Lexical Acquisition.
Ann Arbor,USA.Timothy Baldwin, Mark Dras, Julia Hockenmaier,Tracy Holloway King, and Gertjan van Noord, editors.2007.
Proceedings of the ACL Workshop on Deep Lin-guistic Processing, Prague, Czech Republic.Emily Bender and Tracy Holloway King, editors.
2007.Grammar Engineering Across Frameworks, StanfordUniversity.
CSLI On-line Publications.
to appear.Miriam Butt, Helge Dyvik, T. H. King, Hiroshi Masuichi,and Christian Rohrer.
2002.
The parallel grammarproject.
In COLING Workshop on Grammar Engi-neering and Evaluation, Taipei, Taiwan.Laura Kallmeyer and Tilman Becker, editors.
2006.
Pro-ceedings of the Eighth International Workshop on TreeAdjoining Grammar and Related Formalisms (TAG+),Sydney, Australia.Stephan Oepen, Dan Flickinger, J. Tsujii, and HandUszkoreit, editors.
2002.
Collaborative Language En-gineering: A Case Study in Efficient Grammar-basedProcessing.
CSLI Publications.Leonoor van der Beek, Gosse Bouma, Jan Daciuk, TanjaGaustad, Robert Malouf, Mark-Jan Nederhof, Gert-jan van Noord, Robbert Prins, and Bego na Vil-lada Moiro?n.
2005.
Algorithms for linguistic pro-cessing.
NWO Pionier final report.
Technical report,University of Groningen.Shuly Wintner.
2006.
Large-scale grammar developmentand grammar engineering.
Research workshop of theIsrael Science Foundation.38
