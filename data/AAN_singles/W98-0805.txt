The value of minimal prosodic information in spoken languagecorporaCaro l ine  Lyon and J i l l  Hewi t tComputer Science DepartmentUniversity of Hertfordshire, UKAbst ractThis paper eports on an investigation i to rep-resenting tone unit boundaries (pauses) as wellas words in a corpus of spoken English.
Ananalysis of data from MARSEC (Machine Read-able Spoken English Corpus) shows that, forprofessional speakers, the inclusion of this nfin-imal prosodic information will lower the per-plexity of a language model.
The analysis isbased on information theoretic techniques, andan objective method of evaluation isprovided byentropy indicators, which are explained.
Thisresult is of general interest, and supports thedevelopment of improved language models formany applications.
The automated capture ofpauses eems to be technically feasible, and war-rants further investigation.
The specific issuewhich prompted this investigation is a task inbroadcasting technology: the semi-autonmtedproduction of online subtitles for live televisionprogrammes.
This task is described, and an ap-proach to it using speech recognition technologyis explained.1 In t roduct ionThere are a range of choices to be made whendeciding on the contents of spoken language cor-pora.
Useful information will be made avail-able by including prosodic annotations but thisis difficult to automate.
This paper looks atthe advantages that can be expected to accruefrom the inclusion of minimal prosodic informa-tion, major and minor tone unit boundaries, orpauses.
Capturing this information automati-cally does not present he same difficulties asproducing a full prosodic annotation: a methodis described by (Huckvale and Fang, 1996).
Apurpose of this paper is to show that the devel-opment of such methods warrants further inves-tigation.The task which prompted this investigationwill use trained speakers in a controlled envi-ronment, as described below.We investigate how much extra informationis captured by representing major and minorpauses, as well as words, in a corpus of spokenEnglish.
We find that for speech such as broad-cast news or talks the inclusion of this minimalprosodic annotation will lower the perplexity ofa language model.This result is of general interest, and supportsthe development of improved language modelsfor many applications.
However, the specific is-sue which we address is a task in broadcastingtechnology: the semi-automated production ofonline subtitles for live television programmes.Contemporaneous subtitling of television pro-grammes for the hearing impaired is set to in-crease significantly, and in the UK there is amandatory requirement placed on T\: broad-casters to cover a certain proportion of live pro-grammes.
This skilled task is currently doneby highly trained stenographers (the subtitlers),but it could be semi-automated.
First, the sub-titlers can transfer broadcast speech to writ-ten captions using automated speech recogni-tion systems instead of specialist, keyboards.
Asimilar approach is being introduced for someCourt reporting, where trained speakers in acontrolled environment may replace traditionalstenographers.
Secondly, the display of the cap-tions can be automated, which is the problemwe address here.
It is necessary to place linebreaks in appropriate places o the subtitler canbe relieved of this secondary task.Based on previous work in this field (Lyonand Frank, 1997) a system will be developed toprocess the output of an ASR device.
\Ve needto collect examples of this output as corpora oftraining data, and have investigated the type40of information that it will be useful to obtain.Commercially available speech recognizers typi-cally output only the words spoken by the user,but as an intermediate stage in producing sub-titles we may want to use prosodic informationthat has been made explicitly available.Information theoretic techniques can be usedto determine how useful it is to capture minimalprosodic information, major and minor pauses.The experiments reported here take a prosodi-cally marked corpus of spoken English, the Ma-chine Readable Spoken English Corpus (MAR-SEC).
Different representations are compared,in which the language model either omits or in-cludes major and minor pauses as well as words.We need to assess how nmch the structureof language is captured by the different meth-ods of representation, and an objective methodof evaluation is provided by entropy indicators,described below.The relationship between prosody and syntaxis well known (Arnfield, 1994; Fang and Huck-vale, 1996; Ostendorfand \'ielleux, 1994; Taylorand Black, 1998).
\Vork in this field has typi-cally focussed on the problem in speech syn-thesis of mapping a text onto natural soundingspeech.
Our work investigates the complemen-tary problem of mapping speech onto segmentedtext.It is not claimed that pauses are only pro-duced as structural markers: hesitation phe-nomena perform a number of roles, particularlyin spontaneous speech.
However, it has beenshown that the placement of pauses provideclues to syntactic structure.
We introduce a sta-tistical measure that can help indicate whetherit is worth going to the trouble of capturing cer-tain types of prosodic information for processingthe output of trained speakers.Contents  of paperThis paper is organised in the following way.In Section 2 we describe the MARSEC corpus,which is the basis for the analysis.
Section 3describes the entropy metric, and explains thetheory behind its application.
Section 4 de-scribes the experiments that were done, andgives the results.
These indicate that it will beworthwhile to capture minimal prosodic infor-mation.
In Section 5 we describe the subtitlingtask which prompted this investigation, and thepaper concludes with Section 6 which puts ourwork into a wider context.2 MARSEC : Mach ine  Readab leSpoken English CorpusSince trained speakers will be producing thesubtitles a corpus of speech from professionalbroadcasters i  appropriate for this initial in-vestigation.
The MARSEC corpus has beenmainly collected from the BBC, and is availablefree on the web (see references).
It is markedwith prosodic annotations but not with POStags.
\Ve have used part of the corpus, justover 26,000 words, comprising the 4 categoriesof news commentary (A), news broadcasts (B),lectures aimed at a general audience (C) andlectures aimed at a restricted audience (D).The prosodic markup has been done manu-ally, by two annotators.
Some passages havebeen done by both, and we see that there is ageneral consensus, but some differing decisions.Interpreting the speech data has an element ofsubjectivit3: In Table 1 we show some sampledata as we used it, in which only the major andminor tone unit boundaries are retained.
\Vhenpassages were marked up twice, we chose onein an arbitrary way, so that each annotator waschosen about equally.2.1 Comparison of automated  andmanual  markup of pausesWe suggest hat the production of this type ofdata may be technically feasible for a trainedspeaker using an ASR device, and is worth in-vestigating further.
(Huckvale and Fang, 1996) describe theirmethod of automatically capturing pause infor-mation for the PROSICE corpus.
The detectionof major pauses is technically straightforward:they find regions of the signal that fall belowa certain energy threshold (60Db) for at least250ms.
Minor pauses are more difficult to find,since they can occur within words, and theirdetection is integrated into the signal / wordalignment process.\\re find that in that in the manually anno-tated MARSEC corpus, the ratio of words tomajor pauses is approximately 17.8, to minorpauses 5.4, or 4.1 if both type of pause are takentogether.
(Huckvale and Fang, 1996) quote fig-ures that work out at 7.7 for major pauses, 30.8for minor ones, or 6.15 taken together.
Thissuggests that there is some discrepancy between41Key:\[I is major pause \] is minor pauseannotator 1 annotator 2we weheard heardautomatic automaticfire fireI \]a afew fewyards yardsaway awayI i\]we wedrove droveOnl onII IIa ajet jetappeared appearedTable 1: Example of MARSEC corpus with minimal prosodic annotationswhat is considered a major or minor pause.However, taking both together the resuks fromthe automated system is not out of line with themanual one on this statistical measure.3 Ent ropy  ind icatorsThe entropy is a measure, in a certain sense, ofthe degree of unpredictability.
If one representa-tion captures more of the structure of languagethan another, then the entropy measures shoulddecline.If H represents the entropy of a sequence andP the perplexity, thenp=2 HP can be seen as the branching factor, or num-ber of choices.3.1 Def in i t ion  of  ent ropyLet ..4 be an alphabet, and X be a discrete ran-dom variable.
The probability mass function isthen p(x), such thatp(x) = probabi l i ty(X = x), x E .,4For an initial investigation i to the entropy ofletter sequences the x's would be the 26 lettersof the standard alphabet.The entropy H(X)  is defined asH(X)  = - Z p(x) ?
Io9 p(x)xE.AIf logs to base 2 are used, the entropy mea-sures tile minimum number of bits needed onaverage to represent X: the wider the choicethe more bits will be needed to describe it.\Ve talk loosely of the entropy of a sequence,but more precisely consider a sequence of sym-bols Xi which are outputs of a stochastic pro-cess.
We estimate the entropy of the distribu-tion of which the observed outcome is typical.Further eferences are (Bell et al, 1990; Coverand Thomas, 1991), or, for an introduction,(Charniak, 1993, Chapter 2).3.1.1 Shannon 's  workThough we are investigating sequences of words,the subject is introduced by recalling Shannon'swell known work on the entropy of letter se-quences (Shannon, 1951).
He demonstratedthat the entropy will decline if a representationis found that captures (i) the context and (ii)the structure of the sequence.Shannon produced a series of approximationsto the entropy H of written English, which sue-cessively take more of the statistics of the lan-42guage into account.
H0 represents he averagenumber of bits required to determine a letterwith no statistical information.
Thus, for analphabet of 16 symbols H0 = 4.0.H1 is calculated with information on singleletter probabilities.
If we knew, for example,that letter e had probability of 20~ of occurringwhile z had 1% we could code the alphabet with,on average, fewer bits than we could withoutthis information.
Thus H1 would be lower thanH0.H2 uses information on the probability of 2letters occurring together; Hn, called the n-gram entropy, measures the amount of entropywith information extending over n adjacent let-ters of text, 1 and Hn _< Hn-1.
As n increasesfrom 0 to 3, the n-gram entropy declines: thedegree of predictability is increased as informa-tion from more adjacent letters is taken into ac-count.
This fact is exploited in games where thecontestants have to guess letters in words, suchas the "Shannon game" or "Hangman" (Jelinek,1990).The formula for calculating the entropy of asequence is given in (Lyon and Brown, 1997).An account of the process is also given in (Coverand Thomas, 1991, chapter2) and (Shannon,1951).3.2  Entropy and st ructureThe entropy can also be reduced if some of thestructure of the letter strings is captured.
AsShammn says "a word is a cohesive group of let-ters with strong internal statistical influences"so the introduction of the space character toseparate words should lower the entropy H2 andHa.
With an extra symbol in the alphabet H0will rise.
There will be more potential pairs andtriples, so H2 and H3 could rise.
However, asthe space symbol will prevent "irregular" lettersequences between words, and thus reduce theunpredictability H~ and Ha do in fact decline.For instance, for the wordsCOOKING CHOCOLATEthe trigramsreplaced by"space-C-H'.."N-G-C" and "G-C?H" will be"N-G-space", "G-space-C" and1This notation is derived from that used by Shannon.It differs from that used by (Bell et al, 1990).3.3 The entropy of ASCI I  dataFor other representations too, the insertion ofboundary markers that capture the structure ofa sequence will reduce the entropy.
Gull andSkilling (1987) report on an experiment with astring of 32,768 zeroes and ones that are knownto be ASCII data organised in patterns of 8as bytes, but with the byte boundary markermissing.
By comparing the entropy of the se-quence with the marker in different positionsthe boundary of the data is "determined to aquite astronomical significance l vel".3.4 The entropy of word sequencesThis method of analysis can also be applied tostrings of words.
The entropy indicator willshow if a sequence of words can be decom-posed into segments, o that some of the struc-ture is captured.
Our current work investigateswhether pauses in spoken English perform thisrole.Previously we showed how the entropy oftext mapped onto part-of-speech tags could bereduced if clauses and phrases were explicitlymarked (Lyon and Brown, 1997).
Syntacticmarkers can be considered analogous to spacesbetween words in letter sequence analysis.
Theyare virtual punctuation marks.Consider, for example, how subordinateclauses are discerned.
There may be an explicitopening marker, such as a 'wh' word, but oftenthere is no mark to show the end of the clause.If markers are inserted and treated as virtualptmctuation some of the structure is capturedand the entropy declines.
A sentence withoutopelfing or closing clause boundary markers,likeThe shirt he wants is in the wash.can be represented asThe shirt { he wants } is in thewash.This sentence can be given part-of-speechtags, with two of the classes in the tagset rep-resenting the symbols '{' (virtual-tagl)and '}' (virtual-tag2).
The ordinary part-of-speech tags have probabilistic relationships withthe virtual tags in the same way that they dowith each other.
The pairs and triples generatedby the second string exclude (noun, pronoun),(noun, pronoun, verb) but include, for instance,(noun, virtual-tag1), (noun, virtual-tag1, pro-noun)43Using this representation, the entropy, H2and H3, with virtual tags explicitly raarkingsome constituents i  lower than that without hevirtual tags.
In a similar way the words froma speech signal can be segmented into groups,with periodic pauses.4 Resu l ts  f rom ana lys i s  o f  theMARSEC corpusWe can measure the entropy H0, Hi, .H2 andH3 for the corpus with and without prosodicmarkers for major and minor pauses.
However,rather than use words themseh'es we map themonto part-of-speech tags.
This reduces an in-definite number of words to a limited numberof tags, and makes the investigation computa-tionally feasible.
We expect?
H0 will be higher with a marker, since thealphabet size increases?
H1, which takes into account the singleelement probabilities, will increase or de-crease depending on the frequency of thenew symbol.?
H2 and H3 should fall if the symbols repre-senting prosodic markers capture some ofthe language structure.
We expect: H3 toshow this more than H2.?
If instead of the real pause markers mockones are inserted in an arbitrary thshion:we expect H to rise in all cases.To conduct this investigation the MARSECcorpus was taken off the web; and pre-processedto leave the words plus major and minor toneunit boundaries, or pauses.
Then it was auto-matically tagged, using a version of the Clawstagger 2.
These tags were mapped onto a smallertagset with 26 classes, 28 including the majorand minor pauses.
The tagset is given in theappendix.
Random inspection indicated about96% words correctly tagged.Then the entropy of part of the corpus wascalculated (i) for words only (ii) with minorpauses represented (iii) with major pauses rep-resented and (ix') with major and minor pausesrepresented.
Results are shown in Table 2, andin Figure 1.~Claws4, supplied by the University of Lancaster, de-scribed by Garside (1987)H3 is calculated in two different ways.
First,the sequence of tags is taken as an uninterruptedstring (column H3 (1) in Table 2).
Secondly,we take the major pauses as equivalent to sen-tence ends, points of segmentation, and omitan)" triple that spans 2 sentences (column H3(2)).
In practice, this will be a sensible ap-proach.This experiment shows how the entropy H3declines when information on pauses is ex-plicitly represented.
Though there is not atransparent mapping from prosody to structure,there is a relationship between them which canbe exploited.
These experiments indicate thatEnglish language used by professional speakerscan be coded more efficiently when pauses arerepresented.4.1 Compar i son  wi th  a rb i t ra r i l y  p lacedpausesCompare these results to those of another ex-periment where the corpora of words only weretaken and pauses inserted in an arbitrary man-ner.
Major pauses were inserted every 19 words,minor pauses every 7 words, except where thereis a clash with a major pause.
The numbersof major and minor pauses are comparable tothose in the real data.
Results are shown inTable 3.
H2 and H3 are higher than the compa-rable entropy levels for speech with pauses in-serted as they were actually spoken.
Moreover,the entropy levels are higher than for speechwithout any pauses: the arbitrary insertion hasdisrupted the underlying structure, and raisedthe unpredictability of the sequence.4.2 Ent ropy  and corpus  s\]zeNote that we are interested in comparative en-tropies.
The entropy converges lowly to itsasymptotic value as the size of the corpora in-creases, and this is an upper bound on en-tropy values for smaller corpora.
Ignoring thismay give misleading results (Farach and et al,1995).
The reason why entropy is underesti-mated for small corpora comes from the factthat we approximate probabilities by frequencycounts, and for small corpora these may be poorapproximations.
The count of pairs and triplesis the basis for the probability estimates, andwith small corpora many of the triples in par-ticular that will show later have not occurred.Thus the entropy and perplexity measures un-443.5 I I I I ICOI2.5EO~x= 2 O(D(DP~: 1.5Q.P- 1w0.5Words + markersWords only0 f I I I I0 5000 10000 15000 20000 25000Corpus sizein words30000Figure 1: Comparison of trigram part-of-speech entropy for sections of the MARSEC corpus,(i) with both major and minor pauses marked (ii) without either.
The tagset size is 28 with thepauses represented, 26 without them.
The entropy is calculated with trigrams spanning a majorpause omitted, as in Table 2 column H3 (2)derestimate their true values.5 The  subt i t l ing  taskWe now show how the investigation describedhere is relevant o the subtitling task.
Trainedsubtitlers are employed to output real time cap-tions for some TV programmes, currently as astream of type written text.
In future this maybe done with an ASR system.
In either case,the production of the caption is followed by thetask of displaying the text so that line breaksoccur in natural positions.
The type of pro-grammes for which this is needed include con-temporaneous commentary on on news events,sports, studio discussions, and chat shows.Caption format is limited by line length,and there are usually at most 2 lines per cap-tion.
Some examples of subtitles that have beendisplayed are taken from the broadcast com-mentary o11 Princess Diana's funeral, with linebreaks as shown:As I said the great tenor bell ishalf muffled witha piece of leather around itsclapperThey now bear the coffin of thePrincessof Wales into Westminster Abbey.An example ~oma chat show is:Who told you that you resemble MrHague?45Speech Number of Number ofrepresentation minor pauses major pausesX\brds only 0 0Words + minor pauses 3454 0Words + major pauses 0 1029Words + both pauses 3454 1029H0 H1 H2 H3 H3(1) (2)4.70 4.11 3.29 2.94 2.944.75 4.09 3.18 2.84 2.844.75 4.19 3.32 2.94 2.844.81 4.17 3.16 2.82 2.70Table 2: Entropy measures for 18655 words of the MARSEC corpus, (sections A, B, C concatenated)with and without major and minor pauses.
H3 (2) measures entropy without triples spanning amajor pause (see text).Speech Number of Number ofrepresentation minor pauses major pausesWords + pauses in 3109 1209arbitrary positionsH0 Hi H2 H3(i)4.81 4.19 3.63 3.05Table 3: Entropy measures for the same part of the MARSEC corpus with pauses in arbitrarypositions : a major pause every 19 words, minor pause every 7 words (except for clashes withmajor)I work at a golf club and we havelotsof societies and groups come in.The quality of the subtitles can be improvedby placing the line breaks and caption breaks inplaces where the text would be naturally seg-mented.
Though this is partially a subjectiveprocess, a style book can be produced that givesagreed guidelines.Some of the poor line breaks can be readilycorrected, but the production of a high qualitydisplay overall is not a trivial task.
The pausesin speech do not map straight onto suitable linebreaks, but they are a significant source of infor-mation.
In this work we have been consideringthe output of trained speakers, or the recordingof rehearsed speech.
This differs from ordinary,spontaneous speech, where hesitation phenom-ena may have a number of causes.
However,in the type of speech we are processing we haveshown that the use of pauses captures some syn-tactic structure.
An example given by (Osten-dorf and Vielleux, 1994) isMary was amazed Ann Dewey wasangry.which was produced asMary was amazed \[\[ Ann Dewey wasangry.To illustrate a problem of text segmentationconsider how conjunctions should be handled.Now conjunctions join like with like: verb withverb, noun with noun, clause With clause, andso on.
If a conjunction joins two single words,such as ::black and blue" we do not want it totrigger a line break.
However, it may be a rea-sonable break point if it joins two longer com-ponents.
Consider the following example fromthe MARSEC corpus:it held its trajectory for oneminute I flashes burst I fromits wings I and rocketsexploded \] safely behind us IIThe word "and" without the pause markedis part of a trigram :;noun conjunction oun"which would typically stick together.
In fact,it actually joins two sentences, and would be agood point for a break.
By including the pausemarker we can identify this.The proposed system for finding line breakswill integrate rule based and data driven com-ponents.
This approach is derived from earlierwork in a related field in which a partial parserhas been developed (Lyon and Frank, 1997).
Itwill be based on a part-of-speech trigram modelcombined with lexical information.
We will beable to develop abetter language model if we ex-plicitly include a representation for major andminor pauses.466 Ro le  o f  pauses  in speech  in aw ider  contextAs hypothesized the entropy, H3, declines asthe major and minor boundary markers are in-serted.
This indicates that it will be worthwhileto capture the prosodic information on majorand minor pauses from the ASR, in addition tothe usual transcription of the words themselves.Our investigation was prompted by a spe-cific task in which the output of trained speak-ers is transcribed automatically.
However, itis of wider interest.
\\re show that represent-ing pauses as well as words helps determine thestructure of language, and thus contribute tothe quality of a language model.It is many years since Mandelbrot investi-gated the way in which the statistical structureof language is best adapted to coding words(Mandelbrot, 1952).
He suggested that lan-guage is "intentionally if not consciously pro-duced in order to be decoded word by word inthe easiest possible fashion."
If we accept hissuggestion we would expect that naturally oc-curring events, such as pauses in speech, areutilised to facilitate the transfer of information.Patterns of speech segmentation are likely toemerge to produce an efficient coding (Lyon;1998).Re ferencesS Arnfield.
1994.
P~vsody and Syntax in Cor-pus Based Analysis of Spoken English.
Ph.D.thesis, University of Leeds.T C Bell, J G Cleary, and I H \Vitten.
1990.Text Compression.
Prentice Hall.E Charniak.
1993.
Statistical Lar:guage Learn-ing.
MIT Press.T hi Cover and J A Thomas.
1991.
Elementsof Information Theory.
John Wiley and SonsInc.Alex Chengyu Fang and Mark Huckvale.
1996.Synchronising syntax with speech signals.
InV.Hazan, M.Holland, and S.Rosen, editors,Speech, Hearing and Language.
UniversityCollege London.M Farach and M Noordewier et al 1995.
Onthe entropy of dna.
In Symposium on Dis-crete Algorithms.R Garside.
1987.
The CLAWS word-taggingsystem.
In R Garside, G Leech, and G Samp-son, editors, The Computational Analysis ofEnglish: a corpus based approach, pages 30-41.
Longman.S Gull and J Skilling.
1987.
Recent develop-ments at cambridge.
In C Ray Smith andGary Erickson, editors, Maximum -Entropyand Bayesian Spectral Analysis and Estima-tion Problems.M Huckvale and A C Fang.
1996.
PROSICE:A spoken English database for prosody re-search.
In S Greenbaum, editor, ComparingEnglish Worldwide: The International Cor-pus off English.
0 U P.F Jelinek.
1990.
Self-organized language mod-eling for speech recognition.
In A Waibel andK F Lee, editors, Readings in Speech Recog-nition, pages 450-503.
Morgan Kaufmann.IBM T.J.Watson Research Centre.C Lyon and S Brown.
1997.
Evaluating ParsingSchemes with Entropy Indicators.
In MOL5,5th Meeting on the Mathematics of Language.C Lyon and R Frank.
1997.
Using Single LayerNetworks for Discrete, Sequential Data: anExample from Natural Language Processing.Neural Computing Applications, 5 (4).C Lyon.
1998.
Language evolution: survivalof the fittest in the statistical enviromnent.Technical report, Computer Science Depart-ment, University of Hertfordshire, June.B Mandelbrot.
1952.
An informational theoryof the statistical structure of language.
InSymposium on Applications of CommuT~ica-tion Theory.
Butterworth..~I Ostendorf and N Vielleux.
1994.
A hierar-chical stochastic model for automatic predic-tion of prosodic boundary location.
Compu-tational Linguistics, 20(1).C E Shannon.
1951.
Prediction and Entropy ofPrinted English.
Bell System Technical Jour-nal, pages 50-64.P Taylor and A Black.
1998.
Assigning phrasebreaks from part-of-speech sequences.47AppendixDescr ip t ion  o f  the  TagsetThe tagset used in these experiments i derivedfrom CLAWS4, mapped onto a smaller set ofclasses.
They are as follows* article or determiner - singular?
article or determiner - plural?
predeterminer .g.
"all"?
pronomial  determiner e.g.
"some"* pronomial determiner - singular?
proper noun?
noun-  singular?
noun-  plural?
pronoun - singular?
pronoun - plural?
relative pronoun?
possessive pronoun?
verb - singular?
verb - plural?
auxil iary verb - singular?
auxil iary verb - plural?
existential "here" or "there"?
present participle?
past participle?
infinitive "to"?
preposit ion?
conjunct ion?
adjective?
singular number "one"?
adverb?
exceptionsTwo extra tag classes are added for the anal-5"sis of tone unit boundaries:?
minor pause?
major  pauseThe tagging process includes the identifica-tion of common phrases or idioms, which arethen treated as single lexical items.
For in-stance, "of course" is tagged as an adverb.48
