Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 74?83,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsWeakly Supervised Models of Aspect-Sentimentfor Online Course Discussion ForumsArti Ramesh,1Shachi H. Kumar,2James Foulds,2Lise Getoor21University of Maryland, College Park2University of California, Santa Cruzartir@cs.umd.edu, {shulluma, jfoulds, getoor}@ucsc.eduAbstractMassive open online courses (MOOCs)are redefining the education system andtranscending boundaries posed by tradi-tional courses.
With the increase in pop-ularity of online courses, there is a cor-responding increase in the need to under-stand and interpret the communications ofthe course participants.
Identifying top-ics or aspects of conversation and inferringsentiment in online course forum postscan enable instructor interventions to meetthe needs of the students, rapidly addresscourse-related issues, and increase studentretention.
Labeled aspect-sentiment datafor MOOCs are expensive to obtain andmay not be transferable between courses,suggesting the need for approaches that donot require labeled data.
We develop aweakly supervised joint model for aspect-sentiment in online courses, modeling thedependencies between various aspects andsentiment using a recently developed scal-able class of statistical relational modelscalled hinge-loss Markov random fields.We validate our models on posts sam-pled from twelve online courses, each con-taining an average of 10,000 posts, anddemonstrate that jointly modeling aspectwith sentiment improves the prediction ac-curacy for both aspect and sentiment.1 IntroductionMassive Open Online Courses (MOOCs) haveemerged as a powerful medium for imparting edu-cation to a wide geographical population.
Discus-sion forums are the primary means of communica-tion between MOOC participants (students, TAs,and instructors).
Due to the open nature of thesecourses, they attract people from all over the worldleading to large numbers of participants and hence,large numbers of posts in the discussion forums.In the courses we worked with, we found that overthe course of the class there were typically over10,000 posts.Within this slew of posts, there are valuableproblem-reporting posts that identify issues suchas broken links, audio-visual glitches, and in-accuracies in the course materials.
Automati-cally identifying these reported problems is impor-tant for several reasons: i) it is time-consumingfor instructors to manually screen through all ofthe posts due to the highly skewed instructor-to-student ratio in MOOCs, ii) promptly address-ing issues could help improve student retention,and iii) future iterations of the course could ben-efit from identifying technical and logistical is-sues currently faced by students.
In this paper,we investigate the problem of determining thefine-grained topics of posts (which we refer toas ?MOOC aspects?)
and the sentiment towardthem, which can potentially be used to improvethe course.While aspect-sentiment has been widely stud-ied, the MOOC discussion forum scenariopresents a unique set of challenges.
Labeled dataare expensive to obtain, and posts containing fine-grained aspects occur infrequently in courses anddiffer across courses, thereby making it expensiveto get sufficient coverage of all labels.
Few distinctaspects occur per course, and only 5-10% of postsin a course are relevant.
Hence, getting labels forfine-grained labels involves mining and annotatingposts from a large number of courses.
Further, cre-ating and sharing labeled data is difficult as datafrom online courses is governed by IRB regula-74tions.
Privacy restrictions are another reason whyunsupervised/weakly-supervised methods can behelpful.
Lastly, to design a system capable of iden-tifying all possible MOOC aspects across courses,we need to develop a system that is not fine-tunedto any particular course, but can adapt seamlesslyacross courses.To this end, we develop a weakly supervisedsystem for detecting aspect and sentiment inMOOC forum posts and validate its effectivenesson posts sampled from twelve MOOC courses.Our system can be applied to any MOOC discus-sion forum with no or minimal modifications.Our contributions in this paper are as follows:?
We show how to encode weak supervisionin the form of seed words to extract ex-tract course-specific features in MOOCs us-ing SeededLDA, a seeded variation of topicmodeling (Jagarlamudi et al, 2012).?
Building upon our SeededLDA approach,we develop a joint model for aspects andsentiment using the hinge-loss Markov ran-dom field (HL-MRF) probabilistic modelingframework.
This framework is especiallywell-suited for this problem because of itsability to combine information from multiplefeatures and jointly reason about aspect andsentiment.?
To validate the effectiveness of our system,we construct a labeled evaluation dataset bysampling posts from twelve MOOC courses,and annotating these posts with fine-grainedMOOC aspects and sentiment via crowd-sourcing.
The annotation captures fine-grained aspects of the course such as content,grading, deadlines, audio and video of lec-tures and sentiment (i.e., positive, negative,and neutral) toward the aspect in the post.?
We demonstrate that the proposed HL-MRFmodel can predict fine-grained aspects andsentiment and outperforms the model basedonly on SeededLDA.2 Related WorkTo the best of our knowledge, the problem of pre-dicting aspect and sentiment in MOOC forums hasnot yet been addressed in the literature.
We reviewprior work in related areas here.Aspect-Sentiment in Online Reviews It isvaluable to identify the sentiment of online re-views towards aspects such as hotel cleanlinessand cellphone screen brightness, and sentimentanalysis at the aspect-level has been studied ex-tensively in this context (Liu and Zhang, 2012).Several of these methods use latent Dirichlet alo-cation topic models (Blei et al, 2003) and variantsof it for detecting aspect and sentiment (Lu et al,2011; Lin and He, 2009).
Liu and Zhang (2012)provide a comprehensive survey of techniques foraspect and sentiment analysis.
Here, we discussworks that are closely related to ours.Titov and McDonald (2008) emphasize the im-portance of an unsupervised approach for aspectdetection.
However, the authors also indicate thatstandard LDA (Blei et al, 2003) methods captureglobal topics and not necessarily pertinent aspects?
a challenge that we address in this work.
Brodyand Elhadad (2010), Titov and McDonald (2008),and Jo and Oh (2011) apply variations of LDA atthe sentence level for online reviews.
We find thataround 90% of MOOC posts have only one aspect,which makes sentence-level aspect modeling inap-propriate for our domain.Most previous approaches for sentiment rely onmanually constructed lexicons of strongly positiveand negative words (Fahrni and Klenner, 2008;Brody and Elhadad, 2010).
These methods are ef-fective in an online review context, however senti-ment in MOOC forum posts is often implicit, andnot necessarily indicated by standard lexicons.
Forexample, the post ?Where is my certificate?
Wait-ing over a month for it.?
expresses negative sen-timent toward the certificate aspect, but does notinclude any typical negative sentiment words.
Inour work, we use a data-driven model-based ap-proach to discover domain-specific lexicon infor-mation guided by small sets of seed words.There has also been substantial work on jointmodels for aspect and sentiment (Kim et al, 2013;Diao et al, 2014; Zhao et al, 2010; Lin et al,2012), and we adopt such an approach in this pa-per.
Kim et al (2013) use a hierarchical aspect-sentiment model and evaluate it for online reviews.Mukherjee and Liu (2012) use seed words for dis-covering aspect-based sentiment topics.
Drawingon the ideas of Mukherjee and Liu (2012) andKim et al (2013), we propose a statistical rela-tional learning approach that combines the advan-tages of seed words, aspect hierarchy, and flat75Post 1: I have not received the midterm.Post 2: No lecture subtitles week, will they be uploaded?Post 3: I am ... and I am looking forward to learn more ...Table 1: Example posts from MOOC forums.
As-pect words are highlighted in bold.aspect-sentiment relationships.
It is important tonote that a broad majority of the previous workon aspect sentiment focuses on the specific chal-lenges of online review data.
As discussed in de-tail above, MOOC forum data have substantiallydifferent properties, and our approach is the firstto be designed particularly for this domain.Learning Analytics In another line of research,there is a growing body of work on the analy-sis of online courses.
Regarding MOOC forumdata, Stump et al (2013) propose a frameworkfor taxonomically categorizing forum posts, lever-aging manual annotations.
We differ from theirapproach in that we develop an automatic systemto predict MOOC forum categories without usinglabeled training data.
Ramesh et al (2014b) cat-egorize forum posts into three broad categories inorder to predict student engagement.
Unlike thismethod, our system is capable of fine-grained cat-egorization and of identifying aspects in MOOCS.Chaturvedi et al (2014) focus on predicting in-structor intervention using lexicon features andthread features.
In contrast, our system is capableof predicting fine MOOC aspects and sentiment ofdiscussion forum posts and thus provides a moreinformed analysis of MOOC posts.3 Problem Setting and DataMOOC participants primarily communicatethrough discussion forums, consisting of posts,which are short pieces of text.
Table 1 providesexamples of posts in MOOC forums.
Posts 1 and2 report issues and feedback for the course, whilepost 3 is a social interaction message.
Our goal isto distinguish problem-reporting posts such as 1and 2 from social posts such as 3, and to identifythe issues that are being discussed.We formalize this task as an aspect-sentimentprediction problem (Liu and Zhang, 2012).
Theissues reported in MOOC forums can be related tothe different elements of the course such as lec-tures and quizzes, which are referred to as aspects.The aspects are selected based on MOOC domainexpertise and inspiration from Stump et al (2013),aiming to cover common concerns that could ben-efit from intervention.
The task is to predict theseCOARSE-ASPECT FINE-ASPECT Description # of postsLECTURELECTURE-CONTENT Content of lectures.
559LECTURE-VIDEO Video of lectures.
215LECTURE-SUBTITLES Subtitles of lecture.
149LECTURE-AUDIO Audio of lecture.
136LECTURE-LECTURER Delivery of instructor.
69QUIZQUIZ-CONTENT Content in quizzes.
439QUIZ-GRADING Grading of quizzes.
360QUIZ-SUBMISSION Quiz submission.
329QUIZ-DEADLINE Deadline of quizzes.
142CERTIFICATE Course certificates.
194SOCIAL Social interaction posts.
1187Table 2: Descriptions of coarse and fine aspects.aspects for each post, along with the sentiment po-larity toward the aspect, which we code as posi-tive, negative, or neutral.
The negative-sentimentposts, along with their aspects, allow us to iden-tify potentially correctable issues in the course.As labels are expensive in this scenario, we for-mulate the task as a weakly supervised predictionproblem.
In our work, we assume that a post hasat most one fine-grained aspect, as we found thatthis was true for 90% of the posts in our data.This property is due in part to the brevity of fo-rum posts, which are much shorter documents thanthose considered in other aspect-sentiment scenar-ios such as product reviews.3.1 Aspect HierarchyWhile we do not require labeled data, our ap-proaches allow the analyst to instead relativelyeasily encode a small amount of domain knowl-edge by seeding the models with a few words re-lating to each aspect of interest.
Hence, we referto our approach as weakly supervised.
Our modelscan further make use of hierarchical structure be-tween the aspects.
The proposed approach is flex-ible, allowing the aspect seeds and hierarchy to beselected for a given MOOC domain.For the purposes of this study, we represent theMOOC aspects with a two-level hierarchy.
Weidentify a list of nine fine-grained aspects, whichare grouped into four coarse topics.
The coarseaspects consist of LECTURE, QUIZ, CERTIFICATE,and SOCIAL topics.
Table 2 provides a descriptionof each of the aspects and also gives the number ofposts in each aspect category after annotation.As both LECTURE and QUIZ are key coarse-level aspects in online courses, and more nu-anced aspect information for these is importantto facilitate instructor interventions, we iden-tify fine-grained aspects for these coarse aspects.76For LECTURE we identify LECTURE-CONTENT,LECTURE-VIDEO, LECTURE-AUDIO, LECTURE-SUBTITLES, and LECTURE-LECTURER as fineaspects.
For QUIZ, we identify the fine as-pects QUIZ-CONTENT, QUIZ-GRADING, QUIZ-DEADLINES, and QUIZ-SUBMISSION.
We use thelabel SOCIAL to refer to social interaction poststhat do not mention a problem-related aspect.3.2 DatasetWe construct a dataset by sampling posts fromMOOC courses to capture the variety of aspectsdiscussed in online courses.
We include coursesfrom different disciplines (business, technology,history, and the sciences) to ensure broad coverageof aspects.
Although we adopt an approach thatdoes not require labeled data for training, which isimportant for most practical MOOC scenarios, inorder to validate our methods we obtain labels forthe sampled posts using Crowdflower,1an onlinecrowd-sourcing annotation platform.
Each postwas annotated by at least 3 annotators.
Crowd-flower calculates confidence in labels by comput-ing trust scores for annotators using test questions.Kolhatkar et al (2013) provide a detailed analysisof Crowdflower trust calculations and the relation-ship to inter-annotator agreement.
We follow theirrecommendations and retain only labels with con-fidence > 0.5.4 Aspect-Sentiment Prediction ModelsIn this section, we develop models and feature-extraction techniques to address the challenges ofaspect-sentiment prediction for MOOC forums.We present two weakly-supervised methods?first, using a seeded topic modeling approach (Ja-garlamudi et al, 2012) to identify aspects and sen-timent.
Second, building upon this method, wethen introduce a more powerful statistical rela-tional model which reasons over the seeded LDApredictions as well as sentiment side-informationto encode hierarchy information and correlationsbetween sentiment and aspect.4.1 Seeded LDA ModelTopic models (Blei et al, 2003), which identifylatent semantic themes from text corpora, havepreviously been successfully used to discover as-pects for sentiment analysis (Diao et al, 2014).
Byequating the topics, i.e.
discrete distributions over1http://www.crowdflower.com/words, with aspects and/or sentiment polarities,topic models can recover aspect-sentiment predic-tions.
In the MOOC context we are specifically in-terested in problems with the courses, rather thangeneral topics which may be identified by a topicmodel, such as the topics of the course material.To guide the topic model to identify aspects ofinterest, we use SeededLDA (Jagarlamudi et al,2012), a variant of LDA which allows an analyst to?seed?
topics by providing key words that shouldbelong to the topics.We construct SeededLDA models by providinga set of seed words for each of the coarse and fineaspects in the aspect hierarchy of Table 2.
We alsoseed topics for positive, negative and neutral sen-timent polarities.
The seed words for coarse topicsare provided in Table 3, and fine aspects in Ta-ble 4.
For the sentiment topics (Table 5), the seedwords for the topic positive are positive words of-ten found in online courses such as thank, congrat-ulations, learn, and interest.
Similarly, the seedwords for the negative topic are negative in thecontext of online courses, such as difficult, error,issue, problem, and misunderstand.Additionally, we also use SeededLDA for iso-lating some common problems in online coursesthat are associated with sentiment, such as dif-ficulty, availability, correctness, and course-specific seed words from the syllabus as describedin Table 6.
Finally, having inferred the Seed-edLDA model from the data set, for each post pwepredict the most likely aspect and the most likelysentiment polarity according to the post?s inferreddistribution over topics ?
(p).In our experiments, we tokenize and stem theposts using NLTK toolkit (Loper and Bird, 2002),and use a stop word list tuned to online course dis-cussion forums.
The topic model Dirichlet hyper-parameters are set to ?
= 0.01, ?
= 0.01 in our ex-periments.
For SeededLDA models correspondingto the seed sets in Tables 3, 4, and 5, the numberof topics is equal to the number of seeded topics.For SeededLDA models corresponding to the seedwords in Tables 6 and 3, we use 10 topics, allow-ing for some unseeded topics that are not capturedby the seed words.4.2 Hinge-loss Markov Random FieldsThe approach described in the previous section au-tomatically identifies user-seeded aspects and sen-timent, but it does not make further use of struc-77LECTURE: lectur, video, download, volum, low, headphon, sound, audio, transcript, subtitl, slide, noteQUIZ: quiz, assignment, question, midterm,exam, submiss, answer, grade, score, grad, midterm, due, deadlinCERTIFICATE: certif, score, signatur, statement, final, course, pass, receiv, coursera, accomplish, failSOCIAL: name, course, introduction, stud, group, everyon, studentTable 3: Seed words for coarse aspectsLECTURE-VIDEO: video, problem, download, play, player, watch, speed, length, long, fast, slow, render, qualitiLECTURE-AUDIO: volum, low, headphon, sound, audio, hear, maximum, troubl, qualiti, high, loud, heardLECTURE-LECTURER: professor, fast, speak, pace, follow, speed, slow, accent, absorb, quick, slowliLECTURE-SUBTITLES: transcript, subtitl, slide, note, lectur, difficult, pdfLECTURE-CONTENT: typo, error, mistak, wrong, right, incorrect, mistakenQUIZ-CONTENT: question, challeng, difficult, understand, typo, error, mistak, quiz, assignmentQUIZ-SUBMISSION: submiss, submit, quiz, error, unabl, resubmitQUIZ-GRADING: answer, question, answer, grade, assignment, quiz, respons ,mark, wrong, scoreQUIZ-DEADLINE: due, deadlin, miss, extend, lateTable 4: Seed words for fine aspectsPOSITIVE: interest, excit, thank, great, happi, glad, enjoy, forward, insight, opportun, clear, fantast, fascin, learn, hope, congratulNEGATIVE: problem, difficult, error, issu, unabl, misunderstand, terribl, bother, hate, bad, wrong, mistak, fear, troublNEUTRAL: coursera, class, hello, everyon, greet, nam, meet, group, studi, request, join, introduct, question, thankTable 5: Seed words for sentimentDIFFICULTY: difficult, understand, ambigu, disappoint, hard, follow, mislead, difficulti, challeng, clearCONTENT: typo, error, mistak, wrong, right, incorrect, mistaken, scoreAVAILABILITY: avail, nowher, find, access, miss, view, download, broken, link, bad, access, deni, miss, permissCOURSE-1: develop, eclips, sdk, softwar, hardware, accuser, html, platform, environ, lab, ide, java,COURSE-2: protein, food, gene, vitamin, evolut, sequenc, chromosom, genet, speci, peopl, popul, evolv, mutat, ancestriCOURSE-3: compani, product, industri, strategi, decision, disrupt, technolog, marketTable 6: Seed words for sentiment specific to online coursesture or dependencies between these values, or anyadditional side-information.
To address this, wepropose a more powerful approach using hinge-loss Markov random fields (HL-MRFs), a scalableclass of continuous, conditional graphical mod-els (Bach et al, 2013).
HL-MRFs have achievedstate-of-the-art performance in many domains in-cluding knowledge graph identification (Pujara etal., 2013), understanding engagements in MOOCs(Ramesh et al, 2014a), biomedicine and multi-relational link prediction (Fakhraei et al, 2014),and modelling social trust (Huang et al, 2013).These models can be specified using ProbabilisticSoft Logic (PSL) (Bach et al, 2015), a weightedfirst order logical templating language.
An exam-ple of a PSL rule is?
: P (a) ?Q(a, b)?
R(b),where P, Q, and R are predicates, a and b are vari-ables, and ?
is the weight associated with the rule.The weight of the rule indicates its importance inthe HL-MRF probabilistic model, which defines aprobability density function of the formP (Y|X) ?
exp(?M?r=1?r?r(Y,X))?r(Y,X) = (max{lr(Y,X), 0})?r, (1)where ?r(Y,X) is a hinge-loss potential corre-sponding to an instantiation of a rule, and is spec-ified by a linear function lrand optional exponent?r?
{1, 2}.
For example, in our MOOC aspect-sentiment model, if P and F denote post P andfine aspect F, then we have predicates SEEDLDA-FINE(P, F) to denote the value corresponding totopic F in SeededLDA, and FINE-ASPECT(P, F) isthe target variable denoting the fine aspect of thepost P. A PSL rule to encode that the SeededLDAtopic F suggests that aspect F is present is?
: SEEDLDA-FINE(P, F )?
FINE-ASPECT(P, F ).We can generate more complex rules connectingthe different features and target variables, e.g.?
: SEEDLDA-FINE(P, F ) ?
SENTIMENT(P, S)?
FINE-ASPECT(P, F ).This rule encodes a dependency between SENTI-MENT and FINE-ASPECT, namely that the Seed-78edLDA topic and a strong sentiment score increasethe probability of the fine aspect.
The HL-MRFmodel uses these rules to encode domain knowl-edge about dependencies among the predicates.The continuous value representation further helpsin understanding the confidence of predictions.4.3 Joint Aspect-Sentiment Prediction usingProbabilistic Soft Logic (PSL-Joint)In this section, we describe our joint approach topredicting aspect and sentiment in online discus-sion forums, leveraging the strong dependence be-tween aspect and sentiment.
We present a systemdesigned using HL-MRFs which combines differ-ent features, accounting for their respective uncer-tainty, and encodes the dependencies between as-pect and sentiment in the MOOC context.Table 7 provides some representative rules fromour model.2The rules can be classified into twobroad categories?1) rules that combine multiplefeatures, and 2) rules that encode the dependenciesbetween aspect and sentiment.4.3.1 Combining FeaturesThe first set of rules in Table 7 combine differentfeatures extracted from the post.
SEEDLDA-FINE,SEEDLDA-COARSE and SEEDLDA-SENTIMENT-COURSE predicates in rules refer to SeededLDAposterior distributions using coarse, fine, andcourse-specific sentiment seed words respectively.The strength of our model comes from its abil-ity to encode different combinations of featuresand weight them according to their importance.The first rule in Table 7 combines the SeededLDAfeatures from both SEEDLDA-FINE and SEEDLDA-COARSE to predict the fine aspect.
Interpretingthe rule, the fine aspect of the post is more likelyto be LECTURE-LECTURER if the coarse Seed-edLDA score for the post is LECTURE, and thefine SeededLDA score for the post is LECTURE-LECTURER.
Similarly, the second rule providescombinations of some of the other features usedby the model?two different SeededLDA scoresfor sentiment, as indicated by seed words in Ta-bles 5 and 6.
The third rule states that certain fineaspects occur together with certain values of sen-timent more than others.
In online courses, poststhat discuss grading usually talk about grievancesand issues.
The rule captures that QUIZ-GRADINGoccurs with negative sentiment in most cases.2Full model available at https://github.com/artir/ramesh-acl154.3.2 Encoding Dependencies BetweenAspect and SentimentIn addition to combining features, we also en-code rules to capture the taxonomic dependencebetween coarse and fine aspects, and the depen-dence between aspect and sentiment (Table 7, bot-tom).
Rules 4 and 5 encode pair-wise depen-dency between FINE-ASPECT and SENTIMENT,and COARSE-ASPECT and FINE-ASPECT respec-tively.
Rule 4 uses the SeededLDA value forQUIZ-DEADLINES to predict both SENTIMENT,and FINE-ASPECT jointly.
This together withother rules for predicting SENTIMENT and FINE-ASPECT individually creates a constrained satis-faction problem, forcing aspect and sentiment toagree with each other.
Rule 5 is similar to rule 4,capturing the taxonomic relationship between tar-get variables COARSE-ASPECT and FINE-ASPECT.Thus, by using conjunctions to combine fea-tures and appropriately weighting these rules, weaccount for the uncertainties in the underlying fea-tures and make them more robust.
The combina-tion of these two different types of weighted rules,referred to below as PSL-Joint, is able to reasoncollectively about aspect and sentiment.5 Empirical EvaluationIn this section, we present the quantitative andqualitative results of our models on the annotatedMOOC dataset.
Our models do not require labeleddata for training; we use the label annotations onlyfor evaluation.
Tables 8 ?
11 show the resultsfor the SeededLDA and PSL-Joint models.
Sta-tistically significant differences, evaluated using apaired t-test with a rejection threshold of 0.01, aretyped in bold.5.1 SeededLDA for Aspect-SentimentFor SeededLDA, we use the seed words forcoarse, fine, and sentiment given in Tables 3 ?
5.After training the model, we use the SeededLDAmultinomial posterior distribution to predict thetarget variables.
We use the maximum value inthe posterior for the distribution over topics foreach post to obtain predictions for coarse aspect,fine aspect, and sentiment.
We then calculate pre-cision, recall and F1 values comparing with ourground truth labels.79PSL-JOINT RULESRules combining featuresSEEDLDA-FINE(POST, LECTURE-LECTURER) ?
SEEDLDA-COARSE(POST, LECTURE) ?
FINE-ASPECT(POST, LECTURE-LECTURER)SEEDLDA-SENTIMENT-COURSE(POST, NEGATIVE) ?
SEEDLDA-SENTIMENT(POST, NEGATIVE) ?
SENTIMENT(POST, NEGATIVE)SEEDLDA-SENTIMENT-COURSE(POST, NEGATIVE)?
SEEDLDA-FINE(POST, QUIZ-GRADING) ?
FINE-ASPECT(POST, QUIZ-GRADING)Encoding dependencies between aspect and sentimentSEEDLDA-FINE(POST, QUIZ-DEADLINES) ?
SENTIMENT(POST, NEGATIVE) ?
FINE-ASPECT(POST, QUIZ-DEADLINES)SEEDLDA-FINE(POST, QUIZ-SUBMISSION) ?
FINE-ASPECT(POST, QUIZ-SUBMISSION) ?
COARSE-ASPECT(POST, QUIZ)Table 7: Representative rules from PSL-Joint modelModel LECTURE-CONTENT LECTURE-VIDEO LECTURE-AUDIO LECTURE-LECTURER LECTURE-SUBTITLESPrec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1SEEDEDLDA 0.137 0.057 0.08 0.156 0.256 0.240 0.684 0.684 0.684 0.037 0.159 0.06 0.289 0.631 0.397PSL-JOINT 0.407 0.413 0.410 0.411 0.591 0.485 0.635 0.537 0.582 0.218 0.623 0.323 0.407 0.53 0.461Table 8: Precision, recall and F1 scores for LECTURE fine aspectsModel QUIZ-CONTENT QUIZ-SUBMISSION QUIZ-DEADLINES QUIZ-GRADINGPrec Rec.
F1 Prec Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1SEEDEDLDA 0.042 0.006 0.011 0.485 0.398 0.437 0.444 0.141 0.214 0.524 0.508 0.514PSL-JOINT 0.324 0.405 0.36 0.521 0.347 0.416 0.667 0.563 0.611 0.572 0.531 0.550Table 9: Precision, recall and F1 scores for QUIZ fine aspectsModel LECTURE QUIZ CERTIFICATE SOCIALPrec Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1SEEDEDLDA 0.597 0.673 0.632 0.752 0.583 0.657 0.315 0.845 0.459 0.902 0.513 0.654PSL-JOINT 0.563 0.715 0.630 0.724 0.688 0.706 0.552 0.711 0.621 0.871 0.530 0.659Table 10: Precision, recall and F1 scores for coarse aspectsModel POSITIVE NEGATIVE NEUTRALPrec Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1SEEDEDLDA 0.104 0.721 0.182 0.650 0.429 0.517 0.483 0.282 0.356PSL-JOINT 0.114 0.544 0.189 0.571 0.666 0.615 0.664 0.322 0.434Table 11: Precision, recall and F1 scores for sentiment5.2 PSL for Joint Aspect-Sentiment(PSL-Joint)Tables 8 and 9 give the results for the fine aspectsunder LECTURE and QUIZ.
PSL-JOINT performsbetter than SEEDEDLDA in most cases, with-out suffering any statistically significant losses.Notable cases include the increase in scoresfor LECTURE-LECTURER, LECTURE-SUBTITLES,LECTURE-CONTENT, QUIZ-CONTENT, QUIZ-GRADING, and QUIZ-DEADLINES, for which thescores increase by a large margin over Seed-edLDA.
We observe that for LECTURE-CONTENTand QUIZ-CONTENT, the increase in scores ismore significant than others with SeededLDA per-forming very poorly.
Since both lecture and quizcontent have the same kind of words related to thecourse material, SeededLDA is not able to dis-tinguish between these two aspects.
We foundthat in 63% of these missed predictions, Seed-edLDA predicts LECTURE-CONTENT, instead ofQUIZ-CONTENT, and vice versa.
In contrast, PSL-Joint uses both coarse and fine SeededLDA scoresand captures the dependency between a coarse as-pect and its corresponding fine aspect.
There-fore, PSL-Joint is able to distinguish betweenLECTURE-CONTENT and QUIZ-CONTENT.
In thenext section, we present some examples of poststhat SEEDEDLDA misclassified but were predictedcorrectly by PSL-Joint.Table 10 presents results for the coarse-aspects.We observe that PSL-Joint performs better thanSeededLDA for all classes.
In particular for CER-TIFICATE and QUIZ, PSL-Joint exhibits a markedincrease in scores when compared to SeededLDA.This is also true for sentiment, for which the scoresfor NEUTRAL and NEGATIVE sentiment show sig-nificant improvement (Table 11).80Correct Label PSL SeededLDA PostQUIZ-CONTENT QUIZ-CONTENT LECTURE-CONTENT There is a typo or other mistake in the assignment instructions (e.g.
es-sential information omitted) Type ID: programming-content Problem ID:programming-mistake Browser: Chrome 32 OS: Windows 7QUIZ-CONTENT QUIZ-CONTENT LECTURE-CONTENT There is a typo or other mistake on the page (e.g.
factual error informa-tion omitted) Week 4 Quiz Question 6: Question 6 When a user clickson a View that has registered to show a Context Menu which one of thefollowing methods will be called?LECTURE-AUDIO LECTURE-AUDIO LECTURE-SUBTITLES Thanks for the suggestion about downloading the video and referring tothe subtitles.
I will give that a try but I would also like to point out thatwhat the others are saying is true for me too: The audio is just barelyaudible even when the volume on my computer is set to 100%.SOCIAL SOCIAL LECTURE-VIDEO Let?s start a group for discussing the lecture videos.Table 12: Example posts that PSL-Joint predicted correctly, but were misclassified by SeededLDACorrect Label Predicted Label Second PostPredictionLECTURE-CONTENT QUIZ-CONTENT LECTURE-CONTENT I have a difference of opinion to the answer for Question 6 too.
It differs fromwhat is presented in lecture 1.SOCIAL LECTURE-SUBTITLES SOCIAL Hello guys!!!
I am ...
The course materials are extraordinary.
The subtitles arereally helpful!
Thanks to instructors for giving us all a wonderful opportunity.LECTURE-CONTENT QUIZ-CONTENT LECTURE-CONTENT As the second lecture video told me I started windows telnet and connected tothe virtual device.
Then I typed the same command for sending an sms that thelecture video told me to.
The phone received a message all right and I was able toopen it but the message itself seems to be written with some strange characters.Table 13: Example posts whose second-best prediction is correct5.3 Interpreting PSL-Joint PredictionsTable 12 presents some examples of posts thatPSL-Joint predicted correctly, and which Seed-edLDA misclassified.
The first two examplesillustrate that PSL can predict the subtle dif-ference between LECTURE-CONTENT and QUIZ-CONTENT.
Particularly notable is the third ex-ample, which contains mention of both subtitlesand audio, but the negative sentiment is associ-ated with audio rather than subtitles.
PSL-Jointpredicts the fine aspect as LECTURE-AUDIO, eventhough the underlying SeededLDA feature has ahigh score for LECTURE-SUBTITLES.
This exam-ple illustrates the strength of the joint reasoningapproach in PSL-Joint.
Finally, in the last exam-ple, the post mentions starting a group to discussvideos.
This is an ambiguous post containing thekeyword video, while it is in reality a social postabout starting a group.
PSL-Joint is able to predictthis because it uses both the sentiment scores as-sociated with the post and the SeededLDA scoresfor fine aspect, and infers that social posts are gen-erally positive.
So, combining the feature valuesfor social aspect and positive sentiment, it is ableto predict the fine aspect as SOCIAL correctly.The continuous valued output predictions pro-duced by PSL-Joint allow us to rank the predictedvariables by output prediction value.
Analyzingthe predictions for posts that PSL-Joint misclassi-fied, we observe that for four out of nine fine as-pects, more than 70% of the time the correct labelis in the top three predictions.
And, for all fineaspects, the correct label is found in the top 3 pre-dictions around 40% of the time.
Thus, using thetop three predictions made by PSL-Joint, we canunderstand the fine aspect of the post to a greatextent.
Table 13 gives some examples of posts forwhich the second best prediction by PSL-Joint isthe correct label.
For these examples, we foundthat PSL-Joint misses the correct prediction by asmall margin(< 0.2).
Since our evaluation schemeonly considers the maximum value to determinethe scores, these examples were treated as misclas-sified.5.4 Understanding Instructor Interventionusing PSL-Joint PredictionsIn our 3275 annotated posts, the instructor repliedto 787 posts.
Of these, 699 posts contain a men-tion of some MOOC aspect.
PSL-Joint predicts97.8% from those as having an aspect and 46.9%as the correct aspect.
This indicates that PSL-Jointis capable of identifying the most important posts,i.e.
those that the instructor replied to, with highaccuracy.
PSL-Joint?s MOOC aspect predictionscan potentially be used by the instructor to selecta subset of posts to address in order to cover themain reported issues.
We found in our data thatsome fine aspects, such as CERTIFICATE, have ahigher percentage of instructor replies than oth-ers, such as QUIZ-GRADING.
Using our system,instructors can sample from multiple aspect cate-81gories, thereby making sure that all categories ofproblems receive attention.6 ConclusionIn this paper, we developed a weakly supervisedjoint probabilistic model (PSL-Joint) for predict-ing aspect-sentiment in online courses.
Our modelprovides the ability to conveniently encode do-main information in the form of seed words, andweighted logical rules capturing the dependen-cies between aspects and sentiment.
We validatedour approach on an annotated dataset of MOOCposts sampled from twelve courses.
We com-pared our PSL-Joint probabilistic model to a sim-pler SeededLDA approach, and demonstrated thatPSL-Joint produced statistically significantly bet-ter results, exhibiting a 3?5 times improvement inF1 score in most cases over a system using onlySeededLDA.
As further shown by our qualitativeresults and instructor reply information, our sys-tem can potentially be used for understanding stu-dent requirements and issues, identifying posts forinstructor intervention, increasing student reten-tion, and improving future iterations of the course.Acknowledgements This work was supportedby NSF grant IIS1218488, and IARPA viaDoI/NBC contract number D12PC00337.
TheU.S.
Government is authorized to reproduceand distribute reprints for governmental purposesnotwithstanding any copyright annotation thereon.Disclaimer: The views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the officialpolicies or endorsements, either expressed or im-plied, of IARPA, DoI/NBC, or the U.S. Govern-ment.ReferencesStephen H. Bach, Bert Huang, Ben London, and LiseGetoor.
2013.
Hinge-loss Markov random fields:Convex inference for structured prediction.
In Pro-ceedings of the Conference on Uncertainty in Artifi-cial Intelligence (UAI).S.
H. Bach, M. Broecheler, B. Huang, and L. Getoor.2015.
Hinge-loss Markov random fields and proba-bilistic soft logic.
arXiv:1505.04406 [cs.LG].David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research (JMLR).Samuel Brody and Noemie Elhadad.
2010.
An unsu-pervised aspect-sentiment model for online reviews.In Proceedings of Human Language Technologies:Conference of the North American Chapter of theAssociation for Computational Linguistics (HLT).Snigdha Chaturvedi, Dan Goldwasser, and HalDaum?e III.
2014.
Predicting instructor?s interven-tion in mooc forums.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics (ACL).Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexan-der J. Smola, Jing Jiang, and Chong Wang.
2014.Jointly modeling aspects, ratings and sentiments formovie recommendation (JMARS).
In Proceedingsof the SIGKDD International Conference on Knowl-edge Discovery and Data Mining (KDD).Angela Fahrni and Manfred Klenner.
2008.
Old wineor warm beer: Target-specific sentiment analysis ofadjectives.
In Proceedings of the Symposium on Af-fective Language in Human and Machine (AISB).Shobeir Fakhraei, Bert Huang, Louiqa Raschid, andLise Getoor.
2014.
Network-based drug-targetinteraction prediction with probabilistic soft logic.IEEE/ACM Transactions on Computational Biologyand Bioinformatics (TCBB).Bert Huang, Angelika Kimmig, Lise Getoor, and Jen-nifer Golbeck.
2013.
A flexible framework forprobabilistic models of social trust.
In Proceed-ings of the International Conference on Social Com-puting, Behavioral-Cultural Modeling, & Prediction(SBP).Jagadeesh Jagarlamudi, Hal Daum?e, III, andRaghavendra Udupa.
2012.
Incorporating lex-ical priors into topic models.
In Proceedingsof the European Chapter of the Association forComputational Linguistics (EACL).Y.
Jo and A.H. Oh.
2011.
Aspect and sentiment unifi-cation model for online review analysis.
In Proceed-ings of the International Conference on Web Searchand Data Mining (WSDM).Suin Kim, Jianwen Zhang, Zheng Chen, Alice Oh, andShixia Liu.
2013.
A hierarchical aspect-sentimentmodel for online reviews.
In Proceedings of theAAAI Conference on Artificial Intelligence (AAAI).Varada Kolhatkar, Heike Zinsmeister, and GraemeHirst.
2013.
Annotating anaphoric shell nouns withtheir antecedents.
In Linguistic Annotation Work-shop and Interoperability with Discourse.Chenghua Lin and Yulan He.
2009.
Joint senti-ment/topic model for sentiment analysis.
In Pro-ceedings of the Conference on Information andKnowledge Management (CIKM).Chenghua Lin, Yulan He, R. Everson, and S. Ruger.2012.
Weakly supervised joint sentiment-topic de-tection from text.
IEEE Transactions on Knowledgeand Data Engineering.82Bing Liu and Lei Zhang.
2012.
A survey of opinionmining and sentiment analysis.
In Mining Text Data.Edward Loper and Steven Bird.
2002.
NLTK: Thenatural language toolkit.
In Proceedings of the ACLWorkshop on Effective Tools and Methodologies forTeaching Natural Language Processing and Compu-tational Linguistics (ETMTNLP).Bin Lu, Myle Ott, Claire Cardie, and Benjamin K.Tsou.
2011.
Multi-aspect sentiment analysis withtopic models.
In Proceedings of the InternationalConference on Data Mining Workshops (ICDMW).Arjun Mukherjee and Bing Liu.
2012.
Aspect extrac-tion through semi-supervised modeling.
In Proceed-ings of the Annual Meeting of the Association forComputational Linguistics (ACL).Jay Pujara, Hui Miao, Lise Getoor, and William Cohen.2013.
Knowledge graph identification.
In Interna-tional Semantic Web Conference (ISWC).Arti Ramesh, Dan Goldwasser, Bert Huang, HalDaume III, and Lise Getoor.
2014a.
Learning latentengagement patterns of students in online courses.In Proceedings of the AAAI Conference on ArtificialIntelligence (AAAI).Arti Ramesh, Dan Goldwasser, Bert Huang, HalDaum?e III, and Lise Getoor.
2014b.
Understand-ing MOOC discussion forums using seeded lda.
InACL Workshop on Innovative Use of NLP for Build-ing Educational Applications (BEA).Glenda S. Stump, Jennifer DeBoer, Jonathan Whit-tinghill, and Lori Breslow.
2013.
Development ofa framework to classify MOOC discussion forumposts: Methodology and challenges.
In NIPS Work-shop on Data Driven Education.Ivan Titov and Ryan McDonald.
2008.
Modeling on-line reviews with multi-grain topic models.
In Pro-ceedings of the International Conference on WorldWide Web (WWW).Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-ing Li.
2010.
Jointly modeling aspects and opinionswith a maxEnt-LDA hybrid.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP).83
