Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1348?1357,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPPredicting Subjectivity in Multimodal ConversationsGabriel Murray and Giuseppe CareniniUniversity of British ColumbiaVancouver, Canada(gabrielm, carenini)@cs.ubc.caAbstractIn this research we aim to detect sub-jective sentences in multimodal conversa-tions.
We introduce a novel techniquewherein subjective patterns are learnedfrom both labeled and unlabeled data, us-ing n-gram word sequences with vary-ing levels of lexical instantiation.
Ap-plying this technique to meeting speechand email conversations, we gain signifi-cant improvement over state-of-the-art ap-proaches.
Furthermore, we show that cou-pling the pattern-based approach with fea-tures that capture characteristics of gen-eral conversation structure yields addi-tional improvement.1 IntroductionConversations are rich in subjectivity.
Conversa-tion participants agree and disagree with one other,argue for and against various proposals, and gen-erally take turns expressing their private states.Being able to separate these subjective utterancesfrom more objective utterances would greatly fa-cilitate the analysis, mining and summarization ofa large number of conversations.Two of the most prevalent conversational me-dia are meetings and emails.
Face-to-face meet-ings enable numerous people to exchange a largeamount of information and opinions in a short pe-riod of time, while emails allow for concise ex-changes between potentially far-flung participants.Meetings and emails can also feed into one an-other, with face-to-face meetings occurring at reg-ular intervals and emails continuing the conver-sations in the interim.
This poses several inter-esting questions, such as whether subjective utter-ances are more or less likely to be found in emailexchanges compared with meetings, and whetherthe ratios of positive and negative subjective utter-ances differ between the two modalities.In this paper we describe a novel approach forpredicting subjectivity, and test it in two sets ofexperiments on meetings and emails.
Our ap-proach combines a new general purpose methodfor learning subjective patterns, with features thatcapture basic characteristics of conversation struc-ture across modalities.
The subjective patterns areessentially n-gram sequences with varying levelsof lexical instantiation, and we demonstrate howthey can be learned from both labeled and un-labeled data.
The conversation features capturestructural characteristics of multimodal conversa-tions as well as participant information.We test our approach in two sets of experi-ments.
The goal of the first set of experiments is todiscriminate subjective from non-subjective utter-ances, comparing the novel approach to existingstate-of-the-art techniques.
In the second set ofexperiments, the goal is to discriminate positive-subjective and negative-subjective utterances, es-tablishing their polarity.
In both sets of experi-ments, we assess the impact of features relatingto conversation structure.2 Related ResearchRaaijmakers et al (2008) have approachedthe problem of detecting subjectivity in meetingspeech by using a variety of multimodal featuressuch as prosodic features, word n-grams, charac-ter n-grams and phoneme n-grams.
For subjec-tivity detection, they found that a combination ofall features was best, while prosodic features wereless useful for discriminating between positive andnegative utterances.
They found character n-gramsto be particularly useful.Riloff and Wiebe (2004) presented a method forlearning subjective extraction patterns from a largeamount of data, which takes subjective and non-subjective text as input, and outputs significantlexico-syntactic patterns.
These patterns are basedon syntactic structure output by the Sundance shal-1348low dependency parser (Riloff and Phillips, 2004).They are extracted by exhaustively applying syn-tactic templates such as < subj > passive-verband active-verb < dobj > to a training cor-pus, with an extracted pattern for every instan-tiation of the syntactic template.
These patternsare scored according to probability of relevancegiven the pattern and frequency of the pattern.
Be-cause these patterns are based on syntactic struc-ture, they can represent subjective expressions thatare not fixed word sequences and would thereforebe missed by a simple n-gram approach.Riloff et al (2006) explore feature subsumptionfor opinion detection, where a given feature maysubsume another feature representationally if thestrings matched by the first feature include all ofthe strings matched by the second feature.
To givetheir own example, the unigram happy subsumesthe bigram very happy.
The first feature will be-haviorally subsume the second if it representa-tionally subsumes the second and has roughly thesame information gain, within an acceptable mar-gin.
They show that they can improve opinionanalysis results by modeling these relations andreducing the feature set.Our approach for learning subjective patternslike Raaijmakers et al relies on n-grams, but likeRiloff et al moves beyond fixed sequences ofwords by varying levels of lexical instantiation.Yu and Hatzivassiloglou (2003) addressed threechallenges in the news article domain: discrimi-nating between objective documents and subjec-tive documents such as editorials, detecting sub-jectivity at the sentence level, and determining po-larity at the sentence level.
They found that thelatter two tasks were substantially more difficultthan classification at the document level.
Of par-ticular relevance here is that they found that part-of-speech (POS) features were especially usefulfor assigning polarity scores, with adjectives, ad-verbs and verbs comprising the best set of POStags.
This work inspired us to look at generaliza-tion of n-grams based on POS.On the slightly different task of classifying theintensity of opinions, Wilson et al (2006) em-ployed several types of features including depen-dency structures in which words can be backed offto POS tags.
They found that this feature class im-proved the overall accuracy of their system.Somasundaran et al (2007) investigated subjec-tivity classification in meetings.
Their findings in-dicate that both lexical features (list of words andexpressions) and discourse features (dialogue actsand adjacency pairs) can be beneficial.
In the samespirit, we effectively combine lexical patterns andconversational features.The approach to predicting subjectivity wepresent in this paper is a novel contribution to thefield of opinion and sentiment analysis.
Pang andLee (2008) give an overview of the state of the art,discussing motivation, features, approaches andavailable resources.3 Subjectivity DetectionIn this section we describe our approach to sub-jectivity detection.
We begin by describing howto learn subjective n-gram patterns with varyinglevels of lexical instantiation.
We then describe aset of features characterizing multimodal conver-sation structure which can be used to supplementthe n-gram approach.
Finally, we describe thebaseline subjectivity detection approaches usedfor comparison.3.1 Partially Instantiated N-GramsOur approach to subjectivity detection and polar-ity detection is to learn significant patterns thatcorrelate with the subjective and polar utterances.These patterns are word trigrams, but with varyinglevels of lexical instantiation, so that each unit ofthe n-gram can be either a word or the word?s part-of-speech (POS) tag.
This contrasts, then, withwork such as that of Raaijmakers et al (2008)who include trigram features in their experiments,but where their learned trigrams are fully instanti-ated.
As an example, while they may learn that atrigram really great idea is positive, we may addi-tionally find that really great NN and RB great NNare informative patterns, and these patterns maysometimes be better cues than the fully instanti-ated trigrams.
To differentiate this approach fromthe typical use of trigrams, we will refer to it as theVIN (varying instantiation n-grams) method.In some respects, our approach to subjectiv-ity detection is similar to Riloff and Wiebe?swork cited above, in the sense that their extrac-tion patterns are partly instantiated.
However,the AutoSlog-TS approach relies on deriving syn-tactic structure with the Sundance shallow parser(Riloff and Phillips, 2004).
We hypothesize thatour trigram approach may be more robust to dis-fluent and fragmented meeting speech and emails13491 2 3really great ideareally great NNreally JJ ideaRB great ideareally JJ NNRB great NNRB JJ ideaRB JJ NNTable 1: Sample Instantiation Seton which syntactic parsers may perform poorly.Also, our learned trigram patterns range from fullyinstantiated to completely uninstantiated.
For ex-ample, we might find that the pattern RB JJ NNis a very good indicator of subjective utterancesbecause it matches a variety of scenarios wherepeople are ascribing qualities to things, e.g.
re-ally bad movie, horribly overcooked steak.
Noticethat we do not see our approach and AutoSlog-TSas mutually exclusive, and indeed we demonstratethrough these experiments that they can be effec-tively combined.Our approach begins by running the Brill POStagger (Brill, 1992) over all sentences in a doc-ument.
We then extract all of the word trigramsfrom the document, and represent each trigram us-ing every possible instantiation.
Because we areworking at the trigram level, and each unit of thetrigram can be a word or its POS tag there are23= 8 representations in each trigram?s instantia-tion set.
To continue the example from above, theinstantiation set for the trigram really great idea isgiven in Table 1.
As we scan down the instanti-ation set, we can see that the level of abstractionincreases until it is completely uninstantiated.
It isthis multilevel abstraction that we are hypothesiz-ing will be useful for learning new subjective andpolar cues.All trigrams are then scored according to theirprevalence in relevant versus irrelevant documents(e.g.
subjective vs. non-subjective sentences),following the scoring methodology of Riloff andWiebe (2003).
We calculate the conditional prob-ability p(relevance|trigram) using the actual tri-gram counts in relevant and irrelevant text.
Forlearning negative-subjective patterns, we treat allnegative sentences as the relevant text and the re-mainder of the sentences as irrelevant text, andconduct the same process for learning positive-subjective patterns.
We consider significant pat-terns to be those where the conditional proba-bility is greater than 0.65 and the pattern occursmore than five times in the entire document set(slightly higher than probability >= 0.60 andfrequency >= 2 used by Riloff and Wiebe(2003)).We possess a fairly small amount of conversa-tional data annotated for subjectivity and polarity.The AMI meeting corpus and BC3 email corpusare described in more detail in Section 4.1.
To ad-dress this shortfall in annotated data, we take twoapproaches to learning patterns.
In the first, welearn a set of patterns from the annotated conversa-tion data.
In the second approach, we complementthose patterns by learning additional patterns fromunannotated data that are typically overwhelm-ingly subjective or objective in nature.
We de-scribe these two approaches here in turn.3.1.1 Supervised Learning of Patterns fromConversation DataThe first learning strategy is to apply the above-described methods to the annotated conversationdata, learning the positive patterns by compar-ing positive-subjective utterances to all other ut-terances, and learning the negative patterns bycomparing the negative-subjective utterances toall other utterances, using the described methods.This results in 759 significant positive patterns and67 significant negative patterns.
This difference inpattern numbers can be explained by negative ut-terances being less common in the AMI meetings,as noted by Wilson (2008).
It may be that peopleare less comfortable in expressing negative sen-timents in face-to-face conversations, particularlywhen the meeting participants do not know eachother well (in the AMI scenario meetings, manyparticipants were meeting each other for the firsttime).
But there may be a further explanation forwhy we learn many more positive than negativepatterns.
When conversation participants do ex-press negative sentiments, they may couch thosesentiments in more euphemistic or guarded termscompared with positive sentiments.
Table 2 givesexamples of significant positive and negative pat-terns learned from the labeled meeting data.
Thelast two rows in Table 2 show how two patternsin the same instantiation set can have substantiallydifferent probabilities.1350POS p(r|t) NEG p(r|t)you MD change 1.0 VBD not RB 1.0should VBP DT 1.0 doesn?t RB VB 0.875very easy to 0.88 a bit JJ 0.66we could VBP 0.78 think PRP might 0.66NNS should VBP 0.71 be DT problem 0.71PRP could do 0.66 doesn?t really VB 0.833it could VBP 83 doesn?t RB VB 0.875Table 2: Example Pos.
and Neg.
Patterns (AMI)3.1.2 Unsupervised Learning of Patternsfrom Blog DataThe second pattern learning strategy we take tolearning subjective patterns is to use a relevant,but unannotated corpus.
We focus on weblog(blog) data for several reasons.
First, blog postsshare many characteristics with both meetings andemails: they are conversational, informal and thelanguage can be very ungrammatical.
Second,blog posts are known for being subjective; blog-gers post on issues that are passionate to them, of-fering arguments, opinions and invective.
Third,there is a huge amount of available blog data.
Butbecause we do not possess blog data annotatedfor subjectivity, we take the following approachto learning subjective patterns from this data.
Wework on the assumption that a great many blogposts are inherently subjective, and that compar-ing this data to inherently objective text such asnewswire articles, treating the latter as our irrele-vant text, should lead to the detection of many newsubjective patterns and greatly increase our cover-age.
While the patterns learned will be noisy, wehypothesize that the increased coverage will im-prove our subjectivity detection overall.For our blog data, we use the BLOG06 Corpus1that was featured as training and testing data forthe Text Analysis Conference (TAC) 2008 trackon summarizing blog opinions.
The portion usedtotals approximately 4,000 documents on all man-ner of topics.
Treating that dataset as our rele-vant, subjective data, we then learn the subjec-tive trigrams by comparing with the irrelevantTAC/DUC newswire data from the 2007 and 2008update summarization tasks.
To try to reduce theamount of noise in our learned patterns, we set theconditional probability threshold at 0.75 (vs. 0.65for annotated data), and stipulate that all signif-icant patterns must occur at least once in the ir-relevant text.
This last rule is meant to prevent1http://ir.dcs.gla.ac.uk/test collections/blog06info.htmlPattern p(r|t)can not VB 0.99i can RB 0.99i have not 0.98do RB think 0.97RB think that 0.95RB agree with 0.95IN PRP opinion 0.95Table 3: Example Subjective Patterns (BLOG06)us from learning completely blog-specific patternssuch as posted by NN or linked to DT.
In the end,more than 20,000 patterns were learned from theblog data.
While manual inspection does showthat many undesirable patterns were extracted,among the highest-scoring patterns are many sen-sible subjective trigrams such as those indicated inTable 3.This approach is similar in spirit to the work ofBiadsy et al (2008) on unsupervised biographyproduction.
Without access to labeled biographi-cal data, the authors chose to use sentences fromWikipedia biographies as their positive set andsentences from newswire articles as their negativeset, on the assumption that most of the Wikipediasentences would be relevant to biographies andmost of the newswire sentences would not.3.2 Deriving VIN FeaturesFor our machine learning experiments, we derive,for each sentence, features indicating the presenceof the significant VIN patterns.
Patterns are binnedaccording to their conditional probability range(i.e., 0.65 <= p < 0.75, 0.75 <= p < 0.85,0.85 <= p < 0.95, and 0.95 <= p).
There arethree bins for the blog patterns, since the proba-bility cutoff is 0.75 For each bin, there is a featureindicating the count of its patterns in the given sen-tence.
When attempting to match these trigrampatterns to sentences, we allow up to two wild-card lexical items between the trigram units.
Inthis way a sentence can match a learned patterneven if the units of the n-gram are not contiguous(Raaijmakers et al (2008) similarly include an n-gram feature allowing such intervening material).A key reason for counting the number ofmatched patterns for each probability range as justdescribed, rather than including a feature for eachindividual pattern, is to maintain the same levelof dimensionality in our machine learning exper-iments when comparing the VIN approach to thebaseline approaches described in Section 3.4.13513.3 Conversational FeaturesWhile we hypothesize that the general pur-pose pattern-based approach described above willgreatly aid subjectivity and polarity detection, wealso recognize that there are many additional fea-tures specific for characterizing multimodal con-versations that may correlate well with subjectiv-ity and polarity.
Such features include structuralcharacteristics like the position of a sentence in aturn and the position of a turn in the conversation,and participant features relating to dominance orleadership.
For example, it may be that subjectivesentences are more likely to come at the end of aconversation, or that a person who dominates theconversation may utter more negative sentences.We use the feature set provided by Murray andCarenini (2008), which they used for automaticsummarization of conversations and which areshown in Table 4.
Many of the features are basedon so-called Sprob and Tprob term-weights, theformer of which weights words based on their dis-tributions across conversation participants and thelatter of which similarly weights words based ontheir distributions across conversation turns.
Otherfeatures include word entropy of the candidatesentence, lexical cohesion of the sentence with thegreater conversation, and structural features indi-cating position of the candidate sentence in theturn and in the conversation, such as the elapsedtime since the beginning of the conversation.3.4 Baseline ApproachesThere are two baselines in particular to whichwe are interested in comparing the VIN ap-proach.
As stated earlier, we are hypothesiz-ing that the increasing levels of abstraction foundwith partially instantiated trigrams will lead to im-proved classification compared with using onlyfully instantiated trigrams.
To test this, wealso run the subjective/non-subjective and posi-tive/negative experiments using only fully instan-tiated trigrams.
There are 71 such positive tri-grams and 5 such negative trigrams learned fromthe AMI data.
There are just over 1200 fully in-stantiated trigrams learned from the unannotatedBLOG06 data.Believing that the current approach may offerbenefits over state-of-the-art pattern-based subjec-tivity detection, we also implement the AutoSlog-TS method of Riloff and Wiebe (2003) for extract-ing subjective extraction patterns.
In AutoSlog-Feature ID DescriptionMXS max Sprob scoreMNS mean Sprob scoreSMS sum of Sprob scoresMXT max Tprob scoreMNT mean Tprob scoreSMT sum of Tprob scoresTLOC position in turnCLOC position in conv.SLEN word count, globally normalizedSLEN2 word count, locally normalizedTPOS1 time from beg.
of conv.
to turnTPOS2 time from turn to end of conv.DOM participant dominance in wordsCOS1 cosine of conv.
splits, w/ SprobCOS2 cosine of conv.
splits, w/ TprobPENT entropy of conv.
up to sentenceSENT entropy of conv.
after the sentenceTHISENT entropy of current sentencePPAU time btwn.
current and prior turnSPAU time btwn.
current and next turnBEGAUTH is first participant (0/1)CWS rough ClueWordScore (cohesion)CENT1 cos. of sentence & conv., w/ SprobCENT2 cos. of sentence & conv., w/ TprobTable 4: Features KeyTS, once all of the patterns are extracted usingthe Sundance parser, the scoring methodology ismuch the same as desribed in Section 3.1.
Con-ditional probabilities are calculated by comparingpattern occurrences in the relevant text with oc-currences in all text, and we again use a thresh-old of p >= 0.65 and frequency >= 5 for sig-nificant patterns.
For the BLOG06 data, we usea probability cutoff of 0.75 as before.
For deriv-ing the features used in our machine learning ex-periments, the patterns are similarly grouped ac-cording to conditional probability.
From the anno-tated data, 48 patterns are learned in total, 46 pos-itive and only 2 negative.
From the BLOG06 data,more than 3000 significant patterns are learned.Among significant patterns learned from the AMIcorpus are < subj > BE good, change < dobj >,< subj > agree and problem with < NP >.To gauge the effectiveness of the various featuretypes, for both sets of experiments we build multi-ple models on a variety of feature combinations:fully instantiated trigrams (TRIG), varying in-stantiation n-grams (VIN), AutoSlog-TS (SLOG),conversational structure features (CONV), and theset of all features.4 Experimental SetupIn this section we describe the corpora used, therelevant subjectivity annotation, and the statistical1352classifiers employed.4.1 CorporaWe use two annotated corpora for these experi-ments.
The AMI corpus (Carletta et al, 2005) con-sists of meetings in which participants take part inrole-playing exercises concerning the design anddevelopment of a remote control.
Participants aregrouped in fours, and each group takes part in asequence of four meetings, bringing the remotecontrol from design to market.
The four membersof the group are assigned roles of project man-ager, industrial designer, user interface designer,and marketing expert.
In total there are 140 suchscenario meetings, with individual meetings rang-ing from approximately 15 to 45 minutes.The BC3 corpus (Ulrich et al, 2008) containsemail threads from the World Wide Web Consor-tium (W3C) mailing list.
The threads feature a va-riety of topics such as web accessibility and plan-ning face-to-face meetings.
The annotated portionof the mailing list consists of 40 threads.4.2 Subjectivity AnnotationWilson (2008) has annotated 20 AMI meetings fora variety of subjective phenomena which fall intothe broad classes of subjective utterances, objec-tive polar utterances and subjective questions.
Itis this first class in which we are primarily in-terested here.
Two subclasses of subjective utter-ances are positive subjective and negative subjec-tive utterances.
Such subjective utterances involvethe expression of a private state, such as a posi-tive/negative opinion, positive/negative argument,and agreement/disagreement.
The 20 meetingswere labeled by a single annotator, though Wilson(2008) did conduct a study of annotator agreementon two meetings, reporting a ?
of 0.56 for detect-ing subjective utterances.
Of the roughly 20,000dialogue acts total in the 20 AMI meetings, nearly4000 are labeled as positive-subjective and nearly1300 as negative-subjective.
For the first exper-imental task, we consider the subjective class tobe the union of positive-subjective and negative-subjective dialogue acts.
For the second experi-mental task, the goal is to discriminate positive-subjective from negative-subjective.For the BC3 emails, annotators were initiallyasked to create extractive and abstractive sum-maries of each thread, in addition to labeling avariety of sentence-level phenomena, includingwhether each sentence was subjective.
In a secondround of annotations, three different annotatorswere asked to go through all of the sentences pre-viously labeled as subjective and indicate whethereach sentence was positive, negative, positive-negative, or other.
The definitions for positive andnegative subjectivity mirrored those given by Wil-son (2008).
For the purpose of these experiments,we consider a sentence to be subjective if at leasttwo of the annotators labeled it as subjective, andsimilarly consider a subjective sentence to be pos-itive or negative if at least two annotators label itas such.
Using this majority vote labeling, 172of 1800 sentences are considered subjective, with44% of those labeled as positive-subjective and37% as negative-subjective, showing that there ismuch more of a balance between positive and neg-ative sentiment in these email threads comparedwith meeting speech (note that some subjectivesentences are not positive or negative).
The ?
forlabeling subjective sentences in the email corpusis 0.32.
The lower annotator agreement on emailscompared with meetings suggests that subjectiv-ity in email text may be manifested more subtly orconveyed somewhat amibiguously.4.3 Classifier and Experimental SetupFor these experiments we use a maximum entropyclassifier using the liblinear toolkit2 (Fan et al,2008).
Feature subset selection is carried out bycalculating the F-statistic for each feature, rankingthe features according to the statistic, and train-ing on increasingly smaller subsets of feature ina cross-validation procedure, ultimately choosingthe feature set with the highest balanced accuracyduring cross-validation.Because the annotated portions of our corporaare fairly small (20 meetings, 40 email threads),we employ a leave-one-out method for trainingand testing rather than using dedicated trainingand test sets.
For the polarity labeling task ap-plied to the BC3 corpus, we pool all of the sen-tences and perform 10-fold cross-validation at thesentence level.4.4 Evaluation MetricsWe employ two sets of metrics for evaluating allclassifiers: precision/recall/f-measure and the re-ceiver operator characteristic (ROC) curve.
TheROC curve plots the true-positive/false-positiveratio while the posterior threshold is varied, and2http://www.csie.ntu.edu.tw/ cjlin/liblinear/1353we report the area under the curve (AUROC) as themeasure of interest.
Random performance wouldfeature an AUROC of approximately 0.5, whileperfect classification would yield an AUROC of1.
The advantage of the AUROC score comparedwith precision/recall/f-measure is that it evaluatesa given classifier across all thresholds, indicatingthe classifier?s overall discriminating power.
Thismetric is also known to be appropriate when classdistributions are skewed (Fawcett, 2003), as is ourcase.
For completeness we report both AUROCand p/r/f, but our discussions focus primarily onthe AUROC comparisons.5 ResultsIn this section we describe the experimental re-sults, first for the subjective/non-subjective clas-sification task, and subsequently for the positive-negative classification task.5.1 Subjective / Non-Subjective ClassificationFor the subjectivity detection task, the results onthe AMI and BC3 data closely mirrored eachother, with the VIN approach constituting a veryeffective feature set, outperforming both baselines.We report the results on meeting and emails inturn.5.1.1 AMI corpusFor the subjectivity task with the AMI corpus, wefirst report the precision, recall and f-measure re-sults in Table 5 where the various classifiers arecompared with a lower bound (LB) in which thepositive class is always predicted, leading to per-fect recall.
It can be seen that the novel systemsexhibit substantial improvement in precision andf-measure over this lower-bound.
While the VINapproach yields the best precision scores, the fullfeature set achieves the highest f-measure.As shown in Figure 1, the average AUROC withthe VIN approach is 0.69, compared with 0.61 forAutoSlog-TS, a significant difference according topaired t-test (p<0.01).
The VIN approach is alsosignificantly better than the standard fully instan-tiated trigram pattern approach (p<0.01).
Thislatter result suggests that the increased level ofabstraction found in the varying instantiation n-grams does improve performance.The conversational features alone give compa-rable performance to the VIN method (no signifi-cant difference), and the best results are found us-ing the full feature set, which gives an average AU-Sys Precision Recall F-MeasureAMI CorpusLB 26 100 41Trig 25 63 36Slog 39 48 43VIN 41 58 48Conv 36 73 49All Feas 38 70 49BC3 CorpusLB 10 100 17Trig 27 10 14Slog 24 13 17VIN 27 22 24Conv 25 29 27All Feas 33 34 33Table 5: P/R/F Results, Subjectivity Task0.50.60.70.80.91Trig - AMITrig - BC3Slog - AMISlog - BC3VIN - AMIVIN - BC3Conv - AMIConv - BC3All Feas - AMIAll Feas - BC3AUROCFigure 1: AUROCs on Subjectivity Task for AMIand BC3 corporaROC of 0.71, a significant improvement over VINonly (p<0.05).5.1.2 BC3 corpusFor the subjectivity task with the BC3 corpus, thebest precision and f-measure scores are found bycombining all features, as displayed in Table 5.The f-measure for the VIN approach is ten pointshigher than for the standard trigram approach.The average AUROC with the VIN approach is0.77, compared with 0.70 for AutoSlog-TS (sig-nificant at p<0.05).
The varying instantiation ap-proach is significantly better than the standard tri-gram pattern approach (p<0.01), where the aver-age AUROC is 0.66.
We again find that conver-sational features are very useful for this task, andthat the best overall results utilize the entire fea-ture set.
These results are displayed in Figure 1.5.1.3 Impact of Blog DataAn interesting question is whether our use of theBLOG06 data was worthwhile.
We can measurethis by comparing the VIN AUROC results re-1354Sys Precision Recall F-MeasureAMI CorpusLB 76 100 86Trig 87 8 14Slog 75 46 57VIN 83 60 70Conv 82 47 60All Feas 83 56 67BC3 CorpusLB 54 100 70Trig 50 84 63Slog 58 56 57VIN 53 84 65Conv 63 80 71All Feas 60 76 67Table 6: P/R/F Results, Polarity Taskported above with the VIN AUROC scores usingonly the annotated data for learning the significantpatterns.
The finding is that the blog data wasvery helpful, as the VIN approach averages only0.66 on the BC3 data and 0.63 on the AMI datawhen the blog patterns are not used, both signif-icantly lower (p<0.01).
Figure 2 shows the ROCcurves for the VIN approach with and without blogpatterns applied to the AMI subjectivity detectiontask, illustrating the impact of the unsupervisedpattern-learning strategy.00.20.40.60.810  0.2  0.4  0.6  0.8  1TPFPVIN with Blog PatternsVIN without Blog Patternschance levelFigure 2: Effect of Blog Patterns on AMI Subjec-tivity Task5.2 Positive / Negative ClassificationFor the polarity classification task, the results dif-fer between the two corpora.
We describe the re-sults on meetings and emails in turn.5.2.1 AMI corpusThe p/r/f results for the AMI polarity task are pre-sented in Table 6, with the scores pertaining tothe positive-subjective class.
The VIN classifierand full features classifier achieve the highest pre-cision, but the f-measures are below the lower-bound.Comparing AUROC results, the VIN approachis again significantly better than AutoSlog-TS,averaging 0.65 compared with 0.56, and signifi-cantly better than the standard trigram approach(p<0.01 in both cases).
The results are dis-played in Figure 3.
The conversational features aresignificantly less effective than the VIN features(p<0.05), and the best overall results are found byutilizing all features, with significant improvementover VIN only at p<0.05 and significant improve-ment over AutoSlog-TS only at p<0.01.5.2.2 BC3 corpusThe results of the polarity task on the BC3 cor-pus are markedly different from the other exper-imental results.
In this case, neither VIN norAutoSlog-TS are particularly good for discrimi-nating between positive and negative sentences,and the best strategy is to use features relating toconversational structure.
According to p/r/f (Ta-ble 6), the only method outperforming the lower-bound in terms of f-measure is the conversationalfeatures classifier.
According to AUROC scoresshown in Figure 3, the conversational features bythemselves are significantly better than the VINapproach (p<0.01 for non-paired t-test).
So foremails, we are more likely to correctly classifypositive and negative sentence by looking at fea-tures such as position in the turn and participantdominance than by matching our learned patterns.While we showed previously that pattern-basedapproaches perform well for the subjectivity taskon this dataset, there was less success in using thepatterns to discern the polarity of email sentences.We are again interested in whether the use of theBLOG06 data was beneficial.
For the BC3 data,there is very little difference between the VIN ap-proach with and without the blog patterns, as theyboth perform poorly, but with the AMI corpus, theblog patterns yield significant improvement in po-larity classification, increasing from an average of0.56 without the blog patterns to 0.65 with them(p<0.01).6 Discussion and Future WorkA key difference between the AMI and BC3 datawith regards to subjectivity is that negative ut-terances are much more common in the BC3email threads.
Additionally, the pattern-based ap-proaches fared worst in discriminating between13550.40.50.60.70.80.91Trig - AMITrig - BC3Slog - AMISlog - BC3VIN - AMIVIN - BC3Conv - AMIConv - BC3All Feas - AMIAll Feas - BC3AUROCFigure 3: AUROCs on Polarity Task for AMI andBC3 corporanegative and positive utterances in that corpus.Positive and negative email sentences are moreeasily recognized via features relating to conver-sation structure and participant status than throughthe learned lexical patterns.The use of patterns learned from unlabeled blogdata significantly improved performance.
We arecurrently developing further techniques for learn-ing subjective and polar patterns from such raw,natural text.A potential area of improvement is to reduce thefeature set by eliminating some of the subjectivepatterns.
In Section 2, we briefly described thework of Riloff et al (2006) on feature subsump-tion relationships.
In our case, in a VIN instantia-tion set a given trigram instantiation subsumes theless abstract instantiations in the set, so the mostabstract instantiation (i.e.
completely uninstanti-ated trigram) representationally subsumes the rest.Eliminating some of the representationally sub-sumed instantiations when they are also behav-iorally subsumed may improve our results.It is difficult to compare our results directly withthose of Raaijmakers et al (2008) as they used asmaller set of AMI meetings for their experiments,and because for the first experiment we considerthe subjective class to be the union of positive-subjective and negative-subjective dialogue actswhereas they additionally include subjective ques-tions and dialogue acts expressing uncertainty.These differences are reflected by the substantiallydiffering scores reported for majority-vote base-lines on each task.
However, their success withcharacter n-gram features suggests that we couldimprove our system by incorporating a variety ofcharacter features.
Character n-grams were thebest single feature class in their experiments.The VIN representation is a general one andmay hold promise for learning patterns relevant toother interesting conversation phenomena such asdecision-making and action items.
We plan to ap-ply the methods described here to these other ap-plications in the near future.7 ConclusionIn this work we have shown that learning subjec-tive trigrams with varying instantiation levels fromboth annotated and raw data can improve subjec-tivity detection and polarity labeling for meetingspeech and email threads.
The novel pattern-basedapproach was significantly better than standard tri-grams for three of the four tasks, and was signif-icantly better than a state-of-the-art syntactic ap-proach for those same tasks.
We also found thatfeatures relating to conversational structure werebeneficial for all tasks, and particularly for polar-ity labeling in email data.
Interestingly, in threeout of four cases combining all the features pro-duced the best performance.ReferencesF.
Biadsy, J. Hirschberg, and E. Filatova.
2008.
An un-supervised approach to biography production usingwikipedia.
In Proc.
of ACL-HLT 2008, Columbus,OH, USA.E.
Brill.
1992.
A simple rule-based part of speechtagger.
In Proc.
of DARPA Speech and Natural Lan-guage Workshop, San Mateo, CA, USA, pages 112?116.J.
Carletta, S. Ashby, S. Bourban, M. Flynn,M.
Guillemot, T. Hain, J. Kadlec, V. Karaiskos,W.
Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln,A.
Lisowska, I. McCowan, W. Post, D. Reidsma, andP.
Wellner.
2005.
The AMI meeting corpus: A pre-announcement.
In Proc.
of MLMI 2005, Edinburgh,UK, pages 28?39.R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
2008.
Liblinear: A library for large lin-ear classification.
Journal of Machine Learning Re-search, 9:1871?1874.T.
Fawcett.
2003.
Roc graphs: Notes and practicalconsiderations for researchers.G.
Murray and G. Carenini.
2008.
Summarizing spo-ken and written conversations.
In Proc.
of EMNLP2008, Honolulu, HI, USA.B.
Pang and L. Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in Informa-tion Retrieval, 1-2(2):1?135.1356S.
Raaijmakers, K. Truong, and T. Wilson.
2008.
Mul-timodal subjectivity analysis of multiparty conversa-tion.
In Proc.
of EMNLP 2008, Honolulu, HI, USA.E.
Riloff and W. Phillips.
2004.
An introduction to thesundance and autoslog systems.E.
Riloff and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In Proc.
of EMNLP2003, Sapporo, Japan.E.
Riloff, S. Patwardhan, and J. Wiebe.
2006.
Fea-ture subsumption for opinion analysis.
In Proc.
ofEMNLP 2006, Sydney, Australia.S.
Somasundaran, J. Ruppenhofer, and J. Wiebe.
2007.Detecting arguing and sentiment in meetings.
InProc.
of SIGDIAL 2007, Antwerp, Belgium.J.
Ulrich, G. Murray, and G. Carenini.
2008.
Apublicly available annotated corpus for supervisedemail summarization.
In Proc.
of AAAI EMAIL-2008 Workshop, Chicago, USA.T.
Wilson, J. Wiebe, and R. Hwa.
2006.
Recognizingstrong and weak opinion clauses.
Computational In-telligence, 22(2):73?99.T.
Wilson.
2008.
Annotating subjective content inmeetings.
In Proc.
of LREC 2008, Marrakech, Mo-rocco.H.
Yu and V. Hatzivassiloglou.
2003.
Towards an-swering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proc.
of EMNLP 2003, Sapporo, Japan.1357
