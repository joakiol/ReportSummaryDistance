Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 812?816,Dublin, Ireland, August 23-24, 2014.UW-MRS: Leveraging a Deep Grammar for Robotic Spatial CommandsWoodley PackardUniversity of Washingtonsweaglesw@sweaglesw.orgAbstractThis paper describes a deep-parsing ap-proach to SemEval-2014 Task 6, a novelcontext-informed supervised parsing andsemantic analysis problem in a controlleddomain.
The system comprises a hand-built rule-based solution based on a pre-existing broad coverage deep grammar ofEnglish, backed up by a off-the-shelf data-driven PCFG parser, and achieves the bestscore reported among the task participants.1 IntroductionSemEval-2014 Task 6 involves automatic transla-tion of natural language commands for a roboticarm into structured ?robot control language?
(RCL) instructions (Dukes, 2013a).
Statements ofRCL are trees, with a fixed vocabulary of contentwords like prism at the leaves, and markup likeaction: or destination: at the nonterminals.The yield of the tree largely aligns with the wordsin the command, but there are frequently substitu-tions, insertions, and deletions.A unique and interesting property of this taskis the availability of highly relevant machine-readable descriptions of the spatial context of eachcommand.
Given a candidate RCL fragment de-scribing an object to be manipulated, a spatialplanner provided by the task organizers can auto-matically enumerate the set of task-world objectsthat match the description.
This information canbe used to resolve some of the ambiguity inherentin natural language.The commands come from the Robot Com-mands Treebank (Dukes, 2013a), a crowdsourcedcorpus built using a game with a purpose (vonAhn, 2006).
Style varies considerably, with miss-ing determiners, missing or unexpected punc-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organizers.
Licence details:http://creativecommons.org/licenses/by/4.0/tuation, and missing capitalization all common(Dukes, 2013b).
Examples (1) and (2) show typi-cal commands from the dataset.
(1) drop the blue cube(2) Pick yellow cube and drop it on top of blue cubeAlthough the natural language commands varyin their degree of conformance to what might becalled standard English, the hand-built gold stan-dard RCL annotations provided with them (e.g.Figure 1) are commendable in their uniformity andaccuracy, in part because they have been automat-ically verified against the formal before and afterscene descriptions using the spatial planner.
(event: (action: drop)(entity: (color: blue)(type: cube))Figure 1: RCL corresponding to Example (1).2 Related WorkAutomatic interpretation of natural language isa difficult and long-standing research problem.Some approaches have taken a relatively shal-low view; for instance, ELIZA (Weizenbaum,1966) used pattern matching to somewhat con-vincingly participate in an English conversation.Approaches taking a deeper view tend to parseutterances into structured representations.
Theseare usually abstract and general-purpose in na-ture, e.g.
the syntax trees produced by main-stream PCFG parsers and the DRS produced bythe Boxer system (Bos, 2008).
As a notable ex-ception, Dukes (2014) presents a novel method toproduce RCL output directly.The English Resource Grammar (ERG;Flickinger, 2000) employed as a component inthe present work is a broad-coverage precisionhand-written unification grammar of English,following the Head-driven Phrase StructureGrammar theory of syntax (Pollard & Sag, 1994).The ERG produces Minimal Recursion Semantics812(MRS; Copestake et al., 2005) analyses, whichare flat structures that explicitly encode predicateargument relations (and other data).
A simplifiedMRS structure is shown in Figure 2.
With minormodifications to allow determinerless NPs andsome unexpected measure noun lexemes (as in?two squares to the left?, etc), the ERG yieldsanalyses for 99% of the commands in the trainingportion of the Robot Command Treebank.
(INDEX = e,{pron(x), cube n(y),drop v cause(e, x, y), blue a( , y)})Figure 2: Highly simplified view of the MRS pro-duced by the ERG for Example (1).3 ERG-based RCL SynthesisThis section outlines the method my sys-tem employs to synthesize RCL outputs fromthe MRS analyses produced by the ERG.The ERG provides a ranked list of candidateMRS analyses for each input.
As a first step,grossly inappropriate analyses are ruled out, e.g.those proposing non-imperative main verbs ordomain-inappropriate parts of speech (?block?
asa verb).
An attempt is made to convert each re-maining analysis into a candidate RCL statement.If conversion is successful, the result is tested forcoherence with respect to the known world state,using the supplied spatial planner.
An RCL state-ment is incoherent if it involves picking up or mov-ing an entity which does not exist, or if its com-mand type (take, move, drop) is incompatiblewith the current state of the robot arm, e.g.
dropis incoherent when the robot arm is not holdinganything.
Processing stops as soon as a coherentresult is found.13.1 From MRS to RCLGiven an individual (imperative) MRS structure,the first step in conversion to RCL is to iden-tify the sequence of top-level verbal predications.The INDEX property of the MRS provides an en-try point.
In a simple command like Example (1),the INDEX will point to a single verbal predica-tion, whereas in a compound command such as1Practically speaking, conversion from MRS to RCL isaccomplished by a relatively short C program embodyingthese rules and steps (about 1500 lines in the final version):http://sweaglesw.org/svn/semeval-2014-task6/tags/dublinExample (2), the INDEX will point to a coordina-tion predication, which itself will have left andright arguments which must be visited recursively.Each verbal predication visited in this manner gen-erates an event: RCL statement whose action:property is determined by a looking up the ver-bal predicate in a short hand-written table (e.g.drop v cause maps to action: drop).
If thepredicate is not found in the table, the most com-mon action move is guessed.Every RCL event: element must have anentity: subelement, representing the object tobe moved by the action.
Although in princi-ple MRS makes no guarantees about the gener-alizability of the semantic interpretation of argu-ment roles across different predicates, in prac-tice the third argument of every verbal predicaterelevant to this domain represents the object tobe moved; hence, synthesis of an event: pro-ceeds by inspecting the third argument of theMRS predicate which gave rise to it.
Some typesof event: also involve a destination: subele-ment, which encodes the location where the en-tity should come to rest.
When present, a verbalpredicate?s fourth argument almost always iden-tifies a prepositional predication holding this in-formation, although there are exceptions (e.g.
formove v from-to rel it is the fifth).
When no suchresultative role is present, the first prepositionalmodifier (if any) of the verbal event variable isused for the destination: subelement.Synthesis of an entity: element from areferential index like y in Figure 2 or aspatial-relation: element from a preposi-tional predication proceeds in much the same way:the RCL type: or relation: is determined bya simple table lookup, and subelements are builtbased on connections indicated in the MRS. Onesalient difference is the treatment of predicatesthat are not found in their respective lookup ta-bles.
Whereas unknown command predicates de-fault to the most common action move, unknownmodifying spatial relations are simply dropped,2and unknown entity types cause conversion to fail,on the theory that an incorrect parse is likely.
Pru-dent rejection of suspect parses only rarely elim-inates all available analyses, and generally helpsto find the most appropriate one.
On developmentdata, the first analysis produced by the ERG was2If the spatial relation is part of a mandatorydestination: element, this can then cause conversion to fail.813convertible for 87% of commands, and the firstRCL hypothesis was spatially coherent for 96% ofcommands.
These numbers indicate that the parseranking component of the ERG works quite well.3.2 Polishing the RulesI split the 2500 task-supplied annotated commandsinto a randomly-divided training set (2000 com-mands) and development set (500 commands).Throughout this work, the development set wasonly used for estimating performance on unseendata and tuning system combination settings; thecontents of the development set were never in-spected for rule writing or error analysis pur-poses.
Although the conversion architecture out-lined above constitutes an effective framework,there were quite a few details to be workedthrough, such as the construction of the lookup ta-bles, identification of cases requiring special han-dling, elimination of undesirable parses, modestextension of the ERG, etc.
An error-analysistool which performed a fine-grained comparisonof the synthesized RCL statements with the gold-standard ones and agglomerated common errortypes proved invaluable when writing rules.
3 Pol-ishing the system in this manner took about twoweeks of part-time effort; I maintained a log giv-ing a short summary of each tweak (e.g.
?mapcenter n of rel to type: region?).
Thesetweaks required varying amounts of time to imple-ment, from a few seconds up to perhaps an hour;system accuracy as a function of the number ofsuch tweaks is shown in Figure 3.3.3 Anaphora and EllipsisSome commands use anaphora to evoke the iden-tity or type of previously mentioned entities.
Typ-ically, the pronoun ?it?
refers to a specific entitywhile the pronoun ?one?
refers to the type of anentity (e.g.
?Put the red cube on the blue one.?
).Empirically, the antecedent is nearly always thefirst entity: element in the RCL statement, andthis heuristic works well in the system.
A smallfraction of commands (< 0.5% of the trainingdata) elide the pronoun, in commands like ?Takethe blue tetrahedron and place in front left corner.
?In principle these could be detected and accommo-dated through the addition of a simple mal-rule to3The error-analysis tool walks the system and goldRCL trees in tandem, recording differences and printing themost common mismatches.
It consists of about 100 lines ofPython and shell script, and took perhaps an hour to build.02040608010010 20 30 40 50 60 70 80AccuracyApprox.
Number of TweaksTraining SetDevelopment SetFigure 3: Tuning the MRS-to-RCL conversionsystem by tweaking/adding rules.
Development-set accuracy was only checked occasionally duringrule-writing to avoid over-fitting.the ERG (Bender et al., 2004), but for simplicitymy system ignores this problem, leading to errors.4 Robustness StrategiesIf none of the analyses produced by the ERG resultin coherent RCL statements, the system producesno output.
On the one hand this results in quite ahigh precision: on the training data, 96.75% of theRCL statements produced are exactly correct.
Onthe other hand, in some scenarios a lower precisionresult may be preferable to no result.
The ERG-based system fails to produce any output for 3.1%of the training data inputs, a number that should beexpected to increase for unseen data (since conver-sion can sometimes fail when the MRS containsunrecognized predicates).In order to produce a best-guess answer forthese remaining items, I employed the Berkeleyparser (Petrov et al., 2006), a state-of-the-art data-driven system that induces a PCFG from a user-supplied corpus of strings annotated with parsetrees.
The RCL treebank is not directly suitable astraining material for the Berkeley parser, since theyield of an RCL tree is not identical to (or evenin 1-to-1 correspondence with) the words of theinput utterance.
In the interest of keeping thingssimple, I produced a phrase structure translationof the RCL treebank by simply discarding the el-ements of the RCL trees that did not correspondto any input, and inserting (X word) nodes for in-put words that were not aligned to any RCL frag-ment.
The question of where in the tree to insertthese X nodes is presumably of considerable im-portance, but again in the interest of simplicity Isimply clustered them together with the first RCL-814Sevent:??????action:dropdropentity:????color:?
?Xthebluebluetype:cubecubeFigure 4: Automatic phrase structure tree transla-tion of the RCL statement shown in Figure 1.aligned word appearing after them.
Unaligned in-put tokens at the end of the sentence were addedas siblings of the root node.
Figure 4 shows thephrase structure tree resulting from the translationof the RCL statement shown in Figure 1.Using this phrase structure treebank, the Berke-ley parser tools make it possible to automaticallyderive a similar phrase structure tree for any inputstring, and indeed when the input string is a com-mand such as the ones of interest in this work, theresulting tree is quite close to an RCL statement.Deletion of the X nodes yields a robust systemthat frequently produces the exact correct RCL,at least for those items where only input-alignedRCL leaves are required.
The most common typeof non-input-aligned RCL fragment is the id: el-ement, identifying the antecedent of an anaphor.As with the ERG-based system, a heuristic select-ing the first entity as the antecedent whenever ananaphor is present works quite well.Improving the output of the statistical systemvia tweaks of the type used in the ERG-based sys-tem was much more challenging, due to the rel-ative impoverishedness of the information madeavailable by the parser.
Accurately detecting situ-ations to improve without causing collateral dam-age proved difficult.
However, the base accu-racy of the statistical system was quite good, andwhen used as a back-off it improved overall sys-tem scores considerably, as shown in Table 5.5 Results and DiscussionThe combined system performs best on both por-tions of the data.
Over the development data, theMRS-based system performs considerably betterthan the statistical system, in part due to the useof spatial planning in the MRS-based system (timedid not permit adding spatial planning to the statis-Dev EvalSystem P R P RMRS-only (?SP) 90.7 88.0 92.1 80.3MRS-only (+SP) 95.4 92.2 96.1 82.4Robust-only (?SP) 88.2 88.2 81.5 81.5Combined (?SP) 90.8 90.8 90.5 90.5Combined (+SP) 95.0 95.0 92.5 92.5ERG coverage 98.6 91.0Figure 5: Evaluation results.
?SP indicateswhether or not spatial planning was used.
The ro-bust and combined systems always returned a re-sult, so P = R.tical system).
The statistical system has a slightlyhigher recall than the MRS-only system withoutspatial planning, but the MRS-only system has ahigher precision ?
markedly so on the evalua-tion data.
This is consistent with previous find-ings combining precision grammars with statisti-cal systems (Packard et al., 2014).ERG coverage dropped precipitously fromroughly 99% on the development data to 91%on the evaluation data.
This is likely the majorcause of the 10% absolute drop in the recall of theMRS-only system.
The fact that the robust sta-tistical system encounters a comparable drop onthe evaluation data suggests that the text is qual-itatively different from the (also held-out) devel-opment data.
One possible explanation is thatwhereas the development data was randomly se-lected from the 2500 task-provided training com-mands, the evaluation data was taken as the se-quentially following segment of the treebank, re-sulting in the same distribution of game-with-a-purpose participants (and hence writing styles) be-tween the training and development sets but a dif-ferent distribution for the evaluation data.
4Dukes (2014) reports an accuracy of 96.53%,which appears to be superior to the present system;however, that system appears to have used moretraining data than was available for the shared task,and averaged scores over the entire treebank, mak-ing direct comparison difficult.AcknowledgementsI am grateful to Dan Flickinger, Emily Bender andStephan Oepen for their many helpful suggestions.4Reviewers suggested dropping sentence initial punctua-tion and reading ?cell?
as ?tile.?
This trick boosts the MRS-only recall to 91.1% and the combined system to 94.5%,demonstrating both the frailty of NLP systems to unexpectedinputs and the presence of surprises in the evaluation data.ERG coverage rose from 91.0% to 98.6%.815ReferencesBender, E. M., Flickinger, D., Oepen, S., Walsh, A., &Baldwin, T. (2004).
Arboretum: Using a preci-sion grammar for grammar checking in CALL.In Instil/icall symposium 2004.Bos, J.
(2008).
Wide-coverage semantic analysis withboxer.
In J. Bos & R. Delmonte (Eds.
), Seman-tics in text processing.
step 2008 conference pro-ceedings (pp.
277?286).
College Publications.Copestake, A., Flickinger, D., Pollard, C., & Sag, I.(2005).
Minimal recursion semantics: An intro-duction.
Research on Language & Computation,3(2), 281?332.Dukes, K. (2013a).
Semantic annotation of roboticspatial commands.
In Language and TechnologyConference (LTC).
Poznan, Poland.Dukes, K. (2013b).
Train robots: A dataset fornatural language human-robot spatial interac-tion through verbal commands.
In Interna-tional Conference on Social Robotics (ICSR).Embodied Communication of Goals and Inten-tions Workshop.Dukes, K. (2014).
Contextual Semantic Parsing usingCrowdsourced Spatial Descriptions.
Computa-tion and Language.
arXiv:1405.0145 [cs.CL].Flickinger, D. (2000).
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(01), 15-28.Packard, W., Bender, E. M., Read, J., Oepen, S., & Dri-dan, R. (2014).
Simple negation scope reso-lution through deep parsing: A semantic solu-tion to a semantic problem.
In Proceedings ofthe 52nd annual meeting of the Association forComputational Linguistics.
Baltimore, USA.Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).Learning accurate, compact, and interpretabletree annotation.
In Proceedings of the 21st Inter-national Conference on Computational Linguis-tics and the 44th annual meeting of the Asso-ciation for Computational Linguistics (pp.
433?440).Pollard, C., & Sag, I.
A.
(1994).
Head-Driven PhraseStructure Grammar.
Chicago, USA: The Uni-versity of Chicago Press.von Ahn, L. (2006).
Games with a purpose.
Computer,39(6), 92?94.Weizenbaum, J.
(1966).
ELIZA ?
a computer pro-gram for the study of natural language commu-nication between man and machine.
Communi-cations of the ACM, 9(1), 36?45.816
