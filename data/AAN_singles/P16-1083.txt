Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 876?886,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsNormalized Log-Linear Interpolation of Backoff Language Models isEfficientKenneth HeafieldUniversity of Edinburgh10 Crichton StreetEdinburgh EH8 9ABUnited Kingdomkheafiel@inf.ed.ac.ukChase Geigle Sean MassungUniversity of Illinois at Urbana-Champaign707 S. Mathews Ave.Urbana, IL 61801United States{geigle1,massung1,lanes}@illinois.eduLane SchwartzAbstractWe prove that log-linearly interpolatedbackoff language models can be efficientlyand exactly collapsed into a single nor-malized backoff model, contradicting Hsu(2007).
While prior work reported thatlog-linear interpolation yields lower per-plexity than linear interpolation, normaliz-ing at query time was impractical.
We nor-malize the model offline in advance, whichis efficient due to a recurrence relationshipbetween the normalizing factors.
To tuneinterpolation weights, we apply Newton?smethod to this convex problem and showthat the derivatives can be computed ef-ficiently in a batch process.
These find-ings are combined in new open-source in-terpolation tool, which is distributed withKenLM.
With 21 out-of-domain corpora,log-linear interpolation yields 72.58 per-plexity on TED talks, compared to 75.91for linear interpolation.1 IntroductionLog-linearly interpolated backoff language mod-els yielded better perplexity than linearly interpo-lated models (Klakow, 1998; Gutkin, 2000), butexperiments and adoption were limited due the im-practically high cost of querying.
This cost is dueto normalizing to form a probability distributionby brute-force summing over the entire vocabu-lary for each query.
Instead, we prove that thelog-linearly interpolated model can be normalizedoffline in advance and exactly expressed as an or-dinary backoff language model.
This contradictsHsu (2007), who claimed that log-linearly inter-polated models ?cannot be efficiently representedas a backoff n?gram model.
?We show that offline normalization is efficientdue to a recurrence relationship between thenormalizing factors (Whittaker and Klakow,2002).
This forms the basis for our open-source implementation, which is part of KenLM:https://kheafield.com/code/kenlm/.Linear interpolation (Jelinek and Mercer, 1980),combines several language models piinto a singlemodel pLpL(wn| wn?11) =?i?ipi(wn| wn?11)where ?iare weights and wn1are words.
Becauseeach component model piis a probability distri-bution and the non-negative weights ?isum to 1,the interpolated model pLis also a probability dis-tribution.
This presumes that the models have thesame vocabulary, an issue we discuss in ?3.1.A log-linearly interpolated model pLLuses theweights ?ias powers (Klakow, 1998).pLL(wn| wn?11) ?
?ipi(wn| wn?11)?iThe weights ?iare unconstrained real numbers,allowing parameters to soften or sharpen distribu-tions.
Negative weights can be used to divide amixed-domain model by an out-of-domain model.To form a probability distribution, the product isnormalizedpLL(wn| wn?11) =?ipi(wn| wn?11)?iZ(wn?11)where normalizing factor Z is given byZ(wn?11) =?x?ipi(x | wn?11)?iThe sum is taken over all words x in the combinedvocabulary of the underlying models, which cannumber in the millions or even billions.
Comput-ing Z efficiently is a key contribution in this work.Our proofs assume the component models piarebackoff language models (Katz, 1987) that mem-orize probability for seen n?grams and charge a876backoff penalty bifor unseen n?grams.pi(wn| wn?11) ={pi(wn| wn?11) if wn1is seenpi(wn| wn?12)bi(wn?11) o.w.While linearly or log-linearly interpolated mod-els can be queried online by querying the compo-nent models (Stolcke, 2002; Federico et al, 2008),doing so costs RAM to store duplicated n?gramsand CPU time to perform lookups.
Log-linear in-terpolation is particularly slow due to normalizingover the entire vocabulary.
Instead, it is preferableto combine the models offline into a single back-off model containing the union of n?grams.
Do-ing so is impossible for linear interpolation (?3.2);SRILM (Stolcke, 2002) and MITLM (Hsu andGlass, 2008) implement an approximation.
In con-trast, we prove that offline log-linear interpolationrequires no such approximation.2 Related WorkInstead of building separate models then weight-ing, Zhang and Chiang (2014) show how to trainKneser-Ney models (Kneser and Ney, 1995) onweighted data.
Their work relied on prescriptiveweights from domain adaptation techniques ratherthan tuning weights, as we do here.Our exact normalization approach relies on thebackoff structure of component models.
Sev-eral approximations support general models: ig-noring normalization (Chen et al, 1998), noise-contrastive estimation (Vaswani et al, 2013), andself-normalization (Andreas and Klein, 2015).
Infuture work, we plan to exploit the structure ofother features in high-quality unnormalized log-linear language models (Sethy et al, 2014).Ignoring normalization is particularly commonin speech recognition and machine translation.This is one of our baselines.
Unnormalized mod-els can also be compiled into a single model bymultiplying the weighted probabilities and back-offs.1Many use unnormalized models becauseweights can be jointly tuned along with other fea-ture weights.
However, Haddow (2013) showedthat linear interpolation weights can be jointlytuned by pairwise ranked optimization (Hopkinsand May, 2011).
In theory, normalized log-linearinterpolation weights can be jointly tuned in thesame way.1Missing probabilities are found from the backoff algo-rithm and missing backoffs are implicitly one.Dynamic interpolation weights (Weintraub etal., 1996) give more weight to models famil-iar with a given query.
Typically the weightsare a function of the contexts that appear in thecombined language model, which is compatiblewith our approach.
However, normalizing factorswould need to be calculated in each context.3 Linear InterpolationTo motivate log-linear interpolation, we examinetwo issues with linear interpolation: normalizationwhen component models have different vocabular-ies and offline interpolation.3.1 Vocabulary DifferencesLanguage models are normalized with respect totheir vocabulary, including the unknown word.
?x?vocab(p1)p1(x) = 1If two models have different vocabularies, thenthe combined vocabulary is larger and the sum istaken over more words.
Component models as-sign their unknown word probability to these newwords, leading to an interpolated model that sumsto more than one.
An example is shown in Table 1.p1p2pLZero<unk> 0.4 0.2 0.3 0.3A 0.6 0.4 0.3B 0.8 0.6 0.4Sum 1 1 1.3 1Table 1: Linearly interpolating two models p1and p2with equal weight yields an unnormalizedmodel pL.
If gaps are filled with zeros instead, themodel is normalized.To work around this problem, SRILM (Stol-cke, 2002) uses zero probability instead of the un-known word probability for new words.
This pro-duces a model that sums to one, but differs fromwhat users might expect.IRSTLM (Federico et al, 2008) asks the user tospecify a common large vocabulary size.
The un-known word probability is downweighted so thatall models sum to one over the large vocabulary.A component model can also be renormalizedwith respect to a larger vocabulary.
For unigrams,the extra mass is the number of new words timesthe unknown word probability.
For longer con-texts, if we assume the typical case where the877unknown word appears only as a unigram, thenqueries for new words will back off to unigrams.The total mass in context wn?11is1 + |new|p(<unk>)n?1?i=1b(wn?1i)where new is the set of new words.
This is effi-cient to compute online or offline.
While there aretools to renormalize models, we are not aware of atool that does this for linear interpolation.Log-linear interpolation is normalized by con-struction.
Nonetheless, in our experiments we ex-tend IRSTLM?s approach by training models witha common vocabulary size, rather than retrofittingit at query time.3.2 Offline Linear InterpolationGiven an interpolated model, offline interpolationseeks a combined model meeting three criteria: (i)encoding the same probability distribution, (ii) be-ing a backoff model, and (iii) containing the unionof n?grams from component models.Theorem 1.
The three offline criteria cannot besatisfied for general linearly interpolated backoffmodels.Proof.
By counterexample.
Consider the modelsgiven in Table 2 interpolated with equal weight.p1p2pLp(A) 0.4 0.2 0.3p(B) 0.3 0.3 0.3p(C) 0.3 0.5 0.4p(C | A) 0.4 0.8 0.6b(A)67?
0.857 0.423?
0.667Table 2: Counterexample models.The probabilities shown for pLresult from encod-ing the same distribution.
Taking the union of n?grams implies that pLonly has entries for A, B, C,and A C. Since the models have the same vocabu-lary, they are all normalized to one.p(A | A) + p(B | A) + p(C | A) = 1Since all models have backoff structure,p(A)b(A) + p(B)b(A) + p(C | A) = 1which when solved for backoff b(A) gives the val-ues shown in Table 2.
We then query pL(B | A)online and offline.
Online interpolation yieldspL(B | A) =12p1(B | A) +12p2(B | A)=12p1(B)b1(A) +12p2(B)b2(A) =33175Offline interpolation yieldspL(B | A) = pL(B)bL(A) = 0.2 6=33175?
0.189The same problem happens with real languagemodels.
To understand why, we attempt to solvefor the backoff bL(wn?11).
Supposing wn1is not ineither model, we query pL(wn| wn?11) offlinepL(wn|wn?11)=pL(wn|wn?12)bL(wn?11)=(?1p1(wn|wn?12) + ?2p2(wn|wn?12))bL(wn?11)while online interpolation yieldspL(wn|wn?11)=?1p1(wn|wn?11) + ?2p2(wn|wn?11)=?1p1(wn|wn?12)b1(wn?11) + ?1p2(wn|wn?12)b2(wn?11)Solving for bL(wn?11) we obtain?1p1(wn|wn?12)b1(wn?11) + ?2p2(wn|wn?12)b2(wn?11)?1p1(wn|wn?12) + ?2p2(wn|wn?12)which is a weighted average of the backoff weightsb1(wn?11) and b2(wn?11).
The weights depend onwn, so bLis no longer a function of wn?11.In the SRILM approximation (Stolcke, 2002),probabilities for n?grams that exist in the modelare computed exactly.
The backoff weights arechosen to produce a model that sums to one.However, newer versions of SRILM (Stolcke etal., 2011) interpolate by ingesting one componentmodel at a time.
For example, the first two mod-els are approximately interpolated before addinga third model.
An n?gram appearing only in thethird model will have an approximate probabil-ity.
Therefore, the output depends on the orderin which users specify models.
Moreover, weightswere optimized for correct linear interpolation, notthe approximation.Stolcke (2002) find that the approximation actu-ally decreases perplexity, which we also see in theexperiments (?6).
However, approximation onlyhappens when the model backs off, which is lesslikely to happen in fluent sentences used for per-plexity scoring.8784 Offline Log-Linear InterpolationLog-linearly interpolated backoff models picanbe collapsed into a single offline model pLL.
Thecombined model takes the union of n?grams incomponent models.2For those n?grams, it mem-orizes correct probability pLL.pLL(wn| wn?11) =?ipi(wn| wn?11)?iZ(wn?11)(1)When wn1does not appear, the backoff bLL(wn?11)modifies pLL(wn| wn?12) to make an appropri-ately normalized probability.
To do so, it can-cels out the shorter query?s normalization termZ(wn?12) then applies the correct term Z(wn?11).It also applies the component backoff terms.bLL(wn?11) =Z(wn?12)Z(wn?11)?ibi(wn?11)?i(2)Almost by construction, the model satisfies twoof our criteria (?3.2): being a backoff model andcontaining the union of n?grams.
However, back-off models require that the backoff weight of anunseen n?gram be implicitly 1.Lemma 1.
If wn?11is unseen in the combinedmodel, then the backoff weight bLL(wn?11) = 1.Proof.
Because we have taken the union of en-tries, wn?11is unseen in component models.
Thesecomponents are backoff models, so implicitlybi(wn?11) = 1 ?i.
Focusing on the normalizationterm Z(wn?11),Z(wn?11) =?x?ipi(x | wn?11)?i=?x?ipi(x | wn?12)?ibi(wn?11)?i=?x?ipi(x | wn?12)?i= Z(wn?12)All of the models back off because wn?11x is un-seen, being a superstring of wn?11.
Relevant back-off weights bi(wn?11) = 1 as noted earlier.
Recall-ing the definition of bLL(wn?11),Z(wn?12)Z(wn?11)?ibi(wn?11)?i=Z(wn?12)Z(wn?11)= 12We further assume that every substring of a seen n?gramis also seen.
This follows from estimating on text, except inthe case of adjusted count pruning by SRILM.
In such cases,we add the missing entries to component models, with noadditional memory cost in trie data structures.We now have a backoff model containing theunion of n?grams.
It remains to show that the of-fline model produces correct probabilities.Theorem 2.
The proposed offline model agreeswith online log-linear interpolation.Proof.
By induction on the number of wordsbacked off in offline interpolation.
To disam-biguate, we will use ponto refer to online inter-polation and poffto refer to offline interpolation.Base case: the queried n?gram is in the offlinemodel and we have memorized the online prob-ability by construction.Inductive case: Let poff(wn| wn?11) be a querythat backs off.
In online interpolation,pon(wn| wn?11) =?ipi(wn| wn?11)?iZ(wn?11)Because wn1is unseen in the offline model and wetook the union, it is unseen in every model pi.=?ipi(wn| wn?12)?ibi(wn?11)?iZ(wn?11)=(?ipi(wn| wn?12)?i)?ibi(wn?11)?iZ(wn?11)Recognizing the unnormalized probabilityZ(wn?12)pon(wn| wn?12),=Z(wn?12)pon(wn| wn?12)?ibi(wn?11)?iZ(wn?11)= pon(wn| wn?12)Z(wn?12)Z(wn?11)?ibi(wn?11)?i= pon(wn| wn?12)boff(wn?11)The last equality follows from the definition ofboffand Lemma 1, which extended the domain ofboffto any wn?11.
By the inductive hypothesis,pon(wn| wn?12) = poff(wn| wn?12) because itbacks off one less time.= poff(wn| wn?12)boff(wn?11) = poff(wn| wn?11)The offline model poff(wn| wn?11) backs off be-cause that is the case we are considering.
Combin-ing our chain of equalities,pon(wn| wn?11) = poff(wn| wn?11)By induction, the claim holds for all wn1.8794.1 Normalizing EfficientlyIn order to build the offline model, the normaliza-tion factor Z needs to be computed in every seencontext.
To do so, we extend the tree-structuremethod of Whittaker and Klakow (2002), whichthey used to compute and cache normalization fac-tors on the fly.
It exploits the sparsity of languagemodels: when summing over the vocabulary, mostqueries will back off.
Formally, we define s(wn1)to be the set of words x where pi(x | wn1) does notback off in some model.s(wn1) = {x : wn1x is seen in any model}To exploit this, we use the normalizing factorZ(wn2) from a lower order and patch it up by sum-ming over s(wn1).Theorem 3.
The normalization factors Z obey arecurrence relationship:Z(wn1) =?x?s(wn1)?ipi(x | wn1)?i+??Z(wn2)?
?x?s(wn1)?ipi(x | wn2)?i???ibi(wn1)?iProof.
The first term handles seen n?grams whilethe second term handles unseen n?grams.
Thedefinition of ZZ(wn1) =?x?vocab?ipi(x | wn1)?ican be partitioned by cases.
?x?s(wn1)?ipi(x | wn1)?i+?x 6?s(wn1)?ipi(x | wn1)?iThe first term agrees with the claim, so we focuson the case where x 6?
s(wn1).
By definition of s,all models back off.
?x 6?s(wn1)?ipi(x | wn1)?i=?x 6?s(wn1)?ipi(x | wn2)?ibi(wn1)?i=??
?x 6?s(wn1)?ipi(x | wn2)?i???ibi(wn1)?i=??Z(wn2)?
?x?s(wn1)?ipi(x | wn2)?i??
?ibi(wn1)?iThis is the second term of the claim.LM1LM2.
.
.LM`Merge probabilities (?4.2.1)Apply Backoffs (?4.2.2)Normalize (?4.2.3)Output (?4.2.4)Context sort?wn1, m(wn1),?
?ipi(wn|wn?1mi(wn1))?i),?ipi(wn|wn?1mi(wn2))?i)In context order?wn1,?ibi(wn?11)?i,?
?ipi(wn| wn?11)?i,?ipi(wn| wn?12)?iIn suffix orderbLL(wn1)Suffix sort?wn1, pLL(wn|wn?11)?Figure 1: Multi-stage streaming pipeline for of-fline log-linear interpolation.
Bold arrows indicatesorting is performed.The recurrence structure of the normalizationfactors suggests a computational strategy: com-pute Z() by summing over the unigrams, Z(wn)by summing over bigramswnx, Z(wnn?1) by sum-ming over trigrams wnn?1x, and so on.4.2 Streaming ComputationPart of the point of offline interpolation is thatthere may not be enough RAM to fit all the com-ponent models.
Moreover, with compression tech-niques that rely on immutable models (Whittakerand Raj, 2001; Talbot and Osborne, 2007), a mu-table version of the combined model may not fit inRAM.
Instead, we construct the offline model withdisk-based streaming algorithms, using the frame-work we designed for language model estimation(Heafield et al, 2013).
Our pipeline (Figure 1) hasfour conceptual steps: merge probabilities, applybackoffs, normalize, and output.
Applying back-offs and normalization are performed in the samepass, so there are three total passes.4.2.1 Merge ProbabilitiesThis step takes the union of n?grams and multi-plies probabilities from component models.
We880assume that the component models are sorted insuffix order (Figure 4), which is true of modelsproduced by lmplz (Heafield et al, 2013) orstored in a reverse trie.
Moreover, despite havingdifferent word indices, the models are consistentlysorted using the string word, or a hash thereof.3 2 1AA AA A AB A ABTable 3: Merging probabilities processes n?gramsin lexicographic order by suffix.
Column headingsindicate precedence.The algorithm processes n?grams in lexico-graphic (depth-first) order by suffix (Table 3).
Inthis way, the algorithm processes pi(A) before itmight be used as a backoff point for pi(A | B)in one of the models.
It jointly streams through allmodels, so that p1(A | B) and p2(A | B) are avail-able at the same time.
Ideally, we would computeunnormalized probabilities.
?ipi(wn| wn?11)?iHowever, these queries back off when models con-tain different n?grams.
The appropriate backoffweights bi(wn?11) are not available in a stream-ing fashion.
Instead, we proceed without chargingbackoffs?ipi(wn| wn?1mi(wn1))?iwhere mi(wn1) records what backoffs should becharged later.The normalization step (?4.2.3) also uses lower-order probabilities?ipi(wn| wn?12)?iand needs to access them in a streaming fashion,so we also output?ipi(wn| wn?1mi(wn2))?iSuffix3 2 1Z B AZ A BB B BContext2 1 3Z A BB B BZ B ATable 4: Sorting orders (Heafield et al, 2013).
Insuffix order, the last word is primary.
In contextorder, the penultimate word is primary.
Columnheadings indicate precedence.Each output tuple has the form?wn1,m(wn1),?ipi(wn|wn?1mi(wn1))?i,?ipi(wn|wn?1mi(wn2))?i?wherem(wn1) is a vector of backoff requests, fromwhich m(wn2) can be computed.4.2.2 Apply BackoffsThis step fulfills the backoff requests from merg-ing probabilities.
The merged probabilities aresorted in context order (Table 4) so that n?grams wn1sharing the same context wn?11areconsecutive.
Moreover, contexts wn?11appearin suffix order.
We use this property to streamthrough the component models again in theirnative suffix order, this time reading backoffweights bi(wn?11), bi(wn?12), .
.
.
, bi(wn?1).
Mul-tiplying the appropriate backoff weights by?ipi(wn|wn?1mi(wn1))?iyields unnormalized proba-bility?ipi(wn|wn?11)?iThe same applies to the lower order.
?ipi(wn|wn?12)?iThis step also merges backoffs from componentmodels, with output still in context order.
?wn1,?ibi(wn?11)?i,?ipi(wn|wn?11)?i?ipi(wn|wn?12)?i?The implementation is combined with normaliza-tion, so the tuple is only conceptual.8814.2.3 NormalizeThis step computes normalization factor Z forall contexts, which it applies to produce pLLandbLL.
Recalling ?4.1, Z(wn?11) is efficient to com-pute in a batch process by processing suffixesZ(), Z(wn), .
.
.
Z(wn?12) first.
In order to min-imize memory consumption, we chose to evaluatethe contexts in depth-first order by suffix, so thatZ(A) is computed immediately before it is neededto compute Z(A A) and forgotten at Z(B).Computing Z(wn?11) by applying Theorem 3requires the sum?x?s(wn?11)?ipi(x | wn?11)?iwhere s(wn?11) restricts to seen n?grams.
For this,we stream through the output of the apply backoffsstep in context order, which makes the various val-ues of x consecutive.
Theorem 3 also requires asum over the lower-order unnormalized probabili-ties?x?s(wn?11)?ipi(x | wn?12)?iWe placed these terms in the input tuple forwn?11x.
Otherwise, it would be hard to accessthese values while streaming in context order.While we have shown how to computeZ(wn?11), we still need to normalize the probabil-ities.
Unfortunately, Z(wn?11) is only known afterstreaming through all records of the form wn?11x,which are the very same records to normalize.
Wetherefore buffer up to the vocabulary size for eachorder in memory to allow rewinding.
Processingcontext wn?11thus yields normalized probabilitiespLL(x | wn?11) for all seen wn?11x.
?wn1, pLL(x | wn?11)?These records are generated in context order, thesame order as the input.The normalization step also computes backoffs.bLL(wn?11) =Z(wn?12)Z(wn?11)?ibi(wn?11)?iNormalization Z(wn?11) is computed by this step,numerator Z(wn?12) is available due to depth-firstsearch, and the backoff terms?ibi(wn?11)?iarepresent in the input.
The backoffs bLLare gener-ated in suffix order, since each context produces abackoff value.
These are written to a sidechannelstream as bare values without keys.4.2.4 OutputLanguage model toolkits store probabilitypLL(wn| wn?11) and backoff bLL(wn1) together asvalues for the key wn1.
To reunify them, we sort?wn1, pLL(wn| wn?11)?
in suffix order and mergewith the backoff sidechannel from normalization,which is already in suffix order.
Suffix order isalso preferable because toolkits can easily build areverse trie data structure.5 TuningWeights are tuned to maximize the log probabilityof held-out data.
This is a convex optimizationproblem (Klakow, 1998).
Iterations are expensivedue to the need to normalize over the vocabularyat least once.
However, the number of weights issmall, which makes the Hessian matrix cheap tostore and invert.
We therefore selected Newton?smethod.3The log probability of tuning data w islog?npLL(wn| wn?11)which expands according to the definition of pLL?n(?i?ilog pi(wn| wn?11))?
logZ(wn?11)The gradient with respect to ?ihas a compact form?nlog pi(wn| wn?11) + CH(pLL, pi| wn?11)where CH is cross entropy.
However, comput-ing the cross entropy directly would entail a sumover the vocabulary for every word in the tun-ing data.
Instead, we apply Theorem 3 to ex-press Z(wn?11) in terms of Z(wn?12) before tak-ing the derivative.
This allows us to perform thesame depth-first computation as before (?4.2.3),only this time??
?iZ(wn?11) is computed in termsof??
?iZ(wn?12).The same argument applies when taking theHessian with respect to ?iand ?j.
Rather thancompute it directly in the form?n?
?xpLL(x|wn?11) log pi(x|wn?11) log pj(x|wn?11)+ CH(pLL, pi| wn?11)CH(pLL, pj| wn?11)we apply Theorem 3 to compute the Hessian forwn1in terms of the Hessian for wn2.3We also considered minibatches, though grouping tuningdata to reduce normalization cost would introduce bias.8826 ExperimentsWe perform experiments for perplexity, queryspeed, memory consumption, and effectiveness ina machine translation system.Individual language models were trained on En-glish corpora from the WMT 2016 news transla-tion shared task (Bojar et al, 2016).
This includesthe seven newswires (afp, apw, cna, ltw, nyt,wpb, xin) from English Gigaword Fifth Edition(Parker et al, 2011); the 2007?2015 news crawls;4News discussion; News commmentary v11; En-glish from Europarl v8 (Koehn, 2005); the Englishside of the French-English parallel corpus (Bojaret al, 2013); and the English side of SETIMES2(Tiedemann, 2009).
We additionally built one lan-guage model trained on the concatenation of allof the above corpora.
All corpora were prepro-cessed using the standard Moses (Koehn et al,2007) scripts to perform normalization, tokeniza-tion, and truecasing.
To prevent SRILM from run-ning out of RAM, we excluded the large mono-lingual CommonCrawl data, but included Englishfrom the parallel CommonCrawl data.All language models are 5-gram backoff lan-guage models trained with modified Kneser-Neysmoothing (Chen and Goodman, 1998) usinglmplz (Heafield et al, 2013).
Also to preventSRILM from running out of RAM, we pruned sin-gleton trigrams and above.For linear interpolation, we tuned weights us-ing IRSTLM.
To work around SRILM?s limitationof ten models, we interpolated the first ten thencarried the combined model and added nine morecomponent models, repeating this last step as nec-essary.
Weights were normalized within batchesto achieve the correct final weighting.
This simplyextends the way SRILM internally carries a com-bined model and adds one model at a time.6.1 Perplexity experimentsWe experiment with two domains: TED talks,which is out of domain, and news, which is in-domain for some corpora.
For TED, we tunedon the IWSLT 2010 English dev set and test onthe 2010 test set.
For news, we tuned on theEnglish side of the WMT 2015 Russian?Englishevaluation set and test on the WMT 2014 Russian?English evaluation set.
To measure generalization,we also evaluated news on models tuned for TEDand vice-versa.
Results are shown in Table 5.4For News Crawl 2014, we used version 2.Component ModelsComponent TED test News testGigaword afp 163.48 221.57Gigaword apw 140.65 206.85Gigaword cna 299.93 448.56Gigaword ltw 106.28 243.35Gigaword nyt 97.21 211.75Gigaword wpb 151.81 341.48Gigaword xin 204.60 246.32News 07 127.66 243.53News 08 112.48 202.86News 09 111.43 197.32News 10 114.40 209.31News 11 107.69 187.65News 12 105.74 180.28News 13 104.09 155.89News 14 v2 101.85 139.94News 15 101.13 141.13News discussion 99.88 249.63News commentary v11 236.23 495.27Europarl v8 268.41 574.74CommonCrawl fr-en.en 149.10 343.20SETIMES2 ro-en.en 331.37 521.19All concatenated 80.69 96.15TED weightsInterpolation TED test News testOffline linear 75.91 100.43Online linear 76.93 152.37Log-linear 72.58 112.31News weightsInterpolation TED test News testOffline linear 83.34 107.69Online linear 83.94 110.95Log-linear 89.62 124.63Table 5: Test set perplexities.
In the middle ta-ble, weights are optimized for TED and includea model trained on all concatenated text.
In thebottom table, weights are optimized for news andexclude the model trained on all concatenated text.883LM Tuning Compiling QueryingAll concatenated N/A N/A N/A N/A 0.186?s 46.7GOffline linear 0.876m 60.2G 641m 123G 0.186?s 46.8GOnline linear 0.876m 60.2G N/A N/A 5.67?s 89.1GLog-linear 600m 63.9G 89.8m 63.9G 0.186?s 46.8GTable 6: Speed and memory consumption of LM combination methods.
Interpolated models include theconcatenated model.
Tuning and compiling times are in minutes, memory consumption in gigabytes,and query time in microseconds per query (on 1G of held-out Common Crawl monolingual data).Log-linear interpolation performs better onTED (72.58 perplexity versus 75.91 for offline lin-ear interpolation).
However, it performs worseon news.
In future work, we plan to investigatewhether log-linear wins when all corpora are out-of-domain since it favors agreement by all models.Table 6 compares the speed and memory per-formance of the competing methods.
While thelog-linear tuning is much slower, its compilation isfaster compared to the offline linear model?s longrun time.
Since the model formats are the samefor the concatenation and log-linear, they sharethe fastest query speeds.
Query speed was mea-sured using KenLM?s (Heafield, 2011) faster prob-ing data structure.56.2 MT experimentsWe trained a statistical phrase-based machinetranslation system for Romanian-English on theRomanian-English parallel corpora released aspart of the 2016 WMT news translation sharedtask.
We trained three variants of this MT system.The first used a single language model trained onthe concatenation of the 21 individual LM train-ing corpora.
The second used 22 language mod-els, with each LM presented to Moses as a sep-arate feature.
The third used a single languagemodel which is an interpolation of all 22 mod-els.
This variant was run with offline linear, onlinelinear, and log-linear interpolation.
All MT sys-tem variants were optimized using IWSLT 2011Romanian-English TED test as the developmentset, and were evaluated using the IWSLT 2012Romanian-English TED test set.As shown in Table 7, no significant difference inMT quality as measured by BLEU was observed;the best BLEU score came from separate featuresat 18.40 while log-linear scored 18.15.
We leave5KenLM does not natively implement online linear inter-polation, so we wrote a custom wrapper, which is faster thanquerying IRSTLM.LM BLEU BLEU-c22 separate LMs 18.40 17.91All concatenated 18.02 17.55Offline linear 18.00 17.53Online linear 18.27 17.82Log-linear 18.15 17.70Table 7: Machine translation performance com-parison in an end-to-end system.jointly tuned normalized log-linear interpolationto future work.7 ConclusionNormalized log-linear interpolation is now atractable alternative to linear interpolation forbackoff language models.
Contrary to Hsu (2007),we proved that these models can be exactly col-lapsed into a single backoff language model.This solves the query speed problem.
Empiri-cally, compiling the log-linear model is faster thanSRILM can collapse its approximate offline linearmodel.
In future work, we plan to improve per-formace of feature weight tuning and investigatemore general features.AcknowledgmentsThanks to Jo?ao Sedoc, Grant Erdmann, JeremyGwinnup, Marcin Junczys-Dowmunt, Chris Dyer,Jon Clark, and MT Marathon attendees for discus-sions.
Partial funding was provided by the Ama-zon Academic Research Awards program.
Thismaterial is based upon work supported by the NSFGRFP under Grant Number DGE-1144245.ReferencesJacob Andreas and Dan Klein.
2015.
When and whyare log-linear models self-normalizing?
In NAACL2015.884Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 workshopon statistical machine translation.
In Proceedings ofthe Eighth Workshop on Statistical Machine Trans-lation, pages 1?44, Sofia, Bulgaria, August.
Associ-ation for Computational Linguistics.Ond?rej Bojar, Christian Buck, Rajen Chatterjee, Chris-tian Federmann, Liane Guillou, Barry Haddow,Matthias Huck, Antonio Jimeno Yepes, Aur?elieN?ev?eol, Mariana Neves, Pavel Pecina, Martin Popel,Philipp Koehn, Christof Monz, Matteo Negri, MattPost, Lucia Specia, Karin Verspoor, J?org Tiede-mann, and Marco Turchi.
2016.
Findings of the2016 Conference on Machine Translation.
In Pro-ceedings of the First Conference on Machine Trans-lation (WMT?16), Berlin, Germany, August.Stanley Chen and Joshua Goodman.
1998.
An em-pirical study of smoothing techniques for languagemodeling.
Technical Report TR-10-98, HarvardUniversity, August.Stanley F. Chen, Kristie Seymore, and Ronald Rosen-feld.
1998.
Topic adaptation for language modelingusing unnormalized exponential models.
In Acous-tics, Speech and Signal Processing, 1998.
Proceed-ings of the 1998 IEEE International Conference on,volume 2, pages 681?684.
IEEE.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models.
In Proceed-ings of Interspeech, Brisbane, Australia.Alexander Gutkin.
2000.
Log-linear interpolation oflanguage models.
Master?s thesis, University ofCambridge, November.Barry Haddow.
2013.
Applying pairwise ranked opti-misation to improve the interpolation of translationmodels.
In Proceedings of NAACL.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modi-fied Kneser-Ney language model estimation.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics, Sofia, Bulgaria,August.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, Edin-burgh, UK, July.
Association for Computational Lin-guistics.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1352?-1362, Edinburgh, Scotland, July.Bo-June Hsu and James Glass.
2008.
Iterative lan-guage model estimation: Efficient data structure &algorithms.
In Proceedings of Interspeech, Bris-bane, Australia.Bo-June Hsu.
2007.
Generalized linear interpolationof language models.
In Automatic Speech Recogni-tion & Understanding, 2007.
ASRU.
IEEE Workshopon, pages 136?140.
IEEE.Frederick Jelinek and Robert L. Mercer.
1980.
In-terpolated estimation of Markov source parametersfrom sparse data.
In Proceedings of the Workshopon Pattern Recognition in Practice, pages 381?397,May.Slava Katz.
1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recognizer.
IEEE Transactions on Acoustics,Speech, and Signal Processing, ASSP-35(3):400?401, March.Dietrich Klakow.
1998.
Log-linear interpolation oflanguage models.
In Proceedings of 5th Interna-tional Conference on Spoken Language Processing,pages 1695?1699, Sydney, November.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
InProceedings of the IEEE International Conferenceon Acoustics, Speech and Signal Processing, pages181?184.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Annual Meeting of the Association for Com-putational Linguistics (ACL), Prague, Czech Repub-lic, June.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof MT Summit.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English gigaword fifth edi-tion, June.
LDC2011T07.Abhinav Sethy, Stanley Chen, Bhuvana Ramabhadran,and Paul Vozila.
2014.
Static interpolation of ex-ponential n?gram models using features of features.In 2014 IEEE International Conference on Acoustic,Speech and Signal Processing (ICASSP).Andreas Stolcke, Jing Zheng, Wen Wang, and Vic-tor Abrash.
2011.
SRILM at sixteen: Updateand outlook.
In Proc.
2011 IEEE Workshop onAutomatic Speech Recognition & Understanding(ASRU), Waikoloa, Hawaii, USA.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the Sev-enth International Conference on Spoken LanguageProcessing, pages 901?904.David Talbot and Miles Osborne.
2007.
Randomisedlanguage modelling for statistical machine trans-lation.
In Proceedings of ACL, pages 512?519,Prague, Czech Republic.885J?org Tiedemann.
2009.
News from OPUS - A col-lection of multilingual parallel corpora with toolsand interfaces.
In N. Nicolov, K. Bontcheva,G.
Angelova, and R. Mitkov, editors, RecentAdvances in Natural Language Processing, vol-ume V, pages 237?248.
John Benjamins, Amster-dam/Philadelphia, Borovets, Bulgaria.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, andDavid Chiang.
2013.
Decoding with large-scaleneural language models improves translation.
InProceedings of EMNLP.Mitch Weintraub, Yaman Aksu, Satya Dharanipragada,Sanjeev Khudanpur, Hermann Ney, John Prange,Andreas Stolcke, Fred Jelinek, and Liz Shriberg.1996.
LM95 project report: Fast training andportability.
Research Note 1, Center for Languageand Speech Processing, Johns Hopkins University,February.Edward D. W. Whittaker and Dietrich Klakow.
2002.Efficient construction of long-range language mod-els using log-linear interpolation.
In 7th Interna-tional Conference on Spoken Language Processing,pages 905?908.Edward Whittaker and Bhiksha Raj.
2001.Quantization-based language model compres-sion.
In Proceedings of Eurospeech, pages 33?36,Aalborg, Denmark, December.Hui Zhang and David Chiang.
2014.
Kneser-Neysmoothing on expected counts.
In Proceedings ofthe 52nd Annual Meeting of the Association forComputational Linguistics, pages 765?774.
ACL.886
