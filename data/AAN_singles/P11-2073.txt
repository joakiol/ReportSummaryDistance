Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 418?423,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsImproving Decoding Generalization for Tree-to-String TranslationJingbo ZhuTong XiaoNatural Language Processing Laboratory Natural Language Processing LaboratoryNortheastern University, Shenyang, China Northeastern University, Shenyang, Chinazhujingbo@mail.neu.edu.cn xiaotong@mail.neu.edu.cnAbstractTo address the parse error issue for tree-to-string translation, this paper proposes asimilarity-based decoding generation (SDG)solution by reconstructing similar sourceparse trees for decoding at the decodingtime instead of taking multiple source parsetrees as input for decoding.
Experiments onChinese-English translation demonstratedthat our approach can achieve a significantimprovement over the standard method,and has little impact on decoding speed inpractice.
Our approach is very easy to im-plement, and can be applied to other para-digms such as tree-to-tree models.1 IntroductionAmong linguistically syntax-based statistical ma-chine translation (SMT) approaches, the tree-to-string model (Huang et al 2006; Liu et al 2006) isthe simplest and fastest, in which parse trees onsource side are used for grammar extraction anddecoding.
Formally, given a source (e.g., Chinese)string c and its auto-parsed tree T1-best, the goal oftypical tree-to-string SMT is to find a target (e.g.,English) string e* by the following equation as),|Pr(maxarg 1*besteTcee ?=                  (1)where Pr(e|c,T1-best) is the probability that e is thetranslation of the given source string c and its T1-best.A typical tree-to-string decoder aims to search forthe best derivation among all consistent derivationsthat convert source tree into a target-languagestring.
We call this set of consistent derivations thetree-to-string search space.
Each derivation in thesearch space respects the source parse tree.Parsing errors on source parse trees would causenegative effects on tree-to-string translation due todecoding on incorrect source parse trees.
To ad-dress the parse error issue in tree-to-string transla-tion, a natural solution is to use n-best parse treesinstead of 1-best parse tree as input for decoding,which can be expressed by),|Pr(maxarg* bestneTcee ?=              (2)where <Tn-best> denotes a set of n-best parse treesof c produced by a state-of-the-art syntactic parser.A simple alternative (Xiao et al 2010) to generate<Tn-best> is to utilize multiple parsers, which canimprove the diversity among source parse trees in<Tn-best>.
In this solution, the most representativework is the forest-based translation method (Mi etal.
2008; Mi and Huang 2008; Zhang et al 2009)in which a packed forest (forest for short) structureis used to effectively represent <Tn-best> for decod-ing.
Forest-based approaches can increase the tree-to-string search space for decoding, but face a non-trivial problem of high decoding time complexityin practice.In this paper, we propose a new solution by re-constructing new similar source parse trees for de-coding, referred to as similarity-based decodinggeneration (SDG), which is expressed as}),{,|Pr(maxarg),|Pr(maxarg11*simbestebesteTTceTcee??
?=(3)where <Tsim> denotes a set of similar parse trees ofT1-best that are dynamically reconstructed at the de-418coding time.
Roughly speaking, <Tn-best> is a sub-set of {T1-best, <Tsim>}.
Along this line of thinking,Equation (2) can be considered as a special case ofEquation (3).In our SDG solution, given a source parse treeT1-best, the key is how to generate its <Tsim> at thedecoding time.
In practice, it is almost intractableto directly reconstructing <Tsim> in advance as in-put for decoding due to too high computation com-plexity.
To address this crucial challenge, thispaper presents a simple and effective techniquebased on similarity-based matching constraints toconstruct new similar source parse trees for decod-ing at the decoding time.
Our SDG approach canexplicitly increase the tree-to-string search spacefor decoding without changing any grammar ex-traction and pruning settings, and has little impacton decoding speed in practice.2 Tree-to-String DerivationWe choose the tree-to-string paradigm in our studybecause this is the simplest and fastest among syn-tax-based models, and has been shown to be one ofthe state-of-the-art syntax-based models.
Typically,by using the GHKM algorithm (Galley et al 2004),translation rules are learned from word-alignedbilingual texts whose source side has been parsedby using a syntactic parser.
Each rule consists of asyntax tree in the source language having somewords (terminals) or variables (nonterminals) atleaves, and sequence words or variables in the tar-get language.
With the help of these learned trans-lation rules, the goal of tree-to-string decoding is tosearch for the best derivation that converts thesource tree into a target-language string.
A deriva-tion is a sequence of translation steps (i.e., the useof translation rules).Figure 1 shows an example derivation d thatperforms translation over a Chinese source parsetree, and how this process works.
In the first step,we can apply rule r1 at the root node that matches asubtree {IP[1] (NP[2] VP[3])}.
The correspondingtarget side {x1 x2} means to preserve the top-levelword-order in the translation, and results in twounfinished subtrees with root labels NP[2] and VP[3],respectively.
The rule r2 finishes the translation onthe subtree of NP[2], in which the Chinese word????
is translated into an English string ?theChinese side?.
The rule r3 is applied to performtranslation on the subtree of VP[3], and results in anAn example tree-to-string derivation d consisting of fivetranslation rules is given as follows:r1: IP[1] (x1:NP[2] x2:VP[3]) ?
x1 x2r2: NP[2] (NN (??))
?
the Chinese sider3: VP[3] (ADVP(AD(??))
VP(VV(??)
AS(?
)x1:NP[4])) ?
highly appreciated x1r4: NP[4] (DP(DT(?)
CLP(M(?)))
x1:NP[5]) ?
this x1r5: NP[5] (NN(??))
?
talkTranslation results: The Chinese side highly appreciatedthis talk.Figure 1.
An example derivation performs translationover the Chinese parse tree T.unfinished subtree of NP[4].
Similarly, rules r4 andr5 are sequentially used to finish the translation onthe remaining.
This process is a depth-first searchover the whole source tree, and visits every nodeonly once.3 Decoding Generalization3.1 Similarity-based Matching ConstraintsIn typical tree-to-string decoding, an ordered se-quence of rules can be reassembled to form a deri-vation d whose source side matches the givensource parse tree T. The source side of each rule ind should match one of subtrees of T, referred to asmatching constraint.
Before discussing how to ap-ply our similarity-based matching constraints toreconstruct new similar source parse trees for de-coding at the decoding time, we first define thesimilarity between two tree-to-string rules.Definition 1 Given two tree-to-string rules t and u,we say that t and u are similar such that theirsource sides ts and us have the same root label andfrontier nodes, written as ut ?
, otherwise not.419Figure 2: Two similar tree-to-string rules.
(a) rule r3used by the example derivation d in Figure 1, and (b) asimilar rule ?3 of r3.Here we use an example figure to explain oursimilarity-based matching constraint scheme (simi-larity-based scheme for short).Figure 3: (a) a typical tree-to-string derivation d usingrule t, and (b) a new derivation d* is generated by thesimilarity-based matching constraint scheme by usingrule t* instead of rule t, where t* t?
.Given a source-language parse tree T, in typicaltree-to-string matching constraint scheme shown inFigure 3(a), rule t used by the derivation d shouldmatch a substree ABC of T. In our similarity-basedscheme, the similar rule t* ( t? )
is used to form anew derivation d* that performs translation overthe same source sentence {w1 ... wn}.
In such a case,this new derivation d* can yield a new similarparse tree T* of T.Since an incorrect source parse tree might filterout good derivations during tree-to-string decoding,our similarity-based scheme is much more likely torecover the correct tree for decoding at the decod-ing time, and does not rule out good (potentiallycorrect) translation choices.
In our method, manynew source-language trees T* that are similar to butdifferent from the original source tree T can be re-constructed at the decoding time.
In theory oursimilarity-based scheme can increase the searchspace of the tree-to-string decoder, but we did notchange any rule extraction and pruning settings.In practice, our similarity-based scheme can ef-fectively keep the advantage of fast decoding fortree-to-string translation because its implementa-tion is very simple.
Let?s revisit the example deri-vation d in Figure 1, i.e., d=r1?r2?r3?r4?r51.
Insuch a case, the decoder can effectively produce anew derivation d* by simply replacing rule r3 withits similar rule ?3 ( 3r? )
shown in Figure 2, that is,d*=r1?r2?
?3?r4?r5.With beam search, typical tree-to-string decod-ing with an integrated language model can run intime2 O(ncb2) in practice (Huang 2007).
For ourdecoding time complexity computation, only theparameter c value can be affected by our similar-ity-based scheme.
In other words, our similarity-based scheme would result in a larger c value atdecoding time as many similar translation rulesmight be matched at each node.
In practice, thereare two feasible optimization techniques to allevi-ate this problem.
The first technique is to limit themaximum number of similar translation rulesmatched at each node.
The second one is to prede-fine a similarity threshold to filter out less similartranslation rules in advance.In the implementation, we add a new featureinto the model: similarity-based matching countingfeature.
This feature counts the number of similarrules used to form the derivation.
The weight ?simof this feature is tuned via minimal error rate train-ing (MERT) (Och 2003) with other feature weights.3.2 Pseudo-rule GenerationIn the implementation of tree-to-string decoding,the first step is to load all translation rules matchedat each node of the source tree T. It is possible thatsome nonterminal nodes do not have any matchedrules when decoding some new sentences.
If theroot node of the source tree has no any matchedrules, it would cause decoding failure.
To tacklethis problem, motivated by ?glue?
rules (Chiang2005), for some node S without any matched rules,we introduce a special pseudo-rule which reassem-bles all child nodes with local reordering to formnew translation rules for S to complete decoding.1 The symbol?denotes the composition (leftmost substitution)operation of two tree-to-string rules.2 Where n is the number of words, b is the size of the beam,and c is the number of translation rules matched at each node.420S                S(x1:A x2:B x3:C x4:D)?x1 x2 x3 x4S(x1:A x2:B x3:C x4:D)?x2 x1 x3 x4S(x1:A x2:B x3:C x4:D)?x1 x3 x2 x4A     B     C     D   S(x1:A x2:B x3:C x4:D)?x1 x2 x4 x3(a)                                         (b)Figure 4: (a) An example unseen substree, and (b) itsfour pseudo-rules.Figure 4 (a) depicts an example unseen substreewhere no any rules is matched at its root node S.Its simplest pseudo-rule is to simply combine asequence of S?s child nodes.
To give the modelmore options to build partial translations, we util-ize a local reordering technique in which any twoadjacent frontier (child) nodes are reordered duringdecoding.
Figure 4(b) shows four pseudo-rules intotal generated from this example unseen substree.In the implementation, we add a new feature tothe model: pseudo-rule counting feature.
This fea-ture counts the number of pseudo-rules used toform the derivation.
The weight ?pseudo of this fea-ture is tuned via MERT with other feature weights.4 Evaluation4.1 SetupOur bilingual training data consists of 140K Chi-nese-English sentence pairs in the FBIS data set.For rule extraction, the minimal GHKM rules (Gal-ley et al 2004) were extracted from the bitext, andthe composed rules were generated by combiningtwo or three minimal GHKM rules.
A 5-gram lan-guage model was trained on the target-side of thebilingual data and the Xinhua portion of EnglishGigaword corpus.
The beam size for beam searchwas set to 20.
The base feature set used for all sys-tems is similar to that used in (Marcu et al 2006),including 14 base features in total such as 5-gramlanguage model, bidirectional lexical and phrase-based translation probabilities.
All features werelinearly combined and their weights are optimizedby using MERT.
The development data set usedfor weight training in our approaches comes fromNIST MT03 evaluation set.
To speed up MERT,sentences with more than 20 words were removedfrom the development set (Dev set).
The test setsare the NIST MT04 and MT05 evaluation sets.
Thetranslation quality was evaluated in terms of case-insensitive NIST version BLEU metric.
Statisticalsignificance test was conducted by using the boot-strap re-sampling method (Koehn 2004).4.2 ResultsMT04 MT05  DEVMT03 <=20 ALL <=20 ALLBaseline 32.99 36.54 32.70 34.61 30.60Thiswork34.67*(+1.68)36.99+(+0.45)35.03*(+2.33)35.16+(+0.55)33.12*(+2.52)Table 1.
BLEU4 (%) scores of various methods on Devset (MT03) and two test sets (MT04 and MT05).
Eachsmall test set (<=20) was built by removing the sen-tences with more than 20 words from the full set (ALL).+ and * indicate significantly better on performancecomparison at p < .05 and p < .01, respectively.Table 1 depicts the BLEU scores of various meth-ods on the Dev set and four test sets.
Compared totypical tree-to-string decoding (the baseline), ourmethod can achieve significant improvements onall datasets.
It is noteworthy that the improvementachieved by our approach on full test sets is biggerthan that on small test sets.
For example, ourmethod results in an improvement of 2.52 BLEUpoints over the baseline on the MT05 full test set,but only 0.55 points on the MT05 small test set.
Asmentioned before, tree-to-string approaches aremore vulnerable to parsing errors.
In practice, theBerkeley parser (Petrov et al 2006) we used yieldsunsatisfactory parsing performance on some longsentences in the full test sets.
In such a case, itwould result in negative effects on the performanceof the baseline method on the full test sets.
Ex-perimental results show that our SDG approachcan effectively alleviate this problem, and signifi-cantly improve tree-to-string translation.Another issue we are interested in is the decod-ing speed of our method in practice.
To investigatethis issue, we evaluate the average decoding speedof our SDG method and the baseline on the Dev setand all test sets.Decoding Time(seconds per sentence)<=20 ALLBaseline 0.43s 1.1sThis work 0.50s 1.3sTable 2.
Average decoding speed of various methods onsmall (<=20) and full (ALL) datasets in terms of sec-onds per sentence.
The parsing time of each sentence isnot included.
The decoders were implemented in C++codes on an X86-based PC with two processors of2.4GHZ and 4GB physical memory.421Table 2 shows that our approach only has littleimpact on decoding speed in practice, compared tothe typical tree-to-string decoding (baseline).
No-tice that in these comparisons our method did notadopt any optimization techniques mentioned inSection 3.1, e.g., to limit the maximum number ofsimilar rules matched at each node.
It is obviouslythat the use of such an optimization technique caneffectively increase the decoding speed of ourmethod, but might hurt the performance in practice.Besides, to speed up decoding long sentences, itseems a feasible solution to first divide a long sen-tence into multiple short sub-sentences for decod-ing, e.g., based on comma.
In other words, we cansegment a complex source-language parse tree intomultiple smaller subtrees for decoding, and com-bine the translations of these small subtrees to formthe final translation.
This practical solution canspeed up the decoding on long sentences in real-world MT applications, but might hurt the transla-tion performance.For convenience, here we call the rule ?3 in Fig-ure 2(b) similar-rules.
It is worth investigating howmany similar-rules and pseudo-rules are used toform the best derivations in our similarity-basedscheme.
To do it, we count the number of similar-rules and pseudo-rules used to form the best deri-vations when decoding on the MT05 full set.
Ex-perimental results show that on average 13.97% ofrules used to form the best derivations are similar-rules, and one pseudo-rule per sentence is used.Roughly speaking, average five similar-rules persentence are utilized for decoding generalization.5 Related WorkString-to-tree SMT approaches also utilize thesimilarity-based matching constraint on target sideto generate target translation.
This paper applies iton source side to reconstruct new similar sourceparse trees for decoding at the decoding time,which aims to increase the tree-to-string searchspace for decoding, and improve decoding gener-alization for tree-to-string translation.The most related work is the forest-based trans-lation method (Mi et al 2008; Mi and Huang 2008;Zhang et al 2009) in which rule extraction anddecoding are implemented over k-best parse trees(e.g., in the form of packed forest) instead of onebest tree as translation input.
Liu and Liu (2010)proposed a joint parsing and translation model bycasting tree-based translation as parsing (Eisner2003), in which the decoder does not respect thesource tree.
These methods can increase the tree-to-string search space.
However, the decoding timecomplexity of their methods is high, i.e., more thanten or several dozen times slower than typical tree-to-string decoding (Liu and Liu 2010).Some previous efforts utilized the techniques ofsoft syntactic constraints to increase the searchspace in hierarchical phrase-based models (Martonand Resnik 2008; Chiang et al 2009; Huang et al2010), string-to-tree models (Venugopal et al2009) or tree-to-tree (Chiang 2010) systems.
Thesemethods focus on softening matching constraintson the root label of each rule regardless of its in-ternal tree structure, and often generate many newsyntactic categories3.
It makes them more difficultto satisfy syntactic constraints for the tree-to-stringdecoding.6 Conclusion and Future WorkThis paper addresses the parse error issue for tree-to-string translation, and proposes a similarity-based decoding generation solution by reconstruct-ing new similar source parse trees for decoding atthe decoding time.
It is noteworthy that our SDGapproach is very easy to implement.
In principle,forest-based and tree sequence-based approachesimprove rule coverage by changing the rule extrac-tion settings, and use exact tree-to-string matchingconstraints for decoding.
Since our SDG approachis independent of any rule extraction and pruningtechniques, it is also applicable to forest-based ap-proaches or other tree-based translation models,e.g., in the case of casting tree-to-tree translation astree parsing (Eisner 2003).AcknowledgmentsWe would like to thank Feiliang Ren, Muhua Zhuand Hao Zhang for discussions and the anonymousreviewers for comments.
This research was sup-ported in part by the National Science Foundationof China (60873091; 61073140), the SpecializedResearch Fund for the Doctoral Program of HigherEducation (20100042110031) and the FundamentalResearch Funds for the Central Universities inChina.3 Latent syntactic categories were introduced in the method ofHuang et al (2010).422ReferencesChiang David.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
ofACL2005, pp263-270Chiang David.
2010.
Learning to translate with sourceand target syntax.
In Proc.
of ACL2010, pp1443-1452Chiang David, Kevin Knight and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In Proc.
of NAACL2009, pp218-226Eisner Jason.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proc.
of ACL 2003,pp205-208.Galley Michel, Mark Hopkins, Kevin Knight and DanielMarcu.
2004.
What's in a translation rule?
In Proc.
ofHLT-NAACL 2004, pp273-280.Huang Liang.
2007.
Binarization, synchronous binariza-tion and target-side binarization.
In Proc.
of NAACLWorkshop on Syntax and Structure in StatisticalTranslation.Huang Liang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProc.
of ACL 2007, pp144-151.Huang Liang, Kevin Knight and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proc.
of AMTA 2006, pp66-73.Huang Zhongqiang, Martin Cmejrek and Bowen Zhou.2010.
Soft syntactic constraints for hierarchicalphrase-based translation using latent syntactic distri-bution.
In Proc.
of EMNLP2010, pp138-147Koehn Philipp.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proc.
of EMNLP2004, pp388-395.Liu Yang and Qun Liu.
2010.
Joint parsing and transla-tion.
In Proc.
of Coling2010, pp707-715Liu Yang, Qun Liu and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
of COLING/ACL 2006, pp609-616.Marcu Daniel, Wei Wang, Abdessamad Echihabi andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In Proc.
of EMNLP 2006, pp44-52.Marton Yuval and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrase-based translation.In Proc.
of ACL08, pp1003-1011Mi Haitao and Liang Huang.
2008.
Forest-based Trans-lation Rule Extraction.
In Proc.
of EMNLP 2008,pp206-214.Mi Haitao, Liang Huang and Qun Liu.
2008.
Forest-based translation.
In Proc.
of ACL2008.Och Franz Josef.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL2003.Petrov Slav, Leon Barrett, Roman Thibaux and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
of ACL2006,pp433-440.Xiao Tong, Jingbo Zhu, Hao Zhang and Muhua Zhu.2010.
An empirical study of translation rule extrac-tion with multiple parsers.
In Proc.
of Coling2010,pp1345-1353Venugopal Ashish, Andreas Zollmann, Noah A. Smithand Stephan Vogel.
2009.
Preference grammars: sof-tening syntactic constraints to improve statistical ma-chine translation.
In Proc.
of NAACL2009, pp236-244Zhang Hui, Min Zhang, Haizhou Li, Aiti Aw and ChewLim Tan.
2009.
Forest-based tree sequence to stringtranslation model.
In Proc.
of ACL-IJCNLP2009,pp172-180423
