Joint Incremental Disfluency Detection and Dependency ParsingMatthew HonnibalDepartment of ComputingMacquarie UniversitySydney, Australiamatthew.honnibal@mq.edu.edu.auMark JohnsonDepartment of ComputingMacquarie UniversitySydney, Australiamark.johnson@mq.edu.edu.auAbstractWe present an incremental dependencyparsing model that jointly performs disflu-ency detection.
The model handles speechrepairs using a novel non-monotonic tran-sition system, and includes several novelclasses of features.
For comparison,we evaluated two pipeline systems, us-ing state-of-the-art disfluency detectors.The joint model performed better on bothtasks, with a parse accuracy of 90.5% and84.0% accuracy at disfluency detection.The model runs in expected linear time,and processes over 550 tokens a second.1 IntroductionMost unscripted speech contains filled pauses(ums and uhs), and errors that are usually editedon-the-fly by the speaker.
Disfluency detection isthe task of detecting these infelicities in spokenlanguage transcripts.
The task has some imme-diate value, as disfluencies have been shown tomake speech recognition output much more dif-ficult to read (Jones et al., 2003), but has alsobeen motivated as a module in a natural languageunderstanding pipeline, because disfluencies haveproven problematic for PCFG parsing models.Instead of a pipeline approach, we build on re-cent work in transition-based dependency parsing,to perform the two tasks jointly.
There have beentwo small studies of dependency parsing on un-scripted speech, both using entirely greedy pars-ing strategies, without a direct comparison againsta pipeline architecture (Jorgensen, 2007; Rasooliand Tetreault, 2013).
We go substantially beyondthese pilot studies, and present a system that com-pares favourably to a pipeline consisting of state-of-the-art components.
Our parser largely followsthe design of Zhang and Clark (2011).
We use astructured averaged perceptron model with beam-search decoding (Collins, 2002).
Our feature setis based on Zhang and Clark (2011), and ourtransition-system is based on the arc-eager systemof Nivre (2003).We extend the transition system with a novelnon-monotonic transition, Edit.
It allows sen-tences like ?Pass the pepper uh salt?
to be parsedincrementally, without the need to guess earlythat pepper is disfluent.
This is achieved by re-processing the leftward children of the word Editmarks as disfluent.
For instance, if the parser at-taches the to pepper, but subsequently marks pep-per as disfluent, the will be returned to the stack.We also exploit the ease with which the model canincorporate arbitrary features, and design a set offeatures that capture the ?rough copy?
structure ofsome speech repairs, which motivated the Johnsonand Charniak (2004) noisy channel model.Our main comparison is against two pipelinesystems, which use the two current state-of-the-art disfluency detection systems as pre-processorsto our parser, minus the custom disfluency fea-tures and transition.
The joint model comparedfavourably to the pipeline parsers at both tasks,with an unlabelled attachment score of 90.5%, and84.0% accuracy at detecting speech repairs.
An ef-ficient implementation is available under an open-source license.1 The future prospects of the sys-tem are also quite promising.
Because the parseris incremental, it should be well suited to un-segmented text such as the output of a speech-recognition system.
We consider our main con-tributions to be:?
a novel non-monotonic transition system, forspeech repairs and restarts,1http://github.com/syllog1sm/redshift131Transactions of the Association for Computational Linguistics, 2 (2014) 131?142.
Action Editor: Joakim Nivre.Submitted 11/2013; Revised 2/2014; Published 4/2014.
c?2014 Association for Computational Linguistics.A flight to um????FPBoston?
??
?RMI mean?
??
?IMDenver?
??
?RPTuesdayFigure 1: A sentence with disfluencies annotated inthe style of Shriberg (1994) and the Switchboard cor-pus.
FP=Filled Pause, RM=Reparandum, IM=Interregnum,RP=Repair.
We follow previous work in evaluating the sys-tem on the accuracy with which it identifies speech-repairs,marked reparandum above.?
several novel feature classes,?
direct comparison against the two best disflu-ency pre-processors, and?
state-of-the-art accuracy for both speechparsing and disfluency detection.2 Switchboard Disfluency AnnotationsThe Switchboard portion of the Penn Treebank(Marcus et al., 1993) consists of telephone conver-sations between strangers about an assigned topic.Two annotation layers are provided: one for syn-tactic bracketing (MRG files), and one for disflu-encies (DPS files).
The disfluency layer marks el-ements with little or no syntactic function, such asfilled pauses and discourse markers, and annotatesspeech repairs using the Shriberg (1994) systemof reparandum/interregnum/repair.
An example isshown in Figure 1.In the syntactic annotation, edited words arecovered by a special node labelled EDITED.
Theidea is to mark text which, if excised, would re-sult in a grammatical sentence.
The MRG files donot mark other types of disfluencies.
We followthe evaluation defined by Charniak and Johnson(2001), which evaluates the accuracy of identify-ing speech repairs and restarts.
This definition ofthe task is the standard in recent work.
The reasonfor this is that filled pauses can be detected usinga simple rule-based approach, and parentheticalshave less impact on readability and down-streamprocessing accuracy.The MRG and DPS layers have high but im-perfect agreement over what tokens they mark asspeech repairs: of the text annotated with both lay-ers, 33,720 tokens are marked as disfluent in atleast one layer, 32,310 are only marked as disflu-ent by the DPS files, and 32,742 are only markedas disfluent by the MRG layer.The Switchboard annotation project was notfully completed.
Because disfluency annotation ischeaper to produce, many of the DPS training filesdo not have matching MRG files.
Only 619,236of the 1,482,845 tokens in the DPS disfluency-detection training data have gold-standard syntac-tic parses.
Our system requires the more expen-sive syntactic annotation, but we find that it out-performs the previous state-of-the-art (Qian andLiu, 2013), despite training on less than half thedata.2.1 Dependency ConversionAs is standard in statistical dependency parsingof English, we acquire our gold-standard depen-dencies from phrase-structure trees.
We used the2013-04-05 version of the Stanford dependencyconverter (de Marneffe et al., 2006).
As is standardfor English dependency parsing, we use the Ba-sic Dependencies scheme, which produces strictlyprojective representations.At first we feared that the filled pauses, disfluen-cies and meta-data tokens in the Switchboard cor-pus might disrupt the conversion process, by mak-ing it more difficult for the converter to recognisethe underlying production rules.To test this, we performed a small experiment.We prepared two versions of the corpus: onewhere EDITED nodes, filled pauses and meta-datawere removed before the trees were transformedby the Stanford converter, and one where the dis-fluency removal was performed after the depen-dency conversion.
The resulting corpora werelargely identical: 99.54% of unlabelled and 98.7%of labelled dependencies were the same.
The factthat the Stanford converter is quite robust to dis-fluencies was useful for our baseline joint model,which is trained on dependency trees that also in-cluded governors for disfluent words.We follow previous work on disfluency detec-tion by lower-casing the text and removing punc-tuation and partial words (words tagged XX andwords ending in ?-?).
We also remove one-tokensentences, as their syntactic analyses are trivial.We found that two additional simple pre-processesimproved our results: discarding all ?um?
and ?uh?tokens; and merging ?you know?
and ?i mean?
intosingle tokens.These pre-processes can be completed on the in-put string without losing information: none of the?um?
or ?uh?
tokens are semantically significant,and the bigrams you know and i mean have a de-pendency between the two tokens over 99.9% ofthe times they occur in the treebank, with you andI never having any children.
This makes it easyto unmerge the tokens deterministically after pars-132ing: all incoming and outgoing arcs will point toknow or mean.
The same pre-processing was per-formed for all our parsing systems.3 Transition-based Dependency ParsingA transition-based parser predicts the syntacticstructure of a sentence incrementally, by makinga sequence of classification decisions.
We followthe architecture of Zhang and Clark (2011), whouse beam-search for decoding, and a structured av-eraged perceptron for training.
Despite its simplic-ity, this type of parser has produced highly com-petitive results on the Wall Street Journal: with theextended feature set described by Zhang and Nivre(2011), it achieves 93.5% unlabelled accuracy onStanford basic dependencies (de Marneffe et al.,2006).
Converting the constituency trees producedby the Charniak and Johnson (2005) rerankingparser results in similar accuracy.Briefly, the transition-based parser consists of aconfiguration (or ?state?)
which is sequentially ma-nipulated by a set of possible transitions.
For us, astate is a 4-tuple c = (?, ?,A,D), where ?
and ?are disjoint sets of word indices termed the stackand buffer respectively, A is the set of dependencyarcs, and D is the set of word indices marked dis-fluent.
There are no arcs to or from members ofD,so the dependencies and disfluencies can be imple-mented as a single vector (in our parser, a token ismarked as disfluent by setting it as its own head).We use the arc-eager transition system (Nivre,2003, 2008), which consists of four parsing ac-tions: Shift, Left-Arc, Right-Arc and Reduce.
Wedenote the stack with its topmost element to theright, and the buffer with its first element to theleft.
A vertical bar is used to indicate concate-nation to the stack or buffer, e.g.
?|i indicates astack with the topmost element i and remainingelements ?.
A dependency from a governor i toa child j is denoted i ?
j.
The four arc-eagertransitions are shown in Figure 2.The Shift action moves the first item of thebuffer onto the stack.
The Right-Arc does thesame, but also adds an arc, so that the top twoitems on the stack are connected.
The Reducemove and the Left-Arc both pop the stack, but theLeft-Arc first adds an arc from the first word ofthe buffer to the word on top of the stack.
Con-straints on the Reduce and Left-Arc moves ensurethat every word is assigned exactly one head inthe final configuration.
We follow the suggestion(?, i|?,A,D) ` (?|i, ?, A,D) S(?|i, j|?,A,D) ` (?, j|?,A ?
{j ?
i}, D) LOnly if i does not have an incoming arc.
(?|i, j|?,A,D) ` (?|i|j, ?,A ?
{i?
j}, D) R(?|i, ?, A,D) ` (?, ?,A,D) DOnly if i has an incoming arc.
(?|i, j|?,A,D) ` (?|[x1, xn], j|?,A?, D?)
EWhereA?
= A \ {x?
y or y ?
x : ?x ?
[i, j), ?y ?
N}D?
= D ?
[i, j)x1...xn are the former left children of iFigure 2: Our parser?s transition system.
The first fourtransitions are the standard arc-eager system; the fifth is ournovel Edit transition.of Ballesteros and Nivre (2013) and add a dummytoken that governs root dependencies to the end ofthe sentence.
Parsing terminates when this tokenis at the start of the buffer, and the stack is empty.Disfluencies are added toD via the Edit transition,E, which we now define.4 A Non-Monotonic Edit TransitionOne of the reasons disfluent sentences are hard toparse is that there often appear to be syntactic re-lationships between words in the reparandum andthe fluent sentence.
When these relations are con-sidered in addition to the dependencies betweenfluent words, the resulting structure is not neces-sarily a projective tree.Figure 3 shows a simple example, where the re-pair square replaces the reparandum rectangle.
Anincremental parser could easily become ?garden-pathed?
and attach the repair square to the preced-ing words, constructing the dependencies showndotted in Figure 3.
Rather than attempting to de-vise an incremental model that avoids construct-ing such dependencies, we allow the parser to con-struct these dependencies and later delete them ifthe governor or child are marked disfluent.Psycholinguistic models of human sentenceprocessing have long posited repair mechanisms(Frazier and Rayner, 1982).
Recently, Honnibalet al.
(2013) showed that a limited amount of ?non-monotonic?
behaviour can improve an incremen-tal parser?s accuracy.
We here introduce a non-monotonic transition, Edit, for speech repairs.The Edit transition marks the word i on topof the stack ?|i as disfluent, along with its right-ward descendents ?
i.e., all words in the sequencei...j ?
1, where j is the word at the start of thebuffer.
It then restores the words both precedingand formerly governed by i to the stack.In other words, the word on top of the stack and133Pass me the red rectangle uh I mean squareFigure 3: Example where apparent dependencies betweenthe reparandum and the fluent sentence complicate parsing.The dotted edges are difficult for an incremental parser toavoid, but cannot be part of the final parse if it is to be aprojective tree.
Our solution is to make the transition systemnon-monotonic: the parser is able to delete edges.its rightward descendents are all marked as dis-fluent, and the stack is popped.
We then restoreits leftward children to the stack, and all depen-dencies to and from words marked disfluent aredeleted.
The transition is non-monotonic in thesense that it can delete dependencies created bya previous transition, and replace tokens onto thestack that had been popped.Why revisit the leftward children, but not theright?
We are concerned about dependencieswhich might be mirrored between the reparandumand the repair.
The rightward subtree of the disflu-ency might well be incorrect, but if it is, it wouldstill be incorrect if the word on top of the stackwere actually fluent.
We therefore regard theseas parsing errors that we will train our model toavoid.
In contrast, avoiding the Left-Arc transi-tions would require the parser to predict that thehead is disfluent when it has not necessarily seenany evidence indicating that.4.1 Worked ExampleFigure 4 shows a gold-standard derivation fora disfluent sentence from the development data.Line 1 shows the state resulting from the initialShift action.
In the next three states, His is Left-Arced to company, which is then Shifted onto thestack, and Left-Arced to went in Line 4.The dependency between went and company isnot part of the gold-standard, because went is dis-fluent.
The correct governor of company is the sec-ond went in the sentence.
The Left-Arc move inLine 4 can still be considered correct, however, be-cause the gold-standard analysis is still derivablefrom the resulting configuration, via the Edit tran-sition.
Another non-gold dependency is created inLine 6, between broke and went, before broke isReduced from the stack in Line 7.Lines 9 and 10 show the states before and afterthe Edit transition.
The word on top of the stack inLine 9, went, has one leftward child, and one right-1.
S His company went broke i mean went bankrupt2.
L His company went broke i mean went bankrupt3.
S His company went broke i mean went bankrupt4.
L His company went broke i mean went bankrupt5.
S His company went broke i mean went bankrupt6.
R His company went broke i mean went bankrupt7.
D His company went broke i mean went bankrupt8.
S His company went broke i mean went bankrupt9.
L His company went broke i mean went bankrupt10.
E His company went broke i mean went bankrupt11.
L His company went broke i mean went bankrupt12.
S His company went broke i mean went bankrupt12.
R His company went broke i mean went bankrupt13.
D His company went broke i mean went bankruptFigure 4: A gold-standard transition sequence using ourEDIT transition.
Each line specifies an action and shows thestate resulting from it.
Words on the stack are circled, andthe arrow indicates the start of the buffer.
Disfluent words arestruck-through.ward child.
After the Edit transition is applied,went and its rightward child broke are both markeddisfluent, and company is returned to the stack.
Allof the previous dependencies to and from went andbroke are deleted.Parsing then proceeds as normal, with the cor-rect governor of company being assigned by theLeft-Arc in Line 11, and bankrupt being Right-Arced to went in Line 12.
To conserve space, wehave omitted the dummy ROOT token, which isplaced at the end of the sentence, following thesuggestion of Ballesteros and Nivre (2013).
Thefinal action will be a Left-Arc from the ROOT to-ken to went.4.2 Dynamic Oracle Training AlgorithmOur non-monotonic transition system introducessubstantial spurious ambiguity: the gold-standardparse can be derived via many different transition134sequences.
Recent work has shown that this canbe advantageous (Sartorio et al., 2013; Honnibalet al., 2013; Goldberg and Nivre, 2012), becausedifficult decisions can sometimes be delayed untilmore information is available.Line 5 of Figure 4 shows a state that introducesspurious ambiguity.
From this configuration, thereare multiple actions that could be considered ?cor-rect?, in the sense that the gold-standard analysiscan be derived from them.
The Edit transition iscorrect because went is disfluent, but the Left-Arcand even the Right-Arc are also correct, in thatthere are continuations from them that lead to thegold-standard analysis.We regard all transition sequences that can re-sult in the correct analysis as equally valid, andwant to avoid stipulating one of them during train-ing.
We achieve this by following Goldberg andNivre (2012) in using a dynamic oracle to createpartially labelled training data.2 A dynamic oracleis a function that determines the cost of applyingan action to a state, in terms of gold-standard arcsthat are newly unreachable.We follow Collins (2002) in training an aver-aged perceptron model to predict transition se-quences, rather than individual transitions.
Thistype of model is often referred to as a struc-tured perceptron, or sometimes a global percep-tron.
During training, if the model does not pre-dict the correct sequence, an update is performed,based on the gold-standard sequence and part ofthe sequence predicted by the current weights.Only part of the sequence is used to calculate theweight update, in order to account for search er-rors.
We use the maximum violation strategy de-scribed by Huang et al.
(2012) to select the subse-quence to update from.To train our model using the dynamic oracle,we use the latent-variable structured perceptron al-gorithm described by Sun et al.
(2009).
Beam-search is performed to find the highest-scoringgold-standard sequence, as well as the highest-scoring prediction.
We use the same beam-widthfor both search procedures.4.3 Path Length NormalisationOne problem introduced by the Edit transition isthat the number of actions applied to a sentence is2 The training data is partially labelled in the sense that in-stances can have multiple true labels.
Equivalently, one mightsay that the transitions are latent variables, which generate thedependencies.no longer constant ?
it is no longer guaranteed tobe 2n ?
1, for a sentence of length n. When theEdit transition is applied to a word with leftwardchildren, those children are returned to the stack,and processed again.
This has little to no impacton the algorithm?s empirical efficiency, althoughworst-case complexity is no longer linear, but itdoes pose a problem for decoding.The perceptron model tends to assign large pos-itive scores to its top prediction.
We thus ob-served a problem when comparing paths of differ-ent lengths, at the end of the sentence.
Paths thatincluded Edit transitions were longer, so the sumof their scores tended to be higher.The same problem has been observed duringincremental PCFG parsing, by Zhu et al.
(2013).They introduce an additional transition, IDLE, toensure that paths are the same length.
So long asone candidate in the beam is still being processed,all other candidates apply the IDLE transition.We adopt a simpler solution.
We normalise thefigure-of-merit for a candidate state, which is usedto rank it in the beam, by the length of its transitionhistory.
The new figure-of-merit is the arithmeticmean of the candidate?s transition scores, wherepreviously the figure-of-merit was the sum of thecandidate?s transition scores.Interestingly, Zhu et al.
(2013) report that theytried exactly this, and that it was less effective thantheir solution.
We found that the features associ-ated with the IDLE transition were uninformative(the state is at termination, so the stack and bufferare empty), and had nothing to do with how manyedit transitions were earlier applied.5 Features for the Joint ParserOur baseline parser uses the feature set describedby Zhang and Nivre (2011).
The feature set con-tains 73 templates that mostly refer to the prop-erties of 12 context tokens: the top of the stack(S0), its two leftmost and rightmost children (S0L,S0L2, S0R, S0R2), its parent and grand-parent(S0h, S0h2), the first word of the buffer and its twoleftmost children (N0, N0L, N0LL), and the nexttwo words of the buffer (N1, N2).Atomic features consist of the word, part-of-speech tag, or dependency label for these tokens;and multiple feature atoms are often combined forfeature templates.
There are also features for thestring-distance between S0 and N0, and the leftand right valencies (total number of children) of135S0 and N0, as well as the set of their children?s de-pendency labels.
We restrict these to the first andlast 2 children for implementation efficiency, aswe found this had no effect on accuracy.
Numericfeatures (for distance and valency) are binned withthe function ?x : min(x, 5).
There is only one bi-lexical feature template, which pairs the words ofS0 and N0.
There are also ten tri-tag templates.Our feature set includes additional dependencylabel features not used by Zhang and Nivre (2011),as we found that disfluency detection errors oftenresulted in ungrammatical dependency label com-binations.
The additional templates combine thePOS tag of S0 with two or three dependency la-bels from its left and right subtrees.
Details can befound in the supplementary material.5.1 Brown Cluster FeaturesThe Brown clustering algorithm (Brown et al.,1992) is a well known source of semi-supervisedfeatures.
The clustering algorithm is run overa large sample of unlabelled data, to generate atype-to-cluster map.
This mapping is then used togenerate features that sometimes generalise betterthan lexical features, and are helpful for out-of-vocabulary words (Turian et al., 2010).Koo and Collins (2010) found that Brown clus-ter features greatly improved the performance of agraph-based dependency parser.
On our transition-based parser, Brown cluster features bring a smallbut statistically significant improvement on theWSJ task (0.1-0.3% UAS).
Other developers oftransition-based parsers seem to have found sim-ilar results (personal communication).
Since aBrown cluster mapping computed by Liang (2005)is easily available,3 the features are simple to im-plement and cheap to compute.Our templates follow Koo and Collins (2010)in including features that refer to cluster prefixstrings, as well as the full clusters.
We adapt theirtemplates to transition-based parsing by replacing?head?
with ?item on top of the stack?
and ?child?with ?first word of the buffer?.
The exact templatescan be found in the supplementary material.The Brown cluster features are used in our?baseline?
parser, and in the parsers we use as partof our pipeline systems.
They improved develop-ment set accuracy by 0.4%.
We experimented withthe other feature sets in these parsers, but foundthat they did not improve accuracy on fluent text.3http://www.metaoptimize.com/projects/wordreps5.2 Rough Copy FeaturesJohnson and Charniak (2004) point out that inspeech repairs, the repair is often a ?rough copy?of the reparandum.
The simplest case of this iswhere the repair is a single word repetition.
It iscommon for the repair to differ from the reparan-dum by insertion, deletion or substitution of oneor more words.To capture this regularity, we first extend thefeature-set with three new context tokens:41.
S0re: The rightmost edge of S0 descendants;2.
S0le: The leftmost edge of S0 descendants;3.
N0le: The leftmost edge of N0 descendants.If a word has no leftward children, it will beits own left-edge, and similarly it will be its ownrightward edge if it has no rightward children.Note that the token S0re is necessarily immedi-ately before N0le, unless some of the tokens be-tween them are disfluent.
We use the S0le and N0leto compute the following rough-copy features:1.
How long is the prefix word match betweenS0le...S0 and N0le...N0?If the parser were analysing the red the bluesquare, with red on the stack and square atN0, its value would be 1.2.
How long is the prefix POS tag match be-tween S0le...S0 and N0le...N0?3.
Do the words in S0le...S0 and N0le...N0match exactly?4.
Do the POS tags in S0le...S0 and N0le...N0match exactly?If the parser were analysing the red squarethe blue rectangle, with square on the stackand rectangle at N0, its value would be true.The prefix-length features are binned using thefunction ?x : min(x, 5).5.3 Match FeaturesThis class of features ask which pairs of the con-text tokens match, in word or POS tag.
The con-text tokens in the Zhang and Nivre (2011) fea-ture set are the top of the stack (S0), its head and4As is common in this type of parser, our implementationhas a number of vectors for properties that are defined beforeparsing, such as word forms, POS tags, Brown clusters, etc.
Acontext token is an index into these vectors, allowing featuresconsidering these properties to be computed.136grandparent (S0h, S0h2), its two left- and right-most children (S0L, S0L2, S0R, S0R2), the firstthree words of the buffer (N0, N1, N2), and thetwo leftmost children of N0 (N0L, N0LL).
We ex-tend this set with the S0le, S0re and N0le tokensdescribed above, and also the first left and rightchild of S0 and N0 (S0L0, S0R0, N0L0).All up, there are 18 context tokens, so(182)= 153 token pairs.
For each pair of thesetokens, we add two binary features, indicatingwhether the two tokens match in word form or POStag.
We also have two further classes of features:if the words do match, a feature is added indicat-ing the word form; if the tags match, a feature isadded indicating the tag.
These finer grained ver-sions help the model adjust for the fact that somewords can be duplicated in grammatical sentences(e.g.
?that that?
), while most rare words cannot.5.4 Edited Neighbour FeaturesDisfluencies are usually string contiguous, even ifthey do not form a single constituent.
In these situ-ations, our model has to make multiple transitionsto mark a single disfluency.
For instance, if an ut-terance begins and the and a, the stack will containtwo entries, for and and the, and two Edit transi-tions will be required.To mitigate this disadvantage of our model, weadd four binary features.
Two fire when the wordor pair of words immediately preceding N0 havebeen marked disfluent; the other two fire when theword or pair of words immediately following S0have been marked disfluent.
These features pro-vide an additional string-based view that the parserwould otherwise be missing.
Speakers tend bedisfluent in bursts: if the previous word is dis-fluent, the next word is more likely to be disflu-ent.
These four features are therefore all associ-ated with positive weights for the Edit transition.Without these features, we would miss an aspect ofdisfluency processing that sequence models natu-rally capture.6 Part-of-Speech TaggingWe adopt the standard strategy of using a POStagger as a pre-process before parsing.
Mosttransition-based parsers use a structured averagedperceptron model with beam-search for tagging,as this model achieves competitive accuracy andmatches the standard dependency parsing archi-tecture.
Our tagger also uses this architecture.We performed some additional feature engi-neering for the tagger, in order to improve its accu-racy given the lack of case distinctions and punc-tuation in the data.
Our additional features use twosources of unsupervised information.
First, wefollow the suggestion of Manning (2011) by us-ing Brown cluster features to improve the tagger?saccuracy on unknown words.
Second, we com-pensate for the lack of case distinctions by includ-ing features that ask what percentage of the timea word form was seen title-cased, upper-cased andlower-cased in the Google Web1T corpus.Where most previous work uses cross-foldtraining for the tagger, to ensure that the parseris trained on tags that reflect run-time accuracies,we do online training of the tagger alongside theparser, using the current tagger model to producetags during parser training.
This had no impact onparse accuracy, and made it slightly easier to de-velop our tagger alongside the parser.The tagger achieved 96.5% accuracy on the de-velopment data, but when we ran our final testexperiments, we found its accuracy dropped to96.0%, indicating some over-fitting during ourfeature engineering.
On the development data, ourparser accuracy improves by about 1% when gold-standard tags are used.7 ExperimentsWe use the Switchboard portion of the Penn Tree-bank (Marcus et al., 1993), as described in Sec-tion 2, to train our joint models and evaluate themon dependency parsing and disfluency detection.The pre-processing and dependency conversionare described in Section 2.1.
We use the stan-dard train/dev/test split from Charniak and John-son (2001): Sections 2 and 3 for training, and Sec-tion 4 divided into three held-out sections, the firstof which is used for final evaluation.Our parser evaluation uses the SPARSEVAL(Roark et al., 2006) metric.
However, we wantedto use the Stanford dependency converter, for thereasons described in Section 2.1, so we used ourown implementation.
Because we do not need todeal with recognition errors, we do not need toreport our parsing results using P /R/F -measures.Instead, we report an unlabelled accuracy score,which refers to the percentage of fluent wordswhose governors were assigned correctly.
Notethat words marked as disfluent cannot have any in-coming or out-going dependencies, so if a word is137incorrectly marked as disfluent, all of its depen-dencies will be incorrect.We follow Johnson and Charniak (2004) andothers in restricting our disfluency evaluation tospeech repairs, which we identify as words thathave a node labelled EDITED as an ancestor.
Un-like most other disfluency detection research, wetrain only on the MRG files, giving us 619,236words of training data instead of the 1,482,845used by the pipeline systems.
It may be possibleto improve our system?s disfluency detection byleveraging the additional data that does not havesyntactic annotation in some way.All parsing models were trained for 15 itera-tions.
We found that optimising the number ofiterations on a development set led to small im-provements that did not transfer to a second devel-opment set (part of Section 4, which Charniak andJohnson (2001) reserved for ?future use?
).We test for statistical significance in our resultsby training 20 models for each experimental con-figuration, using different random seeds.
The ran-dom seeds control how the sentences are shuf-fled during training, which the perceptron modelis quite sensitive to.
We use the Wilcoxon rank-sums non-parametric test.
The standard deviationin UAS for a sample was typically around 0.05%,and 0.5% for disfluency F -measure.All of our models use beam-search decoding,with a beam width of 32.
We found that a beamwidth of 64 brought a very small accuracy im-provement (about 0.1%), at the cost of 50% slowerrun-time.
Wider beams than this brought no ac-curacy improvement.
Accuracy seems to plateauwith slightly narrower beams than on newswiretext.
This is probably due to the shorter sentencesin Switchboard.The baseline and pipeline systems are config-ured in the same way, except that the baselineparser is modified slightly to allow it to predictdisfluencies, using a special dependency label,ERASED.
All descendants of a word attached to itshead by this label are marked as disfluent.
Both thebaseline and pipeline/oracle parsers use the samefeature set: the Zhang and Nivre (2011) features,plus our Brown cluster features.The baseline system is a standard arc-eagertransition-based parser with a structured averagedperceptron model and beam-search decoding.
Themodel is trained in the standard way, with a ?static?oracle and maximum-violation update, following(Huang et al., 2012).7.1 Comparison with Pipeline ApproachesThe accuracy of incremental dependency parsersis well established on the Wall Street Journal, butthere are no dependency parsing results in the lit-erature that make it easy to put our joint model?sparsing accuracy into context.
We therefore com-pare our joint model to two pipeline systems,which consist of a disfluency detector, followed byour dependency parser.
We also evaluate parse ac-curacies after oracle pre-processing, to gauge thenet effect of disfluencies on our parser?s accuracy.The dependency parser for the pipeline systemswas trained on text with all disfluencies removed,following Charniak and Johnson (2001).
The twodisfluency detection systems we used were theQian and Liu (2013) sequence-tagging model, anda version of the Johnson and Charniak (2004)noisy channel model, using the Charniak (2001)syntactic language model and the reranking fea-tures of Zwarts and Johnson (2011).
They are thetwo best published disfluency detection systems.8 ResultsTable 1 shows the development set accuracies forour joint parser.
Both the disfluency features andthe Edit transition make statistically significantimprovements, in both disfluency F -measure, un-labelled attachment score (UAS), and labelled at-tachment score (LAS).The Oracle pipeline system, which uses thegold-standard to clean disfluencies prior to pars-ing, shows the total impact of speech-errors on theparser.
The baseline parser, which uses the Zhangand Nivre (2011) feature set plus the Brown clus-ter features, scores 1.8% UAS lower than the ora-cle.When we add the features described in Sec-tions 5.2, 5.3 and 5.4, the gap is reduced to 1.2%(+Features).
Finally, the improved transition sys-tem reduces the gap further still, to 0.8% UAS(+Edit transition).
We also tested these featuresin the Oracle parser, but found they were ineffec-tive on fluent text.The w/s column shows the tokens analysed persecond for each system, including disfluencies,with a single thread on a 2.4GHz Intel Xeon.
Theadditional features reduce efficiency, but the non-monotonic Edit transition does not.
The system iseasily efficient enough for real-time use.138P R F UAS LAS w/sBaseline joint 79.4 70.1 74.5 89.9 86.9 711+Features 86.0 77.2 81.3 90.5 87.5 539+Edit transition 92.2 80.2 85.8 90.9 87.9 555Oracle pipeline 100 100 100 91.7 88.6 782Table 1: Development results for the joint models.
For thebaseline model, disfluencies reduce parse accuracy by 1.7%Unlabelled Attachment Score (UAS).
Our features and Edittransition reduce the gap to 0.7%, and improve disfluency de-tection by 11.3% F -measure.Disfl.
F UASJohnson et al pipeline 82.1 90.3Qian and Liu pipeline 83.9 90.1Baseline joint parser 73.9 89.4Final joint parser 84.1 90.5Table 2: Test-set parse and disfluency accuracies.
The jointparser is improved by the features and Edit transition, and isbetter than pre-processing the text with state-of-the-art disflu-ency detectors.Table 2 shows the final evaluation.
Our maincomparison is with the two pipeline systems, de-scribed in Section 7.1.
The Johnson and Char-niak (2004) system was 1.8% less accurate at dis-fluency detection than the other disfluency detec-tor we evaluated, the state-of-the-art Qian and Liu(2013) system.
However, when we evaluated thetwo systems as pre-processors before our parser,we found that the Johnson et al pipeline achieved0.2% better unlabelled attachment score than theQian and Liu pipeline.
We attribute this to theuse of the Charniak and Johnson (2001) syntac-tic language model in the Johnson et al pipeline,which would help the system produce more syn-tactically consistent output.Our joint model achieved an unlabelled at-tachment score of 90.5%, out-performing bothpipeline systems.
The Baseline joint parser,which did not include the Edit transition or disflu-ency features, scores 1.1% below the Final jointparser.
All of the parse accuracy differences werefound to be statistically significant (p < 0.001).The Edit transition and disfluency features to-gether brought a 10.1% improvement in disfluencyF -measure, which was also found to be statisti-cally significant.
The final joint parser achieved0.2% higher disfluency detection accuracy thanthe previous state-of-the-art, the Qian and Liu(2013) system,5 despite having approximately halfas much training data (we require syntactic anno-5 Our scores refer to an updated version of the system thatcorrects minor pre-processing problems.
We thank Qian Xianfor making his code available.tation, for which there is less data).Our significance testing regime involved using20 different random seeds when training each ofour models, which the perceptron algorithm is sen-sitive to.
This could not be applied to the other twodisfluency detectors, so we cannot test those dif-ferences for significance.
However, we note thatthe 20 samples for our disfluency detector rangedin accuracy from 83.3-84.6, so we doubt that 0.2%mean improvement over the Qian and Liu (2013)result is meaningful.Although we did not systematically optimiseon the development set, our test scores are lowerthan our development accuracies.
Much of theover-fitting seems to be in the POS tagger, whichdropped in accuracy by 0.5%.9 Analysis of Edit BehaviourIn order to understand how the parser appliesthe Edit transition, we collected some additionalstatistics over the development data.
The parserpredicted 2,558 Edit transitions, which togethermarked 2,706 words disfluent (2,495 correctly).The Edit transition can mark multiple words dis-fluent when S0 has one or more rightward descen-dants.
It turns out this case is uncommon; theparser largely assigns disfluency labels word-by-word, only sometimes marking words with right-ward descendents as disfluent.Of the 2,558 Edit transitions, there were 682cases were at least one leftward child was returnedto the stack, and the total number of leftward chil-dren returned was 1,132.
The most common typeof construction that caused the parser to returnwords to the stack were disfluent predicates, whichoften have subjects and discourse conjunctions asleftward children.
An example of a disfluent pred-icate with a fluent subject is shown in Figure 4.There were only 48 cases of the same word be-ing returned to the stack twice.
The possibility ofwords being returned to the stack multiple timesis what gives our system worse than linear worst-case complexity.
In the worst case, the ith wordof a sentence of length n could be returned to thestack n?
(i+1) times.
Empirically, the Edit tran-sition made no difference to run-time.Once a word has been returned to the stack bythe Edit transition, how does the parser end upanalysing it?
If it turned out that almost all ofthe former leftward children of disfluent words aresubsequently marked as disfluent, there would be139little point in returning them to the stack ?
wecould just mark them as disfluent in the originalEdit transition.
On the other hand, if they are al-most all marked as fluent, perhaps they can just beattached as children to the first word of the buffer.In fact the two cases are almost equally com-mon.
Of the 1,132 words returned to the stack,547 were subsequently marked disfluent, and 584were not.
The parser was also quite accurate inits decisions over these tokens.
Of the 547 tokensmarked disfluent, 500 were correct ?
similar tothe overall development set precision, 92.2%.Accuracy over the words returned to the stackmight be improved in future by features referringto their former heads.
For instance, in He wentbroke uh became bankrupt, we do not currentlyhave features that record the deleted dependencybecame he and went.
We thank one of the anony-mous reviewers for this suggestion.10 Related WorkThe most similar system to ours was publishedvery recently.
Rasooli and Tetreault (2013) de-scribe a joint model of dependency parsing anddisfluency detection.
They introduce a secondclassification step, where they first decide whetherto apply a disfluency transition, or a regular pars-ing move.
Disfluency transitions operate eitherover a sequence of words before the start of thebuffer, or a sequence of words from the start ofthe buffer forward.
Instead of the dynamic oracletraining method that we employ, they use a two-stage bootstrap-style process.Direct comparison between our model andtheirs is difficult, as they use the Penn2MALTscheme, and their parser uses greedy decoding,where we use beam search.
They also use gold-standard part-of-speech tags, which would im-prove our scores by around 1%.
The use ofbeam-search may explain much of our perfor-mance advantage: they report an unlabelled at-tachment score of 88.6, and a disfluency detec-tion F -measure of 81.4%.
Our training algorithmwould be applicable to a beam-search version oftheir parser, as their transition-system also intro-duces substantial spurious ambiguity, and somenon-monotonic behaviour.A hybrid transition system would also be possi-ble, as the two types of Edit transition seem to becomplementary.
The Rasooli and Tetreault systemoffers a token-based view of disfluencies, whichis useful for examples such as, and the and the,which would require two applications of our tran-sition.
On the other hand, our Edit transition mayhave the advantage for more syntactically compli-cated examples, particularly for disfluent verbs.The importance of syntactic features for disflu-ency detection was demonstrated by Johnson andCharniak (2004).
Despite this, most subsequentwork has used sequence models, rather than syn-tactic parsers.
The other disfluency system thatwe compare our model to, developed by Qian andLiu (2013), uses a cascade of Maximum MarginMarkov Models to perform disfluency detectionwith minimal syntactic information.One motivation for sequential approaches is thatmost applications of these models will be over un-segmented text, as segmenting unpunctuated textis a difficult task that benefits from syntactic fea-tures (Zhang et al., 2013).We consider the most promising aspect of oursystem to be that it is naturally incremental, so itshould be straightforward to extend the system tooperate on unsegmented text in subsequent work.Due to its use of syntactic features, from the jointmodel, the system is substantially more accuratethan the previous state-of-the-art in incrementaldisfluency detection, 77% (Zwarts et al., 2010).11 ConclusionWe have presented an efficient and accurate jointmodel of dependency parsing and disfluency de-tection.
The model out-performs pipeline ap-proaches using state-of-the-art disfluency detec-tors, and is highly efficient, processing over 550tokens a second.
Because the system is incremen-tal, it should be straight-forward to apply it to un-segmented text.
The success of an incremental,non-monotonic parser at disfluent speech parsingmay also be of some psycholinguistic interest.AcknowledgmentsThe authors would like to thank the anony-mous reviewers for their valuable comments.This research was supported under the Aus-tralian Research Council?s Discovery Projectsfunding scheme (project numbers DP110102506and DP110102593).ReferencesMiguel Ballesteros and Joakim Nivre.
2013.
Go-ing to the roots of dependency parsing.
Compu-tational Linguistics.
39:1.140Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of naturallanguage.
Computational Linguistics, 18:467?479.Eugene Charniak.
2001.
Immediate-head parsingfor language models.
In Proceedings of 39thAnnual Meeting of the Association for Compu-tational Linguistics, pages 124?131.
Associa-tion for Computational Linguistics, Toulouse,France.Eugene Charniak and Mark Johnson.
2001.
Editdetection and parsing for transcribed speech.
InProceedings of the 2nd Meeting of the NorthAmerican Chapter of the Association for Com-putational Linguistics, pages 118?126.
The As-sociation for Computational Linguistics.Eugene Charniak and Mark Johnson.
2005.Coarse-to-fine n-best parsing and MaxEnt dis-criminative reranking.
In Proceedings of the43rd Annual Meeting of the Association forComputational Linguistics, pages 173?180.
As-sociation for Computational Linguistics, AnnArbor, Michigan.Michael Collins.
2002.
Discriminative trainingmethods for hidden Markov models: Theoryand experiments with perceptron algorithms.
InProceedings of the 2002 Conference on Empir-ical Methods in Natural Language Processing,pages 1?8.
Association for Computational Lin-guistics.Marie-Catherine de Marneffe, Bill MacCartney,and Christopher D. Manning.
2006.
Generatingtyped dependency parses from phrase structureparses.
In Proceedings of the 5th InternationalConference on Language Resources and Evalu-ation (LREC).Lyn Frazier and Keith Rayner.
1982.
Making andcorrecting errors during sentence comprehen-sion: Eye movements in the analysis of struc-turally ambiguous sentences.
Cognitive Psy-chology, 14(2):178?210.Yoav Goldberg and Joakim Nivre.
2012.
A dy-namic oracle for arc-eager dependency parsing.In Proceedings of the 24th International Con-ference on Computational Linguistics (Coling2012).
Association for Computational Linguis-tics, Mumbai, India.Matthew Honnibal, Yoav Goldberg, and MarkJohnson.
2013.
A non-monotonic arc-eagertransition system for dependency parsing.
InProceedings of the Seventeenth Conference onComputational Natural Language Learning,pages 163?172.
Association for ComputationalLinguistics, Sofia, Bulgaria.Liang Huang, Suphan Fayong, and Yang Guo.2012.
Structured perceptron with inexactsearch.
In Proceedings of the 2012 Con-ference of the North American Chapter ofthe Association for Computational Linguistics:Human Language Technologies, pages 142?151.
Association for Computational Linguis-tics, Montre?al, Canada.Mark Johnson and Eugene Charniak.
2004.
ATAG-based noisy channel model of speech re-pairs.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Lin-guistics, pages 33?39.Douglas A. Jones, Florian Wolf, Edward Gib-son, Elliott Williams, Evelina Fedorenko, Dou-glas A. Reynolds, and Marc A. Zissman.
2003.Measuring the readability of automatic speech-to-text transcripts.
In INTERSPEECH.
ISCA.Fredrik Jorgensen.
2007.
The effects of disflu-ency detection in parsing spoken language.
InJoakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-chnek, and Mare Koit, editors, Proceedings ofthe 16th Nordic Conference of ComputationalLinguistics NODALIDA-2007, pages 240?244.Terry Koo and Michael Collins.
2010.
Efficientthird-order dependency parsers.
In Proceedingsof the 48th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 1?11.Percy Liang.
2005.
Semi-supervised learning fornatural language.
Ph.D. thesis, MIT.Christopher D. Manning.
2011.
Part-of-speechtagging from 97linguistics?
In Proceedings ofthe 12th international conference on Computa-tional linguistics and intelligent text processing- Volume Part I, CICLing?11, pages 171?189.Springer-Verlag, Berlin, Heidelberg.Michell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313?330.Joakim Nivre.
2003.
An efficient algorithm forprojective dependency parsing.
In Proceedings141of the 8th International Workshop on ParsingTechnologies (IWPT), pages 149?160.Joakim Nivre.
2008.
Algorithms for determinis-tic incremental dependency parsing.
Computa-tional Linguistics, 34:513?553.Xian Qian and Yang Liu.
2013.
Disfluency detec-tion using multi-step stacked learning.
In Pro-ceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics: Human Language Tech-nologies, pages 820?825.
Association for Com-putational Linguistics, Atlanta, Georgia.Mohammad Sadegh Rasooli and Joel Tetreault.2013.
Joint parsing and disfluency detection inlinear time.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural Lan-guage Processing, pages 124?129.
Associationfor Computational Linguistics, Seattle, Wash-ington, USA.Brian Roark, Mary Harper, Eugene Charniak,Bonnie Dorr, Mark Johnson, Jeremy Kahn,Yang Liu, Mary Ostendorf, John Hale, AnnaKrasnyanskaya, Matthew Lease, Izhak Shafran,Matthew Snover, Robin Stewart, and LisaYung.2006.
Sparseval: Evaluation metrics for pars-ing speech.
In Proceedings of Language Re-source and Evaluation Conference, pages 333?338.
European Language Resources Associa-tion (ELRA), Genoa, Italy.Francesco Sartorio, Giorgio Satta, and JoakimNivre.
2013.
A transition-based dependencyparser using a dynamic parsing strategy.
In Pro-ceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics, pages135?144.
Association for Computational Lin-guistics, Sofia, Bulgaria.Elizabeth Shriberg.
1994.
Preliminaries to a The-ory of Speech Disfluencies.
Ph.D. thesis, Uni-versity of California, Berkeley.Xu Sun, Takuya Matsuzaki, Daisuke Okanohara,and Jun?ichi Tsujii.
2009.
Latent variable per-ceptron algorithm for structured classification.In IJCAI, pages 1236?1242.Joseph Turian, Lev-Arie Ratinov, and YoshuaBengio.
2010.
Word representations: A simpleand general method for semi-supervised learn-ing.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguis-tics, pages 384?394.
Association for Computa-tional Linguistics, Uppsala, Sweden.Dongdong Zhang, Shuangzhi Wu, Nan Yang, andMu Li.
2013.
Punctuation prediction withtransition-based parsing.
In Proceedings ofthe 51st Annual Meeting of the Association forComputational Linguistics, pages 752?760.
As-sociation for Computational Linguistics, Sofia,Bulgaria.Yue Zhang and Stephen Clark.
2011.
Syntac-tic processing using the generalized perceptronand beam search.
Computational Linguistics,37(1):105?151.Yue Zhang and Joakim Nivre.
2011.
Transition-based dependency parsing with rich non-localfeatures.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics: Human Language Technologies,pages 188?193.
Association for ComputationalLinguistics, Portland, USA.Muhua Zhu, Yue Zhang, Wenliang Chen, MinZhang, and Jingbo Zhu.
2013.
Fast and accu-rate shift-reduce constituent parsing.
In Pro-ceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics, pages434?443.
Association for Computational Lin-guistics, Sofia, Bulgaria.Simon Zwarts and Mark Johnson.
2011.
The im-pact of language models and loss functions onrepair disfluency detection.
In Proceedings ofthe 49th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies, pages 703?711.
Association forComputational Linguistics, Portland, USA.Simon Zwarts, Mark Johnson, and Robert Dale.2010.
Detecting speech repairs incrementallyusing a noisy channel approach.
In Proceedingsof the 23rd International Conference on Com-putational Linguistics (Coling 2010), pages1371?1378.
Coling 2010 Organizing Commit-tee, Beijing, China.142
