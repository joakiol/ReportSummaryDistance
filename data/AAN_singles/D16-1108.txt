Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1030?1035,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsCharacterizing the Language of Online Communities andits Relation to Community ReceptionTrang Tran and Mari OstendorfElectrical Engineering, University of Washington, Seattle, WA{ttmt001,ostendor}@uw.eduAbstractThis work investigates style and topic aspectsof language in online communities: lookingat both utility as an identifier of the commu-nity and correlation with community receptionof content.
Style is characterized using a hy-brid word and part-of-speech tag n-gram lan-guage model, while topic is represented usingLatent Dirichlet Allocation.
Experiments withseveral Reddit forums show that style is a bet-ter indicator of community identity than topic,even for communities organized around spe-cific topics.
Further, there is a positive cor-relation between the community reception toa contribution and the style similarity to thatcommunity, but not so for topic similarity.1 IntroductionOnline discussion forums provide a rich source ofdata for studying people?s language usage patterns.Discussion platforms take on various forms: arti-cles on many news sites have a comment section,many websites are dedicated to question answer-ing (www.quora.com), and other platforms letusers share personal stories, news, and random dis-coveries (www.reddit.com).
Like their offlinecounterparts, online communities are often com-prised of people with similar interests and opinions.Online communication, however, differs from in-person communication in an interesting aspect: ex-plicit and quantifiable feedback.
Many discussionforums give their users the ability to upvote and/ordownvote content posted by another user.
These ex-plicit reward/penalty labels provide valuable infor-mation on the reaction of users in a community.
Inthis work, we take advantage of the available userresponse to explore the relationship between com-munity reception and the degree of stylistic/topicalcoherence to such communities.
Using hybrid n-grams and Latent Dirichlet Allocation (LDA) topicmodels to represent style and topic for a series ofclassification tasks, we confirm that there exists acommunity language style, which is not simply char-acterized by the topics that online communities areorganized around.
Moreover, we show that languagestyle is better at discriminating communities, espe-cially between different communities that happen todiscuss similar issues.
In addition, we found a posi-tive, statistically significant, correlation between thecommunity feedback to comments and their style,but interestingly not with their topic.
Finally, weanalyze community language on the user level andshow that more successful users (in terms of positivecommunity reception) tend to be more specialized;in other words, analogous to offline communities, itis rare for a person to be an expert in multiple areas.2 Related WorkIt is well known that conversation partners becomemore linguistically similar to each other as their dia-logue evolves, via many aspects such as lexical, syn-tactic, as well as acoustic characteristics (Niederhof-fer and Pennebaker, 2002; Levitan et al, 2011).
Thispattern is observed even when the conversation isfictional (Danescu-Niculescu-Mizil and Lee, 2011),or happening on social media (Danescu-Niculescu-Mizil et al, 2011).
Regarding the language of on-line discussions in particular, it has been shown thatindividual users?
linguistic patterns evolve to match1030those of the community they participate in, reaching?linguistic maturity?
over time (Nguyen and Rose?,2011; Danescu-Niculescu-Mizil et al, 2013).
In amulti-community setting, Tan and Lee (2015) foundthat users tend to explore more in different commu-nities as they mature, adopting the language of thesenew communities.
These works have mainly fo-cused on the temporal evolution of users?
language.Our work differs in that we use different languagemodels to explore the role of topic and style, whilealso considering users in multiple communities.
Inaddition, we look at community language in termsof its correlation with reception of posted content.Other researchers have looked at the role of lan-guage in combination with other factors in Redditcommunity reception.
Lakkaraju et al (2013) pro-posed a community model to predict the popularityof a resubmitted content, revealing that its title playsa substantial role.
Jaech et al (2015) consideredtiming and a variety of language features in rankingcomments for popularity, finding significant differ-ences across different communities.
In our work, wefocus on community language, but explore differentmodels to account for it.3 DataReddit is a popular forum with thousands of sub-communities known as subreddits, each of whichhas a specific theme.
We will refer to subreddits andcommunities interchangeably.
Redditors can submitcontent to initiate a discussion thread whose root textwe will refer to as a post.
Under each post, users candiscuss the post by contributing a comment.
Bothposts and comments can be upvoted and downvoted,and the net feedback is referred to as karma points.We use eight subreddits that reflect Reddit?s di-verse topics, while limiting the amount of data toa reasonable size.
In addition, we create an artificialdistractor merged others that serves as an open classin our classification tasks and for normalizing scoresin correlation analysis.
Statistics are listed in Ta-ble 1.
The merged others set includes 9 other sub-reddits that are similar in size and content diversityto the previous ones: books, chicago, nyc, seattle,explainlikeimfive, science, running, nfl, and today-ilearned.
Among these extra subreddits, the small-est in size is nyc (1.5M tokens, 76K comments), andsubreddit # posts # cmts % k ?
0askmen 4.5K 1.1M 10.6askscience 0.9K 0.3M 9.1askwomen 3.6K 0.8M 7.5atheism 3.1K 1.0M 15.2changemyview 2.3K 0.5M 16.7fitness 2.4K 0.9M 8.6politics 4.9K 2.2M 20.8worldnews 9.9K 6.0M 23.6merged others 28.0K 14.2M 13.2Table 1: Reddit dataset statisticsthe largest is todayilearned (88M tokens, 5M com-ments).
All data is from the period between January1, 2014 and January 31, 2015.
In each subreddit,20% of the threads are held out for testing.We use discussion threads with at least 100 com-ments, hypothesizing that smaller threads will notelicit enough community personality for our study.
(Virtually all threads kept had only upvotes.)
Fortraining our models, we also exclude individualcomments with non-positive karma (k ?
0) in or-der to learn only from content that is less likely tobe downvoted by the Reddit communities; percent-ages are noted in Table 1.4 ModelsWe wish to characterize community language viastyle and topic.
For modeling style, a popular ap-proach has been combining the selected words withpart-of-speech (POS) tags to construct models forgenre detection (Stamatatos et al, 2000; Feldmanet al, 2009; Bergsma et al, 2012) and data selec-tion (Iyer and Ostendorf, 1999; Axelrod, 2014).
Fortopic, a common approach is Latent Dirichlet Allo-cation (LDA) (Blei et al, 2003).
We follow suchapproaches in our work, acknowledging the chal-lenge of completely separating style/genre and topicfactors raised previously (Iyer and Ostendorf, 1999;Sarawgi et al, 2011; Petrenz and Webber, 2011; Ax-elrod, 2014), which also comes out in our analysis.Generative language models are used for character-izing both style and topic, since they are well suitedto handling texts of widely varying lengths.4.1 Representing StyleReplacing words with POS tags reduces the possi-bility that the style model is learning topic, but re-1031placing too many words loses useful community jar-gon.
To explore this tradeoff, we compared four tri-gram language models representing different uses ofwords vs. POS tags in the vocabulary:?
word_only: a regular token-based languagemodel (vocabulary: 156K words)?
hyb-15k: a hybrid word-POS language modelover a vocabulary of 15K most frequent wordsacross all communities in our data; all other wordsare converted to POS tags (vocabulary: 15Kwords + 38 tags)?
hyb-500.30: a hybrid word-POS languagemodel over a vocabulary of 500 most frequentwords in a subset of data balanced across com-munities, combined with the union of the 30 nextmost common words from each of the 17 subred-dits; all other words are converted to POS tags(vocabulary: 854 words + 38 tags)?
tag_only: a language model using only POStags as its vocabulary (vocabulary: 38 tags)The hybrid models represent two intermediate sam-ple points between the extremes of word-only andtag-only n-grams.
For the hyb-500.30 model,the mix of general and community-specific wordswas designed to capture distinctive community jar-gon.
The general words include punctuation, func-tion words, and words that are common in manysubreddits (e.g., sex, culture, see, dumb, simply).The subreddit-specific words seem to reflect bothbroad topical themes and jargon or style words, asin (themes vs. style/jargon):askmen: wife, single vs. whatever, interestedaskwomen: mom, husband vs. especially, totallyaskscience: particle, planet vs. basically, xfitness: exercises, muscles vs. cardio, reps, rackTokenization and tagging are done using Stan-ford coreNLP (Manning et al, 2014).
Punctuationis separated from the words and treated as a word.All language models are trigrams trained using theSRILM toolkit (Stolcke, 2002); modified Kneser-Ney smoothing is applied to the word_only lan-guage model, while Witten-Bell smoothing is ap-plied to the tag_only and both hybrid models.4.2 Representing TopicWe train 100- and 200-dimensional LDA topic mod-els (Blei et al, 2003) using gensim (R?ehu?r?ekand Sojka, 2010).
We remove all stopwords (250ID Frequent words19 -lsb-, -rsb-, -rrb-, -lrb-, **, reddit,comment, confirmed, spanish, fair29 sex, pilots, child, women, abortion,mail, birth, want, episodes, children32 tax, government, taxes, iraq, pay, cia,land, money, income, people34 africa, war, nation, global, germans,rebels, corruption, nations, fuel, worldTable 2: Examples of broadly used topics.words) and use tf-idf normalized word counts ineach comment (as documents).
The vocabulary con-sists of 156K words, similar to the vocabulary ofthe word_only language model.
The topic mod-els were trained on a subset of the training data, us-ing all collected subreddits but randomly excludingroughly 15% of the training data of larger subredditsworldnews, todayilearned, and nfl.The topics learned exhibit a combination of onesthat reflect general characteristics of online discus-sions or topics that arise in many forums, some thathave more specific topics, and others that do notseem particularly coherent.
Topics (from LDA-100)that consistently have high probability in all subred-dits are shown in Table 2 with their top 10 words byfrequency (normalized by the topic average).
Topic19 is likely capturing Reddit?s writing conventionsand formatting rules.
Broadly used topics reflectwomen?s issues (29) and news events (32, 34).Online communities are typically organizedaround a common theme, but multiple topics mightfall under that theme, particularly since some ofthe ?topics?
actually reflect style.
A subreddit asa whole is characterized by a distribution of topicsas learned via LDA, but any particular discussionthread would not necessarily reflect the full distri-bution.
Therefore, we characterize each subredditwith multiple topic vectors.
Specifically, we com-pute LDA topic vectors for each discussion threadin a subreddit, and learn 50 representative topic vec-tors for each subreddit via k-means clustering.5 Community ClassificationOne method for exploring the relative importance oftopic vs. style in online communication is throughcommunity classification experiments: given a dis-cussion thread (or a user?s comments), can we iden-1032tify the community that it comes from more easilyusing style characteristics or topic characteristics?We formulate this task as a multi-class classificationproblem (8 communities and ?other?
), where sam-ples are either at the discussion thread level or theuser level.
At the thread level, all comments (frommultiple people) and the post in a discussion threadare aggregated and treated as a document to be clas-sified.
At the user level, we aggregate all commentsmade by a user in a certain subreddit and treat thecollection (which may reflect multiple topics) as adocument to be classified.We classify document di to a subreddit accordingto j?
= argmaxj si,j , where si,j is a score of the sim-ilarity of di to community j.
For the style models,si,j is the log-probability under the respective tri-gram language model of community j.
For the topicmodel, si,j is computed using di?s topic vector vi asfollows.
For a subreddit j, we compute the cosinesimilarities simj,k between vi and the subreddit?stopic vectors wj,k for k = 1, .
.
.
, 50.
The final topicsimilarity score si,j is the mean of the top 3 highestsimilarities: si,j = (simj,[1]+simj,[2]+simj,[3])/3,where [?]
denotes the sorted cosine similarities?
in-dices.
The top-3 average captures the most promi-nent subreddit topics (as in a nearest-neighbor clas-sifier).
Averaging over all 50 simj,k is ill suited tosubreddits with broad topic coverage, and leads topoor classification results.Table 3 summarizes the community classificationresults (as average accuracy across all subreddits)for each model described in Section 4.
While allmodels beat the random baseline of 11%, the poorperformance of the tag_only model confirms thatPOS tags alone are insufficient to characterize thecommunity.
Both for classifying threads and au-thors, hyb-500.30 yields the best average classi-fication accuracy, due to its ability to generalize POSstructure while covering sufficient lexical contentto capture the community?s jargon and key topicalthemes.
Neither topic model beats hyb-500.30,indicating that topic alone is not discriminativeenough for community identification, even thoughspecific communities coalesce around certain com-mon topics.
The word_only and hyb-15k mod-els have performance on the threads that is similar tothe topic models, since word features are sensitive totopic, as shown in (Petrenz and Webber, 2011).Model by thread by authorrandom 11.1% 11.1%word only 68.9% 46.8%tags only 27.6% 18.8%hyb-15k 69.4% 46.6%hyb-500.30 86.5% 51.0%topic-100 71.1% 27.5%topic-200 69.6% 27.7%Table 3: Average accuracy for classifying by posts and authorsClassifying authors is harder than classifyingthreads.
Two factors are likely to contribute.
First,treating a whole discussion thread as a documentyields more data to base the decision on than a col-lection of author comments, since there are manyauthors who only post a few comments.
Second, au-thors that have multi-community involvement maybe less adapted to a specific community.
The factthat word-based style models outperform topic mod-els may be because the comments are from differentthreads so not matching typical topic distributions.Subreddit confusion statistics indicate that cer-tain communities are easier to identify than others.Both style and topic models do well in recognizingaskscience: classification accuracy for threads is asmuch as 97%.
Communities that were most confus-able are intuitively similar: politics and worldnews,askmen and askwomen.6 Community Feedback CorrelationIn this section, we investigate whether the styleand/or topic scores of a discussion or user are cor-related with community response.
For thread-levelfeedback, we use karma points of the discussionthread itself; for the user-level feedback, we com-pute each user?s subreddit-dependent k-index (Jaechet al, 2015), defined similarly to the well-known h-index (Hirsch, 2005).
Specifically, a user?s k-indexkj in subreddit j is the maximum integer k such thatthe user has at least k comments with karma greaterthan k in that subreddit.
User k-index scores haveZipfian distribution, as illustrated in Figure 1 for theworldnews subreddit.We compute a normalized community similarityscore s?i,j = si,j ?
si,m, where si,m is the corre-sponding score from the subreddit merged others.The correlation between s?i,j and community feed-back is reported for three models in Table 4 for the1033Figure 1: Distribution (log base 10 counts) of user k-indexscores for the worldnews subreddit.subreddit hyb-500.30 word only topic-100askmen 0.392* 0.222* 0.055askscience 0.321* -0.110 -0.166*askwomen 0.501* 0.388* 0.005atheism 0.137* -0.229* -0.251chgmyvw 0.167* -0.121* -0.306*fitness 0.130* 0.017 -0.313*politics 0.533* 0.341* 0.011worldnews 0.374* 0.148* -0.277*Table 4: Spearman rank correlation of thread s?i,j with karmascores.
(*) indicates statistical significance (p < 0.05).thread level, and in Table 5 for the user level.
On thethread level, the hyb-500.30 style model consis-tently finds positive, statistically significant, correla-tion between the post?s stylistic similarity score andits karma.
This result suggests that language styleadaptation does contribute to being well-receivedby the community.
None of the other models ex-plored in the previous section had this property, andfor the topic models the correlation is mostly neg-ative.
On the user level, all correlations between auser?s k-index and their style/topic match are statis-tically significant, though the hyb-500.30 stylemodel shows more positive correlation than othermodels.
In both cases, the word_only model givesresults between the style and topic models.
Thehyb-15k model has results that are similar to theword_only model, and the tag_only model hasmostly negative correlation.Examining users?
multi-community involvement,we also find that users with high k-indices tend toparticipate in fewer subreddits.
Among relativelysubreddit hyb-500.30 word only topic-100askmen 0.402 0.215 0.167askscience 0.343 0.106 0.042askwomen 0.451 0.260 0.165atheism 0.296 0.024 0.107chgmyvw 0.446 0.020 0.091fitness 0.309 0.286 0.127politics 0.453 0.317 0.177worldnews 0.421 0.330 0.166Table 5: Spearman rank correlation of authors?
s?i,j with theirk-indices.
All values are statistically significant (p < 0.05).active users (having at least 100 comments), thosewith a max k-index of at least 100 participated in amedian of 3 communities, while those with a maxk-index of at most 5 participated in a median of 6subreddits.
Of the 42 users with max k-index of atleast 100, only 4 achieve a k-index of at least 50 inone other community, and only 6 achieve a k-indexof at least 20 in one other community.7 ConclusionIn this work, we use hybrid n-grams and topic mod-els to characterize style and topic of language in on-line communities.
Since communities center on acommon theme, topic characteristics are reflectedin language style, but we find that the best modelfor determining community identity uses very fewwords and mostly relies on POS patterns.
UsingReddit?s community response system (karma), wealso show that discussions and users with highercommunity endorsement are more likely to matchthe language style of the community, where the lan-guage model that best classifies the community isalso most correlated with community response.
Inaddition, online users tend to have more positivecommunity response when they specialize in fewersubreddits.
These results have implications for de-tecting newcomers in a community and the popular-ity of posts, as well as for language generation.AcknowledgmentsThis paper is based on work supported by theDARPA DEFT Program.
Views expressed are thoseof the authors and do not reflect the official policyor position of the Department of Defense or the U.S.Government.
We thank the reviewers for their help-ful feedback.1034ReferencesAmittai Axelrod.
2014.
Data Selection for Statisti-cal Machine Translation.
Ph.D. thesis, University ofWashington, Seattle.Shane Bergsma, Matt Post, and David Yarowsky.
2012.Stylometric analysis of scientific articles.
In Proc.Conf.
North American Chapter Assoc.
for Compu-tational Linguistics: Human Language Technologies(NAACL-HLT), pages 327?337.
Association for Com-putational Linguistics.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022, March.Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2011.Chameleons in imagined conversations: A new ap-proach to understanding coordination of linguisticstyle in dialog.
In Proceedings of the ACL Workshopon Cognitive Modeling and Computational Linguis-tics, pages 76?87.Cristian Danescu-Niculescu-Mizil, Michael Gamon, andSusan Dumais.
2011.
Mark my words!
Linguisticstyle accommodation in social media.
In Proceedingsof WWW.Cristian Danescu-Niculescu-Mizil, Robert West, Dan Ju-rafsky, Jure Leskovec, and Christopher Potts.
2013.No country for old members: User lifecycle and lin-guistic change in online communities.
In Proceedingsof WWW.Sergey Feldman, Alex Marin, Mari Ostendorf, and MayaGupta.
2009.
Part-of-speech histogram features forgenre classification of text.
In Proc.
ICASSP, pages4781?4784.Jorge E. Hirsch.
2005.
An index to quantify an indi-vidual?s scientific research output.
Proceedings of theNational Academy of Sciences of the United States ofAmerica, 102(46):16569?16572.Rukmini Iyer and Mari Ostendorf.
1999.
Relevanceweighting for combining multi-domain data for n-gram language modeling.
Comput.
Speech Lang.,13(3):267?282, July.Aaron Jaech, Victoria Zayats, Hao Fang, Mari Osten-dorf, and Hannaneh Hajishirzi.
2015.
Talking to thecrowd: What do people react to in online discussions?In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages2026?2031, Lisbon, Portugal, September.
Associationfor Computational Linguistics.Himabindu Lakkaraju, Julian McAuley, and JureLeskovec.
2013.
What?s in a name?
Understandingthe interplay between titles, content, and communitiesin social media.
In International AAAI Conference onWeb and Social Media.Rivka Levitan, Agust?
?n Gravano, and Julia Hirschberg.2011.
Entrainment in speech preceding backchan-nels.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, HLT ?11, pages 113?117, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Association for Computational Lin-guistics (ACL) System Demonstrations, pages 55?60.Dong Nguyen and Carolyn P. Rose?.
2011.
Languageuse as a reflection of socialization in online communi-ties.
In Proceedings of the Workshop on Languages inSocial Media, LSM ?11, pages 76?85.
Association forComputational Linguistics.Kate Niederhoffer and James Pennebaker.
2002.
Lin-guistic style matching in social interaction.
Journal ofLanguage and Social Psychology, 21:337?360.Philipp Petrenz and Bonnie Webber.
2011.
Stable clas-sification of text genres.
Computational Linguistics,37(2):385?393.Radim R?ehu?r?ek and Petr Sojka.
2010.
Software Frame-work for Topic Modelling with Large Corpora.
InProceedings of the LREC 2010 Workshop on NewChallenges for NLP Frameworks, pages 45?50, Val-letta, Malta, May.
ELRA.
http://is.muni.cz/publication/884893/en.Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi.2011.
Gender attribution: Tracing stylometric evi-dence beyond topic and genre.
In Proceedings of theFifteenth Conference on Computational Natural Lan-guage Learning, CoNLL ?11, pages 78?86.
Associa-tion for Computational Linguistics.Efstathios Stamatatos, Nikos Fakotakis, and GeorgeKokkinakis.
2000.
Text genre detection using com-mon word frequencies.
In Proceedings of the 18thConference on Computational Linguistics - Volume 2,COLING ?00, pages 808?814.
Association for Com-putational Linguistics.Andreas Stolcke.
2002.
SRILM-an extensible languagemodeling toolkit.
In Proceedings International Con-ference on Spoken Language Processing, pages 257?286.Chenhao Tan and Lillian Lee.
2015.
All who wander: Onthe prevalence and characteristics of multi-communityengagement.
In Proceedings of WWW.1035
