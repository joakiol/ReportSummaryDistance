CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 73?80Manchester, August 2008A MDL-based Model of Gender Knowledge AcquisitionHarmony Marchal1, Beno?t Lemaire2, Maryse Bianco1, and Philippe Dessus11L.S.E.
and 2Laboratoire TIMC-IMAGUniversity of Grenoble, FRANCE<first name>.<last name>@upmf-grenoble.frAbstractThis paper presents an iterative model ofknowledge acquisition of gender infor-mation associated with word endings inFrench.
Gender knowledge is representedas a set of rules containing exceptions.Our model takes noun-gender pairs as in-put and constantly maintains a list ofrules and exceptions which is both coher-ent with the input data and minimal withrespect to a minimum description lengthcriterion.
This model was compared tohuman data at various ages and showed agood fit.
We also compared the kind ofrules discovered by the model with rulesusually extracted by linguists and foundinteresting discrepancies.1 IntroductionIn several languages, nouns have a gender.
InFrench, nouns are either masculine or feminine.For example, you should say le camion (thetruck) but la voiture (the car).
Gender assignmentin French can be performed using two kinds ofinformation.
Firstly, lexical information, relatedto the co-occurring words (e.g., articles, adjec-tives) which most of times marks gender unam-biguously.
Secondly, sublexical information, es-pecially noun-endings, are pretty good predictorsof their grammatical gender (e.g., almost allnouns endings in ?age are masculine).
Severalword endings can be used to reliably predictgender of new words but this kind of rules isnever explicitly taught to children: they have toimplicitly learn that knowledge from exposure tonoun-gender pairs.
It turns out that children asyoung as 3 already constructed some of these?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.rules, which can be observed by testing them onpseudo-words (Karmiloff-Smith, 1979).This paper presents an iterative model of theway children may acquire this gender knowl-edge.
Its input is a large random sequence ofnoun-gender pairs following the distribution ofword frequency at a given age.
It is supposed torepresent the words children are exposed to.
Themodel constantly maintains a list of rules andexceptions both coherent with the input data andminimal with respect to an information theorycriterion.
This model was compared to humandata at various ages and showed a good fit.
Wealso compared the kind of rules discovered bythe model with rules usually extracted by lin-guists and found interesting discrepancies.2 Principle of SimplicityGender knowledge is learned from examples.Children are exposed to thousands of nounswhich are most of the time accompanied with agender clue because of their corresponding de-terminer or adjective.
For instance, when hearing?ta poussette est derri?re le fauteuil?
[yourstroller is behind the armchair], a child knowsthat poussette is feminine because of the femi-nine possessive determiner ta, and that fauteuil ismasculine because of the masculine determinerle.
After processing thousands of suchnoun/gender pairs, children acquired some gen-der knowledge which allows them to predict thegender of pseudo-words (Marchal et al, 2007;Meunier et al, 2008).
This knowledge is largelydependent on the end of the words since the end-ings of many nouns in French are associatedmore often with one gender than the other(Holmes & Segui, 2004).
For instance childrenwould predict that pseudo-words such as limetteor mossette are rather feminine words althoughthey never heard them before.
It means that theyshould have constructed a rule-like knowledgesaying that ?words ending in -ette are ratherfeminine?.
Or maybe it is ?words ending in -teare rather feminine?
or even ?words ending in -e73are rather feminine??
Actually, there are manyways to structure this knowledge, especially be-cause this kind of rule generally has exceptions.Let us take an example.
Consider the followingwords and their gender (masculine or feminine):barrage [weir] (m), image [image] (f), courage[courage] (m), plage [beach] (f), ?tage [floor](m), garage [garage] (m), collage [collage] (m).Several rules could be constructed from this data:(1) words ending in -age are masculine exceptimage and plage;(2) words ending in -age are feminine exceptbarrage, courage, ?tage, garage and collage;(3) words ending in -age are feminine exceptwords ending in -rage, ?tage and collage.The latter is an example of a rule whose excep-tions may themselves contain rules.
The questionis to know which rules may be constructed andused by children, and which cognitive mecha-nisms may lead to the construction of such rules.In order to investigate that issue, we relied on theassumption that children minds obey a principleof simplicity.This principle is a cognitive implementation ofthe Occam?s razor, saying that one should choosethe simplest hypothesis consistent with the data.This idea has already been used in the field ofconcept learning where it would dictate that weinduce the simplest category consistent with theobserved examples?the most parsimonious gen-eralization available (Feldman, 2003).
Chater &Vit?nyi (2003) view it as a unifying principle incognitive science to solve the problem of induc-tion in which infinitely many patterns are com-patible with any finite set of data.
They assume?that the learner chooses the underlying theory ofthe probabilistic structure of the language thatprovides the simplest explanation of the historyof linguistic input to which the learner has beenexposed.?
(Chater & Vit?nyi, 2007).One way to implement this idea is to considerthat the simplest description of a hypothesis isthe shortest one.
Without considering frequencyof the rule usage, rule 1 in the previous exampleseems intuitively more likely to be used by hu-mans because it is the shortest.Intuitively, counting the number of charactersof each hypothesis could seem a good methodbut it is better to choose the most compact repre-sentation (Chater, 1999).
More important, thechoice should also depend on the frequency ofrule usage: the description length of a rule thatwould be frequently used should not be countedlike a seldom used rule.
For instance, rule 2could be a more appropriate coding if it is usedvery frequently in the language as opposed to thefrequency of its exceptions.
That is the reasonwhy we rely on word frequencies for variousages in our simulations.Information theory provides a formal versionof this assumption: the minimum descriptionlength (MDL) principle (Rissanen, 1978).
Thegoal is to minimize the coding cost of both thehypothesis and the data reconstructed from thehypothesis (two-part coding).
However, we willsee that, in our case, the model contains all thedata which lead to a simpler mechanism: the ideais to select the hypothesis which represents thedata in the most compact way, that is which hasthe shortest code length.
Given a realization x ofa random variable X with probability distributionp, x can be optimally coded with a size of?log2(p(x)) bits.For instance, suppose you are exposed to only4 words A, B, C and D with frequencies .5, .25,.125, .125.
For example, exposure could be:BAACADBABACADBAA.
An optimal codingwould need only 1 bit (?log2(.5)) to code word Asince it occurs 50% of the time.
For instance, Awould be 0 and all other words would begin with1.
B needs 2 bits (?log2(.25)), for instance 10.
Cand D both needs 3 bits (?log2(.125)), for in-stance 110 for C and 111 for D.The average code length for a realization ofthe random variable X is computed by weightingeach code length by the corresponding probabil-ity.
It is exactly what is called entropy:H(X)= ?
?
p(x).log2(p(x))In the previous example, the average code lengthis 1?.5+2?.25+3?.125+3?.125=1.75 bitsFrom this point of view, learning is data com-pression (Gr?nwald, 2005).
To sum up, the gen-eral idea of our approach is to generate rules thatare coherent with the data observed so far and toselect the one with the smallest entropy.3 ModelSome computational models have been proposedin the literature, but they are concerned with theproblem of gender assignment given an existinglexicon rather than dynamically modeling theacquisition of gender knowledge.
Their input istherefore a set of words representative of all thewords in the language.
Analogical modeling(Skousen, 2003) is such a model.
It predicts thegender of a new word by constructing a set ofwords that are analogous to it, with respect to74morphology.
Matthews (2005) compared ana-logical modeling and a neural net and could notfind any significant difference.
Our model takesnoun-gender pairs as input and dynamically up-dates the set of rules it has constructed so far inorder to minimize their description length.3.1 InputThe input to our model is supposed to representthe noun/gender pairs children are exposed to.We used Manulex (L?t?
et al, 2004), a Frenchlexical database which contains word frequenciesof 48,900 lexical forms from the analysis of 54textbooks.
Word frequencies are provided for 3levels: grades 1, 2 and 3-5.We used the phonetic form of words2 becausethe development of the gender knowledge is onlybased on phonological data during the first sixyears of life.
It would also be interesting to studythe development of written-specific rules, butthis will be done in a future work.We constructed a learning corpus by randomlyselecting in this database 200,000 words andtheir gender such that their distribution is akin totheir frequency distribution in Manulex.
In otherwords, the probability of picking a given word inthe corpus is just its frequency.
In fact, we sup-pose that the construction of the rule depends onthe frequency of words children are exposed toand not just on the words at a type level.It would have been more accurate to take realcorpora as input, in particular because the orderin which words are considered probably plays arole, but such French corpora for specific ages,large enough to be sufficiently accurate, do notexist to our knowledge.We now present how our model handles thesenoun-gender pairs, one after the other.3.2 Knowledge RepresentationGender knowledge is represented as rules con-taining exceptions.
The premise of a rule is aword ending and the conclusion is a gender.
The* character indicates any substring preceding theword ending.
A natural language example of arule is:(4) */yR/ are feminine nouns (f) except/azyR/, /myR/, /myRmyR/ which are mascu-line (m).2We used an ASCII version of the International PhoneticAlphabet.Exceptions may contain words that could also beorganized in rules, which itselves may containexceptions.
Here is an example:(5) */R/?m except:/tiRliR/, /istwaR/?f*/jER/?f except /gRyjER/?m*/yR/?f except /azyR/ and /myR/?mThe gender knowledge corresponding to a givencorpus is represented as a set of such rules.
Sucha set contains about 80 rules for a grade-1 learn-ing corpus.
We now present how this knowledgeis updated according to a new noun-gender pairto be processed.3.3 Rule ConstructionEach time a new noun-gender pair is processed,all possible set of rules that are coherent with thedata are generated, and the best one, with respectto the minimum description length criterion, willbe selected.
As an example, consider this littlecurrent set of two rules which was constructedfrom the words /azyR/, /baRaZ/, /etaZ/, /imaZ/,/plaZ/, /SosyR/ and /vwAtyR/3 (words above be-low square brackets are the examples which wereused to form the rule):(6) */yR/?f [/SosyR/, /vwAtyR/] except/azyR/?m(7a) */aZ/?f [/imaZ/, /plaZ/] except/etaZ/, /baRaZ/?mThen a new word is processed: /kuRaZ/ which isof masculine gender.
Since it is not coherent withthe most specific rule (rule 7a) matching its end-ing (genders are different), the algorithm at-tempts to generalize it with the first-level excep-tions in order to make a new rule.
/etaZ/ is takenfirst.
It can be generalized with the new word/kuRaZ/ to form the new rule:(8a) */aZ/?m [/etaZ/, /kuRaZ/]All other exceptions which could be included areadded.
The new rule becomes:(8b) */aZ/?m [/baRaZ/, /etaZ/, /kuRaZ/]Once a new rule has been created, the algorithmneeds to maintain the coherence of the base.
Itchecks whether this new rule is in conflict withother rules with a different gender.
This is the3Translations: /azyR/ (azur [azure]), /baRaZ/ (bar-rage [weir]), /etaZ/ (?tage [floor]), /imaZ/ (image[image]), /plaZ/ (plage [beach]), /SosyR/ (chaus-sure [shoe]) and /vwAtyR/ (voiture [car])75case since we have the exact same rule but forthe feminine gender (rule 7a).
Conflicting exam-ples are therefore removed from the old rule andput as exceptions to the new rule.
In that case ofidentity between old and new rule, all examplesare removed and the rule disappears.
The newrule is:(8c) */aZ/?m [/baRaZ/, /etaZ/, /kuRaZ/] except/imaZ/, /plaZ/?fAfter having checked for rules with a differentgender, the algorithm now checks for existingrules with the same gender that the new rule, ei-ther more specific or more general.
This is notthe case here.
We thus created our first candidateset of rules (rules 6 and 8c):CANDIDATE SET #1:*/yR/?f [/SosyR/, /vwAtyR/] except/azyR/?m*/aZ/?m [/baRaZ, /etaZ/, /kuRaZ/] except/imaZ/, /plaZ/?fOther rules could have been generated from theset of exceptions of */aZ/?f.
The word /etaZ/ wastaken first but the algorithm needs to consider allother exceptions.
It then takes /baRaZ/ to formthe rule:(9) */RaZ/?m [/baRaZ/, /kuRaZ/]Note that this is a more specific rule than theprevious one: it is based on a 3-letter endingwhereas /etaZ/ and /kuRaZ/ generated a 2-letterending.
No other exceptions can be added.
Thealgorithm now checks for conflicting rules withthe same gender and puts this new rule as an ex-ception of the previous rule.
Then it checks forpossible conflict with rules of different gender,but there are none.
The second candidate set istherefore:CANDIDATE SET #2:*/yR/?f [/SosyR/, /vwAtyR/] except/azyR/?m*/aZ/?f [/imaZ/, /plaZ/] except/etaZ/?m*/RaZ/ [/baRaZ/, /kuRaZ/]?mSomething else needs to be done: removingwords from a rule and putting them as exceptionsmay lead to new generalizations between them orwith other existing words.
In our case, the algo-rithm memorized the fact that /imaZ/ and /plaZ/have been put as exceptions.It now applies the same mechanism as before:adding those words to the new set of rules, as ifthey were new words.
By the same previous al-gorithm, it gives the new rule:(7b) */aZ/?f [/imaZ/, /plaZ/]In order to maintain the coherence of the rulebase, examples of conflicting rules are removedand put as exceptions:(7c) */aZ/?f [/imaZ/, /plaZ/] except/baRaZ/, /etaZ/, /kuRaZ/?mWe now have our third candidate set of rules:CANDIDATE SET #3:*/yR/?f [/SosyR/, /vwAtyR/] except/azyR/?m*/aZ?f/ [imaZ,plaZ] except/etaZ/, /baRaZ/, /kuRaZ/?mFigure 1 summarizes the model?s architecture.Figure 1.
Overall architecture3.4 Model SelectionThis section describes how to choose betweencandidate models.
As we mentioned before, theidea is to select the most compact model.
Foreach exception, we compute its frequency F fromthe number of times it appeared so far.
For eachrule, F is just the sum of the frequencies of allexamples it covered.The description length of each rule or excep-tion is ?log2(F).
Since the overall value needs totake into account the variation of frequency ofeach rule or exception, each description length isweighted by its frequency, which gives the aver-age description length of a candidate set of rules(corresponding to the entropy):weigth(Model) = ??
?Fi.log2 (Fi)Suppose the words of the previous example weregiven in that order: /imaZ/ - /vwAtyR/ - /SosyR/- /imaZ/ - /plaZ/ - /SosyR/ - /plaZ/ - /imaZ/ -/etaZ/ - /vwAtyR/ - /baRaZ/ - /azyR/ - /plaZ/ -/imaZ/ - /imaZ/ - /kuRaZ/76Candidate set #2 would then have an averagedescription length of 1.875 bits:azyR m-1/16 x log2(1/16) = .25*yR f SosyR,vwAtyR-4/16 x log2(4/16) = .5*RaZ m baRaZ,kuRaZ-2/16 x log2(2/16) = .375etaZ m-1/16 x log2(1/16) = .25*aZ f imaZ,plaZ-8/16 x log2(8/16) = .5Sum = 1.875 bitsIn the same way, candidate set #1 would have avalue of 2.18 bits.
Candidate set #3 would have avalue of 2 bits.
The best model is thereforemodel #2 which is the most compact one, ac-cording to the word frequencies.4 ImplementationFor computational purposes, the knowledge in-ternal representation is slightly different than theone we use here: rules and exceptions are repre-sented on different lines such that exceptions arewritten before their corresponding rules and if arule is more specific than another one, it is writ-ten before.
For instance, candidate set #2 is writ-ten that way:azyR m*yR f SosyR,vwAtyR*RaZ m baRaZ,kuRaZetaZ m*aZ f imaZ,plaZThis allows a linear inspection of the rule base inorder to predict the gender of a new word: thefirst rule which matches the new word gives thegender.
For instance, if the previous model wereselected, it would predict that the word /caZ/ isfeminine, the pseudo-word /tapyR/ is feminineand the pseudo-word /piRaZ/ is masculine.We could have improved the efficiency of thealgorithm by organizing words in a prefix treewhere the keys would be in the reverse order ofwords.
However, we are not concerned with theefficiency of the model for the moment, butrather its ability to account for human data.The algorithm is the following (R1<R2 indi-cates that R1 is more specific than R2.
For in-stance, */tyR/ is more specific than */yR/, whichin turn is more specific than */R/).updateModel(word W, rule base B):if W matches a rule R?B thenif R did not contain W as an exampleadd W to the examples of Breturn Belsefor all exceptions E of Bif E and W can be generalizedcreate the new rule N from theminclude possible other exceptions# More general rule of different genderif ?R?B/ R<N and gender(R)?gender(N)put examples of N matching R as exceptionsmemorize those exceptionsif N now contains one exampleput that example as an exceptionif N contains no examplesremove N# More specific rule of different genderif ?R?B/ R?N and gender(R)?gender(N)put examples of R matching N as exceptionsmemorize those exceptionsif R now contains one exampleput that example as an exceptionif R contains no examplesremove R# Conflicting rule of same genderif ?R?B/ N>R and gender(R)=gender(N)include R into Nif ?R?B/ N<R and gender(R)=gender(N)include N into RSolutions = {B}# Run the algorithm with new exceptionsfor all memorized exceptions ESolutions=Solutions ??
updateModel(E,B)if no generalizations was possibleAdd W to BSolutions = {B}return(Solutions)5 SimulationsWe ran this model on two corpora, representingwords grade-1 and grade-2 children are exposedto (each 200,000-word long).
76 rules were ob-tained in running the grade-1 corpus, and 83rules with the grade-2 corpus.End-ingsGen-derGenderPredict-abilityNbExam-plesNbexcep-tions*/l/ f 56% 79 62*/sol/ m 57% 4 3*/i/ m 57% 74 55*/R/ m 72% 188 71*/am/ f 77% 7 2*/sy/ m 83% 5 1*/jER/ f 88% 31 4*/5/ m 97% 91 2*/fon/ m 100% 5 0*/sj6/ f 100% 58 0Table 1.
Sample of rules (with endings and pre-dicted gender) constructed from grade-1 corpus.77Some of the rules of the first set are listed inTable I (from grade-1 corpus).
For each rule, rep-resented by a word ending, is detailed its pre-dicted gender, the number of words (as types)following the rule, the number of exceptions.Moreover, the ?gender predictability?
of eachrule is computed (third column) as the percentageof words matching the rule over the total numberof words with this ending.The results of the simulations show that thelengths of word endings vary from only one pho-neme (e.g., /*l/, /*i/) to three (/*jER/, /*fon/).These rules do not really correspond to the kindof rules linguists would have produced.
Theyusually consider that the appropriate ending toassociate to a given gender is the suffix (Riegelet al, 2005).
Actually, the nature of the wordending that humans may rely on to predict gen-der is an open question in psycholinguistics.
Dowe rely on the suffix, the last morpheme, the lastphoneme?
The results of our model which didnot use any morphological knowledge, suggestsanother answer: it may only depend on the statis-tical regularities of word endings in the languageand can vary in French from one phoneme tothree and these endings are sometimes matchingmorphological units.However, it is worth noting that the model hasyet some obvious limitations.
The first one is thatthe gender predictability of rules is variable:while some rules are highly predictive (e.g.,*/sj?/ 100% feminine, */@/ 97% masculine),other are not (e.g., */l/ 56% feminine, */i/ 57%masculine).
The second limitation is that therules found by our model are accounting for avariable amount of examples.
For instance, therule */R/ masculine accounts for 188 exampleswhile */sol/ masculine does only 4.
One couldwonder what it means from a developmentalpoint of view to create rules that are extractedfrom very few examples.
Do children build suchrules?
This is far from sure and we shall have tofurther address these clear limitations.Another of our research goals was to test towhat extent our model could predict human data.To that end, the model?s gender assignment per-formance was compared to children?s one.6 Comparison to Experimental Data6.1 ExperimentAn experiment was conducted to study how andwhen French native children acquire regularitiesbetween words endings and their associated gen-der.
Nine endings were selected, five which aremore likely associated to the feminine gender(/ad/, /asj?/, /El/, /ot/, /tyR/) and four to the mas-culine gender (/aZ/, /m@/, /waR/, /O/).
Two listsof 30 pseudo-words were created containing each15 pseudo-words whose expected gender is mas-culine (such as ?brido?
or ?rinloir?)
and 15whose expected gender is feminine (such as?surbelle?
or ?marniture?).
The presentation ofeach list was counterbalanced across participants.Participants were 136 children from Grenoble(all French native speakers): 28 children at theend of preschool, 30 children at the beginning ofgrade 1, 36 children at the end of grade 1 and 42children at the beginning of grade 2.
Each par-ticipant was given a list and had to perform acomputer-based gender decision task.
Eachpseudo-word was simultaneously spoken anddisplayed in the center of the screen when thedeterminers ?le?
(masculine) and ?la?
(feminine)were displayed at the bottom of the screen.
Thenchildren had to press the keyboard key corre-sponding to their intuition, which was recorded.Pre-schoolBeg.Grade1EndGrade1Beg.Grade2End-ings Gd.% Exp.Gd.% Exp.Gd.% Exp.Gd.% Exp.Gd./ad/ f 45.24 56.67 67.59** 57.14/asj?/ f 58.33 58.89 70.37** 65.08**/El/ f 60.71* 62.22* 76.85** 64.29**/ot/ f 53.57 71.11** 82.41** 72.22**/tyR/ f 50.00 68.89** 77.78** 68.25**/aZ/ m 51.19 64.44** 64.81** 61.11**/m@/ m 60.71* 55.56 57.41 50.00/O/ m 61.90* 65.56** 80.56** 78.57**/waR/ m 52.38 62.22* 64.81** 68.25**Legend: Gd.
:Gender; Beg.
:Beginning;% Exp.
Gd.
:% Expected Gender;* p<.05,**p<.01Table 2.
Gender attribution rate as a function ofendings and grade level.In brief, results are twofold.
First, childrenhave acquired some implicit knowledge regard-ing gender information associated with wordending.
As can be seen in Table 2, at the begin-ning of grade 1, children respond above chanceand in the expected direction for the majority ofendings (Chi2 test was used to assess statisticalsignificance).
At preschool children respondedalso above chance for three word endings.
Sec-ond, there is a clear developmental trend sincegender attribution increases in the expected di-rection with grade level and more endings aredetermined by the older children.
The exposure78to written language during the first school yearprobably reinforces the implicit knowledge de-veloped by children before primary school.6.2 Human vs. Model Data ComparisonTwo types of analyses were drawn in order tocompare model and data.
Firstly, the gender pre-dictions obtained from the model were correlatedto those given by children, regarding the genderof pseudo-words.
Secondly, the endings createdby the model were compared  to those used in theexperimental material.
Correlations were com-puted between our model and human data (Table3) by taking into account the rate of predictedmasculine gender, for each pseudo-word.Model Grade 1 Model Grade 2Preschool 0.31 0.33Beg.
Grade 1 0.6 0.64End Grade 1 0.82 0.86Beg.
Grade 2 0.74 0.77Table 3.
Correlations between model and data.The highest correlations are obtained for childrenat the end of grade 1 and at the beginning ofgrade 2.
This result is interesting since the cor-pora are precisely intended to represent the lexi-cal knowledge corresponding to the school levelof these children.
Moreover, the correlations ob-tained with the grade-2 model are higher (thoughnot significantly) than those obtained with thegrade-1 model.
It thus seems that our model isfairly well suited to account for children?s re-sults, at least for the older ones.
The low correla-tions observed with the younger children of oursample cannot be interpreted unambiguously;one could say that children before grade 1 havenot built much knowledge regarding gender ofword endings but this conclusion contradictsprevious results (Meunier et al, 2008) and it re-mains to be explored by using a corpora appro-priated to the lexicon of preschool children.The endings used by the model to predict thegender of pseudo-words were also comparedwith the endings used in the experiment.
Table 4presents these endings as well as the rate of mas-culine gender predicted for the experimental end-ings by the two models trained with grade-1 andgrade-2 lexicons.
First, note that the endingsused by the models are the same for both grade-1and grade-2 lexicons.
The growth of the lexiconbetween grade 1 and grade 2 does not modifythese rules.
Secondly, one can notice that grade-2model results are more defined than grade-1 re-sults.
Third, a very salient result is that modelendings are short.
For example, the model didnot create a rule such */ad/ and rather used themore compact rule */d/ to predict the gender ofthe pseudo-word /bOSad/.Model Grade 1 Model Grade 2EndingsEnd-ings% Gd.MascEnd-ings% Gd.Masc/ad/ */d/ 0.28 */d/ 0.17/asj?/ */sj?/ 0 */sj?/ 0/El/ */l/ 0.44 */l/ 0.32/ot/ */t/ 0.14 */t/ 0.09/tyR/ */yR/ 0.09 */yR/ 0.05/aZ/ */Z/ 0.8 */Z/ 0.91/m@/ */@/ 0.95 */@/ 0.98/O/ */O/ 0.93 */O/ 0.96/waR/ */R/ 0.72 */R/ 0.82Table 4.
Rate for expected masculine genderpredicted by our models.In fact, the majority of the endings used by themodel are short, i.e.
composed with one pho-neme.
Very few endings created by the model aremorphological units such as suffixes.
In fact, theendings /d/ or /R/ are not derivational mor-phemes, but the endings /sj?/ or /yR/ are suffixes.So the MDL-based model establishes rules thattake into account different types of linguisticunits from phonemes to morphemes dependingof the statistical predictability of each endingtype.
This result is related to an important con-cern about the study of the acquisition of gram-matical gender: to which unit do children rely onto predict gender?
Do they rely on the last pho-neme, biphone, morpheme?7 Do children rely on morphemes?In grammatical gender acquisition studies, thekind of endings used often mixes up phonologi-cal, derivational and even orthographic cues.Several studies used true suffixes (Marchal et al,2007, Meunier et al, 2008) to ask children toassign gender to pseudo-words.
As those studiesconsistently showed that children from 3 yearsold onwards assign a gender to those pseudo-words following the excepted suffix gender, thetentative conclusion was to say that children relyon suffixes to assign the gender of new words.This is an appealing interpretation as the devel-opment of morphological structure of words is animportant aspect of lexical development andsome of this knowledge is acquired very early(Casalis et al, 2000; Karmiloff-Smith, 1979).79However, the observations from the MDL-based model strongly question this assumption:the units retained in the model?s rules are oftenshorter than suffixes and the last phoneme seemsoften as predictive as the suffix itself as it leadsto satisfying correlations with children?s data.So, one would conclude that gender knowl-edge is not attached to morphological units suchas suffix but is rather a knowledge associatedwith the smaller ending segment that best pre-dicts gender.
Note however that despite the highcorrelations observed, the actual gender predic-tions issued from children?s data and those is-sued from the model are not exactly of the samemagnitude and this would suggest that the MDL-based model presented here must still be workedon in order to better describe gender acquisition.For example, the notion of gender predictabilitywould benefit from being computed from tokencounts instead of type counts.8 ConclusionThe purpose of this research was to know whichkind of gender information may be constructedand used by children, and which cognitivemechanisms may lead to the construction of suchrules.
To investigate that issue, we constructed amodel based on the MDL principle which revealsto be an interesting way to describe the gram-matical gender acquisition in French, althoughwe do not claim that children employ such analgorithm.
Our model predicts the gender of anew word by sequentially scanning exceptionsand rules.
This process appears quite similar tothe decision lists technique in machine learning(Rivest, 1987) which has already been combinedwith the MDL principle (Pfahringer, 1997).However, we are not committed to this formal-ism: we are more interested in the content of themodel rather than its knowledge representation.The comparison between model?s results andhuman data opens a way of reflection on the kindof relevant units on which children would relyon.
Perhaps it is not a kind of ending in particularthat plays a role but different units varying fol-lowing the principle of parsimony.ReferencesCasalis, S., Louis-Alexandre, M.-F. (2000).
Morpho-logical analysis, phonological analysis and learningto read French.
Reading and Writing, 12, 303-335.Chater, N. (1999).
The search for simplicity: A fun-damental cognitive principle?
Quarterly Journal ofExperimental Psychology, 52A, 273-302.Chater, N., & Vit?nyi, P. (2003).
Simplicity: a unify-ing principle in cognitive science?
Trends in Co-gnitive Sciences, 7(1), 19-22.Chater, N., & Vitanyi, P.  (2007) ?Ideal learning?
ofnatural language: Positive results about learningfrom positive evidence.
Journal of MathematicalPsychology, 51(3), 135-163.Feldman, J.
(2003).
Perceptual Grouping by Selectionof a Logically Minimal Model.
International Jour-nal of Computer Vision, 55(1), 5-25.Gr?nwald, P. (2005).
Minimum description lengthtutorial.
In P. D. Gr?nwald, I. J. Myung & M.
Pitt(Eds.
), Advances in MDL: Theory and Applications(pp.
23-80).
Cambridge: MIT Press.Holmes, V.M., & Segui, J.
(2004).
Sublexical andlexical influences on gender assignment in French.Journal of Psycholinguistic Research, 33(6), 425-457.Karmiloff-Smith, A.
(1979).
A functional approach tochild language.
Cambridge University Press.L?t?, B., Sprenger-Charolles, L., & Col?, P. (2004).MANULEX: A grade-level lexical database fromFrench elementary-school readers.
Behavior Re-search Methods, Instruments, & Computers, 36,156-166.Marchal, H., Bianco, M., Dessus, P. & Lemaire, B.(2007).
The Development of Lexical Knowledge:Toward a Model of the Acquisition of LexicalGender in French.
Proceedings of the 2nd Euro-pean Conference on Cognitive Science, 268-273.Matthews, C. A.
(2005).
French gender attribution onthe basis of similarity: A comparison between AMand connectionist models.
Journal of QuantitativeLinguistics, 12(2-3), 262-296.Meunier, F., Seigneuric, A., Spinelli, E. (2008).Themorpheme gender effect.
Journal of Memory andLanguage, 58, 88-99.Pfahringer, B.
(1997).
Compression-Based Pruning ofDecision Lists, in Proceedings of the 9th EuropeanConference on Machine Learning, 199-212.Riegel, M., Pellat, J.C., & Rioul, R. (2005).
Gram-maire m?thodique du fran?ais.
Paris: PUF.Rissanen, J.
(1978).
Modeling by shortest data de-scription.
Automatica, 14, 465-471.Rivest, R.L.
(1987).
Learning Decision Lists.
Ma-chine Learning 2,3 (1987), 229-246.Skousen, R. (2003).
Analogical Modeling: Exemplars,Rules, and Quantum Computing.
Berkeley Linguis-tics Society.80
