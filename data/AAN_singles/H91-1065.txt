STUDIES  IN PART OF SPEECH LABELL INGMarie Meteer, Richard Schwartz, Ralph WeischedelBBN Systems and Technologies10 Moulton St.Cambridge, MA 02138ABSTRACTWe report here on our experiments with POST (Part of SpeechTagger) to address problems of ambiguity and of understandingunknown words.
Part of speech tagging, per se, is a wellunderstood problem.
Our paper reports experiments in threeimportant areas: handling unknown words, limiting the size of thetraining set, and returning a set of the most likely tags for eachword rather than a single tag.
We describe the algorithms that weused and the specific results of our experiments on Wall StreetJournal articles and on MUC terrorist messages.1.
INTRODUCTION 1Natural language processing, and AI in general, havefocused mainly on building rule-based systems withcarefully handcrafted rules and domain knowledge.
Our ownnatural language database query systems, JANUS 2,Par lance ruz, and Delphi 4, use these techniques quitesuccessfully.
However, as we move from the problem ofunderstanding queries in fixed domains to processing opentext for applications such as data extraction, we have foundrule-based techniques too brittle, and the amount of worknecessary to build them intractable, especially whenattempting to use the same system on multiple domains.We report in this paper on one application of probabilisticmodels to language processing, the assignment of part ofspeech to words in open text.
The effectiveness of suchmodels is well known (Church 1988) and they are currentlyin use in parsers (e.g.
de Marcken 1990).
Our work is anincremental improvement on these models in two ways: (1)We have run experiments regarding the amount of trainingdata needed in moving to a new domain; (2) we have addedprobabilistic models of word features to handle unknownwords effectively.
We describe POST and its algorithms andthen we describe our extensions, howing the results of ourexperiments.1 The work reported here was supported by the Advanced ResearchProjects Agency and was monitored by the Rome Air DevelopmentCenter under Contract No.
F30602-87-D-0093.
The views andconclusions contained in this document are those of the authors andshould not be interpreted as necessarily representing the officialpolicies, whether expressed or implied, of the Defense AdvancedResearch Projects Agency or the United States Government.2 Weischedel, et al 1989.3 Parlance is a trademark of BBN Systems and Technologies.4 Stallard, 1989.2.
POST: USING PROBABILITIES TOTAG PART OF SPEECHPredicting the part of speech of a word is onestraightforward way to use probabilities.
Many words areseveral ways ambiguous, such as the following:a round table: adjectivea round of cheese: nounto round out your interests: verbto work the year round: adverbEven in context, part of speech can be ambiguous, as in thefamous example: "Time flies."
where both words are twoways ambiguous,  result ing in two grammaticalinterpretations a sentences.Models predicting part of speech can serve to cut down thesearch space a parser must consider in processing knownwords and make the selection among alternatives moreaccurate.
Furthermore, they can be used as one input tomore complex strategies for inferring lexical and semanticinformation about unknown words.2.1 The n-gram modelIf we want to determine the most likely syntactic part ofspeech or tag for each word in a sentence, we can formulate aprobabilisfic tagging model.
Let us assume that we want toknow the most likely tag sequence, T, given a particularword sequence, W. Using Bayes' rule we can write the aposteriori probability of tag sequence T given word sequenceasrcrl3:1~.~9)'= P('13P(Wrl3P(W)where P(T) is the a priori probability of tag sequence T,P(WIT) is the conditional probability of word sequence Woccurring iven that a sequence of tags T occurred, and P(W)is the unconditioned probability of word sequence W. Then,in principle, we can consider all possible tag sequences,evaluate the a posteriori probability of each, and choose theone that is highest.
Since W is the same for allhypothesized tag sequences, we can disregard P(W).We can rewrite the probability of each sequence as aproduct of the conditional probabilities of each word or taggiven all of the previous tags.331P(TIW) P(W) = I /  P(t0) P(q I ti_ 1 ,t i-2,.--)p(w i I t i..,w i-1..)Now, we can make the approximation that each tagdepends only the immediately preceding tags (say the twopreceding tags for a tri-tag model), and that the word dependsonly on the tag.P(TIW) P(W) = P(t0) lII p(t i I ti_ 1,ti_2) p(w i I t i)That is, once we know the tag that will be used, we gain nofurther information about he likely word from knowing theprevious tags or words.
This model is called a Markovmodel, and the assumption is frequently called the Markovindependence assumption.If we have sufficient training data then we can estimate thetag n-gram sequence probabilities and the probability of eachword given a tag (lexical probabilities).
We use robustestimation techniques that take care of the cases ofunobserved events (i.e.
sequences of tags that have notoccurred in the training data).
However, in real-worldproblems, we also are likely to have words that were neverobserved at all in the training data.
The model given abovecan still be used, simply by defining a generic new wordcalled "unknown-word".
The system can then guess at thetag of the unknown word primarily using the tag sequenceprobabilities.
We return to the problem of unknown wordsin Section 3.Using a tagged corpus to train the model is called"supervised training", since a human has prepared the correcttraining data.
We conducted supervised training to deriveboth a bi-tag and a tri-tag model based on a corpus from theUniversity of Pennsylvania.
The UPenn corpus, which wascreated as part of the TREEBANK project (Santorini 1990)consists of Wall Street Journal (WSJ) articles.
Each wordor punctuation mark has been tagged with one of 47 parts ofspeech 5, as shown in the following example:Terms/NNS were/VBD not/RB disclosed/VBN.
/ 6A bi-tag model predicts the relative likelihood of aparticular tag given the preceding tag, e.g.
how likely is thetag VBD on the second word in the above example, giventhat the previous word was tagged NNS.
A tri-tag modelpredicts the relative likelihood of a particular tag given thetwo preceding tags, e.g.
how likely is the tag RB on thethird word in the above example, given that the twoprevious words were tagged NNS and VBD.
While the bi-tagmodel is faster at processing time, the tri-tag model has alower error ate.5 Of the 47 parts of speech, 36 are word tags and 11 punctuationtags.
Of the word tags, 22 are tags for open class words and 14 forclosed class words.6 NNS is plural noun; VBD is past ense verb; RB is adverbial; VBNis past participle verb.The algorithm for supervised training is straightforward.One counts for each possible pair of tags, the number oftimes that the pair was followed by each possible third tag,and then derived from those counts a probabilistic tri-tagmodel.
One also estimates from the training data theconditional probability of each particular word given aknown tag (e.g., how likely is the word "terms" if the tag isNNS); this is called the "word emit" probability.
Theprobabilities were padded to avoid setting the probability forunseen M-tags or unseen word senses to zero.Given these probabilities, one can then find the mostlikely tag sequence for a given word sequence.
Using theViterbi algorithm, we selected the path whose overallprobability was highest, and then took the tag predictionsfrom that path.
We replicated the result (Church 1988) thatthis process is able to predict he parts of speech with only a3-4% error rate when the possible parts of speech of each thewords in the corpus are known.
This is in fact about therate of discrepancies among human taggers on theTREEBANK project (Marcus, Santorini & Magerman1990).2.2 Quantity of training dataWhile supervised training is shown here to be veryeffective, it requires acorrectly ta~ed corpus.
We have donesome experiments to quantify how much tagged data isreally necessary.In these xperiments, we demonstrated that the training setcan, in fact, be much smaller than might have beenexpected.
One rule of thumb suggests that the training setneeds to be large enough to contain on average 10 instancesof each type of tag sequence inorder for their probabilities tobe estimated with reasonable accuracy.
This would implythat a M-tag model using 47 possible parts of speech wouldneed a bit more than a million words of training.
However,we found that much less training data was necessary.It can be shown that a good estimate of the probability ofa new event is the sum of the probability of all the eventsthat occurred just once.
However, if the average number oftokens of each event hat as been observed is 10, then thelower bound on the probability of new events is 1/10.
Thusthe likelihood of a new tri-gram is fairly low.
In a M-grammodel of part of speech, an event is a particular sequence oftags.
While theoretically the set of possible events is allpermutations of the tags, in practice only a relatively smallnumber of tag sequences actually occur.
We found only6,170 unique triples in our training set, out of a possible97,000.
This would suggest that only 60,000 words wouldbe sufficient for training.In our experiments, the error rate for a supervised tri-tagmodel increased only from 3.30% to 3.87% when the sizeof the training set was reduced from 1 million words to64,000 words.
This is probably because most of thepossible tri-tag sequences never actually appear.
All that is332really necessary, recalling the rule of thumb, is enoughtraining to allow for 10 of each of the tag sequences that doOCCUr.This result is applicable to new tag sets, subdomains, orlanguages.
By beginning with a measure of the number ofevents that actually occur in the data, we can more preciselydetermine the amount of data needed to train the probabilisticmodels.
In applications such as tagging, where a significantnumber of the theoretically possible events do not occur inpractice, we can use supervised training of probabilisticmodels without needing prohibitively large corpora.3.
UNKNOWN WORDSSources of open-ended text, such as a newswire, presentnatural language processing technology with a majorchallenge: what to do with words the system has never seenbefore.
Current echnology depends on handcrafted linguisticand domain knowledge.
For instance, the system thatperformed most successfully in the evaluation of software toextract data from text at the 2nd Message UnderstandingConference held at the Naval Ocean Systems Center, June,1989, would simply halt processing a sentence when a newword was encountered.Determining the part of speech of an unknown word canhelp the system to know how the word functions in thesentence, for instance, that it is a verb stating an action orstate of affairs, that it is a common noun stating a class ofpersons, places, or things, that it is a proper noun naming aparticular person, place, or thing, etc.
If it can do that well,then more precise classification and understanding isfeasible.Using the UPenn set of parts of speech, unknown wordscan be in any of the 22 open-class parts of speech.
The tri-tag model can be used to estimate the most probable one.Random choice among the 22 open classes would beexpected to show an error rate for new words of 95%.
Thebest previously reported error rate was 75% (Kuhn & deMoil 1990).In our first tests using the tri-tag model we showed anerror rate of only 51.6%.
However, this model only tookinto account he context of the word, and no informationabout he word itself.
In many languages, including English,the word endings give strong indicators of the part of speech.Furthermore, capitalization i formation, when available, canhelp to indicate whether aword is a proper noun.We developed a probabilistic model that takes into accountfeatures of the word in determining the likelihood of theword given a part of speech.
This was used instead of the"word emit" probabilities for known words that the systemobtained from training.
To develop the model, we firstdetermined the features we thought would distinguish partsof speech.
There are four independent 7 categories of features:inflectional endings, denvational endings, hyphenation, andcapitalization.
Our initial test had three inflectional endings(-ed, -s, -ing), and 32 denvational endings, (including -ion, -al, -ive, -ly).
Capitalization has four values, in our system(+ initial + capitalized, - initial + capitalized, etc.)
in orderto take into account he first word of a sentence.
We canincorporate these features of the word into the probabilitythat this particular word will occur given a particular tagusingp(wj I t i = p(unknown-word I ti) *p(Capital - feature I t i) *p(endings/hyph I t i )We estimate the probability of each ending for each tagbased on the training data.
While these probabilities are notstrictly independent, the approximation is good enough tomake a marked difference in classification of unknownwords.
As the results in Figure 1 shows, the use of theorthographic endings of the words reduces the error rate onthe unknown words by a factor of 3.We tested capitalization separately, since some data, suchas that in the Third Message Understanding Conference isupper case only.
Titles and bibliographies will causesimilar distortions in a system trained on mixed case andusing capitalization as a feature.
Interestingly, thecapitalization feature contributed very little to the reductionin error rates, whereas using the word features contributed agreat deal.3132C10No Only Endings and AllFeatures Features Capitalization hyphenationOverall Error RateError rate for Known wordsError rate for Unknown words4~3Figure 1: Decreasing error ate with use of word features7 These are not necessarily independent, though we are treatingthem as such for our tests.333In sum, adding a probability model of typical endings ofwords to the tri-tag model has yielded an accuracy of 82% forunknown words.
Adding a model of capitalization to theother two models further increased the accuracy to 85%.
Thetotal effect of BBN's model has been a reduction of a factorof five in the error rate of the best previously reportedperformance.4.
K-BEST TAG SETSAn alternative mode of running POST is to return the setof most likely tags for each word, rather than a single tag foreach.In our first test, the system returned the sequence of mostlikely tags for the sentence.
This has the advantage ofeliminating ambiguity; however, even with a rather lowerror rate of 3.7%, there are cases in which the systemreturns the wrong tag, which can be fatal for a parsingsystem trying to deal with sentences averaging more than 20words in length.We addressed this problem by adding the ability of thetagger to return for each word an ordered list of tags, markedby their probability using the Forward Backward algorithm.The Forward Backward algorithm is normally used inunsupervised training to estimate the model that finds themaximum likelihood of the parameters of that model.
Weuse it in determining the k-best tags for each word bycomputing for each tag the probability of the tag occurringat that position and dividing by the probability of the wordsequence given this model.The following example shows k-best agging output, withthe correct ag for each word marked in bold.
Note that theprobabilities are in natural og base e. Thus for eachdifference of 1, there is a factor of 2.718 in the probability.Bailey Controls, based in Wickliffe Ohio, makescomputerized industrial controls ystems.Bailey (NP.
-1.17) (RB.
-1.35) (FW.
-2.32) (NN.
-2.93)(NPS.
-2.95) (JJS.
-3.06) (JJ.
-3.31) (LS.-3.41) (JJR.-3.70) (NNS.-3.73) (VBG.-3.91)...Controls (VBZ.-0.19) (NNS.
-1.93) (NPS.
-3.75) (NP.
-4.97)based (VBN.
-0.0001)in (IN.
-.001) (RBV.
-7.07) (NP.
-9.002)Wickliffe (NP.
-0.23) (NPS.
-1.54)Ohio (NP.
-0.0001)makes (VBZ.-0.0001)computerized (VBN.
-0.23) (JJ.
-1.56)industrial (JJ.-0.19) (NP.
-1.73)controls (NNS.
-0.18) (VBZ.
-1.77)systems (NNS.-0.43) (NPS.
-1.56) (NP.
-1.95)Figure 2: K-best Tags and ProbabilitiesIn two of the words ("Controls" and "computerized") thefirst tag is not the correct one.
However, in all instances thecorrect ag is included in the set.
Note the first word,"Bailey", is unknown to the system, therefore, all of theopen class tags are possible.In order to reduce the ambiguity further, we tested variousways to limit how many tags were returned based on theirprobabilities.
Often one tag is very likely and the others,while possible, are given a low probability, as in the word"in" above.
Therefore, we tried removing all tags whoseprobability is more than e 2 less likely than the most likelytag.
So only tags within the threshold 2.0 of the mostlikely would be included (i.e.
if the most likely tag had a logprobability of -0.19, only tags with a log probability greaterthan -2.19 would be included).
This reduced the ambiguityfor known words from 1.93 tags per word to 1.23, and forunknown words, from 15.2 to 2.0.However, the negative side of using cut offs is that thecorrect ag may be excluded.
Note that a cut off of 2.0would exclude the correct ag for the word "Controls" above.By changing the cut off to 4.0, we are sure to include all thecorrect ags in this example, but the ambiguity for knownwords raises from 1.23 to 1.24 and for unknown words from2.0 to 3.7, for an ambiguity rating of 1.57 overall.We are continuing experiments o determine the mosteffective way of limiting the number of tags returned, andhence decreasing ambiguity, while ensuring that the correcttag is likely to be in the set.5.
MOVING TO A NEW DOMAINIn all of the tests discussed so far, we both trained andtested on sets of articles in the same domain, the Wall StreetJournal texts used in the Penn Treebank Project.
However,an important measure of the usefulness of the system is howwell it performs in other domains.
While we would notexpect high performance in radically different kinds of text,such as transcriptions ofconversations ortechnical manuals,we would hope for similar performance on newspaperarticles from different sources and on other topics.We tested this hypothesis using data from the ThirdMessage Understanding Conference (MUC-3).
The goal ofMUC-3 is to extract data from texts on terrorism in LatinAmerican countries.
The texts are mainly newspaperarticles, although there are some transcriptions of interviewsand speeches.
The University of Pennsylvania TREEBANKproject agged four hundred MUC messages (approximately100,000 words), which we divided into 90% training and10% testing.334luo20100Average # 1Tags_- Overall Error Rate~ ~.
.
Known Words.~.~.~ .~,.~ .
....
Unknown Wordst = 2 4 6 121.1 1.2 1.3 3Thresho ldFigure 4: Comparison ofFor our first test, we used the original probability tablestrained on the Wall Street Journal articles.
We then retrainedthe probabilities on the MUC messages and ran a secondtest, with an average improvement of three percentage pointsin both bi- and tri- tags.
The full results are shown below:BITAGS: TEST 1 TEST 2Overall error ate: 8.5 5.6Number of correct tags: 10340 10667Number of incorrect tags: 966 639Error rate for known words: 6.3 4.6Error rate for unknown words: 25 16TRITAGS:Overall error ate: 8.3 5.7Number of correct tags: 10358 10651Number of incorrect tags: 948 655Error rate for known words: 5.9 4.6Error rate for unknown words: 26 18Figure 3: Comparison of original and trained probabilitiesWhile the results using the new tables are animprovement in these first-best tests, we saw the bestresults using K-best mode, which obtained a .7% error rate.We ran several tests using our K-best algorithm withvarious thresholds.
As described in Section 4, the thresholdlimits how many tags are returned based on theirprobabilities.
While this reduces the ambiguity compared toconsidering all possibilities, it also increases the error rate.Figure 4 shows this tradeoff rom effectively no threshold,thresholds for K- Beston the right hand side of the g~aph (shown in the figure as athreshold of 12), which has a .7% error rate and anambiguity of 3, through a cut off of 2, which has a error rateof 2.9, but an ambiguity of nearly zero--i.e, one tag preword.
(Note the far left of the graph is the error rate for acut off of 0, that is, only consideering the first of the k-besttags, which is approximately the same as the bi-tag errorrate shown'in Figure 3.)6.
USING DICT IONARIESIn all of the results reported here, we are using word/partof speech tables derived from training, rather than on-linedictionaries to determine the possible tags for a given word.The advantage of the tables is that the training provides theprobability of a word given a tag, whereas the dictionarymakes no distinctions between common and uncommonuses of a word.
The disadvantage of this is that uses of aword that did not occur in the ~aining set will be unknownto the system.
For example, in the training portion of theWSJ corpus, the word "put" only occurred as verb.However, in our test set, it occurred as a noun in thecompound "put option".
Since for efficiency reasons, weonly consider those tags known to be possible for a word,this will cause an error.We are currently integrating on-line dictionaries into thesystem, so that alternative word senses will be considered,335while still not opening the set of tags considered for aknown word to all open class tags.
This will notcompletely eliminate the problem, since words are oftenused in novel ways, as in this example from a public radioplea for funds: "You can Mastercard your pledge.".
We willbe rerunning the experiments reported here to evaluate theeffect of using on-line dictionaries.7.
FUTURE D IRECT IONSIn the work reported here, we have evaluated POST in thelaboratory, comparing its results against he work of peopledoing the same task.
However, the real test of such asystem is how well it functions as a component in a largersystem.
Can it make a parser work faster and moreaccurately?
Can it help to extract certain kinds of phrasesfrom unrestricted text?
We are currently running theseexperiments by making POST a part of existing systems.
Itis being run as a preprocessor to Gfishman's Proteus ystemfor the MUC-3 competition (Gnshman & Sterling 1989).Preliminary results howed it sped up Proteus by a factor oftwo in one-best mode and by a factor of 33% with athreshold of T=2.
It is also being integrated into a newmessage processing system at BBN.
The results of theseexperiments will provide us with new directions and ideasboth for improving POST and for other ways to integrateprobabilistic models into natural language processingsystems.ACKNOWLEDGEMENTSWe would like to acknowledge Lance Ramshaw for hiswork on POST and the results on the size of the trainingset.
We have also benefited from discussions with KenChurch.REFERENCESAyuso, D.M., Boisen, S., Bobrow R., Gish, H., Ingria, R.,Meteer, M., Schwartz, R. and Weischedel, R. (1990a)Adaptive Natural Language Processing: Mid-projectReview.
BBN Technical Report 7524, December 1990.Ayuso, D.M., Bobrow R., MacLaughlin, D., Meteer, M.,Ramshaw, L., Schwartz, R. and Weischedel, R. (1990b)Toward Understanding Text with a Very Large Vocabulary.In Proceedings of the Speech and Natural LanguageWorkshop, Morgan-Kaufmann Publishers, Inc. June, 1990.Church, K. A Stochastic Parts Program and Noun PhraseParser for Unrestricted Text.
Proceedings of the SecondConference on Applied Natural Language Processing, pages136-143.
ACL, 1988.de Marcken, C.G.
Parsing the LOB Corpus.
Proceedings ofthe 28th Annual Meeting of the Association forComputational Linguistics, pages 243-251.
1990.Grishman, R., and Sterling, J.
Preference Semantics forMessage Understanding.
Proceedings of the Speech andNatural Language Workshop, Oct. 1989, 71-74.Kuhn, R., and De Mori, R., A cache-Based NaturalLanguage Model for Speech Recognition.
IEEETransactions on Pattern Analysis and Machine Intelligence12, pages 570-583.
1990.Santorini, Beatrice.
Annotation Manual for the PennTreebank Project.
Technical Report.
CIS Department.University of Pennsylvania.
May 1990.Stallard, D. Unification-Based Semantic Interpretation ithe BBN Spoken Language System.
In Proceedings of theSpeech and Natural Language Workshop, pages 39-46.
Oct.1989.Marcus, Santorini & Magerman 1990, "First Steps Towardsan Annotated Database of American English" in Langendoen& Marcus, "Readings for Tagging Linguistic Information ina Text Corpus", tutorial for the 28th Annual Meeting of theAssociation for Computational Linguistics.Weischedel, R., Bobrow, R., Ayuso, D., and Ramshaw, L.(1989) Portability in the Janus Natural Language Interface.in Speech and Natural Language, Morgan KaufmanPublishers, Inc. p. 112-117.336
