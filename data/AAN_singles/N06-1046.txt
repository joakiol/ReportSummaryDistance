Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 359?366,New York, June 2006. c?2006 Association for Computational LinguisticsAggregation via Set Partitioning for Natural Language GenerationRegina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technologyregina@csail.mit.eduMirella LapataSchool of InformaticsUniversity of Edinburghmlap@inf.ed.ac.ukAbstractThe role of aggregation in natural lan-guage generation is to combine two ormore linguistic structures into a singlesentence.
The task is crucial for generat-ing concise and readable texts.
We presentan efficient algorithm for automaticallylearning aggregation rules from a text andits related database.
The algorithm treatsaggregation as a set partitioning problemand uses a global inference procedure tofind an optimal solution.
Our experimentsshow that this approach yields substan-tial improvements over a clustering-basedmodel which relies exclusively on localinformation.1 IntroductionAggregation is an essential component of many nat-ural language generation systems (Reiter and Dale,2000).
The task captures a mechanism for merg-ing together two or more linguistic structures intoa single sentence.
Aggregated texts tend to be moreconcise, coherent, and more readable overall (Dalia-nis, 1999; Cheng and Mellish, 2000).
Compare,for example, sentence (2) in Table 1 and its non-aggregated counterpart in sentences (1a)?(1d).
Thedifference between the fluent aggregated sentenceand its abrupt and redundant alternative is striking.The benefits of aggregation go beyond makingtexts less stilted and repetitive.
Researchers in psy-cholinguistics have shown that by eliminating re-(1) a. Holocomb had an incompletion in thefirst quarter.b.
Holocomb had another incompletion inthe first quarter.c.
Davis was among four San Franciscodefenders.d.
Holocomb threw to Davis for a leapingcatch.
(2) After two incompletions in the first quar-ter, Holcomb found Davis among four SanFrancisco defenders for a leaping catch.Table 1: Aggregation example (in boldface) from acorpus of football summariesdundancy, aggregation facilitates text comprehen-sion and recall (see Yeung (1999) and the referencestherein).
Furthermore, Di Eugenio et al (2005)demonstrate that aggregation can improve learningin the context of an intelligent tutoring application.In existing generation systems, aggregation typi-cally comprises two processes: semantic groupingand sentence structuring (Wilkinson, 1995).
Thefirst process involves partitioning semantic content(usually the output of a content selection compo-nent) into disjoint sets, each corresponding to a sin-gle sentence.
The second process is concerned withsyntactic or lexical decisions that affect the realiza-tion of an aggregated sentence.To date, this task has involved human analysis of adomain-relevant corpus and manual development ofaggregation rules (Dalianis, 1999; Shaw, 1998).
Thecorpus analysis and knowledge engineering work insuch an approach is substantial, prohibitively so in359large domains.
But since corpus data is already usedin building aggregation components, an appealingalternative is to try and learn the rules of semanticgrouping directly from the data.
Clearly, this wouldgreatly reduce the human effort involved and easeporting generation systems to new domains.In this paper, we present an automatic method forperforming the semantic grouping task.
We addressthe following problem: given an aligned parallel cor-pus of sentences and their underlying semantic rep-resentations, how can we learn grouping constraintsautomatically?
In our case the semantic content cor-responds to entries from a database; however, ouralgorithm could be also applied to other representa-tions such as propositions or sentence plans.We formalize semantic grouping as a set parti-tioning problem, where each partition correspondsto a sentence.
The strength of our approach lies inits ability to capture global partitioning constraintsby performing collective inference over local pair-wise assignments.
This design allows us to inte-grate important constraints developed in symbolicapproaches into an automatic aggregation frame-work.
At a local level, pairwise constraints cap-ture the semantic compatibility between pairs ofdatabase entries.
For example, if two entries sharemultiple attributes, then they are likely to be aggre-gated.
Local constraints are learned using a binaryclassifier that considers all pairwise combinationsattested in our corpus.
At a global level, we searchfor a semantic grouping that maximally agrees withthe pairwise preferences while simultaneously sat-isfying constraints on the partitioning as a whole.Global constraints, for instance, could prevent thecreation of overly long sentences, and, in general,control the compression rate achieved during aggre-gation.
We encode the global inference task as aninteger linear program (ILP) that can be solved us-ing standard optimization tools.We evaluate our approach in a sports domain rep-resented by large real-world databases containinga wealth of interrelated facts.
Our aggregation al-gorithm model achieves an 11% F-score increaseon grouping entry pairs over a greedy clustering-based model which does not utilize global informa-tion for the partitioning task.
Furthermore, these re-sults demonstrate that aggregation is amenable to anautomatic treatment that does not require human in-volvement.In the following section, we provide an overviewof existing work on aggregation.
Then, we define thelearning task and introduce our approach to contentgrouping.
Next, we present our experimental frame-work and data.
We conclude the paper by presentingand discussing our results.2 Related WorkDue to its importance in producing coherent and flu-ent text, aggregation has been extensively studied inthe text generation community.1 Typically, semanticgrouping and sentence structuring are interleaved inone step, thus enabling the aggregation componentto operate over a rich feature space.
The commonassumption is that other parts of the generation sys-tem are already in place during aggregation, and thusthe aggregation component has access to discourse,syntactic, and lexical constraints.The interplay of different constraints is usuallycaptured by a set of hand-crafted rules that guidethe aggregation process (Scott and de Souza, 1990;Hovy, 1990; Dalianis, 1999; Shaw, 1998).
Al-ternatively, these rules can be learned from a cor-pus.
For instance, Walker et al (2001) proposean overgenerate-and-rank approach to aggregationwithin the context of a spoken dialog application.Their system relies on a preference function for se-lecting an appropriate aggregation among multiplealternatives and assumes access to a large featurespace expressing syntactic and pragmatic features ofthe input representations.
The preference functionis learned from a corpus of candidate aggregationsmarked with human ratings.
Another approach is putforward by Cheng and Mellish (2000) who use a ge-netic algorithm in combination with a hand-craftedpreference function to opportunistically find a textthat satisfies aggregation and planning constraints.Our approach differs from previous work in twoimportant respects.
First, our ultimate goal is a gen-eration system which can be entirely induced froma parallel corpus of sentences and their correspond-ing database entries.
This means that our generatorwill operate over more impoverished representationsthan are traditionally assumed.
For example we do1The approaches are too numerous to list; we refer the inter-ested reader to Reiter and Dale (2000) and Reape and Mellish(1999) for comprehensive overviews.360PassingPLAYER CP/AT YDS AVG TD INTCundiff 22/37 237 6.4 1 1Carter 23/47 237 5.0 1 4. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.RushingPLAYER REC YDS AVG LG TDHambrick 13 33 2.5 10 1. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.1 (Passing (Cundiff 22/37 237 6.4 1 1))(Passing (Carter 23/47 237 5.0 1 4))2 (Interception (Lindell 1 52 1))(Kicking (Lindell 3/3 100 38 1/1 10))3 (Passing (Bledsoe 17/34 104 3.1 0 0))4 (Passing (Carter 15/32 116 3.6 1 0))5 (Rushing (Hambrick 13 33 2.5 10 1))6 (Fumbles (Bledsoe 2 2 0 0 0))Table 2: Excerpt of database and (simplified) example of aggregated entries taken from a football domain.This fragment will give rise to 6 sentences in the final text.not presume to know all possible ways in which ourdatabase entries can be lexicalized, nor do we pre-sume to know which semantic or discourse relationsexist between different entries.
In this framework,aggregation is the task of grouping semantic contentwithout making any decisions about sentence struc-ture or its surface realization.
Second, we strive foran approach to the aggregation problem which is asdomain- and representation-independent as possible.3 Problem FormulationWe formulate aggregation as a supervised partition-ing task, where the goal is to find a clustering ofinput items that maximizes a global utility func-tion.
The input to the model consists of a set Eof database entries selected by a content planner.The output of the model is a partition S = {Si} ofnonempty subsets such that each element of E ap-pears in exactly one subset.2 In the context of aggre-gation, each partition represents entries that shouldbe verbalized in the same sentence.
An example of apartitioning is illustrated in the right side of Table 2where eight entries are partitioned into six clusters.We assume access to a relational database whereeach entry has a type and a set of attributes as-sociated with it.
Table 2 (left) shows an ex-cerpt of the database we used for our experiments.The aggregated text in Table 2 (right) contains en-tries of five types: Passing, Interception,Kicking, Rushing, and Fumbles.
Entries oftype Passing have six attributes ?
PLAYER,2By definition, a partitioning of a set defines an equivalencerelation which is reflexive, symmetric, and transitive.CP/AT, YDS, AVG, TD, INT, entries of typeInterception have four attributes, and so on.We assume the existence of a non-empty set of at-tributes that we can use for meaningful comparisonbetween entities of different types.
In the exampleabove, types Passing and Rushing share the at-tributes PLAYER,AVG (short for average), TD (shortfor touchdown) and YDS (short for yards).
These areindicated in boldface in Table 2.
In Section 4.1, wediscuss how a set of shared attributes can be deter-mined for a given database.Our training data consists of entry sets with aknown partitioning.
During testing, our task is toinfer a partitioning for an unseen set of entries.4 ModelingOur model is inspired by research on text aggre-gation in the natural language generation commu-nity (Cheng and Mellish, 2000; Shaw, 1998).
Acommon theme across different approaches is thenotion of similarity ?
content elements describedin the same sentence should be related to each otherin some meaningful way to achieve conciseness andcoherence.
Consider for instance the first cluster inTable 2.
Here, we have two entries of the same type(i.e., Passing).
Furthermore, the entries share thesame values for the attributes YDS and TD (i.e., 237and 1).
On the other hand, clusters 5 and 6 haveno attributes in common.
This observation moti-vates modeling aggregation as a binary classificationtask: given a pair of entries, predict their aggrega-tion status based on the similarity of their attributes.Assuming a perfect classifier, pairwise assignments361will be consistent with each other and will thereforeyield a valid partitioning.In reality, however, this approach may produceglobally inconsistent decisions since it treats eachpair of entries in isolation.
Moreover, a pairwiseclassification model cannot express general con-straints regarding the partitioning as a whole.
Forexample, we may want to constrain the size of thegenerated partitions and the compression rate of thedocument, or the complexity of the generated sen-tences.To address these requirements, our approach re-lies on global inference.
Given the pairwise predic-tions of a local classifier, our model finds a glob-ally optimal assignment that satisfies partitioning-level constraints.
The computational challenge liesin the complexity of such a model: we need to findan optimal partition in an exponentially large searchspace.
Our approach is based on an Integer LinearProgramming (ILP) formulation which can be effec-tively solved using standard optimization tools.
ILPmodels have been successfully applied in severalnatural language processing tasks, including relationextraction (Roth and Yih, 2004), semantic role label-ing (Punyakanok et al, 2004) and the generation ofroute directions (Marciniak and Strube, 2005).In the following section, we introduce our localpairwise model and afterward we present our globalmodel for partitioning.4.1 Learning Pairwise SimilarityOur goal is to determine whether two database en-tries should be aggregated given the similarity oftheir shared attributes.
We generate the training databy considering all pairs ?ei, ej?
?
E ?
E, where Eis the set of all entries attested in a given document.An entry pair forms a positive instance if its mem-bers belong to the same partition in the training data.For example, we will generate 8?72 unordered entrypairs for the eight entries from the document in Ta-ble 2.
From these, only two pairs constitute positiveinstances, i.e., clusters 1 and 2.
All other pairs formnegative instances.The computation of pairwise similarity is basedon the attribute set A = {Ai} shared between thetwo entries in the pair.
As discussed in Section 3,the same attributes can characterize multiple entrytypes, and thus form a valid basis for entry compari-son.
The shared attribute set A could be identified inmany ways.
For example, using domain knowledgeor by selecting attributes that appear across multipletypes.
In our experiments, we follow the second ap-proach: we order attributes by the number of entrytypes in which they appear, and select the top five3.A pair of entries is represented by a binary fea-ture vector {xi} in which coordinate xi indicateswhether two entries have the same value for at-tribute i.
The feature vector is further expanded byconjuctive features that explicitly represent overlapin values of multiple attributes up to size k. Theparameter k controls the cardinality of the maximalconjunctive set and is optimized on the developmentset.To illustrate our feature generation process, con-sider the pair (Passing (Quincy Carter 15/32 116 3.61 0)) and (Rushing (Troy Hambrick 13 33 2.5 10 1))from Table 2.
Assuming A = {Player,Yds,TD}and k = 2, the similarity between the two en-tries will be expressed by six features, three rep-resenting overlap in individual attributes and threerepresenting overlap when considering pairs of at-tributes.
The resulting feature vector has the form?0, 0, 1, 0, 0, 0?.Once we define a mapping from database entriesto features, we employ a machine learning algorithmto induce a classifier on the feature vectors generatedfrom the training documents.
In our experiments, weused a publicly available maximum entropy classi-fier4 for this task.4.2 Partitioning with ILPGiven the pairwise predictions of the local classifier,we wish to find a valid global partitioning for theentries in a single document.
We thus model the in-teraction between all pairwise aggregation decisionsas an optimization problem.Let c?ei,ej?
be the probability of seeing entry pair?ei, ej?
aggregated (as computed by the pairwiseclassifier).
Our goal is to find an assignment thatmaximizes the sum of pairwise scores and forms avalid partitioning.
We represent an assignment us-ing a set of indicator variables x?ei,ej?
that are set3Selecting a larger number of attributes for representing sim-ilarity would result in considerably sparser feature vectors.4The software can be downloaded from http://www.isi.edu/?hdaume/megam/index.html.362to 1 if ?ei, ej?
is aggregated, and 0 otherwise.
Thescore of a global assignment is the sum of its pair-wise scores:??ei,ej??E?Ec?ei,ej?x?ei,ej?+(1?c?ei,ej?)(1?x?ei,ej?
)(1)Our inference task is solved by maximizing theoverall score of pairs in a given document:argmax??ei,ej??E?Ec?ei,ej?x?ei,ej?+(1?c?ei,ej?
)(1?x?ei ,ej?
)(2)subject to:x?ei,ej?
?
{0, 1} ?
ei, ej ?
E ?
E (3)We augment this basic formulation with two typesof constraints.
The first type of constraint ensuresthat pairwise assignments lead to a consistent parti-tioning, while the second type expresses global con-straints on partitioning.Transitivity Constraints We place constraintsthat enforce transitivity in the label assignment: ifx?ei,ej?
= 1 and x?ej ,ek?
= 1, then x?ei,ek?
= 1.A pairwise assignment that satisfies this constraintdefines an equivalence relation, and thus yields aunique partitioning of input entries (Cormen et al,1992).We implement transitivity constraints by intro-ducing for every triple ei, ej , ek (i 6= j 6= k) aninequality of the following form:x?ei,ek?
?
x?ei,ej?
+ x?ej ,ek?
?
1 (4)If both x?ei,ej?
and x?ej ,ek?
are set to one, thenx?ei,ek?
also has to be one.
Otherwise, x?ei,ek?
canbe either 1 or 0.Global Constraints We also want to considerglobal document properties that influence aggrega-tion.
For example, documents with many databaseentries are likely to exhibit different compressionrates during aggregation when compared to docu-ments that contain only a few.Our first global constraint controls the numberof aggregated sentences in the document.
This isachieved by limiting the number of entry pairs withpositive labels for each document:??ei,ej??E?Ex?ei,ej?
?
m (5)Notice that the number m is not known in ad-vance.
However, we can estimate this parameterfrom our development data by considering docu-ments of similar size (as measured by the numberof corresponding entry pairs.)
For example, textswith thousand entry pairs contain on average 70 pos-itive labels, while documents with 200 pairs havearound 20 positive labels.
Therefore, we set m sep-arately for every document by taking the averagenumber of positive labels observed in the develop-ment data for the document size in question.The second set of constraints controls the lengthof the generated sentences.
We expect that there isan upper limit on the number of pairs that can beclustered together.
This restriction can be expressedin the following form:?
ei?ej?Ex?ei,ej?
?
k (6)This constraint ensures that there can be at most kpositively labeled pairs for any entry ei.
In ourcorpus, for instance, at most nine entries can beaggregated in a sentence.
Again k is estimatedfrom the development data by taking into accountthe average number of positively labeled pairs forevery entry type (see Table 2).
We thereforeindirectly capture the fact that some entry types(e.g., Passing) are more likely to be aggregatedthan others (e.g., Kicking).Solving the ILP In general, solving an integer lin-ear program is NP-hard (Cormen et al, 1992).
For-tunately, there exist several strategies for solvingILPs.
In our study, we employed lp solve, an ef-ficient Mixed Integer Programming solver5 whichimplements the Branch-and-Bound algorithm.
Wegenerate and solve an ILP for every document wewish to aggregate.
Documents of average size (ap-proximately 350 entry pairs) take under 30 minuteson a 450 MHz Pentium III machine.5The software is available from http://www.geocities.com/lpsolve/3635 Evaluation Set-upThe model presented in the previous section wasevaluated in the context of generating summary re-ports for American football games.
In this sectionwe describe the corpus used in our experiments, ourprocedure for estimating the parameters of our mod-els, and the baseline method used for comparisonwith our approach.Data For training and testing our algorithm, weemployed a corpus of football game summaries col-lected by Barzilay and Lapata (2005).
The corpuscontains 468 game summaries from the official siteof the American National Football League6 (NFL).Each summary has an associated database contain-ing statistics about individual players and events.
Intotal, the corpus contains 73,400 database entries,7.1% of which are verbalized; each entry is charac-terized by a type and a set of attributes (see Table 2).Database entries are automatically aligned with theircorresponding sentences in the game summaries bya procedure that considers anchor overlap betweenentity attributes and sentence tokens.
Although thealignment procedure is relatively accurate, there isunavoidably some noise in the data.The distribution of database entries per sentenceis shown in Figure 1.
As can be seen, most aggre-gated sentences correspond to two or three databaseentries.
Each game summary contained 14.3 entriesand 9.1 sentences on average.
The training and testdata were generated as described in Section 4.1.
Weused 96,434 instances (300 summaries) for training,59,082 instances (68 summaries) for testing, and53,776 instances (100 summaries) for developmentpurposes.Parameter Estimation As explained in Section 4,we infer a partitioning over a set of database en-tries in a two-stage process.
We first determine howlikely all entry pairs are to be aggregated using a lo-cal classifier, and then infer a valid global partition-ing for all entries.
The set of shared attributes Aconsists of five features that capture overlap in play-ers, time (measured by game quarters), action type,outcome type, and number of yards.
The maximumcardinality of the set of conjunctive features is five.6See http://www.nfl.com/scores.Figure 1: Distribution of aggregated sentences in theNFL corpusOverall, our local classifier used 28 features, includ-ing 23 conjunctive ones.
The maximum entropyclassifier was trained for 100 iterations.
The globalconstraints for our ILP models are parametrized (seeequations (5) and (6)) by m and k which are esti-mated separately for every test document.
The val-ues of m ranged from 2 to 130 and for k from 2 to 9.Baseline Clustering is a natural baseline model forour partitioning problem.
In our experiments, wea employ a single-link agglomerative clustering al-gorithm that uses the scores returned by the maxi-mum entropy classifier as a pairwise distance mea-sure.
Initially, the algorithm creates a separate clus-ter for each sentence.
During each iteration, the twoclosest clusters are merged.
Again, we do not knowin advance the appropriate number of clusters for agiven document.
This number is estimated from thetraining data by averaging the number of sentencesin documents of the same size.Evaluation Measures We evaluate the perfor-mance of the ILP and clustering models by mea-suring F-score over pairwise label assignments.
Wecompute F-score individually for each document andreport the average.
In addition, we compute partitionaccuracy in order to determine how many sentence-level aggregations our model predicts correctly.364Clustering Precision Recall F-scoreMean 57.7 66.9 58.4Min 0.0 0.0 0.0Max 100.0 100.0 100.0StDev 28.2 23.9 23.1ILP Model Precision Recall F-scoreMean 82.2 65.4 70.3Min 37.5 28.6 40.0Max 100.0 100.0 100.0StDev 19.2 20.3 16.6Table 3: Results on pairwise label assignment (pre-cision, recall, and F-score are averaged over doc-uments); comparison between clustering and ILPmodels6 ResultsOur results are summarized in Table 3.
As canbe seen, the ILP model outperforms the clusteringmodel by a wide margin (11.9% F-score).
The twomethods yield comparable recall; however, the clus-tering model lags considerably behind as far as pre-cision is concerned (the difference is 24.5 %).7Precision is more important than recall in the con-text of our aggregation application.
Incorrect aggre-gations may have detrimental effects on the coher-ence of the generated text.
Choosing not to aggre-gate may result in somewhat repetitive texts; how-ever, the semantic content of the underlying text re-mains intact.
In the case of wrong aggregations, wemay group together facts that are not compatible,and even introduce implications that are false.We also consider how well our model performswhen evaluated on total partition accuracy.
Here,we are examining the partitioning as a whole andask the following question: how many clusters ofsize 1, 2 .
.
.
n did the algorithm get right?
This eval-uation measure is stricter than F-score which is com-7Unfortunately we cannot apply standard statistical testssuch as the t-test on F-scores since they violate assumptionsabout underlying normal distributions.
It is not possible to usean assumptions-free test like ?2 either, since F-score is not afrequency-based measure.
We can, however, use ?2 on pre-cision and recall, since these measures are estimated from fre-quency data.
We thus find that the ILP model is significantlybetter than the clustering model on precision (?2 = 16.39,p < 0.01); the two models are not significantly different interms of recall (?2 = 0.02, p < 0.89).Figure 2: Partition accuracy for sentences of differ-ent sizeputed over pairwise label assignments.
The partitionaccuracy for entry groups of varying size is shown inFigure 2.
As can be seen, in all cases the ILP outper-forms the clustering baseline.
Both models are fairlyaccurate at identifying singletons, i.e., database en-tries which are not aggregated.
Performance is natu-rally worse when considering larger clusters.
Inter-estingly, the difference between the two models be-comes more pronounced for partition sizes 4 and 5(see Figure 2).
The ILP?s accuracy increases by 24%for size 4 and 8% for size 5.These results empirically validate the impor-tance of global inference for the partitioning task.Our formulation allows us to incorporate importantdocument-level constraints as well as consistencyconstraints which cannot be easily represented in avanilla clustering model.7 Conclusions and Future WorkIn this paper we have presented a novel data-drivenmethod for aggregation in the context of natural lan-guage generation.
A key aspect of our approach isthe use of global inference for finding aggregationsthat are maximally consistent and coherent.
We haveformulated our inference problem as an integer lin-ear program and shown experimentally that it out-performs a baseline clustering model by a wide mar-gin.
Beyond generation, the approach holds promisefor other NLP tasks requiring the accurate partition-ing of items into equivalence classes (e.g., corefer-ence resolution).365Currently, semantic grouping is carried out in ourmodel sequentially.
First, a local classifier learnsthe similarity of entity pairs and then ILP is em-ployed to infer a valid partitioning.
Although such amodel has advantages in the face of sparse data (re-call that we used a relatively small training corpusof 300 documents) and delivers good performance,it effectively decouples learning from inference.
Anappealing future direction lies in integrating learningand inference in a unified global framework.
Sucha framework would allow us to incorporate globalconstraints directly into the learning process.Another important issue, not addressed in thiswork, is the interaction of our aggregation methodwith content selection and surface realization.
Usingan ILP formulation may be an advantage here sincewe could use feedback (in the form of constraints)from other components and knowlegde sources (e.g.,discourse relations) to improve aggregation or in-deed the generation pipeline as a whole (Marciniakand Strube, 2005).AcknowledgmentsThe authors acknowledge the support of the National ScienceFoundation (Barzilay; CAREER grant IIS-0448168 and grantIIS-0415865) and EPSRC (Lapata; grant GR/T04540/01).Thanks to Eli Barzilay, Michael Collins, David Karger, FrankKeller, Yoong Keok Lee, Igor Malioutov, Johanna Moore,Kevin Simler, Ben Snyder, Bonnie Webber and the anonymousreviewers for helpful comments and suggestions.
Any opinions,findings, and conclusions or recommendations expressed aboveare those of the authors and do not necessarily reflect the viewsof the NSF or EPSRC.ReferencesR.
Barzilay, M. Lapata.
2005.
Collective content se-lection for concept-to-text generation.
In Proceedingsof the Human Language Technology Conference andthe Conference on Empirical Methods in Natural Lan-guage Processing, 331?338, Vancouver.H.
Cheng, C. Mellish.
2000.
Capturing the interactionbetween aggregation and text planning in two genera-tion systems.
In Proceedings of the 1st InternationalNatural Language Generation Conference, 186?193,Mitzpe Ramon, Israel.T.
H. Cormen, C. E. Leiserson, R. L. Rivest.
1992.
Into-duction to Algorithms.
The MIT Press.H.
Dalianis.
1999.
Aggregation in natural language gen-eration.
Computational Intelligence, 15(4):384?414.B.
Di Eugenio, D. Fossati, D. Yu.
2005.
Aggregation im-proves learning: Experiments in natural language gen-eration for intelligent tutoring systems.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics, 50?57, Ann Arbor, MI.E.
H. Hovy.
1990.
Unresolved issues in paragraph plan-ning.
In R. Dale, C. Mellish, M. Zock, eds., Cur-rent Research in Natural Language Generation, 17?41.
Academic Press, New York.T.
Marciniak, M. Strube.
2005.
Beyond the pipeline:Discrete optimization in NLP.
In Proceedings of theAnnual Conference on Computational Natural Lan-guage Learning, 136?143, Ann Arbor, MI.V.
Punyakanok, D. Roth, W. Yih, D. Zimak.
2004.
Se-mantic role labeling via integer linear programminginference.
In Proceedings of the International Con-ference on Computational Linguistics, 1346?1352,Geneva, Switzerland.M.
Reape, C. Mellish.
1999.
Just what is aggrega-tion anyway?
In Proceedings of the 7th EuropeanWorkshop on Natural Language Generation, 20?29,Toulouse, France.E.
Reiter, R. Dale.
2000.
Building Natural LanguageGeneration Systems.
Cambridge University Press,Cambridge.D.
Roth, W. Yih.
2004.
A linear programming formula-tion for global inference in natural language tasks.
InProceedings of the Annual Conference on Computa-tional Natural Language Learning, 1?8, Boston, MA.D.
Scott, C. S. de Souza.
1990.
Getting the mes-sage across in RST-based text generation.
In R. Dale,C.
Mellish, M. Zock, eds., Current Research in Nat-ural Language Generation, 47?73.
Academic Press,New York.J.
Shaw.
1998.
Clause aggregation using linguis-tic knowledge.
In Proceedings of 9th InternationalWorkshop on Natural Language Generation, 138?147,Niagara-on-the-Lake, Ontario, Canada.M.
A. Walker, O. Rambow, M. Rogati.
2001.
Spot:A trainable sentence planner.
In Proceedings of the2nd Annual Meeting of the North American Chapterof the Association for Computational Linguistics, 17?24, Pittsburgh, PA.J.
Wilkinson.
1995.
Aggregation in natural languagegeneration: Another look.
Technical report, ComputerScience Department, University of Waterloo, 1995.A.
S. Yeung.
1999.
Cognitive load and learner expertise:Split-attention and redundancy effects in reading com-prehension tasks with vocabulary definitions.
Journalof Experimental Education, 67(3):197?218.366
