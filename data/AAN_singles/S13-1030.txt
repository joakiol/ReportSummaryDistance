Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 207?215, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsUniMelb NLP-CORE: Integrating predictions from multiple domains andfeature sets for estimating semantic textual similaritySpandana Gella,?
Bahar Salehi,??
Marco Lui,?
?Karl Grieser,?
Paul Cook,?
and Timothy Baldwin,???
NICTA Victoria Research Laboratory?
Department of Computing and Information Systems, The University of Melbournesgella@student.unimelb.edu.au, bsalehi@student.unimelb.edu.aumhlui@unimelb.edu.au, kgrieser@student.unimelb.edu.aupaulcook@unimelb.edu.au, tb@ldwin.netAbstractIn this paper we present our systems for cal-culating the degree of semantic similarity be-tween two texts that we submitted to the Se-mantic Textual Similarity task at SemEval-2013.
Our systems predict similarity usinga regression over features based on the fol-lowing sources of information: string similar-ity, topic distributions of the texts based onlatent Dirichlet alcation, and similarity be-tween the documents returned by an informa-tion retrieval engine when the target texts areused as queries.
We also explore methods forintegrating predictions using different trainingdatasets and feature sets.
Our best system wasranked 17th out of 89 participating systems.In our post-task analysis, we identify simplechanges to our system that further improve ourresults.1 IntroductionSemantic Textual Similarity (STS) measures the de-gree of semantic similarity or equivalence betweena pair of short texts.
STS is related to many naturallanguage processing applications such as text sum-marisation (Aliguliyev, 2009), machine translation,word sense disambiguation, and question answering(De Boni and Manandhar, 2003; Jeon et al 2005).Two short texts are considered similar if they bothconvey similar messages.
Often it is the case thatsimilar texts will have a high degree of lexical over-lap, although this isn?t always so.
For example,SC dismissed government?s review plea in Vodafonetax case and SC dismisses govt?s review petition onVodafone tax verdict are semantically similar.
Thesetexts have matches in terms of exact words (SC,Vodafone, tax), morphologically-related words (dis-missed and dismisses), and abbreviations (govern-ment?s and govt?s).
However, the usages (senses) ofplea and petition, and case and verdict are also sim-ilar.One straightforward way of estimating semanticsimilarity of two texts is by using approaches basedon the similarity of the surface forms of the wordsthey contain.
However, such methods are not capa-ble of capturing similarity or relatedness at the lexi-cal level, and moreover, they do not exploit the con-text in which individual words are used in a targettext.
Nevertheless, a variety of knowledge sources?
including part-of-speech, collocations, syntax,and domain ?
can be used to identify the usage orsense of words in context (McRoy, 1992; Agirre andMartinez, 2001; Agirre and Stevenson, 2006) to ad-dress these issues.Despite their limitations, string similarity mea-sures have been widely used in previous seman-tic similarity tasks (Agirre et al 2012; Islam andInkpen, 2008).
Latent variable models have alsobeen used to estimate the semantic similarity be-tween words, word usages, and texts (Steyvers andGriffiths, 2007; Lui et al 2012; Guo and Diab,2012; Dinu and Lapata, 2010).In this paper, we consider three different ways ofmeasuring semantic similarity based on word andword usage similarity:1.
String-based similarity to measure surface-level lexical similarity, taking into accountmorphology and abbreviations (e.g., dismissesand dismissed, and government?s and govt?s);2072.
Latent variable models of similarity to cap-ture words that have different surface forms,but that have similar meanings or that can beused in similar contexts (e.g., petition and plea,verdict and case); and3.
Topical/domain similarity of the texts with re-spect to the similarity of documents in an ex-ternal corpus (based on information-retrievalmethods) that are relevant to the target texts.We develop features based on all three of theseknowledge sources to capture semantic similarityfrom a variety of perspectives.
We build a regres-sion model, trained on STS training data which hassemantic similarity scores for pairs of texts, to learnweights for the features and rate the similarity of testinstances.
Our approach to the task is to explore theutility of novel features or features that have not per-formed well in previous research, rather than com-bine these features with the myriad of features thathave been proposed by others for the task.2 Text Similarity MeasuresIn this section we describe the various features usedin our system.2.1 String Similarity Measures (SS)Our first set of features contains various string simi-larity measures (SS), which compare the target textsin terms of the words they contain and the orderof the words (Islam and Inkpen, 2008).
In the Se-mEval 2012 STS task (Agirre et al 2012) suchfeatures were used by several participants (Bigginset al 2012; Ba?r et al 2012; Heilman and Mad-nani, 2012), including the first-ranked team (Ba?r etal., 2012) who considered string similarity measuresalongside a wide range of other features.For our string similarity features, the texts werelemmatized using the implementation of LancasterStemming in NLTK 2.0 (Bird, 2006), and all punc-tuation was removed.
Limited stopword removalwas carried out by eliminating the words a, and, andthe.
The output of each string similarity measureis normalized to the range of [0, 1], where 0 indi-cates that the texts are completely different, while 1means they are identical.
The normalization methodfor each feature is described in Salehi and Cook (toappear), wherein the authors applied string similar-ity measures successfully to the task of predictingthe compositionality of multiword expressions.Identical Unigrams (IU): This feature measuresthe number of words shared between the two texts,irrespective of word order.Longest Common Substring (LCS): This mea-sures the longest sequence of words shared betweenthe two texts.
For example, the longest commonsubstring between the following sentences is bolded:A woman and man are dancing in therain.A couple are dancing in the street.Levenshtein (LEV1): Levenshtein distance (alsoknown as edit distance) calculates the number ofbasic word-level edit operations (insertion, deletionand substitution) to transform one text into the other:Levenshtein with substitution penalty (LEV2):This feature is a variant of LEV1 in which substi-tution is considered as two edit operations: an inser-tion and a deletion (Baldwin, 2009).Smith Waterman (SW): This method is designedto locally align two sequences of amino acids (Smithand Waterman, 1981).
The algorithm looks forthe longest similar regions by maximizing the num-ber of matches and minimizing the number of in-sertion/deletion/substitution operations necessary toalign the two sequences.
In other words, it finds thelongest common sequence while tolerating a smallnumber of differences.
We call this sequence, the?aligned sequence?.
It has length equal to or greaterthan the longest common sequence.Not Aligned Words (NAW): As mentionedabove, SW looks for similar regions in the giventexts.
Our last string similarity feature shows thenumber of identical words not aligned by the SW al-gorithm.
We used this feature to examine how simi-lar the unaligned words are.These six features (IU, LCS, LEV1, LEV2, SW,and NAW) form our string similarity (SS) features.LEV2, SW, and NAW have not been previously con-sidered for STS.2082.2 Topic Modelling Similarity Measures (TM)The topic modelling features (TM) are based on La-tent Dirichlet Allocation (LDA), a generative prob-abilistic model in which each document is mod-eled as a distribution over a finite set of topics, andeach topic is represented as a distribution over words(Blei et al 2003).
We build a topic model on a back-ground corpus, and then for each target text we cre-ate a topic vector based on the topic allocations ofits content words, based on the method developedby Lui et al(2012) for predicting word usage simi-larity.The choice of the number of topics, T , canhave a big impact on the performance of thismethod.
Choosing a small T might give overly-broad topics, while a large T might lead to un-interpretable topics (Steyvers and Griffiths, 2007).Moreover smaller numbers of topics have beenshown to perform poorly on both sentence simi-larity (Guo and Diab, 2012) and word usage sim-ilarity tasks (Lui et al 2012).
We therefore buildtopic models for 33 values of T in the range2, 3, 5, 8, 10, 50, 80, 100, 150, 200, ...1350.The background corpus used for generating thetopic models is similar to the COL-WTMF sys-tem (Guo and Diab, 2012) from the STS-2012 task,which outperformed LDA.
In particular, we usesense definitions from WordNet, Wiktionary and allsentences from the Brown corpus.
Similarity be-tween two texts is measured on the basis of the simi-larity between their topic distributions.
We considerthree vector-based similarity measures here: Cosinesimilarity, Jensen-Shannon divergence and KL di-vergence.
Thus for each target text pair we extract99 features corresponding to the 3 similarity mea-sures for each of the 33 T settings.
These featuresare used as the TM feature set in the systems de-scribed below.2.3 IR Similarity Measures (IR)The information retrieval?based features (IR) werebased on a dump of English Wikipedia from Novem-ber 2009.
The entire dump was stripped of markupand tokenised using the OpenNLP tokeniser.
Thetokenised documents were then parsed into TRECformat, with each article forming an individual doc-ument.
These documents were indexed using theIndri IR engine1 with stopword removal.
Eachof the two target texts was issued as a full textquery (without any phrases) to Indri, and the first1000 documents for each text were returned, basedon Okapi term weighting (Robertson and Walker,1994).
These resultant document lists were thenconverted into features using a number of set- andrank-based measures: Dice?s coefficient, Jaccard in-dex, average overlap, and rank-biased overlap (thelatter two are described in Webber et al(2010)).The first two are based on simple set overlap andignore the ranks; average overlap takes into accountthe rank, but equally weights high- and low-rankingdocuments; and rank-biased overlap weights higher-ranked items higher.In addition to comparisons of the document rank-ings for a given target text pair, we also consid-ered a method that compared the top-ranking doc-uments themselves.
To compare two texts, we ob-tain the top-100 documents using each text as aquery as above.
We then calculate the similarity be-tween these two sets of resultant documents usingthe ?2-based corpus similarity measure of Kilgarriff(2001).
In this method the ?2 statistic is calculatedfor the 500 most frequent words in the union of thetwo sets of documents (corpora), and is interpretedas the similarity between the sets of documents.These 5 IR features (4 rank-based, and 1document-based) are novel in the context of STS,and are used in the compound systems described be-low.3 Compound systems3.1 Ridge regressionEach of our features represents a (potentially noisy)measurement of the semantic textual similarity be-tween two texts.
However, the scale of our fea-tures varies, e.g., [0, 1] for the string similarity fea-tures vs. unbounded for KL divergence (one of thetopic modelling features).
To learn the mapping be-tween these features and the graded [0, 5] scale ofthe shared task, we made use of a statistical tech-nique known as ridge regression, as implemented inscikit-learn.2 Ridge regression is a form oflinear regression where the loss function is the ordi-1http://www.lemurproject.org/indri/2http://scikit-learn.org209nary least squares, but with an additional L2 regular-ization term.
In our empirical evaluation, we foundthat ridge regression outperformed linear regressionon our feature set.
For brevity, we only present re-sults from ridge regression.3.2 Domain AdaptationDomain adaptation (Daume?
and Marcu, 2006) is thegeneral term applied to techniques for using labelleddata from a related distribution to label data from atarget distribution.
For the 2013 Shared Task, notraining data was provided for the target datasets,making domain adaptation an important considera-tion.
In this work, we assume that each dataset rep-resents a different domain, and on this basis developapproaches that are sensitive to inter-domain differ-ences.We tested two simple approaches to including do-main information in our trained model.
The first ap-proach, which we will refer to as flagging, simply in-volves appending a boolean vector to each traininginstance to indicate which training dataset it camefrom.
The vector has length D, equal to the numberof training datasets (3 for this task, because we trainon the STS 2012 training data).
All the values of thevector are 0, except for a single 1 according to thedataset that the training instance is drawn from.
Fortest data, the entire vector consists of 0s.The second approach we considered is based onmetalearning, and we will refer to it as domainstacking.
In domain stacking, we train a regressorfor each domain (the level 0 regressors (Wolpert,1992)).
Each of these regressors is then appliedto a test instance to produce a predicted value (thelevel 0 prediction).
These predictions are then com-bined using a second regressor (the level 1 regres-sor), to produce a final prediction for each instance(the level 1 prediction).
This approach is closelyrelated to feature stacking (Lui, 2012) and stackedgeneralization (Wolpert, 1992).
A general princi-ple of metalearning is to combine multiple weaker(?less accurate?)
predictors ?
termed level 0 pre-dictors ?
to produce a stronger (?more accurate?
)predictor ?
the level 1 predictor.
In stacked gener-alization, the level 0 predictors are different learningalgorithms.
In feature stacking, they are the samealgorithm trained on different subsets of features, inthis work corresponding to different methods for es-timating STS (Section 2).
In domain stacking, thelevel 0 predictions are obtained from subsets of thetraining data, where each subset corresponds to allthe instances from a single dataset (e.g.
MSRpar orSMTeuroparl).
In terms of subsampling the trainingdata, this technique is related to bagging (Breiman,1996).
However, rather than generating new train-ing sets by uniform sampling across the whole poolof training data, we treat each domain in the train-ing dataset as a unique sample.
Finally, we also ex-periment with feature-domain stacking, in which thelevel 0 predictions are obtained from the cross prod-uct of subsets of the training data (as per domainstacking) and subsets of the feature set (as per fea-ture stacking).
We report results for all 3 variants inSection 5.This framework of feature-domain stacking canbe applied with any regression or classification al-gorithm (indeed, the level 0 and level 1 predictorscould be trained using different algorithms).
In thiswork, all our regressors are trained using ridge re-gression (Section 3.1).4 Submitted RunsIn this section we describe the three official runs wesubmitted to the shared task.4.1 Run1 ?
BaharFor this run we used just the SS feature set, aug-mented with flagging for domain adaptation.
Ridgeregression was used to train a regressor across thethree training datasets (MSRvid, MSRpar, SMTeu-roparl).
Each instance was then labelled using theoutput of the regressor, and the output range was lin-early re-scaled to [0, 5] as it occasionally producedvalues outside of this range.
Although this approachapproximates STS using only lexical textual similar-ity, it was our best-performing system on the trainingdata (Table 1).
Furthermore the SS features are ap-pealing because of their simplicity and because theydo not make use of any external resources.4.2 Run2 ?
ConcatIn this run, we concatenated the feature vectorsfrom all three of our feature sets (SS, TM andIR), and again trained a regressor on the union ofthe MSRvid, MSRpar and SMTeuroparl trainingdatasets.
As in Run1, the output of the regression210FSet FL FS DS MSRpar MSRvid SMTeuroparl AveSS 0.522 0.537 0.526 0.528(*) SS X 0.552 0.533 0.562 0.549TM 0.270 0.479 0.425 0.391TM X 0.250 0.580 0.427 0.419IR 0.264 0.759 0.407 0.477IR X 0.291 0.754 0.400 0.482(+) ALL 0.401 0.543 0.513 0.485ALL X 0.377 0.595 0.516 0.496ALL X 0.385 0.587 0.520 0.497ALL X 0.452 0.637 0.472 0.521ALL X X 0.429 0.619 0.526 0.524ALL X X 0.429 0.627 0.526 0.527(?)
ALL X X X 0.441 0.645 0.527 0.538Table 1: Pearson?s ?
for each feature set (FSet),as well as combinations of feature sets and adap-tation strategies, on each training dataset, and themicro-average over all training datasets.
(*), (+),and (?)
denote Run1, Run2, and Run3, respectively,our submissions to the shared task; FL=Flagging,FS=Feature stacking, DS=Domain stacking.was also linearly re-scaled to the [0, 5] range.
Un-like the previous run, the flagging approach to do-main adaptation was not used.
This approach re-flects a simple application of machine learning to in-tegrating data from multiple feature sets and trainingdatasets, and provides a useful point of comparisonagainst more sophisticated approaches (i.e., Run3).4.3 Run3 ?
StackingIn this run, we focused on an alternative methodto integrating information from multiple feature setsand training datasets, namely feature-domain stack-ing, as discussed in Section 3.2.
In this approach, wetrain nine regressors using ridge regression on eachcombination of the three training datasets and threefeature sets.
Thus, the level 1 representation for eachinstance is a vector of nine predictions.
For the train-ing data, when computing the level 1 features for thesame training dataset from which a given instance isdrawn, 10-fold cross-validation is used.
Ridge re-gression is again used to combine the level 1 repre-sentations and produce the final prediction for eachinstance.
In addition to this, we also simultaneouslyapply the flagging approach to domain adaptation.This approach incorporates all of our domain adap-tation efforts, and in initial experiments on the train-ing data (Table 1) it was our second-best system.FSet FL FS DS OnWN FNWN Headlines SMT AveSS 0.340 0.366 0.688 0.325 0.453(*) SS X 0.349 0.381 0.711 0.350 0.473TM 0.648 0.358 0.516 0.209 0.433TM X 0.701 0.368 0.614 0.287 0.506IR 0.561 -0.006 0.610 0.228 0.419IR X 0.596 0.002 0.621 0.256 0.441(+) ALL 0.679 0.337 0.709 0.323 0.542ALL X 0.704 0.365 0.718 0.344 0.560ALL X 0.673 0.298 0.714 0.324 0.539ALL X 0.618 0.264 0.717 0.357 0.534ALL X X 0.658 0.309 0.721 0.330 0.540ALL X X 0.557 0.142 0.694 0.280 0.475(?)
ALL X X X 0.614 0.186 0.706 0314 0.509Table 2: Pearson?s ?
for each feature set (FSet),as well as combinations of feature sets and adap-tation strategies, on each test dataset, and themicro-average over all test datasets.
(*), (+), and(?)
denote Run1, Run2, and Run3, respectively,our submissions to the shared task; FL=Flagging,FS=Feature stacking, DS=Domain stacking.5 ResultsFor the STS 2013 task, the organisers advised par-ticipants to make use of the STS 2012 data; we tookthis to mean only the training data.
In our post-taskanalysis, we realised that the entire 2012 dataset, in-cluding the testing data, could be used.
All our of-ficial runs were trained only on the training data forthe 2012 task (made up of MSRpar, MSRvid andSMTeuroparl).
We first discuss preliminary find-ings training and testing on the (STS 2012) trainingdata, and then present results for the (2013) test data.Post-submission, we re-trained our systems includ-ing the 2012 test data.5.1 Experiments on Training DataWe evaluated our models based on a leave-one-outcross-validation across the 3 training datasets.
Thus,for each of the training datasets, we trained a sep-arate model using features from the other two.
Weconsidered approaches based on each individual fea-ture set, with and without flagging.
We further con-sidered combinations of feature sets using featureconcatenation, as well as feature and domain stack-ing, again with and without flagging.3 Results are3We did not consider domain stacking with flagging.211FSet FL FS DS OnWN (?)
FNWN (?)
Headlines (?)
SMT (?)
Ave (?
)SS 0.3566 (+.0157) 0.3741 (+.0071) 0.6994 (+.0111) 0.3386 (+.0131) 0.4663 (+.0133)(*) SS X 0.3532 (+.0042) 0.3809 (?.0004) 0.7122 (+.0003) 0.3417 (?.0090) 0.4714 (?.0016)TM 0.6748 (+.0265) 0.3939 (+.0349) 0.5930 (+.0770) 0.2563 (+.0472) 0.4844 (+.0514)TM X 0.6269 (?.0743) 0.3519 (?.0162) 0.5999 (?.0142) 0.2653 (?.0223) 0.4743 (?.0317)IR 0.6632 (+.1015) 0.1026 (+.1093) 0.6383 (?.0281) 0.2987 (+.0701) 0.4863 (+.0673)IR X 0.6720 (+.0755) 0.0861 (+.0841) 0.6316 (+.0097) 0.2811 (+.0244) 0.4790 (+.0680)(+) ALL 0.6976 (+.0006) 0.4350 (+.0976) 0.7071 (?.0014) 0.3329 (+.0099) 0.5571 (+.0151)ALL X 0.6667 (?.0373) 0.4138 (+.0490) 0.7210 (+.0029) 0.3335 (?.0105) 0.5524 (?.0076)ALL X 0.6889 (+.0149) 0.4620 (+.1636) 0.7309 (+.0167) 0.3538 (+.0295) 0.5721 (+.0331)ALL X 0.6765 (?.0185) 0.4675 (+.1578) 0.7337 (+.0126) 0.3552 (+.0252) 0.5709 (+.0369)ALL X X 0.6369 (+.0208) 0.3615 (+.0970) 0.7233 (+.0060) 0.3736 (+.0157) 0.5554 (+.0154)ALL X X 0.6736 (+.1165) 0.4250 (+.2821) 0.7237 (+0.0297) 0.3404 (+0.0603) 0.5583(+.0833)(?)
ALL X X X 0.6772 (+.0632) 0.3992 (+.2127) 0.7315 (+.0251) 0.3300 (+0.0186) 0.5572 (+.0482)Table 3: Pearson?s ?
for each feature set (FSet), as well as combinations of feature sets and adaptationstrategies, on each test dataset, and the micro-average over all test datasets, using features from all 2012data (test + train).
(*), (+), and (?)
denote Run1, Run2, and Run3, respectively, our submissions to theshared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking.
?
denotes the difference in systemperformance after adding the additional training data.reported in Table 1.The best results on the training data were achievedusing only our SS feature set with flagging (Run1),with an average Pearson?s ?
of 0.549.
This fea-ture set al gave the best performance on MSR-par and SMTeuroparl, although the IR feature setwas substantially better on MSRvid.
On the trainingdatasets, our approaches that combine feature setsdid not give an improvement over the best individ-ual feature set on any dataset, or overall.5.2 Test Set ResultsSTS 2013 included four different test sets.
Table 2presents the Pearson?s ?
for the same methods asSection 5.1 ?
including our submitted runs ?
onthe test data.
Run1 drops in performance on the testset as compared to the training set, where the othertwo runs are more consistent, suggesting that lexi-cal similarity does not generalise well cross-domain.Table 4 shows that all of our systems performedabove the baseline on each dataset, except Run3 onFNWN.
Table 4 also shows that Run2 consistentlyperformed well on all the datasets when comparedto the median of all the systems submitted to the task(Agirre et al to appear).Run2, which was based on the concatenation ofall the feature sets, performed well compared to thestacking-based approaches on the test set, whereasthe stacking approaches all outperformed Run2 onthe training datasets.
This is likely due to theSS features being more effective for STS predic-tion in the training datasets as compared to the testdatasets.
Based on the training datasets, the stack-ing approaches placed greater weight on the pre-dictions from the SS feature set.
This hypothe-sis is supported by the result on Headlines, wherethe SS feature set does relatively well, and thus thestacking approaches tend to outperform the simpleconcatenation-based method.
Finally, an extensionof Run2 with flagging (not submitted to the sharedtask) was the best of our methods on the test data.5.3 Error AnalysisTo better understand the behaviour of our systems,we examined test instances and made the followingobservations.
Systems based entirely on the TM fea-tures and domain adaptation consistently performedwell on sentence pairs for which all of our other sys-tems performed poorly.
One example is the follow-ing OnWN pair, which corresponds to definitions ofnewspaper: an enterprise or company that publishesnewsprint and a business firm that publishes news-papers.
Because these texts do not share many com-mon words, the SS features cannot capture their se-mantic similarity.Stacking based approaches performed well on textpairs which are complex to comprehend, e.g., TwoGerman tourists, two pilots killed in Kenya air crashand Senator Reid involved in Las Vegas car crash,where the individual methods tend to score lower212System Headlines OnWN FNWN SMT Ave(+) Run1 .711 (15) .349 (71) .381 (23) .351 (18) .473 (49)(+) Run2 .709 (17) .679 (18) .337 (33) .323 (43) .542 (17)(+) Run3 .706 (18) .614 (28) .187 (71) .314 (47) .509 (29)Best .718 (14) .704 (15) .365 (28) .344 (24) .560 (7)(?)
Run1 .712 (14) .353 (70) .381 (23) .341 (25) .471 (54)(?)
Run2 .707 (18) .697 (14) .435 (9) .332 (35) .557 (9)(?)
Run3 .731 (11) .677 (19) .399 (17) .330 (38) .557 (8)(?)
Best .730 (11) .688 (17) .462 (7) .353 (18) .572 (4)Baseline .540 (67) .283 (81) .215 (67) .286 (65) .364 (73)Median .640 (45) .528 (45) .327 (45) .318 (45) .480 (45)Best-Score .783 (1) .843 (1) .581 (1) .403 (1) .618 (1)Table 4: Pearson?s ?
(and projected ranking) of runs.The upper 4 runs are trained only on STS 2012 train-ing data.
(+) denotes runs that were submitted forevaluation.
(?)
denotes systems trained on STS 2012training and test data.
For comparison, we include?Best?, the highest-scoring parametrization of oursystem from our post-task analysis (Table 3).
Wealso include the organiser?s baseline, as well as themedian and best systems for each dataset across allcompetitors.than the human rating, but stacking was able to pre-dict a higher score (presumably based on the factthat no method predicted the text pair to be stronglydissimilar; rather, all methods predicted there to besomewhat low similarity).In some cases, the texts are on a similar topic,but semantically different, e.g., Nigeria mourns over193 people killed in plane crash and Nigeria opensprobe into deadly air crash.
In such cases, systemsbased on SS features and stacking perform well.Systems based on TM and IR features, on the otherhand, tend to predict overly-high scores because thetexts relate to similar topics and tend to have similarrelevant documents in an external corpus.5.4 Results with the Full Training datasetWe re-trained all the above systems by extending thetraining data to include the 2012 test data.
Scores onthe 2013 test datasets and the change in Pearson?s ?after adding the extra training data (denoted ?)
arepresented in Table 3.In general, the addition of the 2012 test data tothe training dataset improves the performance of thesystem, though this is often not the case for the flag-ging approach to domain adaptation, which in someinstances drops in performance after adding the ad-ditional training data.
The biggest improvementswere seen for feature-domain stacking, particularlyon FNWN.
This suggests that feature-domain stack-ing is more sensitive to the similarity between train-ing data and test data than flagging, but also that itis better able to cope with variety in training do-mains than flagging.
Given that the pool of anno-tated data for the STS task continues to increase,feature-domain stacking is a promising approach toexploiting the differences between domains to im-prove overall STS performance.To facilitate comparison with the published re-sults for the 2013 STS task, we present a condensedsummary of our results in Table 4, which shows theabsolute score as well as the projected ranking ofeach of our systems.
It also includes the median andbaseline results for comparison.6 Conclusions and Future WorkIn this paper we described our approach to theSTS SemEval-2013 shared task.
While we did notachieve high scores relative to the other submit-ted systems on any of the datasets or overall, wehave identified some novel feature sets which weshow to have utility for the STS task.
We havealso compared our proposed method?s performancewith a larger training dataset.
In future work, weintend to consider alternative ways for combiningfeatures learned from different domains and trainingdatasets.
Given the strong performance of our stringsimilarity features on particular datasets, we also in-tend to consider combining string and distributionalsimilarity to capture elements of the texts that are notcurrently captured by our string similarity features.AcknowledgmentsThis work was supported by the European ErasmusMundus Masters Program in Language and Commu-nication Technologies from the European Commis-sion.NICTA is funded by the Australian governmentas represented by Department of Broadband, Com-munication and Digital Economy, and the AustralianResearch Council through the ICT Centre of Excel-lence program.213ReferencesEneko Agirre and David Martinez.
2001.
Knowl-edge sources for word sense disambiguation.
In Text,Speech and Dialogue, pages 1?10.
Springer.Eneko Agirre and Mark Stevenson.
2006.
Knowledgesources for wsd.
In Eneko Agirre and Philip Edmonds,editors, Word Sense Disambiguation, volume 33 ofText, Speech and Language Technology, pages 217?251.
Springer Netherlands.Eneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In *SEM 2012:The First Joint Conference on Lexical and Computa-tional Semantics ?
Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 385?393,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
to appear.
*sem 2013 sharedtask: Semantic textual similarity, including a pilot ontyped-similarity.
In *SEM 2013: The Second JointConference on Lexical and Computational Semantics,Atlana, USA.
Association for Computational Linguis-tics.Ramiz M Aliguliyev.
2009.
A new sentence similaritymeasure and sentence based extractive technique forautomatic text summarization.
Expert Systems withApplications, 36(4):7764?7772.Timothy Baldwin.
2009.
The hare and the tortoise:Speed and reliability in translation retrieval.
MachineTranslation, 23(4):195?240.Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
Ukp: Computing semantic textual simi-larity by combining multiple content similarity mea-sures.
In *SEM 2012: The First Joint Conferenceon Lexical and Computational Semantics ?
Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation (SemEval2012), pages 435?440, Montre?al, Canada, 7-8 June.Association for Computational Linguistics.Sam Biggins, Shaabi Mohammed, Sam Oakley, LukeStringer, Mark Stevenson, and Judita Preiss.
2012.University of sheffield: Two approaches to semantictext similarity.
In *SEM 2012: The First Joint Confer-ence on Lexical and Computational Semantics ?
Vol-ume 1: Proceedings of the main conference and theshared task, and Volume 2: Proceedings of the SixthInternational Workshop on Semantic Evaluation (Se-mEval 2012), pages 655?661, Montre?al, Canada, 7-8June.
Association for Computational Linguistics.Steven Bird.
2006.
NLTK: The Natural LanguageToolkit.
In Proceedings of the COLING/ACL 2006 In-teractive Presentation Sessions, pages 69?72, Sydney,Australia, July.
Association for Computational Lin-guistics.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alcation.
Journal of MachineLearning Research, 3:993?1022.Leo Breiman.
1996.
Bagging predictors.
Machine learn-ing, 24(2):123?140.Hal Daume?, III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26(1):101?126, May.Marco De Boni and Suresh Manandhar.
2003.
The useof sentence similarity as a semantic relevance metricfor question answering.
In Proceedings of the AAAISymposium on New Directions in Question Answering,Stanford, USA.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1162?1172, Cambridge,MA, October.
Association for Computational Linguis-tics.Weiwei Guo and Mona Diab.
2012.
Weiwei: A sim-ple unsupervised latent semantics based approach forsentence similarity.
In *SEM 2012: The First JointConference on Lexical and Computational Semantics?
Volume 1: Proceedings of the main conference andthe shared task, and Volume 2: Proceedings of theSixth International Workshop on Semantic Evaluation(SemEval 2012), pages 586?590, Montre?al, Canada,7-8 June.
Association for Computational Linguistics.Michael Heilman and Nitin Madnani.
2012.
Ets: Dis-criminative edit models for paraphrase scoring.
In*SEM 2012: The First Joint Conference on Lexi-cal and Computational Semantics ?
Volume 1: Pro-ceedings of the main conference and the shared task,and Volume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation (SemEval 2012),pages 529?535, Montre?al, Canada, 7-8 June.
Associa-tion for Computational Linguistics.Aminul Islam and Diana Inkpen.
2008.
Semantictext similarity using corpus-based word similarity andstring similarity.
ACM Transactions on KnowledgeDiscovery from Data (TKDD), 2(2):10.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In Proceedings of the 14th ACM in-ternational conference on Information and knowledgemanagement, CIKM ?05, pages 84?90, New York, NY,USA.
ACM.Adam Kilgarriff.
2001.
Comparing corpora.
Interna-tional Journal of Corpus Linguistics, 6(1):97?133.214Marco Lui, Timothy Baldwin, and Diana McCarthy.2012.
Unsupervised estimation of word usage simi-larity.
In Proceedings of the Australasian LanguageTechnology Association Workshop 2012, pages 33?41,Dunedin, New Zealand, December.Marco Lui.
2012.
Feature stacking for sentence clas-sification in evidence-based medicine.
In Proceed-ings of the Australasian Language Technology Associ-ation Workshop 2012, pages 134?138, Dunedin, NewZealand, December.Susan W McRoy.
1992.
Using multiple knowledgesources for word sense discrimination.
ComputationalLinguistics, 18(1):1?30.Stephen E Robertson and Steve Walker.
1994.
Somesimple effective approximations to the 2-poissonmodel for probabilistic weighted retrieval.
In Proceed-ings of the 17th annual international ACM SIGIR con-ference on Research and development in informationretrieval, SIGIR ?94, pages 232?241, Dublin, Ireland.Bahar Salehi and Paul Cook.
to appear.
Predictingthe compositionality of multiword expressions usingtranslations in multiple languages.
In *SEM 2013:The Second Joint Conference on Lexical and Com-putational Semantics, Atlana, USA.
Association forComputational Linguistics.TF Smith and MS Waterman.
1981.
Identification ofcommon molecular subsequences.
Molecular Biology,147:195?197.Mark Steyvers and Tom Griffiths.
2007.
Probabilistictopic models.
Handbook of latent semantic analysis,427(7):424?440.William Webber, Alistair Moffat, and Justin Zobel.2010.
A similarity measure for indefinite rankings.ACM Transactions on Information Systems (TOIS),28(4):20.David H. Wolpert.
1992.
Stacked generalization.
NeuralNetworks, 5:241?259.215
