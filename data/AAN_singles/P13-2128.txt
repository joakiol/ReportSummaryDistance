Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731?735,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDerivational Smoothing for Syntactic Distributional SemanticsSebastian Pado??
Jan S?najder?
Britta Zeller?
?Heidelberg University, Institut fu?r Computerlinguistik69120 Heidelberg, Germany?University of Zagreb, Faculty of Electrical Engineering and ComputingUnska 3, 10000 Zagreb, Croatia{pado, zeller}@cl.uni-heidelberg.de jan.snajder@fer.hrAbstractSyntax-based vector spaces are used widelyin lexical semantics and are more versatilethan word-based spaces (Baroni and Lenci,2010).
However, they are also sparse, withresulting reliability and coverage problems.We address this problem by derivationalsmoothing, which uses knowledge aboutderivationally related words (oldish?
old)to improve semantic similarity estimates.We develop a set of derivational smoothingmethods and evaluate them on two lexicalsemantics tasks in German.
Even for mod-els built from very large corpora, simplederivational smoothing can improve cover-age considerably.1 IntroductionDistributional semantics (Turney and Pantel, 2010)builds on the assumption that the semantic similar-ity of words is strongly correlated to the overlapbetween their linguistic contexts.
This hypothesiscan be used to construct context vectors for wordsdirectly from large text corpora in an unsupervisedmanner.
Such vector spaces have been applied suc-cessfully to many problems in NLP (see Turney andPantel (2010) or Erk (2012) for current overviews).Most distributional models in computational lex-ical semantics are either (a) bag-of-words models,where the context features are words within a sur-face window around the target word, or (b) syn-tactic models, where context features are typicallypairs of dependency relations and context words.The advantage of syntactic models is that theyincorporate a richer, structured notion of context.This makes them more versatile; the DistributionalMemory framework by Baroni and Lenci (2010) isapplicable to a wide range of tasks.
It is also able ?at least in principle ?
to capture more fine-grainedtypes of semantic similarity such as predicate-argument plausibility (Erk et al 2010).
At thesame time, syntactic spaces are much more proneto sparsity problems, as their contexts are sparser.This leads to reliability and coverage problems.In this paper, we propose a novel strategyfor combating sparsity in syntactic vector spaces,derivational smoothing.
It follows the intuition thatderivationally related words (feed ?
feeder, blocked?
blockage) are, as a rule, semantically highly simi-lar.
Consequently, knowledge about derivationallyrelated words can be used as a ?back off?
for sparsevectors in syntactic spaces.
For example, the pairoldish ?
ancient should receive a high semanticsimilarity, but in practice, the vector for oldish willbe very sparse, which makes this result uncertain.Knowing that oldish is derivationally related to oldallows us to use the much less sparse vector for oldas a proxy for oldish.We present a set of general methods for smooth-ing vector similarity computations given a resourcethat groups words into derivational families (equiv-alence classes) and evaluate these methods on Ger-man for two distributional tasks (similarity predic-tion and synonym choice).
We find that even forsyntactic models built from very large corpora, asimple derivational resource that groups words onmorphological grounds can improve the results.2 Related WorkSmoothing techniques ?
either statistical, distribu-tional, or knowledge-based ?
are widely applied inall areas of NLP.
Many of the methods were firstapplied in Language Modeling to deal with unseenn-grams (Chen and Goodman, 1999; Dagan et al1999).
Query expansion methods in InformationRetrieval are also prominent cases of smoothingthat addresses the lexical mismatch between queryand document (Voorhees, 1994; Gonzalo et al1998; Navigli and Velardi, 2003).
In lexical se-mantics, smoothing is often achieved by backing731off from words to semantic classes, either adoptedfrom a resource such as WordNet (Resnik, 1996) orinduced from data (Pantel and Lin, 2002; Wang etal., 2005; Erk et al 2010).
Similarly, distributionalfeatures support generalization in Named EntityRecognition (Finkel et al 2005).Although distributional information is often usedfor smoothing, to our knowledge there is littlework on smoothing distributional models them-selves.
We see two main precursor studies for ourwork.
Bergsma et al(2008) build models of se-lectional preferences that include morphologicalfeatures such as capitalization and the presence ofdigits.
However, their approach is task-specific andrequires a (semi-)supervised setting.
Allan and Ku-maran (2003) make use of morphology by buildinglanguage models for stemming-based equivalenceclasses.
Our approach also uses morphologicalprocessing, albeit more precise than stemming.3 A Resource for German DerivationDerivational morphology describes the process ofbuilding new (derived) words from other (basis)words.
Derived words can, but do not have to, sharethe part-of-speech (POS) with their basis (oldA?oldishA vs. warmA?
warmV , warmthN ).
Wordscan be grouped into derivational families by form-ing the transitive closure over individual derivationrelations.
The words in these families are typicallysemantically similar, although the exact degree de-pends on the type of relation and idiosyncratic fac-tors (bookN ?
bookishA, Lieber (2009)).For German, there are several resources withderivational information.
We use version 1.3of DERIVBASE (Zeller et al 2013),1 a freelyavailable resource that groups over 280,000 verbs,nouns, and adjectives into more than 17,000 non-singleton derivational families.
It has a precision of84% and a recall of 71%.
Its higher coverage com-pared to CELEX (Baayen et al 1996) and IMSLEX(Fitschen, 2004) makes it particularly suitable forthe use in smoothing, where the resource shouldinclude low-frequency lemmas.The following example illustrates a family thatcovers three POSes as well as a word with a pre-dominant metaphorical reading (to kneel?
to beg):knieenV (to kneelV ), beknieenV (tobegV ), KniendeN (kneeling personN ),kniendA (kneelingA), KnieNn (kneeN )1Downloadable from: http://goo.gl/7KG2UUsing derivational knowledge for smoothing raisesthe question of how semantically similar the lem-mas within a family really are.
Fortunately, DE-RIVBASE provides information that can be used inthis manner.
It was constructed with hand-writtenderivation rules, employing string transformationfunctions that map basis lemmas onto derived lem-mas.
For example, a suffixation rule using the affix?heit?
generates the derivation dunkel ?
Dunkel-heit (darkA ?
darknessN ).
Since derivational fam-ilies are defined as transitive closures, each pairof words in a family is connected by a derivationpath.
Because the rules do not have a perfect pre-cision, our confidence in pairs of words decreasesthe longer the derivation path between them.
To re-flect this, we assign each pair a confidence of 1/n,where n is the length of the shortest path betweenthe lemmas.
For example, bekleiden (enrobeV ) isconnected to Verkleidung (disguiseN ) through threesteps via the lemmas kleiden (dressV ) and verklei-den (disguiseV ) and is assigned the confidence 1/3.4 Models for Derivational SmoothingDerivational smoothing exploits the fact that deriva-tionally related words are also semantically related,by combining and/or comparing distributional rep-resentations of derivationally related words.
Thedefinition of a derivational smoothing algorithmconsists of two parts: a trigger and a scheme.Notation.
Given a word w, we use w to denoteits distributional vector and D(w) to denote the setof vectors for the derivational family of w. Weassume that w ?
D(w).
For words that have noderivations in DERIVBASE, D(w) is a singletonset, D(w) = {w}.
Let ?(w,w?)
denote the confi-dence of the pair (w,w?
), as explained in Section 3.Smoothing trigger.
As discussed above, there isno guarantee for high semantic similarity within aderivational family.
For this reason, smoothing mayalso drown out information.
In this paper, we reporton two triggers: smooth always always performssmoothing; smooth if sim=0 smooths only whenthe unsmoothed similarity sim(w1,w2) is zero orunknown (due to w1 or w2 not being in the model).Smoothing scheme.
We present three smoothingschemes, all of which apply to the level of completefamilies.
The first two schemes are exemplar-basedschemes, which define the smoothed similarity fora word pair as a function of the pairwise similaritiesbetween all words of the two derivational families.732The first one, maxSim, checks for particularly simi-lar words in the families:maxSim(w1, w2) = maxw?1?D(w1)w?2?D(w2)sim(w?1,w?2)The second one, avgSim, computes the averagepairwise similarity (N is the number of pairs):avgSim(w1, w2) =1N?w?1?D(w1)w?2?D(w2)sim(w?1,w?2)The third scheme, centSim, is prototype-based.
Itcomputes a centroid vector for each derivationalfamily, which can be thought of as a representationfor the concept(s) that it expresses:centSim(w1, w2) = sim(c(D(w1)), c(D(w2)))where c(D(w)) =?w?
?D(w) ?(w,w?)
?w?
is theconfidence-weighted centroid vector.
centSim issimilar to avgSim.
It is more efficient to calculateand effectively introduces a kind or regularization,where outliers in either family have less impact onthe overall result.These models only represents a sample of possi-ble derivational smoothing methods.
We performeda number of additional experiments (POS-restrictedsmoothing, word-based, and pair-based smoothingtriggers), but they did not yield any improvementsover the simpler models we present here.5 Experimental EvaluationSyntactic Distributional Model.
The syntacticdistributional model that we use represents targetwords by pairs of dependency relations and contextwords.
More specifically, we use the W ?
LWmatricization of DM.DE, the German version (Pado?and Utt, 2012) of Distributional Memory (Baroniand Lenci, 2010).
DM.DE was created on the basisof the 884M-token SDEWAC web corpus (Faa?
etal., 2010), lemmatized, tagged, and parsed with theGerman MATE toolkit (Bohnet, 2010).Experiments.
We evaluate the impact of smooth-ing on two standard tasks from lexical semantics.The first task is predicting semantic similarity.
Welemmatized and POS-tagged the German GUR350dataset (Zesch et al 2007), a set of 350 word pairswith human similarity judgments, created analo-gously to the well-known Rubenstein and Good-enough (1965) dataset for English.2 We predict2Downloadable from: http://goo.gl/bFokIsemantic similarity as cosine similarity.
We makea prediction for a word pair if both words are repre-sented in the semantic space and their vectors havea non-zero similarity.The second task is synonym choice on the Ger-man version of the Reader?s Digest WordPowerdataset (Wallace and Wallace, 2005).2 This dataset,which we also lemmatized and POS-tagged, con-sists of 984 target words with four synonym can-didates each (including phrases), one of which iscorrect.
Again, we compute semantic similarity asthe cosine between target and a candidate vectorand pick the highest-similarity candidate as syn-onym.
For phrases, we compute the maximumpairwise word similarity.
We make a prediction foran item if the target as well as at least one candi-date are represented in the semantic space and theirvectors have a non-zero similarity.We expect differences between the two taskswith regard to derivational smoothing, since thewords within derivational families are generally re-lated but often not synonymous (cf.
the examplein Section 3).
Thus, semantic similarity judgmentsshould profit more easily from derivational smooth-ing than synonym choice.Baseline.
Our baseline is a standard bag-of-words vector space (BOW), which represents targetwords by the words occurring in their context.
Weuse standard parameters (?10 word window, 8.000most frequent verb, noun, and adjective lemmas).The model was created from the same corpus asDM.DE.
We also applied derivational smoothingto this model, but did not obtain improvements.Evaluation.
To analyze the impact of smoothing,we evaluate the coverage of models and the qualityof their predictions separately.
In both tasks, cover-age is the percentage of items for which we makea prediction.
We measure quality of the semanticsimilarity task as the Pearson correlation betweenthe model predictions and the human judgmentsfor covered items (Zesch et al 2007).
For syn-onym choice, we follow the method established byMohammad et al(2007), measuring accuracy overcovered items, with partial credit for ties.Results for Semantic Similarity.
Table 1 showsthe results for the first task.
The unsmoothedDM.DE model attains a correlation of r = 0.44and a coverage of 58.9%.
Smoothing increases thecoverage substantially to 88%.
Additionally, con-servative, prototype-based smoothing (if sim = 0)733SmoothingtriggerSmoothingschemer Cov%DM.DE, unsmoothed .44 58.9DM.DE,smooth alwaysavgSim .30 88.0maxSim .43 88.0centSim .44 88.0DM.DE,smooth if sim = 0avgSim .43 88.0maxSim .42 88.0centSim .47 88.0BOW baseline .36 94.9Table 1: Results on the semantic similarity task(r: Pearson correlation, Cov: Coverage)increases correlation somewhat to r = 0.47.
Thedifference to the unsmoothed model is not signif-icant at p = 0.05 according to Fisher?s (1925)method of comparing correlation coefficients.The bag-of-words baseline (BOW) has a greatercoverage than DM.DE models, but at the costof lower correlation across the board.
The onlyDM.DE model that performs worse than the BOWbaseline is the non-conservative avgSim (averagesimilarity) scheme.
We attribute this weak per-formance to the presence of many pairwise zerosimilarities in the data, which makes the avgSimpredictions unreliable.To our knowledge, there are no previous pub-lished papers on distributional approaches to mod-eling this dataset.
The best previous result is aGermaNet/Wikipedia-based model by Zesch et al(2007).
It reports a higher correlation (r = 0.59)but a very low coverage at 33.1%.Results for Synonym Choice.
The results forthe second task are shown in Table 2.
The un-smoothed model achieves an accuracy of 53.7%and a coverage of 80.8%, as reported by Pado?and Utt (2012).
Smoothing increases the cover-age by almost 6% to 86.6% (for example, a ques-tion item for inferior becomes covered after back-ing off from the target to Inferiorita?t (inferiority)).All smoothed models show a loss in accuracy, al-beit small.
The best model is again a conservativesmoothing model (sim = 0) with a loss of 1.1% ac-curacy.
Using bootstrap resampling (Efron and Tib-shirani, 1993), we established that the differenceto the unsmoothed DM.DE model is not signifi-cant at p < 0.05.
This time, the avgSim (averagesimilarity) smoothing scheme performs best, withthe prototype-based scheme in second place.
Thus,the results for synonym choice are less clear-cut:derivational smoothing can trade accuracy againstSmoothingtriggerSmoothingschemeAcc%Cov%DM.DE, unsmoothed (Pado?
& Utt 2012) 53.7 80.8DM.DE,smooth alwaysavgSim 46.0 86.6maxSim 50.3 86.6centSim 49.1 86.6DM.DE,smooth if sim = 0avgSim 52.6 86.6maxSim 51.2 86.6centSim 51.3 86.6BOW ?baseline?
56.9 98.5Table 2: Results on the synonym choice task(Acc: Accuracy, Cov: Coverage)coverage but does not lead to a clear improvement.What is more, the BOW ?baseline?
significantlyoutperforms all syntactic models, smoothed andunsmoothed, with an almost perfect coverage com-bined with a higher accuracy.6 Conclusions and OutlookIn this paper, we have introduced derivationalsmoothing, a novel strategy to combating sparsityin syntactic vector spaces by comparing and com-bining the vectors of morphologically related lem-mas.
The only information strictly necessary forthe methods we propose is a grouping of lemmasinto derivationally related classes.
We have demon-strated that derivational smoothing improves twotasks, increasing coverage substantially and alsoleading to a numerically higher correlation in thesemantic similarity task, even for vectors createdfrom a very large corpus.
We obtained the best re-sults for a conservative approach, smoothing onlyzero similarities.
This also explains our failureto improve less sparse word-based models, wherevery few pairs are assigned a similarity of zero.A comparison of prototype- and exemplar-basedschemes did not yield a clear winner.
The estima-tion of generic semantic similarity can profit morefrom derivational smoothing than the induction ofspecific lexical relations.In future work, we plan to work on other eval-uation tasks, application to other languages, andmore sophisticated smoothing schemes.Acknowledgments.
Authors 1 and 3 were sup-ported by the EC project EXCITEMENT (FP7 ICT-287923).
Author 2 was supported by the CroatianScience Foundation (project 02.03/162: ?Deriva-tional Semantic Models for Information Retrieval?
).We thank Jason Utt for his support and expertise.734ReferencesJames Allan and Giridhar Kumaran.
2003.
Stemmingin the Language Modeling Framework.
In Proceed-ings of SIGIR, pages 455?456.Harald R. Baayen, Richard Piepenbrock, and Leon Gu-likers.
1996.
The CELEX Lexical Database.
Re-lease 2.
LDC96L14.
Linguistic Data Consortium,University of Pennsylvania, Philadelphia, Pennsyl-vania.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional Memory: A General Framework forCorpus-based Semantics.
Computational Linguis-tics, 36(4):673?721.Shane Bergsma, Dekang Lin, and Randy Goebel.
2008.Discriminative Learning of Selectional Preferencefrom Unlabeled Text.
In Proceedings of EMNLP,pages 59?68, Honolulu, Hawaii.Bernd Bohnet.
2010.
Top Accuracy and Fast Depen-dency Parsing is not a Contradiction.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, pages 89?97, Beijing, China.Stanley F. Chen and Joshua Goodman.
1999.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
Computer Speech and Language,13(4):359?394.Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.1999.
Similarity-Based Models of Word Cooccur-rence Probabilities.
Machine Learning, 34(1?3):43?69.Bradley Efron and Robert J. Tibshirani.
1993.
AnIntroduction to the Bootstrap.
Chapman and Hall,New York.Katrin Erk, Sebastian Pado?, and Ulrike Pado?.
2010.A Flexible, Corpus-driven Model of Regular and In-verse Selectional Preferences.
Computational Lin-guistics, 36(4):723?763.Katrin Erk.
2012.
Vector Space Models of Word Mean-ing and Phrase Meaning: A Survey.
Language andLinguistics Compass, 6(10):635?653.Gertrud Faa?, Ulrich Heid, and Helmut Schmid.
2010.Design and Application of a Gold Standard for Mor-phological Analysis: SMOR in Validation.
In Pro-ceedings of LREC-2010, pages 803?810.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating Non-local Informa-tion into Information Extraction Systems by GibbsSampling.
In Proceedings of the 43rd Annual Meet-ing of the ACL, pages 363?370.Ronald Aylmer Fisher.
1925.
Statistical methods forresearch workers.
Oliver and Boyd, Edinburgh.Arne Fitschen.
2004.
Ein computerlinguistischesLexikon als komplexes System.
Ph.D. thesis, IMS,Universita?t Stuttgart.Julio Gonzalo, Felisa Verdejo, Irina Chugur, andJuan M. Cigarra?n.
1998.
Indexing with WordNetSynsets Can Improve Text Retrieval.
In Proceed-ings of the COLING/ACL Workshop on Usage ofWordNet in Natural Language Processing Systems,Montre?al, Canada.Rochelle Lieber.
2009.
Morphology and Lexical Se-mantics.
Cambridge University Press.Saif Mohammad, Iryna Gurevych, Graeme Hirst, andTorsten Zesch.
2007.
Cross-Lingual DistributionalProfiles of Concepts for Measuring Semantic Dis-tance.
In Proceedings of the 2007 Joint Conferenceon EMNLP and CoNLL, pages 571?580, Prague,Czech Republic.Roberto Navigli and Paola Velardi.
2003.
An Analysisof Ontology-based Query Expansion Strategies.
InWorkshop on Adaptive Text Extraction and Mining,Dubrovnik, Croatia.Sebastian Pado?
and Jason Utt.
2012.
A DistributionalMemory for German.
In Proceedings of KONVENS2012 workshop on lexical-semantic resources andapplications, pages 462?470, Vienna, Austria.Patrick Pantel and Dekang Lin.
2002.
DiscoveringWord Senses from Text.
In In Proceedings of ACMSIGKDD Conference on Knowledge Discovery andData Mining, pages 613?619.Philip Resnik.
1996.
Selectional Constraints: AnInformation-theoretic Model and its ComputationalRealization.
Cognition, 61(1-2):127?159.Herbert Rubenstein and John B. Goodenough.
1965.Contextual Correlates of Synonymy.
Communica-tions of the ACM, 8(10):627?633.Peter D. Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning: Vector Space Models of Se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.Ellen M. Voorhees.
1994.
Query Expansion UsingLexical-semantic Relations.
In Proceedings of SI-GIR, pages 61?69.DeWitt Wallace and Lila Acheson Wallace.
2005.Reader?s Digest, das Beste fu?r Deutschland.
VerlagDas Beste, Stuttgart.Qin Iris Wang, Dale Schuurmans, and Dekang Lin.2005.
Strictly Lexical Dependency Parsing.
In Pro-ceedings of IWPT, pages 152?159.Britta Zeller, Jan S?najder, and Sebastian Pado?.
2013.DErivBase: Inducing and Evaluating a DerivationalMorphology Resource for German.
In Proceedingsof ACL, Sofia, Bulgaria.Torsten Zesch, Iryna Gurevych, and Max Mu?hlha?user.2007.
Comparing Wikipedia and German Word-net by Evaluating Semantic Relatedness on Multi-ple Datasets.
In Proceedings of NAACL/HLT, pages205?208, Rochester, NY.735
