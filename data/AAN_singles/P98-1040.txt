Dialogue Management in Vector-Based Call RoutingJennifer Chu-Carroll and Bob CarpenterLucent  Technolog ies  Bel l  Laborator ies600 Mounta in  AvenueMurray  Hil l ,  N J  07974, U.S.A.E-mai l :  { jencc,carp} @research.bel l - labs.cornAbstractThis paper describes a domain independent, automat-ically trained call router which directs customer callsbased on their response to an open-ended "How may I di-rect your call?"
query.
Routing behavior is trained froma corpus of transcribed and hand-routed calls and thencarried out using vector-based information retrieval tech-niques.
Based on the statistical discriminating power ofthe n-gram terms extracted from the caller's request, thecaller is 1) routed to the appropriate destination, 2) trans-ferred to a human operator, or 3) asked a disambigua-tion question.
In the last case, the system dynamicallygenerates queries tailored to the caller's request and thedestinations with which it is consistent.
Our approachis domain independent and the training process is fullyautomatic.
Evaluations over a financial services call cen-ter handling hundreds of activities with dozens of desti-nations demonstrate a substantial improvement on exist-ing systems by correctly routing 93.8% of the calls afterpunting 10.2% of the calls to a human operator.1 IntroductionThe call routing task involves directing a user's call tothe appropriate destination within a call center or pro-viding some simple information, such as loan rates.
Incurrent systems, the user's goals are typically gleanedvia a touch-tone system employing a rigid hierarchicalmenu.
The primary disadvantages of navigating menusfor users are the time it takes to listen to all the optionsand the difficulty of matching their goals to the options;these problems are compounded by the necessity of de-scending a nested hierarchy of choices to zero in on aparticular activity.
Even simple requests such as "I 'd likemy savings account balance" may require users to nav-igate as many as four or five nested menus with four orfive options each.
We have developed an alternative totouch-tone menus that allows users to interact with a callrouter in natural spoken English dialogues just as theywould with a human operator.Human operators respond to a caller request by 1)routing the call to an appropriate destination, or 2) query-ing the caller for further information to determine whereto route the call.
Our automatic call router has these twooptions as well as a third option of sending the call to ahuman operator.
The rest of this paper provides both adescription and an evaluation of an automatic call routerdriven by vector-based information retrieval techniques.After introducing our fundamental routing technique, wefocus on the disambiguation query generation module.Our disambiguation module is based on the same sta-tistical training as routing, and dynamically generatesqueries tailored to the caller's request and the destina-tions with which it is consistent.
The main advantagesof our system are that 1) it is domain independent, 2) itis trained fully automatically to both route and disam-biguate requests, and 3) its performance is sufficient foruse in the field, substantially improving on that of previ-ous systems.2 Related WorkCall routing is similar to topic identification (Mc-Donough et al, 1994) and document routing (Harman,1995) in identifying which one of n topics (destinations)most closely matches a caller's request.
Call routing isdistinguished from these activities by requiring a singledestination, but allowing a request to be refined in an in-teractive dialogue.
We are further interested in carryingout the routing using natural, conversational l nguage.The only work on call routing to date that we areaware of is that by Gorin et al (to appear).
They se-lect salient phrase fragments from caller equests, uch asmade a long distance and the area code for.
These phrasefragments are used to determine the most likely destina-tion(s), which they refer to as call type(s), for the requesteither by computing the a posteriori probability for eachcall type or by passing the fragments through a neuralnetwork classifier.
Abella and Gorin (1997) utilized theBoolean formula minimization algorithm for combiningthe resulting set of call types based on a hand-coded hi-erarchy of call types.
Their intention is to utilize the out-come of this algorithm to select from a set of dialoguestrategies for response generation.3 Corpus  Ana lys i sTo examine human-human dialogue behavior in callrouting, we analyzed a set of 4497 transcribed telephonecalls involving customers interacting with human opera-tors, looking at both the semantics of caller requests and256Name Activity Indirect# of calls 949 3271 277% of all calls 21.1% 72.7% 6.2%Table 1: Semantic Types of Caller Requestsdialogue actions for response generation.
The call cen-ter provides financial services in hundreds of categoriesin the general areas of banking, credit cards, loans, insur-ance and investments; we concentrated on the 23 desti-nations for which we had at least 10 calls in the corpus.3.1 Semantics of Caller RequestsThe operator provides an open-ended prompt of "Howmay I direct your call?"
We classified user responsesinto three categories.
First, callers may explicitly pro-vide a destination name, either by itself or embeddedin a complete sentence, such as "may I have consumerlending?"
Second, callers may describe the activity theywould like to perform.
Such requests may be unambigu-ous, such as "l'd like my checking account balance", orambiguous, uch as "car loans please", which in our callcenter can be resolved to either consumer lending, whichhandles new car loans, or to loan services, which handlesexisting car loans.
Third, a caller can provide an indirectrequest, in which they describe their goal in a round-about way, often including irrelevant information.
Thisoften occurs when the caller either is unfamiliar with thecall center hierarchy or does not have a concrete idea ofhow to achieve the goal, as in "ah I'm calling 'cuz ah afriend gave me this number and ah she told me ah withthis number I can buy some cars or whatever but shedidn't know how to explain it to me so l just called youyou know to get that information.
"Table 1 shows the distribution of caller requests in ourcorpus with respect to these semantic types.
Our analysisshows that in the vast majority of calls, the request wasbased on destination ame or activity.
Since there is afairly small number (dozens to hundreds) of activities be-ing handled by each destination, requests based on nameand activity are expected to be more predictable and thusmore suitable for handling by an automatic all router.Thus, our goal is to automatically route those calls basedon name and activity, while leaving the indirect or inap-propriate requests to human call operators.3.2 Dialogue Actions for Response GenerationWe also analyzed the operator's responses to caller re-quests to determine the dialogue actions needed for re-sponse generation i  our automatic all router.
We foundthat in the call routing task, the call operator either no-tifies the customer of the routing destination or asks adisambiguating query.llln cases where the operator generates an acknowledgment, suchas uh-huh, midway through the caller's request, we analyzed the nextoperator utterance.Notification# of calls 3608% of all calls 80.2%QueryNP I Others657 23214.6% 5.2%Table 2: Call Operator Dialogue ActionsCallerReslxmseCaller Request-I Iandidale D stinationsR,,,a,z t_!..~+o.,~,L.o.,~ 0 Not i f i ca thm~~ ential QueryDisambiRuating Yes f Query ~ Human Query ~ - OperatorFigure 1: Call Router ArchitectureTable 2 shows the frequency that each dialogue ac-tion should be employed based strictly on the presenceof ambiguity in the caller requests in our corpus.
We fur-ther analyzed those calls considered ambiguous withinour call center and noted that 75% of such ambiguous re-quests involve underspecified noun phrases, such as re-questing car loans without specifying whether it is anexisting or new car loan.
The remaining 25% of theambiguous requests involve underspecified verb phrases,such as asking to transfer funds without specifying thetypes of accounts to and from which the transfer will oc-cur, or missing verb phrases, such as asking for directdeposit without specifying whether the caller wants toset up or change an existing direct deposit.4 Dialogue Management in Call RoutingOur call router consists of two components: the rout-ing module and the disambiguation module.
The rout-ing module takes a caller request and determines a set ofdestinations to which the call can reasonably be routed.If there is exactly one such destination, the call is routedthere and the customer notified; if there are multiple des-tinations, the disambiguation module is invoked in an at-tempt o formulate a query; and if there is no appropriatedestination or if a reasonable disambiguation query can-not be generated, the call is routed to an operator.
Fig-ure I shows a diagram outlining this process.4.1 The Routing ModuleOur approach is novel in its application of informationretrieval techniques to select candidate destinations fora call.
We treat call routing as an instance of documentrouting, where a collection of judged documents i usedfor training and the task is to judge the relevance of a setof test documents (Schiitze et al, 1995).
More specifi-257cally, each destination i our call center is represented asa collection of documents (transcriptions of calls routedto that destination), and given a caller request, we judgethe relevance of the request o each destination.4.1.1 The Training ProcessDocument Construction Our training corpus consistsof 3753 calls each of which is hand-routed to one of23 destinations.
2 Our first step is to create one (virtual)document per destination, which contains the text of thecallers' contributions to all calls routed to that destina-tion.Morphological  F i l ter ing We filter each (virtual) doc-ument through the morphological processor of the BellLabs' Text-to-Speech synthesizer (Sproat, 1997) to ex-tract the root form of each word in the corpus.
Next,the root forms of caller utterances are filtered throughtwo lists, the ignore list and the stop list, in order tobuild a better n-gram model.
The ignore list consistsof noise words, such as uh and um, which sometimesget in the way of proper n-gram extraction, as in "I'dlike to speak to someone about a car uh loan".
Withnoise word filtering, we can properly extract he bigram"car, loan".
The stop list enumerates words that do notdiscriminate between destinations, uch as the, be, andafternoon.
We modified the standard stop list distributedwith the SMART information retrieval system (Salton,1971) to include domain specific terms and proper namesthat occurred in the training corpus.
Note that when astop word is filtered out of the caller utterance, a place-holder is inserted to prevent the words preceding and fol-lowing the stop word to form n-grams.
For instance, af-ter filtering the stop words out of "I want to check on anaccount", the utterance becomes "<sw> <sw> <sw>check <sw> <sw> account".
Without he placeholders,we would extract he bigram "check, account", just as ifthe caller had used the term checking account.Term Extract ion We extract he n-gram terms that oc-cur more frequently than a pre-determined threshold anddo not contain any stop words.
Our current system usesunigrams that occurred at least twice and bigrams andtrigrams that occurred at least three times in the corpus.No 4-grams occurred three times.Term-Document Matr ix  Once the set of relevantterms is determined, we construct an m x n term-document frequency matrix A whose rows represent them terms, whose columns represent the n destinations,and where an entry At,a is the frequency with which termt occurs in calls to destination d.It is often advantageous to weight the raw countsto fine tune the contribution of each term to routing.We begin by normalizing the row vectors representingterms by making them each of unit length.
Thus we di-vide each row At in the original matrix by its length,2These 3753 calls are a subset of the corpus of 4497 calls used inour  corpus analysis.
We excluded those ambiguous calls that were notresolved by the operator.A 2 1/2 (E l<e<n t,e) .
Our second weighting is based onthe n-oti-on that a term that only occurs in a few docu-ments is more important in discriminating among docu-ments than a term that occurs in nearly every document.We use the inverse document frequency (IDF) weightingscheme (Sparck Jones, 1972) whereby a term is weightedinversely to the number of documents in which it occurs,by means oflDF(t) = log 2 n/d(t) where t is a term, n isthe total number of documents in the corpus, and d(t) isthe number of documents containing the term t. Thus weobtain a weighted matrix B, whose elements are givenby Bt,a = At,a x IDF(t)/(~-~x<e< n A2,e)x/2.Vector Representation To reduce the dimensional-ity of our vector representations for terms and doc-uments, we applied the singular value decomposition(Deerwester et al, 1990) to the m x n matrix B ofweighted term-document frequencies.
Specifically, wetake B = USV T, where U is an m x r orthonormal ma-trix (where r is the rank of B), V is an n x r orthonor-mal matrix, and S is an r x r diagonal matrix such thatSl,1 ~_~ 82,2 ~> " '"  ~> Sr,r ~ O.We can think of each row in U as an r-dimensionalvector that represents a term, whereas each row in V isan r-dimensional vector epresenting a document.
Withappropriate scaling of the axes by the singular valueson the diagonal of S, we can compare documents todocuments and terms to terms using their correspondingpoints in this new r-dimensional space (Deerwester etal., 1990).
For instance, to employ the dot product oftwo vectors as a measure of their similarity as is com-mon in information retrieval (Salton, 1971), we have thematrix BTB whose elements contain the dot product ofdocument vectors.
Because S is diagonal and U is or-thonormal, BTB = VSZV T = VS(VS)  T. Thus, ele-ment i, j in BTB,  representing the dot product betweendocument vectors i and j ,  can be computed by takingthe dot product between the i and j rows of the matrixVS.
In other words, we can consider ows in the matrixVS as vectors representing documents for the purposeof document/document comparison.
An element of theoriginal matrix Bi,j, representing the degree of associa-tion between the ith term and the j th  document, can berecovered by multiplying the ith term vector by the j thscaled document vector, namely B i j  = Ui((VS)j)  T.4.1.2 Call RoutingGiven the vector epresentations of terms and documents(destinations) in r-dimensional space, how do we deter-mine to which destination a new call should be routed?Our process for vector-based call routing consists of thefollowing four steps:Term Extraction Given a transcription of the caller'sutterance (either from a keyboard interface or from theoutput of a speech recognizer), the first step is to extractthe relevant n-gram terms from the utterance.
For in-stance, term extraction on the request "I want to checkthe balance in my savings account" would result in258one bigram term, "saving, account", and two unigrams,"check" and "balance".Pseudo-Document Generation Given the extractedterms from a caller request, we can represent the requestas an m-dimensional vector Q where each component Qirepresents he number of times that the ith term occurredin the caller's request.
We then create an r-dimensionalpseudo-document vector D = QU, following the stan-dard methodology of vector-based information retrieval(see (Deerwester et al, 1990)).
Note that D is simplythe sum of the term vectors Ui for all terms occurring inthe caller's request, weighted by their frequency of oc-currence in the request, and is scaled properly for docu-ment/document comparison.Scoring Once the vector D for the pseudo-document isdetermined, we compare it with the document vectors bycomputing the cosine between D and each scaled docu-ment vectors in VS. Next, we transform the cosine scorefor each destination using a sigmoid function specificallyfitted for that destination to obtain a confidence score thatrepresents he router's confidence that the call should berouted to that destination.The reason for the mapping from cosine scores to con-fidence scores is because the absolute degree of similar-ity between a request and a destination, as given by thecosine value between their vector epresentations, doesnot translate directly into the likelihood for correct rout-ing.
Instead, some destinations may require a higher co-sine value, i.e., a closer degree of similarity, than othersin order for a request to be correctly associated with thosedestinations.
Thus we collected, for each destination,a set of cosine value/routing value pairs over all callsin the training data, where the routing value is 1 if thecall should be routed to that destination and 0 otherwise.Then for each destination, we used the least squared errormethod in fitting a sigmoid function, 1/(1 + e-(a~+b)),to the set of cosine/routing pairs.We tested the routing performance using cosine vs.confidence values on 307 unseen unambiguous requests.In each case, we selected the destination with the high-est cosine/confidence score to be the target destination.Using strict cosine scores, 92.2% of the calls are routedto the correct destination.
On the other hand, using sig-moid confidence fitting, 93.5% of the calls are correctlyrouted.
This yields a relative reduction in error rate of16.7%.Decision Making The outcome of the routing moduleis a set of destinations whose confidence scores are abovea pre-determined threshold.
These candidate destinationsrepresent those to which the caller's request can reason-ably be routed.
If there is only one such destination, thenthe call is routed and the caller notified; if there are twoor more possible destinations, the disambiguation mod-ule is invoked in an attempt to formulate aquery; other-wise, the the call is routed to an operator.To determine the optimal value for the threshold, weI0.80.60.40.200Uppcd~ndL~,wcrb~mnd0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Thr~h~ddFigure 2: Router Performance vs. Thresholdran a series of experiments ocompute the upperboundand lowerbound of the router's performance varying thethreshold from 0 to 0.9 at 0.1 intervals.
The lowerboundrepresents the percentage of calls that are routed cor-rectly, while the upperbound indicates the percentage ofcalls that have the potential to be routed correctly afterdisambiguation (see section 5 for details on upperboundand lowerbound measures).
The results in Figure 2 show0.2 to be the threshold that yields optimal performance.4.2 The Disambiguation ModuleThe disambiguation module attempts o formulate an ap-propriate query to solicit further information from thecaller in order to determine a unique destination to whichthe call should be routed.
To generate an appropriatequery, the caller's request and the candidate destinationsmust both be taken into account.
We have developeda vector-based method for dynamically generating dis-ambiguation queries by first selecting a set of terms andthen forming a wh or yes-no question from these selectedterms.The terms selected by the disambiguation mechanismare those terms related to the original request hat canlikely be used to disambiguate among the candidate des-tinations.
These terms are chosen by filtering all termsbased on the following three criteria:I. Closeness: We choose terms that are close (bythe cosine measure) to the differences between thescaled pseudo-document query vector, D, and vec-tors representing the candidate destinations in VS.The intuition is that adding terms close to the differ-ences will disambiguate he original query.2.
Relevance: From the close terms, we construct aset of relevant erms which are terms that furtherspecify a term in the original request.
A close termis considered relevant if it can be combined with aterm in the request to form a valid n-gram term, andthe relevant term will be the resulting n-gram tenn.For instance, if "car,loan" is in the original request,then both "new" and "new, car" would produce therelevant term "new, car, loan"3.
Disambiguating power: Finally, we restrict at-tention to relevant erms that can be added to the259original request o result in an unambiguous rout-ing using the routing mechanism described in Sec-tion 4.1.2.
If none of the relevant terms satisfy thiscriterion, then we include all relevant erms in theset of disambiguating terms.
Thus, instead of givingup the disambiguation process when no one term ispredicted to resolve the ambiguity, the system maypose a question which further specifies the requestand then select a disambiguating term based on thisrefined (although still ambiguous) request.The result of this filtering process is a finite set ofterms which are relevant to the original ambiguous queryand, when added to it, are likely to resolve the ambigu-ity.
If a significant number of these terms share a headword, such as loan, the system asks the wh-question "forwhat type of loan ?'"
Otherwise, the term that occurredmost frequently in the training data is selected, based onthe heuristic that a more common term is likely to berelevant han an obscure term, and a yes-no question isformed based on this term.
A third alternative would beto ask a disjunctive question, but we have not yet ex-plored this possibility.
Figure 1 shows that after the sys-tem poses its query, it attempts to route the refined re-quest, which is the original request augmented with thecaller response to the system's query.
In the case of wh-questions, n-gram terms are extracted from the caller'sresponse.
In the case of yes-no questions, the system de-termines whether ayes or no answer is given.
3 In the for-mer case, the disambiguating term used to form the queryis considered the caller response, while in the latter case,the response is treated as in responses to wh-questions.Note that our disambiguation mechanism, like our ba-sic routing technique, is fully domain-indepefident.
Iutilizes a set of n-gram terms, as well as term and doc-ument vectors that were obtained by the training of thecall router.
Thus, porting the call router to a new domainrequires no change in the disambiguation module.4.3 ExampleTo illustrate our call router, consider the request "loansplease."
This request is ambiguous because our callcenter handles mortgage loans separately from all othertypes of loans, and for all other loans, existing loans andnew loans are again handled by different departments.Given this request, the call router first extracts the rel-evant n-gram terms, which in this case results in the uni-gram "'loan".
It then computes a pseudo-document vec-tor that represents his request, which is compared in turnwith the 23 vectors representing all destinations in thecall center.
The cosine values between the request andeach destination are then mapped into confidence values.3 In our current system, aresponse isconsidered ayes response onlyif it explicitly contains the word yes.
However, as discussed in (Greenand Carberry, 1994; Hockey et al, 1997), responses toyes-no questionsmay not explicitly contain a yes or no term.
We leave incorporating amore sophisticated response understanding model, such as (Green andCarberry, 1994), into our system for future work.Using a confidence threshold of 0.2, we have two can-didate destinations, Loan Servicing and Consumer Lend-ing; thus the disambiguation module is invoked.Our disambiguation module first selects from all n-gram terms those whose term vectors are close to the dif-ference between the request vector and either of the twocandidate destination vectors.
This results in a list of 60close terms, the vast majority of which are semanticallyclose to "loan", such as "auto, loan", "payoff", and"owe".
Next, the relevant erms are constructed fromthe set of close terms.
This results in a list of 27 relevantterms, including "'auto, loan" and "loan,payoff", but ex-cluding owe, since neither "loan, owe" nor "owe, loan"constitutes a valid bigram.
The third step is to selectthose relevant erms with disambiguation power, result-ing in 18 disambiguating terms.
Since 11 of these disam-biguating terms share a head noun loan, a wh-question isgenerated based on this head word, resulting in the query"for what type of loan ?
"Suppose in response to the system's query, the useranswers "car loan".
The router then adds the new bi-gram "car, loan" to the original request and attempts toroute the refined request.
This refined request is againambiguous between Loan Servicing and Consumer Lend-ing since the caller did not specify whether it was an ex-isting or new car loan.
Again, the disambiguation mod-ule selects the close, relevant, and disambiguating terms,resulting in a unique term "exist, car, loan".
Thus, thesystem generates the yes-no question "is this about anexisting car loan ?
,4 If the user responds "yes", then thetrigram term "exist, car, loan" is added to the refined re-quest and the call routed to Loan Servicing; if the usersays "'no, it's a new car loan", then "new, car, loan" isextracted from the response and the call routed to Con-sumer Lending.5 Evaluation5.1 The Routing ModuleWe performed an evaluation of the routing module of ourcall router on a fresh set of 389 calls to a human opera-tor.
5 Out of the 389 requests, 307 are unambiguous androuted to their correct destinations, and 82 were ambigu-ous and annotated with a list of candidate destinations.Unfortunately, in this test set, only the caller's first ut-terance was transcribed.
Thus we have no informationabout where the ambiguous calls were eventually routed.The routing decision made for each call is classifiedinto one of 8 groups, as shown in Figure 3.
For instance,4Our current system uses imple template filling for response gener-ation by utilizing a manually constructed mappings from n-gram termsto their inflected forms, such as from "exist, car, loan" to "an existingcar loan ".5The calls in the test set were recorded separately from our trainingcorpus.
In this paper, we focus on evaluation based on transcriptions ofthe calls.
A companion paper compares call performance on transcrip-tions to the output of a speech recognizer (Carpenter and Chu-Carroll,submitted).260Is request actually unambiguous?Is call routed by router?
Is call routed by router?
ye<correct?
contains correct?
one of possible?
overlaps with possible?la lb 2a 2b 3a 3b 4a 4bFigure 3: Classification of Router OutcomeUnambiguous Ambiguous All \]Requests Requests I Requests I LB la/(i+2) 4a/(3+4) | (1 a+4a)/allUBTable 3: Calculation of Upperbounds and Lowerboundsgroup la  contains those calls which are 1) actually unam-biguous, 2) considered unambiguous by the router, and3) routed to the correct destination.
On the other hand,group 3b contains those calls which are 1) actually am-biguous, 2) considered by the router to be unambiguous,and 3) routed to a destination which is not one of thepotential destinations.We evaluated the router's performance on three sub-sets of our test data, unambiguous requests alone, am-biguous requests alone, and all requests combined.
Foreach set of data, we calculated a lowerbound perfor-mance, which measures the percentage of calls that arecorrectly routed, and an upperbound performance, whichmeasures the percentage of calls that are either correctlyrouted or have the potential to be correctly routed.
Ta-ble 3 shows how the upperbounds and lowerbounds arecomputed based on the classification i  Figure 3 for eachof the three data sets.
For instance, for unambiguous re-quests (classes 1 and 2), the lowerbound is the numberof calls actually routed to the correct destination (la)divided by the number of total unambiguous requests,while the upperbound is the number of calls actuallyrouted to the correct destination (1 a) plus the number ofcalls which the router finds to be ambiguous between thecorrect destination and some other destination(s) (2a), di-vided by the number of unambiguous queries.
The callsin category 2a are considered to be potentially correct be-cause it is likely that the call will be routed to the correctdestination after disambiguation.Table 4 shows the upperbound and lowerbound perfor-mance for each of the three test sets.
These results showUnambiguous Ambiguous AllRequests Requests RequestsLB 80.1% 58.5% 75.6%UB 96.7% 98.8% 97.2%Table 4: Router Performance with Threshold = 0.2that the system's overall performance will fall some-where between 75.6% and 97.2%.
The actual perfor-mance of the system is determined by two factors: 1) theperformance of the disambiguation module, which de-termines the correct routing rate of the 16.6% of the un-ambiguous calls that were considered ambiguous by therouter (class 2a), and 2) the percentage of calls that wererouted correctly out of the 40.4% ambiguous calls thatwere considered unambiguous and routed by the router(class 3a).
Note that the performance figures in Table 4are the result of 100% automatic routing, since no re-quest in our test set failed to evoke at least one candidatedestination.
In the next sections, we discuss the perfor-mance of the disambiguation module, which determinesthe overall system performance, and show how allowingcalls to be punted to operators affects the system's per-formance.5.2 The Disambiguation ModuleTo evaluate our disambiguation module, we needed ia-logues which satisfy two criteria: 1) the caller's first ut-terance is ambiguous, and 2) the operator asked a follow-up question to disambiguate he query and subsequentlyrouted the call to the appropriate destination.
We used157 calls that meet hese two criteria as our test set.
Notethat this test set is disjoint from the test set used in theevaluation of the router (Section 5.
I), since none of thetranscribed calls in the latter test set satisfied criterion(2).For each ambiguous call, the first user utterance wasgiven to the router as input.
The outcome of the routerwas classified as follows:1.
Unambiguous: in this case the call was routed to theselected estination.
This routing was consideredcorrect if the selected estination was the same asthe actual destination and incorrect otherwise.2.
Ambiguous: in this case the router attempted to ini-tiate disambiguation.
The outcome of the routing ofthese calls were determined as follows:(a) Correct, if a disambiguation query was gener-ated which, when answered, led to the correctdestination.
(b) Incorrect, if a disambiguation query was gen-erated which, when answered, could not leadto a correct destination.
(c) Reject, if the router could not form a sensi-ble query or was unable to gather sufficient in-formation from the user after its queries androuted the call to an operator.Table 5 shows the number of calls that fall into eachof the 5 categories.
Out of the 157 calls, the router au-tomatically routed 115 of them either with or withoutdisambiguation (73.2%).
Furthermore, 87.0% of theserouted calls were routed to the correct destination.
No-tice that out of the 52 ambiguous calls that the router con-sidered unambiguous, 40 were routed correctly (76.9%).261Routed As Unambiguous Routed As Ambiguousc?
ct 140  lnco ct l 2 C?
c' I X?c??
?t I  eje?t 60 42Table 5: Performance of Disambiguation Module onAmbiguous CallsCorrect Incorrect RejectClass 1 63.2% 1.3% 0%Class 2 7.5% 1.7% 5.3%Class 3 6.5% 2.2% 0%Class 4 7.0% 0.4% 4.9%Total 84.2% 5.6% 10.2%Table 6: Overall Performance ofCall RouterThis is simply because our vector-based router is able todistinguish between cases where an ambiguous query isequally likely to be routed to more than one destination,and situations where the likelihood of one potential desti-nation overwhelms that of the other(s).
In the latter case,the router outes the call to the most likely destination i -stead of initiating disambiguation, which has been shownto be an effective strategy; not surprisingly, human op-erators are also prone to guess the destination based onlikelihood and route callers without disambiguation.5.3 Overall PerformanceCombining results from Section 5.2 for ambiguous callswith results from Section 5.1 for unambiguous calls leadsto the overall performance of the call router in Table 6.The table shows the number of calls that will be correctlyrouted, incorrectly routed, and rejected, if we apply theperformance of the disambiguation module (Table 5) tothe calls that fall into each class in the evaluation ofthe routing module (Section 5.1).
Our results show thatout of the 389 calls in our test set, 89.8% of the callswill be automatically routed by the call router.
Of thesecalls, 93.8% (which constitutes 84.2% of all calls) willbe routed to their correct destinations.
This is substan-tially better than the results obtained by Gorin et al, whoreport an 84% correct routing rate with a 10% false rejec-tion rate (routed to an operator when the call could havebeen automatically routed) on 14 destinations (Gorin etal., to appear).
66 ConclusionsWe described and evaluated a domain independent, au-tomatically trained call router that takes one of three ac-tions in response to a caller's request.
It can route thecall to a destination within the call center, attempt to6Gorin et al's results are measured without he possibility of systemqueries.
To provide afair comparison, we evaluated our muting moduleon all 389 calls in our test set using the scoring method escribed in(Gorin et al, to appear) (which corresponds roughly to our upperboundmeasure), and achieved a94.
!
% correct routing rate to 23 destinationswhen all calls are automatically routed (no false rejection), asubstantialimprovement over their system.formulate a disambiguating query, or route the call to ahuman operator.
The routing module of the call routerselects a set of candidate destinations based on n-gramterms extracted from the caller request and a vector-based comparison between these n-gram terms and eachpossible destination.
If disambiguation is necessary, ayes-no question or wh-question is dynamically generatedfrom among known n-gram terms in the domain basedon closeness, relevance, and disambiguating power, thustailoring the disambiguating query to the original requestand the candidate destinations.
Finally, our system per-forms substantially better than the best previously exist-ing system, achieving an overall 93.8% correct routingrate for automatically routed calls when rejecting 10.2%of all calls.AcknowledgmentsWe would like to thank Christer Samuelsson and Jim Hi-eronymus for helpful discussions, and Diane Litman forcomments on an earlier draft of this paper.ReferencesA.
Abella and A. Gorin.
1997.
Generating semanticallyconsistent inputs to a dialog manager.
In Proc.
EU-ROSPEECH, pages 1879-1882.B.
Carpenter and J. Chu-Carroll.
submitted.
Naturallanguage call routing: A robust, self-organizing ap-proach.S.
Deerwester, S. Dumais, G. Furnas, T. Landauer, andR.
Harshman.
1990.
Indexing by latent semantic anal-ysis.
Journal of the American Society for InformationScience, 41:391-407.A.
Gorin, G. Riccardi, and J. Wright.
to appear.
Howmay I help you?
Speech Communication.N.
Green and S. Carberry.
1994.
A hybrid reasoningmodel for indirect answers.
In Proc.
ACL, pages 58-65.D.
Harman.
1995.
Overview of the fourth Text REtrievalConference.
In Proc.
TREC.B.
Hockey, D. Rossen-Knill, B. Spejewski, M. Stone,and S. Isard.
1997.
Can you predict responses toyes/no questions?
yes, no, and stuff.
In Proc.
EU-ROSPEECH, pages 2267-2270.J.
McDonough, K. Ng, P. Jeanrenaud, H. Gish, and J. R.Rohlicek.
1994.
Approaches to topic identification onthe switchboard corpus.
In Proc.
ICASSP, pages 385-388.G.
Salton.
1971.
The SMART Retrieval System.
PrenticeHall.H.
Schtitze, D. Hull, and J. Pedersen.
1995.
A compari-son of classifiers and document representations fortherouting problem.
In Proc.
SIGIR.K.
Sparck Jones.
1972.
A statistical interpretation ofterm specificity and its application in retrieval.
Jour-nal of Documentation, 28:11-20.R.
Sproat, editor.
1997.
Multilingual Text-to-SpeechSynthesis: The Bell Labs Approach.
Kluwer.262
