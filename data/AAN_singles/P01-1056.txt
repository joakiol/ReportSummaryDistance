Evaluating a Trainable Sentence Planner for a Spoken Dialogue SystemOwen RambowAT&T Labs ?
ResearchFlorham Park, NJ, USArambow@research.att.comMonica RogatiCarnegie Mellon UniversityPittsburgh, PA, USAmrogati+@cs.cmu.eduMarilyn A. WalkerAT&T Labs ?
ResearchFlorham Park, NJ, USAwalker@research.att.comAbstractTechniques for automatically trainingmodules of a natural language gener-ator have recently been proposed, buta fundamental concern is whether thequality of utterances produced withtrainable components can compete withhand-crafted template-based or rule-based approaches.
In this paper We ex-perimentally evaluate a trainable sen-tence planner for a spoken dialogue sys-tem by eliciting subjective human judg-ments.
In order to perform an ex-haustive comparison, we also evaluatea hand-crafted template-based genera-tion component, two rule-based sen-tence planners, and two baseline sen-tence planners.
We show that the train-able sentence planner performs betterthan the rule-based systems and thebaselines, and as well as the hand-crafted system.1 IntroductionThe past several years have seen a large increasein commercial dialog systems.
These systemstypically use system-initiative dialog strategies,with system utterances highly scripted for styleand register and recorded by voice talent.
How-ever several factors argue against the continueduse of these simple techniques for producing thesystem side of the conversation.
First, text-to-speech has improved to the point of being a vi-able alternative to pre-recorded prompts.
Second,there is a perceived need for spoken dialog sys-tems to be more flexible and support user initia-tive, but this requires greater flexibility in utter-ance generation.
Finally, systems to support com-plex planning are being developed, which will re-quire more sophisticated output.As we move away from systems with pre-recorded prompts, there are two possible ap-proaches to producing system utterances.
Thefirst is template-based generation, where ut-terances are produced from hand-crafted stringtemplates.
Most current research systems usetemplate-based generation because it is concep-tually straightforward.
However, while little orno linguistic training is needed to write templates,it is a tedious and time-consuming task: one ormore templates must be written for each combi-nation of goals and discourse contexts, and lin-guistic issues such as subject-verb agreement anddeterminer-noun agreement must be repeatedlyencoded for each template.
Furthermore, main-tenance of the collection of templates becomes asoftware engineering problem as the complexityof the dialog system increases.1The second approach is natural language gen-eration (NLG), which customarily divides thegeneration process into three modules (Rambowand Korelsky, 1992): (1) Text Planning, (2) Sen-tence Planning, and (3) Surface Realization.
Inthis paper, we discuss only sentence planning; therole of the sentence planner is to choose abstractlexico-structural resources for a text plan, wherea text plan encodes the communicative goals foran utterance (and, sometimes, their rhetoricalstructure).
In general, NLG promises portabilityacross application domains and dialog situationsby focusing on the development of rules for eachgeneration module that are general and domain-1Although we are not aware of any software engineeringstudies of template development and maintenance, this claimis supported by abundant anecdotal evidence.independent.
However, the quality of the outputfor a particular domain, or a particular situationin a dialog, may be inferior to that of a template-based system without considerable investment indomain-specific rules or domain-tuning of gen-eral rules.
Furthermore, since rule-based systemsuse sophisticated linguistic representations, thishandcrafting requires linguistic knowledge.Recently, several approaches for automaticallytraining modules of an NLG system have beenproposed (Langkilde and Knight, 1998; Mel-lish et al, 1998; Walker, 2000).
These holdthe promise that the complex step of customiz-ing NLG systems by hand can be automated,while avoiding the need for tedious hand-craftingof templates.
While the engineering benefits oftrainable approaches appear obvious, it is unclearwhether the utterance quality is high enough.In (Walker et al, 2001) we propose a newmodel of sentence planning called SPOT.
InSPOT, the sentence planner is automaticallytrained, using feedback from two human judges,to choose the best from among different optionsfor realizing a set of communicative goals.
In(Walker et al, 2001), we evaluate the perfor-mance of the learning component of SPOT, andshow that SPOT learns to select sentence plansthat are highly rated by the two human judges.While this evaluation shows that SPOT has in-deed learned from the human judges, it does notshow that using only two human judgments issufficient to produce more broadly acceptable re-sults, nor does it show that SPOT performs as wellas optimized hand-crafted template or rule-basedsystems.
In this paper we address these questions.Because SPOT is trained on data from a work-ing system, we can directly compare SPOT to thehand-crafted, template-based generation compo-nent of the current system.
In order to perform anexhaustive comparison, we also implemented tworule-based and two baseline sentence-planners.One baseline simply produces a single sentencefor each communicative goal.
Another baselinerandomly makes decisions about how to combinecommunicative goals into sentences.
We directlycompare these different approaches in an evalua-tion experiment in which 60 human subjects rateeach system?s output on a scale of 1 to 5.The experimental design is described in sectionSystem1: Welcome.... What airport would you like to flyout of?User2: I need to go to Dallas.System3: Flying to Dallas.
What departure airport wasthat?User4: from Newark on September the 1st.System5: Flying from Newark to Dallas, Leaving on the1st of September, And what time did you wantto leave?Figure 1: A dialog with AMELIA2.
The sentence planners used in the evaluationare described in section 3.
In section 4, we presentour results.
We show that the trainable sentenceplanner performs better than both rule-based sys-tems and as well as the hand-crafted template-based system.
These four systems outperform thebaseline sentence planners.
Section 5 summarizesour results and discusses related and future work.2 Experimental Context and DesignOur research concerns developing and evaluat-ing a portable generation component for a mixed-initiative travel planning system, AMELIA, de-veloped at AT&T Labs as part of DARPA Com-municator.
Consider the required generation ca-pabilities of AMELIA, as illustrated in Figure 1.Utterance System1 requests information aboutthe caller?s departure airport, but in User2, thecaller takes the initiative to provide informationabout her destination.
In System3, the system?sgoal is to implicitly confirm the destination (be-cause of the possibility of error in the speechrecognition component), and request information(for the second time) of the caller?s departure air-port.
This combination of communicative goalsarises dynamically in the dialog because the sys-tem supports user initiative, and requires differ-ent capabilities for generation than if the systemcould only understand the direct answer to thequestion that it asked in System1.In User4, the caller provides this informationbut takes the initiative to provide the month andday of travel.
Given the system?s dialog strategy,the communicative goals for its next turn are toimplicitly confirm all the information that the userhas provided so far, i.e.
the departure and desti-nation cities and the month and day information,as well as to request information about the timeof travel.
The system?s representation of its com-municative goals for System5 is in Figure 2.
Asbefore, this combination of communicative goalsarises in response to the user?s initiative.implicit-confirm(orig-city:NEWARK)implicit-confirm(dest-city:DALLAS)implicit-confirm(month:9)implicit-confirm(day-number:1)request(depart-time:whatever)Figure 2: The text plan (communicative goals) forSystem5 in Figure 1Like most working research spoken dialogsystems, AMELIA uses hand-crafted, template-based generation.
Its output is created by choos-ing string templates for each elementary speechact, using a large choice function which dependson the type of speech act and various context con-ditions.
Values of template variables (such as ori-gin and destination cities) are instantiated by thedialog manager.
The string templates for all thespeech acts of a turn are heuristically ordered andthen appended to produce the output.
In order toproduce output that is not highly redundant, stringtemplates must be written for every possible com-bination of speech acts in a text plan.
We refer tothe output generated by AMELIA using this ap-proach as the TEMPLATE output.System RealizationTEMPLATE Flying from Newark to Dallas, Leaving onthe 1st of September, And what time didyou want to leave?SPoT What time would you like to travel onSeptember the 1st to Dallas from Newark?RBS (Rule-Based)What time would you like to travel onSeptember the 1st to Dallas from Newark?ICF (Rule-Based)What time would you like to fly onSeptember the 1st to Dallas from Newark?RANDOM Leaving in September.
Leaving on the1st.
What time would you, traveling fromNewark to Dallas, like to leave?NOAGG Leaving on the 1.
Leaving in September.Going to Dallas.
Leaving from Newark.What time would you like to leave?Figure 3: Sample outputs for System5 of Figure 1for each type of generation system used in theevaluation experiment.We perform an evaluation using human sub-jects who judged the TEMPLATE output ofAMELIA against five NLG-based approaches:SPOT, two rule-based approaches, and two base-lines.
We describe them in Section 3.
An exam-ple output for the text plan in Figure 2 for eachsystem is in Figure 3.
The experiment requiredhuman subjects to read 5 dialogs of real inter-actions with AMELIA.
At 20 points over the 5dialogs, AMELIA?s actual utterance (TEMPLATE)is augmented with a set of variants; each set ofvariants included a representative generated bySPOT, and representatives of the four compari-son sentence planners.
At times two or more ofthese variants coincided, in which case sentenceswere not repeated and fewer than six sentenceswere presented to the subjects.
The subjects ratedeach variation on a 5-point Likert scale, by statingthe degree to which they agreed with the state-ment The system?s utterance is easy to under-stand, well-formed, and appropriate to the dialogcontext.
Sixty colleagues not involved in this re-search completed the experiment.3 Sentence Planning SystemsThis section describes the five sentence plannersthat we compare.
SPOT, the two rule-basedsystems, and the two baseline sentence plannersare all NLG based sentence planners.
In Sec-tion 3.1, we describe the shared representationsof the NLG based sentence planners.
Section 3.2describes the baselines, RANDOM and NOAGG.Section 3.3 describes SPOT.
Section 3.4 de-scribes the rule-based sentence planners, RBSand ICF.3.1 Aggregation in Sentence PlanningIn all of the NLG sentence planners, each speechact is assigned a canonical lexico-structural rep-resentation (called a DSyntS ?
Deep SyntacticStructure (Melc?uk, 1988)).
We exclude issues oflexical choice from this study, and restrict our at-tention to the question of how elementary struc-tures for separate elementary speech acts are as-sembled into extended discourse.
The basis of allthe NLG systems is a set of clause-combining op-erations that incrementally transform a list of el-ementary predicate-argument representations (theDSyntSs corresponding to the elementary speechacts of a single text plan) into a list of lexico-structural representations of one or more sen-tences, that are sent to a surface realizer.
We uti-lize the RealPro Surface realizer with all of theRule Arg 1 Arg 2 ResultMERGE You are leaving from Newark.
You are leaving at 5 You are leaving at 5 from NewarkSOFT-MERGE You are leaving from Newark You are going to Dallas You are traveling from Newark toDallasCONJUNCTION You are leaving from Newark.
You are going to Dallas.
You are leaving from Newark andyou are going to Dallas.RELATIVE-CLAUSEYour flight leaves at 5.
Your flight arrives at 9.
Your flight, which leaves at 5, ar-rives at 9.ADJECTIVE Your flight leaves at 5.
Your flight is nonstop.
Your nonstop flight leaves at 5.PERIOD You are leaving from Newark.
You are going to Dallas.
You are leaving from Newark.You are going to DallasRANDOM CUE-WORDWhat time would yo like toleave?n/a Now, what time would you like toleave?Figure 4: List of clause combining operations with examplessentence planners (Lavoie and Rambow, 1997).DSyntSs are combined using the operations ex-emplified in Figure 4.
The result of applying theoperations is a sentence plan tree (or sp-treefor short), which is a binary tree with leaves la-beled by all the elementary speech acts from theinput text plan, and with its interior nodes la-beled with clause-combining operations.
As anexample, Figure 5 shows the sp-tree for utteranceSystem5 in Figure 1.
Node soft-merge-general  merges an implicit-confirmation of the destina-tion city and the origin city.
The row labelledSOFT-MERGE in Figure 4 shows the result whenArgs 1 and 2 are implicit confirmations of the ori-gin and destination.
See (Walker et al, 2001) formore detail on the sp-tree.
The experimental sen-tence planners described below vary how the sp-tree is constructed.213imp?confirm(month) request(time) soft?merge?generalimp?confirm(dest?city) imp?confirm(orig?city)imp?confirm(day)soft?mergesoft?merge?generalsoft?merge?generalFigure 5: A Sentence Plan Tree for Utterance Sys-tem 5 in Dialog D13.2 Baseline Sentence PlannersIn one obvious baseline system the sp-tree is con-structed by applying only the PERIOD operation:each elementary speech act is realized as its ownsentence.
This baseline, NOAGG, was suggestedby Hovy and Wanner (1996).
For NOAGG, weorder the communicative acts from the text planas follows: implicit confirms precede explicitconfirms precede requests.
Figure 3 includes aNOAGG output for the text plan in Figure 2.A second possible baseline sentence plannersimply applies combination rules randomly ac-cording to a hand-crafted probability distributionbased on preferences for operations such as theMERGE family over CONJUNCTION and PERIOD.In order to be able to generate the resulting sen-tence plan tree, we exclude certain combinations,such as generating anything other than a PERIODabove a node labeled PERIOD in a sentence plan.The resulting sentence planner we refer to asRANDOM.
Figure 3 includes a RANDOM outputfor the text plan in Figure 2.In order to construct a more complex, andhopefully better, sentence planner, we need to en-code constraints on the application of, and order-ing of, the operations.
It is here that the remainingapproaches differ.
In the first approach, SPOT,we learn constraints from training material; in thesecond approach, rule-based, we construct con-straints by hand.3.3 SPoT: A Trainable Sentence PlannerFor the sentence planner SPOT, we reconceptu-alize sentence planning as consisting of two dis-tinct phases as in Figure 6.
In the first phase, thesentence-plan-generator (SPG) randomly gener-ates up to twenty possible sentence plans for agiven text-plan input.
For this phase we use theRANDOM sentence-planner.
In the second phase,the sentence-plan-ranker (SPR) ranks the sample----SPR..Sentence PlannerSPGHRealProRealizerText Plan Chosen sp?tree with associated DSyntS-HSp?trees with associated DSyntSsaDialogSystem .Figure 6: Architecture of SPoTsentence plans, and then selects the top-rankedoutput to input to the surface realizer.
The SPRis automatically trained by applying RankBoost(Freund et al, 1998) to learn ranking rules fromtraining data.
The training data was assembledby using RANDOM to randomly generate up to20 realizations for 100 turns; two human judgesthen ranked each of these realizations (using thesetup described in Section 2).
Over 3,000 fea-tures were discovered from the generated treesby routines that encode structural and lexical as-pects of the sp-trees and the DSyntS.
RankBoostidentified the features that contribute most to arealization?s ranking.
The SPR uses these rulesto rank alternative sp-trees, and then selects thetop-ranked output as input to the surface realizer.Walker et al (2001) describe SPOT in detail.3.4 Two Rule-Based Sentence PlannersIt has not been the object of our research to con-struct a rule-based sentence planner by hand, beit domain-independent or optimized for our do-main.
Our goal was to compare the SPOT sen-tence planner with a representative rule-basedsystem.
We decided against using an existing off-the-shelf rule-based system, since it would be toocomplex a task to port it to our application.
In-stead, we constructed two reasonably representa-tive rule-based sentence planners.
This task wasmade easier by the fact that we could reuse muchof the work done for SPOT, in particular the datastructure of the sp-tree and the implementation ofthe clause-combining operations.
We developedthe two systems by applying heuristics for pro-ducing good output, such as preferences for ag-gregation.
They differ only in the initial orderingof the communicative acts in the input text plan.In the first rule-based system, RBS (for ?Rule-Based System?
), we order the speech acts withexplicit confirms first, then requests, then implicitconfirms.
Note that explicit confirms and requestsdo not co-occur in our data set.
The second rule-based system is identical, except that implicit con-firms come first rather than last.
This system wecall ICF (for ?Rule-based System with ImplicitConfirms First?
).In the initial step of both RBS and ICF,we take the two leftmost members of the textplan and try to combine them using the follow-ing preference ranking of the combination op-erations: ADJECTIVE, the MERGEs, CONJUNC-TION, RELATIVE-CLAUSE, PERIOD.
The firstoperation to succeed is chosen.
This yields a bi-nary sp-tree with three nodes, which becomes thecurrent sp-tree.
As long as the root node ofthe current sp-tree is not a PERIOD, we iteratethrough the list of remaining speech acts on theordered text plan, combining each one with thecurrent sp-tree using the preference-ranked opera-tions as just described.
The result of each iterationstep is a binary, left-branching sp-tree.
However,if the root node of the current sp-tree is a PERIOD,we start a new current sp-tree, as in the initial stepdescribed above.
When the text plan has been ex-hausted, all partial sp-trees (all of which exceptfor the last one are rooted in PERIOD) are com-bined in a left-branching tree using PERIOD.
Cuewords are added as follows: (1) The cue wordnow is attached to utterances beginning a newsubtask; (2) The cue word and is attached to ut-terances continuing a subtask; (3) The cue wordsalright or okay are attached to utterances contain-ing implicit confirmations.
Figure 3 includes anRBS and an ICF output for the text plan in Fig-ure 2.
In this case ICF and RBS differ only inthe verb chosen as a more general verb during theSOFT-MERGE operation.We illustrate the RBS procedure with an ex-ample for which ICF works similarly.
For RBS,the text plan in Figure 2 is ordered so that the re-quest is first.
For the request, a DSyntS is cho-sen that can be paraphrased as What time wouldyou like to leave?.
Then, the first implicit-confirmis translated by lookup into a DSyntS which onits own could generate Leaving in September.We first try the ADJECTIVE aggregation opera-tion, but since neither tree is a predicative ad-jective, this fails.
We then try the MERGE fam-ily.
MERGE-GENERAL succeeds, since the treefor the request has an embedded node labeledleave.
The resulting DSyntS can be paraphrasedas What time would you like to leave in Septem-ber?, and is attached to the new root node ofthe resulting sp-tree.
The root node is labeledMERGE-GENERAL, and its two daughters are thetwo speech acts.
The implicit-confirm of theday is added in a similar manner (adding an-other left-branching node to the sp-tree), yieldinga DSyntS that can be paraphrased as What timewould you like to leave on September the 1st?
(us-ing some special-case attachment for dates withinMERGE).
We now try and add the DSyntS forthe implicit-confirm, whose DSyntS might gener-ate Going to Dallas.
Here, we again cannot useADJECTIVE, nor can we use MERGE or MERGE-GENERAL, since the verbs are not identical.
In-stead, we use SOFT-MERGE-GENERAL, whichidentifies the leave node with the go root node ofthe DSyntS of the implicit-confirm.
When soft-merging leave with go, fly is chosen as a general-ization, resulting in a DSyntS that can be gener-ated as What time would you like to fly on Septem-ber the 1st to Dallas?.
The sp-tree has added alayer but is still left-branching.
Finally, the lastimplicit-confirm is added to yield a DSyntS thatis realized as What time would you like to fly onSeptember the 1st to Dallas from Newark?.4 Experimental ResultsAll 60 subjects completed the experiment in a halfhour or less.
The experiment resulted in a totalof 1200 judgements for each of the systems be-ing compared, since each subject judged 20 ut-terances by each system.
We first discuss overalldifferences among the different systems and thenmake comparisons among the four different typesof systems: (1) TEMPLATE, (2) SPOT, (3) tworule-based systems, and (4) two baseline systems.All statistically significant results discussed herehad p values of less than .01.We first examined whether differences in hu-man ratings (score) were predictable from theSystem Min Max Mean S.D.TEMPLATE 1 5 3.9 1.1SPoT 1 5 3.9 1.3RBS 1 5 3.4 1.4ICF 1 5 3.5 1.4No Aggregation 1 5 3.0 1.2Random 1 5 2.7 1.4Figure 7: Summary of Overall Results for all Sys-tems Evaluatedtype of system that produced the utterance be-ing rated.
A one-way ANOVA with system as theindependent variable and score as the dependentvariable showed that there were significant differ-ences in score as a function of system.
The overalldifferences are summarized in Figure 7.As Figure 7 indicates, some system outputs re-ceived more consistent scores than others, e.g.the standard deviation for TEMPLATE was muchsmaller than RANDOM.
The ranking of the sys-tems by average score is TEMPLATE, SPOT, ICF,RBS, NOAGG, and RANDOM.
Posthoc compar-isons of the scores of individual pairs of systemsusing the adjusted Bonferroni statistic revealedseveral different groupings.2The highest ranking systems were TEMPLATEand SPOT, whose ratings were not statisticallysignificantly different from one another.
Thisshows that it is possible to match the quality of ahand-crafted system with a trainable one, whichshould be more portable, more general and re-quire less overall engineering effort.The next group of systems were the two rule-based systems, ICF and RBS, which were notstatistically different from one another.
HoweverSPOT was statistically better than both of thesesystems (p   .01).
Figure 8 shows that SPOTgot more high rankings than either of the rule-based systems.
In a sense this may not be thatsurprising, because as Hovy and Wanner (1996)point out, it is difficult to construct a rule-basedsentence planner that handles all the rule interac-tions in a reasonable way.
Features that SPoT?sSPR uses allow SPOT to be sensitive to particulardiscourse configurations or lexical collocations.In order to encode these in a rule-based sentence2The adjusted Bonferroni statistic guards against acci-dentally finding differences between systems when makingmultiple comparisons among systems.planner, one would first have to discover theseconstraints and then determine a way of enforc-ing them.
However the SPR simply learns thata particular configuration is less preferred, result-ing in a small decrement in ranking for the cor-responding sp-tree.
This flexibility of increment-ing or decrementing a particular sp-tree by a smallamount may in the end allow it to be more sensi-tive to small distinctions than a rule-based system.Along with the TEMPLATE and RULE-BASEDsystems, SPOT also scored better than the base-line systems NOAGG and RANDOM.
This is alsosomewhat to be expected, since the baseline sys-tems were intended to be the simplest systemsconstructable.
However it would have been a pos-sible outcome for SPOT to not be different thaneither system, e.g.
if the sp-trees produced byRANDOM were all equally good, or if the ag-gregation rules that SPOT learned produced out-put less readable than NOAGG.
Figure 8 showsthat the distributions of scores for SPOT vs. thebaseline systems are very different, with SPOTskewed towards higher scores.Interestingly NOAGG also scored better thanRANDOM (p   .01), and the standard deviationof its scores was smaller (see Figure 7).
Remem-ber that RANDOM?s sp-trees often resulted in ar-bitrarily ordering the speech acts in the output.While NOAGG produced redundant utterances, itplaced the initiative taking speech act at the end ofthe utterance in its most natural position, possiblyresulting in a preference for NOAGG over RAN-DOM.
Another reason to prefer NOAGG could beits predictability.5 Discussion and Future WorkOther work has also explored automatically train-ing modules of a generator (Langkilde andKnight, 1998; Mellish et al, 1998; Walker, 2000).However, to our knowledge, this is the first re-ported experimental comparison of a trainabletechnique that shows that the quality of systemutterances produced with trainable componentscan compete with hand-crafted or rule-based tech-niques.
The results validate our methodology;SPOT outperforms two representative rule-basedsentence planners, and performs as well as thehand-crafted TEMPLATE system, but is more eas-ily and quickly tuned to a new domain: the train-ing materials for the SPOT sentence planner canbe collected from subjective judgements from asmall number of judges with little or no linguisticknowledge.Previous work on evaluation of natural lan-guage generation has utilized three different ap-proaches to evaluation (Mellish and Dale, 1998).The first approach is a subjective evaluationmethodology such as we use here, where humansubjects rate NLG outputs produced by differentsources (Lester and Porter, 1997).
Other work hasevaluated template-based spoken dialog genera-tion with a task-based approach, i.e.
the genera-tor is evaluated with a metric such as task com-pletion or user satisfaction after dialog comple-tion (Walker, 2000).
This approach can workwell when the task only involves one or two ex-changes, when the choices have large effects overthe whole dialog, or the choices vary the con-tent of the utterance.
Because sentence plan-ning choices realize the same content and onlyaffect the current utterance, we believed it impor-tant to get local feedback.
A final approach fo-cuses on subproblems of natural language gener-ation such as the generation of referring expres-sions.
For this type of problem it is possible toevaluate the generator by the degree to which itmatches human performance (Yeh and Mellish,1997).
When evaluating sentence planning, thisapproach doesn?t make sense because many dif-ferent realizations may be equally good.However, this experiment did not show thattrainable sentence planners produce, in general,better-quality output than template-based or rule-based sentence planners.
That would be im-possible: given the nature of template and rule-based systems, any quality standard for the outputcan be met given sufficient person-hours, elapsedtime, and software engineering acumen.
Our prin-cipal goal, rather, is to show that the quality of theTEMPLATE output, for a currently operational dia-log system whose template-based output compo-nent was developed, expanded, and refined overabout 18 months, can be achieved using a train-able system, for which the necessary training datawas collected in three person-days.
Furthermore,we wished to show that a representative rule-based system based on current literature, withoutmassive domain-tuning, cannot achieve the same1 1.5 2 2.5 3 3.5 4 4.5 5020040060080010001200ScoreNumberofplanswiththatscoreormoreAMELIASPOTICRBNOAGGRANFigure 8: Chart comparing distribution of human ratings for SPOT, RBS, ICF, NOAGG and RANDOM.level of quality.
In future work, we hope to extendSPoT and integrate it into AMELIA.6 AcknowledmentsThis work was partially funded by DARPA undercontract MDA972-99-3-0003.ReferencesY.
Freund, R. Iyer, R. E. Schapire, and Y.Singer.
1998.An efficient boosting algorithm for combining pref-erences.
In Machine Learning: Proc.
of the Fif-teenth International Conference.E.H.
Hovy and L. Wanner.
1996.
Managing sentenceplanning requirements.
In Proc.
of the ECAI?96Workshop Gaps and Bridges: New Directions inPlanning and Natural Language Generation.I.
Langkilde and K. Knight.
1998.
Generation that ex-ploits corpus-based statistical knowledge.
In Proc.of COLING-ACL.Benoit Lavoie and Owen Rambow.
1997.
A fast andportable realizer for text generation systems.
InProc.
of the Third Conference on Applied NaturalLanguage Processing, ANLP97, pages 265?268.J.
Lester and B. Porter.
1997.
Developing and em-pirically evaluating robust explanation generators:The knight experiments.
Computational Linguis-tics, 23-1:65?103.C.
Mellish and R. Dale.
1998.
Evaluation in thecontext of natural language generation.
ComputerSpeech and Language, 12(3).C.
Mellish, A. Knott, J. Oberlander, and M.O?Donnell.
1998.
Experiments using stochas-tic search for text planning.
In Proc.
of Interna-tional Conference on Natural Language Genera-tion, pages 97?108.I.
A. Melc?uk.
1988.
Dependency Syntax: Theory andPractice.
SUNY, Albany, New York.O.
Rambow and T. Korelsky.
1992.
Applied textgeneration.
In Proc.
of the Third Conference onApplied Natural Language Processing, ANLP92,pages 40?47.M.
Walker, O. Rambow, and M. Rogati.
2001.
Spot:A trainable sentence planner.
In Proc.
of the NorthAmerican Meeting of the Association for Computa-tional Linguistics.M.
A. Walker.
2000.
An application of reinforcementlearning to dialogue strategy selection in a spokendialogue system for email.
Journal of Artificial In-telligence Research, 12:387?416.C.L.
Yeh and C. Mellish.
1997.
An empirical studyon the generation of anaphora in chinese.
Compu-tational Linguistics, 23-1:169?190.
