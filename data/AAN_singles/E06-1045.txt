Data-driven Generation of Emphatic Facial DisplaysMary Ellen FosterDepartment of Informatics, Technical University of MunichBoltzmannstra?e 3, 85748 Garching, Germanyfoster@in.tum.deJon OberlanderInstitute for Communicating and Collaborative SystemsSchool of Informatics, University of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LW, United Kingdomjon@inf.ed.ac.ukAbstractWe describe an implementation of data-driven selection of emphatic facial dis-plays for an embodied conversationalagent in a dialogue system.
A corpus ofsentences in the domain of the target dia-logue system was recorded, and the facialdisplays used by the speaker were anno-tated.
The data from those recordings wasused in a range of models for generatingfacial displays, each model making use ofa different amount of context or choosingdisplays differently within a context.
Themodels were evaluated in two ways: bycross-validation against the corpus, and byasking users to rate the output.
The predic-tions of the cross-validation study differedfrom the actual user ratings.
While thecross-validation gave the highest scores tomodels making a majority choice within acontext, the user study showed a signifi-cant preference for models that producedmore variation.
This preference was espe-cially strong among the female subjects.1 IntroductionIt has long been documented that there are char-acteristic facial displays that accompany the em-phasised parts of spoken utterances.
For example,Ekman (1979) says that eyebrow raises ?appear tocoincide with primary vocal stress, or more sim-ply with a word that is spoken more loudly.?
Cor-relations have also been found between prosodicfeatures and events such as head nodding and theamplitude of mouth movements.
When Krah-mer and Swerts (2004) performed an empirical,cross-linguistic evaluation of the influence of browmovements on the perception of prosodic stress,they found that subjects preferred eyebrow move-ments to be correlated with the most prominentword in an utterance and that eyebrow movementsboosted the perceived prominence of the wordthey were associated with.While many facial displays have been shownto co-occur with prosodic accents, the converseis not true: in normal embodied speech, manypitch accents and other prosodic events are unac-companied by any facial display, and when dis-plays are used, the selection varies widely.
Cas-sell and Tho?risson (1999) demonstrated that ?en-velope?
facial displays related to the process ofconversation have a greater impact on successfulinteraction with an embodied conversational agentthan do emotional displays.
However, no descrip-tion of face motion is sufficiently detailed that itcan be used as the basis for selecting emphatic fa-cial displays for an agent.
This is therefore a taskfor which data-driven techniques are beneficial.In this paper, we address the task of selectingemphatic facial displays for the talking head inthe COMIC1 multimodal dialogue system.
In thebasic COMIC process for generating multimodaloutput (Foster et al, 2005), facial displays are se-lected using simple rules based only on the pitchaccents specified by the text generation system.
Inorder to make a more sophisticated and naturalis-tic selection of facial displays, we recorded a sin-gle speaker reading a set of sentences drawn fromthe COMIC domain, and annotated the facial dis-plays that he used and the contexts in which heused them.
We then created models based on thedata from this corpus and used them to choose thefacial displays for the COMIC talking head.1http://www.hcrc.ed.ac.uk/comic/353The rest of this paper is arranged as follows.First, in Section 2, we describe previous ap-proaches to selecting non-verbal behaviour forembodied conversational agents.
In Section 3, wethen show how we collected and annotated a cor-pus of facial displays, and give some generalisa-tions about the range of displays found in the cor-pus.
After that, in Section 4, we outline how weimplemented a range of models for selecting be-haviours for the COMIC agent using the corpusdata, using varying amounts of context and differ-ent selection strategies within a context.
Next, wegive the results of two evaluation studies compar-ing the quality of the output generated by the var-ious models: a cross-validation study against thecorpus (Section 5) and a direct user evaluation ofthe output (Section 6).
In Section 7, we discuss theresults of these two evaluations.
Finally, in Sec-tion 8, we draw some conclusions from the currentstudy and outline potential follow-up work.2 Choosing Non-Verbal Behaviour forEmbodied Conversational AgentsEmbodied Conversational Agents (ECAs) arecomputer interfaces that are represented as hu-man bodies, and that use their face and body ina human-like way in conversations with the user(Cassell et al, 2000).
The main benefit of ECAsis that they allow users to interact with a computerin the most natural possible setting: face-to-faceconversation.
However, to realise this advantagefully, the agent must produce high-quality output,both verbal and non-verbal.
A number of previoussystems have based the choice of non-verbal be-haviours for an ECA on the behaviours of humansin conversational situations.
The implementationsvary as to how directly they use the human data.In some systems, motion specifications for theagent are created from scratch, using rules derivedfrom studying human behaviour.
For the REAagent (Cassell et al, 2001a), for example, ges-turing behaviour was selected to perform particu-lar communicative functions, using rules based onstudies of typical North American non-verbal dis-plays.
Similarly, the Greta agent (de Carolis et al,2002) selected its performative facial displays us-ing hand-crafted rules to map from affective statesto facial motions.
Such implementations do notmake direct use of any recorded human motions;this means that they generate average behavioursfrom a range of people, but it is difficult to adaptthem to reproduce the behaviour of an individual.In contrast, other ECA implementations haveselected non-verbal behaviour based directly onmotion-capture recordings of humans.
Stone et al(2004), for example, recorded an actor performingscripted output in the domain of the target system.They then segmented the recordings into coher-ent phrases and annotated them with the relevantsemantic and pragmatic information, and com-bined the segments at run-time to produce com-plete performance specifications that were thenplayed back on the agent.
Cunningham et al(2004) and Shimodaira et al (2005) used similartechniques to base the appearance and motions oftheir talking heads directly on recordings of hu-man faces.
This technique is able to produce morenaturalistic output than the more rule-based sys-tems described above; however, capturing the mo-tion requires specialised hardware, and the agentmust be implemented in such a way that it can ex-actly reproduce the human motions.A middle ground is to use a purely syntheticagent?one whose behaviour is controlled byhigh-level instructions, rather than based directlyon human motions?but to create the instructionsfor that agent using the data from an annotated cor-pus of human behaviour.
Like a motion-captureimplementation, this technique can also produceincreased naturalism in the output and also al-lows choices to be based on the motions of a sin-gle performer if necessary.
However, annotatinga video corpus can be less technically demand-ing than capturing and directly re-using real mo-tions, especially when the corpus and the numberof features under consideration are small.
This ap-proach has been taken, for example, by Cassellet al (2001b) to choose posture shifts for REA,and by Kipp (2004) to select gestures for agents,and it is also the approach that we adopt here.3 Recording and AnnotationThe recording script for the data collection con-sisted of 444 sentences in the domain of theCOMIC multimodal dialogue system; all of thesentences described one or more features of one ormore bathroom-tile designs.
The sentences weregenerated by the full COMIC output planner, andwere selected to provide coverage of all of thesyntactic patterns available to the system.
In ad-dition to the surface text, each sentence includedall of the contextual information from the COMIC35446.
More about the current designthey dislike the first feature, but like the second oneThere are GEOMETRIC SHAPES on thedecorative tiles, but the tiles ARE from theARMONIE series.Figure 1: Sample prompt slideplanner: the predicted pitch accents?selected ac-cording to Steedman?s (2000) theory of informa-tion structure and intonation?along with any in-formation from the user model and dialogue his-tory.
The sentences were presented one at a timeto the speaker, who was instructed to read eachsentence out loud as expressively as possible whilelooking into a camera directed at his face.
The seg-ments for which the presentation planner specifiedpitch accents were highlighted, and any applicableuser-model and dialogue-history information wasincluded.
Figure 1 shows a sample prompt slide.The recorded videos were annotated by the firstauthor, using a purpose-built tool that allowed anyset of facial displays to be associated with any seg-ment of the sentence.
First, the video was split intoclips corresponding to each sentence.
After that,the facial displays in each clip were annotated.The following were the displays that were consid-ered: eyebrow raising and lowering; eye squinting;head nodding (up, small down, large down); headleaning (left and right); and head turning (left andright).
Figure 2 shows examples of two typicaldisplay combinations.
Any combination of thesefacial displays could be associated with any of therelevant segments in the text.
The relevant seg-ments included all mentions of tile-design prop-erties (e.g., colours, designers), modifiers suchas once again and also, deictic determiners (this,these), and verbs in contrastive contexts (e.g., arein Figure 1).
The annotation scheme treated all fa-cial displays as batons rather than underliners (Ek-man, 1979); that is, each display was associatedwith a single segment.
If a facial display spanneda longer phrase in the speech, it was annotated as aseries of identical batons on each of the segments.Any predicted pitch accents and dialogue-history and user-model information from theCOMIC presentation planner were also associatedwith each segment, as appropriate.
We chose notto restrict our annotation to those segments withpredicted pitch accents, because the speaker alsomade a large number of facial displays on seg-ments with no predicted pitch accent; instead, weincorporated the predicted accent as an additionalcontextual factor.
For the most part, the pitch ac-cents used by the speaker followed the specifica-tions on the slides.
We did not explicitly considerthe rhetorical or syntactic structure, as did, e.g.,de Carolis et al (2000); in general, the structurewas fully determined by the context.There were a total of 1993 relevant segments inthe recorded sentences.
Overall, the most frequentdisplay combination was a small downward nodon its own, which occurred on just over 25% of thesegments.
The second largest class was no motionat all (20% of the segments), followed by down-ward nods (large and small) accompanied by browraises.
Further down the order, the various lateralmotions appear; for this speaker, these were pri-marily turns to the right (Figure 2(a)) and leans tothe left (Figure 2(b)).The distribution of facial displays in specificcontexts differed from the overall distribution.
Thebiggest influence was the user-model evaluation:left leans, brow lowering, and eye squinting wereall relatively more frequent on objects with nega-tive user-model evaluations, while right turns andbrow raises occurred more often in positive con-texts.
Other factors also had an influence: for ex-ample, nodding and brow raises were both morefrequent on segments for which the COMIC plan-ner specified a pitch accent.
Foster (2006) gives adetailed analysis of these recordings.4 Modelling the Corpus DataWe built a range of models using the data fromthe annotated corpus to select facial displays toaccompany generated text.
For each segment inthe text, a model selected a display combinationfrom among the displays used by the speaker in asimilar context.
All of the models used the corpuscounts of displays associated with the segments di-rectly, with no back-off or smoothing.The models differed from one another in twoways: the amount of context that they used, andthe way in which they made a selection within acontext.
There were three levels of context:No context These models used the overall corpuscounts for all segments.355(a) Right turn + brow raise (b) Left lean + brow lowerFigure 2: Typical speaker motions from the recordingSurface only These models used only the contextprovided by the word(s)?or, in some cases,a domain-specific semantic class.
For exam-ple, a model would use the class DECORA-TION rather than the specific word artwork.Full context In addition to the surface form, thesemodels also used the pitch-accent specifica-tions and contextual information supplied bythe COMIC presentation planner.
The con-textual information was associated with thetile-design properties included in the sen-tence and indicated (a) whether that propertyhad been mentioned before, (b) whether itwas explicitly contrasted with a property ofa previous design, and (c) the expected userevaluation of that property.Within a context, there were two strategies for se-lecting a facial display:Majority Choose the combination that occurredthe largest number of times in the context.Weighted Make a random choice from all com-binations seen in the context, weighting thechoice according to the relative frequency.For example, in the no-context case, a majority-choice model would choose the small downwardnod (the majority option) for every segment, whilea weighted-choice model would choose a smalldownward nod with probability 0.25, no motionwith probability 0.2, and the other displays withcorrespondingly decreasing probabilities.These two factors produced a set of 6 modelsin total (3 context levels ?
2 selection strategies).Throughout the rest of this paper, we will use two-character labels to refer to the models.
The firstcharacter of each label indicates the amount of     		 Figure 3: Mean F score for all modelscontext that was used, while the second indicatesthe selection method within that context: for ex-ample, SM corresponds to a model that used thesurface form only and made a majority choice.5 Evaluation 1: Cross-validationWe first compared the performance of the modelsusing 10-fold cross-validation against the corpus.For each fold, we built models using 90% of thesentences in the corpus, and then used those mod-els to predict the facial displays for the sentencesin the other 10% of the corpus.
We measured therecall and precision on a sentence by comparingthe predicted facial displays for each segment tothe actual displays used by the speaker and aver-aging those scores across the sentence.
We thenused the recall and precision scores for a sentenceto compute a sentence-level F score.Averaged across all of the cross-validationfolds, the NM model had the highest recall score,while the FM model scored highest for precisionand F score.
Figure 3 shows the average sentence-level F score for all of the models.
All but oneof the differences shown are significant at the p <356(a) Neutral (b) Right turn + brow raise (c) Left lean + brow lowerFigure 4: Synthesised version of motions from Figure 20.01 level on a paired T test; the performance ofthe NM and FW models was indistinguishable onF score, although the FW model scored higher onprecision and the NM model on recall.That the majority-choice models generallyscored better on this measure than the weighted-choice models is not unexpected: a weighted-choice model is more likely to choose a less-common display, and if it chooses it in a contextwhere the speaker did not, the score for that sen-tence is decreased.
It is also not surprising that,within a selection strategy, the models that takeinto account more of the context did better thanthose that use less of it; this is simply an indica-tion that there are patterns in the corpus, and thatall of the contextual information contributes to theselection of displays.6 Evaluation 2: User RatingsThe majority-choice models performed better onthe cross-validation study than the weighted-choice ones did; however, this does not does notmean that users will necessarily like their outputin practice.
A large amount of the lateral motionand eyebrow movements occurs in the second- orthird-largest class in a number of contexts, and istherefore less likely to be selected by a majority-choice model.
If users like to see motion otherthan simple nodding, it might be that the sched-ules generated by the weighted-choice models areactually preferred.
To address this question, weperformed a user evaluation.6.1 Experiment DesignMaterials For this study, we generated 30 newsentences from the COMIC system.
The sen-tences were selected to ensure that they coveredthe full range of syntactic structures available toCOMIC and that none of them was a duplicateof anything from the recording script.
We thengenerated a facial schedule for each sentence us-ing each of the six models.
Note that, for someof the sentences, more than one model producedan identical sequence of facial displays, either be-cause the majority choice in a broader context wasthe same as in a more narrow context, or becausea weighted-choice model ended up selecting themajority option in every case.
All such identicalschedules were retained in the set of materials; inSection 6.2, we discuss their impact on the results.We then made videos of every schedule for ev-ery sentence, using the Festival speech synthesiser(Clark et al, 2004) and the RUTH talking head(DeCarlo et al, 2004).
Figure 4 shows synthesisedversions of the facial displays from Figure 2.Procedure 33 subjects took part in the experi-ment: 17 female subjects and 16 males.
Theywere primarily undergraduate students, between20 and 24 years old, native speakers of English,with an intermediate amount of computer experi-ence.
Each subject in the study was shown videosof all 30 sentences in an individually-chosen ran-dom order.
For each sentence, the subject sawtwo versions, each generated by a different model,and was asked to choose which version they likedbetter.
The displayed versions were counterbal-anced so that every subject performed each pair-wise comparison of models twice, once in eachorder.
The study was run over the web.6.2 Results2Figure 5(a) shows the overall preference rates forall of the models.
For each model, the value shown2 We do not include those trials where both videos wereidentical; if these are included, the results are similar, but thedistinctions described here just fail to reach significance.357     	(a) Overall preference rates   	  !"#!$%&&##(b) Head-to-head preferencesFigure 5: User evaluation resultson that graph indicates the proportion of the timethat model was chosen over any of the alterna-tives.
For example, in all of the trials where theFW model was one of the options, it was chosenover the alternative 55% of the time.
Note that thevalues on that graph should not be directly com-pared against one another; instead, each should beindividually compared with 0.5 (the dotted line) todetermine whether it was chosen more or less fre-quently than chance.
A binomial test on these val-ues indicates that both the FW and the NW mod-els were chosen significantly above chance, whilethose generated by the SM and NM models werechosen significantly below chance (all p < 0.05).The choices on the FM and SW models were in-distinguishable from chance.If we examine the preferences within a context,we also see a preference for the weighted-choicemodels.
Figure 5(b) shows the preferences for se-lection strategy within each context.
For example,when choosing between schedules both generatedby models using the full context (FM vs. FW ),subjects chose the one generated by the FW model60% of the time.
The trend in both the full-contextand no-context cases is in favour of the weighted-choice models, and the combined values over allsuch trials (the rightmost pair of bars in the figure)show a significant preference for weighted choiceover majority choice across all contexts (binomialtest; p < 0.05).Gender differences There was a large gendereffect on the users?
preferences: overall, themale subjects (n = 16) tended to choose the ma-jority and weighted versions with almost equalprobabilities, while the female subjects (n = 17)strongly preferred the weighted versions in anycontext, and chose the weighted versions signif-icantly more often in head-to-head comparisons(p < 0.001).
In fact, all of the overall prefer-ence for weighted-choice models came from theresponses of the female subjects.
The graphs inFigure 6 show the head-to-head preferences in allcontexts for both groups of subjects.7 DiscussionThe predicted rankings from the cross-validationstudy differ from those in the human evalua-tion: while the cross-validation gave the highestscores to the majority-choice models, the humanjudges actually showed an overall preference forthe weighted-choice models.
This provides sup-port for our hypothesis that humans would prefergenerated output that reproduced more of the vari-ation in the corpus, even if the choices made onspecific sentences differ from those mode in thecorpus.
When Belz and Reiter (2006) performeda similar study comparing natural-language gen-eration systems that used different text-planningstrategies, they also found similar results: auto-mated measures tended to favour majority-choicestrategies, while human judges preferred those thatmade weighted choices.
In general, this sort of au-tomated measure will always tend to favour strate-gies that, on average, do not diverge far from whatis found in the corpus, which indicates a drawbackto using such measures alone to evaluate genera-tion systems where variation is expected.The current study also suggests a further draw-back to corpus-based evaluation: users may varysystematically amongst themselves in what theyprefer.
All of the overall preference for weighted-choice models came from the female subjects;358   	  !"#!$%&&##(a) Male subjects   	 !
"#$"%&''$$(b) Female subjectsFigure 6: Gender influence on head-to-head preferencesthe male subjects did not express any significantpreference either way, but had a mild preferencefor the majority-choice models.
Previous stud-ies on embodied conversational agents have ex-hibited gender effects that appear related this re-sult: Robertson et al (2004) found that, amongschoolchildren, girls preferred a tutoring systemthat included an animated agent, while boys pre-ferred one that did not; White et al (2005) foundthat a more expressive talking head decreasedmale subjects?
task performance when using thefull COMIC system; while Bickmore and Cassell(2005) found that women trusted the REA agentmore in embodied mode, while men trusted hermore over the telephone.
Taken together, these re-sults imply that male users prefer and perform bet-ter using an embodied agent that is less expressiveand that shows less variation in its motions, andmay even prefer a system that does not have anagent at all.
These results are independent of thegender of the agent: the COMIC agent is male,REA is female, while the gender of Robertson?sagents was mixed.
In any case, there is more gen-eral evidence that females have superior abilitiesin facial expression recognition (Hall, 1984).8 Conclusions and Future WorkIn this paper, we have demonstrated that there arepatterns in the facial displays that this speaker usedwhen giving different types of object descriptionsin the COMIC system.
The findings from the cor-pus analysis are compatible with previous find-ings on emphatic facial displays in general, andalso provide a fine-grained analysis of the indi-vidual displays used by this speaker.
Basing therecording scripts on the output of the presenta-tion planner allowed full contextual informationto be included in the annotated corpus; indeed,all of the contextual factors were found to influ-ence the speaker?s use of facial displays.
We havealso shown that a generation system that capturesand reproduces the corpus patterns for a synthetichead can produce successful output.
The resultsof the evaluation also demonstrate that female sub-jects are more receptive than male subjects to vari-ation in facial displays; in combination with otherrelated results, this indicates that expressive con-versational agents are more likely to be successfulwith female users, regardless of the gender of theagent.
Finally, we have shown the potential draw-backs of using a corpus to evaluate the output of ageneration system.There are three directions in which the work de-scribed here can be extended: improved corpus an-notation, more sophisticated implementations, andfurther evaluations.
First, the annotation on thecorpus that was used here was done by a single an-notator, in the context of a specific generation task.The findings from the corpus analysis generallyagree with those of previous studies (e.g., the pre-dicted pitch accent was correlated with noddingand eyebrow raises), and the corpus as it standshas proved useful for the task for which it was cre-ated.
However, to get a more definitive picture ofthe patterns in the corpus, it should be re-annotatedby multiple coders, and the inter-annotator agree-ment should be assessed.
Possible extensions tothe annotation scheme include timing informationfor the words and facial displays, and actual?asopposed to predicted?prosodic contours.In the implementation described here, we builtsimple models based directly on the corpus countsand used them to select facial displays to accom-359pany previously-generated text; both of these as-pects of the implementation can be extended infuture.
If we build more sophisticated n-gram-based models of the facial displays, using a fulllanguage-modelling toolkit, we can take into ac-count contextual information from words otherthan those in a single segment, and back offsmoothly through different amounts of context.Such models can also be integrated directly intothe OpenCCG surface realiser (White, 2005)?which is already used as part of the COMICoutput-generation process, and which uses n-grams to guide its search for a good realisation.This will allow the system to choose the text andfacial displays in parallel rather than sequentially.Such an integrated implementation has a betterchance at capturing the complex interactions be-tween the two output channels.Future evaluations should address several ques-tions.
First, we should gather users?
opinions ofthe behaviours annotated in the corpus: it may bethat subjects actually prefer the generated facialdisplays to the displays in the corpus, as was foundby Belz and Reiter (2006).
As well, further stud-ies should look in more detail at the exact nature ofthe gender effect on user preferences, for instanceby systematically varying the motion on differ-ent dimensions individually to see exactly whichtypes of facial displays are liked and disliked bydifferent demographic groups.
Finally, if the ex-tended n-gram-based model mentioned above isimplemented, its performance should be measuredand compared to that of the models described here,through both cross-validation and user studies.AcknowledgementsThanks to Matthew Stone, Michael White, and theanonymous EACL reviewers for their useful com-ments on previous versions of this paper.ReferencesA.
Belz and E. Reiter.
2006.
Comparing automatic and hu-man evaluation of NLG systems.
In Proc.
EACL 2006.T.
Bickmore and J. Cassell.
2005.
Social dialogue with em-bodied conversational agents.
In J. van Kuppevelt, L. Dy-bkj?r, and N. Bernsen, editors, Advances in Natural, Mul-timodal Dialogue Systems.
Kluwer, New York.B.
de Carolis, V. Carofiglio, and C. Pelachaud.
2002.
Fromdiscourse plans to believable behavior generation.
In Proc.INLG 2002.B.
de Carolis, C. Pelachaud, and I. Poggi.
2000.
Verbaland nonverbal discourse planning.
In Proc.
AAMAS 2000Workshop ?Achieving Human-Like Behavior in Interac-tive Animated Agents?.J.
Cassell, T. Bickmore, H. Vilhja?lmsson, and H. Yan.
2001a.More than just a pretty face: Conversational protocols andthe affordances of embodiment.
Knowledge-Based Sys-tems, 14(1?2):55?64.J.
Cassell, Y. Nakano, T. W. Bickmore, C. L. Sidner, andC.
Rich.
2001b.
Non-verbal cues for discourse structure.In Proc.
ACL 2001.J.
Cassell, J. Sullivan, S. Prevost, and E. Churchill.
2000.
Em-bodied Conversational Agents.
MIT Press.J.
Cassell and K. R. Tho?risson.
1999.
The power of a nodand a glance: Envelope vs. emotional feedback in an-imated conversational agents.
Applied Artificial Intelli-gence, 13(4?5):519?538.R.
A. J. Clark, K. Richmond, and S. King.
2004.
Festival 2 ?build your own general purpose unit selection speech syn-thesiser.
In Proc.
5th ISCA Workshop on Speech Synthesis.D.
W. Cunningham, M. Kleiner, H. H. Bu?lthoff, and C. Wall-raven.
2004.
The components of conversational facial ex-pressions.
In Proc.
APGV 2004, pages 143?150.D.
DeCarlo, M. Stone, C. Revilla, and J. Venditti.
2004.
Spec-ifying and animating facial signals for discourse in em-bodied conversational agents.
Computer Animation andVirtual Worlds, 15(1):27?38.P.
Ekman.
1979.
About brows: Emotional and conversationalsignals.
In M. von Cranach, K. Foppa, W. Lepenies, andD.
Ploog, editors, Human Ethology: Claims and limits ofa new discipline.
Cambridge University Press.M.
E. Foster.
2006.
Non-default choice in generation sys-tems.
Ph.D. thesis, School of Informatics, University ofEdinburgh.
In preparation.M.
E. Foster, M. White, A. Setzer, and R. Catizone.
2005.Multimodal generation in the COMIC dialogue system.
InProc.
ACL 2005 Demo Session.J.
A.
Hall.
1984.
Nonverbal sex differences: Communicationaccuracy and expressive style.
The Johns Hopkins Uni-versity Press.M.
Kipp.
2004.
Gesture Generation by Imitation - From Hu-man Behavior to Computer Character Animation.
Disser-tation.com.E.
Krahmer and M. Swerts.
2004.
More about brows:A cross-linguistic study via analysis-by-synthesis.
InC. Pelachaud and Z. Ruttkay, editors, From Browsto Trust: Evaluating Embodied Conversational Agents,pages 191?216.
Kluwer.J.
Robertson, B.
Cross, H. Macleod, and P. Wiemer-Hastings.2004.
Children?s interactions with animated agents in anintelligent tutoring system.
International Journal of Arti-ficial Intelligence in Education, 14:335?357.H.
Shimodaira, K. Uematsu, S. Kawamoto, G. Hofer, andM.
Nakai.
2005.
Analysis and synthesis of head motionfor lifelike conversational agents.
In Proc.
MLMI 2005.M.
Steedman.
2000.
Information structure and the syntax-phonology interface.
Linguistic Inquiry, 31(4):649?689.M.
Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Lees, A. Stere,and C. Bregler.
2004.
Speaking with hands: Creating an-imated conversational characters from recordings of hu-man performance.
ACM Transactions on Graphics (TOG),23(3):506?513.M.
White.
2005.
Efficient realization of coordinate structuresin Combinatory Categorial Grammar.
Research on Lan-guage and Computation.
To appear.M.
White, M. E. Foster, J. Oberlander, and A.
Brown.
2005.Using facial feedback to enhance turn-taking in a multi-modal dialogue system.
In Proc.
HCI International 2005.360
