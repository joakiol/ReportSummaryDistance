Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2028?2034,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsVector-space models for PPDB paraphrase ranking in contextMarianna ApidianakiLIMSI, CNRS, Universite?
Paris-Saclay91403 Orsay, Francemarianna.apidianaki@limsi.frAbstractThe PPDB is an automatically built databasewhich contains millions of paraphrases in dif-ferent languages.
Paraphrases in this resourceare associated with features that serve to theirranking and reflect paraphrase quality.
Thiscontext-unaware ranking captures the seman-tic similarity of paraphrases but cannot serveto estimate their adequacy in specific con-texts.
We propose to use vector-space se-mantic models for selecting PPDB paraphrasesthat preserve the meaning of specific text frag-ments.
This is the first work that addresses thesubstitutability of PPDB paraphrases in con-text.
We show that vector-space models ofmeaning can be successfully applied to thistask and increase the benefit brought by theuse of the PPDB resource in applications.1 IntroductionParaphrases are alternative ways to convey the sameinformation and can improve natural language pro-cessing by making systems more robust to lan-guage variability and unseen words.
The paraphrasedatabase (PPDB) (Ganitkevitch et al, 2013) containsmillions of automatically acquired paraphrases in21 languages associated with features that serve totheir ranking.
In PPDB?s most recent release (2.0),such features include natural logic entailment rela-tions, distributional and word embedding similari-ties, formality and complexity scores, and scores as-signed by a supervised ranking model (Pavlick et al,2015b).
These features serve to identify good qual-ity paraphrases but do not say much about their sub-stitutability in context.To judge the adequacy of paraphrases for specificinstances of words or phrases, the surrounding con-text needs to be considered.
This can be done usingvector-space models of semantics which calculatethe meaning of word occurrences in context basedon distributional representations (Mitchell and La-pata, 2008; Erk and Pado?, 2008; Dinu and Lapata,2010; Thater et al, 2011).
These models capture theinfluence of the context on the meaning of a targetword through vector composition.
More precisely,they represent the contextualised meaning of a targetword w in context c by a vector obtained by com-bining the vectors of w and c using some operationsuch as component-wise multiplication or addition(Thater et al, 2011).
We use this kind of represen-tations to rank the PPDB paraphrases in context andretain the ones that preserve the semantics of spe-cific text fragments.
We evaluate the vector-basedranking models on data hand-annotated with lexicalvariants and compare the obtained ranking to confi-dence estimates available in the PPDB, highlightingthe importance of context filtering for paraphrase se-lection.2 Context-based paraphrase ranking2.1 Paraphrase substitutabilityThe PPDB1 provides millions of lexical, phrasal andsyntactic paraphrases in 21 languages ?
acquiredby applying bi- and multi-lingual pivoting on par-allel corpora (Bannard and Callison-Burch, 2005) ?and is largely exploited in applications (Denkowskiand Lavie, 2010; Sultan et al, 2014; Faruqui et al,1http://paraphrase.org/#/download20282015).
PPDB paraphrases come into packages of dif-ferent sizes (going from S to XXXL): smaller pack-ages contain high-precision paraphrases while largerones aim for high coverage.
Until now, pivot para-phrases have been used as equivalence sets (i.e.
allparaphrases available for a word are viewed as se-mantically equivalent) and their substitutability incontext has not yet been addressed.Substitutability might be restrained by severalfactors which make choosing the appropriate para-phrase for a word or phrase in different contexts anon-trivial task.
In case of polysemous words, para-phrases describe different meanings and can leadto erroneous semantic mappings if substituted intexts (Apidianaki et al, 2014; Cocos and Callison-Burch, 2016).
Even when paraphrases capture thesame general sense, they are hardly ever equiva-lent synonyms and generally display subtle differ-ences in meaning, connotation or usage (Edmondsand Hirst, 2002).
Stylistic variation might also bepresent within paraphrase sets and substituting para-phrases that differ in terms of complexity and for-mality can result in a change in style (Pavlick andNenkova, 2015).
To increase paraphrase applicabil-ity in context, Pavlick et al (2015a) propose to ex-tract domain-specific pivot paraphrases by biasingthe parallel training data used by the pivot methodtowards a specific domain.
This customised modelgreatly improves paraphrase quality for the targetdomain but does not allow to rank and filter theparaphrases already in the PPDB according to spe-cific contexts.
To our knowledge, this is the firstwork that addresses the question of in-context sub-stitutability of PPDB paraphrases.
We show how ex-isting substitutability models can be applied to thistask in order to increase the usefulness of this large-scale resource in applications.2.2 Vector-space models of paraphraseadequacyVector-based models of meaning determine a grad-ual concept of semantic similarity which does notrely on a fixed set of dictionary senses.
They areused for word sense discrimination and induction(Schu?tze, 1998; Turney and Pantel, 2010) and cancapture the contextualised meaning of words andphrases (Mitchell and Lapata, 2008; Erk and Pado?,2008; Thater et al, 2011).
Vector composition meth-ods build representations that go beyond individualwords to obtain word meanings in context.
Somemodels use explicit sense representations while oth-ers modify the basic meaning vector of a target wordwith information from the vectors of the words in itscontext.
In the framework proposed by Dinu andLapata (2010), for example, word meaning is rep-resented as a probability distribution over a set oflatent senses reflecting the out-of-context likelihoodof each sense, and the contextualised meaning of aword is modeled as a change in the original sensedistribution.2 Reisinger and Mooney (2010) pro-pose a multi-prototype vector-space model of mean-ing which produces multiple ?sense-specific?
vec-tors for each word, determined by clustering the con-texts in which the word appears (Schu?tze, 1998).The cluster centroids serve as prototype vectors de-scribing a word?s senses and the meaning of a spe-cific occurrence is determined by choosing the vec-tor that minimizes the distance to the vector repre-senting the current context.
On the contrary, Thateret al (2011) use no explicit sense representation.Their models allow the computation of vector repre-sentations for individual uses of words, characteris-ing the specific meaning of a target word in its sen-tential context.
When used for paraphrase ranking,these models derive a contextualised vector for a tar-get word by reweighting the components of its basicmeaning vector on the basis of the context of oc-currence.3 Paraphrase candidates for a target wordare then ranked according to the cosine similarityof their basic vector representation to the contextu-alised vector of the target.43 Experimental Set-upData In our experiments, we use the COINCO cor-pus (Kremer et al, 2014), a subset of the ?Man-ually Annotated Sub-Corpus?
MASC (Ide et al,2010) which comprises more than 15K word in-2The latent senses are induced using non-negative matrixfactorization (NMF) (Lee and Seung, 2001) and latent Dirichletallocation (LDA) (Blei et al, 2003).3Depending on the model, the vector combination functionmight be addition or multiplication of vector elements.4Thater et al?s (2011) models delivered best results in para-phrase ranking on the CoInCo corpus (Kremer et al, 2014) andthe SEMEVAL-2007 Lexical Substitution dataset (McCarthyand Navigli, 2007).2029|P|> 1 |P| ?
1PPDB # Instances # Lemmas Avg |P| # InstancesS 2146 560 2.67 5573M 3716 855 2.92 7771L 6228 1394 3.57 10100XL 13344 2822 10.33 14060XXL 14507 3308 185.09 14593Table 1: Number of COINCO instances and distinct lemmascovered by each PPDB package.stances manually annotated with single and multi-word substitutes.
The manual annotations serve toevaluate the performance of the vector-space modelson the task of ranking PPDB paraphrases.
For eachannotated English target word (noun, verb, adjectiveor adverb) in COINCO, we collect the lexical para-phrases (P = {p1, p2, ..., pn}) available for the wordin each PPDB package (from S to XXL).5 We do notfilter by syntactic label as annotations often includesubstitutes of different grammatical categories.
Ta-ble 1 shows the number of COINCO tokens withparaphrases in each PPDB package and the averagesize of the retained paraphrase sets.
The larger thesize of the resource, the greater the coverage of tar-get words in COINCO.
The last column of the tablegives the total number of instances covered, includ-ing the ones with only one paraphrase.
In the rank-ing experiments, we focus on lemmas having morethan one paraphrase in the PPDB.6Methodology We follow the methodology proposedin Kremer et al (2014) to explore the extent towhich vector-based models can select appropriateparaphrases for words in context.
Given a targetword w in a sentential context and a set of para-phrases P extracted for w from a PPDB package, thetask is to rank the elements in P according to theiradequacy as paraphrases of w in the given context.We carry out experiments with three versions ofthe Thater et al (2011) ranking model: (a) asyntactically structured model (Syn.Vec) that usesvectors recording co-occurrences based on depen-dency triples, explicitly recording syntactic role in-5Since the XXL package covers almost all annotated in-stances in COINCO (14,507 out of 15,629) and there are 185.09paraphrases in average for each instance, we exclude the XXXLpackage from these experiments.6We retain paraphrases of the lemmatised forms of the tar-get words but these unsupervised ranking models can be easilyapplied to the whole PPDB resource and in different languages.formation within the vectors; (b) a syntactically fil-tered model (Filter.Vec) using dependency-based co-occurrence information without explicitly represent-ing the syntactic role in the vector representations,as in Pado?
and Lapata (2007); (c) a bag of wordsmodel (Bow.Vec) using a window of ?
5 words.Co-occurrence counts were extracted from the En-glish Gigaword corpus7 analysed with Stanford de-pendencies (de Marneffe et al, 2006).
The syntacticmodel vectors are based on dependency triples thatoccur at least 5 times in the corpus and have a PMIscore of at least 2.
The same thresholds apply tothe bag of words model where the frequency thresh-old defines the minimum number of times that twowords have been observed in the same context win-dow.
The task of the vector-space models for eachtarget word instance is to rank the contents of thecorresponding paraphrase set (which contains all thesubstitution candidates available for the target in thePPDB) so that the actual substitutes are ranked higherthan the rest.
For example, newspaper, manuscriptand document are good paraphrase candidates forpaper but we would expect newspaper to be rankedhigher than the other two in this sentence: ?the pa-per?s local administrator?.A contextualised vector is derived from the ba-sic meaning vector of a target word w by reinforc-ing its dimensions that are licensed by the contextof the specific instance under consideration.
In theBow.Vec model, the context is made up of 5 wordsbefore and after the target while in the syntacticmodels, it corresponds to the target?s direct syntac-tic dependents.
The contextualised vector for w isobtained through vector addition and contains infor-mation about the context words.
Paraphrase can-didates are ranked according to the cosine similar-ity between the contextualised vector of the targetword and the basic meaning vectors of the candi-dates.
Following Kremer et al (2014), we com-pare the resulting ranked list to the COINCO goldstandard annotation (the paraphrase set of the tar-get instance) using Generalised Average Precision(GAP) (Kishida, 2005) and annotation frequency asweights.
GAP scores range between 0 and 1: ascore of 1 indicates a perfect ranking in which allcorrect substitutes precede all incorrect ones, and7http://catalog.ldc.upenn.edu/LDC2003T052030PPDB Bow.Vec Syn.Vec Filter.Vec Google AGiga Ppdb1 Ppdb2 Parprob Random (5)|P|>1S 0.91 0.91 0.91 0.78 0.86 0.66 0.83 0.66 0.78M 0.91 0.91 0.92 0.79 0.87 0.68 0.84 0.68 0.79L 0.90 0.90 0.91 0.78 0.85 0.66 0.83 0.66 0.77XL 0.78 0.79 0.79 0.58 0.67 0.44 0.66 0.43 0.58XXL 0.53 0.56 0.57 0.27 0.36 0.12 0.58 0.12 0.27|P|?1S 0.97 0.97 0.97 0.91 0.95 0.87 0.93 0.87 0.91M 0.96 0.96 0.96 0.90 0.94 0.85 0.92 0.85 0.90L 0.94 0.94 0.94 0.87 0.91 0.79 0.90 0.79 0.86XL 0.79 0.80 0.80 0.60 0.69 0.47 0.68 0.46 0.60XXL 0.54 0.56 0.58 0.28 0.37 0.13 0.59 0.14 0.28Table 2: Average GAP scores for the contextual models, five paraphrase adequacy methods and the random ranking baseline againstthe gold COINCO annotations.
Scores reported for different sizes of the PPDB (from S to XXL).correct high-weight substitutes precede low-weightones.
For calculating the GAP score, we assign avery low score (0.001) to paraphrases that are notpresent in COINCO for a target word (i.e.
not pro-posed by the annotators).4 ResultsThe average GAP scores obtained by the threevector-space models (Bow.Vec, Syn.Vec and Fil-ter.Vec) are shown in Table 2.
The upper part of thetable reports scores obtained for words with morethan one paraphrase in the PPDB (|P|> 1) while thelower part gives the scores for all words.We compare the GAP scores to five differentrankings reflecting paraphrase quality in the PPDB(Pavlick et al, 2015b).
We retain the followingscores: 1.
AGigaSim captures the distributionalsimilarity of a phrase e1 and its paraphrase e2 com-puted according to contexts observed in the Anno-tated Gigaword corpus (Napoles et al, 2011); 2.GoogleNgramSim reflects the distributional simi-larity of e1 and e2 computed according to contextsobserved in the Google Ngram corpus (Brants andFranz, 2006); 3.
ParProb: the paraphrase proba-bility of e2 given the original phrase e1 (Bannardand Callison-Burch, 2005); 4.
Ppdb1: the heuris-tic scoring used for ranking in the original releaseof the PPDB (Ganitkevitch et al, 2013); 5.
Ppdb2:the improved ranking of English paraphrases avail-able in PPDB 2.0.
The results are also compared tothe output of a baseline where the paraphrases arerandomly ranked.
The reported baseline figures arePPDB package-specific since a different paraphraseset is retained from each package, and correspondto averages over 5 runs.
The quality of the rank-ing produced by the baseline clearly decreases as thesize of the PPDB resource increases due to the highernumber of retained paraphrases which makes rank-ing harder.The results in the upper part of the table showthat the vector-space models provide a better rank-ing than the PPDB estimates and largely outperformthe random baseline.
The three models performsimilarly on this ranking task according to averageGAP with the syntactically-informed models gettingslightly higher scores.
Differences between Syn.Vecand Filter.Vec, as well as between Bow.Vec and thesyntactic models, are highly significant in the XLand XXL packages (p-value < 0.001) as computedwith approximate randomisation (Pado?, 2006).
Inthe L package, the difference between Syn.Vec andFilter.Vec is significant (p < 0.05) and the one be-tween Bow.Vec and Filter.Vec is highly significant.Finally, in the M package, only the difference be-tween Bow.Vec and Filter.Vec is significant (p <0.05), while Syn.Vec and Filter.Vec seem to dealsimilarly well with the contents of this package.Two PPDB ranking methods, AGiga and Ppdb2,obtain good results.
AgigaSim reflects the distribu-tional similarity of the paraphrases in the AnnotatedGigaword corpus (Napoles et al, 2011).
As notedby Kremer et al (2014), the whole-document an-notation in COINCO faces the natural skewed dis-tribution towards predominant senses which favorsnon-contextualised baseline models.
The good per-formance of Ppdb2 is due to the use of a super-vised scoring model trained on human judgmentsof paraphrase quality.
The human judgments were2031used to fit a regression to the features available inPPDB 1.0 plus numerous new features including co-sine word embedding similarity, lexical overlap fea-tures, WordNet features and distributional similarityfeatures.8 The small difference observed betweenthe Ppdb2 and the syntactic models score in theXXL package is highly significant.
For the moment,Ppdb2 scores are available in the PPDB only for En-glish.
Since the vector-space methodology is unsu-pervised and language independent, it could be eas-ily applied to paraphrase ranking in other languages.The performance of the models remains high withthe XL package which contains paraphrase sets ofreasonable size (about 10 paraphrases per word) andensures a high coverage, and lowers in XXL whichcontains 185 paraphrases in average per word (cf.Table 1).
To use this package more efficiently, onecould initially reduce the number of erroneous para-phrases on the basis of the Ppdb2 score which pro-vides a good ranking of the XXL package contentsbefore applying the vector-based models.The increase in GAP score observed when wordswith one paraphrase are considered shows that theseparaphrases are often correct.
Here too, the contex-tual models provide a better ranking than the out-of-context scores and outperform the random baseline.As in the previous case, the Ppdb2 score is slightlyhigher in the XXL package.5 ConclusionWe have shown that vector-based models of seman-tics can be successfully applied to in-context rankingof PPDB paraphrases.
Allowing for better context-informed substitutions, they can be used to filterPPDB paraphrases on the fly and select variants pre-serving the correct semantics of words and phrasesin texts.
This processing would be beneficial to nu-merous applications that need paraphrase support(e.g.
summarisation, query reformulation and lan-guage learning), providing a practical means for ex-ploiting the extensive multilingual knowledge avail-able in the PPDB resource.This study opens up many avenues for futurework.
Although tested on English, the proposedmethodology can be applied to all languages in the8The features used for computing the paraphrase ranking inPPDB 2.0 are described in detail in Pavlick et al (2015b).PPDB even to the ones that do not dispose of a de-pendency parser (as shown by the high performanceof the Bow.Vec models).An ideal testbed for evaluation in a real applica-tion and on multiple languages is offered by MTevaluation.
The METEOR-NEXT metric (Denkowskiand Lavie, 2010) provides a straightforward frame-work for testing as it already exploits PPDB para-phrases for capturing sense correspondences be-tween text fragments.
In its current version, the met-ric views paraphrases as equivalent classes whichcan lead to erroneous sense mappings due to seman-tic distinctions present in the paraphrase sets.
Wehave recently showed that the context-based filteringof semantic variants improves METEOR?s correlationwith human judgments of translation quality (Marieand Apidianaki, 2015).
We believe that a context-based paraphrase ranking mechanism will enhancecorrect substitutions and further improve the met-ric.
Last but not least, the paraphrase vectors can beused for mapping the contents of the PPDB resourceto other multilingual resources for which vector rep-resentations are available (Camacho-Collados et al,2015a; Camacho-Collados et al, 2015b).
The in-terest of mapping paraphrases in the vector spaceto concepts found in existing semantic resourcesis twofold: it would permit to analyse the seman-tics of the paraphrases by putting them into corre-spondence with explicit concept representations andwould serve to enrich other semantic resources (e.g.BabelNet synsets) with semantically similar para-phrases.Handling phrasal paraphrases is another naturalextension of this work.
We consider using a vectorspace model of semantic composition to calculatethe meaning of longer candidate paraphrases (Dinuet al, 2013; Paperno et al, 2014) and select appro-priate substitutes for phrases in context.AcknowledgmentsWe would like to thank Stefan Thater for sharing thevector-space models, Benjamin Marie for his sup-port with the paraphrase ranking models and theanonymous reviewers for their valuable commentsand suggestions.2032ReferencesMarianna Apidianaki, Emilia Verzeni, and Diana Mc-Carthy.
2014.
Semantic Clustering of Pivot Para-phrases.
In Proceedings of LREC, Reykjavik, Iceland.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Proceed-ings of ACL, pages 597?604, Ann Arbor, Michigan,USA.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Thorsten Brants and Alex Franz.
2006.
The GoogleWeb 1T 5-gram Corpus Version 1.1.
LDC2006T13,Philadelphia.Jose?
Camacho-Collados, Mohammad Taher Pilehvar, andRoberto Navigli.
2015a.
Nasari: a novel approach toa semantically-aware representation of items.
In Pro-ceedings of the 2015 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 567?577, Denver, Colorado, May?June.Jose?
Camacho-Collados, Mohammad Taher Pilehvar, andRoberto Navigli.
2015b.
A unified multilingual se-mantic representation of concepts.
In Proceedings ofthe 53rd Annual Meeting of the Association for Com-putational Linguistics and the 7th International JointConference on Natural Language Processing (Volume1: Long Papers), pages 741?751, Beijing, China, July.Anne Cocos and Chris Callison-Burch.
2016.
Clusteringparaphrases by word sense.
In The 2016 Conferenceof the North American Chapter of the Association forComputational Linguistics (NAACL 2016), San Diego,California, USA.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Toappear at LREC-06.Michael Denkowski and Alon Lavie.
2010.
METEOR-NEXT and the METEOR Paraphrase Tables: Im-proved Evaluation Support for Five Target Languages.In Proceedings of WMT/MetricsMATR, pages 339?342, Uppsala, Sweden.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1162?1172, Cambridge,MA, October.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013.
Dissect - distributional semantics compositiontoolkit.
In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics: SystemDemonstrations, pages 31?36, Sofia, Bulgaria, Au-gust.Philip Edmonds and Graeme Hirst.
2002.
Near-Synonymy and Lexical Choice.
Computational Lin-guistics, 28(2):105?144.Katrin Erk and Sebastian Pado?.
2008.
A StructuredVector Space Model for Word Meaning in Context.In Proceedings of EMNLP, pages 897?906, Honolulu,Hawaii.Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, ChrisDyer, Eduard Hovy, and Noah A. Smith.
2015.Retrofitting word vectors to semantic lexicons.
In Pro-ceedings of the 2015 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 1606?1615, Denver, Colorado.Juri Ganitkevitch, Benjamin VanDurme, and ChrisCallison-Burch.
2013.
PPDB: The ParaphraseDatabase.
In Proceedings of NAACL, Atlanta, Geor-gia, USA.Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-becca Passonneau.
2010.
The manually annotatedsub-corpus: A community resource for and by the peo-ple.
In Proceedings of the ACL 2010 Conference ShortPapers, pages 68?73, Uppsala, Sweden.Kazuaki Kishida.
2005.
Property of average precisionand its generalization: An examination of evaluationindicator for information retrieval experiments.
Tech-nical report, Technical Report NII-2005-014E.Gerhard Kremer, Katrin Erk, Sebastian Pado?, and StefanThater.
2014.
What Substitutes Tell Us - Analysis ofan ?All-Words?
Lexical Substitution Corpus.
In Pro-ceedings of EACL, pages 540?549, Gothenburg, Swe-den.Daniel D. Lee and H. Sebastian Seung.
2001.
Algo-rithms for non-negative matrix factorization.
In Ad-vances in Neural Information Processing Systems 13(NIPS 2000), pages 556?562.
MIT Press.Benjamin Marie and Marianna Apidianaki.
2015.Alignment-based sense selection in METEOR and theRATATOUILLE recipe.
In Proceedings of WMT,pages 385?391, Lisbon, Portugal.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
InProceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 48?53,Prague, Czech Republic.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedModels of Semantic Composition.
In Proceedings ofACL/HLT, pages 236?244, Columbus, Ohio, USA.Courtney Napoles, Chris Callison-Burch, Juri Ganitke-vitch, and Benjamin Van Durme.
2011.
Paraphras-tic sentence compression with a character-based met-ric: Tightening without deletion.
In Proceedings ofthe Workshop on Monolingual Text-To-Text Genera-tion, pages 84?90, Portland, Oregon.2033Sebastian Pado and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Sebastian Pado?, 2006.
User?s guide to sigf: Signifi-cance testing by approximate randomisation.Denis Paperno, Nghia The Pham, and Marco Baroni.2014.
A practical and linguistically-motivated ap-proach to compositional distributional semantics.
InProceedings of the 52nd Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: LongPapers), pages 90?99, Baltimore, Maryland, June.Ellie Pavlick and Ani Nenkova.
2015.
Inducing lexicalstyle properties for paraphrase and genre differentia-tion.
In Proceedings of the 2015 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 218?224, Denver, Colorado.Ellie Pavlick, Juri Ganitkevitch, Tsz Ping Chan, XuchenYao, Benjamin Van Durme, and Chris Callison-Burch.2015a.
Domain-Specific Paraphrase Extraction.
InProceedings of ACL/IJCNLP, pages 57?62, Beijing,China.Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,Benjamin Van Durme, and Chris Callison-Burch.2015b.
PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, andstyle classification.
In Proceedings of ACL/IJCNLP,pages 425?430, Beijing, China.Joseph Reisinger and Raymond J. Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 109?117, Los Angeles, California.Hinrich Schu?tze.
1998.
Automatic Word Sense Discrim-ination.
Computational Linguistics, 24:97?123.Md Arafat Sultan, Steven Bethard, and Tamara Sumner.2014.
Dls@cu: Sentence similarity from word align-ment.
In Proceedings of the 8th International Work-shop on Semantic Evaluation (SemEval 2014), pages241?246, Dublin, Ireland, August.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2011.
Word Meaning in Context: A Simple and Effec-tive Vector Model.
In Proceedings of IJCNLP, pages1134?1143, Chiang Mai, Thailand.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of Artificial Intelligence Research, 37(1):141?188.2034
