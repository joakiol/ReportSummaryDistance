Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1256?1261,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsMining for unambiguous instancesto adapt part-of-speech taggers to new domainsDirk Hovy, Barbara Plank, H?ector Mart?
?nez Alonso, Anders S?gaardCenter for Language TechnologyUniversity of Copenhagen, DenmarkNjalsgade 140{dirk|bplank}@cst.dk,{alonso|soegaard}@hum.ku.dkAbstractWe present a simple, yet effective approachto adapt part-of-speech (POS) taggers to newdomains.
Our approach only requires a dic-tionary and large amounts of unlabeled tar-get data.
The idea is to use the dictionary tomine the unlabeled target data for unambigu-ous word sequences, thus effectively collect-ing labeled target data.
We add the mined in-stances to available labeled newswire data totrain a POS tagger for the target domain.
Theinduced models significantly improve taggingaccuracy on held-out test sets across three do-mains (Twitter, spoken language, and searchqueries).
We also present results for Dutch,Spanish and Portuguese Twitter data, and pro-vide two novel manually-annotated test sets.1 IntroductionPart-of-speech (POS) taggers are typically trainedon newswire and exhibit severe out-of-domainperformance drops (Blitzer et al, 2006; Daume III,2007; Foster et al, 2011).
When faced with a newdomain, one option is to try to leverage availableunlabeled data.
However, rather than resortingto pure self-training approaches (self-labeling),we here resort to another source of information.One way to address the annotation problem isto use collaboratively created resources such asWikipedia for distant supervision (Mintz et al,2009), or the automatically derived dictionariescalled Wiktionary (Li et al, 2012).
We show how toleverage these resources to create labeled trainingdata.
It turns out that many entries in Wiktionaryare actually unambiguous, i.e., there is only onepossible tag for the word.
In fact, for EnglishWiktionary (Li et al, 2012), we find that 93% of theunigram types are unambiguous (cf.
Table 2).Our idea here is simple: we mine for unlabeledsentences that contain only unambiguous items (ac-cording to Wiktionary), and use the resulting dataas additional, labeled training material.
Concretely,we mine unannotated corpora of tweets, transcribedspeech, and search queries for sentences that con-tain only unambiguous tokens, and combine thoseinstances with newswire data to train POS mod-els that adapt better to the respective domains.
Weshow that adding unambiguous data leads to con-siderable improvements over both unadapted andweakly-supervised baselines (Li et al, 2012).Since Wiktionary has relatively low coverage forsome of these domains, we also explore the use ofBrown clusters to extend the coverage.
This enablesus to generalize across spelling variations and syn-onyms.
Additionally, we evaluate our approach onDutch, Portuguese and Spanish Twitter and presenttow novel data sets for the latter two languages.2 Data2.1 WiktionaryIn our experiments, we use the (unigram) tag dic-tionaries from Wiktionary, as collected by Li et al(2012).1The size and quality of our tag dictionariescrucially influence how much unambiguous data wecan extract, and for some languages, the number ofdictionary entries is small.We can resort to normalization dictionaries toextend Wiktionary?s coverage.
We do so for En-glish (Han and Baldwin, 2011).
It replaces some1https://code.google.com/p/wikily-supervised-pos-tagger/1256NEWSWIRE Spielberg took the helm of this big budget live action project with RobinWilliams playing an adult Peter and Dustin Hoffman as the dastardly Captain Hook.TWITTER Rooofiii Oooooo, didn?t think ppl<3the movie as much as me, this movie willalways b the peter pan story2me #robin #williams #hookSPOKEN I loved that movie... Uhm... You know, Hook.
With Robin Williams, uh.QUERIES peter pan williams movieTable 1: Examples from source (top row) and target domains (bottom rows)spelling variations with the standard form (youuuu-uuuu ?
you), which reduces the vocabulary size.For languages where no such normalization dic-tionary is available, we use word clusterings basedon Brown clusters (Brown et al, 1992) to generalizetags from unambiguous words to previously unseenwords in the same class.CLUSTER TOKEN TAG ?
D PROJ.
TAG01011110 offish ADJ ?01011110 alreadyyy ???
ADV01011110 finali ???
ADV01011110 aleady ???
ADV01011110 previously ADV ?01011110 already ADV ?01011110 recently ADV ?Figure 1: Example of a Brown cluster with unambiguoustokens, as well as projected tags for new tokens (tokensmarked ???
are unchanged in D?
).In particular, to extend the dictionary D to D?us-ing clusters, we first run clustering on the unlabeleddata T , using Brown clustering.2We then assign toeach unambiguous word in the cluster its tag fromdictionary D. For all remaining tokens in the samecluster, we assign them the most frequently observedtag in the cluster, provided that label occurred atleast twice as often as the second most frequent one,and the token itself was not already in Wiktionary.As an example, consider the cluster in Figure 1.Since three tokens were unambiguously tagged asADV in the original dictionary (previously, already,recently), we project ADV to all tokens in the clusterthat were not already in D (here: alreadyyy, finali,aleady), and finally add all words to D?.
The tokenoffish remains an ADJ.2https://github.com/percyliang/brown-cluster2.2 Unlabeled dataFor each domain and language, given dictionaryD, we extract unambiguous sentences/tweets.
Usernames and URLs are assumed to be nouns.
If allwords are unambiguous according to the dictionary,we include the sentence/tweet in our training data.For hashtags on Twitter, we remove the ?#?
sign andcheck the remainder against the dictionary.
We ex-clude tweets that only contain users and URLs.The unambiguous subsets of the unlabeled datarepresent very biased samples of the various do-mains.
The ratio of unambiguous English tweets,for example, is only about 0.012 (or 1 in 84), and thedistribution of tags in the Twitter data set is heavilyskewed towards nouns, while several other labels areunder-represented.Twitter We collect the unlabeled data from theTwitter streaming API.3We collected 57m tweetsfor English, 8.2m for Spanish, 4.1m for Portuguese,and 0.5m for Dutch.
We do not perform sentencesplitting on tweets, but take them as unit sequences.Spoken language We use the Switchboard corpusof transcribed telephone conversations (Godfrey etal., 1992), sections 2 and 3, as well as the Englishsection of EuroParl (Koehn, 2005) and CHILDES(MacWhinney, 1997).
We removed all meta-dataand inline annotations (gestures, sounds, etc.
), aswell as dialogue markers.
The final joint corpus con-tains transcriptions of 570k spoken sentences.Search queries For search queries, we use acombination of queries from Yahoo4and AOL.
Weonly use the search terms and ignore any additionalinformation, such as user ID, time, and linkedURLs.
The resulting data set contains 10m queries.3https://github.com/saffsd/langid.py4http://webscope.sandbox.yahoo.com/12572.3 Labeled dataWe train our models on newswire, as well as minedunambiguous instances.
For English, we use theOntoNotes release of the WSJ section of the PennTreebank as training data for Twitter, spoken data,and queries.5For Dutch, we use the training sec-tion of the Alpino treebank from the CoNLL task.6For Portuguese, we use the training section of theBosque treebank.7For Spanish, we use the trainingsection of the Cast3LB treebank.8In order to mapbetween Wiktionary and the treebanks, we need acommon coarse tag set.
We thus map all data to theuniversal tag set (Petrov et al, 2012).Dev and test sets Our approach is basically pa-rameter free.
However, we did experiment with dif-ferent ways of extending Wiktionary and hence usedan average over three English Twitter dev sections asdevelopment set (Ritter et al, 2011; Gimpel et al,2011; Foster et al, 2011), all mapped and normal-ized following Hovy et al (2014).For evaluation, we use three domains: tweets,spoken data and queries.
For Twitter, we performedexperiments in four languages: English, Portuguese,Spanish and Dutch.
The Spanish and Portuguesetweets were annotated in-house, which will be madeavailable.9For the other languages, we use pre-existing datasets for English (Hovy et al, 2014) andDutch (Avontuur et al, 2012).
Table 2 lists the com-plete statistics for the different language data sets.For the other two domains, we use the manuallylabeled data from Switchboard section 4 as spokendata test set.
For queries, we use manually labeleddata from Bendersky et al (2010).3 Experiments3.1 ModelWe use a CRF10model (Lafferty et al, 2001) withthe same features as Owoputi et al (2013) and de-5LDC2011T03.6http://www.let.rug.nl/?vannoord/trees/7http://www.linguateca.pt/floresta/info_floresta_English.html8http://www.iula.upf.edu/recurs01_tbk_uk.htm9http://lowlands.ku.dk/results10https://code.google.com/p/crfpp/fault parameters.
As baselines we consider a) aCRF model trained only on newswire; b) availableoff-the-shelf systems (TOOLS); and c) a weakly su-pervised model (LI10).
For English, the off-the-shelf tagger is the Stanford tagger (Toutanova et al,2003), for the other languages we use TreeTagger(Schmid, 1994) with pre-trained models.The weakly supervised model trained is on theunannotated data.
It is a second-order HMMmodel (Mari et al, 1997; Thede and Harper, 1999)(SOHMM) using logistic regression to estimate theemission probabilities.
This method allows us to usefeature vectors rather than just word identity, as instandard HMMs.
In addition, we constrain the in-ference space of the tagger using type-level tag con-straints derived from Wiktionary.
This model, calledLI10 in Table 3, was originally proposed by Li etal.
(2012).
We extend the model by adding contin-uous word representations, induced from the unla-beled data using the skip-gram algorithm (Mikolovet al, 2013), to the feature representations.
Our lo-gistic regression model thus works over a combina-tion of discrete and continuous variables when esti-mating emission probabilities.
This extended modelis called LI10+.
For both models, we do 50 passesover the data as in Li et al (2012).4 ResultsTable 3 presents results for various models on sev-eral languages.
Our results show that our newswire-trained CRF model with target-specific Brown clus-ters already does better than all our other baselinemodels (TOOLS and weakly LI10) , with the excep-tion of QUERIES, where the Stanford tagger does re-markably well.
All improvements are statisticallysignificant (p < 0.005, calculated using approxi-mate randomization with 10k iterations).Adding the unambiguous unlabeled data leads tofurther improvements, with error reductions (overCRF) of up to 20%.
The exceptions here are Por-tuguese tweets and SPOKEN.
For SPOKEN, this isdue to the small amounts of unlabeled data, so were-used the clusters induced on Twitter, reasoningthat language use in these two domains is similarto each other.
Despite this conjecture, we see smallimprovements.
For English, Portuguese, and Span-ish TWITTER, as well as QUERIES, we see further1258TWITTER SPOKEN QUERIESEN ES PT NL EN ENNEWSWIRE 762k 93k 216k 217k 762k 762kUNLABELED 57m 9m 4.5m 0.5m 0.6m 10.1mTEST 3,064 1,524 1,593 16,725 205k 7,671words in D 380k 240k 43k 55k 380k 380k% unamb.
93% 97% 98% 94% 93% 93%unamb.
inst.
1.1m 148k 134k 10k 98k 1.5mwords in D?458k 279k 332k 129k 381k 388kunamb.
inst.
2.7m 613k 892k 55k 113k 2.3mTable 2: Characteristics of data sets used in this paperDOMAIN LANG TOOLS LI10 LI10+CRF CRF+D +CRF+D?TWITTERen 80.55 81.72 83.26 86.72 87.50 87.76es 75.66 71.40 73.20 78.48 82.74 82.87nl 84.79 74.00 80.50 89.15 89.29 89.08pt 67.17 64.90 72.50 80.04 79.16 80.10SPOKEN en 89.02 38.72 87.86 90.53 90.54 *QUERIES en 88.06 65.96 84.39 85.52 88.06 88.28Table 3: Tagging accuracies.
TOOLS are off-the-shelf taggers (Stanford and TreeTagger), LI10/LI10+the weaklysupervised models with and without embeddings, and CRF the model trained on newswire with in-domain wordclusters.
Last two columns show results when extending with unambiguous data.?
: Unlabeled data too small togenerate clusters with cut-off 100.considerable improvements by using our extendedtag dictionaries.The most obvious reason this approach shouldwork is the decrease in unseen words in the in-domain evaluation data.
Since the unambiguous datais in-domain, the out-of-vocabulary (OOV) rate goesdown when we add the unambiguous data to thenewswire training data.
In fact, for English Twit-ter, the OOV rate is reduced by half, and for Por-tuguese and Spanish, it is reduced by about 40%.For Dutch Twitter, the reduction in OOV rate ismuch smaller, which probably explains the smallgain for this dataset.
The difference in reduction ofOOV rates are due to sample biases in our unlabeleddata.
This probably also explains the difference ingains between SPEECH and QUERIES.
For searchqueries, the OOV rate is reduced by 66%, whereas itstays roughly the same for speech transcripts.5 DiscussionWe have presented a simple, yet effective approachto adapt POS taggers to a new domain.
It requires a)the availability of large amounts of unlabeled dataand b) a lexicon to mine unambiguous sentences.As sentence length increases, the likelihood of be-ing completely unambiguous drops.
For this reason,our approach works well for domains with shorteraverage sentence length, such as Twitter, spoken lan-guage, and search queries.We also experimented with allowing up to oneambiguous item per sentence, i.e., we include a sen-tence in our training data if it contains exactly oneitem that either a) has more than one licensed tagin the dictionary or b) is not in the dictionary.
Inthe first case, we choose the tag randomly at train-ing time from the set of licensed ones.
In the sec-ond case, we assume the unknown word to be aNOUN, since unknown words mostly tend to beproper names.
When added to newswire, this data1259results in worse models, presumably by introduc-ing too much noise.
However, for low-resource lan-guages or domains with longer sentences and noavailable newswire data, this might be a viable al-ternative.6 Related WorkOur approach is similar to mining high-precisionitems.
However, previous approaches on this in NLPhave mainly focused on well-defined classificationtasks, such as PP attachment (Pantel and Lin, 2000;Kawahara and Kurohashi, 2005), or discourse con-nective disambiguation (Marcu and Echihabi, 2002).In contrast, we mine for sequences of unambiguoustokens in a structured prediction task.While we use the same dictionaries as in Li et al(2012) and T?ackstr?om et al (2013), our approachdiffers in several respects.
First, we use Wiktionaryto mine for training data, rather than as type con-straints, and second, we use Brown clusters to ex-tend Wiktionary.
We did experiment with differentways of doing this, including using various formsof word embeddings, leading to models similar tothe baseline models in Socher et al (2013), but theapproach based on Brown clusters led to the best re-sults on our development data.?)
use a different approach to distant supervisionto improve tagging accuracy for Twitter.
They usehyperlinks to fetch additional un-annotated trainingdata that can be used in a self-training loop.
Ourapproach differs in that it produces annotated dataand is more readily applicable to various domains.7 ConclusionWe have presented a domain adaptation approachto POS tagging by augmenting newswire data withautomatically mined unambiguous instances.
Wedemonstrate our approach on Twitter (in several lan-guages), spoken language transcripts, and searchqueries.
We use dictionaries extended with Brownclusters to collect labeled training data from unla-beled data, saving additional annotation work.Our models perform significantly better on held-out data than both off-the-shelf taggers and mod-els trained on newswire data only.
Improvementshold across several languages (English, Spanish,Portuguese, and Dutch).
For spoken language tran-scripts and search queries, we see some improve-ments, but find that extending the dictionaries withclusters has less of an effect than for Twitter.
Ourmethod can provide a viable alternative to costly an-notation when adapting to new domains where unla-beled data and dictionaries are available.AcknowledgementsWe would like to thank the anonymous reviewers forvaluable comments and feedback, as well as ChrisBiemann for help with the Twitter data, and HectorMartinez Alonso for the annotation.
This researchis funded by the ERC Starting Grant LOWLANDSNo.
313695.ReferencesTetske Avontuur, Iris Balemans, Laura Elshof, Nanne vanNoord, and Menno van Zaanen.
2012.
Developing apart-of-speech tagger for dutch tweets.
ComputationalLinguistics in the Netherlands Journal, 2:34?51.Michael Bendersky, Bruce Croft, and David Smith.
2010.Structural annotation of search queries using pseudo-relevance feedback.
In CIKM.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.P.F.
Brown, P.V.
Desouza, R.L.
Mercer, V.J.
DellaPietra,and J.C. Lai.
1992.
Class-based n-gram models of nat-ural language.
Computational linguistics, 18(4):467?479.Hal Daume III.
2007.
Frustratingly easy domain adapta-tion.
In ACL.Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josefvan Genabith.
2011.
From news to comments: Re-sources and benchmarks for parsing the language ofWeb 2.0.
In IJCNLP.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-Speech Tagging forTwitter: Annotation, Features, and Experiments.
InACL.John J Godfrey, Edward C Holliman, and Jane McDaniel.1992.
Switchboard: Telephone speech corpus for re-search and development.
In Acoustics, Speech, andSignal Processing, 1992.
ICASSP-92., 1992 IEEE In-ternational Conference on, volume 1, pages 517?520.IEEE.1260Bo Han and Timothy Baldwin.
2011.
Lexical normalisa-tion of short text messages: Makn sens a# twitter.
InACL.Dirk Hovy, Barbara Plank, and Anders S?gaard.
2014.When pos datasets don?t add up: Combatting samplebias.
In LREC.Daisuke Kawahara and Sadao Kurohashi.
2005.
Pp-attachment disambiguation boosted by a gigantic vol-ume of unambiguous examples.
In IJCNLP, pages188?198.
Springer.Philipp Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In MT summit, volume 5,pages 79?86.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: probabilistic modelsfor segmenting and labeling sequence data.
In ICML.Shen Li, Jo?ao Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In EMNLP.B.
MacWhinney.
1997.
The CHILDES Database.
5thEdition.
Dublin, OH, Discovery Systems.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse rela-tions.
In ACL.Jean-Francois Mari, Jean-Paul Haton, and Abdelaziz Kri-ouile.
1997.
Automatic word recognition based onsecond-order hidden Markov models.
IEEE Transac-tions on Speech and Audio Processing, 5(1):22?25.Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory Cor-rado, and Jeffrey Dean.
2013.
Distributed representa-tions of words and phrases and their compositionality.In NIPS.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In ACL.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A Smith.
2013.Improved part-of-speech tagging for online conversa-tional text with word clusters.
In NAACL.Patrick Pantel and Dekang Lin.
2000.
An unsuper-vised approach to prepositional phrase attachment us-ing contextually similar words.
In ACL.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In LREC.Barbara Plank, Dirk Hovy, Ryan McDonald, and AndersS?gaard.
2014.
Adapting taggers to twitter with not-so-distant supervision.
COLING.Alan Ritter, Sam Clark, Oren Etzioni, et al 2011.
Namedentity recognition in tweets: an experimental study.
InEMNLP.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of interna-tional conference on new methods in language pro-cessing, volume 12, pages 44?49.
Manchester, UK.Richard Socher, Danqi Chen, Chris Manning, and An-drew Ng.
2013.
Reasoning with neural tensor net-works for knowledge base completion.
In NIPS.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan Mc-Donald, and Joakim Nivre.
2013.
Token and typeconstraints for cross-lingual part-of-speech tagging.TACL, 1:1?12.Scott Thede and Mary Harper.
1999.
A second-orderhidden Markov model for part-of-speech tagging.
InACL.Kristina Toutanova, Dan Klein, Christopher D Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In NAACL.1261
