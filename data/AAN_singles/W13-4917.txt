Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsOverview of the SPMRL 2013 Shared Task:Cross-Framework Evaluation of Parsing Morphologically Rich Languages?Djam?
Seddaha, Reut Tsarfatyb, Sandra K?blerc,Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriewaU.
Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU.
Paris-Diderot/INRIA, eIPsoft Inc., f,tU.
of Szeged,gDublin City U., h,iU.
of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIAAbstractThis paper reports on the first shared task onstatistical parsing of morphologically rich lan-guages (MRLs).
The task features data setsfrom nine languages, each available both inconstituency and dependency annotation.
Wereport on the preparation of the data sets, onthe proposed parsing scenarios, and on the eval-uation metrics for parsing MRLs given dif-ferent representation types.
We present andanalyze parsing results obtained by the taskparticipants, and then provide an analysis andcomparison of the parsers across languages andframeworks, reported for gold input as well asmore realistic parsing scenarios.1 IntroductionSyntactic parsing consists of automatically assigningto a natural language sentence a representation ofits grammatical structure.
Data-driven approachesto this problem, both for constituency-based anddependency-based parsing, have seen a surge of inter-est in the last two decades.
These data-driven parsingapproaches obtain state-of-the-art results on the defacto standard Wall Street Journal data set (Marcus etal., 1993) of English (Charniak, 2000; Collins, 2003;Charniak and Johnson, 2005; McDonald et al 2005;McClosky et al 2006; Petrov et al 2006; Nivre etal., 2007b; Carreras et al 2008; Finkel et al 2008;?Contact authors: djame.seddah@paris-sorbonne.fr,reut.tsarfaty@weizmann.ac.il, skuebler@indiana.eduHuang, 2008; Huang et al 2010; Zhang and Nivre,2011; Bohnet and Nivre, 2012; Shindo et al 2012),and provide a foundation on which many tasks oper-ating on semantic structure (e.g., recognizing textualentailments) or even discourse structure (coreference,summarization) crucially depend.While progress on parsing English ?
the mainlanguage of focus for the ACL community ?
has in-spired some advances on other languages, it has not,by itself, yielded high-quality parsing for other lan-guages and domains.
This holds in particular for mor-phologically rich languages (MRLs), where impor-tant information concerning the predicate-argumentstructure of sentences is expressed through word for-mation, rather than constituent-order patterns as is thecase in English and other configurational languages.MRLs express information concerning the grammati-cal function of a word and its grammatical relation toother words at the word level, via phenomena suchas inflectional affixes, pronominal clitics, and so on(Tsarfaty et al 2012c).The non-rigid tree structures and morphologicalambiguity of input words contribute to the challengesof parsing MRLs.
In addition, insufficient languageresources were shown to also contribute to parsingdifficulty (Tsarfaty et al 2010; Tsarfaty et al 2012c,and references therein).
These challenges have ini-tially been addressed by native-speaking experts us-ing strong in-domain knowledge of the linguisticphenomena and annotation idiosyncrasies to improvethe accuracy and efficiency of parsing models.
More146recently, advances in PCFG-LA parsing (Petrov et al2006) and language-agnostic data-driven dependencyparsing (McDonald et al 2005; Nivre et al 2007b)have made it possible to reach high accuracy withclassical feature engineering techniques in additionto, or instead of, language-specific knowledge.
Withthese recent advances, the time has come for estab-lishing the state of the art, and assessing strengthsand weaknesses of parsers across different MRLs.This paper reports on the first shared task on sta-tistical parsing of morphologically rich languages(the SPMRL Shared Task), organized in collabora-tion with the 4th SPMRL meeting and co-locatedwith the conference on Empirical Methods in NaturalLanguage Processing (EMNLP).
In defining and exe-cuting this shared task, we pursue several goals.
First,we wish to provide standard training and test sets forMRLs in different representation types and parsingscenarios, so that researchers can exploit them fortesting existing parsers across different MRLs.
Sec-ond, we wish to standardize the evaluation protocoland metrics on morphologically ambiguous input,an under-studied challenge, which is also present inEnglish when parsing speech data or web-based non-standard texts.
Finally, we aim to raise the awarenessof the community to the challenges of parsing MRLsand to provide a set of strong baseline results forfurther improvement.The task features data from nine, typologically di-verse, languages.
Unlike previous shared tasks onparsing, we include data in both dependency-basedand constituency-based formats, and in addition tothe full data setup (complete training data), we pro-vide a small setup (a training subset of 5,000 sen-tences).
We provide three parsing scenarios: one inwhich gold segmentation, POS tags, and morphologi-cal features are provided, one in which segmentation,POS tags, and features are automatically predictedby an external resource, and one in which we providea lattice of multiple possible morphological analysesand allow for joint disambiguation of the morpholog-ical analysis and syntactic structure.
These scenariosallow us to obtain the performance upper bound ofthe systems in lab settings using gold input, as wellas the expected level of performance in realistic pars-ing scenarios ?
where the parser follows a morpho-logical analyzer and is a part of a full-fledged NLPpipeline.The remainder of this paper is organized as follows.We first survey previous work on parsing MRLs (?2)and provide a detailed description of the present task,parsing scenarios, and evaluation metrics (?3).
Wethen describe the data sets for the nine languages(?4), present the different systems (?5), and empiri-cal results (?6).
Then, we compare the systems alongdifferent axes (?7) in order to analyze their strengthsand weaknesses.
Finally, we summarize and con-clude with challenges to address in future sharedtasks (?8).2 Background2.1 A Brief History of the SPMRL FieldStatistical parsing saw initial success upon the avail-ability of the Penn Treebank (PTB, Marcus et al1994).
With that large set of syntactically annotatedsentences at their disposal, researchers could applyadvanced statistical modeling and machine learningtechniques in order to obtain high quality structureprediction.
The first statistical parsing models weregenerative and based on treebank grammars (Char-niak, 1997; Johnson, 1998; Klein and Manning, 2003;Collins, 2003; Petrov et al 2006; McClosky et al2006), leading to high phrase-structure accuracy.Encouraged by the success of phrase-structureparsers for English, treebank grammars for additionallanguages have been developed, starting with Czech(Hajic?
et al 2000) then with treebanks of Chinese(Levy and Manning, 2003), Arabic (Maamouri etal., 2004b), German (K?bler et al 2006), French(Abeill?
et al 2003), Hebrew (Sima?an et al 2001),Italian (Corazza et al 2004), Spanish (Moreno et al2000), and more.
It quickly became apparent thatapplying the phrase-based treebank grammar tech-niques is sensitive to language and annotation prop-erties, and that these models are not easily portableacross languages and schemes.
An exception to thatis the approach by Petrov (2009), who trained latent-annotation treebank grammars and reported goodaccuracy on a range of languages.The CoNLL shared tasks on dependency parsing(Buchholz and Marsi, 2006; Nivre et al 2007a) high-lighted the usefulness of an alternative linguistic for-malism for the development of competitive parsingmodels.
Dependency relations are marked betweeninput tokens directly, and allow the annotation of147non-projective dependencies that are parseable effi-ciently.
Dependency syntax was applied to the de-scription of different types of languages (Tesni?re,1959; Mel?c?uk, 2001), which raised the hope that inthese settings, parsing MRLs will further improve.However, the 2007 shared task organizers (Nivreet al 2007a) concluded that: "[Performance] classesare more easily definable via language characteris-tics than via characteristics of the data sets.
Thesplit goes across training set size, original data for-mat [...], sentence length, percentage of unknownwords, number of dependency labels, and ratio of(C)POSTAGS and dependency labels.
The classwith the highest top scores contains languages witha rather impoverished morphology."
The problemswith parsing MRLs have thus not been solved by de-pendency parsing, but rather, the challenge has beenmagnified.The first event to focus on the particular challengesof parsing MRLs was a dedicated panel discussionco-located with IWPT 2009.1 Work presented onHebrew, Arabic, French, and German made it clearthat researchers working on non-English parsing facethe same overarching challenges: poor lexical cover-age (due to high level of inflection), poor syntacticcoverage (due to more flexible word ordering), and,more generally, issues of data sparseness (due tothe lack of large-scale resources).
Additionally, newquestions emerged as to the evaluation of parsers insuch languages ?
are the word-based metrics usedfor English well-equipped to capture performanceacross frameworks, or performance in the face ofmorphological complexity?
This event provoked ac-tive discussions and led to the establishment of aseries of SPMRL events for the discussion of sharedchallenges and cross-fertilization among researchersworking on parsing MRLs.The body of work on MRLs that was accumulatedthrough the SPMRL workshops2 and hosting ACLvenues contains new results for Arabic (Attia et al2010; Marton et al 2013a), Basque (Bengoetxeaand Gojenola, 2010), Croatian (Agic et al 2013),French (Seddah et al 2010; Candito and Seddah,2010; Sigogne et al 2011), German (Rehbein, 2011),Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and1http://alpage.inria.fr/iwpt09/panel.en.html2See http://www.spmrl.org/ and related workshops.Elhadad, 2010a), Hindi (Ambati et al 2010), Ko-rean (Chung et al 2010; Choi and Palmer, 2011) andSpanish (Le Roux et al 2012), Tamil (Green et al2012), amongst others.
The awareness of the model-ing challenges gave rise to new lines of work on top-ics such as joint morpho-syntactic processing (Gold-berg and Tsarfaty, 2008), Relational-RealizationalParsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-berg, 2011), PLCFRS parsing (Kallmeyer and Maier,2013), the use of factored lexica (Green et al 2013),the use of bilingual data (Fraser et al 2013), andmore developments that are currently under way.With new models and data, and with lingering in-terest in parsing non-standard English data, questionsbegin to emerge, such as: What is the realistic per-formance of parsing MRLs using today?s methods?How do the different models compare with one an-other?
How do different representation types dealwith parsing one particular language?
Does the suc-cess of a parsing model on a language correlate withits representation type and learning method?
How toparse effectively in the face of resource scarcity?
Thefirst step to answering all of these questions is pro-viding standard sets of comparable size, streamlinedparsing scenarios, and evaluation metrics, which areour main goals in this SPMRL shared task.2.2 Where We Are At: The Need forCross-Framework, Realistic, EvaluationProceduresThe present task serves as the first attempt to stan-dardize the data sets, parsing scenarios, and evalu-ation metrics for MRL parsing, for the purpose ofgaining insights into parsers?
performance across lan-guages.
Ours is not the first cross-linguistic task onstatistical parsing.
As mentioned earlier, two previ-ous CoNLL shared tasks focused on cross-linguisticdependency parsing and covered thirteen differentlanguages (Buchholz and Marsi, 2006; Nivre et al2007a).
However, the settings of these tasks, e.g.,in terms of data set sizes or parsing scenarios, madeit difficult to draw conclusions about strengths andweaknesses of different systems on parsing MRLs.A key aspect to consider is the relation betweeninput tokens and tree terminals.
In the standard sta-tistical parsing setup, every input token is assumedto be a terminal node in the syntactic parse tree (afterdeterministic tokenization of punctuation).
In MRLs,148morphological processes may have conjoined severalwords into a single token.
Such tokens need to be seg-mented and their analyses need to be disambiguatedin order to identify the nodes in the parse tree.
Inprevious shared tasks on statistical parsing, morpho-logical information was assumed to be known in ad-vance in order to make the setup comparable to thatof parsing English.
In realistic scenarios, however,morphological analyses are initially unknown and arepotentially highly ambiguous, so external resourcesare used to predict them.
Incorrect morphologicaldisambiguation sets a strict ceiling on the expectedperformance of parsers in real-world scenarios.
Re-sults reported for MRLs using gold morphologicalinformation are then, at best, optimistic.One reason for adopting this less-than-realisticevaluation scenario in previous tasks has been thelack of sound metrics for the more realistic scenario.Standard evaluation metrics assume that the numberof terminals in the parse hypothesis equals the num-ber of terminals in the gold tree.
When the predictedmorphological segmentation leads to a different num-ber of terminals in the gold and parse trees, standardmetrics such as ParsEval (Black et al 1991) or At-tachment Scores (Buchholz and Marsi, 2006) failto produce a score.
In this task, we use TedEval(Tsarfaty et al 2012b), a metric recently suggestedfor joint morpho-syntactic evaluation, in which nor-malized tree-edit distance (Bille, 2005) on morpho-syntactic trees allows us to quantify the success onthe joint task in realistic parsing scenarios.Finally, the previous tasks focused on dependencyparsing.
When providing both constituency-basedand dependency-based tracks, it is interesting to com-pare results across these frameworks so as to betterunderstand the differences in performance betweenparsers of different types.
We are now faced withan additional question: how can we compare pars-ing results across different frameworks?
Adoptingstandard metrics will not suffice as we would be com-paring apples and oranges.
In contrast, TedEval isdefined for both phrase structures and dependencystructures through the use of an intermediate repre-sentation called function trees (Tsarfaty et al 2011;Tsarfaty et al 2012a).
Using TedEval thus allows usto explore both dependency and constituency parsingframeworks and meaningfully compare the perfor-mance of parsers of different types.3 Defining the Shared-Task3.1 Input and OutputWe define a parser as a structure prediction functionthat maps sequences of space-delimited input tokens(henceforth, tokens) in a language to a set of parsetrees that capture valid morpho-syntactic structuresin that language.
In the case of constituency parsing,the output structures are phrase-structure trees.
In de-pendency parsing, the output consists of dependencytrees.
We use the term tree terminals to refer to theleaves of a phrase-structure tree in the former caseand to the nodes of a dependency tree in the latter.We assume that input sentences are representedas sequences of tokens.
In general, there may be amany-to-many relation between input tokens and treeterminals.
Tokens may be identical to the terminals,as is often the case in English.
A token may bemapped to multiple terminals assigned their own POStags (consider, e.g., the token ?isn?t?
), as is the casein some MRLs.
Several tokens may be grouped intoa single (virtual) node, as is the case with multiwordexpressions (MWEs) (consider ?pomme de terre?
for?potatoe?).
This task covers all these cases.In the standard setup, all tokens are tree terminals.Here, the task of a parser is to predict a syntacticanalysis in which the tree terminals coincide with thetokens.
Disambiguating the morphological analysesthat are required for parsing corresponds to selectingthe correct POS tag and possibly a set of morpho-logical features for each terminal.
For the languagesBasque, French, German, Hungarian, Korean, Polish,and Swedish, we assume this standard setup.In the morphologically complex setup, every tokenmay be composed of multiple terminals.
In this case,the task of the parser is to predict the sequence of treeterminals, their POS tags, and a correct tree associ-ated with this sequence of terminals.
Disambiguatingthe morphological analysis therefore requires split-ting the tokens into segments that define the terminals.For the Semitic languages Arabic and Hebrew, weassume this morphologically complex setup.In the multiword expression (MWEs) setup, pro-vided here for French only, groupings of terminalsare identified as MWEs (non-terminal nodes in con-stituency trees, marked heads in dependency trees).Here, the parser is required to predict how terminalsare grouped into MWEs on top of predicting the tree.1493.2 Data SetsThe task features nine languages from six languagefamilies, from Germanic languages (Swedish andGerman) and Romance (French) to Slavic (Polish),Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic(Hungarian), and the language isolate Basque.These languages cover a wide range of morpho-logical richness, with Arabic, Basque, and Hebrewexhibiting a high degree of inflectional and deriva-tional morphology.
The Germanic languages, Ger-man and Swedish, have greater degrees of phrasalordering freedom than English.
While French is notstandardly classified as an MRL, it shares MRLs char-acteristics which pose challenges for parsing, such asa richer inflectional system than English.For each contributing language, we provide twosets of annotated sentences: one annotated with la-beled phrase-structure trees, and one annotated withlabeled dependency trees.
The sentences in the tworepresentations are aligned at token and POS levels.Both representations reflect the predicate-argumentstructure of the same sentence, but this informationis expressed using different formal terms and thusresults in different tree structures.Since some of our native data sets are larger thanothers, we provide the training set in two sizes: Fullcontaining all sentences in the standard training setof the language, and 5k containing the number ofsentences that is equivalent in size to our smallesttraining set (5k sentences).
For all languages, the datahas been split into sentences, and the sentences areparsed and evaluated independently of one another.3.3 Parsing ScenariosIn the shared task, we consider three parsing scenar-ios, depending on how much of the morphologicalinformation is provided.
The scenarios are listedbelow, in increasing order of difficulty.?
Gold: In this scenario, the parser is providedwith unambiguous gold morphological segmen-tation, POS tags, and morphological features foreach input token.?
Predicted: In this scenario, the parser is pro-vided with disambiguated morphological seg-mentation.
However, the POS tags and mor-phological features for each input segment areunknown.Scenario Segmentation PoS+Feat.
TreeGold X X ?Predicted X 1-best ?Raw (1-best) 1-best 1-best ?Raw (all) ?
?
?Table 1: A summary of the parsing and evaluation sce-narios.
X depicts gold information, ?
depicts unknowninformation, to be predicted by the system.?
Raw: In this scenario, the parser is providedwith morphologically ambiguous input.
Themorphological segmentation, POS tags, andmorphological features for each input token areunknown.The Predicted and Raw scenarios require predict-ing morphological analyses.
This may be done usinga language-specific morphological analyzer, or it maybe done jointly with parsing.
We provide inputs thatsupport these different scenarios:?
Predicted: Gold treebank segmentation is givento the parser.
The POS tags assignment and mor-phological features are automatically predictedby the parser or by an external resource.?
Raw (1-best): The 1st-best segmentation andPOS tags assignment is predicted by an externalresource and given to the parser.?
Raw (all): All possible segmentations and POStags are specified by an external resource.
Theparser selects jointly a segmentation and a tree.An overview of all shown in table 1.
For languagesin which terminals equal tokens, only Gold and Pre-dicted scenarios are considered.
For Semitic lan-guages we further provide input for both Raw (1-best) and Raw (all) scenarios.
33.4 Evaluation MetricsThis task features nine languages, two different repre-sentation types and three different evaluation scenar-ios.
In order to evaluate the quality of the predictedstructures in the different tracks, we use a combina-tion of evaluation metrics that allow us to comparethe systems along different axes.3The raw Arabic lattices were made available later than theother data.
They are now included in the shared task release.150In this section, we formally define the differentevaluation metrics and discuss how they support sys-tem comparison.
Throughout this paper, we will bereferring to different evaluation dimensions:?
Cross-Parser Evaluation in Gold/PredictedScenarios.
Here, we evaluate the results of dif-ferent parsers on a single data set in the Goldor Predicted setting.
We use standard evalu-ation metrics for the different types of anal-yses, that is, ParsEval (Black et al 1991)on phrase-structure trees, and Labeled At-tachment Scores (LAS) (Buchholz and Marsi,2006) for dependency trees.
Since ParsEval isknown to be sensitive to the size and depth oftrees (Rehbein and van Genabith, 2007b), wealso provide the Leaf-Ancestor metric (Samp-son and Babarczy, 2003), which is less sensitiveto the depth of the phrase-structure hierarchy.
Inboth scenarios we also provide metrics to evalu-ate the prediction of MultiWord Expressions.?
Cross-Parser Evaluation in Raw Scenarios.Here, we evaluate the results of different parserson a single data set in scenarios where morpho-logical segmentation is not known in advance.When a hypothesized segmentation is not iden-tical to the gold segmentation, standard evalua-tion metrics such as ParsEval and AttachmentScores break down.
Therefore, we use TedEval(Tsarfaty et al 2012b), which jointly assessesthe quality of the morphological and syntacticanalysis in morphologically-complex scenarios.?
Cross-Framework Evaluation.
Here, we com-pare the results obtained by a dependency parserand a constituency parser on the same set of sen-tences.
In order to avoid comparing apples andoranges, we use the unlabeled TedEval metric,which converts all representation types inter-nally into the same kind of structures, calledfunction trees.
Here we use TedEval?s cross-framework protocol (Tsarfaty et al 2012a),which accomodates annotation idiosyncrasies.?
Cross-Language Evaluation.
Here, we com-pare parsers for the same representation typeacross different languages.
Conducting a com-plete and faithful evaluation across languageswould require a harmonized universal annota-tion scheme (possibly along the lines of (deMarneffe and Manning, 2008; McDonald et al2013; Tsarfaty, 2013)) or task based evaluation.As an approximation we use unlabeled TedEval.Since it is unlabeled, it is not sensitive to labelset size.
Since it internally uses function-trees,it is less sensitive to annotation idiosyncrasies(e.g., head choice) (Tsarfaty et al 2011).The former two dimensions are evaluated on the fullsets.
The latter two are evaluated on smaller, compa-rable, test sets.
For completeness, we provide belowthe formal definitions and essential modifications ofthe evaluation software that we used.3.4.1 Evaluation Metrics for Phrase StructuresParsEval The ParsEval metrics (Black et al 1991)are evaluation metrics for phrase-structure trees.
De-spite various shortcomings, they are the de-facto stan-dard for system comparison on phrase-structure pars-ing, used in many campaigns and shared tasks (e.g.,(K?bler, 2008; Petrov and McDonald, 2012)).
As-sume that G and H are phrase-structure gold andhypothesized trees respectively, each of which is rep-resented by a set of tuples (i, A, j) where A is alabeled constituent spanning from i to j. Assumethat g is the same as G except that it discards theroot, preterminal, and terminal nodes, likewise for hand H .
The ParsEval scores define the accuracy ofthe hypothesis in terms of the normalized size of theintersection of the constituent sets.Precision(g, h) = |g?h||h|Recall(g, h) = |g?h||g|F1(g, h) = 2?P?RP+RWe evaluate accuracy on phrase-labels ignoring anyfurther decoration, as it is in standard practices.Evalb, the standard software that implements Par-sEval,4 takes a parameter file and ignores the labelsspecified therein.
As usual, we ignore root and POSlabels.
Contrary to the standard practice, we do takepunctuation into account.
Note that, as opposed to theofficial version, we used the SANCL?2012 version5modified to actually penalize non-parsed trees.4http://www.spmrl.org/spmrl2013-sharedtask-metrics.html/#Evalb5Modified by Petrov and McDonald (2012) to be less sensi-tive to punctuation errors.151Leaf-Ancestor The Leaf-Ancestor metric (Samp-son and Babarczy, 2003) measures the similarity be-tween the path from each terminal node to the rootnode in the output tree and the corresponding pathin the gold tree.
The path consists of a sequence ofnode labels between the terminal node and the rootnode, and the similarity of two paths is calculatedby using the Levenshtein distance.
This distance isnormalized by path length, and the score of the treeis an aggregated score of the values for all terminalsin the tree (xt is the leaf-ancestor path of t in tree x).LA(h, g) =?t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))|yield(g)|This metric was shown to be less sensitive to dif-ferences between annotation schemes in (K?bler etal., 2008), and was shown by Rehbein and van Gen-abith (2007a) to evaluate trees more faithfully thanParsEval in the face of certain annotation decisions.We used the implementation of Wagner (2012).63.4.2 Evaluation Metrics for DependencyStructuresAttachment Scores Labeled and Unlabeled At-tachment scores have been proposed as evaluationmetrics for dependency parsing in the CoNLL sharedtasks (Buchholz and Marsi, 2006; Nivre et al 2007a)and have since assumed the role of standard metricsin multiple shared tasks and independent studies.
As-sume that g, h are gold and hypothesized dependencytrees respectively, each of which is represented bya set of arcs (i, A, j) where A is a labeled arc fromterminal i to terminal j.
Recall that in the gold andpredicted settings, |g| = |h| (because the number ofterminals determines the number of arcs and hence itis fixed).
So Labeled Attachment Score equals preci-sion and recall, and it is calculated as a normalizedsize of the intersection between the sets of gold andparsed arcs.7Precision(g, h) = |g?h||g|Recall(g, h) = |g?h||h|LAS(g, h) = |g?h||g| =|g?h||h|6The original version is available athttp://www.grsampson.net/Resources.html, ours at http://www.spmrl.org/spmrl2013-sharedtask-metrics.html/#Leaf.7http://ilk.uvt.nl/conll/software.html.3.4.3 Evaluation Metrics for Morpho-SyntacticStructuresTedEval The TedEval metrics and protocols havebeen developed by Tsarfaty et al(2011), Tsarfatyet al(2012a) and Tsarfaty et al(2012b) for copingwith non-trivial evaluation scenarios, e.g., comparingparsing results across different frameworks, acrossrepresentation theories, and across different morpho-logical segmentation hypotheses.8 Contrary to theprevious metrics, which view accuracy as a normal-ized intersection over sets, TedEval computes the ac-curacy of a parse tree based on the tree-edit distancebetween complete trees.
Assume a finite set of (pos-sibly parameterized) edit operations A = {a1....an},and a cost function c : A ?
1.
An edit script is thecost of a sequence of edit operations, and the edit dis-tance of g, h is the minimal cost edit script that turnsg into h (and vice versa).
The normalized distancesubtracted from 1 provides the level of accuracy onthe task.
Formally, the TedEval score on g, h is de-fined as follows, where ted is the tree-edit distance,and the |x| (size in nodes) discards terminals and rootnodes.TedEval(g, h) = 1?ted(g, h)|g|+ |h|In the gold scenario, we are not allowed to manipu-late terminal nodes, only non-terminals.
In the rawscenarios, we can add and delete both terminals andnon-terminals so as to match both the morphologicaland syntactic hypotheses.3.4.4 Evaluation Metrics forMultiword-Expression IdentificationAs pointed out in section 3.1, the French data set isprovided with tree structures encoding both syntacticinformation and groupings of terminals into MWEs.A given MWE is defined as a continuous sequence ofterminals, plus a POS tag.
In the constituency trees,the POS tag of the MWE is an internal node of thetree, dominating the sequence of pre-terminals, eachdominating a terminal.
In the dependency trees, thereis no specific node for the MWE as such (the nodesare the terminals).
So, the first token of a MWE istaken as the head of the other tokens of the sameMWE, with the same label (see section 4.4).8http://www.tsarfaty.com/unipar/download.html.152To evaluate performance on MWEs, we use thefollowing metrics.?
R_MWE, P_MWE, and F_MWE are recall, pre-cision, and F-score over full MWEs, in whicha predicted MWE counts as correct if it has thecorrect span (same group as in the gold data).?
R_MWE +POS, R_MWE +POS, and F_MWE+POS are defined in the same fashion, exceptthat a predicted MWE counts as correct if it hasboth correct span and correct POS tag.?
R_COMP, R_COMP, and F_COMP are recall,precision and F-score over non-head compo-nents of MWEs: a non-head component of MWEcounts as correct if it is attached to the head ofthe MWE, with the specific label that indicatesthat it is part of an MWE.4 The SPMRL 2013 Data Sets4.1 The TreebanksWe provide data from nine different languages anno-tated with two representation types: phrase-structuretrees and dependency trees.9 Statistics about size,average length, label set size, and other character-istics of the treebanks and schemes are provided inTable 2.
Phrase structures are provided in an ex-tended bracketed style, that is, Penn Treebank brack-eted style where every labeled node may be extendedwith morphological features expressed.
Dependencystructures are provided in the CoNLL-X format.10For any given language, the dependency and con-stituency treebanks are aligned at the token and ter-minal levels and share the same POS tagset and mor-phological features.
That is, any form in the CoNLLformat is a terminal of the respective bracketed tree.Any CPOS label in the CoNLL format is the pre-terminal dominating the terminal in the bracketedtree.
The FEATS in the CoNLL format are repre-sented as dash-features decorated on the respectivepre-terminal node in the bracketed tree.
See Fig-ure 1(a)?1(b) for an illustration of this alignment.9Additionally, we provided the data in TigerXML format(Brants et al 2002) for phrase structure trees containing cross-ing branches.
This allows the use of more powerful parsingformalisms.
Unfortunately, we received no submissions for thisdata, hence we discard them in the rest of this overview.10See http://ilk.uvt.nl/conll/.For ambiguous morphological analyses, we pro-vide the mapping of tokens to different segmentationpossibilities through lattice files.
See Figure 1(c) foran illustration, where lattice indices mark the startand end positions of terminals.For each of the treebanks, we provide a three-waydev/train/set split and another train set containing thefirst 5k sentences of train (5k).
This section providesthe details of the original treebanks and their anno-tations, our data-set preparation, including prepro-cessing and data splits, cross-framework alignment,and the prediction of morphological information innon-gold scenarios.4.2 The Arabic TreebanksArabic is a morphologically complex language whichhas rich inflectional and derivational morphology.
Itexhibits a high degree of morphological ambiguitydue to the absence of the diacritics and inconsistentspelling of letters, such as Alif and Ya.
As a conse-quence, the Buckwalter Standard Arabic Morpholog-ical Analyzer (Buckwalter, 2004; Graff et al 2009)produces an average of 12 analyses per word.Data Sets The Arabic data set contains two tree-banks derived from the LDC Penn Arabic Treebanks(PATB) (Maamouri et al 2004b):11 the ColumbiaArabic Treebank (CATiB) (Habash and Roth, 2009),a dependency treebank, and the Stanford versionof the PATB (Green and Manning, 2010), a phrase-structure treebank.
We preprocessed the treebanksto obtain strict token matching between the treebanksand the morphological analyses.
This required non-trivial synchronization at the tree token level betweenthe PATB treebank, the CATiB treebank and the mor-phologically predicted data, using the PATB sourcetokens and CATiB feature word form as a dual syn-chronized pivot.The Columbia Arabic Treebank The ColumbiaArabic Treebank (CATiB) uses a dependency repre-sentation that is based on traditional Arabic grammarand that emphasizes syntactic case relations (Habashand Roth, 2009; Habash et al 2007).
The CATiBtreebank uses the word tokenization of the PATB11The LDC kindly provided their latest version of the ArabicTreebanks.
In particular, we used PATB 1 v4.1 (Maamouri et al2005), PATB 2 v3.1 (Maamouri et al 2004a) and PATB 3 v3.3.
(Maamouri et al 2009)153Arabic Basque French German Hebrew Hungarian Korean Polish Swedishtrain:#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424Lex.
Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911Avg.
Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84#Non Terminals 22 12 32 25 16 8 34#POS tags 35 25 29 54 16 1,975 29#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792Dep.
Label Set Size 9 31 25 43 417 22 27train5k:#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357Lex.
Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110Avg.
Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96#Non Terminals 22 12 29 23 60 16 8 34 8#POS Tags 35 25 29 51 50 16 972 29 25#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845Dep.
Label Set Size 9 31 25 42 43 349 20 27 61dev:#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341Lex.
Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690Avg.
Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10#Non Terminals 21 11 27 24 55 16 8 31 8#POS Tags 32 23 29 50 47 16 760 29 24#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496Dep.
Label Set Size 9 31 25 41 42 210 22 26 59test:#Sents 1959 946 2541 5000 716 1009 2287 822 666#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690Lex.
Size 12254 4685 10048 20149 4305 7856 16475 4336 3112Avg.
Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18#Non Terminals 22 12 30 23 54 15 8 31 8#POS Tags 33 22 30 52 46 16 809 27 25#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118Dep.
Label Set Size 9 31 26 42 41 183 22 27 56Table 2: Overview of participating languages and treebank properties.
?Sents?
= number of sentences, ?Tokens?
=number of raw surface forms.
?Lex.
size?
and ?Avg.
Length?
are computed in terms of tagged terminals.
?NT?
= non-terminals in constituency treebanks, ?Dep Labels?
= dependency labels on the arcs of dependency treebanks.
?
A morecomprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.154(a) Constituency Tree% % every line is a single tree in a bracketed Penn Treebank format(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))(b) Dependency Tree%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel1 John John NNP NNP pers=3|num=sing 2 sbj _ _2 likes like VB VB pers=3|num=sing 0 root _ _3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _Input Lattice0 1 2 3 4 5 61:AIF/NN1:AIF/VB1:AIF/NNT2:LA/RB3:NISH/VB3:NISH/NN4:L/PREP4:LHSTIR/VB4:HSTIR/VB5:ZAT/PRP%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id0 1 AIF AIF NN NN _ 10 1 AIF AIF NNT NNT _ 10 1 AIF AIF VB VB _ 11 2 LA LA RB RB _ 22 3 NISH NISH VB VB _ 32 3 NISH NISH NN NN _ 33 5 LHSTIR HSTIR VB VB _ 43 4 L L PREP PREP _ 44 5 HSTIR HSTIR VB VB _ 45 6 ZAT ZAT PRP PRP _ 5Figure 1: File formats.
Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.Boxed labels are shared across the treebanks.
Figure (c) shows an ambiguous lattice.
The red part represents the yieldof the gold tree.
For brevity, we use empty feature columns, but of course lattice arcs may carry any morphologicalfeatures, in the FEATS CoNLL format.and employs a reduced POS tagset consisting of sixtags only: NOM (non-proper nominals includingnouns, pronouns, adjectives and adverbs), PROP(proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such asprepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the BuckwalterArabic tagset (PATB official tagset) which is almost500 tags.)
To obtain these dependency trees, we usedthe constituent-to-dependency tool (Habash and Roth,2009).
Additional CATiB trees were annotated di-rectly, but we only use the portions that are convertedfrom phrase-structure representation, to ensure thatthe constituent and dependency yields can be aligned.The Stanford Arabic Phrase Structure TreebankIn order to stay compatible with the state of the art,we provide the constituency data set with most of thepre-processing steps of Green and Manning (2010),as they were shown to improve baseline performanceon the PATB parsing considerably.12To convert the original PATB to preprocessedphrase-structure trees ?
la Stanford, we first discardall trees dominated by X, which indicates errors andnon-linguistic text.
At the phrasal level, we collapseunary chains with identical categories like NP?
NP.We finally remove all traces, but, unlike Green andManning (2010), we keep all function tags.In the original Stanford instance, the pre-terminalmorphological analyses were mapped to the short-ened Bies tag set provided with the treebank (whereDeterminer markers, ?DT?, were added to definitenoun and adjectives, resulting in 32 POS tags).
Herewe use the Kulick tagset (Kulick et al 2006) for12Both the corpus split and pre-processing code are availablewith the Stanford parser at http://nlp.stanford.edu/projects/arabic.shtml.155pre-terminal categories in the phrase-structure trees,where the Bies tag set is included as a morphologicalfeature (stanpos) in our PATB instance.Adapting the Data to the Shared Task We con-verted the CATiB representation to the CoNLL rep-resentation and added a ?split-from-previous?
and?split-from-next?
markers as in LDC?s tree-terminalfields.A major difference between the CATiB treebankand the Stanford treebank lies in the way they han-dle paragraph annotations.
The original PATB con-tains sequences of annotated trees that belong to asame discourse unit (e.g., paragraph).
While theCATiB conversion tool considers each sequence asingle parsing unit, the Stanford pre-processor treatseach such tree structure rooted at S, NP or Frag asa tree spanning a single sentence.
To be compati-ble with the predicted morphology data which wasbootstrapped and trained on the CATiB interpretation,we deterministically modified the original PATB byadding pseudo XP root nodes, so that the Stanfordpre-proprecessor will generate the same tree yieldsas the CATiB treebank.Another important aspect of preprocessing (often-delegated as a technicality in the Arabic parsing lit-erature) is the normalization of token forms.
MostArabic parsing work used transliterated text based onthe schemes proposed by Buckwalter (2002).
Thetransliteration schemes exhibit some small differ-ences, but enough to increase the out-of-vocabularyrate by a significant margin (on top of strictly un-known morphemes).
This phenomenon is evident inthe morphological analysis lattices (in the predicteddev set there is a 6% OOV rate without normalization,and half a point reduction after normalization is ap-plied, see (Habash et al 2009b; Green and Manning,2010)).
This rate is much lower for gold tokenizedpredicted data (with an OOV rate of only 3.66%,similar to French for example).
In our data set, alltokens are minimally normalized: no diacritics, nonormalization.13Data Splits For the Arabic treebanks, we use thedata split recommended by the Columbia Arabic andDialect Modeling (CADiM) group (Diab et al 2013).13Except for the minimal normalization present in MADA?sback-end tools.
This script was provided to the participants.The data of the LDC first three annotated Arabic Tree-banks (ATB1, ATB2 and ATB3) were divided intoroughly a 10/80/10% dev/train/test split by word vol-ume.
When dividing the corpora, document bound-aries were maintained.
The train5k files are simplythe first 5,000 sentences of the training files.POS Tagsets Given the richness of Arabic mor-phology, there are multiple POS tag sets and tokeniza-tion schemes that have been used by researchers, (see,e.g., Marton et al(2013a)).
In the shared task, we fol-low the standard PATB tokenization which splits offseveral categories of orthographic clitics, but not thedefinite article Al+.
On top of that, we consider threedifferent POS tag sets with different degrees of gran-ularity: the Buckwalter tag set (Buckwalter, 2004),the Kulick Reduced Tag set (Kulick et al 2006), andthe CATiB tag set (Habash et al 2009a), consideringthat granularity of the morphological analyses mayaffect syntactic processing.
For more information seeHabash (2010).Predicted Morphology To prepare input for theRaw scenarios (?3.3), we used the MADA+TOKANsystem (Habash et al 2009b).
MADA is a systemfor morphological analysis and disambiguation ofArabic.
It can predict the 1-best tokenization, POStags, lemmas and diacritization in one fell swoop.The MADA output was also used to generate thelattice files for the Raw-all scenario.To generate input for the gold token / predictedtag input scenario, we used Morfette (Chrupa?a et al2008), a joint lemmatization and POS tagging modelbased on an averaged perceptron.
We generated twotagging models, one trained with the Buckwalter tagset, and the other with the Kulick tag set.
Both weremapped back to the CATiB POS tag set such that allpredicted tags are contained in the feature field.144.3 The Basque TreebankBasque is an agglutinative language with a high ca-pacity to generate inflected wordforms, with freeconstituent order of sentence elements with respectto the main verb.
Contrary to many other treebanks,the Basque treebank was originally annotated withdependency trees, which were later on converted toconstituency trees.14A conversion script from the rich Buckwalter tagset toCoNLL-like features was provided to the participants.156The Basque Dependency Treebank (BDT) is adependency treebank in its original design, due tosyntactic characteristics of Basque such as its freeword order.
Before the syntactic annotation, mor-phological analysis was performed, using the Basquemorphological analyzer of Aduriz et al(2000).
InBasque each lemma can generate thousands of word-forms ?
differing in morphological properties suchas case, number, tense, or different types of subordi-nation for verbs.
If only POS category ambiguity isresolved, the analyses remain highly ambiguous.For the main POS category, there is an average of1.55 interpretations per wordform, which rises to 2.65for the full morpho-syntactic information, resultingin an overall 64% of ambiguous wordforms.
Thecorrect analysis was then manually chosen.The syntactic trees were manually assigned.
Eachword contains its lemma, main POS category, POSsubcategory, morphological features, and the la-beled dependency relation.
Each form indicates mor-phosyntactic features such as case, number and typeof subordination, which are relevant for parsing.The first version of the Basque Dependency Tree-bank, consisting of 3,700 sentences (Aduriz et al2003), was used in the CoNLL 2007 Shared Task onDependency Parsing (Nivre et al 2007a).
The cur-rent shared task uses the second version of the BDT,which is the result of an extension and redesign of theoriginal requirements, containing 11,225 sentences(150,000 tokens).The Basque Constituency Treebank (BCT) wascreated as part of the CESS-ECE project, where themain aim was to obtain syntactically annotated con-stituency treebanks for Catalan, Spanish and Basqueusing a common set of syntactic categories.
BCTwas semi-automatically derived from the dependencyversion (Aldezabal et al 2008).
The conversion pro-duced complete constituency trees for 80% of thesentences.
The main bottlenecks have been sentenceconnectors and non-projective dependencies whichcould not be straightforwardly converted into projec-tive tree structures, requiring a mechanism similar totraces in the Penn English Treebank.Adapting the Data to the Shared Task As theBCT did not contain all of the original non-projectivedependency trees, we selected the set of 8,000 match-ing sentences in both treebanks for the shared task.15This implies that around 2k trees could not be gen-erated and therefore were discarded.
Furthermore,the BCT annotation scheme does not contain attach-ment for most of the punctuation marks, so thosewere inserted into the BCT using a simple lower-leftattachment heuristic.
The same goes for some con-nectors that could not be aligned in the first phase.Predicted Morphology In order to obtain pre-dicted tags for the non-gold scenarios, we used thefollowing pipeline.
First, morphological analysis asdescribed above was performed, followed by a dis-ambiguation step.
At that point, it is hard to obtain asingle interpretation for each wordform, as determin-ing the correct interpretation for each wordform mayrequire knowledge of long-distance elements on topof the free constituency order of the main phrasal el-ements in Basque.
The disambiguation is performedby the module by Ezeiza et al(1998), which usesa combination of knowledge-based disambiguation,by means of Constraint Grammar (Karlsson et al1995; Aduriz et al 1997), and a posterior statisticaldisambiguation module, using an HMM.16For the shared task data, we chose a setting thatdisambiguates most word forms, and retains ?
97%of the correct interpretations, leaving an ambiguitylevel of 1.3 interpretations.
For the remaining casesof ambiguity, we chose the first interpretation, whichcorresponds to the most frequent option.
This leavesopen the investigation of more complex approachesfor selecting the most appropriate reading.174.4 The French TreebankFrench is not a morphologically rich language per se,though its inflectional system is richer than that ofEnglish, and it also exhibits a limited amount of wordorder variation occurring at different syntactic levelsincluding the word level (e.g.
pre- or post-nominal15We generated a 80/10/10 split, ?
train/dev/test ?
The first 5ksentences of the train set were used as a basis for the train5k.16Note that the statistical module can be parametrized accord-ing to the level of disambiguation to trade off precision andrecall.
For example, disambiguation based on the main cate-gories (abstracting over morpho-syntactic features) maintainsmost of the correct interpretations but still gives an output withseveral interpretations per wordform.17This is not an easy task.
The ambiguity left is the hardest tosolve given that the knowledge-based and statistical disambigua-tion processes have not been able to pick out a single reading.157adjective, pre- or post-verbal adverbs) and the phraselevel (e.g.
possible alternations between post verbalNPs and PPs).
It also has a high degree of multi-word expressions, that are often ambiguous with aliteral reading as a sequence of simple words.
Thesyntactic and MWE analysis shows the same kind ofinteraction (though to a lesser extent) as morphologi-cal and syntactic interaction in Semitic languages ?MWEs help parsing, and syntactic information maybe required to disambiguate MWE identification.The Data Set The French data sets were gener-ated from the French Treebank (Abeill?
et al 2003),which consists of sentences from the newspaper LeMonde, manually annotated with phrase structuresand morphological information.
Part of the treebanktrees are also annotated with grammatical functiontags for dependents of verbs.
In the SPMRL sharedtask release, we used only this part, consisting of18,535 sentences,18 split into 14,759 sentences fortraining, 1,235 sentences for development, and 2,541sentences for the final evaluation.19Adapting the Data to the Shared Task The con-stituency trees are provided in an extended PTBbracketed format, with morphological features at thepre-terminal level only.
They contain slight, auto-matically performed, modifications with respect tothe original trees of the French treebank.
The syntag-matic projection of prepositions and complementiz-ers was normalized, in order to have prepositions andcomplementizers as heads in the dependency trees(Candito et al 2010).The dependency representations are projective de-pendency trees, obtained through automatic conver-sion from the constituency trees.
The conversion pro-cedure is an enhanced version of the one describedby Candito et al(2010).Both the constituency and the dependency repre-sentations make use of coarse- and fine-grained POStags (CPOS and FPOS respectively).
The CPOS arethe categories from the original treebank.
The FPOS18The process of functional annotation is still ongoing, theobjective of the FTB providers being to have all the 20000 sen-tences annotated with functional tags.19The first 9,981 training sentences correspond to the canoni-cal 2007 training set.
The development set is the same and thelast 1235 sentences of the test set are those of the canonical testset.are merged using the CPOS and specific morphologi-cal information such as verbal mood, proper/commonnoun distinction (Crabb?
and Candito, 2008).Multi-Word Expressions The main differencewith respect to previous releases of the bracketedor dependency versions of the French treebanklies in the representation of multi-word expressions(MWEs).
The MWEs appear in an extended format:each MWE bears an FPOS20 and consists of a se-quence of terminals (hereafter the ?components?
ofthe MWE), each having their proper CPOS, FPOS,lemma and morphological features.
Note though thatin the original treebank the only gold informationprovided for a MWE component is its CPOS.
Sinceleaving this information blank for MWE componentswould have provided a strong cue for MWE recog-nition, we made sure to provide the same kind ofinformation for every terminal, whether MWE com-ponent or not, by providing predicted morphologicalfeatures, lemma, and FPOS for MWE components(even in the ?gold?
section of the data set).
This infor-mation was predicted by the Morfette tool (Chrupa?aet al 2008), adapted to French (Seddah et al 2010).In the constituency trees, each MWE correspondsto an internal node whose label is the MWE?s FPOSsuffixed by a +, and which dominates the componentpre-terminal nodes.In the dependency trees, there is no ?node?
for aMWE as a whole, but one node (a terminal in theCoNLL format) per MWE component.
The first com-ponent of a MWE is taken as the head of the MWE.All subsequent components of the MWE depend onthe first one, with the special label dep_cpd.
Further-more, the first MWE component bears a feature mwe-head equal to the FPOS of the MWE.
For instance,the MWE la veille (the day before) is an adverb, con-taining a determiner component and a common nouncomponent.
Its bracketed representation is (ADV+(DET la) (NC veille)), and in the dependency repre-sentation, the noun veille depends on the determinerla, which bears the feature mwehead=ADV+.Predicted Morphology For the predicted mor-phology scenario, we provide data in which themwehead has been removed and with predicted20In the current data, we did not carry along the lemma andmorphological features pertaining to the MWE itself, though thisinformation is present in the original trees.158FPOS, CPOS, lemma, and morphological features,obtained by training Morfette on the whole train set.4.5 The German TreebankGerman is a fusional language with moderately freeword order, in which verbal elements are fixed inplace and non-verbal elements can be ordered freelyas long as they fulfill the ordering requirements ofthe clause (H?hle, 1986).The Data Set The German constituency data setis based on the TiGer treebank release 2.2.21 Theoriginal annotation scheme represents discontinuousconstituents such that all arguments of a predicateare always grouped under a single node regardless ofwhether there is intervening material between themor not (Brants et al 2002).
Furthermore, punctua-tion and several other elements, such as parentheses,are not attached to the tree.
In order to make theconstituency treebank usable for PCFG parsing, weadapted this treebank as described shortly.The conversion of TiGer into dependencies is avariant of the one by Seeker and Kuhn (2012), whichdoes not contain empty nodes.
It is based on the sameTiGer release as the one used for the constituencydata.
Punctuation was attached as high as possible,without creating any new non-projective edges.Adapting the Data to the Shared Task Forthe constituency version, punctuation and otherunattached elements were first attached to the tree.As attachment target, we used roughly the respec-tive least common ancestor node of the right andleft terminal neighbor of the unattached element (seeMaier et al(2012) for details), and subsequently, thecrossing branches were resolved.This was done in three steps.
In the first step, thehead daughters of all nodes were marked using asimple heuristic.
In case there was a daughter withthe edge label HD, this daughter was marked, i.e.,existing head markings were honored.
Otherwise, ifexisting, the rightmost daughter with edge label NK(noun kernel) was marked.
Otherwise, as default, theleftmost daughter was marked.
In a second step, foreach continuous part of a discontinuous constituent,a separate node was introduced.
This corresponds21This version is available from http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/tiger.htmlto the "raising" algorithm described by Boyd (2007).In a third steps, all those newly introduced nodesthat did not cover the head daughter of the originaldiscontinuous node were deleted.
For the secondand the third step, we used the same script as for theSwedish constituency data.Predicted Morphology For the predicted scenario,a single sequence of POS tags and morphologi-cal features has been assigned using the MATEtoolchain via a model trained on the train set via cross-validation on the training set.
The MATE toolchainwas used to provide predicted annotation for lem-mas, POS tags, morphology, and syntax.
In order toachieve the best results for each annotation level, a10-fold jackknifing was performed to provide realis-tic features for the higher annotation levels.
The pre-dicted annotation of the 5k training set were copiedfrom the full data set.224.6 The Hebrew TreebankModern Hebrew is a Semitic language, characterizedby inflectional and derivational (templatic) morphol-ogy and relatively free word order.
The functionwords for from/to/like/and/when/that/the are prefixedto the next token, causing severe segmentation ambi-guity for many tokens.
In addition, Hebrew orthogra-phy does not indicate vowels in modern texts, leadingto a very high level of word-form ambiguity.The Data Set Both the constituency and the de-pendency data sets are derived from the HebrewTreebank V2 (Sima?an et al 2001; Guthmann etal., 2009).
The treebank is based on just over 6000sentences from the daily newspaper ?Ha?aretz?, man-ually annotated with morphological information andphrase-structure trees and extended with head infor-mation as described in Tsarfaty (2010, ch.
5).
Theunlabeled dependency version was produced by con-version from the constituency treebank as describedin Goldberg (2011).
Both the constituency and depen-dency trees were annotated with a set grammaticalfunction labels conforming to Unified Stanford De-pendencies by Tsarfaty (2013).22We also provided a predicted-all scenario, in which weprovided morphological analysis lattices with POS and mor-phological information derived from the analyses of the SMORderivational morphology (Schmid et al 2004).
These latticeswere not used by any of the participants.159Adapting the Data to the Shared Task Whilebased on the same trees, the dependency and con-stituency treebanks differ in their POS tag sets, aswell as in some of the morphological segmentationdecisions.
The main effort towards the shared taskwas unifying the two resources such that the two tree-banks share the same lexical yields, and the samepre-terminal labels.
To this end, we took the layeringapproach of Goldberg et al(2009), and included twolevels of POS tags in the constituency trees.
Thelower level is lexical, conforming to the lexical re-source used to build the lattices, and is shared bythe two treebanks.
The higher level is syntactic, andfollows the tag set and annotation decisions of theoriginal constituency treebank.23 In addition, we uni-fied the representation of morphological features, andfixed inconsistencies and mistakes in the treebanks.Data Split The Hebrew treebank is one of thesmallest in our language set, and hence it is providedin only the small (5k) setting.
For the sake of com-parability with the 5k set of the other treebanks, wecreated a comparable size of dev/test sets containingthe first and last 500 sentences respectively, wherethe rest serve as the 5k training.24Predicted Morphology The lattices encoding themorphological ambiguity for the Raw (all) scenariowere produced by looking up the possible analysesof each input token in the wide-coverage morpholog-ical analyzer (lexicon) of the Knowledge Center forProcessing Hebrew (Itai and Wintner, 2008; MILA,2008), with a simple heuristic for dealing with un-known tokens.
A small lattice encoding the possibleanalyses of each token was produced separately, andthese token-lattices were concatenated to produce thesentence lattice.
The lattice for a given sentence maynot include the gold analysis in cases of incompletelexicon coverage.The morphologically disambiguated input files forthe Raw (1-best) scenario were produced by run-ning the raw text through the morphological disam-23Note that this additional layer in the constituency treebankadds a relatively easy set of nodes to the trees, thus ?inflating?the evaluation scores compared to previously reported results.To compensate, a stricter protocol than is used in this task wouldstrip one of the two POS layers prior to evaluation.24This split is slightly different than the split in previous stud-ies.biguator (tagger) described in Adler and Elhadad(2006; Goldberg et al(2008),Adler (2007).
Thedisambiguator is based on the same lexicon that isused to produce the lattice files, but utilizes an extramodule for dealing with unknown tokens Adler et al(2008).
The core of the disambiguator is an HMMtagger trained on about 70M unannotated tokens us-ing EM, and being supervised by the lexicon.As in the case of Arabic, we also provided datafor the Predicted (gold token / predicted morphol-ogy) scenario.
We used the same sequence labeler,Morfette (Chrupa?a et al 2008), trained on the con-catenation of POS and morphological gold features,leading to a model with respectable accuracy.254.7 The Hungarian TreebankHungarian is an agglutinative language, thus a lemmacan have hundreds of word forms due to derivationalor inflectional affixation (nominal declination andverbal conjugation).
Grammatical information is typ-ically indicated by suffixes: case suffixes mark thesyntactic relationship between the head and its argu-ments (subject, object, dative, etc.)
whereas verbsare inflected for tense, mood, person, number, andthe definiteness of the object.
Hungarian is also char-acterized by vowel harmony.26 In addition, there areseveral other linguistic phenomena such as causa-tion and modality that are syntactically expressed inEnglish but encoded morphologically in Hungarian.The Data Set The Hungarian data set used inthe shared task is based on the Szeged Treebank,the largest morpho-syntactic and syntactic corpusmanually annotated for Hungarian.
This treebankis based on newspaper texts and is available inboth constituent-based (Csendes et al 2005) anddependency-based (Vincze et al 2010) versions.Around 10k sentences of news domain texts weremade available to the shared task.27 Each word ismanually assigned all its possible morpho-syntactic25POS+morphology prediction accuracy is 91.95% overall(59.54% for unseen tokens).
POS only prediction accuracy is93.20% overall (71.38% for unseen tokens).26When vowel harmony applies, most suffixes exist in twoversions ?
one with a front vowel and another one with a backvowel ?
and it is the vowels within the stem that determine whichform of the suffix is selected.27The original treebank contains 82,000 sentences, 1.2 millionwords and 250,000 punctuation marks from six domains.160tags and lemmas and the appropriate one is selectedaccording to the context.
Sentences were manu-ally assigned a constituency-based syntactic struc-ture, which includes information on phrase structure,grammatical functions (such as subject, object, etc.
),and subcategorization information (i.e., a given NPis subcategorized by a verb or an infinitive).
Theconstituency trees were later automatically convertedinto dependency structures, and all sentences werethen manually corrected.
Note that there exist somedifferences in the grammatical functions applied tothe constituency and dependency versions of the tree-bank, since some morpho-syntactic information wascoded both as a morphological feature and as dec-oration on top of the grammatical function in theconstituency trees.Adapting the Data to the Shared Task Origi-nally, the Szeged Dependency Treebank containedvirtual nodes for elided material (ELL) and phonolog-ically covert copulas (VAN).
In the current version,they have been deleted, their daughters have beenattached to the parent of the virtual node, and havebeen given complex labels, e.g.
COORD-VAN-SUBJ,where VAN is the type of the virtual node deleted,COORD is the label of the virtual node and SUBJ isthe label of the daughter itself.
When the virtual nodewas originally the root of the sentence, its daughterwith a predicative (PRED) label has been selected asthe new root of the sentence (with the label ROOT-VAN-PRED) and all the other daughters of the deletedvirtual node have been attached to it.Predicted Morphology In order to provide thesame POS tag set for the constituent and dependencytreebanks, we used the dependency POS tagset forboth treebank instances.
Both versions of the tree-bank are available with gold standard and automaticmorphological annotation.
The automatic POS tag-ging was carried out by a 10-fold cross-validationon the shared task data set by magyarlanc, a natu-ral language toolkit for processing Hungarian texts(segmentation, morphological analysis, POS tagging,and dependency parsing).
The annotation providesPOS tags and deep morphological features for eachinput token (Zsibrita et al 2013).2828The full data sets of both the constituency and de-pendency versions of the Szeged Treebank are available at4.8 The Korean TreebankThe Treebank The Korean corpus is generated bycollecting constituent trees from the KAIST Tree-bank (Choi et al 1994), then converting the con-stituent trees to dependency trees using head-findingrules and heuristics.
The KAIST Treebank consistsof about 31K manually annotated constituent treesfrom 97 different sources (e.g., newspapers, novels,textbooks).
After filtering out trees containing an-notation errors, a total of 27,363 trees with 350,090tokens are collected.The constituent trees in the KAIST Treebank29 alsocome with manually inspected morphological analy-sis based on ?eojeol?.
An eojeol contains root-formsof word tokens agglutinated with grammatical affixes(e.g., case particles, ending markers).
An eojeol canconsist of more than one word token; for instance, acompound noun ?bus stop?
is often represented asone eojeol in Korean, ???????????
?, which can bebroken into two word tokens,????
(bus) and????????(stop).
Each eojeol in the KAIST Treebank is sepa-rated by white spaces regardless of punctuation.
Fol-lowing the Penn Korean Treebank guidelines (Hanet al 2002), punctuation is separated as individualtokens, and parenthetical notations surrounded byround brackets are grouped into individual phraseswith a function tag (PRN in our corpus).All dependency trees are automatically convertedfrom the constituent trees.
Unlike English, whichrequires complicated head-finding rules to find thehead of each phrase (Choi and Palmer, 2012), Ko-rean is a head final language such that the rightmostconstituent in each phrase becomes the head of thatphrase.
Moreover, the rightmost conjunct becomesthe head of all other conjuncts and conjunctions ina coordination phrase, which aligns well with ourhead-final strategy.The constituent trees in the KAIST Treebank donot consist of function tags indicating syntactic orsemantic roles, which makes it difficult to generatedependency labels.
However, it is possible to gener-ate meaningful labels by using the rich morphologyin Korean.
For instance, case particles give goodthe following website: www.inf.u-szeged.hu/rgai/SzegedTreebank, and magyarlanc is downloadable from:www.inf.u-szeged.hu/rgai/magyarlanc.29See Lee et al(1997) for more details about the bracketingguidelines of the KAIST Treebank.161indications of what syntactic roles eojeols with suchparticles should take.
Given this information, 21dependency labels were generated according to theannotation scheme proposed by Choi (2013).Adapting the Data to the Shared Task All detailsconcerning the adaptation of the KAIST treebankto the shared task specifications are found in Choi(2013).
Importantly, the rich KAIST treebank tag setof 1975 POS tag types has been converted to a list ofCoNLL-like feature-attribute values refining coarsegrained POS categories.Predicted Morphology Two sets of automaticmorphological analyses are provided for this task.One is generated by the HanNanum morphologicalanalyzer.30 The HanNanum morphological ana-lyzer gives the same morphemes and POS tags as theKAIST Treebank.
The other is generated by the Se-jong morphological analyzer.31 The Sejong morpho-logical analyzer gives a different set of morphemesand POS tags as described in Choi and Palmer (2011).4.9 The Polish TreebankThe Data Set Sk?adnica is a constituency treebankof Polish (Wolin?ski et al 2011; S?widzin?ski andWolin?ski, 2010).
The trees were generated witha non-probabilistic DCG parser S?wigra and thendisambiguated and validated manually.
The ana-lyzed texts come from the one-million-token sub-corpus of the National Corpus of Polish (NKJP,(Przepi?rkowski et al 2012)) manually annotatedwith morpho-syntactic tags.The dependency version of Sk?adnica is a re-sult of an automatic conversion of manually disam-biguated constituent trees into dependency structures(Wr?blewska, 2012).
The conversion was an entirelyautomatic process.
Conversion rules were basedon morpho-syntactic information, phrasal categories,and types of phrase-structure rules encoded withinconstituent trees.
It was possible to extract dependen-cies because the constituent trees contain informationabout the head of the majority of constituents.
Forother constituents, heuristics were defined in order toselect their heads.30http://kldp.net/projects/hannanum31http://www.sejong.or.krThe version of Sk?adnica used in the shared taskcomprises parse trees for 8,227 sentences.32Predicted Morphology For the shared task Pre-dicted scenario, an automatic morphological an-notation was generated by the PANTERA tagger(Acedan?ski, 2010).4.10 The Swedish TreebankSwedish is moderately rich in inflections, includinga case system.
Word order obeys the verb secondconstraint in main clauses but is SVO in subordinateclauses.
Main clause order is freer than in Englishbut not as free as in some other Germanic languages,such as German.
Also, subject agreement with re-spect to person and number has been dropped inmodern Swedish.The Data Set The Swedish data sets are takenfrom the Talbanken section of the Swedish Treebank(Nivre and Megyesi, 2007).
Talbanken is a syntacti-cally annotated corpus developed in the 1970s, orig-inally annotated according to the MAMBA scheme(Teleman, 1974) with a syntactic layer consistingof flat phrase structure and grammatical functions.The syntactic annotation was later automatically con-verted to full phrase structure with grammatical func-tions and from that to dependency structure, as de-scribed by Nivre et al(2006).Both the phrase structure and the dependencyversion use the functional labels from the originalMAMBA scheme, which provides a fine-grained clas-sification of syntactic functions with 65 different la-bels, while the phrase structure annotation (whichhad to be inferred automatically) uses a coarse setof only 8 labels.
For the release of the Swedish tree-bank, the POS level was re-annotated to conform tothe current de facto standard for Swedish, which isthe Stockholm-Ume?
tagset (Ejerhed et al 1992)with 25 base tags and 25 morpho-syntactic features,which together produce over 150 complex tags.For the shared task, we used version 1.2 of thetreebank, where a number of conversion errors inthe dependency version have been corrected.
Thephrase structure version was enriched by propagatingmorpho-syntactic features from preterminals (POS32Sk?adnica is available from http://zil.ipipan.waw.pl/Sklicense.162tags) to higher non-terminal nodes using a standardhead percolation table, and a version without crossingbranches was derived using the lifting strategy (Boyd,2007).Adapting the Data to the Shared Task Explicitattribute names were added to the feature field and thesplit was changed to match the shared task minimaltraining set size.Predicted Morphology POS tags and morpho-syntactic features were produced using the Hun-PoS tagger (Hal?csy et al 2007) trained on theStockholm-Ume?
Corpus (Ejerhed and K?llgren,1997).5 Overview of the Participating SystemsWith 7 teams participating, more than 14 systems forFrench and 10 for Arabic and German, this sharedtask is on par with the latest large-scale parsing evalu-ation campaign SANCL 2012 (Petrov and McDonald,2012).
The present shared task was extremely de-manding on our participants.
From 30 individuals orteams who registered and obtained the data sets, wepresent results for the seven teams that accomplishedsuccessful executions on these data in the relevantscenarios in the given the time frame.5.1 Dependency TrackSeven teams participated in the dependency track.Two participating systems are based on MaltParser:MALTOPTIMIZER (Ballesteros, 2013) and AI:KU(Cirik and S?ensoy, 2013).
MALTOPTIMIZER usesa variant of MaltOptimizer (Ballesteros and Nivre,2012) to explore features relevant for the processingof morphological information.
AI:KU uses a combi-nation of MaltParser and the original MaltOptimizer.Their system development has focused on the inte-gration of an unsupervised word clustering methodusing contextual and morphological properties of thewords, to help combat sparseness.Similarly to MaltParser ALPAGE:DYALOG(De La Clergerie, 2013) also uses a shift-reducetransition-based parser but its training and decodingalgorithms are based on beam search.
This parser isimplemented on top of the tabular logic programmingsystem DyALog.
To the best of our knowledge, thisis the first dependency parser capable of handlingword lattice input.Three participating teams use the MATE parser(Bohnet, 2010) in their systems: the BASQUETEAM(Goenaga et al 2013), IGM:ALPAGE (Constant etal., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al2013).
The BASQUETEAM uses the MATE parser incombination with MaltParser (Nivre et al 2007b).The system combines the parser outputs via Malt-Blender (Hall et al 2007).
IGM:ALPAGE also usesMATE and MaltParser, once in a pipeline architec-ture and once in a joint model.
The models are com-bined via a re-parsing strategy based on (Sagae andLavie, 2006).
This system mainly focuses on MWEsin French and uses a CRF tagger in combinationwith several large-scale dictionaries to handle MWEs,which then serve as input for the two parsers.The IMS:SZEGED:CIS team participated in bothtracks, with an ensemble system.
For the depen-dency track, the ensemble includes the MATE parser(Bohnet, 2010), a best-first variant of the easy-firstparser by Goldberg and Elhadad (2010b), and turboparser (Martins et al 2010), in combination witha ranker that has the particularity of using featuresfrom the constituent parsed trees.
CADIM (Marton etal., 2013b) uses their variant of the easy-first parsercombined with a feature-rich ensemble of lexical andsyntactic resources.Four of the participating teams use exter-nal resources in addition to the parser.
TheIMS:SZEGED:CIS team uses external morpholog-ical analyzers.
CADIM uses SAMA (Graff et al2009) for Arabic morphology.
ALPAGE:DYALOGand IGM:ALPAGE use external lexicons for French.IGM:ALPAGE additionally uses Morfette (Chrupa?aet al 2008) for morphological analysis and POStagging.
Finally, as already mentioned, AI:KU clus-ters words and POS tags in an unsupervised fashionexploiting additional, un-annotated data.5.2 Constituency TrackA single team participated in the constituency parsingtask, the IMS:SZEGED:CIS team (Bj?rkelund et al2013).
Their phrase-structure parsing system uses acombination of 8 PCFG-LA parsers, trained using aproduct-of-grammars procedure (Petrov, 2010).
The50-best parses of this combination are then rerankedby a model based on the reranker by Charniak and163Johnson (2005).335.3 BaselinesWe additionally provide the results of two baselinesystems for the nine languages, one for constituencyparsing and one for dependency parsing.For the dependency track, our baseline system isMaltParser in its default configuration (the arc-eageralgorithm and liblinear for training).
Results markedas BASE:MALT in the next two sections report theresults of this baseline system in different scenarios.The constituency parsing baseline is based on themost recent version of the PCFG-LA model of Petrovet al(2006), used with its default settings and fivesplit/merge cycles, for all languages.34 We use thisparser in two configurations: a ?1-best?
configura-tion where all POS tags are provided to the parser(predicted or gold, depending on the scenario), andanother configuration in which the parser performsits own POS tagging.
These baselines are referred toas BASE:BKY+POS and BASE:BKY+RAW respec-tively in the following results sections.
Note thateven when BASE:BKY+POS is given gold POS tags,the Berkeley parser sometimes fails to reach a perfectPOS accuracy.
In cases when the parser cannot find aparse with the provided POS, it falls back on its ownPOS tagging for all tokens.6 ResultsThe high number of submitted system variants andevaluation scenarios in the task resulted in a largenumber of evaluation scores.
In the following evalu-ation, we focus on the best run for each participant,and we aim to provide key points on the differentdimensions of analysis resulting from our evaluationprotocol.
We invite our interested readers to browsethe comprehensive representation of our results onthe official shared-task results webpages.3533Note that a slight but necessary change in the configurationof one of our metrics, which occurred after the system submis-sion deadline, resulted in the IMS:SZEGED:CIS team to submitsuboptimal systems for 4 languages.
Their final scores are ac-tually slightly higher and can be found in (Bj?rkelund et al2013).34For Semitic languages, we used the lattice based PCFG-LAextension by Goldberg (2011).35http://www.spmrl.org/spmrl2013-sharedtask-results.html.6.1 Gold ScenariosThis section presents the parsing results in gold sce-narios, where the systems are evaluated on gold seg-mented and tagged input.
This means that the se-quence of terminals, POS tags, and morphologicalfeatures are provided based on the treebank anno-tations.
This scenario was used in most previousshared tasks on data-driven parsing (Buchholz andMarsi, 2006; Nivre et al 2007a; K?bler, 2008).
Notethat this scenario was not mandatory.
We thank ourparticipants for providing their results nonetheless.We start by reviewing dependency-based parsingresults, both on the trees and on multi-word expres-sion, and continue with the different metrics forconstituency-based parsing.6.1.1 Dependency ParsingFull Training Set The results for the gold parsingscenario of dependency parsing are shown in the topblock of table 3.Among the six systems, IMS:SZEGED:CISreaches the highest LAS scores, not only on aver-age, but for every single language.
This shows thattheir approach of combining parsers with (re)rankingprovides robust parsing results across languages withdifferent morphological characteristics.
The secondbest system is ALPAGE:DYALOG, the third best sys-tem is MALTOPTIMIZER.
The fact that AI:KU isranked below the Malt baseline is due to their sub-mission of results for 6 out of the 9 languages.
Simi-larly, CADIM only submitted results for Arabic andranked in the third place for this language, after thetwo IMS:SZEGED:CIS runs.
IGM:ALPAGE andBASQUETEAM did not submit results for this setting.Comparing LAS results across languages is prob-lematic due to the differences between languages,treebank size and annotation schemes (see section 3),so the following discussion is necessarily tentative.
Ifwe consider results across languages, we see that thelowest results (around 83% for the best performingsystem) are reached for Hebrew and Swedish, thelanguages with the smallest data sets.
The next low-est result, around 86%, is reached for Basque.
Otherlanguages reach similar LAS scores, around 88-92%.German, with the largest training set, reaches thehighest LAS, 91.83%.Interstingly, all systems have high LAS scoreson the Korean Treebank given a training set size164team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.1) gold setting / full training setIMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61CADIM 85.56 9.512) gold setting / 5k training setIMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88CADIM 82.67 9.193) predicted setting / full training setIMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03IGM:ALPAGE 85.86 9.54CADIM 83.20 9.244) predicted setting / 5k training setIMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16IGM:ALPAGE 83.60 9.29CADIM 80.51 8.95Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input.
Results in boldshow the best results per language and setting.of approximately 23,000 sentences, which is a littleover half of the German treebank.
For German, onthe other hand, only the IMS:SZEGED:CIS systemreaches higher LAS scores than for Korean.
Thisfinal observation indicates that more than treebanksize is important for comparing system performanceacross treebanks.
This is the reason for introducingthe reduced set scenario, in which we can see how theparticipating system perform on a common ground,albeit small.5k Training Set The results for the gold settingon the 5k train set are shown in the second blockof Table 3.
Compared with the full training, wesee that there is a drop of around 2 points in thissetting.
Some parser/language pairs are more sensi-tive to data sparseness than others.
CADIM, for in-stance, exhibit a larger drop than MALTOPTIMIZERon Arabic, and MALTOPTIMIZER shows a smallerdrop than IMS:SZEGED:CIS on French.
On average,among all systems that covered all languages, MALT-OPTIMIZER has the smallest drop when moving to5k training, possibly since the automatic feature opti-mization may differ for different data set sizes.Since all languages have the same number of sen-tences in the train set, these results can give us limitedinsight into the parsing complexity of the differenttreebanks.
Here, French, Arabic, Polish, and Koreanreach the highest LAS scores while Swedish reaches165Team F_MWE F_COMP F_MWE+POS1) gold setting / full training setAI:KU 99.39 99.53 99.34IMS:SZEGED:CIS 99.26 99.39 99.21MALTOPTIMIZER 98.95 98.99 0ALPAGE:DYALOG 98.32 98.81 0BASE:MALT 68.7 72.55 68.72) predicted setting / full training setIGM:ALPAGE 80.81 81.18 77.37IMS:SZEGED:CIS 79.45 80.79 70.48ALPAGE:DYALOG 77.91 79.25 0BASQUE-TEAM 77.19 79.81 0MALTOPTIMIZER 70.29 74.25 0BASE:MALT 67.49 71.01 0AI:KU 0 0 03) predicted setting / 5k training setIGM:ALPAGE 77.66 78.68 74.04IMS:SZEGED:CIS 77.28 78.92 70.42ALPAGE:DYALOG 75.17 76.82 0BASQUETEAM 73.07 76.58 0MALTOPTIMIZER 65.76 70.42 0BASE:MALT 62.05 66.8 0AI:KU 0 0 0Table 4: Dependency Parsing: MWE resultsthe lowest one.
Treebank variance depends not onlyon the language but also on annotation decisions,such as label set (Swedish, interestingly, has a rela-tively rich one).
A more careful comparison wouldthen take into account the correlation of data size,label set size and parsing accuracy.
We investigatethese correlations further in section 7.1.6.1.2 Multiword ExpressionsMWE results on the gold setting are found atthe top of Table 4.
All systems, with the excep-tion of BASE:MALT, perform exceedingly well inidentifying the spans and non-head components ofMWEs given gold morphology.36 These almost per-fect scores are the consequence of the presence oftwo gold MWE features, namely MWEHEAD andPRED=Y, which respectively indicate the node spanof the whole MWE and its dependents, which do nothave a gold feature field.
The interesting scenario is,of course, the predicted one, where these features arenot provided to the parser, as in any realistic applica-tion.36Note that for the labeled measure F_MWE+POS, bothMALTOPTIMIZER and ALPAGE:DYALOG have an F-score ofzero, since they do not attempt to predict the MWE label at all.6.1.3 Constituency ParsingIn this part, we provide accuracy results for phrase-structure trees in terms of ParsEval F-scores.
SinceParsEval is sensitive to the non-terminals-per-wordratio in the data set (Rehbein and van Genabith,2007a; Rehbein and van Genabith, 2007b), and giventhe fact that this ratio varies greatly within our dataset (as shown in Table 2), it must be kept in mind thatParsEval should only be used for comparing parsingperformance over treebank instances sharing the ex-act same properties in term of annotation schemes,sentence length and so on.
When comparing F-Scoresacross different treebanks and languages, it can onlyprovide a rough estimate of the relative difficulty orease of parsing these kinds of data.Full Training Set The F-score results for the goldscenario are provided in the first block of Table 5.Among the two baselines, BASE:BKY+POS faresbetter than BASE:BKY+RAW since the latter selectsits own POS tags and thus cannot benefit from thegold information.
The IMS:SZEGED:CIS systemclearly outperforms both baselines, with Hebrew asan outlier.37As in the dependency case, the results are notstrictly comparable across languages, yet we candraw some insights from them.
We see consider-able differences between the languages, with Basque,Hebrew, and Hungarian reaching F-scores in the low90s for the IMS:SZEGED:CIS system, Korean andPolish reaching above-average F-scores, and Ara-bic, French, German, and Swedish reaching F-scoresbelow the average, but still in the low 80s.
The per-formance is, again, not correlated with data set sizes.Parsing Hebrew, with one of the smallest trainingsets, obtains higher accuracy many other languages,including Swedish, which has the same training setsize as Hebrew.
It may well be that gold morphologi-cal information is more useful for combatting sparse-ness in languages with richer morphology (thoughArabic here would be an outlier for this conjecture),or it may be that certain treebanks and schemes areinherently harder to parser than others, as we investi-gate in section 7.For German, the language with the largest training37It might be that the easy layer of syntactic tags benefits fromthe gold POS tags provided.
See section 4 for further discussionof this layer.166team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.1) gold setting / full training setIMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.752) gold setting / 5k training setIMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.613) predicted setting / full training setIMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.534) predicted setting / 5k training setIMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input.
Results inbold show the best results per language and setting.set and the highest scores in dependency parsing,the F-scores are at the lower end.
These low scores,which are obtained despite the larger treebank andonly moderately free word-order, are surprising.
Thismay be due to case syncretism; gold morphologicalinformation exhibits its own ambiguity and thus maynot be fully utilized.5k Training Set Parsing results on smaller com-parable test sets are presented in the second blockof Table 5.
On average, IMS:SZEGED:CIS is lesssensitive than BASE:BKY+POS to the reduced size.Systems are not equally sensitive to reduced trainingsets, and the gaps range from 0.4% to 3%, with Ger-man and Korean as outliers (Korean suffering a 6.4%drop in F-score and German 7.3%).
These languageshave the largest treebanks in the full setting, so it isnot surprising that they suffer the most.
But this initself does not fully explain the cross-treebank trends.Since ParsEval scores are known to be sensitive tothe label set sizes and the depth of trees, we provideLeafAncestor scores in the following section.6.1.4 Leaf-Ancestor ResultsThe variation across results in the previous subsec-tion may have been due to differences across annota-tion schemes.
One way to neutralize this difference(to some extent) is to use a different metric.
Weevaluated the constituency parsing results using theLeaf-Ancestor (LA) metric, which is less sensitiveto the number of nodes in a tree (Rehbein and vanGenabith, 2007b; K?bler et al 2008).
As shown inTable 6, these results are on a different (higher) scalethan ParsEval, and the average gap between the fulland 5k setting is lower.Full Training Set The LA results in gold settingfor full training sets are shown in the first block of Ta-ble 6.
The trends are similar to the ParsEval F-scores.German and Arabic present the lowest LA scores(in contrast to the corresponding F-scores, Arabic isa full point below German for IMS:SZEGED:CIS).Basque and Hungarian have the highest LA scores.Hebrew, which had a higher F-score than Basque,has a lower LA than Basque and is closer to French.Korean also ranks worse in the LA analysis.
Thechoice of evaluation metrics thus clearly impacts sys-tem rankings ?
F-scores rank some languages suspi-ciously high (e.g., Hebrew) due to deeper trees, andanother metric may alleviate that.5k Training Set The results for the leaf-ancestor(LA) scores in the gold setting for the 5k training setare shown in the second block of Table 6.
Across167team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.1) gold setting / full training setIMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.372) gold setting / 5k training setIMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.533) predicted setting / full training setIMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.234) predicted setting / 5k training setIMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.parsers, IMS:SZEGED:CIS again has a smaller dropthan BASE:BKY+POS on the reduced size.
Germansuffers the most from the reduction of the trainingset, with a loss of approximately 4 points.
Korean,however, which was also severely affected in termsof F-scores, only loses 1.17 points in the LA score.On average, the LA seem to reflect a smaller dropwhen reducing the training set ?
this underscoresagain the impact of the choice of metrics on systemevaluation.6.2 Predicted ScenariosGold scenarios are relatively easy since syntacticallyrelevant morphological information is disambiguatedin advance and is provided as input.
Predicted scenar-ios are more difficult: POS tags and morphologicalfeatures have to be automatically predicted, by theparser or by external resources.6.2.1 Dependency ParsingEight participating teams submitted dependencyresults for this scenario.
Two teams submitted for asingle language.
Four teams covered all languages.Full Training Set The results for the predictedscenario in full settings are shown in the thirdblock of Table 3.
Across the board, the re-sults are considerably lower than the gold sce-nario.
Again, IMS:SZEGED:CIS is the best per-forming system, followed by ALPAGE:DYALOG andMALTOPTIMIZER.
The only language for whichIMS:SZEGED:CIS is outperformed is French, forwhich IGM:ALPAGE reaches higher results (85.86%vs.
85.24%).
This is due to the specialized treatmentof French MWEs in the IGM:ALPAGE system, whichis thereby shown to be beneficial for parsing in thepredicted setting.If we compare the results for the predicted set-ting and the gold one, given the full training set,the IMS:SZEGED:CIS system shows small differ-ences between 1.5 and 2 percent.
The only ex-ception is French, for which the LAS drops from90.29% to 85.24% in the predicted setting.
Theother systems show somewhat larger differences thanIMS:SZEGED:CIS, with the highest drops for Ara-bic and Korean.
The AI:KU system shows a similarproblem as IMS:SZEGED:CIS for French.5k Training Set When we consider the predictedsetting for the 5k training set, in the last block ofTable 3, we see the same trends as comparing withthe full training set or when comparing to the goldsetting.
Systems suffer from not having gold stan-dard data, and they suffer from the small training set.Interestingly, the loss between the different trainingset sizes in the predicted setting is larger than in the168gold setting, but only marginally so, with a differ-ence < 0.5.
In other words, the predicted settingadds a challenge to parsing, but it only minimallycompounds data sparsity.6.2.2 Multiword Expressions EvaluationIn the predicted setting, shown in the secondblock of table 4 for the full training set and in thethird block of the same table for the 5k training set,we see that only two systems, IGM:ALPAGE andIMS:SZEGED:CIS can predict the MWE label whenit is not present in the training set.
IGM:ALPAGE?sapproach of using a separate classifier in combinationwith external dictionaries is very successful, reach-ing an F_MWE+POS score of 77.37.
This is com-pared to the score of 70.48 by IMS:SZEGED:CIS,which predicts this node label as a side effect oftheir constituent feature enriched dependency model(Bj?rkelund et al 2013).
AI:KU has a zero scorefor all predicted settings, which results from an erro-neous training on the gold data rather than the pre-dicted data.386.2.3 Constituency ParsingFull Training Set The results for the predicted set-ting with the full training set are shown in the thirdblock of table 5.
A comparison with the gold settingshows that all systems have a lower performance inthe predicted scenario, and the differences are in therange of 0.88 for Arabic and 2.54 for Basque.
It isinteresting to see that the losses are generally smallerthan in the dependency framework: on average, theloss across languages is 2.74 for dependencies and1.48 for constituents.
A possible explanation can befound in the two-dimensional structure of the con-stituent trees, where only a subset of all nodes isaffected by the quality of morphology and POS tags.The exception to this trend is Basque, for which theloss in constituents is a full point higher than for de-pendencies.
Another possible explanation is that allof our constituent parsers select their own POS tagsin one way or another.
Most dependency parsers ac-cept predicted tags from an external resource, whichputs an upper-bound on their potential performance.5k Training Set The results for the predicted set-ting given the 5k training set are shown in the bottom38Unofficial updated results are to to be found in (Cirik andS?ensoy, 2013)block of table 5.
They show the same trends as thedependency ones: The results are slightly lower thanthe results obtained in gold setting and the ones uti-lizing the full training set.6.2.4 Leaf Ancestor MetricsFull Training Set The results for the predicted sce-nario with a full training set are shown in the thirdblock of table 6.
In the LA evaluation, the lossin moving from gold morphology are considerablysmaller than in F-scores.
For most languages, theloss is less than 0.5 points.
Exceptions are Frenchwith a loss of 0.72, Hebrew with 0.89, and Koreanwith 1.17.
Basque, which had the highest loss inF-scores, only shows a minor loss of 0.4 points.
Also,the average loss of 0.41 points is much smaller thanthe one in the ParsEval score, 1.48.5k Training Set The results for the predicted set-ting given the 5k training set are shown in the lastblock of table 6.
These results, though considerablylower (around 3 points), exhibit the exact same trendsas observed in the gold setting.6.3 Realistic Raw ScenariosThe previous scenarios assume that input surface to-kens are identical to tree terminals.
For languagessuch as Arabic and Hebrew, this is not always thecase.
In this scenario, we evaluate the capacity of asystem to predict both morphological segmentationand syntactic parse trees given raw, unsegmentedinput tokens.
This may be done via a pipeline as-suming a 1-st best morphological analysis, or jointlywith parsing, assuming an ambiguous morpholog-ical analysis lattice as input.
In this task, both ofthese scenarios are possible (see section 3).
Thus,this section presents a realistic evaluation of the par-ticipating systems, using TedEval, which takes intoaccount complete morpho-syntactic parses.Tables 7 and 8 present labeled and unlabeledTedEval results for both constituency and depen-dency parsers, calculated only for sentence of length<= 70.39 We firstly observe that labeled TedEvalscores are considerably lower than unlabeled Ted-Eval scores, as expected, since unlabeled scores eval-uate only structural differences.
In the labeled setup,39TedEval builds on algorithms for calculating edit distanceon complete trees (Bille, 2005).
In these algorithms, longersentences take considerably longer to evaluate.169Arabic Arabic Hebrew Allfull training set 5k training setAcc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg.
Soft Avg.IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82AI:KU - - - - 78.57 3.37 39.29 78.57Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.The upper part refers to constituency results, the lower part refers to dependency resultsArabic Arabic Hebrew Allfull training set 5k training setAcc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg.
Soft Avg.IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61AI:KU - - - - 86.70 8.98 43.35 86.70Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.Top upper part refers to constituency results, the lower part refers to dependency results.the IMS:SZEGED:CIS dependency parser are thebest for both languages and data set sizes.
Table 8shows that their unlabeled constituency results reacha higher accuracy than the next best system, theirown dependency results.
However, a quick look atthe exact match metric reveals lower scores than forits dependency counterparts.For the dependency-based joint scenarios, thereis obviously an upper bound on parser performancegiven inaccurate segmentation.
The transition-basedsystems, ALPAGE:DYALOG & MALTOPTIMIZER,perform comparably on Arabic and Hebrew, withALPAGE:DYALOG being slightly better on both lan-guages.
Note that ALPAGE:DYALOG reaches closeresults on the 1-best and the lattice-based input set-tings, with a slight advantage for the former.
This ispartly due to the insufficient coverage of the lexicalresource we use: many lattices do not contain thegold path, so the joint prediction can only as be highas the lattice predicted path allows.7 Towards In-Depth Cross-TreebankEvaluationSection 6 reported evaluation scores across systemsfor different scenarios.
However, as noted, these re-sults are not comparable across languages, represen-tation types and parsing scenarios due to differencesin the data size, label set size, length of sentences andalso differences in evaluation metrics.Our following discussion in the first part of thissection highlights the kind of impact that data setproperties have on the standard metrics (label set sizeon LAS, non-terminal nodes per sentence on F-score).Then, in the second part of this section we use theTedEval cross-experiment protocols for comparativeevaluation that is less sensitive to representation typesand annotation idiosyncrasies.7.1 Parsing Across Languages and TreebanksTo quantify the impact of treebank characteristics onparsing parsing accuracy we looked at correlationsof treebank properties with parsing results.
The mosthighly correlated combinations we have found areshown in Figures 2, 3, and 4 for the dependency trackand the constituency track (F-score and LeafAnces-17021/09/13 03:00SPMRL chartsPage 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.htmlCorrelation between label set size, treebank size, and mean LASFrPFrPGePGePHuPHuPSwPArPArPArGArGBaPBaPFrGFrGGeGGeGHePHeGHuGHuGPoPPoPPoGPoGSwGBaGBaGKoPKoPKoGKoG10 50 100 500 1 00072747678808284868890treebank size / #labelsLAS(%)Figure 2: The correlation between treebank size, label set size, and LAS scores.
x: treebank size / #labels ; y: LAS (%)01/10/13 00:43SPMRL charts: all sent.Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.htmlSPMRL Results charts (Parseval): Const.
Parsing Track (gold tokens, all sent.
)(13/10/01 00:34:34Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesispred/full pred/5k gold/full gold/5k Correlation chartsCorrelation between treebank size (#Non terminal), number of sentences (#sent) and mean F1ArabicBasqueFrenchGermanHebrewHungarianKoreanPolishSwedishArPArGBaPBaGFrPFrGGePGeGHePHeGHuPHuGKoPKoGPoPPoGSwPSwG8 9 107274767880828486889092treebank size (#Non terminal) / #sentF1(%)Figure 3: The correlation between the non terminals per sentence ratio and F-scores.
x: #non terminal/ #sentence ; y:F1 (%)171tor) respectively.Figure 2 presents the LAS against the average num-ber of tokens relative to the number of labels.
Thenumbers are averaged per language over all partici-pating systems, and the size of the ?bubbles?
is pro-portional to the number of participants for a givenlanguage setting.
We provide ?bubbles?
for all lan-guages in the predicted (-P) and gold (-G) setting,for both training set sizes.
The lower dot in termsof parsing scores always corresponds to the reducedtraining set size.Figure 2 shows a clear correlation between data-set complexity and parsing accuracy.
The simplerthe data set is (where ?simple" here translates intolarge data size with a small set of labels), the higherthe results of the participating systems.
The bubblesreflects a diagonal that indicates correlation betweenthese dimensions.
Beyond that, we see two interest-ing points off of the diagonal.
The Korean treebank(pink) in the gold setting and full training set can beparsed with a high LAS relative to its size and labelset.
It is also clear that the Hebrew treebank (purple)in the predicted version is the most difficult one toparse, relative to our expectation about its complexity.Since the Hebrew gold scenario is a lot closer to thediagonal again, it may be that this outlier is due to thecoverage and quality of the predicted morphology.Figure 340 shows the correlation of data complex-ity in terms of the average number of non-terminalsper sentence, and parsing accuracy (ParsEval F-score).
Parsing accuracy is again averaged over allparticipating systems for a given language.
In thisfigure, we see a diagonal similar to the one in figure 2,where Arabic (dark blue) has high complexity of thedata (here interpreted as flat trees, low number ofnon terminals per sentence) and low F-scores accord-ingly.
Korean (pink), Swedish (burgundy), Polish(light green), and Hungarian (light blue) follow, andthen Hebrew (purple) is a positive outlier, possiblydue to an additional layer of ?easy" syntactic POSnodes which increases tree size and inflates F-scores.French (orange), Basque (red), and German (darkgreen) are negative outliers, falling off the diago-nal.
German has the lowest F-score with respect to40This figure was created from the IMS:SZEGED:CIS(Const.)
and our own PCFG-LA baseline in POS Tagged mode(BASE:BKY+POS) so as to avoid the noise introduced by theparser?s own tagging step (BASE:BKY+RAW).what would be expected for the non-terminals persentence ratio, which is in contrast to the LAS fig-ure where German occurs among the less complexdata set to parse.
A possible explanation may bethe crossing branches in the original treebank whichwere re-attached.
This creates flat and variable edgeswhich might be hard predict accurately.Figure 441 presents the correlation between parsingaccuracy in terms the LeafAncestor metrics (macroaveraged) and treebank complexity in terms of theaverage number of non-terminals per sentence.
Asin the correlation figures, the parsing accuracy isaveraged over the participanting systems for any lan-guage.
The LeafAncestor accuracy is calculated overphrase structure trees, and we see a similar diago-nal to the one in Figure 3 showing that flatter tree-banks are harder (that is, are correlated with loweraveraged scores) But, its slope is less steep than forthe F-score, which confirms the observation that theLeafAncestor metric is less sensitive than F-score tothe non-terminals-per-sentence ratio.Similarly to Figure 3, German is a negative outlier,which means that this treebank is harder to parse ?
itobtains lower scores on average than we would ex-pect.
As for Hebrew, it is much closer to the diagonal.As it turns out, the "easy" POS layer that inflates thescores does not affect the LA ratings as much.7.2 Evaluation Across Scenarios, Languagesand TreebanksIn this section we analyze the results in cross-scenario, cross-annotation, and cross-framework set-tings using the evaluation protocols discussed in(Tsarfaty et al 2012b; Tsarfaty et al 2011; Tsarfatyet al 2012a).As a starting point, we select comparable sectionsof the parsed data, based on system runs trained onthe small train set (train5k).
For those, we selectedsubsets containing the first 5,000 tree terminals (re-specting sentence boundaries) of the test set.
We onlyused TedEval on sentences up to 70 terminals long,and projectivized non-projective sentences in all sets.We use the TedEval metrics to calculate scores onboth constituency and dependency structures in alllanguages and all scenarios.
Since the metric de-fines one scale for all of these different cases, we can41This figure was created under the same condition as theF-score correlation in figure (Figure 3).17204/10/13 23:05SPMRL charts:Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.htmlSPMRL Results charts (Parseval): Const.
Parsing Track (gold tokens, )(13/10/04 23:05:31Arabic Basque French German Hebrew Hungarian Korean Polish SwedishSynthesispred/full pred/5k gold/full gold/5k Correlation chartsCorrelation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf AccuracyArPArGBaPBaGFrPFrGGePHePHeGHuPHuGKoPKoGPoPPoGSwPSwGGeG8 9 107476788082848688909294treebank size (#Non terminal) / #sentF1(%)Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores.
x: #nonterminal/ #sentence ; y: Acc.
(%)compare the performance across annotation schemes,assuming that those subsets are representative of theiroriginal source.42Ideally, we would be using labeled TedEval scores,as the labeled parsing task is more difficult, and la-beled parses are far more informative than unlabeledones.
However, most constituency-based parsers donot provide function labels as part of the output, tobe compared with the dependency arcs.
Furthermore,as mentioned earlier, we observed a huge differencebetween label set sizes for the dependency runs.
Con-sequently, labeled scores will not be as informativeacross treebanks and representation types.
We willtherefore only use labels across scenarios for thesame language and representation type.42We choose this sample scheme for replicability.
We firsttried sampling sentences, aiming at the same average sentencelength (20), but that seemed to create artificially difficult test setsfor languages as Polish and overly simplistic ones for French orArabic.7.2.1 Cross-Scenario Evaluation: raw vs. goldOne novel aspect of this shared task is the evalu-ation on non-gold segmentation in addition to goldmorphology.
One drawback is that the scenarios arecurrently not using the same metrics ?
the metricsgenerally applied for gold and predicted scenrios can-not apply for raw.
To assess how well state of the artparsers perform in raw scenarios compared to goldscenarios, we present here TedEval results comparingraw and gold systems using the evaluation protocolof Tsarfaty et al(2012b).Table 9 presents the labeled and unlabeled resultsfor Arabic and Hebrew (in Full and 5k training set-tings), and Table 10 presents unlabeled TedEval re-sults (for all languages) in the gold settings.
Theunlabeled TedEval results for the raw settings aresubstantially lower then TedEval results on the goldsettings for both languages.When comparing the unlabeled TedEval results forArabic and Hebrew on the participating systems, wesee a loss of 3-4 points between Table 9 (raw) and Ta-ble 10 (gold).
In particular we see that for the best per-173forming systems on Arabic (IMS:SZEGED:CIS forboth constituency and dependency), the gap betweengold and realistic scenarios is 3.4 and 4.3 points,for the constituency and the dependency parser re-spectively.
These results are on a par with resultsby Tsarfaty et al(2012b), who showed for differentsettings, constituency and dependency based, thatraw scenarios are considerably more difficult to parsethan gold ones on the standard split of the ModernHebrew treebank.For Hebrew, the performance gap between unla-beled TedEval in raw (Table 9) and gold (Table 10)is even more salient, with around 7 and 8 points ofdifference between the scenarios.
We can only specu-late that such a difference may be due to the difficultyof resolving Hebrew morpho-syntactic ambiguitieswithout sufficient syntactic information.
Since He-brew and Arabic now have standardized morpholog-ically and syntactically analyzed data sets availablethrough this task, it will be possible to investigatefurther how cross-linguistic differences in morpho-logical ambiguity affect full-parsing accuracy in rawscenarios.This section compared the raw and gold parsingresults only on unlabeled TedEval metrics.
Accord-ing to what we have seen so far is expected thatfor labeled TedEval metrics using the same protocol,the gap between gold and raw scenario will be evengreater.7.2.2 Cross-Framework Evaluation:Dependency vs. ConstituencyIn this section, our focus is on comparing parsingresults across constituency and dependency parsersbased on the protocol of Tsarfaty et al(2012a) Wehave only one submission from IMS:SZEGED:CISin the constituency track, and.
from the same group,a submission on the dependency track.
We only com-pare the IMS:SZEGED:CIS results on constituencyand dependency parsing with the two baselines weprovided.
The results of the cross-framework evalua-tion protocol are shown in Table 11.The results comparing the two variants of theIMS:SZEGED:CIS systems show that they are veryclose for all languages, with differences ranging from0.03 for German to 0.8 for Polish in the gold setting.It has often been argued that dependency parsersperform better than a constituency parser, but wenotice that when using a cross framework protocol,such as TedEval, and assuming that our test set sam-ple is representative, the difference between the in-terpretation of both representation?s performance isalleviated.
Of course, here the metric is unlabeled, soit simply tells us that both kind of parsing models areequally able to provide similar tree structures.
Saiddifferently, the gaps in the quality of predicting thesame underlying structure across representations forMRLs is not as large as is sometimes assumed.For most languages, the baseline constituencyparser performs better than the dependency base-line one, with Basque and Korean as an exception,and at the same time, the dependency version ofIMS:SZEGED:CIS performs slightly better than theirconstituent parser for most languages, with the excep-tion of Hebrew and Hungarian.
It goes to show that,as far as these present MRL results go, there is noclear preference for a dependency over a constituencyparsing representation, just preferences among par-ticular models.More generally, we can say that even if the linguis-tic coverage of one theory is shown to be better thananother one, it does not necessarily mean that thestatistical version of the formal theory will performbetter for structure prediction.
System performanceis more tightly related to the efficacy of the learningand search algorithms, and feature engineering ontop of the selected formalism.7.2.3 Cross-Language Evaluation: AllLanguagesWe conclude with an overall outlook of the Ted-Eval scores across all languages.
The results on thegold scenario, for the small training set and the 5ktest set are presented in Table 10.
We concentrateon gold scenarios (to avoid the variation in cover-age of external morphological analyzers) and chooseunlabeled metrics as they are not sensitive to labelset sizes.
We emphasize in bold, for each parsingsystem (row in the table), the top two languages thatmost accurately parsed by it (boldface) and the twolanguages it performed the worse on (italics).We see that the European languages Germanand Hungarian are parsed most accurately in theconstituency-based setup, with Polish and Swedishhaving an advantage in dependency parsing.
Acrossall systems, Korean is the hardest to parse, with Ara-174Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG21) Constituency EvaluationLabeled TedEval Unabeled TedEvalIMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.12) Dependency EvaluationLabeled TedEval Unabeled TedEvalIMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.The upper part refers to constituency parsing and the lower part refers to dependency parsing.Arabic Basque French German Hebrew Hungarian Korean Polish Swedish1) Constituency EvaluationIMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.902) Dependency EvaluationIMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90CADIM 94.66 - - - - - - - -Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set anda 5k-terminals test set.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish1) gold settingIMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.352) predicted settingIMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5ksentences and tested on 5k terminals.bic, Hebrew and to some extent French following.
Itappears that on a typological scale, Semitic and Asianlanguages are still harder to parse than a range of Eu-ropean languages in terms of structural difficulty andcomplex morpho-syntactic interaction.
That said,note that we cannot tell why certain treebanks appearmore challenging to parse then others, and it is stillunclear whether the difficulty is inherent on the lan-guage, in the currently available models, or becauseof the annotation scheme and treebank consistency.4343The latter was shown to be an important factor orthogonalto the morphologically-rich nature of the treebank?s language1758 ConclusionThis paper presents an overview of the first sharedtask on parsing morphologically rich languages.
Thetask features nine languages, exhibiting different lin-guistic phenomena and varied morphological com-plexity.
The shared task saw submissions from seventeams, and results produced by more than 14 differentsystems.
The parsing results were obtained in dif-ferent input scenarios (gold, predicted, and raw) andevaluated using different protocols (cross-framework,cross-scenario, and cross-language).
In particular,this is the first time an evaluation campaign reportson the execution of parsers in realistic, morphologi-cally ambiguous, setting.The best performing systems were mostly ensem-ble systems combining multiple parser outputs fromdifferent frameworks or training runs, or integrat-ing a state-of-the-art morphological analyzer on topof a carefully designed feature set.
This is con-sistent with previous shared tasks such as ConLL2007 or SANCL?2012.
However, dealing with am-biguous morphology is still difficult for all systems,and a promising approach, as demonstrated by AL-PAGE:DYALOG, is to deal with parsing and morphol-ogy jointly by allowing lattice input to the parser.
Apromising generalization of this approach would bethe full integration of all levels of analysis that aremutually informative into a joint model.The information to be gathered from the results ofthis shared task is vast, and we only scratched thesurface with our preliminary analyses.
We uncov-ered and documented insights of strategies that makeparsing systems successful: parser combination isempirically proven to reach a robust performanceacross languages, though language-specific strategiesare still a sound avenue for obtaining high qualityparsers for that individual language.
The integrationof morphological analysis into the parsing needs tobe investigated thoroughly, and new approaches thatare morphologically aware need to be developed.Our cross-parser, cross-scenario, and cross-framework evaluation protocols have shown that, asexpected, more data is better, and that performanceon gold morphological input is significantly higherthan that in more realistic scenarios.
We have shownthat gold morphological information is more help-(Schluter and van Genabith, 2007)ful to some languages and parsers than others, andthat it may also interact with successful identificationof multiword expressions.
We have shown that dif-ferences between dependency and constituency aresmaller than previously assumed and that propertiesof the learning model and granularity of the outputlabels are more influential.
Finally, we observedthat languages which are typologically farthest fromEnglish, such as Semitic and Asian languages, arestill amongst the hardest to parse, regardless of theparsing method used.Our cross-treebank, in-depth analysis is still pre-liminary, owing to the limited time between the endof the shared task and the deadline for publicationof this overview.
but we nonetheless feel that ourfindings may benefit researchers who aim to developparsers for diverse treebanks.44A shared task is an inspection of the state of theart, but it may also accelerate research in an areaby providing a stable data basis as well as a set ofstrong baselines.
The results produced in this taskgive a rich picture of the issues associated with pars-ing MRLs and initial cues towards their resolution.This set of results needs to be further analyzed to befully understood, which will in turn contribute to newinsights.
We hope that this shared task will provideinspiration for the design and evaluation of futureparsing systems for these languages.AcknowledgmentsWe heartily thank Miguel Ballesteros and CorentinRibeire for running the dependency and constituencybaselines.
We warmly thank the Linguistic Data Con-sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,Seth Kulick and Mohamed Maamouri for releasingthe Arabic Penn Treebank for this shared task andfor their support all along the process.
We thankAlon Itai and MILA, the knowledge center for pro-cessing Hebrew, for kindly making the Hebrew tree-bank and morphological analyzer available for us,Anne Abeill?
for allowing us to use the French tree-bank, and Key-Sun Choi for the Kaist Korean Tree-bank.
We thank Grzegorz Chrupa?a for providingthe morphological analyzer Morfette, and Joachim44The data set will be made available as soon as possible underthe license distribution of the shared-task, with the exceptionof the Arabic data, which will continue to be distributed by theLDC.176Wagner for his LeafAncestor implementation.
Wefinally thank ?zlem ?etinog?lu, Yuval Marton, BenoitCrabb?
and Benoit Sagot who have been nothing butsupportive during all that time.At the end of this shared task (though watch outfor further updates and analyses), what remains to bementioned is our deep gratitude to all people involved,either data providers or participants.
Without all ofyou, this shared task would not have been possible.ReferencesAnne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.2003.
Building a treebank for French.
In Anne Abeill?,editor, Treebanks.
Kluwer, Dordrecht.Szymon Acedan?ski.
2010.
A Morphosyntactic Brill Tag-ger for Inflectional Languages.
In Advances in NaturalLanguage Processing, volume 6233 of Lecture Notesin Computer Science, pages 3?14.
Springer-Verlag.Meni Adler and Michael Elhadad.
2006.
An unsupervisedmorpheme-based HMM for Hebrew morphological dis-ambiguation.
In Proceedings COLING-ACL, pages665?672, Sydney, Australia.Meni Adler, Yoav Goldberg, David Gabay, and MichaelElhadad.
2008.
Unsupervised lexicon-based resolutionof unknown words for full morphological analysis.
InProceedings of ACL-08: HLT, pages 728?736, Colum-bus, OH.Meni Adler.
2007.
Hebrew Morphological Disambigua-tion: An Unsupervised Stochastic Word-based Ap-proach.
Ph.D. thesis, Ben-Gurion University of theNegev.Itziar Aduriz, Jos?
Mar?a Arriola, Xabier Artola, A D?azde Ilarraza, et al1997.
Morphosyntactic disambigua-tion for Basque based on the constraint grammar for-malism.
In Proceedings of RANLP, Tzigov Chark, Bul-garia.Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?akiAlegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,et al2000.
A word-grammar based morphologicalanalyzer for agglutinative languages.
In Proceedingsof COLING, pages 1?7, Saarbr?cken, Germany.Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-dia, and Maite Oronoz.
2003.
Construction of aBasque dependency treebank.
In Proceedings of the2nd Workshop on Treebanks and Linguistic Theories(TLT), pages 201?204, V?xj?, Sweden.Zeljko Agic, Danijela Merkler, and Dasa Berovic.
2013.Parsing Croatian and Serbian by using Croatian depen-dency treebanks.
In Proceedings of the Fourth Work-shop on Statistical Parsing of Morphologically RichLanguages (SPMRL), Seattle, WA.I.
Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, andK.
Fern?ndez.
2008.
From dependencies to con-stituents in the reference corpus for the processing ofBasque.
In Procesamiento del Lenguaje Natural, no41 (2008), pages 147?154.
XXIV edici?n del CongresoAnual de la Sociedad Espa?ola para el Procesamientodel Lenguaje Natural (SEPLN).Bharat Ram Ambati, Samar Husain, Joakim Nivre, andRajeev Sangal.
2010.
On the role of morphosyntacticfeatures in Hindi dependency parsing.
In Proceedingsof the NAACL/HLT Workshop on Statistical Parsing ofMorphologically Rich Languages (SPMRL 2010), LosAngeles, CA.Mohammed Attia, Jennifer Foster, Deirdre Hogan,Joseph Le Roux, Lamia Tounsi, and Josef van Gen-abith.
2010.
Handling unknown words in statisticallatent-variable parsing models for Arabic, English andFrench.
In Proceedings of the NAACL/HLT Workshopon Statistical Parsing of Morphologically Rich Lan-guages (SPMRL), Los Angeles, CA.Miguel Ballesteros and Joakim Nivre.
2012.
MaltOpti-mizer: An optimization tool for MaltParser.
In Pro-ceedings of EACL, pages 58?62, Avignon, France.Miguel Ballesteros.
2013.
Effective morphological fea-ture selection with MaltOptimizer at the SPMRL 2013shared task.
In Proceedings of the Fourth Workshop onStatistical Parsing of Morphologically-Rich Languages,pages 53?60, Seattle, WA.Kepa Bengoetxea and Koldo Gojenola.
2010.
Appli-cation of different techniques to dependency parsingof Basque.
In Proceedings of the NAACL/HLT Work-shop on Statistical Parsing of Morphologically RichLanguages (SPMRL 2010), Los Angeles, CA.Philip Bille.
2005.
A survey on tree edit distance and re-lated problems.
Theoretical Computer Science, 337(1?3):217?239, 6.Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,Thomas Mueller, and Wolfgang Seeker.
2013.
(Re)ranking meets morphosyntax: State-of-the-art re-sults from the SPMRL 2013 shared task.
In Proceed-ings of the Fourth Workshop on Statistical Parsingof Morphologically-Rich Languages, pages 134?144,Seattle, WA.Ezra Black, Steven Abney, Dan Flickinger, ClaudiaGdaniec, Ralph Grishman, Philip Harrison, DonaldHindle, Robert Ingria, Frederick Jelinek, Judith Kla-vans, Mark Liberman, Mitchell Marcus, Salim Roukos,Beatrice Santorini, and Tomek Strzalkowski.
1991.
Aprocedure for quantitatively comparing the syntacticcoverage of English grammars.
In Proceedings of theDARPA Speech and Natural Language Workshop 1991,pages 306?311, Pacific Grove, CA.177Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Proceed-ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,Korea.Bernd Bohnet.
2010.
Top accuracy and fast dependencyparsing is not a contradiction.
In Proceedings of COL-ING, pages 89?97, Beijing, China.Adriane Boyd.
2007.
Discontinuity revisited: An im-proved conversion to context-free representations.
InProceedings of the Linguistic Annotation Workshop,Prague, Czech Republic.Sabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER treebank.In Proceedings of the First Workshop on Treebanksand Linguistic Theories (TLT), pages 24?41, Sozopol,Bulgaria.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL, pages 149?164, New York, NY.Tim Buckwalter.
2002.
Arabic morphological analyzerversion 1.0.
Linguistic Data Consortium.Tim Buckwalter.
2004.
Arabic morphological analyzerversion 2.0.
Linguistic Data Consortium.Marie Candito and Djam?
Seddah.
2010.
Parsing wordclusters.
In Proceedings of the NAACL/HLT Workshopon Statistical Parsing of Morphologically Rich Lan-guages (SPMRL 2010), Los Angeles, CA.Marie Candito, Benoit Crabb?, and Pascal Denis.
2010.Statistical French dependency parsing: Treebank con-version and first results.
In Proceedings of LREC, Val-letta, Malta.Xavier Carreras, Michael Collins, and Terry Koo.
2008.TAG, dynamic programming, and the perceptron for ef-ficient, feature-rich parsing.
In Proceedings of CoNLL,pages 9?16, Manchester, UK.Eugene Charniak and Mark Johnson.
2005.
Course-to-fine n-best-parsing and maxent discriminative rerank-ing.
In Proceedings of ACL, pages 173?180, Barcelona,Spain.Eugene Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In AAAI/IAAI, pages598?603.Eugene Charniak.
2000.
A maximum entropy inspiredparser.
In Proceedings of NAACL, pages 132?139, Seat-tle, WA.Jinho D. Choi and Martha Palmer.
2011.
Statistical de-pendency parsing in Korean: From corpus generationto automatic parsing.
In Proceedings of Second Work-shop on Statistical Parsing of Morphologically RichLanguages, pages 1?11, Dublin, Ireland.Jinho D. Choi and Martha Palmer.
2012.
Guidelinesfor the Clear Style Constituent to Dependency Conver-sion.
Technical Report 01-12, University of Coloradoat Boulder.Key-sun Choi, Young S. Han, Young G. Han, and Oh W.Kwon.
1994.
KAIST Tree Bank Project for Korean:Present and Future Development.
In In Proceedingsof the International Workshop on Sharable NaturalLanguage Resources, pages 7?14, Nara, Japan.Jinho D. Choi.
2013.
Preparing Korean data for theshared task on parsing morphologically rich languages.arXiv:1309.1649.Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-abith.
2008.
Learning morphology with Morfette.
InProceedings of LREC, Marrakech, Morocco.Tagyoung Chung, Matt Post, and Daniel Gildea.
2010.Factors affecting the accuracy of Korean parsing.
InProceedings of the NAACL/HLT Workshop on Sta-tistical Parsing of Morphologically Rich Languages(SPMRL 2010), Los Angeles, CA.Volkan Cirik and H?sn?
S?ensoy.
2013.
The AI-KUsystem at the SPMRL 2013 shared task: Unsuper-vised features for dependency parsing.
In Proceed-ings of the Fourth Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 68?75, Seat-tle, WA.Michael Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational Linguistics,29(4):589?637.Matthieu Constant, Marie Candito, and Djam?
Seddah.2013.
The LIGM-Alpage architecture for the SPMRL2013 shared task: Multiword expression analysis anddependency parsing.
In Proceedings of the FourthWorkshop on Statistical Parsing of Morphologically-Rich Languages, pages 46?52, Seattle, WA.Anna Corazza, Alberto Lavelli, Giogio Satta, and RobertoZanoli.
2004.
Analyzing an Italian treebank withstate-of-the-art statistical parsers.
In Proceedings ofthe Third Workshop on Treebanks and Linguistic Theo-ries (TLT), T?bingen, Germany.Benoit Crabb?
and Marie Candito.
2008.
Exp?riencesd?analyse syntaxique statistique du fran?ais.
In Actesde la 15?me Conf?rence sur le Traitement Automatiquedes Langues Naturelles (TALN?08), pages 45?54, Avi-gnon, France.D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?sKocsor.
2005.
The Szeged treebank.
In Proceedings ofthe 8th International Conference on Text, Speech andDialogue (TSD), Lecture Notes in Computer Science,pages 123?132, Berlin / Heidelberg.
Springer.Eric De La Clergerie.
2013.
Exploring beam-basedshift-reduce dependency parsing with DyALog: Re-sults from the SPMRL 2013 shared task.
In Proceed-ings of the Fourth Workshop on Statistical Parsing of178Morphologically-Rich Languages, pages 81?89, Seat-tle, WA.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The stanford typed dependencies repre-sentation.
In Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation.Mona Diab, Nizar Habash, Owen Rambow, and RyanRoth.
2013.
LDC Arabic treebanks and associated cor-pora: Data divisions manual.
Technical Report CCLS-13-02, Center for Computational Learning Systems,Columbia University.Eva Ejerhed and Gunnel K?llgren.
1997.
StockholmUme?
Corpus.
Version 1.0.
Department of Linguis-tics, Ume?
University and Department of Linguistics,Stockholm University.Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-nus ?str?m.
1992.
The linguistic annotation systemof the Stockholm?Ume?
Corpus project.
TechnicalReport 33, University of Ume?
: Department of Linguis-tics.Nerea Ezeiza, I?aki Alegria, Jos?
Mar?a Arriola, Rub?nUrizar, and Itziar Aduriz.
1998.
Combining stochasticand rule-based methods for disambiguation in aggluti-native languages.
In Proceedings of COLING, pages380?384, Montr?al, Canada.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, conditionalrandom field parsing.
In Proceedings of ACL, pages959?967, Columbus, OH.Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-jing Wang, and Hinrich Sch?tze.
2013.
Knowledgesources for constituent parsing of German, a morpho-logically rich and less-configurational language.
Com-putational Linguistics, 39(1):57?85.Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza.
2013.Exploiting the contribution of morphological informa-tion to parsing: the BASQUE TEAM system in theSPRML?2013 shared task.
In Proceedings of the FourthWorkshop on Statistical Parsing of Morphologically-Rich Languages, pages 61?67, Seattle, WA.Yoav Goldberg and Michael Elhadad.
2010a.
Easy-firstdependency parsing of Modern Hebrew.
In Proceed-ings of the NAACL/HLT Workshop on Statistical Pars-ing of Morphologically Rich Languages (SPMRL 2010),Los Angeles, CA.Yoav Goldberg and Michael Elhadad.
2010b.
An ef-ficient algorithm for easy-first non-directional depen-dency parsing.
In Proceedings of HLT: NAACL, pages742?750, Los Angeles, CA.Yoav Goldberg and Reut Tsarfaty.
2008.
A single frame-work for joint morphological segmentation and syntac-tic parsing.
In Proceedings of ACL, Columbus, OH.Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008.EM can find pretty good HMM POS-taggers (whengiven a good start).
In Proc.
of ACL, Columbus, OH.Yoav Goldberg, Reut Tsarfaty, Meni Adler, and MichaelElhadad.
2009.
Enhancing unlexicalized parsing per-formance using a wide coverage lexicon, fuzzy tag-setmapping, and EM-HMM-based lexical probabilities.
InProceedings of EALC, pages 327?335, Athens, Greece.Yoav Goldberg.
2011.
Automatic syntactic processing ofModern Hebrew.
Ph.D. thesis, Ben Gurion Universityof the Negev.David Graff, Mohamed Maamouri, Basma Bouziri, Son-dos Krouna, Seth Kulick, and Tim Buckwalter.
2009.Standard Arabic Morphological Analyzer (SAMA) ver-sion 3.1.
Linguistic Data Consortium LDC2009E73.Spence Green and Christopher D. Manning.
2010.
BetterArabic parsing: Baselines, evaluations, and analysis.In Proceedings of COLING, pages 394?402, Beijing,China.Nathan Green, Loganathan Ramasamy, and Zden?k?abokrtsk?.
2012.
Using an SVM ensemble system forimproved Tamil dependency parsing.
In Proceedingsof the ACL 2012 Joint Workshop on Statistical Pars-ing and Semantic Processing of Morphologically RichLanguages, pages 72?77, Jeju, Korea.Spence Green, Marie-Catherine de Marneffe, and Christo-pher D. Manning.
2013.
Parsing models for identify-ing multiword expressions.
Computational Linguistics,39(1):195?227.Noemie Guthmann, Yuval Krymolowski, Adi Milea, andYoad Winter.
2009.
Automatic annotation of morpho-syntactic dependencies in a Modern Hebrew Treebank.In Proceedings of the Eighth International Workshop onTreebanks and Linguistic Theories (TLT), Groningen,The Netherlands.Nizar Habash and Ryan Roth.
2009.
CATiB: TheColumbia Arabic Treebank.
In Proceedings of ACL-IJCNLP, pages 221?224, Suntec, Singapore.Nizar Habash, Ryan Gabbard, Owen Rambow, SethKulick, and Mitch Marcus.
2007.
Determining case inArabic: Learning complex linguistic behavior requirescomplex linguistic features.
In Proceedings of EMNLP-CoNLL, pages 1084?1092, Prague, Czech Republic.Nizar Habash, Reem Faraj, and Ryan Roth.
2009a.
Syn-tactic Annotation in the Columbia Arabic Treebank.
InProceedings of MEDAR International Conference onArabic Language Resources and Tools, Cairo, Egypt.Nizar Habash, Owen Rambow, and Ryan Roth.
2009b.MADA+TOKAN: A toolkit for Arabic tokenization,diacritization, morphological disambiguation, POS tag-ging, stemming and lemmatization.
In Proceedings ofthe Second International Conference on Arabic Lan-guage Resources and Tools.
Cairo, Egypt.179Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publishers.Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and BarboraVidov?-Hladk?.
2000.
The Prague Dependency Tree-bank: A three-level annotation scenario.
In AnneAbeill?, editor, Treebanks: Building and Using ParsedCorpora.
Kluwer Academic Publishers.P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz.
2007.HunPos ?
an open source trigram tagger.
In Proceed-ings of ACL, pages 209?212, Prague, Czech Republic.Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,Be?ta Megyesi, Mattias Nilsson, and Markus Saers.2007.
Single malt or blended?
A study in multilingualparser optimization.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages933?939, Prague, Czech Republic.Chung-hye Han, Na-Rae Han, Eon-Suk Ko, MarthaPalmer, and Heejong Yi.
2002.
Penn Korean Treebank:Development and evaluation.
In Proceedings of the16th Pacific Asia Conference on Language, Informationand Computation, Jeju, Korea.Tilman H?hle.
1986.
Der Begriff "Mittelfeld", Anmerkun-gen ?ber die Theorie der topologischen Felder.
In Ak-ten des Siebten Internationalen Germanistenkongresses1985, pages 329?340, G?ttingen, Germany.Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010.Self-training with products of latent variable grammars.In Proceedings of EMNLP, pages 12?22, Cambridge,MA.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings of ACL,pages 586?594, Columbus, OH.Alon Itai and Shuly Wintner.
2008.
Language resourcesfor Hebrew.
Language Resources and Evaluation,42(1):75?98, March.Mark Johnson.
1998.
PCFG models of linguistic treerepresentations.
Computational Linguistics, 24(4):613?632.Laura Kallmeyer and Wolfgang Maier.
2013.
Data-drivenparsing using probabilistic linear context-free rewritingsystems.
Computational Linguistics, 39(1).Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and ArtoAnttila.
1995.
Constraint Grammar: a language-independent system for parsing unrestricted text.
Wal-ter de Gruyter.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL, pages423?430, Sapporo, Japan.Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.2006.
Is it really that difficult to parse German?
In Pro-ceedings of EMNLP, pages 111?119, Sydney, Australia,July.Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-nick Versley.
2008.
How to compare treebanks.
InProceedings of LREC, pages 2322?2329, Marrakech,Morocco.Sandra K?bler.
2008.
The PaGe 2008 shared task onparsing German.
In Proceedings of the Workshop onParsing German, pages 55?63, Columbus, OH.Seth Kulick, Ryan Gabbard, and Mitch Marcus.
2006.Parsing the Arabic Treebank: Analysis and Improve-ments.
In Proceedings of the Treebanks and LinguisticTheories Conference, pages 31?42, Prague, Czech Re-public.Joseph Le Roux, Benoit Sagot, and Djam?
Seddah.
2012.Statistical parsing of Spanish and data driven lemmati-zation.
In Proceedings of the Joint Workshop on Statis-tical Parsing and Semantic Processing of Morphologi-cally Rich Languages, pages 55?61, Jeju, Korea.Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.1997.
Bracketing Guidelines for Korean Syntactic TreeTagged Corpus.
Technical Report CS/TR-97-112, De-partment of Computer Science, KAIST.Roger Levy and Christopher D. Manning.
2003.
Is itharder to parse Chinese, or the Chinese treebank?
InProceedings of ACL, Sapporo, Japan.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andHubert Jin.
2004a.
Arabic Treebank: Part 2 v 2.0.LDC catalog number LDC2004T02.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004b.
The Penn Arabic Treebank:Building a large-scale annotated Arabic corpus.
InNEMLAR Conference on Arabic Language Resourcesand Tools, pages 102?109, Cairo, Egypt.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andHubert Jin.
2005.
Arabic Treebank: Part 1 v 3.0.
LDCcatalog number LDC2005T02.Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-deche, Wigdan Mekki, Sondos Krouna, and BasmaBouziri.
2009.
The Penn Arabic Treebank part 3 ver-sion 3.1.
Linguistic Data Consortium LDC2008E22.Wolfgang Maier, Miriam Kaeshammer, and LauraKallmeyer.
2012.
Data-driven PLCFRS parsing re-visited: Restricting the fan-out to two.
In Proceedingsof the Eleventh International Conference on Tree Ad-joining Grammars and Related Formalisms (TAG+11),Paris, France.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn TreeBank.
ComputationalLinguistics, 19(2):313?330.Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,and Mario Figueiredo.
2010.
Turbo parsers: Depen-dency parsing by approximate variational inference.
InProceedings of EMNLP, pages 34?44, Cambridge, MA.180Yuval Marton, Nizar Habash, and Owen Rambow.
2013a.Dependency parsing of Modern Standard Arabic withlexical and inflectional features.
Computational Lin-guistics, 39(1):161?194.Yuval Marton, Nizar Habash, Owen Rambow, and SarahAlkhulani.
2013b.
SPMRL?13 shared task system:The CADIM Arabic dependency parser.
In Proceed-ings of the Fourth Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 76?80, Seat-tle, WA.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of HLT:NAACL, pages 152?159, New York, NY.Ryan T. McDonald, Koby Crammer, and Fernando C. N.Pereira.
2005.
Online large-margin training of depen-dency parsers.
In Proceedings of ACL, pages 91?98,Ann Arbor, MI.Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, KuzmanGanchev, Keith Hall, Slav Petrov, Hao Zhang, OscarTackstrom, Claudia Bedini, Nuria Bertomeu Castello,and Jungmee Lee.
2013.
Universal dependency anno-tation for multilingual parsing.
In Proceedings of ACL,Sofia, Bulgaria.Igor Mel?c?uk.
2001.
Communicative Organization in Nat-ural Language: The Semantic-Communicative Struc-ture of Sentences.
J. Benjamins.Knowledge Center for Processing HebrewMILA.
2008.
Hebrew morphological analyzer.http://mila.cs.technion.ac.il.Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-nando Sanchez, and Satoshi Sekine.
2000.
A treebankof Spanish and its application to parsing.
In Proceed-ings of LREC, Athens, Greece.Joakim Nivre and Be?ta Megyesi.
2007.
Bootstrapping aSwedish treeebank using cross-corpus harmonizationand annotation projection.
In Proceedings of the 6thInternational Workshop on Treebanks and LinguisticTheories, pages 97?102, Bergen, Norway.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006.
Tal-banken05: A Swedish treebank with phrase structureand dependency annotation.
In Proceedings of LREC,pages 1392?1395, Genoa, Italy.Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007a.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of the CoNLL Shared Task Ses-sion of EMNLP-CoNLL 2007, pages 915?932, Prague,Czech Republic.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,and Erwin Marsi.
2007b.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Slav Petrov and Ryan McDonald.
2012.
Overview of the2012 Shared Task on Parsing the Web.
In Proceedingsof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), a NAACL-HLT 2012workshop, Montreal, Canada.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of COLING-ACL, Sydney, Australia.Slav Petrov.
2009.
Coarse-to-Fine Natural LanguageProcessing.
Ph.D. thesis, University of California atBekeley, Berkeley, CA.Slav Petrov.
2010.
Products of random latent variablegrammars.
In Proceedings of HLT: NAACL, pages 19?27, Los Angeles, CA.Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa?
L. G?rski,and Barbara Lewandowska-Tomaszczyk, editors.
2012.Narodowy Korpus Jkezyka Polskiego.
WydawnictwoNaukowe PWN, Warsaw.Ines Rehbein and Josef van Genabith.
2007a.
Eval-uating Evaluation Measures.
In Proceedings of the16th Nordic Conference of Computational LinguisticsNODALIDA-2007, Tartu, Estonia.Ines Rehbein and Josef van Genabith.
2007b.
Treebankannotation schemes and parser evaluation for German.In Proceedings of EMNLP-CoNLL, Prague, Czech Re-public.Ines Rehbein.
2011.
Data point selection for self-training.In Proceedings of the Second Workshop on StatisticalParsing of Morphologically Rich Languages, pages 62?67, Dublin, Ireland.Kenji Sagae and Alon Lavie.
2006.
Parser combinationby reparsing.
In Proceedings of HLT-NAACL, pages129?132, New York, NY.Geoffrey Sampson and Anna Babarczy.
2003.
A test ofthe leaf-ancestor metric for parse accuracy.
NaturalLanguage Engineering, 9(04):365?380.Natalie Schluter and Josef van Genabith.
2007.
Prepar-ing, restructuring, and augmenting a French Treebank:Lexicalised parsers or coherent treebanks?
In Proc.
ofPACLING 07, Melbourne, Australia.Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004.SMOR: A German computational morphology coveringderivation, composition and inflection.
In Proceedingsof LREC, Lisbon, Portugal.Djam?
Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,Josef van Genabith, and Marie Candito.
2010.Lemmatization and statistical lexicalized parsing ofmorphologically-rich languages.
In Proceedings of theFirst Workshop on Statistical Parsing of Morphologi-cally Rich Languages (SPMRL), Los Angeles, CA.Wolfgang Seeker and Jonas Kuhn.
2012.
Making el-lipses explicit in dependency conversion for a German181treebank.
In Proceedings of LREC, pages 3132?3139,Istanbul, Turkey.Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, andMasaaki Nagata.
2012.
Bayesian symbol-refined treesubstitution grammars for syntactic parsing.
In Pro-ceedings of ACL, pages 440?448, Jeju, Korea.Anthony Sigogne, Matthieu Constant, and Eric Laporte.2011.
French parsing enhanced with a word clusteringmethod based on a syntactic lexicon.
In Proceedingsof the Second Workshop on Statistical Parsing of Mor-phologically Rich Languages, pages 22?27, Dublin,Ireland.Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,and Noa Nativ.
2001.
Building a tree-bank of ModernHebrew text.
Traitement Automatique des Langues,42:347?380.Marek S?widzin?ski and Marcin Wolin?ski.
2010.
Towardsa bank of constituent parse trees for Polish.
In Pro-ceedings of Text, Speech and Dialogue, pages 197?204,Brno, Czech Republic.Ulf Teleman.
1974.
Manual f?r grammatisk beskrivningav talad och skriven svenska.
Studentlitteratur.Lucien Tesni?re.
1959.
?l?ments De Syntaxe Structurale.Klincksieck, Paris.Reut Tsarfaty and Khalil Sima?an.
2010.
Modeling mor-phosyntactic agreement in constituency-based parsingof Modern Hebrew.
In Proceedings of the First Work-shop on Statistical Parsing of Morphologically RichLanguages (SPMRL), Los Angeles, CA.Reut Tsarfaty, Djame Seddah, Yoav Goldberg, SandraK?bler, Marie Candito, Jennifer Foster, Yannick Vers-ley, Ines Rehbein, and Lamia Tounsi.
2010.
Statisticalparsing for morphologically rich language (SPMRL):What, how and whither.
In Proceedings of the Firstworkshop on Statistical Parsing of MorphologicallyRich Languages (SPMRL), Los Angeles, CA.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2011.
Evaluating dependency parsing: Robust andheuristics-free cross-framework evaluation.
In Pro-ceedings of EMNLP, Edinburgh, UK.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2012a.
Cross-framework evaluation for statistical pars-ing.
In Proceeding of EACL, Avignon, France.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2012b.
Joint evaluation for segmentation and parsing.In Proceedings of ACL, Jeju, Korea.Reut Tsarfaty, Djam?
Seddah, Sandra K?bler, and JoakimNivre.
2012c.
Parsing morphologically rich languages:Introduction to the special issue.
Computational Lin-guistics, 39(1):15?22.Reut Tsarfaty.
2010.
Relational-Realizational Parsing.Ph.D.
thesis, University of Amsterdam.Reut Tsarfaty.
2013.
A unified morpho-syntactic schemeof Stanford dependencies.
In Proceedings of ACL,Sofia, Bulgaria.Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgyM?ra, Zolt?n Alexin, and J?nos Csirik.
2010.
Hungar-ian Dependency Treebank.
In Proceedings of LREC,Valletta, Malta.Joachim Wagner.
2012.
Detecting Grammatical Errorswith Treebank-Induced Probabilistic Parsers.
Ph.D.thesis, Dublin City University.Marcin Wolin?ski, Katarzyna G?owin?ska, and MarekS?widzin?ski.
2011.
A preliminary version ofSk?adnica?a treebank of Polish.
In Proceedings ofthe 5th Language & Technology Conference, pages299?303, Poznan?, Poland.Alina Wr?blewska.
2012.
Polish Dependency Bank.
Lin-guistic Issues in Language Technology, 7(1):1?15.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of ACL:HLT, pages 188?193, Portland,OR.J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.2013.
magyarlanc: A toolkit for morphological anddependency parsing of Hungarian.
In Proceedings ofRANLP, pages 763?771, Hissar, Bulgaria.182
