Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 497?504,Sydney, July 2006. c?2006 Association for Computational LinguisticsQuestionBank: Creating a Corpus of Parse-Annotated QuestionsJohn Judge1, Aoife Cahill1, and Josef van Genabith1,21National Centre for Language Technology and School of Computing,Dublin City University, Dublin, Ireland2IBM Dublin Center for Advanced Studies,IBM Dublin, Ireland{jjudge,acahill,josef}@computing.dcu.ieAbstractThis paper describes the development ofQuestionBank, a corpus of 4000 parse-annotated questions for (i) use in trainingparsers employed in QA, and (ii) evalua-tion of question parsing.
We present a se-ries of experiments to investigate the ef-fectiveness of QuestionBank as both anexclusive and supplementary training re-source for a state-of-the-art parser in pars-ing both question and non-question testsets.
We introduce a new method forrecovering empty nodes and their an-tecedents (capturing long distance depen-dencies) from parser output in CFG treesusing LFG f-structure reentrancies.
Ourmain findings are (i) using QuestionBanktraining data improves parser performanceto 89.75% labelled bracketing f-score, anincrease of almost 11% over the base-line; (ii) back-testing experiments on non-question data (Penn-II WSJ Section 23)shows that the retrained parser does notsuffer a performance drop on non-questionmaterial; (iii) ablation experiments showthat the size of training material providedby QuestionBank is sufficient to achieveoptimal results; (iv) our method for recov-ering empty nodes captures long distancedependencies in questions from the ATIScorpus with high precision (96.82%) andlow recall (39.38%).
In summary, Ques-tionBank provides a useful new resourcein parser-based QA research.1 IntroductionParse-annotated corpora (treebanks) are crucial fordeveloping machine learning and statistics-basedparsing resources for a given language or task.Large treebanks are available for major languages,however these are often based on a specific texttype or genre, e.g.
financial newspaper text (thePenn-II Treebank (Marcus et al, 1993)).
This canlimit the applicability of grammatical resources in-duced from treebanks in that such resources un-derperform when used on a different type of textor for a specific task.In this paper we present work on creating Ques-tionBank, a treebank of parse-annotated questions,which can be used as a supplementary training re-source to allow parsers to accurately parse ques-tions (as well as other text).
Alternatively, the re-source can be used as a stand-alone training corpusto train a parser specifically for questions.
Eitherscenario will be useful in training parsers for usein question answering (QA) tasks, and it also pro-vides a suitable resource to evaluate the accuracyof these parsers on questions.We use a semi-automatic ?bootstrapping?method to create the question treebank from rawtext.
We show that a parser trained on the ques-tion treebank alone can accurately parse ques-tions.
Training on a combined corpus consisting ofthe question treebank and an established trainingset (Sections 02-21 of the Penn-II Treebank), theparser gives state-of-the-art performance on bothquestions and a non-question test set (Section 23of the Penn-II Treebank).Section 2 describes background work and mo-tivation for the research presented in this paper.Section 3 describes the data we used to createthe corpus.
In Section 4 we describe the semi-automatic method to ?bootstrap?
the question cor-pus, discuss some interesting and problematicphenomena, and show how the manual vs. auto-matic workload distribution changed as work pro-gressed.
Two sets of experiments using our newquestion corpus are presented in Section 5.
InSection 6 we introduce a new method for recover-ing empty nodes and their antecedents using Lex-ical Functional Grammar (LFG) f-structure reen-497trancies.
Section 7 concludes and outlines futurework.2 Background and MotivationHigh quality probabilistic, treebank-based parsingresources can be rapidly induced from appropri-ate treebank material.
However, treebank- andmachine learning-based grammatical resources re-flect the characteristics of the training data.
Theygenerally underperform on test data substantiallydifferent from the training data.Previous work on parser performance and do-main variation by Gildea (2001) showed that bytraining a parser on the Penn-II Treebank and test-ing on the Brown corpus, parser accuracy drops by5.7% compared to parsing the Wall Street Journal(WSJ) based Penn-II Treebank Section 23.
Thisshows a negative effect on parser performanceeven when the test data is not radically differentfrom the training data (both the Penn II and Browncorpora consist primarily of written texts of Amer-ican English, the main difference is the consider-ably more varied nature of the text in the Browncorpus).
Gildea also shows how to resolve thisproblem by adding appropriate data to the trainingcorpus, but notes that a large amount of additionaldata has little impact if it is not matched to the testmaterial.Work on more radical domain variance and onadapting treebank-induced LFG resources to anal-yse ATIS (Hemphill et al, 1990) question mate-rial is described in Judge et al (2005).
The re-search established that even a small amount of ad-ditional training data can give a substantial im-provement in question analysis in terms of bothCFG parse accuracy and LFG grammatical func-tional analysis, with no significant negative effectson non-question analysis.
Judge et al (2005) sug-gest, however, that further improvements are pos-sible given a larger question training corpus.Clark et al (2004) worked specifically withquestion parsing to generate dependencies for QAwith Penn-II treebank-based Combinatory Cate-gorial Grammars (CCG?s).
They use ?what?
ques-tions taken from the TREC QA datasets as the ba-sis for a What-Question corpus with CCG annota-tion.3 Data SourcesThe raw question data for QuestionBank comesfrom two sources, the TREC 8-11 QA tracktest sets1, and a question classifier training setproduced by the Cognitive Computation Group(CCG2) at the University of Illinois at Urbana-Champaign.3 We use equal amounts of data fromeach source so as not to bias the corpus to eitherdata source.3.1 TREC QuestionsThe TREC evaluations have become the standardevaluation for QA systems.
Their test sets con-sist primarily of fact seeking questions with someimperative statements which request information,e.g.
?List the names of cell phone manufactur-ers.?
We included 2000 TREC questions in theraw data from which we created the question tree-bank.
These 2000 questions consist of the testquestions for the first three years of the TREC QAtrack (1893 questions) and 107 questions from the2003 TREC test set.3.2 CCG Group QuestionsThe CCG provide a number of resources for de-veloping QA systems.
One of these resources isa set of 5500 questions and their answer types foruse in training question classifiers.
The 5500 ques-tions were stripped of answer type annotation, du-plicated TREC questions were removed and 2000questions were used for the question treebank.The CCG 5500 questions come from a numberof sources (Li and Roth, 2002) and some of thesequestions contain minor grammatical mistakes sothat, in essence, this corpus is more representa-tive of genuine questions that would be put to aworking QA system.
A number of changes in to-kenisation were corrected (eg.
separating contrac-tions), but the minor grammatical errors were leftunchanged because we believe that it is necessaryfor a parser for question analysis to be able to copewith this sort of data if it is to be used in a workingQA system.4 Creating the Treebank4.1 Bootstrapping a Question TreebankThe algorithm used to generate the question tree-bank is an iterative process of parsing, manual cor-rection, retraining, and parsing.1http://trec.nist.gov/data/qa.html2Note that the acronym CCG here refers to CognitiveComputation Group, rather than Combinatory CategorialGrammar mentioned in Section 2.3http://l2r.cs.uiuc.edu/ cogcomp/tools.php498Algorithm 1 Induce a parse-annotated treebankfrom raw datarepeatParse a new section of raw dataManually correct errors in the parser outputAdd the corrected data to the training setExtract a new grammar for the parseruntil All the data has been processedAlgorithm 1 summarises the bootstrapping al-gorithm.
A section of raw data is parsed.
Theparser output is then manually corrected, andadded to the parser?s training corpus.
A new gram-mar is then extracted, and the next section of rawdata is parsed.
This process continues until all thedata has been parsed and hand corrected.4.2 ParserThe parser used to process the raw questions priorto manual correction was that of Bikel (2002)4 ,a retrainable emulation of Collins (1999) model2 parser.
Bikel?s parser is a history-based parserwhich uses a lexicalised generative model to parsesentences.
We used WSJ Sections 02-21 of thePenn-II Treebank to train the parser for the first it-eration of the algorithm.
The training corpus forsubsequent iterations consisted of the WSJ ma-terial and increasing amounts of processed ques-tions.4.3 Basic Corpus Development StatisticsOur question treebank was created over a periodof three months at an average annotation speed ofabout 60 questions per day.
This is quite rapidfor treebank development.
The speed of the pro-cess was helped by two main factors: the questionsare generally quite short (typically about 10 wordslong), and, due to retraining on the continually in-creasing training set, the quality of the parses out-put by the parser improved dramatically during thedevelopment of the treebank, with the effect thatcorrections during the later stages were generallyquite small and not as time consuming as duringthe initial phases of the bootstrapping process.For example, in the first week of the project thetrees from the parser were of relatively poor qual-ity and over 78% of the trees needed to be cor-rected manually.
This slowed the annotation pro-cess considerably and parse-annotated questions4Downloaded from http://www.cis.upenn.edu/?dbikel/software.html#stat-parserwere being produced at an average rate of 40 treesper day.
During the later stages of the project thishad changed dramatically.
The quality of treesfrom the parser was much improved with less than20% of the trees requiring manual correction.
Atthis stage parse-annotated questions were beingproduced at an average rate of 90 trees per day.4.4 Corpus Development Error AnalysisSome of the more frequent errors in the parseroutput pertain to the syntactic analysis of WH-phrases (WHNP, WHPP, etc).
In Sections 02-21of the Penn-II Treebank, these are used more oftenin relative clause constructions than in questions.As a result many of the corpus questions weregiven syntactic analyses corresponding to relativeclauses (SBAR with an embedded S) instead of asquestions (SBARQ with an embedded SQ).
Figure1 provides an example.SBARWHNPWPWhoSVPVBDcreatedNPDTtheNNMuppets(a)SBARQWHNPWPWhoSQVPVBDcreatedNPDTtheNNPSMuppets(b)Figure 1: Example tree before (a) and after correc-tion (b)Because the questions are typically short, an er-ror like this has quite a large effect on the accu-racy for the overall tree; in this case the f-scorefor the parser output (Figure 1(a)) would be only60%.
Errors of this nature were quite frequentin the first section of questions analysed by theparser, but with increased training material becom-ing available during successive iterations, this er-ror became less frequent and towards the end of499the project it was only seen in rare cases.WH-XP marking was the source of a number ofconsistent (though infrequent) errors during anno-tation.
This occurred mostly in PP constructionscontaining WHNPs.
The parser would output astructure like Figure 2(a), where the PP mother ofthe WHNP is not correctly labelled as a WHPP asin Figure 2(b).PPINbyWHNPWP$whoseNNauthorityWHPPINbyWHNPWP$whoseNNauthority(a) (b)Figure 2: WH-XP assignmentThe parser output often had to be rearrangedstructurally to varying degrees.
This was commonin the longer questions.
A recurring error in theparser output was failing to identify VPs in SQswith a single object NP.
In these cases the verband the object NP were left as daughters of theSQ node.
Figure 3(a) illustrates this, and Figure3(b) shows the corrected tree with the VP node in-serted.SBARQWHNPWPWhoSQVBDkilledNPGhandiSBARQWHNPWPWhoSQVPVBDkilledNPGhandi(a) (b)Figure 3: VP missing inside SQ with a single NPOn inspection, we found that the problem wascaused by copular constructions, which, accord-ing to the Penn-II annotation guidelines, do notfeature VP constituents.
Since almost half of thequestion data contain copular constructions, theparser trained on this data would sometimes mis-analyse non-copular constructions or, conversely,incorrectly bracket copular constructions using aVP constituent (Figure 4(a)).The predictable nature of these errors meant thatthey were simple to correct.
This is due to the par-ticular context in which they occur and the finitenumber of forms of the copular verb.SBARQWHNPWPWhatSQVPVBZisNPa fear of shadowsSBARQWHNPWPWhatSQVBZisNPa fear of shadows(a) (b)Figure 4: Erroneous VP in copular constructions5 Experiments with QuestionBankIn order to test the effect training on the questioncorpus has on parser performance, we carried outa number of experiments.
In cross-validation ex-periments with 90%/10% splits we use all 4000trees in the completed QuestionBank as the testset.
We performed ablation experiments to inves-tigate the effect of varying the amount of questionand non-question training data on the parser?s per-formance.
For these experiments we divided the4000 questions into two sets.
We randomly se-lected 400 trees to be held out as a gold standardtest set against which to evaluate, the remaining3600 trees were then used as a training corpus.5.1 Establishing the BaselineThe baseline we use for our experiments is pro-vided by Bikel?s parser trained on WSJ Sections02-21 of the Penn-II Treebank.
We test on all 4000questions in our question treebank, and also Sec-tion 23 of the Penn-II Treebank.QuestionBankCoverage 100F-Score 78.77WSJ Section 23Coverage 100F-Score 82.97Table 1: Baseline parsing resultsTable 1 shows the results for our baseline eval-uations on question and non-question test sets.While the coverage for both tests is high, theparser underperforms significantly on the questiontest set with a labelled bracketing f-score of 78.77compared to 82.97 on Section 23 of the Penn-IITreebank.
Note that unlike the published resultsfor Bikel?s parser in our evaluations we test onSection 23 and include punctuation.5.2 Cross-Validation ExperimentsWe carried out two cross-validation experiments.In the first experiment we perform a 10-fold cross-validation experiment using our 4000 question500treebank.
In each case a randomly selected set of10% of the questions in QuestionBank was heldout during training and used as a test set.
In thisway parses from unseen data were generated forall 4000 questions and evaluated against the Ques-tionBank trees.The second cross-validation experiment wassimilar to the first, but in each of the 10 folds wetrain on 90% of the 4000 questions in Question-Bank and on all of Sections 02-21 of the Penn-IITreebank.In both experiments we also backtest each of theten grammars on Section 23 of the Penn-II Tree-bank and report the average scores.QuestionBankCoverage 100F-Score 88.82Backtest on Sect 23Coverage 98.79F-Score 59.79Table 2: Cross-validation experiment using the4000 question treebankTable 2 shows the results for the first cross-validation experiment, using only the 4000 sen-tence QuestionBank.
Compared to Table 1, the re-sults show a significant improvement of over 10%on the baseline f-score for questions.
However, thetests on the non-question Section 23 data show notonly a significant drop in accuracy but also a dropin coverage.QuestionsCoverage 100F-Score 89.75Backtest on Sect 23Coverage 100F-Score 82.39Table 3: Cross-validation experiment using Penn-II Treebank Sections 02-21 and 4000 questionsTable 3 shows the results for the second cross-validation experiment using Sections 02-21 of thePenn-II Treebank and the 4000 questions in Ques-tionBank.
The results show an even greater in-crease on the baseline f-score than the experimentsusing only the question training set (Table 2).
Thenon-question results are also better and are com-parable to the baseline (Table 1).5.3 Ablation RunsIn a further set of experiments we investigated theeffect of varying the amount of data in the parser?straining corpus.
We experiment with varying boththe amount of QuestionBank and Penn-II Tree-bank data that the parser is trained on.
In eachexperiment we use the 400 question test set andSection 23 of the Penn-II Treebank to evaluateagainst, and the 3600 question training set de-scribed above and Sections 02-21 of the Penn-IITreebank as the basis for the parser?s training cor-pus.
We report on three experiments:In the first experiment we train the parser usingonly the 3600 question training set.
We performedten training and parsing runs in this experiment,incrementally reducing the size of the Question-Bank training corpus by 10% of the whole on eachrun.The second experiment is similar to the first butin each run we add Sections 02-21 of the Penn-IITreebank to the (shrinking) training set of ques-tions.The third experiment is the converse of the sec-ond, the amount of questions in the training setremains fixed (all 3600) and the amount of Penn-II Treebank material is incrementally reduced by10% on each run.506070809010010 20 30 40 50 60 70 80 90 100Coverage/F-ScorePercentage of 3600 questions in the training corpusFScore QuestionsFScore Section 23Coverage QuestionsCoverage Section 23Figure 5: Results for ablation experiment reducing3600 training questions in steps of 10%Figure 5 graphs the coverage and f-score forthe parser in tests on the 400 question test set,and Section 23 of the Penn-II Treebank in tenparsing runs with the amount of data in the 3600question training corpus reducing incrementallyon each run.
The results show that training on onlya small amount of questions, the parser can parsequestions with high accuracy.
For example whentrained on only 10% of the 3600 questions usedin this experiment, the parser successfully parsesall of the 400 question test set and achieves an f-score of 85.59.
However the results for the testson WSJ Section 23 are considerably worse.
Theparser never manages to parse the full test set, andthe best score at 59.61 is very low.Figure 6 graphs the results for the second abla-501506070809010010 20 30 40 50 60 70 80 90 100Coverage/F-ScorePercentage of 3600 questions in the training corpusFScore QuestionsFScore Section 23Coverage QuestionsCoverage Section 23Figure 6: Results for ablation experiment usingPTB Sections 02-21 (fixed) and reducing 3600questions in steps of 10%506070809010010 20 30 40 50 60 70 80 90 100Coverage/F-ScorePercentage of PTB Stetcions 2-21 in the training corpusFScore QuestionsFScore Section 23Coverage QuestionsCoverage Section 23Figure 7: Results for ablation experiment using3600 questions (fixed) and reducing PTB Sections02-21 in steps of 10%tion experiment.
The training set for the parserconsists of a fixed amount of Penn-II Treebankdata (Sections 02-21) and a reducing amount ofquestion data from the 3600 question training set.Each grammar is tested on both the 400 questiontest set, and WSJ Section 23.
The results hereare significantly better than in the previous exper-iment.
In all of the runs the coverage for both testsets is 100%, f-scores for the question test set de-crease as the amount of question data in the train-ing set is reduced (though they are still quite high.
)There is little change in the f-scores for the tests onSection 23, the results all fall in the range 82.36 to82.46, which is comparable to the baseline score.Figure 7 graphs the results for the third abla-tion experiment.
In this case the training set is afixed amount of the question training set describedabove (all 3600 questions) and a reducing amountof data from Sections 02-21 of the Penn Treebank.The graph shows that the parser performs consis-tently well on the question test set in terms of bothcoverage and accuracy.
The tests on Section 23,however, show that as the amount of Penn-II Tree-bank material in the training set decreases, the f-score also decreases.6 Long Distance DependenciesLong distance dependencies are crucial in theproper analysis of question material.
In Englishwh-questions, the fronted wh-constituent refers toan argument position of a verb inside the interrog-ative construction.
Compare the superficially sim-ilar1.
Who1 [t1] killed Harvey Oswald?2.
Who1 did Harvey Oswald kill [t1]?
(1) queries the agent (syntactic subject) of the de-scribed eventuality, while (2) queries the patient(syntactic object).
In the Penn-II and ATIS tree-banks, dependencies such as these are representedin terms of empty productions, traces and coindex-ation in CFG tree representations (Figure 8).SBARQWHNP-1WPWhoSQNP*T*-1VPVBDkilledNPHarvey Oswald(a)SBARQWHNP-1WPWhoSQAUXdidNPHarvey OswaldVPVBkillNP*T*-1(b)Figure 8: LDD resolved treebank style treesWith few exceptions5 the trees produced by cur-rent treebank-based probabilistic parsers do notrepresent long distance dependencies (Figure 9).Johnson (2002) presents a tree-based methodfor reconstructing LDD dependencies in Penn-II trained parser output trees.
Cahill et al(2004) present a method for resolving LDDs5Collins?
Model 3 computes a limited number of wh-dependencies in relative clause constructions.502SBARQWHNPWPWhoSQVPVBDkilledNPHarvey Oswald(a)SBARQWHNPWPWhoSQAUXdidNPHarvey OswaldVPVBkill(b)Figure 9: Parser output treesat the level of Lexical-Functional Grammar f-structure (attribute-value structure encodings ofbasic predicate-argument structure or dependencyrelations) without the need for empty productionsand coindexation in parse trees.
Their method isbased on learning finite approximations of func-tional uncertainty equations (regular expressionsover paths in f-structure) from an automatically f-structure annotated version of the Penn-II treebankand resolves LDDs at f-structure.
In our work weuse the f-structure-based method of Cahill et al(2004) to ?reverse engineer?
empty productions,traces and coindexation in parser output trees.
Weexplain the process by way of a worked example.We use the parser output tree in Figure 9(a)(without empty productions and coindexation) andautomatically annotate the tree with f-structureinformation and compute LDD-resolution at thelevel of f-structure using the resources of Cahillet al (2004).
This generates the f-structure an-notated tree6 and the LDD resolved f-structure inFigure 10.Note that the LDD is indicated in terms of areentrancy 1 between the question FOCUS and theSUBJ function in the resolved f-structure.
Giventhe correspondence between the f-structure and f-structure annotated nodes in the parse tree, wecompute that the SUBJ function newly introducedand reentrant with the FOCUS function is an argu-ment of the PRED ?kill?
and the verb form ?killed?in the tree.
In order to reconstruct the correspond-ing empty subject NP node in the parser outputtree, we need to determine candidate anchor sites6Lexical annotations are suppressed to aid readability.SBARQWHNP?
FOCUS =?WP?=?WhoSQ?=?VP?=?VBD?=?killedNP?
OBJ =?Harvey Oswald(a)??
?FOCUS[PRED who]1PRED ?kill?SUBJ OBJ?
?OBJ[PRED ?Harvey Oswald?
]SUBJ[PRED ?who?]1???
(b)Figure 10: Annotated tree and f-structurefor the empty node.
These anchor sites can only berealised along the path up to the maximal projec-tion of the governing verb indicated by ?=?
anno-tations in LFG.
This establishes three anchor sites:VP, SQ and the top level SBARQ.
From the auto-matically f-structure annotated Penn-II treebank,we extract f-structure annotated PCFG rules foreach of the three anchor sites whose RHSs containexactly the information (daughter categories plusLFG annotations) in the tree in Figure 10 (in thesame order) plus an additional node (of whateverCFG category) annotated ?SUBJ=?, located any-where within the RHSs.
This will retrieve rules ofthe formVP ?
NP [?
SUBJ =?]
V BD[?=?]
NP [?
OBJ =?
]V P ?
.
.
.. .
.SQ ?
NP [?
SUBJ =?]
V P [?=?
]SQ ?
.
.
.. .
.SBARQ ?
.
.
.. .
.each with their associated probabilities.
We selectthe rule with the highest probability and cut therule into the tree in Figure 10 at the appropriateanchor site (as determined by the rule LHS).
In ourcase this selects SQ ?
NP [?
SUBJ=?
]V P [?=?
]and the resulting tree is given in Figure 11.
Fromthis tree, it is now easy to compute the tree withthe coindexed trace in Figure 8 (a).In order to evaluate our empty node and coin-dexation recovery method, we conducted two ex-periments, one using 146 gold-standard ATISquestion trees and one using parser output on thecorresponding strings for the 146 ATIS questiontrees.503SBARQWHNP-1?
FOCUS =?WP?=?WhoSQ?=?NP?
SUBJ =?-NONE-*T*-1VP?=?VBD?=?killedNP?
OBJ =?Harvey OswaldFigure 11: Resolved treeIn the first experiment, we delete empty nodesand coindexation from the ATIS gold standardtrees and and reconstruct them using our methodand the preprocessed ATIS trees.
In the secondexperiment, we parse the strings corresponding tothe ATIS trees with Bikel?s parser and reconstructthe empty productions and coindexation.
In bothcases we evaluate against the original (unreduced)ATIS trees and score if and only if all of inser-tion site, inserted CFG category and coindexationmatch.Parser Output Gold Standard TreesPrecision 96.77 96.82Recall 38.75 39.38Table 4: Scores for LDD recovery (empty nodesand antecedents)Table 4 shows that currently the recall of ourmethod is quite low at 39.38% while the accu-racy is very high with precision at 96.82% on theATIS trees.
Encouragingly, evaluating parser out-put for the same sentences shows little change inthe scores with recall at 38.75% and precision at96.77%.7 ConclusionsThe data represented in Figure 5 show that train-ing a parser on 50% of QuestionBank achieves anf-score of 88.56% as against 89.24% for trainingon all of QuestionBank.
This implies that whilewe have not reached an absolute upper bound, thequestion corpus is sufficiently large that the gainin accuracy from adding more data is so small thatit does not justify the effort.We will evaluate grammars learned fromQuestionBank as part of a working QA sys-tem.
A beta-release of the non-LDD-resolvedQuestionBank is available for download athttp://www.computing.dcu.ie/?jjudge/qtreebank/4000qs.txt.
The fi-nal, hand-corrected, LDD-resolved version will beavailable in October 2006.AcknowledgmentsWe are grateful to the anonymous reviewers fortheir comments and suggestions.
This researchwas supported by Science Foundation Ireland(SFI) grant 04/BR/CS0370 and an Irish ResearchCouncil for Science Engineering and Technology(IRCSET) PhD scholarship 2002-05.ReferencesDaniel M. Bikel.
2002.
Design of a multi-lingual, parallel-processing statistical parsing engine.
In Proceedings ofHLT 2002, pages 24?27, San Diego, CA.Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef vanGenabith, and Andy Way.
2004.
Long-Distance De-pendency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations.
In Pro-ceedings of ACL-04, pages 320?327, Barcelona, Spain.Stephen Clark, Mark Steedman, and James R. Curran.2004.
Object-extraction and question-parsing using ccg.In Dekang Lin and Dekai Wu, editors, Proceedings ofEMNLP-04, pages 111?118, Barcelona, Spain.Michael Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. thesis, University ofPennsylvania, Philadelphia, PA.Daniel Gildea.
2001.
Corpus variation and parser perfor-mance.
In Lillian Lee and Donna Harman, editors, Pro-ceedings of EMNLP, pages 167?202, Pittsburgh, PA.Charles T. Hemphill, John J. Godfrey, and George R. Dod-dington.
1990.
The ATIS Spoken Language Systems pi-lot corpus.
In Proceedings of DARPA Speech and NaturalLanguage Workshop, pages 96?101, Hidden Valley, PA.Mark Johnson.
2002.
A simple pattern-matching algorithmfor recovering empty nodes and their antecedents.
In Pro-ceedings ACL-02, University of Pennsylvania, Philadel-phia, PA.John Judge, Aoife Cahill, Michael Burke, Ruth O?Donovan,Josef van Genabith, and Andy Way.
2005.
Strong DomainVariation and Treebank-Induced LFG Resources.
In Pro-ceedings LFG-05, pages 186?204, Bergen, Norway, July.Xin Li and Dan Roth.
2002.
Learning question classifiers.
InProceedings of COLING-02, pages 556?562, Taipei, Tai-wan.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large Annotated Cor-pus of English: The Penn Treebank.
Computational Lin-guistics, 19(2):313?330.504
