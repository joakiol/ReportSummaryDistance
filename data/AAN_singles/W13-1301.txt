Proceedings of the NAACL HLT Workshop on Vision and Language (WVL ?13), pages 1?9,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsAnnotation of Online Shopping Images without Labeled Training ExamplesRebecca Mason and Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown University, Providence, RI 02912{rebecca,ec}@cs.brown.eduAbstractWe are interested in the task of image an-notation using noisy natural text as trainingdata.
An image and its caption convey dif-ferent information, but are generated by thesame underlying concepts.
In this paper, welearn latent mixtures of topics that generateimage and product descriptions on shoppingwebsites by adapting a topic model for multi-lingual data (Mimno et al 2009).
We use thetrained model to annotate test images withoutcorresponding text.
We capture visual prop-erties such as color, texture, shape, and ori-entation by computing low-level image fea-tures, and measure the contribution of eachtype of visual feature towards the accuracy ofthe model.
Our model significantly outper-forms both a competitive baseline and a pre-vious topic model-based system.1 IntroductionImage annotation is a classic problem in ComputerVision.
Given a query image, the task is to gen-erate a set of textual labels that describe the visualcontent.
The typical approach to these problems isto use supervised models, which require large num-bers of hand-annotated examples for each of the la-bels.
However, the amount of information availableon the web continues to grow, the task of organiz-ing and describing visual data becomes increasinglycomplex.
For example, a shopping website might ar-range products into broad categories such as ?shoes?and ?handbags?
with each category containing tensof thousands of products that are difficult for usersto search and navigate.
It is often infeasible to dis-cover all of the attributes within those categories thatare relevant to users and create labeled training ex-amples for each of them.Instead, we approach this problem by discoveringvisual attributes from noisy natural language cap-tions.
That is, given a collection of images and cap-tions found on the web, we learn a model of visualand textual features.
Then given a query image withno text, we can generate likely descriptive words.This is a difficult task because image captions on theweb are often noisy and incomplete: some captionsmight not describe a particular visual feature, mightuse a synonym for that feature, or might describe in-formation that is not visual in the image at all.A secondary motivation for this work is to use theimage annotations as a component in language gen-eration systems such as for automatic image caption-ing.
We point to examples of previous work suchas Feng and Lapata (2010a) where image annota-tions generated from a topic model are used to helpgenerate full sentences to describe images.
Much ofthe current research in image captioning is limitedby the current technology for object recognition inComputer Vision.
For example, SBU-Flickr dataset(Ordonez et al 2011) with 1 million images andcaptions, is considered to be general-domain but isactually built by querying Flickr using a pre-definedterm list related to visual attributes that there aretrained recognition systems for.
While these sys-tems can accurately generate descriptions for com-mon visual objects and attributes, they are not aswell-suited for describing the ?long-tail?
of visualattributes which appear in many domain-specific1Two adjustable buckle straps top aclassic rubber rain boot grounded bya thick lug sole for excellent wet-weather traction.Size(s) Available: 6, 11.5.
Brand &Style - VANS Kvd Width - Medium(B, M) Heel Height - Shoe Size isWomens Size 11.5 = Mens Size 101 Inch Heel Material - Canvas Upperand Man Made SoleCarlo Fellini - Evening clutchbeaded on a wave patternTable 1: Examples of data from the Attribute Discovery Dataset (Berg et al 2010).
The images are fairly clean anduniform, while captions have more noise and variation.datasets.In this paper, we model image and text featuresfrom the training data using a generative model.
Weadapt the Polylingual topic model from Mimno etal.
(2009) to train on multi-modal data, and then usethe trained model to generate annotations for test im-ages.
We evaluate our model on two categories ofshopping images using a variety of types of com-puted image features.
For image annotation we out-perform both a difficult baseline and previous work.2 Related WorkWe use the polylingual topic model from Mimno etal.
(2009), which was developed to model multi-lingual corpora that are topically comparable be-tween languages ?
the documents are not directtranslations, but they cover the same ideas.
For ex-ample, English and Finnish Wikipedia pages aboutskiing are roughly similar, but the subject is coveredmore thoroughly in Finnish.
Therefore, the numberof tokens assigned to the Finnish topic for skiing ismuch higher than it is in the English.
While Mimnoet al(2009) show that the model is effective in taskssuch as modeling topically comparable documentsacross languages, our work is the first to show thatthis model can be used to model data of differentmodalities.
Another quality of the polylingual topicmodel is that words in different languages do not di-rectly correspond with each other.
This is a featureof other multi-lingual topic models but would notwork for multi-modal data because a textual wordcan carry more meaning by itself than an image fea-ture can.Countless approaches have been proposed for theuse of topic models in image annotation, but thevast majority of these approaches consider the textmodality merely as labels for the image modality.The most highly cited of these is the Correspon-dence LDA (corr-LDA) model of Blei et al(2003),where topics are learned using the image modalityalone, and each textual word must be generated by aspecific region in the image.
However, more recentwork has started to recognize the textual modalityas a source of information in its own right.
Jia etal.
(2011) present a model that allows different in-formation to be emphasized in each modality, but itrequires very clean text; they do not use documentswith captions that cannot be easily parsed or pro-cessed.
Then, they stem all words, and disregardsparse word tokens.
This works when working withsources such as Wikipedia, where text captions arehighly edited and consistently formatted.
In compar-ision, our work can be trained on corpra where thetext has poor or inconsistant quality.
Additionally,their work was for the task of image retrieval from atext query, while we are generating text annotationsfor a query image.Our work is most similar to the MixLDA modelof Feng and Lapata (2010b), except MixLDA mod-2els images and their related text as a single bag-of-features, with visual and textual features comingfrom the same vocabulary.
This means that sometopics should have a greater proportion of featuresfrom one of modalities, if there is an idea that is bet-ter expressed in one over the other.
Their model wasdeveloped for finding descriptive words given bothan image and a news article, and can also be usedon large and noisy amounts of data, so we compareMixLDA against our model in the experiments.Although we use the Attribute Discovery Datasetof Berg et al(2010), their work is different fromours in both problem formulation and the types ofattributes discovered.
Their primary interest is tocharacterize attributes according to how they are vi-sually represented: global or local; color, texture, orshape.
Their work does not address the task of pre-dicting attributes for unseen images.
Additionally,they do not work with individual descriptive words,but cluster them using mutual information of vi-sual attributes, creating a smaller number of ?visualsynsets?.
For example, one of their visual synsetsfor images and descriptions of womens handbags is{mesh, interior, metal} and another is {silver, metal-lic}.
In comparison, in the topic model the sameword can be generated by more than one topic.Liu et al(2010) examine the use of a variety ofimage features in a Bayesian model in order to mea-sure which are the best for classifying diverse ma-terials such as stone, glass, and plastic.
They foundthat the image features they used for shape and colorwere better indicators of the material of an objectthan texture features, and their best combined modeldid not include texture as a feature at all.
We are alsointerested in finding out whether our performance ongenerating descriptive words is affected by differenttypes of image features.3 DatasetWe use the Attribute Discovery Dataset from Berget al(2010).1 The dataset consists of pairs of im-ages and captions taken from the shopping websitelike.com.
The data has four categories: women?sshoes, handbags, earrings, and neckties.
We run ourmodel on two categories, shoes and handbags, due1http://tamaraberg.com/attributesDataset/index.htmlto their larger sizes ?
14764 and 9145 image-captionpairs respectively ?
and diversity of features.
This isa reasonable amount of data in the shopping imagesdomain; more than half of the number of compa-rable products sold on large retail websites such asZappos.com or Amazon.com.Compared to general datasets such as PascalSentences, the images in the Attribute DiscoveryDataset are more uniform.
All image files are280x280 pixel JPEGs, and images of products aretypically taken from similar angles against a whiteor a light-colored solid background.
Only rarely dothe images have noisy backgrounds, such as a per-son wearing the item, or the same item displayed inmultiple colors in one image.
However, this does notnecessarily make our task much easier, since the vi-sual attributes we wish to learn are not pre-defined asthey are in a general-domain dataset.
And the lack ofhand-annotated data means no negative examples ofwhen an attribute is not present, which are typicallyused to train visual classifiers.Furthermore, the captions are extremely noisy inthis dataset.
Compared to the 20 object types in thePascal Sentences dataset, or about one hundred inCOREL, here there are thousands of words that canbe used to describe features in the images, includ-ing synonyms, multiple stems of words, and mis-spellings.
In addition to explicit visual descriptionsof the products, the captions describe ?less visual?features such as details about the construction of theitem, during which season or activity it would be ap-propriate to wear, or feelings that could be evokedby looking at the item.
These features are difficultto represent as specific visual attributes, but can beidentified visually by domain-experts.
Captions canalso include information that is non-visual such assizing and shipping information, or whether the itemis on sale.The captions can be either full English sentences,a list of features, or sometimes just a few words.Longer captions in the dataset are truncated to 250characters in length.From our own obervations, we estimate about10% of the captions in the shoes dataset containfew or no descriptive words.
At least 3.7% of theshoes captions are entirely Javascript code, have sig-nificant portions of code, or very long URLs.
An-other 5-6% either contain no information besides3sizing or shipping information, only the brand nameor model number of the shoe, or the caption is soshort that there are only one or two descriptive wordsthat could be used in our model.
In the womens?shoes category, we take some simple steps to removeURLs and code to avoid learning accidental correla-tion with legitimate features.2 However, we still useall image and caption pairs in the training set, in-cluding those which end up having empty captions,since they are still useful for learning topics for vi-sual features.
For the handbags captions, we did nottry to remove code or long URLs since it seemed tobe less of a problem in that category.4 Feature Representation4.1 Text FeaturesThe bag-of-words model is used for text.
We useMxterminator (Reynar and Ratnaparkhi, 1997) tosplit sentences in the captions (in many instances,nothing is done in this step becuase there are nofull sentences in the caption), Stanford POS Tag-ger(Toutanova et al 2003) to tag words, then in-clude adjectives, adverbs, verbs, and nouns in thetopic model (except for proper nouns and commonbackground English words from a stoplist).
How-ever, these tags are really more a rough estimate ofparts of speech due to the number of incomplete sen-tences and phrases, and the fact that many of thewords used to describe styles or attributes of cloth-ing have different meanings in colloqueal English.3All tokens are converted to lower case, but there isno stemming or lemmatization.
After preprocessing,the size of the shoes text vocabulary is 9578 words,with an average of 16.33 descriptive words per im-age, while the bags have a text vocabulary of 6309word types with 15.41 descriptive words per imageon average.4.2 Visual FeaturesThe bag-of-features model is used for visual featuresas well.
Most of these features are standard in com-puter vision research, and are also used in work wecited in Section 2.2Tokens removed: URLs, all tokens that end in ?.sh?, and afew tokens obviously related to Javascript eg script, src, typeof,var.3Some examples of domain-specific words used in shoppingimage descriptions: www.zappos.com/glossaryShape: A SIFT descriptor describes which wayedges are oriented at a certain point in an image(Lowe, 1999).
It was develped to recognize the sameobject under different scales and rotations.
How-ever, it is also commonly used for recognizing moregeneralized types or features of objects.
We use theVLFeat open source library (Vedaldi and Fulkerson,2008) to compute SIFT features at points of interestand to cluster the SIFT features into discrete ?visualterms?
using the k-means algorithm.
There are 750visual terms for SIFT features.Color: We use two representations for color,RGB (red, green, blue) and HSV (hue, satura-tion, value).
25 pixels are sampled from the cen-ter 100x100 pixels of the image (to avoid samplingfrom the background of the image).
Those pixel val-ues are also clustered to visual terms using k-means,with 100 visual terms each.Texture: Images are convolved with Gabor filtersat multiple orientations and scales, sampled at ran-dom locations, then clustered to form texton featuresfor texture (Leung and Malik, 2001).
We convert allimages to grayscale, then sample 25 locations fromthe center of the image, and cluster to 100 visualterms.
We also have a color texton feature, wherewe sample and cluster textons separately for the red,green, and blue color channels.Reflectance/Curvature:4 We use three types ofrelated features for gradients and curvature.
Thefirst is a bag-of-HOG (histogram of gradients) fea-ture set (Dalal and Triggs, 2005) computed over aregular grid on the image to measure changes in in-tensity.5 The most significant of those features (asdetermined by L2 norm) are selected for each im-age, and like previous features are clustered into vi-sual terms using k-means.
The second two types arederivatives of HOG which include information aboutthe amount of curvature at each orientation of theHOG descriptor.64Figure 1: Polylingual topic model (Mimno et al 2009)5 ModelWe model textual and visual features using thepolylingual topic model by Mimno et al(2009).
Inthis section, we describe how the generative processand inference of this model is adapted to topicallycomparable multi-modal data.Figure 1 shows the original polylingual topicmodel.
We model multi-modal data using two ?lan-guages?
: txt for the bag-of-words captions, and imgfor the combined visual terms.
The generative pro-cess is defined for an image and caption pair, w =<wimg, wtxt >:?
?
Dir(?, ?m)zimg ?
P (zimg|?)
=?n?zimgnztxt ?
P (ztxt|?)
=?n?ztxtnwimg ?
P (wimg|zimg,?img) = ?imgwimgn |zimgnwtxt ?
P (wtxt|ztxt,?txt) = ?txtwtxtn |ztxtnFirst, a topic distribution for w is drawn from anasymmetric Dirichlet prior with concentration pa-rameter ?
and base measure m. Then a latent topicassignment is drawn for each word token in wtxt,and each discrete image feature in wimg.
Once thetopic assignments are sampled, the observed tokensare sampled according to their probability in themodality-specific topics ?img = {?img1 , ..., ?imgT }and ?txt = {?txt1 , ..., ?txtT }.4Note: These features are implemented using code from(Felzenszwalb et al ).5There is significant overlap between these features, al-though the benefits of overlap are lost due to the bag-of-featuresmodel.6Personal correspondance, work in progress.To find the most probable descriptive words for anunseen image, the first step is to estimate the topicdistribution that generated the image.
Gibbs sam-pling is used to sample topic assignments for visualterms in the test image dimg:P (zn = t|dimg, z\n,?img, ?m)?
?imgdimgn |t(Nt)\n + ?mt?tNt ?
1 + ?Assuming that the descriptive words are indepen-dent, the probability of text word wi given dimg is:P (wi|dimg) =?tP (wi|ztxtt )P (zt|dimg)summing over all topics t ?
T .For training the model, we used the Polylingualtopic model implementation from the Mallet toolkit(McCallum, 2002) (with some small modifcationsto use it for generation).
We use 1000 iterations forinference, with hyperparameter optimization every10 iterations.
In both shoes and bags categories, thenumber of topics is 200, which was minimally tunedby hand on the shoes data.6 Experimental Setup and EvaluationWe first run our model on the larger category, shoes.For both systems and baselines, we find the 10, 15,and 20 most likely words for the test images.
Weevaluate by computing precision and recall againstdescriptive words from the held-out captions forthose images.7 We compute macro-averages of thesescores because there is a lot of variation between thesizes of the captions in the dataset.
The split betweentraining and test instances is 80/20%.We also evaluate the contributions of differenttypes of image features.
We evaluate the model foreach image feature individually (along with the textfeatures), as well as combinations of image features.We compare against the MixLDA system and astrong baseline.
We choose MixLDA because itis relatively easy to re-implement and because it7We find descriptive words for test instances in the exactsame way we did for training instances in Section 4.1.
Instanceswhere we did not find any useable descriptive words did notcount towards the evaluation.510 words 15 words 20 wordsP R F1 P R F1 P R F1BaselinesMixLDA 21.02 13.80 16.66 17.41 17.15 17.13 14.88 19.53 16.89Corpus frequency 21.03 13.73 16.61 17.51 17.14 17.32 15.41 20.12 17.45Single AttributeSIFT 27.00 16.30 20.34 22.84 20.65 21.69 20.09 24.22 21.96Grayscale Texture 21.26 13.88 16.80 18.25 17.87 18.06 15.71 20.52 17.80RGB Texture 24.77 14.93 18.63 21.01 18.99 19.95 18.49 22.29 20.21HSV Color 22.17 13.35 16.67 18.59 16.79 17.65 16.48 19.85 18.01RGB Color 23.21 13.98 17.45 19.78 17.88 18.78 17.53 21.12 19.15HOG 26.33 15.87 19.80 22.36 20.21 21.23 19.60 23.62 21.42TriHOG 24.60 14.82 18.50 20.64 18.66 19.60 18.14 21.87 19.83TriHOG-Polar 26.03 15.69 19.58 22.06 19.94 20.95 19.32 23.29 21.12Combined ModelsAll-Color 24.22 14.60 18.22 20.62 18.65 19.59 18.11 21.83 19.80All-Texture 25.50 15.41 19.24 21.63 19.55 20.53 18.88 22.75 20.64All-HOG 27.36 16.50 20.58 23.31 21.07 22.14 20.40 24.58 22.30Combine All 29.31 17.70 22.04 24.88 22.49 23.63 21.71 26.16 23.73SIFT+RGB Texture+HOG 28.62 17.25 21.52 24.35 22.01 23.12 21.20 25.55 23.17Table 2: Results of evaluation in the women?s shoes cateogory (top 10-20 words).has previously outperformed other image annota-tion systems when trained on natural language cap-tions.
Because the MixLDA model originally onlyused SIFT features, we compare it against the SIFT-only version of our model, with each system usingthe same computed image and text features.
Were-implement the MixLDA system mostly as it isdescribed in Feng and Lapata (2010b), with a fewchanges to make it more comparable to our model:Obviously in our version of MixLDA the test in-stances are only the unseen image as there is no othersurrounding text.
The number of topics is 200 (theoriginal MixLDA had more but that did not seem tohelp here), and the ?
and ?
hyperparameters are op-timized every 10 iterations.8We also compare our model against corpus fre-quency of words in the training set.
Although thismay seem like a trivial baseline, previous work8We used the Mallet toolkit?s Parallel LDA sampler for in-ference, while a variational approach is used in the original.However, we do not believe this would change the outcome ofthis experiment.
We also tried MixLDA without hyperparam-eter optimization but we do not show those results as they aresignificantly worse.on image annotation from both computer vision(Mu?ller et al 2002; Monay and Gatica-Perez, 2003;Barnard et al 2003) and natural language process-ing (Mason and Charniak, 2012) has shown that alarge portion of the keyword probability mass canoften be accounted for by a very small number ofwords, allowing systems to game better-looking re-sults by simply guessing the frequency distributionof the text vocabulary.
We find this to be espe-cially true in the domain-specific case, where com-mon terms (eg shoe, sole, heel, upper) are used inalmost every caption, and in some captions accountfor most words used (such as the second examplein Table 1).
While domain-frequent words are alsoneeded for generating new captions, we don?t wantthem to account for all of the words our system gen-erates.
Of course, a human evaluation would beanother possible way of addressing this issue, butit would be difficult and expensive to find enoughpeople who have sufficient knowledge of womens?clothing and would be able to accurately say whetherthe generated words are appropriate or not (wordssuch as hobo, PU, stacked, upper, and vamp).
Also,although the gold image captions are noisy, the num-6sole upper detail heel print fun fab-ric patent uppers soles high shoerounded leather rubber lining elasticanimal toe feetstyle upper heel leather strap sandallining toe dress satin shoe comfortankle sole adjustable outsole plat-form stiletto rhinestone sandalsbag, leather, zip, pocket, hardware,features, shoulder, flap, main, cell,perfect, length, drop, zipper, clo-sure, bold, phone, evening, holds,hoboThis high heel platform shoe has apatent leather upper with an orna-menting bow at the toe, a leather lin-ing, a rounded toe, and a rubber bot-tom.Available Colors: Black Patent,Cheetah Print PU.Create a timeless look with theseAndie dress sandals from Col-oriffics.
Dyeable white satin mattesatin or metallic satin upper in atwo-piece dress-sandal style withan open round toe crossing pleatedvamp straps with a dazzling rhine-stone clasp and a wraparound heelstrap with an adjustable buckle clo-sure.Treesje Dakota Shoulder Bag BlackShine - Designer HandbagsTable 3: Example results for unseen images.
Both the top words generated by our model and the original held-outcaptions for the images are shown.
(Note: In the third example, ?hobo?
is actually the term that is used to describethat shape of handbag.
)ber of test documents is very large so we can findsignificance on precision and recall using bootstrapresampling.We also ran the baseline system and our systemon the handbags category of the dataset.
We did notmodify the system in any way when using the bagsdataset, just gave it different file for input.7 Results and DiscussionThe results of our evaluations are in Table 2.
Aswe expected, the corpus frequency baseline doesvery well.
It is comparable to MixLDA for 10and 15 words, and significantly better than MixLDAfor over 20 words.
However, the Polylingual topicmodel using only SIFT features and text is much bet-ter than both.
The trained MixLDA model has topicswith both image and text features, so when estimat-ing topics given only an image, it estimates that itwas generated by topics that have a high proportionof image features.
Though it also estimates sometopics that have a mix of visual and text features,being able to generate good text descriptions fromthose topics, the topics that have less text featureswill be mainly determined by the smoothing param-eter ?
the uniform distribution, worse than guessingthe corpus distribution.Out of the single attribute models, all except threeof the single feaure models were significantly higherthan corpus frequency on both precision and recallat 10, 15, and 20 words.
The exceptions are the twocolor features and grayscale texture.
For grayscaletexture, we had expected it would correlate well withthe material of the shoe; but either the low resolu-tion of the images makes it difficult to distinguishmaterials by their texture, or materials don?t corre-late with the ?less visual?
features as much as weexpected.
Interestingly, since the material of an itemtends to correlate strongly with other attributes suchas shape and color, so our model still generates cor-rect descriptive words for material in many cases.While neither color nor texture were useful fea-710 words 15 words 20 wordsP R F1 P R F1 P R F1Corpus frequency 17.58 13.19 11.76 13.19 12.70 12.94 11.76 15.10 13.22Combined Model 24.41 15.67 19.09 21.01 20.22 20.61 18.76 24.09 21.10Table 4: Results of evaluation in the handbags cateogory (top 10-20 words).tures on their own, RGB Texture did very well as asingle attribute, and was within signficance of boththe combined color and combined texture models.This may be related to the fact that RGB Textonshave a larger number of visual terms than those otherfeatures, 100 for each of the three color channels.Unlike material, we observed that the color of anobject is often not mentioned in the human-writtencaption (as seen in the examples in Table 1), or sev-eral colors are described in the caption where onlyone is seen in the image (seen in some of the exam-ples in Table 3).
We also observed that our systemgenerates very few color words.The gradient and shape-based features have thebest single-attribute performance by far.
Both SIFTand HOG capture shape at local points, but whileSIFT features are invariant to differences in positionor scale, HOG features are more sensitive to the waythe item is oriented in the image.
Although the cur-vature features TriHOG and TriHog-Polar are nearlyas good as HOG on their own, combining the threeHOG features does not significantly improve perfor-mance of the model over HOG alone.Not all of the single-attribute models performedas well as others, but there was no case where re-moving one of the features improved the perfor-mance of the combined model.
The fewest num-ber of image attributes that our model could useand still get within significance to the full combinedmodel is three ?
SIFT, RGB Texture, and HOG.However, we found that each image attribute doesslightly improve, the model, even if not by a signifi-cant amount.Our results on the handbags category of thedataset are shown in Table 4.
Although our scoresare not as high as they were in the shoes category,the scores of the corpus frequency baseline are notas high either, and our model does about as well overthe baseline in each category.
But is worth reiterat-ing that we were able to run our system on both thebags and shoes shopping categories with absolutelyno modifications or tuning of parameters.8 Conclusion and Future WorkIn conclusion, we have shown that the polylingualtopic model works well for modeling topically com-parable images and related text, and obtain competi-tive results for the image annotation task.
Our modelis trained on noisy image captions from the web,rather than hand-labeled data.For future work, we would like to further adaptthe polylingual topic model for multi-modal data byallowing some topics to be generated only by onemodality or the other.
We are also interested in char-acterizing the image annotations in order to generatea single most likely annotation for different types offeatures such as texture or color.
Finally, we are in-terested in extending this model to use with other do-mains of data.
For natural images, we could use im-age segmentation algorithms to separate the objectof interest from the background of the image, or wecould use scene classifcation to cluster the trainingimages by their background scene and train sepratemodels for each.ReferencesK.
Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D.M.Blei, and M.I.
Jordan.
2003.
Matching words andpictures.
The Journal of Machine Learning Research,3:1107?1135.Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.2010.
Automatic attribute discovery and character-ization from noisy web data.
In Proceedings ofthe 11th European conference on Computer vision:Part I, ECCV?10, pages 663?676, Berlin, Heidelberg.Springer-Verlag.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
J. Mach.
Learn.Res., 3:993?1022, March.N.
Dalal and B. Triggs.
2005.
Histograms of orientedgradients for human detection.
In Computer Vision8and Pattern Recognition, 2005.
CVPR 2005.
IEEEComputer Society Conference on, volume 1, pages 886?893 vol.
1, june.P.
F. Felzenszwalb, R. B. Girshick, and D. McAllester.Discriminatively trained deformable part models, re-lease 4. http://people.cs.uchicago.edu/ pff/latent-release4/.Yansong Feng and Mirella Lapata.
2010a.
How manywords is a picture worth?
automatic caption gener-ation for news images.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 1239?1249, Stroudsburg,PA, USA.
Association for Computational Linguistics.Yansong Feng and Mirella Lapata.
2010b.
Topic mod-els for image annotation and text illustration.
In HLT-NAACL, pages 831?839.Yangqing Jia, M. Salzmann, and T. Darrell.
2011.
Learn-ing cross-modality similarity for multinomial data.
InComputer Vision (ICCV), 2011 IEEE InternationalConference on, pages 2407 ?2414, nov.T.
Leung and J. Malik.
2001.
Representing and recog-nizing the visual appearance of materials using three-dimensional textons.
International Journal of Com-puter Vision, 43(1):29?44.C.
Liu, L. Sharan, E.H. Adelson, and R. Rosenholtz.2010.
Exploring features in a bayesian framework formaterial recognition.
In Computer Vision and PatternRecognition (CVPR), 2010 IEEE Conference on, pages239?246.
IEEE.D.G.
Lowe.
1999.
Object recognition from local scale-invariant features.
In Computer Vision, 1999.
The Pro-ceedings of the Seventh IEEE International Confer-ence on, volume 2, pages 1150 ?1157 vol.2.R.
Mason and E. Charniak.
2012.
Apples to oranges:Evaluating image annotations from natural languageprocessing systems.
NAACL.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.David Mimno, Hanna M. Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 2 - Volume 2, EMNLP ?09,pages 880?889, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Florent Monay and Daniel Gatica-Perez.
2003.
On im-age auto-annotation with latent space models.
In Pro-ceedings of the eleventh ACM international conferenceon Multimedia, Multimedia ?03, pages 275?278, NewYork, NY, USA.
ACM.Henning Mu?ller, Ste?phane Marchand-Maillet, andThierry Pun.
2002.
The truth about corel - evaluationin image retrieval.
In Proceedings of the InternationalConference on Image and Video Retrieval, CIVR ?02,pages 38?49, London, UK, UK.
Springer-Verlag.V.
Ordonez, G. Kulkarni, and T.L.
Berg.
2011.
Im2text:Describing images using 1 million captioned pho-tographs.
NIPS.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
Amaximum entropy approach to identifying sentenceboundaries.
In In Proceedings of the Fifth Conferenceon Applied Natural Language Processing, pages 16?19.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In In Pro-ceedings of HLT-NAACL 2003, pages 252?259.A.
Vedaldi and B. Fulkerson.
2008.
VLFeat: An openand portable library of computer vision algorithms.http://www.vlfeat.org/.9
