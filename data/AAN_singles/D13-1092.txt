Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 903?907,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsImplicit Feature Detection via a Constrained Topic Model and SVMWei Wang ?, Hua Xu ?
and Xiaoqiu Huang ?
?State Key Laboratory of Intelligent Technology and Systems,Tsinghua National Laboratory for Information Science and Technology,Department of Computer Science and Technology, Tsinghua University,Beijing 100084, China?Beijing University of Posts and Telecommunications, Beijing 100876, Chinaww880412@gmail.com, xuhua@tsinghua.edu.cn, alexalexhxqhxq@gmail.comAbstractImplicit feature detection, also known as im-plicit feature identification, is an essential as-pect of feature-specific opinion mining butprevious works have often ignored it.
Wethink, based on the explicit sentences, sever-al Support Vector Machine (SVM) classifier-s can be established to do this task.
Never-theless, we believe it is possible to do bet-ter by using a constrained topic model insteadof traditional attribute selection methods.
Ex-periments show that this method outperformsthe traditional attribute selection methods bya large margin and the detection task can becompleted better.1 IntroductionFeature-specific opinion mining has been well de-fined by Ding and Liu(2008).
Example 1 is a cellphone review in which two features are mentioned.Example 1 This cell phone is fashion in appear-ance, and it is also very cheap.If a feature appears in a review directly, it is calledan explicit feature.
If a feature is only implied, it iscalled an implicit feature.
In Example 1, appearanceis an explicit feature while price is an implicit fea-ture, which is implied by cheap.
Furthermore, an ex-plicit sentence is defined as a sentence containing atleast one explicit feature, and an implicit sentence isthe sentence only containing implicit features.
Thus,the first sentence is an explicit sentence, while thesecond is an implicit one.This paper proposes an approach for implicit fea-ture detection based on SVM and Topic Model(TM).The Topic Model, which incorporated into con-straints based on the pre-defined product feature,is established to extract the training attributes forSVM.
In the end, several SVM classifiers are con-structed to train the selected attributes and utilizedto detect the implicit features.2 Related WorkThe definition of implicit feature comes from Liuet al(2005)?s work.
Su et al(2006) used Point-wise Mutual Information (PMI) based semantic as-sociation analysis to identify implicit features, butno quantitative experimental results were provided.Hai et al(2011) used co-occurrence association rulemining to identify implicit features.
However, theyonly dealt with opinion words and neglected thefacts.
Therefore, in this paper, both the opinions andfacts will be taken into account.Blei et al(2003) proposed the original LDA us-ing EM estimation.
Griffiths and Steyvers (2004)applied Gibbs sampling to estimate LDA?s parame-ters.
Since the inception of these works, many vari-ations have been proposed.
For example, LDA haspreviously been used to construct attributes for clas-sification; it often acts to reduce data dimension(Bleiand Jordan, 2003; Fei-Fei and Perona, 2005; Quel-has et al 2005).
Here, we modify LDA and adopt itto select the training attributes for SVM.3 Model Design3.1 Introduction to LDAWe briefly introduce LDA, following the notationof Griffiths(Griffiths and Steyvers, 2004).
Given D903documents expressed over W unique words and Ttopics, LDA outputs the document-topic distribution?
and topic-word distribution ?, both of which canbe obtained with Gibbs Sampling.
For this scheme,the core process is the topic updating for each wordin each document according to Equation 1.P (zi = j|z?i,w, ?, ?)
=(n(wi)?i,j + ??Ww?
n(w?
)?i,j +W?
)(n(di)?i,j + ?
?Tj n(di)?i,j + T?
)(1)where zi = j represents the assignment of the ithword in a document to topic j, z?i represents allthe topic assignments excluding the ith word.
n(w?
)jis the number of instances of word w?
assigned totopic j and n(di)j is the number of words from doc-ument di assigned to topic j, the ?i notation sig-nifies that the counts are taken omitting the valueof zi.
Furthermore, ?
and ?
are hyper-parametersfor the document-topic and topic-word Dirichlet dis-tributions, respectively.
After N iterations of Gibbssampling for all words in all documents, the distri-bution ?
and ?
are finally estimated using Equations2 and 3.?
(wi)j =n(wi)j + ??Ww?
n(w?
)j +W?(2)?
(di)j =n(di)j + ?
?Tj n(di)j + T?
(3)3.2 FrameworkAlgorithm 1 summarizes the main steps.
When aspecific product and the reviews are provided, theexplicit sentences and corresponding features areextracted(Line 1) by word segmentation, part-of-speech(POS) tagging and synonyms feature cluster-ing.
Then the prior knowledge are drawn from theexplicit sentences automatically and integrated in-to the constrained topic model((Line 3 - Line 5).The word clusters are chosen as the training at-tributes(Line 6).
Finally, several SVM classifier-s are generated and applied to detect implicit fea-tures(Line 7 - Line 12).Algorithm 1 Implicit Feature Detection1: ES ?
extract explicit sentence set2: NES ?
non-explicit sentence set3: CS ?
constraint set from ES4: CPK ?
correlation prior knowledge from ES5: ETM?ConstrainedTopicModel(T ,ES,CS,CPK)6: TA?
select training attributes from ETM7: for each fi in feature clusters do8: TDi ?
GenerateTrainingData(TAi,ES)9: Ci?
BuildClassificationModelBySVM(TDi)10: PRi?
positive result of Classify(Ci,NES)11: the feature of sentence in PRi ?
fi12: end for3.3 Prior Knowledge Extraction andIncorporationIt is obvious that the pre-existing knowledge can as-sist to produce better and more significant clusters.In our work, we use a constrained topic model to s-elect attributes for each product features.
Each topicis first pre-defined a product feature.
Then two type-s of prior knowledge, which are derived from thepre-defined product features, are extracted automat-ically and incorporated: must-link/cannot-link andcorrelation prior knowledge.3.3.1 Must-link and Cannot-linkMust-link: It specifies that two data instancesmust be in the same cluster.
Here is the must-linkfrom an observation: as ?cheap?
to ?price?, somewords must be associated with a feature.
In orderto mine these words, we compute the co-occurrencedegree by frequency*PMI(f,w), whose formula is asfollowing: Pf&w ?
log2Pf&wPfPw , where P is the proba-bility of subscript occurrence in explicit sentences,f is the feature, w is the word, and f&w meansthe co-occurrence of f and w. A higher value offrequency*PMI signifies that w often indicates f .For a feature fi, the top five words and fi consti-tute must-links.
For example, the co-occurrence of?price?
and ?cheap?
is very high, then the must-linkbetween ?price?
and ?cheap?
can be identified.Cannot-link: It specifies that two data instancescannot be in the same cluster.
If a word and a fea-ture never co-occur in our corpus, we assume themto form a cannot-link.
For example, the word low-cost has never co-occurred with the product featurescreen, so they constitute a cannot-link in our cor-904pus.In this paper, the pre-defined process, must-link,and cannot-link are derived from Andrzejewski andZhu (2009)?s work, all must-links and cannot-linksare incorporated our constrained topic model.
Wemultiply an indicator function ?
(wi, zj), which rep-resents a hard constraint, to the Equation 1 as thefinal probability for topic updating (see Equation 4).P (zi = j|z?i,w, ?, ?)
=?
(wi, zj)(n(wi)?i,j + ??Ww?
n(w?
)?i,j +W?
)(n(di)?i,j + ?
?Tj n(di)?i,j + T?
)(4)As illustrated by Equations 1 and 4, ?
(wi, zj),which represents intervention or help from pre-existing knowledge of must-links and cannot-links,plays a key role in this study.
In the topic updatingfor each word in each document, we assume that thecurrent word is wi and its linked feature topic set isZ(wi), then for the current topic zj , ?
(wi, zj) is cal-culated as follows:1.
If wi is constrained by must-links and thelinked feature belongs to Z(wi), ?
(wi, zj |zj ?Z(wi)) = 1 and ?
(wi, zj |zj /?
Z(wi)) = 0.2.
If wi is constrained by cannot-links and thelinked feature belongs to Z(wi), ?
(wi, zj |zj ?Z(wi)) = 0 and ?
(wi, zj |zj /?
Z(wi)) = 1.3.
In other cases, ?
(wi, zj |j = 1, .
.
.
, T ) = 1.3.3.2 Correlation Prior KnowledgeIn view of the explicit product feature of each top-ic, the association of the word and the feature totopic-word distribution should be taken into accoun-t.
Therefore, Equation 2 is revised as the following:?
(wi)j =(1 + Cwi,j)(n(wi)j ) + ??Ww?
(1 + Cw?,j)(n(w?
)j ) +W?
(5)where Cw?,j reflects the correlation of w?
with thetopic j, which is centered on the product feature fzj .The basic idea is to determine the association of w?and fzj , if they have the high relevance,Cw?,j shouldbe set as a positive number.
Otherwise, if we candetermine w?
and fzj are irrelevant, Cw?,j should beset as a positive number.
In this paper, we attemptto using PMI or dependency relation to judge therelevance.
For word w?
and feature fzj :1.
Dependency relation judgement: If w?
as par-ent node in the syntax tree mainly co-occurswith fzj ,Cw?,j will be set positive.
Ifw?
mainlyco-occurs with several features including fzj ,Cw?,j will be set negative.
Otherwise, Cw?,jwill be set 0.2.
PMI judgement: If w?
mainly co-occurs withfzj and PMI(w?, fzj ) is greater than the giv-en value, Cw?,j will be set positive.
Otherwise,Cw?,j will be set negative.3.4 Attribute SelectionSome words, such as ?good?, can modify sever-al product features and should be removed.
In theresult of run once, if a word appears in the topicswhich relates to different features, it is defined as aconflicting word.
If a term is thought to describeseveral features or indicate no features, it is definedas a noise word .When each topic has been pre-allocated, we runthe explicit topic model 100 times.
If a word turnsinto a conflicting word Tcw times(Tcw is set to 20),we assume that it is a noise word.
Then the noiseword collection is obtained and applied to filter theexplicit sentences.
Actually, here 100 is just an esti-mated number.
And for Tcw, when it is between 15and 25, the result is same, and when it exceeds 25,the result does not change a lot.
The most importantpart to filter noise words is the correlation compu-tation.
So the experiment can work well with onlyestimated parameters.Next, By integrating pre-existing knowledge, theexplicit topic model, which runs Titer times, sever-s as attribute selection for SVM.
In every result foreach topic cluster, we remove the least four prob-able of word groups and merge the results by thepre-defined product feature.
For a feature, if a wordappears in its topic words more than Titer ?
tratiotimes, it is selected as one of the training attributesfor the feature.
In the end, if an attribute associateswith different features, it is deleted.9050 10 20 30 40 50 60 70 80 90 100Attribute Factor NumberChiSquare GainRatio InfoGain(a) SVM based on traditional attributeselection method0.0 0.1 0.2 0.3 0.4 0.5t ratioTM TM+must TM+cannot TM+must+cannot TM+syntactic TM+must+cannot+syntactic TM+PMI TM+must+cannot+PMI TM+correlation knowledge(PMI+syntactic) TM+must+cannot+correlation knowledge(b) our constrained topic model bydifferent tratio (Titer = 20)0 10 20 30 40 50T iterTM TM+must TM+cannot TM+must+cannot TM+syntactic TM+must+cannot+syntactic TM+PMI TM+must+cannot+PMI TM+correlation knowledge(PMI+syntactic) TM+must+cannot+correlation knowledge(c) our constrained topic model bydifferent Titer (tratio = 0.1)Figure 1: Performance of different cases3.5 Implicit Feature Detection via SVMAfter completing attribute selection, vector spacemodel(VSM) is applied to the selected attributes onthe explicit sentences.
For each feature fi, a SVMclassifierCi is adopted.
In train-set, the positive cas-es are the explicit sentences of fi, and the negativecases are the other explicit sentences.
For a non-explicit sentence, if the classification result of Ci ispositive, it is an implicit sentence which implies fi.4 Evaluation of Experimental Results4.1 Data SetsThere has no standard data set yet, we crawled theexperiment data, which included reviews about acellphone, from a famous Chinese shopping web-site1.
The data contains 14218 sentences.
The fea-ture of each sentence was manually annotated bytwo research assistants.
A handful of sentenceswhich were annotated inconsistently were deleted.Table 1 depicts the data set which is evaluated.
Otherfeatures were ignored because of their rare appear-ance.Here are some explanations: (1)The sentencescontaining several explicit features were not addedto the train-set.
(2) A tiny number of sentences con-tain both explicit and implicit features, and they canonly be regarded as explicit sentences.
(3) The train-ing set contains 3140 explicit sentences, the test setcontains 7043 non-explicit sentences and more than5500 sentences have no feature.
(4) According tothe ratio among the explicit sentences(6:1:2:3:1:2),it is reasonable that the most suitable number of top-ics should be 14.
For example, the ratio of the prod-1http://www.360buy.com/Table 1: Experiment dataFeatures Explicit Implicit Totalscreen 1165 244 1409quality 199 83 282battery 456 205 661price 627 561 1188appearance 224 167 391software 469 129 598uct feature screen is 6, so we can assign the featureto topic 0,1,2,3,4,5.
In our experiment, the perfor-mance of algorithm 1 is evaluated using F-measure.
(5) Although the size of dataset is limited, out pro-posed is based on the constraint-based topic model,which has been widely used in different NLP field-s.
So, our approach can generalize well in differentdatasets.
Of course, more high quality data will becollected to do the experiment in the future.4.2 Experimental ResultsFigure 1a depicts the performance of using tradi-tional attribute selection methods on SVM.
Using?2 test on SVM can achieve the best performance,which is about 66.7%.
In our constrained topicmodel, we use different Titer and tratio.
We con-ducted experiments by incorporating different typesprior knowledge.
From Figure 1b and 1c, we con-clude that: (1)All these methods perform much bet-ter than the traditional feature selection methods, theimprovements are more than 6%.
(2)The reason forthe little improvement of must-links is that the top-ic clusters have already obtained these linked word-906s.
(3)All the pre-existing knowledge performs bestand shows 3% improvement over non prior knowl-edge.
(4)Different types of prior knowledge havedifferent impact on the stabilities of different pa-rameters.
(5)As we have expected, by combing al-l prior knowledge, the best performance can reach77.78%.
Furthermore, as tratio or Titer changes,our constrained topic model incorporating all priorknowledge look like very stable.5 ConclusionsIn this paper, we adopt a constrained topic modelincorporating prior knowledge to select attribute forSVM classifiers to detect implicit features.
Exper-iments show this method outperforms the attributefeature selection methods and detect implicit fea-tures better.6 AcknowledgmentsThis work is supported by National Natural ScienceFoundation of China (Grant No: 61175110) and Na-tional Basic Research Program of China (973 Pro-gram, Grant No: 2012CB316305).ReferencesDavid Andrzejewski and Xiaojin Zhu.
2009.
Laten-t dirichlet alcation with topic-in-set knowledge.
InProceedings of the NAACL HLT 2009 Workshop onSemi-Supervised Learning for Natural Language Pro-cessing, pages 43?48.
Association for ComputationalLinguistics.D.M.
Blei and M.I.
Jordan.
2003.
Modeling annotateddata.
In Proceedings of the 26th annual internationalACM SIGIR conference on Research and developmentin informaion retrieval, pages 127?134.
ACM.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Laten-t dirichlet alcation.
the Journal of machine Learningresearch, 3:993?1022.Xiaowen Ding, Bing Liu, and Philip S. Yu.
2008.
Aholistic lexicon-based approach to opinion mining.
InProceedings of the international conference on Websearch and web data mining, WSDM ?08, pages 231?240, New York, NY, USA.
ACM.L.
Fei-Fei and P. Perona.
2005.
A bayesian hierarchicalmodel for learning natural scene categories.
In Com-puter Vision and Pattern Recognition, 2005.
CVPR2005.
IEEE Computer Society Conference on, vol-ume 2, pages 524?531.
IEEE.T.L.
Griffiths and M. Steyvers.
2004.
Finding scientif-ic topics.
Proceedings of the National Academy ofSciences of the United States of America, 101(Suppl1):5228?5235.Z.
Hai, K. Chang, and J. Kim.
2011.
Implicit featureidentification via co-occurrence association rule min-ing.
Computational Linguistics and Intelligent TextProcessing, pages 393?404.B.
Liu, M. Hu, and J. Cheng.
2005.
Opinion observer:analyzing and comparing opinions on the web.
In Pro-ceedings of the 14th international conference on WorldWide Web, pages 342?351.
ACM.P.
Quelhas, F. Monay, J.M.
Odobez, D. Gatica-Perez,T.
Tuytelaars, and L. Van Gool.
2005.
Modelingscenes with local descriptors and latent aspects.
InComputer Vision, 2005.
ICCV 2005.
Tenth IEEE In-ternational Conference on, volume 1, pages 883?890.IEEE.Q.
Su, K. Xiang, H. Wang, B.
Sun, and S. Yu.
2006.
Us-ing pointwise mutual information to identify implicitfeatures in customer reviews.
Computer Processing ofOriental Languages.
Beyond the Orient: The ResearchChallenges Ahead, pages 22?30.907
