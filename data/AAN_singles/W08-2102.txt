CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 9?16Manchester, August 2008TAG, Dynamic Programming, and the Perceptronfor Efficient, Feature-rich ParsingXavier Carreras Michael Collins Terry KooMIT CSAIL, Cambridge, MA 02139, USA{carreras,mcollins,maestro}@csail.mit.eduAbstractWe describe a parsing approach that makes useof the perceptron algorithm, in conjunction withdynamic programming methods, to recover fullconstituent-based parse trees.
The formalism allowsa rich set of parse-tree features, including PCFG-based features, bigram and trigram dependency fea-tures, and surface features.
A severe challenge inapplying such an approach to full syntactic pars-ing is the efficiency of the parsing algorithms in-volved.
We show that efficient training is feasi-ble, using a Tree Adjoining Grammar (TAG) basedparsing formalism.
A lower-order dependency pars-ing model is used to restrict the search space of thefull model, thereby making it efficient.
Experimentson the Penn WSJ treebank show that the modelachieves state-of-the-art performance, for both con-stituent and dependency accuracy.1 IntroductionIn global linear models (GLMs) for structured pre-diction, (e.g., (Johnson et al, 1999; Lafferty et al,2001; Collins, 2002; Altun et al, 2003; Taskar etal., 2004)), the optimal label y?
for an input x isy?= arg maxy?Y(x)w ?
f(x, y) (1)where Y(x) is the set of possible labels for the in-put x; f(x, y) ?
Rd is a feature vector that rep-resents the pair (x, y); and w is a parameter vec-tor.
This paper describes a GLM for natural lan-guage parsing, trained using the averaged percep-tron.
The parser we describe recovers full syntac-tic representations, similar to those derived by aprobabilistic context-free grammar (PCFG).
A keymotivation for the use of GLMs in parsing is thatthey allow a great deal of flexibility in the featureswhich can be included in the definition of f(x, y).c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.A critical problem when training a GLM forparsing is the computational complexity of theinference problem.
The averaged perceptron re-quires the training set to be repeatedly decodedunder the model; under even a simple PCFG rep-resentation, finding the argmax in Eq.
1 requiresO(n3G) time, where n is the length of the sen-tence, and G is a grammar constant.
The averagesentence length in the data set we use (the PennWSJ treebank) is over 23 words; the grammar con-stant G can easily take a value of 1000 or greater.These factors make exact inference algorithms vir-tually intractable for training or decoding GLMsfor full syntactic parsing.As a result, in spite of the potential advantagesof these methods, there has been very little previ-ous work on applying GLMs for full parsing with-out the use of fairly severe restrictions or approxi-mations.
For example, the model in (Taskar et al,2004) is trained on only sentences of 15 words orless; reranking models (Collins, 2000; Charniakand Johnson, 2005) restrict Y(x) to be a small setof parses from a first-pass parser; see section 1.1for discussion of other related work.The following ideas are central to our approach:(1) A TAG-based, splittable grammar.
Wedescribe a novel, TAG-based parsing formalismthat allows full constituent-based trees to be recov-ered.
A driving motivation for our approach comesfrom the flexibility of the feature-vector represen-tations f(x, y) that can be used in the model.
Theformalism that we describe allows the incorpora-tion of: (1) basic PCFG-style features; (2) theuse of features that are sensitive to bigram depen-dencies between pairs of words; and (3) featuresthat are sensitive to trigram dependencies.
Anyof these feature types can be combined with sur-face features of the sentence x, in a similar way9to the use of surface features in conditional ran-dom fields (Lafferty et al, 2001).
Crucially, inspite of these relatively rich representations, theformalism can be parsed efficiently (in O(n4G)time) using dynamic-programming algorithms de-scribed by Eisner (2000) (unlike many other TAG-related approaches, our formalism is ?splittable?in the sense described by Eisner, leading to moreefficient parsing algorithms).
(2) Use of a lower-order model for pruning.The O(n4G) running time of the TAG parser isstill too expensive for efficient training with theperceptron.
We describe a method that leveragesa simple, first-order dependency parser to restrictthe search space of the TAG parser in training andtesting.
The lower-order parser runs in O(n3H)time where H ?
G; experiments show that it isremarkably effective in pruning the search spaceof the full TAG parser.Experiments on the Penn WSJ treebank showthat the model recovers constituent structures withhigher accuracy than the approaches of (Charniak,2000; Collins, 2000; Petrov and Klein, 2007),and with a similar level of performance to thereranking parser of (Charniak and Johnson, 2005).The model also recovers dependencies with sig-nificantly higher accuracy than state-of-the-art de-pendency parsers such as (Koo et al, 2008; Mc-Donald and Pereira, 2006).1.1 Related WorkPrevious work has made use of various restrictionsor approximations that allow efficient training ofGLMs for parsing.
This section describes the rela-tionship between our work and this previous work.In reranking approaches, a first-pass parseris used to enumerate a small set of candidateparses for an input sentence; the reranking model,which is a GLM, is used to select between theseparses (e.g., (Ratnaparkhi et al, 1994; Johnson etal., 1999; Collins, 2000; Charniak and Johnson,2005)).
A crucial advantage of our approach is thatit considers a very large set of alternatives in Y(x),and can thereby avoid search errors that may bemade in the first-pass parser.1Another approach that allows efficient trainingof GLMs is to use simpler syntactic representa-tions, in particular dependency structures (McDon-1Some features used within reranking approaches may bedifficult to incorporate within dynamic programming, but itis nevertheless useful to make use of GLMs in the dynamic-programming stage of parsing.
Our parser could, of course,be used as the first-stage parser in a reranking approach.ald et al, 2005).
Dependency parsing can beimplemented in O(n3) time using the algorithmsof Eisner (2000).
In this case there is no gram-mar constant, and parsing is therefore efficient.
Adisadvantage of these approaches is that they donot recover full, constituent-based syntactic struc-tures; the increased linguistic detail in full syntac-tic structures may be useful in NLP applications,or may improve dependency parsing accuracy, asis the case in our experiments.2There has been some previous work on GLMapproaches for full syntactic parsing that make useof dynamic programming.
Taskar et al (2004)describe a max-margin approach; however, in thiswork training sentences were limited to be of 15words or less.
Clark and Curran (2004) describea log-linear GLM for CCG parsing, trained on thePenn treebank.
This method makes use of paral-lelization across an 18 node cluster, together withup to 25GB of memory used for storage of dy-namic programming structures for training data.Clark and Curran (2007) describe a perceptron-based approach for CCG parsing which is consid-erably more efficient, and makes use of a super-tagging model to prune the search space of the fullparsing model.
Recent work (Petrov et al, 2007;Finkel et al, 2008) describes log-linear GLMs ap-plied to PCFG representations, but does not makeuse of dependency features.2 The TAG-Based Parsing Model2.1 DerivationsThis section describes the idea of derivations inour parsing formalism.
As in context-free gram-mars or TAGs, a derivation in our approach is adata structure that specifies the sequence of opera-tions used in combining basic (elementary) struc-tures in a grammar, to form a full parse tree.
Theparsing formalism we use is related to the tree ad-joining grammar (TAG) formalisms described in(Chiang, 2003; Shen and Joshi, 2005).
However,an important difference of our work from this pre-vious work is that our formalism is defined to be?splittable?, allowing use of the efficient parsingalgorithms of Eisner (2000).A derivation in our model is a pair ?E,D?
whereE is a set of spines, and D is a set of dependencies2Note however that the lower-order parser that we use torestrict the search space of the TAG-based parser is based onthe work of McDonald et al (2005).
See also (Sagae et al,2007) for a method that uses a dependency parser to restrictthe search space of a more complex HPSG parser.10(a) SVPVBDateNPNNcake(b) SVPVPVBDateNPNNcakeFigure 1: Two example trees.specifying how the spines are combined to forma parse tree.
The spines are similar to elementarytrees in TAG.
Some examples are as follows:NPNNPJohnSVPVBDateNPNNcakeADVPRBquicklyADVPRBluckilyThese structures do not have substitution nodes, asis common in TAGs.3 Instead, the spines consistof a lexical anchor together with a series of unaryprojections, which usually correspond to differentX-bar levels associated with the anchor.The operations used to combine spines are sim-ilar to the TAG operations of adjunction and sis-ter adjunction.
We will call these operations regu-lar adjunction (r-adjunction) and sister adjunction(s-adjunction).
As one example, the cake spineshown above can be s-adjoined into the VP node ofthe ate spine, to form the tree shown in figure 1(a).In contrast, if we use the r-adjunction operation toadjoin the cake tree into the VP node, we get a dif-ferent structure, which has an additional VP levelcreated by the r-adjunction operation: the resultingtree is shown in figure 1(b).
The r-adjunction op-eration is similar to the usual adjunction operationin TAGs, but has some differences that allow ourgrammars to be splittable; see section 2.3 for morediscussion.We now give formal definitions of the sets E andD.
Take x to be a sentence consisting of n + 1words, x0.
.
.
xn, where x0is a special root sym-bol, which we will denote as ?.
A derivation for theinput sentence x consists of a pair ?E,D?, where:?
E is a set of (n + 1) tuples of the form ?i, ?
?,where i ?
{0 .
.
.
n} is an index of a word in thesentence, and ?
is the spine associated with theword xi.
The set E specifies one spine for eachof the (n + 1) words in the sentence.
Where it is3It would be straightforward to extend the approach to in-clude substitution nodes, and a substitution operation.clear from context, we will use ?ito refer to thespine in E corresponding to the i?th word.?
D is a set of n dependencies.
Each depen-dency is a tuple ?h,m, l?.
Here h is the index ofthe head-word of the dependency, correspondingto the spine ?hwhich contains a node that is beingadjoined into.
m is the index of the modifier-wordof the dependency, corresponding to the spine ?mwhich is being adjoined into ?h.
l is a label.The label l is a tuple ?POS, A, ?h, ?m, L?.
?hand?mare the head and modifier spines that are be-ing combined.
POS specifies which node in ?hisbeing adjoined into.
A is a binary flag specifyingwhether the combination operation being used is s-adjunction or r-adjunction.
L is a binary flag spec-ifying whether or not any ?previous?
modifier hasbeen r-adjoined into the position POS in ?h.
By aprevious modifier, we mean a modifier m?
that wasadjoined from the same direction as m (i.e., suchthat h < m?
< m or m < m?
< h).It would be sufficient to define l to be the pair?POS, A?
?the inclusion of ?h, ?mand L adds re-dundant information that can be recovered fromthe set E, and other dependencies in D?but itwill be convenient to include this information inthe label.
In particular, it is important that giventhis definition of l, it is possible to define a func-tion GRM(l) that maps a label l to a triple of non-terminals that represents the grammatical relationbetween m and h in the dependency structure.
Forexample, in the tree shown in figure 1(a), the gram-matical relation between cake and ate is the tripleGRM(l) = ?VP VBD NP?.
In the tree shown infigure 1(b), the grammatical relation between cakeand ate is the triple GRM(l) = ?VP VP NP?.The conditions under which a pair ?E,D?
formsa valid derivation for a sentence x are similar tothose in conventional LTAGs.
Each ?i, ??
?
Emust be such that ?
is an elementary tree whoseanchor is the word xi.
The dependencies D mustform a directed, projective tree spanning words0 .
.
.
n, with ?
at the root of this tree, as is alsothe case in previous work on discriminative ap-proches to dependency parsing (McDonald et al,2005).
We allow any modifier tree ?mto adjoininto any position in any head tree ?h, but the de-pendencies D must nevertheless be coherent?forexample they must be consistent with the spines inE, and they must be nested correctly.4 We will al-4For example, closer modifiers to a particular head mustadjoin in at the same or a lower spine position than modifiers11low multiple modifier spines to s-adjoin or r-adjoininto the same node in a head spine; see section 2.3for more details.2.2 A Global Linear ModelThe model used for parsing with this approach isa global linear model.
For a given sentence x, wedefine Y(x) to be the set of valid derivations for x,where each y ?
Y(x) is a pair ?E,D?
as describedin the previous section.
A function f maps (x, y)pairs to feature-vectors f(x, y) ?
Rd.
The param-eter vector w is also a vector in Rd.
Given thesedefinitions, the optimal derivation for an input sen-tence x is y?
= argmaxy?Y(x)w ?
f(x, y).We now come to how the feature-vector f(x, y)is defined in our approach.
A simple ?first-order?model would definef(x, y) =??i,??
?E(y)e(x, ?i, ??)
+??h,m,l?
?D(y)d(x, ?h,m, l?)
(2)Here we use E(y) and D(y) to respectively referto the set of spines and dependencies in y. Thefunction e maps a sentence x paired with a spine?i, ??
to a feature vector.
The function d maps de-pendencies within y to feature vectors.
This de-composition is similar to the first-order model ofMcDonald et al (2005), but with the addition ofthe e features.We will extend our model to include higher-order features, in particular features based on sib-ling dependencies (McDonald and Pereira, 2006),and grandparent dependencies, as in (Carreras,2007).
If y = ?E,D?
is a derivation, then:?
S(y) is a set of sibling dependencies.
Eachsibling dependency is a tuple ?h,m, l, s?.
For each?h,m, l, s?
?
S the tuple ?h,m, l?
is an element ofD; there is one member of S for each member ofD.
The index s is the index of the word that wasadjoined to the spine for h immediately before m(or the NULL symbol if no previous adjunction hastaken place).?
G(y) is a set of grandparent dependencies oftype 1.
Each type 1 grandparent dependency is atuple ?h,m, l, g?.
There is one member of G forevery member of D. The additional information,the index g, is the index of the word that is the firstmodifier to the right of the spine for m.that are further from the head.
(a) SVPVPVPVBDateNPNNcakeNPNNtodayADVPRBquickly(b) SNPNNPJohnVPADVPRBluckilyVPVBDateFigure 2: Two Example TreesSNPNNPJohnVPVPADVPRBluckilyVPVBDateNPNNcakeNPNNtodayADVPRBquicklyFigure 3: An example tree, formed by a combina-tion of the two structures in figure 2.?
Q(y) is an additional set of grandparent de-pendencies, of type 2.
Each of these dependenciesis a tuple ?h,m, l, q?.
Again, there is one memberof Q for every member of D. The additional infor-mation, the index q, is the index of the word that isthe first modifier to the left of the spine for m.The feature-vector definition then becomes:f(x, y) =X?i,??
?E(y)e(x, ?i, ??)
+X?h,m,l?
?D(y)d(x, ?h,m, l?)
+X?h,m,l,s?
?S(y)s(x, ?h,m, l, s?)
+X?h,m,l,g?
?G(y)g(x, ?h,m, l, g?)
+X?h,m,l,q?
?Q(y)q(x, ?h,m, l, q?
)(3)where s, g and q are feature vectors correspondingto the new, higher-order elements.52.3 Recovering Parse Trees from DerivationsAs in TAG approaches, there is a mapping fromderivations ?E,D?
to parse trees (i.e., the type oftrees generated by a context-free grammar).
In ourcase, we map a spine and its dependencies to a con-stituent structure by first handling the dependen-5We also added constituent-boundary features to themodel, which is a simple change that led to small improve-ments on validation data; for brevity we omit the details.12cies on each side separately and then combiningthe left and right sides.First, it is straightforward to build the con-stituent structure resulting from multiple adjunc-tions on the same side of a spine.
As one exam-ple, the structure in figure 2(a) is formed by firsts-adjoining the spine with anchor cake into the VPnode of the spine for ate, then r-adjoining spinesanchored by today and quickly into the same node,where all three modifier words are to the right ofthe head word.
Notice that each r-adjunction op-eration creates a new VP level in the tree, whereass-adjunctions do not create a new level.
Now con-sider a tree formed by first r-adjoining a spine forluckily into the VP node for ate, followed by s-adjoining the spine for John into the S node, inboth cases where the modifiers are to the left ofthe head.
In this case the structure that would beformed is shown in figure 2(b).Next, consider combining the left and rightstructures of a spine.
The main issue is how tohandle multiple r-adjunctions or s-adjunctions onboth sides of a node in a spine, because our deriva-tions do not specify how adjunctions from differentsides embed with each other.
In our approach, thecombination operation preserves the height of thedifferent modifiers from the left and right direc-tions.
To illustrate this, figure 3 shows the resultof combining the two structures in figure 2.
Thecombination of the left and right modifier struc-tures has led to flat structures, for example the ruleVP?
ADVP VP NP in the above tree.Note that our r-adjunction operation is differentfrom the usual adjunction operation in TAGs, inthat ?wrapping?
adjunctions are not possible, andr-adjunctions from the left and right directions areindependent from each other; because of this ourgrammars are splittable.3 Parsing Algorithms3.1 Use of Eisner?s AlgorithmsThis section describes the algorithm for findingy?= argmaxy?Y(x)w ?
f(x, y) where f(x, y) isdefined through either the first-order model (Eq.
2)or the second-order model (Eq.
3).For the first-order model, the methods describedin (Eisner, 2000) can be used for the parsing algo-rithm.
In Eisner?s algorithms for dependency pars-ing each word in the input has left and right finite-state (weighted) automata, which generate the leftand right modifiers of the word in question.
Wemake use of this idea of automata, and also makedirect use of the method described in section 4.2 of(Eisner, 2000) that allows a set of possible sensesfor each word in the input string.
In our use ofthe algorithm, each possible sense for a word cor-responds to a different possible spine that can beassociated with that word.
The left and right au-tomata are used to keep track of the last positionin the spine that was adjoined into on the left/rightof the head respectively.
We can make use of sep-arate left and right automata?i.e., the grammar issplittable?because left and right modifiers are ad-joined independently of each other in the tree.
Theextension of Eisner?s algorithm to the second-ordermodel is similar to the algorithm described in (Car-reras, 2007), but again with explicit use of wordsenses and left/right automata.
The resulting algo-rithms run in O(Gn3) and O(Hn4) time for thefirst-order and second-order models respectively,where G and H are grammar constants.3.2 Efficient ParsingThe efficiency of the parsing algorithm is impor-tant in applying the parsing model to test sen-tences, and also when training the model using dis-criminative methods.
The grammar constants Gand H introduced in the previous section are poly-nomial in factors such as the number of possiblespines in the model, and the number of possiblestates in the finite-state automata implicit in theparsing algorithm.
These constants are large, mak-ing exhaustive parsing very expensive.To deal with this problem, we use a simple ini-tial model to prune the search space of the morecomplex model.
The first-stage model we useis a first-order dependency model, with labeleddependencies, as described in (McDonald et al,2005).
As described shortly, we will use this modelto compute marginal scores for dependencies inboth training and test sentences.
A marginal score?
(x, h,m, l) is a value between 0 and 1 that re-flects the plausibility of a dependency for sentencex with head-word xh, modifier word xm, and la-bel l. In the first-stage pruning model the labels lare triples of non-terminals representing grammat-ical relations, as described in section 2.1 of thispaper?for example, one possible label would be?VP VBD NP?, and in general any triple of non-terminals is possible.Given a sentence x, and an index m of a wordin that sentence, we define DMAX(x,m) to be the13highest scoring dependency with m as a modifier:DMAX(x,m) = maxh,l?
(x, h,m, l)For a sentence x, we then define the set of allow-able dependencies to bepi(x) = {?h,m, l?
: ?
(x, h,m, l) ?
?DMAX(x,m)}where ?
is a constant dictating the beam size thatis used (in our experiments we used ?
= 10?6).The set pi(x) is used to restrict the set of pos-sible parses under the full TAG-based model.
Insection 2.1 we described how the TAG model hasdependency labels of the form ?POS, A, ?h, ?m, L?,and that there is a function GRM that maps labelsof this form to triples of non-terminals.
The ba-sic idea of the pruned search is to only allow de-pendencies of the form ?h,m, ?POS, A, ?h, ?m, L?
?if the tuple ?h,m, GRM(?POS, A, ?h, ?m, L?)?
is amember of pi(x), thus reducing the search spacefor the parser.We now turn to how the marginals ?
(x, h,m, l)are defined and computed.
A simple approachwould be to use a conditional log-linear model(Lafferty et al, 2001), with features as defined byMcDonald et al (2005), to define a distributionP (y|x) where the parse structures y are depen-dency structures with labels that are triples of non-terminals.
In this case we could define?
(x, h,m, l) =?y:(h,m,l)?yP (y|x)which can be computed with inside-outside stylealgorithms, applied to the data structures from(Eisner, 2000).
The complexity of training and ap-plying such a model is again O(Gn3), where G isthe number of possible labels, and the number ofpossible labels (triples of non-terminals) is aroundG = 1000 in the case of treebank parsing; thisvalue for G is still too large for the method to be ef-ficient.
Instead, we train three separate models ?1,?2, and ?3for the three different positions in thenon-terminal triples.
We then take ?
(x, h,m, l) tobe a product of these three models, for example wewould calculate?
(x, h,m, ?VP VBD NP?)
=?1(x, h,m, ?VP?)?
?2(x, h,m, ?VBD?)?
?3(x, h,m, ?NP?
)Training the three models, and calculating themarginals, now has a grammar constant equalto the number of non-terminals in the grammar,which is far more manageable.
We use the algo-rithm described in (Globerson et al, 2007) to trainthe conditional log-linear model; this method wasfound to converge to a good model after 10 itera-tions over the training data.4 Implementation Details4.1 FeaturesSection 2.2 described the use of feature vectorsassociated with spines used in a derivation, to-gether with first-order, sibling, and grandparentdependencies.
The dependency features used inour experiments are closely related to the featuresdescribed in (Carreras, 2007), which are an ex-tension of the McDonald and Pereira (2006) fea-tures to cover grandparent dependencies in addi-tion to first-order and sibling dependencies.
Thefeatures take into account the identity of the la-bels l used in the derivations.
The features couldpotentially look at any information in the la-bels, which are of the form ?POS, A, ?h, ?m, L?,but in our experiments, we map labels to a pair(GRM(?POS, A, ?h, ?m, L?
), A).
Thus the label fea-tures are sensitive only to the triple of non-terminals corresponding to the grammatical rela-tion involved in an adjunction, and a binary flagspecifiying whether the operation is s-adjunctionor r-adjunction.For the spine features e(x, ?i, ??
), we use fea-ture templates that are sensitive to the identity ofthe spine ?, together with contextual features ofthe string x.
These features consider the iden-tity of the words and part-of-speech tags in a win-dow that is centered on xiand spans the rangex(i?2).
.
.
x(i+2).4.2 Extracting Derivations from Parse TreesIn the experiments in this paper, the followingthree-step process was used: (1) derivations wereextracted from a training set drawn from the PennWSJ treebank, and then used to train a parsingmodel; (2) the test data was parsed using the re-sulting model, giving a derivation for each testdata sentence; (3) the resulting test-data deriva-tions were mapped back to Penn-treebank styletrees, using the method described in section 2.1.To achieve step (1), we first apply a set of head-finding rules which are similar to those describedin (Collins, 1997).
Once the head-finding ruleshave been applied, it is straightforward to extract14precision recall F1PPK07 ?
?
88.3FKM08 88.2 87.8 88.0CH2000 89.5 89.6 89.6CO2000 89.9 89.6 89.8PK07 90.2 89.9 90.1this paper 91.4 90.7 91.1CJ05 ?
?
91.4H08 ?
?
91.7CO2000(s24) 89.6 88.6 89.1this paper (s24) 91.1 89.9 90.5Table 1: Results for different methods.
PPK07, FKM08,CH2000, CO2000, PK07, CJ05 and H08 are results on section23 of the Penn WSJ treebank, for the models of Petrov et al(2007), Finkel et al (2008), Charniak (2000), Collins (2000),Petrov and Klein (2007), Charniak and Johnson (2005), andHuang (2008).
(CJ05 is the performance of an updatedmodel at http://www.cog.brown.edu/mj/software.htm.)
?s24?denotes results on section 24 of the treebank.s23 s24KCC08 unlabeled 92.0 91.0KCC08 labeled 92.5 91.7this paper 93.5 92.5Table 2: Table showing unlabeled dependency accuracy forsections 23 and 24 of the treebank, using the method of (Ya-mada and Matsumoto, 2003) to extract dependencies fromparse trees from our model.
KCC08 unlabeled is from (Kooet al, 2008), a model that has previously been shown to havehigher accuracy than (McDonald and Pereira, 2006).
KCC08labeled is the labeled dependency parser from (Koo et al,2008); here we only evaluate the unlabeled accuracy.derivations from the Penn treebank trees.Note that the mapping from parse trees toderivations is many-to-one: for example, the ex-ample trees in section 2.3 have structures that areas ?flat?
(have as few levels) as is possible, giventhe set D that is involved.
Other similar trees,but with more VP levels, will give the same setD.
However, this issue appears to be benign in thePenn WSJ treebank.
For example, on section 22 ofthe treebank, if derivations are first extracted usingthe method described in this section, then mappedback to parse trees using the method described insection 2.3, the resulting parse trees score 100%precision and 99.81% recall in labeled constituentaccuracy, indicating that very little information islost in this process.4.3 Part-of-Speech Tags, and SpinesSentences in training, test, and development dataare assumed to have part-of-speech (POS) tags.POS tags are used for two purposes: (1) in thefeatures described above; and (2) to limit the setof allowable spines for each word during parsing.Specifically, for each POS tag we create a separate1st stage 2nd stage?
active coverage oracle F1speed F110?4 0.07 97.7 97.0 5:15 91.110?5 0.16 98.5 97.9 11:45 91.610?6 0.34 99.0 98.5 21:50 92.0Table 3: Effect of the beam size, controlled by ?, on theperformance of the parser on the development set (1,699 sen-tences).
In each case ?
refers to the beam size used in bothtraining and testing the model.
?active?
: percentage of de-pendencies that remain in the beam out of the total number oflabeled dependencies (1,000 triple labels times 1,138,167 un-labeled dependencies); ?coverage?
: percentage of correct de-pendencies in the beam out of the total number of correct de-pendencies.
?oracle F1?
: maximum achievable score of con-stituents, given the beam.
?speed?
: parsing time in min:secfor the TAG-based model (this figure does not include the timetaken to calculate the marginals using the lower-order model);?F1?
: score of predicted constituents.dictionary listing the spines that have been seenwith this POS tag in training data; during parsingwe only allow spines that are compatible with thisdictionary.
(For test or development data, we usedthe part-of-speech tags generated by the parser of(Collins, 1997).
Future work should consider in-corporating the tagging step within the model; it isnot challenging to extend the model in this way.
)5 ExperimentsSections 2-21 of the Penn Wall Street Journal tree-bank were used as training data in our experiments,and section 22 was used as a development set.
Sec-tions 23 and 24 were used as test sets.
The modelwas trained for 20 epochs with the averaged per-ceptron algorithm, with the development data per-formance being used to choose the best epoch.
Ta-ble 1 shows the results for the method.Our experiments show an improvement in per-formance over the results in (Collins, 2000; Char-niak, 2000).
We would argue that the Collins(2000) method is considerably more complex thanours, requiring a first-stage generative model, to-gether with a reranking approach.
The Char-niak (2000) model is also arguably more com-plex, again using a carefully constructed genera-tive model.
The accuracy of our approach alsoshows some improvement over results in (Petrovand Klein, 2007).
This work makes use of aPCFG with latent variables that is trained usinga split/merge procedure together with the EM al-gorithm.
This work is in many ways comple-mentary to ours?for example, it does not makeuse of GLMs, dependency features, or of repre-sentations that go beyond PCFG productions?and15some combination of the two methods may givefurther gains.Charniak and Johnson (2005), and Huang(2008), describe approaches that make use of non-local features in conjunction with the Charniak(2000) model; future work may consider extend-ing our approach to include non-local features.Finally, other recent work (Petrov et al, 2007;Finkel et al, 2008) has had a similar goal of scal-ing GLMs to full syntactic parsing.
These mod-els make use of PCFG representations, but do notexplicitly model bigram or trigram dependencies.The results in this work (88.3%/88.0% F1) arelower than our F1score of 91.1%; this is evidenceof the benefits of the richer representations enabledby our approach.Table 2 shows the accuracy of the model inrecovering unlabeled dependencies.
The methodshows improvements over the method describedin (Koo et al, 2008), which is a state-of-the-artsecond-order dependency parser similar to that of(McDonald and Pereira, 2006), suggesting that theincorporation of constituent structure can improvedependency accuracy.Table 3 shows the effect of the beam-size on theaccuracy and speed of the parser on the develop-ment set.
With the beam setting used in our exper-iments (?
= 10?6), only 0.34% of possible depen-dencies are considered by the TAG-based model,but 99% of all correct dependencies are included.At this beam size the best possible F1constituentscore is 98.5.
Tighter beams lead to faster parsingtimes, with slight drops in accuracy.6 ConclusionsWe have described an efficient and accurate parserfor constituent parsing.
A key to the approach hasbeen to use a splittable grammar that allows effi-cient dynamic programming algorithms, in com-bination with pruning using a lower-order model.The method allows relatively easy incorporation offeatures; future work should leverage this in pro-ducing more accurate parsers, and in applying theparser to different languages or domains.Acknowledgments X. Carreras was supported by theCatalan Ministry of Innovation, Universities and Enterprise,by the GALE program of DARPA, Contract No.
HR0011-06-C-0022, and by a grant from NTT, Agmt.
Dtd.
6/21/1998.T.
Koo was funded by NSF grant IIS-0415030.
M. Collinswas funded by NSF grant IIS-0347631 and DARPA contractNo.
HR0011-06-C-0022.
Thanks to Jenny Rose Finkel forsuggesting that we evaluate dependency parsing accuracies.ReferencesAltun, Y., I. Tsochantaridis, and T. Hofmann.
2003.
Hiddenmarkov support vector machines.
In ICML.Carreras, X.
2007.
Experiments with a higher-order projec-tive dependency parser.
In Proc.
EMNLP-CoNLL SharedTask.Charniak, E. and M. Johnson.
2005.
Coarse-to-fine n-bestparsing and maxent discriminative reranking.
In Proc.ACL.Charniak, E. 2000.
A maximum-entropy-inspired parser.
InProc.
NAACL.Chiang, D. 2003.
Statistical parsing with an automaticallyextracted tree adjoining grammar.
In Bod, R., R. Scha, andK.
Sima?an, editors, Data Oriented Parsing, pages 299?316.
CSLI Publications.Clark, S. and J. R. Curran.
2004.
Parsing the wsj using ccgand log-linear models.
In Proc.
ACL.Clark, Stephen and James R. Curran.
2007.
Perceptron train-ing for a wide-coverage lexicalized-grammar parser.
InProc.
ACL Workshop on Deep Linguistic Processing.Collins, M. 1997.
Three generative, lexicalised models forstatistical parsing.
In Proc.
ACL.Collins, M. 2000.
Discriminative reranking for natural lan-guage parsing.
In Proc.
ICML.Collins, M. 2002.
Discriminative training methods for hid-den markov models: Theory and experiments with percep-tron algorithms.
In Proc.
EMNLP.Eisner, J.
2000.
Bilexical grammars and their cubic-timeparsing algorithms.
In Bunt, H. C. and A. Nijholt, editors,New Developments in Natural Language Parsing, pages29?62.
Kluwer Academic Publishers.Finkel, J. R., A. Kleeman, and C. D. Manning.
2008.
Effi-cient, feature-based, conditional random field parsing.
InProc.
ACL/HLT.Globerson, A., T. Koo, X. Carreras, and M. Collins.
2007.Exponentiated gradient algorithms for log-linear struc-tured prediction.
In Proc.
ICML.Huang, L. 2008.
Forest reranking: Discriminative parsingwith non-local features.
In Proc.
ACL/HLT.Johnson, M., S. Geman, S. Canon, Z. Chi, and S. Riezler.1999.
Estimators for stochastic unification-based gram-mars.
In Proc.
ACL.Koo, Terry, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Proc.ACL/HLT.Lafferty, J., A. McCallum, and F. Pereira.
2001.
Conditonalrandom fields: Probabilistic models for segmenting and la-beling sequence data.
In Proc.
ICML.McDonald, R. and F. Pereira.
2006.
Online learning of ap-proximate dependency parsing algorithms.
In Proc.
EACL.McDonald, R., K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
In Proc.ACL.Petrov, S. and D. Klein.
2007.
Improved inference for unlex-icalized parsing.
In Proc.
of HLT-NAACL.Petrov, S., A. Pauls, and D. Klein.
2007.
Discriminative log-linear grammars with latent variables.
In Proc.
NIPS.Ratnaparkhi, A., S. Roukos, and R. Ward.
1994.
A maximumentropy model for parsing.
In Proc.
ICSLP.Sagae, Kenji, Yusuke Miyao, and Jun?ichi Tsujii.
2007.
Hpsgparsing with shallow dependency constraints.
In Proc.ACL, pages 624?631.Shen, L. and A.K.
Joshi.
2005.
Incremental ltag parsing.
InProc HLT-EMNLP.Taskar, B., D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proceedings of theEMNLP-2004.Yamada, H. and Y. Matsumoto.
2003.
Statistical dependencyanalysis with support vector machines.
In Proc.
IWPT.16
