Workshop on Computational Linguistics for Literature, pages 69?77,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsA Dictionary of Wisdom and Wit:Learning to Extract Quotable Phrases?Michael BenderskyDept.
of Computer ScienceUniversity of MassachusettsAmherst, MAbemike@cs.umass.eduDavid A. SmithDept.
of Computer ScienceUniversity of MassachusettsAmherst, MAdasmith@cs.umass.eduAbstractReaders suffering from information overloadhave often turned to collections of pithy andfamous quotations.
While research on large-scale analysis of text reuse has found effectivemethods for detecting widely disseminatedand famous quotations, this paper explores thecomplementary problem of detecting, frominternal evidence alone, which phrases arequotable.
These quotable phrases are mem-orable and succinct statements that people arelikely to find useful outside of their originalcontext.
We evaluate quotable phrase extrac-tion using a large digital library and demon-strate that an integration of lexical and shallowsyntactic features results in a reliable extrac-tion process.
A study using a reddit commu-nity of quote enthusiasts as well as a simplecorpus analysis further demonstrate the prac-tical applications of our work.1 IntroductionReaders have been anxious about information over-load for a long time: not only since the rise of theweb, but with the earlier explosion of printed books,and even in manuscript culture (Blair, 2010).
Onetraditional response to the problem has been ex-cerpting passages that might be useful outside theiroriginal sources, copying them into personal com-monplace books, and publishing them in dictionariessuch as Bartlett?s Familiar Quotations or the Oxford?
?The book is a dictionary of wisdom and wit...?
(SamuelSmiles, ?A Publisher and His Friends?)
This and all the subse-quent quotations in this paper were discovered by the proposedquotable phrase extraction process.Dictionary of Quotations.
Even on the web, collec-tion of quotable phrases continues to thrive1, as evi-denced by the popularity of quotation websites suchas BrainyQuote and Wikiquote.According to a recent estimate, there are closeto 130 million unique book records in world li-braries today (Taycher, 2010).
Many of thesebooks are being digitized and stored by commercialproviders (e.g., Google Books and Amazon), as wellas non-profit organizations (e.g., Internet Archiveand Project Gutenberg).As a result of this digitization, the developmentof new methods for preserving, accessing and ana-lyzing the contents of literary corpora becomes animportant research venue with many practical appli-cations (Michel et al, 2011).
One particularly in-teresting line of work in these large digital librarieshas focused on detecting text reuse, i.e., passagesfrom one source that are quoted in another (Kolakand Schilit, 2008).In contrast, in this paper we explore the modelingof phrases that are likely to be quoted.
This phrasemodeling is done based on internal evidence alone,regardless of whether or not the phrase actually isquoted in existing texts.We call such potential quotation a quotablephrase ?
a meaningful, memorable, and succinctstatement that can be quoted without its originalcontext.
This kind of phrases includes aphorisms,epigrams, maxims, proverbs, and epigraphs.1?Nothing is so pleasant as to display your worldly wis-dom in epigram and dissertation, but it is a trifle tedious tohear another person display theirs.?
(Kate Sanborn, ?The Witof Women?
)69BookSentenceSegmentationNaive BayesFilteringQuotable PhraseDetectionLabeledQuotation SetExternalQuotation SetExtractedQuotesFigure 1: Diagram of the quotable phrase extraction process.A computational approach to quotable phrase ex-traction has several practical applications.
For in-stance, it can be used to recommend new additions toexisting quotable phrase collections, especially fo-cusing on lesser read and studied authors and liter-ary works2.
It can also generate quotable phrasesthat will serve as catchy and entertaining previewsfor book promotion and advertisement3 .In this work, we describe such a computationalapproach to quotable phrase extraction.
Our ap-proach leverages the Project Gutenberg digital li-brary and an online collection of quotations to builda quotable language model.
This language modelis further refined by a supervised learning algorithmthat combines lexical and shallow syntactic features.In addition, we demonstrate that a computationalapproach can help to address some intriguing ques-tions about the nature of quotability.
What are thelexical and the syntactic features that govern thequotability of a phrase?
Which authors and booksare highly quotable?
How much variance is there inthe perceived quotability of a given phrase?The remainder of this paper is organized as fol-lows.
In Section 2 we provide a detailed descriptionof the entire process of quotable phrase extraction.In Section 3 we review the related work.
In Sections4 and 5 we evaluate the quotable phrase extractionprocess, and provide some corpus quotability analy-sis.
We conclude the paper in Section 6.2 Quotable Phrase ExtractionThere are three unique challenges that need to beaddressed in the design of the process of quotable2?There is life in a poet so long as he is quoted...?
(Sir AlfredComyn Lyall, ?Studies in Literature and History?
)3As an example, see the ?Popular Highlights?
feature forKindle e-books in the Amazon bookstore.phrase extraction.
The first challenge stems from thefact that the boundaries of potential quotes are oftenambiguous.
A quotable phrase can consist of a sen-tence fragment, a complete sentence, or a passage oftext that spans several sentences.The second challenge is that the occurrence ofquotable phrases is a rare phenomena in literary cor-pora.
A randomly selected book passage is unlikelyto be quotable without any additional context.The third challenge is related to the syntax and se-mantics of quotable phrases.
For instance, considerthe phrase?Evil men make evil use of the law, thoughthe law is good, while good men die well, al-though death is an evil.?
(Thomas Aquinas,?Summa Theologica?
)and contrast it with?Of the laws that he can see, the great se-quences of life to death, of evil to sorrow,of goodness to happiness, he tells in burningwords.?
(Henry Fielding, ?The Soul of a Peo-ple?
)While both of these phrases share a common vocab-ulary (law, death, good and evil), the latter sentencecontains unresolved pronouns (he, twice) that makeit less amenable to quotation out of context.Accordingly, we design a three-stage quotablephrase extraction process, with each stage corre-sponding to one of challenges described above.
Thediagram in Figure 1 provides a high-level overviewof the entire extraction process on a single book.Next, we provide a brief description of this diagram.Then, in the following sections, we focus on individ-ual stages of the extraction process.To address the first challenge of quote boundarydetection, at the first stage of the extraction process70(Sentence Segmentation) we segment the text of theinput book into sentences using an implementationof the Punkt sentence boundary detection algorithm(Kiss and Strunk, 2006).
In an initial experiment, wefound that 78% of the approximately 4,000 quota-tions collected from the QuotationsPage4 consist ofa single sentence.
From now on, therefore, we makea simplifying assumption that an extracted quotablephrase is confined within the sentence boundaries.The second processing stage (Na?
?ve Bayes Filter-ing) aims to address the second challenge (the rar-ity of quotable phrases) and significantly increasesthe ratio of quotable phrases that are considered ascandidates in the final processing stage (QuotablePhrase Detection).
To this end, we use a set of quo-tations collected from an external resource to build aquotable language model.
Only sentences that havea sufficiently high likelihood of being drawn fromthis language model are considered at the final pro-cessing stage.For this final processing stage (Quotable PhraseDetection), we develop a supervised algorithm thatfocuses on the third challenge, and analyzes the syn-tactic structure of the input sentences.
This super-vised algorithm makes use of structural and syntac-tic features that may effect phrase quotability, in ad-dition to the vocabulary of the phrase.2.1 Na?
?ve Bayes FilteringIn order to account for the rarity of quotable phrasesin the book corpus, we use a filtering approach basedon a pre-built quotable language model.
Using thisfiltering approach, we significantly reduce the num-ber of sentences that need to be considered in the su-pervised quotable phrase detection stage (describedin Section 2.2).
In addition, this approach increasesthe ratio of quotable phrases considered at the super-vised stage, addressing the problem of the sparsity ofpositive examples.To build the quotable language model, we boot-strap the existing quotation collections on the web.In particular, we collect approximately 4,000 quoteson more than 200 subjects from the QuotationsPage.This collection provides a diverse set of high-qualityquotations on subjects ranging from Laziness andGenius to Technology and Taxes.4www.quotationspage.comThen, we build two separate unigram languagemodels.
The first one is the quotable languagemodel, which is built using the collected quotations(LQ).
The second one is the background languagemodel, which is built using the entire book corpus(LC).
Using these language models we compute alog-likelihood ratio for each processed sentence s,asLLRs =?w?sln p(w|LQ)p(w|LC), (1)where the probabilities p(w|?)
are computed using amaximum likelihood estimate with add-one smooth-ing.A sentence s is allowed to pass the filtering stageif and only if LLRs ?
[?, ?
], where ?, ?
are posi-tive constants5.
The lower bound on the LLRs, ?,requires the sentence to be highly probable given thequotable language model LQ.
The upper bound onthe LLRs, ?, filters out sentences that are highly im-probable given the background language model LC .Finally, the sentences for which LLRs ?
[?, ?
]are allowed to pass through the Na?
?ve Bayes filter.They are forwarded to the next stage, in which a su-pervised quotable phrase detection is performed.2.2 Supervised Quotable Phrase DetectionIn a large corpus, a supervised quotable phrase de-tection method needs to handle a significant num-ber of input instances (in our corpus, an average-sized book contains approximately 2,000 sentences).Therefore, we make use of a simple and efficientperceptron algorithm, which is implemented follow-ing the description by Bishop (2006).We note, however, that the proposed superviseddetection method can be also implemented using avariety of other binary prediction techniques.
Inan initial experiment, we found that more complexmethods (e.g., decision trees) were comparable to orworse than the simple perceptron algorithm.Formally, we define a binary function f(s) whichdetermines whether an input sentence s is a quotable(q) or a non-quotable (q) phrase, based on:f(s) ={q if wxs > 0q else, (2)5In this work, we set ?
= 1, ?
= 25.
This setting is doneprior to seeing any labeled data.71Feature DescriptionLexicalLLR Sentence log-likelihood ratio (Eq.
1)#word Number of words in s.#char Number of characters in s.wordLenAgg Feature for each aggregate Agg of word length in s.Agg = {min, max, mean}#capital Number of capitalized words in s.#quantifier Number of universal quantifiers in s (from a list of 13 quantifiers, e.g., all, whole, nobody).#stops Number of common stopwords in s.beginStop True if s begins with a stopword, False otherwise.hasDialog True if s contains at least one of the three common dialog terms {say, says, said}.#abstract Number of abstract concepts (e.g., adventure, charity, stupidity ) in s.PunctuationhasP Five features to indicate whether punctuation of type P is present in s.P = {quotations, parentheses, colon, dash, semi-colon}.Parts of Speech#POS Four features for the number of occurrences of part-of-speech POS in s.POS = {noun, verb, adjective, adverb, pronoun}.hasComp True if s contains a comparative adjective or adverb, False otherwise.hasSuper True if s contains a superlative adjective or adverb, False otherwise.hasPP True if s contains a verb in past participle, False otherwise.#IGSeq[i] Count of the POS sequence with the i-th highest IG(X,Y ) (Eq.
3) in s.Table 1: Description of the quotability features that are computed for each sentence s .where xs is a vector of quotability features com-puted for the sentence s, and w is a weight vectorassociated with these features.
The weight vector wis updated using stochastic gradient descent on theperceptron error function (Bishop, 2006).Since Eq.
2 demonstrates that the supervisedquotable phrase detection can be formulated as astandard binary classification problem, its successwill be largely determined by an appropriate choiceof feature vector xs.
As we are unaware of anyprevious work on supervised detection of quotablephrases, we develop an initial set of easy-to-computefeatures that considers the lexical and shallow syn-tactic structure of the analyzed sentence.2.3 Quotability FeaturesA decision about phrase quotability is often sub-jective; it is strongly influenced by personal tasteand circumstances6 .
Therefore, the set of featuresthat we describe in this section is merely a coarse-grained approximation of the true intrinsic qualitiesof a quotable phrase.
Nevertheless, it is important to6?One man?s beauty another?s ugliness; one man?s wisdomanother?s folly.?
(Ralph Waldo Emerson, ?Essays?
)note that these features do prove to be beneficial inthe context of the quote detection task, as is demon-strated by our empirical evaluation in Section 5.Table 1 details the quotability features, which aredivided into 3 groups: lexical, punctuation-basedand POS-based features.
All of these features areconceptually simple and can be efficiently computedeven for a large number of input sentences.Some of these features are inspired by existingtext analysis tasks.
For instance, work on readabil-ity detection for the web (Kanungo and Orr, 2009;Si and Callan, 2001) examined features which aresimilar to the lexical features in Table 1.
Parts ofspeech features (e.g., the presence of comparativeand superlative adjectives and adverbs) have beenextensively used for sentiment analysis and opinionmining (Pang and Lee, 2008).In addition, we use a number of features based onsimple hand-crafted word lists.
These lists includeword categories that could be potential indicators ofquotable phrases such as universal quantifiers (e.g.,all, everyone) and abstract concepts7.7For abstract concept modeling we use a list of 176 abstractnouns available at www.englishbanana.com.72The novel features in Table 1 that are specificallydesigned for quotable phrase detection are based onpart of speech sequences that are highly indicativeof quotable (or, conversely, non-quotable) phrase(features #IGSeq[i]).
In order to compute thesefeatures we first manually label a validation set of500 sentences that passed the Na?
?ve Bayes Filtering(Section 2.1).
Then, we apply a POS tagger to thesesentences, and for each POS tag sequence of lengthn, seqn, we compute its information gainIG(X,Y ) = H(X) ?H(X|Y ).
(3)In Eq.
3, X is a binary variable indicating the pres-ence or the absence of seqn in the sentence, andY ?
{q, q}.We select k sequences seqn with the highest valueof IG(X,Y )8.
We use the count in the sentence ofthe sequence seqn with the i-th highest informationgain as the feature #IGSeq[i].
Intuitively, the fea-tures #IGSeq[i] measure how many POS tag se-quences that are indicative of a quotable phrase (or,conversely, indicative of a non-quotable phrase) thesentence contains.3 Related WorkThe increasing availability of large-scale digital li-braries resulted in a recent surge of interest in com-putational approaches to literary analysis.
To namejust a few examples, Genzel et al (2010) examinedmachine translation of poetry; Elson et al (2010)extracted conversational networks from Victoriannovels; and Faruqui and Pado?
(2011) predicted for-mal and informal address in English literature.In addition, computational methods are increas-ingly used for identification of complex aspectsof writing such as humor (Mihalcea and Pulman,2007), double-entendre (Kiddon and Brun, 2011)and sarcasm (Tsur et al, 2010).
However, whilesuccessful, most of this work is still limited to ananalysis of a single aspect of writing style.In this work, we propose a more general compu-tational approach that attempts to extract quotablephrases.
A quotability of a phrase can be affectedby various aspects of writing including (but not lim-8In this work, we set n = 3, k = 50.
This setting is doneprior to seeing any labeled data.Number of books 21, 492Number of authors 8, 889Total sentences 4.45 ?
107After Na?
?ve Bayes filtering 1.75 ?
107Table 2: Summary of the Project Gutenberg corpus.ited to) humor and irony9, use of metaphors10 , andhyperbole11 .It is important to note that our approach is con-ceptually different from the previous work on para-phrase and quote detection in book corpora (Kolakand Schilit, 2008), news stories (Liang et al, 2010)and movie scripts (Danescu-Niculescu-Mizil et al,2012).
While this previous work focuses on miningpopular and oft-used quotations, we are mainly in-terested in discovering quotable phrases that mighthave never been quoted by others.4 Experimental SetupTo evaluate the quotable phrase extraction processin its entirety (see Figure 1), we use a collection ofProject Gutenberg (PG) books12 ?
a popular digitallibrary containing full texts of public domain booksin a variety of formats.
In particular, we harvest theentire corpus of 21,492 English books in textual for-mat from the PG website.The breakdown of the PG corpus is shown in Ta-ble 2.
The number of detected sentences in the PGcorpus exceeds 44 million.
Roughly a third of thesesentences are able to pass through the Na?
?ve BayesFiltering (described in Section 2.1) to the supervisedquotable phrase detection stage (Section 2.2).For each of these sentences, we compute a set oflexical and syntactic features described in Section2.3.
For computing the features that require the partof speech tags, we use the MontyLingua package(Liu, 2004).9?To be born with a riotous imagination and then hardly everto let it riot is to be a born newspaper man.?
(Zona Gale, ?Ro-mance Island?
)10?If variety is the spice of life, his life in the north has beenone long diet of paprika.?
(Fullerton Waldo, ?Grenfell: Knight-Errant of the North?
)11?The idea of solitude is so repugnant to human nature, thateven death would be preferable.?
(William O.S.
Gilly, ?Nar-ratives of Shipwrecks of the Royal Navy; between 1793 and1849?
)12http://www.gutenberg.org/730.0 0.2 0.4 0.6 0.8 1.00.20.40.60.81.0Precision?Recall CurvesRecallPrecisionWord FeaturesAll FeaturesFigure 2: Prec.
vs. recall for quotable phrase detection.We find that the extraction process shown in Fig-ure 1 is efficient and scalable.
On average, the entireprocess requires less than ten seconds per book on asingle machine.The complete set of extracted quotable phrasesand annotations is available upon request from theauthors.
In addition, the readers are invited to visitwww.noisypearls.com, where a quotable phrasefrom the set is published daily.5 Evaluation and Analysis5.1 Na?
?ve Bayes Filtering EvaluationIn the Na?
?ve Bayes Filtering stage (see Section 2.1)we evaluate two criteria.
First, we measure its abil-ity to reduce the number of sentences that pass to thesupervised quotable phrase detection stage.
As Ta-ble 2 shows, the Na?
?ve Bayes Filtering reduces thenumber of these sentences by more than 60%.Second, we evaluate the recall of the Na?
?ve BayesFiltering.
We are primarily interested in its abilityto reliably detect quotable phrases and pass themthrough to the next stage, while still reducing thetotal number of sentences.For recall evaluation, we collect a set of2, 817 previously unseen quotable phrases from theGoodreads website13, and run them through theNa?
?ve Bayes Filtering stage.
2, 262 (80%) of thequotable phrases pass the filter, indicating a highquotable phrase recall.13http://www.goodreads.com/quotes1 #abstract +91.642 #quantifier +61.673 hasPP ?60.344 #IGSeq[16](VB IN PRP) +39.715 #IGSeq[6](PRP MD VB) ?38.786 #adjective +37.717 #IGSeq[14](DT NN VBD) ?36.888 #verb +35.229 beginStop +31.7310 #noun +29.63Table 3: Top quotability features.Based on these findings, we conclude that the pro-posed Na?
?ve Bayes Filtering is able to reliably detectquotable phrases, while filtering out a large numberof non-quotable ones.
It can be further calibrated toreduce the number of non-quotable sentences or toincrease the quotable phrase recall, by changing thesetting of the parameters ?
and ?, described in Sec-tion 2.1.
In the remainder of this section, we use itsoutput to analyze the performance of the supervisedquotable phrase detection stage.5.2 Quotable Phrase Detection EvaluationTo evaluate the performance of the supervisedquotable phrase detection stage (see Section 2.2) werandomly sample 1,500 sentences that passed theNa?
?ve Bayes Filtering (this sample is disjoint fromthe sample of 500 sentences used for computingthe IGTagSeq feature in Section 2.3).
We anno-tate these sentences with q (Quotable) and q (Non-Quotable) labels.Of these sentences, 381 (25%) are labeled asQuotable.
This ratio of quotable phrases is muchhigher than what is expected from a non-filtered con-tent of a book, which provides an indication that theNa?
?ve Bayes Filtering provides a relatively balancedinput to the supervised detection stage.We use this random sample of 1,500 labeled sen-tences to train a perceptron algorithm (as describedin Section 2.2) using 10-fold cross-validation.
Wetrain two variants of the perceptron.
The first variantis trained using only the lexical features in Table 1,while the second variant uses all the features.Figure 2 compares the precision-recall curves ofthese two variants.
It demonstrates that using thesyntactic features based on punctuation and part ofspeech tags significantly improves the precision of74Popular ??
10 12Upvoted 1 ???
10 34No upvotes ??
0 14p(?> 0) = .77Table 4: Distribution of reddit upvote scores.quote phrase detection at all recall levels.
For in-stance at the 0.4 recall level, it can improve precisionby almost 25%.Figure 2 also shows that the proposed methodis reliable for high-precision quotable phrase de-tection.
This is especially important for applica-tions where recall is given less consideration, suchas book preview using quotable phrases.
The pro-posed method reaches a precision of 0.7 at the 0.1recall level.It is also interesting to examine the importance ofdifferent features for the quotable phrase detection.Table 3 shows the ten highest-weighted features, aslearned by the perceptron algorithm on the entire setof 1,500 labeled examples.The part of speech features #IGTagSeq[i] oc-cupy three of the positions in the Table 3.
It is inter-esting to note that two of them have a high negativeweight.
In other words, some of the POS sequencesthat have the highest information gain (see Eq.
3)are sequences that are indicative of non-quotablephrases, rather than quotable phrases.The two highest-weighted features are basedon handcrafted word lists (#abstract and#quantifier, respectively).
This demonstratesthe importance of task-specific features such asthese for quotability detection.Finally, the presence of different parts of speechin the phrase (nouns, verbs and adjectives), as wellas their verb tenses, are important features.
Forinstance, the presence of a verb in past participle(hasPP) is a strong negative indicator of phrasequotability.5.3 The reddit StudyAs mentioned in Section 2.3, the degree of thephrase quotability is often subjective, and thereforeits estimation may vary among individuals.
To val-idate that our quotability detection method is notbiased by our training data, and that the detectedquotes will have a universal appeal, we set up a veri-fication study that leverages an online community ofquote enthusiasts.For our study, we use reddit, a social content web-site where the registered users submit content, in theform of either a link or a text post.
Other regis-tered users then upvote or downvote the submission,which is used to rank the post.Specifically, we use the Quotes subreddit14, an ac-tive reddit community devoted to discovering andsharing quotable phrases.
At the time of this writ-ing, the Quotes subreddit has more than 12,000 sub-scribers.
A typical post to this subreddit contains asingle quotable phrase with attribution.
Any reddituser can then upvote or downvote the quote based onits perceived merit.To validate the quality of the quotes which wereused for training the perceptron algorithm, we sub-mitted 60 quotes, which were labeled as quotable byone of the authors, to the Quotes subreddit.
At mostone quote per day was submitted, to avoid negativefeedback from the community for ?spamming?.Table 4 presents the upvote scores of the submit-ted quotes.
An upvote score, denoted ?, is computedas?= # upvotes ?
# downvotes.Table 4 validates that the majority of the quotes la-beled as quotable, were also endorsed by the red-dit community, and received a non-negative upvotescore.
As an illustration, in Table 5, we present fivequotes with the highest upvote scores.
Anecdotally,at the time of this writing, only one of the quotesin Table 5 (a quote by Mark Twain) appeared inweb search results in contexts other than the origi-nal book.5.4 Project Gutenberg Corpus AnalysisIn this section, we briefly highlight an interesting ex-ample of how the proposed computational approachto quotable phrase extraction can be used for a liter-ary analysis of the PG digital library.
To this end,we train the supervised quotable phrase detectionmethod using the entire set of 1,500 manually la-beled sentences.
We then run this model over all the17.5 million sentences that passed the Na?
?ve Bayesfiltering stage, and retain only the sentences that getpositive perceptron scores (Eq.
2).14http://www.reddit.com/r/quotes75Quote ?
?One hour of deep agony teaches man more love and wisdom than a whole long life of happiness.?
49(Walter Elliott, ?Life of Father Hecker?
)?As long as I am on this little planet I expect to love a lot of people and I hope they will love me in return.?
43(Kate Langley, Bosher, ?Kitty Canary?
)?None of us could live with an habitual truth-teller; but thank goodness none of us has to.?
40(Mark Twain, ?On the Decay of the Art of Lying?
)?A caged bird simply beats its wings and dies, but a human being does not die of loneliness, even when he prays for death.?
33(George Moore, ?The Lake?
)?Many will fight as long as there is hope, but few will go down to certain death.?
30(G. A. Henty, ?For the Temple?
)Table 5: Five quotes with the highest upvote scores on reddit.
(a) Authors (b) Books1 Henry Drummond .0452 Ella Wheeler Wilcox .0413 S. D. Gordon .0404 Andrew Murray .0385 Ralph Waldo Emerson .0376 Orison Swett Marden .0347 Mary Baker Eddy .0318 ?Abdu?l-Baha?
.0299 John Hartley .02910 Rabindranath Tagore .0281 ?Friendship?
(Hugh Black) .1132 ?The Dhammapada?
(Translated by F. Max Muller ) .1123 ?The Philosophy of Despair?
(David Starr Jordan) .1064 ?Unity of Good?
(Mary Baker Eddy) .0975 ?Laments?
(Jan Kochanowski) .0846 ?Joy and Power?
(Henry van Dyke) .0797 ?Polyeucte?
(Pierre Corneille) .0788 ?The Forgotten Threshold?
(Arthur Middleton) .0789 ?The Silence?
(David V. Bush) .07710 ?Levels of Living?
(Henry Frederick Cope) .075Table 6: Project Gutenberg (a) authors and (b) books with the highest quotability index.This procedure yields 701,418 sentences, whichwe call quotable phrases in the remainder of thissection.
These quotable phrases are less than 2% ofthe entire Project Gutenberg corpus; however, theystill constitute a sizable collection with some poten-tially interesting properties.We propose a simple example of a literary anal-ysis that can be done using this set of quotablephrases.
We detect books and authors that have ahigh quotability index, which is formally defined asQI(x) = # quotable phrases(x)# total sentences(x) ,where x is either a book or an author.
To ensure thestatistical validity of our analysis, we limit our atten-tion to books with at least 25 quotable phrases andauthors with at least 5 books in the PG collection.Using this definition, we can easily compile a listof authors and books with the highest quotability in-dex (see Table 6).
An interesting observation is thatmany of the authors and books in Table 6 deal withreligious themes: Christianity (e.g., Mary BakerEddy, S. D. Gordon), Baha???
?sm (?Abdu?l-Baha?)
andBuddhism (?The Dhammapada?).
This is not sur-prising considering the figurative language commonin the religious prose15.15?If a man speaks or acts with an evil thought, pain follows6 ConclusionsAs the number of digitized books increases, a com-putational analysis of literary corpora becomes anactive research field with many practical applica-tions.
In this paper, we focus on one such appli-cation: extraction of quotable phrases from books.Quotable phrase extraction can be used, amongother things, for finding new original quotationsfor dictionaries and online quotation repositories, aswell as for generating catchy previews for book ad-vertisement.We develop a quotable phrase extraction processthat includes sentence segmentation, unsupervisedsentence filtering based on a quotable languagemodel, and a supervised quotable phrase detectionusing lexical and syntactic features.
Our evaluationdemonstrates that this process can be used for high-precision quotable phrase extraction, especially inapplications that can tolerate lower recall.
A studyusing a reddit community of quote enthusiasts aswell as a simple corpus analysis further demonstratethe practical applications of our work.him, as the wheel follows the foot of the ox that draws the car-riage.?
(?The Dhammapada?, translated by F. Max Muller )767 AcknowledgmentsThis work was supported in part by the Center forIntelligent Information Retrieval, in part by NSFgrant IIS-0910884 and in part by ARRA NSF IIS-9014442.
Any opinions, findings and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthose of the sponsor.ReferencesChristopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Springer.Ann M. Blair.
2010.
Too Much to Know: ManagingScholarly Information before the Modern Age.
YaleUniversity Press.Cristian Danescu-Niculescu-Mizil, Justin Cheng, JonKleinberg, and Lillian Lee.
2012.
You had me athello: How phrasing affects memorability.
In Proc.of ACL, page To appear.David K. Elson, Nicholas Dames, and Kathleen R. McK-eown.
2010.
Extracting social networks from literaryfiction.
In Proc.
of ACL, pages 138?147.Manaal Faruqui and Sebastian Pado?.
2011.
?I thou thee,thou traitor?
: predicting formal vs. informal address inEnglish literature.
In Proceedings of ACL-HLT, pages467?472.Dmitriy Genzel, Jakob Uszkoreit, and Franz Och.
2010.?Poetic?
Statistical Machine Translation: Rhyme andMeter.
In Proc.
of EMNLP, pages 158?166.Tapas Kanungo and David Orr.
2009.
Predicting thereadability of short web summaries.
In Proc.
ofWSDM, pages 202?211.Chloe Kiddon and Yuriy Brun.
2011.
That?s What SheSaid: Double Entendre Identification.
In Proc.
ofACL-HLT, pages 89?94.T.
Kiss and J. Strunk.
2006.
Unsupervised multilingualsentence boundary detection.
Computational Linguis-tics, 32(4):485?525.Okan Kolak and Bill N. Schilit.
2008.
Generating linksby mining quotations.
In Proc.
of 19th ACM confer-ence on Hypertext and Hypermedia, pages 117?126.Jisheng Liang, Navdeep Dhillon, and Krzysztof Koper-ski.
2010.
A large-scale system for annotating andquerying quotations in news feeds.
In Proc.
of Sem-Search.Hugo Liu.
2004.
Montylingua: An end-to-end naturallanguage processor with common sense.
Available at:web.media.mit.edu/?hugo/montylingua.Jean-Baptiste Michel, Yuan Kui Shen, Aviva PresserAiden, Adrian Veres, Matthew K. Gray, TheGoogle Books Team, Joseph P. Pickett, Dale Hoiberg,Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker,Martin A. Nowak, and Erez Lieberman Aiden.
2011.Quantitative analysis of culture using millions of digi-tized books.
Science, 331(6014):176?182.Rada Mihalcea and Stephen Pulman.
2007.
Character-izing humour: An exploration of features in humoroustexts.
In Proc.
of CICLing, pages 337?347.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.Luo Si and Jamie Callan.
2001.
A statistical model forscientific readability.
In Proc.
of CIKM, pages 574?576.Leonid Taycher.
2010.
Books of the world, stand up andbe counted!
All 129,864,880 of you.
Inside GoogleBooks blog.Oren Tsur, Dimitry Davidov, and Avi Rappoport.
2010.ICWSM?A great catchy name: Semi-supervisedrecognition of sarcastic sentences in online product re-views.
In Proc.
of ICWSM, pages 162?169.77
