Proceedings of the ACL 2010 Student Research Workshop, pages 19?24,Uppsala, Sweden, 13 July 2010.c?2010 Association for Computational LinguisticsA probabilistic generative model for an intermediateconstituency-dependency representationFederico SangatiInstitute for Logic, Language and ComputationUniversity of Amsterdam, the Netherlandsf.sangati@uva.nlAbstractWe present a probabilistic model exten-sion to the Tesni`ere Dependency Structure(TDS) framework formulated in (Sangatiand Mazza, 2009).
This representation in-corporates aspects from both constituencyand dependency theory.
In addition, itmakes use of junction structures to handlecoordination constructions.
We test ourmodel on parsing the English Penn WSJtreebank using a re-ranking framework.This technique allows us to efficiently testour model without needing a specializedparser, and to use the standard evalua-tion metric on the original Phrase Struc-ture version of the treebank.
We obtainencouraging results: we achieve a smallimprovement over state-of-the-art resultswhen re-ranking a small number of candi-date structures, on all the evaluation met-rics except for chunking.1 IntroductionSince its origin, computational linguistics hasbeen dominated by Constituency/Phrase Structure(PS) representation of sentence structure.
How-ever, recently, we observe a steady increase inpopularity of Dependency Structure (DS) for-malisms.
Several researchers have comparedthe two alternatives, in terms of linguistic ade-quacy (Nivre, 2005; Schneider, 2008), practicalapplications (Ding and Palmer, 2005), and eval-uations (Lin, 1995).Dependency theory is historically accredited toLucien Tesni`ere (1959), although the relation ofdependency between words was only one of thevarious key elements proposed to represent sen-tence structures.
In fact, the original formulationincorporates the notion of chunk, as well as a spe-cial type of structure to represent coordination.The Tesni`ere Dependency Structure (TDS) rep-resentation we propose in (Sangati and Mazza,2009), is an attempt to formalize the original workof Tesni`ere, with the intention to develop a simplebut consistent representation which combines con-stituencies and dependencies.
As part of this work,we have implemented an automatic conversion1ofthe English Penn Wall Street Journal (WSJ) tree-bank into the new annotation scheme.In the current work, after introducing the keyelements of TDS (section 2), we describe a firstprobabilistic extension to this framework, whichaims at modeling the different levels of the repre-sentation (section 3).
We test our model on parsingthe WSJ treebank using a re-ranking framework.This technique allows us to efficiently test our sys-tem without needing a specialized parser, and touse the standard evaluation metric on the originalPS version of the treebank.
In section 3.4 we alsointroduce new evaluation schemes on specific as-pects of the new TDS representation which we willinclude in the results presented in section 3.4.2 TDS representationIt is beyond the scope of this paper to provide anexhaustive description of the TDS representationof the WSJ.
It is nevertheless important to give thereader a brief summary of its key elements, andcompare it with some of the other representationsof the WSJ which have been proposed.
Figure 1shows the original PS of a WSJ tree (a), togetherwith 3 other representations: (b) TDS, (c) DS2,and (d) CCG (Hockenmaier and Steedman, 2007).1staff.science.uva.nl/?fsangati/TDS2The DS representation is taken from the conversion pro-cedure used in the CoNLL 2007 Shared Task on dependencyparsing (Nivre et al, 2007).
Although more elaborate rep-resentation have been proposed (de Marneffe and Manning,2008; Cinkov?a et al, 2009) we have chosen this DS repre-sentation because it is one of the most commonly used withinthe CL community, given that it relies on a fully automaticconversion procedure.19(a) (b)NPNPNNSactivitiesSBARWHNPWDTthatSVPVBPencourage,,VBPpromoteCCorVBPadvocateNPNNabortionactivitiesthat encourage , promote or advocateabortionNN JVJVJVJVNN(c) (d)NNSactivitiesWDTthat VBPencourage ,, VBPpromoteCCorVBPadvocate NNabortionS[dcl]S[dcl]NPNP[nb]/NThe NruleS[dcl]\NP(S\NP)/(S\NP)also S[dcl]\NP(S[dcl]\NP)/NPprohibits NPNPNfundingNP\NP(NP\NP)/NPfor NPNPNactivitiesNP\NP(NP\NP)/(S[dcl]\NP)that S[dcl]\NP(S[dcl]\NP)/NP(S[dcl]\NP)/NPencourage (S[dcl]\NP)/NP[conj],,(S[dcl]\NP)/NP(S[dcl]\NP)/NPpromote (S[dcl]\NP)/NP[conj]conjor (S[dcl]\NP)/NPadvocateNPNabortion..Figure 1: Four different structure representations, derived from a sentence of the WSJ treebank (section00, #977).
(a) PS (original), (b) CCG, (c) DS, (d) TDS.Words and Blocks In TDS, words are di-vided in functional words (determiners, preposi-tions, etc.)
and content words (verbs, nouns, etc.
).Blocks are the basic elements (chunks) of a struc-ture, which can be combined either via the depen-dency relation or the junction operation.
Blockscan be of two types: standard and junction blocks.Both types may contain any sequence of func-tional words.
Standard blocks (depicted as blackboxes) represent the elementary chunks of theoriginal PS, and include exactly one content word.Coordination Junction blocks (depicted as yel-low boxes) are used to represent coordinated struc-tures.
They contain two or more blocks (con-juncts) possibly coordinated by means of func-tional words (conjunctions).
In Figure 1(d) theyellow junction block contains three separate stan-dard blocks.
This representation allows to cap-ture the fact that these conjuncts occupy the samerole: they all share the relativizer ?that?, they alldepend on the noun ?activities?, and they all gov-ern the noun ?abortion?.
In Figure 1(a,c), we cannotice that both PS and DS do not adequately rep-resent coordination structures: the PS annotationis rather flat, avoiding to group the three verbs in aunique unit, while in the DS the last noun ?abor-tion?
is at the same level of the verbs it shouldbe a dependent of.
On the other hand, the CCGstructure of Figure 1(d), properly represents thecoordination.
It does so by grouping the first threeverbs in a unique constituent which is in turn bi-narized in a right-branching structure.
One of thestrongest advantages of the CCG formalism, isthat every structure can be automatically mappedto a logical-form representation.
This is one rea-son why it needs to handle coordinations properly.Nevertheless, we conjecture that this representa-tion of coordination might introduce some diffi-culties for parsing: it is very hard to capture therelation between ?advocate?
and ?abortion?
sincethey are several levels away in the structure.Categories and Transference There are 4 dif-ferent block categories, which are indicated withlittle colored bricks (as well as one-letter abbrevi-ation) on top and at the bottom of the correspond-ing blocks: verbs (red, V), nouns (blue, N), ad-verbs (yellow, A), and adjectives (green, J).
Everyblock displays at the bottom the original categorydetermined by the content word (or the originalcategory of the conjuncts if it is a junction struc-ture), and at the top, the derived category whichrelates to the grammatical role of the whole blockin relation to the governing block.
In several caseswe can observe a shift in the categories of a block,from the original to the derived category.
Thisphenomenon is called transference and often oc-curs by means of functional words in the block.
InFigure 1(b) we can observe the transference of thejunction block, which has the original category ofa verb, but takes the role of an adjective (throughthe relativizer ?that?)
in modifying the noun ?activ-ities?.20P (S) = PBGM(S) ?
PBEM(S) ?
PWFM(S) (1)PBGM(S) =?B ?
dependentBlocks(S)P (B|parent(B), direction(B), leftSibling(B)) (2)PBEM(S) =?B ?
blocks(S)P (elements(B)|derivedCat(B)) (3)PWFM(S) =?B ?
standardBlocks(S)P (cw(B)|cw(parent(B)), cats(B), fw(B), context(B)) (4)Table 1: Equation (1) gives the likelihood of a structure S as the product of the likelihoods of generatingthree aspects of the structure, according to the three models (BGM, BEM, WFM) specified in equations(2-4) and explained in the main text.3 A probabilistic Model for TDSThis section describes the probabilistic generativemodel which was implemented in order to dis-ambiguate TDS structures.
We have chosen thesame strategy we have described in (Sangati et al,2009).
The idea consists of utilizing a state of theart parser to compute a list of k-best candidates ofa test sentence, and evaluate the new model by us-ing it as a reranker.
How well does it select themost probable structure among the given candi-dates?
Since no parser currently exists for the TDSrepresentation, we utilize a state of the art parserfor PS trees (Charniak, 1999), and transform eachcandidate to TDS.
This strategy can be considereda first step to efficiently test and compare differentmodels before implementing a full-fledged parser.3.1 Model descriptionIn order to compute the probability of a given TDSstructure, we make use of three separate proba-bilistic generative models, each responsible for aspecific aspect of the structure being generated.The probability of a TDS structure is obtained bymultiplying its probabilities in the three models, asreported in the first equation of Table 2.The first model (equation 2) is the Block Gen-eration Model (BGM).
It describes the event ofgenerating a block B as a dependent of its parentblock (governor).
The dependent block B is identi-fied with its categories (both original and derived),and its functional words, while the parent block ischaracterized by the original category only.
More-over, in the conditioning context we specify thedirection of the dependent with respect to the par-ent3, and its adjacent left sister (null if not present)specified with the same level of details of B. Themodel applies only to dependent blocks4.The second model (equation 3) is the Block Ex-pansion Model (BEM).
It computes the probabil-ity of a generic block B of known derived cate-gory, to expand to the list of elements it is com-posed of.
The list includes the category of thecontent word, in case the expansion leads to astandard block.
In case of a junction structure, itcontains the conjunctions and the conjunct blocks(each identified with its categories and its func-tional words) in the order they appear.
Moreover,all functional words in the block are added to thelist5.
The model applies to all blocks.The third model (equation 4) is the Word Fill-ing Model (WFM), which applies to each stan-dard block B of the structure.
It models the eventof filling B with a content word (cw), given thecontent word of the governing block, the cate-gories (cats) and functional words (fw) of B, andfurther information about the context6in which Boccurs.
This model becomes particularly interest-3A dependent block can have three different positionswith respect to the parent block: left, right, inner.
The firsttwo are self-explanatory.
The inner case occurs when the de-pendent block starts after the beginning of the parent blockbut ends before it (e.g.
a nice dog).4A block is a dependent block if it is not a conjunct.
Inother words, it must be connected with a line to its governor.5The attentive reader might notice that the functionalwords are generated twice (in BGM and BEM).
This deci-sion, although not fully justified from a statistical viewpoint,seems to drive the model towards a better disambiguation.6context(B) comprises information about the grandpar-ent block (original category), the adjacent left sibling block(derived category), the direction of the content word with re-spect to its governor (in this case only left and right), and theabsolute distance between the two words.21ing when a standard block is a dependent of a junc-tion block (such as ?abortion?
in Figure 1(d)).
Inthis case, the model needs to capture the depen-dency relation between the content word of thedependent block and each of the content words be-longing to the junction block7.3.2 SmoothingIn all the three models we have adopted a smooth-ing techniques based on back-off level estima-tion as proposed by Collins (1999).
The differentback-off estimates, which are listed in decreasinglevels of details, are interpolated with confidenceweights8derived from the training corpus.The first two models are implemented with twolevels of back-off, in which the last is a constantvalue (10?6) to make the overall probability smallbut not zero, for unknown events.The third model is implemented with three lev-els of back-off: the last is set to the same con-stant value (10?6), the first encodes the depen-dency event using both pos-tags and lexical infor-mation of the governor and the dependent word,while the second specifies only pos-tags.3.3 Experiment SetupWe have tested our model on the WSJ section ofPenn Treebank (Marcus et al, 1993), using sec-tions 02-21 as training and section 22 for testing.We employ the Max-Ent parser, implemented byCharniak (1999), to generate a list of k-best PScandidates for the test sentences, which are thenconverted into TDS representation.Instead of using Charniak?s parser in its origi-nal settings, we train it on a version of the corpusin which we add a special suffix to constituentswhich have circumstantial role9.
This decision isbased on the observation that the TDS formalismwell captures the argument structure of verbs, and7In order to derive the probability of this multi-event wecompute the average between the probabilities of the singleevents which compose it.8Each back-off level obtains a confidence weight whichdecreases with the increase of the diversity of the context(?
(Ci)), which is the number of separate events occurringwith the same context (Ci).
More formally if f(Ci) is thefrequency of the conditioning context of the current event,the weight is obtained as f(Ci)/(f(Ci) ?
?
?
?
(Ci)); seealso (Bikel, 2004).
In our model we have chosen ?
to be5 for the first model, and 50 for the second and the third.9Those which have certain function tags (e.g.
ADV, LOC,TMP).
The full list is reported in (Sangati and Mazza, 2009).It was surprising to notice that the performance of this slightlymodified parser (in terms of F-score) is only slightly lowerthan how it performs out-of-the-box (0.13%).we believe that this additional information mightbenefit our model.We then applied our probabilistic model to re-rank the list of available k-best TDS, and evalu-ate the selected candidates using several metricswhich will be introduced next.3.4 Evaluation Metrics for TDSThe re-ranking framework described above, al-lows us to keep track of the original PS of eachTDS candidate.
This provides an implicit advan-tage for evaluating our system, viz.
it allows us toevaluate the re-ranked structures both in terms ofthe standard evaluation benchmark on the originalPS (F-score) as well as on more refined metricsderived from the converted TDS representation.In addition, the specific head assignment that theTDS conversion procedure performs on the origi-nal PS, allows us to convert every PS candidate toa standard projective DS, and from this represen-tation we can in turn compute the standard bench-mark evaluation for DS, i.e.
unlabeled attachmentscore10(UAS) (Lin, 1995; Nivre et al, 2007).Concerning the TDS representation, we haveformulated 3 evaluation metrics which reflect theaccuracy of the chosen structure with respect to thegold structure (the one derived from the manuallyannotated PS), regarding the different componentsof the representation:Block Detection Score (BDS): the accuracy of de-tecting the correct boundaries of the blocks in thestructure11.Block Attachment Score (BAS): the accuracyof detecting the correct governing block of eachblock in the structure12.Junction Detection Score (JDS): the accuracy ofdetecting the correct list of content-words com-posing each junction block in the structure13.10UAS measures the percentage of words (excluding punc-tuation) having the correct governing word.11It is calculated as the harmonic mean between recall andprecision between the test and gold set of blocks, where eachblock is identified with two numerical values representing thestart and the end position (punctuation words are discarded).12It is computed as the percentage of words (both func-tional and content words, excluding punctuation) having thecorrect governing block.
The governing block of a word, isdefined as the governor of the block it belongs to.
If the blockis a conjunct, its governing block is computed recursively asthe governing block of the junction block it belongs to.13It is calculated as the harmonic mean between recall andprecision between the test and gold set of junction blocks ex-pansions, where each expansion is identified with the list ofcontent words belonging to the junction block.
A recursivejunction structure expands to a list of lists of content-words.22F-Score UAS BDS BAS JDSCharniak (k = 1) 89.41 92.24 94.82 89.29 75.82Oracle Best F-Score (k = 1000) 97.47 96.98 97.03 95.79 82.26Oracle Worst F-Score (k = 1000) 57.04 77.04 84.71 70.10 43.01Oracle Best JDS (k = 1000) 90.54 93.77 96.20 90.57 93.55PCFG-reranker (k = 5) 89.03 92.12 94.86 88.94 75.88PCFG-reranker (k = 1000) 83.52 87.04 92.07 82.32 69.17TDS-reranker (k = 5) 89.65 92.33 94.77 89.35 76.23TDS-reranker (k = 10) 89.10 92.11 94.58 88.94 75.47TDS-reranker (k = 100) 86.64 90.24 93.11 86.34 69.60TDS-reranker (k = 500) 84.94 88.62 91.97 84.43 65.30TDS-reranker (k = 1000) 84.31 87.89 91.42 83.69 63.65Table 2: Results of Charniak?s parser, the TDS-reranker, and the PCFG-reranker according to severalevaluation metrics, when the number k of best-candidates increases.Figure 2: Left: results of the TDS-reranking model according to several evaluation metrics as in Table 2.Right: comparison between the F-scores of the TDS-reranker and a vanilla PCFG-reranker (togetherwith the lower and the upper bound), with the increase of the number of best candidates.3.5 ResultsTable 2 reports the results we obtain when re-ranking with our model an increasing number ofk-best candidates provided by Charniak?s parser(the same results are shown in the left graph ofFigure 2).
We also report the results relative to aPCFG-reranker obtained by computing the prob-ability of the k-best candidates using a standardvanilla-PCFG model derived from the same train-ing corpus.
Moreover, we evaluate, by means of anoracle, the upper and lower bound of the F-Scoreand JDS metric, by selecting the structures whichmaximizes/minimizes the results.Our re-ranking model performs rather well fora limited number of candidate structures, and out-performs Charniak?s model when k = 5.
In thiscase we observe a small boost in performance forthe detection of junction structures, as well as forall other evaluation metrics, except for the BDS.The right graph in Figure 2 compares the F-score performance of the TDS-reranker against thePCFG-reranker.
Our system consistently outper-forms the PCFG model on this metric, as for UAS,and BAS.
Concerning the other metrics, as thenumber of k-best candidates increases, the PCFGmodel outperforms the TDS-reranker both accord-ing to the BDS and the JDS.Unfortunately, the performance of the re-ranking model worsens progressively with the in-crease of k. We find that this is primarily due tothe lack of robustness of the model in detecting theblock boundaries.
This suggests that the systemmight benefit from a separate preprocessing stepwhich could chunk the input sentence with higheraccuracy (Sang et al, 2000).
In addition the samemodule could detect local (intra-clausal) coordina-tions, as illustrated by (Marin?ci?c et al, 2009).234 ConclusionsIn this paper, we have presented a probabilisticgenerative model for parsing TDS syntactic rep-resentation of English sentences.
We have givenevidence for the usefulness of this formalism: weconsider it a valid alternative to commonly usedPS and DS representations, since it incorporatesthe most relevant features of both notations; in ad-dition, it makes use of junction structures to repre-sent coordination, a linguistic phenomena highlyabundant in natural language production, but of-ten neglected when it comes to evaluating parsingresources.
We have therefore proposed a specialevaluation metrics for junction detection, with thehope that other researchers might benefit from itin the future.
Remarkably, Charniak?s parser per-forms extremely well in all the evaluation metricsbesides the one related to coordination.Our parsing results are encouraging: the over-all system, although only when the candidates arehighly reliable, can improve on Charniak?s parseron all the evaluation metrics with the exception ofchunking score (BDS).
The weakness on perform-ing chunking is the major factor responsible forthe lack of robustness of our system.
We are con-sidering to use a dedicated pre-processing moduleto perform this step with higher accuracy.Acknowledgments The author gratefully ac-knowledge funding by the Netherlands Organiza-tion for Scientific Research (NWO): this work isfunded through a Vici-grant ?Integrating Cogni-tion?
(277.70.006) to Rens Bod.
We also thank3 anonymous reviewers for very useful comments.ReferencesDaniel M. Bikel.
2004.
Intricacies of Collins?
ParsingModel.
Comput.
Linguist., 30(4):479?511.Eugene Charniak.
1999.
A Maximum-Entropy-Inspired Parser.
Technical report, Providence, RI,USA.Silvie Cinkov?a, Josef Toman, Jan Haji?c, Krist?yna?Cerm?akov?a, V?aclav Klime?s, Lucie Mladov?a,Jana?Sindlerov?a, Krist?yna Tom?s?u, and Zden?ek?Zabokrtsk?y.
2009.
Tectogrammatical Annotationof the Wall Street Journal.
The Prague Bulletin ofMathematical Linguistics, (92).Michael J. Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D. the-sis, University of Pennsylvania.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford Typed DependenciesRepresentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, pages 1?8, Manchester, UK.Yuan Ding and Martha Palmer.
2005.
Machine Trans-lation Using Probabilistic Synchronous DependencyInsertion Grammars.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL?05), pages 541?548.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.Computational Linguistics, 33(3):355?396.Dekang Lin.
1995.
A Dependency-based Method forEvaluating Broad-Coverage Parsers.
In In Proceed-ings of IJCAI-95, pages 1420?1425.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a Large Anno-tated Corpus of English: The Penn Treebank.
Com-putational Linguistics, 19(2):313?330.Domen Marin?ci?c, Matja?z Gams, and Toma?z?Sef.
2009.Intraclausal Coordination and Clause Detection as aPreprocessing Step to Dependency Parsing.
In TSD?09: Proceedings of the 12th International Confer-ence on Text, Speech and Dialogue, pages 147?153,Berlin, Heidelberg.
Springer-Verlag.Joakim Nivre, Johan Hall, Sandra K?ubler, Ryan Mc-donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 Shared Task onDependency Parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages915?932, Prague, Czech Republic.Joakim Nivre.
2005.
Dependency Grammar and De-pendency Parsing.
Technical report, V?axj?o Univer-sity: School of Mathematics and Systems Engineer-ing.Erik F. Tjong Kim Sang, Sabine Buchholz, and KimSang.
2000.
Introduction to the CoNLL-2000Shared Task: Chunking.
In Proceedings of CoNLL-2000 and LLL-2000, Lisbon, Portugal.Federico Sangati and Chiara Mazza.
2009.
An EnglishDependency Treebank `a la Tesni`ere.
In The 8th In-ternational Workshop on Treebanks and LinguisticTheories, pages 173?184, Milan, Italy.Federico Sangati, Willem Zuidema, and Rens Bod.2009.
A generative re-ranking model for depen-dency parsing.
In Proceedings of the 11th In-ternational Conference on Parsing Technologies(IWPT?09), pages 238?241, Paris, France, October.Gerold Schneider.
2008.
Hybrid long-distance func-tional dependency parsing.
Ph.D. thesis, Universityof Zurich.Lucien Tesni`ere.
1959.
El?ements de syntaxe struc-turale.
Editions Klincksieck, Paris.24
