Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 65?72,Trento, Italy, April 2006. c?2006 Association for Computational LinguisticsAutomatic Identification of English Verb Particle Constructionsusing Linguistic FeaturesSu Nam Kim and Timothy BaldwinDepartment of Computer Science and Software EngineeringUniversity of Melbourne, Victoria 3010 Australia{snkim,tim}@csse.unimelb.edu.auAbstractThis paper presents a method for identify-ing token instances of verb particle con-structions (VPCs) automatically, based onthe output of the RASP parser.
The pro-posed method pools together instances ofVPCs and verb-PPs from the parser out-put and uses the sentential context of eachsuch instance to differentiate VPCs fromverb-PPs.
We show our technique to per-form at an F-score of 97.4% at identifyingVPCs in Wall Street Journal and BrownCorpus data taken from the Penn Tree-bank.1 IntroductionMultiword expressions (hereafter MWEs) arelexical items that can be decomposed into multi-ple simplex words and display lexical, syntacticand/or semantic idiosyncracies (Sag et al, 2002;Calzolari et al, 2002).
In the case of English,MWEs are conventionally categorised syntactico-semantically into classes such as compound nom-inals (e.g.
New York, apple juice, GM car), verbparticle constructions (e.g.
hand in, battle on),non-decomposable idioms (e.g.
a piece of cake,kick the bucket) and light-verb constructions (e.g.make a mistake).
MWE research has focussedlargely on their implications in language under-standing, fluency and robustness (Pearce, 2001;Sag et al, 2002; Copestake and Lascarides, 1997;Bannard et al, 2003; McCarthy et al, 2003; Wid-dows and Dorow, 2005).
In this paper, our goalis to identify individual token instances of En-glish verb particle constructions (VPCs hereafter)in running text.For the purposes of this paper, we follow Bald-win (2005) in adopting the simplifying assump-tion that VPCs: (a) consist of a head verb and aunique prepositional particle (e.g.
hand in, walkoff); and (b) are either transitive (e.g.
hand in, puton) or intransitive (e.g.
battle on).
A defining char-acteristic of transitive VPCs is that they can gen-erally occur with either joined (e.g.
He put on thesweater) or split (e.g.
He put the sweater on) wordorder.
In the case that the object is pronominal,however, the VPC must occur in split word order(c.f.
*He handed in it) (Huddleston and Pullum,2002; Villavicencio, 2003).The semantics of the VPC can either derivetransparently from the semantics of the head verband particle (e.g.
walk off ) or be significantly re-moved from the semantics of the head verb and/orparticle (e.g.
look up); analogously, the selectionalpreferences of VPCs can mirror those of their headverbs or alternatively diverge markedly.
The syn-tax of the VPC can also coincide with that of thehead verb (e.g.
walk off ) or alternatively diverge(e.g.
lift off ).In the following, we review relevant pastresearch on VPCs, focusing on the extrac-tion/identification of VPCs and the prediction ofthe compositionality/productivity of VPCs.There is a modest body of research on the iden-tification and extraction of VPCs.
Note that inthe case of VPC identification we seek to detectindividual VPC token instances in corpus data,whereas in the case of VPC extraction we seekto arrive at an inventory of VPC types/lexicalitems based on analysis of token instances in cor-pus data.
Li et al (2003) identify English VPCs(or ?phrasal verbs?
in their parlance) using hand-coded regular expressions.
Baldwin and Villavi-cencio (2002) extract a simple list of VPCs fromcorpus data, while Baldwin (2005) extracts VPCswith valence information under the umbrella ofdeep lexical acquisition.1 The method of Baldwin(2005) is aimed at VPC extraction and takes intoaccount only the syntactic features of verbs.
In thispaper, our interest is in VPC identification, and wemake use of deeper semantic information.In Fraser (1976) and Villavicencio (2006) it isargued that the semantic properties of verbs candetermine the likelihood of their occurrence with1The learning of lexical items in a form that can be feddirectly into a deep grammar or other richly-annotated lexicalresource65particles.
Bannard et al (2003) and McCarthy etal.
(2003) investigate methods for estimating thecompositionality of VPCs based largely on dis-tributional similarity of the head verb and VPC.O?Hara and Wiebe (2003) propose a method fordisambiguating the verb sense of verb-PPs.
Whileour interest is in VPC identification?a fundamen-tally syntactic task?we draw on the shallow se-mantic processing employed in these methods inmodelling the semantics of VPCs relative to theirbase verbs.The contribution of this paper is to combinesyntactic and semantic features in the task of VPCidentification.
The basic intuition behind the pro-posed method is that the selectional preferences ofVPCs over predefined argument positions,2 shouldprovide insight into whether a verb and preposi-tion in a given sentential context combine to forma VPC (e.g.
Kim handed in the paper) or alter-natively constitute a verb-PP (e.g.
Kim walked inthe room).
That is, we seek to identify individualpreposition token instances as intransitive preposi-tions (i.e.
prepositional particles) or transitive par-ticles based on analysis of the governing verb.The remainder of the paper is structured as fol-lows.
Section 2 outlines the linguistic features ofverbs and their co-occuring nouns.
Section 3 pro-vides a detailed description of our technique.
Sec-tion 4 describes the data properties and the identi-fication method.
Section 5 contains detailed evalu-ation of the proposed method.
Section 6 discussesthe effectiveness of our approach.
Finally, Sec-tion 7 summarizes the paper and outlines futurework.2 Linguistic FeaturesWhen verbs co-occur with particles to form VPCs,their meaning can be significantly different fromthe semantics of the head verb in isolation.
Ac-cording to Baldwin et al (2003), divergences inVPC and head verb semantics are often reflectedin differing selectional preferences, as manifestedin patterns of noun co-occurrence.
In one examplecited in the paper, the cosine similarity betweencut and cut out, based on word co-occurrence vec-tors, was found to be greater than that between cutand cut off, mirroring the intuitive compositional-ity of these VPCs.
(1) and (2) illustrate the difference in the selec-tional preferences of the verb put in isolation ascompared with the VPC put on.32Focusing exclusively on the subject and object argumentpositions.3All sense definitions are derived from WordNet 2.1.
(1) put = placeEX: Put the book on the table.ARGS: bookOBJ = book, publication, objectANALYSIS: verb-PP(2) put on = wearEX: Put on the sweater .ARGS: sweaterOBJ = garment, clothingANALYSIS: verb particle constructionWhile put on is generally used in the context ofwearing something, it usually occurs with clothing-type nouns such as sweater and coat, whereas thesimplex put has less sharply defined selectional re-strictions and can occur with any noun.
In termsof the word senses of the head nouns of the ob-ject NPs, the VPC put on will tend to co-occurwith objects which have the semantics of clothesor garment.
On the other hand, the simplex verbput in isolation tends to be used with objects withthe semantics of object and prepositional phrasescontaining NPs with the semantics of place.Also, as observed above, the valence of a VPCcan differ from that of the head verb.
(3) and (4)illustrate two different senses of take off with in-transitive and transitive syntax, respectively.
Notethat take cannot occur as a simplex intransitiveverb.
(3) take off = lift offEX: The airplane takes off.ARGS: airplaneSUBJ = airplane, aeroplaneANALYSIS: verb particle construction(4) take off = removeEX: They take off the cape .ARGS: theySUBJ = person, individualcapeOBJ = garment, clothingANALYSIS: verb particle constructionNote that in (3), take off = lift off co-occurs witha subject of the class airplane, aeroplane.
In (4), onthe other hand, take off = remove and the corre-sponding object noun is of class garment or cloth-ing.
From the above, we can see that head nounsin the subject and object argument positions canbe used to distinguish VPCs from simplex verbswith prepositional phrases (i.e.
verb-PPs).663 ApproachOur goal is to distinguish VPCs from verb-PPs incorpus data, i.e.
to take individual inputs such asKim handed the paper in today and tag each aseither a VPC or a verb-PP.
Our basic approach isto parse each sentence with RASP (Briscoe andCarroll, 2002) to obtain a first-gloss estimate ofthe VPC and verb-PP token instances, and alsoidentify the head nouns of the arguments of eachVPC and simplex verb.
For the head noun of eachsubject and object, as identified by RASP, we useWordNet 2.1 (Fellbaum, 1998) to obtain the wordsense.
Finally we build a supervised classifier us-ing TiMBL 5.1 (Daelemans et al, 2004).3.1 MethodCompared to the method proposed by Baldwin(2005), our approach (a) tackles the task of VPCidentification rather than VPC extraction, and (b)uses both syntactic and semantic features, employ-ing the WordNet 2.1 senses of the subject and/orobject(s) of the verb.
In the sentence He put thecoat on the table, e.g., to distinguish the VPC puton from the verb put occurring with the preposi-tional phrase on the table, we identify the sensesof the head nouns of the subject and object(s) ofthe verb put (i.e.
he and coat, respectively).First, we parse all sentences in the given corpususing RASP, and identify verbs and prepositionsin the RASP output.
This is a simple process ofchecking the POS tags in the most-probable parse,and for both particles (tagged RP) and transitiveprepositions (tagged II) reading off the governingverb from the dependency tuple output (see Sec-tion 3.2 for details).
We also retrieved the headnouns of the subject and object(s) of each headverb directly from the dependency tuples.
UsingWordNet 2.1, we then obtain the word sense of thehead nouns.The VPCs or verb-PPs are represented with cor-responding information as given below:P (type|v, p,wsSUBJ,wsDOBJ,ws IOBJ)where type denotes either a VPC or verb-PP, v isthe head verb, p is the preposition, and ws* is theword sense of the subject, direct object or indirectobject.Once all the data was gathered, we separated itinto test and training data.
We then used TiMBL5.1 to learn a classifier from the training data,which was then run and evaluated over the testdata.
See Section 5 for full details of the results.Figure 1 depicts the complete process used todistinguish VPCs from verb-PPs.textrawParticles ObjectsSensescorpusSubjectsWordNetWordv+p with SemanticsVerbsTiMBL Classifierlook_after := [..put_on := [..take_off := [..e.g.Preprocessing RASPparserFigure 1: System Architecture3.2 On the use of RASP, WordNet andTiMBLRASP is used to identify the syntactic structureof each sentence, including the head nouns of ar-guments and first-gloss determination of whethera given preposition is incorporated in a VPC orverb-PP.
The RASP output contains dependencytuples derived from the most probable parse, eachof which includes a label identifying the natureof the dependency (e.g.
SUBJ, DOBJ), the headword of the modifying constituent, and the head ofthe modified constituent.
In addition, each wordis tagged with a POS tag from which it is possi-ble to determine the valence of any prepositions.McCarthy et al (2003) evaluate the precision ofRASP at identifying VPCs to be 87.6% and the re-call to be 49.4%.
However the paper does not eval-uate the parser?s ability to distinguish sentencescontaining VPCs and sentences with verb-PPs.To better understand the baseline performanceof RASP, we counted the number of false-positiveexamples tagged with RP and false-negative ex-amples tagged with II, relative to gold-standarddata.
See Section 5 for details.We use WordNet to obtain the first-sense wordsense of the head nouns of subject and objectphrases, according to the default word sense rank-ing provided within WordNet.
McCarthy et al(2004) found that 54% of word tokens are usedwith their first (or default) sense.
With the per-formance of current word sense disambiguation(WSD) systems hovering around 60-70%, a sim-ple first-sense WSD system has room for improve-ment, but is sufficient for our immediate purposes67in this paper.To evaluate our approach, we built a super-vised classifier using the TiMBL 5.1 memory-based learner and training data extracted from theBrown and WSJ corpora.4 Data CollectionWe evaluated out method by running RASP overBrown Corpus and Wall Street Journal, as con-tained in the Penn Treebank (Marcus et al, 1993).4.1 Data ClassificationThe data we consider is sentences containingprepositions tagged as either RP or II.
Based onthe output of RASP, we divide the data into fourgroups:Group A Group BGroup CRP & II tagged dataRP tagged data II tagged dataGroup DGroup A contains the verb?preposition tokeninstances tagged tagged exclusively as VPCs (i.e.the preposition is never tagged as II in combi-nation with the given head verb).
Group B con-tains the verb?preposition token instances iden-tified as VPCs by RASP where there were alsoinstances of that same combination identified asverb-PPs.
Group C contains the verb?prepositiontoken instances identified as verb-PPs by RASPwhere there were also instances of that same com-bination identified as VPCs.
Finally, group Dcontains the verb-preposition combinations whichwere tagged exclusively as verb-PPs by RASP.We focus particularly on disambiguating verb?preposition token instances falling into groups Band C, where RASP has identified an ambiguityfor that particular combination.
We do not furtherclassify token instances in group D, on the groundsthat (a) for high-frequency verb?preposition com-binations, RASP was unable to find a single in-stance warranting a VPC analysis, suggesting ithad high confidence in its ability to correctly iden-tify instances of this lexical type, and (b) for low-frequency verb?preposition combinations wherethe confidence of there definitively no being aVPC usage is low, the token sample is too smallto disambiguate effectively and the overall impactwould be negligible even if we tried.
We do, how-ever, return to considered data in group D in com-puting the precision and recall of RASP.Naturally, the output of RASP parser is noterror-free, i.e.
VPCs may be parsed as verb-PPsFPR FNR AgreementGroup A 4.08% ?
95.24%Group B 3.96% ?
99.61%Group C ?
10.15% 93.27%Group D ?
3.4% 99.20%Table 1: False positive rate (FPR), false negativerate (FNR) and inter-annotator agreement acrossthe four groups of token instancesf ?
1 f ?
5VPC V-PP VPC V-PPGroup A 5,223 0 3,787 0Group B 1,312 0 1,108 0Group C 0 995 0 217Total 6,535 995 4,895 217Table 2: The number of VPC and verb-PP tokeninstances occurring in groups A, B and C at vary-ing frequency cut-offsand vice versa.
In particular, other than the re-ported results of McCarthy et al (2003) targetingVPCs vs. all other analyses, we had no a priorisense of RASP?s ability to distinguish VPCs andverb-PPs.
Therefore, we manually checked thefalse-positive and false-negative rates in all fourgroups and obtained the performance of parserwith respect to VPCs.
The verb-PPs in group Aand B are false-positives while the VPCs in groupC and D are false-negatives (we consider the VPCsto be positive examples).To calculate the number of incorrect examples,two human annotators independently checkedeach verb?preposition instance.
Table 1 details therate of false-positives and false-negative examplesin each data group, as well as the inter-annotatoragreement (calculated over the entire group).4.2 CollectionWe combined together the 6,535 (putative) VPCsand 995 (putative) verb-PPs from groups A, B andC, as identified by RASP over the corpus data.
Ta-ble 2 shows the number of VPCs in groups A andB and the number of verb-PPs in group C. Thefirst number is the number of examples occuringat least once and the second number that of exam-ples occurring five or more times.From the sentences containing VPCs and verb-PPs, we retrieved a total of 8,165 nouns, including68Type Groups A&B Group Ccommon noun 7,116 1,239personal pronoun 629 79demonstrative pronoun 127 1proper noun 156 18who 94 6which 32 0No sense (what) 11 0Table 3: Breakdown of subject and object headnouns in group A&B, and group Cpronouns (e.g.
I, he, she), proper nouns (e.g.
CITI,Canada, Ford) and demonstrative pronouns (e.g.one, some, this), which occurred as the head nounof a subject or object of a VPC in group A or B.We similarly retrieved 1,343 nouns for verb-PPs ingroup C. Table 3 shows the distribution of differentnoun types in these two sets.We found that about 10% of the nouns are pro-nouns (personal or demonstrative), proper nounsor WH words.
For pronouns, we manually re-solved the antecedent and took this as the headnoun.
When which is used as a relative pronoun,we identified if it was coindexed with an argumentposition of a VPC or verb-PP, and if so, manuallyidentified the antecedent, as illustrated in (5).
(5) EX: Tom likes the books which he sold off.ARGS: heSUBJ = personwhichOBJ = bookWith what, on the other hand, we were gener-ally not able to identify an antecedent, in whichcase the argument position was left without a wordsense (we come back to this in Section 6).
(6) Tom didn?t look up what to do.What went on?We also replaced all proper nouns with cor-responding common noun hypernyms based onmanual disambiguation, as the coverage of propernouns in WordNet is (intentionally) poor.
The fol-lowing are examples of proper nouns and theircommon noun hypernyms:Proper noun Common noun hypernymCITI bankCanada countryFord companySmith humanproduce, green goods, ...food(3rd)...reproductive structure...pome, false fruitreproductive structurefruitfruit(2nd)citrus, citrus fruit, citrous fruitedible fruit(2nd)edible fruit(1st)appleSense 1Sense 1orangeproduce, green goods, ...food(4th).....fruit(3rd)Figure 2: Senses of apple and orangeWhen we retrieved the first word sense of nounsfrom WordNet, we selected the first sense and theassociated hypernyms (up to) three levels up theWordNet hierarchy.
This is intended as a crudeform of smoothing for closely-related word senseswhich occur in the same basic region of the Word-Net hierarchy.
As an illustration of this process,in Figure 2, apple and orange are used as edi-ble fruit, fruit or food, and the semantic overlap ispicked up on by the fact that edible fruit is a hy-pernym of both apple and orange.
On the otherhand, food is the fourth hypernym for orange so itis ignored by our method.
However, because weuse the four senses, the common senses of nounsare extracted properly.
This approach works rea-sonably well for retrieving common word sensesof nouns which are in the immediate vicinity ofeach other in the WordNet hierarchy, as was thecase with apple and orange.
In terms of featurerepresentation, we generate an individual instancefor each noun sense generated based on the abovemethod, and in the case that we have multiple ar-guments for a given VPC or verb-PP (e.g.
both asubject and a direct object), we generate an indi-vidual instance for the cross product of all sensecombinations between the arguments.We use 80% of the data for training and 20%for testing.
The following is the total number oftraining instances, before and after performing hy-pernym expansion:Training InstancesBefore expansion After expansionGroup A 5,223 24,602Group B 1,312 4,158Group C 995 5,98569Group Frequency of VPCs SizeB (f?1 ) test:272(f?5 ) train:1,040BA (f?1 & f?1 ) test:1,327(f?5 & f?5 ) train:4,163BC (f?1 & f?1 ) test:498(f?5 & f?1 ) train:1,809BAC (f?1 & f?1 & f?1 ) test:1,598(f?5 & f?5 & f?1 ) train:5,932Table 4: Data set sizes at different frequency cut-offs5 EvaluationWe selected 20% of the test data from differentcombinations of the four groups and over the twofrequency thresholds, leading to a total of 8 testdata sets.
The first data set contains examples fromgroup B only, the second set is from groups B andA, the third set is from groups B and C, and thefourth set is from groups B, A and C. Addition-ally, each data set is divided into: (1) f ?
1, i.e.verb?preposition combinations occurring at leastonce, and (2) f ?
5, i.e.
verb?preposition com-binations occurring at least five times (hereafter,f ?
1 is labelled f?1 and f ?
5 is labelled f?5 ).In the group C data, there are 217 verb-PPs withf?5 , which is slightly more than 20% of the dataso we use verb-PPs with f?1 for experiments in-stead of verb-PP with f?5 .
The first and seconddata sets do not contain negative examples whilethe third and fourth data sets contain both positiveand negative examples.
As a result, the precisionfor the first two data sets is 1.0.Table 5 shows the precision, recall and F-scoreof our method over each data set, relative to theidentification of VPCs only.
A,B,C are groups andf# is the frequency of examples.Table 6 compares the performance of VPC iden-tification and verb-PP identification.Table 7 indicates the result using four wordsenses (i.e.
with hypernym expansion) and onlyone word sense (i.e.
the first sense only).6 DiscussionThe performance of RASP as shown in Tables 5and 6 is based on human judgement.
Note thatwe only consider the ability of the parser to distin-guish sentences with prepositions as either VPCsor verb-PPs (i.e.
we judge the parse to be correct ifthe preposition is classified correctly, irrespectiveof whether there are other errors in the output).Data Freq P R FRASP f?1 .959 .955 .957B f?1 1.0 .819 .901f?5 1.0 .919 .957BA f?1 f?1 1.0 .959 .979f?5 f?5 1.0 .962 .980BC f?1 f?1 .809 .845 .827f?5 f?1 .836 .922 .877BAC f?1 f?1 f?1 .962 .962 .962f?5 f?5 f?1 .964 .983 .974Table 5: Results for VPC identification only (P =precision, R = recall, F = F-score)Data Freq Type P R FRASP f?1 P+V .933 ?
?BC f?1 f?1 P+V .8068 .8033 .8051f?5 f?1 P+V .8653 .8529 .8591BAC f?1 f?1 P+V .8660 .8660 .8660f?5 f?1 P+V .9272 .8836 .9054Table 6: Results for VPC (=V) and verb-PP (=P)identification (P = precision, R = recall, F = F-score)Also, we ignore the ambiguity between particlesand adverbs, which is the principal reason for ourevaluation being much higher than that reportedby McCarthy et al (2003).
In Table 5, the preci-sion (P) and recall (R) for VPCs are computed asfollows:P = Data Correctly Tagged as VPCsData Retrieved as VPCsR = Data Correctly Tagged as VPCsAll VPCs in Data SetThe performance of RASP in Table 6 showshow well it distinguishes between VPCs and verb-PPs for ambiguous verb?preposition combina-tions.
Since Table 6 shows the comparative per-formance of our method between VPCs and verb-PPs, the performance of RASP with exampleswhich are misrecognized as each other should bethe guideline.
Note, the baseline RASP accuracy,based on assigning the majority class to instancesin each of groups A, B and C, is 83.04%.In Table 5, the performance over high-frequency data identified from groups B, A andC is the highest (F-score = .974).
In general, wewould expect the data set containing the high fre-quency and both positive and negative examples70Freq Type # P R Ff?1 V 4WS .962 .962 .9621WS .958 .969 .963f?1 P 4WS .769 .769 .7691WS .800 .743 .770f?5 V 4WS .964 .983 .9741WS .950 .973 .962f?5 P 4WS .889 .783 .8321WS .813 .614 .749Table 7: Results with hypernym expansion (4WS)and only the first sense (1WS), in terms of preci-sion (P), recall (R) and F-score (F)to give us the best performance at VPC identifi-cation.
We achieved a slightly better result thanthe 95.8%-97.5% performance reported by Li etal.
(2003).
However, considering that Li et al(2003) need considerable time and human labourto generate hand-coded rules, our method has ad-vantages in terms of both raw performance andlabour efficiency.Combining the results for Table 5 and Table 6,we see that our method performs better for VPCidentification than verb-PP identification.
Sincewe do not take into account the data from groupD with our method, the performance of verb-PPidentification is low compared to that for RASP,which in turn leads to a decrement in the overallperformance.Since we ignored the data from group D con-taining unambiguous verb-PPs, the number of pos-itive training instances for verb-PP identificationwas relatively small.
As for the different numberof word senses in Table 7, we conclude that themore word senses the better the performance, par-ticularly for higher-frequency data items.In order to get a clearer sense of the impact ofselectional preferences on the results, we investi-gated the relative performance over VPCs of vary-ing semantic compositionality, based on 117 VPCs(f?1 ) attested in the data set of McCarthy et al(2003).
According to our hypothesis from above,we would expect VPCs with low composition-ality to have markedly different selectional pref-erences to the corresponding simplex verb, andVPCs with high compositionality to have similarselectional preferences to the simplex verb.
Interms of the performance of our method, therefore,we would expect the degree of compositionalityto be inversely proportional to the system perfor-mance.
We test this hypothesis in Figure 3, wherewe calculate the error rate reduction (in F-score)0204060801000  1  2  3  4  5  6  7  8  9  10 020406080100Error RateReduction(%)TypesCompositionalityFigure 3: Error rate reduction for VPCs of varyingcompositionalityfor the proposed method relative to the majority-class baseline, at various degrees of composition-ality.
McCarthy et al (2003) provides compo-sitionality judgements from three human judges,which we take the average of and bin into 11 cate-gories (with 0 = non-compositional and 10 = fullycompositional).
In Figure 3, we plot both the er-ror rate reduction in each bin (both the raw num-bers and a smoothed curve), and also the numberof attested VPC types found in each bin.
Fromthe graph, we see our hypothesis born out that,with perfect performance over non-compositionalVPCs and near-baseline performance over fullycompositional VPCs.
Combining this result withthe overall results from above, we conclude thatour method is highly successful at distinguishingnon-compositional VPCs from verb-PPs, and fur-ther that there is a direct correlation between thedegree of compositionality and the similarity ofthe selectional preferences of VPCs and their verbcounterparts.Several factors are considered to have influ-enced performance.
Some data instances are miss-ing head nouns which would assist us in determin-ing the semantics of the verb?preposition combi-nation.
Particular examples of this are imperativeand abbreviated sentences:(7) a.
Come in.b.
(How is your cold?)
Broiled out.Another confounding factor is the lack of wordsense data, particularly in WH questions:(8) a.
What do I hand in?b.
You can add up anything .717 ConclusionIn this paper, we have proposed a method for iden-tifying VPCs automatically from raw corpus data.We first used the RASP parser to identify VPCand verb-PP candidates.
Then, we used analysis ofthe head nouns of the arguments of the head verbsto model selectional preferences, and in doing so,distinguish between VPCs and verb-PPs.
UsingTiMBL 5.1, we built a classifier which achievedan F-score of 97.4% at identifying frequent VPCexamples.
We also investigated the comparativeperformance of RASP at VPC identification.The principal drawback of our method is that itrelies on the performance of RASP and we assumea pronoun resolution oracle to access the wordsenses of pronouns.
Since the performance of suchsystems is improving, however, we consider ourapproach to be a promising, stable method of iden-tifying VPCs.AcknowledgementsThis material is based upon work supported in part by theAustralian Research Council under Discovery Grant No.DP0663879 and NTT Communication Science Laboratories,Nippon Telegraph and Telephone Corporation.
We wouldlike to thank the three anonymous reviewers for their valu-able input on this research.ReferencesTimothy Baldwin and Aline Villavicencio.
2002.
Extract-ing the unextractable: A case study on verb-particles.
InProc.
of the 6th Conference on Natural Language Learn-ing (CoNLL-2002), pages 98?104, Taipei, Taiwan.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Do-minic Widdows.
2003.
An empirical model of multiwordexpression decomposability.
In Proc.
of the ACL-2003Workshop on Multiword Expressions: Analysis, Acquisi-tion and Treatment, pages 89?96, Sapporo, Japan.Timothy Baldwin.
2005.
The deep lexical acquisition ofEnglish verb-particle constructions.
Computer Speechand Language, Special Issue on Multiword Expressions,19(4):398?414.Colin Bannard, Timothy Baldwin, and Alex Lascarides.2003.
A statistical approach to the semantics of verb-particles.
In Proc.
of the ACL-2003 Workshop on Multi-word Expressions: Analysis, Acquisition and Treatment,pages 65?72, Sapporo, Japan.Ted Briscoe and John Carroll.
2002.
Robust accurate statisti-cal annotation of general text.
In Proc.
of the 3rd Interna-tional Conference on Language Resources and Evaluation(LREC 2002), pages 1499?1504, Las Palmas, Canary Is-lands.Nicoletta Calzolari, Charles Fillmore, Ralph Grishman,Nancy Ide, Alessandro Lenci, Catherine MacLeod, andAntonio Zampolli.
2002.
Towards best practice for mul-tiword expressions in computational lexicons.
In Proc.
ofthe 3rd International Conference on Language Resourcesand Evaluation (LREC 2002), pages 1934?40, Las Pal-mas, Canary Islands.Ann Copestake and Alex Lascarides.
1997.
Integrating sym-bolic and statistical representations: The lexicon pragmat-ics interface.
In Proc.
of the 35th Annual Meeting of theACL and 8th Conference of the EACL (ACL-EACL?97),pages 136?43, Madrid, Spain.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-tal van den Bosch.
2004.
TiMBL: Tilburg Memory BasedLearner, version 5.1, Reference Guide.
ILK Technical Re-port 04-02.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, USA.B.
Fraser.
1976.
The Verb-Particle Combination in English.The Hague: Mouton.Rodney Huddleston and Geoffrey K. Pullum.
2002.
TheCambridge Grammar of the English Language.
Cam-bridge University Press, Cambridge, UK.Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang, and Ro-hini K. Srihari.
2003.
An expert lexicon approach to iden-tifying English phrasal verbs.
In Proc.
of the 41st AnnualMeeting of the ACL, pages 513?20, Sapporo, Japan.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: the Penn treebank.
Computational Linguis-tics, 19(2):313?30.Diana McCarthy, Bill Keller, and John Carroll.
2003.
De-tecting a continuum of compositionality in phrasal verbs.In Proc.
of the ACL-2003 Workshop on Multiword Ex-pressions: Analysis, Acquisition and Treatment, Sapporo,Japan.Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-roll.
2004.
Finding predominant senses in untagged text.In Proc.
of the 42nd Annual Meeting of the ACL, pages280?7, Barcelona, Spain.Tom O?Hara and Janyce Wiebe.
2003.
Preposition semanticclassification via Treebank and FrameNet.
In Proc.
of the7th Conference on Natural Language Learning (CoNLL-2003), pages 79?86, Edmonton, Canada.Darren Pearce.
2001.
Synonymy in collocation extraction.In Proceedings of the NAACL 2001 Workshop on WordNetand Other Lexical Resources: Applications, Extensionsand Customizations, Pittsburgh, USA.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger.
2002.
Multiword expressions:A pain in the neck for NLP.
In Proc.
of the 3rd Interna-tional Conference on Intelligent Text Processing and Com-putational Linguistics (CICLing-2002), pages 1?15, Mex-ico City, Mexico.Aline Villavicencio.
2003.
Verb-particle constructions andlexical resources.
In Proc.
of the ACL-2003 Workshop onMultiword Expressions: Analysis, Acquisition and Treat-ment, pages 57?64, Sapporo, Japan.Aline Villavicencio.
2006.
Verb-particle constructions in theworld wide web.
In Patrick Saint-Dizier, editor, Compu-tational Linguistics Dimensions of Syntax and Semanticsof Prepositions.
Springer, Dordrecht, Netherlands.Dominic Widdows and Beate Dorow.
2005.
Automatic ex-traction of idioms using graph analysis and asymmetriclexicosyntactic patterns.
In Proc.
of the ACL-SIGLEX2005 Workshop on Deep Lexical Acquisition, pages 48?56, Ann Arbor, USA.72
