NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model?
On the Future of Language Modeling for HLT, pages 1?10,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsMeasuring the Influence of Long Range Dependencies with Neural NetworkLanguage ModelsLe Hai Son and Alexandre Allauzen and Franc?ois YvonUniv.
Paris-Sud and LIMSI/CNRSrue John von Neumann, 91 403 Orsay cedex, FranceFirstname.Lastname@limsi.frAbstractIn spite of their well known limitations,most notably their use of very local con-texts, n-gram language models remain an es-sential component of many Natural LanguageProcessing applications, such as AutomaticSpeech Recognition or Statistical MachineTranslation.
This paper investigates the po-tential of language models using larger con-text windows comprising up to the 9 previ-ous words.
This study is made possible bythe development of several novel Neural Net-work Language Model architectures, whichcan easily fare with such large context win-dows.
We experimentally observed that ex-tending the context size yields clear gains interms of perplexity and that the n-gram as-sumption is statistically reasonable as long asn is sufficiently high, and that efforts shouldbe focused on improving the estimation pro-cedures for such large models.1 IntroductionConventional n-gram Language Models (LMs) are acornerstone of modern language modeling for Natu-ral Language Processing (NLP) systems such as sta-tistical machine translation (SMT) and AutomaticSpeech Recognition (ASR).
After more than twodecades of experimenting with these models in avariety of languages, genres, datasets and appli-cations, the vexing conclusion is that these mod-els are very difficult to improve upon.
Many vari-ants of the simple n-gram model have been dis-cussed in the literature; yet, very few of these vari-ants have shown to deliver consistent performancegains.
Among these, smoothing techniques, such asGood-Turing, Witten-Bell and Kneser-Ney smooth-ing schemes (see (Chen and Goodman, 1996) for anempirical overview and (Teh, 2006) for a Bayesianinterpretation) are used to compute estimates for theprobability of unseen events, which are needed toachieve state-of-the-art performance in large-scalesettings.
This is because, even when using the sim-plifying n-gram assumption, maximum likelihoodestimates remain unreliable and tend to overeresti-mate the probability of those rare n-grams that areactually observed, while the remaining lots receivea too small (null) probability.One of the most successful alternative to date isto use distributed word representations (Bengio etal., 2003) to estimate the n-gram models.
In thisapproach, the discrete representation of the vocabu-lary, where each word is associated with an arbitraryindex, is replaced with a continuous representation,where words that are distributionally similar are rep-resented as neighbors.
This turns n-gram distribu-tions into smooth functions of the word representa-tion.
These representations and the associated esti-mates are jointly computed using a multi-layer neu-ral network architecture.
The use of neural-networkslanguage models was originally introduced in (Ben-gio et al, 2003) and successfully applied to large-scale speech recognition (Schwenk and Gauvain,2002; Schwenk, 2007) and machine translationtasks (Allauzen et al, 2011).
Following these ini-tial successes, the neural approach has recently beenextended in several promising ways (Mikolov et al,2011a; Kuo et al, 2010; Liu et al, 2011).Another difference between conventional and1neural network language models (NNLMs) that hasoften been overlooked is the ability of the latter tofare with extended contexts (Schwenk and Koehn,2008; Emami et al, 2008); in comparison, standardn-gram LMs rarely use values of n above n = 4or 5, mainly because of data sparsity issues andthe lack of generalization of the standard estimates,notwithstanding the complexity of the computationsincurred by the smoothing procedures (see however(Brants et al, 2007) for an attempt to build verylarge models with a simple smoothing scheme).The recent attempts of Mikolov et al (2011b)to resuscitate recurrent neural network architecturesgoes one step further in that direction, as a recur-rent network simulates an unbounded history size,whereby the memory of all the previous words ac-cumulates in the form of activation patterns on thehidden layer.
Significant improvements in ASR us-ing these models were reported in (Mikolov et al,2011b; Mikolov et al, 2011a).
It must however beemphasized that the use of a recurrent structure im-plies an increased complexity of the training and in-ference procedures, as compared to a standard feed-forward network.
This means that this approach can-not handle large training corpora as easily as n-grammodels, which makes it difficult to perform a faircomparison between these two architectures and toassess the real benefits of using very large contexts.The contribution is this paper is two-fold.
Wefirst analyze the results of various NNLMs to assesswhether long range dependencies are efficient in lan-guage modeling, considering history sizes rangingfrom 3 words to an unbounded number of words (re-current architecture).
A by-product of this study is aslightly modified version of n-gram SOUL model(Le et al, 2011a) that aims at quantitatively esti-mating the influence of context words both in termsof their position and their part-of-speech informa-tion.
The experimental set-up is based on a largescale machine translation task.
We then propose ahead to head comparison between the feed-forwardand recurrent NNLMs.
To make this comparisonfair, we introduce an extension of the SOUL modelthat approximates the recurrent architecture with alimited history.
While this extension achieves per-formance that are similar to the recurrent model onsmall datasets, the associated training procedure canbenefit from all the speed-ups and tricks of standardfeedforward NNLM (mini-batch and resampling),which make it able to handle large training corpora.Furthermore, we show that this approximation canalso be effectively used to bootstrap the training of a?true?
recurrent architecture.The rest of this paper is organized as follows.
Wefirst recollect, in Section 2, the basics of NNLMs ar-chitectures.
We then describe, in Section 3, a num-ber of ways to speed up training for our ?pseudo-recurrent?
model.
We finally report, in Section 4,various experimental results aimed at measuring theimpact of large contexts, first in terms of perplexity,then on a realistic English to French translation task.2 Language modeling in a continuousspaceLet V be a finite vocabulary, language models de-fine distributions over sequences1 of tokens (typi-cally words) wL1 in V+ as follows:P (wL1 ) =L?i=1P (wi|wi?11 ) (1)Modeling the joint distribution of several discreterandom variables (such as words in a sentence) isdifficult, especially in NLP applications where Vtypically contains hundreds of thousands words.
Inthe n-gram model, the context is limited to the n?1previous words, yielding the following factorization:P (wL1 ) =L?i=1P (wi|wi?1i?n+1) (2)Neural network language models (Bengio et al,2003) propose to represent words in a continuousspace and to estimate the probability distribution asa smooth function of this representation.
Figure 1provides an overview of this approach.
The contextwords are first projected in a continuous space usingthe shared matrix R. Denoting v the 1-of-V codingvector of word v (all null except for the vth compo-nent which is set to 1), its projection vector is thevth line of R: RTv.
The hidden layer h is thencomputed as a non-linear function of these vectors.Finally, the probability of all possible outcomes arecomputed using one or several softmax layer(s).1wji denotes a sequence of tokens wi .
.
.
j when j ?
i, orthe empty sequence otherwise.20...010010...0000...0010v-3v-2v-1RRRshared input spaceinput layerhidden layersshortlistsub-classlayerswordlayersclasslayerinput part output partWFigure 1: 4-gram model with SOUL at the output layer.This architecture can be divided in two parts, withthe hidden layer in the middle: the input part (on theleft hand side of the graph) which aims at represent-ing the context of the prediction; and the output part(on the right hand side) which computes the proba-bility of all possible successor words given the con-text.
In the remaining of this section, we describethese two parts in more detail.2.1 Input Layer StructureThe input part computes a continuous representationof the context in the form of a context vector h to beprocessed through the hidden layer.2.1.1 N -gram Input LayerUsing the standard n-gram assumption of equa-tion (2), the context is made up of the sole n?1 pre-vious words.
In a n-gram NNLM, these words areprojected in the shared continuous space and theirrepresentations are then concatenated to form a sin-gle vector i, as illustrated in the left part of Figure 1:i = {RTv?(n?1);RTv?
(n?2); .
.
.
;RTv?1}, (3)where v?k is the kth previous word.
A non-lineartransformation is then applied to compute the firsthidden layer h as follows:h = sigm (Wi+ b) , (4)with sigm the sigmoid function.
This kind of archi-tecture will be referred to as a feed-forward NNLM.Conventional n-gram LMs are usually limited tosmall values of n, and using n greater that 4 or 5does not seem to be of much use.
Indeed, previ-ous experiments using very large speech recognitionsystems indicated that the gain obtained by increas-ing the n-gram order from 4 to 5 is almost negli-gible, whereas the model size increases drastically.While using large context seems to be very imprac-tical with back-off LMs, the situation is quite dif-ferent for NNLMs due to their specific architecture.In fact, increasing the context length for a NNLMmainly implies to expend the projection layer withone supplementary projection vector, which can fur-thermore be computed very easily through a sim-ple look-up operation.
The overall complexity ofNNLMs thus only grows linearly with n in the worstcase (Schwenk, 2007).In order to better investigate the impact of eachcontext position in the prediction, we introduce aslight modification of this architecture in a man-ner analog to the proposal of Collobert and Weston(2008).
In this variation, the computation of the hid-den layer defined by equation (4) is replaced by:h = sigm(maxk[WkRTv?k]+ b), (5)where Wk is the sub-matrix of W comprising thecolumns related to the kth history word, and the maxis to be understood component-wise.
The productWkRT can then be considered as defining the pro-jection matrix for the kth position.
After the projec-tion of all the context words, the max function se-lects, for each dimension l, among the n ?
1 values([WkRTv?k]l) the most active one, which we alsoassume to be the most relevant for the prediction.2.1.2 Recurrent LayerRecurrent networks are based on a more complexarchitecture designed to recursively handle an arbi-trary number of context words.
Recurrent NNLMsare described in (Mikolov et al, 2010; Mikolov etal., 2011b) and are experimentally shown to outper-form both standard back-off LMs and feed-forwardNNLMs in terms of perplexity on a small task.
Thekey aspect of this architecture is that the input layerfor predicting the ith word wi in a text contains botha numeric representation vi?1 of the previous wordand the hidden layer for the previous prediction.3The hidden layer thus acts as a representation of thecontext history that iteratively accumulates an un-bounded number of previous words representations.Our reimplementation of recurrent NNLMsslightly differs from the feed-forward architecturemainly by its input part.We use the same deep archi-tecture to model the relation between the input wordpresentations and the input layer as in the recurrentmodel.
However, we explicitly restrict the context tothe n?1 previous words.
Note that this architectureis just a convenient intermediate model that is usedto efficiently train a recurrent model, as described inSection 3.
In the recurrent model, the input layer isestimated as a recursive function of both the currentinput word and the past input layer.i = sigm(Wi?1 +RTv?1) (6)As in the standard model, RTv?k associates eachcontext word v?k to one feature vector (the corre-sponding row in R).
This vector plays the role ofa bias at subsequent input layers.
The input part isthus structured in a series of layers, the relation be-tween the input layer and the first previous word be-ing at level 1, the second previous word is at level 2and so on.
In (Mikolov et al, 2010; Mikolov et al,2011b), recurrent models make use of the entire con-text, from the current word position all the way backto the beginning of the document.
This greatly in-creases the complexity of training, as each documentmust be considered as a whole and processed posi-tion per position.
By comparison, our reimplemen-tation only considers a fixed context length, whichcan be increased at will, thus simulating a true recur-rent architecture; this enables us to take advantageof several techniques during training that speed uplearning (see Section 3).
Furthermore, as discussedbelow, our preliminary results show that restrictingthe context to the current sentence is sufficient to at-tain optimal performance 2.2.2 Structured Output LayerA major difficulty with the neural network approachis the complexity of inference and training, whichlargely depends on the size of the output vocabu-2The test sets used in MT experiments are made of variousNews extracts.
Their content is thus not homogeneous and us-ing words from previous sentences doesn?t seem to be relevant.lary ,i.e.
of the number of words that have to be pre-dicted.
To overcome this problem, Le et al (2011a)have proposed the structured Output Layer (SOUL)architecture.
Following (Mnih and Hinton, 2008),the SOUL model combines the neural network ap-proach with a class-based LM (Brown et al, 1992).Structuring the output layer and using word class in-formation makes the estimation of distribution overlarge output vocabulary computationally feasible.In the SOUL LM, the output vocabulary is struc-tured in a clustering tree, where every word is asso-ciated to a unique path from the root node to a leafnode.
Denoting wi the ith word in a sentence, the se-quence c1:D(wi) = c1, .
.
.
, cD encodes the path forword wi in this tree, with D the tree depth, cd(wi)the class or sub-class assigned to wi, and cD(wi) theleaf associated with wi, comprising just the word it-self.
The probability of wi given its history h canthen be computed as:P (wi|h) =P (c1(wi)|h)?D?d=2P (cd(wi)|h, c1:d?1).
(7)There is a softmax function at each level of thetree and each word ends up forming its own class(a leaf).
The SOUL architecture is represented inthe right part of Figure 1.
The first (class layer)estimates the class probability P (c1(wi)|h), whilesub-class layers estimate the sub-class probabili-ties P (cd(wi)|h, c1:d?1), d = 2 .
.
.
(D ?
1).
Fi-nally, the word layer estimates the word probabili-ties P (cD(wi)|h, c1:D?1).
As in (Schwenk, 2007),words in the short-list remain special, as each ofthem represents a (final) class on its own right.3 Efficiency issuesTraining a SOUL model can be achieved by maxi-mizing the log-likelihood of the parameters on sometraining corpus.
Following (Bengio et al, 2003),this optimization is performed by Stochastic Back-Propagation (SBP).
Recurrent models are usuallytrained using a variant of SBP called the Back-Propagation Through Time (BPTT) (Rumelhart etal., 1986; Mikolov et al, 2011a).Following (Schwenk, 2007), it is possible togreatly speed up the training of NNLMs using,4for instance, n-gram level resampling and bunchmode training with parallelization (see below); thesemethods can drastically reduce the overall trainingtime, from weeks to days.
Adapting these meth-ods to recurrent models are not straightforward.
Thesame goes with the SOUL extension: its trainingscheme requires to first consider a restricted outputvocabulary (the shortlist), that is then extended to in-clude the complete prediction vocabulary (Le et al,2011b).
This technique is too time consuming, inpractice, to be used when training recurrent mod-els.
By bounding the recurrence to a dozen or soprevious words, we obtain a recurrent-like n-grammodel that can benefit from a variety of speed-uptechniques, as explained in the next sections.Note that the bounded-memory approximation isonly used for training: once training is complete, wederive a true recurrent network using the parameterstrained on its approximation.
This recurrent archi-tecture is then used for inference.3.1 Reducing the training dataOur usual approach for training large scale modelsis based on n-gram level resampling a subset of thetraining data at each epoch.
This is not directly com-patible with the recurrent model, which requires toiterate over the training data sentence-by-sentence inthe same order as they occur in the document.
How-ever, by restricting the context to sentences, data re-sampling can be carried out at the sentence level.This means that the input layer is reinitialized atthe beginning of each sentence so as to ?forget?, asit were, the memory of the previous sentences.
Asimilar proposal is made in (Mikolov et al, 2011b),where the temporal dependencies are limited to thelevel of paragraph.
Another useful trick, which isalso adopted here, is to use different sampling ratesfor the various subparts of the data, thus boosting theuse of in-domain versus out-of-domain data.3.2 Bunch modeBunch mode training processes sentences by batchesof several examples, thus enabling matrix operationthat are performed very efficiently by the existingBLAS library.
After resampling, the training data isdivided into several sentence flows which are pro-cessed simultaneously.
While the number of exam-ples per batch can be as high as 128 without anyvisible loss of performance for n-gram NNLM, wefound, after some preliminary experiments, that thevalue of 32 seems to yield a good tradeoff betweenthe computing time and the performance for recur-rent models.
Using such batches, the training timecan be speeded up by a factor of 8 at the price of aslight loss (less than 2%) in perplexity.3.3 SOUL training schemeThe SOUL training scheme integrates several stepsaimed at dealing with the fact that the output vocab-ulary is split in two sub-parts: very frequent wordsare in the so-called short-list and are treated differ-ently from the less frequent ones.
This setting cannot be easily reproduced with recurrent models.
Bycontrast, using the pseudo-recurrent n-gram NNLM,the SOUL training scheme can be adopted; the re-sulting parameter values are then plugged in into atruly recurrent architecture.
In the light of the resultsreported below, we content ourselves with values ofn in the range 8-10.4 Experimental ResultsWe now turn to the experimental part, starting with adescription of the experimental setup.
We will thenpresent an attempt to quantify the relative impor-tance of history words, followed by a head to headcomparison of the various NNLM architectures dis-cussed in the previous sections.4.1 Experimental setupThe tasks considered in our experiments are derivedfrom the shared translation track of WMT 2011(translation from English to French).
We only pro-vide here a short overview of the task; all the neces-sary details regarding this evaluation campaign areavailable on the official Web site3 and our systemis described in (Allauzen et al, 2011).
Simply notethat our parallel training data includes a large Webcorpus, referred to as the GigaWord parallel cor-pus.
After various preprocessing and filtering steps,the total amount of training data is approximately12 million sentence pairs for the bilingual part, andabout 2.5 billion of words for the monolingual part.To built the target language models, the mono-lingual corpus was first split into several sub-parts3http://www.statmt.org/wmt115based on date and genre information.
For each ofthese sub-corpora, a standard 4-gram LM was thenestimated with interpolated Kneser-Ney smoothing(Chen and Goodman, 1996).
All models were cre-ated without any pruning nor cutoff.
The baselineback-off n-gram LM was finally built as a linearcombination of several these models, where the in-terpolation coefficients are chosen so as to minimizethe perplexity of a development set.All NNLMs are trained following the prescrip-tions of Le et al (2011b), and they all share thesame inner structure: the dimension of the projec-tion word space is 500; the size of two hidden lay-ers are respectively 1000 and 500; the short-list con-tains 2000 words; and the non-linearity is introducedwith the sigmoid function.
For the recurrent model,the parameter that limits the back-propagation of er-rors through time is set to 9 (see (Mikolov et al,2010) for details).
This parameter can be consideredto play a role that is similar to the history size inour pseudo-recurrent n-gram model: a value of 9 inthe recurrent setting is equivalent to n = 10.
AllNNLMs are trained with the following resamplingstrategy: 75% of in-domain data (monolingual Newsdata 2008-2011) and 25% of the other data.
At eachepoch, the parameters are updated using approxi-mately 50 millions words for the last training stepand about 140 millions words for the previous ones.4.2 The usefulness of remote wordsIn this section, we analyze the influence of each con-text word with respect to their distance from the pre-dicted word and to their POS tag.
The quantitativeanalysis relies on the variant of the n-gram architec-ture based on (5) (see Section 2.1), which enablesus to keep track of the most important context wordfor each prediction.
Throughout this study, we willconsider 10-gram NNLMs.Figure 2 represents the selection rate with respectto the word position and displays the percentage ofcoordinates in the input layer that are selected foreach position.
As expected, close words are the mostimportant, with the previous word accounting formore than 35% of the components.
Remote words(at a distance between 7 and 9) have almost thesame, weak, influence, with a selection rate close to2.5%.
This is consistent with the perplexity resultsof n-gram NNLMs as a function of n, reported inTag Meaning ExampleABR abreviation etc FC FMIABK other abreviation ONG BCE CEADJ adjective officielles alimentaire mondialADV adverb contrairement assez alorsDET article; une les lapossessive pronoun ma taINT interjection oui adieu tic-tacKON conjunction que et commeNAM proper name Javier Mercure PaulineNOM noun surprise inflation criseNUM numeral deux cent premierPRO pronoun cette il jePRP preposition; de en danspreposition plus article au du aux desPUN punctuation; : , -punctuation citation ?SENT sentence tag ?
.
!SYM symbol %VER verb ont fasse parlent<s> start of sentenceTable 1: List of grouped tags from TreeTagger.Table 2: the difference between all orders from 4-gram to 8-gram are significant, while the differencebetween 8-gram and 10-gram is negligible.POS tags were computed using the TreeTag-ger (Schmid, 1994); sub-types of a main tag arepooled to reduce the total number of categories.
Forexample, all the tags for verbs are merged into thesame VER class.
Adding the token <s> (sentencestart), our tagset contains 17 tags (see Table 1).The average selection rates for each tag are shownin Figure 3: for each category, we display (in bars)the average number of components that correspondto a word in that category when this word is in pre-vious position.
Rare tags (INT, ABK , ABR andSENT) seem to provide a very useful informationand have very high selection rates.
Conversely, DET,PUN and PRP words occur relatively frequently andbelong to the less selective group.
The two mostfrequent tags (NOM and VER ) have a medium se-lection rate (approximately 0.5).4.3 Translation experimentsThe integration of NNLMs for large SMT tasks isfar from easy, given the computational cost of com-puting n-gram probabilities, a task that is performedrepeatedly during the search of the best translation.Our solution was to resort to a two-pass approach:the first pass uses a conventional back-off n-grammodel to produce a list of the k most likely trans-lations; in the second pass, the NNLMs probability61 2 3 4 5 6 7 8 90.000.050.100.150.200.250.300.35Figure 2: Average selection rate per word position for themax-based NNLM, computed on newstest2009-2011.
Onx axis, the number k represents the kth previous word.0 5 10 150.00.20.40.60.81.0PUN DET SYM PRP NUM KON ADV SENT PRO VER <s> ADJ NOM ABR NAM ABK INTFigure 3: Average selection rate of max function of thefirst previous word in terms of word POS-tag information,computed on newstest2009-2011.
The green line repre-sents the distribution of occurrences of each tag.of each hypothesis is computed and the k-best list isaccordingly reordered.
The NNLM weights are op-timized as the other feature weights using MinimumError Rate Training (MERT) (Och, 2003).
For allour experiments, we used the value k = 300.To clarify the impact of the language model or-der in translation performance, we considered threedifferent ways to use NNLMs.
In the first setting,the NNLM is used alone and all the scores providedby the MT system are ignored.
In the second set-ting (replace), the NNLM score replaces the scoreof the standard back-off LM.
Finally, the score ofthe NNLM can be added in the linear combination(add).
In the last two settings, the weights used forModel Perplexity BLEUalone replace addBaseline 90 29.4 31.3 -4-gram 92 29.8 31.1 31.56-gram 82 30.2 31.6 31.88-gram 78 30.6 31.6 31.810-gram 77 30.5 31.7 31.8recurrent 81 30.4 31.6 31.8Table 2: Results for the English to French task obtainedwith the baseline system and with various NNLMs.
Per-plexity is computed on newstest2009-2011 while BLEU ison the test set (newstest2010).n-best reranking are re-tuned with MERT.Table 2 summarizes the BLEU scores obtained onthe newstest2010 test set.
BLEU improvements areobserved with feed-forward NNLMs using a valueof n = 8 with respect to the baseline (n = 4).Further increase from 8 to 10 only provides a verysmall BLEU improvement.
These results strengthenthe assumption made in Section 3.3: there seem tobe very little information in remote words (aboven = 7-8).
It is also interesting to see that the 4-gramNNLM achieves a comparable perplexity to the con-ventional 4-gram model, yet delivers a small BLEUincrease in the alone condition.Surprisingly4, on this task, recurrent models seemto be comparable with 8-gram NNLMs.
The rea-son may be the deep architecture of recurrent modelthat makes it hard to be trained in a large scale task.With the recurrent-like n-gram model described inSection 2.1.2, it is feasible to train a recurrent modelon a large task.
With 10% of perplexity reduction ascompared to a backoff model, its yields comparableperformances as reported in (Mikolov et al, 2011a).To the best of our knowledge, it is the first recurrentNNLM trained on a such large dataset (2.5 billionwords) in a reasonable time (about 11 days).5 Related workThere have been many attempts to increase thecontext beyond a couple of history words (see eg.
(Rosenfeld, 2000)), for example: by modeling syn-4Pers.
com.
with T. Mikolov: on the ?small?
WSJ dataset, the recurrent model described in (Mikolov et al, 2011b)outperforms the 10-gram NNLM.7tactic information, that better reflects the ?distance?between words (Chelba and Jelinek, 2000; Collinset al, 2005; Schwartz et al, 2011); with a unigrammodel of the whole history (Kuhn and Mori, 1990);by using trigger models (Lau et al, 1993); or by try-ing to model document topics (Seymore and Rosen-feld, 1997).
One interesting proposal avoids the n-gram assumption by estimating the probability of asentence (Rosenfeld et al, 2001).
This approachrelies on a maximum entropy model which incor-porates arbitrary features.
No significant improve-ments were however observed with this model, a factthat can be attributed to two main causes: first, thepartition function can not be computed exactly as itinvolves a sum over all the possible sentences; sec-ond, it seems that data sparsity issues for this modelare also adversely affecting the performance.The recurrent network architecture for LMs wasproposed in (Mikolov et al, 2010) and then ex-tended in (Mikolov et al, 2011b).
The authors pro-pose a hierarchical architecture similar to the SOULmodel, based however on a simple unigram clus-tering.
For large scale tasks (?
400M trainingwords), advanced training strategies were investi-gated in (Mikolov et al, 2011a).
Instead of resam-pling, the data was divided into paragraphs, filteredand then sorted: the most in-domain data was thusplaced at the end of each epoch.
On the other hand,the hidden layer size was decreased by simulating amaximum entropy model using a hash function onn-grams.
This part represents direct connections be-tween input and output layers.
By sharing the pre-diction task, the work of the hidden layer is madesimpler, and can thus be handled with a smallernumber of hidden units.
This approach reintroducesinto the model discrete features which are somehowone main weakness of conventional backoff LMs ascompared to NNLMs.
In fact, this strategy can beviewed as an effort to directly combine the two ap-proaches (backoff-model and neural network), in-stead of using a traditional way, through interpola-tion.
Training simultaneously two different modelsis computationally very demanding for large vocab-ularies, even with help of hashing technique; in com-parison, our approach keeps the model architecturesimple, making it possible to use the efficient tech-niques developed for n-gram NNLMs.The use the max, rather than a sum, on the hid-den layer of neural network is not new.
Within thecontext of language modeling, it was first proposedin (Collobert et al, 2011) with the goal to model avariable number of input features.
Our motivationfor using this variant was different, and was mostlyaimed at analyzing the influence of context wordsbased on the selection rates of this function.6 ConclusionIn this paper, we have investigated several typesof NNLMs, along with conventional LMs, in or-der to assess the influence of long range dependen-cies within sentences in the language modeling task:from recurrent models that can recursively handlean arbitrary number of context words to n-gramNNLMs with n varying between 4 and 10.
Our con-tribution is two-fold.First, experimental results showed that the influ-ence of word further than 9 can be neglected for thestatistical machine translation task 5.
Therefore, then-gram assumption with n ?
10 appears to be well-founded to handle most sentence internal dependen-cies.
Another interesting conclusion of this studyis that the main issue of the conventional n-grammodel is not its conditional independence assump-tions, but the use of too small values for n.Second, by restricting the context of recurrent net-works, the model can benefit of the advanced train-ing schemes and its training time can be divided bya factor 8 without loss on the performances.
To thebest of our knowledge, it is the first time that a re-current NNLM is trained on a such large dataset ina reasonable time.
Finally, we compared these mod-els within a large scale MT task, with monolingualdata that contains 2.5 billion words.
Experimentalresults showed that using long range dependencies(n = 10) with a SOUL language model significantlyoutperforms conventional LMs.
In this setting, theuse of a recurrent architecture does not yield any im-provements, both in terms of perplexity and BLEU.AcknowledgmentsThis work was achieved as part of the Quaero Pro-gramme, funded by OSEO, the French State agencyfor innovation.5The same trend is observed in speech recognition.8ReferencesAlexandre Allauzen, Gilles Adda, He?le`ne Bonneau-Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,Guillaume Wisniewski, and Franc?ois Yvon.
2011.LIMSI @ WMT11.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 309?315, Edinburgh, Scotland.Y Bengio, R Ducharme, P Vincent, and C Jauvin.
2003.A neural probabilistic language model.
Journal of Ma-chine Learning Research, 3(6):1137?1155.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 858?867.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Comput.Linguist., 18(4):467?479.Ciprian Chelba and Frederick Jelinek.
2000.
Structuredlanguage modeling.
Computer Speech and Language,14(4):283?332.Stanley F. Chen and Joshua Goodman.
1996.
An empiri-cal study of smoothing techniques for language model-ing.
In Proc.
ACL?96, pages 310?318, San Francisco.Michael Collins, Brian Roark, and Murat Saraclar.2005.
Discriminative syntactic language modeling forspeech recognition.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics (ACL?05), pages 507?514, Ann Arbor, Michigan,June.
Association for Computational Linguistics.Ronan Collobert and Jason Weston.
2008.
A uni-fied architecture for natural language processing: deepneural networks with multitask learning.
In Proc.of ICML?08, pages 160?167, New York, NY, USA.ACM.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 12:2493?2537.Ahmad Emami, Imed Zitouni, and Lidia Mangu.
2008.Rich morphology based n-gram language models forarabic.
In INTERSPEECH, pages 829?832.R.
Kuhn and R. De Mori.
1990.
A cache-based naturallanguage model for speech recognition.
IEEE Trans-actions on Pattern Analysis and Machine Intelligence,12(6):570?583, june.Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, andImed Zitouni.
2010.
Morphological and syntactic fea-tures for arabic speech recognition.
In Proc.
ICASSP2010.Raymond Lau, Ronald Rosenfeld, and Salim Roukos.1993.
Adaptive language modeling using the maxi-mum entropy principle.
In Proc HLT?93, pages 108?113, Princeton, New Jersey.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-LucGauvain, and Franc?ois Yvon.
2011a.
Structured out-put layer neural network language model.
In Proceed-ings of ICASSP?11, pages 5524?5527.Hai-Son Le, Ilya Oparin, Abdel.
Messaoudi, Alexan-dre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.2011b.
Large vocabulary SOUL neural network lan-guage models.
In Proceedings of InterSpeech 2011.Xunying Liu, Mark J. F. Gales, and Philip C. Woodland.2011.
Improving lvcsr system combination using neu-ral network language model cross adaptation.
In IN-TERSPEECH, pages 2857?2860.Toma?s?
Mikolov, Martin Karafia?t, Luka?s?
Burget, JanC?ernocky?, and Sanjeev Khudanpur.
2010.
Recurrentneural network based language model.
In Proceedingsof the 11th Annual Conference of the InternationalSpeech Communication Association (INTERSPEECH2010), volume 2010, pages 1045?1048.
InternationalSpeech Communication Association.Toma?s?
Mikolov, Anoop Deoras, Daniel Povey, Luka?s?Burget, and Jan C?ernocky?.
2011a.
Strategies for train-ing large scale neural network language models.
InProceedings of ASRU 2011, pages 196?201.
IEEE Sig-nal Processing Society.Toma?s?
Mikolov, Stefan Kombrink, Lukas Burget, JanCernocky?, and Sanjeev Khudanpur.
2011b.
Exten-sions of recurrent neural network language model.
InProc.
of ICASSP?11, pages 5528?5531.Andriy Mnih and Geoffrey E Hinton.
2008.
A scalablehierarchical distributed language model.
In D. Koller,D.
Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-vances in Neural Information Processing Systems 21,volume 21, pages 1081?1088.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics - Volume 1, ACL ?03, pages 160?167, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.2001.
Whole-sentence exponential language models:A vehicle for linguistic-statistical integration.
Com-puters, Speech and Language, 15:2001.R.
Rosenfeld.
2000.
Two decades of statistical languagemodeling: Where do we go from here ?
Proceedingsof the IEEE, 88(8).D.
E. Rumelhart, G. E. Hinton, and R. J. Williams.
1986.Parallel distributed processing: explorations in the mi-crostructure of cognition, vol.
1. chapter Learning9internal representations by error propagation, pages318?362.
MIT Press, Cambridge, MA, USA.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of Interna-tional Conference on New Methods in Language Pro-cessing.Lane Schwartz, Chris Callison-Burch, William Schuler,and Stephen Wu.
2011.
Incremental syntactic lan-guage models for phrase-based translation.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 620?631, Portland, Oregon, USA,June.
Association for Computational Linguistics.Holger Schwenk and Jean-Luc Gauvain.
2002.
Connec-tionist language modeling for large vocabulary contin-uous speech recognition.
In Proc.
ICASSP, pages 765?768, Orlando, FL.H.
Schwenk and P. Koehn.
2008.
Large and diverse lan-guage models for statistical machine translation.
InInternational Joint Conference on Natural LanguageProcessing, pages 661?666, Janv 2008.Holger Schwenk.
2007.
Continuous space languagemodels.
Comput.
Speech Lang., 21(3):492?518.Kristie Seymore and Ronald Rosenfeld.
1997.
Usingstory topics for language model adaptation.
In Proc.
ofEurospeech ?97, pages 1987?1990, Rhodes, Greece.Yeh W. Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proc.
ofACL?06, pages 985?992, Sidney, Australia.10
