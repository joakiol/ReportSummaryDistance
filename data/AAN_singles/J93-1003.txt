Accurate Methods for the Statisticsof Surprise and CoincidenceTed Dunn ing*New Mexico State UniversityMuch work has been done on the statistical analysis of text.
In some cases reported in the lit-erature, inappropriate statistical methods have been used, and statistical significance of resultshave not been addressed.
In particular, asymptotic normality assumptions have often been usedunjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events.
Unfortu-nately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield goodresults with relatively small samples.
These tests can be implemented efficiently, and have beenused for the detection of composite terms and for the determination of domain-specific terms.In some cases, these measures perform much better than the methods previously used.
In caseswhere traditional contingency table methods work well, the likelihood ratio tests described hereare nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied tothe analysis of text.1.
IntroductionThere has been a recent rend back towards the statistical analysis of text.
This trendhas resulted in a number of researchers doing good work in information retrieval andnatural anguage processing in general.
Unfortunately much of their work has beencharacterized by a cavalier approach to the statistical issues raised by the results.The approaches taken by such researchers can be divided into three rough cate-gories...Collect enormous volumes of text in order to make straightforward,statistically based measures work well.Do simple-minded statistical analysis on relatively small volumes of textand either 'correct empirically' for the error or ignore the issue.3.
Perform no statistical analysis whatsoever.The first approach is the one taken by the IBM group researching statistical ap-proaches to machine translation (Brown et al 1989).
They have collected nearly one* Computing Research Laboratory, New Mexico State University, Las Cruces, NM 88003-0001.?
1993 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 1billion words of English text from such diverse sources as internal memos, technicalmanuals, and romance novels, and have aligned most of the electronically availableportion of the record of debate in the Canadian parliament (Hansards).
Their effortshave been Augean, and they have been well rewarded by interesting results.
The sta-tistical significance of most of their work is above reproach, but the required volumesof text are simply impractical in many settings.The second approach is typified by much of the work of Gale and Church (Galeand Church this issue, and in press; Church et al 1989).
Many of the results from theirwork are entirely usable, and the measures they use work well for the examples givenin their papers.
In general, though, their methods lead to problems.
For example, mu-tual information estimates based directly on counts are subject o overestimation whenthe counts involved are small, and z-scores ubstantially overestimate the significanceof rare events.The third approach is typified by virtually all of the information-retrieval literature.Even recent and very innovative work such as that using Latent Semantic Indexing(Dumais et al 1988) and Pathfinder Networks (Schvaneveldt 1990) has not addressedthe statistical reliability of the internal processing.
They do, however, use good statis-tical methods to analyze the overall effectiveness of their approach.Even such well-accepted techniques as inverse document frequency weighting ofterms in text retrieval (Salton and McGill 1983) is generally only justified on verysketchy grounds.The goal of this paper is to present a practical measure that is motivated by statis-tical considerations and that can be used in a number of settings.
This measure worksreasonably well with both large and small text samples and allows direct comparisonof the significance of rare and common phenomena.
This comparison is possible be-cause the measure described in this paper has better asymptotic behavior than moretraditional measures.In the following, some sections are composed largely of background material ormathematical details and can probably be skipped by the reader familiar with statisticsor by the reader in a hurry.
The sections that should not be skipped are marked with**, those with substantial background with *, and detailed erivations are unmarked.This 'good parts' convention should make this paper more useful to the implementeror reader only wishing to skim the paper.2.
The Assumption of Normality *The assumption that simple functions of the random variables being sampled aredistributed normally or approximately normally underlies many common statisticaltests.
This particularly includes Pearson's X2 test and z-score tests.
This assumption isabsolutely valid in many cases.
Due to the simplification of the methods involved, itis entirely justifiable ven in marginal cases.When comparing the rates of occurrence of rare events, the assumptions on whichthese tests are based break down because texts are composed largely of such rareevents.
For example, simple word counts made on a moderate-sized corpus showthat words that have a frequency of less than one in 50,000 words make up about20-30% of typical English language news-wire reports.
This 'rare' quarter of Englishincludes many of the content-bearing words and nearly all the technical jargon.
As anillustration, the following is a random selection of approximately 0.2% of the wordsfound at least once but fewer than five times in a sample of a half million words ofReuters' reports.62Ted Dunning Accurate Methods for the Statisticsabandonmentaerobicsalternatingaltitudeamateurappearanceassertionbarrackbiasedbookiesbroadcastercadreschargingclausecollatingcompileconfirmingcontemptuouslycorridorscrusheddeadlydementeddetailing landscape seldomdirectorship lobbyists sheetdispatched malfeasances simplifieddogfight meat snortduds miners specifyeluded monsoon staffingenigmatic napalm substituteeuphemism northeast surreptitiousexperiences oppressive tallfares overburdened terracedfinals parakeets tippingfoiling penetrate transformgangsters poi turbidguide praised understatementheadache prised unprofitablehobbled protector vagariesidentities query villasinappropriate redoubtable watchfulinflamed remark winterinstilling resignationsintruded ruinunction scantThe only word in this list that is in the least obscure is poi (a native Hawaiian dishmade from taro root).
If we were to sample 50,000 words instead of the half millionused to create the list above, then the expected number of occurrences of any of thewords in this list would be less than one hall well below the point where commonlyused tests should be used.If such ordinary words are 'rare,' any statistical work with texts must deal withthe reality of rare events.
It is interesting that while most of the words in running textare common ones, most of the words in the total vocabulary are rare.Unfortunately, the foundational ssumption of most common statistical analysesused in computational linguistics is that the events being analyzed are relatively com-mon.
For a sample of 50,000 words from the Reuters' corpus mentioned previously,none of the words in the table above is common enough to expect such analyses towork well.3.
The Tradition of Chi-Squared Tests *In text analysis, the statistically based measures that have been used have usuallybeen based on test statistics that are useful because, given certain assumptions, theyhave a known distribution.
This distribution is most commonly either the normal orX 2 distribution.
These measures are very useful and can be used to accurately assesssignificance in a number of different settings.
They are based, however, on severalassumptions that do not hold for most textual analyses.The details of how and why the assumptions behind these measures do not holdis of interest primarily to the statistician, but the result is of interest o the statisticalconsumer (in our case, somebody interested in counting words).
More applicable tech-niques are important in textual analysis.
The next section describes one such technique;implementation f this technique is described in later sections.63Computational Linguistics Volume 19, Number 10.1800.1600.1400.1200.1000.0800.0600.0400.02O0.00JI0.00 10.00 20.00 30.00Figure 1Normal and binomial distributions.4.
Binomial  Distr ibutions for Text Analysis **Binomial distributions arise commonly in statistical nalysis when the data to be ana-lyzed are derived by counting the number of positive outcomes of repeated identicaland independent experiments.
Flipping a coin is the prototypical experiment of thissort.The task of counting words can be cast into the form of a repeated sequenceof such binary trials comparing each word in a text with the word being counted.These comparisons can be viewed as a sequence of binary experiments similar to coinflipping.
In text, each comparison is clearly not independent of all others, but thedependency falls off rapidly with distance.
Another assumption that works relativelywell in practice is that the probability of seeing a particular word does not vary.
Ofcourse, this is not really true, since changes in topic may cause this frequency tovary.
Indeed it is the mild failure of this assumption that makes shallow informationretrieval techniques possible.To the extent hat these assumptions of independence and stationarity are valid,we can switch to an abstract discourse concerning Bernoulli trials instead of words intext, and a number of standard results can be used.
A Bernoulli trial is the statisticalidealization of a coin flip in which there is a fixed probability of a successful outcomethat does not vary from flip to flip.In particular, if the actual probability that the next word matches a prototype is p,then the number of matches generated in the next n words is a random variable (K)with binomial distributionP(K = k) = pk(1-- p)n-k ( nwhose mean is np and whose variance is np(1 -p).
If np(1-p) > 5, then the distributionof this variable will be approximately normal, and as np(1 - p) increases beyond thatpoint, the distribution becomes more and more like a normal distribution.
This can beseen in Figure 1 above, where the binomial distribution (dashed lines) is plotted alongwith the approximating normal distributions (solid lines) for np set to 5, 10, and 20,64Ted Dunning Accurate Methods for the StatisticsTable 1Error introduced by normal approximations.p(k > 1)Using binomial Est.
using normalnp = 0 .001  0.000099 0.34 X 10 -217np = 0.01 0.0099 0.29 X 10 -22np -- 0.1 0.095 0.0022np= 1 0.63 0.5with n fixed at 100.
Larger values of n with np held constant give curves that are notvisibly different from those shown.
For these cases, np ~ np(1 - p).This agreement between the binomial and normal distributions is exactly whatmakes test statistics based on assumptions of normality so useful in the analysis ofexperiments based on counting.
In the case of the binomial distribution, normalityassumptions are generally considered to hold well enough when np(1 - p) > 5.The situation is different when np(1 -p)  is less than 5, and is dramatically differentwhen np(1 -p )  is less than 1.
First, it makes much less sense to approximate a discretedistribution such as the binomial with a continuous distribution such as the normal.Second, the probabilities computed using the normal approximation are less and lessaccurate.Table 1 shows the probabil ity that one or more matches are found in 100 wordsof text as computed using the binomial and normal distributions for np = 0.001,np = 0.01, np = 0.1, and np = 1 where n = 100.
Most words are sufficiently rare sothat even for samples of text where n is as large as several thousand, np will be atthe bottom of this range.
Short phrases are so numerous that np << 1 for almost allphrases even when n is as large as several million.Table 1 shows that for rare events, the normal distribution does not even approx-imate the binomial distribution.
In fact, for np -- 0.1 and n = 100, using the normaldistribution overestimates the significance of one or more occurrences by a factor of40, while for np = 0.01, using the normal distribution overestimates the significance byabout 4 x 1020.
When n increases beyond 100, the numbers in the table do not changesignificantly.If this overestimation were constant, then the estimates using normal distributionscould be corrected and would still be useful, but the fact that the errors are not constantmeans that methods dependent on the normal approximation should not be used toanalyze Bernoulli trials where the probabil ity of positive outcome is very small.
Yet,in many real analyses of text, comparing cases where np -- 0.001 with cases wherenp > 1 is a common problem.5.
L ike l ihood  Ratio Tests *There is another class of tests that do not depend so critically on assumptions ofnormality.
Instead they use the asymptotic distribution of the generalized likelihoodratio.
For text analysis and similar problems, the use of likelihood ratios leads tovery much improved statistical results.
The practical effect of this improvement  is thatstatistical textual analysis can be done effectively with very much smaller volumes oftext than is necessary for conventional tests based on assumed normal distributions,65Computational Linguistics Volume 19, Number 1and it allows comparisons to be made between the significance of the occurrences ofboth rare and common phenomenon.5.1 Parameter Spaces and Likelihood FunctionsLikelihood ratio tests are based on the idea that statistical hypotheses can be saidto specify subspaces of the space described by the unknown parameters of the sta-tistical model being used.
These tests assume that the model is known, but that theparameters of the model are unknown.
Such a test is called parametric.
Other tests areavailable that make no assumptions about he underlying model at all; they are calleddistribution-free.
Only one particular parametric test is described here.
More informa-tion on parametric and distribution-free tests is available in Bradley (1968) and Mood,Graybill, and Boes (1974).The probability that a given experimental outcome described by kl,..., kn will beobserved for a given model described by a number of parameters Pl, p2,.., is calledthe likelihood function for the model and is written asH(pl,p2,...;kl,...,km)where all arguments of H left of the semicolon are model parameters, and all argu-ments right of the semicolon are observed values.
In the continuous case, the proba-bility is replaced by a probability density.
With binomial and multinomials, we onlydeal with the discrete case.For repeated Bernoulli trials, m = 2 because we observe both the number of trialsand the number of positive outcomes and there is only one p. The explicit form forthe likelihood function isH(p;n'k)=pk(1-P)"-k ( k )The parameter space is the set of all values for p and the hypothesis that p = p0is a single point.
For notational brevity the model parameters can be collected into asingle parameter, as can the observed values.
Then the likelihood function is writtenasH(~;k)where w is considered to be a point in the parameter space f~, and k a point in thespace of observations K. Particular hypotheses or observations are represented bysubscripting f~ or K respectively.More information about likelihood ratio tests can be found in texts on theoreticalstatistics (Mood et al 1974).5.2 The Likelihood RatioThe likelihood ratio for a hypothesis  the ratio of the maximum value of the likelihoodfunction over the subspace represented by the hypothesis to the maximum value ofthe likelihood function over the entire parameter space.
That is,A = max~f~?
H(a;; k)max~en H(a;; k)where f~ is the entire parameter space and f~0 is the particular hypothesis being tested.The particularly important feature of likelihood ratios is that the quantity -2  log )~is asymptotically X 2 distributed with degrees of freedom equal to the difference indimension between f~ and f~0.
Importantly, this asymptote is approached very quicklyin the case of binomial and multinomial distributions.66Ted Dunning Accurate Methods for the Statistics5.3 Likelihood Ratio for Binomial and Multinomial DistributionsThe comparison of two binomial or multinomial processes can be done rather easilyusing likelihood ratios.
In the case of two binomial distributions,H(pl ,p2;k l , r l l ,  ka, na)~-p lk l (1 - -p l )n l -k l  ( nl p2 k2 (1 - p2) n2-k2 (n2).k2The hypothesis that the two distributions have the same underlying parameter isrepresented by the set {(pl, p2) I Pl = p2}.The likelihood ratio for this test is= maxpH(p, p; kl, nl, k2, n2)maxpl ,p2 H(pl, P2; kl,//1, k2,//2)"These maxima are achieved with Pl = ~ and P2 = ~ for the denominator, andfor the numerator.
This reduces the ratio to P = ~1+~2maxp L(p, kl , nl )L(p, k2, n2)maxp, ,p2 L(pl , kl , nl )L(p2, k2~ /'/2 )whereL(p,k, n) = pk(1 -- p)n-k.Taking the logarithm of the likelihood ratio gives-2  log ,~ = 2 \[log L(pl, kl, nl) + log L(p2, k2, n2) - log L(p, kl, nl) - log L(p, k2, n2)\] ?For the multinomial case, it is convenient to use the double subscripts and the abbre-viationsso that we can writeThe likelihood ratio isPi = pli~ p2i~ .
.
.
~ pji~ .
.
.Ki ~- kli~k2i~...~kji~...~Q = ql,q2~...~qj,...~H(P I 'P2 ;K I 'n l ,K2 ,  n2) = I I  rli!
I I  pjikji"i=1,2 j kji!
)~ = maxQ H(Q, Q;K1, nl, K2~ n2)max/'l,p2 H ( P1, P2; KI ~ nl , K2~ n2)"This can be separated in a similar fashion as the binomial case by using the functionLIP, K/- IIdJ)~= maXQL(Q, K1)L(Q, K2)maxp,,e2 L(P1, K1)L(P2, K2)"67Computational Linguistics Volume 19, Number 1This expression implicitly involves n because  ~j  kj = n.Maximizing and taking the logarithm,-2 log A = 2 \[log n(Pl, K 1 ) -}- log L(P2~ K2) -- log L(Q, K~) - log L(Q, K2)\]whereandpji - ~-.~ikji~i  kjiqJ - G i j k /If the null hypothesis holds, then the log-likelihood ratio is asymptotically X 2 dis-tributed with k/2 - 1 degrees of freedom.
When j is 2 (the binomial), -2  log )~ will beX 2 distributed with one degree of freedom.If we had initially approximated the binomial distribution with a normal distri-bution with mean np and variance np(1 - p), then we would have arrived at anotherform that is a good approximation of -2 log ~ when np(1 - p) is more than roughly 5.This form is(kji - niqj) 2-21og ,~ .~.
~/~/qj~_--q~where~ik j iqJ = kjias in the multinomial case above andni --- y~ kji.JInterestingly, this expression is exactly the test statistic for Pearson's X2 test, althoughthe form shown is not quite the customary one.
Figure 2 shows the reasonably goodagreement between this expression and the exact binomial log-likelihood ratio derivedearlier where p -- 0.1 and nl -- n2 -- 1000 for various values of kl and k2.Figure 3, on the other hand, shows the divergence between Pearson's tatistic andthe log-likelihood ratio when p = 0.01, nl = 100, and n2 -- 10000.
Note the largechange of scale on the vertical axis.
The pronounced disparity occurs when k I is largerthan the value expected based on the observed value of k2.
The case where nl < n2and ~ > ~ is exactly the case of most interest in many text analyses.T~e convergence of the log of the likelihood ratio to the asymptotic distribution isdemonstrated dramatically in Figure 4.
In this figure, the straighter line was computedusing a symbolic algebra package and represents he idealized one degree of freedomcumulative X2 distribution.
The rougher curve was computed by a numerical experi-ment in which p -- 0.01, nl = 100, and n2 = 10000, which corresponds to the situationin Figure 3.
The close agreement shows that the likelihood ratio measure producesaccurate results over six decades of significance ven in the range where the normalX 2 measure diverges radically from the ideal.68Ted Dunning Accurate Methods for the Statistics200.00150.002 100.00 %50.000.00?
o c~g~J 000 0 0 ?
O / -0.00Figure 2Log-likelihood versus Pearson X 2100.00-2 log200.00500.00450.00400.00350.002 300.00 %250.00200.00150.00100.0050.000.0000O0 00000 0 00 0 0o oo  ?I OoO:0.00 2O.O0-2 logFigure 3Log-likelihood versus Pearson X 240.006.
Practical Results6.1 Bigram Analys is  of a Smal l  TextTo test the efficacy of the likelihood methods, an analysis was made of a 30,000-wordsample of text obtained from the Union Bank of Switzerland, with the intention of69Computational Linguistics Volume 19, Number 1log (1-P(k 1 , k2))0.00-1.00-2.00-3.00-4.00-5.00-6.00-7.000.00 20.00 40.00-2 log k or 2Figure 4Ideal versus simulated Log-likelihoodfinding pairs of words that occurred next to each other with a significantly higherfrequency than would be expected, based on the word frequencies alone.
The text was31,777 words of financial text largely describing market conditions for 1986 and 1987.The results of such a bigram analysis hould highlight collocations common inEnglish as well as collocations peculiar to the financial nature of the analyzed text.As will be seen, the ranking based on likelihood ratio tests does exactly this.
Similarcomparisons made between a large corpus of general text and a domain-specific textcan be used to produce lists consisting only of words and bigrams characteristic ofthe domain-specific texts.This comparison was done by creating a contingency table that contained thefollowing counts of each bigram that appeared in the text:k(A B) I k("~ A B) Ik(A~B) k(~A ,,~B)where the ~ A B represents he bigram in which the first word is not word A and thesecond is word/3.If the words A and B occur independently, then we would expect p(AB) = p(A)p(B)where p(AB) is the probability of A and B occurring in sequence, p(A) is the probabilityof A appearing in the first position, and p(B) is the probability of B appearing in thesecond position.
We can cast this into the mold of our earlier binomial analysis byphrasing the null hypothesis that A and B are independent asp(A I B) = p(A \[,~ B) =p(A).
This means that testing for the independence of A and B can be done by testingto see if the distribution of A given that B is present (the first row of the table) isthe same as the distribution of A given that B is not present (the second row of thetable).
In fact, of course, we are not really doing a statistical test to see if A and B are70Ted Dunning Accurate Methods for the Statisticsindependent; we know that they are generally not independent in text.
Instead we justwant to use the test statistic as a measure that will help highlight particular As andBs that are highly associated in text.These counts were analyzed using the test for binomials described earlier, and the50 most significant are tabulated in Table 2.
This table contains the most significant200 bigrams and is reverse sorted by the first column, which contains the quantity-2  log &.
Other columns contain the four counts from the contingency table describedabove, and the bigram itself.Examination of the table shows that there is good correlation with intuitive feelingsabout how natural the bigrams in the table actually are.
This is in distinct contrast withTable 3, which contains the same data except hat the first column is computed usingPearson's ~2 test statistic.
The overestimate of the significance of items that occur onlya few times is dramatic.
In fact, the entire first portion of the table is dominated bybigrams rare enough to occur only once in the current sample of text.
The misspellingin the bigram 'sees posibilities' is in the original text.Out of 2693 bigrams analyzed, 2682 of them fall outside the scope of applicabilityof the normal X 2 test.
The 11 bigrams that were suitable for analysis with the X 2 testare listed in Table 4.
It is notable that all of these bigrams contain the word the, whichis the most common word in English.7.
ConclusionsStatistics based on the assumption of normal distribution are invalid in most casesof statistical text analysis unless either enormous corpora are used, or the analysis isrestricted to only the very most common words (that is, the ones least likely to be ofinterest).
This fact is typically ignored in much of the work in this field.
Using suchinvalid methods may seriously overestimate he significance of relatively rare events.Parametric statistical analysis based on the binomial or multinomial distribution ex-tends the applicability of statistical methods to much smaller texts than models usingnormal distributions and shows good promise in early applications of the method.Further work is needed to develop software tools to allow the straightforwardanalysis of texts using these methods.
Some of these tools have been developed andwill be distributed by the Consortium for Lexical Research.
For further informationon this software, contact he author or the Consortium via e-mail at ted@nmsu.edu orlexical@nmsu.edu.In addition, there are a wide variety of distribution free methods that may avoideven the assumption that text can be modeled by multinomial distributions.
Measuresbased on Fischer's exact method may prove even more satisfactory than the likelihoodratio measures described in this paper.
Also, using the Poisson distribution instead ofthe multinomial as the limiting distribution for the distribution of counts may providesome benefits.
All of these possibilities hould be tested.8.
Summary of Formulae **For the binomial case, the log likelihood statistic is given by-2  log & = 2 \[log L(pl~ kl~ ?/1 ) q- log L(p2~ k2, ?/2) -- log L(p, kl, nl) - log L(p, k2, ?/2)\]wherelogL(p, n, k) = klogp + (n - k) log(1 - p)also, pl = k z p2 = ~ and p = k~+kan I ' ~t 2 ' n I q -?
l  2 ?71Computational Linguistics Volume 19, Number 1Table 2Bigrams Ranked by Log-Likelihood Test-2  log A k(AB) k(A ~ B) k(,,~ AB) k(~ A ~ B) A B270.72 110 2442 111 29114 the263.90 29 13 123 31612 can256.84 31 23 139 31584 previous167.23 10 0 3 31764 mineral157.21 76 104 2476 29121 at157.03 16 16 51 31694 real146.80 9 0 5 31763 natural115.02 16 0 865 30896 owing104.53 10 9 41 31717 health100.96 8 2 27 31740 stiff98.72 12 111 14 31640 is95.29 8 5 24 31740 qualified94.50 10 93 6 31668 an91.40 12 111 21 31633 is81.55 10 45 35 31687 176.30 5 13 0 31759 balance73.35 16 2536 1 29224 the68.96 6 2 45 31724 accident68.61 24 43 1316 30394 terms61.61 3 0 0 31774 natel60.77 6 92 2 31677 will57.44 4 11 1 31761 great57.44 4 11 1 31761 government57.14 13 7 1327 30430 part53.98 4 1 18 31754 waste53.65 4 13 2 31758 machine52.33 7 61 27 31682 rose52.30 5 9 25 31738 passenger49.79 4 61 0 31712 not48.94 9 12 429 31327 affected48.85 13 1327 12 30425 of48.80 9 4 872 30892 continue47.84 4 41 1 31731 247.20 8 27 157 31585 competition46.38 10 472 20 31275 a45.53 4 18 6 31749 per44.36 7 0 1333 30437 course43.93 5 18 33 31721 generally43.61 19 50 1321 30387 level43.35 20 2532 25 29200 the43.07 6 875 0 30896 to43.06 3 1 10 31763 french41.69 3 29 0 31745 341.67 3 1 13 31760 knitting40.68 4 5 40 31728 2539.23 9 5 1331 30432 because39.20 5 40 25 31707 stock38.87 2 0 1 31774 scanner38.79 3 0 48 31726 pent38.51 3 23 1 31750 firms38.46 4 2 98 31673 restaurant38.28 3 12 3 31759 fell38.14 6 4 432 31335 climbed37.20 6 41 70 31660 total37.15 2 0 2 31773 hay36.98 3 10 5 31759 currentswissbeyearwaterthetermsgastoinsurancecompetitionlikelypersonnelestimatedexpected2sheetunitedinsuranceofcprobablydealbondsofpaperexhibitionslightlyserviceyetbyseptembertondfrompositive100ofgoodofstockregisterspeakingrdmachines000ofmarketscashupsurveyedbusinessbackbyproductioncroptransactions72Ted Dunning Accurate Methods for the StatisticsTable 3Bigrams Ranked by X 2 Test31777.00 3 0 0 31774 natel31777.00 1 0 0 31776 write31777.00 1 0 0 31776 wood31777.00 1 0 0 31776 window31777.00 1 0 0 31776 upholstery31777.00 1 0 0 31776 surveys31777.00 1 0 0 31776 sees31777.00 1 0 0 31776 practically31777.00 1 0 0 31776 poultry31777.00 1 0 0 31776 physicians'31777.00 1 0 0 31776 paints31777.00 1 0 0 31776 maturity31777.00 1 0 0 31776 listeriosis31777.00 1 0 0 31776 la31777.00 1 0 0 31776 instance31777.00 1 0 0 31776 cans31777.00 1 0 0 31776 bluche31777.00 1 0 0 31776 a31324441.54 10 0 3 31764 mineral21184.00 2 0 1 31774 scanner20424.86 9 0 5 31763 natural15888.00 1 1 0 31775 suva's15888.00 1 1 0 31775 suva's15888.00 1 1 0 31775 responsible15888.00 1 1 0 31775 red15888.00 1 1 0 31775 joined15888.00 1 1 0 31775 highest15888.00 1 1 0 31775 generating15888.00 1 1 0 31775 enables15888.00 1 1 0 31775 dessert15888.00 1 1 0 31775 consolidated15888.00 1 1 0 31775 catalytic15888.00 1 1 0 31775 bread15888.00 1 1 0 31775 bottlenecks15888.00 1 1 0 31775 bankers'15888.00 1 1 0 31775 appenzell15888.00 1 1 0 31775 5615888.00 1 1 0 31775 5615888.00 1 1 0 31775 4615888.00 1 1 0 31775 4315888.00 1 1 0 31775 4315888.00 1 0 1 31775 wheel15888.00 1 0 1 31775 shops15888.00 1 0 1 31775 selected15888.00 1 0 1 31775 propelled15888.00 1 0 1 31775 overcapacities15888.00 1 0 1 31775 listed15888.00 1 0 1 31775 liquid15888.00 1 0 1 31775 incl.15888.00 1 0 1 31775 fats15888.00 1 0 1 31775 drastically15888.00 1 0 1 31775 completing15888.00 1 0 1 31775 cider15888.00 1 0 1 31775 bicycle15888.00 1 0 1 31775 auctioning15887.50 2 0 2 31773 haycoffspulpframesleathersexpertposibilitiesdrawnfarmsfeesvarnisheshoveredbacteriapresse280casingcransintercontinentalwatercashgasresponsibilitiesquestionableclientsinkforcesdensitymodestconversationscherrylaggingconvertergrainsbookingassociation'sabrupt513O82520classified502drivejoinedcollectionsrailcarsarisingjobfuelscelluloseoilsdeteriorateconstructionsapplestagscollectionscrop73Computational Linguistics Volume 19, Number 1Table 4Bigrams where X 2 analysis is applicable.
)~2 k(AB) k(A ,-.
B) k(~ AB) k(~ A .., B) A B525.02 110 2442 111 29114 the swiss286.52 76 104 2476 29121 at the51.12 26 2526 66 29159 the volume6.03 4 148 2548 29077 be the4.48 1 73 2551 29152 months the4.31 1 71 2551 29154 increased the0.69 4 70 2548 29155 1986 the0.42 7 62 2545 29163 level the0.28 4 60 2548 29165 again the0.12 5 2547 67 29158 the increased0.03 18 198 2534 29027 as theFor the mult inomial case, this statistic becomes-2  log A = 2 \[log L(P1, K1) + log L(P2~ K2) - log L(Q, K1) - log L(Q, K2)\]whereqjlog L(P, K)kji Pji - ~j kji= ~k j logp jJReferencesBradley, James V. (1968).
Distribution-FreeStatistical Tests.
Prentice Hall.Brown, Peter E; Cocke, John; Della Pietra,Stephen A.; Della Pietra, Vincent J.;Jelinek, Frederick; Lafferty, John D.;Mercer, Robert L.; and Roossin, Paul S.(1989).
"A statistical approach to machinetranslation."
Technical Report RC 14773(#66226), IBM Research Division.Church, Ken W.; Gale, William A.; Hanks,Patrick; and Hindle, Donald (1989).
"Parsing, word associations and typicalpredicate-argument relations."
InProceedings, International Workshop onParsing Technologies, CMU.Dumais, S.; Furnas, G.; Landauer, T.;Deerwester, S.; and Harshman, R.
(1988).
"Using latent semantic analysis toimprove access to textual information.
"In Proceedings, CHI '88.
281-285.Gale, William A., and Church, Ken W.(1993).
"A program for aligningsentences in bilingual corpora.
"Computational Linguistics, 19(1), 00--00.Gale, William A., and Church, Ken W. (inpress).
"Identifying wordcorrespondences in parallel texts.
"McDonald, James E.; Plate, Tony; andSchvaneveldt, Roger (1990).
"UsingPathfinder to extract semanticinformation from text."
In PathfinderAssociative Networks: Studies in KnowledgeOrganization, edited by RogerSchvaneveldt, 149-164.
Ablex.Mood, A. M.; Graybill, E A.; and Boes,D.
C. (1974).
Introduction to the Theory ofStatistics.
McGraw Hill.Schvaneveldt, Roger, ed.
(1990).
PathfinderAssociative Networks: Studies in KnowledgeOrganization.
Ablex.Salton, Gerald, and McGill, M. J.
(1983).Introduction to Modern InformationRetrieval.
McGraw Hill.74
