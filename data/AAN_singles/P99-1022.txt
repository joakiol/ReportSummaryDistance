Dynamic  Non loca l  Language Mode l ing  v iaHierarchical  Top ic -Based  Adaptat ionRadu F lo r ian  and  Dav id  YarowskyComputer  Science Depar tment  and Center  for Language and Speech Processing,Johns Hopkins UniversityBalt imore,  Mary land 21218{rf lor ian,yarowsky}@cs.
jhu.eduAbst rac tThis paper presents a novel method of generatingand applying hierarchical, dynamic topic-based lan-guage models.
It proposes and evaluates new clus-ter generation, hierarchical smoothing and adaptivetopic-probability estimation techniques.
These com-bined models help capture long-distance l xical de-pendencies.
?Experiments on the Broadcast Newscorpus show significant improvement in perplexity(10.5% overall and 33.5% on target vocabulary).1 In t roduct ionStatistical language models are core components ofspeech recognizers, optical character recognizers andeven some machine translation systems Brown etal.
(1990).
The most common language model-ing paradigm used today is based on n-grams, localword sequences.
These models make a Markovianassumption on word dependencies; usually that wordpredictions depend on at most m previous words.Therefore they offer the following approximation forthe computation of a word sequence probability:P(wU) = -') = 1-I =lP(w,where w{ denotes the sequence wi.. .
wj ; a commonsize for m is 3 (trigram language models).Even if n-grams were proved to be very power-ful and robust in various tasks involving languagemodels, they have a certain handicap: because ofthe Markov assumption, the dependency is limitedto very short local context.
Cache language models(Kuhn and de Mori (1992),Rosenfeld (1994)) try toovercome this limitation by boosting the probabil-ity of the words already seen in the history; triggermodels (Lau et al (1993)), even more general, try tocapture the interrelationships between words.
Mod-els based on syntactic structure (Chelba and Jelinek(1998), Wright et al (1993)) effectively estimateintra-sentence syntactic word dependencies.The approach we present here is based on theobservation that certain words tend to have differ-ent probability distributions in different opics.
Wepropose to compute the conditional language modelprobability as a dynamic mixture model of K topic-specific language models:E inp i r l ca l  Observat /on :Lexical Probabi l i t ies are Sens i t ive  to Top ic  and Subtop icP(  peace  !subtop ic  )0~csoJ~cso.oo4~ o~l'=i~olso.l~lo .~s Maj~ Topl~ amd SO sub*op l~ fnme the Bm*d~st N~ ?oqpwFigure 1: Conditional probability of the word peacegiven manually assigned Broadcast News topicsKP (w, lw~ -1) = E P (tlw~-X) "V (wilt, w~ -x)t= lKE P (tlw -a) ?et ,-x (1)t= lThe motivation for developing topic-sensitive lan-guage models is twofold.
First, empirically speaking,many n-gram probabilities vary substantially whenconditioned on topic (such as in the case of contentwords following several function words).
A more im-portant benefit, however, is that even when a givenbigram or trigram probability is not topic sensitive,as in the case of sparse n-gram statistics, the topic-sensitive unigram or bigram probabilities may con-stitute a more informative backoff estimate than thesingle global unigram or bigram estimates.
Discus-sion of these important smoothing issues is given inSection 4.Finally, we observe that lexical probability distri-butions vary not only with topic but with subtopictoo, in a hierarchical manner.
For example, con-sider the variation of the probability of the wordpeace given major news topic distinctions (e.g.
BUSI-NESS and INTERNATIONAL news) as illustrated inFigure 1.
There is substantial subtopic proba-bility variation for peace within INTERNATIONALnews (the word usage is 50-times more likely167in INTERNATIONAL:MIDDLE-EAST than INTERNA-TIONAL:JAPAN).
We propose methods of hierarchicalsmoothing of P(w~ Itopict) in a topic-tree to capturethis subtopic variation robustly.1.1 Related WorkRecently, the speech community has begun to ad-dress the issue of topic in language modeling.
Lowe(1995) utilized the hand-assigned topic labels forthe Switchboard speech corpus to develop topic-specific language models for each of the 42 switch-board topics, and used a single topic-dependent lan-guage model to rescore the lists of N-best hypothe-ses.
Error-rate improvement over the baseline lan-guage model of 0.44% was reported.Iyer et al (1994) used bottom-up clustering tech-niques on discourse contexts, performing sentence-level model interpolation with weights updated y-namically through an EM-like procedure.
Evalu-ation on the Wall Street Journal (WSJ0) corpusshowed a 4% perplexity reduction and 7% word er-ror rate reduction.
In Iyer and Ostendorf (1996),the model was improved by model probability rees-timation and interpolation with a cache model, re-sulting in better dynamic adaptation and an overall22%/3% perplexity/error rate reduction due to bothcomponents.Seymore and Rosenfeld (1997) reported significantimprovements when using a topic detector to buildspecialized language models on the Broadcast News(BN) corpus.
They used TF-IDF and Naive Bayesclassifiers to detect he most similar topics to a givenarticle and then built a specialized language modelto rescore the N-best lists corresponding to the arti-cle (yielding an overall 15% perplexity reduction us-ing document-specific parameter re-estimation, andno significant word error rate reduction).
Seymoreet al (1998) split the vocabulary into 3 sets: gen-eral words, on-topic words and off-topic words, andthen use a non-linear interpolation to compute thelanguage model.
This yielded an 8% perplexity re-duction and 1% relative word error rate reduction.In collaborative work, Mangu (1997) investigatedthe benefits of using existing an Broadcast Newstopic hierarchy extracted from topic labels as a ba-sis for language model computation.
Manual treeconstruction and hierarchical interpolation yieldeda 16% perplexity reduction over a baseline uni-gram model.
In a concurrent collaborative effort,Khudanpur and Wu (1999) implemented clusteringand topic-detection techniques similar on those pre-sented here and computed a maximum entropy topicsensitive language model for the Switchboard cor-pus, yielding 8% perplexity reduction and 1.8% worderror rate reduction relative to a baseline maximumentropy trigram model.2 The  DataThe data used in this research isthe Broadcast News(BN94) corpus, consisting of radio and TV newstranscripts form the year 1994.
From the total of30226 documents, 20226 were used for training andthe other 10000 were used as test and held-out data.The vocabulary size is approximately 120k words.3 Optimizing Document  Cluster ingfor Language Model ingFor the purpose of language modeling, the topic la-bels assigned to a document or segment of a doc-ument can be obtained either manually (by topic-tagging the documents) or automatically, by usingan unsupervised algorithm to group similar docu-ments in topic-like clusters.
We have utilized thelatter approach, for its generality and extensibility,and because there is no reason to believe that themanually assigned topics are optimal for languagemodeling.3.1 Tree Generat ionIn this study, we have investigated a range of hierar-chical clustering techniques, examining extensions ofhierarchical gglomerative clustering, k-means clus-tering and top-down EM-based clustering.
The lat-ter underperformed on evaluations in Florian (1998)and is not reported here.A generic hierarchical gglomerative clustering al-gorithm proceeds as follows: initially each documenthas its own cluster.
Repeatedly, the two closest clus-ters are merged and replaced by their union, untilthere is only one top-level cluster.
Pairwise docu-ment similarity may be based on a range of func-tions, but to facilitate comparative analysis we haveutilized standard cosine similarity (d(D1,D2) =<D1,D2~ ) and IR-style term vectors (see Salton IIDx Ih liD2 Ihand McGill (1983)).This procedure outputs a tree in which documentson similar topics (indicated by similar term content)tend to be clustered together.
The difference be-tween average-linkage and maximum-linkage algo-rithms manifests in the way the similarity betweenclusters is computed (see Duda and Hart (1973)).
Aproblem that appears when using hierarchical c us-tering is that small centroids tend to cluster withbigger centroids instead of other small centroids, of-ten resulting in highly skewed trees such as shownin Figure 2, a=0.
To overcome the problem, we de-vised two alternative approaches for computing theintercluster similarity:?
Our first solution minimizes the attraction oflarge clusters by introducing a normalizing fac-tor a to the inter-cluster distance function:< c(C1),c(C2) >d(C1,C2) = N(C1), ~ Ilc(C,)ll N(C2) ~ IIc(C2)ll (2)168a=O a = 0.3 a = 0.5Figure 2: As a increases, the trees become morebalanced, at the expense of forced clusteringe=0 e = 0.15 e = 0.3 e = 0.7Figure 3: Tree-balance is also sensitive to thesmoothing parameter e.3.2 Opt imiz ing  the  H ierarch ica l  S t ructureTo be able to compute accurate language models,one has to have sufficient data for the relative fre-quency estimates to be reliable.
Usually, even withenough data, a smoothing scheme is employed to in-sure that P (wdw~ -1) > 0 for any given word sequencew~.The trees obtained from the previous step havedocuments in the leaves, therefore not enough wordmass for proper probability estimation.
But, on thepath from a leaf to the root, the internal nodes growin mass, ending with the root where the counts fromthe entire corpus are stored.
Since our intention is touse the full tree structure to interpolate between thein-node language models, we proceeded to identifya subset of internal nodes of the tree, which containsufficient data for language model estimation.
Thecriteria of choosing the nodes for collapsing involvesa goodness function, such that the cut I is a solu-tion to a constrained optimization problem, giventhe constraint that the resulting tree has exactly kleaves.
Let this evaluation function be g(n), wheren is a node of the tree, and suppose that we wantto minimize it.
Let g(n, k) be the minimum cost ofcreating k leaves in the subtree of root n. When theevaluation function g (n) satisfies the locality con-dition that it depends olely on the values g (nj,.
),(where (n#)j_ 1kare the children of node n), g (root)can be coml)uted efficiently using dynamic program-ming 2 :where N (Ck) is the number of vectors (docu-ments) in cluster Ck and c (Ci) is the centroidof the i th cluster.
Increasing a improves treebalance as shown in Figure 2, but as a becomeslarge the forced balancing degrades cluster qual-ity.A second approach we explored is to performbasic smoothing of term vector weights, replac-ing all O's with a small value e. By decreasinginitial vector orthogonality, this approach facili-tates attraction to small centroids, and leads tomore balanced clusters as shown in Figure 3.Instead of stopping the process when the desired?
number of clusters is obtained, we generate the fulltree for two reasons: (1) the full hierarchical struc-ture is exploited in our language models and (2) oncethe tree structure is generated, the objective func-tion we used to partition the tree differs from thatused when building the tree.
Since the clusteringprocedure turns out to be rather expensive for largedatasets (both in terms of time and memory), only10000 documents were used for generating the initialhierarchical structure.
?Section 3.2 describes the choice of optimum a.gCn, 1) = g(n)g(n, k) = min h (g (n l ,  j l ) , .
.
*  , g (n/c, jk))(3)j l , , j k  > 1Let us assume for a moment hat we are inter-ested in computing a unigram topic-mixture lan-guage model.
If the topic-conditional distributionshave high entropy (e.g.
the histogram of P(wltopic )is fairly uniform), topic-sensitive language model in-terpolation will not yield any improvement, no mat-ter how well the topic detection procedure works.Therefore, we are interested in clustering documentsin such a way that the topic-conditional distributionP(wltopic) is maximally skewed.
With this in mind,we selected the evaluation function to be the condi-tional entropy of a set of words (possibly the wholevocabulary) given the particular classification.
Theconditional entropy of some set of words )~V given apartition C isHCWIC) = ~ PCC~) ~ P(wlC,).
log(P(wlC,))i=1 wEWCIC d= ~ ~ ~_, cCw, C,).
logCP(wlC,)) (4)i=1  wEWnC i1the collection of nodes that collapse2h is an operator through which the valuesg (nl,jl) ..... g (nk,jk) are combined, as ~ or YI1695.555.55.455A5.355.35.25323.135.15.05Ccad~tiooal F.~opy in the Avenge-Linkage Case, u , I n 64 C in~ - -77 CinSlCn ......100 clus, ters .
.
.
.
.~ ;.................'" ..................................................I "'1' I I0.l 0.2 0-~ 0.4 ~5 01.63.853.83.753.70.7Couditinnal Eam~py inin?
Maximum.Linkage Case3.653.63.550n77 dusters .
.
.
.
.
."'".,..
....., ........"'-.,.
................... ...'".
.
.
.
.
.
-?
.
.
.
.
.
.
.
.
.
.
.
.
."
"~.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?.-?
**?I I I0., 0.2 03 01.4 01., 01.6(I 0.7Figure 4: Conditional entropy for different a, cluster sizes and linkage methodswhere c (w, Ci) is the TF-IDF factor of word w inclass Ci and T is the size of the corpus.
Let usobserve that the conditional entropy does satisfy thelocality condition mentioned earlier.Given this objective function, we identified the op-timal tree cut using the dynamic-programming tech-nique described above.
We also optimized ifferentparameters ( uch as a and choice of linkage method).Figure 4 illustrates that for a range of cluster sizes,maximal linkage clustering with a=0.15-0.3 yieldsoptimal performance given the objective function inequation (2).The effect of varying a is also shown graphically inFigure 5.
Successful tree construction for languagemodeling purposes will minimize the conditional en-tropy of P (~VIC).
This is most clearly illustratedfor the word politics, where the tree generated witha = 0.3 maximally focuses documents on this topicinto a single cluster.
The other words shown alsoexhibit this desirable highly skewed distribution ofP (}4;IC) in the cluster tree generated when a = 0.3.Another investigated approach was k-means clus-tering (see Duda and Hart (1973)) as a robust andproven alternative to hierarchical c ustering.
Its ap-plication, with both our automatically derived clus-ters and Mangn's manually derived clusters (Mangn(1997)) used as initial partitions, actually ielded asmall increase in conditional entropy and was notpursued further.4 Language Model Construction andEvaluationEstimating the language model probabilities is atwo-phase process.
First, the topic-sensitive lan-i - -1  gnage model probabilities P (wilt, wi_,~+~ ) are com-puted during the training phase.
Then, at run-time,or in the testing phase, topic is dynamically iden-tified by computing the probabilities P (tlw~ -1) asin section 4.2 and the final language model proba-bilities are computed using Equation (1).
The treeused in the following experiments was generated us-ing average-linkage agglomerative clustering, usingparameters that optimize the objective function inSection 3.4.1 Language Mode l  Const ruct ionThe topic-specific language model probabilities arecomputed in a four phase process:1.
Each document is assigned to one leaf in thetree, based on the similarity to the leaves' cen-troids (using the cosine similarity).
The doc-ument counts are added to the selected leaf'scount.2.
The leaf counts are propagated up the tree suchthat, in the end, the counts of every inter-nal node are equal to the sum of its children'scounts.
At this stage, each node of the tree hasan attached language model - the relative fre-quencies.3.
In the root of the tree, a discounted Good-Turing language model is computed (see Katz(1987), Chen and Goodman (1998)).4. m-gram smooth language models are computedfor each node n different han the root bythree-way interpolating between the m-gramlanguage model in the parent parent(n), the(m - 1)-gram smooth language model in noden and the m-gram relativeffrequency estimatein node n:-1) =~1 \[wm--l~ .
1 J par.
t(.
)(wmlw; (5)( ml 7+.xs.
(w~ '-~) f .
(w~lw?
-1)with  + + =for each node n in the tree.
Based on how~k (w~,-1) depend on the particular node n andthe word history w~ -1, various models can beobtained.
We investigated two approaches: abigram model in which the ,k's are fixed overthe tree, and a more general trigram model in170Case 1: fnode (Wl) ~ 0P root (w2 lw l ),~1 fnode (w21wl) "?node (Wl) + ,~2/~node (W,.
)Pnode (I/\]211?1) = -~ (1 -- )~1 -- ~2) Pp .
.
.
.
t(node) (~21~)~.ode (~I) Pnode (~2)where?node (flY1) =if w2 E ~'(~O1)if w2 E 7~(Wl)if w2 E/-4 (wl)w2 E~'(tOl) w2E3~(Wl)(1-F-/3) y \ ] .
fnode(W21Wl) '  Otnode (I#1) = )-,2e~(,,1) 0+~) - ~ P,,ode ("2)tv2 E 3c(1~'1 ) U'R.
( tv I )?
Case 2: fnode (Wl) = 0I P root (w=lwl) if w2 E ~(Wl)~2Pnode (~O2) ''}'node (101)Pnode (w2lwl) = + (1 -- AS) Pp .
.
.
.
t (node)  (w2lwl) if w2 e "R. (Wl)anode (I/31) Pnode (W2) if W2 e/4 (wl)where ?node (I/)1) and anode (I/31) are computed in a similar fashion such that the probabilities do sum to 1.Figure 5: Basic Bigram Language Model Specificationswhich A's adapt using an EM reestimation pro-cedure.4.1.1 B igram Language Mode lNot all words are topic sensitive.
Mangu (1997) ob-served that closed-class function words (FW), suchas the, of, and with, have minimal probability vari-ation across different opic parameterizations, whilemost open-class content words (CW) exhibit sub-stantial topic variation.
This leads us to divide thepossible word pairs in two classes (topic-sensitiveand not) and compute the A's in Equation (5) insuch a way that the probabilities in the former setare constant in all the models.
To formalize this:* Y (Wl )  = {w2 ?
~1 (Wl,W2) is fixed}-the'Taxed" space;?
T~(Wl) = {w2 ?
"~l (Wl,W2) is free/variable}-the '~ree" space;?
b/(Wl) = {w2 ?
121 (Wl,W2) was never seen}-the "unknown" space.The imposed restriction is, then: for every wordwland any word w2 ?
Y (wl )  Pn(w21wl) =Proof (w21wl) in any node n.The distribution of bigrams in the training datais as follows, with roughly 30% bigram probabilitiesallowed to vary in the topic-sensitive models:This approach raises one interesting issue: thelanguage model in the root assigns some probabil-ity mass to the unseen events, equal to the single-tons' mass (see Good (1953),Katz (1987)).
In ourcase, based on the assumptions made in the Good-Turing formulation, we considered that the ratio ofthe probability mass that goes to the unseen eventsand the one that goes to seen, free events hould beModelf ixedfixedfreefreeB igrsm-type  Exsmplep(FWIFW) p(thel~)p(FWICW) ~,(o.t'i.e.,~a,'io)p(CWICW) p(air lco/d)n(CWlFW) n(oi,.Ith=)Freq.45.3~ Iesst top ic  sens i t ive24.8~ .t5.3% .t24.5~ most topic  sens i t ivefixed over the nodes of the tree.
Let/3 be this ratio.Then the language model probabilities are computedas in Figure 5.4.1.2 Ngram Language Mode l  Smooth ingIn general, n gram language model probabili-ties can be computed as in formula (5), where(A~ (w"'-~'J'l are adapted both for the partic-~.
1 I / k -~ l .
.
.3ular node n and history w~ -1.
The proposed de-pendency on the history is realized through the his-tory count c (w~'-1) and the relevance of the historyw~ -1 to the topic in the nodes n and parent (n).The intuition is that if a history is as relevant in thecurrent node as in the parent, then the estimates inthe parent should be given more importance, sincethey are better estimated.
On the other hand, if thehistory is much more relevant in the current node,then the estimates in the node should be trustedmore.
The mean adapted A for a given height his the tree is shown in Figure 6.
This is consistentwith the observation that splits in the middle of thetree tend to be most informative, while those closerto the leaves suffer from data fragmentation, andhence give relatively more weight to their parent.As before, since not all the m-grams are expected tobe topic-sensitive, we use a method to insure thatthose rn grams are kept 'Taxed" to minimize noiseand modeling effort.
In this case, though, 2 lan-guage models with different support are used: one171It is at least on the Serb side a real setback to thepeacea3cA~ o.~oTopi?
ID0.0160.014"~ 0.012, .~  0.01o.l~leo.oo4o' I t ~11 P~ce~c I history) II?
- -  ,n  _ l  I I  - ?
, b - -  n .m_  In0 2O 3O 4o f*opiece~3: o.2o.ls"~ o.!~ o.o5oTopic ID0.00060.0005~ 0.0004P(piccc I history)Figure 7: Topic sensitive probability estimation for peace and piece in context"~ 0.8"J 0.60.40.2I I I I4 5 6 7 sNode HeightFigure 6: Mean of the estimated As at node heighth, in the unigram casethat supports the topic insensitive m-grams and thatis computed only once (it's a normalization of thetopic-insensitive part of the overall model), and onethat supports the rest of the mass and which is com-puted by interpolation using formula (5).
Finally,the final language model in each node is computedas a mixture of the two.4.2 Dynamic  Topic AdaptationConsider the example of predicting the word follow-ing the Broadcast News fragment: "It is at least onthe Serb side a real drawback to the ~-?--~'.
Our topicdetection model, as further detailed later in this sec-tion, assigns a topic distribution to this left context(including the full previous discourse), illustrated inthe upper portion of Figure 7.
The model identi-fies that this particular context has greatest affinitywith the empirically generated topic clusters #41and #42 (which appear to have one of their foci oninternational events).The lower portion of Figure 7 illustrates the topic-conditional bigram probabilities P(w\[the, topic) fortwo candidate hypotheses for w: peace (the actu-ally observed word in this case) and piece (an in-correct competing hypothesis).
In the former case,P(peace\[the, topic) is clearly highly elevated in themost probable topics for this context (#41,#42),and thus the application of our core model combi-nation (Equation 1) yields a posterior joint productP (w, lw~ -1) = ~'~K= 1P ($lw~-l) ?
Pt (w, lw~_-~+l) that is12-times more likely than the overall bigram proba-bility, P(air\[the) = 0.001.
In contrast, the obviousaccustically motivated alternative piece, has great-est probability in a far different and much more dif-fuse distribution of topics, yielding a joint modelprobability for this particular context that is 40%lower than its baseline bigram probability.
Thiscontext-sensitive adaptation illustrates the efficacyof dynamic topic adaptation i  increasing the modelprobability of the truth.Clearly the process of computing the topic de-tector P (tlw~ -1) is crucial.
We have investigatedseveral mechanisms for estimating this probability,the most promising is a class of normalized trans-formations of traditional cosine similarity betweenthe document history vector w~ -x and the topic cen-troids:P (tlw~-') = f (Cosine-Sire (t,w~-i))f (Cosine-Sire (t', w~-l)) (6)tlOne obvious choice for the function f would be theidentity.
However, considering a linear contribution172Language Perplexity on Perplexity onMode l  the  ent i re  the  targetvocabulary vocabularyStandard  B igram Mode l  215 584H is tory  s ize Sca led1005OO0.2 50005000yes1000 yesyes*yesno5000 yes5000 yesg(x) f (x)  k-NNX X ~ -X X Z -X* X Z* -*1 x -X ~z _x x z 15-NNe z ~e z -206195192 (-10%)460405389(-33%)202 444193 394192 390196 411Table 1: Perplexity results for topic sensitive bigram language model, different history lengthsof similarities poses a problem: because topic de-tection is more accurate when the history is long,even unrelated topics will have a non-trivial contri-bution to the final probability 3, resulting in poorerestimates.One class of transformations weinvestigated, thatdirectly address the previous problem, adjusts thesimilarities uch that closer topics weigh more andmore distant ones weigh less.
Therefore, f is chosensuch thatI(=~} < ~-~ for ~E1 < X2 ?~s?
.~) -  ~ - (7)f(zl) < for zz < z2X I ~ ag 2that is, ~ should be a monotonically increas-ing function on the interval \[0, 1\], or, equivalentlyf (x) = x.  g (x), g being an increasing function on\[0,1\].
Choices for g(x) include x, z~(~f > 0), log (z),e z .Another way of solving this problem is through thescaling operator f '  (xi) = ,~-mm~ By apply- max z i  - -min  z i  "ing this operator, minimum values (corresponding tolow-relevancy topics) do not receive any mass at all,and the mass is divided between the more relevanttopics.
For example, a combination of scaling andg(x) = x ~ yields:p( jlwi-l!
=($ im( 'w~- - l ' t ' ) - -min~Sim( 'w~- - l ' tk )  )"Y(8)A third class of transformations we investigatedconsiders only the closest k topics in formula (6)and ignores the more distant opics.4.3 Language Mode l  Eva luat ionTable 1 briefly summarizes a larger table of per-formance measured on the bigram implementation3Due to  un impor tant  word  co -occur rencesof this adaptive topic-based LM.
For the defaultparameters (indicated by *), a statistically signif-icant overall perplexity decrease of 10.5% was ob-served relative to a standard bigram model mea-sured on the same 1000 test documents.
System-atically modifying these parameters, we note thatperformance is decreased by using shorter discoursecontexts (as histories never cross discourse bound-aries, 5000-word histories essentially correspond tothe full prior discourse).
Keeping other parame-ters constant, g(x) = x outperforms other candidatetransformations g(x) = 1 and g(x) = e z. Absenceof k-nn and use of scaling both yield minor perfor-mance improvements.It is important to note that for 66% of the vo-cabulary the topic-based LM is identical to the corebigram model.
On the 34% of the data that falls inthe model's target vocabulary, however, perplexityreduction isa much more substantial 33.5% improve-ment.
The ability to isolate a well-defined targetsubtask and perform very well on it makes this workespecially promising for use in model combination.5 Conc lus ionIn this paper we described a novel method of gen-erating and applying hierarchical, dynamic topic-based language models.
Specifically, we have pro-posed and evaluated hierarchical cluster genera-tion procedures that yield specially balanced andpruned trees directly optimized for language mod-eling purposes.
We also present a novel hierar-chical interpolation algorithm for generating a lan-guage model from these trees, specializing in thehierarchical topic-conditional probability estimationfor a target opic-sensitive ocabulary (34% of theentire vocabulary).
We also propose and evalu-ate a range of dynamic topic detection proceduresbased on several transformations of content-vectorsimilarity measures.
These dynamic estimations ofP(topici \[history) are combined with the hierarchicalestimation of P(word j  Itopici, history) in a productacross topics, yielding a final probability estimate173of P(wordj Ihistory) that effectively captures long-distance lexical dependencies via these intermediatetopic models.
Statistically significant reductions inperplexity are obtained relative to a baseline model,both on the entire text (10.5%) and on the targetvocabulary (33.5%).
This large improvement on areadily isolatable subset of the data bodes well forfurther model combination.AcknowledgementsThe research reported here was sponsored by Na-tional Science Foundation Grant IRI-9618874.
Theauthors would like to thank Eric Brill, Eugene Char-niak, Ciprian Chelba, Fred Jelinek, Sanjeev Khudan-pur, Lidia Mangu and Jun Wu for suggestions andfeedback during the progress of this work, and An-dreas Stolcke for use of his hierarchical clusteringtools as a basis for some of the clustering softwaredeveloped here.ReferencesP.
Brown, J. Cocke, S. Della Pietra, V. Della Pietra,F.
Jelinek, J. Lafferty, R. Mercer, and P. Roossin'.1990.
A statistical approach to machine transla-tion.
Computational Linguistics, 16(2).Ciprian Chelba and Fred Jelinek.
1998.
Exploitingsyntactic structure for language modeling.
In Pro-ceedings COLING-ACL, volume 1, pages 225-231,August.Stanley F. Chen and Joshua Goodman.
1998.An empirical study of smoothing techinques forlanguage modeling.
Technical Report TR-10-98,Center for Research in Computing Technology,Harvard University, Cambridge, Massachusettes,August.Richard O. Duda and Peter E. Hart.
1973.
PaternClassification and Scene Analysis.
John Wiley &Sons.R~u Florian.
1998.
Exploiting nonlo-cal word relationships in language mod-els.
Technical report, Computer ScienceDepartment, Johns Hopkins University.http://nlp.cs.jhu.edu/-rflorian/papers/topic-lm-tech-rep.ps.J.
Good.
1953.
The population of species and theestimation of population parameters.
Biometrica,40, parts 3,4:237-264.Rukmini Iyer and Mari Ostendorf.
1996.
Modelinglong distance dependence in language: Topic mix-tures vs. dynamic cache models.
In Proceedingsof the International Conferrence on Spoken Lan-guage Processing, volume 1, pages 236-239.Rukmini Iyer, Mari Ostendorf, and J. RobinRohlicek.
1994.
Language modeling withsentence-level mixtures.
In Proceedings ARPAWorkshop on Human Language Technology, pages82-87.Slava Katz.
1987.
Estimation of probabilities fromsparse data for the language model componentof a speech recognizer.
In IEEE Transactions onAcoustics, Speech, and Signal Processing, 1987,volume ASSP-35 no 3, pages 400-401, March1987.Sanjeev Khudanpur and Jun Wu.
1999.
A maxi-mum entropy language model integrating n-gramand topic dependencies for conversational speechrecognition.
In Proceedings on ICASSP.R.
Kuhn and R. de Mori.
1992.
A cache based nat-ural language model for speech recognition.
IEEETransaction PAMI, 13:570-583.R.
Lau, Ronald Rosenfeld, and Salim Roukos.
1993.Trigger based language models: a maximum en-tropy approach.
In Proceedings ICASSP, pages45-48, April.S.
Lowe.
1995.
An attempt at improving recognitionaccuracy on switchboard by using topic identifi-cation.
In 1995 Johns Hopkins Speech Workshop,Language Modeling Group, Final Report.Lidia Mangu.
1997.
Hierarchical topic-sensitivelanguage models for automatic speech recog-nition.
Technical report, Computer Sci-ence Department, Johns Hopkins University.http://nlp.cs.jhu.edu/-lidia/papers/tech-repl .ps.Ronald Rosenfeld.
1994.
A hybrid approach toadaptive statistical language modeling.
In Pro-ceedings ARPA Workshop on Human LanguageTechnology, pages 76-87.G.
Salton and M. McGill.
1983.
An Introduc-tion to Modern Information Retrieval.
New York,McGram-Hill.Kristie Seymore and Ronald Rosenfeld.
1997.
Usingstow topics for language model adaptation.
InEuroSpeech97, volume 4, pages 1987-1990.Kristie Seymore, Stanley Chen, and Ronald Rosen-feld.
1998.
Nonlinear interpolation of topic mod-els for language model adaptation.
In Proceedingsof ICSLP98.J.
H. Wright, G. J. F. Jones, and H. Lloyd-Thomas.1993.
A consolidated language model for speechrecognition.
In Proceedings EuroSpeech, volume 2,pages 977-980.174
