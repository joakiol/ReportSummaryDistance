Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 187?195,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsAIDArabicA Named-Entity Disambiguation Framework for Arabic TextMohamed Amir Yosef, Marc Spaniol, Gerhard WeikumMax-Planck-Institut f?ur Informatik, Saarbr?ucken, Germany{mamir|mspaniol|weikum}@mpi-inf.mpg.deAbstractThere has been recently a great progress inthe field of automatically generated knowl-edge bases and corresponding disambigua-tion systems that are capable of mappingtext mentions onto canonical entities.
Ef-forts like the before mentioned have en-abled researchers and analysts from vari-ous disciplines to semantically ?understand?contents.
However, most of the approacheshave been specifically designed for the En-glish language and - in particular - sup-port for Arabic is still in its infancy.
Sincethe amount of Arabic Web contents (e.g.in social media) has been increasing dra-matically over the last years, we see agreat potential for endeavors that supportan entity-level analytics of these data.
Tothis end, we have developed a frameworkcalled AIDArabic that extends the existingAIDA system by additional componentsthat allow the disambiguation of Arabictexts based on an automatically generatedknowledge base distilled from Wikipedia.Even further, we overcome the still exist-ing sparsity of the Arabic Wikipedia by ex-ploiting the interwiki links between Arabicand English contents in Wikipedia, thus,enriching the entity catalog as well as dis-ambiguation context.1 Introduction1.1 MotivationInternet data including news articles and web pages,contain mentions of named-entities such as people,places, organizations, etc.
While in many casesthe intended meanings of the mentions is obvi-ous (and unique), in many others, the mentionsare ambiguous and have many different possiblemeanings.
Therefore, Named-Entity Disambigua-tion (NED) is essential for many application in thedomain of Information Retrieval (such as informa-tion extraction).
It also enables producing moreuseful and accurate analytics.
The problem hasbeen exhaustively studied in the literature.
Theessence of all NED techniques is using backgroundinformation extracted from various sources (e.g.Wikipedia), and use such information to know thecorrect/intended meaning of the mention.The Arabic content is enormously growing onthe Internet, nevertheless, background ground in-formation is clearly lacking behind other languagessuch as English.
Consider Wikipedia for example,while the English Wikipedia contains more than 4.5million articles, the Arabic version contains lessthan 0.3 million ones1.
As a result, and up to ourknowledge, there is no serious work that has beendone in the area of performing NED for Arabicinput text.1.2 Problem statementNED is the problem of mapping ambiguous namesof entities (mentions) to canonical entities regis-tered in an entity catalog (knowledgebase) such asFreebase (www.freebase.com), DBpedia (Auer etal., 2007), or Yago (Hoffart et al., 2013).
For ex-ample, given the text ?I like to visit Sheikh Zayed.Despite being close to Cairo, it is known to be aquiet district?, or in Arabic,?
qJ??
@?PA KP I.k@??
A ?E.Q???
?
?Q ?A K.Z?Y?
?A K.Q?JK ????
.
Y K@P?Q?A??
@?.
When processing this text automatically,we need to be able to tell that Sheikh Zayed de-notes the the city in Egypt2, not the mosque inAbu Dhabi3or the President of the United Arab1as of July 20142http://en.wikipedia.org/wiki/Sheikh Zayed Cityhttp://ar.wikipedia.org/wiki/YK@P_ qJ?.?
@_?JKY?3http://en.wikipedia.org/wiki/Sheikh Zayed Mosquehttp://ar.wikipedia.org/wiki/YK@P_ qJ?.?
@_ ??Ag.187Emirates4.
In order to automatically establish suchmappings, the machine needs to be aware of thecharacteristic description of each entity, and try tofind the most suitable one given the input context.In our example, knowing that the input text men-tioned the city of Cairo favors the Egyptian cityover the mosque in Abu Dhabi, for example.
Inprinciple, state-of-the-art NED frameworks requiremain four ingredients to solve this problem:?
Entity Repository: A predefined universalcatalog of all entities known to the NEDframework.
In other words, each mention inthe input text must be mapped to an entity inthe repository, or to null indicating the correctentity is not included in the repository.?
Name-Entity Dictionary: It is a many-to-many relation between possible mentions andthe entities in the repository.
It connects anentity with different possible mentions thatmight be used to refer to this entity, as well asconnecting a mention with all potential candi-date entity it might denote.?
Entity-Descriptions: It keeps per entity abag of characteristic keywords or keyphrasesthat distinguishes an entity from another.
Inaddition, they come with scoring scheme thatsignify the specificity of such keyword to thatentity.?
Entity-Entity Relatedness Model: For co-herent text, the entities that are used for map-ping all the mentions in the input text, shouldbe semantically related.
For that reason, anentity-entity relatedness model is required toasses the coherence.For the English language, all of the ingredi-ents mentioned above are richly available.
Forinstance, the English Wikipedia is a comprehen-sive up-to-date resource.
Many NED systemsuse Wikipedia as their entity repository.
Further-more, many knowledge bases are extracted fromWikipedia as well.
When trying to apply the exist-ing NED approaches on the Arabic text, we facethe following challenges:?
Entity Repository: There is no such compre-hensive entity catalog.
Arabic Wikipedia is an4http://en.wikipedia.org/wiki/Zayed bin Sultan Al Nahyanhttp://ar.wikipedia.org/wiki/?AJ?E_ ?@_?A??
?_?K._ YK@Porder of magnitude smaller than the Englishone.
In addition, many entities in the ArabicWikipedia are specific to the Arabic culturewith no corresponding English counterpart.As a consequence, even many prominent enti-ties are missing from the Arabic Wikipedia.?
Name-Entity Dictionary: Most of the name-entity dictionary entries originate from man-ual input (e.g.
anchor links).
Like outlinedbefore, Arabic Wikipedia has fewer resourcesto extract name-entity mappings, caused bythe lack of entities and lack of manual input.?
Entity-Descriptions: As already mentioned,there is a scarcity of anchor links in the ArabicWikipedia.
Further, the categorization systemof entities is insufficient, Both are essentialsources of building the entities descriptions.Hence, it is more challenging to produce com-prehensive description of each entity.?
Entity-Entity Relatedness Model: Related-ness estimation among entities is usually com-puted using the overlap in the entities descrip-tion and/or link structure of Wikipedia.
Due tothe previously mentioned scarcity of contentsin the Arabic Wikipedia, it is also difficult toaccurately estimate the entity-entity related-ness.As a consequence, the main challenge in per-forming NED on Arabic text is the lack of a com-prehensive entity catalog together with rich descrip-tions of each entity.
We considered our open sourceAIDA system5(Hoffart et al., 2011)- mentioned asstate-of-the-art NED System by (Ferrucci, 2012) -as a starting point and modified its data acquisitionpipeline in order to generate a schema suitable forperforming NED on Arabic text.1.3 ContributionWe developed an approach to exploit and fuse cross-lingual evidences to enrich the background informa-tion we have about entities in Arabic to build a com-prehensive entity catalog together with their con-text that is not restricted to the Arabic Wikipedia.Our contributions can be summarized in the follow-ing points:?
Entity Repository: We switched toYAGO3(Mahdisoltani et al., 2014), the5https://www.github.com/yago-naga/aida188multilingual version of YAGO2s.
YAGO3comes with a more comprehensive catalogthat covers entities from different languages(extracted from different Wikipedia dumps).While we selected YAGO3 to be our back-ground knowledge base, any multi-lingualknowledge base such as Freebase could beused as well.?
Name-Entity Dictionary: We compiled adictionary from YAGO3 and Freebase to pro-vide the potential candidate entities for eachmention string.
While the mention is in Ara-bic, the entity can belong to either the Englishor the Arabic Wikipedia.?
Entity-Descriptions: We harnessed differentingredients in YAGO3, and Wikipedia to pro-duce a rich entity context schema.
For thesake of precision, we did not employ any au-tomated translation.?
Entity-Entity Relatedness Model: Wefused the link structure of both the Englishand Arabic Wikipedia?s to compute a com-prehensive relatedness measure between theentities.2 Related WorkNED is one of the classical NLP problems thatis essential for many Information Retrieval tasks.Hence, it has been extensively addressed in NLPresearch.
Most of NED approaches use Wikipediaas their knowledge repository.
(Bunescu and Pasca,2006) defined a similarity measure that comparedthe context of a mention to the Wikipedia cate-gories of the entity candidate.
(Cucerzan, 2007;Milne and Witten, 2008; Nguyen and Cao, 2008)extended this framework by using richer featuresfor similarity comparison.
(Milne and Witten,2008) introduced the notion of semantic related-ness and estimated it using the the co-occurrencecounts in Wikipedia.
They used the Wikipedia linkstructure as an indication of occurrence.
Below,we give a brief overview on the most recent NEDsystems:The AIDA system is an open source systemthat employs contextual features extracted fromWikipedia (Hoffart et al., 2011; Yosef et al., 2011).It casts the NED problem into a graph problemwith two types of nodes (mention nodes, and en-tity nodes).
The weights on the edges between thementions and the entities are the contextual similar-ity between mention?s context and entity?s context.The weights on the edges between the entities arethe semantic relatedness among those entities.
In asubsequent process, the graph is iteratively reducedto achieve a dense sub-graph where each mentionis connected to exactly one entity.The CSAW system uses local scores computedfrom 12 features extracted from the context sur-rounding the mention, and the candidate entities(Kulkarni et al., 2009).
In addition, it computesglobal scores that captures relatedness among anno-tations.
The NED is then formulated as a quadraticprogramming optimization problem, which nega-tively affects the performance.
The software, how-ever, is not available.DBpedia Spotlight uses Wikipedia anchors, ti-tles and redirects to search for mentions in the inputtext (Mendes et al., 2011).
It casts the context of themention and the entity into a vector-space model.Cosine similarity is then applied to identify thecandidate with the highest similarity.
Nevertheless,their model did not incorporate any semantic relat-edness among entities.
The software is currentlyavailable as a service.TagMe 2 exploits the Wikipedia link structure toestimate the relatedness among entities (Ferraginaand Scaiella, 2010).
It uses the measure defined by(Milne and Witten, 2008) and incorporates a votingscheme to pick the right mapping.
According tothe authors, the system is geared for short inputtext with limited context.
Therefore, the approachfavors coherence among entities over contextualsimilarity.
TagMe 2 is available a service.Illinois Wikifier formulates NED as an opti-mization problem with an objective function de-signed for higher global coherence among all men-tions (Ratinov et al., 2011).
In contrast to AIDAand TagMe 2, it does not incorporate the link struc-ture of Wikipedia to estimate the relatedness amongentities.
Instead, it uses normalized Google sim-ilarity distance (NGD) and pointwise mutual in-formation.
The software is as well available as aservice.Wikipedia Miner is a machine-learning basedapproach (Milne and Witten, 2008).
It exploitsthree features in order to train the classifier.
Thefeatures it employs are prior probability that a men-tion refers to a specific entity, properties extractedfrom the mention context, and finally the entity-entity relatedness.
The software of Wikipedia189YAGO3EnglishWikipediaArabicWikipediaYAGOExtractorEntitiesDictionaryCategoriesDictionaryStandard AIDABuilderMixedAIDASchemaTranslatorMixedAIDASchemaFilterArabicAIDASchemaFreebaseFreebase-to-YAGODictionaryOriginal AIDA PipelineExtraction AIDA Schema Building Translation FiltrationFigure 1: AIDArabic ArchitectureMiner is available on their Website.The approaches mentioned before have been de-veloped for English language NED.
As such, noneof them is ready to handle Arabic input withoutmajor modification.As of now, no previous research exploits cross-lingual resources to enable NED for Arabic text.Nevertheless, cross-lingual resources have beenused to improve Arabic NER (Darwish, 2013).They used Arabic and English Wikipedia togetherwith DBpedia in order to build a large Arabic-English dictionary for names.
This augments theArabic names with a capitalization feature, whichis missing in the Arabic language.3 ArchitectureIn order to build AIDArabic, we have extended thepipeline used for building an English AIDA schemafrom the YAGO knowledge base.
The new archi-tecture is shown in Figure 1 and indicates thosecomponents, that have been added for AIDArabic.These are pre- and post-processing stages to theoriginal AIDA schema extractor.
The new pipelinecan be divided into the following stages:ExtractionWe have configured a dedicated YAGO3 extrac-tor to provide the data necessary for AIDAra-bic.
To this end, we feed the English and ArabicWikipedia?s into YAGO3 extractor to provide threemajor outputs:?
Entity Repository: A comprehensive set ofentities that exist in both, the English and Ara-bic Wikipedia?s.
In addition, the correspond-ing anchortexts, categories as well as linksfrom and/to each entity.?
Entity Dictionary: This is an automaticallycompiled mappings that captures the inter-wiki links among the English and the ArabicWikipedia?s.?
Categories Dictionary: This is also an auto-matically harvested list of mappings betweenthe English and Arabic Wikipedia categories.More details about data generated by each andevery extractor will be given in Section 4.AIDA Schema BuildingIn this stage we invoke the original AIDA schemabuilder without any language information.
How-ever, we additionally add the Freebase knowledgebase to AIDA and map Freebase entities to YAGO3entities.
Freebase is used here solely to harness itscoverage of multi-lingual names of different enti-ties.
It is worth noting that Freebase is used merelyto enrich YAGO3, but the set of entities are gath-ered from YAGO.
In other words, if there is anentity in Freebase without a YAGO counter part, itgets discarded.TranslationAlthough it is generally viable to use machine trans-lation or ?off the shelf?
English-Arabic dictionariesto translate the context of entities.
However, weconfine ourselves to the dictionaries extracted fromWikipedia that maps entities as well as categories190from English to Arabic.
This is done in order toachieve a high precision derived from the manuallabor inherent in interwiki links and assigned cate-gories.FiltrationThis is a final cleaning stage.
Despite translatingthe context of entities using the Wikipedia-baseddictionaries as comprehensive as possible, a con-siderable amount of context information remainsin English (e.g.
those English categories that donot have an Arabic counterpart).
To this end, anyremaining leftovers in English are being discarded.4 ImplementationThis section explains the implementation of thepipeline described in Section 3.
We first high-light the differences between YAGO2 and YAGO3,which justify the switch of the underlying knowl-edge base.
Then, we present the techniques wehave developed in order to build the dictionary be-tween mentions and candidate entities.
After that,we explain the context enrichment for Arabic enti-ties by exploiting cross-lingual evidences.
Finally,we briefly explain the entity-entity relatedness mea-sure applied for disambiguation.
In the followingtable (cf.
Table 1 for details) we summarize theterminology used in the following section.4.1 Entity RepositoryYAGO3 has been specifically designed as a multi-lingual knowledge base.
Hence, standard YAGO3extractors take as an input a set of Wikipedia dumpsfrom different languages, and produce a unifiedrepository of named entities across all languages.This is done by considering inter-wiki links.
If anentity in language l ?
L ?
{en} has an Englishcounter part, the English one is kept instead ofthat in language l, otherwise, the original entityis kept.
For example, in our repository, the entityused to represent Egypt is ?Egypt?
coming fromthe English Wikipedia instead of ?ar/Q????
comingfrom the Arabic Wikpedia.
However, the entity thatrefers to the western part of Cairo is identified as?ar/?Q?A??
@ H.Q??
because it has no counter-part inthe English Wikipedia.
Formally, the set of entitiesin YAGO3 are defined as follows:E = Een?
EarAfter the extraction is done, YAGO3 generatesan entity dictionary for each and every language.This dictionary translates any language specificentity into the one that is used in YAGO3 (whetherthe original one, or the English counter part).Based on the the previous example, the followingentries are created in the dictionary:ar/Q???
?
Egyptar/?Q?A??
@ H.Q?
?
ar/?Q?A??
@ H.Q?Such a dictionary is essential for all further pro-cessing we do over YAGO3 to enrich the Arabicknowledge base using the English one.
It is worthnoting here, that this dictionary is completely au-tomatically harvested from the inter-wiki links inWikipedia, and hence no automated machine trans-lation and/or transliteration are invoked (e.g.
forPerson Names, Organization Names, etc.).
Whilethis may harm the coverage of our linkage, it guar-antees the precision of our mapping at the sametime.
This is thanks to the high quality of inter-wiki between named-entities in Wikipedia.4.2 Name-Entity DictionaryThe dictionary in the context of NED refers to therelation that connects strings to canonical entities.In other words, given a mention string, the dictio-nary provides a list of potential canonical entitiesthis string may refer to.
In our original implemen-tation of AIDA, this dictionary was compiled fromfour sources extracted from Wikipedia (titles, dis-ambiguation pages, redirects, and anchor texts).We used the same sources after adapting them tothe Arabic domain, and added to them entries com-ing from Freebase.
In the following, we brieflysummarize the main ingredients used to populateour dictionary:?
Titles: The most natural possible name of acanonical entity is the title of its correspond-ing page in Wikipedia.
This is different fromthe entity ID itself.
For example, in our exam-ple for the entity ?Egypt?
that gets its id fromthe English Wikipeida, we consider the title?Q????
coming from the Arabic Wikipedia.?
Disambiguation Pages: These pagesare called in the Arabic Wikipedia?iJ??J?
@HAj???.
They are dedicatedpages to list the different possible meaningsof a specific name.
We harness all the linksin a disambiguation page and add them as191l A language in WikipediaL Set of all languages in WikipediaeenAn entity originated from the English WIkipediaearAn entity originated from the Arabic WIkipediae An entity in the final collection of YAGO3E Set of the corresponding entitiesCaten(e) Set of Categories of an entity e in the English WikipediaCatar(e) Set of Categories of an entity e in the Arabic WikipediaInlinken(e) Set of Incoming Links to an entity e in the English WikipediaInlinkar(e) Set of Incoming Links to an entity e in the Arabic WikipediaTrans(S) Translation of each element in S from English to Arabic using the appropriate dictionariesen?arTable 1: Terminologypotential entities for that name.
To this end,we extract our content solely from the ArabicWikipedia.
For instance, the phrase ??J KY?YK@P?
has a disambiguation page that lists allthe cities that all called Zayed including theones in Egypt, Bahrain and United Arab Emi-rates.?
Redirects: ?HCK?m'?
denotes redirects inArabic Wikipedia.
Those are pages whereyou search for a name and it redirects youto the most prominent meaning of this name.This we extract from the Arabic Wikipedia aswell.
For example, if you search in the ArabicWikipedia for the string ?YK@P?, you will be au-tomatically redirected to page of the presidentof the United Arabic Emirates.?
Anchor Text: When people create linksin Wikipedia, sometimes they use differentnames from the title of the entity page as an an-chor text.
This indicates that this new name isalso a possible name for that entity.
Therefore,we collect all anchors in the Arabic Wikipediaand associate them with the appropriate en-tities.
For example, in the Arabic Wikipediapage of Sheikh Zayed, there is a anchor linkto the city of Al Ain ?ar/???
@?, while the an-chor text reads ??JQ????
@ ???J??@?
(in English:?The Eastern Area?).
Therefore, when there isa mention called ?The Eastern Area?, one ofthe potential candidate meanings is the city ofAl-Ain in United Arab Emirates.?
Freebase: Freebase is a comprehensive re-source which comes with multi-lingual labelsof different entities.
In addition, there is aone-to-one mapping between (most of) Free-base entities and YAGO3 entities, becauseFreebase is extracted from Wikipedia as well.Therefore, we carry over the Arabic namesof the entities from Freebase to our AIDAschema after mapping the entities to their cor-responding ones in YAGO3.4.3 Entity-DescriptionsThe context of an entity is the cornerstone in thedata required to perform NED task with high qual-ity.
Having a comprehensive and ?clean?
contextfor each entity facilitates the task of the NED al-gorithm by providing good clues for the correctmapping.
We follow the same approach that weused in the original AIDA framework by repre-senting an entity context as a set of characteristickeyphrases that captures the specifics of such en-tity.
The keyphrases are further decomposed intokeywords with specificity scores assigned to eachof them in order to estimate the global and entity-specific prominence of this keyword.
The origi-nal implementation of AIDA extracted keyphrasesfrom 4 different sources (anchor text, inlink titles,categories, as well as citation titles and externallinks).
Below we summarize how we adopted theextraction to accommodate the disambiguation ofArabic text.?
Anchor Text: Anchors in a Wikipedia pageare usually good indicators of the most im-192portant aspects of that page.
In the originalimplementation of AIDA, all anchors in apage are associated with the correspondingentity of this page, and added to the set ofits keyphrases.The same holds for AIDAra-bic.
However, we extract the anchors from theArabic Wikipedia to get Arabic context.?
Inlink Titles: In the same fashion that linksto other entities are good clues for the aspectsof the entity, links coming from other entitiesare as well.
In AIDA, the set of the titlesof the pages that has links to an entity wereconsidered among the keyphrases of such anentity.
We pursued the same approach here,and fused incoming links to an entity fromboth English and Arabic Wikipedia.
Onceset of the incoming links was fully built, weapplied - when applicable - interwiki linksto get the translation of titles of the entitiescoming from the English Wikipedia into theArabic language.
Formally:Inlink(e) =Inlinkar(e)?Transen?ar(Inlinken(e))?
Categories: Each Wikipedia page belongs toone or more categories, which are mentionedat the bottom part of the page.
We configuredYAGO3 to provide the union of the categoriesfrom both, the English and Arabic Wikipedia.We exploit the interwiki links among cate-gories to translate the English categories toArabic.
This comes with two benefits, weuse the category mappings which result infairly accurate translation in contrast to ma-chine translation.
In addition, we enrich thecategory system of the Arabic Wikipedia bycategories from the English for entities thathave corresponding English counterpart.Cat(e) = Catar(e) ?
Transen?ar(Caten(e))?
Citation Titles and External Links: Thosewere two sources of entities context in theoriginal Wikipedia.
Due to the small coveragein the Arabic Wikipedia, we ignored them inAIDArabic.Table 2 summarizes which context resource hasbeen translated and/or enriched from the EnglishWikipedia.4.4 Entity-Entity Relatedness ModelFor coherent text, there should be connection be-tween all entities mentioned in the text.
In otherwords, a piece of text cannot cover too many as-pects at the same time.
Therefore, recent NED tech-niques exploit entity-entity relatedness to furtherimprove the quality of mapping mentions to enti-ties.
The original implementation of AIDA usedfor that purpose a measure introduced by (Milneand Witten, 2008) that estimates the relatednessor coherence between two entities using the over-lap in the incoming links to them in the EnglishWikipedia.Despite the cultural difference, it is fairly con-ceivable to assume that if two entities are related inthe English Wikipedia, they should also be relatedin the Arabic one.
In addition, we enrich the linkstructure used in AIDA with the link structure ofthe Arabic Wikipedia.
Hence, we estimate the relat-edness between entities using overlap in incominglinks in both the English and Arabic Wikipedia?stogether.5 Experimentation5.1 Setup and ResultsUp to our knowledge, there is no standard Arabicdata set available for a systematic evaluation ofNED.
In order to assess the quality of our system,we manually prepared a small benchmark collec-tion.
To this end, we gathered 10 news articles fromwww.aljazeera.net from the domains of sports andpolitics including regional as well as internationalnews.
We manually annotated the mentions in thetext, and disambiguated the text by using AIDAra-bic.
In our setup, we used the LOCAL configu-ration setting of AIDA together with the originalweights.
The data set contains a total of 103 men-tions.
AIDArabic managed to annotate 34 of themcorrectly, and assigned 68 to NULL, while onemention was mapped wrongly.5.2 DiscussionAIDArabic performance in terms of precision isimpressive (%97.1).
Performance in that regard ispositively influenced by testing on a ?clean?
inputof news articles.
Nevertheless, AIDArabic loses onrecall.
Mentions that are mapped to NULL, either193Context Source Arabic Wikipedia English WikipediaAnchor Text + -Categories + +Title of Incoming Links + +Table 2: Entities Context Sourceshave no correct entity in the entity repository, orthe entity exists but lacks the corresponding name-entity dictionary entry.This observation confirms our initial hypothe-sis that lack of data is one of the main challengesfor applying NED on Arabic text.
Another aspectthat harms recall is the nature of Arabic language.Letters get attached to the beginning and/or theend of words (e.g.
connected prepositions and pro-nouns).
In such a case, when querying the dictio-nary, AIDArabic is not able to retrieve the correctcandidates for a mention like ?
A?Q?K.
?, because ofthe ?H.?
in the beginning.
Similar difficulties arisewhen matching the entities description.
Here, manykeywords do not be to match the input text becausethey appear in a modified version augmented withsome extra letters.6 Conclusion & OutlookIn this paper, we have introduced the AIDArabicframework, which allows named entity disambigua-tion of Arabic texts based on an automatically gen-erated knowledge based derived from Wikipedia.Our proof-of-concept implementation shows thatentity disambiguation for Arabic texts becomes vi-able, although the underlying data sources (in par-ticular Wikipedia) still is relatively sparse.
Sinceour approach ?integrates?
knowledge encapsulatedin interwiki links from the English Wikipedia, weare able to boost the amount of context informa-tion available compared to a solely monolingualapproach.As a next step, intend to build up a properdataset that we will use for a systematic evalua-tion of AIDArabic.
In addition, we plan to applymachine translation/transliteration techniques forkeyphrases and/or dictionary lookup for keywordsin order to provide even more context informa-tion for each and every entity.
In addition, wemay employ approximate matching approaches forkeyphrases to account for the existence of addi-tional letter connected to words.
As a byproductwe will be able to apply AIDArabic on less formaltext (e.g.
social media) which contains a consid-erable amount of misspellings for example.
Apartfrom assessing and improving AIDArabic, a naturalnext step is to extend the framework by extractorsfor other languages, such as French or German.By doing so, we are going to create a framework,which will be in its final version fully languageagnostic.AcknowledgmentsWe would like to thank Fabian M. Suchanek andJoanna Biega for their help with adopting YAGO3extraction code to fulfill AIDArabic requirements.References[Auer et al.2007] S?oren Auer, Christian Bizer, GeorgiKobilarov, Jens Lehmann, and Zachary Ives.
2007.DBpedia: A nucleus for a web of open data.
In Pro-ceedings of the 6th Intl Semantic Web Conference,pages 11?15, Busan, Korea.
[Bunescu and Pasca2006] Razvan Bunescu and MariusPasca.
2006.
Using encyclopedic knowledge fornamed entity disambiguation.
In Proceedings ofthe 11th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL2006), pages 9?16, Trento, Italy.
[Cucerzan2007] S. Cucerzan.
2007.
Large-scalenamed entity disambiguation based on Wikipediadata.
In Proceedings of EMNLP-CoNLL 2007,pages 708?716, Prague, Czech Republic.
[Darwish2013] Kareem Darwish.
2013.
Named entityrecognition using cross-lingual resources: Arabic asan example.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(ACL 2013) , pages 1558?1567, Sofia, Bulgaria.
[Ferragina and Scaiella2010] Paolo Ferragina and UgoScaiella.
2010.
Tagme: On-the-fly annotation ofshort text fragments (by Wikipedia entities).
In Pro-ceedings of the 19th ACM International Conferenceon Information and Knowledge Management (CIKM2010), pages 1625?1628, New York, NY, USA.
[Ferrucci2012] D. A. Ferrucci.
2012.
Introduction to?This is Watson?.
IBM Journal of Research and De-velopment (Volume 56, Issue 3), pages 235?249.194[Hoffart et al.2011] Johannes Hoffart, Mohamed AmirYosef, Ilaria Bordino, Hagen F?urstenau, ManfredPinkal, Marc Spaniol, Bilyana Taneva, StefanThater, and Gerhard Weikum.
2011.
Robust disam-biguation of named entities in text.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2011), pages 782?792, Edinburgh, Scotland.
[Hoffart et al.2013] Johannes Hoffart, Fabian M.Suchanek, Klaus Berberich, and Gerhard Weikum.2013.
YAGO2: A spatially and temporally en-hanced knowledge base from Wikipedia.
ArtificialIntelligence (Volume 194), pages 28?61.
[Kulkarni et al.2009] Sayali Kulkarni, Amit Singh,Ganesh Ramakrishnan, and Soumen Chakrabarti.2009.
Collective annotation of Wikipedia entitiesin web text.
In Proceedings of the 15th ACM Inter-national Conference on Knowledge Discovery andData Mining (SIGKDD 2009), pages 457?466, NewYork, NY, USA.
[Mahdisoltani et al.2014] Farzane Mahdisoltani,Joanna Biega, and Fabian M. Suchanek.
2014.A knowledge base from multilingual Wikipedias?
yago3.
Technical report, Telecom ParisTech.http://suchanek.name/work/publications/yago3tr.pdf.
[Mendes et al.2011] Pablo N. Mendes, Max Jakob,Andr?es Garc?
?a-Silva, and Christian Bizer.
2011.DBbpedia Spotlight: Shedding light on the webof documents.
In Proceedings of the 7th In-ternational Conference on Semantic Systems ( I-Semantics 2011), pages 1?8, New York, NY, USA.
[Milne and Witten2008] David N. Milne and Ian H.Witten.
2008.
Learning to link with Wikipedia.
InProceedings of the 17th ACM International Confer-ence on Information and Knowledge Management(CIKM 2008), pages 509?518, New York, NY, USA.
[Nguyen and Cao2008] Hien T. Nguyen and Tru H.Cao.
2008.
Named entity disambiguation on an on-tology enriched by Wikipedia.
In Proceedings ofIEEE International Conference on Research, Inno-vation and Vision for the Future (RIVF 2008), pages247?254, Ho Chi Minh City, Vietnam.
[Ratinov et al.2011] Lev Ratinov, Dan Roth, DougDowney, and Mike Anderson.
2011.
Local andglobal algorithms for disambiguation to wikipedia.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies (HLT 2011), pages 1375?1384, Stroudsburg, PA, USA.
[Yosef et al.2011] Mohamed Amir Yosef, JohannesHoffart, Ilaria Bordino, Marc Spaniol, and GerhardWeikum.
2011.
AIDA: An online tool for accu-rate disambiguation of named entities in text and ta-bles.
In Proceedings of the 37th International Con-ference on Very Large Data Bases (VLDB 2011),pages 1450?1453, Seattle, WA, USA.195
