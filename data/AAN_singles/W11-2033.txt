Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 294?300,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsMulti-Policy Dialogue ManagementPierre LisonLogic and Natural Language GroupDepartment of InformaticsUniversity of Oslo, NorwayAbstractWe present a new approach to dialogue man-agement based on the use of multiple, inter-connected policies.
Instead of capturing thecomplexity of the interaction in a single largepolicy, the dialogue manager operates with acollection of small local policies combinedconcurrently and hierarchically.
The meta-control of these policies relies on an activationvector updated before and after each turn.1 IntroductionMany dialogue domains are naturally open-ended.This is especially the case in situated dialogue,where the conversational agent must operate in con-tinuously changing environments where there is of-ten no single, pre-specified goal to achieve.
De-pending on the situation and the (perceived) user re-quests, many distinct tasks may be performed.
Forinstance, a service robot for the elderly might beused for cleaning, monitoring health status, and de-livering information.
Each of these tasks features aspecific set of observations, goals, constraints, inter-nal dynamics, and associated actions.This diversity of tasks and models poses signif-icant challenges for dialogue systems, and particu-larly for dialogue management.
Open-ended inter-actions are indeed usually much more difficult tomodel than classical slot-filling applications, wherethe application domain can provide strong con-straints on the possible dialogue transitions.
Usingmachine learning techniques to learn the model pa-rameters can help alleviate this issue, but only if thetask can be efficiently factored and if a sufficientamount of data is available.
Once a model of theinteraction and its associated environment is avail-able, a control policy then needs to be learned ordesigned for the resulting state space.
The extrac-tion of good control policies can be computationallychallenging, especially for interactions which si-multaneously combine partial observability (to dealwith noisy and incomplete observations) and largestate spaces (if the optimal behaviour depends on awide range of user- and context-specific factors) ?which is the case for many open-ended domains.In this paper, we present ongoing work on a newapproach to dialogue management which seeks toaddress these issues by leveraging prior knowledgeabout the interaction structure to break up the fulldomain into a set of smaller, more predictable sub-domains.
Moving away from the idea of capturingthe full interaction complexity into a unique, mono-lithic policy, we extend the execution algorithm ofthe dialogue manager to directly operate with a col-lection of small, interconnected local policies.Viewing dialogue management as a decision pro-cess over multiple policies has several benefits.First, it is usually easier for the application devel-oper to model several small, local interactions thana single large one.
Each local model can also be in-dependently modified, extended or replaced withoutinterfering with the rest of the system, which is cru-cial for system maintenance.
Finally, different the-oretical frameworks can be used for different poli-cies, which means that the developer is free to decidewhich approach is most appropriate to solve a spe-cific problem, without having to commit to a uniquetheoretical framework for the whole application.
Forinstance, one policy might be expressed as a solu-tion to a Partially Observable Markov Decision Pro-cess (POMDP) while another policy is encoded as a294hand-crafted finite-state controller, and the two canbe integrated in the same control algorithm.One of the challenges when operating with mul-tiple policies is the ?meta-control?
of these policies.At each turn, the system must know which policyis currently in focus and is responsible for decidingthe next action to perform.
Since dialogue manage-ment operates under significant uncertainty, the sys-tem can never be sure whether a given policy is ter-minated or not.
We thus need a ?soft?
control mech-anism which is able to explicitly account for the un-certainty about the completion status of each policy.This is precisely what we present in this paper.The rest of the paper is as follows.
We first pro-vide general definitions of dialogue policies, andpresent an algorithm for dialogue management oper-ating on multiple policies.
We then present an imple-mentation of the algorithm together with an empiri-cal evaluation of its performance, and conclude thepaper by comparing our approach to related work.2 BackgroundWe start by providing a generic definition of a pol-icy which can hold independently of any particularencoding.
Dialogue policies can indeed generallybe decomposed in three basic functions, which arecalled consecutively upon each turn: (1) observationupdate, (2) action selection and (3) action update.2.1 Observation updateThe role of observation update is to modify the pol-icy?s current state1 upon receiving a new observa-tion, which can be linguistic or extra-linguistic.Observation update is formally defined as a func-tion OBS-UPDATE : S ?
O ?
S which takes as in-put the current state s and a new observation o, andoutputs the updated state s?.
For instance, a finite-state controller is expressed by a set of nodesN andedges E , where the state is expressed by the currentnode, and the update mechanism is defined as:OBS-UPDATE(s, o) ={s?
if ?
an edge so??
s?s otherwiseIn information-state approaches (Larsson andTraum, 2000), the update is encoded in a collection1We adopt here a broad definition of the term ?state?
to ex-press any description of the agent?s current knowledge.
In aPOMDP, the state thus corresponds to the belief state.of update rules which can be applied to infer the newstate.
In POMDP-based dialogue managers (Younget al, 2010), the observation update corresponds tothe belief monitoring/filtering function.2.2 Action selectionThe second mechanism is action selection, whoserole is to select the optimal (communicative) actionto perform based on the new estimated state.
Theaction selection is a function pi : S ?
Awhich takesthe updated state as input, and outputs the optimalaction to execute (which might be void).Different encodings are possible for the action se-lection mechanism.
Finite-state controllers use astraightforward mechanism for pi, since each statenode in the graph is directly associated with a uniqueaction.
Information-state approaches provide a map-ping between particular sets of states and actionsby way of selection rules.
Decision-theoretic ap-proaches such as MDPs and POMDPs rely on anestimated action-value function which is to be max-imised: pi(s) = argmaxaQ(s, a).
The utility func-tion Q(s, a) can be either learned from experienceor provided by the system designer.2.3 Action updateOnce the next action is selected and sent for execu-tion, the final step is to re-update the dialogue stategiven the action.
Contrary to the two previous func-tions which can be found in all approaches, this thirdmechanism is optional and is only implemented insome approaches to dialogue management.Action update is formally defined as a functionACT-UPDATE : S ?
A ?
S. Finite-state andinformation-state approaches typically have no ex-plicit account of action update.
In (PO)MDPs ap-proaches, the action update function is computedwith the transition function of the model.3 Approach3.1 Activation vectorTo enable the dialogue manager to operate with mul-tiple policies, we introduce the notion of activationvalue.
The activation value of a policy i is the prob-ability P (?i) that this policy is in focus for the in-teraction, where the random variable ?i denote theactivation of policy i.
In the rest of this paper, we295shall use bt(?i) to denote the activation value of pol-icy ?
at time t, given all available information.
Thebt(?i) value is dependent on both the completionstatus of the policy itself and the activations of theother policies: bt(?i) = P (?i|si, bt(?1), ...bt(?n)).We group these values in an activation vector b?
=?b(?1)...b(?n)?
which is updated after each turn.3.2 Activation functionsTo compute the activation values, we define the twofollowing functions associated with each policy:1.
LIKELIHOODi(s, o) : S ?
O ?
[0, 1] computesthe likelihood of the observation o if the policyi is active and currently in state s. It is thereforean estimate of the probability P (o|?i, s).2.
ACTIVATIONi(s) : S ?
[0, 1] is used to deter-mine the probability of policy i being active ata given state s. In other words, it provides anestimate for the probability P (?i|s).These functions are implemented using heuris-tics which depend on the encoding of the policy.For a finite-state controller, we realise the functionLIKELIHOOD(s, o) by checking whether the observa-tion matches one of the outward edges of the currentstate node ?
the likelihood returns a high probabilityif such a match exists, and a low probability oth-erwise.
Similarly, the ACTIVATION function can bedefined using the graph structure of the controller:ACTIVATION(s) ={1 if s non-final?
if s final with outgoing edges0 if s final w/o outgoing edgeswhere ?
is a constant between 0 and 1.3.3 Constraints between policiesIn addition to these activation functions, variousconstraints can hold between the activation of re-lated policies.
Policies can be related with eachother either hierarchically or concurrently.In a hierarchical mode, a policy A triggers an-other policy B, which is then executed and returnsthe control to policyA once it is finished.
As in hier-archical planning (Erol, 1996; Pineau, 2004), we im-plement such hierarchy by distinguishing betweenprimitive actions and abstract actions.
An abstractaction is an action which corresponds to the execu-tion of another policy instead of leading directly toa primitive action.
With such abstract actions, thesystem designer can define a hierarchical structureof policies as illustrated in Figure 1.
When a policyA executes an abstract action pointing to policy B,the activation value of policy B is increased and theone of policy A proportionally decreased.
This re-mains so until policy B terminates, at which pointthe activation is then transferred back to policy A.Figure 1: Graphical illustration of a hierarchical policystructure.
Dotted lines denote abstract actions.In a concurrent mode, policies stand on an equalfooting.
When a given policy takes the turn after anobservation, the activations of all other concurrentpolicies are decreased to reflect the fact that this partof the interaction is now in focus.
This redistributionof the activation mass allows us to run several poli-cies in parallel while at the same time expressing a?preference?
for the policy currently in focus.
The?focus of attention?
is indeed crucial in verbal inter-actions, and in linguistic discourse in general (Groszand Sidner, 1986) ?
humans do not arbitrarily switchfrom one topic to another and back, but rather con-centrate on the most salient elements.The set of constraints holding between the activa-tion values of hierarchical and concurrent policies isencoded in a simplified Bayesian network.3.4 Execution algorithmAlgorithm 1 illustrates how the activation valuesare exploited to select the optimal action for mul-tiple policies.
The algorithm relies on a set of pro-cesses P , where a process i is associated with a spe-cific policy, a current state si for the policy, and acurrent activation value b(?i) ?
b?.
As we haveseen, each policy is fully described with five func-tions: LIKELIHOOD(s, o), OBS-UPDATE(s, o), pi(s),ACT-UPDATE(s, a), and ACTIVATION(s).
A network296of conditional constraints C on the activation vectoris also given as input to the algorithm.Algorithm 1 operates as follows.
Upon receiv-ing a new observation, the procedure loops overall processes in P and updates the activation val-ues b?
(?i) for each given the likelihood of the ob-servation (with ?
as a normalisation factor).
Oncethis update is completed, the process p with thehighest activation is selected, and the functionGET-OPTIMAL-ACTION(p, o) is triggered.Algorithm 1 : MAIN-EXECUTION (P, o)Require: P: the current set of processesRequire: C: network of constraints on b?Require: o: a new observation1: for all i ?
P do2: P (o|?i, si)?
LIKELIHOODi(si, o)3: b?(?i)?
?
?
P (o|?i, si) ?
b(?i)4: end for5: Select process p?
argmaxi b?
(?i)6: a?
?
GET-OPTIMAL-ACTION(p, o)7: for all i ?
P do8: P (?i|si)?
ACTIVATIONi(si)9: Prune i from P if inactive10: Compute b(?i) given P (?i|si) and C11: end for12: return a?Within GET-OPTIMAL-ACTION, the state of the pro-cess is updated given the observation, the next actiona?
is selected using pi(s) and the state is updatedagain given this selection.
If the action is abstract,the above-mentioned procedure is repeated until aprimitive action is reached.
The resulting hierarchi-cal structure is recorded in children(p) which details,for each process p ?
P , the list of its children pro-cesses.
To ensure consistency among the activationvalues in this hierarchy, a constraint is added to C foreach process visited during execution.Once the action a?
is found, the activation valuesb(?i) are recomputed according to the local activa-tion function combined with the constraints C. Pro-cesses which have become inactive (i.e.
which havetransferred control to one parent process) are alsopruned from P .
Finally, the action a?
is returned.Algorithm 2 : GET-OPTIMAL-ACTION (p, o)Require: p: process with current state spRequire: o: a new observationRequire: children(p): list of current processes di-rectly or indirectly forked from p1: sp ?
OBS-UPDATEp(sp, o)2: a?
?
pip(sp)3: sp ?
ACT-UPDATEp(sp, a?
)4: if a?
is an abstract action then5: Fork new process q with policy from a?6: Add q to set of current processes P7: a?
?
GET-OPTIMAL-ACTION(q, o)8: children(p)?
?q?+ children(q)9: else10: children(p)?
?
?11: end if12: Add to C the constraint b(?p) =(1?
?i?children(p) b(?i)) ?
P (?p|sp)13: return a?4 EvaluationThe described algorithm has been implemented andtested with different types of policies.
We presenthere a preliminary experiment performed with asmall dialogue domain.
The domain consists of a(simulated) visual learning task between a humanand a robot in a shared scene including a small num-ber of objects, described by various properties suchas color or shape.
The human asks questions re-lated to these object properties, and subsequentlyconfirms or corrects the robot?s answers ?
as the casemay be.
We account for the uncertainty both in thelinguistic inputs and in the visual perception.We model this domain with two connected poli-cies, one top policy handling the general interac-tion (including engagement and closing acts), andone bottom policy dedicated to answering each userquestion.
The top policy is encoded as a finite-statecontroller and the bottom policy as a POMDP solvedusing the SARSOP algorithm, available in the APPLtoolkit2 (Kurniawati et al, 2008).
A sample run isprovided in Appendix A.The experiment was designed to empirically com-pare the performance of the presented algorithm2http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/297with a simpler hierarchical control algorithm whichdoes not use any activation vector, but where thetop policy is blocked until the sub-policy releasesits turn.
The policies themselves remain identicalin both scenarios.
We implemented a handcrafteduser simulator for the domain, and tested the poli-cies with various levels of artificial noise.The average return for the two scenarios are pro-vided in Figure 2.
The results show that activationvalues are beneficial for multi-policy dialogue man-agement, especially in the presence of noise..
This isdue to the soft control behaviour provided by the ac-tivation vector, which is more robust than hierarchi-cal control.
Activation values provide a more fine-grained mechanism for expressing the completionstatus of a policy, and therefore avoid fully ?block-ing?
the control at a given level.036912150 5 10 15 20 25 30 35 40 45 50AveragereturnperdialogueLevel of random noise (in %)Policies with activation functionPolicies with strict hierarchical controlFigure 2: Average return (as generated by the hand-crafted user simulator) for the two connected policies,using either the present algorithm or strict hierarchicalcontrol.
400 runs are used for each level of noise.5 Related workThe exploitation of prior structural knowledge incontrol has a long history in the planning commu-nity (Erol, 1996; Hauskrecht et al, 1998), and hasalso been put forward in some approaches to di-alogue modelling and dialogue management ?
seee.g.
(Grosz and Sidner, 1990; Allen et al, 2000;Steedman and Petrick, 2007; Bohus and Rudnicky,2009).
These approaches typically rely on a task de-composition in goals and sub-goals, and assume thatthe completion of each of these goals can be fullyobserved.
The novel aspect of our approach is pre-cisely that we seek to relax this assumption of per-fect knowledge of task completion.
Instead, we treatthe activation/termination status of a given policy asa hidden variable which is only indirectly observedand whose value at each turn is determined via prob-abilistic reasoning operations.The idea of combining different dialogue man-agement frameworks in a single execution processhas also been explored in previous work such as(Williams, 2008), but only as a filtering mecha-nism ?
one policy constraining the results of an-other.
Related to the idea of concurrent policies,(Turunen et al, 2005) describes a software frame-work for distributed dialogue management, mostlyfocussing on architectural aspects.
In the same vein,(Lemon et al, 2002; Nakano et al, 2008) describetechniques for dialogue management respectivelybased on multi-threading and multi-expert models.
(Cuaya?huitl et al, 2010) describe an reinforcementlearning approach for the optimisation of hierarchi-cal MDP policies, but is not extended to other typesof policies.
Closest to our approach is the PolCA+algorithm for hierarchical POMDPs presented in(Pineau, 2004), but unlike our approach, her methoddoes not support temporally extended actions, as thetop-down trace is repeated after each time step.6 ConclusionWe introduced a new approach to dialogue manage-ment based on multiple, interconnected policies con-trolled by activation values.
The values are updatedat the beginning and the end of each turn to reflectthe part of the interaction currently in focus.It is worth noting that the only modification re-quired in the policy specifications to let them runin a multi-policy setting is the introduction of thetwo functions LIKELIHOOD(s, o) and ACTIVATION(s).The rest remains untouched and can be defined in-dependently.
The presented algorithm is thereforewell suited for the integration of dialogue policiesencoded in different theoretical frameworks.Future work will focus on various extensions ofthe approach and the use of more extensive evalua-tion metrics.
We are also investigating how to ap-ply reinforcement learning techniques to learn themodel parameters in such multi-policy paradigms.298AcknowledgementsThis work was supported by the EU FP7 IP project?
?ALIZ-E: Adaptive Strategies for Sustainable Long-Term Social Interaction?
(FP7-ICT-248116) and bya PhD research grant from the University of Oslo.The author would like to thank Stephan Oepen, ErikVelldal and Alex Rudnicky for their comments andsuggestions on earlier drafts of this paper.ReferencesJ.
Allen, D. Byron, M. Dzikovska, G. Ferguson,L: Galescu, and A. Stent.
2000.
An architecture fora generic dialogue shell.
Natural Language Engineer-ing, 6:213?228, September.D.
Bohus and A. I. Rudnicky.
2009.
The RavenClawdialog management framework: Architecture and sys-tems.
Computer Speech & Language, 23:332?361,July.H.
Cuaya?huitl, S. Renals, O.
Lemon, and H. Shimodaira.2010.
Evaluation of a hierarchical reinforcementlearning spoken dialogue system.
Computer Speech& Language, 24:395?429, April.K.
Erol.
1996.
Hierarchical task network planning: for-malization, analysis, and implementation.
Ph.D. the-sis, College Park, MD, USA.B.
J. Grosz and C. L. Sidner.
1986.
Attention, inten-tions, and the structure of discourse.
ComputationalLinguistics, 12:175?204, July.B.
J. Grosz and C. L. Sidner.
1990.
Plans for discourse.In P. R. Cohen, J. Morgan, and M. E. Pollack, ed-itors, Intentions in Communication, pages 417?444.MIT Press, Cambridge, MA.M.
Hauskrecht, N. Meuleau, L. P. Kaelbling, T. Dean, andC.
Boutilier.
1998.
Hierarchical solution of markovdecision processes using macro-actions.
In Proceed-ings of Uncertainty in Artificial Intelligence (UAI),pages 220?229.H.
Kurniawati, D. Hsu, and W.S.
Lee.
2008.
SARSOP:Efficient point-based POMDP planning by approxi-mating optimally reachable belief spaces.
In Proc.Robotics: Science and Systems.S.
Larsson and D. R. Traum.
2000.
Information state anddialogue management in the trindi dialogue move en-gine toolkit.
Natural Language Engineering, 6:323?340, September.O.
Lemon, A. Gruenstein, A.
Battle, and S. Peters.
2002.Multi-tasking and collaborative activities in dialoguesystems.
In Proceedings of the 3rd SIGDIAL work-shop on Discourse and Dialogue, pages 113?124,Stroudsburg, PA, USA.M.
Nakano, K. Funakoshi, Y. Hasegawa, and H. Tsujino.2008.
A framework for building conversational agentsbased on a multi-expert model.
In Proceedings of the9th SIGDIAL Workshop on Discourse and Dialogue,pages 88?91, Stroudsburg, PA, USA.J.
Pineau.
2004.
Tractable Planning Under Uncertainty:Exploiting Structure.
Ph.D. thesis, Robotics Institute,Carnegie Mellon University, Pittsburgh, PA.M.
Steedman and R. P. A. Petrick.
2007.
Planningdialog actions.
In Proceedings of the 8th SIGDIALWorkshop on Discourse and Dialogue (SIGdial 2007),pages 265?272, Antwerp, Belgium, September.M.
Turunen, J. Hakulinen, K.-J.
Ra?iha?, E.-P. Salonen,A.
Kainulainen, and P. Prusi.
2005.
An architectureand applications for speech-based accessibility sys-tems.
IBM Syst.
J., 44:485?504, August.J.
D. Williams.
2008.
The best of both worlds: Unify-ing conventional dialog systems and POMDPs.
In In-ternational Conference on Speech and Language Pro-cessing (ICSLP 2008), Brisbane, Australia.S.
Young, M.
Gas?ic?, S. Keizer, F. Mairesse, J. Schatz-mann, B. Thomson, and K. Yu.
2010.
The hiddeninformation state model: A practical framework forpomdp-based spoken dialogue management.
Com-puter Speech & Language, 24:150?174, April.299A Example of execution with two policiesWe provide here an example of execution of Algo-rithm 1 with the two policies described in the evalua-tion section.
Figure 3 illustrates the policy hierarchy,which consists of two policies connected with an ab-stract action.
The finite-state graph of the top policyis shown in Figure 4.?hi!
?Bottom policy (POMDP)Top policy (finite-state)?how may Ihelp you???goodbye!?
?the object is X?
?I don?t know thecolour/shape?
?do you mean the1st/2nd object??
?could yourepeat???whichobject??(X=green,red,blue,round,squared,etc.
)......Figure 3: Hierarchical structure of the two policies.
(wait 1 s.)Robot: ?hi!
?User: (received new intention)Robot: ?howmay I help you?
?User: ?hi robot!?
(timeout)Robot: (triggerPOMDP)Robot: ?goodbye!
?User: ?goodbye robot!
?User: (receivednew intention)Figure 4: Finite-state controller for the top policy.A.1 Example of interactionTranscript A.1 provides a full example of interactionas generated by the user simulator.
Human utter-ances can contain variable noise levels, which makesit necessary to use clarification requests when theuser intention is unclear (lines 5 and 10).Contextual knowledge can also influence how thedialogue unfolds ?
here, the robot knows that thefirst object is blue but is uncertain about the secondobject.
Such contextual knowledge is expressed interms of hidden variables in the POMDP model.The execution of Algorithm 1 for the above ex-ample proceeds as follows.
From line 1 to line 4, theset P contains one single process, based on the topTranscript A.1 Example of interaction1: HUMAN: Hi robot!2: ROBOT: Hi!
[wait 1 s.]3: ROBOT: How may I help you?4: HUMAN: Could you tell me the colour ofthe first object?
[trigger POMDP]5: ROBOT: Sorry, could you repeat?6: HUMAN: What colour is the first object?7: ROBOT: It is blue8: HUMAN: That?s right9: HUMAN: Now, could you tell me the colourof the second object?
[trigger POMDP]10: ROBOT: Do you mean the second object?11: HUMAN: Yes12: ROBOT: I don?t know13: HUMAN: It is green14: ROBOT: Thanks15: HUMAN: Ok I?m done.
Goodbye!16: ROBOT: Goodbye!
[close interaction]policy.
This process is fully activated.
At line 5, thenew user intention is mapped to the abstract action,and thus to the creation of a second process basedthe POMDP policy.
This second process becomeshighly activated due its function ACTIVATION(s).Given the constraint holding between the activationvalues of two processs, the high activation of the bot-tom process leads to a proportional reduction of theactivation value of the top process.The utterance at line 8 is interpreted as a signalthat the user intention has been met, and the acti-vation value of the bottom process is subsequentlydecreased.
The top process is then partially reac-tivated, and interprets the utterance at line 9 as anew user intention, leading to the creation of a newPOMDP process.
This process handles the interac-tion until the line 14, where the control is transferredback to the finite-state controller.The better performance of Algorithm 1 comparedto strict hierarchical control is due to the lines 8?
9.In the presence of noise, the transition to the secondquestion might not be detected (if the confidencescores of the utterance is below a fixed threshold).
Insuch case, the dialogue manager might stay ?stuck?in the first POMDP process instead of interpretingthe utterance as a new question.300
