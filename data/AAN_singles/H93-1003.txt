BENCHMARK TESTS FOR THE DARPASPOKEN LANGUAGE PROGRAMDavid S. Pallett, Johathan G. Fiscus,William M. Fisher, and John S. GarofoloNational Institute of Standards and TechnologyRoom A216, Building 225 (Technology)Gaithersburg, MD 208991.
INTRODUCTIONThis paper documents benchmark tests implemented withinthe DARPA Spoken Language Program during the periodNovember, 1992 - January, 1993.
Tests were conductedusing the Wall Street Journal-based Continuous SpeechRecognition (WSJ-CSR) corpus and the Air Travel Infor-mation System (ATIS) corpus collected by the Multi-siteATIS Data COllection Working (MADCOW) Group.
TheWSJ-CSR tests consist of tests of large vocabulary (lexi-cons of 5,000 to more than 20,000 words) continuousspeech recognition systems.
The ATIS tests consist of testsof (1) ATIS-domain spontaneous speech (lexicons typicallyless than 2,000 words), (2) natural language understanding,and (3) spoken language understanding.
These tests werereported on and discussed in detail at the Spoken LanguageSystems Technology Workshop held at the MassachusettsInstitute of Technology, January 20-22, 1993.Tests implemented during this period also included experi-mental or "dry run" implementation f two new tests.
In theWSJ-CSR domain, a"stress test" was implemented, usingtest material that was drawn from unidentified sub-corpora.In the ATIS domain, an experimental "end-to-end" evalua-tion was conducted that included examination f the sub-ject-session "logfile".
Following precedents establishedpreviously, the results of these dry-run tests are notincluded as part of the "official" NIST test results and arenot discussed at length in this paper.Prior benchmark tests conducted within the DARPA Spo-ken Language Program are described in papers by Pallett,et al in the several proceedings of the DARPA Speech andNatural Language Workshops from 1989 to 1992.
Papers inthe Proceedings of the February 1992 Speech and NaturalLanguage Workshop describe the development ofthe WSJ-CSR corpus, collection procedures and initial experience inbuilding systems for this domain.
Initial use of the PilotCorpus for a "dry run" of benchmark test procedures priorto the Februai-y 1992 Speech and Natural Language Work-shop is reported in \[1\].
ATIS-domain tests that werereported at the February 1992 meeting are documented in\[2\].System descriptions were submitted to NIST by the bench-mark test participants and distributed at the Spoken Lan-guage Systems Technology Workshop.
Additional informa-tion describing these systems can be found references 5-23.Detailed information isnot available (in published papers)for some systems.2.
WSJ-CSR TESTS: NEW CONDITIONS2.1.
Stress TestThe established benchmark test protocols for speech recog-nition systems are such that system developers have priorknowledge of the nature of the test material, based onaccess to similar development test sets.
Some developershave consistently declined to report results for material ofparticular interest o DARPA program management (e.g.,for secondary microphone data).
Concern has beenexpressed that the sensitivity or "robustness" of someDARPA-sponsored recognition algorithms has not beenadequately probed or the systems "stressed".DARPA program management requested that NIST imple-ment, in early December, 1992, a "dry run" of a "stresstest" in which the nature of the test material was unspeci-fied.
Participating DARPA contractors were required todocument and freeze the system configuration used to pro-cess the test material prior to implementing the test, and toprovide data for a baseline test of this system using the 20KNVP test subset of the Nov.'92 test material, as well as forthe stress test set.
Test hypotheses were scored by NISTusing "conditional scoring" -- partitioning and reportingtest results for individual test subsets.The stress test material consisted of a set of 320 utterancefiles, chosen from three components: (1) read 20K sen-tences, for 4 female speakers, (2) read 5K sentences, for 4female speakers, and (3) spontaneously dictated news arti-cles, for 2 male and 2 female speakers.
The read speechincluded both primary and secondary microphones, so thatthere were 5 test subsets in all, each consisting of either 60or 80 utterances.Reactions to the stress test, as well as to the test results,were mixed.
In general, as would be expected, systems withtrigram language models did better than those with bi-grams.
Degradations in performance for the secondarymicrophone data were relatively smaller for some systemsthan others -- particularly for those sites that had devotedspecial effort o the issue of "noise robustness".
However,because the individual test subsets and the number ofspeakers were small, the results of many of the paired com-parison significance tests were inconclusive, suggestingthat future applications of such a test procedure mustinvolve larger test subsets.2.2.
New Significance TestsFor several years, NIST has implemented two tests of sta-tistical significance for the results of benchmark tests ofspeech recognition systems: the McNemar sentence errortest (MN) and a Matched-Pair-Sentence-Segment-Word-Error (MAPSSWE) test, on the word error rate found insentence segments.
In niore recent ests, NIST has alsoimplemented two additional tests: a Signed-pair (SI) test,and the Wllcoxon signed rank (WI) test.
These additionaltests are relevant to the word error rates found for individ-ual speakers, and as such are particularly sensitive to thenumber of speakers in the test set.
References to these testscan be found in the literature on nonpararnetric or distribu-tion-free statistics.2.3.
Uncertainty of Performance MeasurementResultsIncreasing attention is being paid, at NIST, to evaluatingand expressing the uncertainty ofmeasurement results.
Thisattention is motivated, in part, by the realization that "ingeneral, it is not possible to know in detail all of the uses towhich a particular NIST measurement result will beput.
"\[3\] Current NIST policy is that "all NIST measure-ment results are to be accompanied byquantitative mea-surements of uncertainty".
In substance, the recommendedapproach to expressing measurement uncertainty is that rec-ommended by the International Committee for Weights andMeasures (CIPM).The CIPM-recommended approach includes: (1) determin-ing and reporting the "standard uncertainty" orpositivesquare root of the estimated variance for each component ofuncertainty hat contributes to the uncertainty of the mea-surement result, (2) combining the individual standarduncertainties into a determination f the "combined stan-dard uncertainty", (3) multiplying the combined standarduncertainty by a factor of 2 (a "coverage factor", that fornormally distributed ata corresponds to the 95% confi-dence interval), and specifying this quantity as the"expanded uncertainty".
The expanded uncertainty, alongwith the coverage factor, or else the combined standarduncertainty, is to be reported.The paired-comparison significance t sts outlined in theprevious ection represent specific instantiations of teststhat evaluate the validity of null hypotheses regarding dif-ferences (in measured performance) between systems.
Inmany cases, however, sufficiently detailed ata is not avail-able to implement these tests.
In these cases it is importantto refer to explicit estimates of uncertainties.The case of evaluating the uncertainties a sociated with per-formance measurements forspoken language technology isparticularly complex because of the number of known com-plicating factors.
These factors include properties of thespeaker population (e.g., gender, dialect region, speakingrate, vocal effort, etc.
), properties of the training and testsets (e.g., vocabulary size, syntactic and semantic proper-ties, microphone/channel, etc.)
and other factors \[4\].Performance measures used to date within the DARPA spo-ken language research community (and included in thispaper) do not conform to the recommended approach, sincethe scoring software, in general, generates a single measure-merit for the ensemble of test data (e.g., one datum indicat-ing word or utterance error rate for the entire multi-speaker,multi-utterance, t st subset, rather than the mean error ratefor the ensemble of speakers).
These single-measurementperformance evaluation procedures do not yield estimatesof the variances "for each component of uncertainty hatcontributes to the uncertainty of the measurement result"that are required in order to implement the CIPM-recom-mended practice.In future tests, revisions to the scoring software that wouldpermit estimates of the variance across the speaker popula-tion (at the least) are in order.
However, it would seem to bethe case that identifying and obtaining quantitative esti-mates of "each component of uncertainty hat contributes tothe uncertainty of the measurement" will be difficult.3.
WSJ-CSR NOVEMBER 1992 TESTMATERIALThe test material, as distributed, included a total of 16 iden-tiffed test subsets.
In general, these can be sub-categorizedfive ways: speaker dependent/independent (SD/SI), 5K/20K reference vocabularies, the use of verbalized/non-ver-balized punctuation (VP/NVP), read/spontaneous speech,and primary (Sennheiser, close-talking)/secondary micro-phone.
No one participant reported results on all subsets --most reported results on only one or two, corresponding toconditions of particular local interest and/or algorithmicstrength.All of the test material was drawn from the WSJ-CSR PilotCorpus that was collected at MIT/LCS, SRI International,and TI.
The "spontaneous dictation" data was collected onlyat SRI.Individual test set sizes varied from 72 utterances to(moretypically) approximately 320 utterances.
The number ofspeakers in each subset varied from 3 to 12 speakers.
Theactual number of sentence utterances per speaker variedsomewhat, because the material was selected in paragraphblocks.
A total of 8 secondary microphones was included inthe various test subsets, including one speakerphone, a tele-phone handset, 3 boundary effect microphones (CrownPCC-160, PZM-6FS, and Shure SM91), two lavalier micro-phones, and a desk-stand mounted microphone.4.
WSJ -CSR TEST  PROTOCOLSTest protocols were similar to prior speech recognitionbenchmark tests.
Test material was shipped to the partici-pating sites on October 20th, results were reported on Nov.23rd, and NIST reported scored results via ftp to the partici-pants on Dec. 2nd.
The stress test was conducted betweenNov.
30th and Dec. 15th.A "required baseline" test was defined for all participants.
Itconsisted of processing the 5K word speaker independent,non-verbalized punctuation test set using a (common) bi-gram grammar.
Six sites reported 5K baseline test results.5.
WSJ -CSR TEST  SCORINGAs for the test protocols, much of the scoring was routine,except for one new additional factor.
Since previous "offi-cial" CSR benchmark tests had not included spontaneousspeech, the commtmity had not reviewed the adequacy ofthe transcription convention used for spontaneous speech,and several inconsistencies in the transcriptions were notedfollowing release of the preliminary results.
Some of theseinconsistencies were resolved prior to releasing "official"results.6.
WSJ -CSR TEST  PART IC IPANTSParticipants m these WSJ-CSR tests included the followingDARPA contractors: BBN, CMU, Dragon Systems, MITLincoln Laboratory, and SRI International.
A "volunteer"participant was the French CNRS LIMSI.
LIMSI declinedto participate in the "stress test".7.
WSJ-CSR BENCHMARK TEST RESULTSAND DISCUSSION7.1.
Test Results: Word and Utterance (Sen-tence) Error RatesTable 1 presents the results for the several test sets on whichresults were reported.
Section I of that able includes resultsreported by Paul at MIT Lincoln Laboratory \[5\] for Longi-tudinal Speaker Dependent (LSD) technology.
Section IIincludes results reported by BBN for Speaker Dependent(SD) technology.
Section III includes the results of SpeakerIndependent (SI) technology, for a number of sites for (a)the 20K NVP test set for both baseline and non-baseline SIsystems, (b) the 5K NVP test set for both baseline and non-baseline SI systems, (c) the 5K NVP test set "other micro-phone" test set data, and (d) the 5K VP test set (on whichonly LIMSI reported results \[6\]).
Section IV of Table 1includes the results reported by BBN for the SpontaneousDictation test set.For the test set on which the largest number of results werereported -- the 5K NVP set, using the close-talking micro-phone -- the lowest word error rates were reported by CMU\[7-9\]: 6.9% for the baseline, bigram language model, and5.3% using a trigram language model.
The range of worderror rates for the baseline condition for all systems testedwas 6.9% to 15.0%, while for non-baseline conditions, therange was from 5.3% to 16.8%.For the 5K NVP test set's secondary microphone data, asreported by CMU \[8\] and SRI \[10,11\], word error ratesranged from 17.7% to 38.9%.For the 20K NVP test set, on which other baseline data werereported, the word error rates range from 15.2% to 27.8%.The lowest error rate, reported by CMU, can be shown to besignificantly different for all 4 significance t sts when com-pared with the Dragon \[13\] and MIT Lincoln systems, butshown to be significantly different only for the MAPSSWEtest when compared with the BBN system \[14\].
Thus theperformance differences between the CMU and BBN sys-tems for this baseline condition test are very small.7.2.
Significance Test ResultsTable 2 presents the results, in a matrix form, of 4 paired-comparison significance tests for the baseline tests for the5K NVP test set.
The convention i this form of results tab-ulation is that if the result of a null-hypothesis test is valid,the word "same" is printed in the appropriate matrix ele-ment.
If the null hypothesis  not valid, the identifier for thesystem with the lower (and significantly different) error rateis printed.For this test set, recall that the CMU system (here identifiedas cmul-a) had a word error rate of 6.9%.
By comparing theresults for the CMU system with the other 5 systems report-ing baseline results, note that he significance t st results allindicate that the null hypothesis not valid.
In other words,the error rates for the CMU system are significantly differ-ent (lower) than those for the other 5 systems for this test setand baseline conditions.In general, for this test set, with 12 speakers and 310 utter-ances, the Wilcoxon signed rank test (WI) is more sensitivethan the (ordinary) sign test (SI).
As noted in previous tests,the McNemar test (MN), operating on the sentence rrorrate, is in general less sensitive than the matched-pair-sen-tence segment word error rate test (MAPSSWE).8.
ATIS TESTS:  NEW CONDIT IONSWithin the community of ATIS system developers, there isa continuing search for evaluation methodologies to com-plement the current evaluation methodology.
In particularthere is a recognized need for evaluation methodologiesthat can be shown to correlate well with expected perfor-mance of the technology in applications.
Toward-the end of1992, several sites participated inan experimental "end-to-end" evaluation to assess ystems in an interactive form.The end-to-end evaluation i cluded (1) objective measuressuch as timing information and time to task completion, (2)human-derived judgements on correctness of systemanswers and user solutions, and (3) a user satisfaction ques-tionnaire.
The results of this "dry rtm" complementary eval-uation experiment are reported by Hirschman et al in \[15\].9.
ATIS TEST  MATERIALTest material for the ATIS benchmark tests consisted of1002 queries, for 118 subject-scenarios, involving 37 sub-jects.
It was selected by NIST from set-aside materialdrawn from data previously collected within the MAD-COW community at AT&T, BBN, CMU, MIT/LCS, andSRI.
The selection and composition of this test material isdescribed in more detail in \[15\].As m previous years, queries were categorized into two cat-egories of "answerable" queries, Class A, which are con-text-independent, and Class D, which are context-depen-dent; and "unanswerable", or Class X queries.
In the finaladjudicated test set, there were a total of 427 Class A que-ries, 247 Class D queries, and 328 Class X queries.10.
ATIS TEST  PROTOCOLSAs was the case for the speech recognition benchmark tests,ATIS test protocols were similar to prior ATIS benchmarktests.
The test material was shipped to the participating siteson October 20th, results were reported on Nov. 16th, andNIST reported preliminary scored results via ftp to the par-ticipants on Nov. 20th.
After the process of formal "adjudi-cation" had taken place, official results were reported onDec.
20th.11.
ATIS SCORING AND ADJUDICAT IONAfter the preliminary scoring results were distributed, theparticipating sites were invited to send requests for adjudi-cation ("bug reports") to NIST, asking for changes in thescoring of specific queries.
A total of 146 of these bugreports were adjudicated by NIST and SRI jointly.
Sincemany of these requests for adjudication were duplicates, thenumber of distinct problems reported was less than 100.
Adecision was made on each request for adjudication and thecorrected reference material or procedure was used in afinal adjudicated re-run of the evaluation.
The judgmentwas in favor of the plaintiff in approximately 2/3 of thecases .A number of problems uncovered by this procedure weresystematic, in that the same root problem affected severaldifferent queries.
Most of these were simply human error,which can be made less likely in the future by working lesshectically and making software to double-check the testmaterial.The major problem that cannot be attributed to just humanerror is that of transcribing and scoring correctly speech thatis difficult o hear and understand.
Some of this speech was"sotto voce"; some was mispronounced; some was trun-cated; and in some cases the phonetic transcription wouldhave been unproblematical but division into lexical wordswas unclear, as in some contractions and compound words.The short-term solution adopted was just to make our bestjudgement on orthographic transcription, considering bothacoustics and higher-level language modeling.
But a betterlong-term cure is to make and use transcriptions that canindicate alternatives when the word spoken is uncertain;proposals to this effect are being considered by relevantcommittees.12.
ATIS TEST  PART IC IPANTSParticipants in these ATIS tests included the followingDARPA contractors: BBN, CMU, MIT Laboratory forComputer Science (MIT/LCS), and SRI.
There were sev-eral "volunteers": AT&T Bell Laboratories \[16\], who haveparticipated inprevious years; Paramax \[17\], not a DARPAcontractor at the time of these tests, but who have also par-ticipated in prior years' tests; and two participants fromCanada, CRIM and INRS.
A total of 8 system developersparticipated insome of the tests (i.e., the NL tests).13.
ATIS BENCHMARK TEST  RESULTS13.1.
ATIS SPontaneous peech RECognitionTests (SPREC)Table 3 presents the results for the SPREC tests for all sys-tems and all subsets of the data.
For the interesting case ofthe subset of all answerable queries, Class A+D, the worderror rate ranged from 4.3% to 100%.
The lowest value wasreported by BBN \[18,19\], and the value of 100% wasreported by INRS, for an incomplete ATIS system that (ineffect) rejected every utterance, resulting in a scored worddeletion error of 100%.Table 4 presents a matrix tabulation of ATIS SPREC resultsfor the set of answerable queries, Class A+D.
This form ofmatrix tabulation is discussed in \[2\] for the February 1992test results.
Considerable variability can be noted for theperformance ofsome systems on "local data", and there areindications of varying degree of difficulty for the subsetscollected at different sites.
As m the Feb.'92 test set, partic-ipants noted the presence of more disfluencies in the AT&Tdata than for other originating sites.10Word error rates for the "volunteers" in these tests (AT&T,CRIM and INRS) are in general higher than for DARPAcontractors, perhaps reflecting areduced level-of-effort, rel-ative to "funded" efforts.Table 5 presents the results, in a matrix form, of 4 paired-comparison significance tests for the 7 SPREC systems forthe Class A+D subset.For this test set, recall that he BBN system (here identifiedas bbn2a_d) had a word error rate of 4.3%.
By comparingthe results for this BBN system with the other 6 ATISSPREC systems, note that the null hypothesis  not validfor all 4 significance t sts for the comparisons with theAT&T, CRIM, INRS, MIT/LCS and SRI systems.
In otherwords, the differences in performance are significant.
How-ever, when comparing the BBN and CMU SPREC systems,the null hypothesis valid for 3 of the 4 tests.
Thus, as wasthe case for the WSJ-CSR data, the performance differ-ences, in this case for ATIS spontaneous speech, betweenthe CMU and BBN speech recognition systems are verysmall.
',13.2.
Natural Language Understanding Tests(NL)Table 6 presents atabulation of the results for the NL testsfor all systems and the "answerable" ATIS queries, ClassA+D, as well as the subsets, Class A and Class D.For the set of answerable queries, Class A+D, the weightederror ranges from 101.5% to 12.3%.
For the Class A que-ries, the range is from 79.9% to 12.2%.
And for the Class Dqueries, the range is from 138.9% to 12.6%.
In each case,the lowest weighted error rate was reported by the CMUsystem \[20\].Note that in general performance is considerably worse forClass D than for Class A.
However, for the CMU and MIT/LCS \[21\] systems, performance for the Class D test mate-rial is comparable tothat for Class A.
These systems wouldappear to have superior procedures for handling context.Table 7 presents amatrix tabulation of the NL results forthe several subsets of test material.
Note, however, thatsince the differences in performance between DARPA-con-tractor-developed systems and those of "volunteers", ingeneral, are significant, he column averages presented inthis table are not very informative.Of the 3 CRIM systems, the best performing one (crim3) isone using neural networks to classify each query into 1 of10 classes based on relation ames in the underlying ATISrelational database, with subsequent use of specific parsersbuilt for each class and another parser that determines theconstraints \[22\].There are two SRI NL systems \[23\].
The SRI NL-TM sys-tem, here designated sril, uses template matching to gener-ate database queries.
The other SRI system, termed the"Gemini+TM ATIS System" by SRI, and here designatedsri2, is an integration of SRI's unification-based natural-lan-guage processing system and the Template Matcher.
Differ-ences in performance do not appear to be pronounced.As in previous ATIS NL tests, it is important to note thatappropriate sts of statistical significance have not yet beendeveloped for ATIS NL tests.
Small differences inweightederror rate are probably of no significance.
However, large,systematic, differences are noteworthy, even if of unknownstatistical significance.
The weighted error rates for theCMU NL system, which are in many cases approximatelyone-half those of the next best systems, are certainly note-worthy.13.3.
Spoken Language System Understanding(SLS)Table 8 presents a tabulation of the results for the SLS testsfor all systems and the "answerable" ATIS queries, ClassA+D, as well as the subsets, Class A and Class D.For the set of answerable queries, Class A+D, the weightederror ranges from 100% to 21.6%.
For the Class A queries,the range is from 100% to 19.7%.
And for the Class D que-ries, the range is from 140.1% to 23.9%.
As in the case ofthe NL test results, and in each case, the lowest weightederror rate was reported for the CMU system.The INRS data signify 100% usage of the No_Answeroption, since the INRS SPREC system provided nullhypothesis strings, causing the NL component to return theNo_Answer esponse.Note again that the CMU and MIT/LCS systems both han-dle context sensitivity well.Table 9 presents amatrix tabulation of the SLS results forthe several subsets of test material.For the ATIS SLS with lowest overall weighted error rate(21.6%), the cmul system, there is an almost en-fold rangein error rate over the several test subsets: from 37.1%, forthe AT&T subset, to 3.9% for the SRI subset.
The CMUSLS weighted error rates for Class A+D are approximatelytwo-thirds those of the next-best-performing systems,although for the Class A subset, differences m performancebetween the CMU system and the BBN and SRI systemsare less pro-nounced.14.
ACKNOWLEDGEMENTAt NIST, our colleague Nancy Dahlgren contributed signif-icantly to the DARPA ATIS community and had a majorrole m annotating data and implementing "bug fixes" in col-laboration with the SRI annotation group and others.
Nancywas severely injured in an automobile accident in Novem-ber, 1992, and is undergoing rehabilitation therapy for treat-11ment of head trauma.
It is an understatement to say that wemiss her very much.Brett Tj~en also assisted us at NIST in preparing test mate-rial and other ways.The cooperation of the many participants in the DARPAdata and test infrastructure -- typically several individuals ateach site'.
-- is gratefully acknowledged.References1.
Pallett, D.S., "DARPA February 1992 Pilot Corpus CSR'Dry Run' Benchmark Test Results", in Proceedings ofSpeech and Natural Language Workshop, February 1992(M. Marcus, ed.)
ISBN 1-55860-272-0, Morgan KaufmannPublishers, Inc., pp.
382-386.2.
Pallett, D.S., et al, "DARPA February 1992 ATISBench-mark Test Results", in Proceedings ofSpeech andNatural Language Workshop, February 1992 (M.
Marcus,ed.)
ISBN 1-55860-272-0, Morgan Kaufrnann Publishers,Inc., pp.
15-27.3.
Taylor, B.N.
and Kuyatt, C.E., "Guidelines for Evaluat-ing and Expressing the Uncertainty of NIST MeasurementResults", NIST Technical Note 1297, January 1993.4.
Pallett, D.S.
"Performance Assessment of AutomaticSpeech Recognizers", J. Res.
National Bureau of Standards,Volume 90, #5, Sept.-Oct. 1985, pp.
371-387.5.
Paul, D.B.
and Necioglu, B.F., "The Lincoln Large-Vocabulary Stack-Decoder HMM CSR", Proceedings ofICASSP'93.6.
Gauvain, J.L., et al, "LIMSI Nov92 Evaluation", OralPresentation at the Spoken Language Systems TechnologyWorkshop, January 20-22, 1993, Cambridge, MA.7.
Huang, X., et al, "The SPHINX-II Speech RecognitionSystem: An Overview", Computer Speech and Language,in press (1993).8.
Alleva, E, et al, "An Improved Search Algorithm forContinuous Speech Recognition", Proceedings ofICASSP'93.9.
Hwang, M.Y., et al, "Predicting Unseen Triphones withSenones", Proceedings ofICASSP'93.10.
Liu, E-H., et al, "Efficient Cepstral Normalization forRobust Speech Recognition", inProceedings of the HumanLanguage Technology Workshop, March 1993 (M.
Bates,ed.)
Morgan Kaufmann Publishers, Inc.11.
Murveit, H., et al, "Large-Vocabulary Dictation usingSRI's DECIPHER (tm) Speech Recognition System: Pro-gressive Search Techniques", Proceedings of ICASSP'93.12.
Murveit, H., et al, "Progressive-search Algorithms forLarge Vocabulary Speech Recognition", inProceedings ofthe Human Language Technology Workshop, March 1993(M. Bates, ed.)
Morgan Kaufmann Publishers, Inc.13.
Roth, R., et al, "Large Vocabulary Continuous SpeechRecognition of Wall Street Journal Data", Proceedings ofICASSP'93.14.
Schwartz, R., et al, "Comparative Experiments onLarge Vocabulary Speech Recognition", inProceedings ofthe Human Language Technology Workshop, March 1993(M. Bates, ed.)
Morgan Kaufmann Publishers, Inc.15.
Hirschman, L., et al, "Multi-Site Data Collection andEvaluation in Spoken Language Understanding", in Pro-ceedings of the Human Language Technology Workshop,March 1993 (M. Bates, ed.)
Morgan Kaufmann Publishers,Inc.16.
Tzoukerrnann, E., (Untitled) Oral Presentation attheSpoken Language Systems Technology Workshop, January20-22, 1993, Cambridge, MA.17.
Linebarger, M.C., Norton, L.M.
and Dahl, D.A., "A por-table approach to last resort parsing and interpretation", inProceedings of the Human Language Technology Work-shop, March 1993 (M. Bates, ed.)
Morgan Kaufmann Pub-lishers, Inc.18.
Bates, M., et al, "Design and Performance ofHARC,the BBN Spoken Language Understanding System", Pro-ceedings of ICSLP-92, Banff, Alberta, Canada, October,1992.19.
Bates, M., et al, "The BBN/HARC Spoken LanguageUnderstanding System", Proceedings of ICASSP'93.20.
Ward, W. and Issar, S., "CMU ATIS Benchmark Evalu-ation", Oral Presentation atthe Spoken Language SystemsTechnology Workshop, January 20-22, 1993, Cambridge,MA.21.
Glass, et al, "The MIT ATIS System: January 1993Progress Report", Oral Presentation atthe Spoken Lan-guage Systems Technology Workshop, January 20-22,1993, Cambridge, MA.22.
Cardin, R., et al, "CRIM's Speech Understanding Sys-tem for the ATIS Task", Oral Presentation atthe SpokenLanguage Systems Technology Workshop, January 20-22,1993, Cambridge, MA.23.
Dowding, J., et al, "Gemini: A Natural Language Sys-tem for Spoken-Language Understanding", in Proceedingsof the Human Language Technology Workshop, March1993 (M. Bates, ed.)
Morgan Kaufmann Publishers, Inc.12I.
Long l tud lna l  Speaker  Depen=ent  Testsa.
LSD EVL 20K NVP Test SetSystems W.Err  U.Errmlt l14-n 14.6 78.2mlt  i15-~ 11.2 71.8IDENTIF IERLL NOV92 CSR LSD 20K CLOSED NVP BIGRAMLL NOV92 CSR LSD 20K CLOSED NVP TRIGRAMb.
LSD EVL  20K VP Test Setnlt 114- i  11.6 70.7mlt  115-i  7.6 56.0LL NOV92 CSR LSD 20K CLOSED VP B IGRAMLL NOV92 CSR LSD 20K CLOSED VP TRIGRAMc.
LSD EVL  5K NVP Test Setmlt l l4- f  8.3 62.5mlt  l lS- f  5.6 48.8LL NOV92 CSR LSD 5K CLOSED NVP B IGRAMLL NOV92 CSR LSD 5K CLOSED NVP TRIGRAMd.
LSD EVL 5K VP Test Setml t_ l l4 -g  6.7 68.1ml t_ l lS -g  4.5 44.4LL NOV92 CSR LSD 5K CLOSED VP B IGRAMLL NOV92 CSR LSD 5K CLOSED VP TRIGRAMII.
Speaker  Dependent  Testsa.
SD EVL 5K NVP Test SetSystems W.Err  U .Er rbDn2-e  8.2 54.5bbn3-e  6.1 44.5IDENTIF IERBBN NOV92 CSR BYBLOS SD-600 5K B IGRAMBBN NOV92 CSR BYBLOS SD-600 5K TRIGRAMIII.
Speaker  Independent :Tests :  Read Speecha.
SI Test Set (Basel ine Tests)W.Err  U.Err  IDENTIF IER16.7 81.1 BBN NOV92 CSR BYBLOS SI-12 20K BIGRAM BASELINE15.2 79.0 CMU NOV92 CSR SPHINX- i I  SI-84 20K BASELINE25.0 86.8 DRAGON NOV92 CSR MULTIPLE S I -12 20K NVP BASEL INE25.2 88.0 LL NOV92 CSR SI-84 20K OPEN NVP B IGRAM BASELINETest Set (Non-Basel lne Tests)14.8 75.7 BBN NOV92 CSR BYBLOS SI-12 20K TRIGRAM12.8 71.8 CMU NOV92 CSR SPHINX- i I  SI-84 20K TRIGRAM24.8 87.4 DRAGON NOV92 CSR GD SI-12 20K NVP27.8 87.4 DRAGON NOV92 CSR GI SI-12 20K NVP19.4 84.1 LL NOV92 CSR SI-84 20K OPEN NVP TRIGRAM ADAPT IVEb.
SI Test Set (Basel ine Tests)8.7 63.6 BBN NOV92 CSR BYBLOS SI-12 5K B IGRAM BASELINE6.9 57.6 CMU NOV92 CSR SPHINX- i I  SI-84 5K BASELINE14.1 78.2 DRAGON NOV92 CSR MULTIPLE SI -12 5K NVP BASELINE9.7 64.5 L IMSI  NOV92 CSR SI-84 5K-NVP BASELINE15.0 78.2 LL NOV92 CSR SI-84 5K CLOSED NVP B IGRAM BASEL INE13.0 73.9 SRI NOV92 CSR DECIPHER(TM) SI -84 BIGRAM BASELINEEVL 20K NVPSystemsbbn l -dcmul -ddragon3-dmJ.t l l l -dSI  EVL 20K NVPbbn3-dcmu2-ddragon l -ddragon2-dmlt l l3 -dEVL  5K ~Pbbn l -acmul -adragon3-al lms l l -am!t l l l -as r !~-aSI EVL  5K NVPbbn3-a  7.3 53.0cmu2-a  5.3 45.2cmu3-a  8.1 63.0cmu4-a  9.4 67.9cmu5-a  8.4 63.0cmu6-a  8.1 65.2dragcn l -a  13.6 76.7dragon2-a  16.8 76.4mlt l12-a 10.5 61.2mlt  l lS-a 9.1 56.7c.
SI EVL  5K ~P OTHERcmu3-c  38.5 88.2cmu4-c  17.7 75.8cmuS-c  38.9 87.3cmu6-c  19.3 77.9sr l l -c  27.3 87.6d.
SI EVL  5K VP Test Setl lms l l -D  7.8 58.9Test Set (Non-Basel lne Tests)BBN NOV92 CSR BYBLOS SI-12 5K TRIGRAMCMU NOV92 CSR SPHINX- I I  SI-84 5K TRIGRAMCMU NOV92 SPHINX- i IA  MFCDCN W/O COMP CSR SI-84 5K NVPCMU NOV92 SPHINX- I IA  MFCDCN W/ COMP CSR SI-84 5K NVPCMU NOV92 SPHINX- I IA  CDCN W/O COMP CSR SI-84 5K NVP~4U NOV92 SPHINX- I IA  CDCN W COMP CSR SI-84 5K NVPDRAGON NOV92 CSR GD SI-12 5K NVPDRAGON NOV92 CSR GI SI-12 5K NVPLL NOV92 CSR SI-84 5K CLOSED NVP TRIGRAMLL NOV92 CSR SI-84 5K CLOSED NVP TRIGRAM ADAPT IVEMICROPHONE Test SetCMU NOV92 SPHINX- i IA  M~CDCN W/O COMP CSR SI-84 5K NVPCMU NOV92 SPHINX- i IA  MFCDCN W/ COMP CSR SI-84 5K NVPCMU NOV92 SPHINX- i iA  CDCN W/O COMP CSR SI-84 5K NVPCMU NOV92 SPHINX- I IA  CDCN W COMP CSR SI-84 5K NVPSRI NOV92 CSR DECIPHER(TM) SI-84 BIGRAM BASEL INELIMSI NOV92 CSR SI-84 5K-VPIV.
Speaker  Incepenaent  Tesl: Spontaneous  Speecha.
SI SPONTANEOUS DICTATION NVP Test SetSystems W.Err  U.Err  IDENTIF IERbbn 2- 3 26.5 94.1 BBN NOV92 CSR BYBLOS SI-!2 SPON BIGRAMbbn3-~ 24.9 93.4 BBN NOV92 CSR BYBLOS SI-!2 SPON TRIGKAMTable ": WSJ -CSR ~encnmark  Test Results 13Compos i te  Repor t  o f  A l l  S ign i f i cance  TestsFor  the  WSJ -CSR Nov  92 SI 5K NVP Base l ine  (B igram)  TestTes t  Name Abbrev .. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Matched Pa i r  Sentence  Segment  (Word Er ror )  Tes t  MPS igned Pa i red  Compar i son  (Speaker  Word  Accuracy)  Test  SIWi l coxon  S igned Rank  (Speaker  Word  Accuracy)  Test  WIMcNemar  (Sentence  Er ror )  Tes t  MN.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
lI bbn l -a  cmul -a  I d ragon3-a  I l ims i l -a  I m i t  l l l -a  I s r i l -a. .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.bbn l -a  l MP  cmul -a  1 5~ bbn l -a  IMP  same IMP bbn l -a  IMP  bbn l -a1 SI  cmul -a  I S I  bbn l -a  I SI  same I SI  bbn l -a  I S I  bbn l -aI WI  cmul -a  I WI  bbn l -a  I WI  same I WI bbn l -a  I WI bbn l -aI MN cmul -a  I MN bbn l -a  I MN same \[ MN bbn l -a  I MN bbn l -a. .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.cmul -a  IlII+dragon3-al ims i l -ami t  l l l -as r i l -allI4lll,+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.III-4IMP  omul -a  IMP  cmul -a  L MP cmul -a  IMP  cmul -aI S I  cmul -a  I SI  cmul -a  I S I  cmul -a  I S I  cmul -aI WI  cmul -a  1WI  cmul -a  I WI  cmul -a  i WI  cmul -aI MN cmul -a  I MN cmul -a  k MN cmul -a  I MN cmul -a. .
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
+ +1 IMP  l ims i l -a  I MP  same I MP  sameI I SI  l ims i l -a  I SI same I SI  sameI I WI l ims i l -a  I WI  same L WI  sameI L MN l ims i l -a  I MN same I MN same?
?
?
+l ll lI Il l+ ?i MP  l ims i l -a  IMP  l ims i l -aI S I  l ims i l -a  I S I  samei WI  l ims i l -a  I WI  l ims i l -aI MN l ims i l -a  t MN l ims i l -a?
+I I i i MP  s r i l -aI I I I S I  same1 I I I WI  s r i l -aI l l I MN same+ ?
+ +l L l l1 1 l ll I l I\[ l l ITab le  2: S iqn f i cance  Test  Resu l ts :  Base l ine  Tests  Us ing  the  5K NVP Test  Set(See text  fo r  exp lanat ion  o f  fo rmat )14Nov92 ATIS SPREC Test ResultsClass A+D+X SubsetW.
Err Corr Sub Delatt2-adx !1.7 90.8 6.8 2.4bbn2-adx 7.6 94.2 4.2 1.6cmu2-adx 8.3 92.9 4.2 2.9cr im4-adx 19.3 84.1 12.1 3.8inrs2-adx i00.0 0.0 0.0 I00.0mit ics2-adx 12.6 89.8 7.3 2.9sr i3-adx 9.1 93.2 5.4 1.4Class A+D SubsetW.
Err Corr  Sub Delatt 2-a d 8.4 93.6 4.6 1.8bbn2-a--d 4.3 96.7 2.5 0.9cmu2-a--d 4.7 96.0 2.8 i.
2cr im4-a d 14.1 88.7 8.4 2.9inrs2-a d 100.0 0.0 0.0 100.0mit ics2-a d 8.1 93.3 4.5 2.2sr i3-a d - 5.7 95.7 3.5 0.9Class A SubsetW.
Err Corr  Sub Delatt2-a 8.0 93.8 4.4 i.
8bbn2-a 4.0 96 .7  2.3 I.
0cmu2-a 4.4 96.1 2.7 i.
2cr im4-a 13.5 88.9 8.0 3.1inrs2-a i00.0 0.0 0.0 i00.0mit ics2-a 7.8 93.5 4.4 2.2sr i~-a 5.2 96.0 3.2 0.9Ins U. Err | Utt.2.5 52.4 9671.8 35.6 9671.2 38.3 9673.4 64.1 9670.0 i00.0 9672.4 47.8 9672.3 43.3 967Descr ipt ionATT Nov 92 SPREC ResultsBBN Nov 92 SPREC ResultsCMU Nov 92 SPREC ResultsCRIM Nov 92 SPREC ResultsINRS Late Nov 92 SPREC ResultsMIT-LCS Nov 92 SPREC ResultsSRI Nov 92 SPREC ResultsClass D SubsetIns U. Err | Utt.2.0 44.7 6740.9 25.2 6740.7 28.9 6742.8 56.4 6740.0 i00.0 6741.4 37.8 6741.4 33.8 674Descr ipt ionATT Nov 92 SPREC Results Class A+DBBN Nov 92 SPREC Results Class A+DCMU Nov 92 SPREC Results Class A+DCRIM Nov 92 SPREC Results Class A+DINRS Late Nov 92 SPREC Results Class A+DMIT-LCS Nov 92 SPREC Results Class A+DSRI Nov 92 SPREC Results Class A+DW.
Err Corr  Sub Delat t2-d  9.2 93.2 5.0 1.7bbn2-d 4.8 96.5 2.8 0.7cmu2-d 5.4 95.7 3.2 i.
1cr im4-d 15.4 88.2 9.4 2.4inrs2-d i00.0 0.0 0.0 100.0mit ics2-d 8.9 92.9 5.0 2.1sr i~-d 7.1 95.0 4.1 0.8Ins U. Err # Utt.1.8 45 .4  4270.8 25.3 4270.5 30.7 4272.4 57.8 4270.0 i00.0 4271.3 38.2 427i.i 34.2 427Descr ipt ionATT Nov 92 SPREC Results Class ABBN Nov 92 SPREC Results Class ACMU Nov 92 SPREC Results Class ACRIM Nov 92 SPREC Results Class AINRS Late Nov 92 SPREC Results Class AMIT-LCS Nov 92 SPREC Results Class ASRI Nov 92 SPREC Results Class AClass X SubsetIns U. Err f Utt.2.4 43.3 2471.3 25.1 247i.i 25.9 2473.6 53.8 2470.0 100.0 2471.8 37.2 2472.1 33.2 247Descr ipt ionATT Nov 92 SPREC Results Class DBBN Nov 92 SPREC Results Class DCMU Nov 92 SPREC Results Class DCRIM Nov 92 SPREC Results Class DINRS Late Nov 92 SPREC Results Class DMIT-LCS Nov 92 SPREC Results Class DSRI Nov 92 SPREC Results Class DW.
Err Corr  Sub Delatt2-x 18.5 85.1 11.3 3.6bbn2-x 14.5 89.2 7.8 3.0cmu2-x 15.6 86.6 7.0 6.5cr im4-x 20.1 74.7 19.7 5.6inrs2-x I00.0 0.0 0.0 i00.0mit Ics2-x 21.7 82.6 12.9 4.6sr i~-x 15.8 88.1 9.4 2.4Ins U. Err | Utt.3.5 70.3 2933.7 59.0 2932.2 59.7 2934.8 81.6 2930.0 i00.0 2934.2 70.6 2934.0 64.8 293Descr ipt ionATT Nov 92 SPREC Results Class XBBN Nov 92 SPREC Results Class XCMU Nov 92 SPREC Results Class XCRIM Nov 92 SPREC Results Class XINRS Late Nov 92 SPREC Results Class XMIT-LCS Nov 92 SPREC Results Class XSRI Nov 92 SPREC Results Class XTable 3: ATIS SPREC Benchmark Test Results15NOV92 AT IS  5PP.EC Test  Resu l t s. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.1 C lass  A%D Subset  11 \[J Or lg lnat lng  S l tS  Of Tes t  Data  l J Overa l l  I Fore lqnI ATT I BBN I CCMU \] MIT  I SRI I I Tota l s  I Coi l .
S i teI (89 Utt . )
i (124 Utt.
l  I (142 Utt.)
I (167 Utt . )
I (152 Utt . )
\]i 674 I Tota l sar t2  l 8.7 5.4 3.01 7.7 1.9 2.II 1.8 2.2 3.01 3.9 i .
I  0.91 118 0.8 1.211 4.6 1.8 2.01 3.9 1.5 1 .8} 15.1 74 .2  } 11 .7  58.1 { 7.0 44.4 J 5 .9  54.1 1 3.8 20.3  li 8.4 44.7 I 7.1 40.2. .
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
-~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
1 I .
.
.
.
.
.
.
.
.
.
-e .
.
.
.
.
.
.
.
.
.
.
.
.
.hhn2  ~ 4.7 1.7 1.9J 4.2 1.4 0.71 1.5 0.8 1.2 l 1.8 0 .3  0.6J 0 .5  0.4 0.4it 2.5 0 .9  0.91 2.0 0.7 l .Oi 8.4 50 .6  ~ 6.3 34.
'7 I 3.5 22.5 1 2 .8  21 .6  \] 1.3 9.2 jJ 4.3 25.2  ~ 3.~ 2~.
I. .
.
.
.
.
.
* .
.
.
.
.
.
.
.
.
.
.
.
.
-t- .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
I I .
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.S Cmlu2 1 5.8 2 .6  1.31 4.1 1.4 O.Ti 1.4 1.4 8.91 1.6 0 .6  0.31 2.0 0.4 0.6~I 2.8 1.2 0.71 3.2 i. i  0 .6Y l 9.7 57 .3  I 6.
I  39.5 I 3.7 21.8 1 2 .5  19.2  1 3.8 21.1 I I 4.7 28 .9  I 5.0 30 .8S .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
-+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
.
.
.
.
.
.
I I .
.
.
.
.
.
.
.
.
.
.
- *  .
.
.
.
.
.
.
.
.
.
.
.
.T c r lm4 J 14 .1  4.4 5.5t 12 .9  5.1 3.11 4.7 1.5 2.11 6.8 2 .2  1.51 4.9 1.4 2.5~J 8.4 2 .8  2.81 8.4 2 .8  2 .8E I 24.0 86 .5  i 21 .1  74.2  I 8.4 38.7 ~ 10 .5  55.1  J 8.8 42 .1  L~ 14.1 56.4  ~ 14.1  56.4. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.S ln rs2  1 0 .0  100 .0  O .01  0 .0  1O0.O 0 .0 i  0 .0  100 .0  0 .01  0 .0  100 .0  O .01  0 .0  100 .0  0 .8~1 O.0  100 .0  0 .01  0 .O  100 .0  0 .0i 100 .0  100 .0  100 .0  100 ,0  I 100 .O 100 .0  I 100 .0  100 .0  I 100 .0  1O0.O I I  i00 .0  100 .0  I 100 .0  100 .0rolE_Its2 I 8.9  3.5 3.51 6.8 2.8 !.81 4.4 2.3 1.ST 1.7 1 .3  0.31 2.3 1.2 0.8J~ 4.5 2.2 1.41 5.4 2.4 1.8I 15 .9  57 .3  11 .4  54.0 I 8 .2  45.0 I 3 .3  19.2  I 4.2 28 .9  I I 8.1 37 .8  J 9.7 44.0. .
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ + .
.
.
.
.
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
.
.
.
.
.
I I .
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.s r l3  ~ 4 .9  1 .5  3.4 i 5.8 1.4 1.2J 3.2 1.0 1-?
l  2.3 0 .3  0.81 1.4 0.3 O.711 3.5 0 .8  1.4~ 3.8 1.0 1 .6J 9.8 61 .8  8 .4  50.0 I 5 .9  33.8 i 3 .4  25 .1  ~ 2.4 13.8 l J 5.7 33.8  i 6.5 39.7=~I~==~===~===~===~=~=====~=~====~=~=~=====~=======~=======~=~===~=~=======~===~=~======"~=====~=========Overa l l  I 6.7 16.7  2.71 5 .9  16.3 1.4T 2.4 15 .6  1.51 2.6 15 .1  0.61 1.8 14 .9  0.9~iTota l s  I 26.1  89 .7  23 .6  58.6  ~ 19.5  43.5 I 18.3  38.2  ~ 17.6  34.8 I I+ .
.
.
.
.
.
.
.
.
.
.
.  '
* .
.
.
.
.
.
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
.
.
.
.
I I  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Forezgm 1 6.4 18 .9  2.61 6 .2  18.8  1.51 2 .6  18.0  1.61 2.7 17 .4  0.71 1 .9  17.4  0.9~I ~ %SUb ~Del %Ins  ISys te Jn  I 27 .9  68 .9  26 .4  82 .6  I 22 .2  47 .1  I 20 .8  42 .5  \[ 20 .2  38 .3  I I  I tW.Er r  %Ut t .E r r  IMatr ix  tabu la t ion  of resu l t s  fo r  the Nov92 AT IS  SPREC Test  Resu l t s t  fo r  the  C lass  A+D Subset .Mat r ix  co lumns  present  resu l t s  ?~r  Tes t  Data  Subsets  co l lec ted  at severa l  s l iest  and  matr lx  rows present  resu l t s  ~or  d l~ferentsys tems.Numbers  pr ln ted  at the  top  of  the  mat r lx  co lumns  Ind lcate  the number  of  u t te rances  in the  Test  Data  (sub)set  f rom the  cor respond ings i te.?
Overa l l  To ta l s "  (column) present  resu l t s  for  the  ent i re  C lass  A+D Subset  fo r  the  sys tem cor respond ing  to  that  mat r ix  row.
"Fore lgn  Col l .
S l te  Tota l s .
p resent  resu l t s  fo r  "~ore lgm s~te"  data  ( l .e.~ exc lud ing  loca l ly  co l lec ted  data)  for  the  C lass  A~DSubset .?
Overa l l  To ta l s "  (row\] p resent  resu l t s  accumulated  over  al l  sys tems cor respond l~g to  the  Tes t  Data  (sub)set  cor respond ing  to  thatmat r ix  co lumn.
"Fore lgn  Sys tem Tota l s "  p resent  resu l t s  accumulated  over  "~ore lgn  sys tems,  (l.e.~ exc lud ing  resu l t s  for  thesys tem(s )  deve loped  at the  s l te  respons ib le  ~or  co l lec t ion  o~ that  Test  Data  subset .
)Tab le  4:  ATIS  SPREC Resu l t s :  C lass  (A,D) by Co l lec t ion  S l teCompos2te  Report: ot A l l  SSgn2?1cance  TestsFor  the  Nov92 AT IS  SPREC C lass  A+D Test  Resu l t s  Tes tTest  Name Abbrev.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Matcbecl Pa.tr Sentence  Segment  (WOES Er ror )  Tes t  MPSl~nned Pa l r~ Compar i son  (8pea~er  Word  Accuracy)  Tes t  SIWl l coxon  S igned  Ran3?
(Speaker  Word  Accuracy)  Tes t  WIMcNemar  (Sentence  Error )  Tes t  MN.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.tl art  ad  I hbn2  a d I cmu2 a d c r lm4 a d l .... 2 ad  ml t  los2  a d ~ sr13 ada t t2 -a_d  1 ~ ~P bbr~2-a_d I Ml ~ cmu2-a  d MP a t t2~a cl J MP a t t2 -a  d ~'~ same I MP  s r13-a  dI I SI bbn2-a  d I $I cmu2-a  d SI a t t2~a d I 5 I  a t t2 -a  d SI same I SI s r13-a  ~i I W~ bbn2-a_d  I WI cmu2-a_d  WI a~t2-a  d I WI a t t2 -a_d  WI same J WI s r :3 -s_d} ~ bbn2-a_d  I ~ Crnu2-a d MN at t2 -a  d I HN at r2 -a  d HN ml t  i cs2 -a  d J MN s r13-a  d. .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
-- .
.
.
.
.
.
.
.
.
.
.
.
.
.
: - -?
.
.
.
.
.
.
.
.
.
.
.
.
D .
.
.
.
.
.
.
.
.
: .
.
.
.
.
: - -+  .
.
.
.
.
.
.
.
.
.
.
.
~__hbn2-a  d I I I ~ sane  MP bbn2-a_d  I ~ bhn2-a  d MP hbn2-a  d I ~ hbn2-a_dI I i SI same SI bbn2-a  d I SI hbn2-a  d $I bbn2-a  d I SI hbn2-a_di i I Wl same WI bbn2-a  d I W~ bbn2-a_d  WI  bDn2-a  d I WI bbn2-a  d\] I I ~ hbn2-a  d MN bbr l2 -a_d  I ~ bbn2-a  d MN hbn2-a  d I MN bbn2-a  d. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: - -~  .
.
.
.
_'_ .
.
.
.
.
.
.
: _ _c~au2-a ~ 1 J I ~I  cmlu2-a d I MP cmlu2-a d HP cmu2-a  d J HP  cmu2-a  d\] I 1 Cmu2-a  d I 8I cmu2-a_d  SI cmu2-ad  I SI sameI ~ I WI Cmu2-a--d I W~ cmnu2-a d WI cmu2-a  d t WZ samei I I ~ Cmu2-s_d  I MM cmu2-a--d ~ clnu2-a--d ~ HN c~lu2-a dc r lm4-a  d ~ ~ I I ~ cr lm4-a  d MP ml t  I cs2 -a  d L MP s r13-a  dI I I I SI c r lm4-a  d $I ml t  i cs2 -a  d I S~ s r l3 -a  ~I I ~ I WI c r lm4-a  d WI ml t  i cs2 -a_d  I W~ sr13-a  dI I I ~ c r lm4-a  d HN ml t  l cs2 -a  d ~ MN s r13-a  d .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
: .
.
.
.
.
.
.
.
.
.
: .
.
.
.
.
:_-~.
.
.
.
.
.
.
.
.
.
.
.
.
_--__~nrs2 -a_d  i I I I MP mlt  I cs2 -a  d \] ~P  s r13-a  dI I i ~ Sl ml t  I cs2 -a  d I S I  s r13-a_d\[ I I I WI ml t  i cs2 -a  d ~ W~ sr !3 -a  dI I !
I ~ ml t~ lcs2 -a_ - -d  ~ HN s r~.3 -a -d. .
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
_ - _ml t  _I cs2 -a_d  1 I I I I Mp Sr13-a  dI i I 1 I Sl srl ~-a~dI J I I I WI Sr l3 -a  dI I I I I HN s r~.3 -a  ds r l3~a._~ I I I I ~ II I ~ I I Ii I I I I I. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,TaJole 5: ~zgnf :cance  Test  ~esu l t s :  i T IS i~REC 3ystemsClass  A+D C lass  A C lass  D674 Utt .
427 Utt.
247 Utt.sys tem W. Err (%)  W. Er r (%)  W. Er r (%)a~t!
42 .4  34 .7  55 .9bbn l  22 .0  15.7 32 .8cmu!
12.3 12 .2  12.6c r lml  71 .2  40 .5  124 .3c r lm2 69.4 50 .1  102 .8c r lm3 49.7 31 .1  81 .8in rs l  101 .5  79 .9  138 .9ml t  !cs l  18.4 18 .3  18 .6paramax 55 .6  44 .0  75 .7s r l !
27 .6  22 .2  36 .8s r l2  23 .6  14 .8  38 .9Tab le  6: AT IS  NL Test  Resu l tsDescr lp t lonATT i  Nov  92 AT IS  NL  Resu l tsBBNi  Nov  92 AT IS  NL Resu l tsCMUl  Nov  92 AT IS  NL  Resu l tsCR IMi  CHANEL  Nov  92 AT IS  NL  Resu l tsCR IM2 CHANEL  CD Nov  92 AT IS  NL  Resu l tsCR IM3 NEURON Nov  92 AT IS  NL  Resu l tsINRS Late  Nov  92 AT IS  NL  Resu l tsM IT  LCS i  Nov  92 AT IS  NL  Resu l tsPARAMAX Nov  92 AT IS  NL  Resu l tsSR I I  TM Nov  92 AT IS  NL  Resu l tsSR I2  GEMIN I+TM Nov  92 AT IS  NL  Resu l tsa t t lbbn lI Class  (A+D) Set  I I  I\] Or lg lnat lng  S l te  o?
Test  Data  I I  Overa l l  I Fore lgnI ATT  I BBN I CMU I MIT  I SRI  II To ta l s  I Co l l .
S l te\[ 89 I 124 I 142 I 167 I 152 II 674 I Tota l s+ + + + + I I\[ 71 14 4 I 79 29 16 1 93 45 4 i 137 25 5 i 135 14 3 il 515 127 32 I 444 113 28I 80 16 , 4 I 64 23 13 i 65 32 3 l 82 15 3 1 89 9 2 Ll 76 19 5 1 76 19 5I 36.0  I 59.7  I 66 .2  I 32.9  I 20.4 I\] 42.4 I 43.4+ .
.
.
.
.
.
.
.
.
.
.
.
.
* .
+ + -4  11 +I 76 3 i0 I 95 15 14 I 116 15 II I 150 5 12 I 136 9 7 II 573 47 54 I 478 32 40I 85 3 i i  i 77 12 I i  I 82 I I  8 L 90 3 7 I 89 6 5 li 85 7 8 I 87 6 7I 18 .0  I 35.5  I 28 .9  ~ 13.2 I 16.4 I I  22.0  I 18 .9+ + + + + \ [ I  +cmul  1 84 5 0 1 I00 20 4 1 138 4 0 1 158 8 1 1 150 2 0 II 630 39 5 i 492 35 5I 94 6 0 1 81 16 3 1 97 3 0 1 95 5 1 ~ 99 1 0 II 93 6 1 1 92 7 1i 11 .2  I 35 .5  I 5.6 I 10 .2  I 2.6 II 12.3  I 14.1+ +, + ~ 4 I I  +cr lml  I 36 17 36 I 67 24 33 I 65 41 36 I 77 28 62 I 91 32 29 li 336 142 196 I 336 142 196i 40 19 40 I 54 19 27 i 46 29 25 I 46 17 37 I 60 21 19 II 50 21 29 i 50 21 29I 78 .7  I 65 .3  1 83 .1  I 70 .7  I 61.2 II 71 .2  { 71 .2. .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
++ ?
.
.
.
.
.
.
.
.
.
.
.
.
.
+ 4 \[I + .
.
.
.
.
.
.
.
.
.
.
.
.c r lm2 I 43 27 19 I 67 39 18 i 69 54 19 I 95 23 49 I 106 31 15 il 380  174 120 I 380 174 1201 48 30 21 I 54 31 15 i 49 38 13 I 57 14 29 I 70 20 I0 il 56 26 18 I 56 26 18S I 82 .0  I 77 .4  ~ 89.4 I 56 .9  I 50 .7  II 69.4  I 69.4Y .
.
.
.
.
.
.
.
.
- .
.
.
.
.
.
.
.
.
.
.
.
.
~+ + .
.
.
.
.
.
.
.
.
.
.
.
~ + II + .
.
.
.
.
.
.
.
.
.
.
.
.S c r lm3 I 63 21 5 i 88 32 4 1 101 39 2 i 119 40 8 i 126 26 0 II 497 158 19 1 497 158 19T I 71 24 6 I 71 26 3 1 71 27 1 1 71 24 5 1 83 17 0 In 74 23 3 I 74 23 3E I 52 .8  I 54 .8  I 56 .3  I 52 .7  i 34.2 II 49 .7  I 49 .7M .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
~ + .
.
.
.
.
.
.
.
.
.
.
.
.
In .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.S in rs l  I 38 47 4 t 51 65 8 1 56 83 3 \] 74 79 14 1 98 53 1 II 317 327 30 I 317 327 30I 43 53 4 i 41 52 6 i 39 58 2 i 44 47 8 1 64 35 1 II 47 49 4 I 47 49 4I i i 0 .
i  I i i i  3 I 119 .0  I 103 .0  \[ 70.4 II 101 .5  I 101 .5. .
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
+ + .
.
.
.
.
.
.
.
.
.
.
.
II .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.m l t  i cs l  i 78 7 4 i 93 21 i0 I 132 8 2 i 154 9 4 I 143 5 4 il 600 50 24 L 446 41 20i 88 8 4 I 75 17 8 I 93 6 1 I 92 5 2 I 94 3 3 I I 89 7 4 I 88 8 4I 20.2  I 41 9 I 12.7 I 13 .2  I 9.2 I I  18.4 I 20 .1. .
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
II .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.paramax !
33 I0 46 1 59 17 48 I 65 37 40 1 I I0 Ii 46 1 121 14 17 II 388 89 197 1 388 89 197I 37 I I  52 ~ 48 14 39 t 46 26 28 1 66 7 28 I 80 9 ii I I  58 13 29 I" 58 13 29I 74 .2  I 66 1 I 80 .3  I 40 .7  I 29.6  I I  55.6  I 55 .6. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
II .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.s r l l  I 69 12 8 !
91 19 14 i 109 17 16 1 144 7 16 i 137 7 8 II 550 62 62 l 413 55 54I 78 13 9 1 73 15 Ii I 77 12 ii I 86 4 i0 1 90 5 5 il 82 9 9 L 79 ii I0I 36.0  \[ 41 9 I 35 .2  i 18 .0  I 14.5 II 27 .6  I 31.4. .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
I I  .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.s r l2  i 74 Ii 4 i 93 16 15 i 108 19 15 \] 150 5 12 I 146 5 1 II 571 56 47 i 425 51 46I 83 12 4 !
75 13 12 i 76 13 Ii I 90 3 7 I 96 3 1 II 85 8 7 l 81 i0 9i 29 .2  I 37 9 I 37.3  I 13.2  i 7.2 ~1 23.6  \] 28.4=================================================================================================================Overa l l  !
665 174 140 i 883 297 184 I i052 362 148 I1368 240 229 I1389 198 85 IITo ta l s  !
68 18 14 I 65 22 13 I 67 23 9 I 74 13 12 I 83 12 5 el!
49 .8  i 57 0 I 55.8 1 38 .6  I 28 .8  II Legend:.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
\[i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Yore lgn  I 594 160 136 I 788 282 170 I 914 358 148 I1214 231 225 11106 186 76 II I #T #F #NA ISystem ~ 67 18 15 I 64 23 14 I 64 25 i0 1 73 14 13 t 81 14 6 I I  { %T ~F ~NA ITota l s  ~ 51.2  I 59 2 I 60.8 I 41 .1  I 32.7 I I  ~ % Welghted  Er ror.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Tab!e  7: AT IS  NL ~esu!
ts :  ~ss  ~A+D) by Co l lec t lon  S l teClass  A+D C lass  A C lass  D674 Ut t .
427 Ut t .
247 Utt .sys tem W. Er r (%)  w. E r r (%)  W. Er r (%)a t t l  82 .8  49 .6  140 .1bbn l  30 .6  23 .7  42 .5cmul  21 .2  19 .7  23 .9c rZml  82 .3  56 .9  126 .3c r lm2 82 .9  66 .3  111 .7Cr lm3 75 .2  57 .1  106 .5in rs l  i00 .0  I00 .0  I00 .0ml t  I cs l  29 .7  30 .4  28 .3s r l~  37 .4  31 .9  47 .0s r l2  33 .2  26 .5  44 .9Tab le  8: AT IS  SLS  Test  Resu l tsDescr !pt lonATT I  Nov  92 AT IS  SLS  Resu l tsBBNi  Nov  92 AT IS  SLS  Resu l tsCMUi  Nov  92 AT IS  SLS  Resu l tsCR IMI  CHANEL Nov  92 AT IS  SLS  Resu l tsCR IM2 CHANEL CD Nov  92 AT IS  SLS  Resu l tsCR IM3 NEURON Nov  92 AT IS  SLS Resu l tsINRS I  LATE Nov  92 AT IS  SLS  Resu l tsM IT  LCS I  Nov  92 AT IS  SLS  Resu l tsSR I \ [  TM Nov  92 AT IS  SLS  Resu l tsSR I2  GEMIN I+TM Nov  92 AT IS  SLS  Resu l tsa t t lI Class  (A+D) Seti Or lg lnat lng  S l te  o f  Test  DataI ATT I BBN I CMU I MIT  I SRIJ 89 \] 124 I 142 I 167 ~ 152~ ~ + +I 35 41 13 i 62 42 20 I 61 76  5 I 98 56 13 I 110 35J 39 46 ,  15 ~ 50 34 16 I 43 54 4 i 59 34 8 ~ 72 23I06 .~ J 83 .9  J 110.6  I 74.9  I 50.7I I  Ii l  Overa l l  I Fore lgnI~ Tota ls  I Col l .
S i teI I  674 I Tota ls7 II 366 250 58 I 331  209 455 II 54 37 9 I 57 36 8II 82 .8  I 79 .1I\] +bbn l  J 60 14 15 i 88 17 19 I 112 22I 67 16 17 S 71 14 15 i 79 15I 48 .3  I 42 .7  I 36 .68 I 147 14 6 i 139 I I6 1 88 8 4 1 91 7I 20 .4  i 15 .8+2 II 546 78 50 I 458  61 311 II 81 12 7 i 83 11 6II 30 .6  I 27 .8-Jl +cmul  I 72 16 I J 92 27 5 i 129  13 0 } 157 9 1 i 149 3 0 II 599 68 7 I 470  55 7l 81 18 1 I 74 22 4 I 91 9 0 I 94 5 1 ~ 98 2 0 II 89 10 1 I 88 10 1I 37 .1  I 47 .6  I 18 .3  I 11 .4  I 3 .9  21 .2  i 22 .0~ ~ ~ + .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+c r lml  i 27 12 50 I 45 34 45 l 59 44 39 i 67 33  67 I 83 39 30 281 162 231 t 281  162 231l 30 13 56 ~ 36 27 36 i 42 31 27 I 40 20 40 I 55 26 20 42 24 34 I 42 24 34I 83 .1  I 91 .1  I 89 .4  i 79 .6  l 71 .1  82 .3  i 82 .3~- ~ ~ +- + .
.
.
.
.
.
.
.
.
.
.
.
.S c r lm2 I 36 18 35 i 43 43 38 I 66 54 22 I 74 31 62 I 89 47 16 308 193 173 I 308  193 173Y ~ 40 20 39 i 35 35  31 i 46 38 15 I 44 19 37 ~ 59 31 ii 46 29 26 I 46 29 26S I 79 .8  I 100 .0  I 91 .5  i 74 .3  i 72 .4  82 .9  ( 82 .9T ~ ~ ~ ~ + .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.E c r lm3 I 46 39 4 I 55 62 7 \[ 88 49  5 I 99 47 21 \[ I I0  34 8 398 231 45 \[ 398  231 45M \[ 52 44 4 \[ 44 50 6 i 62 35 4 1 59 28 13 \[ 72 22 5 59 34 7 1 59 34 7S \[ 92 .1  t 105 .6  I 72 .5  i 68 .9  I 50 .0  75 .2  \[ 75 .2. .
.
.
.
.
.
.
.
.
.
.
.
+ ~ .
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.in rs l  J 0 0 89 i 0 0 124 I 0 0 142 I 0 0 167 I 0 0 152 0 0 674 I 0 0 674I 0 0 i00 1 0 0 100  i 0 0 i00 1 0 0 I00 I 0 0 i00 0 0 I00 { 0 0 i00l i00 .0  1 i00 .0  I i 00 .0  I I00 .0  I i 00 .0  i00 .0  I I00 .0~ .
.
.
.
.
.
.
.
.
.
.
.
~ + -+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4ml t_ i cs l  I 57 12 20 1 79 28 17 i 120 12 I0 I 149 i i  7 I 140 8 4 545 71 58 L 396  60 51i 64 13 22 ~ 64 23 14 I 85 8 7 i 89 7 4 i 92 5 3 81 11 9 I 78 12 10I 49 .4  I 58 .9  I 23 .9  I 17 .4  I 13 .2  29 .7  I 33 .7?
~ ~ + .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.s r l l  { 60 16 13 i 75 27 22 I i01  23 18 i 141 9 17 i 132 12 8 509 87 78 i 377  75 70I 67 18 15 1 60 22 18 1 71 16 13 1 84 5 I0 I 87 8 5 76 13 12' i  72 14 131 50 .6  i 61 .3  J 45 .1  I 21 .0  I 21 .1  37 .4  i 42 .1. .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
~ ~ ~ .
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
,, .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.s r l2  I 65 13 ii 1 75 26 23 i i01  25 16 1 149 6 12 i 139 9 4 529 79 66 i 390  70 621 73 15 12 i 60 21 19 i 71 18 i i I 89 4 7 i 91 6 3 78 12 I0 1 75 13 12i 41 .6  1 60 .5  I 46 .5  i 14 .4  i 14 .5  33 .2  i 38 .7==============================================================================================================Overa l l  I 458  181 251 I 614 306 320 I 837 318 265 I i081 216 373 11091 198 231Tota ls  i 51 20 28 i 50 25 26 i 59 22 19 I 65 13 22 i 72 13 15I 68 .9  I 75 .2  I 63 .5  1 48 .2  I 41 .2  Legend:.
.
.
.
.
.
.
.
.
.
.
.
.
4 ~ + .
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Fore lgn  I 423  140 238 I 526 289 301 i 708  305 265 I 932 205 366 i 820 177 219 i #T #F #NA ISys tem ~ 53 17 30 1 47 26 27 1 55 24 21 1 62 14 24 I 67 15 18 1 ~T %F %NA iTota ls  I 64 .7  I 78 .8  ~ 68 .5  I 51 .6  ~ 47 .1  I ~ Welghted  Er ror  ITab le  9: AT IS  SLS  Resu l ts :  C lass  (A+D) by Co l lec t ion  S l te18
