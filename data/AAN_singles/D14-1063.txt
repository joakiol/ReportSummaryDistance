Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 577?588,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsTranslation Rules with Right-Hand Side LatticesFabien Cromi?resJapan Science and Technology AgencyKawaguchi-shiSaitama 332-0012fabien@pa.jst.jpSadao KurohashiGraduate School of InformaticsKyoto UniversityKyoto 606-8501kuro@i.kyoto-u.ac.jpAbstractIn Corpus-Based Machine Translation,the search space of the translationcandidates for a given input sentenceis often defined by a set of (cycle-free) context-free grammar rules.
Thishappens naturally in Syntax-BasedMachine Translation and Hierarchi-cal Phrase-Based Machine Translation(where the representation will be theset of the target-side half of the syn-chronous rules used to parse the inputsentence).
But it is also possible todescribe Phrase-Based Machine Trans-lation in this framework.
We proposea natural extension to this representa-tion by using lattice-rules that allowto easily encode an exponential num-ber of variations of each rules.
We alsodemonstrate how the representation ofthe search space has an impact on de-coding efficiency, and how it is possibleto optimize this representation.1 IntroductionA popular approach to modern MachineTranslation is to decompose the translationproblem into a modeling step and a searchstep.
The modeling step will consist in defin-ing implicitly a set of possible translations Tfor each input sentence.
Each translation inT being associated with a real-valued modelscore.
The search step will then consist in find-ing the translation in T with the highest modelscore.
The search is non-trivial because it isusually impossible to enumerate all membersof T (its cardinality being typically exponen-tially dependent on the size of the sentence tobe translated).Since at least (Chiang, 2007), a commonway of representing T has been through acycle-free context-free grammar.
In sucha grammar, T is represented as a set ofcontext-free rules such as can be seen on fig-ure 1.
These rules themselves can be gener-ated by the modeling step through the useof phrase tables, synchronous parsing, tree-to-string rules, etc.
If the model score of eachtranslation is taken to be the sum of rule scoresindependently given to each rule, the searchfor the optimal translation is easy with someclassic dynamic programming techniques.However, if the model score is going to takeinto account informations such as the lan-guage model score of each sentence, it cannotbe expressed in such a way.
Since the lan-guage model score has proven empirically tobe a very good source of information, (Chiang,2007) proposed an approximate search algo-rithm called cube pruning.We propose here to represent T usingcontext-free lattice-rules such as shown in fig-ure 2.
This allows us to compactly encode alarge number of rules.
One benefit is that itadds flexibility to the modeling step, makingit easier: many choices such as whether or nota function word should be included, the rela-tive position of words and non-terminal in thetranslation, as well as morphological variationscan be delegated to the search step by encod-ing them in the lattice rules.
While it is truethat the same could be achieved by an explicitenumeration, lattice rules make this easier andmore efficient.In particular, we show that a decoding al-gorithm working with such lattice rules canbe more efficient than one working directly onthe enumeration of the rules encoded in thelattice.A distinct but related idea of this paper isto consider how transforming the structure ofthe rules defining T can lead to improvements577Figure 1: A simple cycle-free context grammardescribing a set of possible translations.in the speed/memory performances of the de-coding.
In particular, we propose a method tomerge and reduce the size of the lattice rulesand show that it translates into better perfor-mances at decoding time.In this paper, we will first define more pre-cisely our concept of lattice-rules, then try togive some motivation for them in the contextof a tree-to-tree MT system (section 3).
In sec-tion 4, we then propose an algorithm for pre-processing a representation given in a lattice-rule form that allows for more efficient search.In section 5, we describe a decoding algorithmspecially designed for handling lattice-rules.In section 6, we perform some experimentsdemonstrating the merit of our approach.2 Notations and TerminologyHere, we define semi-formally the terms wewill use in this paper.
We assume knowledgeof the classic terminology of graph theory andcontext-free grammar.2.1 Expansion rulesA flat expansion rule is the association of anon-terminal and a ?flat?
right hand side thatwe note RHS.
A flat RHS is a sequence ofwords and non-terminal.
See figure 1 for anexample of a set of flat expansion rules.A set of expansion rules is often producedin Hierarchical or Syntax-Based MT, by pars-ing with synchronous grammars or otherwise.In such a case, the set of rules define a rep-resentation of the (weighted) set of possibletranslations T of an input sentence.2.2 LatticeIn the general sense, a lattice can be describedas a labeled directed acyclic graph.
More pre-cisely, the type of lattice that we consider inthis work is such that:?
Edges are labeled by either a word, anon-terminal or an epsilon (ie.
an emptystring).?
Vertices are only labeled by a unique idby which they can be designated.Additionally, edges can also be labeled by areal-valued edge score and some real-valuededge features.
Alternatively, a lattice couldalso be seen as an acyclic Finite State Automa-ton, with vertices and edges corresponding tostates and transitions in the FSA terminology.For simplicity, we also set the constraintthat each lattice has a unique ?start?
ver-tex labeled vSfrom which each vertex can bereached and a unique ?end?
vertex vEthat canbe reached from each vertex.
Each path fromvSto vEdefine thus a flat RHS, with scoreand features obtained by summing the scoreand features of each edge of the path.A lattice expansion rule is similar to a flatexpansion rule, but with the RHS being a lat-tice.
Thus a set of lattice expansion rules canalso define a set of possible translations T ofan input sentence.For a given lattice L, we will often note v ?L a vertex of L and e : v1?
v2?
L an edgeof L going from vertex v1to vertex v2.Figures 2 and 3 show examples of such lat-tices.2.3 Translation set andRepresentationsWe note T a set of weighted sentences.
T is in-tended as representing the set of scored trans-lation candidates generated by a MT systemfor a given input sentence.
As is customary inCorpus-Based MT literature, we will call de-coding the process of searching for the trans-lation with highest score in T .A representation of T , noted RTis a set ofrules in a given formalism that implicitly de-fine T .
As we mentioned earlier, in MT, RTisoften a set of cycle-free context-free grammarrules.In this paper, we consider representationsRTconsisting in a set of lattice expansionrules.
With normal context-free grammar, itis usually necessary that a non-terminal is the578Figure 2: A simple example of lattice rule fornon-terminal X0.
The lower part list the setof ?flat?
rules that would be equivalent to theones expressed by the lattice.left-hand side of several rules.
Using latticeexpansion rules, however, it is not necessary,as one lattice RHS can encode an arbitrarynumber of flat rules (see for example the RHSof X0 in figure 3).
Therefore, we set the con-straint that there is only one lattice expansionrule for each left-hand non-terminal.
And wewill note unambiguously RHS(X) the latticethat is the right hand side of this rule.3 Motivation3.1 SettingThis work was developed mainly in the contextof a syntactic-dependency-based tree-to-treetranslation system described in (Richardson etal., 2014).
Although it is a tree-to-tree sys-tem, we simplify the decoding step by ?flatten-ing?
the target-side tree translation rules intostring expansion rules (keeping track of the de-pendency structure in state features).
Thusour setting is actually quite similar to thatof many tree-to-string and string-to-string sys-tems.
Aiming at simplicity and generality, wewill set aside the question of target-side syn-tactic information and only describe our algo-rithms in a ?tree-to-string?
setting.
We willalso consider a n-gram language model scoreas our only stateful non-local feature.However, this tree-to-tree original settingshould be kept in mind, in particular whenwe describe the issue of the relative positionof heads and dependents in section 3.2.2, assuch issues do not appear as commonly in ?X-to-string?
settings.3.2 Rule ambiguitiesExpansion rules are typically created bymatching part of the input sentence withsome aligned example bilingual sentence.
Thealignment (and the linguistic structure ofthe phrase in the case of Syntax-Based Ma-chine Translation) is then used to produce thetarget-side rule.
However, it is often the casethat it is difficult to fully specify a rule froman example.
Such cases often come from twomain reasons:?
Imperfect knowledge (eg.
it is unclearwhether a given unaligned word shouldbelong to the translation)?
Context dependency (eg.
the question ofwhether ?to be?
should be in plural formor not, depending on its subject in theconstructed translation).In both situation, it seems like it would bebetter to delay the full specification of therule until decoding time, when the decodercan have access to the surrounding context ofthe rule and make a more informed choice.
Inparticular, we can expect features such as lan-guage model or governor-dependent features(in the case of tree-to-tree Machine transla-tion) to help remove the ambiguities.We detail some cases for which we encodevariations as lattice-rule.3.2.1 Non-aligned wordsWhen rules are extracted from aligned exam-ples, we often find some target words whichare not aligned to any source-side word andfor which it is difficult to decide whether ornot they should be included in the rule.
Suchwords are often function words that do nothave an equivalent in the source language.In Japanese-English translations, for example,articles such as ?a?
and ?the?
do not typicallyhave equivalent in the Japanese side, and theirnecessity in the final sentence will often be amatter of context.
We can make these edges579optionals by doubling them with an epsilon-edge.
Different weights and features can begiven to the epsilon edges to balance the ten-dency of the decoder to skip edges.
In figure 2,this is illustrated by the epsilon edges allowingto skip ?for?
and ?the?3.2.2 Non-terminal positionsIn the context of our tree-to-tree translationsystem, we often find that we know which tar-get word should be the governor of a givennon-terminal, but that we are unsure of theorder of the words and non-terminals sharinga common governor.
It can be convenient torepresent such ambiguities in a lattice formatas shown in figure 2.
In this figure, one can seethat the RHS of X0 encode two possible order-ing for the word ?bus?
and the non-terminalX2.3.2.3 Word variationsLinguistics phenomenons such as morpholog-ical variations can naturally create many mi-nor problems in the setting of Corpus-BasedTranslation.
Especially if the variations inthe target language have no equivalence inthe source language.
An example of this inJapanese-English translation is the fact thatverbs in Japanese are ?plural-independent?,while the verb ?to be?
in English is not.
There-fore, a RHS that is a candidate for translatinga large part of a Japanese input sentence caneasily use one of the variant of ?to be?
that isnot consistent with the full sentence.
To solvethis, for each edge corresponding to the words?is?
or ?are?, we add an alternative edge withthe same start and end vertices as the otherword.
The decoder will then be able to choosethe edge that gives the best language modelscore.
The same can be done, for example, forthe article ?a/an?.
Figure 2 provides an exam-ple of this, with two edges ?is?
and ?are?
inthe RHS of X0.Alternative edges can be labeled with differ-ent weights and features to tune the tendencyof the decoder to choose a morphological vari-ation.While such variations could be fixed in apost-processing step, we feel it is a better op-tion to let the decoder be aware of the possibleoptions, lest it would discard rules due to lan-guage model considerations when these rulesFigure 3: The lattice RHS(X0) optimized withthe algorithm described in section 4could actually have been useful with a simplechange.4 Representation optimisation4.1 GoalGiven a description as a set of rule and scoresR1Tof T , it is often possible to find another de-scription R2Tof T having the same formalismbut a different set of rules.
Although the Tthat is described remains the same, the samesearch algorithm applied to R1Tor R2Tmightmake approximations in a different way, befaster or use less memory.It is an interesting question to try to trans-form an initial representation R1Tinto a rep-resentation R2Tthat will make the search stepfaster.
This is especially interesting if one isgoing to search the same T several times, as isoften done when one is fine-tuning the param-eters of a model, as this representation opti-misation needs only be done once.The optimisation we propose is a natural fitto our framework of lattice rules.
As lattice area special case of Finite-State Automata (FSA),it is easy to adapt existing algorithms for FSAminimization.
We describe a procedure in al-gorithm 1, which is essentially a simplificationand adaptation to our case of the more gen-eral algorithm of (Hopcroft, 1971) for FSA.The central parts of the algorithm are the twosub-procedures backward vertex merging andforward vertex merging.
An example of theresult of an optimisation is given on figure 3.580Data: Representation RTResult: Optimized Representation1 for non-terminal X ?
RTdo2 Apply backward vertex merging toRHS(X);3 Apply forward vertex merging toRHS(X);4 endAlgorithm 1: Representation optimisation4.2 Forward and backward mergingWe describe the forward vertex merging inalgorithm 2.
This merging will merge ver-tices and suppress redundant edges, proceed-ing from left to right.
The end result is alattice with a reduced number of vertices andedges, but encoding the same paths as the ini-tial one.The basic idea here is to check the verticesfrom left to right and merge the ones that haveidentical incoming edges.
After having beenprocessed by the algorithm, a vertex is put inthe set P (line 9).
At each iteration, the can-didate set C contains the set of vertices thatcan potentially be merged together.
It is up-dated at each iteration to contain the set ofnot-yet-processed vertices for which all incom-ing edges come from processed vertices (doneby marking edges at line 6 and then updatingC at line 10).
At each iteration, the mergingprocess consists in:1.
Eliminating duplicate edges from the pro-cessed vertices to the candidate vertices(line 5).
These duplicate edges could havebeen introduced by the merging of previ-ously processed vertices.2.
Merging vertices whose set of incom-ing edges is identical.
Here, merg-ing two vertices v1and v2meansthat we create a third vertex v3such that incoming(v3) = incoming(v1)= incoming(v2), and outgoing(v3) =outgoing(v31) ?
outgoing(v2), then re-move v1and v2.The backward vertex merging is definedsimilarly to the forward merging, but with go-ing right to left and inverting the role of theincoming and outgoing edges.Data: Lattice RHS LResult: Optimized Lattice RHS1 P ?
?
//processed vertices;2 C ?
{vS} //candidate set ;3 while |C| > 0 do4 for v ?
C do5 Eliminate duplicate edges inincoming(v);6 Mark edges in outgoing(v);7 end8 Merge all vertices v1, v2?
C such thatincoming(v1) = incoming(v2);9 P ?
P?C;10 C ?
{v ?
L?
P s.t.
all edges inincoming(v) are marked};11 endAlgorithm 2: Forward Vertex Merging4.3 Optimizing the wholerepresentationAlgorithm 1 describe the global optimisationprocedure.
For each lattice RHS, we just per-form first a backward merge and then a for-ward merge.We have set the constraint in section 2.3that each non-terminal should have only onelattice RHS.
Note here that if there are sev-eral RHS for a given non-terminal, we can firstmerge them by merging their start vertex andend vertex, then apply this optimisation al-gorithm to obtain a representation with oneoptimised RHS per non-terminal.This optimisation could be seen as doingsome form of hypothesis recombination, but of-fline.In term of rule optimisations, we only con-sider here transformations that do not mod-ify the number of non-terminals.
But it isworthwhile to note that there are some se-quence appearing in the middle of some rulesthat cannot be merged through a lattice rep-resentation, but could be factored as sub-rulesappearing in different non-terminals.
Indeed,a lattice rule could actually be encoded as aset of ?flat?
rules by introducing a sufficientnumber of non-terminals, but this could pos-sibly be less efficient from the search algorithmpoint of view.
We plan to investigate the ef-fects of this type of rule optimisations in con-junction with the described lattice-type opti-581misations in the future.4.4 Handling of Edge FeaturesIn the context of parameter tuning, we usuallywant the decoder to output not only the trans-lations, but also a list of features characteriz-ing the way the translation was constructed.Such features are, for example, the number ofrules used, the language model of the transla-tion, etc.
In out context, some features will bedependent on the specific edges used in a rule.For example, the epsilon edge used to option-ally skip non-aligned words (see section 3.2.1)is labeled with a feature ?nb-words-skipped?set to 1, so that we can obtain the numberof words skipped in a given translation andtune a score penalty for skipping such words.Similar features also exist for picking a wordvariation (section 3.2.3).In the description of the merging processof section 4.2, one should thus be aware thattwo edges are to be considered identical onlyif both their associated word and their set offeature values are identical.
This can some-times prevent useful merging of states to takeplace.
A solution to this could be to follow(de Gispert et al., 2010) and to discard allthese features information during the decod-ing.
The features values are then re-estimatedafterward by aligning the translation and theinput with a constrained version of the de-coder.We prefer to actually keep track of the fea-tures values, even if it can reduce the efficiencyof vertex merging.
In that setting, we can alsoadapt the so-called Weight Pushing algorithm(Mohri, 2004) to a multivalues case in orderto improve the ?mergeability?
of vertices.
Theresults of section 6.1 shows that it is still pos-sible to strongly reduce the size of the latticeseven when keeping track of the features values.5 Decoding algorithmIn order to make an optimal use of theselattice-rule representations, we developed adecoding algorithm for translation candidatesets represented as a set of lattice-rules.
Forthe most part, this algorithm re-use many ofthe techniques previously developed for decod-ing translation search spaces, but adapt themto our setting.5.1 OverviewThe outline of the decoding algorithm is de-scribed by algorithm 3.
For simplicity, thedescription only compute the optimal modelscore over the translations in the candidate set.It is however trivial to adapt the descriptionto keep track of which sentence correspond tothis optimal score and output it instead of thescore.
Likewise, using the technique describedin (Huang and Chiang, 2005), one can easilyoutput k-best lists of translations.
For sim-plicity again, we consider that a n-gram lan-guage model score is the only stateful non-local feature used for computing the modelscore, although in a tree-to-tree setting, otherfeatures (local in a tree representation but notin a string representation) could be used.
Themodel score of a translation t has therefore theshape:score(t) = ?
?
lm(t) +?escore(e)where ?
is the weight of the language model,lm(t) is the language model log-probability oft and the sum is over all edges e crossed toobtain t.5.2 Scored language model statesConceptually, in a lattice L, at each vertexv, we can consider the partial translations ob-tained by starting at vSand concatenating thewords labeling each edge not labeled by a non-terminal until v. If an edge is labeled by a non-terminal X, we first traverse the correspond-ing lattice RHS(X) following the same pro-cess.
Such a partial translation can be reducedcompactly to a scored language model state(l, r, s), where l represent the first n words1 ofthe partial translation, r its last n words and sits partial score.
It is clear that if two partialtranslations have the same l and r parts butdifferent score, we can discard the one withthe lowest score, as it cannot be a part of theoptimal translation.Further, using the state reduction tech-niques described in (Li and Khudanpur, 2008)and (Heafield et al., 2011), we can often reducethe size of l and r to less than n, allowing fur-ther opportunities for discarding sub-optimal1n being the order of the language mode582partial translations.
For better behavior dur-ing the cube-pruning step of the algorithm (seelater), the partial score s of a partial transla-tion includes rest-costs estimates (Heafield etal., 2012).We define the concatenation operationon scored language model states to be:(l1, r1, s1) ?
(l2, r2, s2) = (l3, r3, s3), wheres3= s1+ s2+ ?lm(r1, l2), with lm(r1, l2) be-ing the language model probability of l2givenr1with rest-costs adjustments.
r3and l3arethe resulting minimized states.
Similarly, ifan edge e is labeled by a word, we definethe concatenation of a scored state with anedge to be (l1, r1, s1) ?
e = (l2, r2, s2) wheres2= s1+ score(e) + ?lm(word(e)|r1).Conveniently for us, the KenLM2 open-source library (Heafield, 2011) provides func-tionalities for easily computing such concate-nation operations.5.3 AlgorithmHaving defined these operations, we can nowmore easily describe algorithm 3.
Each vertexv has a list best[v] of the scored states of thebest partial translations found to be endingat v. On line 1, we initialize best[vS] with(., ., 0), where ?.?
represent an empty languagemodel state.
We then traverse the vertices ofthe lattice in topological order.For each edge e : v1?
v2, we compute newscored states for best[v2] as follow:?
if e is labeled by a word or an epsilon, wecreate a state st2= st1?
e for each st1inbest[v1] (line 10).?
if e is labeled by a non-terminal X, we re-cursively call the decoding algorithm onthe lattice RHS(X).
The value returnedby the line 15 will be a set of states corre-sponding to optimal partial translationstraversing RHS(X).
We can concate-nate these states with the ones in best[v1]to obtain states corresponding to partialtranslations ending at v2(line 6).Results of the calls decode(X) are memo-ized, as the same non-terminal is likely to ap-pear in several edges of a RHS and in severalRHS.2http://kheafield.com/code/kenlm/Lines 5 and 6 are the ?cube-pruning-like?part of the algorithm.
The function pruneKreturns the K best combinations of statesin best[v] and decode(RHS(X)), where bestmeans ?whose sum of partial score is highest?.It can be implemented efficiently through thealgorithms proposed in (Huang and Chiang,2005) or (Chiang, 2007).The L ?maxst operation on lines 6 and10 has the following meaning: L is a list ofscored language model state and st is a scoredlanguage model state.
L?maxst means that,if L already contains a state st2with same leftand right state as st, L is updated to containonly the scored state with the maximum score.If L do not contain a state similar to st, st insimply inserted into L. This is the ?hypothe-sis recombination?
part of the algorithm.
Thefunction truncK?
truncate the list best[v] to itsK?
highest-scored elements.The final result is obtained by callingdecode(X0), where X0is the ?top-level?
non-terminal.
The result of decode(X0) willcontain only one scored state of the form(BOS,EOS, s), with s being the optimalscore.The search procedure of algorithm 3 couldbe described as ?breadth-first?, since we sys-tematically visit each edge of the lattice.
Analternative would be to use a ?best-first?search with an A*-like procedure.
We havetried this, but either because of optimisationissues or heuristics of insufficient qualities, wedid not obtain better results than with the al-gorithm we describe here.6 EvaluationWe now describe a set of experiments aimedat evaluating our approach.We use the Japanese-English data from theNTCIR-10 Patent MT task3 (Goto et al.,2013).
The training data contains 3 millionsparallel sentences for Japanese-English.6.1 Effect of Lattice Representationand OptimisationWe first evaluate the impact of the lattice rep-resentation on the performances of our decod-ing algorithm.
This will allow us to measure3http://ntcir.nii.ac.jp/PatentMT-2/583Data: Lattice RHS LResult: Sorted list of best states1 best[vE] = {(.,.,0.0)};2 for vertex v ?
L in topological order do3 for edge e : v ?
v2?
outgoing(v) do4 if label(e) = X then5 for st1, st2?
pruneK(best[v],decode(RHS(X)) do6 best[v2]?maxst1?
st2;7 end8 else9 for st ?
truncK?
(best[v]) do10 best[v2]?maxst?
e;11 end12 end13 end14 end15 return best[vE];Algorithm 3: Lattice-rule decoding.
Seebody for detailed explanations.the benefits of our compact lattice represen-tation of rules, as well as the benefits of therepresentation optimisation algorithm of sec-tion 4.We use our Syntactic-dependency system togenerate a lattice-rule representation of thepossible translations of the 1800 sentences ofthe development set of the NTCIR-10 PatentMT task.
We then produce two additional rep-resentations:1.
An optimized lattice-rule representationusing the method described in section 4.2.
An expanded representation, that un-fold the original lattice-rule representa-tion into ?flat rules?
enumerating eachpath in the original lattice-rule represen-tation (like the listX0?
enumerate the lat-tice X0 in figure 2).Table 1 shows 3 columns.
One for each ofthese 3 representations.
We can see that, asexpected, the performances in term of averagesearch time or peak memory used are directlyrelated to the number of vertices and edgesin the representation.
We can also see thatour representation optimisation step is quiteefficient, since it is able to divide by two thenumber of vertices in the representation, onaverage.
This leads to a 2-fold speed improve-ment in the decoding step, as well as a largereduction of memory usage.6.2 Decoding performancesIn order to further evaluate the merit of ourapproach, we now compare the results ob-tained by using our decoder with lattice-ruleswith using a state-of-the-art decoder on theset of flat expanded rules equivalent to theselattice rules.We use the decoder described in (Heafieldet al., 2013), which is available under an open-source license4 (henceforth called K-decoder).In this experience, we expanded the latticerules generated by our MT system for 1800sentences into files having the required formatfor the K-decoder.
This basically mean wecomputed an equivalent of the expanded rep-resentation of section 6.1.
This process gener-ated files ranging in size from 20MB to 17GBdepending on the sentence.
We then ran theK-decoder on these files and compared the re-sults with our own.
We used a beam-widthof 10000 for the K-decoder.
Experiments wererun in single thread mode.
Partly to obtainmore consistent results, and partly because theK-decoder was risking using too much memoryfor our system.The results on table 3 show that, as the K-decoder do not have access to a more compactrepresentation of the rules, it end up needinga much larger amount of memory for decodingthe same sentences.In term of model score obtained, the perfor-mances are quite similar, with the lattice-ruledecoder providing slightly better model score.It is interesting to note that, on ?fair-ground?
comparison, that is if our decoder donot have the benefit of a more compact lattice-rule representation, it actually perform quiteworse as we can see by comparing with thethird column of table 1 (at least in term of de-coding time and memory usage, while it wouldstill have a very slight edge in term of modelscore with the selected settings).
On the otherhand, the K-decoder is a rather strong base-line, shown to perform several times fasterthan a previous state-of-the-art implementa-tion in (Heafield et al., 2013).
It is well opti-4http://kheafield.com/code/search/584Representation: Original Optimized ExpandedPeak memory used 39 GB 16GB 85GBAverage search time 6.13s 3.31s 9.95s#vertices (avg/max) 65K (1300K) 32K (446K) 263K (5421K)#edges (avg/max) 92K (1512K) 83K (541K) 263K (5421K)Table 1: Impact of the lattice representation on performances.System JA?ENLattice 29.43No-variations 28.91Moses (for scale) 28.86Table 2: Impact on BLEU of using flexiblelattice rules.mized and makes use of advanced techniqueswith the language model (as the one describedin (Heafield et al., 2013)) for which we do nothave implemented an equivalent yet.
There-fore, we are hopeful we can further improveour decoder in the future.Also, note that, for practical reason, whilewe only measured the decoding time for ourdecoder 5, the K-decoder time include the timetaken for loading the rule files.6.3 Translation qualityFinally, we evaluate the advantages of ex-tracting lattice rules such as proposed in sec-tion 3.
That is, we consider rules for whichnull-aligned words are bypassable by epsilon-edges, for which Non-terminal are allowed totake several alternative positions around theword that is thought to be their governor, andfor which we consider alternative morphologiesof a few words (?is/are?, ?a/an?).
We comparethis approach with heuristically selecting onlyone possibility for each variation present in thelattice rule extracted from a single example.Results shown on figure 2 show that wedo obtain a significant improvement in trans-lation quality.
Note that the Moses score(Koehn et al., 2007), taken from the official re-sults of NTCIR-10 is only here ?for scale?, asour MT system uses a quite different pipeline.5in particular, we factored out the representationoptimisation time, which is reasonable if we are in thesetting of a parameter tuning step in which the samesentences are translated repeatedly7 Related workSearching for the most optimal translation inan implicitly defined set has been the focus ofa lot of research in Machine Translation andit would be difficult to cover all of it.
Amongthe most influential approaches, (Koehn et al.,2003) was using a form of stack based de-coding for Phrase-Based Machine Translation.
(Chiang, 2007) introduced the cube-pruningapproach, which has been further improvedin the previously mentioned (Heafield et al.,2013).
(Rush and Collins, 2011) recently pro-posed an algorithm promising to find the op-timal solution, but that is rather slow in prac-tice.Weighted Finite State Machines have seena variety of use in NLP (Mohri, 1997).
Morespecifically, some other previous work on Ma-chine Translation have used lattices (or moregenerally Weighted Finite State Machines).
Inthe context of Corpus-Based Machine Trans-lation, (Knight and Al-Onaizan, 1998) was al-ready proposing to use Weighted Transducersto decode the ?IBM?
models of translation(Brown et al., 1993).
(Casacuberta and Vi-dal, 2004) and (Kumar et al., 2006) also pro-pose to directly model the translation processwith Finite State Transducers.
(Graehl andKnight, 2004) propose to use Tree Transducersfor modeling Syntactic Machine Translation.These approaches are however based on differ-ent paradigm, typically trying to directly learna transducer rather than extracting SCFG-likerules.Closer to our context, (de Gispert et al.,2010) propose to use Finite-State Transducersin the context of Hierarchical Phrase BasedTranslation.
Their method is to iterativelyconstruct and minimize the full ?top-level lat-tice?
representing the whole set of translationsbottom-up.
It is an approach more focusedon the Finite State Machine aspect than our,585System K-decoder Lattice-rule decoderPeak memory used 52G 16GAverage search time 3.47s 3.31sAverage model score -107.55 -107.39Nb wins 401 579Table 3: Evaluation of the performances of our lattice-rule decoder compared with a state-of-the-art decoder using an expanded flat representation of the lattice rules.
?Nb wins?
is thenumber of times one of the decoder found a strictly better model score than the other one, outof 1800 search.which is more of an hybrid approach that stayscloser to the paradigm of cube-pruning.
Themerit of their approach is that they can applyminimization globally, allowing for more possi-bilities for vertex merging.
On the other hand,for large grammars, the ?top-level lattice?
willbe huge, creating the need to prune verticesduring the construction.
Furthermore, thecomposition of the ?top-level lattice?
with alanguage model will imply redundant compu-tations (as lower-level lattices will potentiallybe expanded several times in the top-level lat-tice).
As we do not construct the global latticeexplicitly, we do not need to prune vertices (weonly prune language model states).
And eachedge of each lattice rule is crossed only onceduring our decoding.Very recently, (Heafield et al., 2014) alsoconsidered using the redundancy of translationhypotheses to optimize phrase-based stack de-coding.
To do so, they group the partial hy-potheses in a trie structure.We are not aware of other work proposing?lattice rules?
as a native format for express-ing translational equivalences.
Work like (deGispert et al., 2010) rely on SCFG rules cre-ated along the (Chiang, 2007) approach, whilework like (Casacuberta and Vidal, 2004) adopta pure Finite State Transducer paradigm (thuswithout explicit SCFG-like rules).8 ConclusionThis work proposes to use a lattice-rule repre-sentation of the translation search space withtwo main goals:?
Easily represent the translation ambigui-ties that arise either due to lack of contextor imperfect knowledge.?
Have a method for optimizing the repre-sentation of a search space to make thissearch more efficient.We demonstrate that many types of am-biguities arising when extracting translationrules can easily be expressed in this frame-work, and that making these ambiguities ex-plicit and solvable at compile time throughlattice-rules leads to improvement in transla-tion quality.We also demonstrate that making a directuse of the lattice-rules representation allows adecoder to perform better than if working onthe expanded set of corresponding ?flat rules?.And we propose an algorithm for computingmore efficient representations of a translationcandidate set.We believe that the the link between therepresentation of a candidate set and the de-coding efficiency is an interesting issue andwe intend to explore further the possibilitiesof optimizing representations both in the con-texts we considered in this paper and in otherssuch as Phrase-Based Machine Translation.The code of the decoder we implemented forthis paper is to be released under a GPL li-cense6.AcknowledgementsThis work is supported by the Japanese Sci-ence and Technology Agency.
We want tothank the anonymous reviewers for many veryuseful comments.ReferencesPeter F Brown, Vincent J Della Pietra, StephenA Della Pietra, and Robert L Mercer.
1993.
Themathematics of statistical machine translation:6http://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/586Parameter estimation.
Computational linguis-tics, 19(2):263?311.Francisco Casacuberta and Enrique Vidal.
2004.Machine translation with inferred stochasticfinite-state transducers.
Computational Linguis-tics, 30(2):205?225.David Chiang.
2007.
Hierarchical phrase-based translation.
computational linguistics,33(2):201?228.Adri?
de Gispert, Gonzalo Iglesias, GraemeBlackwood, Eduardo R Banga, and WilliamByrne.
2010.
Hierarchical phrase-based transla-tion with weighted finite-state transducers andshallow-n grammars.
Computational linguistics,36(3):505?533.Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita,and Benjamin K Tsou.
2013.
Overview of thepatent machine translation task at the ntcir-10 workshop.
In Proceedings of the 10th NT-CIR Workshop Meeting on Evaluation of Infor-mation Access Technologies: Information Re-trieval, Question Answering and Cross-LingualInformation Access, NTCIR-10.Jonathan Graehl and Kevin Knight.
2004.
Train-ing tree transducers.
In Proceedings of HLT-NAACL.Kenneth Heafield, Hieu Hoang, Philipp Koehn,Tetsuo Kiso, and Marcello Federico.
2011.Left language model state for syntactic ma-chine translation.
In Proceedings of the Inter-national Workshop on Spoken Language Trans-lation, pages 183?190, San Francisco, California,USA, December.Kenneth Heafield, Philipp Koehn, and Alon Lavie.2012.
Language model rest costs and space-efficient storage.
In Proceedings of the JointConference on Empirical Methods in NaturalLanguage Processing and Computational Natu-ral Language Learning, pages 1169?1178, JejuIsland, Korea, July.Kenneth Heafield, Philipp Koehn, and Alon Lavie.2013.
Grouping language model boundarywords to speed k-best extraction from hyper-graphs.
In Proceedings of the 2013 Conferenceof the North American Chapter of the Asso-ciation for Computational Linguistics: HumanLanguage Technologies, pages 958?968, Atlanta,Georgia, USA, June.Kenneth Heafield, Michael Kayser, and Christo-pher D. Manning.
2014.
Faster Phrase-Baseddecoding by refining feature state.
In Proceed-ings of the Association for Computational Lin-guistics, Baltimore, MD, USA, June.Kenneth Heafield.
2011.
KenLM: faster andsmaller language model queries.
In Proceedingsof the EMNLP 2011 Sixth Workshop on Statis-tical Machine Translation, pages 187?197, Edin-burgh, Scotland, United Kingdom, July.John Hopcroft.
1971.
An n log n algorithm forminimizing states in a finite automaton.
Theoryof Machines and Computations, pages 189?196.Liang Huang and David Chiang.
2005.
Better k-best parsing.
In Proceedings of the Ninth In-ternational Workshop on Parsing Technology,pages 53?64.
Association for Computational Lin-guistics.Kevin Knight and Yaser Al-Onaizan.
1998.
Trans-lation with finite-state devices.
In Machinetranslation and the information soup, pages 421?437.
Springer.Philipp Koehn, Franz Josef Och, and DanielMarcu.
2003.
Statistical phrase-based transla-tion.
In Proceedings of the 2003 Conference ofthe North American Chapter of the Associationfor Computational Linguistics on Human Lan-guage Technology-Volume 1, pages 48?54.
Asso-ciation for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, et al.
2007.
Moses: Opensource toolkit for statistical machine translation.In Proceedings of the 45th Annual Meeting of theACL on Interactive Poster and DemonstrationSessions, pages 177?180.
Association for Com-putational Linguistics.Shankar Kumar, Yonggang Deng, and WilliamByrne.
2006.
A weighted finite state transducertranslation template model for statistical ma-chine translation.
Natural Language Engineer-ing, 12(01):35?75.Zhifei Li and Sanjeev Khudanpur.
2008.
A scal-able decoder for parsing-based machine transla-tion with equivalent language model state main-tenance.
In Proceedings of the Second Workshopon Syntax and Structure in Statistical Trans-lation, pages 10?18.
Association for Computa-tional Linguistics.Mehryar Mohri.
1997.
Finite-state transducers inlanguage and speech processing.
Computationallinguistics, 23(2):269?311.Mehryar Mohri.
2004.
Weighted finite-statetransducer algorithms.
an overview.
In For-mal Languages and Applications, pages 551?563.Springer.John Richardson, Fabien Cromi?res, ToshiakiNakazawa, and Sadao Kurohashi.
2014.
Ky-otoebmt: An example-based dependency-to-dependency translation framework.
In Proceed-ings of ACL (System Demonstration), Balti-more, MD, USA, June.587Alexander M Rush and Michael Collins.
2011.Exact decoding of syntactic translation mod-els through lagrangian relaxation.
In Proceed-ings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: HumanLanguage Technologies-Volume 1, pages 72?82.Association for Computational Linguistics.588
