A PARSING ALGORITHM FOR UNIFICATION GRAMMARAndrew HaasDepartment of Computer  ScienceState University of New York at AlbanyAlbany,  New York 12222We describe a table-driven parser for unification grammar that combines bottom-up construction ofphrases with top-down filtering.
This algorithm works on a class of grammars called depth-boundedgrammars, and it is guaranteed to halt for any input string.
Unlike many unification parsers, ouralgorithm works directly on a unification grammar--it does not require that we divide the grammar intoa context-free "backbone" and a set of feature agreement constraints.
We give a detailed proof ofcorrectness.
For the case of a pure bottom-up parser, our proof does not rely on the details of unification- - i t  works for any pattern-matching technique that satisfies certain simple conditions.1 INTRODUCTIONUnrestricted unification grammars have the formalpower of a Turing machine.
Thus there is no algorithmthat finds all parses of a given sentence in any unifica-tion grammar and always halts.
Some unification gram-mar systems just live with this problem.
Any generalparsing method for definite clause grammar will enter aninfinite loop in some cases, and it is the task of thegrammar writer to avoid this.
Generalized phrase struc-ture grammar avoids the problem because it has onlythe formal power of context-free grammar (Gazdar et al1985), but according to Shieber (1985a) this is notadequate for describing human language.Lexical functional grammar employs a better solu-tion.
A lexical functional grammar must include afinitely ambiguous context-free grammar, which we willcall the context-free backbone (Barton 1987).
A parserfor lexical functional grammar first builds the finite setof context-free parses of the input and then eliminatesthose that don't  meet the other requirements of thegrammar.
This method guarantees that the parser will halt.This solution may be adequate for lexical functionalgrammars, but for other unification grammars finding afinitely ambiguous context-free backbone is a problem.In a definite clause grammar, an obvious way to build acontext-free backbone is to keep only the topmostfunction letters in each rule.
Thus the rules ----> np(P,N) vp(P ,N)becomess - ->npvp(In this example we use the notation of Pereira andWarren 1980, except hat we do not put square bracketsaround terminals, because this conflicts with standardnotation for context-free grammars.)
Suppose we use asimple X-bar theory.
Let major-category (Type, Bar-level) denote a phrase in a major category.
A nounphrase may consist of a single noun, for instance, John.This suggests a rule like this:major-category (n,2) --~ major-category (n, 1)In the context-free backbone this becomesmajor-category --* major-categoryso the context-free backbone is infinitely ambiguous.One could devise more elaborate xamples, but this onesuffices to make the point: not every natural unificationgrammar has an obvious context-free backbone.
There-fore it is useful to have a parser that does not require usto find a context-free backbone, but works directly on aunification grammar (Shieber 1985b).We propose to guarantee that the parsing problem issolvable by restricting ourselves to depth-boundedgrammars.
A unification grammar is depth-bounded iffor every L > 0 there is a D > 0 such that every parsetree for a sentential form of L symbols has depth lessthan D. In other words, the depth of a tree is boundedby the length of the string it derives.
A context-freegrammar is depth-bounded if and only if every string ofsymbols is finitely ambiguous.
We will generalize thenotion of finite ambiguity to unification grammars andshow that for unification grammars, depth-boundednessis a stronger property than finite ambiguity.Copyright 1989 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is granted providedthat the copies are not made for direct commercial dvantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, or to republish, requires a fee and/or specific permission.0362-613X/89/010219-232503.00Computational Linguistics, Volume 15, Number 4, December 1989 219Andrew Haas A Parsing Algorithm for Unification GrammarDepth-bounded unification grammars have more for-mal power than context-free grammars.
As an examplewe give a depth-bounded grammar for the language xx,which is not context-free.
Suppose the terminal symbolsare a through z.
We introduce function letters a 'through z' to represent the terminals.
The rules of thegrammar are as follows, with e denoting the emptystring.s ~ x(L)x(L)x(cons(A,L)) ~ pre-terminal(A) x(L)x(nil) ~ epre-terminal(a') ~ apre-terminal(z') ~ zThe reasoning behind the grammar should be c lear- -x(cons(a',cons(b',nil))) derives ab,  and the first ruleguarantees that every sentence has the form xx.
Thegrammar is depth-bounded because the depth of a tree isa linear function of the length of the string it derives.
Asimilar grammar can derive the crossed serial dependen-cies of Swiss German, which according to Shieber(1985a) no context-free grammar can derive.
It is clearwhere the extra formal power comes from: a context-free grammar has a finite set of nonterminals, but aunification grammar can build arbitrarily large nonter-minal symbols.It remains to show that there is a parsing algorithmfor depth-bounded unification grammars.
We have de-veloped such an algorithm, based on the context-freeparser of Graham et al 1980, which is a table-drivenparser.
I f  we generalize the table-building algorithm to aunification grammar in an obvious way, we get analgorithm that is guaranteed to halt for all depth-bounded grammars (not for all unification grammars).Given that the tables can be built, it is easy to show thatthe parser halts on every input.
This is not a specialproperty of our parser - -a  straightforward bottom-upparser will also halt on all depth-bounded grammars,because it builds partial parse trees in order of theirdepth.
Our contribution is to show that a simple algo-rithm will verify depth-boundedness when in fact itholds.
If  the grammar is not depth-bounded, the table-building algorithm will enter an infinite loop, and it is upto the grammar writer to fix this.
In practice we havenot found this troublesome, but it is still an unpleasantproperty of our method.
Section 7 will describe apossible solution for this problem.Sections 2 and 3 of this paper define the basicconcepts of our formalism.
Section 4 proves the sound-ness and completeness of our simplest parser, whichis purely bottom-up and excludes rules with emptyright-hand sides.
Section 5 admits rules with emptyright sides, and section 6 adds top-down filtering.
Sec-tion 7 discusses the implementation a d possible exten-sions.2 BASIC CONCEPTSThe following definitions are from Gallier 1986.
Let S bea finite, nonempty set of sorts.
An S-ranked alphabet isa pair (E,r) consisting of a set E together with a functionr :E ~ S* x S assigning a rank (u,s) to each symbol f in~'~.
The string u in S* is the arity o f f  and s is the type ofJ;The S-ranked alphabets used in this paper have thefollowing property.
For every sort s E S there is acountably infinite set V s of symbols of sort s calledvariables.
The rank of each variable in V S is (e,s),  wheree is the empty string.
Variables are written as stringsbeginning with capitals for instance X, Y, Z. Symbolsthat are not variables are called function letters, andfunction letters whose arity is e are called constants.There can be only a finite number of function letters inany sort.The set of terms is defined recursively as follows.For every symbol f of rank (Ul ...Un, S) and any termst~...tn, with each ti of sort ui, f(t~ .... tn) is a term of sorts.
Since every sort in S includes variables, whose arityis e, it is clear that there are terms of every sort.A term is called a ground term if it contains novariables.
We make one further requirement on ourranked alphabets: that every sort contains a groundterm.
This can be guaranteed by just requiring at leastone constant of every sort.
It is not clear, however, thatthis solution is linguistically acceptable- -we do notwisla to include constants without linguistic significancejust to make sure that every sort includes a ground term.Therefore, we give a simple algorithm for checking thatevery sort in S includes a ground term.Let T 1 be the set of sorts in S that include a constant.Let T i + i be the union of T i and the set of all s in S suchthat for some function le t te r fo f  sort s, the arity of f isUl.
.
.u n and the sorts u~,...,Un are in T i.
Every sort in T~includes a ground term, and if every sort in T~ includesa ground term then every sort in T i + l includes a groundterm.
Then for all n, every sort in T n includes a groundterra.
The algorithm will compute T,  for successivevalues of n until it finds an N such that T N = T N + 1 (thisN must exist, because S is finite).
I f  TN = S, then everysort in S includes a ground term, otherwise not.As an illustration, let S = {phrase, person, number}.Let the function letters of E be {np, vp, s, 1st, 2nd, 3rd,singular, plural}.
Let ranks be assigned to the functionletters as follows, omitting the variables.r(np) = (\[person, number\], phrase)r(vp) = (\[person, number\], phrase)r(s) = (e, phrase)r(lst) = (e, number)r(2nd) = (e, number)r(3rd) = (e, number)r \[(singular) = (e, person)r(plural) = (e,person)We have used the notation \[a,b,c\] for the string of a, b,and c. Typical terms of this ranked alphabet are np(lst,220 Computational Linguistics, Volume 15, Number 4, December 1989Andrew Haas A Parsing Algorithm for Unification Grammarsingular) and vp(2nd, plural).
The reader can verify,using the above algorithm, that every sort includes aground term.
In this case, T 1 = {person, number}, T2 ={person, number, phrase}, and T a = T 2.To summarize: we define ranked alphabets in astandard way, adding the requirements hat every sortincludes a countable infinity of variables, a finite num-ber of function letters, and at least one ground term.
Wethen define the set of terms in a standard way.
Allunification in this paper is unification of terms, as inRobinson 1965--not graphs or other structures, as inmuch recent work (Shieber 1985b).A unification grammar is a five-tuple G = (S, (~,r) T,P, Z) where S is a set of sorts, (~,r) an S-rankedalphabet, T a finite set of terminal symbols, and Z afunction letter of arity e in (~,r).
Z is called the startsymbol of the grammar (the standard notation is S notZ, but by bad luck that conflicts with standard notationfor the set of sorts).
P is a finite set of rules; each rulehas the form (A ~ a), where A is a term of the rankedalphabet and a is a sequence of terms of the rankedalphabet and symbols from T.We define substitution and substitution instances ofterms in the standard way (Gallier 1986).
We also defineinstances of rules: if s is a substitution and (A ---> B l...B n) is a rule, then (s(A) ---> s(B I ) .
.
.s(B,))  is an instanceof the rule (A --~ B1...B,).
A ground instance of a term orrule is an instance that contains no variables.Here is an example, using the set of sorts S from theprevious example.
Let the variables of sort person be P1,P2 .... and the variables of sort number  be N, ,N2... etc.Then the rule (start ---> np(P  1 , N l ) vp(P,  , N ,  ) has sixground instances, since there are three possible substi-tutions for the variable Pj and two possible substitu-tions for N~ .We come now to the key definition of this paper.
LetG = (S, (E,r) T, P, Z) be a unification grammar.
Theground grammar for G is the four-tuple (N, T, P' ,  Z),where N is the set of all ground terms of (E,r), T is theset of terminals of G, P '  is the set of all ground instancesof rules in P, and Z is the start symbol of G. If N and P'are finite, the ground grammar is a context-free gram-mar.
If N or P'  is infinite, the ground grammar is not acontext-free grammar, and it may generate a languagethat is not context-free.
Nonetheless we can definederivation trees just as in a cfg.
Following Hopcroft andUllman (1969), we allow derivation trees with nonter-minals at their leaves.
Thus a derivation tree mayrepresent a partial derivation.
We differ from Hopcroftand Ullman by allowing nonterminals other than thestart symbol to label the root of the tree.
A derivationtree is an A-tree if the non-terminal A labels its root.The yield of a derivation tree is the string formed byreading the symbols at its leaves from left to right.
As ina cfg, A ~ a iff there is an A-tree with yield a. Thelanguage generated by a ground grammar is the set ofterminal strings derived from the start symbol.
Thelanguage generated by a unification grammar is thelanguage generated by its gl:ound grammar.The central idea of this approach is to regard aunification grammar as an abbreviation for its groundgrammar.
Ground grammars are not always cfgs, butthey share many properties of cfgs.
Therefore if weregard unification grammars as abbreviations for groundgrammars, our understanding of cfgs will help us tounderstand unification grammars.
This is of courseinspired by Robinson's work on resolution, in which heshowed how to "l i ft" a proof procedure for proposi-tional logic up to a proof procedure for general first-order logic (Robinson 1965).The case of a finite ground grammar is important,since it is adequate for describing many syntactic phe-nomena.
A simple condition will guarantee that theground grammar is finite.
Suppose s I and s 2 are sorts,and there is a function letter of sort Sl that has anargument of sort s2.
Then we say that Sl > s2.
Let >* bethe transitive closure of this relation.
If >* is irreflex-ive, and D is the number of sorts, every term of theground grammar has depth -< D. To see this, think of aground term as a labeled tree.
A path from the root to aleaf generates a sequence of sorts: the sorts of thevariables and functions letters encountered on thatpath.
It is a strictly decreasing sequence according to>*.
Therefore, no sort occurs twice; therefore, thelength of the sequence is at most D. Since there are onlya finite number of function letters in the ranked alpha-bet, each taking a fixed number of arguments, thenumber of possible ground terms of depth D is finite.Then the ground grammar is finite.A ground grammar G' is depth-bounded if for everyinteger n there exists an integer d such that everyderivation tree in G' with a yield of length n has a depthless than d. In other words, a depth-bounded grammarcannot build an unbounded amount of tree structurefrom a bounded number of symbols.
Remember thatthese symbols may be either terminals or nonterminals,because we allow nonterminals at the leaves of aderivation tree.
A unification grammar G is depth-bounded if its ground grammar is depth-bounded.We say that a unification grammar is finitely ambig-uous if its ground grammar is finitely ambiguous.
Wecan now prove the result claimed above: that a unifica-tion grammar can be finitely ambiguous but not depth-bounded.
In fact, the following grammar is completelyunambiguous but still not depth-bounded.
It has just oneterminal symbol, b, and its start symbol is start.start ~ p(O)p(N) ~ p(succ(N))p(N)  ---, q(N)q (succ(N)) ~ b q(N)q(0) ~ eThe function letter "succ"  represents the successorfunction on the integers, and the terms 0, succ(0),succ(succ(0)).., represent the integers 0, 1, 2... etc.
Forconvenience, we identify these terms with the integersComputational Linguistics, Volume 15, Number 4, December 1989 221Andrew Haas A Parsing Algorithm for Unification Grammarthey represent.
A string of N occurrences of b has justone parse tree.
In this tree the start symbol derives p(0),which derives p(N) by N applications of the secondrule.
p(N) derives q(N), which derives N occurrences ofb by N applications of the fourth rule and one applica-tion of the last rule.
The reader can verify that thisderivation is the only possible one, so the grammar isunambiguous.
Yet the start symbol derives p(N) by atree of depth N, for every N. Thus trees whose frontierhas only one symbol can still be arbitrarily deep.
Thenthe grammar cannot be depth-bounded.We have defined the semantics of our grammarformalism without mentioning unification.
This is delib-erate; for us unification is a computational tool, not apart of the formalism.
It might be better to call theformalism "substitution grammar," but the other nameis already established.Notation: The letters A, B, and C denote symbols ofa ground grammar, including terminals and nontermi-nals.
Lowercase Greek letters denote strings of sym-bols.
a \[i k\] is the substring of a from space i to space k,where the space before the first symbol is space zero.
eis always the empty string.
We write x tO y or U(x,y) forthe union of sets x and y, and also (U i<j<kfl j))  for theunion of the sets flj) for all j such that i < j < k.If a is the yield of a tree t, then to every occurrenceof a symbol A in ~ there corresponds a leaf of t labeledwith A.
To every node in t there corresponds anoccurrence of a substring in ~----the substring dominatedby that node.
Here is a lemma about trees and theiryields that will be useful when we consider top-downfiltering.Lemma 2.1.
Suppose t is a tree with yield ~fla' and nis the node of t corresponding to the occurrence of/3after a in ot/3a'.
Let A be the label of n. If t' is the treeformed by deleting all descendants of n from t, the yieldof t' is aAa'.Proof: This is intuitively clear, but the careful readermay prove it by induction on the depth of t.3 OPERATIONS ON SETS OF RULES AND TERMSThe parser must find the set of ground terms that derivethe input string and check whether the start symbol isone of them.
We have taken the rules of a unificationgrammar as an abbreviation for the set of all theirground instances.
In the same way, the parser will usesets of terms and rules containing variables as a repre-sentation for sets of ground terms and ground rules.
Inthis section we show how various functions needed forparsing can be computed using this representation.A grammatical expression, or g-expression, is eithera term of L, the special symbol nil, or a pair ofg-expressions.
The letters u, v, w, x, y, and z denoteg-expressions, and X, Y, and Z denote sets of g-expressions.
We use the usual LISP functions andpredicates to describe g-expressions.
\[x y\] is anothernotation for cons (x,y).
For any substitution s, s (cons(x,y)) = cons (s(x),s(y)) and s(Nil) = Nil.
A selector is afianction from g-expressions to g-expressions formed bycomposition from the functions car, cdr, and identity.Thus a selector picks out a subexpression from ag-expression.
A constructor is a function that maps twog-expressions to a g-expression, formed by compositionfirom the functions cons, car, cdr, nil, (A x y. x), and (Ax y. y).
A constructor builds a new g-expression fromparts of two given g-expressions.
A g-predicate is afunction from g-expressions to Booleans formed bycomposition from the basic functions car, cdr, (A x. x),consP, and null.Let ground(X) be the set of ground instances of g-expressions in X. I f f  is a selector function, let fiX) bethe set of all fix) such that x E X.
If p is a g-predicate,let separate (p,x) be the set of all x E X such that p(x).The following lemmas are easily established from thedefinition of s(x) for a g-expression x.Lemma 2.2.
I f f i s  a selector funct ion, f  (ground(X)) =ground (f(x)).Lemma 2.3.
If p is a g-predicate, separate (p,ground(X)) = ground(separate (p,x)).Lemma 2.4.
Ground (X U I") = ground (X) U ground(I1).Lemma 2.5.
I fx  is a ground term, x E ground(X) i f fxis an instance of some y E X.Lemma 2.6.
Ground (X) is empty iff X is empty.Proof.
A nonempty set of terms must have a non-empty set of ground instances, because very variablebelongs to a sort and every sort includes at least onegrotmd term.These lemmas tell us that if we use sets X and Y ofterms to represent the sets ground(X) and ground(Y) ofgrotmd terms, we can easily construct representationsfor ./(ground(x)), separate(p,ground (X)), and ground(X) U ground (Y).
Also we can decide whether a givenground term is contained in ground(X) and whetherground(X) is empty.
All these operations will be neededin the parser.The parser requires one more type of operation,defined as follows.Definition.
Let f  l andf  2 be selectors and g a construc-tor, and suppose g(x,y) is well defined whenever f l(x)andJ2(y) are well defined.
The symbol ic  product definedby j~, f2, and g is the function(AX Y.
{ g(x,y) I x E X A y E Y A f,(x) = f2(Y) })where X and Y range over sets of ground g-expressions.Note thatfl(x) = f2(Y) is considered false if either side ofthe equation is undefined.The symbolic product matches every x in X againstevery y in Y.
If f l(x) equals f2(Y), it builds a newstructure from x and y using the function g. As anexample, suppose X and Y are sets of pairs of groundterms, and we need to find all pairs \[A C\] such that forsome B, \[A B\] is in X and \[B C\] is in Y.
We can do thisby finding the symbolic product withfl  = cdr, f2 = car,and g = (A x y. cons(car(x), cdr(y))).
To see that this iscorrect, notice that if \[A B\] is in X and \[B C\] is in Y, then222 Computational Linguistics, Volume 15, Number 4, December 1989Andrew Haas A Parsing Algorithm for Unification Grammarf~(\[A B\]) =f2 (\[B C\]), so the pa i rg  (\[A B\],\[B C\]) = \[A C\]must be in the answer set.A second example: we can find the intersection oftwo sets of terms by using a symbolic product withfl  =(A X .
X), f2 = ()t X .
X), and g = (A x y. x).I f  X is a set of g-expressions and n an integer,rename(X,n) is an alphabetic variant of X.
For all X, Y,m, and n, if m # n then rename(X,n) and rename(Y,m)have no variables in common.
The following theoremtells us that if we use sets of terms X and Y to representthe sets ground(X) and ground(Y) of ground terms, wecan use unification to compute any symbolic product ofground(X) and ground(Y).
We assume the basic factsabout unification as in Robinson (1965).Theorem 2.1.
If  h is the symbolic product defined byf~, f2 and g, and X and Y are sets of g-expressions, thenh (ground(X),ground(Y)) =ground({s(g(u,v)) lu E rename(X,1) /~ v Erename(Y,2)/~ s is the m.g.u, of fl(u) and fz(v)})Proof.
The first step is to show that if Z and W share novariables(1) {g(z,w) I z E ground(Z)/k w E ground(W)/~ fl(z)= t"2 (w)} = ground({s(g(u,v)) I u E Z /~ v ~ W/~ s isthe m.g.u, of fl(u) and f2(v) })Consider any element of the right side of equation (1).
Itmust be a ground instance of s(g(u,v)), where u E Z, vE W, and s is the m.g.u, of f l (u  ) andfz(v ).
Any groundinstance of s(g(u,v)) can be written as s'(s(g(u,v))),where s' is chosen so that s'(s(u)) and s'(s(v)) are groundterms.
Then s'(s(g(u,v))) = g(s'(s(u)),s'(s(v))) andfl(s'(s(u))) = s'(sOCl(U))) = s'(s(f2 (v))) = f2(s'(s(v))).Therefore s'(s(g(u,v))) belongs to the set on the left sideof equation (1).Next consider any element of the left side of (I).
Itmust have the form g(z,w), where z E ground(Z), w Eground(W), and f l  (z) = fz (w).
Then for some u E Z andv E W, z is a ground instance of u and w is a groundinstance of v. Since u and v share no variables, there isa substitution s' such that s'(u) = z and s'(v) = w. Thens'( f  l (u)) = f l  (s'(u)) = f2 (s'(v)) = s'0C2 (V)), SO thereexists a most general unifier s forf l  (u) andfz (v), and s'is the composition of s and some substitution s".
Theng(z,w)  = g(s(s(u)) s(s(v))) = s(s(g(u,v))),  g(z ,w)  is aground term because z and w are ground terms, sog(z,w) is a ground instance of s(g(u,v)) and thereforebelongs to the set on the right side of equation (1).We have proved that if Z and W share no variables,(2) h(ground(Z),ground(W)) = ground({s(g(u,v)) I uE Z /~ v E W/~ s is the m.g.u, of fl(u) and f2(v)})For any X and Y, rename(X, I) and rename(Y,2) share novariables.
Then we can let Z = rename(X,1) and W =rename(Y,2) in formula (2).
Since h(ground(X),ground(Y)) = h(ground(rename(X, 1)), ground(rename(Y,2))), the theorem follows by transitivity of equality.This completes the proof.FqAs an example, suppose X = {\[a(F) b(F)\]} and Y ={\[b(G) c(G)\]}.
Suppose the variables F and G belong toa sort s that includes just two ground terms, m and n.We wish to compute the symbolic product of ground(X)and ground(Y), usingfl  = cdr, f2 = car, and g = (A x y.cons(car(x), cdr(y))) (as in our previous example).ground(X) equals {\[a(m) b(m)\],\[a(n) b(n)\]} andground(Y) equals {\[b(m) c(m)\],\[b(n) c(n)\]}, so the sym-bolic product is {\[a(m) c(m)\],\[a(n) c(n)\]}.
We will verifythat the unification method gets the same result.
Since Xand Y share no variables, we can skip the renamingstep.
Let x = \[a(F) b(F)\] and y = \[b(G) c(G)\].
Thenf  1 (x)= b(PO, f2 (Y) = b(G), and the most general unifier is thesubstitution s that replaces F with G. Then g(x,y) =\[a(F) c(G)\] and s(g(x,y)) = \[A(G) C(G)\].
The set ofground instances of this g-expression is {\[A(m) C(m)\],\[A(n)C(n)\]}, as desired.Definition.
Let f be a function from sets of g-expressions to sets of g-expressions, and suppose thatwhen X C_ X'  and Y C_ Y ' , f (X ,Y )  C_ f(X',Y') .
Thenf i smonotonic.All symbolic products are monotonic functions, asthe reader can easily show from the definition of sym-bolic products.
Indeed, every function in the parser thatreturns a set of g-expressions i monotonic.4 THE PARSER WITHOUT EMPTY SYMBOLSOur first parser does not allow rules with empty rightsides, since these create complications that obscure themain ideas.
Therefore, throughout this section let G bea ground grammar in which no rule has an empty side.When we say that a derives/3 we mean that ~ derives/3in G. Thus a ~ e iff a = e.A dotted rule in G is a rule of G with the right sidedivided into two parts by a dot.
The symbols to the leftof the dot are said to be before the dot, those to the rightare after the dot.
DR is the set of all dotted rules in G.A dotted rule (A --> a./3) derives a string if a derives thatstring.
To compute symbolic products on sets of rules ordotted rules, we must represent them as g-expressions.We represent the rule (A --~ B C) as the list (A B C), andthe dotted rule (A --> B.C) as the pair \[(A B C) (C) \].We write A ~ B if A derives B by a tree with morethan one node.
The parser relies on a chain table- -atable of all pairs \[A B\] such that A :~, B.
Let C O be theset of all \[A B\] such that A ~ B by a derivation tree ofdepth d. Clearly C1 is the set of all \[A B\] such that (AB) is a rule of G. I f  S l and $2 are sets of pairs of terms,definelink(S~,S 2) = {\[A C\] \[ (3 B.
\[A B\] E $1/~ \[B C\] E $2) }The function link is equal to the symbolic productdefined by f l  = cdr, f2 = car, and g = (h x y .cons(car(x), cdr(y))).
Therefore we can compute link($1, $2) by applying Theorem 2.1.
Clearly C O ?
i =link(Cd,C0.
Since the grammar is depth-bounded, thereexists a number D such that every derivation tree whoseyield contains exactly one symbol has depth less thanD.
Then C D is empty.
The algorithm for building thechain table is this: compute C, for increasing values ofComputational Linguistics, Volume 15, Number 4, December 1989 223Andrew Haas A Parsing Algorithm for Unification Grammarn until C n is empty?
Then the union of all the C,'s is thechain table.We give an example from a finite ground grammar.Suppose the rules are(a ~ b)(b -'-> c)(c--, d)(d----> k f )(k ~ g)(f"-~ h)The terminal symbols are g and h .
Then Cl = {\[a b\],\[b c\], \[c d\]}, C 2 : {\[a c\],\[b d\]}, and C 3 = {\[a d\]}.
C 4 isempty.Definitions.
ChainTable is the set of all \[A B\] suchthat A ~ B.
If S is a set of dotted pairs of symbols andS' a set of symbols, ChainUp(S,S') is the set of symbolsA such that \[A B\] ~ S for some B ~ S'.
"ChainUp" isclearly a symbolic product.
If S is a set of symbols,close(S) is the union of S and ChainUp(ChainTable,S).By the definition of ChainTable, close(S) is the set ofsymbols that derive a symbol of S.In the example grammar, ChainTable is the union ofCl ,  C2, and C3--that is, the set {\[ a b\],\[b c\],\[c d\],\[a c\],\[b d\],\[a d\]}.
ChainUp({ a}) = {}, but ChainUp({ d}) ={ a,b,c}, close({ a}) = { a}, while close({ at}) = { a,b,c,d}.Let a be an input string of length L > 0.
For each a\[ik\] the parser will construct he set of dotted rules thatderive a\[i k\].
The start symbol appears on the left sideof one of these rules iff a\[i k\] is a sentence of G. Bylemma 2.5 this can be tested, so we have a recognizerfor the language generated by G. With a small modifi-cation the algorithm can find the set of derivation treesof a.
We omit details and speak of the algorithm as aparser when strictly speaking it is a recognizer only.The dotted rules that derive a\[i k\] can be partitionedinto two sets: rules with many symbols before the dotand rules with exactly one.
For each a\[i k\], the algo-rithm will carry out three steps.
First it collects alldotted rules that derive a\[i k\] and have many symbolsbefore the dot.
From this set it constructs the set of allsymbols that derive a\[i k\], and from these symbols itconstructs the set of all dotted rules that derive a\[i k\]with one symbol before the dot.
The union of the twosets of dotted rules is the set of all dotted rules thatderive a\[i k\].
Note that a dotted rule derives a\[i k\] withmore than one symbol before the dot iff it can be writtenin the form (A ~ fiB./3') where/3 ~ a\[ij\], B ~ a\[j k\], andi < j < k (this follows because a nonempty string/3 cannever derive the empty string in G).If (A --* B .
(7) derives a\[ij\] and B derives a\[j k\], then(A ~ B C .)
derives a\[i k\].
This observation motivatesthe following.Definition.
If S is a set of dotted rules and S' a set ofsymbols, AdvanceDot(S,S') is the set of rules (AaB./3) such that (A ~ a.Bfl) ~ S and B E S'.
ClearlyAdvanceDot is a symbolic product.For example, AdvanceDot({( d ~ k.  fi},{ ao'}) equals{( d----> k f .
)}.Suppose that for each proper substring of a\[i k\] wehave already found the dotted rules and symbols thatderive that substring.
The following lemma tells us thatwe can then find the set of dotted rules that derive a\[i k\]with many symbols before the dot.Lemma 3.1.
For i < j < k, let S(ij) be the set ofdotted rules that derive a\[i j\], and S'(j,k) the set ofsymbols that derive a\[j k\].
The set of dotted rules thatderive a\[i k\] with many symbols before the dot isU AdvanceDot(S(ij),S'(j,k)) i< j<kProof.
We haveU AdvanceDot({(B ~/3.
/3 s) ~ DR I /3 ~ a\[i j\]}, i< j<k{A I A ~ a\[j k\]})U {(B ___> /3A.
/32) E DR i /3 ~ a\[i j \ ] / \  A ~ a\[j k\] } i<:j<kby definition of AdvanceDot{(\]3~ f lA .&)EDRI  (3 j .
i< j  <k /X /3~a\ [ i j \ ] /XA ~ a\[j k\])}\[~As noted above, this is the set of dotted rules that derivea\[i k\] with more than one symbol before the dot.Definition.
If S is a set of dotted rules, finished(S) ={ A I (a-- , /3.)
~ S }.When the dot reaches the end of the right side of arule, the parser has finished building the symbol on theleft side--hence the name finished.
For example,finislaed({( d ~ k f .
),(a ~ .
b)}) is the set { d}.The next lemma tells us that if we have the set ofdotted rules that derive a\[i k\] with many symbols beforethe dot, we can construct the set of symbols that derivea\[i k\].Lemma 3.2.
Suppose length(a) > 1 and S is the set ofdotted rules that derive a with more than one symbolbefore the dot.
The set of symbols that derive a isclose(finished(S)).Proof.
Suppose first that A ~ close(finished(S)).Then for some B, A ~B,  (B ~/3 . )
is a dotted rule, and/3 ~ a.
Then A ~ a.
Suppose next that A derives a. Weshow by induction that if t is a derivation tree in G andA ~ a by t, then A E close(finished (S)).
t contains morethan one node because length(a) > 1, so there is a rule(A -* A l ... A n) that admits the root of t. If n > 1, (AAt....An.)
E S and A is in close(finished(S)).
I fn  = 1 thenA n -"~ a and by induction hypothesis A n ~ close(fin-ished(S)).
Since A ~A 1, A ~ close(finished(S)).In our example grammar, the set of dotted rulesderiving a\[0 2\] = gh with more than one symbol beforethe dot is {(d ~ kf.
)}, finished({( d ~ kf.)}
) is { d}, andclose({ d} ) = { a,b,c,d}.
It is easy to check that these areall the symbols that derive gh.\[3Definitions.
RuleTable is the set of dotted rules (A.a) such that (A ~ a) is a rule of G. If S is a set ofsymbols, NewRules(S) is AdvanceDot(RuleTable, S).In our example grammar, NewRules ({k}) = {( d ~ k?
J S} ,Lemma 3.3.
If S is the set of symbols that derive a,224 Computational Linguistics, Volume 15, Number 4, December 1989Andrew Haas A Parsing Algorithm for Unification Grammarthe set of dotted rules that derive a with one symbolbefore the dot is NewRules(S).Proof.
Expanding the definitions gives AdvanceDot({( A ---> ./3l (A ---> /3)EP}, { C\ [  C ~ a}) = {(A -->C./3') (A --> C/3') E P /~ C ~a}.
This is the set of dottedrules that derive a with one symbol before the dot.Let terminals(i,k) be the set of terminals that derivea\[i k\]; that is, i f /+  1 = k then terminals(i,k) = {a\[i k\]},and otherwise terminals(i,k) = f~.
Let a be a string oflength L > 0.
For 0 < i < k -< L, definedr(i,k) =if i+  1 =kthen NewRules(close({ a\[i i + 1\]}))U AdvanceDot(dr(i j),  else (let rules 1 = i <j < k\[finished(dr(j,k)) U terminals(j ,k)\])(let rules 2 = NewRules(close(finished(rules0) )ruleSl U rules2))Theorem 3.1.
For 0 <- i < k <- L, dr(i,k) is the set ofdotted rules that derive a\[i k\].Proof.
By induction on the length of a\[i k\].
If thelength is 1, then i + 1 = k. The algorithm returnsNewRules(close({a\[i i + 1\]})).
close({ a\[i i + 1\]} ) is theset of symbols that derive a\[ i i + 1\] (by the definition ofChainTable), and NewRules(close({a\[i i + 1\]})) is the setof dotted rules that derive a\[i i + 1\] with one symbolbefore the dot (by lemma 3.3).
No rule can derivea\[i i + 1\] with many symbols before the dot, becausea\[i i + 1\] has only one symbol.
Then NewRules(close({a\[i k\]})) is the set of all dotted rules that derive a\[i k\].Suppose a\[i k\] has a length greater than 1.
If i <j<k,dr(Q) contains the dotted rules that derive a\[i j\] anddr(j,k) contains the dotted rules that derive o~\[j k\], byinduction hypothesis.
Then finished(drfj, k)) is the set ofnonterminals that derive a\[j k\], and terminals(j,k) is theset of terminals that derive a\[j k\], so the union of thesetwo sets is the set of all symbols that derive a\[j k\].
Bylemma 3.1, rules~ is the set of dotted rules that derivea\[i k\] with many symbols before the dot.
By lemma 3.2,close(finished(rules1)) is the set of symbols that derivea\[i k\], so by lemma 3.3 rules2 is the set of dotted rulesthat derive a\[i k\] with one symbol before the dot.
Theunion of rulesl and rules2 is the set of dotted rules thatderive a\[i k\], and this completes the proof.\[\]Suppose we are parsing the string gh with our exam-ple grammar.
We havedr(O, 1) = {(k ---> g .
),(d ----> k .
j")}dr(l,2) = {(f---> h .
)}dr(0,2) = {(d----> kf  .
),(c---> d .
),(b ~ c .
),(a----> b .
)}5 ThE PARSER WITH EMPTY SYMBOLSThroughout this section, G is an arbitrary depth-bounded unification grammar, which may contain ruleswhose right side is empty.
If there are empty rules in thegrammar, the parser will require a table of symbols thatderive the empty string, which we also call the table ofempty symbols.
Let E d be the set of symbols that derivethe empty string by a derivation of depth d, and let E'dbe the set of symbols that derive the empty string by aderivation of depth d or less.
Since the grammar isdepth-bounded, it suffices to construct E d for succes-sive values of d until a D > 0 is found such that E D is theempty set.E I is the set of symbols that immediately derive theempty string; that is, the set of all A such that (A ---> e)is a rule.
A ~ E d ?
1 iffthere is a rule (A ---> B 1 ...Bn) suchthat for each i, B~ ~ e at depth di, and d is the maximumof the di's.
In other words: A E Ed ?
i iff there is a rule(A ~ aB/3) such that B E Ed and every symbol of a and/3 is in E' d.Let DR be a set of dotted rules and S a set ofsymbols.
DefineAdvanceDot*(DR,S) =if DR = O then Qelse (DR U AdvanceDot*(AdvanceDot(DR,S),S))I f  DR is the set of ground instances of a finite set of ruleswith variables, there is a finite bound on the length ofthe right sides of rules in DR (because the right side ofa ground instance of a rule r has the same length as theright side of r).
If L is'the length of the right side of thelongest rule in DR, then AdvanceDot*(DR,S) is welldefined because the recursion stops at depth L orbefore.
Clearly AdvanceDot*(DR,S) is the set of rules(A --> a/3.y) such that (A --> a.
/33') E DR and everysymbol of/3 is in S.LetS 1 =$2 =$3=AdvanceDot*(RuleTable, E'd)AdvanceDot(S l, E'd)AdvanceDot*(S2, E'o)S 4 = finished(S3)S~ is the set of dotted rules (A ---> a./30) such that everysymbol of a is in E'd.
$2 is then the set of dotted rules (A---> aB./3 0 such that B ~ Ed and every symbol of a is inE'd.
Therefore $3 is the set of dotted rules (A ---> aB/3./32)such that B E Ed and every symbol of a and/3 is in E'd.Finally $4 is the set of symbols A such that for somerule (A ---> aBfl), B E E d and every symbol of a and/3 isin E' d. Then $4 is E d + i- In this way we can construct Edfor increasing values of d until the table of emptysymbols is complete.Here is an example grammar with symbols thatderive the empty string:(a ---> e)(b ---> e)(c ---> ab)(k ---> cfcgc)Oc---> r)(g ~ s)The terminal symbols are r and s. In this grammar, E l ={a,b}, E2 = {c}, and E 3 = Q.Definitions Let EmptyTable be the set of symbolsthat derive the empty string.
If S is a set of dotted rules,let SkipEmpty(S) be AdvanceDot*(S, EmptyTable).Computational Linguistics, Volume 15, Number 4, December 1989 225Andrew Haas A Parsing Algorithm for Unification GrammarNote that SkipEmpty(S) is the set of dotted rules (A --->a/3!./32) such that (A ~ a./31/32) E S and/3!
~ e.SkipEmpty(S) contains every dotted rule that can beformed from a rule in S by moving the dot past zero ormore symbols that derive the empty string.
In theexample grammar EmptyTable = {a,b,c}, soSkipEmpty({( k --.-> .
cfcgc)}) = {( k ---> .
cfcgc), (k ----> c .fcgc)}.
If the dotted rules in S all derive a, then thedotted rules in SkipEmpty(S) also derive a.Let Ca be the set of pairs \[A B\] such that A ~ B by aderivation tree in which the unique leaf labelled B is atdepth d (note: this does not imply that the tree is ofdepth d).
C~ is the set of pairs \[A B\] such that (A ---> aB/3)is a rule and every symbol of a and/3 derives the emptystring.
CI is easily computed using SkipEmpty.
AlsoCa + i = link(Ca,C0, so we can construct the chain tableas before.In the example grammar there are no A and B suchthat A ~B,  but if we added the rule (k ~ cfc), we wouldhave k ~f .
Note that k der ivesfby a tree of depth 3, butthe path from the root of this tree to the leaf labeledfisof length one.
Therefore the pair \[k\]\] is in C~.The parser of Section 4 relied on the distinctionbetween dotted rules with one and many symbols beforethe dot.
If empty symbols are present, we need aslightly more complex distinction.
We say that thestring a derives /3 using one symbol if there is aderivation of/3 from a in which exactly one symbol of aderives a non-empty string.
We say that a derives 13using many symbols if there is a derivation of/3 from ain which more than one symbol of a derives a nonemptystring.
If a string a derives a string/3, then a derives/3using one symbol, or a derives/3 using many symbols,or both.
In the example grammar, cfc derives r usingone symbol, and cfcg derives rs using many symbols.We say that a dotted rule derives /3 using one (ormany) symbols if the string before the dot derives /3using one (or many) symbols.
Note that a dotted rulederives a\[i k\] using many symbols iff it can be written as(A --~ /3B/3'./30 where /3~a\ [ i j \ ] ,  B ~ a\[j k\], /3' ~ e, andi < j < k. This is true because whenever a dotted rulederives a string using many symbols, there must be alast symbol before the dot that derives a nonemptystring.
Let B be that symbol; it is followed by a/3' thatderives the empty string, and preceded by a/3 that mustcontain at least one more symbol deriving a non-emptystring.We prove lemmas analogous to 3.1, 3.2, and 3.3.Lemma 4.1.
For i < j  < k let S(id) be the set of dottedrules that derive a\[ij\] and S'(j ,k) the set of symbols thatderive a\[j k \].
The set of dotted rules that derive a\[i k\]using many symbols isSkipEmptY(i <?< k AdvanceDot(S( id) ,S ' ( j ,k) ) )Proof.
Expanding definitions and using the argumentof lemma 3.3 we have<~< k AdvanceDot({(B ---> /3./30 E SkipEmpty( iJDRI /3~a\ [ i j \ ]} ,{A IAa\[j k\]})) =SkipEmpty ({(B--->/3A./32) E DR I (3 j. i < j < k/X/33 a\[i j\] A A ~ a\[j k\])})This in turn is equal to{(B -~/3A/3'./3a) E DR \[ (=1 j. i < j < k/k/3 ~ a\[ i j \ ] /kA ~ a\[j k\]) A/3' ~ e}This is the set of rules that derive a\[i k\] using manysymbols, as noted above.If we have a = rs, then the set of dotted rules thatderive a\[0 1\] is{ff--~ r .
),(k--~ c f  .
cgc),(k ~ cfc .
gc)}The set of symbols that derive a\[1 2\] is {g,s}.
The set ofdotted rules that derive a\[0 2\] using many symbols is{(k ~ cfcg .
c),(k--> cfcgc .
)}Lemma 4.1 tells us that to compute this set we mustapply SkipEmpty to the output of AdvanceDot.
If wefailed to apply SkipEmpty we would omit the dottedrule (k ~ cfcgc .)
from our answer.Lemma 4.2.
Suppose length(a) > 1 and S is the set ofdotted rules that derive a using many symbols.
The setof symbols that derive a is close(finished(S)).Proof.
By induction as in Lemma 3.2.Definitions.
Let RuleTable' be SkipEmpty({( A --> .a)( ,4-~a)  EP}) = {(A--~ a .a ' )EDR \[ a~e}.
I fS i saset of symbols let NewRules'(S) be SkipEmpty(AdvanceDot(RuleTable' ,S)).RuleTable' is like the RuleTable defined in Section 4,except hat we apply SkipEmpty.
In the example gram-mar, this means adding the following dotted rules:(c ~ a .
b)( c --> ab .
)(k ~ c .
fcgc)NewRules'({f}) is equal to {( k--* c f  .
cgc),(k ~ cfc .gc)}.The following lemma tells us that NewRules' willperform the task that NewRules performed in Section 4.Lemma 4.3.
If S is the set of symbols that derive a,the set of dotted rules that derive a using one symbol isNewRules'(S).Proof.
Expanding definitions givesSkipEmpty(AdvanceDot({(A--->/3./31) E DR I /3 ~e},{c Ic  a}))SkipEmpty({(A --> tiC.&) E DR \[ fl ~ e A C ~ a}){ (A~ /3C/3'./33) EDRI  /3~e/XC~a/%/3 '~e}This is the set of dotted rules that derive a using onesymbol, by definition.Let a be a string of length L. For 0 -< i < k - L,definedr(i,k) =if i + 1 = kthen NewRules'(close({ a\[i k\]}))else (let rules 1 =226 Computational Linguistics, Volume 15, Number 4, December 1989Andrew Haas A Parsing Algorithm for Unification GrammarSkipEmptY(i < 7< k AdvanceDot(dr(i j) ,\[finished(drfj,k)) U terminals(j,k)\]))(let rules 2 = NewRules'(close (finished(rules0))rules I tO rules2))Theorem 4.1. dr(i,k) is the set of dotted rules that derivea\[i k\].Proof.
By induction on the length of a\[i k\] as in theproof of theorem 3.1, but with lemmas 4.1, 4.2, and 4.3replacing 3.1, 3.2, and 3.3, respectively.DIf a = rs we find thatdr(0,1) = {(f---~ r .
),(k ~ cf .
cgc),(k ~ cfc .
gc)}dr(l,2) = {(g--* s.)}dr(0,2) = {(k ~ cfcg.
c),(k ~ cfcgc .
)}6 THE PARSER WITH ToP-DOWN FILTERINGWe have described two parsers that set dr(i,k) to the setof dotted rules that derive a\[i k\].
We now consider aparser that uses top-down filtering to eliminate someuseless rules from dr(i, k).
Let us say that A follows/3 ifthe start symbol derives a string beginning with/3A.
Adotted rule (A ~ X) follows/3 if A follows/3.
The newalgorithm will set dr(i,k) to the set of dotted rules thatderive a\[i k\] and follow a\[0 i\].If A derives a string beginning with B, we say that Acan begin with B.
The new algorithm requires a predic-tion table, which contains all pairs \[A B\] such that A canbegin with B.
Let P1 be the set of pairs \[A B\] such that(A --~ /3B/3') is a rule and /3 ~ e?
Let P .
+ 1 be Pn tOLink(P,, P1).Lemma 5.1.
The set of pairs \[A B\] such that A canbegin with B is the union of Pn for all n -> 1.Proof.
By induction on the tree by which A derives astring beginning with B.
Details are left to the reader.I\]Our problem is to construct a finite representation forthe prediction table?
To see why this is difficult, con-sider a grammar containing the rule(f(a,s(X)) --~ f(a,X) g)Computing the P.s  gives us the following pairs of terms:\[f(a,s(X)) f(a,X)\]\[f(a,s(s(Y))) f(a,Y)\]\[f(a,s(s(s(Z)))) f(a,Z)\]Thus if we try to build the prediction table in theobvious way, we get an infinite set of pairs of terms.The key to this problem is to recognize that it is notnecessary or even useful to predict every possiblefeature of the next input.
It makes sense to predict thepresence of traces, but predicting the subcategorizationframe of a verb will cost more than it saves.
To avoidpredicting certain features, we use a weak predictiontable; that is, a set of pairs of symbols that properlycontains the set of all \[A B\] such that A ~ B.
This weakprediction table is guaranteed to eliminate no more thanwhat the ideal prediction table eliminates.
It may leavesome dotted rules in dr(i,k) that the ideal predictiontable would remove, but it may also cost less to use.Sato and Tamaki (1984) proposed to analyze thebehavior of Prolog programs, including parsers, byusing something much like a weak prediction table.
Toguarantee that the table was finite, they restricted thedepth of terms occurring in the table?
Shieber (1985b)offered a more selective approach--his program pre-dicts only those features chosen by the user as mostuseful for prediction.
Pereira and Shieber (1987) discussboth approaches.
We will present a variation of Shie-ber's ideas that depends on using a sorted language.To build a weak prediction table we begin with a setQ1 of terms such that Pi C ground(Q0.
DefineLP(Q,Q') = {(s \[x z\]) \] (3 y,y'.
\[x y\] E Q/~ \[y' z\] EQ' A s = m.g.u, of y and y')}By Theorem 2.1, ground(LP(Q,Q')) = Link(ground(Q),ground(Q')).
Let Oi + 1 equal Oi LI LP (Oi,Ol).
Then bylemma 2.3 and induction,i~ lP i  c_ ground(i >_to l ai)That is, the union of the Qi s represents a weak predic-tion table?
Thus we have shown that if a weak predictiontable is adequate, we are free to choose any Q1 such thatPt c ground(Q 0.Suppose that QD subsumes LP(QD,QO.
Thenground(LP(Qo,QO) c_ ground(QD) and ground(QD + 1)= ground(QD) tO ground(LP(QD, Q0) = ground(QD).Since ground(Q i ?
1) is a function of ground(Qi) for all i,it follows that ground(Qi) = ground(Qo) for all i -> D, soground(QD) = (tO i >-- 1 ground(Qi))?
That is, QD is afinite representation of a weak prediction table.
Ourproblem is to choose QI so that QD subsumes Qo + i forsome D.Let sl and s z be sorts.
In section 2 we defined sl > s 2if there is a function letter of sort s~ that has anargument of sort s 2.
Let >* be the transitive closure of>; a sort t is cyclic if t >* t ,  and a term is cyclic if itssort is cyclic?
P~ is equal to{\[A B\] \] (A ~/3?B/3') E RuleTable'}so we can build a representation for P~.
Let us form Q~from this representation by replacing all cyclic termswith new variables.
More exactly, we apply the follow-ing recursive transformation to each term t in therepresentation f PI:transformfflh...tn)) =if the sort of f is cyclicthen new-variable0else fltransform (h)--.transform(tn))whei-e new-variable is a function that returns a newvariable each time it is called.Then P1 C_ ground(Ql), and Q1 contains no functionletters of cyclic sorts.
For example, if the function letters belongs to a cyclic sort, we will turn\[ f(a,s(s(X))) f(a,X)\]into\[ f(a,Z) f(a, Y)\]If Ql = {\[f(a,Z)f(a, Y)\]}, then Q2 = {\[f(a,V)J(a,W)\], soQi subsumes Q2, and Q1 is already a finite representa-tion of a weak prediction table.
The following lemmaComputational Linguistics, Volume 15, Number 4, December 1989 227Andrew Haas A Parsing Algorithm for Unification Grammarshows that in general, the Ql defined above allows us tobuild a finite representation f a weak prediction table.Lemma 5.2.
Let Q~ be a set of pairs of terms thatcontains no function letters of cyclic sorts, and let Qi beas defined above for all i > 1.
Then for some D, QDsubsumes LP(QD,Q O.Proof.
Note first that since unification never intro-duces a function letter that did not occur in the input, Q~contains no function letters of cyclic sort for any i -> 1.Let C be the number of noncyclic sorts in thelanguage.
Then the maximum depth of a term thatcontains no function letters of cyclic sorts is C + 1.Consider a term as a labeled tree, and consider any pathfrom the root of such a tree to one of its leaves.
The pathcan contain at most one variable or function letter ofeach noncyclic sort, plus one variable of a cyclic sort.Then its length is at most C + 1.Consider the set S of all pairs of terms in L thatcontain no function letters of cyclic sorts.
Let uspartition this set into equivalence classes, counting twoterms equivalent if they are alphabetic variants.
Weclaim that the number of equivalence classes is finite.Since there is a finite bound on the depths of terms in S,and a finite bound on the number of arguments of afunction letter in S, there is a finite bound V on thenumber of variables in any term of S. Let v~...vK be alist of variables containing V variables from each sort.Then there is a finite number of pairs in S that use onlyvariables from v~...vK; let S' be the set of all such pairs.Now each pair p in S is an alphabetic variant of a pair inS', for we can replace the variables of p one-for-onewith variables from v~...vK.
Therefore the number ofequivalence classes is no more than the number ofelements in S'.
We call this number E. We claim that QDsubsumes LP(QD,QI) for some D -< E.To see this, suppose that Qi does not subsumeLP(Qi,Q1) for all i < E. If Qi does not subsumeLP(Qi,Q1), then Qi?~ intersects more equivalenceclasses than Qi does.
Since Q~ intersects at least oneequivalence class, QE intersects all the equivalenceclasses.
Therefore QE subsumes LP(QE,QI), which wasto be proved.
I \ ]This lemma tells us that we can build a weak predic-tion table for any grammar by throwing away all sub-terms of cyclic sort.
In the worst case, such a tablemight be too weak to be useful, but our experiencesuggests that for natural grammars a prediction table ofthis kind is very effective in reducing the size of thedr(i,k) s. In the following discussion we will assume thatwe have a complete prediction table; at the end of thissection we will once again consider weak predictiontables.Definitions.
If S is a set of symbols, let first(S) = S U{ B I (q A E S. \[A B\] ~ PredTable }.
If PredTable isindeed a complete prediction table, first(S) is the set ofsymbols B such that some symbol in S can begin with B.I fR  is a set of dotted rules let next(R) = {B \] (3 A,/3,/3'.
(A --> /3.B/3') E R }.Consider the following example grammar:starl ----> aa-~, rgc --> rhg- - ,  sh-~, sThe terminal symbols are r and s. In this grammarfirst({start}) = {start, a,r}, and next({( a --> r .
g)}) = { g}.>The following lemma shows that we can find the setof symbols that follow a\[0 j'\] if we have a predictiontable and the sets of dotted rules that derive a\[ij\] for alli < j.Lemma 5.3.
Let j satisfy 0 -< j -< length(a).
Supposethat for 0 < i < j,  S(/) is the set of dotted rules thatfollow a\[0 i\] and derive a\[ij\] ( i f j  = 0 this is vacuous).Let start be the start symbol of the grammar.
Then theset of symbols that follow a\[0 j'\] isfirst(ifj = 0{start}U next(S(i)))) (o<_i_<<jProof.
We show first that every member of the given setfollows a\[0 Jl.
If j = 0, certainly every member offirst({start}) follows a\[0 0\] = e. I f j  > 0, suppose that Cfollows a\[0 i\], (C --->/3B/3') is a rule, and/3 ~ a\[ij\]; thenclearly B follows a\[0 j\].Next we show that if A follows a\[0 Jl, A is in thegiven set.
We prove by induction on d that if start ~ a\[0j lAa'  by a tree t, and the leaf corresponding to theoccula'ence of A after a\[0 ./1 is at depth d in t, then Abelongs to the given set.
If d = 0, then A = start, andj = 0.
We must prove that start E first({start}), which isobvious.If d > 0 there are two cases.
Suppose first that theleaf n corresponding to the occurrence of A after a\[0 j\]has younger brothers dominating a nonempty string(younger brothers of n are children of the same fatheroccuJrring to the left of n).
Then the father of n isadmitted by a rule of the form (C-->/3A/3').
C is the labelof the father of n, and /3 consists of the labels of theyounger brothers of n in order.
Then/3 ~ a\[i j\], where0 --- i < j.
Removing the descendants of n's father fromt giw~s a tree t' whose yield is a\[0 i\]Ca'.
Therefore Cfollows a\[0 i\].
We have shown that (C --->/3A/3') is a rule,C follows a\[0 i\], and/3 ~ a\[i j\].
Then (C ---> /3.A/3') ES(i), A E next(S(/)), and A E (U 0-- < i < j next(S(/))).Finally suppose that the younger brothers of n dom-inate the empty string in t. Then if C is the label of n'sfather, C can begin with A.
Removing the descendantsof' n's father from t gives a tree t' whose yield beginswith a\[0 j\]C. Then C belongs to the given set byinduction hypothesis.
If C E first(X) and C can beginwith A, then A E first(X).
Therefore A belongs to thegiven set.
This completes the proof.As an example, let a = rs.
Then the set of dottedrules that derive a\[0 1\] and follow a\[0 0\] is {(a ---> r .
g)}.The dotted rule (c ~ r .
h) derives a\[0 I\], but it does notfollow a\[0 0\] because c is not an element of first({start}).228 Computational Linguistics, Volume 15, Number 4, December 1989Andrew Haas A Parsing Algorithm for Unification GrammarWe are finally ready to present the analogs oflemmas3.1, 3.2, and 3.3 for the parser with prediction.
Wherethe earlier lemmas mentioned the set of symbols (ordotted rules) that derive a\[ij\], these lemmas mentionthe set of symbols (or dotted rules) that follow a\[0 i\] andderive a\[i j\].Lemma 5.4.
Let a be a nonempty string.
Supposethat for i < j < k, S(ij) is the set of dotted rules thatfollow a\[0 i\] and derive a\[ij\], while S'(j,k) is the set ofsymbols that follow a\[0 j\] and derive a\[j k\].
The set ofdotted rules that follow a\[0 i\] and derive a\[i k\] usingmany symbols isSkipEmptY(i<jU<k AdvanceDot(S(i j),S'(j,k)))Proof.
Expanding definitions and using the same argu-ment as in lemma 3.1, we haveSkipEmptY(i<jU<k AdvanceDot({(B ~/3./31) ~ DR \[ Bfollows a\[0 i\] /k/3 ~ a\[i j\]}{n \[ n follows a\[0 j \ ] /k n ~ a\[j k\]}) =SkipEmpty({(B ~/3A./3z) E DR \[ B follows a\[0 i\]/k (3j.
i<j <k/k/3 ~ a\[i j \]/k A follows a\[0j\]/k Aa\[j k\])} )I fB follows a\[0 i\], (B--->/3A/32) is a rule, and/3 ~ a\[ij\],then A follows a\[0 j\].
Therefore the statement that Afollows a\[0 j\] is redundant and can be deleted, givingSkipEmpty({(B -->/3A./32) E DR I B follows a\[0 i\]/k (a j. i< j<k/k/3  ~ a\[i j \ ] /k A ~ a\[i k\])})This in turn is equal to{(B ~ flA/3'.fl3 ) E DR \] B follows a\[O i\]A (3 j. i<j<k A/3 ~ a\[ij\] A A ~ a\[j k\]) A/3' ~e}This is the set of dotted rules that follow a\[0 i\] andderive a\[i k\] using many symbols.DLemma 5.5.
Suppose length(a\[ij\]) > 1, S is the set ofsymbols that follow a\[0 i\], and S' is the set of dottedrules that follow a\[0 i\] and derive a\[i j\] using manysymbols.
Then S n close(finished(S')) is the set ofsymbols that follow a\[0 i\] and derive a\[ij\].Proof.
S' is a subset of the set of dotted rules thatderive a\[i j\], so by lemma 4.2 and monotonicity,close(finished(S')) is a subset of the set of symbols thatderive a\[ij\].
Therefore very symbol in S n close(fin-ished(S'))) derives a\[ij\] and follows a\[0 i\].
This provesinclusion in one direction.For the other direction, suppose A follows a\[0 i\] andderives a\[ij\].
Then by lemma 4.2 there is a dotted rule(B --->/3.)
such that/3 ~ a\[i j\] using many symbols andA ~ B.
Then B follows a\[0 i\], so B is in finished(S'),which means that A is in S n close(finished(S')).\[-\]Definition.
If S is a set of symbols and R a set ofdotted rules, filter(S,R) is the set of rules in R whose leftsides are in S. In other words, filter(S,R) = {( A --->/3./3')CR I  AES}.Lemma 5.6.
Suppose S is the set of symbols thatfollow a\[0 i\], and S' is the set of symbols that followa\[0 i\] and derive a\[ij\].
Then the set of rules that followa\[0 i\] and derive a\[i j\] using one symbol isfilter(S,NewRules'(S')).Proof: S' is a subset of the set of symbols that deriveComputational Linguistics, Volume 15, Number 4, December 1989a\[i j\].
By lemma 4.3 and monotonicity, we know thatevery dotted rule in NewRules'(S') derives a\[ij\] usingone symbol.
Therefore very dotted rule in filter(S,Ne-wRules(S')) follows a\[0 i\] and derives a\[i j\] using onesymbol.
This proves inclusion in one direction.For the other direction, consider any dotted rule thatfollows a\[0 i\] and derives a\[ij\] using one symbol; it canbe written in the form (A --> flB/3'.fll), where 13 and/3'derive e, B derives a\[i j\], and A follows a\[0 i\].
Since/3 ~ e, B follows a\[0 i\].
Therefore B E S' and (A -->/3B/3'./31) is in NewRules'(S').
Since A follows a\[0 i\], (A-->/3B/3'./3 0 is in filter(S,NewRules'(S')).Let a be a string of length L. For O<-i<k < L, definepred( j )  =first(ifj = 0then {Start}else (U 0~i<j next(dr(i,j))))dr(iJ~) =f f i+  1 =kthen filter(pred(i),NewRules'(\[pred(i) O close({a\[i k\]})\]))else (let rules1 =SkipEmpty(U AdvanceDot(dr(i,j ),l<j <k \[ finished(dr(j 2))U terminals(j~k) \])(let zules 2 = filter(pred(i),NewRules'(\[pred(i) Oclose(finished( rules 1)) \]))rules I U rules~))Note that the new version of dr(i,k) is exactly like theprevious version except hat we filter the output of closeby intersecting it with pred(i), and we filter the output ofNewRules' by applying the function filter.Theorem 5.6 For O<-k<-L, pred(k) is the set of sym-bols that follow a\[0 i\], and if 0-<i< k, dr(i,k) is the set ofdotted rules that follow a\[0 i\] and derive a\[i k\].Proof.
This proof is similar to the proof of theorem3.4, but it is more involved because we must show thatpred(k) has the desired values.
Once more we argue byinduction, but this time it is a double induction: an outerinduction on k, and an inner induction on the length ofstrings that end at k.We show by induction on k that pred(k) has thedesired value and for O<-i<k, dr(i,k) has the desiredvalue.
If k = 0, lemma 5.3 tells us that pred(O) is the setof symbols that follow a\[0 0\], and the second part of theinduction hypothesis vacuously true.If k > 0, we first show by induction on the length ofa\[i k\] that dr(i,k) has the desired value for 0 <-i<k.
Thispart of the proof is much like the proof of 3.4.
If a\[i k\]has length 1, then pred(i) is the set of symbols thatfollow a\[0 i\] by the hypothesis of the induction on k.Then pred(i) n close({a\[i k\]}) is the set of symbols thatfollow a\[0 i\] and derive a\[i k\], so lemma 5.6 tells us thatfilter(pred (i),NewRules'(pred(i) n close({a\[i k\]})))is the set of dotted rules that follow a\[0 i\] and derivea\[i k\].If length(a\[/k\]) > 1, consider any j such that i<j<k.dr(i,j) and dr(j,k) have the desired values by induction229Andrew Haas A Parsing Algorithm for Unification Grammarhypothesis.
Then lemma 5.4 tells us that rules~ is the setof dotted rules that follow a\[0 i\] and derive a\[i k\] usingmany symbols, pred(i) is the set of symbols that followa\[0 i\], so pred(i) fq close(finished(rulesO) isthe set ofsymbols that follow a\[0 i\] and derive a\[i k\], by lemma5.5.
Therefore rulesz is the set of dotted rules that followa\[0/\] and derive a\[i k\] using one symbol, by lemma 5.6.The union of rules~ and rulesz is the set of dotted rulesthat follow a\[0 i\] and derive a\[i k\], and this completesthe inner induction.To complete the outer induction, we use lemma 5.3to show that pred(k) is the set of symbols that follow a\[0k\].
This completes the proof.E3Corollary: Start E finished(dr(O,L)) iff a is a sentenceof the language generated by G.Suppose we are parsing the string rs using the exam-ple grammar.
Then we havepred(O) = {start,a,r}dr(0,1) = {(a---> r .
g)}pred(1) = {g,s}dr(l,2) = {(g ---> s .
)}dr(0,2) = {(a ----> rg .
),(start ---> a .
)}We have proved the correctness of the parser when ituses an ideal prediction table.
We must still considerwhat happens when the parser uses a weak predictiontable.Theorem 5.7.
If PredTable is a superset of the set ofall \[A B\] such that A can begin with B, then start Efinished(dr(O,L)) iff a is a sentence of the languagegenerated by G.Proof.
Note that the parser with filtering alwaysbuilds a smaller dr(i,k) than the parser without filtering.Since all the operations of the parser are monotonic,this is an easy induction.
So if the parser with filteringputs the start symbol in dr(O,L), the parser withoutfiltering will do this also, implying that a is a sentence.Note also that the parser with filtering produces a largerdr(i,k) given a larger PredTable (again, this followseasily because all operations in the parser are monoton-ic).
So if a is a sentence, the parser with the idealprediction table includes Start in dr(O,L), and so doesthe parser with the weak prediction table.\[\]7 DISCUSSION AND IMPLEMENTATION NOTES7.1 RELATED WORK AND POSSIBLE EXTENSIONSThe chief contribution of the present paper is to definea class of grammars on which bottom-up arsers alwayshalt, and to give a semi-decision procedure for thisclass.
This in turn makes it possible to prove a com-pleteness theorem, which is impossible if one considersarbitrary unification grammars.
One can obtain similarresults for the class of grammars whose context-freebackbone is finitely ambiguous--what Pereira and War-ren (1983) called the offline-parsable grammars.
How-ever, as Shieber (1985b) observed, this class of gram-mars excludes many linguistically interesting grammarsthat do not use atomic category symbols.230The present parser (as opposed to the table-buildingalgorithm) is much like those in the literature.
Likenear\]ty all parsers using term unification, it is a specialcase of Earley deduction (Pereira and Warren 1985).The tables are simply collections of theorems proved inadvance and added to the program component of Earleydeduction.
Earley deduction is a framework for parsingrather than a parser.
Among implemented parsers, BUP(Matsumota et al 1983) is particularly close to thepresent work.
It is a bottom-up left-corner parser usingterm unification.
It is written in Prolog and uses back-tracking, but by recording its results as clauses in theProlog database it avoids most backtracking, so that it isclose to a chart parser.
It also includes top-downfiltering, although it uses only category symbols infiltering.
The paper includes suggestions for handlingrules with empty right sides as well.
The main differencefrom the present work is that the authors do notdescribe the class of grammars on which their algorithmhalts., and as a result they cannot prove completeness.Tlae grammar formalism presented here is muchsimpler than many formalisms called "unification gram-mars."
There are no meta-rules, no default values offeatures, no general agreement principles (Gazdar et al1986).
We have found this formalism adequate to de-scribe a substantial part of English syntax--at least,substantial by present-day standards.
Our grammarcurrently contains about 300 syntactic rules, not count-ing simple rules that introduce single terminals.
Itincludes athorough treatment ofverb subcategorizationand less thorough treatments of noun and adjectivesubcategorization.
It covers major construction types:raising, control, passive, subject-aux inversion, imper-atives, wh-movement (both questions and relativeclauses), determiners, and comparatives.
It assignsparses to 85% of a corpus of 791 sentences.
See Ayusoet al 1988 for a description of the grammar.It is clear that some generalizations are being missed.For example, to handle passive we enumerate by handtile rules that other formalisms would derive by meta-rule.
We are certainly missing a generalization here, butwe have found this crude approach quite practical---ourcoverage is wide and our grammar is not hard tomaintain.
Nevertheless, we would like to add meta-rule,~ and probably some general feature-passing princi-ples.
We hope to treat them as abbreviation mecha-nisms-we would define the semantics of a generalfeature-passing principal by showing how a grammarusing that principal can be translated into a grammarwritten in our original formalism.
We also hope to addfeature disjunction to our grammar (see Kasper 1987;Kasper and Rounds 1986).Though our formalism is limited, it has one propertythat is theoretically interesting: a sharp separation be-tween the details of unification and the parsing mecha-nism.
We proved in Section 3 that unification allows usto compute certain functions and predicates on sets ofgrammatical expressions--symbolic products, unions,Computational Linguistics, Volume 15, Number 4, December 1989Andrew Haas A Parsing Algorithm for Unification Grammarand so forth.
In Section 4 and 5 we assumed that thesefunctions were available as primitives and used them tobuild bottom-up arsers.
Nothing in Sections 4 and 5depends on the details of unification.
If we replacestandard unification with another mechanism, we haveonly to re-prove the results of Section 3 and the cor-rectness theorems of Sections 4 and 5 follow at once.
Tosee that this is not a trivial result, notice that we failedto maintain this separation in Section 6.
To show thatone can build a complete prediction table, we had toconsider the details of unification: we mentioned termslike "alphabetic variant" and "subsumption."
We havepresented a theory of bottom-up arsing that is generalin the sense that it does not rely on a particularpattern-matching mechanism--it applies to any mecha-nism for which the results of Section 3 hold.
We claimthat these results should hold for any reasonablepattern-matching mechanism; the reader must judge thisclaim by his or her own intuition.One drawback of this work is that depth-bounded-ness is undecidable.
To prove this, show that anyTuring machine can be represented as a unificationgrammar, and then show that an algorithm that decidesdepth-boundedness can also solve the halting problem.This result raises the question: is there a subset of thedepth-bounded grammars that is strong enough to de-scribe natural language, and for which membership isdecidable?Recall the context-free backbone of a grammar,described in the Introduction.
One can form a context-free backbone for a unification grammar by keepingonly the topmost function letters in each rule.
There isan algorithm to decide whether this backbone is depth-bounded, and if the backbone is depth-bounded, so isthe original grammar (because the backbone admitsevery derivation tree that the original grammar admits).Unfortunately this class of grammars is too restricted--it excludes rules like (major-category(n,2) ~ major-category(n,1)), which may well be needed in grammarsfor natural anguage.Erasing everything but the top function letter of eachterm is drastic.
Instead, let us form a "backbone" byapplying the transformation of Section 6, which elimi-nates cyclic function letters.
We can call the resultinggrammar the acyclic backbone of the original grammar.We showed in Section 6 that if we eliminate cyclicfunction letters, then the relation of alphabetic variancewill partition the set of all terms into a finite number ofequivalence classes.
We used this fact to prove that thealgorithm for building a weak prediction table alwayshalts.
By similar methods we can construct an algorithmthat decides depth-boundedness for grammars withoutcyclic function letters.
Then the grammars whose acy-clic backbones are depth-bounded form a decidablesubset of the depth-bounded grammars.
One can provethat this class of grammars generates the same lan-guages as the off-line parsable grammars.
Unlike theoff-line parsable grammars, they do not require atomiccategory symbols.
A forthcoming paper will discussthese matters in detail.7.2 THE IMPLEMENTATIONOur implementation is a Common Lisp program on aSymbolics Lisp Machine.
The algorithm as stated isrecursive, but the implementation is a chart parser.
Itbuilds a matrix called "ru les"  and sets rules\[/k\] equal todr(i,k), considering pairs \[i k\] in the same order used forthe induction argument in the proof.
It also builds amatrix "symbols"  and sets symbols\[/k\] to the set ofsymbols that derive a\[i k\], and a matrix pred withpred\[i\] equal to the set of symbols that follow a\[0 i\].Currently the standard parser does not incorporateprediction.
We have found that prediction reduces thesize of the chart dramatically, but the cost of predictionis so great that a purely bottom-up arser runs faster.Table 1.
Chart Sizes and Total Time for Parsing with PredictionNoSentence Prediction Categories Traces1 524 517 248 1502 878 867 686 6673 799 713 500 3874 936 921 558 4675 283 279 145 906 997 969 524 3687 531 525 323 2478 982 950 640 5079 1519 1503 1007 71110 930 920 495 40011 2034 2014 1128 771totaltime 917 2201 1538 1085(in seconds)Traces andVerb FormTable 1 presents the results of predicting differentfeatures on a sample of 11 sentences.
It describesparsing without prediction, with prediction of categoriesonly, with traces and categories, and finally with cate-gories, traces, and verb form information.
In each caseit lists the total number of entries in the matrices"rules" and "symbols"  for every sentence, and thetotal time to parse the 11 sentences.
The reader shouldcompare this table with the one in Shieber 1985.
Shiebertried predicting subcategorization i formation alongwith categories.
In our grammar there is a separate VPrule for each subcategorization frame, and this rulegives the categories of all arguments of the verb.Shieber eliminated these multiple VP rules by makingthe list of arguments a feature of the verb.
Therefore bypredicting categories alone, we get the same informa-tion that Shieber got by predicting subcategorizationinformation.
The table shows that for our grammar,prediction reduces the chart size drastically, but it is socostly that a straight bottom-up arser runs faster thanany version of prediction.The parsing tables for the present grammar are quitetractable.
The largest table is the table of chain rules,which has 2,270 entries and takes under ten minutes tobuild.
A prediction table that predicts categories,traces, and verb forms has 1,510 entries and takes sixminutes to build.Computational Linguistics, Volume 15, Number 4, December 1989 231Andrew Haas A Parsing Algorithm for Unification GrammarIn the special case of a context-free grammar, ourparsing program is essentially the same as the parser ofGraham et al (1980), in particular algorithm 2.2 of thatpaper.
The only significant differences are that theirchart includes entries for empty substrings, which weomit, and that we record symbols while they recordonly dotted rules.
When running on a context-freegrammar, the parser takes time proportional to the cubeof the length n of the input string--because the numberof symbolic products is proportional to n 3, and the timefor a symbolic product is independent of the inputstring.
This result also holds for a grammar withoutcyclic function letters.
If there are cyclic functionletters, the size of the nonterminals built by the parserdepends on the length of the input, so the time forunifications and symbolic products is no longer inde-pendent of the input, and the parsing time is notbounded by n 3.To save storage we use a simplified version ofstructure-sharing (Boyer and Moore 1972).
Followingthe suggestion of Pereira and Warren (1983), we usestructure-sharing only for dotted rules with symbolsremaining after the dot.
When the dot reaches the end ofthe right side of a rule, we translate the left side of therule back to standard representation.
This method guar-antees that in each resolution only one resolvent is instructure-sharing representation.
Instead of general res-olution we are doing what the theorem-proving litera-ture calls input resolution.
This allows us to represent asubstitution as a simple association list, using the func-tion assoc to retrieve the substitutions that have beenmade for variables.Pereira (1985) describes a more sophisticated versionof structure-sharing.
This method has two advantagesover our version.
First, the time to retrieve a substitu-tion is O(log n), where n is the length of the derivation,compared to O(n) for Boyer-Moore.
Second, only sym-bols that derive the empty string need to be translatedfrom structure-sharing form to the standard representa-tion, and this saves storage.
The first advantage may notbe important, for two reasons.
By using a single assocto retrieve a substitution, we reduce the constant factorin O(n).
Also by eliminating the structure sharing eachtime the dot reaches the end of a rule, we keep ourderivations short--n is no more than the length of theright side of the longest rule.
The second advantage ofPereira's method is more important, since our currentparser uses a lot of storage.The other optimizations are fairly obvious.
As usualwe skip the occur check in our unifications (as long asthere are no cyclic sorts, this is guaranteed to be safe).In each symbolic product, one set is indexed by thetopmost function letter of the term to be matched,which saves a good number of failed unifications.
Thesesimple techniques gave us adequate performance forsome time, but as the grammar grew the parser sloweddown, and we decided to rewrite the program in C. Thisversion, running on a Sun 4, is much more efficient.
Itparses a corpus of 790 sentences, with an average lengthof nine words, in half an hour.ACKNOWLEDGEMENTSI wish to thank an anonymous referee, whose careful reading andde,tailed comments greatly improved this paper.
This work wassupported by the Defense Advanced Research Projects Agency undercontract numbers N00014-87-C-0085 and N00014--85-C-0079.REFERENCESAyuso, Damaris; Chow, Yen-lu; Haas, Andrew; Ingria, Robert;Roucos, Salim; Scha, Remko; and Stallard, David 1988 Integrationof Speech and Natural Language Interim Report.
Report No.
6813,BBN Laboratories Inc., Cambridge, MA.Barton, G. Edward; Berwick, Robert C.; and Ristad, Eric S. 1987Computational Complexity and Natural Language.
MIT Press,Cambridge, MA.Boyer, Robert and Moore, Jay S. 1972 The Sharing of Structure inTheorem-Proving Programs.
In: Meltzer, Bernard and Michie,Donald (eds.
), Machine Intelligence 7.
John Wiley and Sons, NewYork, NY: 101-116.Gallier, Jean 1986 Logic for Computer Science.
Harper and Row,New York, NY.Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey; and Sag, Ivan 1985Generalized Phrase Structure Grammar.
Harvard UniversityPress, Cambridge, MA.Graham, Susan L.; Harrison, Michael A.; and Ruzzo, Walter L. 1980An Improved Context-free Recognizer.
ACM Transactions onProgramming Languages and Systems 2(3): 415-462.Hopcroft, John E. and Ullman, Jeffrey D. 1969 Formal Languagesand Their Relation to Automata.
Addison-Wesley PublishingCompany, Reading, MA.Kasper, Robert 1987 Feature Structures: A Logical Theory withApplication to Language Analysis.
Ph.D. Thesis, University ofMichigan, Ann Arbor, MI.Kasper, Robert and Rounds, William 1986 A Logical Semantics forFeature Structures.
In: Proceedings of the 24th Annual Meeting ofAssociation for Computational Linguistics.
Columbia University,New York, NY: 257-266.Matsumoto, Yuji; Tanaka, Hozumi; Hirakawa, Hideki; Miyoshi,Hideo; and Yasukawa, Hideki 1983 BUP: A Bottom-up ParserEmbedded in Prolog.
New Generation Computing, 1(2): 145-158.Pereira, Fernando 1985 A Structure-Sharing Representation for Uni-fication-Based Grammar Formalisms.
In: Proceedings of the 23rdAnnual Meeting of the Association for Computational Linguistics.University of Chicago, Chicago, IL: 137-144.Pereira, Fernando and Sheiber, Stuart 1987 Prolog and Natural-Language Analysis.
Center for the Study of Language and Infor-mation, Stanford, CA.
Distributed by Chicago University Press.Pereira, Fernando and Warren, David H. D. 1980 Definite ClauseGrammars for Natural Language Analysis--A Survey of theFormalism and a Comparison with Augmented Transition Net-works.
Artificial Intelligence 13(3): 231-278.Robinson, John A.
1965 A Machine-Oriented Logic Based on theResolution Principle.
Journal of the ACM 12(1): 23-41.Sato, Taisuke and Tamaki, Hisao 1984 Enumeration of SuccessPatterns in Logic Programs.
Theoretical Computer Science 34:22'7-240.Shieber, Stuart 1985 Evidence against the Context-Freeness of Nat-ural Language.
Linguistics and Philosophy 8(3): 333-343.Shieber, Stuart 1985 Using Restriction to Extend Parsing Algorithmsfor Complex-Feature-Based Formalisms.
In: Proceedings of the23rd Annual Meeting of the Association for Computational Lin-guistics.
University of Chicago, Chicago, IL: 145-152.232 Computational Linguistics, Volume 15, Number 4, December 1989
