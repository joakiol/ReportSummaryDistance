Language, Embodiment and Social IntelligenceMatthew StoneComputer Science and Cognitive ScienceRutgers, The State University of New Jersey110 Frelinghuysen Road, Piscataway NJ 08854-8019Matthew.Stone@Rutgers.EDUAbstractIt is an honor to have this chance to tie togetherthemes from my recent research, and to sketchsome challenges and opportunities for NLG inface-to-face conversational interaction.Communication reflects our general involvementin one anothers?
lives.
Through the choices we man-ifest with one another, we share our thoughts andfeelings, strengthen our relationships and further ourjoint projects.
We rely not only on words to artic-ulate our perspectives, but also on a heterogeneousarray of accompanying efforts: embodied deixis, ex-pressive movement, presentation of iconic imageryand instrumental action in the world.
Words show-case the distinctive linguistic knowledge which hu-man communication exploits.
But people?s diversechoices in conversation in fact come together to re-veal multifaceted, interrelated meanings, in whichall our actions, verbal and nonverbal, fit the situationand further social purposes.
In the best case, they letinterlocutors understand not just each other?s words,but each other.As NLG researchers, I argue, we have good rea-son to work towards models of social cognition thatembrace the breadth of conversation.
Scientifically,it connects us to an emerging consensus in favor ofa general human pragmatic competence, rooted incapacities for engagement, coordination, shared in-tentionality and extended relationships.
Technically,it lets us position ourselves as part of an emergingrevolution in integrative Artificial Intelligence, char-acterized by research challenges like human?robotinteraction and the design of virtual humans, andapplications in assistive and educational technologyand interactive entertainment.Researchers are already hard at work to place ouraccounts of embodied action in conversation in con-tact with pragmatic theories derived from text dis-course and spoken dialogue.
In my own experi-ence, such work proves both illuminating and ex-citing.
For example, it challenges us to support andrefine theories of discourse coherence by accountingfor the discourse relations and default inference thatdetermine the joint interpretation of coverbal gestureand its accompanying speech (Lascarides and Stone,2008).
And it challenges us to show how speak-ers work across modalities to engage with, disam-biguate, and (on acceptance) recapitulate each oth-ers?
communicative actions, to ground their mean-ings (Lascarides and Stone, In Preparation).
Thecloser we look at conversation, the more we can fitall its behaviors into a unitary framework?invitingus to implement behavioral control for embodied so-cial agents through a pervasive analogy to NLG.We can already pursue such implementations eas-ily.
Computationally, motion is just sequence data,and we can manipulate it in parallel ways to thespeech data we already use in spoken language gen-eration (Stone et al, 2004).
At a higher level, wecan represent an embodied performance through amatrix of discrete actions selected and synchronizedto an abstract time-line, as in our RUTH system (De-Carlo et al, 2004; Stone and Oh, 2008).
This lets ususe any NLG method that manipulates structured se-lections of discrete actions as an architecture for theproduction of embodied behavior.
Templates, as in(Stone and DeCarlo, 2003; Stone et al, 2004), offer5a good illustration.Nevertheless, face-to-face dialogue does demandqualitatively new capabilities.
In fact, people?schoices and meanings in interactive conversation areprofoundly informed by their social settings.
Weare a long way from general models that could al-low NLG systems to recognize and exploit theseconnections in the words and other behaviors theyuse.
In my experience, even the simplest social prac-tices, such as interlocutors?
cooperation on an on-going practical task, require new models of linguis-tic meaning and discourse context.
For example,systems must be creative to evoke the distinctionsthat matter for their ongoing task, and use mean-ings that are not programmed or learned but inventedon the fly (DeVault and Stone, 2004).
They mustcount on their interlocutors to recognize the back-ground knowledge they presuppose by general infer-ence from the logic of their behavior as a coopera-tive contribution to the task (Thomason et al, 2006).Such reasoning becomes particularly important inproblematic cases, such as when systems must fine-tune the form and meaning of a clarification requestso that the response is more likely to resolve a pend-ing task ambiguity (DeVault and Stone, 2007).
I ex-pect many further exciting developments in our un-derstanding of meaning and interpretation as we en-rich the social intelligence of NLG.Modeling efforts will remain crucial to the explo-ration of these new capabilities.
When we build andassemble models of actions and interpretations, weget systems that can plan their own behavior simplyby exploiting what they know about communication.These systems give new evidence about the informa-tion and problem-solving that?s involved.
The chal-lenge is that these models must describe semanticsand pragmatics, as well as syntax and behavior.
Myown slow progress (Cassell et al, 2000; Stone et al,2003; Koller and Stone, 2007) shows that there?sstill lots of hard work needed to develop suitabletechniques.
I keep going because of the method-ological payoffs I see on the horizon.
Modeling letsus take social intelligence seriously as a general im-plementation principle, and thus to aim for systemswhose multimodal behavior matches the flexibilityand coordination that distinguishes our own embod-ied meanings.
More generally, modeling replacesprogramming with data fitting, and a good model ofaction and interpretation in particular would let anagent?s own experience in conversational interactiondetermine the repertoire of behaviors and meaningsit uses to make itself understood.AcknowledgmentsTo colleagues and coauthors, especially David DeVaultand the organizers of INLG 2008, and to NSF IGERT0549115, CCF 0541185 and HSD 0624191.ReferencesJ.
Cassell, M. Stone, and H. Yan.
2000.
Coordinationand context-dependence in the generation of embodiedconversation.
In INLG, pages 171?178.D.
DeCarlo, M. Stone, C. Revilla, and J. J. Venditti.2004.
Specifying and animating facial signals for dis-course in embodied conversational agents.
ComputerAnimation and Virtual Worlds, 15(1):27?38.D.
DeVault and M. Stone.
2004.
Interpreting vague ut-terances in context.
In COLING, pages 1247?1253.D.
DeVault and M. Stone.
2007.
Managing ambiguitiesacross utterances in dialogue.
In DECALOG: Work-shop on the Semantics and Pragmatics of Dialogue.A.
Koller and M. Stone.
2007.
Sentence generation as aplanning problem.
In ACL, pages 336?343.A.
Lascarides and M. Stone.
2008.
Discourse coherenceand gesture interpretation.
Ms, Edinburgh?Rutgers.A.
Lascarides and M. Stone.
In Preparation.
Groundingand gesture.
Ms, Edinburgh?Rutgers.M.
Stone and D. DeCarlo.
2003.
Crafting the illusionof meaning: Template-based generation of embodiedconversational behavior.
In Computer Animation andSocial Agents (CASA), pages 11?16.M.
Stone and I. Oh.
2008.
Modeling facial ex-pression of uncertainty in conversational animation.In I. Wachsmuth and G. Knoblich, editors, Model-ing Communication with Robots and Virtual Humans,pages 57?76.
Springer.M.
Stone, C. Doran, B. Webber, T. Bleam, and M. Palmer.2003.
Microplanning with communicative inten-tions: The SPUD system.
Computational Intelligence,19(4):311?381.M.
Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Stere,A.
Lees, and C. Bregler.
2004.
Speaking withhands: Creating animated conversational charactersfrom recordings of human performance.
ACM Trans-actions on Graphics, 23(3):506?513.R.
Thomason, M. Stone, and D. DeVault.
2006.
Enlight-ened update: a computational architecture for presup-position accommodation.
Ms, Michigan?Rutgers.6
