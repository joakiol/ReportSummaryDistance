Applying SPHINX-II to theDARPA Wall Street Journal CSR TaskF.
Alleva, H. Hon, X. Huang, M. Hwang, R. Rosenfeld, R. WeideSchool of  Computer ScienceCarnegie Mellon UniversityPittsburgh, Pennsylvania 15213ABSTRACTThis paper eports recent efforts to apply the speaker-independentSPHINX-H system to the DARPA Wall Street Journal continuousspeech recognition task.
In SPHINX-H, we incorporated ad-ditional dynamic and speaker-normalized f atures, replaced is-crete models with sex-dependent semi-continuous hidden Markovmodels, augmented within-word triphones with between-wordtriphones, and extended generalized triphone models to shared-distribution models.
The configuration of SPHINX-II being usedfor this task includes sex-dependent, semi-continuous, shared-distribution hidden Markov models and left context dependentbetween-word triphones.
In applying our technology to this taskwe addressed issues that were not previously of concern owing tothe (relatively) small size of the Resource Management task.
11.
IntroductionExtending a continuous peech recognition system to alarger vocabulary and more general task domain requiresmore than a new dictionary and language model.
Theprimary problem in the application of the SPHINX-II \[1\]system to the Wall Street Journal (WSJ) CSR task was toextend the Viterbi beam-search used in the SPHINX \[2\]system to be able to run experiments given the constraintsof available processing and memory resources.First, we developed a practical form of between-word co-articulation modelng that was both time and memory ef-ficient The use of left context dependent between-wordtriphones is a departure from the left and fight between-word context modeling but it allows the system to retainpartial between-word co-articulation modelng despite thesize and complexity of the task.
Second, we significantlyreduced the size of the memory required.
To reduce thememory requirements of our search component it wasnecessary to change the Viterbi evaluation to use an in-1This research is spomm~'ed bythe Defense Advanced Research Projects Aganey,DoD, through J~pA Order 7239, and monitored by the Space and Naval WarfareSystems Cenamand under contract N00039-91-C-0158.
Views and cenclusionscontained in this document are those of the authors and should not be interpreted asrepresenting official polieie~, either expxessed or implied, of the Defense AdvancedResearch Pr'ojeets Agency or of the United States Government.place algorithm instead of a non-in-place one.
Additionallywe replaced the stack data structure used to recover theword sequence from the search, with a dictionary datastructure.
We decoupled the proto-type HMM state tran-sition probabilities from the word specific HMM instancesto avoid duplicating memory.
We also found that ourpointerless implementation f the HMM topology saved usboth memory and time.
Finally, we improved decodingefficiency substantially.
One way to improve decoder ef-ficiency is to reduce the search space.
SPHINX-II reducesthe search space with three pruning thresholds that are ap-plied at the state, model, and word levels.
In addition,evaluating a state requires an acoustic score computationand a graph update operation.
Both of these operations runin constant time over one state.
For discrete models, thecost of computing the acoustic score was on a par with thegraph update operation since the acoustic score was com-puted by table lookup.
With the introduction of semi-continuous models the cost of computing the acousticscore in the straight forward implementation is as much asan order of magnitude greater than the discrete model.
Thisincrease directly effects the overall time required by thesearch.
To address this problem we decomposed thesearch into four phases.
Shared distribution probabilitycomputation, HMM arc evaluation, active HMM instanceevaluation and language model application.
The shareddistribution probability computation and HMM arc evalua-tion allow us to share computations that potentially wouldbe repeated many times.
Lastly, the introduction of fullbackoff language models made the previous approach ofprecomputing the entire table of non-zero arc probabilitiesimpractical.
For the SPHINX-II CSR decoder we use acache table of active states in the language model to reducethe cost of accessing the language model.2.
Review of the SPHINX-II SystemIn comparison with the SPHINX system \[2\], the SPHINX-II system \[1\] has reduced the word error rate by more than50% on most tasks by incorporating between-word coar-ticulation modelng \[3\], high-order dynamics \[4\], sex-dependent semi-continuous hidden Markov models \[4\],and shared-distribution models \[5\].
This section will393review SPHINX-H that will be used as the baseline acous-tic modeling system for this study.2.1 Signal ProcessingThe input speech signal is sampled at 16 kHz with a pre-emphasized filter, 1 - 0.9 Z "1.
A Hamming window with awidth of 20 msec.
is applied to the speech signal every 10msec.
A 32nd-order LPC analysis is used to compute the12th-order cepstral coefficients.
A bilinear transformationof cepstral coefficients is employed to approximate themel-scale representation.
In addition, relative power isalso computed together with cepstral coefficients.
Thespeech features used in SPHINX-II include LPC cepstralcoefficients; 40-msec.
and 80-msec differenced LPCcepstral coefficients; second-order differenced cepstralcoefficients; and power, 40-msec differenced power,second-order differenced power.
These features are vectorquantized into four independent codebooks by the Linde-Buzo-Gray algorithm \[6\], each of which has 256 entries.2.2 TrainingTraining procedures are based on the forward-backwardalgorithm.
Word models are formed by concatenatingphonetic models; sentence models by concatenating wordmodels.
There are two stages of training.
The first stage isto generate the shared-distribution mapping table.
Forty-eight context-independent discrete phonetic models are in-itially estimated from the uniform distribution.
Deletedinterpolation\[7\] is used to smooth the estimatedparameters with the uniform distribution.
Then context-dependent models are estimated based on the context-independent ones.
There are 16,713 triphones in theDARPA WSJ-CSR training corpus when both within-wordand left-context-dependent between-word triphones areconsidered.
To simplify training, one codebook discretemodels were used, where the acoustic features consist ofthe cepstral coefficients, 40-msec differenced cepstrum,and power and 40-msec differenced power.
After the16,713 discrete models are obtained, the shared-distribution clustering procedure \[5\] is applied to create thesenones, 6255 in the case of the WSJ-CSR task.
Thesecond stage is to train 4-codebook models.
We first es-timate 51 context independent, four-codebook discretemodels with the uniform distribution.
With these contextindependent models and the senone table, we then estimatethe shared-distribution SCHMMs.
Because of substantialdifference between male and female speakers, two sets ofsex-dependent SCHMMs are are separately trained to en-hance performance.To summarize, the configuration of the SPHINX-II forWSJ-CSR system is:?
four codebooks of acoustic features,?
semi-continuous, shared-distribution triphonesmodels, over?
left-context-dependent between-word andwithin-word triphone models,?
sex-dependent SCHMMs.2.3 RecognitionFor each input utterance, the artificial sex is first deter-mined automatically \[8, 9\].
After the sex is determined,only the models of the determined sex are activated uringrecognition.
This saves both time and memory.
For eachinput utterance, a Viterbi beam search is used to determinethe optimal state sequence in the language network.3.
New Techniques for CSR Decoding3.1 Left  Context DependentCross-Word ModelsUsing context dependent acoustic models across wordboundaries presents two problems.
The first of which istraining the models and the second of which is using themin a decoder.
The training problem is a relatively simpleone.
Since we are using a supervised training procedure itis simply a matter of transcribing the acoustic sequence toaccount for the cross-word phonetic ontext.
An additionalcomplication is introduced when optional silences can ap-pear between words but this is also relatively easy to dealwith by adding the appropriate optional phonetic se-quences.
One question that does arise is whether contextdependent models for word beginning, word ending andword middle should be considered separately.
InSPHINX-II they are kept separate \[10\].The decoding problem is difficult since instead of a singleword sequence to consider there are many alternative wordsequences to consider.
Consider the extension of a singleword sequence W 1..n. Each possible one word extension ofW gives rise to a particular phonetic right context at theend of w n. There may be as many as N of these, where N isthe number of basic phonetic units in the system.
Asimilar problem appears when considering the best wordsequence prior to a word wn+ 1, each possible prior word,w n, gives rise to a particular phonetic left context for thestart of wn+ 1.
The final case to consider is a word that isexactly one phonetic unit in length.
Here the number ofpossibilities to consider is order N 2.
None the less, forsmall tasks (< 1000 words) with artificial grammars, it ispossible to precompile only the relevant phonetic tran-sitions since not all possible transitions will be allowed bythe artificial grammar.
When a larger and more natural taskis considered, one such as WSJ CSR, these techniques are394~ US us ~ usFigure 1: When decoding with the Bakis model the out-put distributions, lci, depend only on the name of themodel.
In the multiplexed Bakis model each lc i is a func-tion of the model name and the word sequence history,hist i.not applicable because of memory and run time con-straints.We made two important modifications in the application ofcross-word context dependent phonetic models.
The firstwas to model only the left context at word beginnings andignore the right context at word endings.
The second wasto use the word-sequence-history information i  each stateto select he appropriate l ft context model for that state.See figure 1.
An advantage afforded by left-context-only-modeling is that on each inter-word transition only onecontext is considered since the left context is uniquelydetermined by the word history W1..n. If the right contextis modeled all possible right contexts must be consideredat word endings ince the future is not yet known.
Theadvantages afforded by using the best-word-sequence toselect the appropriate left context model come in bothspace and time savings.
Space is saved since only onemodel is needed at word beginnings rather than N. Time issaved since only one model is evaluated at word begin-nings.3.2 Memory OrganizationThe WSJ-CSR task is significantly different from the pre-vious CSR tasks in the size of the lexicon and in the styleof the language model.
The lexicon is nearly an order ofmagnitude larger than previous lexicons and the languagemodel contains more than two orders of magnitude moretransitions than the Resource Management task.
Severalchanges were required in the decoder design so that itcould be run with out paging to secondary storage becauseof limited memory.
Our redesign entailed changing theViterbi evaluation to use an in-place algorithm, changingthe management of history pointers to use a hash tablerather than a stack, decoupling the prroto-type HMM statetransition probabilities from the word specific HMM in-stances, and changing from a statically compiled languagemodel to dynamically interpreted language model.
Finally,the pointerless implementation f the HMM topology con-tinued to save both memory and time.In Place Viterbi Evaluation.
In our previous decoder theViterbi evaluation used a separate set of source and des-tination states.
The advantage tothis approach is that statesmay be updated without regard to order.
The disadvantageto this approach is that two sets fields must be kept foreach state.
By changing to an in-place evaluation only oneset of fields is needed.
Another feature of the previousdecoder was that a word HMM was instantiated by makinga copy of the appropriate HMMs and concatenating themtogether.
As result duplicate copies of the arc transitionprobabilties would be made for each occurrence of HMM iin a word.
To save this space a pointer to the proto-typeHMM is kept in the instance HMM and the arc transitionprobabilities are omitted.The pointerless topology is a feature of the previousdecoder \[11\] that implicitly encodes the topology of themodel in the evaluation procedure.
Not only does this savethe memory and time associated with pointer following butit also allows, at no additional cost, the order dependentevaluation required by the in place Viterbi evaluation.Taken together these changes reduced the per statememory cost from 28 bytes/state o8 bytes/states.History Pointers and Language Model.
By using adictionary data structure instead of a stack data structurewe reduced the amount of memory devoted to the wordhistory sequences by an order of magnitude.
The reductioncomes because the dictionary does not differentiate iden-tical word histories with differing segmentations.
Besidesthe memory savings an advantage to this approach is thatword histories can be rapidly compared for equality.
Adisadvantage is that the true segmentation cannot berecovered using this data structure.
Finally, a consequenceof using a fully backed-off language model is that it wasno longer practical to precompile a graph that encoded allthe language model transitions.
Instead the language modelis dynamically interpreted atmn time.3.2 Search ReductionViterbi beam search depends on the underlying dynamicprogramming algorithm that restricts the number of statesto be ISI, where is S is the set of Markov states.
For thebigram language model ISI is a linear function of W, thesize of the lexicon.
Therefore the time to decode an ut-terance is O(ISI * 1) where I is the length of the input.
Theproblem, at least when bigram language models are used,is not to develop a more efficient algorithm but to developstrategies for reducing the size of S. Beam search does thisby considering only those states that fall with in somebeam.
The beam is defined to be all those states , wherescore(s) is with in e of the best_score(S).
In the WSJ-CSRtask the size of S has increased by almost an order ofmagnitude.
With this motivation a refinement of the beamsearch strategy was developed that reduces the number ofthe states kept in the beam by a factor of two.395In the previous implementation f the decoder the beamwas defined as beam = {s I score(s) > t~ + best_score(S)}.To further educe the size of the beam two additional prun-ing thresholds have been added.
The first threshold, re, isnominally for phone level pruning and the second, to, isnominally for word level pruning.
The set of states, P thatit is applied to corresponds to the final (dummy) states ofeach instance of a phonetic model.
The set of states W,that co is applied to corresponds tothe final (dummy) statesof the final phonetic models of each word.
The inequalityrelationship among the three beam thresholds i  given byeqn.
1.
The set containment relationship among the threesets is given by eqn.
2.1. tx~x ~ ?o 2.
SDPDW.The motivation for partitioning the state space into subsetsof states that are subject to different pruning thresholdscomes from the observation that leads to the use of a prun-ing threshold in the first place.
A state s is most likely toparticipate in the final decoding of the input when score(s)is closest o best_score(S).
Similarly a phonetic sub-wordunit is most likely to participate in the final decoding whenscore(p) is closest o best_score(S).
Likewise for the wordunits.
The difference between the state sets P and W andthe state set S is that there is more than a single state ofcontextual information available.
Put another way, whenthere is more information a tight pruning threshold can beapplied with out an increase in search errors.
Currently allthe pruning thresholds are determined empirically.
Infor-mally we have found that the best threshold settings forand to are two and four orders of magnitude tighter than t~.3.3 Search DecompositionThe search is divided into four phases.1.
shared istribution probability computation2.
HMM arc probability evaluation3.
active HMM instance valuation4.
language model applicationFor each time frame the shared distribution probabilitycomputation first computes the probabilities of the topN=4 codewords in the codebook.
Then the top Ncodewords and their probabilities are combined with eachof D=6255 discrete output probability distribution func-tions.
Although not all distributions will be used at everyframe of the search a sufficiently large number are used sothat computation on demand is less efficient.The D output probabilities are then combined with theM=I 6,713 models in the HMM arc probability evaluation.Here we only compute the arc probabilities of thoseHMMs that have active instances as part of a word.
Twoadvantages accrue from separating the arc probability com-putation from the state probability computation.
First thearc transition probability and acoustic probability needDecoder Development SummarySize x RealCondition Error % (Mb) Timebaseline 24.7% 172 167+ left context 19.5%+ Into Lang.
Model 77 217+ Word Hist.
Dict.
57+ Inplaee Viterbi 53+ Multiple Pruning 63+ Acoustic Score 53+ HMM Arc 46+ LM.
Cache 19.5% 57 40Table 1: The effect of each change to the decoder issummarizedin terms of error rata, memory size and run time.
The baselineresult refers to the results obtained with original decoder thatimplemented no cross word modeling.only be combined once.
Second this naturally leads to stor-ing HMM arc transition probabilities eparately from theHMM instances which results in a space savings.The active HMMs, ie.
those HMM instances correspond-ing to phones in an active word, are updated with arcprobabilities from the corresponding HMM protc-type.
Inthis case updating an HMM means combining all theHMM instance state probabilities with the appropriate arcprobabilities of the proto-type HMM and performing theViterbi update procedure.For each word history h ending at time t the languagemodel is consulted for the vector of probabilities cor-responding to the probability of each of one word exten-sion of h. Between the language model and the word tran-sition module sits a cache.
For the WSJ-CSR 5000 wordsystem, a 200 entry LRU 2 cache provides a hit rate of92%.
The cache reduces the cost of using this languagemodel by an order of magnitude.
For the a 5000 wordlexicon, a 200 entry cache requires four megabytes.4.
WSJ-CSR Experimental SetupThe WSJ corpus consists of approximately 45-millionwords of text published by the Wall Street Journal betweenthe years 1987 and 1989.
This corpus was made availablethrough the Association for Computation Linguistics/DataCollection Initiative (ACL/DCI) \[12\].2LRU - least recently used3964.1.
Language ModelsFor the purposes of the February dry run eight standardbigram language models were provided by D. Paul at Lin-coln Labs \[13\].
The language models were trained only onthe WSJ data that was not held out for acoustic trainingand testing.
The language models are characterized alongthree dimensions, lexicon size (5k or 20k), closed or openvocabulary, and verbalized (vp) or non-verbalized pronun-ciation (nvp).
The distinction between open closedvocabulary models is in the method used to chose the lex-icon.
For the open vocabulary the lexicon approximatelyconsists of the N most common words in the corpus.
Forthe closed vocabulary, a set of N words were selected in amanner that would allow the creation of a sub-corpus thatwould have 100% lexical coverage by this closedvocabulary.
For further details see \[14\].
The developmenttest set perplexities for the eight language models are givenin table 2.4.2.
Training and EvaluationAcoustic Data SetsThe base line speaker independent training data setprovided by the National Institute of Standards and Tech-nology (NIST) \[15\] consisted of 7240 utterances of readWSJ text equally divided among VP and NVP texts.
Thetexts chosen to train the system were quality filtered toremove very long and very short short sentences as well asremoving sentences containing words not among the 64kmost frequently occurring words in the WSJ corpus \[13\].The data was collected from 843 speakers, equally dividedamong male and female persons.
Data recording was per-formed at three different locations, MIT, SRI and TI.
At allthree locations the same close speaking, noise cancelingmicrophone was used however envkonmental conditionsvary from a sound both to a laboratory environment.
AtCMU we used a subset of the 7240 utterances, excluding89 of the 7240 utterances because they contained cross talkor over-laying noise events as indicated by the detailedorthographic transcription (DOT) of the utterance.Lexicon Size5k 20kdosed open closed openvp 80 72 158 135nvp 118 105 236 198Table 2: Perplexity of the eight standard language models on thedevelopment test set.
VP - verbalized pronunciation.
NVP - non-verbalized pronunciation.3One of the speakers in the training data set was recorded twice but atdifferent sites and so this person is counted as two different speakers.The speaker independent evaluation data set consisted ofeight data sets containing a total of 1200 utterances from10 speakers.
Again each data set was equally dividedamong male and female speakers.
For further details on theevaluation test sets see \[14\].4.3 Acoustic ConfigurationThe configuration of SPHINX-II for WSJ-CSR consists of16,713 phonetic models that share 6255 semi-continuousdistributions.
For between word modeling only the leftcontext is considered.
There is no speaker normalizationcomponent or vocabulary adaptation component.
The dic-tionary provided by Dragon Systems was programaticaUyconverted into the CMU style phonetic baseforms withsome additional manual post processing to fix problemswith the transcription of flaps/dx/.4.4 ResultsThe official NIST results are given in the following table.Each line of the table gives results for a particular test fromthe si_evl test suite.
The test sets are 5 (5000 word closed),20 (20000 word closed), sp (spontaneous) and rs (readspontaneous).
These four test sets are further subdivided tovp and nvp conditions.
The final condition for each test isthe language model used.
For these tests only two models,5c (5000 word closed) and 50 (5000 word open) wereused.
For further details on the testing datasets ee \[14\].The table is largely self explanatory other than the columnlabeled 2or.
This column is simply two times the standarddeviation of the average word error rate computed fromword error rates on a sentence by sentence basis.
Asexpected the vp tests out perform the nvp tests and the theopen language model out performs the closed languagemodel when the test data set contains words from outsidethe language models lexicon.
It should be noted howeverthat the vp portion of the test is probably the more difficultset since when we remove the highly reliable punctuationwords words from the scoring, the error rate for theremaining words is actually higher than the one obtained inthe nvp case.
We attribute this to the increased number ofdisfluencies caused by verbalized pronunciation and to thedetrimental effect on the bigram language model.5.
SummaryThe successful application of SPHINX-II to the WSJ-CSRtask demonstrates the utility of distribution sharing fortraining a large number of triphones with a relatively smallamount of data.
We also have demonstrated the utility ofthe Viterbi-beam search for decoding in the context of amuch larger task.
Beyond the algorithmic improvementsmade to the decoder a major factor in reducing decodingtime to just under 50 times real-time, is the availability ofcrisp acoustic models.397Sphinx H WSJ CSR PerformanceTestCondition Insertion Error 20si_ev15.nvp-5c 2.1% 19.5% ?
2.38si_evl5.vp-5c 3.0% 18.4% ?
2.47si_evl5.5c 2.7% 18.9% ?
1.72si_evl20mvp-5o 7.5% 37.9% ?
3.30si evl20.vp-5o 6.6% 32.7% 5:3.34si ev120.5o 7.1% 35.2% ?
2.37si evl20.nvp-5c 7.6% 43.6% ?
3.56si_evl20.vp-5c 6.9% 36.1% 5:3.43si_evl20.5e 7.2% 39.6% 5:2.46iisievlrs.nvp-5o 10.3% I 50.4% ?
5.86si_evks.vp-5o 7.7% 41.4% ?
4.29si_evks.5o 8.9% 45.4% ?
3.46si_evlsp.nvp-5o 11.7% 56.0% ?
5.64si_evlsp.vp-5o 9.2% 45.5% ?
4.42si_evlsp.vp 10.3% 50.2% 5:3.47Future plans include introducing our speaker normalizationand vocabulary adaptation technology as well as ex-perimenting with longer ange language models.REFERENCES1.
Huang, X. and Alleva, F. and Hon, H. and Hwang, M. andRosenfeld, R., "The SPHINX-1I Speech Recognition Sys-tem: An Overview", Technical Report CMU-CS-92-112,School of Computer Science, Carnegie Mellon Univer-sity, February 1992.2.
Lee, K.F.
and Hon, H.W.
and Reddy, R., "An Overviewof the SPHINX Speech Recognition System", IEEETransactions on Acoustics, Speech, and SignalProcessing, January 1990, pp.
35-45.3.
Hwang, M.Y.
and Hon, H.W.
and Lee, K.F., "ModelingBetween-Word Coartieulafion in Continuous SpeechRecognition", Proceedings of Eurospeech, Paris,FRANCE, September 1989, pp.
5-8.4..6.7.8.9.10.11.12.13.14.15.Huang, X.D.
and Alleva, F.A.
and Hayamizu, S. and Hon,H.W.
and Hwang, M.Y.
and Lee., K.F., "Improwd Hid-den Markov Modeling for Speaker-Independent Con-tinuous Speech Recognition", DARPA Speech and Lan-guage Workshop, Morgan Kaufmann Publishers, HiddenValley, PA, June 1990, pp.
327-331.Hwang, M.Y.
and Huang, X.D., "Subphonefic Modelingwith Markov States - Senone", IEEE International Con-ference on Acoustics, Speech, and Signal Processing,April 1992.Linde, Y. and Buzo, A. and Gray, R.M., "An Algorithmfor Vector Quantizer Design", IEEE Transactions onCommunication, Vol.
COM-28, No.
1, January 1980, pp.84-95.Jelinek, F. and Mercer, R.L., "Interpolated Estimation ofMarkov Source Parameters from Sparse Data", in PatternRecognition in Practice, E.S.
Gelserna nd L.N.
Kanal,ed., North-Holland Publishing Company, Amsterdam, theNetherlands, 1980, pp.
381-397.Huang, X.D, "A Study on Speaker-Adaptive SpeechRecognition", DARPA Speech and Language Workshop,Morgan Kaufmarm Publishers, San Mateo, CA, Feb 1991.Soong, F. and Rosenberg, A. and Rabiner, L. and Juang,B., "A Vector Quantization Approach to Speaker Recog-nition", IEEE International Conference on Acoustics,Speech, and Signal Processing, March 1985, pp.
387-390.Hwang, M.Y.
and Hort, H.W.
and Lee, K.F., "ModelingInter-Word Coarticulafion Using Generalized Triphones",The 117th Meeting of the Acoustical Society of America,Syracuse, NY, May 1989.Alleva, F., "Search Organization for Large VocabularyContinuous Speech Recognition", NATO ASI SpeechRecognition and Understanding: Recent Advances,Trends and Applications, 1990.Liberman, M., "Text on Tap: the ACL/DCI", DARPASpeech and Natural Language Workshop, October 1989,pp.
173-188.Paul, D.B., "New Results with the Lincoln Tied-MixtureHMM CSR System", DARPA Speech and LanguageWorkshop, Morgan Kaufmarm Publishers, San Mateo,CA, Feb 1991.Paul, D.B., Baker, J.M., "The Design for the Wall StreetJournal-based CSR Corpus", DARPA Speech and Lan-guage Workshop, Morgan Kaufmann Publishers, SanMateo, CA, Feb 1992.National Institute of Standards and Technology, Pallet,D., "WSJ Pilot Corpus", Limited distribution CD-ROM,1991.398
