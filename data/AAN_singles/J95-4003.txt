Modularity and Information ContentClasses in Principle-based ParsingPaola Merlo*Universit6 de Gen6veIn recent years models of parsing that are isomorphic to a principle-based theory of grammar (mostnotably Government and Binding (GB) Theory) have been proposed (Berwick et al 1991).
Thesemodels are natural and direct implementations of the grammar, but they are not efficient, becauseGB is not a computationally modular theory.
This paper investigates one problem related to thetension between building linguistically based parsers and building efficient ones.
In particular, theissue of what is a linguistically motivated way of deriving a parser from principle-based theoriesof grammar is explored.
It is argued that an efficient and faithful parser can be built by takingadvantage of the way in which principles are stated.
To support this claim, two features of animplemented parser are discussed.
First, configurations and lexical information are precompiledseparately into two tables (an X table and a table of lexical co-occurrence) which gives rise tomore compact data structures.
Secondly, precomputation f syntactic features (O-roles, case, etc.
)results in efficient computation of chains, because it reduces everal problems of chain formationto a local computation, thus avoiding extensive search of the tree for an antecedent orextensivebacktracking.
It is also shown that this method of building long-distance dependencies can becomputed incrementally.1.
IntroductionIn the development of parsers for syntactic analysis, it is standard practice to posittwo working levels: the grammar, on the one hand, and the algorithms, which producethe analysis of the sentence by using the grammar as the source of syntactic knowl-edge, on the other hand.
Usually the grammar is derived directly from the work oftheoretical linguists.
The interest in building a parser that is grounded in a linguistictheory as closely as possible rests on two sets of reasons: first, theories are developedto account for empirical facts about language in a concise way--they seek general,abstract, language-independent explanations for linguistic phenomena; second, cur-rent linguistic theories are supposed to be models of humans' knowledge of language.Parsers that can use grammars directly are more likely to have wide coverage, and tobe valid for many languages; they also constitute the most economical model of thehuman ability to put knowledge of language to use.
Therefore, postulating a directcorrespondence b tween the parser and theories of grammar is, methodologically, thestrongest position, and is usually assumed as a starting point of investigation.
How-ever, experiments with parsers that are tightly related to linguistic principles haveoften been a disappointment, largely because these parsers are inefficient.Inefficiency is a problem that cannot simply be cast aside.
Computationally, itrenders the use of linguistic theories impractical, and, empirically, it clashes with the* D6partement de Linguistique G6n6rale, Universit6 de Gen6ve, 2 rue de Candolle, 1204 Gen6ve,Switzerland(~) 1995 Association for Computational LinguisticsComputational Linguistics Volume 21, Number 4observation that humans make use of their knowledge of language very effectively.In this paper, I investigate the computational problem related to the tension betweenbuilding linguistically based parsers and building efficient ones, which, I argue, derivesfrom the particular forms linguistic theories have taken recently.
In particular, I explorethe issue of what is a good parsing technique to apply to principle-based theories ofgrammar.
I take Government-Binding (GB) theory (Chomsky 1986a,b; Rizzi 1990) tobe a suitable illustration of such theories, and also to show in all clarity the problemsthat might arise.
I differ from other investigations on the import of principle-basedparsing in not drawing on cognitive issues or psycholinguistic results to justify myassumptions.
Indeed, part of the spirit of this work is to explore how far one can go inadvocating principle-based parsing, in the absence of motivations given by cognitivemodelling.1.1 The ProblemWhen generative grammatical theory in the '70s talked about "dative shift," "topi-calization," passive," it meant that each of these constructions was captured in thegrammar by a specific rule.
Consequently, rules were not only construction-specific,but also language-specific (French, Italian and Spanish, for instance, have no "dativeshift").
The conceptual development of the '80s, in many frameworks, consists in hav-ing identified the unifying principles of many of these construction-specific rules.
Forexample, according to GB theory, the same set of principles are at work in the "raising"construction, (la) and in passive, (lb).
The principles are X theory, the Theta Criterion,and the Case Filter.
In both cases, the relation between the underlying position andthe surface string is expressed by chains.
Chains consist of the word that undergoesmovement and all the positions this word occupies in the course of a derivation.
In(1) the chains are (John, t) and (The children, t).
(1) a. John seems \[ip t to like Bill \]b.
The children are loved t by John.The advantage of this treatment is that common properties of language, here certainclasses of verbs, are expressed by common principles.This search for generality is not unique to GB theory.
Feature-structure formalismsalso use rule schemata to capture similarities among grammar ules.
Moreover, reen-trancy as a notational device to express common features eeks the same type ofrepresentational economy that is expressed by the use of "traces" in GB theory.It is desirable for a syntactic analyser to make use of linguistic theories to obtain,at least in principle, the same empirical coverage as the theory, and to capture thesame generalizations.
Moreover, a parser that makes direct use of a linguistic theory ismore explanatory.
A guiding belief for the development of the generative frameworkis that a theory that can derive its descriptions from the interaction of a small set ofgeneral principles is more explanatory than a theory in which descriptive adequacy isobtained by the interaction of a greater number of more particular, specific principles(Chomsky 1965).
This is because the former theory is smaller.
Thus, each principle cangenerate a set the encoding of which would require a much larger number of bits thanthe bits needed to encode the principle itself.
The classic example is the use of naturalclasses of distinctive features in phonology, in order to compact several rules into one.A modular theory that encodes universal principles has obtained a greater degree of?
succinctness than a nonmodular theory, and is considered more explanatory.
Since it516Paola Merlo Modularity and Information Content Classesis desirable for the parser to maintain the level of explanatory power of the theory, itmust maintain its modularity.It has also been argued (Berwick 1991) that the current shift from rules to a modu-lar system of principles has computational dvantages.
Principle-based grammars en-gender compactness: Given a set of principles, P1, P2, .
.
.
,  Pn, the principles are storedseparately and their interaction is computed on-line; the multiplicative interaction ofthe principles, P1 x P2 x ... ?
Pn does not need to be stored.
Hence, the size of thegrammar is the sum of the sizes of its components: IGI = P~ + P2 + "'" 4- Pn.
Con-sequently, a parser based on such a grammar is compact, and, theoretically, easierto debug, maintain and update.
1 In practice, however, designing and implementingfaithful and efficient parsers is not a simple matter.Defining "faithfulness" to a linguistic theory is not a trivial task, as a direct relationbetween the grammar and the parser is not the only option (see Bresnan 1978; Berwickand Weinberg 1984; van de Koot 1990, and references therein).
In general, it is notnecessary for a parser to implement the principles of the grammar directly.
Rather, acovering grammar could be used, more suited to the purpose of parsing.
However, itis important hat such covering be done in such a way that accidental properties ofa particular grammar, which would not hold under counterfactual changes, are notused.
Otherwise, the covering grammar would not be sufficiently general.A faithful implementation is particularly difficult in the GB framework, as GBprinciples are informally expressed as English statements, and can take a variety offorms.
For example, X theory (a condition on graphs), the Case Filter (an output filteron strings), and the 0 criterion (a bijection relation on predicates and arguments) allfall under the label of principles.
Attempts have been made to formalize GB principlesto a set of axioms (Stabler 1992).One possible, extreme interpretation of the direct use of principles is an approachwhere no grammar compilation is allowed (Abney 1989; Frank 1992; Crocker 1992).
2This approach is appealing because it reflects, intuitively, the idea of using the grammaras a set of axioms and reduces parsing to a deduction process.
This is very much inthe spirit of the current shift in linguistic theories from construction-dependent rulesto general principles, and it separates quite clearly the grammar from the parsingalgorithm.However, it is not obvious that this approach is efficient.
Partial evaluation andvariable substitution can increase performance, but, as usual, a space/time trade-offwill ensue.
Excess of partial evaluation off-line increases the size of the grammar,which might, in turn, slow down the parse.
Experimentation with different kindsof algorithms uggests that some amount of compilation of the principles might benecessary to alleviate the problem of inefficiency, but that too much compilation slowsdown the parser again.1 Berwick (1982, 403ff.)
shows that the size of a cascade of distinct principles (viewed as machines) is thesize of its subparts, while if these same principles are collapsed, the size of the entire system growsmulfiplicatively.
Modularity corresponds to maximal succinctness when all independent principles arestated separately.
Independent principles are, intuitively, principles that can be computedindependently of each other, and therefore whose interactions are all possible.
Barton et al (1987) andBerwick (1990) attempt to formalize the concept of independence asseparability, assuming that thetopology of a principle-based theory like GB can be mapped onto a planar graph.
In fact, ifindependent modules are separable modules, there is little reason to think that GB is modular, as itcorresponds to a highly connected graph.2 By compilation, here and below, I mean off-line computation of some general property of the grammar,for example the off-line computation of the interaction of principles, using partial evaluation orvariable substitution.517Computational Linguistics Volume 21, Number 41.2 On-line Computation is InefficientSeveral researchers note that principle-based parsers allowing no grammar precom-pilation are inefficient.
Firstly, Johnson (1989), Stabler (1990), and van de Koot (1991)note that the computation of a multi-level theory without any precompilation mightnot even terminate.
Secondly, experimental results show that an entirely deductiveapproach is inefficient.
Kashket (1991) discusses a principle-based parser, where nogrammar precompilation is performed, and which parses English and Warlpiri us-ing a parameterized theory of grammar.
The parsing algorithm is a generate-and-test,backtracking regime.
Kashket (1991) reports, for instance, that a 5-word sentence inWarlpiri (which can have 5!
analyses, given the free word order of the language) cantake up to 40 minutes to parse.
He concludes that, although no mathematical nalysisfor the algorithm is available, the complexity appears to increase xponentially withthe input size.Fong (1991, 123) discusses a parsing algorithm.
He shows that an initial version ofthe parser, where the phrase structure rules were expressed as a DCG and interpretedon-line, spent 80% of the total parsing time building structure.
In a later version,where rules were compiled into an LR(1) table, structure-building constituted 20% ofthe total parsing time.
This same parser includes a module for the computation oflong distance dependencies, which works by generate-and-test.
Fong finds that thisparsing approach is also inefficient.Dorr (1987) notices similar effects in a parser that uses an algorithm more parallelin spirit (Earley 1970).
Dorr notes that a limited amount of precompilation of the prin-ciples speeds up the parse, otherwise too many incorrect alternatives are carried alongbefore being eliminated.
For example, in her design, X theory and the other principlesare coroutined.
She finds that precompiling the principles that license empty categorieswith the phrase structure rules reduces considerably the number of structures that aresubmitted to the filtering action of the other principles, and thus speeds up the parse.In all these cases, the source of inefficiency stems from the principle-based design.Because ach principle is formulated to be as general as possible, the "logical" abstrac-tion of each principle from the others causes a lot of overgeneration f structure and,consequently, a very large search space.1.3 Too Much Precompilation is InefficientSimple precompilation is not a solution to the inefficiency of principle-based parsing,however.
Experimentation with different amounts of precompilation shows that off-line precompilation speeds up parsing only up to a certain point, and that too muchprecompilation slows down the parser again.The logic of why this happens is clear.
The complexity of a parsing algorithm isa composite function of the length of the input and the size of the grammar.
For thekind of input lengths that are relevant for natural anguage, the size of the grammareasily becomes the predominant factor.
If principles are precompiled in the form ofgrammar ules, the size of the grammar increases.As Tomita (1986) points out, input length does not cause a noticeable increase inrunning time up to 35 to 40 input tokens.
For sentences of this length, grammar size be-comes a relevant factor for grammars that contain more than approximately 220 rules,in his algorithm (an LR parser with parallel stacks).
Both Dorr (1987) and Tomita (1986)show experimental results confirming that there is a critical point beyond which theparser is slowed down by the increasing size of the grammar.
In the Generalized PhraseStructure Grammar (GPSG) formalism (Gazdar et al 1985), similar experiments havebeen performed, which confirm this result.
Parsers for GPSG are particularly interest-ing, because they use a formalism that expresses many grammatical generalizations in518Paola Merlo Modular i ty  and Information Content Classesa uniform format.
Therefore, GPSG is, in principle, more amenable to being processedby known parsing techniques.
Thompson (1982) finds that expanding metarules, ratherthan computing them on-line, is advantageous, but that instantiating the variables inthe expanded rules is not.
Phillips and Thompson (1985) also remark that compilingout a grammar of twenty-nine phrase-structure rules and four metarules i equivalentto "several tens of millions of context-free rules."
Phillips (1992) proposes a modifica-tion to GPSG that makes it easier to parse, by using propagation rules, but still notesthat variables hould not be expanded.In conclusion, the lesson from experimentation is that parsing done totally on-lineis inefficient, but that compilation is not always a solution.
A parser that uses linguisticprinciples directly must fulfill apparently contradictory demands: for the parser to belinguistically valid it must use the grammar directly, while a limited amount of off-lineprecompilation might make the parser more efficient.
3 In the next section, I proposeand discuss a solution to this problem that builds on other approaches and relates theparser to the grammar in a principled way.2.
The ProposalTwo avenues have generally been pursued to build efficient GB parsers.
In one case, a"covering rammar" is compiled, which overgenerates and is then filtered by con-straints.
The compilation is done in such a way that the overgeneration is well-behaved.
For instance, the correct distribution of empty categories i  calculated off-line(Dorr 1993).
In the other case, all the principles are applied on line, but they applyonly to a portion of the tree, and are therefore restricted to a local computation (Frank1992).
4My proposal combines these two approaches: it adopts the idea of compilingthe grammar, at least partially, off-line but it attempts to find a principled way ofdoing so.
In this, I differ from Dorr, where the amount of compilation is heuristicand based on practical experimentation.
The approach shares Frank's intuition thatlinguistic principles have a form, which can be exploited in structuring the parser.This proposal is based on two observations.
First, each principle of linguistic theoryhas a canonical form, and second, primitives of linguistic theories can be partitionedinto classes, based on their content.As an illustration of the first observation, we can look at the principle that regulatesthe distribution of the empty categories in the phrase marker, the Empty CategoryPrinciple (ECP), as stated below (adapted from Rizzi 1990, 25).
(2).The Empty Category PrincipleAn empty category x is licensed if the 3 following conditions aresatisfied:x is in the domain of a head H3 For CF parsers, just how much compilation speeds up the parser is defined precisely by the analysis ofthe algorithm.
No such precise analysis is available for principle-based algorithms.4 Frank (1992) presents a parsing model that is claimed not to allow any compilation of the linguistictheory, and to operate in linear time.
Two objections can be raised to these claims: first, the use of TAGelementary trees to restrict he working space of the parser amounts to a precompilation fphrase-structure and locality constraints, o that locality is not computed in the course of the parse, butbasically done as template matching.
Second, in the measure of complexity, Frank does not count thecost of choosing which elementary tree to unadjoin or unsubstitute, or the cost of backtracking if thewrong decision is made.
There are indeed cases where, in order to perform the correct operation, morethan one elementary tree must be spanned.
It is not clear that linear time complexity can actually beclaimed if all factors are taken into account.
For a more detailed iscussion, see Merlo 1992, to appear.519Computational Linguistics Volume 21, Number 4.3.the category of H E {A, Agr, N, P, T, V}there is no barrier or head H r that intervenes between H and xIt can be observed that this principle has an internal structure and can be decom-posed into separate pieces of information: (2.1) imposes a condition on configurations,namely, a condition on the shape of the tree; (2.2) imposes a condition on the labellingof the nodes in the tree; and (2.3) imposes a locality condition, as it defines the subtreeavailable for the computation of the principle.
These three conditions are independent.For instance, the configuration does not depend on the categorial labelling of the headnode.
The precompilation of these conditions would require computing all the possiblecombinations, without any reduction of the space of analysis.The second observation is based on a detailed inspection of the form of the prin-ciples of the grammar.
What is presented in (2) as an illustrative xample is, in fact, aconsistent form of organization of the principles.
If one looks at several of the princi-ples of the grammar that are involved in building structure and annotating the phrasemarker, one finds the same internal organization.Theta-assignment occurs in the configuration of sisterhood, it requires a 0-assigninghead, and it must occur between a node and its most local assigner.
Assignment ofCase occurs in a given configuration (according to Chomsky (1988, 1992) it is alwaysa specifier-head configuration), given a certain lexical property of the head (\[-N\]), andlocally, within the same maximal projection).
The same restriction occurs again forwhat is called the wh-criterion (Rizzi 1991), which regulates wh-movement, where thehead must have a +wh feature and occur within a specifier-head configuration.
Cat-egorial selection and functional selection also occur under the same restrictions, inthe complement configuration (i.e., between a head and a maximal projection).
Thelicensing of subjects in the phrase marker, done by predication, must occur in thespecifier-head configuration.
The licensing of the empty category pro also requires theinflectional head of the sentence to bear the feature Strong Agr, and it occurs in thespecifier-head configuration.
The assignment of the feature \[+ barrier\] depends onL-marking, which in turn requires that the head is lexical, and that marking occurs inthe complement configuration.Thus, each different "factor" that composes a principle can be considered a sepa-rate primitive, and such primitives can be grouped into classes defined according totheir content.
Linguistic information can be classified into five different classes:(3) a. Configurations: isterhood, c-command, m-command, + maximalb.
Lexical features: iN ,  +V, +Funct, +c-selectedc.
Syntactic features: +Case, +0, +% +barrier, +Strong Agrd.
Locality information: minimality, antecedent governmente.
Referential information: q-anaphor, q-pronominal, indicesThis qualitative classification forms a partitioning into natural classes based oninformation content.
I call these IC ClassesP5 Differently from Crocker (1992, to appear) and Frazier (1985), this partitioning does not rely on theparticular epresentation used.
The spirit of the hypothesis i that linguistic theory is formed byheterogeneous types of information, and that the representation used to describe them is a derivedconcept.
Frazier (1990) proposes an evolutionary partitioning of the parser based on tasks.
This520Paola Merlo Modularity and Information Content ClassesIt can then be hypothesized that the amount of compilation (or, conversely, themodularity of the parser) is captured by the notion of IC classes as follows:IC Modularity Hypothesis (ICMH)Precompilation within IC Classes improves efficiency.Precompilation across IC Classes does not.In other words, a parser that takes advantage of the structure of linguistic principleswill maintain a modular design based on the five classes in (3).Although the ICMH is not so stringent as to make predictions that converge ona single parsing architecture, it does provide some predictive power about the orga-nization of the parser.
First, structural information is encoded separately from lexicalinformation.
Standard context-free rules, specified with category, such as VP --*V NP,are not compatible with the ICMH, nor are proposals in the spirit of licensing gram-mars (Abney 1989, Frank 1992), where information is encoded in each lexical item.Second, the ICMH predicts that long-distance dependencies, represented as chains,are computed in steps.
Empty categories are licensed in two computational steps:structural licensing by an appropriate head, and feature instantiation.
With respect ofeature instantiation in particular, it is predicted that precompiling syntactic featuresspeeds up the parsing process.
This is different from functional approaches such asFong (1991), and Fong and Berwick (1992), in which there is no precompilation.
6These predictions eem to be supported (and, consequently, so is the ICMH) bytwo main results, which are illustrated below:.mseparating X from lexical information yields more compact datastructures; I propose a parser that uses two compiled tables: one thatencodes tructural information, and the other that encodes lexicalinformation.. using syntactic features to compute mpty categories reduces the searchspace, complex chains can be computed efficiently.These claims are supported in the next section, where I discuss the properties of animplemented parser, which computes imple, complex, and multiple chain formation,as exemplified in Figure 1.
This subset of constructions has been chosen because it con-stitutes the crucial test set for principle-based parsers: it involves complex interactionsof principles over large portions of the tree.
7perspective is not in opposition to the current proposal, as the specialization f the parser in differenttasks is likely to be an adaptive reaction to the different types of inputs.At first sight it might appear that the notion of types proposed by Fong (1991) is similar to ICclasses.
In fact, the similarity is superficial.
Clearly, both notions constitute an attempt to partition theset of principles into smaller subsets.
However, Fong's types are a mechanism tointerleave constraintsand phrase structure rules automatically.
They are a method to schedule the on-line computation fprinciples that are the direct ranslation ofthe theory, and not a way of defining the design of theparser.
In Fong's view, all computations are done on-line and the parser eflects the theory as directlyas possible.6 It is difficult o separate precisely "lexical" from "syntactic" features.
One can consider "syntactic"those features that are used to determine the well-formedness of yntactic trees.
In the spirit of morerecent developments in syntactic theory, I consider syntactic those features that are involved in someparticular incarnation ofthe "Generalized Licensing Condition" (Sportiche 1992.)
These include 0-roles,case, (possibly all ~6 features), and, following Rizzi (1991), Haegemann a d Zanuttini (1991), andLaenzlinger (1993), also wh, neg, adverb.7 Many other proposals either do not deal with all types of chains (Frank 1992; Johnson 1989, for521Computational Linguistics Volume 21, Number 4TYPE EXAMPLE1 Simple Transitive2 Simple Intransitive3 Simple Passive4 Simple Raising5 Embedded Transitive6 Embedded Intransitive7 Embedded Raising8 Simple Question9 Embedded Question10 Embedded Question and Raising11 Embedded Wh-Questionjohn loves maryjohn runsmary was lovedmary seems to like johnjohn thinks that mary loves billjohn thinks that mary runsmary thinks that john seems to like billwho does john love ?who do you think that john likes ?who did you think that john seemed to like ?
* who did you wonder why mary liked ?Figure 1Types of Sentences.In the rest of the paper, I first discuss the advantages of stor ing X informationseparately f rom lexical information (section 3).
I then turn to the computat ion  of long-distance dependencies.
I i l lustrate two algor i thms to compute  chains: I show that aparticular use of syntactic feature information speeds up the parse, and I discuss theplausibil ity of using algor i thms that require strict left-to-right annotat ion of the nodes(section 4).
In fact, the a lgor i thm I propose appears to be interestingly correlated to agap in the typo logy  of natural  languages.3.
The Computation of Phrase StructureIn order to explore the val idity of the proposed hypothesis  about the modular i ty  ofthe parser, an analyzer for English was developed.
Each of the data structures is thedirect implementat ion of l inguistic objects with different information contents.
Theinput to the algor i thm is an unannotated sentence.
The output  consists of a tree and alist of two chains: the list of A chains and the list of A chains, that is, chains formedby wh-movement  and NP  movement ,  respectively.
The main  pars ing algor i thm is amodif ied LR parsing algor i thm augmented  by mult i -action entries and constraints onreduction.
8The structure-bui lding component  of the parser is dr iven by an LR(k) parser(Knuth 1965) which consults two tables.
One table encodes X information (fol lowingKornai and Pul lum 1990).
The other table encodes lexical information.
Lexical infor-mat ion is consulted only if it is needed to d isambiguate a state containing mult ipleactions in the LR parser.
An  overv iew of this design is shown in Figure 2.instance) or they require extensive backtracking (Fong 1991; Fong and Berwick 1992).
In formalismother than GB theory, gaps are encoded irectly into the rules.
Both GPSG and HPSG use slash featuresto percolate features to gaps.
The use of slash features probably simplifies the computation.
There hasbeen a debate on the explanatory adequacy of grammars that employ slash features (see van de Koot1990, and Stabler 1994).
For my purposes, note that, if anything, I am dealing with the worst case forthe parser.8 The ICMH is not sufficient o predict a specific parsing architecture, but rather it loosely dictates theorganization of the parser.
The choice of an LR parser then is the result of the ICMH (with which theparser's organization must be compatible) and additional independent factors.
First, LR parsers havethe valid prefix property, namely they recognize that a string is not in the language as soon as possible(other parsing methods have this property as well, for instance Schabes 1991).
A parser with thisproperty is incremental, in the sense that it does not perform unnecessary work, and it fails as soon asan error occurs.
Second, the stack of an LR parser encodes the notion of c-command implicitly.
This is522Paola Merlo Modularity and Information Content ClassesInput \[\StackLR ParsingProgramI \ I,I' LR TableIFigure 2I Chains I/ ~constr )o-occurrence TableI !q / I  I I I I Ili I I I I I~'NI  I Iil I 1/1'1 I i l  I I I I I III I ' i  I I \[ \]~- I i I Il l I,," ' iLi ,  iI i R I ' 1 -1  IP i i  I I I I I,1111  I,I k l l  I I I L I,,,.
- jIIOrganization of the Parser: The data structures (tables, stack and chains) are represented asrectangles.
Operations on feature annotation are performed by constraints, represented asovals.Figure 3Category-Neutral Grammar.X" --* Y" X' specificationX" --* X' Y"X' --* X Y" complementationX' ~ Y" XX' --* Y" X' modificationX' --* X' Y"X' ~ X unary headX" ~ X' unary XmaxX ~ empty empty headsX" --~ empty empty XmaxnThe context-free grammar compiled in the LR table is shown in Fi__gure 3.
Thecrucial feature of this grammar is that nonterminals pecify only the X projectionlevel, and not the category.
Because the LR table is underspecified with respect o thecategorial labels of the input, many instances of LR conflicts arise, which can be teasedapart by looking at the co-occurrence r strictions on categories.
This information wouldbe stored in the rules themselves in ordinary context-free rules.
However, ordinarycontext-free rules do not encode many other types of lexical information also used inparsing.
Thus, they lose generality, without exploiting all the available information.As an illustration, consider the following set of context-free rules.crucial for fast computation of chains.
Third, LR parsers are fast.523Computational Linguistics Volume 21, Number 4(4) 1.
C' -+ Co IP2.
I' --* I0 VP3.
V' ~ V0 NP4.
V' ~ V0 e5.
V ~ ~ V0mRules 1-4 have the same X structure, but they differ in the labels of the nodes.
Inrules I and 2 the heads, Co and I0 respectively, are followed by IP and VP obligatorily.Rules 4 and 5 cover the same string.
Clearly, by writing 1-4 as different rules, the factthat they are instances of the same structure is not captured.
Similarly, the obligatori-ness of IP and VP as complements of Co and I0 is lost.
Finally, the choice of rule 4 orrule 5 depends on the actual verb in the string.
If the verb is intransitive, rule 4 cannotapply.In the parser, structural information is separate from information about co-occur-rence (rules 1-4), functional selection (rules 1, 2) and subcategorization (rules 4, 5).This information is stored in a table, called a co-occurrence table.
The table storesinformation about obligatory complementation, such as the fact that I0 must be fol-lowed by a VP.
It also stores compatible continuations based on subcategorization.For instance, consider the case in which the current oken is an intransitive verb.
TheLR table contains two actions that match the input: one action generates a projectionof the input node (V'), without branching, while the other action creates an emptyobject NP.
By consulting the subcategorization information, the parser can eliminatethe second option as incorrect.Using an LR table together with a co-occurrence table is equivalent in coverage to afully instantiated LR table, but it is more advantageous in other espects.
Conceptually,the latter organization encodes X theory directly, and it maintains a general design,which makes it applicable to several anguages.
Practically, there is reason to thinkthat it is more efficient.3.1 Testing the ICMH for phrase structureThe prediction made by the ICMH is that compiling together X theory and categorialinformation will increase the size of the grammar without reducing the nondetermin-ism contained in the grammar, because category/subcategory information belongs toa different IC Class than structural (i.e., X) information.Method and Materials.
The size of the grammar is measured as the number of rules ornumber of states in the LR table.
The amount of nondeterminism is measured as theaverage number of conflicts (the ratio between the number of actions and the numberof entries in a table.)
9Three grammars were constructed, constituting (pairwise) as close an approxima-tion as possible to minimal pairs (with respect o IC Classes).
They are shown in theAppendix.
Grammar 1 differs minimally from Grammar 2, because ach head is in-stantiated by category.
The symbol YP stands for any maximal projection admittedby linguistic theory.
Grammar 3 differs minimally from Grammar 2, because it also9 The average number of conflicts in the table gives a rough measure of the amount of nondeterminismthe parser has to face at each step.
However, it is only an approximate measure for at least tworeasons: taking the mean of the conflicts abstracts away from the size of the grammar, which might bea factor, as the search in the table becomes more burdensome for larger tables (but, if anything, it playsagainst small grammars/tables); moreover, it does not take into account he fact that some states mightbe visited more often than others.524Paola Merlo Modularity and Information Content ClassesTable 1Comparison of the 3 grammars (compiled into LR tables)NB OF NB OF NB OF AVERAGEENTRIES ACTIONS RULES COI~FLICTSGRAMMAR 1 63 123 16 1.95GRAMMAR 2 793 1319 51 1.78GRAMMAR 3 251 962 41 3.83Table 2Number of actions in the 3 LR tablesNUMBER OF ACTIONSENTRIES 1 2 3 4 5 6 7 8 9 10 11 12 13 14GRAMMAR 1 38 6 8 8 1 2GRAMMAR 2 465 68 168 6 42GRAMMAR 3 144 43 3 8 5 4 30 14includes some subcategorization information (such as transitive, intransitive, raising),and some co-occurrence r strictions and functional selection.
Moreover, empty cate-gories are "moved up," so that they are encountered as high in the tree as possible.These three grammars are then compiled by the same program (BISON) into three(LA)LR tables.
The results are shown in Table 1, which compares ome of the indicesof the nondeterminism in a given grammar to its size, and Table 2, which shows thedistribution of actions in each of the grammars.Discussion.
Consider Grammar I and Grammar 2 in Table 1.
Grammar 2 has a slightlysmaller average of conflicts, while it has three times the number of rules and twelvetimes the number of entries, compared to Grammar 1.
The fact that Grammar 2 islarger than Grammar 1, with only a slightly smaller average of conflicts, confirms theprediction made by the ICMH that compiling X theory with categorical informationwill increase the size of the grammar without decreasing nondeterminism.
Since thenumber of rules is expanded, but no "filtering" constraint is incorporated in Grammar2 with respect o Grammar 1, this result might not seem surprising.However, the ICMH is also confirmed by the other pairwise comparisons and bythe global results.
Grammar 3 has a higher number of average conflicts than Grammar2, but it is smaller, both by rules and LR entries, so it is more compact.
Notice thatadding information (subcategory, selection, etc.)
has a filtering effect, and the resultinggrammar is smaller.
However, adding information does not reduce nondeterminism.Compared to Grammar 1, Grammar 3 does not show any improvement on eitherdimension: Grammar 3 is both larger (four times as many LR entries) and more non-deterministic than Grammar 1.
Globally, one can observe that an increase in grammarsize, either as a number of rules or number of LR entries, does not correspond to aparallel decrease in nondeterminism.As Table 2 shows, the distribution of the conflicts in Grammar 3 presents omegaps.
This occurs because certain groups of actions go together.
Two main patterns ofconflict are observed: In those states that have the highest number of conflicts, all rulesthat cover the empty string can apply; in those states that have an intermediate number525Computational Linguistics Volume 21, Number 4Table 3Comparison of the 3 grammars (compiled into LL tables)NB OF NB OF NB OF AVERAGEENTRIES ACTIONS RULES CONFLICTSGRAMMAR 1 19 62 16 3.26GRAMMAR 1' 19 46 13 2.42GRAMMAR 2 112 255 51 2.28GRAMMAR 3 144 368 41 2.62of conflicts, only some rules can apply, namely, those that have a certain X projectionlevel, and that cover the empty string (e.g., all XP's, independent of category, thatcover the empty string).
This observation confirms that categorial information doesnot reduce nondeterminism.
On the contrary, adding categorical information multi-plies nondeterminism by adding structural configurations.
Even introducing "filter-ing" lexical information (co-occurrence r strictions and functional complementation)does not appear to help.
In fact, ambiguities caused by empty categories occur accord-ing to structural partitions.
The qualitative observation supports the numerical results:Introducing categorial information is not advantageous, because it increases the sizeof the grammar without decreasing significantly the average number of conflicts.3.2 Extending the test to other compilation techniquesThe effects discussed above could be an artifact of the compilation technique.
In orderto check that this is not the case, the same three grammars (reported in the appendix)were compiled into LL and Left Corner (LC) tables.LL compilation: Discussion.
The LL compilation method yields results similar to thoseof the LR compilation, although less clear cut.
This confirms the intuition that theresults reflect some structural property of the grammar, and are not an artifact of theLR compilation.The results of the compilation of the same grammars into LL tables are shown inTable 3.
Grammar 1' is a modified version of Grammar 1, without adjunction rules.These figures show that there is no relation between the increased specialization ofthe grammar and the decrease of nondeterminism.
Note that the LL compilation doesnot maintain the paired rankings of actions and rules.
So, for the LL table, the co-occurrence of lexical categories does not play a filtering role.Globally, there appears to be an inverse relation between the size of the grammar,measured by the number of rules, and the average number of conflicts: the largerthe grammar the smaller the number of conflicts.
This might make one think thatthere is some sort of relation between grammar size and nondeterminism after all.However, this is not true if we use the number of entries as the relevant measure ofsize.
Moreover, if one looks at Grammar 1', which is smaller than Grammar 1, one cansee that the average number of conflicts decreases quite a bit.
This confirms a weakerhypothesis, which is nonetheless related to the initial one, namely that nondeterminismdoes not vary in an inverse function to "content of information.
"Some qualitative observations might help clarify the sources of ambiguity in thetables.
In all three grammars, the same ambiguities are repeated, for each terminalitem.
In other words, all columns of the LL table are identical (with the exception526Paola Merlo Modularity and Information Content ClassesTable 4Comparison of the 3 grammars (compiled into LC tables)NB OF NB OF NB OF AVERAGEENTRIES ACTIONS RULES CONFLICTSGRAMMAR 1 49 136 16 2.77GRAMMAR 2 1456 4030 51 2.76GRAMMAR 3 398 610 41 1.53Table 5Number of actions in the 3 compiled LC tablesNUMBER OF ACTIONSENTRIES 1 2 3 4 5 6 7 18 19GRAMMAR I 4 18 15 9 3GRAMMAR 2 602 702 48 96 8GRAMMAR 3 282 92 4 4 4 12of cell \[X0, wp\] in Grammar  1.)
This suggests that lexical tokens do not provide anyselective information.
Moreover, as we saw in th.e LR tables, projections to the samelevel have the same pattern of conflicts.
(In Grammar  2, the number  of conflicts ismultipl ied by the number  of categories.)
1?LC-compilation: Discussion.
The same three grammars  were compiled in left corner (LC)tables.
The result of the compilation are shown in Table 4, and the distribution of theconflicts is shown in Table 5.
As can be seen from Table 4, Grammar  2 is three timeslarger than Grammar  1 and is compiled in a table that has twenty-nine times as manyentries, but the average number  of conflicts is not significantly smaller.The interpretation of the LC table derived from Grammar  3 poses a problem forthe ICMH.
Grammar  3 is larger than Grammar  1, as it contains category and someco-occurrence information, but its average of conflicts is smaller.
In this case, it seemsthat adding information reduces nondeterminism.
On the other hand, compared toGrammar  2, both the table and the average number  of conflicts are smaller.
I take this tomean that the ICMH is confirmed only by a global assessment of the relation betweenthe content of information and the average conflicts, but not by pairwise comparisonsof the grammars.
Notice however, that the difference in the two pairwise comparisonsconfirms that simple categorial information does not perform a filtering action on thestructure, while lexical co-occurrence does.
This is precisely what I propose to compilein the lexical co-occurrence table.The qualitative inspection of the tables confirms the clustering of conflicts sug-gested by Table 5.
Grammar  1 and Grammar  2 show the same patterns of conflicts asthe LR and LL tables: conflicting actions cluster with the bar level of the category.
So,for example, in Grammar  2, one finds that when the left corner is a maximal projection10 In all cases, this is caused by the X form of the grammar.
Namely, the loci of recursion and gapping areat both sides of the head, and anything can occur there.
Eliminating this property would  be incorrect,as it would amount  o el iminating one of the crucial principles of GB, namely move-c~, which says thatany maximal  projection or head can be gapped.527Computational Linguistics Volume 21, Number 4the action is unique, while when the corner is a bar level projection there are multipleactions and they are the same, independently of the input token.
In Grammar 3, thesame patterns of actions are repeated for each left corner, independently of the goalor of the input token.The qualitative inspection of the compiled tables is coherent across compilationmethods and appears, in general, to support he ICMH, as the interaction of structuraland lexical information is the cause of repeated patterns of conflicts.
Quantitatively, theresults, which are very suggestive in the LR compilation, are less clear in the other twomethods.
However, in no case do they clearly disconfirm the hypothesis.
I concludethat categorial information should be factored out of the compiled table and separatedata structures hould be used.
u4.
The Computation of ChainsAs a result of using a category-neutral context-free backbone to parse, most of thefeature annotation is performed by conditions on rule reduction associated with eachcontext-free rule, which are shown in Figure 4.The most interesting issues arise in the treatment of tiller-gap dependencies, whichare represented as chains.
Informally, a chain is a syntactic object that defines anequivalence class of positions for the purpose of feature assignment and interpretation.
(5) a. Maryi was loved tib.
Whoi did John love ti ?c.
Maryi seemed t I to have been loved ti .d.
Whoi did John think t I that Mary loved ti ?The sentence in (5a), for example, contains the chain (Maryi, ti), which encodes the factthat Mary is the object of love, represented by the empty category t.In this parser, empty categories are postulated by the LR parser, when buildingstructure, and their licensing is immediately checked by the appropriate condition onrule reductions, shown in Figure 4.Many principles regulate the distribution of chains.
For the purpose of the follow-ing discussion, it is only necessary to recall that a chain can only contain one thematicposition and one position that receives case.
Moreover, chains divide into two types:11 It should be noted that, although phrase-structure rules are reduced to the bare bones, they cannot beeliminated altogether.
Parsers that project phrase structure and attachments entirely from the lexiconhave been presented by Abney (1989) and Frank (1992), using licensing rammars (LS).
They stifferfrom serious shortcomings when faced with ambiguous input, as they do not have enough globalknowledge of the possible structures in the language to recover from erroneous parses.
Abneyalleviates this problem by attaching LR states to the constructed nodes, thus losing much of the initialmotivation of the licensing approach.
Frank's parser is augmented by a parse stack to parse head-finallanguages.
Frank does not discuss this issue in detail, but it seems that a "shift" operation must beadded to the operations of the parser.
As there could always be a licensing head in the right context,which would license a left-branching structure, the "shift" operation is always correct.
But then, theparser might reach the end of the input (or at least the end of the relevant elementary tree, i.e., themain predicate-argument structure) before realizing either that it pursued an incorrect analysis, in thecase of ambiguous input, or that the input is ill-formed.
Thus, this augmented parser could notrecognize rrors as soon as they are encountered.
Finally, note that all the augmentation necessary tomake the LS grammar work make it equivalent to a phrase-structure grammar, possibly with thedisadvantage of being procedurally instead of declaratively encoded.
On the other hand, a precompiledtable which keeps track of all the alternative configurations guarantees that incorrect parses aredetected as soon as possible, and, if alternative parses exist, they will be found.528Paola Merlo Modularity and Information Content ClassesCONSTRAINT FUNCTION0-criterion checks if all chains in the chain list havereceived a 0-roleCase filter checks if all chains in the chain list haveCasenode labelling determines what kind of chain link the cur-rent node is: head, intermediate, footchain selection selects chain to unify with current nodechain unification unifies node with selected chainhead feature percolation consults cooccurrence table anddetermines cooccurrence r strictionsamong heads0-marked marks node with available 0-rolecase marked marks node with available Casec-select categorial selectionis-a barrier checks if maximal projection is a barrierlicense empty head checks features of closest lexical headlicensing head finds a lexical head to license a maximalprojectionlocality checks that the maximal projections be-tween antecedent and empty category arenot barriersFigure 4The Constraints.mwh-chains, also cal led A-chains,  and  NP-movement  chains,  also cal led A-chains;  theempty  categor ies that occur in these chains have di f ferent propert ies .
More  than onechain can occur in a sentence.
Mul t ip le  chains occurr ing  in the same sentence caneither be dis jo int  or  intersected.
12 Disjoint chains are nested,  as in (6a).
If chains in-tersect, they share the same index and  they have exact ly  one e lement  in common,  asin (6b).
(6) a. Whoi  d id  Maryi  seem tj to l ike ti?b.
Who i  d id  you  th ink ti seemed ti to l ike Mary?4.1 The AlgorithmsWhen bu i ld ing  chains,  several  p rob lems must  be solved.
First of all, the parser  mustdec ide whether  to start  a new chain or not.
It must  also dec ide  whether  to start  achain headed by  an e lement  in an argument  pos i t ion  (A-chain),  such as the head of apass ive  chain, or a chain headed by  an e lement  in a non-argument  pos i t ion  (A-chain),12 Actually, chains can also compose.
If chains compose they do not have intersecting elements, but theycreate a new link.
This type of chain is exemplified in (i).
We will only discuss chains of the types of (6).
(i) Who i did you meet ti Oi without greeting ti?529Computational Linguistics Volume 21, Number 4such as the head of a wh-chain.
Second, on positing an empty element, the parser mustdecide to which chain it belongs) 3The two decisions can be seen as instances of the same problem, which consistsin identifying the type of link in the chain that a given input node can form (whetherhead, intermediate or foot, abbreviated as H,I,F in what follows.)
One can describe thissequence of decisions as two problems that must be solved in order to form chains: theNode Labelling Problem (NLAB), and the Chain Selection Problem (CSEL), formulatedbelow.The Node Labelling Problem (NLAB).Given a node N to be inserted in a chain, determine its label L, whereL ?
{AH, AH, AI, AI, AF, AF}.This problem defines a relation R: N x L, where N belongs to the set of nodes, andL belongs to the set of labels for the elements of chains.
The labels of possible chainlinks reflect the theoretical distinctions between A-movement and A-movement, andthe fact that links of a chain can be either the first element of the chain, the head (H),or an intermediate element (I) in the case of chains formed by several links, or the lastelement, the foot (F).
14Algorithm 1Input: Node, Local ConfigurationOutput: List of LabelsIf Node is not empty thenIf Node is \[+wh\] then Label +-- AHelse Label *-- AHelseIf Node has 0-role thenIf Node has Case then Label +-- AFelse Label ~ AFelseIf Local Configuration = Spec of C then Label +-- AIelse Label +-- AIThere are six possible outputs for this algorithm.
The first case arises when thenode N is a lexical wh-word, which starts a wh-chain.
The second possibility is if thehead is lexical, but not a question word.
In this case, an argument chain (A-chain)is started, as in passives.
The last four cases deal with empty categories.
The feature13 Strictly speaking, it must also provide a rescuing procedure.
This can be done by checking whether allthe chains satisfy the well-formedness.conditions.
If ot all the chains satisfy the well-formednessconstraints, the parser can attempt to intersect or compose two or more chains in order to satisfy thewell-formedness conditions.
These two problems are not treated here.
For an illustration, under thename of Chain Intersection Problem and Chain Composition Problem, see Merlo 1992.14 I present here a simplified version of the algorithm, to avoid technical linguistic details, which are notrelevant for the following discussion.
However, one should also output a label AOp, which designatesthe empty operator that binds, for instance, the empty variable in a parasitic gap construction andother cases of non-overt movement, such as relative clauses.
In the man OP !
saw an empty operator ispostulated by analogy to the man whom/that I saw.
AOp is licensed by the same conditions that licensean intermediate A trace.530Paola Merlo Modularity and Information Content Classesannotation of the category is inspected: case distinguishes the foot of an A-chain fromthe foot of an A-chain, while intermediate traces are characterized by a lack of 0-roleand by their configurations (i.e., intermediate A empty categories occur in A positions(spec of I), while intermediate A empty categories occur in A positions (spec of C)).Once the potential chain links have been labelled, a second algorithm looks for achain that can "accept" a node with that label.The Chain Selection Problem (CSEL)Given a node N of label L, and an ordered list of chains C, return the chain Ci, possiblynone, to which N has unified.Algorithm 2Input: Node, Label(s), Ordered List of ChainsOutput: Chain or empty setmIf Label E {AH, AH} thenstart new chainelseelseIf Label E {AF, AF, AI} thenchoose (nearest) unsaturated chainmIf Label = AI thenchoose nearest unsaturated chain,unless it is the immediately preceding element in the stack.The list of chains given as input is ordered by the structure-building algorithm:when new chains are started, they are added at the end of the list.
The first clause;ofAlgorithm 2 starts a new chain whenever a lexical element is seen.
No other type ofchain link can start a chain.
The second clause selects a chain when the foot is seen.
Bychoosing the nearest chain (i.e., the last one in the list), only nested dependencies arebuilt.
The third clause assigns AI in a condition that is more complex than the others,to deal with subject-oriented parasitic-gaps.
15In Figure 5, I show schematically how these algorithms build chains.
A pseudo-Prolog notation is used, which is similar to the output of the parser, where chains arerepresented as lists enclosed in square brackets.
I show the I /O  of each algorithm,given the sentence Who did you think that John seemed to like?, where a multiple A-chainand an A-chain must be recovered.
NLAB takes an input word and outputs a label,while CSEL takes a triple (Node, Label, Chains) as input, and returns a new chain list.Note that, in Algorithms I and 2, features uch as Case and 0-role must be availableas input for the correct labelling and chain assignment of the empty category.
This isa crucial feature of the algorithms for chain formation proposed here.In GB theory, empty categories can be freely coindexed with an antecedent, fromwhich they inherit their features.
Features that are incompatible with a given contextare automatically excluded, since the sentence will be ungrammatical (Brody 1984).This theory is called functional determination of empty categories.
In GB parsing, therehave been two approaches to the implementation of chains: one that mirrors directly15 This restriction handles entences such as A man \[ that I whenever I meet \] looks old.\] This construction,although marginal, like all parasitic gaps, is accepted by many speakers.
Parasitic gap constructionshave many interesting properties that must be dealt with for the algorithms that treat chains to be fullygeneral.531Paola Merlo Modularity and Information Content Classesfeatures uch as case and thematic roles when building chains leads to an exponentialgrowth of the space of hypotheses; econd, I argue that using these features does notrestrict he validity of the algorithm to specific constructions or languages.4.2 Restricting the Search SpaceAs the previous section on phrase structure has shown, computing features is notalways profitable, as some features reduce the search space while others do not.
Tosee that checking features does indeed pay off, the cost of checking these features mustbe compared to the benefit of reducing the search space.This analysis mostly concerns the first algorithm, NLAB, which is constituted ofa series of binary choices.
More precisely, recall that the relevant information is: a)whether a node is lexical or not; b) whether it has a 0-role or not; c) whether it hasCase or not; d) whether it is a sister of C (hence, in an A-position) or not (if not, itcounts as an A-position).
For the chain selection algorithm (CSEL) there are four mainconstraints: first, A-nodes can only be inserted in A-chains and A-nodes can only beinserted in A-chains.
Second, empty nodes never start a new chain.
Third, the closesthead is always chosen as a potential chain to which to unify.
Finally, only unsaturatedchains are chosen.Consider what would result if NLAB did not check for all of these factors.
If (b)were not checked, NLAB ~ would not distinguish between feet and intermediate races,even in the same type of chain, thus it would output four sets of labels: AH, AH, {AF,AI}, {__AF, AI}.
If (c) were not checked, NLAB" would not distinguish between A-feetand A-feet, thus it would output AH, AH, AI, AI, {AF, AF}.
If (d) were not checked,NLAB'" would output AH, AH, {AI, AI}, AF, AF.
If (b), (c) and (d) together were notchecked, NLAB"" would output AH, AH, {AI, AI, AF, AF}.In accounting for the growth rate in the space of hypothesis of these modifiedalgorithms, two factors must be taken into consideration.
One factor is the number ofactive chain types, namely, whether a sentence presents only A-chains, only A-chains,or both.
This factor encodes the second and third restriction of the CSEL algorithm,with the consequence that not all combinations are attempted.
The second factor ac-counts for the growth rate proper, which is reducible to counting the set of k-stringsover an n-sized alphabet, hence n k. Here, k is the number of relevant links in the sen-tence (for instance, feet in NLAB'), and n is given by the size of the set of featurescollapsed by lifting some of these checks, hence, 2, 2, 2 and 4, respectively.The hypothesis pace in the three algorithms grows in slightly different ways.In NLAB ~, where there is no restriction on the number of active chains, the growthrate is n k. For NLAB" and NLAB m, the formula is NA k, where NA is the number ofactive chains.
Practically, this amounts to 2 k at most, as the number of active chainsis not more than 2, because of the restriction requiring that the nearest unsaturatedchain be selected.
For NLAB",  the restriction for active chains no longer holds.
Inthis algorithm, no features are checked, so it is impossible to establish if a chain issaturated or not until structure building ends.
Thus, the growth factor is a functionof the number of heads seen up to a certain point in the parse, the number of emptycategories, and their respective order in the input.
Notice that the different size of thecollapsed feature set, which is larger for NLAB",  is implicitly taken into account byk, as the number of relevant links varies with the size of the collapsed feature sets.
Forthe same sentence, there are more relevant links if the collapsed feature set is larger.Now, in all cases, growth is exponential in the number of relevant links, whilethe possible gain obtained by not checking features can be at most logarithmic in thenumber of potential empty categories.
Since the number of potential empty categoriesis at most 2f, for f binary features, this gain is expressed as f. Hence, suppressing533Computational Linguistics Volume 21, Number 4Table 6Growth of Hypothesis Space: S = sentence; TL = Total number of links; RL= Relevant Links;AC = Number of Active Chains; G = Growth rateS TL NLAB' NLAB" NLAB"' NLAB""RL G RL AC G RL AC G RL G3 2 1 2 1 1 1 - - 1 1 14 3 1 2 1 1 1 - - 1 1 15 3 - - 0 1 1 - - 1 - -6 2 - - 0 1 1 - - 1 - -7 4 1 2 1 1 1 - - 1 1 28 3 - - 1 2 2 - - 1 1 29 5 - - 1 2 2 1 2 2 2 610 6 1 2 2 2 4 1 2 2 3 18feature checks becomes beneficial only if kf > n k. Now notice that 2 _< n _< 2d.
Forn = 2 and f = 3, the inequality is satisfied for k < 4.
This means that for algorithmsNLAB" and NLAB "p, all sentences with more than three relevant links are computedfaster if features are checked.
For n = 4, i.e.
algorithm NLAB" ,  the inequality is neversatisfied.
17The results of some calculations are reported in Table 6.
The numbers in the "sen-tence" column refer to the type of construction, as exemplified in Figure 1 (sentencetypes 1 and 2 are not considered because they contain only trivial chains).
If one con-siders a sentence such as Who did you say that John thought hat Mary seemed to like?, withfour gaps and four heads, there are 96 hypotheses about chain formation to exploreusing NLAB" .
Clearly, checking features and using them for building chains, andkeeping the hypothesis earch space small, is beneficial in most cases.Extensibility.
These algorithms deal in detail with the somewhat neglected problemof what to do when more than one chain has to be constructed.
They do not discussspecifically the issues of adjunction or rightward movement.
However, they could beextended.In the unextended algorithm, the postulation and structural licensing of emptycategories is always performed by the same mechanism.
According to the ECP (asformulated in Rizzi 1990, 25; Cinque 1990; Chomsky 1986b, among others), for anempty category to be licensed, two conditions must be satisfied: the empty categorymust be within the maximal projection of a lexical head to be licensed structurally,and it must be identified by an antecedent.
The structural licenser and the antecedentneed not be the same element.
In fact, they hardly ever are.
Whether movement is tothe left or to the right does not affect structural licensing (which is here performed bythe conditions that apply to the reduction of an ~-rule).Rightward movement requires an extension of the algorithm to incorporate theempty category in a chain.
An empty category that is the foot of rightward move-ment must be licensed structurally, before its antecedent is seen.
When the NP thatis the antecedent (head of chain) is found, it starts a new chain, according to CSEL.Therefore, an extension is needed to check if there are any empty categories wait-17 Note that here I am assuming that checking a feature and checking a chain have the samecomputational cost, which is an approximation, asa chain cannot be checked with a single operation.534Computational Linguistics Volume 21, Number 4L-attributed rule into an S-attributed rule (Aho, Sethi and Ullman 1977, 282ff discussthe marker nonterminals technique).
Such a transformation is possible if the attributesof the tokens on the left of the current oken are at a fixed position in the stack.We can use this S-attribution transformation for Case assignment to the subject(nominative Case or structural case).
In English, structural case is assigned to thesubject position, if the subject is a sibling of (the projection of) a finite inflectionalnode.
This position can occur both in main and embedded clauses.
English is head-initial, and the Specifier precedes the head.
These properties interact, so that whenthe subject NP is reduced, INFL is always the next token in the parsing configuration.Thus, rule (8) can be used .
21(8) IP --+ NP {Case assign, if I +fin} I'This rule assigns case correctly only if the attribution is not a function of thesubconstituents of I.
This is precisely what distinguishes case assigned to the subject(structural case assignment) from other types of case assignments (e.g., case assignedto the object by either a verb or a preposition): it is assigned independently of theproperties of the main verb.The S-attribution transformation is not restricted to languages with the propertiesof English; it can also be extended to head-final anguages.
In verb-final anguages(German, for example) the subject of the sentence in embedded clauses is not stringadjacent o the head of the sentence, as it is in English.
However, structural case canbe assigned from left to right, since the complementizer, which necessarily marks theleft edge of an IP, is obligatory, and the finite complementizer is always different fromthe infinitival complementizer.S-attribution could not be performed, however, in parsing a language with all thecharacteristics given in (9).
(9) a. no overt case markingb.
no distinct finite complementizerc.
verb finald.
right branching in the projections other than the verb21 At first sight, this might appear as a wild overidealization.
I  fact, there are both theoretical ndempirical reasons to think that this is the right way to idealize the data.
A corpus analysis on 111occurrences of the verb announce  in the Penn Treebank shows that the subject is followed by anaspectual dverb 11 times, twice by incidental phrases, and 4 times by an apposition.
In all other casesthe subject and the verb are indeed adjacent.
I do not consider appositions and incidentals aschallenging for the general claim: incidentals are clearly outside of an X structure assigned to thesentence; while appositions are "internal" to the NP, thus when the verb is reached, the phrase sittingon the stack is indeed the NP subject, which can therefore receive Case.
The treatment of aspectualadverbs is more complex.
There are at least two possible tacks.
First, one can notice that adverbs,although they are analysed as maximal projections because they can be modified, never take acomplement, thus they are usually limited to a very short sequence of words, and they do not have arecursive structure.
A minimum amount of lookahead, even limited to these particular instances ofaspectual dverbs, would solve the problem.
Clearly, this is an inelegant solution.
A more principledtreatment comes from recent developments in the theory, that have changed somewhat therepresentation used for adverbs.
Laenzlinger (1993) suggests that all maximal projections have twospecifiers, one A and one A, the higher of the two is the A-position, which can be occupied by adverbs,if they are licensed by the appropriate head (the Adv-Criterion).
For these adverbs, the appropriatehead is Asp0 which we find only with finite verbs.
The parser could compile this information andassign case directly, without even waiting to see the (lexical) verb.536Paola Merlo Modularity and Information Content ClassesBecause of property (9a), case could not be inferred from explicit information containedin the input (unlike Japanese or German); because of property (9b) the subject positionof an embedded clause would not be unmistakably signalled (unlike German but likeJapanese); because of property (9c), the inflectional head would occur after the NP thatneeds to be assigned case; finally, because of property (9d), an LR parser could giveworst-case results (which is not the case for verb-final, but left-branching, languages,like Japanese): it could require the entire sentence to be stacked before starting toassemble it.Although a problem in principle, this limitation disappears in practice.
Inspec-tion of some of the sources on language typology shows that such languages arevery difficult o find (Steele 1978; Shopen 1985; Comrie 1981).
According to Downing(1978), verb-final languages usually have prenominal relative clauses, which is a signthat they are left branching.
Only two verb-final languages have postnominal relativeclauses, Persian and Turkish.
In Persian, the clause boundary is overtly marked bythe suffix -i on the antecedent.
Moreover, both languages have overt case markingof the subject.
Although this is by no means definitive vidence, it suggests that thealgorithm for chain formation and feature assignment that I have presented is notobviously inadequate, and that it is applicable to a variety of languages with differentproperties.5.
ConclusionThe parser described in this paper has been implemented for English.
It parses ahomogeneous, though small, set of sentences.
As a matter of fact, one of the interestingfeatures of this implementation is that it offers a unified treatment of all of the chaintypes presented above.The parser has clear limitations due to the fact that it was developed mainly forexploratory purposes.
For instance, it deals only with very simple nominal phrasesand it does not treat adjunction.
In other respects, however, this design lends itselfreadily to extensions: The structure building and chain formation routines do not relyon characteristics that are found only in English or in a head initial language, as wasdiscussed in the previous ection.In the course of pondering the relation between the grammar and the parser,and mostly how the conceptual modularity of current linguistic theories can be imple-mented, one learns that, in fact, the notion of modular theory is both true and false,at least in its present incarnation.
All linguists trive to develop theories that rest ongeneral, abstract principles, which interact in complex ways, so that many empiricalfacts "fall out" from a few principles.
Such a theory is clearly not modular, althoughhighly general and abstract.
On the other hand, linguistic oncepts operate on differentprimitives: intuitively, X-theory, and principles of argument structure or coreferenceare different objects.
Future research must lead in a direction that enables us to de-fine more precisely this basic intuition.
Modularity, if it exists, is to be found in thelinguistic ontent, and not in the organization of the theory.AcknowledgmentsI would like to thank those who havehelped me in this work: Michael Brent,Bonnie Dorr, Uli Frauenfelder, Paul Gorrell,Luigi Rizzi, Graham Russell, Eric Wehrli,Amy Weinberg, and two anonymousreviewers.
All remaining errors are my own.ReferencesAbney, Steven (1989).
"A ComputationalModel of Human Parsing."
Journal ofPsycholinguistic Research, 18, 129-144.Aho, Alfred V.; Sethi, Ravi; and Ullman,Jeffrey D. (1977).
Compilers: Principles,Techniques and Tools.
Addison-Wesley ?537Computational Linguistics Volume 21, Number 4Publishing Company, Reading, MA.Barton, Edward; Berwick, Robert; andRistad, Eric (1987).
ComputationalComplexity and Natural Language.
MITPress, Cambridge, MA.Berwick, Robert (1982).
"Locality Principlesand the Acquisition of SyntacticKnowledge."
Doctoral dissertation, MIT,Cambridge, MA.Berwick, Robert (1990).
"Ross was Right:Constraints on Variables."
Manuscript,MIT.Berwick, Robert (1991).
"Principle-BasedParsing."
In Foundational Issues in NaturalLanguage Processing, edited by Peter Sells,Stuart M. Shieber, and Thomas Wasow.MIT Press, 115-226.Berwick, Robert; Abney, Steven; and Tenny,Carol (1991).
Principle-Based Parsing.Kluwer Academic Publishers, Dordrecht.Berwick, Robert, and Weinberg, Amy (1984).The Grammatical Basis of LinguisticPerformance.
MIT Press, Cambridge, MA.Berwick, Robert, and Weinberg, Amy (1985).
"Deterministic Parsing and LinguisticExplanation."
Language and CognitiveProcesses, 1(2), 109-134.Bresnan, Joan (1978).
"A RealisticTransformational Grammar."
In LinguisticTheory and Psychological Reality, edited byMorris Halle, Joan Bresnan, and GeorgeMiller.
MIT Press, Cambridge, MA.Brody, Michael (1984).
"On ContextualDefinitions and the Role of Chains.
"Linguistic Inquiry, 15(3), 355-380.Chomsky, Noam (1965).
Aspects of the Theoryof Syntax.
MIT Press, Cambridge, MA.Chomsky, Noam (1986a).
Knowledge ofLanguage: Its Nature, Origin and Use.Praeger, New York, New York.Chomsky, Noam (1986b).
Barriers.
MITPress, Cambridge, MA.Chomsky, Noam (1988).
"Some Notes onEconomy of Derivation andRepresentation."
MIT Working Papers inLinguistics 10, MIT, 43-74.Chomsky, Noam (1992).
"A MinimalistProgram for Linguistic Theory.
"Occasional MIT Working Papers inLinguistics 1.
(also appeared in The Viewfrom Building 20, edited by Ken Hale andJay Keyser.
MIT Press, Cambridge, MA,1993, 1-52.
)Cinque, Guglielmo (1990).
Types of ADependencies.
MIT Press, Cambridge, MA.Comrie, Bernard (1981).
Language Universalsand Linguistic Typology.
Basil Blackwell,Oxford.Correa, Nelson (1988).
Syntactic Analysis ofEnglish with Respect to Government-BindingTheory.
Doctoral dissertation, SyracuseUniversity, Syracuse, NY.Correa, Nelson (1991).
"Empty Categories,Chain Binding and Parsing."
InPrinciple-Based Parsing, edited by RobertBerwick, Steven Abney, and Carol Tenny.Kluwer Academic Publishers, Dordrecht,83-122.Crocker, Matthew (1991).
"A Principle-basedSystem for Syntactic Analysis."
CanadianJournal of Linguistics, 36(1), 1-26.Crocker, Matthew (1992).
"A Logical Modelof Competence and Performance in theHuman Sentence Processor."
Doctoraldissertation, University of Edinburgh,Edinburgh.Crocker, Matthew (to appear).
ComputationalPsycholinguistics: An InterdisciplinaryPerspective.
Kluwer Academic Publishers,Dordrecht.Dorr, Bonnie (1987).
"UNITRAN: aPrinciple-based Approach to MachineTranslation."
AI Lab Memo 100, MIT,Cambridge, MA.Dorr, Bonnie (1993).
Machine Translation: AView from the Lexicon.
MIT Press,Cambridge, MA.Downing, Bruce T. (1978).
"Some Universalsof Relative Clause Structure."
InUniversals of Human Language, dited byJoseph H. Greenberg.
Stanford UniversityPress, 375-418.Earley, Jay (1970).
"An EfficientContext-Free Parsing Algorithm.
"Communications of the Association forComputing Machinery, 14, 453-460.Fong, Sandiway (1990).
"Free Indexation:Combinatorial Analysis and aCompositional Algorithm."
In Proceedings,28th Meeting of the ACL, Pittsburgh, PA,105-110.Fong, Sandiway (1991).
"ComputationalProperties of Principle-based GrammaticalTheories."
Doctoral dissertation, MIT,Cambridge, MA.Fong, Sandiway, and Berwick, Robert(1992).
"Isolating Cross-linguistic ParsingComplexity with a Principle-andParameters Parser: a Case Study ofJapanese and English."
In Proceedings,COLING 92, Nantes, France, 631-637.Frank, Robert (1992).
"Syntactic Locality andTree Adjoining Grammar: Grammatical,Acquisition and Processing Perspectives.
"Doctoral dissertation, University of?
Pennsylvania, Philadelphia, PA.Frazier, Lyn (1985).
"Modularity and theRepresentational Hypothesis."
InProceedings ofNELS 16, 131-146.Frazier, Lyn (1990).
"Exploring theArchitecture of the Language ProcessingSystem."
In Cognitive Models of Speech538Paola Merlo Modularity and Information Content ClassesProcessing, edited by Gerry Altmann, MITPress, 409-433.Gazdar, Gerald; Klein, Ewan; Pullum,Geoffrey; and Sag, Ivan (1985).
GeneralizedPhrase Structure Grammar.
Blackwell,Oxford.Grimshaw, Jane (1986).
"Subjacency and theS/S' Parameter."
Linguistic Inquiry.
17(2),364-369.Haegemann, Liliane, and Zanuttini,Raffaella (1991).
"Negative Heads and theNeg Criterion."
The Linguistic Review, 8(2/4), 233-251.Irons, Edgar (1961).
"A Syntax DirectedCompiler for ALGOL 60."
Communicationsof the Association for Computing Machinery,4(1) : 51-55.Johnson, Mark (1989).
"Parsing asDeduction: The Use of Knowledge ofLanguage."
Journal of PsycholinguisticResearch, 18(1), 233-251.Kashket, Michael (1991).
"A ParameterizedParser for English and Warlpiri."
Doctoraldissertation, MIT, Cambridge, MA.Kayne, Richard (1994).
"The Antisymmetryof Syntax."
Linguistic Inquiry Monograph25, MIT Press, Cambridge, MA.Knuth, Donald E. (1965).
"On theTranslation of Languages from Left toRight."
Information and Control, 8: 607-639.Knuth, Donald E. (1968).
"Semantics ofContext-free Languages."
MathematicalSystems Theory, 2: 127-145.van de Koot, Hans (1990).
Essay on theGrammar-Parser Relation.
Foris, Dordrecht.van de Koot, Hans (1991).
"Parsing withPrinciples: on Constraining Derivations.
"UCL Working Papers in Linguistics,University College, London, 369-396.Kornai, Andrais, and Pullum, Geoffrey(1990).
"The X-bar Theory of PhraseStructure."
Language, 66, 24-50.Laenzlinger, Christopher (1993).
"Principlesfor a Formal Account of Adverb Syntax.
"Geneva Generative Papers, 1(2),University of Geneva, 47-75.Marcus, Mitchell (1980).
A Theory of SyntacticRecognition for Natural Language.
MITPress, Cambridge MA.Merlo, Paola (1992).
"On Modularity andCompilation in a Government andBinding Parser."
Doctoral dissertation,University of Maryland, College Park,MD.Merlo, Paola (to appear).
Parsing withPrinciples and Classes of Information.
KluwerAcademic Publishers, Dordrecht.Phillips, John (1992).
"A ComputationalRepresentation for Generalized PhraseStructure Grammars."
Linguistics andPhilosophy, 15(3): 255-287.Phillips, John, and Thompson, Henry (1985).
"GPSGP: A Parser for Generalized PhraseStructure Grammars."
Linguistics, 23(2),245-262.Rizzi, Luigi (1982).
Issues in Italian Syntax.Foris, Dordrecht.Rizzi, Luigi (1990).
Relativized Minimality.MIT Press, Cambridge, MA.Rizzi, Luigi (1991).
"Residual Verb Secondand the Wh-Criterion," Technical Reportsin Formal and Computational Linguistics,2, University of Geneva, Geneva.Schabes, Yves (1991).
"Polynomial Time andSpace Shift-reduce Parsing of ArbitraryContext-free Grammars."
In Proceedings,29th Meeting of the ACL, Berkeley, CA,106-113.Shieber, Stuart M., and Johnson, Mark(1993).
"Variations on IncrementalInterpretations."
Journal of PsycholinguisticResearch, 22(2), 287-319.Shopen, Timothy (1985).
Language Typologyand Syntactic Description.
CambridgeUniversity Press, Cambridge, England.Sportiche, Dominique (1992).
"CliticConstructions."
Manuscript, UCLA.Stabler, Edward (1990).
"RelaxationTechniques for Principle-based Parsing.
"Manuscript, UCLA.Stabler, Edward (1991).
"Avoid thePedestrian's Paradox."
In Principle-BasedParsing, edited by Robert Berwick, StevenAbney, and Carol Tenny.
KluwerAcademic Publishers, Dordrecht, 199-238.Stabler, Edward (1992).
The Logical Approachto Syntax.
MIT Press, Cambridge, MA.Stabler, Edward (1994).
"The FiniteConnectivity of Linguistic Structure."
InPerspectives onSentence Processing, editedby Charles Clifton, Lyn Frazier, and KeithRayner.
Lawrence Erlbaum, Hillsdale, NJ,303-336.Steedman, Mark (1989).
"Grammar,Interpretation a d Processing from theLexicon."
In Lexical Representation a dProcesses, edited by WilliamMarslen-Wilson.
M1T Press, Cambridge,MA, 463-504.Steele, Susan (1978).
"Word OrderVariation."
In Universals of HumanLanguage, dited by Joseph H. Greenberg.Stanford University Press, 585-623.Thompson, Henry (1982).
"HandlingMetarules in a Parser for GPSG.
"Research Paper 175, Department ofArtificial Intelligence, University ofEdinburgh, Edinburgh, UK.Tomita, Masaru (1986).
Efficient Parsing forNatural Language.
Kluwer, Hingham, MA.539Paola Merlo Modular i ty  and  In format ion Content  Classesp2 : p l ;  / *  40 */p l  : pO; /*  41 */d2 : d l ;  / *  42 */d l  : dO; /*  43 */y2 : /*empty*/ In21c21i2lv21a21p2; /* 44-51 */A.3 Grammar 3s : i2 \] c2 ;c2 : y2 c l l  c l l  /*  empty */  ;c l  : cO i2  I cO;cO : c I / *empty* /  ;i2 : y2 il I i l  I /* empty */;il : iO v2 I iO;iO : i I /*empty*/ ;v2 : y2 vl I /*empty */Iv1 ;vl : vOint p2 ;vOint : vintl /* empty */;vl : vOt n2;vOt : vt I /*empty*/ ;vl : vOrais i2;vOrais : vrais I /*empty*/ ;vl : vOint c2;y2 : c2 I i2 I n2 I v2 I p2I /* empty */ ;n2 : n I /* empty */;p2 : pO y2 I /* empty */;pO : p I / *empty* / ;541
