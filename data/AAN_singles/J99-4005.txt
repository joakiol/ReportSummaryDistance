Squibs and DiscussionsDecoding Complexity in Word-ReplacementTranslation ModelsKev in  Knight*University of Southern CaliforniaStatistical machine translation is a relatively new approach to the long-standing problem of trans-lating human languages by computer.
Current statistical techniques uncover translation rulesfrom bilingual training texts and use those rules to translate new texts.
The general architectureis the source-channel model: an English string is statistically generated (source), then statisticallytransformed into French (channel).
In order to translate (or "decode") a French string, we lookfor the most likely English source.
We show that for the simplest form of statistical models, thisproblem is NP-complete, i.e., probably exponential in the length of the observed sentence.
Wetrace this complexity to factors not present in other decoding problems.1.
In t roduct ionStatistical models are widely used in attacking natural anguage problems.
The source-channel  framework is especially popular, finding applications in part-of-speech tag-ging, accent restoration, transliteration, speech recognition, and many other areas.
Inthis framework, we build an underspecified model of how certain structures (such asstrings) are generated and transformed.
We then instantiate the model through trainingon a database of sample structures and transformations.Recently, Brown et al (1993) built a source-channel model of translation betweenEnglish and French.
They assumed that English strings are produced according to somestochastic process (source model) and transformed stochastically into French strings(channel model).
To translate French to English, it is necessary to find an Englishsource string that is likely according to the models.
With a nod to its cryptographicantecedents, this kind of translation is called decoding.
This paper looks at decodingcomplexity.2.
Part -of -Speech TaggingThe prototype source-channel application in natural anguage is part-of-speech tagging(Church 1988).
We review it here for purposes of comparison with machine translation.Source strings comprise sequences of part-of-speech tags like noun, verb, etc.
Asimple source model assigns a probability to a tag sequence tl .. ?tm based on the prob-abilities of the tag pairs inside it.
Target strings are English sentences, e.g., wl ... win.The channel model assumes each tag is probabilistically replaced by a word (e.g., nounby dog) without considering context.
More concretely, we have:?
v total tags?
A bigram source model with v 2 parameters of the form b(t\]t), whereP(tl .
.
.
tin) "" b(tllboundary) ?
b(t2\]tl) .
.
.
.
.
b(tn\]tm-1) " b(boundary\]tm)?
Information Sciences Institute, Marina del Rey, CA 90292@ 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 4?
A substitution channel model with parameters of the form s(w\]t), whereP(wl ... Wmlh... tm) ~ S(Wllh)" S(W21t2)" ..." S(Wraltm)?
an m-word text annotated with correct ags?
an m-word unannotated textWe can assign parts-of-speech to a previously unseen word sequence wl.
.
.
Wmby finding the sequence t l .
.
.
tm that maximizes P(h. .
.
tmlWl... Wm).
By Bayes' rule,we can equivalently maximize P(h ... tm)'P(wl.., wmlh.., tin), which we can calculatedirectly from the b and s tables above.Three interesting complexity problems in the source-channel framework are:?
Can parameter values be induced from annotated text efficiently??
Can optimal decodings be produced efficiently??
Can parameter values be induced from unannotated text efficiently?The first problem is solved in O(m) time for part-of-speech tagging--we simplycount tag pairs and word/tag pairs, then normalize.
The second problem seems torequire enumerating all O(v m) potential source sequences to find the best, but canactually be solved in O(mv 2) time with dynamic programming.
We turn to the thirdproblem in the context of another application: cryptanalysis.3.
Subst i tu t ion  C iphersIn a substitution cipher, a plaintext message like HELLO WORLD is transformed intoa ciphertext message like EOPPX YXAPF via a fixed letter-substitution table.
As withtagging, we can assume an alphabet of v source tokens, a bigram source model, asubstitution channel model, and an m-token coded text.If the coded text is annotated with corresponding English, then building sourceand channel models is trivially O(m).
Comparing the situation to part-of-speech tag-ging:?
(Bad news.)
Cryptanalysts rarely get such coded/decoded text pairs andmust employ "ciphertext-only" attacks using unannotated training data.?
(Good news.)
It is easy to train a source model separately, on rawunannotated English text that is unconnected to the ciphertext.Then the problem becomes one of acquiring a channel model, i.e., a table s(fle ) withan entry for each code-letter/plaintext-letter pair.
Starting with an initially uniformtable, we can use the estimation-maximization (EM) algorithm to iteratively revises(fle ) so as to increase the probability of the observed corpus P(f).
Figure 1 shows anaive EM implementation that runs in O(mv m) time.
There is an efficient O(mv 2) EMimplementation based on dynamic programming that accomplishes the same thing.Once the s(fle ) table has been learned, there is a similar O(mv 2) algorithm for optimaldecoding.
Such methods can break English letter-substitution ciphers of moderatesize.608Knight Decoding ComplexityGiven coded text f of length m, a plaintext vocabulary of v tokens, and a source model b:1. set the s0Cle) table initially to be uniform2.
for several iterations do:a,b.C.d.set up a count table c0CI e) with zero entriesP(f) = 0for all possible source texts el.
.
.
em (el drawn from vocabulary)compute P(e) = b(ell boundary), b(boundary lem).
\[Ii~=2 b(eilei_l)m compute P(fle) = I~j=l s(fjleJ)P(f) += P(e).
P(fle)for all source texts e of length mcompute P(elf ) = P(e)'P(fle)P(f)for j = 1 to mc0~lej) += P(e~)normalize c0Ci e) table to create a revised s0CI e)Figure 1A naive application of the EM algorithm to break a substitution cipher.
It runs in O(mv m) time.4.
Machine TranslationIn our discussion of substitution ciphers, we were on relatively sure ground thechannel model we assumed in decoding is actually the same one used by the cipherwriter for encoding.
That is, we know that plaintext is converted to ciphertext, letter byletter, according to some table.
We have no such clear conception about how Englishgets converted to French, although many theories exist.
Brown et al (1993) recently castsome simple theories into a source-channel framework, using the bilingual Canadianparliament proceedings as training data.
We may assume:?
v total English words.?
A bigram source model with V 2 parameters.?
Various substitut ion/permutation channel models.?
A collection of bilingual sentence pairs (sentence lengths < m).?
A collection of monolingual French sentences (sentence lengths < m).Bilingual texts seem to exhibit English words getting substituted with French ones,though not one-for-one and not without changing their order.
These are importantdepartures from the two applications discussed earlier.In the main channel model of Brown et al (1993), each English word token eiin a source sentence is assigned a "fertility" @, which dictates how many Frenchwords it will produce.
These assignments are made stochastically according to a tablen(~le ).
Then actual French words are produced according to s(fie ) and permuted intonew positions according to a distortion table d(jli, m, 1).
Here, j and i are absolute tar-get/source word positions within a sentence, and m and I are target/source sentencelengths.Inducing n, s, and d parameter estimates i easy if we are given annotations in theform of word alignments.
An alignment is a set of connections between English andFrench words in a sentence pair.
In Brown et al (1993), aligrtrnents are asymmetr ic- -each French word is connected to exactly one English word.609Computational Linguistics Volume 25, Number 4Given a collection of sentence pairs:1. collect estimates for the ~(m\]l) table directly from the data2.
set the s0e\]e) table initially to be uniform3.
for several iterations do:a .b.C.set up a count table c(f\]e) with zero entriesfor each given sentence pair e, f with respective l ngths I, m:fo ra l= l to lfo r  a2 = 1 to 1 /* select connections for a word alignment */for am = 1 to lcompute P(al ...... \]e, f) - p(f' al ...... \]e)P(f\]e)for j = 1 to mc0~l%) += P(al... amle, f)normalize c0~\]ei ) table to create new s(fi\]ei)m 1-Ij=, s~l%)G'o; =, ' m " ?
~ , , -= ,  I - \ [ j= ,  s~le,;)Figure 2Naive EM training for the Model 1 channel model.Word-aligned data is usually not available, but large sets of unaligned bilin-gual sentence pairs do sometimes exist.
A single sentence pair will have \[m possibleal ignments--for each French word position 1. .
.
m, there is a choice of I English po-sitions to connect o.
A naive EM implementation will collect n, s, and d counts byconsidering each alignment, but this is expensive.
(By contrast, part-of-speech tagginginvolves a single alignment, leading to O(m) training).
Lacking a polynomial refor-mulation, Brown et al (1993) decided to collect counts only over a subset of likelyalignments.
To bootstrap, they required some initial idea of what alignments are rea-sonable, so they began with several iterations of a simpler channel model (calledModel 1) that has nicer computational properties.In the following description of Model 1, we represent an aligmnent formally as avector al .
.
.
.
.
am, with values aj ranging over English word positions 1. .
.
I.Model 1 ChannelParameters: c(mll ) and s(f\[e).Given a source sentence of length I:1. choose a target sentence length m according to ?
(mll )2. for j = 1 to m, choose an English word position aj according to theuniform distribution over 1. .
.
l3.
for j = 1 to m, choose a French word j~ according to s~\]%)4. read off fl ...fro as the target sentenceBecause the same e may produce the same f by means of many different align-ments, we must sum over all of them to obtain P(fle):1 l 1 l m P(fl e) = c(mll) T~ Y~al=l ~a2=l """ Y~am=l I\]j=l s(fjleai)Figure 2 illustrates naive EM training for Model 1.
If we compute P(fle) once periteration, outside the "for a" loops, then the complexity is O(ml m) per sentence pair,per iteration.610Knight Decoding ComplexityMore efficient O(lm) training was devised by Brown et al (1993).
Instead of pro-cessing each alignment separately, they modified the algorithm in Figure 2 as follows:b. for each given sentence pair e, f of respective lengths l, m:for j = 1 to msum = 0for i = 1 to Isum += s(fjlei)for i = 1 to Ic(fjlei ) += s(fjlei ) / sumThis works because of the algebraic trick that the portion of P(fle) we originally wrote1 1 m e m as ~al=," "" Y~am=l 1-Ij=l S(J~\[ aj) can be rewritten as YIj=I Y~I=I s(fjlei)"We next consider decoding.
We seek a string e that maximizes P(elf), or equiva-lently maximizes P(e) ?
P(fle).
A naive algorithm would evaluate all possible sourcestrings, whose lengths are potentially unbounded.
If we limit our search to stringsat most twice the length m of our observed French, then we have a naive O(m2v 2m)method:Given a string f of length m1.
for all source strings e of length I _ 2m:a. compute P(e) = b(el I boundary) - b(boundary Iet) " I - l l i=2 b(eilei-1)m b. compute P(fle) = c(mll ) ~ l-\[j=1 ~1i=1 s(fjlei)c. compute P(elf) ,-~ P(e) ?
P(fle)d. if P(elf ) is the best so far, remember it2.
print best eWe may now hope to find a way of reorganizing this computation, using tricks likethe ones above.
Unfortunately, we are unlikely to succeed, as we now show.
Forproof purposes, we define our optimization problem with an associated yes-no decisionproblem:Definition: M1-OPTIMIZEGiven a string f of length m and a set of parameter tables (b, e, s), return a string e oflength I < 2m that maximizes P(elf), or equivalently maximizes1 P(e) - P(fle) = b(el I boundary) -b(boundary I el ) ?
1 - \ [ i=2  b(eilei-1)?
c (ml l  ) ?
v i  m x-,!
l m l l j= l  / ' i=1  s ( f j l e i )Definition: M1-DECIDEGiven a string f of length m, a set of parameter tables (b, e, s), and a real number k,does there exist a string e of length l < 2m such that P(e) ?
P(fle) > k?We will leave the relationship between these two problems somewhat open andintuitive, noting only that M1-DECIDE's intractability does not bode well for M1-OPTIMIZE.611Computational Linguistics Volume 25, Number 4TheoremM1-DECIDE is NP-complete.To show inclusion in NP, we need only nondeterministically choose e for anyproblem instance and verify that it has the requisite P(e) ?
P(fle) in O(m 2) time.
Nextwe give separate polynomial-time r ductions from two NP-complete problems.
Eachreduction highlights a different source of complexity.4.1 Reduction 1 (from Hamilton Circuit Problem)The Hamilton Circuit Problem asks: given a directed graph G with vertices labeled0 , .
.
.
,n ,  does G have a path that visits each vertex exactly once and returns to itsstarting point?
We transform any Hamilton Circuit instance into an M1-DECIDE in-stance as follows.
First, we create a French vocabulary fl .
.
.
.
.
fn, associating word fiwith vertex i in the graph.
We create a slightly larger English vocabulary e0 .
.
.
.
.
en,with e0 serving as the "boundary" word for source model scoring.
Ultimately, we willask M1-DECIDE to decode the string f l .
.
.
fn .We create channel model tables as follows:s~.lei) = {10 i f i= jotherwise?
(mll) = {10 i f l=motherwiseThese tables ensure that any decoding e off1 ...fn will contain the n words el .
.
.
.
, en(in some order).
We now create a source model.
For every pair (i,j) such that 0 G i,j G n:= ~l /n  if graph G contains an edge from vertex i to vertex jb(ej\[ei)to otherwiseFinally, we set k to zero.
To solve a Hamilton Circuit Problem, we transform it asabove (in quadratic time), then invoke M1-DECIDE with inputs b, c, s, k, and f l .
.
.
fm.If M1-DECIDE returns yes, then there must be some string e with both P(e) andP(fle) nonzero.
The channel model lets us conclude that if P(f\[e) is nonzero, then econtains the n words e l , .
.
.
,  en in some order.
If P(e) is nonzero, then every bigram ine (including the two boundary bigrams involving e0) has nonzero probability.
Becauseeach English word in e corresponds to a unique vertex, we can use the order of wordsin e to produce an ordering of vertices in G. We append vertex 0 to the beginningand end of this list to produce a Hamilton Circuit.
The source model constructionguarantees an edge between each vertex and the next.If M1-DECIDE returns no, then we know that every string e includes at least onezero value in the computation of either P(e) or P(fle).
From any proposed HamiltonCircuit--i.e., some ordering of vertices in G- -we can construct a string e using thesame ordering.
This e will have P(f\]e) = 1 according to the channel model.
Therefore,P(e) = 0.
By the source model, this can only happen if the proposed "circuit" is actuallybroken somewhere.
So no Hamilton Circuit exists.Figure 3 illustrates the intuitive correspondence b tween selecting a good wordorder and finding a Hamilton Circuit.
We note that Brew (1992) discusses the NP-completeness of a related problem, that of finding some permutation of a string thatis acceptable to a given context-free grammar.
Both of these results deal with decisionproblems.
Returning to optimization, we recall another circuit task called the Traveling612Knight Decoding Complexitymyb?uid~N  r ~ ' /~~falls ThursdayFigure 3Selecting a good source word order is like solving the Hamilton Circuit Problem.
If we assumethat the channel model offers deterministic, word-for-word translations, then the bigramsource model takes responsibility for ordering them.
Some word pairs in the source languagemay be illegal.
In that case, finding a legal word ordering is like finding a complete circuit in agraph.
(In the graph shown above, a sample circuit is boundary --, this ---* year ~ comma ~ my--* birthday --~ falls --~ on ---* a --+ Thursday ~ boundary).
If word pairs have probabilities attachedto them, then word ordering resembles the finding the least-cost circuit, also known as theTraveling Salesman Problem.Salesman Problem.
It introduces edge costs dq and seeks a minimum-cost circuit.
Byviewing edge costs as log probabilities, we can cast the Traveling Salesman Problemas one of optimizing P(e), that is, of finding the best source word order in Model 1decoding.4.2 Reduction 2 (from Minimum Set Cover Problem)The Minimum Set Cover Problem asks: given a collection C of subsets of finite set S,and integer n, does C contain a cover for S of size ~ n, i.e., a subcollection whoseunion is S?
We now transform any instance of Minimum Set Cover into an instanceof M1-DECIDE, using polynomial time.
This time, we assume a rather neutral sourcemodel in which all strings of a given length are equally likely, but we construct a morecomplex channel.We first create a source word ei for each subset in C, and let gi be the size ofthat subset.
We create a table b(ei lej)  with values set uniformly to the reciprocal of thesource vocabulary size (i.e., the number of subsets in C).Assuming S has m elements, we next create target words fl .
.
.
.
.
fm correspondingto each of those elements, and set up channel model tables as follows:if the element in S corresponding toj~ is also in the subsetcorresponding to eiotherwise?
(mll) = {10 i f l~notherwisef l  i f  l>n  ~(m otherwiseFinally, we set k to zero.
This completes the reduction.
To solve an instance ofMinimum Set Cover in polynomial time, we transform it as above, then call M1-DECIDE with inputs b, c, s, k, and the words fl .
.
.
.
.
fm in any order.613Computational Linguistics Volume 25, Number 4obtainedm~however)tedJ .
.
.
.
.
~d left left the mealFigure 4Selecting a concise set of source words is like solving the Minimum Set Cover Problem.
Achannel model with overlapping, one-to-many dictionary entries will typically license manydecodings.
The source model may prefer short decodings over long ones.
Searching for adecoding of length _< n is difficult, resembling the problem of covering a finite set with a smallcollection of subsets.
In the example shown above, the smallest acceptable set of source wordsis {and, cooked, however, left, comma, period}.If M1-DECIDE returns yes, then some decoding e with P(e) ?
P(f\]e) > 0 must  exist.We know that e must  contain n or fewer words- -otherwise P(f\[e) = 0 by the c table.Furthermore, the s table tells us that every word fj is covered by at least one Englishword in e. Through the one-to-one correspondence b tween elements of e and C, weproduce a set cover of size G n for S.Likewise, if M1-DECIDE returns no, then all decodings have P(e) ?
P(f\[e) = 0.Because there are no zeroes in the source table b, every e has P(f\[e) = 0.
Thereforeeither (1) the length of e exceeds n, or (2) somef j  is left tmcovered by the words in e.Because source words cover target words in exactly the same fashion as elements of Ccover S, we conclude that there is no set cover of size < n for S. Figure 4 illustrates theintuitive correspondence b tween source word selection and min imum set covering.5.
D iscuss ionThe two proofs point up separate factors in MT decoding complexity.
One is word-order selection.
But even if any word order will do, there is still the problem of pickinga concise decoding in the face of overlapping bil ingual dictionary entries.
The formeris more closely tied to the source model, and the latter to the channel model,  thoughthe complexity arises from the interaction of the two.We should note that Model 1 is an intentionally simple translation model,  onewhose pr imary purpose in machine translation has been to allow bootstrapping intomore complex translation models (e.g., IBM Models 2-5).
It is easy to show that theintractability results also apply to stronger "fert i l i ty/distortion" models; we assignzero probabil ity to fertilities other than 1, and we set up uniform distortion tables.Simple translation models like Model 1 find more direct use in other applications(e.g., lexicon construction, idiom detection, psychological norms, and cross-languageinformation retrieval), so their computational properties are of wider interest.614Knight Decoding ComplexityThe proofs we presented are based on a worst-case analysis.
Real s, e, and b ta-bles may have properties that permit faster optimal decoding than the artificial tablesconstructed above.
It is also possible to devise approximation algorithms like those de-vised for other NP-complete problems.
To the extent hat word ordering is like solvingthe Traveling Salesman Problem, it is encouraging substantial progress continues to bemade on Traveling Salesman algorithms.
For example, it is often possible to get withintwo percent of the optimal tour in practice, and some researchers have demonstratedan optimal tour of over 13,000 U.S. cities.
(The latter experiment relied on things likedistance symmetry and the triangle inequality constraint, however, which do not holdin word ordering.)
So far, statistical translation research as either opted for heuristicbeam-search algorithms or different channel models.
For example, some researchersavoid bag generation by preprocessing bilingual texts to remove word-order differ-ences, while others adopt channels that eliminate syntactically unlikely alignments.Finally, expensive decoding also suggests expensive training from unannotated(monolingual) texts, which presents a challenging bottleneck for extending statisticalmachine translation to language pairs and domains where large bilingual corpora donot exist.ReferencesBrew, Chris.
1992.
Letting the cat out of thebag: Generation for shake-and-bake MT.In Proceedings ofthe 14th InternationalConference on Computational Linguistics(COLING), pages 610-616, Nantes, France,August.Brown, Peter, Stephen Della-Pietra, VincentDella-Pietra, and Robert Mercer.
1993.
Themathematics of statistical machinetranslation: Parameter estimation.Computational Linguistics, 19(2):263-311.Church, Kenneth.
1988.
A stochastic partsprogram and noun phrase parser forunrestricted text.
In Proceedings ofthe 2ndConference on Applied Natural LanguageProcessing, pages 136-143, Austin, TX,June.615
