Real-Time Stochastic Language Generation for Dialogue SystemsNathanael ChambersFlorida Institute for Human and Machine Cognition40 South Alcaniz StreetPensacola, FL 32502nchambers@ihmc.usAbstractThis paper describes Acorn, a sentence planner andsurface realizer for dialogue systems.
Improve-ments to previous stochastic word-forest based ap-proaches are described, countering recent criticismof this class of algorithms for their slow speed.An evaluation of the approach with semantic in-put shows runtimes of a fraction of a second andpresents results that suggest it is also portableacross domains.1 IntroductionThis paper describes Acorn, a real-time sentence planner andsurface realizer for dialogue systems that is independent ofa specific domain.
Acorn is based on a two-phased gram-mar and stochastic approach, such as the HALogen system[Langkilde-Geary, 2002], but offers several improvements tomake it more realistic for dialogue use.
The first is to offeran algorithm for trickle-down features that passes head/footfeatures through the grammar as the initial word forest is cre-ated, allowing the grammar to broadly represent phenomenasuch as wh-movement.
The second is to more tightly link thegrammar to a lexicon and represent syntactic properties suchas number, person, and tense to constrain the over-generationprocess.
Lastly, efficiency improvements are described whichfurther decrease the runtime of the system, allowing Acorn tobe used in a real-time dialogue context.
It is named Acorn,based on the word forests that are created and searched.The task of Natural Language Generation is frequentlysplit into three somewhat disjoint steps: document planning,microplanning (reference and sentence planning) and surfacerealization.
Document planning is a more reduced task in di-alogue, mainly involving content determination since there isno need for a document.
Since the system follows a notionof discourse, content determination is typically performed bysome reasoner external to generation, such as a Task Man-ager.
This paper addresses the sentence planning and sur-face realization steps, assuming that content determinationand referential generation has already occurred and is rep-resented in a high-level semantics.This stochastic approach involves two phases; the first usesa grammar to over-generate the possible realizations of aninput form into a word forest, and the second uses a lan-guage model to choose the preferred path through the forest.This approach is attractive to dialogue systems because it of-fers flexibility and adaptivity that cannot be achieved throughmost symbolic systems.
By over-generating possible utter-ances, the (sometimes dynamic) language models can decidewhich is more natural in the current context.
Other advan-tages include domain independence and an under-specifiedinput.
The main disadvantages most often cited include avery slow runtime and the inability to capture complex lin-guistic constraints, such as wh-movement.
The latter is a sideeffect of the word-forest creation algorithm and a solution tobroaden the coverage of language is presented in this paper.The issue of runtime is critical to dialogue.Slow runtime is a two-fold problem: the word-forest thatis generated is extremely large and often not linguisticallyconstrained, and second, the algorithm has not been effi-ciently implemented.
These issues must be addressed be-fore stochastic approaches can be suited for dialogue.
Langk-ilde [Langkilde, 2000] provides an evaluation of coverage ofHALogen and shows runtimes around 28 seconds for sen-tences with average lengths of 22 words.
Callaway [Call-away, 2003] later commented on the runtime that HALogen isanywhere from 6.5 to 16 times slower than the symbolic real-izer FUF/SURGE (which may also be too slow for dialogue).This paper shows that more work can be done in stochasticgeneration to reduce the runtime by constraining the gram-mar and making simple algorithm improvements.
Runtimesof only a fraction of one second are presented.The next section provides a brief background on stochas-tic generation, followed by a description of Acorn in section3.
The description presents several new grammar additions tobroaden language coverage, including a mechanism, calledtrickle-down features, for representing head and foot featuresin the grammar.
Section 4 describes the evaluation of Acorn,as well as the results concerning domain independence andthe overall runtime.
A brief discussion and related work fol-lows the evaluation.2 BackgroundThe task of Content Determination is typically relegated to amodule outside of the Generation component, such as witha Task Manager or other reasoning components.
This leavesthe tasks of Sentence Planning and Surface Realization as themain steps in dialogue generation, and this paper is describinga module that performs both.
The task of referential genera-tion is not addressed, and it is assumed that each logical inputis a single utterance, thus removing the need for multiple sen-tence generation.Traditionally, surface realization has been performedthrough templates or more complex syntactic grammars, suchas the FUF/SURGE system [Elhadad and Robin, 1996].Template-based approaches produce inflexible output thatmust be changed in every new domain to which the system isported.
Symbolic approaches produce linguistically correctutterances, but require a syntactic input and typically haveruntimes that are impractical for dialogue.
Requiring wordchoice to be finished beforehand, including most syntactic de-cisions, puts a heavy burden on dialogue system designers.Stochastic approaches have recently provided a newmethod of reducing the need for syntactic input and produceflexible generation in dialogue.
HALogen [Langkilde-Geary,2002] was one of the first stochastic generation systems, pro-viding a two-phased approach that allowed the system de-signer to use an under-specified input.
The first phase usesa hand written grammar that over-generates possible wordorderings into a word forest.
The second phase uses an n-gram language model to choose the highest probability paththrough the forest, returning this path as the generated sen-tence.
This approach was first used in a dialogue system in[Chambers and Allen, 2004] as an attempt to create a domainindependent surface realizer.
A human evaluation showed aslight decline in naturalness when moved to a new domain.The stochastic approach was shown in [Langkilde, 2000] toproduce good coverage of the Penn Treebank, but its runtimewas significantly slow and others have suggested the stochas-tic approach is not feasible for dialogue.3 Acorn: System Description3.1 Input FormThe input to Acorn is a semantic feature-value form rooted onthe type of speech act.
On the top level, the :speechact featuregives the type (i.e.
sa tell, sa yn-question, sa accept, etc.
),the :terms feature gives the list of semantic, content bear-ing terms, and the :root feature gives the variable of the rootterm in the utterance.
Other features are allowed and oftenrequired, such as a :focus for wh-questions.
Each term in the:terms list is a feature-value structure based on thematic roles,as used in many other representations (e.g.
Verbnet [Kipperet al, 2000]).
This utterance input is a syntactically modifiedversion of the domain independent Logical Form described in[Dzikovska et al, 2003].Each term is specified by the features: :indicator, :class,optional :lex, and any other relevant thematic roles (e.g.
:agent, :theme, etc.).
The :indicator indicates the type orfunction of the term and takes the values THE, A, F, PRO, andQUANTITY-TERM.
THE represents a grounded object in thediscourse, A represents an abstract object, F is a functionaloperator, PRO is used for references, and QUANTITY-TERMrepresents quantities expressed in various scales.
There areother indicators, but the details are beyond the scope of thispaper.
The :class specifies the semantic class of the term, and<UTTERANCE> ::=(utt :speechact <act> :root <variable><FEATURE-VALUE>*:terms (<TERM>*))<TERM> ::=(<variable> :indicator <indicator>:class <class> <FEATURE-VALUE>*)<FEATURE-VALUE> ::=<keyword> <value>Figure 1: BNF for the input to Acorn.
A keyword is a symbolpreceded by a colon, and a value is any valid symbol, variable,or list.
(utt :speechact sa tell :root v8069 :terms((v8324 :indicator speechact :class sa tell :content v8069)(v8069 :indicator f :class want :lex want :theme v8173:experiencer v7970 :tense present)(v7970 :indicator pro :class person :lex i :context-rel i)(v8173 :indicator a :class computer :lex computer:assoc-with v8161)(v8161 :indicator quantity-term :class speed-unit:lex gigahertz :quantity v8225)(v8225 :indicator quantity-term :class number :value 2.4)))Figure 2: Input to Acorn for the utterance, ?I want a 2.4 ghzcomputer.?
This input provides the lexical items for the utter-ance, but these are typically absent in most cases.the :lex is the root lexical item for the term.
Lex is an optionalfeature and is created from the :class if it is not present in theinput.
Figure 1 gives the specification of the input, and fig-ure 2 shows an example input to Acorn for the utterance, ?Iwant a 2.4 gigahertz computer?.
Appendix A provides furtherexamples of both semantic and lexical inputs.3.2 Grammar RulesThe grammar rules in Acorn convert the input utterance intoword orderings by matching keywords (features) in eachterm.
A unique aspect of Acorn is that the utterance levelfeatures can also be matched at any time.
It is often neces-sary to write a rule based on the current speech act type.
Theleft-hand side (LHS) of a rule showing both options is givenhere:(grule focus(:subject ?s):g (:speechact ?act sa_tell)>>...)Each rule matches keywords in its LHS to the current termand binds the values of the keywords in the term to the vari-ables in the LHS.
In the above example, the variable ?s wouldbe bound to the subject of the term, and the variable ?act isbound to the top-level :speechact value.
A LHS element thatis preceded by the :g symbol indicates a top-level (global)feature.
In this example, the value sa tell is also specified asa requirement before the rule can match.When matched, the right-hand side (RHS) offers severaldifferent options of processing.
As in HALogen, the recast-ing (changing a keyword to a new keyword, such as convert-ing a semantic role into a syntactic one), substitution (remov-ing a keyword and its value, or just changing its value), andordering rules (specifying phrasal and word-level ordering inthe word forest) are supported.
Two additional rules are sup-ported in Acorn that are able to handle wh-movement andother head features.
The first is called empty-creation and itscomplement is filling.
In order to effectively use these rules,a method of passing head and/or foot features is needed.
Thefollowing describes trickle-down features, followed by a de-scription of the empty-creation and filling rules.Trickle-Down FeaturesA drawback of the grammar phase is that all features in theterms must be explicitly coded in the rules, otherwise they arediscarded when ordering rules are applied.
Using a simpleexample of subject-object placement, the following orderingrule places the subject in front of the verb, and the objectbehind.
(grule subject(:subject ?s)(:object ?o)>>(-> (?s) (?rest) (?o)))Three new branches are created in the forest, one each for(?s), (?rest), and (?o).
This rule creates a branch in the wordforest that is a conjunct of three non-terminal nodes:N3 -> N4 N5 N6Processing of the (?s) and (?o) branches is restarted at the topof the grammar, but they do not contain any features (the ?restvariable is a catch-all variable that represents all features notmatched in the LHS).
Indeed, it is possible to write rules witha list of optional features, and include them in the RHS:(grule subject(:subject ?s)(:object ?o)&keep &optional(:gap ?g)>>(-> (?s :gap ?g) (?rest) (?o :gap ?g)))However, this quickly leads to bloated rules and can slowthe matching procedure considerably.
It is very intuitive tokeep features like head and foot features hidden from thegrammar writer as much as possible.
This is accomplishedthrough what we are calling trickle-down features.
The syn-tax for these special case features includes an asterisk beforethe name, as in :*gap.
The result of using these features is toget the effect of the latter rule with the ease of use in the for-mer rule.
It essentially trickles down the features until theirappropriate place in the input utterance is found.
Figure 3shows the feature ?searching?
for its correct path.
One use ofthis is shown in the following examples of the empty-creationand filling rules.G4*gap G4*gap G4*gap G4*gap G4"now""did" "buy""they" "what"*gap G4*gap G4*gap G4123 45 6 87Figure 3: A graphical representation of trickle down features.The gap head feature can be seen percolating to each node,finding its true path (1->2->6) to the wh-term what, and link-ing the filler with the gap (6->G4).Empty-CreationWhen building the word forest, we often need to create agap node that will be filled later by movement phenomena,such as in wh-questions.
The content of the node may notbe known, but through empty-creation, we can instantiate avariable and link it to the current location in the word forest.This variable can then be attached to a special trickle-downfeature which is implicitly passed through the grammar.
Thefollowing is an example of an empty-creation rule:(grule wh-question(:root ?r +) ;; root term:g (:speechact ?s sa_wh-question)((gentemp "?GAP") ?wh-gap)>>(g-> ?wh-gap)(-> ?wh-gap (?rest :*gap ?wh-gap)))The first half of the RHS (the g-> rule) creates a globalvariable and binds a new word forest node label to it.
Thislabel is then used in the second half of the RHS where thenode is inserted into the word forest, and as of now, is empty.The variable is then passed as a trickle-down feature :*gap tothe current term using the ?rest catch-all variable.
This ruleis applied to node 1 in figure 3, creating gap node G4 and the?rest node 2, passing the :*gap through the forest.FillingFilling rules perform the wh-movement needed in wh-questions and many complement structures.
Filling in thecontext of Acorn can be seen as binding a gap variable thathas already been created through an empty-creation rule.
Thefollowing is an example filling rule that completes the abovewh-gap example.
(grule gap-fill(:indicator ?i wh-term)(:*gap ?gap)>>(b-> ?gap (?rest)))This rule checks that the current term is a wh-term that hasa gap head feature.
The RHS (the b-> rule) binds the currentterm to the gap that has already been created, filling the emptynode in the word forest.
The Filling rule essentially grafts abranch onto a location in the word forest that has previouslybeen created by an Empty-Creation rule.
The dotted line infigure 3 is created by such a Filling rule.3.3 Grammar Over-GenerationOne of the main attractions of the two-phased approach isthat the grammar in the first phase can be left linguisticallyunconstrained and over-generates many possibilities for aninput utterance.
However, the statistical second phase maythen be over-burdened with the task of searching it.
The con-verse problem arises when the first stage is too constrainedand does not produce enough realizations to be natural andflexible, perhaps removing the need for a stochastic phase en-tirely.
There needs to be a balance between the two stages.The processing time is also critical in that over-generationcan take too much time to be useful for dialogue.The grammar used in HALogen largely relied on the over-generation first phase to ensure full coverage of the output.It also reduced the number of rules in the grammar.
Subject-verb agreement was loosely enforced, particularly with sub-ject number.
Also, singular and plural nouns were both gener-ated when the input was unspecified, doubling the size of thenoun phrase possibilities.
One of the biggest over-generationswas in morphology.
HALogen has its own morphology gen-erator that relies on over-generating algorithms rather thana lexicon to morph words.
The typical word forest thencontains many unknown words that are ignored during thestochastic search, but which explode the size of the word for-est.
Lastly, modifiers are over-generated to appear both infront of and behind the head words.Our approach removes the above over-generation and linksa lexicon to the grammar for morphology.
Subject-verbagreement is enforced where possible without dramaticallyincreasing the grammar size, nouns are only made pluralwhen the input specifies so (under the assumption that theinput would contain such semantically critical information),and modifiers are placed in specific locations on certainphrases (i.e.
adjectives are always premodifiers for nouns,complements of infinitive verbs are postmodifiers, etc.
).These changes greatly reduce the runtime of the first phaseand directly affect the runtime of the second phase by creatingsmaller word forests.3.4 AlgorithmForest CreationWord forest creation begins with the input utterance, such asthe one in figure 2.
The top level utterance features are storedin a global feature list, easily accessed by the grammar rulesif need be.
The :root feature points to the root semantic termgiven in the list of :terms.
This root term is then processed,beginning at the top of the grammar.The grammar is pre-processed and each rule is indexed ina hash table of features according to the least popular featurein the rule.
For example, if a rule has two features, :themeand :agent, and :agent only appears in 8 rules while :theme(v87 :indicator f :class purchase:subject v91 :object v93)(v91 ... :position subject) (v87 :indicator f :class purchase) (v93 ... :position object)"she" "bought" "computer""a"Figure 4: A word forest created from the Acorn grammar.appears in 14, the rule will be added to the list of rules inthe :agent bin.
During processing of an input term, all ofthe term?s features are extracted and the rules under each fea-ture in the hash table are merged into an ordered subset ofthe full grammar.
This process differs from HALogen andits successors by vastly limiting the number of rules that arechecked against each input.
Instead of checking 250 rules, wemay only check the relevant 20 rules.
After a grammar rulematches, the index is queried again with the new term(s) fromthe RHS of the rule.
A new subset of the grammar is createdand used to continue processing through the grammar.RHS expansions create (1) ordering constraints, (2) newbranches, and (3) feature modifications to the current term.Options (1) and (2) are typically done with ordering rulessuch as the following RHS:(-> (?s :position subject)(?rest) (?o :position object))The variables are either bound from the LHS conditions, orare unbound (conditions that follow the &optional indicatorin the LHS) and ignored during RHS expansion.
The ?restvariable is a special case variable which refers to the currentterm and its features that do not appear in the LHS (by default,features in the LHS that are matched are removed from theterm, unless they follow a &keep indicator).
In the aboveexample, there will be a new conjunction branch with threechild nodes in the word forest, as shown in figure 4.When this rule is matched, the ?s node will bind its vari-able that must point to one of the terms in the input utterance?s:terms list.
Processing will now begin with that term, attach-ing any features in the RHS to it (in this example, :positionsubject), at the top of the grammar.
Once completed, pro-cessing will continue with the current term (?rest) until thegrammar is exhausted.
Finally, the third term (?o ...) will be-gin at the top of the grammar.
As discussed in section 3.2,any trickle-down features in the current term are appended tothe three terms when processing begins/continues on each ofthem.A term attempts to match each rule in the grammar untila RHS creates a leaf node.
This is accomplished by a RHSexpansion into an initial atom that is a string.
Finally, inlinefunctions are allowed to be used in the grammar.
The follow-ing example calls the function stringify and its returned valueis bound to the ?str variable.
These calls are typically used toaccess the lexicon.
(grule stringify(:lex ?lex);; convert lexical item to string((stringify ?lex) ?str)&optional(:cat ?cat)>>(-> (?str :cat ?cat)))PathFinderThe PathFinder module of Acorn is the second stage, respon-sible for determining the most likely path through the forest.In this stage, the hypotheses from the grammar are analyzedand the top word ordering is chosen based on n-gram stochas-tic models derived from corpora.The algorithm we implemented in PathFinder is largely thesame as the one described in [Langkilde, 2000].
It is a dy-namic programming algorithm that stores the top m phrasesat each decision point based on the leading and trailing wordsin the phrase.
When dealing with n-grams, we only needto keep track of the first n ?
1 and the last n ?
1 words ineach phrase.
Our approach not only tracks these features asLangkilde calls them, but PathFinder also sorts the top mphrases and prunes any duplicates.
Pruning duplicates of-fers an advantage in runtime when the phrases are mergedwith neighboring phrases.
The complexity analysis is stillO(m?m) = O(m2), but in practice, pruning phrases reducesthe number of phrases to some number less than m.The largest change to the algorithm is that we added dy-namic interpolation of language models.
PathFinder can loadany number of models and interpolate them together duringn-gram analysis using an input set of weights.
PathFinderalso has the capability to use feature-based models and wordhistory models.Feature models, such as part of speech n-grams, model thefeatures1 of forest leaves instead of the lexical items.
TheForest Creation stage is able to output features in addition tolexical items, as seen in the RHS of this forest leaf:N6 :POS NN :BASE COMPUTER -> "COMPUTERS"There are two ?features?
on this leaf, pos and base.
Parame-ters can be passed to PathFinder that command it to use thefeatures instead of the RHS string when applying a languagemodel to the forest.
This option is not evaluated in this paper,but is a promising option for future work.Word history models keep track of the current discourseand monitor word usage, providing a history of word choiceand calculating a unigram probability for each word.
ThePathFinder is updated on each utterance in the dialogue andapplies a decaying word history approach, similar to the workin [Clarkson and Robinson, 1997].
This model is not evalu-ated in this paper, but is useful in portraying the breadth ofcoverage that a stochastic phase can provide to dialogue.1Here we refer to features in the grammar phase, as in feature-values.
These are not to be confused with the features of Langkildein the forest search phase.
:action :co-theme :property:addressee :cognizer :purpose:affected :compared-to :rank:agent :cost :result:along-with :effect :sit-val:associated :entity :state:attribute :event-relative :theme:beneficiary :experiencer :time-duration-rel:cause :of :value:center :patientFigure 5: The main semantic features in Acorn?s grammar.4 EvaluationThe three factors that are most important in evaluating dia-logue generation is portability, coverage, and speed.
Otherfactors include naturalness, flexibility, and many more, butthe above three are evaluated in this paper to address con-cerns of domain independent generation and real-time dia-logue.
During one?s efforts to address the latter concern byconstraining the size of the word forest, it is very easy to losethe former.4.1 The GrammarAcorn?s grammar contains 189 rules and is heavily seman-tic based, although the semantic features and concepts aretransformed into syntactic features before word ordering isdecided.
It is possible to input a syntactic utterance, but thisevaluation is only concerned with semantic input.
The gram-mar was created within the context of a computer purchasingdomain in which the dialogue system is a collaborative assis-tant that helps the user define and purchase a computer.
Wehad a corpus of 216 utterances from developers of the systemwho created their own mock dialogues.
The grammar wasconstructed mainly based on these parsed utterances.
Otherdomains such as an underwater robotic mine search and adatabase query interface were used to represent as many se-mantic roles as possible.
The list of the main semantic fea-tures in Acorn?s grammar is provided in figure 5.4.2 Evaluation MethodologyEach utterance that was able to be parsed in our target dia-logues was automatically transformed into the input syntax ofAcorn.
These inputs were pushed through Acorn, resulting ina single, top ranked utterance.
This utterance was comparedto the target utterance using the Generation String Accuracymetric.
This metric compares a target string to the generatedstring and counts the number of word movements (M), sub-stitutions (S), deletions (D), and insertions (I) (not countingdeletions and insertions implicitly included in movements).The metric is given below (L is the number of tokens in thetarget string):1?M + I +D + SL(1)Before comparison, all contractions were split into singlelexical items to prevent the metric from penalizing seman-tically similar phrases (e.g.
aren?t to are not).
The SimpleUtterance Lengthsnumber of words 1-2 3-5 6-9 10-Number of utterances 661 177 109 39Figure 6: Number of utterances of each word length.
The ma-jority are grounding/acknowledgements (661 utterances outof 986).
We only evaluated those of length 3 or more, 325utterances.String Accuracy metric was also applied to provide compar-ison against studies that may not use the Generation Metric;however, the Generation Metric intuitively repairs some ofthe former?s failings, namely double penalization for wordmovement.
More on these and other metrics can be found in[Bangalore et al, 2000].4.3 Domain Independent EvaluationAcorn was evaluated using the Monroe Corpus [Stent, 2000],a collection of 20 dialogues.
Each dialogue is a conversationbetween two English speakers who were given a map of Mon-roe County, NY and a description of a task that needed to besolved.
There were eight different disaster scenarios rangingfrom a bomb attack to a broken leg, and the participants wereto act as emergency dispatchers.
It is a significantly differentdomain from computer purchasing and was chosen becauseit offers a corpus that has been parsed by our parser and thushas readily available logical forms for input to Acorn.
Thelength of utterances are shown in figure 6.The four dialogues that had most recently been updatedto our logical form definitions were chosen for the evalua-tion.
The remaining sixteen are used by PathFinder as a bi-gram language model of the domain?s dialogue.
Two seriesof tests were run.
The first includes the lexical items as inputto Acorn and the second only includes the ontology concepts.Generation String Accuracy is used to judge the output ofthe system against the original utterances in the Monroe dia-logues.
While there have been other generation metrics thathave been proposed, such as the Bleu Metric [Papineni et al,2001], the Generation String Accuracy metric still providesa measure of system improvement and a comparison againstother systems.
Bleu requires more than one correct outputoption to be of worthwhile (?quantity leads to quality?
), so isnot as applicable with only one target utterance.4.4 Domain Specific EvaluationIn order to compare the domain independent evaluation witha domain specific evaluation, the same evaluation describedin 4.2 was used on the computer purchasing corpus that in-cludes the logical forms on which Acorn?s grammar is based.As described in 4.1, the domain is an assistant that collabora-tively purchases computers online for the user.
There are 132utterances of length three or more in this corpus.
The n-grammodels were automatically generated using a hand formedword grammar of sample sentences.
Both Simple and Gen-eration String Accuracy were used to compare the output ofAcorn to the target utterances in the corpus.Domain Independent: Monroe RescueSimple String AccuracyBaseline Random Path FinalLexical Items 0.28 0.55 0.67Semantic Concepts N/A 0.38 0.59Generation String AccuracyBaseline Random Path FinalLexical Items 0.28 0.59 0.70Semantic Concepts N/A 0.40 0.62Figure 7: The Simple and Generation String Accuracy resultsof Acorn in the Monroe domain.
The two baseline metricsand the final Acorn scores are given.4.5 BaselinesTwo baselines were included in the evaluation as compara-tive measures.
The first is named simply, baseline, and is arandom ordering of the lexical inputs to Acorn.
Instead ofusing a grammar to choose the ordering of the input lexicalitems, the baseline is a simple procedure which traverses theinput terms, outputting each lexical item as it comes acrossthem.
When there are multiple modifiers on a term, the orderof which to follow first is randomly chosen.
This baseline isonly run when lexical items are provided in the input.The second baseline is called Random Path and serves asa baseline before the second phase of Acorn.
A random paththrough the resulting word forest of the first phase of Acornis extracted and compared against the target utterance.
Thisallows us to evaluate the usefulness of the second stochas-tic phase.
Both these baselines are included in the followingresults.4.6 ResultsTwo different tests were performed.
The first included lexicalchoice in the input utterances and the second included onlythe ontology concepts.
The accuracy scores for the Monroedomain are shown in figure 7.
A semantic input with all lex-ical items specified scored an average of 0.70 (or 70%) on325 input utterances.
A purely semantic input with just theontology classes scored 0.62 (or 62%).The results from Acorn in the Computer Purchasing Do-main are shown in figure 8.
Both the semantic and lexicalevaluations were run, resulting in an average score of 0.85(85%) and 0.69 (69%) respectively.In order to judge usefulness for a real-time dialogue sys-tem, the runtime for both phases of Acorn was recorded foreach utterance.
We also ran HALogen for comparison.
Sinceits grammar is significantly different from Acorn?s, the out-put from HALogen is not relevant since little time was spentin conforming its grammar to our logical form; however, theruntimes are useful for comparison.
The times for both Acornand HALogen are shown in figure 9.
With a purely seman-tic input, Acorn took 0.16 seconds to build a forest and 0.21seconds to rank it for a total time of 0.37 seconds.
HALogentook a total time of 19.29 seconds.
HALogen runs quickerwhen lexical choice is performed ahead of time, finishing inDomain Specific: Computer PurchasingSimple String AccuracyBaseline Random Path FinalLexical Items 0.28 0.66 0.82Semantic Concepts N/A 0.47 0.67Generation String AccuracyBaseline Random Path FinalLexical Items 0.28 0.69 0.85Semantic Concepts N/A 0.49 0.69Figure 8: The Simple and Generation String Accuracy resultsof Acorn in the Monroe domain.
The two baseline metricsand the final Acorn scores are given.System RuntimeBuild Rank Total RuntimeAcorn Lexical 0.06s 0.00s 0.06sAcorn Semantic 0.16s 0.21s 0.37sHALogen Lexical 2.26s 0.47s 2.73sHALogen Semantic 11.51s 7.78s 19.29sFigure 9: A comparison of runtimes (in seconds) betweenAcorn and HALogen.
Both the lexical item and the semanticconcept input are shown.2.73 seconds.
The reason is mainly due to its over-generationof noun plurals, verb person and number, and morphology.Finally, the runtime improvement of using the grammarrule indexing algorithm was analyzed.
All utterances of wordlength five or more with correct parses were chosen from thedialogues to create forests of sufficient size, resulting in 192tests.
Figure 10 shows the average forest building time withthe indexing algorithm versus the old approach of checkingeach grammar rule individually.
A 30% improvement wasachieved.5 DiscussionWhile it is difficult to quantify, the implementation of trickle-down features and Empty-Creation and Filling rules accom-modate well the construction of a grammar that can capturehead/foot features.
The forest creation algorithm of HALo-gen and others is much too cumbersome to implement within,and representing lexical movement is impossible without it.The above result of 62% coverage in a new domain is com-Build Forest RuntimeNormal Grammar 0.30sIndexed Grammar 0.21s% Improvement 30%Figure 10: Runtimes of a sequential grammar rule search formatching rules versus the rule indexing approach describedin this paper.
The average runtime for 192 word forests isshown.parable, and arguably better than those given in Langkilde[Langkilde-Geary, 2002].
This paper uses a semantic ut-terance input which is most similar to the Min spec test ofLangkilde.
The Min spec actually included both the lexicalchoice and the surface syntactic roles (such as logical-subject,instead of theme or agent), resulting in a Simple String Ac-curacy of 55.3%.
Acorn?s input is even more abstract by onlyincluding the semantic roles.
Its lexical input, most similarto the Min spec, but still more abstract with thematic roles,received 70%.
This comparison should only be taken at facevalue since dialogue utterances are shorter than the WSJ, butit provides assurance that a constrained grammar can producegood output even with a more abstract input.
It must also benoted that the String Accuracy approaches do not take into ac-count synonyms and paraphrases that are semantically equiv-alent.These results also evaluate the amount of effect thestochastic phase of this approach has on the overall results.Figure 7 shows that the average random path through theword forest (the result of the first grammar-based phase) wasonly 0.40 (40%).
After PathFinder chooses the most prob-able path, the average is 0.62 (62%).
We can conclude thatthe grammar is still over-generating possible realizations andthat this approach does require the second stochastic phase tochoose a realization based on previously seen corpora.The difference between the results in the known domain(computer purchasing) and the new domain (monroe rescue)is 85% to 70% (69% to 62% without lexical items).
Whilethe difference is too great to claim domain independence ona semantic input, one of the main advantages of the over-generation grammar is that it requires less work to construct anew grammar when domains are switched.
Here we see 70%achieved for zero invested time.
A study that analyzes thetime it takes a programmer to reach 85% has yet to be done.The runtime improvement of our approach is more drasticthan originally thought possible.
An average runtime of 0.37seconds is decidedly within the time constraints of an effec-tive dialogue system.
While the 30% improvement in gram-mar indexing is also significant, the larger gains appear tobe results of finer morphology and person/number agreementbetween verbs and their subjects.
Compared with 19.29 sec-onds of the previous implementation, it shows that a middleground between over-generation and statistical determinationis a viable solution.Finally, more work is needed to produce better output.
Themajority of errors in this approach are modifier placementchoices.
Without a formal grammar, the final placement de-cisions are ultimately decided by an n-gram language model,resulting in short-sighted decisions.
Even though 85% froma semantic input is a good result, modifiers tend to be the onearea that falls behind.
Several examples of this can be seen inAppendix B where some poor generations are shown.6 Related WorkStochastic work on the FERGUS system [Chen et al, 2002]uses a TAG grammar to produce a word lattice of possiblerealizations.
The lattice is traversed to find the most likelypath.
The work in [Chen et al, 2002] generated sentences in0.28 seconds for an Air-Travel Domain.
This paper differsin that the input to FERGUS is a shallow syntactic tree, con-taining all lexemes and function words.
In addition, surfacesyntax trees were mapped one-to-one with each template inthe Air-Travel domain.
There was little, if any, flexibility inthe semantic input.
This paper presents a result of 0.37 sec-onds that includes both the sentence planner, surface realizer,and a grammar that generates multiple realizations based onboth syntax and semantics.Work was done on the Oxygen system [Habash, 2000] toimprove the speed of the two-phased Nitrogen generator, apredecessor to HALogen.
The work pre-compiled a declar-ative grammar into a functional program, thus removing theneed to match rules during forest creation.
This paper differsin that similar performance was achieved without the needfor pre-compiling nor a more complex grammar syntax.
Thispaper also described lexical movement and trickle down fea-tures not supported in Oxygen.Chambers [Chambers and Allen, 2004] used HALogen ina dialogue system and performed a human evaluation of themixed syntax/semantic input.
Their input converted their do-main independent logical form into the HALogen input.
Thiswork differs in that we obviously did not use the HALo-gen system, but implemented a more efficient two-phased ap-proach.
The work by Chambers and Allen did not analyzeruntime, perform sentence planning (not a full semantic in-put), nor provide results from the common String Accuracymetrics for comparison to other approaches.7 ConclusionStochastic approaches to natural language processing are of-ten criticized for being too slow, particularly in recent at-tempts in language generation.
This paper describes Acorn,a system that generates dialogue utterances in an averageof 0.37 seconds.
The approach and its additional advancesin word forest creation were described, such as a techniquecalled trickle-down features that allow a grammar to passhead/foot features through a generation input, enabling lan-guage phenomena such as wh-movement to be represented.The grammar syntax and an evaluation of the coverage in anunknown domain were presented.
The coverage is compa-rable and the runtime drastically out-performs previous ap-proaches.A Example Semantic and Lexical InputBelow is an example utterance from the Monroe corpus andits purely semantic and lexical input to Acorn.
In this exam-ple, only the words have, helicopter, and Strong Memorial areabsent in the semantic input.
The resulting generation outputfrom Acorn is also shown.Original utterance:?and i also have a helicopter at strong memorial?Semantic Input to Acorn:((utt :speechact sa tell :mods v05 :saterm v88 :terms((v88 :indicator speechact :class sa tell :content v27:mods v05)(v05 :indicator f :class conjunct :lex and :of v88)(v27 :indicator f :class have :co-theme v63 :theme v09:mods v64 :mods v23 :tense present)(v09 :indicator pro :class person :context-rel i)(v23 :indicator f :class additive :lex also :of v27)(v63 :indicator a :class air-vehicle)(v64 :indicator f :class spatial-loc :lex at :of v27 :val v75)(v75 :indicator the :class facility:name-of (strong memorial)))))Lexical Input to Acorn:((utt :speechact sa tell :mods v05 :saterm v88 :terms((v88 :indicator speechact :class sa tell :content v27:mods v05)(v05 :indicator f :class conjunct :lex and :of v88)(v27 :indicator f :class have :lex have :co-theme v63:theme v09 :mods v64 :mods v23 :tense present)(v09 :indicator pro :class person :lex i :context-rel i)(v23 :indicator f :class additive :lex also :of v27)(v63 :indicator a :class air-vehicle :lex helicopter)(v64 :indicator f :class spatial-loc :lex at :of v27 :val v75)(v75 :indicator the :class facility :lex strong-memorial:name-of (strong memorial)))))Acorn Generation:?and i have a helicopter also at strong memorial?B Example Poor OutputBelow are some target and generated utterances from Acorn,illustrating several common errors, and are not examples ofsuccess.
The first utterance is the real target one, and thesecond is the Acorn generated utterance.1.
?i think i have a disability with maps?
?i think i have disability with maps?2.
?they should have stayed in front of the tv?
?in a front of the tv should stay they?3.
?and i also have a helicopter at strong memorial?
?and i have a helicopter also at strong memorial?4.
?i can?t see it on the map?
?i can not on the map see it?5.
?probably all of them are hospitals?
?probably hospitals are all them?6.
?are you talking to me?
?are you talking me?7.
?and there are three people on a stretcher at the airport ?
?and three people on a stretcher are at the airport?8.
?then there?s one stretcher patient at the mall?
?then stretcher one patient is at the mall?9.
?so that guy should just walk to the hospital?
?so that guy should walk to the hospital just?10.
?i think that?s a very good plan?
?i think that is very good plan?C Example Good OutputBelow are a list of target utterances that Acorn matched ex-actly, word for word.
It is obviously not a complete list.1.
?i?m not doing this on purpose?2.
?we can bring it to strong memorial?3.
?it?s on elmwood and mount hope ?4.
?so the heart attack person can?t go there?5.
?and bring them to saint mary?s?6.
?do you have any suggestions??7.
?we can put him in one ambulance?8.
?because we have only six wounded?9.
?i think that?s a good idea?10.
?and the other one is at the airport?11.
?what can i say?
?References[Bangalore et al, 2000] Srinivas Bangalore, Owen Rambow,and Steve Whittaker.
Evaluation metrics for generation.
InINLG, Saarbrucken, Germany, August 2000.
[Callaway, 2003] Charles Callaway.
Evaluating coverage forlarge symbolic nlg grammars.
In IJCAI, Acapulco, Mex-ico, August 2003.
[Chambers and Allen, 2004] Nathanael Chambers andJames Allen.
Stochastic language generation in a dialoguesystem: Toward a domain independent generator.
InProceedings of the 5th SIGdial Workshop on Discourseand Dialogue, Boston, USA, May 2004.
[Chen et al, 2002] John Chen, Srinivas Bangalore, OwenRambow, and Marilyn A. Walker.
Towards automatic gen-eration of nautral language generation systems.
In COL-ING, Taipei, Taiwan, 2002.
[Clarkson and Robinson, 1997] P.R.
Clarkson and A.J.Robinson.
Language model adaptation using mixturesand an exponentially decaying cache.
In Proceedings ofICASSP-97, pages II:799?802, 1997.
[Dzikovska et al, 2003] M. Dzikovska, M. Swift, andJ.
Allen.
Constructing custom semantic representationsfrom a generic lexicon.
In 5th International Workshop onComputational Semantics, 2003.
[Elhadad and Robin, 1996] M. Elhadad and J. Robin.
Anoverview of surge: A reusable comprehensive syntacticrealization component.
Tech Report 96-03, Ben GurionUniversity, Beer Sheva, Israel, 1996.
[Habash, 2000] Nizar Habash.
Oxygen: A language inde-pendent linearization engine.
In AMTA-2000, Cuernavaca,Mexico, October 2000.
[Kipper et al, 2000] Karin Kipper, Hoa Trang Dang, andMartha Palmer.
Class-based construction of a verb lexi-con.
In Proceedings of the 17th National Conference onArtificial Intelligence, Austin, TX, 2000.
[Langkilde-Geary, 2002] Irene Langkilde-Geary.
An empir-ical verification of coverage and correctness for a general-purpose sentence generator.
In INLG, New York, 2002.
[Langkilde, 2000] Irene Langkilde.
Forest-based statisticalsentence generation.
In NAACL, 2000.
[Papineni et al, 2001] K. Papineni, S. Roukos, T. Ward, andW.
Zhu.
Bleu: a method for automatic evaluation ofmachine translation.
Research Report RC22176, IBM,September 2001.
[Stent, 2000] A. Stent.
The monroe corpus.
Research Report728, Computer Science Dept., University of Rochester,March 2000.
99-2.
