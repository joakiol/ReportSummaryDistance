Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 63?71,Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational LinguisticsA Joint Model of Implicit Arguments for Nominal PredicatesMatthew Gerber and Joyce Y. ChaiDepartment of Computer ScienceMichigan State UniversityEast Lansing, Michigan, USA{gerberm2,jchai}@cse.msu.eduRobert BartComputer Science and EngineeringUniversity of WashingtonSeattle, Washington, USArbart@cs.washington.eduAbstractMany prior studies have investigated the re-covery of semantic arguments for nominalpredicates.
The models in many of these stud-ies have assumed that arguments are indepen-dent of each other.
This assumption simpli-fies the computational modeling of semanticarguments, but it ignores the joint nature ofnatural language.
This paper presents a pre-liminary investigation into the joint modelingof implicit arguments for nominal predicates.The joint model uses propositional knowledgeextracted from millions of Internet webpagesto help guide prediction.1 IntroductionMuch recent work on semantic role labeling has fo-cused on joint models of arguments.
This work ismotivated by the fact that one argument can eitherpromote or inhibit the presence of another argument.Because most of this work has been done for verbalSRL, nominal SRL has lagged behind somewhat.
Inparticular, the ?implicit?
nominal SRL model cre-ated by Gerber and Chai (2010) does not addressjoint argument structures.
Implicit arguments aresimilar to standard SRL arguments, a primary differ-ence being their ability to cross sentence boundaries.In the model created by Gerber and Chai, implicit ar-gument candidates are classified independently anda heuristic post-processing method is applied to de-rive the final structure.
This paper presents a prelim-inary joint implicit argument model.Consider the following sentences:11We will use the notation of Gerber and Chai (2010), where(1) [c1 The president] is currently struggling tomanage [c2 the country?s economy].
(2) If he cannot get it under control, [p loss] of[arg1 the next election] might result.In Example 2, we are searching for the iarg0 of loss(the entity that is losing).
The sentence in Exam-ple 1 supplies two candidates c1 and c2.
If one onlyconsiders the predicate loss, then c1 and c2 wouldboth be reasonable fillers for the iarg0: presidentsoften lose things (e.g., votes and allegiance) andeconomies often lose things (e.g., jobs and value).However, the sentence in Example 2 supplies addi-tional information.
It tells the reader that the nextelection is the entity being lost.
Given this infor-mation, one would likely prefer c1 over c2 becauseeconomies don?t generally lose elections, whereaspresidents often do.
This type of inference is com-mon in textual discourses because authors assumea shared knowledge base with their readers.
Thisknowledge base contains information about eventsand their typical participants (e.g., the fact that pres-idents lose elections but economies do not).The model presented in this paper relies on aknowledge base constructed by automatically min-ing semantic propositions from Internet webpages.These propositions help to identify likely joint im-plicit argument configurations.
In the following sec-tion, we review work on joint inference within se-mantic role labeling.
In Sections 4 and 5, we presentthe joint implicit argument model and its features.Evaluation results for this model are given in Sec-standard nominal arguments are indicated with argn and im-plicit arguments are indicated with iargn.63tion 6.
The joint model contains many simplifyingassumptions, which we address in Section 7.
Weconclude in Section 8.2 Related workA number of recent studies have shown that seman-tic arguments are not independent and that systemperformance can be improved by taking argumentdependencies into account.
Consider the followingexamples, due to Toutanova et al (2008):(3) [Temporal The day] that [arg0 the ogre][Predicate cooked] [arg1 the children] is stillremembered.
(4) [arg1 The meal] that [arg0 the ogre][Predicate cooked] [Beneficiary the children]is still remembered.These examples demonstrate the importance ofinter-argument dependencies.
The change from dayin Example 3 to meal in Example 4 affects morethan just the Temporal label: additionally, the arg1changes to Beneficiary, even though the underlyingtext (the children) does not change.
To capture thisdependency, Toutanova el al.
first generate an n-best list of argument labels for a predicate instance.They then re-rank this list using joint features thatdescribe multiple arguments simultaneously.
Thefeatures help prevent globally invalid argument con-figurations (e.g., ones with multiple arg0 labels).Punyakanok et al (2008) formulate a variety ofconstraints on argument configurations.
For exam-ple, arguments are not allowed to overlap the predi-cate, nor are they allowed to overlap each other.
Theauthors treat these constraints as binary variableswithin an integer linear program, which is optimizedto produce the final labeling.Ritter et al (2010) investigated joint selectionalpreferences.
Traditionally, a selectional preferencemodel provides the strength of association betweena predicate-argument position and a specific textualexpression.
Returning to Examples 1 and 2, onesees that the selectional preference for president andeconomy in the iarg0 position of loss should be high.Ritter et al extended this single-argument modelusing a joint formulation of Latent Dirichlet Allo-cation (LDA) (Blei et al, 2003).
In the generativeversion of joint LDA, text for the argument posi-tions is generated from a common hidden variable.This approach reflects the intuition behind Exam-ples 1 and 2 and would help identify president as theiarg0.
Training data for the model was drawn froma large corpus of two-argument tuples extracted bythe TextRunner system, which we describe next.Both Ritter et al?s model and the model describedin this paper rely heavily on information extractedby the TextRunner system (Banko et al, 2007).The TextRunner system extracts tuples from Inter-net webpages in an unsupervised fashion.
One keydifference between TextRunner and other informa-tion extraction systems is that TextRunner does notuse a closed set of relations (compare to the workdescribed by ACE (2008)).
Instead, the relation setis left open, leading to the notion of Open Informa-tion Extraction (OIE).
Although OIE often has lowerprecision than traditional information extraction, itis able to extract a wider variety of relations at preci-sion levels that are often useful (Banko and Etzioni,2008).3 Using TextRunner to assess jointargument assignmentsReturning again to Examples 1 and 2, one can queryTextRunner in the following way:arg0 : ?Predicate : lose2arg1 : electionIn the TextRunner system, arg0 typically indicatesthe Agent and arg1 typically indicates the Theme.TextRunner provides many tuples in response to thisquery, two of which are shown below:(5) Usually, [arg0 the president?s party][Predicate loses] [arg1 seats in the mid-termelection].
(6) [arg0 The president] [Predicate lost] [arg1 theelection].The tuples present in these sentences give strong in-dicators about the type of entity that loses elections.2Nominal predicates are mapped to their verbal forms usinginformation provided by the NomBank lexicon.64Given all of the returned tuples, only a single oneinvolves economy in the arg0 position:(7) Any president will take credit for [arg0 a goodeconomy] or [Predicate lose] [arg1 anelection] over a bad one.In Example 7, TextRunner has not analyzed the ar-guments correctly (president should be the arg0, noteconomy).3 In Section 5, we show how evidencefrom the tuple lists can be aggregated such that cor-rect analyses (5 and 6) are favored over incorrectanalyses (7).
The primary contribution of this paperis an exploration of how the aggregated evidence canbe used to identify implicit arguments (e.g., presi-dent in Example 1).4 Joint model formulationTo simplify the experimental setting, the model de-scribed in this paper targets the specific situationwhere a predicate instance p takes an implicit iarg0and an implicit iarg1.4 Whereas the model proposedby Gerber and Chai (2010) classifies candidates forthese positions independently, the model in this pa-per classifies joint structures by evaluating the fol-lowing binary prediction function:P (+| ?p, iarg0, ci, iarg1, cj?)
(8)Equation 8 gives the probability of the joint assign-ment of ci to iarg0 and cj to iarg1.
Given a set of ncandidates c1, .
.
.
, cn ?
C , the best labeling is foundby considering all possible assignments of ci and cj :argmax(ci,cj)?CxC s.t.
i 6=jP (+| ?p, iarg0, ci, iarg1, cj?
)(9)Consider modified versions of Examples 1 and 2:(10) [c1 The president] is currently struggling tomanage [c2 the country?s economy].
(11) If he cannot get it under control before [c3 thenext election], a [p loss] might result.3Banko and Etzioni (2008) cite a precision score of 88% fortheir system.4This simplifying assumption does not hold for real data,and is addressed further in Section 7.2.In this case, we are looking for the iarg0 as well asthe iarg1 for the loss predicate.
Three candidates c1,c2, and c3 are marked.
The joint model would eval-uate the following probabilities, taking the highestscoring to be the final assignment:P (+| ?loss, iarg0, president, iarg1, economy?
)*P (+| ?loss, iarg0, president, iarg1, election?
)P (+| ?loss, iarg0, economy, iarg1, president?
)P (+| ?loss, iarg0, economy, iarg1, election?
)P (+| ?loss, iarg0, election, iarg1, president?
)P (+| ?loss, iarg0, election, iarg1, economy?
)Intuitively, only the starred item should have a highprobability.
In the following section, we describehow these probabilities can be estimated using in-formation extracted by TextRunner.5 Joint model featuresAs mentioned in Section 2, the TextRunner systemhas been extracting massive amounts of knowledgein the form of tuples such as the following:?president, lose, election?The database of tuples can be queried by supplyingone or more of the tuple arguments.
For example,the following is a partial result list for the query?president, lose, ??
:?Kenyan president, lose, election?
?president?s party, lose seat in, election?
?president, lose, ally?The final position in each of these tuples (e.g.,election) provides a single answer to the question?What might a president lose??.
Aggregation beginsby generalizing each answer to its WordNet synset(glosses are shown after the arrows):?Kenyan president, lose, election?
?
a vote?president?s party, lose seat in, election?
(same)?president, lose, ally?
?
friendly nationIn cases where a tuple argument has multipleWordNet senses, the tuple is mapped to the mostcommon sense as listed in the WordNet database.65Having mapped each tuple to its synset, each synsetis ranked according to the number of tuples thatit covers.
For the query ?president, lose, ?
?, thisproduces the following ranked list of WordNetsynsets (only the top five are shown, with thenumber in parentheses indicating how many tuplesare covered):1. election (77)2. war (51)3. vote (39)4. people (34)5. support (26)...The synsets above indicate likely answers to the pre-vious question of ?What might a president lose?
?.In a similar manner, one can answer a questionsuch as ?What might lose an election??
using tu-ples extracted by TextRunner.
The procedure de-scribed above produces the following ranked list ofWordNet synsets to answer this question:...9. people (62)10.
Republican (51)11.
Republican party (51)12.
Hillary (50)13. president (49)...In this case, the expected answer (president) ranks13th in the list of answer synsets.
It is importantto note that lower ranked answers are not necessar-ily incorrect answers.
It is a simple fact that a widevariety of entities can lose an election.
Items 9-13are all reasonable answers to the original questionof what might lose an election.The two symmetric questions defined and an-swered above are closely connected to the implicitargument situation discussed in Examples 10 and11.
In Example 11, one is searching for the implicitiarg0 and iarg1 to the loss predicate.
Candidates ciand cj that truly fill these positions should be com-patible with questions in the following forms:Question: What did ci lose?Answer: cjQuestion: What entity lost cj?Answer: ciIf either of these question-answer pairs is not satis-fied, then the joint assignment of ci to iarg0 and cjto iarg1 should be considered unlikely.
Using thefirst question-answer pair above as an example, sat-isfaction is determined in the following way:1.
Query TextRunner for ?ci, lose, ?
?, retrievingthe top n tuples.2.
Map the final argument of each tuple to itsWordNet synset and rank the synsets by fre-quency, producing the ranked list A of answersynsets.3.
Map cj to its most common WordNet synsetsynsetcj and determine whether synsetcj ex-ists in A.
If it does, the question-answer pair issatisfied.Some additional processing is required to determinewhether synsetcj exists in A.
This is due to the hi-erarchical organization of WordNet.
For example,suppose that synsetcj is the synset containing ?pri-mary election?
and A contains synsets paraphrasedas follows:1. election2.
war3.
vote...synsetcj does not appear directly in this list; how-ever, its existence in the list is implied by the follow-ing hypernymy path within WordNet:primary election is-a???
electionIntuitively, if synsetcj is connected to a highlyranked synset in A by a short path, then one has ev-idence that synsetcj answers the original question.66The evidence is weaker if the path is long, as in thefollowing example:open primary is-a???
direct primaryis-a???
primary election is-a???
electionAdditionally, a path between more specific synsets(i.e., those lower in the hierarchy) indicates astronger relationship than a path between more gen-eral synsets (i.e., those higher in the hierarchy).These two situations are depicted in Figure 1.
Thesynset similarity metric defined by Wu and Palmer(1994) combines the path length and synset depthintuitions into a single numeric score that is definedas follows:2 ?
depth(lca(synset1, synset2))depth(synset1) + depth(synset2)(12)In Equation 12, lca returns the lowest common an-cestor of the two synsets within the WordNet is-ahierarchy.To summarize, Equation 12 indicates the strengthof association between synsetcj (e.g., primary elec-tion) and a ranked synset synseta from A that an-swers a question such as ?What might a presidentlose??.
If the association between synsetcj andsynseta is small, then the assignment of cj to iarg1is unlikely.
The process works similarly for assess-ing ci as the filler of iarg0.
In what follows, wequantify this intuition with features used to repre-sent the conditioning information in Equation 8.Feature 1: Maximum association strength.
Giventhe conditioning variables in Equation 8, there aretwo questions that can be asked:Question: What did ci p?Answer: cjQuestion: What entity p cj?Answer: ciEach of these questions produces a ranked list ofanswer synsets using the approach described previ-ously.
The synset for each answer string will matchzero or more of the answer synsets, and each of thesematches will be associated with a similarity score asdefined in Equation 12.
Feature 1 considers all suchsimilarity scores and selects the maximum.
A highvalue for this feature indicates that one (or both) ofthe candidates (ci or cj) is likely to fill its associatedimplicit argument position.Feature 2: Maximum reciprocal rank.
Of all theanswer matches described for Feature 1, Feature 2selects the highest ranking and forms the reciprocalrank.
Thus, values for Feature 2 are in [0,1] withlarger values indicating matches with higher rankedanswer synsets.Feature 3: Number of matches.
This featurerecords the total number of answer string matchesfrom either of the questions described for Feature 1.Feature 4: Sum reciprocal rank.
Feature 2 consid-ers answer synset matches from either of the posedquestions; ideally, each question-answer pair shouldhave some influence on the probability estimate inEquation 8.
Feature 4 looks at the answer synsetmatches from each question individually.
The matchwith highest rank for each question is selected, andthe reciprocal rank 2r1 + r2 is computed.
The valueof this feature is zero if either of the questions failsto produce a matching answer synset.Features 5 and 6: Local classification scores.
Thejoint model described in this paper does not replacethe local prediction model presented by Gerber andChai (2010).
The latter uses a wide variety of impor-tant features that cannot be ignored.
Like previousjoint models (e.g., the one described by Toutanova etal.
(2008)), the joint model works on top of the lo-cal prediction model, whose scores are incorporatedinto the joint model as feature-value pairs.
Given thelocal prediction scores for the iarg0 and iarg1 posi-tions in Equation 8, the joint model forms two fea-tures: (1) the sum of the scores for ci filling iarg0and cj filling iarg1, and (2) the product of these twoscores.6 EvaluationWe evaluated the joint model described in the pre-vious sections over the manually annotated implicit67entity (a)physical entity (b)thingbody of water (c)bay (d)matterabstract entityFigure 1: Effect of depth on WordNet synset similarity.
All links indicate is-a relationships.
Although the linkdistance from (a) to (b) equals the distance from (c) to (d), the latter are more similar due to their lower depth withinthe WordNet hierarchy.argument data created by Gerber and Chai (2010).This dataset contains full-text implicit argumentannotations for approximately 1,200 predicate in-stances within the Penn TreeBank.
As mentionedin Section 4, all experiments were conducted us-ing predicate instances that take an iarg0 and iarg1in the ground-truth annotations.
We used a ten-fold cross-validation setup and the evaluation met-rics proposed by Ruppenhofer et al (2009), whichwere also used by Gerber and Chai.
For each evalu-ation fold, features were selected using only the cor-responding training data and the greedy selection al-gorithm proposed by Pudil et al (1994), which startswith an empty feature set and incrementally addsfeatures that provide the highest gains.For comparison with Gerber and Chai?s model,we also evaluated the local prediction model on theevaluation data.
Because this model predicted im-plicit arguments independently, it continued to usethe heuristic post-processing algorithm to arrive atthe final labeling.
However, the prediction thresholdt was eliminated because the system could safely as-sume that a true filler for the iarg0 and iarg1 posi-tions existed.Table 1 presents the evaluation results.
The firstthing to note is that these results are not comparableto the results presented by Gerber and Chai (2010).In general, performance is much higher becausepredicate instances reliably take implicit argumentsin the iarg0 and iarg1 positions.
The overall perfor-mance increase versus the local model is relativelysmall (approximately 1 percentage point); however,the bid predicate in particular showed a substantialincrease (greater than 11 percentage points).7 Discussion7.1 Example improvement versus local modelThe bid and investment predicates showed thelargest increase for the joint model versus the localmodel.
Below, we give an example of the investmentpredicate for which the joint model correctly identi-fied the iarg0 and the local model did not.
(13) [Big investors] can decide to ride out marketstorms without jettisoning stock.
(14) Most often, [c they] do just that, becausestocks have proved to be the best-performinglong-term [Predicate investment], attractingabout $1 trillion from pension funds alone.Both models identified the iarg1 as money from aprior sentence (not shown).
The local model in-correctly predicted $1 trillion in Example 14 as theiarg0 for the investment event.
This mistake demon-strates a fundamental limitation of the local model:it cannot detect simple incompatibilities in the pre-dicted argument structure.
It does not know that?money investing money?
is a rare or impossibleevent in the real world.For the joint model?s prediction, consider the con-stituent marked with c in Example 14.
This con-68Local model Joint model# Imp.
args.
P R F1 P R F1price 40 65.0 65.0 65.0 67.5 67.5 67.5sale 34 86.5 86.5 86.5 84.3 84.3 84.3plan 30 60.0 60.0 60.0 56.7 56.7 56.7bid 26 66.7 66.7 66.7 78.2 78.2 78.2fund 18 83.3 83.3 83.3 83.3 83.3 83.3loss 14 100.0 100.0 100.0 100.0 100.0 100.0loan 12 63.6 58.3 60.9 50.0 50.0 50.0investment 8 57.1 50.0 53.3 62.5 62.5 62.5Overall 182 72.6 71.8 72.2 73.1 73.1 73.1Table 1: Joint implicit argument evaluation results.
The second column gives the total number of implicit argumentsin the ground-truth annotations.
P , R, and F1 indicate precision, recall, and f-measure (?
= 1) as defined by Ruppen-hofer et al (2009).stituent is resolved to Big investors in the precedingsentence.
Thus, the two relevant questions are asfollows:Question: What did big investors invest?Answer: moneyQuestion: What entity invested money?Answer: big investorsThe first question produces the following ranked listof answer synsets (the number in parentheses indi-cates the number of answer tuples mapped to thesynset):money (71)amount (38)million (38)billion (22)capital (21)As shown, the answer string of money matches thetop-ranked answer synset.
The second question pro-duces the following ranked list of answer synsets:company (642)people (460)government (275)business (75)investor (70)In this case, the answer string Big investors matchesthe fifth answer synset.
The combined evidenceof these two question-answer pairs allows the jointsystem to successfully identify Big investors as theiarg0 of the investment predicate in Example 14.7.2 Toward a generally applicable joint modelThe joint model presented in this paper assumes thatall predicate instances take an iarg0 and iarg1.
Thisassumption clearly does not hold for real data (thesepositions are often not expressed in the text), but re-laxing it will require investigation of the followingissues:1.
Explicit arguments should also be consideredwhen determining whether a candidate c fillsan implicit argument position iargn.
The mo-tivation here is similar to that given elsewherein this paper: arguments (whether implicit orexplicit) are not independent.
This is demon-strated by Example 2 at the beginning of thispaper, where election is an explicit argument tothe predicate and affects the implicit argumentinference.
The model developed in this paperonly considers jointly occurring implicit argu-ments.2.
Other implicit argument positions (e.g.,iarg2, iarg3, etc.)
need to be accountedfor as well.
This will present a challengewhen it comes to extracting the necessary69propositions from TextRunner.
Currently,TextRunner only handles tuples of the form?arg0, p, arg1?.
Other argument positions arenot directly analyzed by the system; however,because TextRunner also returns the sentencefrom which a tuple is extracted, these addi-tional argument positions could be extracted inthe following way:(a) For an instance of the sale predicatewith an arg0 of company, to findlikely arg2 fillers (the entity purchas-ing the item), query TextRunner with?company, sell, ??.
(b) Perform standard verbal SRL on the sen-tences for the resulting tuples, identifyingany arg2 occurrences.
(c) Cluster and rank the arg2 fillers accordingto the method described in this paper.This approach combines Open Information Ex-traction with traditional information extraction(i.e., verbal SRL).3.
Computational complexity and probabilityestimation is a problem for many joint mod-els.
The model presented in this paper quicklybecomes computationally intractable when thenumber of candidates and implicit argumentpositions becomes moderately large.
This isbecause Equation 9 considers all possible as-signments of candidates to implicit argumentpositions.
With as few as thirty candidates andfive argument positions (not uncommon), onemust evaluate 30!/25!
= 17, 100, 720 possibleassignments.
Although this particular formula-tion is not tractable, one based on dynamic pro-gramming or heuristic search might give rea-sonable results.
Efficient estimation of the jointprobability via Gibbs sampling would also be apossible approach (Resnik and Hardisty, 2010).8 ConclusionsMany prior studies have investigated the recoveryof semantic arguments for nominal predicates.
Themodels in many of these studies have assumed thatthe arguments are independent of each other.
Thisassumption simplifies the computational modelingof semantic arguments, but it ignores the joint na-ture of natural language.
In order to take advantageof the information provided by jointly occurring ar-guments, the independent prediction models must beenhanced.This paper has presented a preliminary investiga-tion into the joint modeling of implicit argumentsfor nominal predicates.
The model relies heavilyon information extracted by the TextRunner extrac-tion system, which pulls propositional tuples frommillions of Internet webpages.
These tuples encodeworld knowledge that is necessary for resolving se-mantic arguments in general and implicit argumentsin particular.
This paper has proposed methods ofaggregating tuple knowledge to guide implicit argu-ment resolution.
The aggregated knowledge is ap-plied via a re-ranking model that operates on topof the local prediction model described in previouswork.The performance gain across all predicate in-stances is relatively small; however, larger gains areobserved for the bid and investment predicates.
Theimprovement in Example 14 shows that the jointmodel is capable of correcting a bad local predic-tion using information extracted by the TextRunnersystem.
This type of information is not used by thelocal prediction model.Although the results in this paper show that someimprovement is possible through the use of a jointmodel of implicit arguments, a significant amountof future work will be required to make the modelwidely applicable.ReferencesACE, 2008.
The ACE 2008 Evaluation Plan.
NIST, 1.2dedition, August.Michele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InProceedings of ACL-08: HLT, pages 28?36, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Michele Banko, Michael J Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open in-formation extraction from the web.
In Proceedings ofthe 20th International Joint Conference on ArtificialIntelligence.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.702003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022.Matthew Gerber and Joyce Chai.
2010.
Beyond Nom-Bank: A study of implicit arguments for nominal pred-icates.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages1583?1592, Uppsala, Sweden, July.
Association forComputational Linguistics.P.
Pudil, J. Novovicova, and J. Kittler.
1994.
Floatingsearch methods in feature selection.
Pattern Recogni-tion Letters, 15:1119?1125.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Comput.
Linguist., 34(2):257?287.Philip Resnik and Eric Hardisty.
2010.
Gibbs samplingfor the uninitiated.
Technical report, University ofMaryland, June.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A latentdirichlet alocation method for selectional preferences.In Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics.Josef Ruppenhofer, Caroline Sporleder, Roser Morante,Collin Baker, and Martha Palmer.
2009.
Semeval-2010 task 10: Linking events and their participants indiscourse.
In Proceedings of the Workshop on Seman-tic Evaluations: Recent Achievements and Future Di-rections (SEW-2009), pages 106?111, Boulder, Col-orado, June.
Association for Computational Linguis-tics.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Comput.
Linguist., 34(2):161?191.Zhibiao Wu and Martha Palmer.
1994.
Verb seman-tics and lexical selection.
In Proceedings of the 32ndAnnual Meeting of the Association for ComputationalLinguistics, pages 133?138, Las Cruces, New Mex-ico, USA, June.
Association for Computational Lin-guistics.71
