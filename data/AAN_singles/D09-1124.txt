Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1192?1201,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPCross-lingual Semantic Relatedness Using Encyclopedic KnowledgeSamer Hassan and Rada MihalceaDepartment of Computer ScienceUniversity of North Texassamer@unt.edu, rada@cs.unt.eduAbstractIn this paper, we address the task of cross-lingual semantic relatedness.
We intro-duce a method that relies on the informa-tion extracted from Wikipedia, by exploit-ing the interlanguage links available be-tween Wikipedia versions in multiple lan-guages.
Through experiments performedon several language pairs, we show thatthe method performs well, with a perfor-mance comparable to monolingual mea-sures of relatedness.1 MotivationGiven the accelerated growth of the number ofmultilingual documents on the Web and else-where, the need for effective multilingual andcross-lingual text processing techniques is becom-ing increasingly important.
In this paper, weaddress the task of cross-lingual semantic relat-edness, and introduce a method that relies onWikipedia in order to calculate the relatedness ofwords across languages.
For instance, given theword factory in English and the word lavoratorein Italian (En.
worker), the method can measurethe relatedness of these two words despite the factthat they belong to two different languages.Measures of cross-language relatedness are use-ful for a large number of applications, includingcross-language information retrieval (Nie et al,1999; Monz and Dorr, 2005), cross-language textclassification (Gliozzo and Strapparava, 2006),lexical choice in machine translation (Och andNey, 2000; Bangalore et al, 2007), inductionof translation lexicons (Schafer and Yarowsky,2002), cross-language annotation and resourceprojections to a second language (Riloff et al,2002; Hwa et al, 2002; Mohammad et al, 2007).The method we propose is based on a measureof closeness between concept vectors automati-cally built from Wikipedia, which are mapped viathe Wikipedia interlanguage links.
Unlike previ-ous methods for cross-language mapping, whichare typically limited by the availability of bilingualdictionaries or parallel texts, the method proposedin this paper can be used to measure the related-ness of word pairs in any of the 250 languages forwhich a Wikipedia version exists.The paper is organized as follows.
We first pro-vide a brief overview of Wikipedia, followed bya description of the method to build concept vec-tors based on this encyclopedic resource.
We thenshow how these concept vectors can be mappedacross languages for a cross-lingual measure ofword relatedness.
Through evaluations run on sixlanguage pairs, connecting English, Spanish, Ara-bic and Romanian, we show that the method is ef-fective at capturing the cross-lingual relatedness ofwords, with results comparable to the monolingualmeasures of relatedness.2 WikipediaWikipedia is a free online encyclopedia, represent-ing the outcome of a continuous collaborative ef-fort of a large number of volunteer contributors.Virtually any Internet user can create or edit aWikipedia webpage, and this ?freedom of contri-bution?
has a positive impact on both the quantity(fast-growing number of articles) and the quality(potential errors are quickly corrected within thecollaborative environment) of this online resource.The basic entry in Wikipedia is an article (orpage), which defines and describes an entity oran event, and consists of a hypertext documentwith hyperlinks to other pages within or outsideWikipedia.
The role of the hyperlinks is to guidethe reader to pages that provide additional infor-mation about the entities or events mentioned inan article.
Articles are organized into categories,which in turn are organized into hierarchies.
Forinstance, the article automobile is included in thecategory vehicle, which in turn has a parent cate-1192Language Articles UsersEnglish 2,221,980 8,944,947German 864,049 700,980French 765,350 546,009Polish 579,170 251,608Japanese 562,295 284,031Italian 540,725 354,347Dutch 519,334 216,938Portuguese 458,967 503,854Spanish 444,696 966,134Russian 359,677 226,602Table 1: Top ten largest Wikipediasgory named machine, and so forth.Each article in Wikipedia is uniquely referencedby an identifier, consisting of one or more wordsseparated by spaces or underscores and occasion-ally a parenthetical explanation.
For example, thearticle for bar with the meaning of ?counter fordrinks?
has the unique identifier bar (counter).Wikipedia editions are available for more than250 languages, with a number of entries vary-ing from a few pages to two millions articles ormore per language.
Table 1 shows the ten largestWikipedias (as of December 2008), along withthe number of articles and approximate number ofcontributors.1Relevant for the work described in this paper arethe interlanguage links, which explicitly connectarticles in different languages.
For instance, theEnglish article for bar (unit) is connected, amongothers, to the Italian article bar (unit?a di misura)and the Polish article bar (jednostka).
On average,about half of the articles in a Wikipedia versioninclude interlanguage links to articles in other lan-guages.
The number of interlanguage links per ar-ticle varies from an average of five in the EnglishWikipedia, to ten in the Spanish Wikipedia, and asmany as 23 in the Arabic Wikipedia.3 Concept Vector Representations usingExplicit Semantic AnalysisTo calculate the cross-lingual relatedness of twowords, we measure the closeness of their con-cept vector representations, which are built fromWikipedia using explicit semantic analysis (ESA).Encyclopedic knowledge is typically organizedinto concepts (or topics), each concept beingfurther described using definitions, examples,1http://meta.wikimedia.org/wiki/List of Wikipedias#Grand Totaland possibly links to other concepts.
ESA(Gabrilovich and Markovitch, 2007) relies on thedistribution of words inside the encyclopedic de-scriptions, and builds semantic representations fora given word in the form of a vector of the encyclo-pedic concepts in which the word appears.
In thisvector representation, each encyclopedic conceptis assigned with a weight, calculated as the termfrequency of the given word inside the concept?sarticle.Formally, let C be the set of all the Wikipediaconcepts, and let a be any content word.
We define~a as the ESA concept vector of term a:~a = {wc1, wc2...wcn} , (1)where wciis the weight of the concept ciwith re-spect to a. ESA assumes the weight wcito be theterm frequency tfiof the word a in the article cor-responding to concept ci.We use a revised version of the ESA algorithm.The original ESA semantic relatedness betweenthe words in a given word pair a ?
b is defined asthe cosine similarity between their correspondingvectors:Relatedness(a, b) =~a ?~b?~a????~b???.
(2)To illustrate, consider for example the construc-tion of the ESA concept vector for the word bird.The top ten concepts containing this word, alongwith the associated weight (calculated using equa-tion 7), are listed in table 2.
Note that the the ESAvector considers all the possible senses of bird, in-cluding Bird as a surname as in e.g., ?Larry Bird.
?Weight Wikipedia concept51.4 Lists Of Birds By Region44.8 Bird40.3 British Birds Rarities Committee32.8 Origin Of Birds31.5 Ornithology30.1 List Of Years In Birding And Ornithology29.8 Bird Vocalization27.4 Global Spread Of H5n1 In 200626.5 Larry Bird22.3 BirdwatchingTable 2: Top ten Wikipedia concepts for the word?bird?In our ESA implementation, we make threechanges with respect to the original ESA algo-rithm.
First, we replace the cosine similarity with1193a Lesk-like metric (Lesk, 1986), which places lessemphasis on the distributional differences betweenthe vector weights and more emphasis on the over-lap (mutual coverage) between the vector features,and thus it is likely to be more appropriate for thesparse ESA vectors, and for the possible asymme-try between languages.
Let a and b be two termswith the corresponding ESA concept vectors ~Aand ~B respectively.
Let A and B represent the setsof concepts with a non-zero weight encountered in~A and ~B respectively.
The coverage of ~A by ~B isdefined as:G(~B|~A) =?i?Bwai(3)and similarly, the coverage of ~B by ~A is:G(~A|~B) =?i?Awbi(4)where waiand wbirepresent the weight associ-ated with concept ciin vectors ~A and ~B respec-tively.
By averaging these two asymmetric scores,we redefine the relatedness as:Relatedness(a, b) =G(~B|~A) + G(~A|~B)2(5)Second, we refine the ESA weighting schemato account for the length of the articles describingthe concept.
Since some concepts have lengthydescriptions, they may be favored due to their highterm frequencies when compared to more compactdescriptions.
To eliminate this bias, we calculatethe weight associated with a concept cias follows:wci= tfi?
log(M/ |ci|), (6)where tfirepresents the term frequency of theword a in concept ci, M is a constant representingthe maximum vocabulary size of Wikipedia con-cepts, and |ci| is the size of the vocabulary used inthe description of concept ci.Finally, we use the Wikipedia category graphto promote category-type concepts in our featurevectors.
This is done by scaling the concept?sweight by the inverse of the distance dito theroot category.
The concepts that are not categoriesare treated as leaves, and therefore their weight isscaled down by the inverse of the maximum depthin the category graph.
The resulting weightingscheme is:wci= tfi?
log(M/ |ci|)/di(7)4 Cross-lingual RelatednessWe measure the relatedness of concepts in differ-ent languages by using their ESA concept vectorrepresentations in their own languages, along withthe Wikipedia interlanguage links that connect ar-ticles written in a given language to their corre-sponding Wikipedia articles in other languages.For example, the English Wikipedia article mooncontains interlanguage links to Q ??
in the Ara-bic Wikipedia, luna in the Spanish Wikipedia, andluna?
in the Romanian Wikipedia.
The interlan-guage links can map concepts across languages,and correspondingly map concept vector represen-tations in different languages.Formally, let Cxand Cybe the sets of allWikipedia concepts in languages x and y, withcorresponding translations in the y and x lan-guages, respectively.
If trxy() is a translationfunction that maps a concept ci?
Cxinto the con-cept c?i?
Cyvia the interlanguage links, we canwrite:trxy(ci) = c?i, (8)The projection of the ESA vector ~t from lan-guage x onto y can be written as:trxy(~t) ={wtrxy(c1)...wtrxy(cn)}.
(9)Using equations 5, 7, and 9, we can calculate thecross-lingual semantic relatedness between anytwo content terms axand byin given languagesx and y as:sim(ax, by) =G(tryx(~B)|~A) + G(~A|tryx(~B))2.
(10)Note that the weights assigned to Wikipediaconcepts inside the concept vectors are languagespecific.
That is, two Wikipedia concepts fromdifferent languages, mapped via an interlanguagelink, can, and often do have different weights.Intuitively, the relation described by the inter-language links should be reflective and transi-tive.
However, due to Wikipedia?s editorial pol-icy, which accredits users with the responsibility1194of maintaining the articles, these properties are notalways met.
Table 3 shows real cases where thetransitive and the reflective properties fail due tomissing interlanguage links.Relation ExistsReflectivityKafr-El-Dawwar Battle(en) 7?
P@ ?Y?
@ Q ??
??Q??
(ar) YesP@?Y?
@ Q????Q??
(ar) 7?
Kafr-El-Dawwar Battle(en) NoTransitivityIntifada(en) 7?
Intifada(es) YesIntifada(es) 7?
?
?A ?J K @(ar) YesIntifada(en) 7?
?
?A ?J K @(ar) NoTable 3: Reflectivity and transitivity in WikipediaWe solve this problem by iterating over thetranslation tables and extracting all the missinglinks by enforcing the reflectivity and the transi-tivity properties.
Table 4 shows the initial numberof interlanguage links and the discovered links forthe four languages used in our experiments.
Thetable also shows the coverage of the interlanguagelinks, measured as the ratio between the total num-ber of interlanguage links (initial plus discovered)originating in the source language towards the tar-get language, divided by the total number of arti-cles in the source language.Interlanguage linksLanguage pair Initial Discov.
Cover.English ?
Spanish 293,957 12,659 0.14English ?
Romanian 86,719 4,641 0.04English ?
Arabic 56,233 3,916 0.03Spanish ?
English 294,266 7,328 0.58Spanish ?
Romanian 39,830 3,281 0.08Spanish ?
Arabic 33,889 3,319 0.07Romanian ?
English 75,685 6,783 0.46Romanian ?
Spanish 36,002 3,546 0.22Romanian ?
Arabic 15,777 1,698 0.10Arabic ?
English 46,072 3,170 0.33Arabic ?
Spanish 28,142 3,109 0.21Arabic ?
Romanian 15,965 1,970 0.12Table 4: Interlanguage links (initial and discov-ered) and their coverage in Wikipedia versions infour languages.5 Experiments and EvaluationsWe run our experiments on four languages: En-glish, Spanish, Romanian and Arabic.
For eachof these languages, we use a Wikipedia down-load from October 2008.
The articles were pre-processed using Wikipedia Miner (Milne, 2007)to extract structural information such as general-ity, and interlanguage links.
Furthermore, arti-cles were also processed to remove numerical con-tent, as well as any characters not included in thegiven language?s alphabet.
The content words arestemmed, and words shorter than three charactersare removed (a heuristic which we use as an ap-proximation for stopword removal).
Table 5 showsthe number of articles in each Wikipedia versionand the size of their vocabularies, as obtained af-ter the pre-processing step.Articles VocabularyEnglish 2, 221, 980 1, 231, 609Spanish 520, 154 406, 134Arabic 149, 340 216, 317Romanian 179, 440 623, 358Table 5: Number of articles and size of vocabularyfor the four Wikipedia versionsAfter pre-processing, the articles are indexedto generate the ESA concept vectors.
From eachWikipedia version, we also extract other featuresincluding article titles, interlanguage links, andWikipedia category graphs.
The interlanguagelinks are further processed to recover any missinglinks, as described in the previous section.5.1 DataFor the evaluation, we build several cross-lingualdatasets based on the standard Miller-Charles(Miller and Charles, 1998) and WordSimilarity-353 (Finkelstein et al, 2001) English word relat-edness datasets.The Miller-Charles dataset (Miller and Charles,1998) consists of 30-word pairs ranging from syn-onymy pairs (e.g., car - automobile) to completelyunrelated terms (e.g., noon - string).
The relat-edness of each word pair was rated by 38 hu-man subjects, using a scale from 0 (not-related)to 4 (perfect synonymy).
The dataset is avail-able only in English and has been widely usedin previous semantic relatedness evaluations (e.g.,(Resnik, 1995; Hughes and Ramage, 2007; Zeschet al, 2008)).The WordSimilarity-353 dataset (also known asFinkelstein-353) (Finkelstein et al, 2001) consistsof 353 word pairs annotated by 13 human experts,on a scale from 0 (unrelated) to 10 (very closelyrelated or identical).
The Miller-Charles set is asubset in the WordSimilarity-353 data set.
Unlikethe Miller-Charles data set, which consists only of1195Word pairEnglish coast - shore car - automobile brother - monkSpanish costa - orilla coche - automovil hermano - monjeArabic ?gA ?
- Z??A??PAJ?
- ?K.Q??J??
- I.?
@PRomanian t?a?rm - mal mas?fina?
- automobil frate - ca?luga?rTable 6: Word pair translation examplessingle words, the WordSimilarity-353 set alo fea-tures phrases (e.g., ?Wednesday news?
), thereforeposing an additional degree of difficulty for a re-latedness metric applied on this data.Native speakers of Spanish, Romanian and Ara-bic, who were also highly proficient in English,were asked to translate the words in the two datasets.
The annotators were provided one word pairat a time, and asked to provide the appropriatetranslation for each word while taking into accounttheir relatedness within the word pair.
The relat-edness was meant as a hint to disambiguate thewords, when multiple translations were possible.The annotators were also instructed not to usemulti-word expressions in their translations.
Theywere also allowed to use replacement words toovercome slang or culturally-biased terms.
For ex-ample, in the case of the word pair dollar-buck,annotators were allowed to use PAJKX2 as a transla-tion for buck.To test the ability of the bilingual judges to pro-vide correct translations by using this annotationsetting, we carried out the following experiment.We collected Spanish translations from five differ-ent human judges, which were then merged intoa single selection based on the annotators?
trans-lation agreement; the merge was done by a sixthhuman judge, who also played the role of adjudi-cator when no agreement was reached between theinitial annotators.Subsequently, five additional human experts re-scored the word-pair Spanish translations by usingthe same scale that was used in the construction ofthe English data set.
The correlation between the2Arabic for dinars ?
the commonly used currency in theMiddle East.relatedness scores assigned during this experimentand the scores assigned in the original English ex-periment was 0.86, indicating that the translationsprovided by the bilingual judges were correct andpreserved the word relatedness.For the translations provided by the five humanjudges, in more than 74% of the cases at least threehuman judges agreed on the same translation for aword pair.
When the judges did not provide iden-tical translations, they typically used a close syn-onym.
The high agreement between their trans-lations indicates that the annotation setting waseffective in pinpointing the correct translation foreach word, even in the case of ambiguous words.Motivated by the validation of the annotationsetting obtained for Spanish, we used only one hu-man annotator to collect the translations for Arabicand Romanian.
Table 6 shows examples of trans-lations in the three languages for three word pairsfrom our data sets.Using these translations, we create six cross-lingual data sets, one for each possible languagepair (English-Spanish, English-Arabic, English-Romanian, Spanish-Arabic, Spanish-Romanian,Arabic-Romanian).
Given a source-target lan-guage pair, a data set is created by first using thesource language for the first word and the targetlanguage for the second word, and then reversingthe order, i.e., using the source language for thesecond word and the target language for the firstword.
The size of the data sets is thus doubledin this way (e.g., the 30 word pairs in the EnglishMiller-Charles set are transformed into 60 wordpairs in the English-Spanish Miller-Charles set).5.2 ResultsWe evaluate the cross-lingual measure of related-ness on each of the six language pairs.
For com-parison purposes, we also evaluate the monolin-gual relatedness on the four languages.For the evaluation, we use the Pearson (r)and Spearman (?)
correlation coefficients, whichare the standard metrics used in the past for theevaluation of semantic relatedness (Finkelstein et1196al., 2001; Zesch et al, 2008; Gabrilovich andMarkovitch, 2007).
While the Pearson correla-tion is highly dependent on the linear relationshipbetween the distributions in question, Spearmanmainly emphasizes the ability of the distributionsto maintain their relative ranking.Tables 7 and 8 show the results of the evalua-tions of the cross-lingual relatedness, when usingan ESA concept vector with a size of maximum10,000 concepts.3English Spanish Arabic RomanianMiller-CharlesEnglish 0.58 0.43 0.32 0.50Spanish 0.44 0.20 0.38Arabic 0.36 0.32Romanian 0.58WordSimilarity-353English 0.55 0.32 0.31 0.29Spanish 0.45 0.32 0.28Arabic 0.28 0.25Romanian 0.30Table 7: Pearson correlation for cross-lingual relatedness on the Miller-Charles andWordSimilarity-353 data setsEnglish Spanish Arabic RomanianMiller-CharlesEnglish 0.75 0.56 0.27 0.55Spanish 0.64 0.17 0.32Arabic 0.33 0.21Romanian 0.61WordSimilarity-353English 0.71 0.55 0.35 0.38Spanish 0.50 0.29 0.30Arabic 0.26 0.20Romanian 0.28Table 8: Spearman correlation for cross-lingual relatedness on the Miller-Charles andWordSimilarity-353 data setsAs a validation of our ESA implementation, wecompared the results obtained for the monolingualEnglish relatedness with other results reported inthe past for the same data sets.
Gabrilovich andMarkovitch (2007) reported a Spearman correla-tion of 0.72 for the Miller-Charles data set and0.75 for the WordSimilarity-353 data set, respec-3The concepts are selected in reversed order of theirweight inside the vector in the respective language.
Note thatthe cross-lingual mapping between the concepts in the ESAvectors is done after the selection of the top 10,000 conceptsin each language.tively.
Zesch et al (2008) reported a Spear-man correlation of 0.67 for the Miller-Charles set.These values are comparable to the Spearman cor-relation scores obtained in our experiments for theEnglish data sets (see Table 8), with a fairly largeimprovement obtained on the Miller-Charles dataset when using our implementation.6 DiscussionOverall, our method succeeds in capturing thecross-lingual semantic relatedness between words.As a point of comparison, one can use the mono-lingual measures of relatedness as reflected by thediagonals in Tables 7 and 8.Looking at the monolingual evaluations, the re-sults seem to be correlated with the Wikipedia sizefor the corresponding language, with the Englishmeasure scoring the highest.
These results are notsurprising, given the direct relation between theWikipedia size and the sparseness of the ESA con-cept vectors.
A similar trend is observed for thecross-lingual relatedness, with higher results ob-tained for the languages with large Wikipedia ver-sions (e.g., English-Spanish), and lower results forthe languages with a smaller size Wikipedia (e.g.,Arabic-Spanish).For comparison, we ran two additional experi-ments.
In the first experiment, we compared thecoverage of our cross-lingual relatedness methodto a direct use of the translation links available inWikipedia.
The cross-lingual relatedness is turnedinto a monolingual relatedness by using the in-terlanguage Wikipedia links to translate the firstof the two words in a cross-lingual pair into thelanguage of the second word in the pair.4 Fromthe total of 433 word pairs available in the twodata sets, this method can produce translationsfor an average of 103 word pairs per languagepair.
This means that the direct Wikipedia inter-language links allow the cross-lingual relatednessmeasure to be transformed into a monolingual re-latedness in about 24% of the cases, which is alow coverage compared to the full coverage thatcan be obtained with our cross-lingual method ofrelatedness.In an attempt to raise the coverage of the trans-lation, we ran a second experiment where we useda state-of-the-art translation engine to translate thefirst word in a pair into the language of the sec-4We use all the interlanguage links obtained by combiningthe initial and the discovered links, as described in Section 4.1197ond word in the pair.
We use Google Translate,which is a statistical machine translation enginethat relies on large parallel corpora, to find themost likely translation for a given word.
Unlikethe previous experiment, this time we can achievefull translation coverage, and thus we are able toproduce data sets of equal size that can be usedfor a comparison between relatedness measures.Specifically, using the translation produced by themachine translation engine for the first word in apair, we calculate the relatedness within the spaceof the language of the second word using a mono-lingual ESA also based on Wikipedia.
The resultsobtained with this method are compared againstthe results obtained with our cross-lingual ESA re-latedness.Using a Pearson correlation, our cross-lingualrelatedness method achieves an average scoreacross all six language pairs of 0.36 for the Miller-Charles data set and 0.30 for the WordSimilarity-353 data set,5 which is higher than the 0.33 and0.28 scores achieved for the same data sets whenusing a translation obtained with Google Trans-late followed by a monolingual measure of re-latedness.
These results are encouraging, alsogiven that the translation-based method is limitedto those language pairs for which a translation en-gine exists (e.g., Google Translate covers 40 lan-guages), whereas our method can be applied to anylanguage pair from the set of 250 languages forwhich a Wikipedia version exists.To gain further insights, we also determined theimpact of the vector length in the ESA conceptvector representation, by calculating the Pearsoncorrelation for vectors of different lengths.
Fig-ures 1 and 2 show the Pearson score as a func-tion of the vector length for the Miller-Charlesand WordSimilarity-353 data sets.
The plots showthat the cross-lingual measure of relatedness is notsignificantly affected by the reduction or increaseof the vector length.
Thus, the use of vectors oflength 10,000 (as used in most of our experiments)appears as a reasonable tradeoff between accuracyand performance.Furthermore, by comparing the performance ofthe proposed Lesk-like model to the traditionalcosine-similarity (Figures 3 and 4), we note thatthe Lesk-like model outperforms the cosine modelon most language pairs.
We believe that this is5This average considers all the cross-lingual relatednessscores listed in Table 7; it does not include the monolingualscores listed on the table diagonal.0.10.20.30.40.50.65000  10000  15000  20000  25000  30000PearsoncorrelationVector lengthar?arar?enar?esar?roen?enen?eses?eses?roen?roro?roFigure 1: Pearson correlation vs. ESA vectorlength on the Miller-Charles data set0.10.20.30.40.50.65000  10000  15000  20000  25000  30000AveragePearsonVector Sizear?arar?enar?esar?roen?enen?eses?eses?roen?roro?roFigure 2: Pearson correlation vs. ESA vectorlength on the WordSimilarity-353 data setdue to the stricter correlation conditions imposedby the cosine-metric in such sparse vector-basedrepresentations, as compared to the more relaxedhypothesis used by the Lesk model.Finally, we also looked at the relation betweenthe number of interlanguage links found for theconcepts in a vector and the length of the vector.Figures 5 and 6 display the average number of in-terlanguage links as a function of the concept vec-tor length.By analyzing the effect of the average numberof interlanguage links found per word in the givendatasets (Figures 5 and 6), we notice that theselinks increase proportionally with the vector size,as expected.
However, this increase does not leadto any significant improvements in accuracy (Fig-ures 1 and 2).
This implies that while the presenceof interlanguage links is a prerequisite for the mea-11980.10.20.30.40.50.60.75000  10000  15000  20000  25000  30000PearsonVector Sizelsk(ar?ar)lsk(en?en)lsk(es?es)lsk(ro?ro)cos(ar?ar)cos(en?en)cos(es?es)cos(ro?ro)Figure 3: Lesk vs. cosine similarity for the Miller-Charles data set0.10.20.30.40.50.60.75000  10000  15000  20000  25000  30000PearsonVector Sizelsk(ar?ar)lsk(en?en)lsk(es?es)lsk(ro?ro)cos(ar?ar)cos(en?en)cos(es?es)cos(ro?ro)Figure 4: Lesk vs. cosine similarity for theWordSimilarity-353 data setsure of relatedness,6 their effect is only significantfor the top ranked concepts in a vector.
Therefore,increasing the vectors size to maximize the match-ing of the projected dimensions does not necessar-ily lead to accuracy improvements.7 Related WorkMeasures of word relatedness were found useful ina large number of natural language processing ap-plications, including word sense disambiguation(Patwardhan et al, 2003), synonym identification(Turney, 2001), automated essay scoring (Foltz etal., 1999), malapropism detection (Budanitsky andHirst, 2001), coreference resolution (Strube andPonzetto, 2006), and others.
Most of the work todate has focused on measures of word relatednessfor English, by using methods applied on knowl-6Two languages with no interlanguage links betweenthem will lead to a relatedness score of zero for any wordpair across these languages, no matter how strongly relatedthe words are.05001000150020005000  10000  15000  20000  25000  30000Number of interlanguagelinksVector lengthar?enar?esar?roen?esen?roes?roFigure 5: Number of interlanguage links vs. vec-tor length for the Miller-Charles data set050010001500200025003000350040005000  10000  15000  20000  25000  30000Number of interlanguagelinksVector lengthar?enar?esar?roen?esen?roes?roFigure 6: Number of interlanguage links vs. vec-tor length for the WordSimilarity-353 data setedge bases (Lesk, 1986; Wu and Palmer, 1994;Resnik, 1995; Jiang and Conrath, 1997; Hughesand Ramage, 2007) or on large corpora (Saltonet al, 1997; Landauer et al, 1998; Turney, 2001;Gabrilovich and Markovitch, 2007).Although to a lesser extent, measures of wordrelatedness have also been applied on other lan-guages, including German (Zesch et al, 2007;Zesch et al, 2008; Mohammad et al, 2007), Chi-nese (Wang et al, 2008), Dutch (Heylen et al,2008) and others.
Moreover, assuming resourcessimilar to those available for English, e.g., Word-Net structures or large corpora, the measures ofrelatedness developed for English can be in prin-ciple applied to other languages as well.All these methods proposed in the past havebeen concerned with monolingual word related-ness calculated within the boundaries of one lan-guage, as opposed to cross-lingual relatedness,which is the focus of our work.The research area closest to the task of cross-1199lingual relatedness is perhaps cross-language in-formation retrieval, which is concerned withmatching queries posed in one language to docu-ment collections in a second language.
Note how-ever that most of the approaches to date for cross-language information retrieval have been based ondirect translations obtained for words in the queryor in the documents, by using bilingual dictionar-ies (Monz and Dorr, 2005) or parallel corpora (Nieet al, 1999).
Such explicit translations can iden-tify a direct correspondence between words in twolanguages (e.g., they will find that fabbrica (It.
)and factory (En.)
are translations of each other),but will not capture similarities of a different de-gree (e.g., they will not find that lavoratore (It.
;worker in En.)
is similar to factory (En.
).Also related are the areas of word alignmentfor machine translation (Och and Ney, 2000),induction of translation lexicons (Schafer andYarowsky, 2002), and cross-language annotationprojections to a second language (Riloff et al,2002; Hwa et al, 2002; Mohammad et al,2007).
As with cross-language information re-trieval, these areas have primarily considered di-rect translations between words, rather than an en-tire spectrum of relatedness, as we do in our work.8 ConclusionsIn this paper, we addressed the problem ofcross-lingual semantic relatedness, which is acore task for a number of applications, includ-ing cross-language information retrieval, cross-language text classification, lexical choice for ma-chine translation, cross-language projections of re-sources and annotations, and others.We introduced a method based on concept vec-tors built from Wikipedia, which are mappedacross the interlanguage links available betweenWikipedia versions in multiple languages.
Ex-periments performed on six language pairs, con-necting English, Spanish, Arabic and Romanian,showed that the method is effective at captur-ing the cross-lingual relatedness of words.
Themethod was shown to be competitive when com-pared to methods based on a translation using thedirect Wikipedia links or using a statistical trans-lation engine.
Moreover, our method has wide ap-plicability across languages, as it can be used forany language pair from the set of 250 languagesfor which a Wikipedia version exists.The cross-lingual data sets introducedin this paper can be downloaded fromhttp://lit.csci.unt.edu/index.php/Downloads.AcknowledgmentsThe authors are grateful to Carmen Banea for herhelp with the construction of the data sets.
Thismaterial is based in part upon work supported bythe National Science Foundation CAREER award#0747340.
Any opinions, findings, and conclu-sions or recommendations expressed in this mate-rial are those of the authors and do not necessarilyreflect the views of the National Science Founda-tion.ReferencesS.
Bangalore, P. Haffner, and S. Kanthak.
2007.
Statis-tical machine translation through global lexical se-lection and sentence reconstruction.
In Proceedingsof the Annual Meeting of the Association of Compu-tational Linguistics, Prague, Czech Republic.A.
Budanitsky and G. Hirst.
2001.
Semantic distancein WordNet: An experimental, application-orientedevaluation of five measures.
In Proceedings of theNAACL Workshop on WordNet and Other LexicalResources, Pittsburgh.L.
Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,Z.
Solan, G. Wolfman, and E. Ruppin.
2001.
Plac-ing search in context: the concept revisited.
InWWW, pages 406?414.P.
Foltz, D. Laham, and T. Landauer.
1999.
Automatedessay scoring: Applications to educational technol-ogy.
In Proceedings of World Conference on Edu-cational Multimedia, Hypermedia and Telecommu-nications, Chesapeake, Virginia.E.
Gabrilovich and S. Markovitch.
2007.
Comput-ing semantic relatedness using wikipedia-based ex-plicit semantic analysis.
In Proceedings of the Inter-national Joint Conference on Artificial Intelligence,pages 1606?1611.A.
Gliozzo and C. Strapparava.
2006.
Exploiting com-parable corpora and bilingual dictionaries for cross-language text categorization.
In Proceedings of theConference of the Association for ComputationalLinguistics, Sydney, Australia.K.
Heylen, Y. Peirsman, D. Geeraerts, and D. Speel-man.
2008.
Modelling word similarity: an evalu-ation of automatic synonymy extraction algorithms.In Proceedings of the Sixth International LanguageResources and Evaluation, Marrakech, Morocco.T.
Hughes and D. Ramage.
2007.
Lexical semanticknowledge with random graph walks.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, Prague, Czech Republic.R.
Hwa, P. Resnik, A. Weinberg, and O. Kolak.
2002.Evaluating translational correspondence using anno-tation projection.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Lin-guistics (ACL 2002), Philadelphia, July.1200J.
Jiang and D. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProceedings of the International Conference on Re-search in Computational Linguistics, Taiwan.T.
K. Landauer, P. Foltz, and D. Laham.
1998.
Intro-duction to latent semantic analysis.
Discourse Pro-cesses, 25.M.E.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: How to tell a pinecone from an ice cream cone.
In Proceedings of theSIGDOC Conference 1986, Toronto, June.G.
Miller and W. Charles.
1998.
Contextual corre-lates of semantic similarity.
Language and Cogni-tive Processes, 6(1).D.
Milne.
2007.
Computing semantic relatedness us-ing wikipedia link structure.
In European LanguageResources Association (ELRA), editor, In Proceed-ings of the New Zealand Computer Science Re-search Student Conference (NZCSRSC 2007), NewZealand.S.
Mohammad, I. Gurevych, G. Hirst, and T. Zesch.2007.
Cross-lingual distributional profiles ofconcepts for measuring semantic distance.
InProceedings of the Joint Conference on Empir-ical Methods in Natural Language Processingand Computational Natural Language Learning(EMNLP/CoNLL-2007), Prague, Czech Republic.C.
Monz and B.J.
Dorr.
2005.
Iterative translationdisambiguation for cross-language information re-trieval.
In Proceedings of the 28th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, Salvador,Brazil.J.-Y.
Nie, M. Simard, P. Isabelle, and R. Durand.
1999.Cross-language information retrieval based on paral-lel texts and automatic mining of parallel texts fromthe Web.
In Proceedings of the 22nd annual inter-national ACM SIGIR conference on Research anddevelopment in information retrieval.F.
Och and H. Ney.
2000.
A comparison of align-ment models for statistical machine translation.
InProceedings of the 18th International Conference onComputational Linguistics (COLING 2000), Saar-brucken, Germany, August.S.
Patwardhan, S. Banerjee, and T. Pedersen.
2003.Using measures of semantic relatedness for wordsense disambiguation.
In Proceedings of the FourthInternational Conference on Intelligent Text Pro-cessing and Computational Linguistics, MexicoCity, February.P.
Resnik.
1995.
Using information content to evalu-ate semantic similarity.
In Proceedings of the 14thInternational Joint Conference on Artificial Intelli-gence, Montreal, Canada.E.
Riloff, C. Schafer, and D. Yarowsky.
2002.
In-ducing information extraction systems for new lan-guages via cross-language projection.
In Proceed-ings of the 19th International Conference on Com-putational Linguistics, Taipei, Taiwan, August.G.
Salton, A. Wong, and C.S.
Yang.
1997.
A vec-tor space model for automatic indexing.
In Read-ings in Information Retrieval, pages 273?280.
Mor-gan Kaufmann Publishers, San Francisco, CA.C.
Schafer and D. Yarowsky.
2002.
Inducing trans-lation lexicons via diverse similarity measures andbridge languages.
In Proceedings of the 6th Confer-ence on Natural Language Learning (CoNLL 2003),Taipei, Taiwan, August.M.
Strube and S. P. Ponzetto.
2006.
Wikirelate!
com-puting semantic relatedeness using Wikipedia.
InProceedings of the American Association for Artifi-cial Intelligence, Boston, MA.P.
Turney.
2001.
Mining the web for synonyms: PMI-IR versus LSA on TOEFL.
In Proceedings of theTwelfth European Conference on Machine Learning(ECML-2001), Freiburg, Germany.X.
Wang, S. Ju, and S. Wu.
2008.
A survey of chi-nese text similarity computation.
In Proceedings ofthe Asia Information Retrieval Symposium, Harbin,China.Z.
Wu and M. Palmer.
1994.
Verb semantics and lex-ical selection.
In Proceedings of the 32nd AnnualMeeting of the Association for Computational Lin-guistics, Las Cruces, New Mexico.T.
Zesch, I. Gurevych, and M. Mu?hlha?user.
2007.Comparing Wikipedia and German Wordnet byEvaluating Semantic Relatedness on MultipleDatasets.
In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.T.
Zesch, C. Mu?ller, and I. Gurevych.
2008.
UsingWiktionary for Computing Semantic Relatedness.In Proceedings of the American Association for Ar-tificial Intelligence, Chicago.1201
