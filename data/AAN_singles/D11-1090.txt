Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 970?979,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsEnhancing Chinese Word Segmentation Using Unlabeled DataWeiwei Sun??
and Jia Xu?
?Department of Computational Linguistics, Saarland University?German Research Center for Artificial Intelligence (DFKI)D-66123, Saarbru?cken, Germanywsun@coli.uni-saarland.de, Jia.Xu@dfki.deAbstractThis paper investigates improving supervisedword segmentation accuracy with unlabeleddata.
Both large-scale in-domain data andsmall-scale document text are considered.
Wepresent a unified solution to include featuresderived from unlabeled data to a discrimina-tive learning model.
For the large-scale data,we derive string statistics from Gigaword toassist a character-based segmenter.
In addi-tion, we introduce the idea about transductive,document-level segmentation, which is de-signed to improve the system recall for out-of-vocabulary (OOV) words which appear morethan once inside a document.
Novel features1result in relative error reductions of 13.8% and15.4% in terms of F-score and the recall ofOOV words respectively.1 IntroductionChinese sentences are written in continuous se-quence of characters without explicit delimiters suchas space characters.
To find the basic language units,i.e.
words, segmentation is a necessary initial stepfor Chinese language processing.
Previous researchshows that word segmentation models trained on la-beled data are reasonably accurate.
In this paper,we investigate improving supervised word segmen-tation with unlabeled data.We distinguish three types of unlabeled data,namely large-scale in-domain data, out-of-domaindata and small-scale document text.
Both large-scale1You can download our derived features athttp://www.coli.uni-saarland.de/?wsun/semi-cws-feats-emnlp11.tgz.in-domain and out-of-domain data is popular for en-hancing NLP tasks.
Learning from these two typesof unlabeled data normally involves semi-supervisedlearning.
The difference between them is that out-of-domain data is usually used for domain adapta-tion.
For a number of NLP tasks, there are relativelylarge amounts of labeled training data.
In this sit-uation, supervised learning can provide competitiveresults, and it is difficult to improve them any furtherby using extra unlabeled data.
Chinese word seg-mentation is one of this kind of tasks, since severallarge-scale manually annotated corpora are publiclyavailable.
In this work, we first exploit unlabeled in-domain data to improve strong supervised models.We leave domain adaptation for our future work.We introduce the third type of unlabeled data witha transductive learning, document-level view.
Manyapplications of word segmentation involve process-ing a whole document, such as information retrieval.In this situation, the text of the current documentcan provide additional useful information to seg-ment a sentence.
Take the word ???
?/elastane?for example2.
As a translated terminology word, itlacks compositionality.
Moreover, this word appearsrarely in general texts.
As a result, if it does not ap-pear in the training data, it is very hard for statis-tical models to recognize this word.
Nevertheless,when we deal with an article discussing an elastanecompany, this word may appear more than once inthis article, and the document information can helprecognize this word.
This idea is closely related totransductive learning in the sense that the segmen-tation model knows something about the problem it2This example is from an article indexed as chtb 0041 in thePenn Chinese Treebank corpus.970is going to resolve.
In this work, we are also con-cerned with enhancing word segmentation with thedocument information.We present a unified ?feature engineering?
ap-proach for learning segmentation models from bothlabeled and unlabeled data.
Our method is a simpletwo-stage process.
First, we use unannotated corpusto extract string and document information, and thenwe use these information to construct new statistics-based and document-based feature mapping for adiscriminative word segmenter.
We are relying onthe ability of discriminative learning method to iden-tify and explore informative features, which playcentral role to boost the segmentation performance.This simple solution has been shown effective fornamed entity recognition (Miller et al, 2004) anddependency parsing (Koo et al, 2008).
In their im-plementations, word clusters derived from unlabeleddata are imported as features to discriminative learn-ing approaches.To demonstrate the effectiveness of our approach,we conduct experiments on the Penn Chinese Tree-bank (CTB) data.
CTB is a collection of docu-ments which are separately annotated.
This anno-tation style allows us to evaluate our transductivesegmentation method.
Our experiments show thatboth statistics-based and document-based featuresare effective in the word segmentation application.In general, the use of unlabeled data can be moti-vated by two concerns: First, given a fixed amountof labeled data, we might wish to leverage unla-beled data to improve the performance of a super-vised model.
Second, given a fixed target perfor-mance level, we might wish to use unlabeled datato reduce the amount of annotated data necessaryto reach this target.
We show that our approachyields improvements for fixed data sets, even whenlarge-scale labeled data is available.
The new fea-tures result in relative error reductions of 13.8% and15.4% in terms of the balanced F-score and the re-call of out-of-vocabulary (OOV) words respectively.By conducting experiments on data sets of varyingsizes, we demonstrate that for fixed levels of perfor-mance, the new features derived from unlabeled datacan significantly reduce the need of labeled data.The remaining part of the paper is organized asfollows.
Section 2 describes the details of our sys-tem, especially the design of the derived features.B Current character is the start of a word con-sisting of more than one character.E Current character is the end of a word con-sisting of more than one character.I Current character is a middle of a word con-sisting of more than two characters.S Current character is a word consisting ofonly one character.Table 1: The start/end representation.Section 3 presents experimental results and empir-ical analysis.
Section 4 reviews the related work.Section 5 concludes the paper.2 Method2.1 Discriminative Character-based WordSegmentationThe Character-based approach is a dominant wordsegmentation solution for Chinese text process-ing.
This approach treats word segmentation as asequence tagging problem, assigning labels to thecharacters indicating whether a character locates atthe beginning of, inside or at the end of a word.
Thischaracter-by-character method was first proposedby (Xue, 2003), and a number of discriminativesequential learning algorithms have been exploited,including structured perceptron (Jiang et al, 2009),the Passive-Aggressive algorithm (Sun, 2010),conditional random fields (CRFs) (Tseng et al,2005), and latent variable CRFs (Sun et al, 2009).In this work, we use the Start/End representation toexpress the position information of every character.Table 2.1 shows the meaning of each characterlabel.
For example, the target label representationof the book title ??????????
?/The Se-cret Journal of Premier Zhao Ziyang?
is as follows.?
?
?
?
?
?
?
?
?
?B I E B E S B E B EKey to our approach is to allow informative fea-tures derived from unlabeled data to assist the seg-menter.
In our experiments, we employed threedifferent feature sets: a baseline feature set whichdraws upon ?normal?
information from labeledtraining data, a statistics-based feature set that usesstatistical information derived from a large-scale in-domain corpus, and a document-based feature set971that uses information encoded in the surroundingtext.2.2 Baseline FeaturesIn this work, to train a good traditional supervisedsegmenter, our baseline feature templates includesthe ones described in (Sun et al, 2009; Sun, 2010).These features are divided into two types: char-acter features and word type features.
Note thatthe word type features are indicator functions thatfire when the local character sequence matches aword uni-gram or bi-gram.
Dictionary containingword uni-grams and bi-grams is collected from thetraining data.
To conveniently illustrate, we de-note a candidate character token ci with a context...ci?1cici+1.... We use c[s:e] to express a string thatstarts at the position s and ends at the position e.For example, c[i:i+1] expresses a character bi-gramcici+1.
The character features are listed below.?
Character uni-grams: cs (i?
3 < s < i+ 3)?
Character bi-grams: cscs+1 (i?3 < s < i+3)?
Whether cs and cs+1 are identical, for i ?
2 <s < i+ 2.?
Whether cs and cs+2 are identical, for i ?
4 <s < i+ 2.The word type features are listed as follows.?
The identity of the string c[s:i] (i?
6 < s < i),if it matches a word from the list of uni-gramwords;?
The identity of the string c[i:e] (i < e < i + 6),if it matches a word; multiple features could begenerated.?
The identity of the bi-gram c[s:i?1]c[i:e] (i?6 <s, e < i+6), if it matches a word bi-gram fromthe list of uni-gram words.?
The identity of the bi-gram c[s:i]c[i+1:e] (i?6 <s, e < i + 6), if it matches a word bi-gram;multiple features could be generated.Idiom In linguistics, idioms are usually presumedto be figures of speech contradicting the principle ofcompositionality.
As a result, it is very hard to rec-ognize out-of-vocabulary idioms for word segmen-tation.
Nonetheless, the lexicon of idioms can betaken as a close set, which helps resolve the problemwell.
In our previous work (Sun, 2011), we collect12992 idioms from several free online Chinese dic-tionaries.
This linguistic resource is publicly avail-able3.
In this paper, we use this idiom dictionary toderive the following feature.?
Does ci locate at the beginning of, inside orat the end of an idiom?
If the string c[s:i](s < i) matches an item from the idiom lexi-con, the feature template receives a string value?E-IDIOM?.
Similarly, we can define when thisfeature ought to be set to ?B-IDIOM?
or ?I-IDIOM?.
Note that all idioms are larger thanone character, so there is no ?S-IDIOM?
fea-ture here.2.3 Statistics-based FeaturesIn order to distill information from unlabeled data,we borrow ideas from some previous research onunsupervised word segmentation.
The statistical in-formation acquired from a relatively large amountof unlabeled data are designed as features correlatedwith the position where a character locates in a wordtoken.
These features are based on three widely usedcriteria.2.3.1 Mutual InformationEmpirical mutual information is widely used inNLP.
Informally, mutual information compares theprobability of observing x and y together with theprobabilities of observing x and y independently.
Ifthere is a genuine association between x and y, theI(x, y) = log p(x,y)p(x)p(y) should be greater than 0.Some previous work claimed that the largerthe mutual information between two consecutivestrings, the higher the possibility of the two stringsbeing combined together.
We adopt this idea in ourcharacter-based segmentation model.
The empiri-cal mutual information between two character bi-grams is computed by counting how often they ap-pear in the large-scale unlabeled corpus.
Given a3http://www.coli.uni-saarland.de/?wsun/idiom.txt.972Chinese character string c[i?2:i+1], the mutual infor-mation between substrings c[i?2:i?1] and c[i:i+1] iscomputed as:MI(c[i?2:i?1], c[i:i+1]) = logp(c[i?2:i+1])p(c[i?2:i?1])p(c[i:i+1])For each character ci, we incorporate the MI of thecharacter bi-grams into our model.
They include,?
MI(c[i?2:i?1], c[i:i+1]),?
MI(c[i?1:i], c[i+1:i+2]).2.3.2 Accessor VarietyWhen a string appears under different linguisticenvironments, it may carry a meaning.
This prin-ciple is introduced as the accessor variety criterionfor identifying meaningful Chinese words in (Fenget al, 2004).
This criterion evaluates how indepen-dently a string is used, and thus how likely it is thatthe string can be a word.
Given a string s, whichconsists of l (l ?
2) characters, we define the leftaccessor variety of Llav(s) as the number of distinctcharacters that precede s in a corpus.
Similarly, theright accessor variety Rlav(s) is defined as the num-ber of distinct characters that succeed s.We first extract all strings whose length are be-tween 2 and 4 from the unlabeled data, and calculatetheir accessor variety values.
For each character ci,we then incorporate the following information intoour model,?
Accessor variety of strings with length 4:L4av(c[i:i+3]), L4av(c[i+1:i+4]), R4av(c[i?3:i]),R4av(c[i?4:i?1]);?
Accessor variety of strings with length 3:L3av(c[i:i+2]), L3av(c[i+1:i+3]), R3av(c[i?2:i]),R3av(c[i?3:i?1]);?
Accessor variety of strings with length 2:L2av(c[i:i+1]), L2av(c[i+1:i+2]), R2av(c[i?1:i]),R2av(c[i?2:i?1]).2.3.3 Punctuation as Anchor WordsPunctuation marks are symbols that indicate thestructure and organization of written language, aswell as intonation and pauses to be observed whenreading aloud.
Punctuation marks can be taken asperfect word delimiters, and can be used as anchorwords to harvest lexical knowledge.
The preced-ing and succeeding strings of punctuations carry ad-ditional wordbreak information, since punctuationsshould be segmented as a word.
Note that such in-formation is biased because not all words can appearbefore or after punctuations.
For example, punctua-tions can not be followed by particles, such as ???,???
and ???
which are indicators of aspects.
Nev-ertheless, our experiments will show this kind of in-formation is still useful for word segmentation.When a string appears many times preceding orsucceeding punctuations, there tends to be word-breaks succeeding or preceding that string.
To uti-lize the wordbreak information provided by punctu-ations, we extract all strings with length l(2 ?
l ?4) which precede or succeed punctuations in the un-labeled data.
We define the left punctuation varietyof Llpv(s) as the number of times a punctuation pre-cedes s in a corpus.
Similarly, the right punctua-tion variety Rlpv(s) is defined as the number of howmany times a punctuation succeeds s. These twovariables evaluate how likely a string can be sepa-rated at its start or end positions.We first gather all strings surrounding punctua-tions in the unlabeled data, and calculate their punc-tuation variety values.
The length of each string isalso restricted between 2 and 4.
For each charac-ter ci, we import the following information into ourmodel,?
Punctuation variety of strings with length 4:L4pv(c[i:i+3]), R4pv(c[i?3:i]);?
Punctuation variety of strings with length 3:L3pv(c[i:i+2]), R3pv(c[i?2:i]);?
Punctuation variety of strings with length 2:L2pv(c[i:i+1]), R2pv(c[i?1:i]).Punctuations can be viewed as mark-up?s of Chi-nese text.
Our motivation to use the punctuation in-formation to assist a word segmenter is similiar to(Spitkovsky et al, 2010) in a way to explore ?artifi-cial?
word (or phrase) break symbols.
In their work,four common HTML tags are successfully used asraw phrase bracketings to improve unsupervised de-pendency parsing.9732.3.4 Binary or Numeric FeaturesThe derived information introduced above is allexpressed as real values.
The natural way to in-corporate these statistics into a discriminative learn-ing model is to directly use them as numeric fea-tures.
However, our experiments show that this sim-ple choice does not work well.
The reason is thatthese statistics actually behave non-linearly to pre-dict character labels.
For each type of statistics, oneweight alone cannot capture the relation between itsvalue and the possibility that a string forms a word.Instead, we represent these statistics as discrete fea-tures.For the mutual information, this is done by round-ing down decimal number.
The integer part of eachMI value is used as a string feature.
For the ac-cessor variety and punctuation variety information,since their values are integer, we can directly usethem as string features.
The accessor variety andpunctuation variety could be very large, so we setthresholds to cut off large values to deal with thedata sparse problem.
Specially, if an accessor va-riety value is greater than 50, it is incorporated asa feature ?> 50?
; if the value is greater than 30but not greater than 50, it is incorporated as a fea-ture ?30 ?
50?
; else the value is individually in-corporated as a string feature.
For example, if theleft accessory variety of a character bi-gram c[i:i+1]is 29, the binary feature ?L2av(c[i:i+1])=29?
will beset to 1, while other related binary features such as?L2av(c[i:i+1]) = 15?
or ?L2av(c[i:i+1]) > 50?
willbe set to 0.
Similarly, we can discretize the punc-tuation variety features.
However, we only set onethreshold, 30, for this value.
These thresholds canbe tuned by using held-out data.2.4 Document-based FeaturesIt is meaningless to derive statistics of a documentand use it for word segmentation, since most doc-uments are relatively short, and values are statisti-cally unreliable.
Our experiments confirm this idea.Instead, we propose the following binary featureswhich are based on the string count in the given doc-ument that is simply the number of times a givenstring appears in that document.
For each characterci, our document-based features include,?
Whether the string count of c[s:i] is equal to thatof c[s:i+1] (i ?
3 ?
s ?
i).
Multiple featuresare generated for different string length.?
Whether the string count of c[i:e] is equal to thatof c[i?1:e] (i ?
e ?
i + 3).
Multiple featuresare generated for different string length.The intuition is as follows.
The string counts ofc[s:i] and c[s:i+1] being equal means that when c[s:i]appears, it appears inside c[s:i+1].
In this case, c[s:i]is not independently used in this document, and thisfeature suggests the segmenter not assign a ?S?
or?E?
label to the character ci.
Similarly, the stringcounts of c[i:e] and c[i?1:e] being equal means c[i:e]is not independently used in this document, and thisfeature suggests segmenter not assign a ?S?
or ?B?label to ci.
We do not directly use the string countsto prevent a bias towards longer documents.3 Experiments3.1 SettingThe SIGHAN Bakeoffs provide several large-scalelabeled data for the research on Chinese word seg-mentation.
Although these data sets are labeled oncontinuous run texts, they do not contain the docu-ment boundary information.
CTB is a segmented,part-of-speech tagged, and fully bracketed corpusin the constituency formalism.
It is also an popu-lar data set to evaluate word segmentation methods,such as (Jiang et al, 2009; Sun, 2011).
CTB is acollection of documents which are separately anno-tated.
This annotation style allows us to calculatethe so-called document-based features and to furtherevaluate our approach.
In this paper, we use CTB 6.0as our main corpus and define the training, develop-ment and test sets according to the Chinese sub-taskof the CoNLL 2009 shared task4.
Table 2 shows thestatistics of our experimental settings.Data set # of sent.
# of words # of char.Training 22277 609060 1004266Devel.
1763 49646 83710Test 2557 73152 121008Table 2: Training, development and test data on CTB 6.04We would like to thank Prof. Nianwen Xue for the helpwith the division of the data974Chinese Gigaword is a comprehensive archiveof newswire text data that has been acquired overseveral years by the Linguistic Data Consortium(LDC).
The large-scale unlabeled data we use inour experiments comes from the Chinese Gigaword(LDC2005T14).
We choose the Mandarin news text,i.e.
Xinhua newswire.
This data covers all newspublished by Xinhua News Agency (the largest newsagency in China) from 1991 to 2004, which containsover 473 million characters.F-score is used as the accuracy measure.
Defineprecision p as the percentage of words in the decoderoutput that are segmented correctly, and recall r asthe percentage of gold standard output words that arecorrectly segmented by the decoder.
The (balanced)F-score is 2pr/(p + r).
We also report the recall ofOOV words.
Note that, all idioms in our extra idiomlexicon are added into the in-vocabulary word list.CRFsuite (Okazaki, 2007) is an implementationof Conditional Random Fields (CRFs) (Laffertyet al, 2001) for labeling sequential data.
It is aspeed-oriented implementation, which is written inpure C. In our experiments, we use this toolkit tolearn global linear models for segmentation.
We usethe stochastic gradient descent algorithm to resolvethe optimization problem, and set default values forother learning parameters.3.2 Main ResultsTable 3 summarizes the segmentation results on thedevelopment data with different configurations, rep-resenting a few choices between baseline, statistics-based and document-based feature sets.
In this table,the symbol ?+?
means features of current configura-tion contains both the baseline features and new fea-tures for semi-supervised or transductive learning.From this table, we can clearly see the impact of fea-tures derived from the large-scale unlabeled data andthe current document.
Comparison between the per-formance of the baseline and ?+MI?
shows that thewidely used mutual information is not helpful.
Bothgood segmentation techniques and valuable labeledcorpora have been developed, and pure supervisedsystems can provide strong performance.
It is nota trial to design new features to enhance supervisedmodels.There are significant increases when accessor va-riety features and punctuation variety features areDevel.
P R F?=1 RoovBaseline 95.41 95.52 95.46 77.68+MI 95.50 95.48 95.49 77.98+AV(2) 95.85 96.04 95.94 79.31+AV(2,3) 95.95 96.19 96.07 80.61+AV(2,3,4) 96.14 95.99 96.07 81.83+PU(2) 95.86 96.07 95.97 79.70+PU(2,3) 95.98 96.25 96.11 80.42+PU(2,3,4) 96.00 96.19 96.10 80.53+MI+AV(2,3,4)+PU(2,3,4)96.17 96.22 96.19 80.42+DOC 95.69 95.64 95.66 79.89+MI+AV(2,3,4)+PU(2,3,4)+DOC96.21 96.23 96.22 81.75Table 3: Segmentation performance with different featuresets on the development data.
Abbreviations: MI=mutualinformation; AV=accessor variety; PU=punctuation va-riety; DOC=document features.
The numbers in eachbracket pair are the lengths of strings.
For example,PU(2,3) means punctuation variety features of characterbi-grams and tri-grams are added.separately added.
Extending the length of neigh-boring string helps a little from 2 to 3.
Al-though the OOV recall increases when the lengthis extended from 3 to 4, there is no improve-ment of the overall balanced F-score.
Theline ?+MI+AV(2,3,4)+PU(2,3,4)?
shows the perfor-mance when all statistics-based features are added.The combination of the ?AV?
and ?PU?
featuresgives further helps.
This system can be seen as apure semi-supervised system.
The line ?+DOC?
isthe result when document-based features are added.In spite of its simplicity, the document-based fea-tures can help the task.
However, when we combinestatistics-based features with document-based fea-tures, we cannot get further improvement in termsof F-score.Table 4 shows the segmentation perfor-mance on the test data set.
The final re-sults of our system are achieved with the?+MI+AV(2,3,4)+PU(2,3,4)+DOC?
feature config-uration.
The new features result in relative errorreductions of 13.8% and 15.4% in terms of thebalanced F-score and the recall of OOV wordsrespectively.97587 8889 9091 9293 9495 9697100  200  300  400  500  600  700  800  900  1000F-scoreTraining data size (thousands of characters)Baseline features+Statistics-based features+Document-based featuresAll features687072747678808284100  200  300  400  500  600  700  800  900  1000OOVRecall(%)Training data size (thousands of characters)Baseline features+Statistics-based features+Document-based featuresAll featuresFigure 1: The learning curves of different models.-15-10-5 05 1015 20255  10  15  20  25  30ScoreFeature valueLabel ?B?-20-15-10-50 510 15205  10  15  20  25  30ScoreFeature valueLabel ?I?-30-25-20-15-10-50 5105  10  15  20  25  30ScoreFeature valueLabel ?E?-15-10-5 05 10155  10  15  20  25  30ScoreFeature valueLabel ?S?-14-12-10-8-6-4-2 02 45  10  15  20  25  30ScoreFeature valueLabel ?B?-10-50 510 1520 25305  10  15  20  25  30ScoreFeature valueLabel ?I?-5 05 1015 20255  10  15  20  25  30ScoreFeature valueLabel ?E?-25-20-15-10-5 05 105  10  15  20  25  30ScoreFeature valueLabel ?S?Figure 2: Scatter plot of feature score against feature value.
The left side shows is L2pv(c[i:i+1] feature while the rightside is the R2pv(c[i:i+1] feature.Test P R F?=1 RoovBaseline 95.21 94.90 95.06 75.52Final 95.86 95.62 95.74 79.28Table 4: Segmentation performance on the test data.3.3 Learning CurvesWe performed additional experiments to evaluate theeffect of the derived features as the amount of train-ing data is varied.
Figure 1 displays the F-scoreand the OOV recall of systems with different featuresets when trained on smaller portions of the labeleddata.
Note that there is no change in the configura-tion of the unlabeled data.
We can clearly see thatthe derived features obtain consistent gains regard-less of the size of the training set.
The improvementis more significant when little labeled data is ap-plied.
Both statistics-based features and document-based features can help improve the overall perfor-mance.
Especially, they can help to recognize moreunknown words, which is important for many appli-cations.
The F-score of semi-supervised models, i.e.models trained with statistics-based features, doesnot achieve further improvement when document-based features are added.
Nonetheless, the OOV re-call obtains slightly improvements.It is interesting to consider the amount by whichderived features reduce the need for supervised data,given a desired level of accuracy.
The change ofthe F-score in Figure 1 suggests that derived fea-tures reduce the need for supervised data by roughlya factor of 2.
For example, the performance of themodel with extra features trained on 500k characters976is slightly higher than the performance of the modelwith only baseline features trained on the whole la-beled data.3.4 Feature AnalysisWe discussed the choice of using binary or numericfeatures in Section 2.3.4.
In our experiment, whenthe accessor variety and punctuation variety infor-mation are integrated as numeric features, they donot contribute.
To show the non-linear way thatthese features contribute to the prediction problem,we present the scatter plots of the score of eachfeature (i.e.
the weight multiply the feature value)against the value of the feature.
Figure 2 showsthe relation between the score and the value ofthe punctuation variety features.
For example, theweight of the binary feature ?L2pu(c[i:i+1]) = 26combined with the label ?B?
learned by the finalmodel is 0.815141, so the score of this combina-tion is 0.815141 ?
26 = 21.193666 and a point(26, 21.193666) is drawn.
These plots indicate thepunctuation variety features contribute to the finalmodel in a very complicated way.
It is impossibleto use one weight to capture it.
The accessor va-riety features affect the model in the same way, sowe do not give detailed discussions.
We only showthe same scatter plot of the L2av(c[i:i+1]) feature tem-plate in Figure 3.-20-15-10-50 5105  10  15  20  25  30ScoreFeature valueLabel ?B?-10-8-6-4-2 02 46 8105  10  15  20  25  30ScoreFeature valueLabel ?I?-10-8-6-4-2 02 46 85  10  15  20  25  30ScoreFeature valueLabel ?E?-5051015205  10  15  20  25  30ScoreFeature valueLabel ?S?Figure 3: Scatter plot of feature score against featurevalue for L2av(c[i:i+1]).4 Related WorkXu et al (2008) presented a Bayesian semi-supervised approach to derive task-oriented wordsegmentation for machine translation (MT).
Thismethod learns new word types and word distribu-tions on unlabeled data by considering segmentationas a hidden variable in MT.
Different from their con-cern, our focus is general word segmentation.The ?feature-engineering?
semi-supervised ap-proach has been successfully applied to named en-tity recognition (Miller et al, 2004) and depen-dency parsing (Koo et al, 2008).
These two papersdemonstrated the effectiveness of using word clus-ters as features in discriminative learning.
More-over, Turian et al (2010) compared different wordclustering algorithms and evaluated their effect onboth named entity recognition and text chunking.As mentioned earlier, the feature design is in-spired by some previous research on word segmen-tation.
The accessor variety criterion is proposed toextract word types, i.e.
the list of possible words,in (Feng et al, 2004).
Different from their work,our method resolves the segmentation problem ofrunning texts, in which this criterion is used to de-fine features correlated with the character positionlabels.
Li and Sun (2009) observed that punctuationsare perfect delimiters which provide useful informa-tion for segmentation.
Their method can be viewedas a self-training procedure, in which extra punctu-ation information is incorporated to filter out auto-matically predicted samples.
We use the punctua-tion information in a different way.
In our method,the counts of the preceding and succeeding stringsof punctuations are incorporated directly as featuresinto a supervised model.In machine learning, transductive learning is alearning framework that typically makes use of un-labeled data.
The goal of transductive learning isto only infer labels for the unlabeled data points inthe test set rather than to learn a general classifica-tion function that can be applied to any future datasets.
This means that the test data is known as apriori knowledge and can be used to construct bet-ter hypotheses.
Although the idea to explore thedocument-level information in our work is similarto transductive learning, we do not use state-of-the-art transductive learning algorithms which involvelearning when they meet the test data.
For real-worldapplications, our approach is efficient by avoidingre-training.9775 Conclusion and Future WorkIn this paper, we have presented a simple yet effec-tive approach to explore unlabeled data for Chineseword segmentation.
We are concerned with large-scale in-domain data and the document text.
Ex-periments show that our approach achieves substan-tial improvement over a competitive baseline.
Es-pecially, the informative features derived from un-labeled data lead to significant improvement of therecall of unknown words.
Our immediate concernfor future work is to exploit the out-of-domain datato improve the robustness of current word segmen-tation systems.
The idea would be to extract do-main information from unlabeled data and definethem as features in our unified approach.
The word-based approach is an alternative for word segmenta-tion.
This kind of segmenters sequentially predictswhether the local sequence of characters make up aword.
A natural avenue for future work is the exten-sion of our method to the word-based approach.
Theword segmentation task is similar to constituencyparsing, in the sense of finding boundaries of lan-guage units.
Another interesting question is whetherour method can be adapted to resolve constituencyparsing.AcknowledgmentsThe work is supported by the project TAKE (Tech-nologies for Advanced Knowledge Extraction),funded under contract 01IW08003 by the GermanFederal Ministry of Education and Research.
Theauthor is also funded by German Academic Ex-change Service (DAAD).ReferencesHaodi Feng, Kang Chen, Xiaotie Deng, and WeiminZheng.
2004.
Accessor variety criteria for Chi-nese word extraction.
Comput.
Linguist., 30:75?93.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.Automatic adaptation of annotation standards:Chinese word segmentation and pos tagging ?
acase study.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on NaturalLanguage Processing of the AFNLP, pages 522?530.
Association for Computational Linguistics,Suntec, Singapore.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency pars-ing.
In Proceedings of ACL-08: HLT, pages 595?603.
Association for Computational Linguistics,Columbus, Ohio.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In ICML ?01: Proceedings ofthe Eighteenth International Conference on Ma-chine Learning, pages 282?289.
Morgan Kauf-mann Publishers Inc., San Francisco, CA, USA.Zhongguo Li and Maosong Sun.
2009.
Punctuationas implicit annotations for Chinese word segmen-tation.
Comput.
Linguist., 35:505?512.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and dis-criminative training.
In Daniel Marcu Susan Du-mais and Salim Roukos, editors, HLT-NAACL2004: Main Proceedings, pages 337?342.
As-sociation for Computational Linguistics, Boston,Massachusetts, USA.Naoaki Okazaki.
2007.
Crfsuite: a fast implementa-tion of conditional random fields (crfs).Valentin I. Spitkovsky, Daniel Jurafsky, and HiyanAlshawi.
2010.
Profiting from mark-up: Hyper-text annotations for guided parsing.
In Proceed-ings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, pages 1278?1287.
Association for Computational Linguistics,Uppsala, Sweden.Weiwei Sun.
2010.
Word-based and character-based word segmentation models: Comparisonand combination.
In Coling 2010: Posters, pages1211?1219.
Coling 2010 Organizing Committee,Beijing, China.Weiwei Sun.
2011.
A stacked sub-word modelfor joint Chinese word segmentation and part-of-speech tagging.
In Proceedings of the ACL 2011Conference.
Association for Computational Lin-guistics, Portland, Oregon, United States.Xu Sun, Yaozhong Zhang, Takuya Matsuzaki,Yoshimasa Tsuruoka, and Jun?ichi Tsujii.
2009.
A978discriminative latent variable Chinese segmenterwith hybrid word/character information.
In Pro-ceedings of Human Language Technologies: The2009 Annual Conference of the North AmericanChapter of the Association for ComputationalLinguistics, pages 56?64.
Association for Com-putational Linguistics, Boulder, Colorado.Huihsin Tseng, Pichuan Chang, Galen Andrew,Daniel Jurafsky, and Christopher Manning.
2005.A conditional random field word segmenter.
In InFourth SIGHAN Workshop on Chinese LanguageProcessing.Joseph Turian, Lev-Arie Ratinov, and Yoshua Ben-gio.
2010.
Word representations: A simple andgeneral method for semi-supervised learning.
InProceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, pages384?394.
Association for Computational Linguis-tics, Uppsala, Sweden.Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-mann Ney.
2008.
Bayesian semi-supervised Chi-nese word segmentation for statistical machinetranslation.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics(Coling 2008), pages 1017?1024.
Coling 2008Organizing Committee, Manchester, UK.Nianwen Xue.
2003.
Chinese word segmentationas character tagging.
In International Journalof Computational Linguistics and Chinese Lan-guage Processing.979
