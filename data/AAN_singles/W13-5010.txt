Proceedings of the TextGraphs-8 Workshop, pages 70?78,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsGraph-based Approaches for Organization Entity Resolution in MapReduceHakan Kardes, Deepak Konidena, Siddharth Agrawal, Micah Huff and Ang Suninome Inc.Bellevue, WA, USA{hkardes,dkonidena,sagrawal,mhuff,asun}@inome.comAbstractEntity Resolution is the task of identifying whichrecords in a database refer to the same entity.
Astandard machine learning pipeline for the entity res-olution problem consists of three major components:blocking, pairwise linkage, and clustering.
Theblocking step groups records by shared properties todetermine which pairs of records should be exam-ined by the pairwise linker as potential duplicates.Next, the linkage step assigns a probability score topairs of records inside each block.
If a pair scoresabove a user-defined threshold, the records are pre-sumed to represent the same entity.
Finally, the clus-tering step turns the input records into clusters ofrecords (or profiles), where each cluster is uniquelyassociated with a single real-world entity.
This paperdescribes the blocking and clustering strategies usedto deploy a massive database of organization entitiesto power a major commercial People Search Engine.We demonstrate the viability of these algorithms forlarge data sets on a 50-node hadoop cluster.1 IntroductionA challenge for builders of databases whose informationis culled from multiple sources is the detection of dupli-cates, where a single real-world entity gives rise to mul-tiple records (see (Elmagarmid, 2007) for an overview).Entity Resolution is the task of identifying which recordsin a database refer to the same entity.
Online citationindexes need to be able to navigate through the differ-ent capitalization and abbreviation conventions that ap-pear in bibliographic entries.
Government agencies needto know whether a record for ?Robert Smith?
living on?Northwest First Street?
refers to the same person as onefor a ?Bob Smith?
living on ?1st St. NW?.
In a standardmachine learning approach to this problem all recordsfirst go through a cleaning process that starts with the re-moval of bogus, junk and spam records.
Then all recordsare normalized to an approximately common representa-tion.
Finally, all major noise types and inconsistenciesare addressed, such as empty/bogus fields, field duplica-tion, outlier values and encoding issues.
At this point, allrecords are ready for the major stages of the entity resolu-tion, namely blocking, pairwise linkage, and clustering.Since comparing all pairs of records is quadratic in thenumber of records and hence is intractable for large datasets, the blocking step groups records by shared proper-ties to determine which pairs of records should be exam-ined by the pairwise linker as potential duplicates.
Next,the linkage step assigns a score to pairs of records insideeach block.
If a pair scores above a user-defined thresh-old, the records are presumed to represent the same entity.The clustering step partitions the input records into setsof records called profiles, where each profile correspondsto a single entity.In this paper, we focus on entity resolution for the or-ganization entity domain where all we have are the orga-nization names and their relations with individuals.
Let?sfirst describe the entity resolution for organization names,and discuss its significance and the challenges in more de-tail.
Our process starts by collecting billions of personalrecords from three sources of U.S. records to power a ma-jor commercial People Search Engine.
Example fieldson these records might include name, address, birthday,phone number, (encrypted) social security number, rel-atives, friends, job title, universities attended, and orga-nizations worked for.
Since the data sources are hetero-geneous, each data source provides different aliases ofan organization including abbreviations, preferred names,legal names, etc.
For example, Person A might haveboth ?Microsoft?, ?Microsoft Corp?, ?Microsoft Corpo-ration?, and ?Microsoft Research?
in his/her profile?s or-ganization field.
Person B might have ?University ofWashington?, while Person C has ?UW?
as the organi-zation listed in his/her profile.
Moreover, some organi-zations change their names, or are acquired by other in-70stitutions and become subdivisions.
There are also manyorganizations that share the same name or abbreviation.For instance, both ?University of Washington?, ?Univer-sity of Wisconsin Madison?, ?University of Wyoming?share the same abbreviation, ?UW?.
Additionally, someof the data sources might be noisier than the others andthere might be different kind of typos that needs to beaddressed.Addressing the above issues in organization fields iscrucial for data quality as graphical representations of thedata become more popular.
If we show different represen-tations of the same organization as separate institutions ina single person?s profile, it will decrease the confidence ofa customer about our data quality.
Moreover, we shouldhave a unique representation of organizations in order toproperly answer more complicated graph-based queriessuch as ?how am I connected to company X?
?, or ?whoare my friends that has a friend that works at organizationX, and graduated from school Y?
?.We have developed novel and highly scalable com-ponents for our entity resolution pipeline which is cus-tomized for organizations.
The focus of this paper is thegraph-based blocking and clustering components.
In theremainder of the paper, we first describe these compo-nents in Section 2.
Then, we evaluate the performance ofour entity resolution framework using several real-worlddatasets in Section 3.
Finally, we conclude in Section 4.2 MethodologyIn this section, we will mainly describe the blocking andclustering strategies as they are more graph related.
Wewill also briefly mention our pairwise linkage model.The processing of large data volumes requires highlyscalable parallelized algorithms, and this is only possiblewith distributed computing.
To this end, we make heavyuse of the hadoop implementation of the MapReducecomputing framework, and both the blocking and cluster-ing procedures described here are implemented as a seriesof hadoop jobs written in Java.
It is beyond the scope ofthis paper to fully describe the MapReduce framework(see (Lin, 2010) for an overview), but we do discuss theways its constraints inform our design.
MapReduce di-vides computing tasks into a map phase in which the in-put, which is given as (key,value) pairs, is split up amongmultiple machines to be worked on in parallel and a re-duce phase in which the output of the map phase is putback together for each key to independently process thevalues for each key in parallel.
Moreover, in a MapRe-duce context, recursion becomes iteration.2.1 BlockingHow might we subdivide a huge number of organiza-tions based on similarity or probability scores when allwe have is their names and their relation with people?
Wecould start by grouping them into sets according to thewords they contain.
This would go a long way towardsputting together records that represent the same organiza-tion, but it would still be imperfect because organizationsmay have nicknames, abbreviations, previous names, ormisspelled names.
To enhance this grouping we couldconsider a different kind of information like soundex or asimilar phonetic algorithm for indexing words to addresssome of the limitations of above grouping due to typos.We can also group together the organizations which ap-pear in the same person?s profile.
This way, we will beable to block the different representations of the same or-ganization to some extent.
With a handful of keys like thiswe can build redundancy into our system to accommodatedifferent types of error, omission, and natural variability.The blocks of records they produce may overlap, but thisis desirable because it gives the clustering a chance to joinrecords that blocking did not put together.The above blocks will vary widely in size.
For exam-ple, we may have a small set of records containing theword ?Netflix?
which can then be passed along immedi-ately to the linkage component.
However, we may havea set of millions of records containing the word ?State?which still needs to be cut down to subsets with manage-able sizes, otherwise it will be again impractical to doall pairwise computations in this block.
One way to dothis is to find other common properties to further subdi-vide this set.
The set of all records containing not only?State?
but also a specific state name like ?Washington?is smaller than the set of all records containing the word?State?, and intuitively records in this set will be morelikely to represent the same organization.
Additionallywe could block together all the ?State?
records with thesame number of words, or combination of the initials ofeach word.
As with the original blocks, overlap betweenthese sub-blocks is desirable.
We do not have to be par-ticularly artful in our choice of sub-blocking criteria: anyproperty that seems like it might be individuating willdo.
As long as we have an efficient way to search thespace, we can let the data dynamically choose differentsub-blocking strategies for each oversize block.
To thisend, we use the ordering on block keys to define a bino-mial tree where each node contains a list of block keysand is the parent of nodes that have keys that come laterin the ordering appended to the list.
Figure 1 shows atree for the oversize top-level set tTkn1 with three sub-blocking tokens sTkn1 < sTkn2 < sTkn3.
With eachnode of the tree we can associate a block whose key isthe list of blocks keys in that node and whose records arethe intersection of the records in those blocks, e.g.
thetTkn1 ?
sTkn1 ?
sTkn2 node represents all the recordsfor organizations containing all these tokens.
Because thecardinality of an intersected set is less than or equal to thecardinalities of the sets that were intersected, every block71tTkn1tTkn1 ?
sTkn3tTkn1 ?
sTkn2tTkn1 ?
sTkn2 ?
sTkn3tTkn1 ?
sTkn1tTkn1 ?
sTkn1 ?
sTkn3tTkn1 ?
sTkn1 ?
sTkn2tTkn1 ?
sTkn1 ?
sTkn2 ?
sTkn3Figure 1: The root node of this tree represents an oversized block for the name Smith and the other nodes represent possiblesub-blocks.
The sub-blocking algorithm enumerates the tree breadth-first, stopping when it finds a correctly-sized sub-block.in the tree is larger than or equal to any of its children.
Wetraverse the tree breadth-first and only recurse into nodesabove the maximum block size.
This allows us to explorethe space of possible sub-blocks in cardinality order for agiven branch, stopping as soon as we have a small enoughsub-block.The algorithm that creates the blocks and sub-blockstakes as input a set of records and a maximum block sizeM .
All the input records are grouped into blocks definedby the top-level properties.
Those top-level blocks thatare not above the maximum size are set aside.
The re-maining oversized blocks are partitioned into sub-blocksby sub-blocking properties that the records they con-tain share, and those properties are appended to the key.The process is continued recursively until all sub-blockshave been whittled down to an acceptable size.
Thepseudo code of the blocking algorithm is presented inFigure 2.
We will represent the key and value pairsin the MapReduce framework as < key; value >.The input organization records are represented as <INPUT FLAG,ORG NAME >.
For the first iter-ation, this job takes the organization list as input.
Inlater iterations, the input is the output of the previousblocking iteration.
In the first iteration, the mapperfunction extracts the top-level and sub-level tokens fromthe input records.
It combines the organization nameand all the sub-level tokens in a temp variable callednewV alue.
Next, for each top-level token, it emits thistop-level token and the newValue in the following for-mat: < topToken, newV alue >.
For the later itera-tions, it combines each sub level token with the currentblocking key, and emits them to the reducer.
Also notethat the lexicographic ordering of the block keys allowsseparate mapper processes to work on different nodes in alevel of the binomial tree without creating redundant sub-blocks (e.g.
if one mapper creates a International ?
Busi-ness ?
Machines block another mapper will not create aInternational ?
Machines ?
Business one).
This is nec-essary because individual MapReduce jobs run indepen-dently without shared memory or other runtime commu-nication mechanisms.
In the reduce phase, all the recordswill be grouped together for each block key.
The reducerfunction iterates over all the records in a newly-createdsub-block, counting them to determine whether or not theblock is small enough or needs to be further subdivided.The blocks that the reducer deems oversized become in-puts to the next iteration.
Care is taken that the memoryrequirements of the reducer function are constant in thesize of a fixed buffer because otherwise the reducer runsout of memory on large blocks.
Note that we create ablack list from the high frequency words in organizationnames, and we don?t use these as top-level properties assuch words do not help us with individuating the records.More formally, this process can be understood in termsof operations on sets.
In a set of N records there are12N(N ?
1) unique pairs, so an enumeration over allof them is O(N2).
The process of blocking divides thisoriginal set into k blocks, each of which contains at mosta fixed maximum of M records.
The exhaustive compar-ison of pairs from these sets is O(k), and the constantfactors are tractable if we choose a small enough M .
Inthe worst case, all the sub-blocks except the ones with thevery longest keys are oversize.
Then the sub-blocking al-gorithm will explore the powerset of all possible block-ing keys and thus have exponential runtime.
However, asthe blocking keys get longer, the sets they represent getsmaller and eventually fall beneath the maximum size.
Inpractice these two countervailing motions work to keepthis strategy tractable.2.2 Pairwise Linkage ModelIn this section, we give just a brief overview of our pair-wise linkage system as a detailed description and evalua-tion of that system is beyond the scope of this paper.We take a feature-based classification approach to pre-dict the likelihood of two organization names < o1, o2>referring to the same organization entity.
Specifically, weuse the OpenNLP1 maximum entropy (maxent) packageas our machine learning tool.
We choose to work withmaxent because the training is fast and it has a good sup-port for classification.
Regarding the features, we mainlyhave two types: surface string features and context fea-tures.
Examples of surface string features are edit dis-1http://opennlp.apache.org/72Blocking Iterationsmap(key, value)if(key.equals(EntityName)String[] tokens ?
value.split(?
?
)sublevelTokenSet ?
?toplevelTokenSet ?
?for each (token ?
tokens)sublevelTokenSet.add(token.hashCode())if(notExist(blackList, token))toplevelTokenSet.add(token.hashCode())String newValue ?
valuefor each (sToken ?
sublevelTokenSet)newValue ?
newV alue.append(STR + sToken)for each (tToken ?
toplevelTokenSet)emit(tToken, newV alue)elseString[] keyTokens ?
key.split(STR)String[] valueTokens ?
value.split(STR)for each (token ?
valueTokens)if(token > keyTokens[keyTokens.length?
1])emit(key.append(STR + token), value)reduce(key,< iterable > values)buffer ?
?for each (value ?
values)buffer.add(value)if(buffer.length ?
MAXBLOCKSIZE)breakif(buffer.length ?
MAXBLOCKSIZE)for each (ele ?
buffer)emit(key, ele)for each (value ?
values)emit(key, value)elseif(buffer.length ?
1)blocks.append(key, buffer)Figure 2: Alg.1 - Blockingtance of the two names, whether one name is an abbre-viation of the other name, and the longest common sub-string of the two names.
Examples of context features arewhether the two names share the same url and the numberof times that the two names co-occur with each other in asingle person record.2.3 ClusteringIn this section, we present our clustering approach.
Let?s,first clarify a set of terms/conditions that will help us de-scribe the algorithms.Definition (Connected Component): Let G = (V,E)be an undirected graph where V is the set of vertices andE is the set of edges.
C = (C1, C2, ..., Cn) is the set ofdisjoint connected components in this graph where (C1?C2?
... ?
Cn) = V and (C1 ?
C2 ?
... ?
Cn) = ?.
Foreach connected component Ci ?
C, there exists a path inG between any two vertices vk and vl where (vk, vl) ?Ci.
Additionally, for any distinct connected componentsClustTransitiveClosure Edge ListNode -ClusterIDmappinganyCluster >maxSizenoyesExtract PairsFigure 3: Clustering Component(Ci, Cj) ?
C, there is no path between any pair vk andvl where vk ?
Ci, vl ?
Cj .
Moreover, the problem offinding all connected components in a graph is findingthe C satisfying the above conditions.
Definition (Component ID): A component id is aunique identifier assigned to each connected component.Definition (Max Component Size): This is the maxi-mum allowed size for a connected component.
Definition (Cluster Set): A cluster set is a set ofrecords that belong to the same real world entity.
Definition (Max Cluster Size): This is the maximumallowed size for a cluster.
Definition (Match Threshold): Match threshold is ascore where pairs scoring above this score are said to rep-resent the same entity.
Definition (No-Match Threshold): No-Match thresh-old is a score where pairs scoring below this score are saidto represent different entities.
Definition (Conflict Set): Each record has a conflictset which is the set of records that shouldn?t appear withthis record in any of the clusters.
The naive approach to clustering for entity resolu-tion is transitive closure by using only the pairs havingscores above the match threshold.
However, in practicewe might see many examples of conflicting scores.
Forexample, (a,b) and (b,c) pairs might have scores abovematch threshold while (a,c) pair has a score below no-match threshold.
If we just use transitive closure, wewill end up with a single cluster with these three records(a,b,c).
Another weakness of the regular transitive clo-sure is that it creates disjoint sets.
However, organiza-tions might share name, or abbreviation.
So, we need asoft clustering approach where a record might be in dif-ferent clusters.On the other hand, the large volume of our data re-quires highly scalable and efficient parallelized algo-rithms.
However, it is very hard to implement par-73TC-Iterate TC-Dedup Edge ListNode -ComponentIDmappingnewPair> 0noyesiterationID=1iterationID>1Figure 4: Transitive Closure Componentallelized clustering approaches with high precision forlarge scale graphs due to high time and space complexi-ties (Bansal, 2003).
So, we propose a two-step approachin order to build both a parallel and an accurate clusteringframework.
The high-level architecture of our cluster-ing framework is illustrated in Figure 3.
We first find theconnected components in the graph with our MapReducebased transitive closure approach, then further, partitioneach connected component in parallel with our novel softclustering algorithm, sClust.
This way, we first combinesimilar record pairs into connected components in an effi-cient and scalable manner, and then further partition eachconnected component into smaller clusters for better pre-cision.
Note that there is a dangerous phenomenon, blackhole entities, in transitive closure of the pairwise scores(Michelson, 2009).
A black hole entity begins to pullan inordinate amount of records from an increasing num-ber of different true entities into it as it is formed.
Thisis dangerous, because it will then erroneously match onmore and more records, escalating the problem.
Thus, bythe end of the transitive closure, one might end up withblack hole entities with millions of records belonging tomultiple different entities.
In order to avoid this problem,we define a black hole threshold, and if we end up witha connected component above the size of the black holethreshold, we increment the match threshold by a deltaand further partition this black hole with one more tran-sitive closure job.
We repeat this process until the sizesof all the connected components are below the black holethreshold, and then apply sClust on each connected com-ponent.
Hence at the end of the entire entity resolutionprocess, the system has partitioned all the input recordsinto cluster sets called profiles, where each profile corre-sponds to a single entity.2.4 Transitive ClosureIn order to find the connected components in a graph, wedeveloped the Transitive Closure (TC) module shown inFigure 4.
The input to the module is the list of all pairshaving scores above the match threshold.
As an outputfrom the module, what we want to obtain is the mappingfrom each node in the graph to its corresponding com-ponentID.
For simplicity, we use the smallest node id ineach connected component as the identifier of that com-ponent.
Thus, the module should output a mapping tablefrom each node in the graph to the smallest node id inits corresponding connected component.
To this end, wedesigned a chain of two MapReduce jobs, namely, TC-Transitive Closure Iteratemap(key, value)emit(key, value)emit(value, key)reduce(key,< iterable > values)minV alue ?
values.next()if(minV alue < key)emit(key,minV alue)for each (value ?
values)Counter.NewPair.increment(1)emit(value,minV alue)(a) Transitive Closure - IterateTransitive Closure Dedupmap(key, value)emit(key.append(STR + value), null)reduce(key,< iterable > values)String[] keyTokens ?
key.split(STR)emit(keyTokens[0], keyTokens[1])(b) Transitive Closure - DedupFigure 5: Alg.3 - Transitive ClosureIterate, and TC-Dedup, that will run iteratively till wefind the corresponding componentIDs for all the nodesin the graph.TC-Iterate job generates adjacency lists AL =(a1, a2, ..., an) for each node v, and if the node id of thisnode vid is larger than the min node id amin in the adja-cency list, it first creates a pair (vid, amin) and then a pairfor each (ai, amin) where ai ?
AL, and ai = amin.
Ifthere is only one node in AL, it means we will generatethe pair that we have in previous iteration.
However, ifthere is more than one node in AL, it means we mightgenerate a pair that we didn?t have in the previous itera-tion, and one more iteration is needed.
Please note that,if vid is smaller than amin, we don?t emit any pair.The pseudo code of TC-Iterate is given in Figure 5-(a).
For the first iteration, this job takes the pairs havingscores above the match threshold from the initial edge listas input.
In later iterations, the input is the output of TC-Dedup from the previous iteration.
We first start with theinitial edge list to construct the first degree neighborhoodof each node.
To this end, for each edge < a; b >, themapper emits both < a; b >, and < b; a > pairs so thata should be in the adjacency list of b and vice versa.
Inthe reduce phase, all the adjacent nodes will be groupedtogether for each node.
Reducers don?t receive the valuesin a sorted order.
So, we use a secondary sort approachto pass the values to the reducer in a sorted way with cus-tom partitioning (see (Lin, 2010) for details).
This way,the first value becomes the minValue.
If the minValue islarger than the key, we don?t emit anything.
Otherwise,74Bring together all edges for each partitionPhase-1map(key, value)if(key.equals(ConflationOutput))if ((value.score ?
NO MATCH THR)||(value.score ?
MATCH THR))emit(value.entity1, value)else //TCDedupOutputtemp.entity1 ?
valuetemp.entity2 ?
nulltemp.score ?
nullemit(key, temp)emit(value, temp)reduce(key,< iterable > values)valueList ?
?for each (value ?
values)if(value.entity2 = null)clusID ?
value.entity1elsevalueList.add(value)for each (value ?
valueList)emit(clusID, value)Phase-2map(key, value)emit(key, value)reduce(key,< iterable > values)valueList ?
?for each (value ?
values)valueList.add(value)emit(key, valueList)Figure 6: Alg.3 - Bring together all edges for each partitionwe first emit the < key;minV alue > pair.
Next, weemit a pair for all other values as < value;minV alue >,and increase the global NewPair counter by 1.
If thecounter is 0 at the end of the job, it means that we foundall the components and there is no need for further itera-tions.During the TC-Iterate job, the same pair might be emit-ted multiple times.
The second job, TC-Dedup, just dedu-plicates the output of the CCF-Iterate job.
This job in-creases the efficiency of TC-Iterate job in terms of bothspeed and I/O overhead.
The pseudo code for this job isgiven in Figure 5-(b).The worst case scenario for the number of necessaryiterations is d+1 where d is the diameter of the net-work.
The worst case happens when the min node inthe largest connected component is an end-point of thelargest shortest-path.
The best case scenario takes d/2+1iterations.
For the best case, the min node should be atthe center of the largest shortest-path.2.5 sClust: A Soft Agglomerative ClusteringApproachAfter partitioning the records into disjoint connectedcomponents, we further partition each connected compo-nent into smaller clusters with sClust approach.
sClustis a soft agglomerative clustering approach, and its maindifference from any other hierarchical clustering methodis the ?conflict set?
term that we described above.
Any ofthe conflicting nodes cannot appear in a cluster with thisapproach.
Additionally, the maximum size of the clusterscan be controlled by an input parameter.First as a preprocessing step, we have a two-stepMapReduce job (see Figure 6) which puts together andsorts all the pairwise scores for each connected compo-nent discovered by transitive closure.
Next, sClust jobtakes the sorted edge lists for each connected componentas input, and partitions each connected component in par-allel.
The pseudo-code for sClust job is given in Figure 7.sClust iterates over the pairwise scores twice.
During thefirst iteration, it generates the node structures, and conflictsets for each of these structures.
For example, if the pair-wise score for (a, b) pair is below the no-match threshold,node a is added to node b?s conflict set, and vice versa.
Bythe end of the first iteration, all the conflict sets are gen-erated.
Now, one more pass is needed to build the finalclusters.
Since the scores are sorted, we start from thehighest score to agglomeratively construct the clusters bygoing over all the scores above the match threshold.
Let?sassume we have a pair (a, b) with a score above the matchthreshold.
There might be 4 different conditions.
First,both node a and node b are not in any of the clusters yet.In this case, we generate a cluster with these two recordsand the conflict set of this cluster becomes the union ofconflict sets of these two records.
Second, node a mightalready be assigned to a set of clusters C?
while node b isnot in any of the clusters.
In these case, we add node b toeach cluster in C?
if it doesn?t conflict with b.
If there isno such cluster, we build a new cluster with nodes a andb.
Third is the opposite version of the second condition,and the procedure is the same.
Finally, both node a andnode b might be in some set of clusters.
If they alreadyappear in the same cluster, no further action needed.
Ifthey just appear in different clusters, these clusters willbe merged as long as there is no conflict between theseclusters.
If there are no such unconflicting clusters, weagain build a new cluster with nodes a and b.
This way,we go over all the scores above the match threshold andbuild the cluster sets.
Note that if the clusters are merged,their conflict sets are also merged.
Additionally, if themax cluster size parameter is defined, this condition isalso checked before merging any two clusters, or addinga new node to an existing cluster.75Clusteringmap(key, valueList)for each (value ?
valueList)if(value.score ?
MATCH THR)nodes.insert(value.entity1)nodes.insert(value.entity2)elsenode1Index ?
find(value.entity1, nodes)node2Index ?
find(value.entity2, nodes)nodes[node1Index].conflictSet.insert(node2Index)nodes[node2Index].conflictSet.insert(node1Index)for each (value ?
valueList)if(value.score ?
MATCH THR)node1Index ?
find(value.entity1, nodes)node2Index ?
find(value.entity2, nodes)node1ClusIDLength ?
nodes[node1Index].clusIDs.lengthnode2ClusIDLength ?
nodes[node2Index].clusIDs.lengthif((node1ClusIDLength = 0) && (node2ClusIDLength = 0))clusters[numClusters].nodes[0] ?
node1Indexclusters[numClusters].nodes[1] ?
node2Indexclusters[numClusters].confSet ?mergeSortedLists(nodes[node1Index].confSet, nodes[node2Index].confSet)nodes[node1Index].clusIDs.insert(numClusters)nodes[node2Index].clusIDs.insert(numClusters)numClusters++elseif(node1ClusIDLength = 0)for each (node2ClusID ?
nodes[node2Index].clusIDs)if(notContain(clusters[node2ClusID].confSet, node1Index))insertToSortedList(clusters[node2ClusID].nodes, node1Index)clusters[node2ClusID].confSet ?mergeSortedLists(clusters[node2ClusID].confSet, nodes[node1Index].confSet)nodes[node1Index].clusIDs.insert(node2ClusID)elseif(node2ClusIDLength = 0)for each (node1ClusID ?
nodes[node1Index].clusIDs)if(notContain(clusters[node1ClusID].confSet, node2Index))insertToSortedList(clusters[node1ClusID].nodes, node2Index)clusters[node1ClusID].confSet ?mergeSortedLists(clusters[node1ClusID].confSet, nodes[node2Index].confSet)nodes[node2Index].clusIDs.insert(node1ClusID)elseif(notIntersect(clusters[node1ClusID].clusIDs, clusters[node2ClusID].clusIDs))for each (node1ClusID ?
nodes[node1Index].clusIDs)for each (node2ClusID ?
nodes[node2Index].clusIDs)if( notIntersect(clusters[node1ClusID].confSet, clusters[node2ClusID].nodes) &&notIntersect(clusters[node2ClusID].confSet, clusters[node1ClusID].nodes) )clusters[node1ClusID].nodes ?mergeSortedList(clusters[node1ClusID].nodes, clusters[node2ClusID].nodes)clusters[node1ClusID].confSet ?mergeSortedLists(clusters[node1ClusID].confSet, clusters[node2ClusID].confSet)for each (nodeIndex ?
clusters[node2ClusID].nodes)nodes[nodeIndex].clusIDs.insert(node1ClusID)clusters[node2ClusID].isRemoved ?
trueclusters[node2ClusID].nodes ?
nullclusters[node2ClusID].confSet ?
nullFigure 7: Alg.4 - Clustering76(a) Block Dist.
(iterations)(b) Block Dist.
(overall)(c) Component & Cluster Size Dist.Figure 8: Size Distributions3 EvaluationIn this section, we present the experimental results forour entity resolution framework.
We ran the experimentson a hadoop cluster consisting of 50 nodes, each with 8cores.
There are 10 mappers, and 6 reducers availableat each node.
We also allocated 3 Gb memory for eachmap/reduce task.We used two different real-world datasets for our ex-periments.
The first one is a list of 150K organizationsalong with their aliases provided by freebase2.
By usingthis dataset, we both trained our pairwise linkage modeland measured the precision and recall of our system.
Werandomly selected 135K organizations from this list forthe training.
We used the rest of the organizations to mea-2http://www.freebase.com/precision recall f-measurePairwise Classifier 97 63 76Transitive Closure 64 98 77sClust 95 76 84Table 1: Performance Comparisonsure the performance of our system.
Next, we generatedpositive examples by exhaustively generating a pair be-tween all the aliases.
We also randomly generated equalnumber of negative examples among pairs of differentorganization alias sets.
We trained our pairwise classi-fier with the training set, then ran it on the test set andmeasured its performance.
Next, we extracted all the or-ganization names from this set, and ran our entire entityresolution pipeline on top of this set.
Table 1 presentsthe performance results.
Our pairwise classifier has 97%precision and 63% recall when we use a match thresholdof 0.65.
Using same match threshold, we then performedtransitive closure.
We also measured the precision andrecall numbers for transitive closure as it is the naive ap-proach for the entity resolution problem.
Since transitiveclosure merges records transitively, it has very high recallbut the precision is just 64%.
Finally, we performed oursClust approach with the same match threshold.
We setthe no-match threshold to 0.3.
The pairwise classifier hasslightly better precision than sClust but sClust has muchbetter recall.
Overall, sClust has a much better f-measurethan both the pairwise classifier and transitive closure.Second, we used our production set to show the viabil-ity of our framework.
In this set, we have 68M organiza-tion names.
We ran our framework on this dataset.
Block-ing generated 14M unique blocks, and there are 842Munique comparisons in these blocks.
The distribution ofthe block sizes presented in Figure 8-(a) and (b).
Block-ing finished in 42 minutes.
Next, we ran our pairwiseclassifier on these 842M pairs and it finished in 220 min-utes.
Finally, we ended up with 10M clusters at the endof the clustering stage which took 3 hours.
The distribu-tion of the connected components and final clusters arepresented in Figure 8-(c).4 ConclusionIn this paper, we presented a novel entity resolution ap-proach for the organization entity domain.
We have im-plemented this in the MapReduce framework with lowmemory requirements so that it may scale to large scaledatasets.
We used two different real-world datasets in ourexperiments.
We first evaluated the performance of ourapproach on truth data provided by freebase.
Our cluster-ing approach, sClust, significantly improved the recall ofthe pairwise classifier.
Next, we demonstrated the viabil-ity of our framework on a large scale dataset on a 50-nodehadoop cluster.77ReferencesA.
K. Elmagarmid, P. G. Iperirotis, and V. S. Verykios.
Dupli-cate record detection: A survey.
In IEEE Transactions onKnowledge and Data Engineering, pages 1?16, 2007.J.
Lin and C. Dyer.
Data-Intensive Text Processing withMapReduce.
Synthesis Lectures on Human LangugageTechnologies.
Morgan & Claypool, 2010.N.
Bansal, A. Blum and S. Chawla Correlation Clustering.
Ma-chine Learning, 2003.M.
Michelson and S.A. Macskassy Record Linkage Measuresin an Entity Centric World.
In Proceedings of the 4th work-shop on Evaluation Methods for Machine Learning, Mon-treal, Canada, 2009.N.
Adly.
Efficient record linkage using a double embeddingscheme.
In R. Stahlbock, S. F. Crone, and S. Lessmann, edi-tors, DMIN, pages 274?281.
CSREA Press, 2009.A.
N. Aizawa and K. Oyama.
A fast linkage detection schemefor multi-source information integration.
In WIRI, pages 30?39.
IEEE Computer Society, 2005.R.
Baxter, P. Christen, and T. Churches.
A comparison of fastblocking methods for record linkage, 2003.M.
Bilenko and B. Kamath.
Adaptive blocking: Learning toscale up record linkage.
In Data Mining, 2006.
ICDM?, num-ber December, 2006.P.
Christen.
A survey of indexing techniques for scalable recordlinkage and deduplication.
IEEE Transactions on Knowl-edge and Data Engineering, 99(PrePrints), 2011.T.
de Vries, H. Ke, S. Chawla, and P. Christen.
Robust recordlinkage blocking using suffix arrays.
In Proceedings of the18th ACM conference on Information and knowledge man-agement, CIKM ?09, pages 305?314, New York, NY, USA,2009.
ACM.T.
de Vries, H. Ke, S. Chawla, and P. Christen.
Robust recordlinkage blocking using suffix arrays and bloom filters.
ACMTrans.
Knowl.
Discov.
Data, 5(2):9:1?9:27, Feb. 2011.M.
A. Hernandez and S. J. Stolfo.
Real-world data is dirty.
datacleansing and the merge/purge problem.
Journal of DataMining and Knowledge Discovery, pages 1?39, 1998.L.
Jin, C. Li, and S. Mehrotra.
Efficient record linkage in largedata sets.
In Proceedings of the Eighth International Confer-ence on Database Systems for Advanced Applications, DAS-FAA ?03, pages 137?, Washington, DC, USA, 2003.
IEEEComputer Society.A.
McCallum, K. Nigam, and L. H. Ungar.
Efficient clusteringof high-dimensional data sets with application to referencematching.
In Proceedings of the ACM International Confer-ence on Knowledge Discover and Data Mining, pages 169?178, 2000.J.
Nin, V. Muntes-Mulero, N. Martinez-Bazan, and J.-L.Larriba-Pey.
On the use of semantic blocking techniques fordata cleansing and integration.
In Proceedings of the 11thInternational Database Engineering and Applications Sym-posium, IDEAS ?07, pages 190?198, Washington, DC, USA,2007.
IEEE Computer Society.A.
D. Sarma, A. Jain, and A. Machanavajjhala.
CBLOCK:An Automatic Blocking Mechanism for Large-Scale De-duplication Tasks.
Technical report, 2011.M.
Weis, F. Naumann, U. Jehle, J. Lufter, and H. Schuster.Industry-scale duplicate detection.
Proc.
VLDB Endow.,1(2):1253?1264, Aug. 2008.S.
E. Whang, D. Menestrina, G. Koutrika, M. Theobald, andH.
Garcia-Molina.
Entity resolution with iterative blocking.In Proceedings of the 35th SIGMOD international confer-ence on Management of data, SIGMOD ?09, pages 219?232,New York, NY, USA, 2009.
ACM.W.
Winkler.
Overview of record linkage and current researchdirections.
Technical report, U.S. Bureau of the Census,2006.S.
Yan, D. Lee, M.-Y.
Kan, and L. C. Giles.
Adaptive sortedneighborhood methods for efficient record linkage.
In Pro-ceedings of the 7th ACM/IEEE-CS joint conference on Dig-ital libraries, JCDL ?07, pages 185?194, New York, NY,USA, 2007.
ACM.L.
Kolb, and E. Rahm.
Parallel entity resolution with Dedoop.Datenbank-Spektrum (2013): 1-10.B.
McNeill, H. Kardes, and A. Borthwick.
DynamicRecord Blocking: Efficient Linking of Massive Databases inMapReduce.
In QDB (2012).78
