Proceedings of the 7th Workshop on Statistical Machine Translation, pages 145?151,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsThe SDL Language Weaver Systems in the WMT12 Quality EstimationShared TaskRadu Soricut Nguyen Bach Ziyuan WangSDL Language Weaver6060 Center Drive, Suite 101Los Angeles, CA, USA{rsoricut,nbach,zwang}@sdl.comAbstractWe present in this paper the system sub-missions of the SDL Language Weaverteam in the WMT 2012 Quality Estimationshared-task.
Our MT quality-prediction sys-tems use machine learning techniques (M5Pregression-tree and SVM-regression models)and a feature-selection algorithm that has beendesigned to directly optimize towards the of-ficial metrics used in this shared-task.
Theresulting submissions placed 1st (the M5Pmodel) and 2nd (the SVM model), respec-tively, on both the Ranking task and the Scor-ing task, out of 11 participating teams.1 IntroductionThe WMT 2012 Quality Estimation shared-task fo-cused on automatic methods for estimating machinetranslation output quality at run-time (sentence-levelestimation).
Different from MT evaluation met-rics, quality prediction (QP) systems do not relyon reference translations and are generally built us-ing machine learning techniques to estimate qualityscores (Specia et al, 2009; Soricut and Echihabi,2010; Bach et al, 2011; Specia, 2011).Some interesting uses of sentence-level MT qual-ity prediction are the following: decide whether agiven translation is good enough for publishing as-is (Soricut and Echihabi, 2010), or inform monolin-gual (target-language) readers whether or not theycan rely on a translation; filter out sentences thatare not good enough for post-editing by professionaltranslators (Specia, 2011); select the best translationamong options from multiple MT systems (Soricutand Narsale, 2012), etc.This shared-task focused on estimating the qual-ity of English to Spanish automatic translations.
Thetraining set distributed for the shared task comprisedof 1, 832 English sentences taken from the news do-main and their Spanish translations.
The translationswere produced by the Moses SMT system (Koehn etal., 2007) trained on Europarl data.
Translations alsohad a quality score derived from an average of threehuman judgements of Post-Editing effort using a 1-5 scale (1 for worse-quality/most-effort, and 5 forbest-quality/least-effort).
Submissions were evalu-ated using a blind official test set of 422 sentencesproduced in the same fashion as the training set.Two sub-tasks were considered: (i) scoring transla-tions using the 1-5 quality scores (Scoring), and (ii)ranking translations from best to worse (Ranking).The official metrics used for the Ranking task wereDeltaAvg (measuring how valuable a proposed rank-ing is from the perspective of extrinsic values asso-ciated with the test entries, in this case post-editingeffort on a 1-5 scale; for instance, a DeltaAvg of 0.5means that the top-ranked quantiles have +0.5 bet-ter quality on average compared to the entire set), aswell as the Spearman ranking correlation.
For theScoring task the metrics were Mean-Absolute-Error(MAE) and Root Mean Squared Error (RMSE).
Theinterested reader is referred to (Callison-Burch et al,2012) for detailed descriptions of both the data andthe evaluation metrics used in the shared-task.The SDL Language Weaver team participatedwith two submissions based on M5P and SVM re-gression models in both the Ranking and the Scoring145tasks.
The models were trained and used to predictPost-Editing?effort scores.
These scores were usedas-such for the Scoring task, and also used to gener-ate sentence rankings for the Ranking task by simply(reverse) sorting the predicted scores.
The submis-sions of the SDL Language Weaver team placed 1st(the M5P model) and 2nd (the SVM model) on boththe Ranking task (out of 17 entries) and the Scoringtask (out of 19 entries).2 The Feature SetBoth SDLLW system submissions were createdstarting from 3 distinct sets of features: the baselinefeature set (here called BFs), the internal featuresavailable in the decoder logs of Moses (here calledMFs), and an additional set of features that we de-veloped internally (called LFs).
We are presentingeach of these sets in what follows.2.1 The Baseline FeaturesThe WMT Quality Estimation shared-task defineda set of 17 features to be used as ?baseline?
fea-tures.
In addition to that, all participants had accessto software that extracted the corresponding featurevalues from the inputs and necessary resources (suchas the SMT-system?s training data, henceforth calledSMTsrc and SMTtrg).
For completeness, we areproviding here a brief description of these 17 base-line features (BFs):BF1 number of tokens in the source sentenceBF2 number of tokens in the target sentenceBF3 average source token lengthBF4 LM probability of source sentenceBF5 LM probability of the target sentenceBF6 average number of occurrences of the targetword within the target translationBF7 average number of translations per source wordin the sentence (as given by IBM 1 table thresh-olded so that Prob(t|s) > 0.2)BF8 average number of translations per source wordin the sentence (as given by IBM 1 table thresh-olded so that Prob(t|s) > 0.01) weightedby the inverse frequency of each word in thesource corpusBF9 percentage of unigrams in quartile 1 of fre-quency (lower frequency words) in SMTsrcBF10 percentage of unigrams in quartile 4 of fre-quency (higher frequency words) in SMTsrcBF11 percentage of bigrams in quartile 1 of fre-quency of source words in SMTsrcBF12 percentage of bigrams in quartile 4 of fre-quency of source words in SMTsrcBF13 percentage of trigrams in quartile 1 of fre-quency of source words in SMTsrcBF14 percentage of trigrams in quartile 4 of fre-quency of source words in SMTsrcBF15 percentage of unigrams in the source sentenceseen in SMTsrcBF16 number of punctuation marks in source sen-tenceBF17 number of punctuation marks in target sentenceThese features, together with the other ones wepresent here, are entered into a feature-selectioncomponent that decides which feature set to use foroptimum performance (Section 3.2).In Table 1, we are presenting the performanceon the official test set of M5P and SVM-regression(SVR) models using only the BF features.
TheM5P model is trained using the Weka package 1and the default settings for M5P decision-trees(weka.classifiers.trees.M5P).
The SVR model istrained using the LIBSVM toolkit 2.
The follow-ing options are used: ?-s 3?
(-SVR) and ?-t 2?
(ra-dial basis function).
The following parameters wereoptimized via 5-fold cross-validation on the train-ing data: ?-c cost?, the parameter C of -SVR; ?-ggamma?, the ?
parameter of the kernel function; ?-pepsilon?, the  for the loss-function of -SVR.1http://www.cs.waikato.ac.nz/ml/weka/2http://www.csie.ntu.edu.tw/?cjlin/libsvm/146Systems Ranking ScoringDeltaAvg Spearman MAE RMSE Predict.
Interval17 BFs with M5P 0.53 0.56 0.69 0.83 [2.3-4.9]17 BFs with SVR 0.55 0.58 0.69 0.82 [2.0-5.0]best-system 0.63 0.64 0.61 0.75 [1.7-5.0]Table 1: Performance of the Baseline Features using M5P and SVR models on the test set.The results in Table 1 are compared against the?best-system?
submission, in order to offer a com-parison point.
The ?17 BFs with SVM?
system ac-tually participated as an entry in the shared-task, rep-resenting the current state-of-the-art in MT quality-prediction.
This system has been ranked 6th (out of17 entries) in the Ranking task, and 8th (out of 19entries) in the Scoring task.2.2 The Decoder FeaturesThe current Quality Estimation task has been de-fined as a glass-box task.
That is, the predictioncomponent has access to everything related to theinternal workings of the MT system for which thequality prediction is made.
As such, we have cho-sen to use the internal scores of the Moses 3 decoder(available to all the participants in the shared-task)as a distinct set of features.
These features are thefollowing:MF1 Distortion costMF2 Word penalty costMF3 Language-model costMF4 Cost of the phrase-probability of source giventarget ?
(s|t)MF5 Cost of the word-probability of source giventarget ?lex(s|t)MF6 Cost of the phrase-probability of target givensource ?
(t|s)MF7 Cost of the word-probability of target givensource ?lex(t|s)MF8 Phrase penalty cost3http://www.statmt.org/moses/These features are then entered into a feature-selection component that decides which feature setto use for achieving optimal performance.The results in Table 2 present the performanceon the test set of the Moses features (with an M5Pmodel), presented against the ?best-system?
sub-mission.
These numbers indicate that the Moses-internal features, by themselves, are fueling a QPsystem that surpasses the performance of the strong?baseline?
system.
We note here that the ?8 MFswith M5P?
system would have been ranked 4th (outof 17 entries) in the Ranking task, and 5th (out of 19entries) in the Scoring task.2.3 Language Weaver FeaturesIn addition to the features presented until this point,we have created and tested additional features thathelped our systems achieve improved performance.In addition to the SMT training corpus, these fea-tures also use the SMT tuning dev set (henceforthcalled Devsrc and Devtrg).
These features are thefollowing:LF1 number of out-of-vocabulary tokens in thesource sentenceLF2 LM perplexity for the source sentenceLF3 LM perplexity for the target sentenceLF4 geometric mean (?-smoothed) of 1-to-4?gramprecision scores (i.e., BLEU score withoutbrevity-penalty) of source sentence against thesentences of SMTsrc used as ?references?LF5 geometric mean (?-smoothed) of 1-to-4?gramprecision scores of target translation against thesentences of SMTtrg used as ?references?147Systems Ranking ScoringDeltaAvg Spearman-Corr MAE RMSE Predict.
Interval8 MFs with M5P 0.58 0.58 0.65 0.81 [1.8-5.0]best-system 0.63 0.64 0.61 0.75 [1.7-5.0]Table 2: Performance of the Moses-based Features with an M5P model on the test set.LF6 geometric mean (?-smoothed) of 1-to-4?gramprecision scores of source sentence against thetop BLEU-scoring quartile of DevsrcLF7 geometric mean (?-smoothed) of 1-to-4?gramprecision scores of target translation against thetop BLEU-scoring quartile of DevtrgLF8 geometric mean (?-smoothed) of 1-to-4?gramprecision scores of source sentence against thebottom BLEU-scoring quartile of DevsrcLF9 geometric mean (?-smoothed) of 1-to-4?gramprecision scores of target translation against thebottom BLEU-scoring quartile DevtrgLF10 geometric mean (?-smoothed) of 1-to-4?gramprecision scores of target translation against apseudo-reference produced by a second MTEng-Spa systemLF11 count of one-to-one (O2O) word alignmentsbetween source and target translationLF12 ratio of O2O alignments over source sentenceLF13 ratio of O2O alignments over target translationLF14 count of O2O alignments with Part-of-Speech?agreementLF15 ratio of O2O alignments with Part-of-Speech?agreement over O2O alignmentsLF16 ratio of O2O alignments with Part-of-Speech?agreement over sourceLF17 ratio of O2O alignments with Part-of-Speech?agreement over targetMost of these features have been shown to helpQuality Prediction performance, see (Soricut andEchihabi, 2010) and (Bach et al, 2011).
Some ofthem are inspired from word-based confidence esti-mation, in which the alignment consensus betweenthe source words and target-translation words areinformative indicators for gauging the quality of atranslation hypothesis.
The one-to-one (O2O) wordalignments are obtained from the decoding logs ofMoses.
We use the TreeTagger to obtain SpanishPOS tags4 and a maximum-entropy POS tagger forEnglish.
Since Spanish and English POS tag setsare different, we normalize their fine-grained POStag sets into a coarser tag set by mapping the orig-inal POS tags into more general linguistic conceptssuch as noun, verb, adjective, adverb, preposition,determiner, number, and punctuation.3 The Models3.1 The M5P Prediction ModelRegression-trees built using the M5P algo-rithm (Wang and Witten, 1997) have been previ-ously shown to give good QP performance (Soricutand Echihabi, 2010).
For these models, the num-ber of linear equations used can provide a goodindication whether the model overfits the trainingdata.
In Table 3, we compare the performance ofseveral M5P models: one trained on all 42 featurespresented in Section 2, and two others trained ononly 15 and 14 features, respectively (selected usingthe method described in Section 3.2).
We alsopresent the number of linear equations (L.Eq.)
usedby each model.
Aside from the number of featuresthey employ, these models were trained underidentical conditions: default parameters of the Wekaimplementation, and 1527 training instances (305instances were held-out for the feature-selectionstep, from the total 1832 labeled instances availablefor the shared-task).As the numbers in Table 3 clearly show, the set of4http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/148Systems #L.Eq.
Dev Set Test SetDeltaAvg MAE DeltaAvg MAE42 FFs with M5P 10 0.60 0.58 0.56 0.64(best-system) 15 FFs with M5P 2 0.63 0.52 0.63 0.6114 FFs with M5P 6 0.62 0.50 0.61 0.62Table 3: M5P-model performance for different feature-function sets (15-FFs ?
42-FFs; 14-FFs ?
42-FFs).feature-functions that an M5P model is trained withmatters considerably.
On both our development setand the official test set, the 15-FF M5P model out-performs the 42-FF model (even if 15-FF ?
42-FF).The 42-FF model would have been ranked 5th (outof 17 entries) in the Ranking task, and also 5th (outof 19 entries) in the Scoring task.
In comparison, the15-FF model (feature set optimized for best perfor-mance under the DeltaAvg metric) was our officialM5P submission (SDLLW M5PBestDeltaAvg), andranked 1st in the Ranking task and also 1st in theScoring task.
The 14-FF model (also a subset of the42-FF set, optimized for best performance under theMAE metric) was not part of our submission, butwould have been ranked 2nd on both the Rankingand Scoring tasks.The number of linear equations used (see #L.Eq.in Table 3) is indicative for our results.
When using42 FFs, the M5P model seems to overfit the train-ing data (10 linear equations).
In contrast, the modeltrained on a subset of 15 features has only 2 linearequations.
This latter model is less prone to overfit-ting, and performs well given unseen test data.
Thesame number for the 14-FF model indicates slightoverfit on the training and dev data: with 6 equa-tions, this model has the best MAE numbers on theDev set, but slightly worse MAE numbers on theTest score compared to the 15-FF model.3.2 Feature SelectionAs we already pointed out, some of the features ofthe entire 42-FF set are highly overlapping and cap-ture roughly the same amount of information.
Toachieve maximum performance given this feature-set, we applied a computationally-intensive feature-selection method.
We have used the two officialmetrics, DeltaAvg and MAE, and a development setof 305 instances to perform an extensive feature-selection procedure that directly optimizes the twoofficial metrics using M5P regression-trees.The overall space that needs to be explored for 42features is huge, on the order of 242 possible com-binations.
We performed the search in this space inseveral steps.
In a first step, we eliminated the obvi-ously overlapping features (e.g., BF5 and MF3 areboth LM costs of the target translation), and alsoexcluded the POS-based features (LF14-LF17, seeSection 2.3).
This step reduced the overall num-ber of features to 24, and therefore left us with anorder of 224 possible combinations.
Next, we ex-haustively searched all these combinations by build-ing and evaluating M5P models.
This operationis computationally-intensive and takes approxima-tively 60 hours on a cluster of 800 machines.
Atthe conclusion of this step, we ranked the resultsand considered the top 64 combinations.
The perfor-mance of these top combinations was very similar,and a set of 15 features was selected as the supersetof active feature-functions present in most of the top64 combinations.DeltaAvg optim.
BF1 BF3 BF4 BF6 BF12BF13 BF14 MF3 MF4 MF6LF1 LF10 LF14 LF15 LF16MAE optim.
BF1 BF3 BF4 BF6 BF12BF14 BF16 MF3 MF4 MF6LF1 LF10 LF14 LF17Table 4: Feature selection results.The second round of feature selection consid-ers these 15 feature-functions plus the 4 POS-basedfeature-functions, for a total of 19 features and there-fore a space of 219 possible combinations (215 ofthese already covered by the first search pass).
Asecond search procedure was executed exhaustively149Dev Set Test SetSVR Model (C;?
;) #S.V.
DeltaAvg MAE DeltaAvg MAE1.0 ; 0.00781; 0.50 695 0.62 0.52 0.60 0.661.74; 0.00258; 0.3299 952 0.63 0.51 0.61 0.648.0 ; 0.00195; 0.0078 1509 0.64 0.50 0.60 0.6816.0; 0.00138; 0.0884 1359 0.63 0.51 0.59 0.70Table 5: SVR-model performance for dev and test sets.over the set of all the new possible combinations.In the end, we selected the winning feature-functioncombination as our final feature-function sets: 15features for DeltaAvg optimization and 14 featuresfor MAE optimization.
They are given in Table 4,using the feature id-s given in Section 2.
The perfor-mance of these two feature-function sets using M5Pmodels can be found in Table 3.3.3 The SVM Prediction ModelThe second submission of our team consists of rank-ings and scores produced by a system using an -SVM regression model (-SVR) and a subset of 19features.
This model is trained on 1,527 trainingexamples by the LIBSVM package using radial ba-sis function (RBF) kernel.
We have found that thefeature-set obtained by the feature-selection opti-mization for M5P models described in Section 3.2does not achieve the same performance for SVRmodels on our development set.
Therefore, wehave performed our SVR experiments using a hand-selected set of features: 9 features from the BF fam-ily (BF1 BF3 BF4 BF6 BF10 BF11 BF12 BF14BF16); all 8 features from the MF family; and 2 fea-tures from the LF family (LF1 LF10).We optimize the three hyper parameters C, ?, and of the SVR method using a grid-search method andmeasure their performance on our development setof 305 instances.
The C parameter is a penalty fac-tor: if C is too high, we have a high penalty for non-separable points and may store many support vec-tors and therefore overfit the training data; if C istoo low, we may end up with a model that is poorlyfit.
The  parameter determines the level of accuracyof the approximated function; however, getting tooclose to zero may again overfit the training data.
The?
parameter relates to the RBF kernel: large ?
val-ues give the model steeper and more flexible kernelfunctions, while small gamma values give the modelsmoother functions.
In general, C, , and ?
are allsensitive parameters and instantiate -SVR modelsthat may behave very differently.In order to cope with the overfitting issue givena small amount of training data and grid search op-timization, we train our models with 10-fold crossvalidation and restart the tuning process severaltimes using different starting points and step sizes.We select the best model parameters based on a cou-ple of indicators: the performance on the develop-ment set and the number of support vectors of themodel.
In Table5 we present the performance of dif-ferent model parameters on both the developmentset and the official test set.
Our second submis-sion (SDLLW SVM), which placed 2nd in both theRanking and the Scoring tasks, is the entry in boldfont.
It was chosen based on good performance onthe Dev set and also a setting of the (C, ?, ) pa-rameters that provides a number of support vectorsthat is neither too high nor too low.
As a contrastivepoint, the model on the row below it uses 1,509 sup-port vectors extracted from 1,527 training vectors,which represents a clear case of overfitting.
Indeed,the performance of this model is marginally betteron the Dev set, but ends up underperforming on theTest data.4 ConclusionsThe WMT 2012 Quality Estimation shared-task pro-vided the opportunity for the comparing differentQP systems using shared datasets and standardizedevaluation metrics.
Our participation in this shared-task revealed two important aspects of Quality Pre-diction for MT that we regard as important for thefuture.
First, our experiments indicated that the150Moses-internal features, by themselves, can fuel aQP-system that surpasses the performance of thestrong ?baseline?
system used in this shared task torepresent state-of-the-art performance in MT qual-ity prediction.
This is a surprising finding, consid-ering that these decoder-internal features have beenprimarily designed to gauge differences in transla-tion quality when starting from the same source sen-tence.
In contrast, for quality-prediction tasks likeranking one needs to gauge differences in quality oftranslations of different source sentences.The second aspect relates to the importance offeature selection.
Given the availability and goodscalability of Machine Learning toolkits today, itis tempting to throw as much features as possibleat this problem and let the built-in mechanisms ofthese learning algorithms deal with issues relatingto feature overlapping, training-data overfitting, etc.However, these learning algorithms have their ownlimitations in these regards, and, in conjunction withthe limited availability of the labeled data, can easilyproduce models that are underperforming on blindtests.
There is a need for careful engineering ofthe models and evaluation of the resulting perfor-mance in order to achieve optimal performance us-ing the current state-of-the-art supervised learningtechniques.ReferencesNguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011.Goodness: A method for measuring machine transla-tion confidence.
In Proceedings of the ACL/HLT, Port-land, Oregon, USA.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical MachineTranslation.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, Montreal, Canada,June.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translations viaranking.
In Proceedings of ACL.Radu Soricut and Sushant Narsale.
2012.
Combiningquality prediction and system selection for improvedautomatic translation output.
In Proceedings of theSeventh Workshop on Statistical Machine Translation,Montreal, Canada, June.
Association for Computa-tional Linguistics.Lucia Specia, Nicola Cancedda, Marc Dymetman, Mar-cho Turchi, and Nello Cristianini.
2009.
Estimatingthe sentence-level quality of machine translation.
InProceedings of EAMT.Lucia Specia.
2011.
Exploiting objective annotations formeasuring translation post-editing effort.
In Proceed-ings of EAMT.Y.
Wang and I. H. Witten.
1997.
Induction of model treesfor predicting continuous classes.
In Proceedings ofthe 9th European Conference on Machine Learning.151
