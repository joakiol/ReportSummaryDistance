Parse Fitting and Prose Fixing:Gett ing a Hold on I I I - formedness 1K.
Jensen ,  G. E. He idorn ,  L. A.  M i l le r ,  and  Y. Rav inComputer  Sciences Depar tmentIBM Thomas  J. Watson  Research CenterP.O.
Box 218York town Heights,  New York  10598Processing syntactically il l-formed language is an important mission of the EPISTLEsystem, l l l - formed input is treated by this system in various ways.
Misspellings arehighlighted by a standard spelling checker; syntactic errors are detected and corrections aresuggested; and stylistic infelicities are called to the user's attention.Central to the EPISTLE processing strategy is its technique of fitted parsing.
When therules of a conventional syntactic grammar are unable to produce a parse for an input string,this technique can be used to produce a reasonable approximate parse that can serve asinput to the remaining stages of processing.This paper first describes the fitting process and gives examples of il l-formed languagesituations where it is called into play.
We then show how a fitted parse allows EPISTLE tocarry on its text-critiquing mission where conventional grammars would fail either becauseof input problems or because of limitations in the grammars themselves.
Some inherentdifficulties of the fitting technique are also discussed.
In addition, we explore how stylecritiquing relates to the handling of ill-formed input, and how a fitted parse can be used instyle checking.IntroductionIn its current form, the EPISTLE system addresses theproblems of grammar and style checking of texts writ-ten in ordinary English (letters, reports, and manuals,as opposed to novels, plays, and poems).
It is thisgoal that involves us so intimately with the processingof il l-formed language.
Grammar checking deals withsuch errors as disagreement in number between sub-ject and verb; style checking calls attention to suchinfelicities as sentences that are too wordy or too com-plex.
A standard spelling checker is also included.Our grammar is written in NLP (Heidorn 1972), anaugmented phrase structure language which is current-ly implemented in LISP/370.
At this time the EPISTLEgrammar uses syntactic, but not semantic, information.Access to an on-line standard dictionary with about130,000 entries, including part -of -speech and someother syntactic information (such as transitivity of1 The work described here is a continuation of work firstpresented at the Conference on Applied Natural Language Process-ing in Santa Monica, California (Jensen and Heidorn 1983).verbs), makes the system's vocabulary essentially un-limited.
We test and improve the grammar by regular-ly running it on a data base of 2254 sentences from411 actual business letters.
Most of these sentencesare rather complicated; the longest contains 63 words,and the average length is 19.2 words.Since the subset of English represented in businessdocuments is very large, we need a very comprehen-sive grammar and a robust parser.
We take a heuristicapproach and consider that a natural language parsercan be divided into three parts:(a) a set of rules, called the core grammar ,  that pre-cisely defines the central, agreed-upon grammat-ical structures of a language;(b) peripheral procedures that handle parsing ambi-guity: when the core grammar produces morethan one parse, these procedures decide whichof the multiple parses is to be preferred;(c) peripheral procedures that handle parsing fail-ure: when the core grammar cannot define anacceptable parse, these procedures assign somereasonable structure to the input.Copyright 1984 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is grantedprovided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included onthe first page.
To copy otherwise, or to republish, requires a fee and/or  specific permission.0362-613X/83/030147-14503.00American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 147K.
Jensen, G.E.
Heidorn, L.A. Mil ler, and Y. Ravin Parse Fitting and Prose FixingIn EPISTLE,(a) the core grammar consists at present of a set ofabout 300 syntax rules;(b) ambiguity is resolved by using a metric thatranks alternative parses (Heidorn 1982); and(c) parse failure is handled by the fitting proceduredescribed here.In using the terms core grammar and periphery, weare consciously echoing recent work in generativegrammar, but we are applying the terms in a somewhatdifferent way.
Core grammar, in current linguistictheory, suggests the notion of a set of very generalrules that define universal properties of human lan-guage and effectively set limits on the types of gram-mars any particular language may have; peripheryphenomena re those constructions that are peculiar toparticular languages and that require rules beyondwhat the core grammar will provide (Lasnik and Freid-in 1981).
Our current work is not concerned with themeta-rules of a Universal Grammar.
But we havefound that a distinction between core and periphery isuseful even within a grammar of a particular language- in this case, English.Parsing in EPISTLEEPISTLE's parser is written in the NLP programminglanguage, which works with augmented phrase struc-ture rules and with attribute-value records, which aremanipulated by the rules.
When NLP is used to parsenatural language text, the records describe constitu-ents, and the rules put these constituents together toform ever larger constituent (or record) structures.Records contain all the computational  and linguisticinformation associated with words, with larger constit-uents, and with the parse formation.
At this time ourgrammar is sentence-based; we do not, for instance,create record structures to describe paragraphs.
De-tails of the EPISTLE system and of its core grammarmay be found in Heidorn et al (1982).
An earlieroverview of the system is presented in Miller et al(1981).A close examination of parse trees produced by thecore grammar will often reveal branch attachmentsthat are not quite right: for example, semanticallyincongruous prepositional phrase attachments.
In linewith our pragmatic parsing philosophy, our core gram-mar is designed to produce unique approximate parses.
(Recall that we currently have access only to syntacticand morphological information about constituents.)
Inthe cases where semantic or pragmatic information isneeded before a proper attachment can be made, rath-er than produce a confusion of multiple parses weforce the grammar to try to assign a single parse.
Thisis usually done by forcing some attachments to bemade to the closest, or rightmost, available constitu-ent.
This strategy only rarely impedes the type ofgrammar-checking and style-checking that we areworking on.
And we feel that a single parse with aconsistent attachment scheme will yield much moreeasily to later semantic processing than would a largenumber of different structures.The rules of the core grammar (CG) produce asingle approximate parse for almost 70% percent ofinput text, and a small number of multiple parses foranother 16% .
The CG can always be improved andits coverage xtended; work on improving the EPISTLECG is continual.
But the coverage of a core grammarwill never reach 100%.
For those strings that cannotbe fully parsed by rules of the core grammar we use aheuristic best f i t  procedure that produces a reasonableparse structure.The Fitting ProcedureThe fitting procedure begins after the CG rules havebeen applied in a bottom-up, parallel fashion, but havefailed to produce an S node that covers the string.
Atthis point, as a by-product of bottom-up parsing, rec-ords are available for inspection that describe the vari-ous segments of the input string from many perspec-tives, according to the rules that have been applied.The term fitting has to do with selecting and fittingthese pieces of the analysis together in a reasonablefashion.The fitting algorithm, which is itself implementedas a set of NLP rules, proceeds in two main stages:first, a head constituent is chosen; next, remainingconstituents are fitted in.
In our current implementa-tion, candidates for the head are tested preferentiallyas follows, from most to least desirable:(a) VPs with tense and subject;(b) VPs with tense but no subject;(c) phrases without verbs (e.g., NPs, PPs);(d) non-finite VPs;(e) others.If more than one candidate is found in any category,the one preferred is the widest (covering most text).If there is a tie for widest, the leftmost of those ispreferred.
If there is a tie for leftmost, the one withthe best value for the parse metric is chosen.
If thereis still a tie (a very unlikely case), an arbitrary choiceis made.
(Note that we consider a VP to be any seg-ment of text that has a verb as its head element.
)The fitting process is complete if the head constitu-ent covers the entire input string (as would be the caseif the string contained just a noun phrase, for example,"Salutations and congratulations").
If the head con-stituent does not cover the entire string, remainingconstituents are added on either side, with the follow-ing order of preference:(a) segments other than VP;(b) untensed VPs;(c) tensed VPs.As with the choice of head, the widest candidate ispreferred at each step.
The fit moves outward from148 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983K.
Jensen, G.E.
Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose FixingF ITTED - - -NP  .
.
.
.
.
.
NOUN*- - - "Example"- - -PUNC .
.
.
.  "
: "- - -VP*  .
.
.
.
NP I  .
.
.
.
.
DET  .
.
.
.
.
AD J*  .
.
.
.
.
.- - -PUNC .
.
.
.
"Your"I .
.
.
.
.
NOUN*- - -  "percentage"I .
.
.
.
.
PP  i .
.
.
.
.
PREP  .
.
.
.
.
.
.
.
o f "I .
.
.
.
.
MONEY*  .
.
.
.
.
.
$250 .00".
.
.
.
VERB*- - - "  i s ".
.
.
.
NP  .
.
.
.
.
.
MONEY* - - "$187.50"i v  .
, vFigure 1.
An example of a fitted parse tree.the head, both leftward to the beginning of the string,and rightward to the end, until the entire input stringhas been fitted into a best approximate parse tree.The overall effect of the fitting process is to select thelargest chunk of sentence-like material within a textstring and consider it to be central, with left-overchunks of text attached in some reasonable manner.As a simple example, consider this text string:"Example: Your percentage of $250.00 is $187.50.
"Because this string has a capitalized first word and aperiod at its end, it is submitted to the core grammarfor consideration as a sentence.
But it is not a sen-tence, and so the CG will fail to arrive at a completedparse.
However, during processing, the CG will haveassigned many structures to its substrings.
Lookingfor a head constituent among these structures, thefitting procedure will first seek VPs with tense andsubject.
Several are present: "$250.00 is","percentage of $250.00 is", "$250.00 is $187.50",and so on.
The widest and leftmost of these VP con-stituents is the one which covers the string "Your per-centage of $250.00 is $187.50", so it will be chosen ashead.The fitting process then looks for additional con-stituents to the left, favoring ones other than VP.
Itfinds first the colon, and then the word "Example".In this string the only constituent following the head isthe final period, which is duly added.
The completefitted parse is shown in Figure 1.The form of parse tree used here shows the top-down structure of the string from left to right, with theterminal nodes being the last item on each line.
Ateach level of the tree (in a vertical column), the headelement of a constituent is marked with an asterisk.The other elements above and below are pre- andpost-modifiers.
The highest element of the treesshown here is F ITTED,  rather than the more usualSENT.
(It is important o remember that these parsediagrams are only shorthand representations for theNLP record structures, which contain an abundance ofinformation about the string processed.
)The tree of Figure 1, which would be lost if werestricted ourselves to the rules of the core grammar,is now available for examination, for grammar andstyle checking, and ultimately for semantic interpreta-tion.
It can take its place in the stream of continuoustext and be analyzed for what it is - a sentence frag-ment, interpretable only by reference to other sen-tences in context.Fur ther  ExamplesThe fitted parse approach can help to deal with manydifficult natural language problems, including frag-ments, difficult cases of ellipsis, proliferation of rulesto handle single phenomena, phenomena for which norule seems adequate, and punctuation horrors.
Eachof these is discussed here with examples.Fragments.
There are many of these in runningtext; they are frequently NPs, as in Figure 2, andinclude common greetings, farewells, and sentiments.
(N.B., most of the examples in this paper are takenfrom the EPISTLE data base.
)Di f f i cu l t  cases  o f  e l l ips is .
In the sentence ofFigure 3, what we really have semantically is a con-junction of two proposit ions which, if generated di-rectly, would read: "Secondly, the Annual CommissionF ITTEDI - - -NP* J  .
.
.
.
NP i  .
.
.
.
.
A JP  .
.
.
.
.
ADJ*  .
.
.
.
"Good"I I I .
.
.
.
.
NOUN*- - - " Iuck"I I .
.
.
.
CONJ* - - - "and"I I .
.
.
.
NP J  .
.
.
.
.
A JP  .
.
.
.
.
ADJ*  .
.
.
.
"good"I I .
.
.
.
.
NOUN*  .
.
.
.
.
se l l ing"P - - -PUNC .
.
.
.  "
.
"Figure 2.
Fitted noun phrase (fragment).American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 149K.
Jensen, G.E.
Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose FixingF ITTED .
.
.
.
AVP I  .
.
.
.
ADV*  .
.
.
.
"Second ly"I .
.
.
.
PUNC .
.
.
.
.
.
, ".
.
.
.
NP  I .
.
.
.
.
A JP  .
.
.
.
.
AD J*  .
.
.
.  "
the"i .
.
.
.
.
NP  .
.
.
.
.
.
NOUN*  - - -  "Annua i "I .
.
.
.
.
NP  .
.
.
.
.
.
NOUN*- - -  "Commi  s s ion"I .
.
.
.
.
NP  .
.
.
.
.
.
NOUN*  .
.
.
.
.
S ta tement"i .
.
.
.
.
NOUN*  .
.
.
.
.
to ta l ".
.
.
.
VERB .
.
.
.
.
.
shou ld".
.
.
.
VERB*  - - -  "be".
.
.
.
NP  .
.
.
.
.
.
MONEY*  .
.
.
.
$14 ,682 .61  "- - -PUNC .
.
.
.  "
,  "- - -AVP  .
.
.
.
.
ADV*  .
.
.
.
"not"- - -NP  .
.
.
.
.
.
MONEY*  .
.
.
.
$14 ,682 .67"- - - PUNC .
.
.
.  "
.
"Figure 3.
Fitted sentence with ellipsis.F ITTED I - - -NP  .
.
.
.
.
.
NOUN*- - -  "B i l  i "- - -PUNC .
.
.
.  "
,  "I .
.
.
.
NP  .
.
.
.
.
.
PRON*  - - -  " I "i .
.
.
.
VERB .
.
.
.
.  "
' ve"i .
.
.
.
VERB .
.
.
.
"been"I .
.
.
.
VERB*  .
.
.
.
.
asked"i .
.
.
.
INFCL  l - - INFTO- - - "  to"I - -VERB*- - - "  c la r i fy"I - -NP  I .
.
.
.
.
A JP  .
.
.
.
.
AD J*  .
.
.
.
.
.
the"i .
.
.
.
.
A JP  .
.
.
.
.
VERB*  - - -  "e  nc  io  sed"i .
.
.
.
.
NOUN*  .
.
.
.
.
l e t te r "- - -PUNC .
.
.
.  "
.
"Figure 4.
Fitted sentence with initial vocative.F ITTED I - - -NP  I .
.
.
.
.
A JP  .
.
.
.
.
AD J*  .
.
.
.
"Good"I .
.
.
.
.
NOUN*- - - "  luck"- - -PP  I .
.
.
.
.
PREP  .
.
.
.  "
to"J .
.
.
.
.
NP  .
.
.
.
.
.
PRON*  - - -  "you"i .
.
.
.
.
CONJ* - - - "and"I .
.
.
.
.
NP  .
.
.
.
.
.
PRON*  - - -  "yours"- - -CONJ  .
.
.
.  "
,and"- - -VP*  .
.
.
.
NP  .
.
.
.
.
.
PRON*- - - "  I ".
.
.
.
VERB*- - - "w ish".
.
.
.
NP  .
.
.
.
.
.
PRON*- - - "you".
.
.
.
NP  .
.
.
.
.
A JP  .
.
.
.
.
AD J*  .
.
.
.  "
the".
.
.
.
.
ADV .
.
.
.
.
"VERY".
.
.
.
.
AD J*  .
.
.
.
"best ".
.
.
.
PP  .
.
.
.
.
PREP  .
.
.
.  "
in".
.
.
.
.
A JP  .
.
.
.
.
AD J*  .
.
.
.
"your".
.
.
.
.
A JP  .
.
.
.
.
AD J*  .
.
.
.  "
fu ture".
.
.
.
.
NOUN*  - - - "e  f fo r t  s "- - -PUNC .
.
.
.  "
.
"Figure 5.
Fitted conjunction of noun phrase with clause.150 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983K.
Jensen, G.E.
Heidorn, L.A. Miller, and Y. Ravin Parse Fitt ing and Prose FixingStatement total should be $14,682.61; the AnnualCommission Statement total should not be$14,682.67."
Deletion processes operating on thesecond proposit ion are lawful (deletion of identicalelements) but massive.
It would be unwise to write acore grammar rule that routinely allowed negativizedNPs to follow main clauses, because:(a) the proper analysis of this sentence would beobscured: some pieces - namely, the inferredconcepts - are missing from the second part ofthe surface sentence;(b) the linguistic generalization would be lost: anytwo conjoined propositions can undergo deletionof identical (recoverable) elements.A fitted parse such as Figure 3 allows us to inspect themain clause for syntactic and stylistic deviances, andat the same time makes clear the breaking point be-tween the two propositions and opens the door for alater semantic processing of the elided elements.Proliferation of  ru les  to hand le  singlephenomena.
There are some English constructionsthat, although they have a fairly simple and unitaryform, do not hold anything like a unitary orderingrelation within clause boundaries.
The vocative is oneof these:(a) Bill I 've been asked to clarify the enclosedletter.
(b) I 've been asked, Bill to clarify the enclosedletter.
(c) I 've been asked to clarify the enclosed letter,Bill.In longer sentences there would be even more possibleplaces to insert the vocative.Rules could be written that would explicitly allowthe placement of a proper name, surrounded by com-mas, at different positions in the sentence - a differentrule for each position.
But this solution lacks ele-gance, makes a simple phenomenon seem complicated,and always runs the risk of overlooking yet one moreposition where some other writer might insert a voca-tive.
The parse fitting procedure provides an alterna-tive that preserves the integrity of the main clause andallows the vocative to be added onto the structure, asshown, for example, in Figure 4.
Other similar phe-nomena, such as parenthetical expressions, can behandled in this same fashion.Phenomena fo r  wh ich  no ru le  seems adequate .The sentence "Good luck to you and yours, and 1 wishyou the very best in your future efforts."
is, on theface of it, a conjunction of a noun phrase (or NP plusPP) with a finite verb phrase.
Such constructions arenot usually considered to be fully grammatical, and acore grammar that contains a rule describing this con-struction ought probably to be called a faulty gram-mar.
Nevertheless, ordinary English correspondenceabounds with strings of this sort, and readers have nodifficulty construing them.
The fitted parse for thissentence in Figure 5 presents the finite clause as itshead and adds the remaining constituents in a reasona-ble fashion.
From this structure later semantic proc-essing could infer that "Good luck to you and yours"really means "I  express/send/wish good luck to youand yours" - a special case of formalized, ritualizedellipsis.Punctuation horrors.
In any large sample of natu-ral language text, there will be many irregularities ofpunctuation that, although perfectly understandable toreaders, can completely disable an explicit computa-tional grammar.
In business text these difficulties arefrequent.
Some can be caught and corrected by punc-tuation checkers and balancers.
But others cannot,sometimes because, for all their trickiness, they are notreally wrong.
Yet few grammarians would care todignify, by describing it with rules of the core gram-mar, a text string like:"Options: A l - (Transmit ter  Clocked by Data-set) B3-(without the 605 Recall Unit) C5-(with ABC Ring Indicator) D8-(without AutoAnswer) E10-(Auto Ring Selective).
"Our parse fitting procedure handles this example bybuilding a string of NPs separated with punctuationmarks, as shown in Figure 6.
This solution at leastenables us to get a handle on the contents of thestring.Benef i t sThere are two main benefits to be gained from usingthe fitted parse approach.
First, it allows for syntacticprocessing - for our purposes, grammar and stylechecking - to proceed in the absence of a perfectparse.
Second, it provides a promising structure tosubmit to later semantic processing routines.
Andparenthetically, a fitted parse diagram is a great aid togrammar rule debugging.
The place where the firstbreak occurs between the head constituent and its pre-or post-modif iers usually indicates fairly preciselywhere the core grammar failed.It should be emphasized that a fitting procedurecannot be used as a substitute for explicit rules, andthat it in no way lessens the importance of the coregrammar.
There is a tight interaction between the twocomponents.
The success of the fitted parse dependson the accuracy and completeness of the core rules; afit is only as good as its grammar.Correct ing  Syntact i c  Errors  in a F i t ted  ParseSuppose the text string in Figure 1 had contained anungrammatical ity, such as disagreement in numberbetween its subject and its verb.
Then our troubleswould be compounded.
There would be two reasonsfor the CG to reject that string: (a) it is a fragment;and (b) it contains a syntax error.American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 151K.
Jensen, G.E.
Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose FixingF ITTED - - -NP  .
.
.
.
.
.
NOUN*- - -  "Opt  ions"- - -PUNC .
.
.
.  "
: "- - -NP  .
.
.
.
.
.
NOUN*- - - "A  I "- - -PUNC .
.
.
.  "
- "- - -PUNC .
.
.
.  "
( "- - -NP  I .
.
.
.
.
NP  .
.
.
.
.
.
NOUN*- - - "T ransmi t te r "I .
.
.
.
.
NOUN*  .
.
.
.
.
C locked"- - -PP  I .
.
.
.
.
PREP  .
.
.
.
"by"I .
.
.
.
.
NOUN*  .
.
.
.
.
Datas  e t "- - -PUNC .
.
.
.
.
.  )
"- - -NP  .
.
.
.
.
.
NOUN*- - - "B3"- - -  PUNC .
.
.
.
.
.
.
.
.I - - -PP*  .
.
.
.
PUNC .
.
.
.  "
( "I .
.
.
.
PREP  .
.
.
.
.
.
w i thout"I .
.
.
.
A JP  .
.
.
.
.
AD J*  .
.
.
.  "
the"I .
.
.
.
QUANT- - -NUM* .
.
.
.
.
.
6 0 5".
.
.
.
NP  .
.
.
.
.
.
NOUN*- - - "Reca l I ".
.
.
.
NOUN*- - - "Un i t ".
.
.
.
PUNC .
.
.
.
.
.  )
"- - -NP  .
.
.
.
.
.
NOUN*  - - -  "C  5"- - -PUNC .
.
.
.  "
- "- - -PP  .
.
.
.
.
PUNC .
.
.
.  "
( ".
.
.
.
.
PREP  .
.
.
.
"w i th".
.
.
.
.
NP  .
.
.
.
.
.
NOUN*  - - -  "ABC".
.
.
.
.
NP  .
.
.
.
.
.
NOUN*  - - -  "R ing".
.
.
.
.
NOUN*- - - "  Ind icator".
.
.
.
.
PUNC .
.
.
.
.
.  )
"- - -NP  .
.
.
.
.
.
NOUN*- - - "D8"- - - PUNC .
.
.
.  "
- "- - -PP  I .
.
.
.
.
PUNC .
.
.
.
.
.
( "I .
.
.
.
.
PREP  .
.
.
.
"w i thout"I .
.
.
.
.
NP  .
.
.
.
.
.
NOUN*- - -  "Auto"I .
.
.
.
.
NOUNS - - -  "Answer"I .
.
.
.
.
PUNC .
.
.
.
.
.  )
"- - -NP  .
.
.
.
.
.
NOUN*  - - -  "E  I 0"- - -  PUNC .
.
.
.  "
- "- - -NP  I .
.
.
.
.
PUNC .
.
.
.  "
( "I .
.
.
.
.
NP  .
.
.
.
.
.
NOUN*- - -  " Auto"I .
.
.
.
.
NP  .
.
.
.
.
.
NOUN*- - -  "R ing"I .
.
.
.
.
NOUNS .
.
.
.
.
Se lec t  i ve"I .
.
.
.
.
PUNC .
.
.
.
.
.  )
"- - -PUNC .
.
.
.  "
.
"Figure 6.
Fitted list.But the CG can recover from many syntax errors:it can diagnose and correct them, producing the parsetree that would be appropriate if the correction weremade.
Figure 7 illustrates this ability.
This number-disagreement phenomenon is fairly common in currentAmerican English.
The tensed verb seems to want toagree with its closest noun neighbor (in this sentence,"forms...are") rather than with its subject NP ("a car-bon copy... is").
A prescriptive rule still insists thatsubject and verb should agree in number, however,and the EPISTLE grammar provides a correction forsuch cases.
Note that in the last line of Figure 7 theword "are" has been changed to "is".
(See Heidornet al (1982) for a more thorough discussion of theerror correction technique.
)And now the fitting procedure allows us to contin-ue this work even under wildly ungrammatical condi-tions.
Figure 8 is a fitted parse for the string in Figure1, with a number disagreement error introduced intothe fragment.152 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983K.
Jensen, G.E.
Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose FixingDECL - - -NP I  .
.
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.
"A"I .
.
.
.
.
NP  .
.
.
.
.
.
NOUN*- - - "carbon"i .
.
.
.
.
NOUN*- - - "copy"i .
.
.
.
PP  .
.
.
.
.
PREP  .
.
.
.
"o f ".
.
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.  "
the".
.
.
.
.
NP I  .
.
.
.
.
NOUN*  .
.
.
.
.
Workman"I .
.
.
.
.
POSS .
.
.
.  "
' s ".
.
.
.
.
NP  .
.
.
.
.
.
NOUN*- - - "Compensat ion  ''.
.
.
.
.
NOUN*- - - " fo rms"- - -VERB .
.
.
.
"a re"- - -VERB*- - - "enc losed"- - -PP I  .
.
.
.
.
PREP  .
.
.
.  "
fo r "J .
.
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.
"your"J .
.
.
.
.
NOUN*- - - " in fo rmat ion"- - -PUNC .
.
.
.  "
.
"GRAMMATICAL  ERROR:  SUBJECT-VERB NUMBER DISAGREEMENT.A carbon  copy .
.
.ARE enc losed  fo r  your  in fo rmat ion .CONSIDER:A carbon  copy .
.
.
IS  enc losed  fo r  your  in fo rmat ion .Figure 7.
Diagnosis and correction of a syntax error (not a fitted parse).F ITTEDI - - -NP  .
.
.
.
.
.
NOUN*  .
.
.
.
.
Example"- - -PUNC .
.
.
.  "
: "- - -Vp*  .
.
.
.
NP I  .
.
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.
"your"I .
.
.
.
.
NOUN*  .
.
.
.
.
percentage"i .
.
.
.
.
PP i  .
.
.
.
.
PREP  .
.
.
.
"o f "I .
.
.
.
.
MONEY*- - "$250.00".
.
.
.
VERB, - - - , ,a re  ,,.
.
.
.
NP  .
.
.
.
.
.
MONEY*  ... .
$ \ ]87 .50"- - -PUNC .
.
.
.  "
.
"POSSIBLE  GRAMMATICAL  ERROR:  SUBJECT-VERB NUMBERExample :  your  percentage .
.
.ARE $187.50 .CONSIDER:Example :  your  percentage .
.
.
IS  $ \ ]87 .50 .D ISAGREEMENT.Figure 8.
Fitted parse containing clause with syntax error.F ITTEDI - - -PP* I  .
.
.
.
PREP  .
.
.
.
"Between"I I .
.
.
.
NP  .
.
.
.
.
.
PRON*  .
.
.
.
.
you"I I .
.
.
.
CONJ*  .
.
.
.
.
and"I I .
.
.
.
NP  .
.
.
.
.
.
PRON*- - - " I "I - - -PUNC .
.
.
.  "
.
"POSSIBLE  GRAMMATICAL  ERROR:BETWEEN you  and  I .CONSIDER:BETWEEN you  and  ME.WRONG PRONOUN IN  OBJECTFigure 9.
Case error in prepositional phrase.POSIT ION.American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983 153K.
Jensen, G.E.
Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose FixingThanks to the flexibility of this approach, it is pos-sible to check grammar within the smallest imaginableconstituents (Figure 9) - and in the largest imaginable(Figure 10).In summary, there are many different causes forsyntactic i l l -formedness in the processing of text:misspellings, ungrammaticalit ies, fragments, crazypunctuation, deficits in the processing grammar, etc.The techniques described here give us a chance torecover from all such cases of il l-formedness.
First wedevelop a core grammar, which itself is capable ofdetecting spelling mistakes, and of correcting certainF ITTED - - -VP*  .
.
.
.
SUBCL  - -CONJ  .
.
.
.
.
.
Be fore"- -NP I  .
.
.
.
.
DET  .
.
.
.
.
AD J*  .
.
.
.
"an"I .
.
.
.
.
NOUN*  .
.
.
.
.
approva l "- -VERB .
.
.
.
"can"- -VERB .
.
.
.
"be"- -VERB*- - - " i ssued".
.
.
.
NP  .
.
.
.
.
.
PRON*- - - " i t ".
.
.
.
VERB .
.
.
.
.
.
w i l l ".
.
.
.
VERB*- - -  "be".
.
.
.
A JP i  .
.
.
.
AD J*  .
.
.
.
"necessary"I .
.
.
.
INFCL  - - INFTO .
.
.
.
.
to"- -VERB*  - - - "  submi t "- -NP  .
.
.
.
.
NP  .
.
.
.
.
.
NOUN*  "b luepr in t ".
.
.
.
.
NOUN*- - - "drawings"- -PP  .
.
.
.
.
PREP  .
.
.
.  "
in".
.
.
.
.
A JP  .
.
.
.
.
AD J*  .
.
.
.  "
t r ip l i ca te".
.
.
.
.
NOUN*- - - "se ts"- -PP  .
.
.
.
.
PREP  .
.
.
.
"on".
.
.
.
.
NOUN*- - - "sheets".
.
.
.
.
A JP I  .
.
.
.
AVP  .
.
.
.
.
ADV*  .
.
.
.
"no"i .
.
.
.
AD J*  .
.
.
.
.
.
sma l le r "I .
.
.
.
PP I  .
.
.
.
.
PREP  .
.
.
.
.
.
than"J .
.
.
.
.
QUANT- - -ADJ*  .
.
.
.
.
15"I .
.
.
.
.
NOUN*- - - " inches"- - -CONJ  .
.
.
.
"and"- - -PTPRTCL IVERB*- - - "drawn"iPP i  .
.
.
.
.
PREP  .
.
.
.
.
.
to"J .
.
.
.
.
DET  .
.
.
.
.
AD J*  .
.
.
.
"a"E .
.
.
.
.
NOUN*- - - "sca le"I .
.
.
.
.
A JP  .
.
.
.
AVP  .
.
.
.
.
ADV*- - -  PUNC .
.
.
.  "
.
""no".
.
.
.
AD J*  .
.
.
.  "
smal le r ".
.
.
.
PP \ [  .
.
.
.
.
PREP  .
.
.
.  "
than"i .
.
.
.
.
NOUN*- - - " I /8 th"i .
.
.
.
.
PP I  .
.
.
.
.
PREP  .
.
.
.
"o f "I .
.
.
.
.
DET  .
.
.
.
.
AD J*  .
.
.
.
"an"i .
.
.
.
.
NOUN*  .
.
.
.
.
i nch".
.
.
.
PP I  .
.
.
.
.
PREP  .
.
.
.  "
to"i .
.
.
.
.
DET  .
.
.
.
.
AD J*  .
.
.
.  "
the"i .
.
.
.
.
NOUN*  .
.
.
.
.
foo t "POSS IBLE  GRAMMATICAL  ERROR:  M ISS ING COMMA.Before  an  approva l  can  be  i ssued  i t  w i l l  be  necessary .
.
.CONSIDER:Be fore  an  approva l  can  be  i ssued ,  i t  w i l l  be  necessary .
.
.A COMMA IS  NEEDED TO DEF INE  CLAUSE BOUNDARIESFigure 10.
Comma error in long complex sentence.154 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983K.
Jensen, G.E.
Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose Fixingsyntactic mistakes when they occur in otherwise legiti-mate sentences.
To this core grammar we couple afitting procedure that produces a reasonable best-guessparse for all other text strings, regardless of whetherthey meet the grammar's  criteria for sentencehood.The fitted parse then allows us to check even non-sentences for those categories of syntactic errorsthat we can correct.Cr i t iqu ing  S ty l i s t i c  I I I - Formedness  in EP ISTLEThe style component of EPISTLE uses the sentencestructures provided by the parser as input for stylisticcritiquing.
It consists of a set of NLP rules that applyto parse trees of sentences, identify stylistic errors andsuggest corrections.
There is a fundamental differencebetween style analysis and grammar analysis, however:the grammar ule-system is based on a set of objectivesyntactic criteria that determine whether the input iswell-formed or not.
The style rules, by contrast, arebased on relative criteria.
They place the input on acontinuum of stylistic acceptability so that what is astylistic error becomes a matter of degree.Types of styl ist ic i l l -formedness.
(1) Punctuation.
Stylistic i l l -formedness isrelative since it depends on both the linguistic and theextra-linguistic contexts.
Linguistically, a combinationof grammatical factors in the sentence can make asentence more or less ill-formed.
For example, theneed for a comma in a compound sentence increaseswith the length of the conjoined clauses.
Thus, in "adecision was reached and the meeting ended," a com-ma before the "and"  is optional; but in "a decisionwhich was moderate nough to satisfy even my objec-tions was reached, and the meeting was finallyadjourned," a comma is necessary.
An even longersentence containing several other commas might re-quire a semi-colon before the "and".To be able to detect the missing comma, the stylis-tic rule must have access to syntactic information pro-vided by the parser.
It has to know that the sentenceis compound.
Moreover,  it has to know that eachclause contains its own subject (in this case, "adecision" and "the meeting"),  since a compound sen-tence with only one subject does not take a comma.After all the syntactic conditions are checked and met,the rule measures the length of the clauses to deter-mine how badly the comma is needed.
The output isshown in Figure 11.
(2) Other Types of Stylistic I l l -Formedness.The style component of EPISTLE detects other in-stances of missing or faulty punctuation (a comma atthe end of a subordinate clause, no colon before asingle noun-phrase, etc).
It also addresses ome typesof complicated grammatical constructions that mayimpede the reader's comprehension, such as excessivelength, excessive noun-modif ication (e.g.
"early child-hood thought disorder misdiagnosis"),  and excessivenegation (e.g., "neither the professor, nor his two as-sistants, who have been working with him on this pro-ject, haven't noticed the theft").
Some usage viola-tions are signaled, such as "split infinitives" and theusage of "most"  instead of "a lmost" ;  and finally,some cosmetic changes are proposed when the syntac-tic structure is too uniform or when there is excessiverepetition.
(3) Repetition.
Repetit ion is another instanceof the relative nature of stylistic ill-formedness.
Gen-erally, repetition of strings is to be avoided; however,some cases of repetit ion are more acceptable thanothers.
The degree of acceptabil ity depends on thesyntactic function of the repeated strings.
In "themeeting is very very important,"  the two instances of"very"  have the same syntactic function: they bothintensify the adjective " important."
This double repe-tition, lexical and syntactic, is considered poor style.The error correction for this sentence can be seen inFigure 12.
By contrast, in "it does not surprise methat that institution no longer exists," the two in-stances of " that"  have different syntactic roles - oneis a conjunction; the other, a determiner.
This sen-tence is stylistically more acceptable.
Finally, in "whathe does does not concern us," the two instances of"does"  belong to two different clauses.
The style rulesaccept lexical repetition of this kind.Correct ing sty l ist ic  errors  in a f i t ted  parse.Because syntactic information is always available, thestyle rules can apply to fitted parses, as they do toregular sentences.
They not only signal stylistic errorswithin the fitted parse but also assign different degreesof acceptability to different types of fitted parses.
Asnoun phrases are the most commonly encountered typeof fragment, the rules accept noun phrases ( "Mywarmest regards to your son") but mark subordinateclauses as incomplete ("Because he refused to sign thepapers"),  as shown in Figures 13 and 14.Extra- l inguist ic  factors.
The degree of stylistici l l -formedness of a sentence depends on extra-linguistic factors.
The use of contracted verb-forms(e.g.
"don ' t " ,  "I'11") is quite acceptable in informalwriting; it is to be avoided, though, in formal docu-ments.
Style rules should accommodate different de-grees of formality.
They should also be sensitive to thestylistic norms observed in different domains.
In atechnical manual, for example, a uniform sentence-pattern is preferred, as it facilitates the reader's com-prehension; in freshman compositions, on the otherhand, a variety of sentence-patterns is more appropri-ate as it breaks the monotony.
The style component ofEPISTLE will address such extra-linguistic factors inaddition to the purely linguistic factors.
In order to doso, it will present the user with a menu of style op-tions.
The selection of the formal option will activatethe "no verb-contract ion" rule; the selection of theinformal option will suppress it.
Similarly, theAmerican Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 155K.
Jensen, G.E.
Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose FixingCMPD - - -VP I  .
.
.
.
.
NP I  .
.
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.
"A"I .
.
.
.
.
NOUN*- - - "dec is ion"I .
.
.
.
.
RELCLL- -NP  .
.
.
.
.
.
PRON*- - - "wh ich"i - -VERB*- - - "was"I - -A JP I  .
.
.
.
ADJ*  .
.
.
.
"moderate"I .
.
.
.
ADV .
.
.
.
.
"enough"I .
.
.
.
INFCL I - - INFTO- - - " to"l - -VERB*- - - "sat i s fy"i - -NP l  .
.
.
.
.
A JP  .
.
.
.
.
ADV*  .
.
.
.
"even"I .
.
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.
"myI .
.
.
.
.
NOUN*- - - "ob jec t ions".
.
.
.
.
VERB .
.
.
.
"was".
.
.
.
.
VERB*  .
.
.
.
.
reached"- - -CONJ*  .
.
.
.
.
and"- - -VP  l .
.
.
.
.
NP  I .
.
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.  "
the"I I .
.
.
.
.
NOUN*- - - "meet ing"I .
.
.
.
.
VERB .
.
.
.
"was"I .
.
.
.
.
AVP  .
.
.
.
.
ADV*  .
.
.
.  "
f ina l ly "I .
.
.
.
.
VERB*  .
.
.
.
.
ad journed"- - -PUNC .
.
.
.  "
.
"STYL IST IC  WEAKNESS:  M ISS ING COMMA IN  COMPOUND SENTENCE.WHY NOT HAVE A COMMA BEFORE THE CONJUNCTION?.
.
.was  reached,  and  the  meet ing  was  f ina l ly  ad journed.Figure 11.
Diagnosis of a punctuation problem.DECL I - - -NP  I .
.
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.
.
.
The"I I .
.
.
.
.
NOUN*  - - - "meet ing"i - - -VERB*  - - - "  i s "i - - -A JP  i .
.
.
.
AVP  .
.
.
.
.
ADV*  .
.
.
.
"ve  ry"I I .
.
.
.
AVP  .
.
.
.
.
ADV*  .
.
.
.
.
.
very"I I .
.
.
.
ADJ*  .
.
.
.
.
.
impor tant"I - - -  PUNC .
.
.
.
.
.
.
"STYL IST IC  WEAKNESS:  REPET IT ION.WHY NOT AVOID REPET IT ION?The  meet ing  i s  very  impor tant .Figure 12.
Diagnosis of a repetition problem.F ITTED I - - -NP*  I .
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.
.
.
My"i .
.
.
.
A JP  .
.
.
.
.
ADJ*  .
.
.
.
"warme s t "i .
.
.
.
NOUN*  .
.
.
.
.
regards"- - -PP  I .
.
.
.
.
PREP  .
.
.
.  "
to"E .
.
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.
"your"I .
.
.
.
.
NOUN*  - - - "  s on"- - -PUNC .
.
.
.  "
.
"Figure 13.
Fitted noun phrase (no style problems).156 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983K.
Jensen, G.E.
Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose FixingF ITTED - - -CONJ  .
.
.
.
"Because"- - -VP* I  .
.
.
.
NP  .
.
.
.
.
.
PRON*- - - "he"I .
.
.
.
VERB*  .
.
.
.
.
re fused"I .
.
.
.
INFCL I - - INFTO .
.
.
.
.
to"l - -VERB*- - - "s ign"I - -NP I  .
.
.
.
.
DET  .
.
.
.
.
AD J*  .
.
.
.
.
.
the"I .
.
.
.
.
NOUN*- - - "papers"- - -PUNC .
.
.
.  "
.
"POSS IBLE  STYL IST IC  WEAKNESS:  INCOMPLETE SENTENCE.WHY NOT COMPLETE THIS  SENTENCE BY  ADDING A MAIN  CLAUSE?Figure 14.
Fragment (subordinate clause) with diagnosis.technical-writing option will diagnose excessive syntac-tic variety, whereas the creative-writ ing option willdiagnose monotonous regularity.Potential Diff icultiesBecause the error-detection and fitting procedurespermit all sorts of non-sentences to survive, they willinevitably increase the number of ambiguities that thesystem produces, and will require that additional effortbe spent to restrict the number of possible parses.This is a difficult but by no means impossible task,since all it entails is the addition of more thoroughconstraints on the core grammar.As an example, consider the input string"What exactly does that 15 months do.
"The intended meaning of this string could probablybe paraphrased as "What exactly does that 15-monthperiod mean?"
But there are two problems with theinput.
First and most confusingly, the phrase "that 15months," as given, has a plural head noun ("months")and a singular determiner ( " that" ) .
Since the CGcannot understand meaning, it has no way of tellingthat the given phrase might be an elided form of "that15-month period."
It therefore detects and corrects asyntactic error: number disagreement between premo-difier and noun.
Secondly, the input string should endwith a question mark.
But in order to diagnose thiserror, the grammar needs to realize that a questionwas intended.When the problem sentence was submitted to anearlier version of the CG, three parses resulted(Figures 15-17).The parse in Figure 15 would be appropriate for asentence such as "Who(ever)  exactly does that jobprevails."
However, it is thoroughly unhelpful for thesentt:nce at hand.
It diagnoses two errors, neither ofwhich really exists.Figure 16 is close to acceptable for the input string.If the time adverbial NP (AVPNP in the parse tree)were replaced by a subject NP, the syntactic structurewould be correct for the intended meaning.
As thingsstand, this parse is only 50% helpful: it correctlydiagnoses the missing question mark, but it incorrectlyinsists that "month"  should be singular in number.The third parse (Figure 17) gives the desired singleerror correction, but it does so on the basis of a totallyinappropriate parse.
The structure in this figure wouldfit a question like "Who exactly suffers (in order) thatmany people might live?
"The current version of the CG blocks the threeparses in Figures 15 through 17 on a principled basis.Figure 15 can be blocked by tightening some con-straints on the diagnosis of subject-verb number disa-greement in fitted parses.
Figure 16 is blocked be-cause of the presence of an adverbial NP where thesubject NP ought to be.
Figure 17 is blocked by stipu-lating that all subordinate clauses beginning with a" that"  conjunction should have modal or subjunctivepredicates.An acceptable parse (Figure 18) is provided whenthe number agreement restriction is removed fromphrases like "that 15 months."
This is accomplishedby telling the proper rule to ignore number agreementin particular cases that involve a small subset of quan-tified English time words.
Admittedly, the fix wouldbe more pleasing if it were part of a larger scheme forunderstanding meaning and context.
But the correc-tion moves in the right direction, and certainly doesnot prevent future processing with a more intelligentsemantic component.
This situation clearly illustrateshow error detection results in the addition of finerconstraints on the core grammar.Related WorkThe parsing approach closest in spirit to our fittingprocedure is that described in Slocum (1983, p. 170):the LRC Machine Translation System uses a "shortestpath" technique to construct a "phrasal analysis" ofungrammatical input.
With this analysis, phrases canbe translated separately, even in the absence of a totalsentence parse.
Aside from Slocum's work, most ofthe reports in this field suggest that unparsable orAmerican Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly-December 1983 157K.
Jensen, G.E.
Heidorn L.A. Miller, and Y. Ravin Parse Fitting and Prose FixingDECL - - -NP I  .
.
.
.
.
PRON*- - - "What"I .
.
.
.
.
VP I  .
.
.
.
.
AVP  .
.
.
.
.
ADV*  .
.
.
.
.
.
exact ly"I .
.
.
.
.
VERB .
.
.
.
"does"I .
.
.
.
.
NP I  .
.
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.  "
that"I .
.
.
.
.
QUANT- - -ADJ*  .
.
.
.
"15"I .
.
.
.
.
NOUN*- - - "months"- - -VERB*- - - "do"- - -PUNC .
.
.
.  "
.
"GRAMMATICAL  ERROR:  SUBJECT-VERB NUMBER DISAGREEMENT.WHAT exact ly  does  that  15 months  DO.CONSIDER:WHAT exact ly  does  that  15 months  DOES.GRAMMATICAL  ERROR:  PREMODIF IER-NOUN NUMBER DISAGREEMENT.What  exact ly  does  THAT.
.
.MONTHS do .CONSIDER:What  exact ly  does  THAT.
.
.MONTH do .THE COMBINED GRAMMATICAL  CORRECTIONS ARE:What  exact ly  does  that  15 month  does .Figure 15.
Two faulty error diagnoses; inappropriate parse.il l-formed input should be handled by relaxationtechniques, that is, by relaxing restrictions in the gram-mar rules in some principled way.
This is undoubtedlya useful strategy - one which EPISTLE makes use of,in fact, in its rules for detecting grammatical errors(Heidorn et al 1982).
However, it is questionablewhether such a strategy can ultimately succeed in theface of the overwhelming (for all practical purposes,infinite) variety of ill-formedness with which we arefaced when we set out to parse truly unrestricted natu-ral language input.
If all ill-formedness is rule-based(Weischedel and Sondheimer 1981, p. 3), it can onlybe by some very loose definition of the term rule, suchas that which might apply to the fitting algorithm de-scribed here.Thus Weischedel and Black (1980) suggest threetechniques for responding intelligently to unparsableinputs:(a) using presuppositions to determine user assump-tions; this course is not available to a syntacticgrammar like EPISTLE's;(b) using relaxation techniques;(c) supplying the user with information about thepoint where the parse is blocked; this wouldrequire an interactive nvironment, which wouldnot be possible for every type of natural lan-guage processing application.Kwasny and Sondheimer (1981) are strong propo-nents of relaxation techniques, which they use to han-dle both cases of clearly ungrammatical structures,such as co-occurrence violations like subject/verb disa-greement, and cases of perfectly acceptable but diffi-cult constructions (ellipsis and conjunction).Weischedel and Sondheimer (1982) describe animproved ellipsis processor.
No longer is ellipsis han-dled with relaxation techniques, but by predictingtransformations of previous parsing paths that wouldallow for the matching of fragments with plausiblecontexts.
This plan would be appropriate as a nextstep after the fitted parse, but it does not guarantee aparse for all elided inputs.Hayes and Mouradian (1981) also use the relaxa-tion method.
They achieve flexibility in their parserby relaxing consistency constraints (grammatical restric-tions, like Kwasny and Sondheimer's co-occurrenceviolations) and also by relaxing ordering constraints.However, they are working with a restricted-domainsemantic system and their approach, as they admit,"does not embody a solution for flexible parsing ofnatural anguage in general" (p. 236).The work of Wilks is heavily semantic and there-fore quite different from EPISTLE, but his generalphilosophy meshes nicely with the philosophy of thefitted parse: "It  is proper to prefer the normal ... butit would be absurd ... not to accept the abnormal if itis described" (Wilks 1975, p. 267).158 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983K.
Jensen, G.E.
Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose FixingDECL - - -NP  .
.
.
.
.
.
.
PRON*- - - "What"- - -AVP  .
.
.
.
.
.
ADV*  .
.
.
.
"exact ly"- - -VERB .
.
.
.
.
"does"- - -AVPNPI - - -DET  .
.
.
.
.
ADJ*  .
.
.
.  "
that"I - - -QUANT- - -ADJ*  ...... 15"l - - -NOUN*- - - "months"- - -VERB*  .
.
.
.
"do"- - -PUNC .
.
.
.
.  "
.
"GRAMMATICAL  ERROR:  MISS ING QUEST ION MARK.What  exact ly  does  that  15 months  do .CONSIDER:What  exact ly  does  that  15 months  do?GRAMMATICAL  ERROR:  PREMODIF IER-NOUN NUMBER DISAGREEMENT.What  exact ly  does  THAT.
.
.MONTHS do .CONSIDER:What  exact ly  does  THAT.
.
.MONTH do .THE COMBINED GRAMMATICAL  CORRECTIONS ARE:What  exact ly  does  that  15 month  do?Figure l6.
Onefaulty diagnosis, onecorrect;near-satisfactory parse.DECL I - - -NP  .
.
.
.
.
.
PRON*  ..... What"- - -AVP  .
.
.
.
.
ADV*  .
.
.
.
"exact ly"- - -VERB*- - - "does"- - -SUBCL I - -CONJ  ...... that"I - -NP I  .
.
.
.
.
.
QUANT- - -ADJ  ~ ...... 15"I I .
.
.
.
.
NOUN*- - -~ 'months"I - -VERB*- - - "do"- - -PUNC .
.
.
.  "
.
"GRAMMATICAL  ERROR:  MISS ING QUEST ION MARK.What  exact ly  does  that  15 months  do .CONSIDER:What  exact ly  does  that  15 months  do?Figure 17.
Correct error diagnosis but misleading parse.DECL I - - -NP  .
.
.
.
.
.
PRON*- - - "What"- - -AVP  .
.
.
.
.
ADV * .
.
.
.
"exact ly"- - -VERB .
.
.
.
"does"- - -NP I  .
.
.
.
.
DET  .
.
.
.
.
ADJ*  .
.
.
.  "
that"I .
.
.
.
.
QUANT- - -ADJ*  ...... 15"I .
.
.
.
.
NOUN*  ..... months"- - -VERB, - - - , ,do  ,,- - -PUNC .
.
.
.  "
.
"GRAMMATICAL  ERROR:  MISS ING QUEST ION MARK.What  exact ly  does  that  15 months  do .CONSIDER:What  exact ly  does  that  15 months  do?Figure 18.
Correct error diagnosis; satisfactory parse.American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 159K.
Jensen, G.E.
Heidorn, L.A. Miller, and Y. Ravin Parse Fitting and Prose FixingReferencesHayes, P.J.
and Mouradian, G.V.
1981 Flexible Parsing.
Am.
J.Comp.
Ling.
7(4): 232-242.Heidorn, G.E.
1972 Natural Language Inputs to a SimulationProgramming System.
Technical Report NPS-55HD72101A.Monterey, California: Naval Postgraduate School.Heidorn, G.E.
1982 Experience with an Easily Computed Metricfor Ranking Alternative Parses.
Proc.
20th Annual Meeting ofthe ACL.
Toronto, Canada: 82-84.Heidorn, G.E.
; Jensen, K.; Miller, L.A.; Byrd, R.J.; and Chodorow,M.S.
1982 The EPISTLE Text-Critiquing System.
IBM Sys-tems Journal 21(3): 305-326.Jensen, K. and Heidorn, G.E.
1983 The Fitted Parse: 100%Parsing Capability in a Syntactic Grammar of English.
Proc.Conf.
on Applied Natural Language Processing.
Santa Monica,California: 93-98.Kwasny, S.C. and Sondheimer, N.K.
1981 Relaxation Techniquesfor Parsing Ill-Formed Input.
Am.
J. Comp.
Ling.
7(2): 99-108.Lasnik, H. and Freidin, R. 1981 Core Grammar, Case Theory,and Markedness.
Proc.
1979 GLOW Conf.
Pisa, Italy.Miller, L.A.; Heidorn, G.E; and Jensen, K. 1981 Text-Critiquingwith the EPISTLE System: An Authors's Aid to Better Syn-tax.
AFIPS Conf.
Proc., Vol.
50.
Arlington, Virginia: 649-655.Slocum, Jonathan.
1983 A Status Report on the LRC MachineTranslation System.
Proc.
Conf.
on Applied Natural LanguageProcessing.
Santa Monica, California: 166-173.Weischedel, R.M.
and Black, J.E.
1980 Responding Intelligentlyto Unparsable Inputs.
Am.
J. Comp.
Ling.
6(2): 97-109.Weischedel, R.M.
and Sondheimer, N.K.
1981 A Framework forProcessing Ill-Formed Input.
Research Report.
University ofDelaware.Weischedel, R.M.
and Sondheimer, N.K.
1982 An ImprovedHeuristic for Ellipsis Processing.
Proc.
20th Annual Meeting ofthe ACL.
Toronto, Canada: 85-88.Wilks, Yorick 1975 An Intelligent Analyzer and Understander ofEnglish.
Comm.
ACM 18(5): 264-274.160 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
