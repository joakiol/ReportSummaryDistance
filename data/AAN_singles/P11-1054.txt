Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 530?540,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsIn-domain Relation Discovery with Meta-constraintsvia Posterior RegularizationHarr Chen, Edward Benson, Tahira Naseem, and Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{harr, eob, tahira, regina} @csail.mit.eduAbstractWe present a novel approach to discovering re-lations and their instantiations from a collec-tion of documents in a single domain.
Ourapproach learns relation types by exploitingmeta-constraints that characterize the generalqualities of a good relation in any domain.These constraints state that instances of asingle relation should exhibit regularities atmultiple levels of linguistic structure, includ-ing lexicography, syntax, and document-levelcontext.
We capture these regularities via thestructure of our probabilistic model as wellas a set of declaratively-specified constraintsenforced during posterior inference.
Acrosstwo domains our approach successfully recov-ers hidden relation structure, comparable toor outperforming previous state-of-the-art ap-proaches.
Furthermore, we find that a smallset of constraints is applicable across the do-mains, and that using domain-specific con-straints can further improve performance.
11 IntroductionIn this paper, we introduce a novel approach for theunsupervised learning of relations and their instan-tiations from a set of in-domain documents.
Givena collection of news articles about earthquakes, forexample, our method discovers relations such as theearthquake?s location and resulting damage, and ex-tracts phrases representing the relations?
instantia-tions.
Clusters of similar in-domain documents are1The source code for this work is available at:http://groups.csail.mit.edu/rbg/code/relation extraction/A strong earthquake rocked the Philippine island of Min-doro early Tuesday, [destroying]ind [some homes]arg ...A strong earthquake hit the China-Burma border earlyWednesday ...
The official Xinhua News Agency said[some houses]arg were [damaged]ind ...A strong earthquake with a preliminary magnitude of 6.6shook northwestern Greece on Saturday, ... [destroying]ind[hundreds of old houses]arg ...Figure 1: Excerpts from newswire articles about earth-quakes.
The indicator and argument words for the dam-age relation are highlighted.increasingly available in forms such as Wikipedia ar-ticle categories, financial reports, and biographies.In contrast to previous work, our approach learnsfrom domain-independent meta-constraints on rela-tion expression, rather than supervision specific toparticular relations and their instances.
In particular,we leverage the linguistic intuition that documentsin a single domain exhibit regularities in how theyexpress their relations.
These regularities occur bothin the relations?
lexical and syntactic realizations aswell as at the level of document structure.
For in-stance, consider the damage relation excerpted fromearthquake articles in Figure 1.
Lexically, we ob-serve similar words in the instances and their con-texts, such as ?destroying?
and ?houses.?
Syntacti-cally, in two instances the relation instantiation is thedependency child of the word ?destroying.?
On thediscourse level, these instances appear toward thebeginning of their respective documents.
In general,valid relations in many domains are characterized bythese coherence properties.We capture these regularities using a Bayesianmodel where the underlying relations are repre-530sented as latent variables.
The model takes as in-put a constituent-parsed corpus and explains how theconstituents arise from the latent variables.
Each re-lation instantiation is encoded by the variables asa relation-evoking indicator word (e.g., ?destroy-ing?)
and corresponding argument constituent (e.g.,?some homes?
).2 Our approach capitalizes on rela-tion regularity in two ways.
First, the model?s gen-erative process encourages coherence in the localfeatures and placement of relation instances.
Sec-ond, we apply posterior regularization (Grac?a etal., 2007) during inference to enforce higher-leveldeclarative constraints, such as requiring indicatorsand arguments to be syntactically linked.We evaluate our approach on two domains pre-viously studied for high-level document structureanalysis, news articles about earthquakes and finan-cial markets.
Our results demonstrate that we cansuccessfully identify domain-relevant relations.
Wealso study the importance and effectiveness of thedeclaratively-specified constraints.
In particular, wefind that a small set of declarative constraints areeffective across domains, while additional domain-specific constraints yield further benefits.2 Related WorkExtraction with Reduced Supervision Recentresearch in information extraction has taken largesteps toward reducing the need for labeled data.
Ex-amples include using bootstrapping to amplify smallseed sets of example outputs (Agichtein and Gra-vano, 2000; Yangarber et al, 2000; Bunescu andMooney, 2007; Zhu et al, 2009), leveraging ex-isting databases that overlap with the text (Mintzet al, 2009; Yao et al, 2010), and learning gen-eral domain-independent knowledge bases by ex-ploiting redundancies in large web and news cor-pora (Hasegawa et al, 2004; Shinyama and Sekine,2006; Banko et al, 2007; Yates and Etzioni, 2009).Our approach is distinct in both the supervisionand data we operate over.
First, in contrast to boot-strapping and database matching approaches, welearn from meta-qualities, such as low variability insyntactic patterns, that characterize a good relation.2We do not use the word ?argument?
in the syntactic sense?a relation?s argument may or may not be the syntactic depen-dency argument of its indicator.We hypothesize that these properties hold across re-lations in different domains.
Second, in contrast towork that builds general relation databases from het-erogeneous corpora, our focus is on learning the re-lations salient in a single domain.
Our setup is moregermane to specialized domains expressing informa-tion not broadly available on the web.Earlier work in unsupervised information extrac-tion has also leveraged meta-knowledge indepen-dent of specific relation types, such as declaratively-specified syntactic patterns (Riloff, 1996), frequentdependency subtree patterns (Sudo et al, 2003), andautomatic clusterings of syntactic patterns (Lin andPantel, 2001; Zhang et al, 2005) and contexts (Chenet al, 2005; Rosenfeld and Feldman, 2007).
Our ap-proach incorporates a broader range of constraintsand balances constraints with underlying patternslearned from the data, thereby requiring more so-phisticated machinery for modeling and inference.Extraction with Constraints Previous work hasrecognized the appeal of applying declarative con-straints to extraction.
In a supervised setting, Rothand Yih (2004) induce relations by using linear pro-gramming to impose global declarative constraintson the output from a set of classifiers trained on lo-cal features.
Chang et al (2007) propose an objec-tive function for semi-supervised extraction that bal-ances likelihood of labeled instances and constraintviolation on unlabeled instances.
Recent work hasalso explored how certain kinds of supervision canbe formulated as constraints on model posteriors.Such constraints are not declarative, but insteadbased on annotations of words?
majority relation la-bels (Mann and McCallum, 2008) and pre-existingdatabases with the desired output schema (Bellareand McCallum, 2009).
In contrast to previous work,our approach explores a different class of constraintsthat does not rely on supervision that is specific toparticular relation types and their instances.3 ModelOur work performs in-domain relation discovery byleveraging regularities in relation expression at thelexical, syntactic, and discourse levels.
These regu-larities are captured via two components: a proba-bilistic model that explains how documents are gen-erated from latent relation variables and a technique531?
?
?
?is_verb 0 1 0earthquake 1 0 0hit 0 1 0?
?
?
?has_proper 0 0 1has_number 0 0 0depth 1 3 2Figure 2: Words w and constituents x of syntactic parsesare represented with indicator features ?i and argumentfeatures ?a respectively.
A single relation instantiation isa pair of indicator w and argument x; we filter w to benouns and verbs and x to be noun phrases and adjectives.for biasing inference to adhere to declaratively-specified constraints on relation expression.
Thissection describes the generative process, while Sec-tions 4 and 5 discuss declarative constraints.3.1 Problem FormulationOur input is a corpus of constituent-parsed docu-ments and a number K of relation types.
The outputis K clusters of semantically related relation instan-tiations.
We represent these instantiations as a pairof indicator word and argument sequence from thesame sentence.
The indicator?s role is to anchor arelation and identify its type.
We only allow nounsor verbs to be indicators.
For instance, in the earth-quake domain a likely indicator for damage wouldbe ?destroyed.?
The argument is the actual rela-tion value, e.g., ?some homes,?
and corresponds toa noun phrase or adjective.3Along with the document parse trees, we utilizea set of features ?i(w) and ?a(x) describing eachpotential indicator word w and argument constituentx, respectively.
An example feature representationis shown in Figure 2.
These features can encodewords, part-of-speech tags, context, and so on.
Indi-cator and argument feature definitions need not bethe same (e.g., has number is important for argu-3In this paper we focus on unary relations; binary relationscan be modeled with extensions of the hidden variables and con-straints.ments but irrelevant for indicators).43.2 Generative ProcessOur model associates each relation type k with a setof feature distributions ?k and a location distribution?k.
Each instantiation?s indicator and argument, andits position within a document, are drawn from thesedistributions.
By sharing distributions within eachrelation, the model places high probability mass onclusters of instantiations that are coherent in featuresand position.
Furthermore, we allow at most one in-stantiation per document and relation, so as to targetrelations that are relevant to the entire document.There are three steps to the generative process.First, we draw feature and location distributions foreach relation.
Second, an instantiation is selectedfor every pair of document d and relation k. Third,the indicator features of each word and argumentfeatures of each constituent are generated based onthe relation parameters and instantiations.
Figure 3presents a reference for the generative process.Generating Relation Parameters Each relation kis associated with four feature distribution param-eter vectors: ?ik for indicator words, ?bik for non-indicator words, ?ak for argument constituents, and?bak for non-argument constituents.
Each of these isa set of multinomial parameters per feature drawnfrom a symmetric Dirichlet prior.
A likely indica-tor word should have features that are highly proba-ble according to ?ik, and likewise for arguments and?ak.
Parameters ?bik and ?bak represent background dis-tributions for non-relation words and constituents,similar in spirit to other uses of background distri-butions that filter out irrelevant words (Che, 2006).5By drawing each instance from these distributions,we encourage the relation to be coherent in local lex-ical and syntactic properties.Each relation type k is also associated with a pa-rameter vector ?k over document segments drawnfrom a symmetric Dirichlet prior.
Documents aredivided into L equal-length segments; ?k states howlikely relation k is for each segment, with one nulloutcome for the relation not occurring in the doc-ument.
Because ?k is shared within a relation, its4We consider only categorical features here, though the ex-tension to continuous or ordinal features is straightforward.5We use separate background distributions for each relationto make inference more tractable.532For each relation type k:?
For each indicator feature ?i draw feature distri-butions ?ik,?i , ?bik,?i ?
Dir(?0)?
For each argument feature ?a draw feature dis-tributions ?ak,?a , ?bak,?a ?
Dir(?0)?
Draw location distribution ?k ?
Dir(?0)For each relation type k and document d:?
Select document segment sd,k ?
Mult(?k)?
Select sentence zd,k uniformly from segmentsd,k, and indicator id,k and argument ad,k uni-formly from sentence zd,kFor each word w in every document d:?
Draw each indicator feature ?i(w) ?Mult(1Z?Kk=1 ?k,?i), where ?k,?i is ?ik,?iif id,k = w and ?bik,?i otherwiseFor each constituent x in every document d:?
Draw each argument feature ?a(x) ?Mult(1Z?Kk=1 ?k,?a), where ?k,?a is ?ak,?aif ad,k = x and ?bak,?a otherwiseFigure 3: The generative process for model parametersand features.
In the above Dir and Mult refer respectivelyto the Dirichlet distribution and multinomial distribution.Fixed hyperparameters are subscripted with zero.instances will tend to occur in the same relative po-sitions across documents.
The model can learn, forexample, that a particular relation typically occurs inthe first quarter of a document (if L = 4).Generating Relation Instantiations For every rela-tion type k and document d, we first choose whichportion of the document (if any) contains the instan-tiation by drawing a document segment sd,k from?k.
Our model only draws one instantiation per pairof k and d, so each discovered instantiation within adocument is a separate relation.
We then choose thespecific sentence zd,k uniformly from within the seg-ment, and the indicator word id,k and argument con-stituent ad,k uniformly from within that sentence.Generating Text Finally, we draw the feature val-ues.
We make a Na?
?ve Bayes assumption betweenfeatures, drawing each independently conditionedon relation structure.
For a word w, we want all re-lations to be able to influence its generation.
Towardthis end, we compute the element-wise product offeature parameters across relations k = 1, .
.
.
,K,using indicator parameters ?ik if relation k selectedw as an indicator word (if id,k = w) and backgroundparameters ?bik otherwise.
The result is then normal-ized to form a valid multinomial that produces wordw?s features.
Constituents are drawn similarly fromevery relations?
argument distributions.4 Inference with ConstraintsThe model presented above leverages relation reg-ularities in local features and document placement.However, it is unable to specify global syntacticpreferences about relation expression, such as indi-cators and arguments being in the same clause.
An-other issue with this model is that different relationscould overlap in their indicators and arguments.6To overcome these obstacles, we apply declara-tive constraints by imposing inequality constraintson expectations of the posterior during inferenceusing posterior regularization (Grac?a et al, 2007).In this section we present the technical detailsof the approach; Section 5 explains the specificlinguistically-motivated constraints we consider.4.1 Inference with Posterior RegularizationWe first review how posterior regularization impactsthe variational inference procedure in general.
Let?, z, and x denote the parameters, hidden struc-ture, and observations of an arbitrary model.
Weare interested in estimating the posterior distributionp(?, z | x) by finding a distribution q(?, z) ?
Q thatis minimal in KL-divergence to the true posterior:KL(q(?, z) ?
p(?, z | x))=?q(?, z) logq(?, z)p(?, z, x)d?dz + log p(x).
(1)For tractability, variational inference typicallymakes a mean-field assumption that restricts the setQ to distributions where ?
and z are independent,i.e., q(?, z) = q(?)q(z).
We then optimize equa-tion 1 by coordinate-wise descent on q(?)
and q(z).To incorporate constraints into inference, we fur-ther restrict Q to distributions that satisfy a given6In fact, a true maximum a posteriori estimate of the modelparameters would find the same most salient relation over andover again for every k, rather than finding K different relations.533set of inequality constraints, each of the formEq[f(z)] ?
b.
Here, f(z) is a deterministic func-tion of z and b is a user-specified threshold.
Inequal-ities in the opposite direction simply require negat-ing f(z) and b.
For example, we could apply a syn-tactic constraint of the form Eq[f(z)] ?
b, wheref(z) counts the number of indicator/argument pairsthat are syntactically connected in a pre-specifiedmanner (e.g., the indicator and argument modify thesame verb), and b is a fixed threshold.Given a set C of constraints with functions fc(z)and thresholds bc, the updates for q(?)
and q(z) fromequation 1 are as follows:q(?)
= argminq(?)KL(q(?)
?
q?(?
)), (2)where q?(?)
?
expEq(z)[log p(?, z, x)], andq(z) = argminq(z)KL(q(z) ?
q?(z))s.t.
Eq(z)[fc(z)] ?
bc, ?c ?
C, (3)where q?
(z) ?
expEq(?
)[log p(?, z, x)].
Equation 2is not affected by the posterior constraints and is up-dated by setting q(?)
to q?(?).
We solve equation 3in its dual form (Grac?a et al, 2007):argmin?
?c?C?cbc + log?zq?
(z)e?Pc?C ?cfc(z)s.t.
?c ?
0, ?c ?
C. (4)With the box constraints of equation 4, a numericaloptimization procedure such as L-BFGS-B (Byrdet al, 1995) can be used to find optimal dual pa-rameters ??.
The original q(z) is then updated toq?
(z) exp(?
?c?C ?
?cfc(z))and renormalized.4.2 Updates for our ModelOur model uses this mean-field factorization:q(?, ?, z, a, i)=K?k=1q(?k; ?
?k)q(?ik; ?
?ik)q(?bik ; ?
?bik )q(?ak; ?
?ak)q(?bak ; ?
?bak )?
?dq(zd,k, ad,k, id,k; c?d,k) (5)In the above, ??
and ??
are Dirichlet distribution pa-rameters, and c?
are multinomial parameters.
Notethat we do not factorize the distribution of z, i, anda for a single document and relation, instead repre-senting their joint distribution with a single set ofvariational parameters c?.
This is tractable because asingle relation occurs only once per document, re-ducing the joint search space of z, i, and a. Thefactors in equation 5 are updated one at a time whileholding the other factors fixed.Updating ??
Due to the Na?
?ve Bayes assumptionbetween features, each feature?s q(?)
distributionscan be updated separately.
However, the productbetween feature parameters of different relations in-troduces a nonconjugacy in the model, precludinga closed form update.
Instead we numerically opti-mize equation 1 with respect to each ?
?, similarly toprevious work (Boyd-Graber and Blei, 2008).
Forinstance, ??ik,?
of relation k and feature ?
is updatedby finding the gradient of equation 1 with respect to??ik,?
and applying L-BFGS.
Parameters ?
?bi, ?
?a, and?
?ba are updated analogously.Updating ??
This update follows the standardclosed form for Dirichlet parameters:?
?k,` = ?0 + Eq(z,a,i)[C`(z, a, i)], (6)whereC` counts the number of times z falls into seg-ment ` of a document.Updating c?
Parameters c?
are updated by first com-puting an unconstrained update q?
(z, a, i; c??):c?
?d,k,(z,a,i) ?
exp?
?Eq(?k)[log p(z, a, i | ?k)]+ Eq(?ik)[log p(i | ?ik)] +?w 6=iEq(?bik )[log p(w | ?bik )]+ Eq(?ak)[log p(a | ?ak)] +?x 6=aEq(?bak )[log p(x | ?bak )]?
?We then perform the minimization on the dual inequation 4 under the provided constraints to derive afinal update to the constrained c?.Simplifying Approximation The update for ??
re-quires numerical optimization due to the nonconju-gacy introduced by the point-wise product in fea-ture generation.
If instead we have every relationtype separately generate a copy of the corpus, the ?
?534Quantity f(z, a, i) ?
or ?
bSyntax ?k Counts i, a of relation k that match a pattern (see text) ?
0.8DPrevalence ?k Counts instantiations of relation k ?
0.8DSeparation (ind) ?w Counts times w selected as i ?
2Separation (arg) ?w Counts times w selected as part of a ?
1Table 1: Each constraint takes the form Eq[f(z, a, i)] ?
b or Eq[f(z, a, i)] ?
b; D denotes the number of corpusdocuments, ?k means one constraint per relation type, and ?w means one constraint per token in the corpus.updates becomes closed-form expressions similar toequation 6.
This approximation yields similar pa-rameter estimates as the true updates while vastlyimproving speed, so we use it in our experiments.5 Declarative ConstraintsWe now have the machinery to incorporate a va-riety of declarative constraints during inference.The classes of domain-independent constraints westudy are summarized in Table 1.
For the propor-tion constraints we arbitrarily select a threshold of80% without any tuning, in the spirit of building adomain-independent approach.Syntax As previous work has observed, most rela-tions are expressed using a limited number of com-mon syntactic patterns (Riloff, 1996; Banko and Et-zioni, 2008).
Our syntactic constraint captures thisinsight by requiring that a certain proportion of theinduced instantiations for each relation match one ofthese syntactic patterns:?
The indicator is a verb and the argument?sheadword is either the child or grandchild ofthe indicator word in the dependency tree.?
The indicator is a noun and the argument is amodifier or complement.?
The indicator is a noun in a verb?s subject andthe argument is in the corresponding object.Prevalence For a relation to be domain-relevant, itshould occur in numerous documents across the cor-pus, so we institute a constraint on the number oftimes a relation is instantiated.
Note that the effectof this constraint could also be achieved by tuningthe prior probability of a relation not occurring in adocument.
However, this prior would need to be ad-justed every time the number of documents or fea-ture selection changes; using a constraint is an ap-pealing alternative that is portable across domains.Separation The separation constraint encouragesdiversity in the discovered relation types by restrict-ing the number of times a single word can serve aseither an indicator or part of the argument of a re-lation instance.
Specifically, we require that everytoken of the corpus occurs at most once as a wordin a relation?s argument in expectation.
On the otherhand, a single word can sometimes be evocative ofmultiple relations (e.g., ?occurred?
signals both dateand time in ?occurred on Friday at 3pm?).
Thus, weallow each word to serve as an indicator more thanonce, arbitrarily fixing the limit at two.6 Experimental SetupDatasets and Metrics We evaluate on two datasets,financial market reports and newswire articles aboutearthquakes, previously used in work on high-levelcontent analysis (Barzilay and Lee, 2004; Lap-ata, 2006).
Finance articles chronicle daily mar-ket movements of currencies and stock indexes, andearthquake articles document specific earthquakes.Constituent parses are obtained automatically us-ing the Stanford parser (Klein and Manning, 2003)and then converted to dependency parses using thePennConvertor tool (Johansson and Nugues, 2007).We manually annotated relations for both corpora,selecting relation types that occurred frequently ineach domain.
We found 15 types for finance and9 for earthquake.
Corpus statistics are summarizedbelow, and example relation types are shown in Ta-ble 2.Docs Sent/Doc Tok/Doc VocabFinance 100 12.1 262.9 2918Earthquake 200 9.3 210.3 3155In our task, annotation conventions for desiredoutput relations can greatly impact token-level per-formance, and the model cannot learn to fit a par-ticular convention by looking at example data.
Forexample, earthquakes times are frequently reportedin both local and GMT, and either may be arbitrar-ily chosen as correct.
Moreover, the baseline we535Finance Bond 104.58 yen, 98.37 yenDollar Change up 0.52 yen, down 0.01 yenTokyo Index Change down 5.38 points or 0.41 percent, up 0.16 points, insignificant in percentage termsEarthquake Damage about 10000 homes, some buildings, no informationEpicenterPatuca about 185 miles (300 kilometers) south of Quito, 110 kilometers (65 miles)from shore under the surface of the Flores sea in the Indonesian archipelagoMagnitude 5.7, 6, magnitude-4Table 2: Example relation types identified in the finance and earthquake datasets with example instance arguments.compare against produces lambda calculus formulasrather than spans of text as output, so a token-levelcomparison requires transforming its output.For these reasons, we evaluate on both sentence-level and token-level precision, recall, and F-score.Precision is measured by mapping every induced re-lation cluster to its closest gold relation and comput-ing the proportion of predicted sentences or wordsthat are correct.
Conversely, for recall we map ev-ery gold relation to its closest predicted relation andfind the proportion of gold sentences or words thatare predicted.
This mapping technique is based onthe many-to-one scheme used for evaluating unsu-pervised part-of-speech induction (Johnson, 2007).Note that sentence-level scores are always at least ashigh as token-level scores, since it is possible to se-lect a sentence correctly but none of its true relationtokens while the opposite is not possible.Domain-specific Constraints On top of the cross-domain constraints from Section 5, we studywhether imposing basic domain-specific constraintscan be beneficial.
The finance dataset is heav-ily quantitative, so we consider applying a singledomain-specific constraint stating that most rela-tion arguments should include a number.
Likewise,earthquake articles are typically written with a ma-jority of the relevant information toward the begin-ning of the document, so its domain-specific con-straint is that most relations should occur in thefirst two sentences of a document.
Note that thesedomain-specific constraints are not specific to in-dividual relations or instances, but rather encode apreference across all relation types.
In both cases,we again use an 80% threshold without tuning.Features For indicators, we use the word, part ofspeech, and word stem.
For arguments, we use theword, syntactic constituent label, the head word ofthe parent constituent, and the dependency label ofthe argument to its parent.Baselines We compare against three alternative un-supervised approaches.
Note that the first two onlyidentify relation-bearing sentences, not the specificwords that participate in the relation.Clustering (CLUTO): A straightforward way ofidentifying sentences bearing the same relation isto simply cluster them.
We implement a cluster-ing baseline using the CLUTO toolkit with word andpart-of-speech features.
As with our model, we setthe number of clusters K to the true number of rela-tion types.Mallows Topic Model (MTM): Another techniquefor grouping similar sentences is the Mallows-basedtopic model of Chen et al (2009).
The datasets weconsider here exhibit high-level regularities in con-tent organization, so we expect that a topic modelwith global constraints could identify plausible clus-ters of relation-bearing sentences.
Again, K is set tothe true number of relation types.Unsupervised Semantic Parsing (USP): Our fi-nal unsupervised comparison is to USP, an unsuper-vised deep semantic parser introduced by Poon andDomingos (2009).
USP induces a lambda calculusrepresentation of an entire corpus and was shown tobe competitive with open information extraction ap-proaches (Lin and Pantel, 2001; Banko et al, 2007).We give USP the required Stanford dependency for-mat as input (de Marneffe and Manning, 2008).
Wefind that the results are sensitive to the cluster granu-larity prior, so we tune this parameter and report thebest-performing runs.We recognize that USP targets a different out-put representation than ours: a hierarchical semanticstructure over the entirety of a dependency-parsedtext.
In contrast, we focus on discovering a limitednumberK of domain-relevant relations expressed asconstituent phrases.
Despite these differences, both536methods ultimately aim to capture domain-specificrelations expressed with varying verbalizations, andboth operate over in-domain input corpora supple-mented with syntactic information.
For these rea-sons, USP provides a clear and valuable point ofcomparison.
For this comparison, we transformUSP?s lambda calculus formulas to relation spans asfollows.
First, we group lambda forms by a combi-nation of core form, argument form, and the parent?score form.7 We then filter to the K relations thatappear in the most documents.
For token-level eval-uation we take the dependency tree fragment corre-sponding to the lambda form.
For example, in thesentence ?a strong earthquake rocked the Philippinesisland of Mindoro early Tuesday,?
USP learns thatthe word ?Tuesday?
has a core form correspondingto words {Tuesday, Wednesday, Saturday}, a parentform corresponding to words {shook, rock, hit, jolt},and an argument form of TMOD; all phrases withthis same combination are grouped as a relation.Training Regimes and Hyperparameters For eachrun of our model we perform three random restartsto convergence and select the posterior with lowestfinal free energy.
We fix K to the true number ofannotated relation types for both our model and USPand L (the number of document segments) to five.Dirichlet hyperparameters are set to 0.1.7 ResultsTable 3?s first two sections present the results of ourmain evaluation.
For earthquake, the far more diffi-cult domain, our base model with only the domain-independent constraints strongly outperforms allthree baselines across both metrics.
For finance,the CLUTO and USP baselines achieve performancecomparable to or slightly better than our base model.Our approach, however, has the advantage of provid-ing a formalism for seamlessly incorporating addi-tional arbitrary domain-specific constraints.
Whenwe add such constraints (denoted as model+DSC),we achieve consistently higher performance than allbaselines across both datasets and metrics, demon-strating that this approach provides a simple and ef-fective framework for injecting domain knowledgeinto relation discovery.7This grouping mechanism yields better results than onlygrouping by core form.The first two baselines correspond to a setupwhere the number of sentence clusters K is set tothe true number of relation types.
This has the effectof lowering precision because each sentence must beassigned a cluster.
To mitigate this impact, we exper-imented with using K+N clusters, with N rangingfrom 1 to 30.
In each case, we then keep only the Klargest clusters.
For the earthquake dataset, increas-ing N improves performance until some point, afterwhich performance degrades.
However, the best F-Score corresponding to the optimal number of clus-ters is 42.2, still far below our model?s 66.0 F-score.For the finance domain, increasing the number ofclusters hurts performance.Our results show a large gap in F-score betweenthe sentence and token-level evaluations for both theUSP baseline and our model.
A qualitative analysisof the results indicates that our model often picks upon regularities that are difficult to distinguish with-out relation-specific supervision.
For earthquake, alocation may be annotated as ?the Philippine islandof Mindoro?
while we predict just the word ?Min-doro.?
For finance, an index change can be anno-tated as ?30 points, or 0.8 percent,?
while our modelidentifies ?30 points?
and ?0.8 percent?
as separaterelations.
In practice, these outputs are all plausi-ble discoveries, and a practitioner desiring specificoutputs could impose additional constraints to guiderelation discovery toward them.The Impact of Constraints To understand the im-pact of the declarative constraints, we perform anablation analysis on the constraint sets.
We con-sider removing the constraints on syntactic patterns(no-syn) and the constraints disallowing relations tooverlap (no-sep) from the full domain-independentmodel.8 We also try a version with hard syntac-tic constraints (hard-syn), which requires that everyextraction match one of the three syntactic patternsspecified by the syntactic constraint.Table 3?s bottom section presents the results ofthis evaluation.
The model?s performance degradeswhen either of the two constraint sets are removed,demonstrating that the constraints are in fact benefi-cial for relation discovery.
Additionally, in the hard-syn case, performance drops dramatically for finance8Prevalence constraints are always enforced, as otherwisethe prior on not instantiating a relation would need to be tuned.537Finance EarthquakeSentence-level Token-level Sentence-level Token-levelPrec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1Model 82.1 59.7 69.2 42.2 23.9 30.5 54.2 68.1 60.4 20.2 16.8 18.3Model+DSC 87.3 81.6 84.4 51.8 30.0 38.0 66.4 65.6 66.0 22.6 23.1 22.8CLUTO 56.3 92.7 70.0 ?
?
?
19.8 58.0 29.5 ?
?
?MTM 40.4 99.3 57.5 ?
?
?
18.6 74.6 29.7 ?
?
?USP 91.3 66.1 76.7 28.5 32.6 30.4 61.2 43.5 50.8 9.9 32.3 15.1No-sep 97.8 35.4 52.0 86.1 8.7 15.9 42.2 21.9 28.8 16.1 4.6 7.1No-syn 83.3 46.1 59.3 20.8 9.9 13.4 53.8 60.9 57.1 14.0 13.8 13.9Hard-syn 47.7 39.0 42.9 11.6 7.0 8.7 55.0 66.2 60.1 20.1 17.3 18.6Table 3: Top section: our model, with and without domain-specific constraints (DSC).
Middle section: The threebaselines.
Bottom section: ablation analysis of constraint sets for our model.
For all scores, higher is better.while remaining almost unchanged for earthquake.This suggests that formulating constraints as soft in-equalities on posterior expectations gives our modelthe flexibility to accommodate both the underlyingsignal in the data and the declarative constraints.Comparison against Supervised CRF Our finalset of experiments compares a semi-supervised ver-sion of our model against a conditional random field(CRF) model.
The CRF model was trained usingthe same features as our model?s argument features.To incorporate training examples in our model, wesimply treat annotated relation instances as observedvariables.
For both the baselines and our model,we experiment with using up to 10 annotated docu-ments.
At each of those levels of supervision, we av-erage results over 10 randomly drawn training sets.At the sentence level, our model compares veryfavorably to the supervised CRF.
For finance, it takesat least 10 annotated documents (corresponding toroughly 130 annotated relation instances) for theCRF to match the semi-supervised model?s perfor-mance.
For earthquake, using even 10 annotateddocuments (about 71 relation instances) is not suf-ficient to match our model?s performance.At the token level, the supervised CRF base-line is far more competitive.
Using a single la-beled document (13 relation instances) yields su-perior performance to either of our model variantsfor finance, while four labeled documents (29 re-lation instances) do the same for earthquake.
Thisresult is not surprising?our model makes strongdomain-independent assumptions about how under-lying patterns of regularities in the text connect torelation expression.
Without domain-specific super-vision such assumptions are necessary, but they canprevent the model from fully utilizing available la-beled instances.
Moreover, being able to annotateeven a single document requires a broad understand-ing of every relation type germane to the domain,which can be infeasible when there are many unfa-miliar, complex domains to process.In light of our strong sentence-level performance,this suggests a possible human-assisted application:use our model to identify promising relation-bearingsentences in a new domain, then have a human an-notate those sentences for use by a supervised ap-proach to achieve optimal token-level extraction.8 ConclusionsThis paper has presented a constraint-based ap-proach to in-domain relation discovery.
We haveshown that a generative model augmented withdeclarative constraints on the model posterior cansuccessfully identify domain-relevant relations andtheir instantiations.
Furthermore, we found that asingle set of constraints can be used across divergentdomains, and that tailoring constraints specific to adomain can yield further performance benefits.AcknowledgementsThe authors gratefully acknowledge the supportof Defense Advanced Research Projects Agency(DARPA) Machine Reading Program under AirForce Research Laboratory (AFRL) prime contractno.
FA8750-09-C-0172.
Any opinions, findings,and conclusion or recommendations expressed inthis material are those of the authors and do not nec-essarily reflect the view of the DARPA, AFRL, orthe US government.
Thanks also to Hoifung Poonand the members of the MIT NLP group for theirsuggestions and comments.538ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of DL.Michele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InProceedings of ACL.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open in-formation extraction from the web.
In Proceedings ofIJCAI.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofHLT/NAACL.Kedar Bellare and Andrew McCallum.
2009.
Gen-eralized expectation criteria for bootstrapping extrac-tors using record-text alignment.
In Proceedings ofEMNLP.Jordan Boyd-Graber and David M. Blei.
2008.
Syntactictopic models.
In Advances in NIPS.Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to extract relations from the web using mini-mal supervision.
In Proceedings of ACL.Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and CiyouZhu.
1995.
A limited memory algorithm for boundconstrained optimization.
SIAM Journal on ScientificComputing, 16(5):1190?1208.Ming-Wei Chang, Lev Ratinov, and Dan Roth.2007.
Guiding semi-supervision with constraint-driven learning.
In Proceedings of ACL.2006.
Modeling general and specific aspects of docu-ments with a probabilistic topic model.
In Advancesin NIPS.Jinxiu Chen, Dong-Hong Ji, Chew Lim Tan, and Zheng-Yu Niu.
2005.
Automatic relation extraction withmodel order selection and discriminative label identi-fication.
In Proceedings of IJCNLP.Harr Chen, S.R.K.
Branavan, Regina Barzilay, andDavid R. Karger.
2009.
Content modeling using la-tent permutations.
Journal of Artificial IntelligenceResearch, 36:129?163.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The stanford typed dependencies repre-sentation.
In Proceedings of the COLING Workshopon Cross-framework and Cross-domain Parser Evalu-ation.Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar.
2007.Expectation maximization and posterior constraints.In Advances in NIPS.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.2004.
Discovering relations among named entitiesfrom large corpora.
In Proceedings of ACL.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for english.
InProceedings of NODALIDA.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Proceedings of EMNLP.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL.Mirella Lapata.
2006.
Automatic evaluation of informa-tion ordering: Kendall?s tau.
Computational Linguis-tics, 32(4):471?484.Dekang Lin and Patrick Pantel.
2001.
DIRT - discov-ery of inference rules from text.
In Proceedings ofSIGKDD.Gideon S. Mann and Andrew McCallum.
2008.
General-ized expectation criteria for semi-supervised learningof conditional random fields.
In Proceedings of ACL.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In Proceedings of ACL/IJCNLP.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of EMNLP.Ellen Riloff.
1996.
Automatically generating extractionpatterns from untagged texts.
In Proceedings of AAAI.Benjamin Rosenfeld and Ronen Feldman.
2007.
Clus-tering for unsupervised relation identification.
In Pro-ceedings of CIKM.Dan Roth and Wen-tau Yih.
2004.
A linear programmingformulation for global inference in natural languagetasks.
In Proceedings of CoNLL.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted relationdiscovery.
In Proceedings of HLT/NAACL.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern representationmodel for automatic IE pattern acquisition.
In Pro-ceedings of ACL.Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen.
2000.
Automatic acquisition ofdomain knowledge for information extraction.
In Pro-ceedings of COLING.Limin Yao, Sebastian Riedel, and Andrew McCallum.2010.
Cross-document relation extraction without la-belled data.
In Proceedings of EMNLP.Alexander Yates and Oren Etzioni.
2009.
Unsupervisedmethods for determining object and relation synonymson the web.
Journal of Artificial Intelligence Research,34:255?296.Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, andChew Lim Tan.
2005.
Discovering relations betweennamed entities from a large raw corpus using treesimilarity-based clustering.
In Proceedings of IJC-NLP.539Jun Zhu, Zaiqing Nie, Xiaojing Liu, Bo Zhang, and Ji-Rong Wen.
2009.
StatSnowball: a statistical approachto extracting entity relationships.
In Proceedings ofWWW.540
