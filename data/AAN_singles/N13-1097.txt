Proceedings of NAACL-HLT 2013, pages 789?795,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsSeparating Fact from Fear: Tracking Flu Infections on TwitterAlex Lamb, Michael J. Paul, Mark DredzeHuman Language Technology Center of ExcellenceDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218{alamb3,mpaul19,mdredze}@jhu.eduAbstractTwitter has been shown to be a fast and reli-able method for disease surveillance of com-mon illnesses like influenza.
However, previ-ous work has relied on simple content anal-ysis, which conflates flu tweets that reportinfection with those that express concernedawareness of the flu.
By discriminating thesecategories, as well as tweets about the authorsversus about others, we demonstrate signifi-cant improvements on influenza surveillanceusing Twitter.1 IntroductionTwitter is a fantastic data resource for many tasks:measuring political (O?Connor et al 2010; Tumas-jan et al 2010), and general sentiment (Bollen etal., 2011), studying linguistic variation (Eisensteinet al 2010) and detecting earthquakes (Sakaki etal., 2010).
Similarly, Twitter has proven useful forpublic health applications (Dredze, 2012), primar-ily disease surveillance (Collier, 2012; Signorini etal., 2011), whereby public health officials track in-fection rates of common diseases.
Standard govern-ment data sources take weeks while Twitter providesan immediate population measure.Strategies for Twitter influenza surveillance in-clude supervised classification (Culotta, 2010b; Cu-lotta, 2010a; Eiji Aramaki and Morita, 2011), un-supervised models for disease discovery (Paul andDredze, 2011), keyword counting1, tracking geo-graphic illness propagation (Sadilek et al 2012b),and combining tweet contents with the social net-work (Sadilek et al 2012a) and location informa-1The DHHS competition relied solely on keyword counting.http://www.nowtrendingchallenge.com/tion (Asta and Shalizi, 2012).
All of these methodsrely on a relatively simple NLP approach to analyz-ing the tweet content, i.e.
n-gram models for classi-fying related or not related to the flu.
Yet examiningflu tweets yields a more complex picture:?
going over to a friends house to check on her son.he has the flu and i am worried about him?
Starting to get worried about swine flu...Both are related to the flu and express worry, buttell a different story.
The first reports an infec-tion of another person, while the second expressesthe author?s concerned awareness.
While infectiontweets indicate a rise in infection rate, awarenesstweets may not.
Automatically making these dis-tinctions may improve influenza surveillance, yet re-quires more than keywords.We present an approach for differentiating be-tween flu infection and concerned awareness tweets,as well as self vs other, by relying on a deeper analy-sis of the tweet.
We present our features and demon-strate improvements in influenza surveillance.1.1 Related WorkMuch of the early work on web-based influenzasurveillance relied on query logs and click-throughdata from search engines (Eysenbach, 2006), mostfamously Google?s Flu Trends service (Ginsberg etal., 2008; Cook et al 2011).
Other sources of in-formation include articles from the news media andonline mailing lists (Brownstein et al 2010).2 Capturing Nuanced TrendsPrevious work has classified messages as being re-lated or not related to influenza, with promisingsurveillance results, but has ignored nuanced differ-ences between flu tweets.
Tweets that are related to789flu but do not report an infection can corrupt infec-tion tracking.Concerned Awareness vs.
Infection (A/I) Manyflu tweets express a concerned awareness as opposedto infection, including fear of getting the flu, anawareness of increased infections, beliefs related toflu infection, and preventative flu measures (e.g.
flushots.)
Critically, these people do not seem to havethe flu, whereas infection tweets report having theflu.
This distinction is similar to modality (Prab-hakaran et al 2012a).
Conflating these tweets canhurt surveillance, as around half of our annotatedflu messages were awareness.
Identifying awarenesstweets may be of use in-and-of itself, such as forcharacterizing fear of illness (Epstein et al 2008;Epstein, 2009), public perception, and discerningsentiment (e.g.
flu is negative, flu shots may be pos-itive.)
We focus on surveillance improvements.2Self vs. Other (S/O) Tweets for both awarenessand infection can describe the author (self) or oth-ers.
It may be that self infection reporting is moreinformative.
We test this hypothesis by classifyingtweets as self vs. other.Finding Flu Related Tweets (R/U) We must firstidentify messages that are flu related.
We constructa classifier for flu related vs. unrelated.3 FeaturesToken sequences (n-grams) are an insufficient fea-ture set, since our classes share common vocabular-ies.
Consider,?
A little worried about the swine flu epidemic!?
Robbie might have swine flu.
I?m worried.Both tweets mention flu and worried, which distin-guish them as flu related but not specifically aware-ness or infection, nor self or other.
Motivated byBergsma et al(2012), we complement 3-grams withadditional features that capture longer spans of textand generalize using part of speech tags.
We beginby processing each tweet using the ARK POS tag-ger (Gimpel et al 2011) and find phrase segmen-tations using punctuation tags.3 Most phrases weretwo (31.2%) or three (26.6%) tokens long.2While tweets can both show awareness and report an in-fection, we formulate a binary task for simplicity since only asmall percentage of tweets were so labeled.3We used whitespace for tokenization, which did about thesame as Jerboa (Van Durme, 2012).Class Name Words in ClassInfection getting, got, recovered, have, hav-ing, had, has, catching, catch, cured,infectedPossession bird, the flu, flu, sick, epidemicConcern afraid, worried, scared, fear, worry,nervous, dread, dreaded, terrifiedVaccination vaccine, vaccines, shot, shots, mist,tamiflu, jab, nasal sprayPast Tense was, did, had, got, were, or verb withthe suffix ?ed?Present Tense is, am, are, have, has, or verb withthe suffix ?ing?Self I, I?ve, I?d, I?m, im, myOthers your, everyone, you, it, its, u, her,he, she, he?s, she?s, she, they, you?re,she?ll, he?ll, husband, wife, brother,sister, your, people, kid, kids, chil-dren, son, daughterTable 1: Our manually created set of word class features.Word Classes For our task, many word types canbehave similarly with regard to the label.
We createword lists for possessive words, flu related words,fear related words, ?self?
words, ?other?
words, andfear words (Table 1).
A word?s presence triggers acount-based feature corresponding to each list.Stylometry We include Twitter-specific style fea-tures.
A feature is included for retweet, hashtags,and mentions of other users.
We include a featurefor emoticons (based on the emoticon part-of-speechtag).
We include a more specific feature for positiveemoticons (:) :D :)).
We also include a featurefor negative emoticons (:( :/).
Additionally, weinclude a feature for links to URLs.Part of Speech Templates We include featuresbased on a number of templates matching specificsequences of words, word classes, and part of speechtags.
Where any word included in the templatematches a word in one of the word classes, an ad-ditional feature is included indicating that the wordclass was included in that template.?
Tuples of (subject,verb,object) and pairs of (sub-ject, verb), (subject, object), and (verb, object).
Weuse a simple rule to construct these tuples: the firstnoun or pronoun is taken as the subject, and the firstverb appearing after the subject is taken as the verb.The object is taken as any noun or pronoun that ap-pears before a verb or at the end of a phrase.790?
A pairing of the first pronoun with last noun.These are useful for S/O, e.g.
I am worried that myson has the flu to recognize the difference betweenthe author (I) and someone else.?
Phrases that begin with a verb (pro-drop).
This ishelpful for S/O, e.g.
getting the flu!
which can indi-cate self even without a self-related pronoun.
An ad-ditional feature is included if this verb is past-tense.?
Numeric references.
These often indicate aware-ness (number of people with the flu) and are gen-erally not detected by an n-gram model.
We add aseparate feature if the word following has the root?died?, e.g.
So many people dying from the flu, I?mscared!?
Pair of first pronoun/noun with last verb in aphrase.
Many phrases have multiple verbs, but thelast verb is critical, e.g.
I had feared the flu.
Ad-ditional features are added if the noun/pronoun is inthe ?self?
or ?other?
word class, and if the verb is inthe ?possessive?
word class.?
Flu appears as a noun before first verb in a phrase.This indicates when flu is a subject, which is morelikely to be about awareness.?
Pair of verb and following noun.
This indicates theverbs object, which can change the focus of A/I,e.g., I am getting a serious case of the flu vs.
I amgetting a flu shot.
Additional features are added ifthe verb is past tense (based on word list and suffix?-ed?.)?
Whether a flu related word appears as a noun oran adjective.
When flu is used as an adjective, itmay indicate a more general discussion of the flu,as opposed to an actual infection I hate this flu vs. Ihate this flu hype.?
If a proper noun is followed by a possessive verb.This may indicate others for the S/O task Looks likeDenmark has the flu.
An additional feature fires forany verb that follows a proper noun and any pasttense verb that follows a proper noun.?
Pair each noun with ???.
While infection tweetsare often statements and awareness questions, thesubject matters, e.g.
Do you think that swine fluis coming to America?
as awareness.
An equivalentfeature is included for phrases ending with ?!
?.While many of our features can be extracted usinga syntactic parser (Foster et al 2011), tweets arevery short, so our simple rules and over-generatingfeatures captures the desired effects without parsing.Self Other TotalAwareness 23.15% 24.07% 47.22%Infection 37.21% 15.57% 52.78%Total 60.36% 39.64%Table 2: The distribution over labels of the data set.
In-fection tweets are more likely to be about the author (self)than those expressing awareness.3.1 LearningWe used a log-linear model from Mallet (McCal-lum, 2002) with L2 regularization.
For each task, wefirst labeled tweets as related/not-related and thenclassified the related tweets as awareness/infectionand self/others.
We found this two phase approachworked better than multi-class.4 Data CollectionWe used two Twitter data sets: a collection of 2billion tweets from May 2009 and October 2010(O?Connor et al 2010)4 and 1.8 billion tweets col-lected from August 2011 to November 2012.
Toobtain labeled data, we first filtered the data setsfor messages containing words related to concernand influenza,5 and used Amazon Mechanical Turk(Callison-Burch and Dredze, 2010) to label tweetsas concerned awareness, infection, media and un-related.
We allowed multiple categories per tweet.Annotators also labeled awareness/infection tweetsas self, other or both.
We included tweets we anno-tated to measure Turker quality and obtained threeannotations per tweet.
More details can be found inLamb et al(2012).To construct a labeled data set we removed lowquality annotators (below 80% accuracy on goldtweets.)
This seemed like a difficult task for anno-tators as a fifth of the data had no annotations afterthis step.
We used the majority label as truth and tieswere broken using the remaining low quality anno-tators.
We then hand-corrected all tweets, changing13.5% of the labels.
The resulting data set contained11,990 tweets (Table 2), 5,990 from 2011-2012 fortraining and the remaining from 2009-2010 as test.64This coincided with the second and larger H1N1 (swineflu) outbreak of 2009; swine flu is mentioned in 39.6% of theannotated awareness or infection tweets.5e.g.
?flu?, ?worried?, ?worry?, ?scared?, ?scare?, etc.6All development was done using cross-validation on train-ing data, reserving test data for the final experiments.791Feature Removed A/I S/On-grams 0.6701 0.8440Word Classes 0.7735 0.8549Stylometry 0.8011 0.8522Pronoun/Last Noun 0.7976 0.8534Pro-Drop 0.7989 0.8523Numeric Reference 0.7988 0.8530Pronoun/Verb 0.7987 0.8530Flu Noun Before Verb 0.7987 0.8526Noun in Question 0.8004 0.8534Subject,Object,Verb 0.8005 0.8541Table 3: F1 scores after feature ablation.5 ExperimentsWe begin by evaluating the accuracy on the bi-nary classification tasks and then measure the re-sults from the classifiers for influenza surveillance.We created precision recall curves on the test data(Figure 1), and measured the highest F1, for thethree binary classifiers.
For A/I and S/O, our addi-tional features improved over the n-gram baselines.We performed feature ablation experiments (Table3) and found that for A/I, the word class featureshelped the most by a large margin, while for S/Othe stylometry and pro-drop features were the mostimportant after n-grams.
Interestingly, S/O doesequally well removing just n-gram features, sug-gesting that the S/O task depends on a few wordscaptured by our features.Since live data will have classifiers run in stages?
to filter out not-related tweets ?
we evaluatedthe performance of two-staged classification.
F1dropped to 0.7250 for A/I and S/O dropped to0.8028.5.1 Influenza surveillance using TwitterWe demonstrate how our classifiers can improve in-fluenza surveillance using Twitter.
Our hypothesisis that by isolating infection tweets we can improvecorrelations against government influenza data.
Weinclude several baseline methods:Google Flu Trends: Trends from search queries.7Keywords: Tweets that contained keywords fromthe DHHS Twitter surveillance competition.ATAM: We obtained 1.6 million tweets that wereautomatically labeled as influenza/other by ATAM7http://www.google.org/flutrends/Data System 2009 2011Google Flu Trends 0.9929 0.8829TwitterATAM 0.9698 0.5131Keywords 0.9771 0.6597All Flu 0.9833 0.7247Infection 0.9897 0.7987Infection+Self 0.9752 0.6662Table 4: Correlations against CDC ILI data: Aug 2009-Aug 2010, Dec 2011 to Aug 2012.
(Paul and Dredze, 2011).
We trained a binary classi-fier with n-grams and marked tweets as flu infection.We evaluated three trends using our three binaryclassifiers trained with a reduced feature set close tothe n-gram features:8All Flu: Tweets marked as flu by Keywords orATAM were then classified as related/unrelated.9This trend used all flu-related tweets.Infection: Related tweets were classified as eitherawareness or infection.
This used infection tweets.Infection+Self: Infection were then labeled as selfor other.
This trend used self tweets.All five of these trends were correlated againstdata from the Centers for Disease Control and Pre-vention (CDC) weekly estimates of influenza-likeillness (ILI) in the U.S., with Pearson correlationscomputed separately for 2009 and 2011 (Table 4).10Previous work has shown high correlations for 2009data, but since swine flu had so dominated social me-dia, we expect weaker correlations for 2011.Results are show in Table 4 and Figure 2 showstwo classifiers against the CDC ILI data.
We seethat in 2009 the Infection curve fits the CDC curvevery closely, while the All Flu curve appears tosubstantially overestimate the flu rate at the peak.While 2009 is clearly easier, and all trends havesimilar correlations, our Infection classifier beats theother Twitter methods.
All trends do much worse in8Classifiers trained on 2011 data and thresholds selected tomaximize F1 on held out 2009 data.9Since our data set to train related or unrelated focused ontweets that appeared to mention the flu, we first filtered out ob-vious non-flu tweets by running ATAM and Keywords.10While the 2009 data is a 10% sample of Twitter, we used adifferent approach for 2011.
To increase the amount of data, wecollected Tweets mentioning health keywords and then normal-ized by the public stream counts.
For our analysis, we excludeddays that were missing data.
Additionally, we used a geolocatorbased on user provided locations to exclude non-US messages.See (Dredze et al 2013) for details and code for the geolocator.79255 60 65 70 75 80 85Precision020406080100RecallF1 = 0.7665F1 = 0.7562N-GramsAll Features 40 50 60 70 80 90 100Precision020406080100RecallF1 = 0.7891 F1 = 0.7985N-GramsAll Features 70 75 80 85 90 95Precision020406080100RecallF1 = 0.8499 F1 = 0.8550N-GramsAll FeaturesFigure 1: Left to right: Precision-recall curves for related vs. not related, awareness vs. infection and self vs. others.08/30/09 11/08/09 01/17/10 03/28/10 06/06/10 08/15/10DateFlu Rate2009-2010CDCTwitter (All Flu)Twitter (Infection Only)11/27/11 01/15/12 03/04/12 04/22/12 06/10/12 07/29/12DateFlu Rate2011-2012Figure 2: The Twitter flu rate for two years alongside the ILI rates provided by the CDC.
The y-axes are not comparablebetween the two years due to differences in data collection, but we note that the 2011-12 season was much milder.the 2011 season, which was much milder and thusharder to detect.
Of the Twitter methods, those us-ing our system were dramatically higher, with theInfection curve doing the best by a significant mar-gin.
Separating out infection from awareness (A/I)led to significant improvements, while the S/O clas-sifier did not, for unknown reasons.The best result using Twitter reported to date hasbeen by Doan et al(2012), whose best system hada correlation of 0.9846 during the weeks beginning8/30/09?05/02/10.
Our Infection system had a cor-relation of 0.9887 during the same period.
WhileGoogle does better than any of the Twitter systems,we note that Google has access to much more (pro-prietary) data, and their system is trained to predictCDC trends, whereas our Twitter system is intrinsi-cally trained only on the tweets themselves.Finally, we are also interested in daily trends inaddition to weekly, but there is no available evalu-ation data on this scale.
Instead, we computed thestability of each curve, by measuring the day-to-daychanges.
In the 2009 season, the relative increaseor decrease from the previous day had a variance of3.0% under the Infection curve, compared to 4.1%under ATAM and 6.7% under Keywords.6 DiscussionPrevious papers have implicitly assumed that flu-related tweets mimick the infection rate.
While thiswas plausible on 2009 data that focused on the swineflu epidemic, it is clearly false for more typical fluseasons.
Our results show that by differentiating be-tween types of flu tweets to isolate reports of infec-tion, we can recover reasonable surveillance.
Thisresult delivers a promising message for the NLPcommunity: deeper content analysis of tweets mat-ters.
We believe this conclusion is applicable to nu-merous Twitter trend tasks, and we encourage othersto investigate richer content analyses for these tasks.In particular, the community interested in modelingauthor beliefs and influence (Diab et al 2009; Prab-hakaran et al 2012b; Biran and Rambow, 2011)may find our task and data of interest.
Finally, be-yond surveillance, our methods can be used to studydisease awareness and sentiment, which has impli-cations for how public health officials respond tooutbreaks.
We conclude with an example of this dis-tinction.
On June 11th, 2009, the World Health Or-ganization declared that the swine flu had become aglobal flu pandemic.
On that day, flu awareness in-creased 282%, while infections increased only 53%.793ReferencesDena Asta and Cosma Shalizi.
2012.
Identifying in-fluenza trends via Twitter.
In NIPS Workshop on So-cial Network and Social Media Analysis: Methods,Models and Applications.Shane Bergsma, Matt Post, and David Yarowsky.
2012.Stylometric analysis of scientific articles.
In Proc.NAACL-HLT, pages 327?337.O.
Biran and O. Rambow.
2011.
Identifying justifi-cations in written dialogs.
In Semantic Computing(ICSC), 2011 Fifth IEEE International Conference on,pages 162?168.
IEEE.J.
Bollen, A. Pepe, and H. Mao.
2011.
Modeling pub-lic mood and emotion: Twitter sentiment and socio-economic phenomena.
In Proceedings of the Fifth In-ternational AAAI Conference on Weblogs and SocialMedia, pages 450?453.John S. Brownstein, Clark C. Freifeld, Emily H. Chan,Mikaela Keller, Amy L. Sonricker, Sumiko R. Mekaru,and David L. Buckeridge.
2010.
Information tech-nology and global surveillance of cases of 2009h1n1 influenza.
New England Journal of Medicine,362(18):1731?1735.Chris Callison-Burch and Mark Dredze.
2010.
Creatingspeech and language data with Amazon?s MechanicalTurk.
In NAACL Workshop on Creating Speech andLanguage Data With Mechanical Turk.N.
Collier.
2012.
Uncovering text mining: A surveyof current work on web-based epidemic intelligence.Global Public Health, 7(7):731?749.Samantha Cook, Corrie Conrad, Ashley L. Fowlkes, andMatthew H. Mohebbi.
2011.
Assessing google flutrends performance in the united states during the2009 influenza virus a (h1n1) pandemic.
PLOS ONE,6(8):e23610.A.
Culotta.
2010a.
Towards detecting influenza epi-demics by analyzing Twitter messages.
In ACM Work-shop on Soc.Med.
Analytics.Aron Culotta.
2010b.
Detecting influenza epidemicsby analyzing Twitter messages.
arXiv:1007.4748v1[cs.IR], July.Mona T. Diab, Lori Levin, Teruko Mitamura, OwenRambow, Vinodkumar Prabhakaran, and Weiwei Guo.2009.
Committed belief annotation and tagging.
InACL Third Linguistic Annotation Workshop.S.
Doan, L. Ohno-Machado, and N. Collier.
2012.
En-hancing Twitter data analysis with simple semantic fil-tering: Example in tracking influenza-like illnesses.arXiv preprint arXiv:1210.0848.Mark Dredze, Michael J. Paul, Shane Bergsma, and HieuTran.
2013.
A Twitter geolocation system with appli-cations to public health.
Working paper.Mark Dredze.
2012.
How social media will change pub-lic health.
IEEE Intelligent Systems, 27(4):81?84.Sachiko Maskawa Eiji Aramaki and Mizuki Morita.2011.
Twitter catches the flu: Detecting influenza epi-demics using Twitter.
In Empirical Natural LanguageProcessing Conference (EMNLP).Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2010.
A latent variable model for ge-ographic lexical variation.
In Empirical Natural Lan-guage Processing Conference (EMNLP).Joshua Epstein, Jon Parker, Derek Cummings, and RossHammond.
2008.
Coupled contagion dynamics offear and disease: Mathematical and computational ex-plorations.
PLoS ONE, 3(12).J.M.
Epstein.
2009.
Modelling to contain pandemics.Nature, 460(7256):687?687.G.
Eysenbach.
2006.
Infodemiology: tracking flu-related searches on the web for syndromic surveil-lance.
In AMIA Annual Symposium, pages 244?248.AMIA.J.
Foster, O?.
C?etinoglu, J. Wagner, J.
Le Roux, S. Hogan,J.
Nivre, D. Hogan, J.
Van Genabith, et al2011.
#hardtoparse: Pos tagging and parsing the Twitterverse.In proceedings of the Workshop On Analyzing Micro-text (AAAI 2011), pages 20?25.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging forTwitter: Annotation, features, and experiments.
In As-sociation for Computational Linguistics (ACL).J.
Ginsberg, M.H.
Mohebbi, R.S.
Patel, L. Brammer,M.S.
Smolinski, and L. Brilliant.
2008.
Detectinginfluenza epidemics using search engine query data.Nature, 457(7232):1012?1014.Alex Lamb, Michael J. Paul, and Mark Dredze.
2012.Investigating Twitter as a source for studying behav-ioral responses to epidemics.
In AAAI Fall Symposiumon Information Retrieval and Knowledge Discovery inBiomedical Text.A.K.
McCallum.
2002.
MALLET: A machine learningfor language toolkit.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.
FromTweets to polls: Linking text sentiment to publicopinion time series.
In ICWSM.Michael J. Paul and Mark Dredze.
2011.
You are whatyou Tweet: Analyzing Twitter for public health.
InICWSM.Vinodkumar Prabhakaran, Michael Bloodgood, MonaDiab, Bonnie Dorr, Lori Levin, Christine D. Piatko,Owen Rambow, and Benjamin Van Durme.
2012a.794Statistical modality tagging from rule-based annota-tions and crowdsourcing.
In Extra-Propositional As-pects of Meaning in Computational Linguistics (Ex-ProM 2012).Vinodkumar Prabhakaran, Owen Rambow, and MonaDiab.
2012b.
Predicting overt display of power inwritten dialogs.
In North American Chapter of the As-sociation for Computational Linguistics (NAACL).Adam Sadilek, Henry Kautz, and Vincent Silenzio.2012a.
Modeling spread of disease from social inter-actions.
In Sixth AAAI International Conference onWeblogs and Social Media (ICWSM).Adam Sadilek, Henry Kautz, and Vincent Silenzio.2012b.
Predicting disease transmission from geo-tagged micro-blog data.
In Twenty-Sixth AAAI Con-ference on Artificial Intelligence.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes Twitter users: real-timeevent detection by social sensors.
In WWW, New York,NY, USA.A.
Signorini, A.M. Segre, and P.M. Polgreen.
2011.
Theuse of Twitter to track levels of disease activity andpublic concern in the US during the influenza a H1N1pandemic.
PLoS One, 6(5):e19467.A.
Tumasjan, T.O.
Sprenger, P.G.
Sandner, and I.M.Welpe.
2010.
Predicting elections with twitter: What140 characters reveal about political sentiment.
InProceedings of the fourth international aaai confer-ence on weblogs and social media, pages 178?185.B.
Van Durme.
2012.
Jerboa: A toolkit for randomizedand streaming algorithms.
Technical report, Techni-cal Report 7, Human Language Technology Center ofExcellence, Johns Hopkins University.795
