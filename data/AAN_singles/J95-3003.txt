Collaborating on Referring ExpressionsPeter  A. Heeman*University of RochesterGraeme H i rs t  tUniversity of TorontoThis paper presents a computational model of how conversational participants collaborate inorder to make a referring action successful.
The model is based on the view of language asgoal-directed behavior.
We propose that the content of a referring expression can be accountedfor by the planning paradigm.
Not only does this approach allow the processes of building re-ferring expressions and identifying their referents to be captured by plan construction and planinference, it also allows us to account for how participants clarify a referring expression by us-ing meta-actions that reason about and manipulate the plan derivation that corresponds to thereferring expression.
To account for how clarification goals arise and how inferred clarificationplans affect he agent, we propose that the agents are in a certain state of mind, and that this stateincludes an intention to achieve the goal of referring and a plan that the agents are currentlyconsidering.
It is this mental state that sanctions the adoption of goals and the acceptance ofinferred plans, and so acts as a link between understanding and generation.1.
IntroductionPeople are goal-oriented and can plan courses of actions to achieve their goals.
Butsometimes they might lack the knowledge needed to formulate a plan of action, orsome of the actions that they plan might depend on coordinating their activity withother agents.
How do they cope?
One way is to work together, or collaborate, in for-mulating a plan of action with other people who are involved in the actions or whoknow the relevant information.Even the apparently simple linguistic task of referring, in an utterance, to someobject or idea can involve exactly this kind of activity: a collaboration between thespeaker and the hearer.
The speaker has the goal of the hearer identifying the objectthat the speaker has in mind.
The speaker attempts to achieve this goal by constructinga description of the object hat she thinks will enable the hearer to identify it.
But sincethe speaker and the hearer will inevitably have different beliefs about the world, thehearer might not be able to identify the object.
Often, when the hearer cannot do so, thespeaker and hearer collaborate in making a new referring expression that accomplishesthe goal.This paper presents a computational model of how a conversational participantcollaborates in making a referring action successful.
We use as our basis the modelproposed by Clark and Wilkes-Gibbs (1986), which gives a descriptive account ofthe conversational moves that participants make when collaborating upon a referringexpression.
We cast their work into a model based on the planning paradigm.We propose that referring expressions can be represented by plan derivations, andthat plan construction and plan inference can be used to generate and understandthem.
Not only does this approach allow the processes of building referring expres-* Department of Computer Science, Rochester, New York 14627.
E-mail: heeman@cs.rochester.edu.t Department of Computer Science, Toronto, Canada, M5S 1A4.
E-maih gh@cs.toronto.edu.?
1995 Association for Computational LinguisticsComputational Linguistics Volume 21, Number 3sions and identifying their referents to be captured in the planning paradigm, but it alsoallows us to use the planning paradigm to account for how participants clarify a refer-ring expression.
In this case, we use meta-actions that encode how a plan derivationcorresponding to a referring expression can be reasoned about and manipulated.To complete the picture, we also need to account for the fact that the conversantsare collaborating.
We propose that the agents are in a mental state that includes notonly an intention to achieve the goal of the collaborative activity but also a plan thatthe participants are currently considering.
In the case of referring, this will be the planderivation that corresponds to the referring expression.
This plan is in the commonground of the participants, and we propose rules that are sanctioned by the mentalstate both for accepting plans that clarify the current plan, and for adopting goals to dolikewise.
The acceptance of a clarification results in the current plan being updated.So, it is these rules that specify how plan inference and plan construction affect andare affected by the mental state of the agent.
Thus, the mental state, together with therules, provides the link between these two processes.
An important consequence ofour proposal is that the current plan need not allow the successful achievement of thegoal.
Likewise, the clarifications that agents propose need not result in a successfulplan in order for them to be accepted.As can be seen, our approach consists of two tiers.
The first tier is the planningcomponent, which accounts for how utterances are both understood and generated.Using the planning paradigm has several advantages: it allows both tasks to be cap-tured in a single paradigm that is used for modeling general intelligent behavior; itallows more of the content of an utterance to be accounted for by a uniform process;and only a single knowledge source for referring expressions i  needed instead ofhaving this knowledge mbedded in special algorithms for each task.
The second tieraccounts for the collaborative behavior of the agents: how they adopt goals and co-ordinate their activity.
It provides the link between the mental state of the agent andthe planning processes.In accounting for how agents collaborate in making a referring action, our workaims to make the following contributions to the field.
First, although much work hasbeen done on how agents request clarifications, or respond to such requests, littleattention has been paid to the collaborative aspects of clarification discourse.
Ourwork attempts a plan-based formalization of what linguistic collaboration is, both interms of the goals and intentions that underlie it and the surface speech acts thatresult from it.
Second, we address the act of referring and show how it can be betteraccounted for by the planning paradigm.
Third, previous plan-based linguistic researchhas concentrated on either construction or understanding of utterances, but not both.By doing both, we will give our work generality in the direction of a complete modelof the collaborative process.
Finally, by using Clark and Wilkes-Gibbs's model as abasis for our work, we aim not only to add support o their model, but also to gain amuch richer understanding of the subject.In order to address the problem that we have set out, we have limited the scopeof our work.
First, we look at referring expressions in isolation, rather than as partof a larger speech act.
Second, we assume that agents have mutual knowledge of themechanisms of referring expressions and collaboration.
Third, we deal with objectsthat both the speaker and hearer know of, though they might have different beliefsabout what propositions hold for these objects.
Fourth, as the input and the outputto our system, we use representations of surface speech actions, not natural anguagestrings.
Finally, although belief revision is an important part of how agents collaborate,we do not explicitly address this.352Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions2.
Referring as a Col laborative ProcessClark and Wilkes-Gibbs (1986) investigated how participants in a conversation collab-orate in making a referring action successful.
They conducted experiments in whichparticipants had to refer to objects--tangram patterns--that re difficult to describe.They found that typically the participant trying to describe a tangram pattern wouldpresent an initial referring expression.
The other participant would then pass judg-ment on it, either accepting it, rejecting it, or postponing his decision.
If it was rejected orthe decision postponed, then one participant or the other would refashion the referringexpression.
This would take the form of either repairing the expression by correctingspeech errors, expanding it by adding further qualifications, or replacing the originalexpression with a new expression.
The referring expression that results from this isthen judged, and the process continues until the referring expression is acceptableenough to the participants for current purposes.
This final expression is contributedto the participants' common ground.Below are two excerpts from Clark and Wilkes-Gibbs's experiments hat illustratethe acceptance process.
(2.1) A: 1 Um, third one is the guy reading with, holding his book to the left.B- 2 Okay, kind of standing up?A: 3 Yeah.B" 4 Okay.In this dialog, person A makes an initial presentation i line 1.
Person B postponeshis decision in line 2 by voicing a tentative "okay," and then proceeds to refashionthe referring expression, the result being "the guy reading, holding his book to theleft, kind of standing up."
A accepts the new expression in line 3, and B signals hisacceptance in line 4.
(2.2) A: 1 Okay, and the next one is the person that looks like they're carryingsomething and it's sticking out to the left.
It looks like a hat that'supside down.B: 2 The guy that's pointing to the left again?A" 3 Yeah, pointing to the left, that's it!
(laughs)B: 4 Okay.In the second dialog, B implicitly rejects A's initial presentation by replacing it with anew referring expression in line 2, "the guy that's pointing to the left again."
A thenaccepts the refashioned referring expression in line 3.Below, we give an algorithmic interpretation of Clark and Wilkes-Gibbs's collabo-rative model, where present, judge, and refashion are the conversational moves thatthe participants make, and ref, re, and judgment are variables that represent the referent,the current referring expression, and its judgment, respectively.
(Since the conversa-tional moves update the referring expression and its judgment, they are presented asfunctions.
)353Computational Linguistics Volume 21, Number 3re = present(ref)judgment = judge(ref, re)while (judgment  accept)re = refashion(ref, re)judgment = judge(ref, re)end-whileThe algorithm illustrates how the collaborative activity progresses by the participantsjudging and refashioning the previously proposed referring expression.
1 In fact, wecan see that the state of the process is characterized by the current referring expression,re, and the judgment of it, judgment, and that this state must be part of the commonground of the participants.
The algorithm also illustrates how the model of Clark andWilkes-Gibbs minimizes the distinction between the roles of the person who initiatedthe referring expression and the person who is trying to identify it.
Both have the samemoves available to them, for either can judge the description and either can refashionit.
Neither is controlling the dialog, they are simply collaborating.In later work, Clark and Schaefer (1989) propose that "each part of the acceptancephase is itself a contribution" (p. 269), and the acceptance of these contributions de-pends on whether the hearer "believes he is understanding well enough for currentpurposes" (p. 267).
Although Clark and Schaefer use the term contribution with respectto the discourse, rather than the collaborative effort of referring, their proposal is stillrelevant here: judgments and refashionings are contributions to the collaborative effortand are subjected to an acceptance process, with the result being that once they are ac-cepted, the state of the collaborative activity is updated.
So, what constitutes groundsfor accepting a judgment or clarification?
In order to be consistent with Clark andWilkes-Gibbs's model, we can see that if one agent finds the current referring expres-sion problematic, the other must accept hat judgment.
Likewise, if one agent proposesa referring expression, through a refashioning, the other must accept he refashioning.3.
Referring Expressions3.1 Planning and ReferringBy viewing language as action, the planning paradigm can be applied to natural an-guage processing.
The actions in this case are speech acts (Austin 1962; Searle 1969),and include such things as promising, informing, and requesting.
Cohen and Perrault(1979) developed a system that uses plan construction to map an agent's goals tospeech acts, and Allen and Perrault (1980) use plan inference to understand an agent'splan from its speech acts.
By viewing it as action (Searle 1969), referring can be incor-porated into a planning model.
Cohen's model (1981) planned requests that the heareridentify a referent, whereas Appelt (1985) planned concept activations, a generalizationof referring actions.Although acts of reference have been incorporated into plan-based models, de-termining the content of referring expressions hasn't been.
For instance, in Appelt'smodel, concept activations can be achieved by the action describe, which is a primitive,not further decomposed.
Rather, this action has an associated procedure that deter-mines a description that satisfies the preconditions of describe.
Such special procedureshave been the mainstay for accounting for the content of referring expressions, both inconstructing and in understanding them, as exemplified by Dale (1989), who chose de-scriptors on the basis of their discriminatory power; Ehud Reiter (1990), who focused1 For simplicity, we have not shown the change in speakers between refashionings and judgments.354Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressionson avoiding misleading conversational irnplicatures when generating descriptions; andMellish (1985), who used a constraint satisfaction algorithm to identify referents.Our work follows the plan-based approach to language generation and under-standing.
We extend the earlier approaches of Cohen and Appelt by accounting forthe content of the description at the planning level.
This is done by having surfacespeech actions for each component of a description, plus a surface speech action thatexpresses a speaker's intention to refer.
A referring action is composed of these prim-itive actions, and the speaker utters them in her attempt o refer to an object.These speech actions are the building blocks that referring expressions are madefrom.
Acting as the mortar are intermediate actions, which have constraints that theplan construction and plan inference processes can reason about.
These constraintsencode the knowledge of how a description can allow a hearer to identify an object.First, the constraints express the conditions under which an attribute can be usedto refer to an object; for instance, that it be mutually believed that the object has acertain property (Clark and Marshall 1981; Perrault and Cohen 1981; Nadathur andJoshi 1983).
Second, the constraints keep track of which objects could be believed to bethe referent of the referring expression.
Third, the constraints ensure that a sufficientnumber of surface speech actions are added so that the set of candidates associatedwith the entire referring expression consists of only a single object, the referent.
Theseconstraints enable the speaker to construct a referring expression that she believes willallow the hearer to identify the referent.
As for the hearer, the explicit encoding of theadequacy of referring expressions allows referent identification to fall out of the planinference process.Our approach to treating referring as a plan in which surface speech actions cor-respond to the components of the description allows us to capture how participantscollaborate in building a referring expression.
Plan repair techniques can be used torefashion an expression if it is not adequate, and clarifications can refer to the partof the plan derivation that is in question or is being repaired.
Thus we can model acollaborative dialog in terms of the changes that are being made to the plan derivation.The referring expression plans that we propose are not simply data structures, butare mental objects that agents have beliefs about (Pollack 1990).
The plan derivationexpresses beliefs of the speaker: how actions contribute to the achievement of the goal,and what constraints hold that will allow successful identification.
2 So plan construc-tion reasons about the beliefs of the agent in constructing a referring plan; likewise,plan inference, after hypothesizing a plan that is consistent with the observed actions,reasons about the other participant's (believed) beliefs in satisfying the constraints ofthe plan.
If the hearer is able to satisfy the constraints, then he will have understoodthe plan and be able to identify the referent, since a term corresponding to it wouldhave been instantiated in the inferred plan.
Otherwise, there will be an action thatincludes a constraint that is unsatisfiable, and the hearer construes the action as beingin error.
(We do not reason about how the error affects the satisfiability of the goal ofthe plan nor use the error to revise the beliefs of the hearer.
)3.2 Vocabulary and NotationBefore we present he action schemas for referring expressions, we need to introducethe notation that we use.
Our terminology for planning follows the general literature.
32 Since we assume that the agents have mutual knowledge of the action schemas and that agents canexecute surface speech actions, we do not consider beliefs about generation or about the executabilityof primitive actions.3 See the introductory chapter of Allen, Hendler, and Tate (1990) for an overview of planning.355Computational Linguistics Volume 21, Number 3We use the terms action schema, plan derivation, plan construction, and plan inference.An action schema consists of a header, constraints, a decomposition, a d an effect; andit encodes the constraints under which an effect can be achieved by performing thesteps in the decomposition.
A plan derivation is an instance of an action that hasbeen recursively expanded into primitive actions--its yield.
Each component in theplan--the action headers, constraints, teps, and effects--are referred to as nodes ofthe plan, and are given names so as to distinguish two nodes that have the samecontent.
Finally, plan construction is the process of finding a plan derivation whoseyield will achieve a given effect, and plan inference is the process of finding a planderivation whose yield is a set of observed primitive actions.The action schemas make use of a number of predicates, and these are definedin Table 1.
We adopt the Prolog convention that variables begin with an uppercaseletter, and all predicates and constants begin with a lowercase letter.
Two constantsthat need to be mentioned are system and user.
The first denotes the agent hat we aremodeling, and the latter, her conversational partner.
Since the action schemas are usedfor both constructing the plans of the system, and inferring the plans of the user, it issometimes necessary to refer to the speaker or hearer in a general way.
For this we usethe propositions speaker(Speaker) and hearer(Hearer).
These instantiate the variablesSpeaker and Hearer to system or user; which is which depends on whether the ruleis being used for plan construction or plan inference.
These propositions are includedas constraints in the action schemas as needed.3.3 Action SchemasThis section presents action schemas for referring expressions.
(We omit discussion ofactions that account for superlative adjectives, uch as "largest," that describe an objectrelative to the set of objects that match the rest of the description.
A full presentationis given by Heeman \[1991\].
)As we mentioned, the action for referring, called refer, is mapped to the surfacespeech actions through the use of intermediate actions and plan decomposition.
All ofthe reasoning is done in the refer action and the intermediate actions, so no constraintsor effects are included in the surface speech actions.We use three surface speech actions.
The first is s-refer(Entity), which is used toexpress the speaker's intention to refer.
The second is s-attrib(Entity, Predicate), andis used for describing an object in terms of an attribute; Entity is the discourse ntityof the object, and Predicate is a lambda expression, such as &X. category(X, bird), thatencodes the attribute.
The third is s-attrib-rel(Entity, OtherEntity, Predicate), and isused for describing an object in terms of some other object.
In this case Predicate isa lambda expression of two variables, one corresponding to Entity, and the other toOtherEntity; for instance, ~X.
~ Y. in(X, Y).Refer Action.
The schema for refer is shown in Figure 1.
The refer action decomposesHeader:Constraint:Decomposition:Effect:refer(Entity, Object)knowref(Hearer, Speaker, Entity, Object)s-refer(Entity)describe(Entity, Object)bel(Hearer, goal(Speaker, knowref(Hearer, Speaker, Entity, Object)))Figure 1refer schema.356Peter A. Heeman and Graeme Hirst Collaborating on Referring ExpressionsTable 1Predicates and actions.Beliefbel(Agt, Prop): Agt believes that Prop is true.bmb(Agtl,Agt2,Prop): Agtl believes that it is mutually believed between himself and Agt2 thatProp is true.knowref(Agtl,Agt2,Ent, Obj): Agtl knows the referent hat Agt2 associates with the discourseentity Ent (Webber 1983), which Agtl believes to be Obj.
(Proving this proposition withEnt unbound will cause a unique identifier to be created for Ent.
)Goals and Plansgoal(Agt, Goal): Agt has the goal Goal.
Agents act to make their goals true.plan(Agt, Plan, Goal): Agt has the goal of Goal and has adopted the plan derivation Plan as ameans to achieve it.
The agent believes that each action of Plan contributes tothe goal, butnot necessarily that all of the constraints hold; in other words, the plan must be coherentbut not necessarily valid (Pollack 1990, p. 94).content(Plan,Node, Content): The node named by Node in Plan has content Content.yield(Plan,Node,Actions): The subplan rooted at Node in Plan has a yield of the primitiveactions Actions.achieve(Plan, Goal): Executing Plan will cause Goal to be true.error(Plan,Node)" Plan has an error at the action labeled Node.
Errors are attributed to the actionthat contains the failed constraint.
This predicate is used to encode an agent's belief aboutan invalidity in a plan.Plan Repairsubstitute(Plan,Node, N wAction,NewPlan)" Undo all variable bindings in Plan (except hosein primitive actions, and then substitute the action header NewAction into Plan at Node,resulting in the partial plan NewPlan.replan(Plan,Actions): Complete the partial plan Plan.
Actions are the primitive actions that areadded to the plan.replace(Plan,NewPlan): The plan NewPlan replaces Plan.Miscellaneoussubset(Set, Lambda, Subset): Compute the subset, Subset, of Set that satisfies the lambda expres-sion Lambda.modifier-absolute-pred(Pred): Pred is a predicate that an object can be described in terms of.Used by the modifier-absolute schema given in Figure 6.modifier-relative-pred(Pred): Pred is a predicate that describes the relationship between twoobjects.
Used by the modifier-relative schema given in Figure 7.pick-one(Object, Set): Pick one object, Object, of the members of Set.speaker(Agt): Agt is the current speaker.hearer(Agt): Agt is the current hearer.into two steps: s-refer, which expresses the speaker's intention to refer, and describe,which accounts for the content of the referring expression (given next).
The effectof refer is that the hearer should believe that the speaker has a goal of the hearerknowing the referent of the referring expression.
The effect has been formulated inthis way because we are assuming that when a speaker has a communicative goalshe plans to achieve the goal by making the hearer recognize it; the effect will be357Computational Linguistics Volume 21, Number 3Header:Decomposition:describe(Entity, Object)headnoun(Entity, Object, Cand)modifiers(Entity, Object, Cand)Figure 2describe schema.Header:Constraint:Decomposition:headnoun(Entity, Object, Cand)world(World)bmb(Speaker, Hearer, category(Object, Category))subset(World, XX.
bmb(Speaker, Hearer, category(X, Category)),Cand)s-attrib(Entity,&X, category(X, Category))Figure 3headnoun schema.achieved by the hearer inferring the speaker's plan, regardless of whether or not thehearer is able to determine the actual referent.
To simplify our implementation, thisis the only effect that is stated for the action schemas for referring expressions.
Itcorresponds to the literal goal that Appelt and Kronfeld (1987) propose (whereas theactual identification is their condition of satisfaction).Intermediate Actions.
The describe action, shown in Figure 2, is used to construct adescription of the object through its decomposition i to headnoun and modifiers.
Thevariable Cand is the candidate set, the set of potential referents associated with thehead noun that is chosen, and it is passed to the modifiers action so that it can ensurethat the rest of the description rules out all of the alternatives.The action headnoun, shown in Figure 3, has a single step, s-attrib, which is thesurface speech action used to describe an object in terms of some predicate, whichfor the headnoun schema, is restricted to the category of the object.
4The schema alsohas two constraints.
The first ensures that the referent is of the chosen category andthe second determines the candidate set, Cand, associated with the head noun thatis chosen.
The candidate set is computed by finding the subset of the objects in theworld that the speaker believes could be referred to by the head noun- - the objectsthat the speaker and hearer have an appropriate mutual belief about.The modifiers action attempts to ensure that the referring expression that is beingconstructed is believed by the speaker to allow the hearer to uniquely identify thereferent.
We have defined modifiers as a recursive action, with two schemas.
5 The firstschema, shown in Figure 4, is used to terminate the recursion, and its constraint spec-ifies that only one object can be in the candidate set.
6 The second schema, shown inFigure 5, embodies the recursion.
It uses the modifier plan, which adds a componentto the description and updates the candidate set by computing the subset of it that sat-isfies the new component.
The modifier plan thus accounts for individual componentsof the description.There are two different action schemas for modifier; one is for absolute modifiers,4 Note that several category predications might be true of an object, and we do not explore which wouldbe best o use, but see Edmonds (1994) for how preferences can be encoded.5 We use specialization axioms (Kautz and Allen 1986) to map the modifiers action to the two schemas:modifiers-terminate and modifiers-recurse.6 In order to distinguish this action from the primitive actions, it has a step that is marked null.358Peter A. Heeman and Graeme Hirst Collaborating on Referring ExpressionsHeader:Constraint:Decomposition:modifiers-terminate(Entity, Object, Cand)Cand = \[Object\]nullFigure 4modifiers schema for terminating the recursion.Header:Decomposition:modifiers-recurse(Entity, Object, Cand)modifier(Entity, Object, Cand, NewCand)modifiers(Entity, Object, NewCand)Figure 5modifiers schema for recursing.Header:Constraint:Decomposition:modifier-absolute(Entity, Object, Cand, NewCand)modifier-pred(Pred)bmb(Speaker, Hearer, Pred(X) )subset( C and, XX.
bmb (Speaker, Hearer, Pred (X)  ,N ewC and)s-attrib(Entity, Pred)Figure 6modifier schema for absolute modifiers.Header:Constraint:Decomposition:modifier-relative(Entity, Object, Cand, NewCand)modifier-rel-pred(Pred)bmb(Speaker, Hearer, Pred(Object, OtherObject))subset(Cand, XX.
bmb(Speaker, Hearer, Pred(X)(Other)),NewCand)s-attrib-rel(Entity, OtherEntity, Pred)refer(OtherEntity, Other)Figure 7modifier schema for relative modifiers.such as "black," and the other is for relative modifiers, such as "larger."
The formeris shown in Figure 6; it decomposes into the surface speech action s-attrib and hasa constraint that determines the new candidate set, NewCand, by including only theobjects from the old candidate set, Cand, for which the predicate could be believedto be true.
The other schema is shown in Figure 7 and is used for describing objectsin terms of some other object.
It uses the surface speech action s-attrib-rel and alsoincludes a step to refer to the object of comparison.3.4 Plan Construction and Plan InferenceThe goals that we are interested in achieving are communicative goals.
Since thesegoals cannot be directly achieved by a plan of action, the speaker must instead planactions that will achieve them indirectly, for instance by planning an utterance thatresults in the hearer ecognizing her goal.
So, if the speaker wants to achieve Goal,she will attempt to construct a plan whose effect is bel(Hearer, goal(Speaker, Goal)).Plan Construction.
Given an effect, the plan constructor finds a plan derivation thathas a minimal number of primitive actions, that is valid (with respect to the planningagent's beliefs), and whose root action achieves the effect.
The plan constructor uses a359Computational Linguistics Volume 21, Number 3best-first search strategy, expanding the derivation with the fewest number of surfacespeech actions.
The yield of this plan derivation can then be given as input to a modulethat generates the surface form of the utterance.Plan Inference.
Following Pollack (1990), our plan inference process can infer plans inwhich, in the hearer's view, a constraint does not hold.
In inferring a plan derivation,we first find the set of plan derivations that account for the primitive actions thatwere observed, without regard to whether the constraints hold.
This is done by usinga chart parser that parses actions rather than words (Sidner 1994; Vilain 1990).
Forreferring plans that contain more than one modifier, there will be multiple derivationscorresponding to the order of the modifiers.
We avoid this ambiguity by choosing anarbitrary ordering of the modifiers for each such plan.In the second part of the plan inference process, we evaluate ach derivation byattempting to find an instantiation for the variables uch that all of the constraintshold with respect to the hearer's beliefs about he speaker's beliefs.
It could, however,be the case that there is no instantiation, either because this is not the right derivationor because the plan is based on beliefs not shared by the speaker and the hearer.
Inthe latter case, we need to determine which action in the plan is to blame, so that thisknowledge can be shared with the other participant.After each derivation has been evaluated, if there is just one valid derivation, aninstantiated derivation whose constraints all hold, then the hearer will believe that hehas understood.
If there is just one derivation and it is invalid, the action containingthe constraint that is the source of the invalidity is noted.
(We have not exploredambiguous ituations, those in which more than one valid derivation remains, or, inthe absence of validity, more than one invalid derivation.
)We now need to address how we evaluate a derivation.
In the case where the planis invalid, we need to partially evaluate the plan in order to determine which actioncontains a constraint that cannot be satisfied.
However, any instantiation will lead tosome constraint being found not to hold.
Care must therefore be taken in finding theright instantiation sothat blame is attributed to the action at fault.
So, we evaluate theconstraints in order of mention in the derivation, but postpone any constraints thathave multiple solutions until the end.
We have found that this simple approach canfind the instantiation for valid plans and can find the action that is in error for theothers.To illustrate this, consider the headnoun action, which has the following con-straints.speaker(Speaker)hearer(Hearer)world(World)bmb(Speaker, Hearer, category(Object, Category))subset(World, AX.
bmb(Speaker, Hearer, category(X, Category)),Cand)During the first step, finding the derivation, all co-referential variables will be unified.In particular, the variable Category will be instantiated from the co-referential variablein the surface speech action.
The first three constraints have only a single solution, sothey are instantiated.
The fourth constraint contains Object.
If there is exactly oneobject hat the system believes to be mutually believed to be of Category, then Objectis instantiated to it.
If there is none, then this constraint is unsatisfiable, and so theevaluation of this plan stops with this action marked as being in error, since no objectmatches this part of the description.
If there is more than one, then this constraint is360Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressionspostponed and the evaluator moves on to the subset constraint.
This constraint hasone uninstantiated variable, Cand, which has a unique (non-null) solution, namely thecandidate set associated with the head noun.
So, this constraint is evaluated.The evaluation then proceeds through the actions in the rest of the plan.
Assum-ing that no intervening errors are encountered, the evaluator will eventually reach theconstraint on the terminating instance of modifiers, Cand = \[Object\], with Cand instan-tiated to a non-null set.
If Cand contains more than one object, then this constraint willfail, pinning the blame on the terminating instance of modifiers for there not beingenough descriptors to allow the referent to be identified.
Otherwise, the terminatingconstraint will be satisfiable, and so Object will be instantiated to the single object inthe candidate set.
This will then allow all of the mutual belief constraints that werepostponed to be evaluated, since they will now have only a single solution.4.
Clarifications4.1 Planning and ClarifyingClark and Wilkes-Gibbs (1986) have presented a model of how conversational partic-ipants collaborate in making a referring action successful (see Section 2 above).
Theirmodel consists of conversational moves that express a judgment of a referring expres-sion and conversational moves that refashion an expression.
However, their modelis not computational.
They do not account for how the judgment is made, how thejudgment affects the refashioning, or the content of the moves.Following the work of Litman and Allen (1987) in understanding clarification sub-dialogs, we formalize the conversational moves of Clark and Wilkes-Gibbs as discourseactions.
These discourse actions are meta-actions that take as a parameter a referringexpression plan.
The constraints and decompositions of the discourse actions encodethe conditions under which they can be applied, how the referring expression deriva-tions can be refashioned, and how the speaker's beliefs can be communicated to thehearer.
So, the conversational moves, or clarifications, can be generated and under-stood within the planning paradigm.
7Surface Speech Actions.
An important part of our model is the surface speech ac-tions.
These actions serve as the basis for communication between the two agents,and so they must convey the information that is dictated by Clark and Wilkes-Gibbs'smodel.
For the judgment plans, we have the surface speech actions -accept, s-reject,and s-postpone, corresponding to the three possibilities in their model.
These take asa parameter the plan that is being judged, and for s-reject, also a subset of the speechactions of the referring expression plan.
The purpose of this subset is to inform thehearer of the surface speech actions that the speaker found problematic.
So, if the re-ferring expression was "the weird creature," and the hearer couldn't identify anythingthat he thought "weird," he might say "what weird thing," thus indicating he hadproblems with the surface speech action corresponding to "weird.
"For the refashioning plans, we propose that there is a single surface speech action,s-actions, that is used for both replacing a part of a plan, and expanding it.
Thisaction takes as a parameter the plan that is being refashioned and a set of surfacespeech actions that the speaker wants to incorporate into the referring expressionplan.
Since there is only one action, if it is uttered in isolation, it will be ambiguous7 We use the term clarification, since the conversational moves of judging and refashioning a referringexpression can be viewed as clarifying it.361Computational Linguistics Volume 21, Number 3between a replacement and an expansion; however, the speech action resulting fromthe judgment will provide the proper context o disambiguate its meaning.
In fact,during linguistic realization, if the two actions are being uttered by the same person,they could be combined into a single utterance.
For instance, the utterance "no, thered one" could be interpreted as an s-reject of the color that was previously used todescribe something and an s-actions for the color "red.
"So, as we can see, the surface speech actions for clarifications operate on compo-nents of the plan that is being built, namely the surface speech actions of referringexpression plans.
This is consistent with our use of plan derivations to represent ut-terances.
Although we could have viewed the clarification speech actions as acts ofinforming (Litman and Allen 1987), this would have shifted the complexity into theparameter of the inform, and it is unclear whether anything would have been gained.Instead, we feel that a parser with a model of the discourse and the context can de-termine the surface speech actions.
8 Additionally, it should be easier for the generatorto determine an appropriate surface form.Judgment Plans.
The evaluation of the referring expression plan indicates whetherthe referring action was successful or not.
If it was successful, then the referent hasbeen identified, and so a goal to communicate his is input to the plan constructor.This goal would be achieved by an instance of accept-plan, which decomposes intothe surface speech action s-accept.If the evaluation wasn't successful, then the goal of communicating the error isgiven to the plan constructor, where the error is simply represented by the node inthe derivation that the evaluation failed at.
There are two reasons why the evaluationcould have failed: either no objects match or more than one matches.
In the first case,the referring expression is overconstrained, and the evaluation would have failed onan action that decomposes into surface speech actions.
In the second case, the referringexpression is underconstrained, and so the evaluation would have failed on the con-straint hat specifies the termination of the addition of modifiers.
In our formalizationof the conversational moves, we have equated the first case to reject-plan and thesecond case to postpone-plan, and their constraints est for the abovementioned con-ditions.
The actions reject-plan and postpone-plan decompose into the surface speechactions -reject and s-postpone, respectively.By observing the surface speech action corresponding to the judgment, he hearer,using plan inference, should be able to derive the speaker's judgment plan.
If thejudgment was reject-plan or postpone-plan, then the evaluation of the judgment planshould enable the hearer to determine the action in the referring plan that the speakerfound problematic due to the constraints specified in the action schemas.
The identityof the action in error will provide context for the subsequent refashioning of thereferring expression.
9Refashioning Plans.
If a conversant rejects a referring expression or postpones judg-ment on it, then either the speaker or the hearer will refashion the expression i  thecontext of the rejection or postponement.
In keeping with Clark and Wilkes-Gibbs, weuse two discourse plans for refashioning: replace-plan and expand-plan.
The first is8 See Levelt (1989, Chapter 12) for how prosody and clue words can be used in determining the type ofclarification.9 Another approach would be to use the identity of the action in error to revise the beliefs that the agenthas attributed to the other conversant and to use the revised beliefs in refashioning the plan.
However,such reasoning is beyond the scope of this work.362Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressionsused to replace some of the actions in the referring expression plan with new ones, andthe second is to add new actions.
Replacements can be used if the referring expressioneither overconstrains or underconstrains the choice of referent, while the expansioncan be used only if it underconstrains the choice.
So, these plans can check for theseconditions.The decomposition of the refashioning plans encodes how a new referring ex-pression can be constructed from the old one.
This involves three tasks: first, a sin-gle candidate referent is chosen; second, the referring expression is refashioned; andthird, this is communicated to the hearer by way of the action s-actions, which wasalready discussed.
1?The first step involves choosing a candidate.
If the speaker of therefashioning is the agent who initiated the referring expression, then this choice isobviously pre-determined.
Otherwise, the speaker must choose the candidate.
Good-man (1985) has addressed this problem for the case of when the referring expressionoverconstrains the choice of referent.
He uses heuristics to relax the constraints of thedescription and to pick one that nearly fits it.
This problem is beyond the scope of thispaper, and so we choose one of the referents arbitrarily (but see Heeman \[1991\] forhow a simplified version of Goodman's algorithm that relaxes only a single constraintcan be incorporated into the planning paradigm).The second step is to refashion the referring expression so that it identifies thecandidate chosen in the first step.
This is done by using plan repair techniques (Hayes1975; Wilensky 1981; Wilkens 1985).
Our technique is to remove the subplan rooted atthe action in error and replan with another action schema inserted in its place.
Thistechnique has been encoded into our refashioning plans, and so can be used for bothconstructing repairs and inferring how another agent has repaired a plan.Now we consider the effect of these refashioning plans.
As we mentioned in Sec-tion 2, once the refashioning plan is accepted, the common ground of the participantsis updated with the new referring expression.
So, the effect of the refashioning plansis that the hearer will believe that the speaker wants the new referring expressionplan to replace the current one.
Note that this effect does not make any claims aboutwhether the new expression will in fact enable the successful identification of the ref-erent.
For if it did, and if the new referring expression were invalid, this would implythat the refashioning plan was also invalid, which is contrary to Clark and Wilkes-Gibbs's model of the acceptance process.
So, the understanding of a refashioning doesnot depend on the understanding of the new proposed referring expression, but onlyon its derivation.4.2 Action SchemasThis section presents action schemas for clarifications.
Each clarification action includesa surface speech action in its decomposition.
However, all reasoning is done at the levelof the clarification actions, and so the surface actions do not include any constraintsor effects.
The notation used in the action schemas was given in Table 1 above.accept-plan.
The discourse action accept-plan, shown in Figure 8, is used by thespeaker to establish the mutual belief that a plan will achieve its goal.
The constraintsof the schema specify that the plan being accepted achieves its goal and the decompo-sition is the surface speech action s-accept.
The effect of the schema is that the hearer10 Another approach would have been to separate the communicative task from the first two (Lambertand Carberry 1991).363Computational Linguistics Volume 21, Number 3Header:Constraint:Decomposition:Effect:accept-plan(Plan)bel(Speaker, achieve(Plan, Goal))s-accept(Plan)bel(Hearer, goal(Speaker, bel(Hearer, bel(Speaker,achieve(Plan, Goal)))))Figure 8accept-plan schema.Header:Constraint:Decomposition:Effect:reject-plan(Plan)bel(Speaker, error(Plan, ErrorNode))yield(Plan, ErrorNode,Acts)not(Acts = \[J)s-reject(Plan,Acts)" bel(Hearer, goal(Speaker, bel(Hearer, bel(System,error(P lan, ErrorNo de )) ) ) )Figure 9reject-plan schema.Header:Constraint:Decomposition:Effect:postpone-plan(Plan)bel(Speaker, error(Plan, ErrorNode))yield(Plan, ErrorNode,Acts)Acts = lJs-postpone(Plan,Acts)bel(Hearer, goal(Speaker, b el(Hearer, bel(Speaker,error(P lan, ErrorNode) )  ) )Figure 10postpone-plan schema.will believe that the speaker has the goal that it be mutually believed that the planachieves its goal.reject-plan.
The discourse action reject-plan, shown in Figure 9, is used by the speakerif the referring expression plan overconstrains the choice of referent.
The speaker usesthis schema in order to tell the hearer that the plan is invalid and which action instancethe evaluation failed in.
The constraints require that the error occurred in an actioninstance whose yield includes at least one primitive action.
The decomposition consistsof s-reject, which takes as its parameter the surface speech actions that are in the yieldof the problematic action.postpone-plan.
The schema for postpone-plan, shown in Figure 10, is similar to reject-plan.
However, it requires that the error in the evaluation occurred in an action thatdoes not decompose into any primitive actions, which for referring expressions willbe the instance of modifiers that terminates the addition of modifiers.replace-plan.
The replace-plan schema is used by the speaker to replace some of theprimitive actions in a plan with new actions.
Because we need knowledge of the typeof action where the error occurred in order to refashion the invalid plan, the constraintsof this schema re more specific than those of the judgment plans.
The schema that364Peter A. Heeman and Graeme Hirst Collaborating on Referring ExpressionsHeader:Constraint:Decomposition:Effect:replace-plan(Plan)bel(Speaker, ror(Plan, ErrorNode))content(Plan, ErrorNode, ErrorContent)ErrorContent =modifier(Entity, Object l, Cand, Candl)pick-one(Object, Cand)Replacement = modifier(Entity, Object, Cand, Cand2)substitute(Plan, Node, Replacement, NewPlan)replan(NewPlan,Acts)s-actions(Plan, Acts)bel(Hearer, goal(Speaker, bel(Hearer, bel(Speaker,replace(Plan, NewPlan)))))Figure 11replace-plan schema.we give in Figure 11, for instance, is used to refashion a referring expression plan inwhich the error occurred in an instance of a modifier action, uThe decomposition of the schema specifies how a new referring expression plancan be built.
12 The first step, pick-one(Object, Cand), chooses one of the objects thatmatched the part of the description that preceded the error; if the speaker is not theinitiator of the referring expression, then this is an arbitrary choice.
The second stepspecifies the header of the action schema that will be used to replace the subplanthat contained the error.
The third step substitutes the replacement into the referringexpression plan, undoing all variable instantiations in the old plan.
This results inthe partial plan NewPlan.
The fourth step calls the plan constructor to complete thepartial plan.
Finally, the fifth step is the surface speech action s-actions, which is usedto inform the hearer of the surface speech actions that are being added to the referringexpression plan.expand-plan.
The expand-plan schema, shown in Figure 12, is similar to the replace-plan schema shown in Figure 11.
The difference is that instead of replacing one ofthe instances of modifier, it replaces the terminal instance of modifiers by a modifierssubplan that distinguishes one of the objects from the others that match, thus effect-ing an expansion of the surface speech actions.
Even if the speaker thought hat thereferring expression as it stands were adequate (since the candidate set Cand containsonly one member), she will construct a non-null expansion since the replacement isthe recursive version of modifiers.4.3 Plan Construction and Plan InferenceThe general plan construction and plan inference processes are essentially the sameas those for referring expressions.
However, the plan inference process has been aug-mented so as to embody the criteria for understanding that were outlined in Section 4.1.The inference of judgment plans must be sensitive to the fact that such a plan includesthe constraint that the speaker found the judged plan to be in error even though the11 If the error occurred in an instance of headnoun, a different replace-plan schema would need to beused, one that for instance relaxed the category that was used in describing the object (Goodman 1985;Heeman 1991).12 We refer to the steps in the decomposit ion that are not action headers as mental ctions.
They need to beproved, just like constraints.365Computational Linguistics Volume 21, Number 3Header:Constraint:Decomposition:Effect:expand-plan(Plan)bel(Speaker, ror(Plan, ErrorNode))content(Plan, ErrorNode, ErrorContent)ErrorContent =modifiers-terminate(Entity, Objectl, Cand)pick-one(Object, Cand)Replacement = modifiers-recurse(Entity, Object, Cand)substitute(Plan,ErrorNode, Replacement, NewPlan)replan (NewPlan, Acts)s-actions(Plan, Acts)bel(Hearer, goal(Speaker, bel(Hearer, bel(Speaker,replace(Plan, NewPlan)))))Figure 12expand-plan schema.hearer might not believe it to be.
So, the inference process is allowed to assume thatthe speaker believes any constraint that the goal of the plan implies.In the case of a refashioning, the hearer might not view the proposed referringexpression plan as being sufficient for identifying the referent, but would nonethelessunderstand the refashioning.
So, the inference process requires only that the proposedreferring expression be derived--so that it can serve to replace the current plan--butnot that it be acceptable.
So, when a replan action is part of a plan that is beingevaluated, the success of this action depends only on whether the plan that is itsparameter can be derived, but not whether the derived plan is valid.
135.
Modeling CollaborationIn the last two sections, we discussed how initial referring expressions, judgments,and refashionings can be generated and understood in our plan-based model.
In thissection, we show how plan construction and plan inference fit into a complete modelof how an agent collaborates in making a referring action successful.
Previous naturallanguage systems that use plans to account for the surface speech acts underlying anutterance (such as Cohen and Perrault 1979; Allen and Perrault 1980; Appelt 1985;Litman and Allen 1987) model only the recognition or only the construction of anagent's plans, and so do not address this issue.In order to model an agent's participation i a dialog, we need to model how themental state of the agent changes as a result of the contributions that are made to thedialog.
The change in mental state can be modeled by the beliefs and goals that a par-ticipant adopts.
When a speaker produces an utterance, as long as the hearer finds it co-herent, he can add a belief that the speaker has made the utterance to accomplish somecommunicative goal.
The hearer might then adopt some goal of his own in response tothis, and make an utterance that he believes will achieve this goal.
Participants expecteach other to act in this way.
These social norms allow participants to add to theircommon ground by adopting the inferences about an utterance as mutual beliefs.To account for how conversants collaborate in dialog, however, this cooperationis not strong enough.
Not only must participants form mutual beliefs about what wassaid, they must also form mutual beliefs about the adequacy of the plan for the task13 Another approach would be to have the plan inference process reason about the intended effects of theplan that it is inferring in order to decide whether it should evaluate embedded plans and whether thisevaluation should affect the evaluation f the parent plan.366Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressionsthey are collaborating upon.
If the plan is not adequate, then they must work togetherto refashion it.
This level of cooperation is due to what Clark and Wilkes-Gibbs referto as a mutual responsibility, orwhat Searle (1990) refers to as a we-intention.
This allowsthe agents to interact so that neither assumes control of the dialog, thus allowing bothto contribute to the best of their ability without being controlled or impeded by theother.
This is different from what Grosz and Sidner (1990) have called master-servantdialogs, which occur in teacher-apprentice or information-seeking dialogs, in whichone of the participants i  controlling the conversation (cf.
Walker and Whittaker 1990).Note that the noncontrolling agent may be helpful by anticipating obstacles in theplan (Allen and Perrault 1980), but this is not the same as collaborating.The mutual responsibility that the agents hare not only concerns the goal theyare trying to achieve, but also the plan that they are currently considering.
This planserves to coordinate their activity and so agents will have intentions to keep thisplan in their common ground.
The plan might not be valid (unlike the shared plan ofGrosz and Sidner \[1990\]), so the agents might not mutually believe that each actioncontributes to the goal of the plan.
Because of this, agents will have a belief regardingthe validity of the plan, and an intention that this belief be mutually believed.The discourse plans that we described in the previous ection can now be seenas plans that can be used to further the collaborative activity.
Judgment plans expressbeliefs about he success of the current plan, and refashioning plans update it.
So, themental state of an agent sanctions the adoption both of goals to express judgment andof goals to refashion.
It also sanctions the adoption of beliefs about he current plan.
14If it is mutually believed that one of the conversants believes there is an error withthe current plan, the other also adopts this belief.
Likewise, if one of the conversantsproposes a replacement, the other accepts it.
Since both conversants expect he other tobehave in this way, each judgment and refashioning, so long as they are understood,results in the judgment or refashioning being mutually believed.
Thus the current plan,through all of its refashionings, remains in the common ground of the participants.Below, we discuss the rules for updating the mental state after a contribution ismade.
We then give rules that account for the collaborative process.
155.1 Rules for Updating the Mental StateAfter a plan has been contributed to the conversation, by way of its surface speechactions, the speaker and hearer update their beliefs to reflect he contribution that hasbeen made.
Both assume that the hearer is observant, can derive a coherent plan (notnecessarily valid), and can infer the communicative goal, which is expressed by theeffect of the top-level action in the plan.
We capture this by having the agent hat weare modeling, the system, adopt he belief that it is mutually believed that the speakerintends to achieve the goal by means of the plan.
16bmb(system, user, plan(Speaker, Plan, Goal))The system will also add a belief about whether she believes the plan will achieve thegoal, and if not, the action that she believes to be in error.
So, one of the followingpropositions will be adopted.14 The collaborative activity also sanctions discourse expectations that he other participant's utteranceswill pertain to the collaborative activity.
We do not explicitly address this, however.15 For simplicity, we represent the rules for entering into a collaborative activity, adopting beliefs, andadopting goals with the same operator, 4==.
For a more formal account, three different operatorsshould be used.16 See Perrault (1990) for how these inferences can be drawn by using default rules.367Computational Linguistics Volume 21, Number 3bel(system, achieve(Plan, Goal))bel(system, error(Plan, Node))After the above beliefs have been added, there are a number of inferences thatthe agents can make and, in fact, can believe will be made by the other participantas well, and so these inferences can be mutually believed.
The first rule is that if it ismutually believed that the speaker intends to achieve Goal by means of Plan, then itwill be mutually believed that the speaker has Goal as one of her goals.
17Rule 1bmb(system, user, goal(Agtl, Goal))bmb(system, user, plan(Agt l,Plan, Goal)) &Agtl E {system, user}The next rule concerns the adoption by the hearer of the intended goals of com-municative acts.
The communicative goal that we are concerned with is where thespeaker wants the hearer to believe that the speaker believes some proposition.
Thisonly requires that the hearer believe the speaker to be sincere.
We assume that bothconversants are sincere, and so when such a communicative goal arises, both partici-pants will assume that the hearer has adopted the goal.
This is captured by Rule 2.Rule 2bmb(system, user, bel(Agtl,Prop))bmb(system, user, goal (A gt l, bel (Agt 2,bel (Agt l,Prop) ) ) &Agtl,Agt2 C {system, user} &not(Agtl = Agt2)The last rule involves an inference that is not shared.
When the user makes acontribution to a conversation, the system assumes that the user believes that the planwill achieve its intended goal.Rule 3bel(system, bel(user, achieve(Plan, Goal)))bmb (system, user, plan (user, Plan, Goal))5.2 Rules for Updating the Collaborative StateThe second set of rules that we give concern how the agents update the collaborativestate.
These rules have been revised from an earlier version (Heeman 1991) so as tobetter model the acceptance process.5.2.1 Entering into a Collaborative Activity.
We need a rule that permits an agentto enter into a collaborative activity.
We use the predicate cstate to represent that anagent is in such a state, and this predicate takes as its parameters the agents involved,the goal they are trying to achieve, and their current plan.
Our view of when such acollaborative activity can be entered is very simple: the system believes it is mutuallybelieved that one of them has a goal to refer and has a plan for doing so, but one ofthem believes this plan to be in error.
The last part of the condition states that if thespeaker's referring expression was successful from the beginning, no collaboration is17 All variables mentioned in the rules are existentially quantified.368Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressionsnecessary.
It is not required that both participants mutually believe there is an error.Rather, if either detects an error, then that conversant can presuppose that they arecollaborating, and make a judgment.
Once the other recognizes the judgment that theplan is in error, the criteria for him entering will be fulfilled for him as well.Rule 4cstate(system, user, Plan, Goal)bmb(system, user, goal(Agtl,Goal)) &bmb(system, user, plan(Agt l,Plan, Goal))&Goal = knowref(Agt2,Agt l,Entity, Object)&bel(system, bel(Agt3,error(Plan,Node))) &Agtl,Agt2,Agt3 C {system, user} &not(Agtl = Agt2)5.2.2 Adoption of Mutual Beliefs.
In order to model how the state of the collaborativeactivity progresses, we need to account for the mutual beliefs that the agents adopt asa result of the utterances that are made.The first rule is for judgment moves in which the speaker finds the current plan inerror.
Given that the move is understood, both conversants, by way of the rules givenin Section 5.1, will believe that it is mutually believed that the speaker believes thecurrent plan to be in error.
In this case, the hearer, in the spirit of collaboration, mustaccept he judgment and so also adopt the belief that the plan is in error, even if heinitially found the plan adequate.
Since both conversants expect he hearer to behavein this way, the belief that there is an error can be mutually believed.
Rule 5, below,captures this.
(The adoption of this belief will cause the retraction of any beliefs thatthe plan is adequate.
)Rule 5bmb(system, user, error(Plan, Node))cstate(system, user, Plan, Goal)&bmb(system, user, bel(Agtl, error(Plan, Node))) &Agtl E {system, user}The second rule is for refashioning moves.
After such a move, the conversantswill believe it mutually believed that the speaker has a replacement, NewPlan, forthe current plan, Plan.
Again, in the spirit of collaboration, the hearer must acceptthis replacement, and since both expect each other to behave this way, both adopt thebelief that it is mutually believed that the new referring expression plan replaces theold one.Rule 6bmb(system, user, replace(Plan, NewPlan))Agtl E {system, user} &cstate(system, user, Plan, Goal)&bmb(system, user, error(Plan,Node)) &bmb(system, user, bel(Agtl,replace(Plan,NewPlan)))In adopting this belief, the system updates the cstate by replacing the current plan withthe new plan, and adding beliefs that capture the utterance of NewPlan as outlinedin Section 5.1 above.369Computational Linguistics Volume 21, Number 3The third rule is for judgment moves in which the speaker finds the current planacceptable.
Given that the move has been understood, each conversant will believe it ismutually believed that the speaker believes that the current plan will achieve the goal(second condition of the rule).
However, in order to accept his move, each participantalso needs to believe that the hearer also finds the plan acceptable (third condition).This belief would have been inferred if it were the hearer who had proposed thecurrent plan, or the last refashioning.
In this case, the speaker (of the acceptance)would have inferred by way of Rule 3 that the hearer believes the plan to be valid; asfor the hearer, given that he contributed the current plan, he undoubtedly also believesit to be acceptable.Rule 7bmb(system, user, achieve(Plan, Goal))cstate(system, user, Plan, Goal)&bmb(system, user, bel(Agtl, achieve(Plan, Goal)))&bel(system, bel(Agt2,achieve(Plan, Goal))) &Agtl,Agt2 E {system, user} &not(Agtl = Agt2)5.2.3 Adopting Goals.
The last set of rules completes the circle.
They account for howagents adopt goals to further the collaborative activity.
These goals lead to judgmentand refashioning moves, and so correspond to the rules that we just gave for adoptingmutual beliefs.The first goal adoption rule is for informing the hearer that there is an error in thecurrent plan.
The conditions pecify that Plan is the current plan of a collaborativeactivity and that the speaker believes that there is an error in it.Rule 8goal(system, bel(user, bel(system, error(Plan,Node))))cstate(system, user, Plan, Goal)&bel(system, error(Plan, Node))The second rule is used to adopt the goal of replacing the current plan, Plan, if ithas an error.
The rule requires that the agent believe that it is mutually believed thatthere is an error in the current plan.
So, this goal cannot be adopted before the goalof expressing judgment has been planned.
Note that the consequent has an unboundvariable, NewPlan.
This variable will become bound when the system develops a planto achieve this goal, by using the action schema replace-plan (see Figure 11 above).Rule 9goal(system, bel(user, bel(system, replace(Plan,NewPlan))))cstate(system, user, Plan, Goal)&bmb(system, user, error(Plan, Node))The third rule is used to adopt the goal of communicating the system's acceptanceof the current plan.
Not only must the system believe that the plan achieves the goal,but it must also believe that the user also believes this.
As mentioned above for Rule 7,this last condition prevents the system from trying to accept a plan that it has itselfjust proposed.
Rather, it can only try to accept a plan that the other agent contributed,for it is just such plans for which it will have the belief, by way of Rule 3, that the370Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressionsuser believes the plan achieves the goal.Rule 10goal(system, bel(user, bel(system, achieve(Plan, Goal))))cstate(system, user, Plan, Goal)&bel(system, achieve(Plan, Goal)) &bel(system, bel(user, achieve(Plan, Goal)))5.3 Applying the RulesThe rules that we have given are used to update the mental state of the agent andto guide its activity.
Acting as the hearer, the system performs plan inference on eachset of actions that it observes, and then applies any rules that it can.
When all of theobserved actions are processed, the system switches from the role of hearer to speaker.As the speaker, the system checks whether there is a goal that it can try to achieve,and if so, constructs a plan to achieve it.
Next, presupposing its partner's acceptanceof the plan, it applies any rules that it can.
It repeats this until there are no more goals.The actions of the constructed plans form the response of the system; in a completenatural language system, they would be converted to a surface utterance.
The systemthen switches to the role of hearer.6.
An ExampleWe are now ready to illustrate our system in action.
TM For this example, we use asimplified version of a subdialog from the London-Lund corpus (Svartvik and Quirk1980, S.2.4a:1-8):(6.1) A: t See the weird creature.B: 2 In the corner?A: 3 No, on the television.B: 4 Okay.The system will take the role of person B and we will give it the belief that there aretwo objects that are "weird"--a television antenna, which is on the television, and afern plant, which is in the corner.6.1 Understanding "The Weird Creature"For the first sentence, the system is given as input the surface speech actions under-lying "the weird creature," as shown below:s-refer(entity1)s-attrib(entityl, XX.
assessment(X, weird))s-attrib(entityl,&X, category(X, creature))The system invokes the plan inference process, which finds the plan derivations whoseyield is the above set of surface speech actions.
In this case, there is only one, and thesystem labels it pl.
Figure 13 shows the derivation; arrows represent decomposition,and for brevity, constraints and mental actions have been omitted and the parametersonly of the surface speech actions are shown.18 The system isimplemented in C-Prolog under UNIX.371Computational Linguistics Volume 21, Number 3refers-refer(entity1) describeheadno~~~ modifiers-recurses-attrib(entityl,)~X, category(X ,c~~'~ modifiers-terminatemodifier-absolutes-attrib(entityl,)~X, assessment(X, weird)) nttllFigure 13Plan derivation (pl) for "The weird creature.
"Next, the plan derivation isevaluated.
The subset constraint in the headnoun actionis evaluated, which narrows the candidate set to the antenna nd the fern plant.
Thesubset constraint in the modifier action is then evaluated, which does not eliminateeither of the candidates, ince the system finds both of them "weird."
The constrainton the modifiers action that terminates the addition of modifiers is then evaluated.However, this constraint fails, since there are two objects that match the descriptionrather than one, as required.The system then updates its beliefs.
As described in Section 5.1, the system addsthe following beliefs to capture the results of the plan inference process: that it ismutually believed that the user has the goal of knowref and has adopted pl as ameans to achieve it, and that pl has an error on the terminating instance of modifiers,node p22.bmb(system, user, plan(user, pl,knowref(system, user, entityl,0bject)))bel(system, error(p1,p22))(11)(12)The system next tries to apply the belief and goal adoption rules.
From Rule 1 andbelief (11), the system adds the belief that it is mutually believed that the user has thegoal that the system knowref, and from Rule 3 and belief (11), it adds the belief thatthe user believes that the plan achieves its goal.bmb(system, user, goal(user, knowref(system, user, entityl,0bject)))bel(system, bel(user, achieve(pl, knowref(system, user, entityl,0bject))))(13)(14)Belief (13), along with (11) and (12), allows the system to apply Rule 4, and so thesystem enters into a collaborative activity, in which the goal is for it to know thereferent and in which the current plan is pl.cstate(system, user, pl,knowref(system, user, entityl,0bject)) (15)Since the system believes there is an error in the current plan, it applies Rule 8, andso gives itself the communicative goal of informing the user of the error in the currentplan.372Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressionsgoal(system, bel(user, bel(system, error(p1,p22)))) (16)6.2 Constructing "In the Corner?
"Since there are no further belief or goal adoption rules that can be applied, the systemnext checks for any goals that it can try to achieve.
The only goal is (16), which is toinform the user of the error in the plan.
Since the error in the referring plan is in theterminating instance of modifiers, the plan constructor builds an instance of postpone-plan, which it names p26.
(The schema was given in Figure 10.)
Rather than realizingthe surface speech action immediately, the system plans ahead.
(This would allow anopportunistic process to combine surface speech actions into a single utterance \[Appelt1985\].)
So, the system, presupposing that the user understands the system's plan, addsthe following belief.bmb (user, system,p lan (system, p 2 6,be l (user, be l(system, error(p l,p 22 )) ) ) ) (17)It also adds the belief that this plan will achieve its goal.bel (system, achieve(p 26,bel (user, bel (system, error(p l,p 22) ) ) ) (18)Then by Rule 1, the system adds the belief that it is mutually believed that it has thegoal.bmb(system, user, goal(system, bel(user, bel(system, error(p l,p22))))) (19)Then by Rule 2, which captures the cooperativity of the agents in communicativegoals, it adds the belief that it is mutually believed that the system believes there isan error.bmb(system, user, bel(system, error(p1,p22))) (20)Then, on the basis of (15) and (20) the system applies Rule 5, thus adopting the beliefthat it is mutually believed that there is an error in the plan.
This presupposes theuser's acceptance of the judgment plan.bmb (system, user, error(p1,p22)) (21)The system is now able to apply Rule 9, on the basis of (15) and (21), and so adoptsthe goal of refashioning the invalid referring expression plan and of informing theuser of the new plan.goal(system, bel(user, bel(system, replace(pl,RPlan)))) (22)Since no further rules can be applied, the system checks for goals that it can tryto fulfill, which will result in choosing (22).
To achieve this goal, the plan constructorbuilds an instance of expand-plan (previously shown in Figure 12).
In doing this, thesystem chooses one of the objects that matched the original description as the likelyreferent; in this case it happens to choose the object in the corner, the fern plant, whichthe system represents as fern1.
It then substitutes the modifiers ubplan that terminatesthe addition of modifiers with the header of the modifiers-recurse action (with thechosen object instantiated in).
The plan constructor is then called to fill in the details,thereby creating the expansion.
The expansion it chooses includes a relative modifier(see Figure 7) that describes the object as being in the corner.
The new referring plan(labeled p34) is shown in Figure 14, with the expansion circled (we have abbreviated373Computational Linguistics Volume 21, Number 3refers-refer(entity1) describeheadnounmodifiem-recurses-attrib(entityl,AX, c a t e g o r y ( ~modifie?bsolute / ~ ~.k xs-atri " ?Figure 14Plan derivation (p34) for "The weird creature in the corner.
"the derivation of "the corner").
The surface speech action of expand-plan is s-actions,which takes the surface speech actions of the expansion, listed below, as its parameter.s-attrib-rel(entityl, entity2,XX.
AY.
in(X,Y))s-refer(entity2)s-attrib(entity2,)C~, category(X, corner))Next, the system assumes the user will understand the refashioning, and, by wayof Rules I and 2, will be cooperative and adopt the communicative goal that the systembelieves that the new expanded plan replaces the old referring expression plan.
Theend result is given below as (23).bmb(system, user, bel(system, replace(p1,p34))) (23)The system, on the basis of (15) and (23), applies Rule 6, and so assumes that the userwill accept he refashioning.
So, the system adds the belief that it is mutually believedthat the new expanded plan replaces the old referring expression.bmb(system, user, rep lace(p1,p34) ) (24)This causes the belief module to update the current plan of the collaborative activity(25).
Also, it adds the beliefs that capture the utterance of the refashioned plan: thatthe system intends it as a means to achieve the referring action and that it does achievethis goal.
1919 Even though the system has the referent incorrectly identified in the goal of knowref, the goal itself is374Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressionscstate(system, user, p34,knowref(system, user, entity1,0bject))) (25)bmb(system, user, plan(system,p34,knowref(system, u er, entity1,fern1))) (26)bel(system, achieve(p34,knowref(system, us r, entity1,fern1))) (27)The two plans that were constructed, postpone-plan and expand-plan, give riseto the output of the surface speech actions s-postpone and s-expand, which would berealized as "in the corner?'.
2?6.3 Understanding "No, on the Television"The user next utters "No, on the television."
This would get parsed into two separatesurface speech actions, an s-reject corresponding to "no," and an s-actions corre-sponding to "on the television."
For simplicity, the plan inference process is invokedseparately on each.The system starts with the s-reject action.
We assume that the parser can determinefrom context hat the "no" is rejecting the surface speech actions that were previouslyadded and so the parameter of s-reject is a list of these actions.
From this, it derivesa plan whose yield is the s-reject action, and this plan is an instance of reject-plan(previously shown in Figure 9).
The system then evaluates the constraints of the plan,which results in it determining which action in the plan the user found to be in error.This is done by evaluating the constraints of reject-plan, and so finding the actionwhose yield is the surface speech actions that were rejected.
This will be p56, themodifiers-relative action that described the object as being in the corner.
The resultingbelief, after applying Rules 1 and 2, is the following.bmb(system, user, bel (user, error(p34,p56) ) ) (28)The system then applies the appropriate acceptance rule, Rule 5, and so adopts thebelief that the error is mutually believed.bmb (system, user, error(p 34,p56) ) (29)With this belief, the system will have the context hat it needs to understand the user'srefashioning plan.The system next performs plan recognition starting with the second surface speechaction, s-actions, which corresponds to the refashioning "on the television."
It takesas a parameter the following list of actions: 21s-attrib-rel(entityl, entity3,XX.
A Y. on(X,Y))s-refer(entity3)s-attrib(entity3,XX, category(X, television))The system finds two plan derivations that account for the primitive action, one aninstance of replace-plan (see Figure 11) and the other an instance of expand-plan (seeFigure 12).
Next it evaluates the constraints of each derivation.
The constraints ofstill valid: for it to identify the referent corresponding to entity1.20 Although our model does not account for the questioning intonation, it could be a manifestation f thes-postpone.21 We assume that the parser determines the appropriate discourse ntities in these actions: entity1 is thediscourse ntity for the object being referred to, and entity3 is another discourse ntity.375Computational Linguistics Volume 21, Number 3expand-plan do not hold since the action in error, p56, is not an instance of modifiers-terminate, so this plan is eliminated.
The constraints (and mental actions) of replace-plan do hold, and so the system is able to derive the refashioned referring plan, whichit labels p104.Since this instance of replace-plan is the only valid derivation corresponding tothe surface speech actions observed, the system takes it as the plan behind the user'sutterance.
As a result, the system adds the following belief (after applying Rules I and2).bmb (system, user, bel (user, eplace(p34,p104) ) ) (30)The system then applies the acceptance rule for refashioning plans, Rule 6, and soadopts the refashioning as mutually believed.bmb(system, user, replace(p34,p104) ) (31)This causes the belief module to update the current plan of the collaborative activityand to add the belief that the user contributed the new referring expression plan.cstate(system, user, p104,knowref(system, userentity1,0bject)) (32)bmb(system, user, plan(user, p104,knowref(system, user, entity1,antenna1)) (33)The new referring plan will already have been evaluated.
The subplan correspondingto "the television" would have been understood without problem, 22 and the modifiercorresponding to "on the television" would have narrowed own the candidates thatmatched "weird creature" to a single object, antenna1.
So, the belief module adds thebelief that the system finds the new referring plan to be valid.
Also, by way of Rule 3,the system adds the belief that the user also does, since the user had proposed it.bel(system, achieve(p104,knowref(system, user, entity1,antenna1))) (34)bel(system, bel(user, achieve(plO4,knowref(system, us r, entityl,antennal)))) (35)6.4 Constructing "Okay"On the basis of (32), (34), and (35), the system is able to apply Rule 10, and so adoptsthe goal of accepting the plan.goal(system, bel(user, bel(system, achieve(p104,knowref(system, user, entityl,antennal)))))) (36)The plan constructor achieves this by planning an instance of accept-plan, whichresults in the surface speech action s-accept, which would be realized as "Okay.
"Then, after the application of Rules 1, 2, and most importantly 7, the system adoptsthe belief that it is mutually believed that the plan achieves the goal of referring.bmb(system, user, achieve(p104,knowref(system, user, entihd1,antenna1))) (37)22 If "the television" is not understood, then since it is a referring expression in its own right, theconversants could collaborate on identifying its referent independently of the referent of "the weirdcreature"; that is, the participants could enter into an embedded collaborative activity by focusing onone part of the current plan.376Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressions7.
Comparisons to Related WorkIn providing a computational model of how agents collaborate upon referring expres-sions, we have touched on several different areas of research.
First, our work has builton previous work in referring expressions, especially their incorporation i to a modelbased on the planning paradigm.
Second, our work has built on the research done inmodeling clarifications in the planning paradigm and on plan repair.
Third, our workis related to the research being done on modeling collaborative and joint activity.7.1 Referring ExpressionsCohen (1981) and Appelt (1985) have also addressed the generation ofreferring expres-sions in the planning paradigm.
They have integrated this into a model of generatingutterances, a step that we haven't aken.
However, we have extended their modelby incorporating even the generation of the components of the description into ourplanning model.
One result of this is that our surface speech actions are much morefine-grained.7.2 Clarifications and Plan RepairAn important part of our work involves accounting for clarifications of referring ex-pressions by using meta-actions that incorporate plan repair techniques.
This approachis based on Litman and Allen's work (1987) on understanding clarification subdialogs,in which meta-actions were used to model discourse relations, such as clarifications.There are several major differences between our work and theirs.
First, our work ad-dresses not only understanding, but also generation, and how these two tasks fit intoa model of how agents collaborate in discourse.
Second, Litman and Allen use a stackof unchanging plans to represent the state of the discourse.
We, however, use a singlecurrent plan, modifying it as clarifications are made.
This difference has an impor-tant ramification, for it results in different interpretations of the discourse structure.Consider dialog (7.1), which was collected at an information booth in a Toronto trainstation (Horrigan 1977).
(Although the participants are not collaborating in making areferring expression, the dialog will serve to illustrate our point.
)(7.1) P- 1 The 8:50 to Montreal?C" 2 8:50 to Montreal.
Gate 7.p: 3 Where is it?C: 4 Down this way to your left.
Second one on the left.p.
5 OK.
Thank you.Litman and Allen represent the state of the discourse after the second utterance asa clarification of the passenger's take-train-trip lan.
The information that the trainboards at gate 7 is represented only in the clarification plan.
So, when the passengerasks "Where is it?," their system, acting as the clerk, cannot interpret this as a clarifica-tion of the take-train-trip lan, since the utterance "cannot be seen as a step of \[that\]plan" (p. 188).
So, it is interpreted instead as a request for a clarification of the clerk's"Gate 7" response, implicitly assuming that "Gate 7" was not accepted.
In our model,the acceptance of "Gate 7" would be presupposed, and so it would be incorporatedinto the take-train-trip lan.
So, the passenger's question of "Where is it?"
would beviewed as a request for the clerk to clarify that plan.The work of Moore and Swartout (1991), Cawsey (1991), and Carletta (1991) oninteractive explanations also addresses clarifications using plan repair techniques.
Thisbody of work uses plan construction techniques to generate explanations, and uses the377Computational Linguistics Volume 21, Number 3constructed plan as a basis for recovery strategies if the user doesn't understand theexplanation.
In the cases of Cawsey and Carletta, both use meta-actions to encode theplan repair techniques.
However, none of these approaches i within a collaborativeframework, in which either agent can contribute to the development of the plan.Other relevant work is that of Lambert and Carberry (1991).
In their model of un-derstanding information-seeking dialogs, they propose a distinction between problem-solving activities and discourse activities.
In contrast, our clarifications embody bothfunctions in the same actions, thus allowing for a simpler approach to inferring therefashioned referring expressions, ince we need not chain to a meta-operator.
In laterwork, Chu-Carroll and Carberry (1994) extended this model to generate responses toproposals that are viewed as sub-optimal or invalid.
Like Litman and Allen (1987),they adopt the view that subsequent modifications apply to the preceding modifica-tion, rather than the underlying plan.7.3 CollaborationGrosz, Sidner, and Lochbaum (Grosz and Sidner, 1990; Lochbaum, Grosz, and Sidner,1990) are interested in the type of plans that underlie discourse in which the agentsare collaborating in order to achieve some goal.
They propose that agents are buildinga shared plan in which participants have a collection of beliefs and intentions about theactions in the plan.
Our model differs from theirs in two important aspects.
First, notonly do agents have a collection of beliefs and intentions regarding the actions of ashared plan, but we feel that they also have an intention about the goal (Searle 1990;Cohen and Levesque 1991).
It is this intention, in conjunction with the current plan,that sanctions the adoption of beliefs and intentions about potential actions that willcontribute to the goal, rather than just the shared plan.Second, we feel that their definition of a partial shared plan is too restrictive.Although they address partial plans, they require, in order for an action to be part ofa partial shared plan, that both agents believe that the action contributes to the goal.However, this is too strong.
In collaborating to achieve a mutual goal, participantssometimes propose an action that is not believed by the other participant or even bythe participant that is proposing it.
In failing to represent such states, their model isunable to represent the intermediate states in which a hearer might have understoodhow the speaker's utterance contributes to a plan, but doesn't agree with it.
This isimportant, since if the refashioned plan is invalid, only the referring expression shouldbe refashioned, not the refashioning itself.Traum (1991; Traum and Hinkelman, 1992) is concerned with providing a compu-tational model of grounding, the process in which conversational participants add to thecommon ground of a conversation (Clark and Schaefer 1989; Clark and Brennan 1990).Traum models the grounding process by proposing that utterances move through anumber of states, 'pushed' by grounding acts, which include initiate, continue, repair,request repair, acknowledge, and request acknowledge.
Once an utterance has beenacknowledged, it will reside in mutual belief as a proposal of the person who initiatedit.
The proposal state is a subspace of the mutual belief space of the conversants.
Onlyonce it has been accepted will it be moved into the shared space (also in mutual belief).Unlike Traum's, our work does not differentiate the proposal state from the sharedstate.
If a proposal is understood, it is incorporated into the current plan.
Judgmentsof acceptability are not on proposals but on the current plan, or a part of it.Sidner (1994) addressed the issue of how conversational participants collaboratein building a shared plan.
In this work, Sidner presents a number of speech actionsfor use in collaborative tasks.
These actions are those that an artificial agent could usein negotiating which actions or beliefs to accept into the shared plan of the agents.
As378Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressionswith Traum, it is the proposals that are refashioned, before they are integrated into theshared plan, rather than the shared plan.Cohen and Levesque (1991) focus on formalizing joint intention in a logic.
They usethis formalism to explain how such elements of communication as confirmations arisewhen agents are engaging in a joint action.
However, they have not addressed howagents collaborate in building a plan, only how agents collaborate while executinga plan.
Once this limitation is overcome, their approach could offer us a route forformalizing the mental states of the collaborating agents in our model and for provingthat our acceptance and goal adoption rules follow from such states.8.
ConclusionWe have presented a computational model of how a conversational participant col-laborates in making and understanding a referring expression, based on the view thatlanguage is goal-oriented behavior.
This has allowed us to do the following.
First,we have accounted for the tasks of building a referring expression and identifying itsreferent by using plan construction and plan inference.
Second, we have accountedfor the conversational moves that participants make during the acceptance process byusing meta-actions.
Third, we have accounted for collaborative activity by proposingthat agents are in a certain mental state that includes a goal, a plan that they arecurrently considering, and intentions.
This mental state sanctions the acceptance ofclarification plans, and sanctions the adoption of goals to clarify.
Although our workhas focused on referring expressions, we feel that it is relevant o collaboration ingeneral and to how agents contribute to discourse.This paper is based on the model of collaboration proposed by Clark and Wilkes-Gibbs (1986).
Their model makes two strong claims about how agents collaborate.First, it minimizes the distinction between the roles of the person who initiates thereferring expression and the person who is trying to identify it.
Both have the samemoves available to them, for either can judge the description and either can refashionit.
This allows both participants to contribute without being controlled or impededby the other.
Second, their model gives special status to the role of the current refer-ring expression (current plan): participants judge and refashion the current referringexpression directly, rather than recursively modifying modifications (e.g.
Litman andAllen 1987; Chu-Carroll and Carberry 1994) or incrementally adding to the currentplan with each accepted proposal (e.g.
Traum and Hinkelman 1992; Sidner 1992).
Inour work, we have taken Clark and Wilkes-Gibbs's descriptive model and recast itinto a computational one, thus demonstrating the computational feasibility of theirwork and its compatibility with current practices in artificial intelligence.There are many ways that this research could be extended.
Perhaps the most ob-vious would be to extend the planning component of our model.
First, our coverageof referring expressions could be extended to handle references to objects in focusand to descriptions that include a plan of physical actions for identifying the referent.Second, the treatment of clarifications could be improved; specifically, how plan fail-ures are reasoned about, how plan failures affect the agent's beliefs, and how thesefailures are repaired.
Third, this research needs to be integrated into a more completeplan-based approach to language, and needs to be extended so as to handle moregeneral discourse plan failures (McRoy and Hirst 1993; McRoy and Hirst 1995; Hor-ton and Hirst 1991; Heeman 1993; Edmonds 1994; Hirst et al 1994).
A benchmark forsuch future work could be dialog (8.1) below, from the London-Lund corpus (Svartvikand Quirk 1980, S.2.4a:1-8, which is the basis of the example used in Section 6.
Thisdialog shows how collaboration on a referring expression can be embedded in other379Computational Linguistics Volume 21, Number 3activities, how agents can return back to a collaborative activity, and even how agentscan take advantage of a mistaken referent.
(8.1) A'- 1 What's that weird creature over there?B: 2 In the corner?_~: 3 affirmative noiseB."
4 It's just a fern plant.A: 5 No, the one to the left of it.B-" 6 That's the television aerial.
It pulls out.A second avenue for future work is to further investigate collaborative behaviorand protocols for interaction.
We need to formalize what it means for agents to becollaborating, in a theory that takes account of rational interaction and the beliefsand knowledge of the participants.
Such a theory would do the following.
First, itwould give a more complete motivation for the processing rules that we used for howagents interact in a collaborative activity.
Second, it would account for why agentswould enter into such a mode of interaction, how it is initiated, how it is carriedforward (especially how agents' beliefs and knowledge influence their actions), andhow it ends.
Third, it would be extendable to other forms of interaction, such asinformation-seeking dialogs.
Fourth, it would specify how collaborative activity couldbe embedded in, or embed, other types of interactions.
By answering these questions,we will not only have a better model to base natural anguage interfaces on, but wewill also have a better understanding of how people interact.AcknowledgmentsThis research was begun at the Departmentof Computer Science, University of Toronto,as part of the first author's M.S.
thesisunder the supervision of the second author.We would like to thank James Allen,Hector Levesque, and the referees atComputational Linguistics for their commentson an earlier version of this paper.
Wewould also like to especially thank JanyceWiebe for her invaluable contribution to thedevelopment of this work.
As well, we aregrateful for comments from, and discussionswith, Diane Horton, Susan McRoy, MassimoPoesio, and David Traum.
Funding at theUniversity of Toronto and the University ofRochester was provided by the NaturalSciences and Engineering Research Councilof Canada, with additional funding atRochester provided by NSF under GrantIRI-90-13160 and ONR/DARPA underGrant N00014-92-J-1512.ReferencesAlien, James; Hendler, James; and Tate,Austin (editors) (1990).
Readings inPlanning.
Morgan Kaufmann Publishers.Allen, James E, and Perrault, C. Raymond(1980).
"Analyzing intention inutterances."
Artificial Intelligence, 15,143-178.
Reprinted in Readings in NaturalLanguage Processing, edited by Barbara J.Grosz, Karen Sparck Jones, and BonnieLynn Weber, 441-458.
Morgan KaufmannPublishers.Appelt, Douglas E. (1985).
"PlanningEnglish referring expressions."
ArtificialIntelligence, 26(1), 1-33.
Reprinted inReadings in Natural Language Processing,edited by Barbara J. Grosz, Karen SparckJones, and Bonnie Lynn Webber, 501-517.Morgan Kaufmann Publishers.Appelt, Douglas, and Kronfeld, Amichai(1987).
"A computational model ofreferring."
In Proceedings, InternationalJoint Conference on Artificial Intelligence(IJCAI '87), 640-647.Austin, J. L. (1962).
How to Do Things withWords.
Oxford University Press.Carletta, Jean (1991).
"Recovering from planfailure using a layered architecture.
"Research Paper 524, Department ofArtificial Intelligence, University ofEdinburgh.Cawsey, Alison (1991).
"Generatinginteractive xplanations."
In Proceedings,National Conference on Artificial Intelligence(AAAI '91), 86-91.Chu-Carroll, Jennifer, and Carberry, Sandra(1994).
"A plan-based model for responsegeneration i collaborative task-oriented380Peter A. Heeman and Graeme Hirst Collaborating on Referring Expressionsdialogues."
In Proceedings, NationalConference on Artifi'cial Intelligence(AAAI "94), 799-805.Clark, Herbert H. (editor) (1992).
Arenas ofLanguage Use.
University of Chicago Pressand CSLI.Clark, Herbert H., and Brennan, S. E.
(1990).
"Grounding in communication."
InPerspectives on Socially Shared Cognition,edited by L.B.
Resnick, J. Levine, andS.D.
Behreno, 127-149.
APA.Clark, Herbert H., and Marshall,Catherine R. (1981).
"Definite referenceand mutual knowledge."
In Elements ofDiscourse Understanding, edited byAravind K. Joshi, Bonnie Lynn Webber,and Ivan Sag, Cambridge UniversityPress, 10--62.Clark, Herbert H., and Schaefer, Edward F.(1989).
"Contributing to discourse.
"Cognitive Science, 13, 259-294.
Reprinted inArenas of Language Use, edited by HerbertH.
Clark, 144-175.
University of ChicagoPress and CSLI.Clark, Herbert H., and Wilkes-Gibbs,Deanna (1986).
"Referring as acollaborative process."
Cognition, 22, 1-39.Reprinted in Arenas of Language Use,edited by Herbert H. Clark, 107-143.University of Chicago Press and CSLI.Cohen, Philip R. (1981).
"The need forreferent identification as a plannedaction."
In Proceedings, International JointConference on Artificial Intelligence(IJCAI '81), 31-36.Cohen, Philip R., and Levesque, Hector J.(1991).
"Confirmation and joint action."
InProceedings, International Joint Conference onArtificial Intelligence (IJCAI "91), 951-958.Cohen, Philip R., and Perrault, C. Raymond(1979).
"Elements of a plan-based theoryof speech acts."
Cognitive Science, 3(3),177-212.
Reprinted in Readings in NaturalLanguage Processing, edited by Barbara J.Grosz, Karen Sparck Jones, and BonnieLynn Webber, 423-440.
MorganKaufmann Publishers.Dale, R. (1989).
"Cooking up referringexpressions."
In Proceedings, 27th AnnualMeeting of the Association for ComputationalLinguistics, 68-75.Edmonds, Philip G. (1994).
"Collaborationon reference to objects that are notmutually known."
In Proceedings, 15thInternational Conference on ComputationalLinguistics (COLING "94), Kyoto,1118-1122.Goodman, Bradley A.
(1985).
"Repairingreference identification failures byrelaxation."
In Proceedings, 23rd AnnualMeeting of the Association for ComputationalLinguistics, 204-217.Grosz, Barbara J., and Sidner, Candace L.(1990).
"Plans for discourse."
In Intentionsin Communication, SDF Benchmark Series,edited by Philip R. Cohen, Jerry Morgan,and Martha E. Pollack, 417-444.
MITPress.Hayes, Philip J.
(1975).
"A representationfor robot plans."
In Proceedings,International Joint Conference on ArtificialIntelligence (IJCAI '75), 181-188.
Reprintedin Readings in Planning, edited by JamesAllen, James Hendler, and Austin Tate,154-161.
Morgan Kaufrnann Publishers.Heeman, Peter A.
(1993).
"Speech actionsand mental states in task-orienteddialogs."
In Working Notes AAAI SpringSymposium on Reasoning about MentalStates: Formal Theories and Applications,Stanford, 68-73.Heeman, Peter Anthony (1991).
"Acomputational model of collaboration onreferring expressions."
Master's thesis,Technical Report CSRI 251, Department ofComputer Science, University of Toronto.Hirst, Graeme; McRoy, Susan; Heeman,Peter; Edmonds, Philip; and Horton,Diane (1994).
"Repairing conversationalmisunderstandings andnon-understandings."
SpeechCommunications, 15(3/4), 213-229.Horrigan, Mary Katherine (1977).
"Modelling simple dialogs."
Master'sthesis, Technical Report 108, Departmentof Computer Science, University ofToronto.Horton, Diane, and Hirst, Graeme (1991).
"Discrepancies in discourse models andmiscommunication in conversation."
InWorking Notes of the AAAI Symposium:Discourse Structure in Natural LanguageUnderstanding and Generation, 31-32.Kautz, Henry A., and Allen, James E (1986).
"Generalized plan recognition."
InProceedings, National Conference on ArtificialIntelligence (AAAI "86), 32-37.Lambert, Lynn, and Carberry, Sandra (1991).
"A tripartite plan-based model fordialogue."
In Proceedings, 29th AnnualMeeting of the Association for ComputationalLinguistics, 47-54.Levelt, Willem J. M. (1989).
Speaking: FromIntention to Articulation.
CambridgeUniversity Press.Litman, Diane J., and Allen, James E (1987).
"A plan recognition model for subdialogsin conversations."
Cognitive Science, 11(2),163-200.Lochbaum, Karen E.; Grosz, Barbara J.; andSidner, Candace L. (1990).
"Models ofplans to support communication: An381Computational Linguistics Volume 21, Number 3initial report."
In Proceedings, NationalConference on Artificial Intelligence(AAAI '90), 485-490.McRoy, Susan, and Hirst, Graeme (1993).
"Abductive xplanations of dialoguemisunderstanding."
In Proceedings, 6thConference ofthe European Chapter of theAssociation for Computational Linguistics.Utrecht, Netherlands, April 1993, 277-286.McRoy, Susan, and Hirst, Graeme (1995).
"The repair of speech actmisunderstandings by abductiveinference."
Computational Linguistics, 21(4),to appear.Mellish, C. S. (1985).
Computer Interpretationof Natural Language Descriptions, EllisHorwood Series in Artificial Intelligence.Ellis Horwood.Moore, Jahanna D., and Swartout,William R. (1991).
"A reactive approach toexplanation: Taking the user's feedbackinto account."
In Natural LanguageGeneration i  Artificial Intelligence andComputational Linguistics, edited byC4cile L. Paris, William R. Swartout, andWilliam C. Mann, 3-48.
Kluwer AcademicPublishers.Nadathur, Gopalan, and Johi, Aravind K.(1983).
"Mutual beliefs in conversationalsystems: Their role in referringexpressions."
In Proceedings, InternationalJoint Conference on Artificial Intelligence(IJCAI "83), 603~05.Perrault, C. R. (1990).
"An application ofdefault logic to speech act theory."
InIntentions in Communication, SDFBenchmark Series, edited by Philip R.Cohen, Jerry Morgan, and Martha E.Pollack, 161-185.
MIT Press.Perrault, C. Raymond, and Cohen, Philip R.(1981).
"It's for your own good: A note oninaccurate reference."
In Elements ofDiscourse Understanding, edited byAravind K. Joshi, Bonnie Lynn Webber,and Ivan Sag, 217-230.
CambridgeUniversity Press.Pollack, Martha E. (1990).
"Plans as complexmental attitudes."
In Intentions inCommunication, SDF Benchmark Series,edited by Philip R. Cohen, Jerry Morgan,and Martha E. Pollack, 77-103.
MIT Press.Reiter, Ehud (1990).
"The computationalcomplexity of avoiding conversationalimplicature."
In Proceedings, 28th AnnualMeeting of the Association for ComputationalLinguistics, 97-104.Searle, J. R. (1969).
Speech Acts: An Essay inthe Philosophy of Language.
CambridgeUniversity Press.Searle, John R. (1990).
"Collective intentionsand actions."
In Intentions inCommunication, SDF Benchmark Series,edited by Philip R. Cohen, Jerry Morgan,and Martha E. Pollack, 401-415.
MITPress.Sidner, Candace L. (1985).
"Plan parsing forintended response recognition indiscourse."
Computational Intelligence, 1(1),1-10.Sidner, Candace L. (1994).
"An ArtificialDiscourse Language for CollaborativeNegotiation."
In Proceedings ofthe NationalConference on Artificial Intelligence(AAAI'94).
814-819.Svartvik, J., and Quirk, R. (1980).
A Corpus ofEnglish Conversation.
Lund Studies inEnglish, 56.
C. W. K. Gleerup.Traum, David R. (1991).
"Towards acomputational theory of grounding innatural anguage conversation."
TechnicalReport 401, Department of ComputerScience, University of Rochester,Rochester, New York.Traum, David R., and Hinkelman,Elizabeth A.
(1992).
"Conversation acts intask-oriented spoken dialogue."
Specialissue on non-literal language,Computational Intelligence, 8(3), 575-599.Vilain, Marc (1990).
"Getting serious aboutparsing plans: A grammatical nalysis ofplan recognition."
In Proceedings, NationalConference on Artificial Intelligence(AAAI "90), 190-197.Walker, Marilyn, and Whittaker, Steve(1990).
"Mixed initiative in dialogue: Aninvestigation i to discoursesegmentation."
In Proceedings, 28th AnnualMeeting of the Association for ComputationalLinguistics, 70-78.Webber, Bonnie Lynn (1983).
"So what canwe talk about now?"
In ComputationalModels of Discourse, edited by MichaelBrady and Robert C. Berwick, 331-371.MIT Press.
Reprinted in Readings inNatural Language Processing, edited byBarbara J. Grosz, Karen Sparck Jones, andBonnie Lynn Webber, 395-414.
MorganKaufmann Publishers.Wilensky, Robert (1981).
"A model forplanning in complex situations."
Cognitionand Brain Theory, 4.
Reprinted in Readingsin Planning, edited by James Allen, JamesHendler, and Austin Tate, 263-374.Morgan Kaufmann Publishers.Wilkens, David E. (1985).
"Recovering fromexecution errors in SIPE."
ComputationalIntelligence, 1 33-45.382
