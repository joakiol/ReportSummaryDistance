Large-scale Word Alignment Using Soft Dependency CohesionConstraintsZhiguo Wang and Chengqing ZongNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Sciences{zgwang, cqzong}@nlpr.ia.ac.cnAbstractDependency cohesion refers to theobservation that phrases dominated bydisjoint dependency subtrees in the sourcelanguage generally do not overlap in thetarget language.
It has been verified to be auseful constraint for word alignment.However, previous work either treats thisas a hard constraint or uses it as a feature indiscriminative models, which is ineffectivefor large-scale tasks.
In this paper, we takedependency cohesion as a soft constraint,and integrate it into a generative model forlarge-scale word alignment experiments.We also propose an approximate EMalgorithm and a Gibbs sampling algorithmto estimate model parameters in anunsupervised manner.
Experiments onlarge-scale Chinese-English translationtasks demonstrate that our model achievesimprovements in both alignment qualityand translation quality.1 IntroductionWord alignment is the task of identifying wordcorrespondences between parallel sentence pairs.Word alignment has become a vital component ofstatistical machine translation (SMT) systems,since it is required by almost all state-of-the-artSMT systems for the purpose of extracting phrasetables or even syntactic transformation rules(Koehn et al 2007; Galley et al 2004).During the past two decades, generative wordalignment models such as the IBM Models (Brownet al 1993) and the HMM model (Vogel et al1996) have been widely used, primarily becausethey are trained on bilingual sentences in anunsupervised manner and the implementation isfreely available in the GIZA++ toolkit (Och andNey, 2003).
However, the word alignment qualityof generative models is still far from satisfactoryfor SMT systems.
In recent years, discriminativealignment models incorporating linguisticallymotivated features have become increasinglypopular (Moore, 2005; Taskar et al 2005; Riesaand Marcu, 2010; Saers et al 2010; Riesa et al2011).
These models are usually trained withmanually annotated parallel data.
However, whenmoving to a new language pair, large amount ofhand-aligned data are usually unavailable andexpensive to create.A more practical way to improve large-scaleword alignment quality is to introduce syntacticknowledge into a generative model and train themodel in an unsupervised manner (Wu, 1997;Yamada and Knight, 2001; Lopez and Resnik,2005; DeNero and Klein, 2007; Pauls et al 2010).In this paper, we take dependency cohesion (Fox,2002) into account, which assumes phrasesdominated by disjoint dependency subtrees tendnot to overlap after translation.
Instead of treatingdependency cohesion as a hard constraint (Lin andCherry, 2003) or using it as a feature indiscriminative models (Cherry and Lin, 2006b), wetreat dependency cohesion as a distortionconstraint, and integrate it into a modified HMMword alignment model to softly influence theprobabilities of alignment candidates.
We alsopropose an approximate EM algorithm and anexplicit Gibbs sampling algorithm to train themodel in an unsupervised manner.
Experiments ona large-scale Chinese-English translation taskdemonstrate that our model achievesimprovements in both word alignment quality andmachine translation quality.The remainder of this paper is organized asfollows: Section 2 introduces dependency cohesion291Transactions of the Association for Computational Linguistics, 1 (2013) 291?300.
Action Editor: Chris Callison-Burch.Submitted 5/2013; Published 7/2013.
c?2013 Association for Computational Linguistics.constraint for word alignment.
Section 3 presentsour generative model for word alignment usingdependency cohesion constraint.
Section 4describes algorithms for parameter estimation.
Wediscuss and analyze the experiments in Section 5.Section 6 gives the related work.
Finally, weconclude this paper and mention future work inSection 7.2 Dependency Cohesion Constraint forWord AlignmentGiven a source (foreign) sentence ?1?
= ?1, ?2, ?
, ?
?and a target (English) sentence ?1?
= ?1, ?2, ?
, ??
,the alignment ?
between ?1?and ?1?
is defined as asubset of the Cartesian product of word positions:?
?
{(?, ?
): ?
= 1,?
, ?
; ?
= 1,?
, ?
}When given the source side dependency tree ?, wecan project dependency subtrees in ?
onto thetarget sentence through the alignment ?
.Dependency cohesion assumes projection spans ofdisjoint subtrees tend not to overlap.
Let ?(??)
bethe subtree of ?
rooted at ?
?, we define two kindsof projection span for the node ??
: subtree span andhead span.
The subtree span is the projection spanof the total subtree ?(??
), while the head span isthe projection span of the node ??
itself.
FollowingFox (2002) and Lin and Cherry (2003), weconsider two types of dependency cohesion: head-modifier cohesion and modifier-modifier cohesion.Head-modifier cohesion is defined as the subtreespan of a node does not overlap with the head spanof its head (parent) node, while modifier-modifiercohesion is defined as subtree spans of two nodesunder the same head node do not overlap eachother.
We call a situation where cohesion is notmaintained crossing.Using the dependency tree in Figure 1 as anexample, given the correct alignment ?R?, thesubtree span of ??/have?
is [8, 14] , and the headspan of its head node ??
?/one of?
is [3, 4].
Theydo not overlap each other, so the head-modifiercohesion is maintained.
Similarly, the subtree spanof ???/few?
is [6, 6], and it does not overlap thesubtree span of  ?
?/have?, so a modifier-modifiercohesion is maintained.
However, when ?R?
isreplaced with the incorrect alignment ?W?, thesubtree span of ??/have?
becomes [3, 14], and itoverlaps the head span of its head ??
?/one of?,so a head-modifier crossing occurs.
Meanwhile,the subtree spans of the two nodes ??/have?
and???
/few?
overlap each other, so a modifier-modifier crossing occurs.Fox (2002) showed that dependency cohesion isgenerally maintained between English and French.To test how well this assumption holds betweenChinese and English, we measure the dependencycohesion between the two languages with amanually annotated bilingual Chinese-English dataset of 502 sentence pairs 1 .
We use the head-modifier cohesion percentage (HCP) and themodifier-modifier cohesion percentage (MCP) tomeasure the degree of cohesion in the corpus.
HCP(or MCP) is used for measuring how many head-modifier (or modifier-modifier) pairs are actuallycohesive.
Table 1 lists the relative percentages inboth Chinese-to-English (ch-en, using Chinese sidedependency trees) and English-to-Chinese (en-ch,using English side dependency trees) directions.As we see from Table 1, dependency cohesion is1 The data set is the development set used in Section 5.???
?
???
?????????
?AustraliaisoneofthefewcountriesthathavediplomaticrelationswithNorthKorea.123456789101112131415Figure 1: A Chinese-English sentence pairincluding the word alignments and the Chineseside dependency tree.
The Chinese and Englishwords are listed horizontally and vertically,respectively.
The black grids are gold-standardalignments.
For the Chinese word ?
?/have?,we give two alignment positions, where ?R?
isthe correct alignment and ?W?
is the incorrectalignment.292generally maintained between Chinese and English.So dependency cohesion would be helpful forword alignment between Chinese and English.However, there are still a number of crossings.
Ifwe restrict alignment space with a hard cohesionconstraint, the correct alignments that result incrossings will be ruled out directly.
In the nextsection, we describe an approach to integratingdependency cohesion constraint into a generativemodel to softly influence the probabilities ofalignment candidates.
We show that our newapproach addresses the shortcomings of usingdependency cohesion as a hard constraint.3 A Generative Word Alignment Modelwith Dependency Cohesion ConstraintThe most influential generative word alignmentmodels are the IBM Models 1-5 and the HMMmodel (Brown et al 1993; Vogel et al 1996; Ochand Ney, 2003).
These models can be classifiedinto sequence-based models (IBM Models 1, 2 andHMM) and fertility-based models (IBM Models 3,4 and 5).
The sequence-based model is easier toimplement, and recent experiments have shownthat appropriately modified sequence-based modelcan produce comparable performance withfertility-based models (Lopez and Resnik, 2005;Liang et al 2006; DeNero and Klein, 2007; Zhaoand Gildea, 2010; Bansal et al 2011).
So we builta generative word alignment model withdependency cohesion constraint based on thesequence-based model.3.1 The Sequence-based Alignment ModelAccording to Brown et al(1993) and Och and Ney(2003), the sequence-based model is built as anoisy channel model, where the source sentence ?1?and the alignment ?1?
are generated conditioning onthe target sentence ?1?
.
The model assumes eachsource word is assigned to exactly one target word,and defines an asymmetric alignment for thesentence pair as ?1?
= ?1, ?2, ?
, ??
, ?
, ?
?, where each??
?
[0, ?]
is an alignment from the source position jto the target position ??
, ??
= 0  means ??
is notaligned with any target words.
The sequence-basedmodel divides alignment procedure into two stages(distortion and translation) and factors as:?
(?1?, ?1?|?1? )
= ?
??(??|??
?1, ?)??(??|???)?
?=1       (1)where ??
is the distortion model and ??
is thetranslation model.
IBM Models 1, 2 and the HMMmodel all assume the same translation model??(??|???)
.
However, they use three differentdistortion models.
IBM Model 1 assumes auniform distortion probability 1/(I+1), IBM Model2 assumes ??(??|?)
that depends on word position jand HMM model assumes ??(??|??
?1, ?)
thatdepends on the previous alignment ???1.
Recently,tree distance models (Lopez and Resnik, 2005;DeNero and Klein, 2007) formulate the distortionmodel as ??(??|??
?1, ?)
, where the distancebetween ??
and ??
?1  are calculated by walkingthrough the phrase (or dependency) tree T.3.2 Proposed ModelTo integrate dependency cohesion constraint into agenerative model, we refine the sequence-basedmodel in two ways with the help of the source sidedependency tree ?
?.First, we design a new word alignment order.
Inthe sequence-based model, source words arealigned from left to right by taking source sentenceas a linear sequence.
However, to applydependency cohesion constraint, the subtree spanof a head node is computed based on thealignments of its children, so children must bealigned before the head node.
Riesa and Marcu(2010) propose a hierarchical search procedure totraverse all nodes in a phrase structure tree.Similarly, we define a bottom-up topological order(BUT-order) to traverse all words in the sourceside dependency tree ??
.
In the BUT-order, treenodes are aligned bottom-up with ??
as a backbone.For all children under the same head node, leftchildren are aligned from right to left, and thenright children are aligned from left to right.
Forexample, the BUT-order for the followingdependency tree is  ?C B E F D A H G?.A HGFEDCBch-en en-chHCP MCP HCP MCP88.43 95.82 81.53 91.62Table 1: Cohesion percentages (%) of a manuallyannotated data set between Chinese and English.293For the sake of clarity, we define a function tomap all nodes in ??
into their BUT-order, andnotate it as BUT(??)
= ?1, ?2, ?
, ??
, ?
, ??
, where ?
?means the j-th node in BUT-order is the ?
?-th wordin the original source sentence.
We arrangealignment sequence ?1?
according the BUT-orderand notate it as ?[1,?]
= ?
?1 , ?
, ???
, ?
, ???
, where???
is the aligned position for a node ???.
We alsonotate the sub-sequence ???
, ?
, ??
?as ?[?,?
].Second, we keep the same translation model asthe sequence-based model and integrate thedependency cohesion constraints into the distortionmodel.
The main idea is to influence the distortionprocedure with the dependency cohesionconstraints.
Assume node ??
and node ??
are ahead-modifier pair in ?
?, where ??
is the head and??
is the modifier.
The head-modifier cohesionrelationship between them is notated as ??,?
?{???????
?, ????????}
.
When the head-modifiercohesion is maintained ??,?
= ???????
?, otherwise??,?
= ????????
.
We represent the set of head-modifier cohesion relationships for all the head-modifier pairs in ??
as:?
= {??,?
| ?
?
[1, ?
], ?
?
[1, ?
], ?
?
?,??
and ??
are a head-modifier pair in ??
}The set of head-modifier cohesion relationships forall the head-modifier pairs taking ??
as the headnode can be represented as:??
= {??,?
| ?
?
[1, ?
], ?
?
?,??
and ??
are a head-modifier pair in ??
}Obviously, ?
= ?
???
?=0 .Similarly, we assume node ??
and node ??
are amodifier-modifier pair in ??
.
To avoid repetition,we assume ??
is the node sitting at the positionafter  ??
in BUT-order and call ??
as the higher-order node of the pair.
The modifier-modifiercohesion relationship between them is notated as??,?
?
{???????
?, ????????}
.
When the modifier-modifier cohesion is maintained ??,?
= ????????
,otherwise ??,?
= ????????.
We represent the set ofmodifier-modifier cohesion relationships for all themodifier-modifier pairs in ??
as:?
= {??,?
| ?
?
[1, ?
], ?
?
[1, ?
], ?
?
?,??
and ??
are a modifier-modifier pair in ??
}The set of modifier-modifier cohesionrelationships for all the modifier-modifier pairstaking ??
as the higher-order node can berepresented as:??
= {??,?
| ?
?
[1, ?
], ?
?
?,??
and ??
are a modifier-modifier pair in ??
}Obviously, ?
= ?
???
?=0 .With the above notations, we formulate thedistortion probability for a node ???
as??
(???
, ???
,???|?[1,?
?1]).According to Eq.
(1) and the two improvements,we formulated our model as:?
(?1?, ?[1,?]|?1?
, ??)
= ?(?[1,?
], ?,?, ?1?
, |?1?
, ??)?
?
??
(???
, ???
,???|?[1,?
?1]) ??
(???|????)??????(??
)(2)Here, we use the approximation symbol,because the right hand side is not guaranteed tobe normalized.
In practice, we only computeratios of these terms, so it is not actually aproblem.
Such model is called deficient (Brownet al 1993), and many successful unsupervisedmodels are deficient, e.g., IBM model 3 andIBM model 4.3.3 Dependency Cohesive Distortion ModelWe assume the distortion procedure is influencedby three factors: words distance, head-modifiercohesion and modifier-modifier cohesion.Therefore, we further decompose the distortionmodel ??
into three terms as follows:??
(???
, ???
,???|?[1,?
?1])= ?
(???|?[1,?
?1]) ?
(???|?[1,?])
?
(???|?[1,?
], ???)?
???
(???|???
?1 , ?)
???
(???|?[1,?])
???
(???|?[1,?
])(3)where ???
is the words distance term, ???
is  thehead-modifier cohesion term and ???
is themodifier-modifier cohesion term.The word distance term ???
has been verified tobe very useful in the HMM alignment model.However, in our model, the word distance iscalculated based on the previous node in BUT-order rather than the previous word in the originalsentence.
We follow the HMM word alignmentmodel (Vogel et al 1996) and parameterize ???
interms of the jump width:???(?|?
?, ?)
=?(????)?
?(??????)???
(4)where ?(?)
is the count of jump width.294The head-modifier cohesion term ???
is used topenalize the distortion probability according torelationships between the head node and itschildren (modifiers).
Therefore, we define ???
asthe product of probabilities for all head-modifierpairs taking ???
as head node:???
(???|?[1,?])
= ?
??
(???,?|?
?, ???
?, ???)???,?????
(5)where ???,?
?
{???????
?, ????????}
is the head-modifier cohesion relationship between ???
andone of its child ??
,  ??
is the correspondingprobability, ???
?and ???
are the aligned words for???
and ?
?.Similarly, the modifier-modifier cohesion term???
is used to penalize the distortion probabilityaccording to relationships between ???
and itssiblings.
Therefore, we define  ???
as the productof probabilities for all the modifier-modifier pairstaking ???
as the higher-order node:???
(???|?[1,?])
= ?
??
(???,?|?
?, ???
?, ???)???,?????
(6)where ???,?
?
{???????
?, ????????}
is the modifier-modifier cohesion relationship between  ???
andone of its sibling ??
, ??
is the correspondingprobability, ???
?and ???
are the aligned words for???
and ?
?.Both  ??
and ??
in Eq.
(5) and Eq.
(6) areconditioned on three words, which would makethem very sparse.
To cope with this problem, weuse the word clustering toolkit, mkcls (Och et al1999), to cluster all words into 50 classes, andreplace the three words with their classes.4 Parameter EstimationTo align sentence pairs with the model in Eq.
(2),we have to estimate some parameters: ?
?, ??
?, ?
?and ??
.
The traditional approach for sequence-based models uses Expectation Maximization (EM)algorithm to estimate parameters.
However, in ourmodel, it is hard to find an efficient way to sumover all the possible alignments, which is requiredin the E-step of EM algorithm.
Therefore, wepropose an approximate EM algorithm and a Gibbssampling algorithm for parameter estimation.4.1 Approximate EM AlgorithmThe approximate EM algorithm is similar to thetraining algorithm for fertility-based alignmentmodels (Och and Ney, 2003).
The main idea is toenumerate only a small subset of good alignmentsin the E-step, then collect expectation counts andestimate parameters among the small subset in M-step.
Following with Och and Ney (2003), weemploy neighbor alignments of the Viterbialignment as the small subset.
Neighboralignments are obtained by performing one swapor move operation over the Viterbi alignment.Obtaining the Viterbi alignment itself is not soeasy for our model.
Therefore, we take the Viterbialignment of the sequence-based model (HMMmodel) as the starting point, and iterate the hill-climbing algorithm (Brown et al 1993) manytimes to get the best alignment greedily.
In eachiteration, we find the best alignment with Eq.
(2)among neighbor alignments of the initial point, andthen make the best alignment as the initial point forthe next iteration.
The algorithm iterates until noupdate could be made.4.2 Gibbs Sampling AlgorithmGibbs sampling is another effective algorithm forunsupervised learning problems.
As is described inthe literatures (Johnson et al 2007; Gao andJohnson, 2008), there are two types of Gibbssamplers: explicit and collapsed.
An explicitsampler represents and samples the modelparameters in addition to the word alignments,while in a collapsed sampler the parameters areintegrated out and only alignments are sampled.Mermer and Sara?lar (2011) proposed a collapsedsampler for IBM Model 1.
However, their samplerupdates parameters constantly and thus cannot runefficiently on large-scale tasks.
Instead, we takeadvantage of explicit Gibbs sampling to make ahighly parallelizable sampler.
Our Gibbs sampleris similar to the MCMC algorithm in Zhao andGildea (2010), but we assume Dirichlet priorswhen sampling model parameters and take adifferent sampling approach based on the sourceside dependency tree.Our sampler performs a sequence of consecutiveiterations.
Each iteration consists of two samplingsteps.
The first step samples the aligned positionfor each dependency node according to the BUT-order.
Concretely, when sampling the aligned295position ???
(?+1)for node ???
on iteration ?+1,  thealigned positions for ?[1,?
?1] are fixed on the newsampling results ?[1,?
?1](?+1)on iteration ?+1, and thealigned positions for ?[?+1,?]
are fixed on the oldsampling results ?[?+1,?](?
)on iteration ?
.
Therefore,we sample the aligned position ???
(?+1)as follows:???
(?+1)~   ?
(???|?[1,?
?1](?+1), ?[?+1,?](?
), ?1?, ?1?)=?
(?1?, ??????|?1?
)?
?
(?1?
, ??????|?1?
)????{0,1,?,?
}(7)where ?????
?= ?[1,??1](?+1)?
???
?
?[?+1,?](?
), the numeratoris the probability of aligning ???
with ????
(thealignments for other nodes are fixed at ?[1,??1](?+1)and?[?+1,?](?))
calculated with Eq.
(2), and thedenominator is the summation of the probabilitiesof aligning ???
with each target word.
The secondstep of our sampler calculates parameters ?
?, ???,??
and ??
using their counts, where all thesecounts can be easily collected during the firstsampling step.
Because all these parameters followmultinomial distributions, we consider Dirichletpriors for them, which would greatly simplify theinference procedure.In the first sampling step, all the sentence pairsare processed independently.
So we can make thisstep parallel and process all the sentence pairsefficiently with multi-threads.
When using theGibbs sampler for decoding, we just ignore thesecond sampling step and iterate the first samplingstep many times.5 ExperimentsWe performed a series of experiments to evaluateour model.
All the experiments are conducted onthe Chinese-English language pair.
We employtwo training sets: FBIS and LARGE.
The size andsource corpus of these training sets are listed inTable 2.
We will use the smaller training set FBISto evaluate the characters of our model and use theLARGE training set to evaluate whether our modelis adaptable for large-scale task.
For wordalignment quality evaluation, we take the hand-aligned data sets from SSMT20072, which contains2 http://nlp.ict.ac.cn/guidelines/guidelines-2007-SSMT(English).doc505 sentence pairs in the testing set and 502sentence pairs in the development set.
FollowingOch and Ney (2003), we evaluate word alignmentquality with the alignment error rate (AER), wherelower AER is better.Because our model takes dependency trees asinput, we parse both sides of the two training sets,the development set and the testing set withBerkeley parser (Petrov et al 2006), and thenconvert the generated phrase trees into dependencytrees according to Wang and Zong (2010; 2011).Our model is an asymmetric model, so we performword alignment in both forward (Chinese?English)and reverse (English?Chinese) directions.5.1 Effectiveness of Cohesion ConstraintsIn Eq.
(3), the distortion probability ??
isdecomposed into three terms: ???
, ???
and ???
.To study whether cohesion constraints are effectivefor word alignment, we construct four sub-modelsas follows:(1) wd: ??
= ???
;(2) wd-hc: ??
= ???
?
???
;(3) wd-mc: ??
= ???
?
???
;(4) wd-hc-mc: ??
= ???
?
???
?
??
?.We train these four models with the approximateEM and the Gibbs sampling algorithms on theFBIS training set.
For approximate EM algorithm,we first train a HMM model (with 5 iterations ofIBM model 1 and 5 iterations of HMM model),then train these four sub-models with 10 iterationsof the approximate EM algorithm.
For Gibbssampling, we choose symmetric Dirichlet priorsidentically with all hyper-parameters equals 0.0001to obtain a sparse Dirichlet prior.
Then, we makethe alignments produced by the HMM model as theinitial points, and train these sub-models with 20iterations of the Gibbs sampling.AERs on the development set are listed in Table3.
We can easily find: 1) when employing thehead-modifier cohesion constraint, the wd-hcmodel yields better AERs than the wd model; 2)Train Set Source Corpus # WordsFBIS FBIS newswire data Ch: 7.1MEn: 9.1MLARGELDC2000T50, LDC2003E14,LDC2003E07, LDC2004T07,LDC2005T06, LDC2002L27,LDC2005T10, LDC2005T34Ch: 27.6MEn: 31.8MTable 2: The size and the source corpus of the twotraining sets.296when employing the modifier-modifier cohesionconstraint, the wd-mc model also yields betterAERs than the wd model; and 3) when employingboth head-modifier cohesion constraint andmodifier-modifier cohesion constraint together, thewd-hc-mc model yields the best AERs among thefour sub-models.
So both head-modifier cohesionconstraint and modifier-modifier cohesionconstraint are helpful for word alignment.
Table 3also shows that the approximate EM algorithmyields better AERs in the forward direction thanreverse direction, while the Gibbs samplingalgorithm yields close AERs in both directions.5.2 Comparison with State-of-the-Art ModelsTo show the effectiveness of our model, wecompare our model with some of the state-of-the-art models.
All the systems are listed as follows:1) IBM4: The fertility-based model (IBM model 4)which is implemented in GIZA++ toolkit.
Thetraining scheme is 5 iterations of IBM model 1,5 iterations of the HMM model and 10iterations of IBM model 4.2) IBM4-L0: A modification to the GIZA++toolkit which extends IBM models with ?0 -norm (Vaswani et al 2012).
The trainingscheme is the same as IBM4.3) IBM4-Prior: A modification to the GIZA++toolkit which extends the translation model ofIBM models with Dirichlet priors (Riley andGildea, 2012).
The training scheme is the sameas IBM4.4) Agree-HMM: The HMM alignment model byjointly training the forward and reverse models(Liang et al 2006), which is implemented inthe BerkeleyAligner.
The training scheme is 5iterations of jointly training IBM model 1 and 5iterations of jointly training HMM model.5) Tree-Distance: The tree distance alignmentmodel proposed in DeNero and Klein (2007),which is implemented in the BerkeleyAligner.The training scheme is 5 iterations of jointlytraining IBM model 1 and 5 iterations of jointlytraining the tree distance model.6) Hard-Cohesion: The implemented ?CohesionChecking Algorithm?
(Lin and Cherry, 2003)which takes dependency cohesion as a hardconstraint during beam search word alignmentdecoding.
We use the model trained by theAgree-HMM system to estimate alignmentcandidates.We also build two systems for our softdependency cohesion model:7) Soft-Cohesion-EM: the wd-hc-mc sub-modeltrained with the approximate EM algorithm asdescribed in sub-section 5.1.8) Soft-Cohesion-Gibbs: the wd-hc-mc sub-modeltrained with the Gibbs sampling algorithm asdescribed in sub-section 5.1.We train all these systems on the FBIS trainingset, and test them on the testing set.
We alsocombine the forward and reverse alignments withthe grow-diag-final-and (GDFA) heuristic (Koehnet al 2007).
All AERs are listed in Table 4.
Wefind our soft cohesion systems produce betterAERs than the Hard-Cohesion system as well asthe other systems.
Table 5 gives the head-modifiercohesion percentage (HCP) and the modifier-modifier cohesion percentage (MCP) of eachsystem.
We find HCPs and MCPs of our softcohesion systems are much closer to the gold-standard alignments.To evaluate whether our model is adaptable forlarge-scale task, we retrained these systems usingthe LARGE training set.
AERs on the testing setare listed in Table3 6.
Compared with Table 4, we3 Tree-Distance system requires too much memory to run onour server when using the LARGE data set, so we can?t get theresult.forward reverse GDFAIBM4 42.90 42.81 44.32IBM4-L0 42.59 41.04 43.19IBM4-Prior 41.94 40.46 42.44Agree-HMM 38.03 37.91 41.01Tree-Distance 34.21 37.22 38.42Hard-Cohesion 37.32 38.92 38.92Soft-Cohesion-EM 33.65 34.74 35.85Soft-Cohesion-Gibbs 34.45 33.72 34.46Table 4: AERs on the testing set (trained on theFBIS data set).EM Gibbsforward reverse forward reversewd 26.12  28.66  27.09  26.40wd-hc 24.67  25.86  26.24  24.39wd-mc 24.49  26.53  25.51  25.40wd-hc-mc 23.63  25.17  24.65  24.33Table 3: AERs on the development set (trainedon the FBIS data set).297find all the systems yield better performance whenusing more training data.
Our soft cohesionsystems still produce better AERs than othersystems, suggesting that our soft cohesion model isvery effective for large-scale word alignment tasks.5.3 Machine Translation Quality ComparisonWe then evaluate the effect of word alignment onmachine translation quality using the phrase-basedtranslation system Moses (Koehn et al 2007).
Wetake NIST MT03 test data as the development set,NIST MT05 test data as the testing set.
We train a5-gram language model with the Xinhua portion ofEnglish Gigaword corpus and the English side ofthe training set using the SRILM Toolkit (Stolcke,2002).We train machine translation models usingGDFA alignments of each system.
BLEU scoreson NIST MT05 are listed in Table 7, where BLEUscores are calculated using lowercased andtokenized data (Papineni et al 2002).
Althoughthe IBM4-L0, Agree-HMM, Tree-Distance andHard-Cohesion systems improve word alignmentthan IBM4, they fail to outperform the IBM4system on machine translation.
The BLEU score ofour Soft-Cohesion-EM system is better than theIBM4 system when using the FBIS training set, butworse when using the LARGE training set.
OurSoft-Cohesion-Gibbs system produces the bestBLEU score when using both training sets.
Wealso performed a statistical significance test usingbootstrap resampling with 1000 samples (Koehn,2004; Zhang et al 2004).
Experimental resultsshow the Soft-Cohesion-Gibbs system issignificantly better (p<0.05) than the IBM4 system.The IBM4-Prior system slightly outperforms IBM4,but it?s not significant.6 Related WorkThere have been many proposals of integratingsyntactic knowledge into generative alignmentmodels.
Wu (1997) proposed the inversiontransduction grammar (ITG) to model wordalignment as synchronous parsing for a sentencepair.
Yamada and Knight (2001) representedtranslation as a sequence of re-ordering operationsover child nodes of a syntactic tree.
Gildea (2003)introduced a ?loosely?
tree-based alignmenttechnique, which allows alignments to violatesyntactic constraints by incurring a cost inprobability.
Pauls et al(2010) gave a new instanceof the ITG formalism, in which one side of thesynchronous derivation is constrained by thesyntactic tree.Fox (2002) measured syntactic cohesion in goldstandard alignments and showed syntacticcohesion is generally maintained between Englishand French.
She also compared three variantsyntactic representations (phrase tree, verb phraseflattening tree and dependency tree), and found thedependency tree produced the highest degree ofcohesion.
So Cherry and Lin (2003; 2006a) useddependency cohesion as a hard constraint torestrict the alignment space, where all potentialalignments violating cohesion constraint are ruledforward reverseHCP MCP HCP MCPIBM4 60.53 63.94 56.15 64.80IBM4-L0 60.57 62.53 66.49 65.68IBM4-Prior 66.48 74.65 67.19 72.32Agree-HMM 75.52 66.61 73.88 66.07Tree-Distance 81.37 74.69 78.00 71.73Hard-Cohesion 98.70 97.43 98.25 97.84Soft-Cohesion-EM 85.21 81.96 82.96 81.36Soft-Cohesion-Gibbs 88.74 85.55 87.81 84.83gold-standard 88.43 95.82 81.53 91.62Table 5: HCPs and MCPs on the developmentset.FBIS LARGEIBM4 30.7 33.1IBM4-L0 30.4 32.3IBM4-Prior 30.9 33.2Agree-HMM 27.2 30.1Tree-Distance 28.2 N/AHard-Cohesion 30.4 32.2Soft-Cohesion-EM 30.9 33.1Soft-Cohesion-Gibbs   31.6*   33.9*Table 7: BLEU scores, where * indicatessignificantly better than IBM4 (p<0.05).forward reverse GDFAIBM4 37.45 39.18 40.52IBM4-L0 38.17 38.88 39.82IBM4-Prior 35.86 36.71 37.08Agree-HMM 35.58 35.73 39.10Hard-Cohesion 35.04 37.59 37.63Soft-Cohesion-EM 30.93 32.67 33.65Soft-Cohesion-Gibbs 32.07 32.68 32.28Table 6: AERs on the testing set (trained on theLARGE data set).298out directly.
Although the alignment quality isimproved, they ignored situations where a small setof correct alignments can violate cohesion.
Toaddress this limitation, Cherry and Lin (2006b)proposed a soft constraint approach, which tookdependency cohesion as a feature of adiscriminative model, and verified that the softconstraint works better than the hard constraint.However, the training procedure is very time-consuming, and they trained the model with only100 hand-annotated sentence pairs.
Therefore, theirmethod is not suitable for large-scale tasks.
In thispaper, we also use dependency cohesion as a softconstraint.
But, unlike Cherry and Lin (2006b), weintegrate the soft dependency cohesion constraintinto a generative model that is more suitable forlarge-scale word alignment tasks.7 Conclusion and Future WorkWe described a generative model for wordalignment that uses dependency cohesion as a softconstraint.
We proposed an approximate EMalgorithm and an explicit Gibbs samplingalgorithm for parameter estimation in anunsupervised manner.
Experimental resultsperformed on a large-scale data set show that ourmodel improves word alignment quality as well asmachine translation quality.
Our experimentalresults also indicate that the soft constraintapproach is much better than the hard constraintapproach.It is possible that our word alignment model canbe improved further.
First, we generated wordalignments in both forward and reverse directionsseparately, but it might be helpful to usedependency trees of the two sides simultaneously.Second, we only used the one-best automaticallygenerated dependency trees in the model.
However,errors are inevitable in those trees, so we willinvestigate how to use N-best dependency trees ordependency forests (Hayashi et al 2011) to see ifthey can improve our model.AcknowledgmentsWe would like to thank Nianwen Xue forinsightful discussions on writing this article.
Weare grateful to anonymous reviewers for manyhelpful suggestions that helped improve the finalversion of this article.
The research work has beenfunded by the Hi-Tech Research and DevelopmentProgram ("863" Program) of China under Grant No.2011AA01A207, 2012AA011101, and2012AA011102 and also supported by the KeyProject of Knowledge Innovation Program ofChinese Academy of Sciences under GrantNo.KGZD-EW-501.
This work is also supported inpart by the DAPRA via contract HR0011-11-C-0145 entitled "Linguistic Resources forMultilingual Processing".ReferencesMohit Bansal, Chris Quirk, and Robert Moore, 2011.Gappy Phrasal Alignment By Agreement.
In Proc.
ofACL 2011.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra and Robert L. Mercer, 1993.
Themathematics of statistical machine translation:Parameter estimation.
Computational Linguistics, 19(2).
pages 263-311.C.
Cherry and D. Lin, 2003.
A probability model toimprove word alignment.
In Proc.
of ACL '03, pages88-95.C.
Cherry and D. Lin, 2006a.
A comparison ofsyntactically motivated word alignment spaces.
InProc.
of EACL '06, pages 145-152.C.
Cherry and D. Lin, 2006b.
Soft syntactic constraintsfor word alignment through discriminative training.In Proc.
of COLING/ACL '06, pages 105-112.John DeNero and Dan Klein, 2007.
Tailoring wordalignments to syntactic machine translation.
In Proc.of ACL '07, pages 17.C.
Dyer, J. Clark, A. Lavie and N.A.
Smith, 2011.Unsupervised word alignment with arbitrary features.In Proc.
of ACL '11, pages 409-419.Heidi J.
Fox, 2002.
Phrasal cohesion and statisticalmachine translation.
In Proc.
of EMNLP '02, pages304-3111.Michel Galley, Mark Hopkins, Kevin Knight, DanielMarcu, 2004.
What's in a translation rule?
In Proc.
ofNAACL '04, pages 344-352.J.
Gao and M. Johnson, 2008.
A comparison ofBayesian estimators for unsupervised HiddenMarkov Model POS taggers.
In Proc.
of EMNLP '08,pages 344-352.Daniel Gildea, 2003.
Loosely Tree-Based Alignment forMachine Translation.
In Proc.
of ACL'03, pages 80-87.299K.
Hayashi, T. Watanabe, M. Asahara and Y.Matsumoto, 2011.
Third-order Variational Rerankingon Packed-Shared Dependency Forests.
In Proc.
ofEMNLP '11.M.
Johnson, T. Griffiths and S. Goldwater, 2007.Bayesian inference for PCFGs via Markov chainMonte Carlo.
In Proc.
of NAACL '07, pages 139-146.Philipp Koehn, 2004.
Statistical significance tests formachine translation evaluation.
In Proc.
ofEMNLP'04.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moranand R. Zens, 2007.
Moses: Open source toolkit forstatistical machine translation.
In Proc.
of ACL '07,Demonstration Session, pages 177-180.Percy Liang, Ben Taskar and Dan Klein, 2006.Alignment by agreement.
In Proc.
of HLT-NAACL06, pages 104-111.D.
Lin and C. Cherry, 2003.
Word alignment withcohesion constraint.
In Proc.
of NAACL '03, pages49-51.Adam Lopez and Philip Resnik, 2005.
Improved HMMalignment models for languages with scarceresources.
In ACL Workshop on Building and UsingParallel Texts '05, pages 83-86.Cos k?un Mermer and Murat Sara?lar, 2011.
Bayesianword alignment for statistical machine translation.
InProc.
of ACL '11, pages 182-187.R.C.
Moore, 2005.
A discriminative framework forbilingual word alignment.
In Proc.
of EMNLP '05,pages 81-88.F.J.
Och, C. Tillmann and H. Ney, 1999.
Improvedalignment models for statistical machine translation.In Proc.
of EMNLP/WVLC '99, pages 20-28.Franz Josef Och and Hermann Ney, 2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29 (1).
pages 19-51.K.
Papineni, S. Roukos, T. Ward and W.J.
Zhu, 2002.BLEU: a method for automatic evaluation ofmachine translation.
In Proc.
of ACL '02, pages 311-318.Adam Pauls, Dan Klein, David Chiang and KevinKnight, 2010.
Unsupervised Syntactic Alignmentwith Inversion Transduction Grammars.
In Proc.
ofNAACL '10.Slav Petrov, Leon Barrett, Romain Thibaux and DanKlein, 2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proc.
of ACL 2006.Jason Riesa and Daniel Marcu, 2010.
Hierarchicalsearch for word alignment.
In Proc.
of ACL '10,pages 157-166.Jason Riesa, Ann Irvine and Daniel Marcu, 2011.Feature-Rich Language-Independent Syntax-BasedAlignment for Statistical Machine Translation.
InProc.
of EMNLP '11.Darcey Riley and Daniel Gildea, 2012.
Improving theIBM Alignment Models Using Variational Bayes.
InProc.
of ACL '12.M.
Saers, J. Nivre and D. Wu, 2010.
Word alignmentwith stochastic bracketing linear inversiontransduction grammar.
In Proc.
of NAACL '10, pages341-344.A.
Stolcke, 2002.
SRILM-an extensible languagemodeling toolkit.
In ICSLP '02.B.
Taskar, S. Lacoste-Julien and D. Klein, 2005.
Adiscriminative matching approach to word alignment.In Proc.
of EMNLP '05, pages 73-80.Ashish Vaswani, Liang Huang, and David Chiang, 2012.Smaller alignment models for better translations:unsupervised word alignment with the l0 norm.
InProc.
ACL'12, pages 311?319.Stephan Vogel, Hermann Ney and Christoph Tillmann,1996.
HMM-based word alignment in statisticaltranslation.
In Proc.
of COLING-96, pages 836-841.D.
Wu, 1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23 (3).
pages 377-403.Zhiguo Wang, Chengqing Zong, 2010.
Phrase StructureParsing with Dependency Structure, In Proc.
ofCOLING 2010, pages 1292-1300.Zhiguo Wang, Chengqing Zong, 2011.
Parse RerankingBased on Higher-Order Lexical Dependencies, InProc.
Of IJCNLP 2011, pages 1251-1259.Kenji Yamada and Kevin Knight, 2001.
A syntax-basedstatistical translation model.
In Proc.
of ACL '01,pages 523-530.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.Interpreting BLEU/NIST scores: How muchimprovement do we need to have a better system?
InProc.
of LREC.Shaojun Zhao and Daniel Gildea, 2010.
A fast fertilityhidden Markov model for word alignment usingMCMC.
In Proc.
of EMNLP '10, pages 596-605.300
