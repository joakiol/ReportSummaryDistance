Introduction to the Special Issueon Computational LinguisticsUsing Large CorporaKenneth  W. Church*AT&T Bell LaboratoriesRober t  L. Mercer  tIBM T.J. Watson Research CenterThe 1990s have witnessed a resurgence of interest in 1950s-style mpirical and statisti-cal methods of language analysis.
Empiricism was at its peak in the 1950s, dominatinga broad set of fields ranging from psychology (behaviorism) to electrical engineering(information theory).
At that time, it was common practice in linguistics to classifywords not only on the basis of their meanings but also on the basis of their co-occurrence with other words.
Firth, a leading figure in British linguistics during the1950s, summarized the approach with the memorable line: "You shall know a wordby the company it keeps" (Firth 1957).
Regrettably, interest in empiricism faded in thelate 1950s and early 1960s with a number of significant events including Chomsky'scriticism of n-grams in Syntactic Structures (Chomsky 1957) and Minsky and Papert'scriticism of neural networks in Perceptrons (Minsky and Papert 1969).Perhaps the most immediate reason for this empirical renaissance is the availabil-ity of massive quantities of data: more text is available than ever before.
Just ten yearsago, the one-million word Brown Corpus (Francis and Ku~era, 1982) was consideredlarge, but even then, there were much larger corpora such as the Birmingham Cor-pus (Sinclair et al 1987; Sinclair 1987).
Today, many locations have samples of textrunning into the hundreds of millions or even billions of words.
Collections of thismagnitude are becoming widely available, thanks to data collection efforts such as theAssociation for Computational Linguistics' Data Collection Initiative (ACL/DCI), theEuropean Corpus Initiative (ECI), ICAME, the British National Corpus (BNC), the Lin-guistic Data Consortium (LDC), the Consortium for Lexical Research (CLR), ElectronicDictionary Research (EDR), and standardization efforts such as the Text Encoding Ini-tiative (TEI).
1 The data-intensive approach to language, which is becoming known asText Analysis, takes a pragmatic approach that is well suited to meet the recent empha-sis on numerical evaluations and concrete deliverables.
Text Analysis focuses on broad(though possibly superficial) coverage of unrestricted text, rather than deep analysisof (artificially) restricted omains.
* AT&T Bell Laboratories, Office 2B-421, 600 Mountain Ave., Murray Hill, NJ 07974.t IBM T.J. Watson Research Center, P.O.
Box 704, J2-H24, Yorktown Heights, NY 10598.1 For more information on the ACL/DC1, contact Felicia Hurewitz, ACL/DCI, Room 619, Williams Hall,University of Pennsylvania, Philadelphia, PA 19104-6305, USA, (tel) 215-898-0083, (fax) 215-573-2091,(e-mail) fel@unagi.cis.upenn.edu.
For more information on the LDC, contact Elizabeth Hodas,Linguistic Data Consortium, Room 441, Williams Hall, University of Pennsylvania, Philadelphia, PA,19104-6305, USA, (tel) 215-898-0464, (fax) 215-573-2175, (e-mail) ehodas@unagi.cis.upenn.edu.
Sende-mail to smbowie@vax.oxford.ac.uk for information on the BNC, to lexical@nmsu.edu for informationon the CLR, and to eucorp@cogsci.edinburgh.ac.uk for information on the ECI.
Information on theLondon-Lund Corpus and other corpora vailable through ICAME can be found in the ICAMEJournal, edited by Stig Johansson, Department ofEnglish, University of Oslo, Norway.
(~) 1993 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 1The case for the resurgence of empiricism in computational linguistics is nicelysummarized in Susan Warwick-Armstrong's call-for-papers for this special issue:The increasing availability of machine-readable corpora has suggestednew methods for studies in a variety of areas such as lexical knowl-edge acquisition, grammar construction, and machine translation.Though common in the speech community, the use of statistical andprobabilistic methods to discover and organize data is relatively newto the field at large.
The various initiatives currently under way to lo-cate and collect machine-readable corpora have recognized the poten-tial of using this data and are working toward making these materialsavailable to the research community.
Given the growing interest incorpus studies, it seems timely to devote an issue of CL to this topic.In Section 1, we review the experience of the speech recognition community.
Stochasticmethods based on Shannon's noisy channel model have become the methods of choicewithin the speech community.
Knowledge-based approaches were tried during the firstDARPA speech recognition project in the early 1970s, but have largely been abandonedin favor of stochastic approaches that have become the main focus of DARPA's morerecent efforts.In Section 2, we discuss how this experience is influencing the language commu-nity.
Many of the most successful speech techniques are achieving major improvementsin performance in the language area.
In particular, probabilistic taggers based on Shan-non's noisy channel model are becoming the method of choice because they correctlytag 95% of the words in a new text, a major improvement over earlier technologies thatignored lexical probabilities and other preferences that can be estimated statisticallyfrom corpus evidence.In Section 3, we discuss a number of frequency-based preferences such as collo-cations and word associations.
Although often ignored in the computational linguisticsliterature because they are difficult to capture with traditional parsing technology,they can" easily overwhelm syntactic factors (as any psycholinguist knows).
Four arti-cles in this special issue take a first step toward preference-based parsing, an empiricalalternative to the rational tradition of principle-based parsing, ATNs, unification, etc.In Section 4, we discuss entropy and evaluation issues, which have become rela-tively important in recent years.In Section 5, we discuss the application of noisy channel models to bilingual ap-plications uch as machine translation and bilingual exicography.In Section 6, we discuss the use of empirical methods in monolingual lexicogra-phy, contrasting the exploratory data analysis (EDA) view of statistics with other per-spectives uch as hypothesis testing and supervised/unsupervised learning/training.There are five articles in this special issue on computational lexicography, using boththe exploratory and the self-organizing approaches to statistics.1.
The Influence from the Speech Community1.1 Consensus in Speech Community: Stochastic Methods Are OutperformingKnowledge-Based MethodsOver the past 20 years, the speech community has reached a consensus in favor of em-pirical methods.
As observed by Waibel and Lee in the introduction to their collection2Kenneth W. Church and Robert L. Mercer Introductionof reprints on speech recognition (Waibel and Lee 1990):Chapter 5 describes the knowledge-based approach, proposed in the 1970sand early 1980s.
The pure knowledge-based approach emulates hu-man speech knowledge using expert systems.
Rule-based systemshave had only limited success... Chapter 6 describes the stochasticapproach...
Most successful large-scale systems today use a stochasticapproach.
(Waibel and Lee 1990; p. 4)A number of data collection efforts have helped to bring about this change in thespeech community, especially the Texas Instruments' Digit Corpus (Leonard 1984),TIMIT and the DARPA Resource Management (RM) Database (Price et al 1988).
Ac-cording to the Linguistic Data Consortium (LDC), the RM database was used by everypaper that reported speech recognition results in the 1988 Proceedings oflEEE ICASSP,the major technical society meeting where speech recognition results are reported.This is especially significant given that abstracts for this meeting were due just a fewmonths after the release of the corpus, attesting to the speech recognition community'shunger for standard corpora for development and evaluation.Back in the 1970s, the more data-intensive methods were probably beyond themeans of many researchers, especially those working in universities.
Perhaps someof these researchers turned to the knowledge-based approach because they couldn'tafford the alternative.
It is an interesting fact that most of the authors of the knowledge-based papers in Chapter 5 of Waibel and Lee (1990) have a university affiliationwhereas most of the authors of the data-intensive papers in Chapter 6 have an in-dustrial affiliation.
Fortunately, as a result of improvements in computer technologyand the increasing availability of data due to numerous data collection efforts, thedata-intensive methods are no longer restricted to those working in affluent industriallaboratories.1.2 The Anti-Empiricist Period in Speech ResearchAt the time, of course, the knowledge-based approach was not advocated on economicgrounds.
Rather, the knowledge-based approach was advocated as necessary in orderto deal with the lack of allophonic invariance.
The mapping between phonemes andtheir allophonic realizations is highly variable and ambiguous.
The phoneme/ t / ,  forexample, may be realized as a released stop in "Tom," as a flap in "butter," or asa glottal stop in "bottle."
Two different phonemes may lead to the same allophonicvariant in some contexts.
For example, "writer" and "rider" are nearly identical inmany dialects of American English.
Residual differences, such as the length of thepreconsonantal vowel, are easily overwhelmed by the context in which the word ap-pears.
Thus, if one says "Joe is a rider of novels," listeners hear "Joe is a writer ofnovels," while if one says "Joe is a writer of horses," listeners hear "Joe is a rider ofhorses."
Listeners usually have little problem with the wild variability and ambiguityof speech because they know what the speaker is likely to say.In most systems for sentence recognition, such modifications must beviewed as a kind of 'noise' that makes it more difficult to hypothesizelexical candidates given an input phonetic transcription.
To see thatthis must be the case, we note that each phonological rule \[in theutterance: "Did you hit it to Tom?
"\] results in irreversible ambiguity--the phonological rule does not have a unique inverse that could beused to recover the underlying phonemic representation for a lexicalComputational Linguistics Volume 19, Number 1item.
For example, ... \[t\]he tongue flap ... could have come from a/ t /o r  a /d / .
(Klatt 1980; pp.
548-549)The first DARPA Speech Understanding project emphasized the use of high-level con-straints (e.g., syntax, semantics, and pragmatics) as a tool to disambiguate the al-lophonic information in the speech signal by understanding the message.
At BBN,researchers called their system HWIM for (Hear What IMean).
They hoped to use NLPtechniques uch as ATNs (Woods 1970) to understand the sentences that they weretrying to recognize ven though the output of their front end was highly variable andambiguous.The emphasis today on empirical methods in the speech recognition communityis a reaction to the failure of knowledge-based approaches of the 1970s.
It has becomepopular once again to focus on high-level natural language constraints in order toreduce the search space.
But this time, n-gram methods have become the methods ofchoice because they seem to work better than the alternatives, at least when the searchspace is measured in terms of entropy.
Ideally, we might hope that someday parsersmight reduce entropy beyond that of n-grams, but right now, parsers eem to be moreuseful for other tasks such as understanding who did what to whom, and less usefulfor predicting what the speaker is likely to say.1.3 The Raleigh System: A Foundation for the Revival of EmpiricismIn the midst of all of this excitement over high-level, knowledged-based NLP tech-niques, IBM formed a new speech group around the nucleus of an existing groupthat was moved from Raleigh, North Carolina, to Yorktown Heights early in 1972.The Raleigh group brought to Yorktown a working speech recognition system, thathad been designed in accordance with prevailing anti-empiricist attitudes of the time,though it would soon serve as a foundation for the revival of empiricism in the speechand language communities.The front end of the Raleigh system converted the speech signal (20,000 samplesper second) first into a sequence of 80 filter bank outputs (100 80-dimensional vectorsper second), and then into a sequence of phoneme-like labels (100 labels per second),using an elaborate set of hand-tuned rules that would soon be replaced with an auto-matically trained procedure.
The back end converted these labels into a sequence ofwords using an artificial finite-state grammar that was so small that the finite-state ma-chine could be written down on a single piece of paper.
Today, many systems attemptto model unrestricted language using methods that will be discussed in Section 3, butat the time, it was standard practice to work with artificial grammars of this kind.When it worked perfectly, the front end produced a transcription of the speechsignal such as might be produced by a human phonetician listening carefully to theoriginal speech.
Unfortunately, it almost never worked perfectly, even on so small astretch as a single word.
Rapid phones, such as flaps, were often missed; long phones,such as liquids and stressed vowels, were often broken into several separate seg-ments; and very often phones were simply mislabeled.
The back end was designed toovercome these problems by navigating through the finite-state network, applying acomplicated set of hand-tuned penalties and bonuses to the various paths in order tofavor those paths where the low-level acoustics matched the high-level grammaticalconstraints.
This system of hand-tuned penalties and bonuses correctly recognized 35%of the sentences (and 77% of the words) in the test set.
At the time, this level of perfor-mance was actually quite impressive, but these days, one would expect much more,now that most systems use parameters trained on real data, rather than a complicatedset of hand-tuned penalties and bonuses.4Kenneth W. Church and Robert L. Mercer Introduction1.4 The Noisy Channel ModelAlthough the penalties and bonuses were sometimes thought of as probabilities, theearly Raleigh system lacked a complete and unified probabilistic framework.
In a rad-ical departure from the prevailing attitudes of the time, the Yorktown group turnedto Shannon's theory of communication i the presence of noise and recast he speechrecognition problem in terms of transmission through a noisy channel.
Shannon's the-ory of communication (Shannon 1948), also known as Information Theory, was originallydeveloped at AT&T Bell Laboratories to model communication along a noisy channelsuch as a telephone line.
See Fano (1961) for a well-known secondary source on thesubject, or Cover and Thomas (1991) or Bell, Cleary, and Witten (1990) for more recenttreatments.The noisy channel paradigm can be applied to other recognition applications suchas optical character recognition (OCR) and spelling correction.
Imagine a noisy chan-nel, such as a speech recognition machine that almost hears, an optical character recog-nition (OCR) machine that almost reads, or a typist who almost types.
A sequence ofgood text (/) goes into the channel, and a sequence of corrupted text (O) comes outthe other end.I --* Noisy Channel --, 0How can an automatic procedure recover the good input text,/, from the corruptedoutput, O?
In principle, one can recover the most likely input, I, by hypothesizing allpossible input texts,/, and selecting the input text with the highest score, Pr(I I 0).Symbolically,-- argmaxPr(I \[ O) = argmaxPr(I) Pr(O I I )I Iwhere ARGMAX finds the argument with the maximum score.The prior probability, Pr(I), is the probability that I will be presented at the inputto the channel.
In speech recognition, it is the probability that the talker utters I; inspelling correction (Damerau 1964; Kukich 1992), it is the probability that the typistintends to type I.
In practice, the prior probability is unavailable, and consequently,we have to make do with a model of the prior probability, such as the trigram model.The parameters of the language model are usually estimated by computing variousstatistics over a large sample of text.The channel probability, Pr(O I I), is the probability that O will appear at theoutput of the channel when I is presented at the input; it is large if I is similar, insome appropriate sense, to O, and small, otherwise.
The channel probability dependson the application.
In speech recognition, for example, the output for the word "writer"may look similar to the word "rider"; in character recognition, this will not be the case.Other examples are shown in Table 1.1.5 Training Is Better than GuessingRather than rely on guesses for the values of the bonuses and penalties as the Raleighgroup had done, the Yorktown group used three levels of hidden Markov models(HMMs) to compute the conditional probabilities necessary for the noisy channel.A Markov model is a finite state machine with probabilities governing transitionsbetween states and controlling the emission of output symbols.
If the sequence of statetransitions cannot be determined when the sequence of outputs is known, the Markovmodel is said to be hidden.
In practice, the Forward-Backward algorithm is often usedto estimate the values of the transition and emission parameters on the basis of corpusevidence.
See Furui (1989; Appendix D.3, pp.
343-347) for a brief description of theComputational Linguistics Volume 19, Number 1Table 1Examples of channel confusions in different applications.Application Input OutputSpeech writer riderRecognition here hearOptical all all (A-one-L)Character of o{Recognition form farmSpelling government govermentCorrection occurred occuredcommercial commericalTable 2Performance after training (Bahl et al 1975)Training Set Size Test Sentences Correctly Decoded Decoding Problems0 2/10 8/10200 77/100 3/100400 80/100 2/100600 85/100 1/100800 82/100 3/1001070 83/100 3/100Forward-Backward algorithm, and (Rabiner 1989) for a longer tutorial on HMMs.
Thegeneral procedure, of which the Forward-Backward algorithm is a special case, wasfirst published and shown to converge by Baum (1972).The first level of the Raleigh system converted spelling to phonemic base forms,rather like a dictionary; the second level dealt with the problems of allophonic variationmentioned above; the third level modeled the front end.
At first, the values of theparameters in these HMMs were carefully constructed by hand, but eventually theywould all be replaced with estimates obtained by training on real data using statisticalestimation procedures such as the Forward-Backward algorithm.The advantages of training are apparent in Table 2.
Note the astounding improve-ment in performance.
Despite a few decoding problems, which indicate limitationsin the heuristic search procedure mployed by the recognizer, sentence accuracy hadimproved from 35% to 82-83%.Moreover, training turned out to be important for speeding up the search.
The firstrow shows the results for the initial estimates, which were very carefully prepared bytwo members of the group over several weeks.
Despite all of the careful hand work,the search was so slow that only 10 of the 100 test sentences could be recognized.The initial estimates were unusable without at least some training.
These days, mostresearchers find that they do not need to be nearly so careful in obtaining initialestimates.Emboldened by this success, the group began to explore other areas where trainingmight be helpful.
They began by throwing out the phonological rules.
Thus, theyaccepted only a single pronunciation for each word.
B-u-t-t-e-r had to be pronouncedKenneth W. Church and Robert L. Mercer Introductionbutter and s-o-m-e-t-h-i-n-g had to be pronounced something, and that was that.
Anychange in these pronunciations was treated as a mislabeling from the front end.
Aftertraining, this simplified system correctly decoded 75% of 100 test sentences, whichwas very encouraging.Finally, they removed the dictionary lookup HMM, taking for the pronunciationof each word its spelling.
Thus, a word like t-h-r-o-u-g-h was assumed to have apronunciation like tuh huh ruh oh uu guh huh.
After training, the system learned thatwith words like l-a-t-e the front end often missed the e. Similarly, it learned that g'sand h's were often silent.
This crippled system was still able to recognize 43% of 100test sentences correctly as compared with 35% for the original Raleigh system.These results firmly established the importance of a coherent, probabilistic ap-proach to speech recognition and the importance of data for estimating the parametersof a probabilistic model.
One by one, pieces of the system that had been assiduouslyassembled by speech experts yielded to probabilistic modeling.
Even the elaborate setof hand-tuned rules for segmenting the frequency bank outputs into phoneme-sizedsegments would be replaced with training (Bakis 1976; Bahl et al 1978).
By the sum-mer of 1977, performance had reached 95% correct by sentence and 99.4% correct byword, a considerable improvement over the same system with hand-tuned segmenta-tion rules (73% by sentence and 95% by word).Progress in speech recognition at Yorktown and almost everywhere else as well hascontinued along the lines drawn in these early experiments.
As computers increasedin power, ever greater tracts of the heuristic wasteland opened up for colonization byprobabilistic models.
As greater quantities of recorded ata became available, theseareas were tamed by automatic training techniques.
Today, as indicated in the intro-duction of Waibel and Lee (1990), almost every aspect of most speech recognitionsystems is dominated by probabilistic models with parameters determined from data.2.
Part-of-Speech TaggingMany of the very same methods are being applied to problems in natural anguageprocessing by many of the very same researchers.
As a result, the empirical approachhas been adopted by almost all contemporary part-of-speech programs: Bahl and Mer-cer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo(1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle(1989), Kupiec (1989, 1992), Ayuso et al (1990), deMarcken (1990), Karlsson (1990),Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, andAnttila (1992).
These programs input a sequence of words, e.g., The chair will table themotion, and output a sequence of part-of-speech tags, e.g., art noun modal verb art noun.Most of these programs correctly tag at least 95% of the words, with practically norestrictions on the input text, and with very modest space and time requirements.Perhaps the most important indication of success is that many of these statistical tag-ging programs are now being used on large volumes of data (hundreds of millions ofwords of text) in a number of different application areas including speech synthesis(Sproat, personal communication; Liberman and Church 1991), speech recognition (Je-linek 1985; Jelinek, Mercer, and Roukos 1991), information retrieval (Salton, Zhao, andBuckley 1990; Croft, Turtle, and Lewis 1991), sense disambiguation (Hearst 1991), andcomputational lexicography (Klavans and Tzoukermann 1990a, 1990b; Church andGale 1991).
Apparently, these programs must be addressing some important needs ofthe research community or else they wouldn't be as widely cited as they are.
Many ofthe papers in this special issue refer to these taggers.Computational Linguistics Volume 19, Number 1As in speech recognition, data collection efforts have played a pivotal role inadvancing data-intensive approaches to part-of-speech tagging.
The Brown Corpus(Francis and Kueera 1982) and similar efforts within the ICAME community, havecreated invaluable opportunities.
The Penn Treebank (see the paper by Marcus andSantorini 1983) is currently being distributed by the ACL/DCI.
The European CorpusInitiative (ECI) plans to distribute similar material in a variety of languages.
Evengreater esources are expected from the Linguistic Data Consortium (LDC).
And theConsortium for Lexical Research (CLR) is helping to make dictionaries more accessibleto the research community.
For information on contacting these organizations, seefootnote 1.2.1 Recasting Part-of-Speech Tagging as a Noisy Channel ProblemMany of the tagging programs mentioned above are based on Shannon's Noisy Chan-nel Model.
Imagine that a sequence of parts of speech, P, is presented at the input tothe channel and for some crazy reason, it appears at the output of the channel in acorrupted form as a sequence of words, W. Our job is to determine P given W.P --+ Noisy Channel --+ WBy analogy with the noisy channel formulation of the speech recognition problem, themost probable part-of-speech sequence,/~, is given by:~P = argmaxPr(P) Pr(W I P )PIn theory, with the proper choice for the probability distributions Pr(P) and Pr(W I P),this algorithm will perform as well as, or better than, any possible alternative thatone could imagine.
Unfortunately, the probability distributions Pr(P) and Pr(W I P)are enormously complex: Pr(W I P) is a table giving for every pair W and P of thesame length a number between 0 and 1 that is the probability that a sequence ofwords chosen at random from English text and found to have the part-of-speechsequence P will turn out to be the word sequence W. Changing even a single wordor part-of-speech in a long sequence may change this number by many orders ofmagnitude.
However, experience has shown that surprisingly high tagging accuracycan be achieved in practice using very simple approximations to Pr(P) and Pr(W I P).In particular, it is possible to replace Pr(P) by a trigram approximation:NPr(P1, P2,..
., PN) ~ I I  Pr(Pi \[ Pi-2Pi-1)i=iand to replace Pr(W \] P) by an approximation i which each word depends only onits own part of speech:NPr(W1,W2,..., WN \] P1,P2,... ,PN) "~ HPr(Wi  I Pi)i=iIn these equations, Pi is the /th part of speech in the sequence P, and Wi is the /thword in W. The parameters of this model, the lexical probabilities, Pr(Wi I Pi), andthe contextual probabilities, Pr(Pi I Pi-2Pi-1), are generally estimated by computingvarious statistics over large bodies of text.
One can view the first set of parameters asa dictionary and the second set of parameters as a grammar.Kenneth W. Church and Robert L. Mercer IntroductionTable 3Lexical ambiguity is hard (if we ignore preferences).Word Part-of-Speech TagsMore Likely Less LikelyI pronoun noun (letter of alphabet)see verb noun (the Holy See)a article noun (letter of alphabet)bird noun verb2.2 Why Traditional Methods Have FailedTraditional methods have tended to ignore lexical preferences, which are the single-most important source of constraint for part-of-speech tagging, and are sufficient bythemselves to resolve 90% of the tags.
Consider the trivial sentence, "I see a bird,"where every word is almost unambiguous.
In the Brown Corpus (Francis and Ku~era1982), the word "I" appears as a pronoun in 5,131 times out of 5,132 (~ 100%), "see"appears as a verb in 771 times out of 772 (~ 100%), "a" appears as an article in 22,938times out of 22,944 (~ 100%) and "bird" appears as a noun in 25 times out of 25(~ 100%).
However, in addition to the desired tag, many dictionaries uch as Web-ster's Ninth New Collegiate Dictionary (Mish 1983) also list a number of extremely rarealternatives, as illustrated in Table 3.
These alternatives can usually be eliminated onthe basis of the statistical preferences, but traditional parsers don't, and consequentlyrun into serious difficulties.
Attempts to eliminate unwanted tags on syntactic groundshave not been very successful.
For example, I/noun see~noun a/noun bird~noun, cannotbe ruled out as syntactically ill-formed, because the parser must accept sequences offour nouns in other situations: city school committee meeting.
Apparently, syntactic rulesare not nearly as effective as lexical preferences, at least for this application.The tradition of ignoring preferences dates back to Chomsky's introduction of thecompetence approximation (Chomsky 1957, pp.
15-17).
Recall that Chomsky was con-cerned that approximations, such as Shannon's n-gram approximation, which was verymuch in vogue at the time, were inappropriate for his needs, and therefore, he intro-duced an alternative with complementary strengths and weaknesses.
The competenceapproximation is more appropriate for modeling long-distance dependences such asagreement constraints and wh-movement, but at the cost of missing certain cruciallocal constraints, especially the kinds of preferences that are extremely important forpart-of-speech tagging.2.3 Using Statistics to Fit Probabilistic Models to DataProbabilistic models provide a theoretical abstraction of language, very much likeChomsky's competence model.
They are designed to capture the more important as-pects of language and ignore the less important aspects, where what counts as im-portant depends on the application.
Statistics are often used to estimate the values ofthe parameters in these probabilistic models.
Thus, for example, we might estimatethe probability distribution for the word Kennedy in the Brown Corpus by modelingthe distribution with a binomial, and then use the frequency of Kennedy in the BrownCorpus (140) to fit the model to the data.Computational Linguistics Volume 19, Number 1The classic example of a binomial process is coin tossing.
Suppose that the coincomes up heads with probability p. Then the probability that it will come up headsexactly m times in n tosses isn / pro(1 -P)"-~"mHere(n )m 'which is called the binomial coefficient, is the number of ways the m positions can bechosen from the n coin tosses.
It is equal ton!m!
(n - m)!
'where n!
(n factorial) is equal to 1 x 2 x .. .
x n. For example, tossing a fair coin threetimes (n = 3, p = 1/2) will result in 0, 1, 2, and 3 heads with probability 1/8, 3/8,3/8, and 1/8, respectively.
This set of probabilities is called the binomial distributionfor n and p. The expected value of the binomial distribution is np and the variance is0 .2  np(1 - p).
Thus, tossing a fair coin three times will produce an average of 3/2heads with a variance of 3/4.How can the binomial be used to model the distribution of Kennedy?
Let p be theprobability that a word chosen at random in English text is Kennedy.
We can think ofa series of words in English text as analogous to tosses of a coin that comes up headswith probability p: the coin is heads if the word is Kennedy, and is tails otherwise.Of course, we don't really know the value of p, but in a sample of n words, weshould expect o find about np occurrences of Kennedy.
There are 140 occurrences ofKennedy in the Brown Corpus, for which n is approximately 1,000,000.
Therefore, wecan argue that 1,000,000p must be about 140 and we can make an estimate, ~, of pequal to 140/1,00G000.
If we really believe that words in English text come up likeheads when we flip a biased coin, then ~ is the value of p that makes the BrownCorpus as probable as possible.
Therefore, this method of estimating parameters icalled maximum likelihood estimation (MLE).
For simple models, MLE is very easy toimplement and produces reasonable stimates in many cases.
More elaborate methodssuch as the Good-Turing Method (Good 1953) or Deleted Estimation (,Jelinek and Mercer1980, 1985) should be used when the frequencies are small (e.g., less than 10).It is often convenient to use these statistical estimates as if they are the same as thetrue probabilities, but this practice can lead to trouble, especially when the data don'tfit the model very well.
In fact, content words don't fit a binomial very well, becausecontent words tend to appear in "bursts."
That is, content words are like buses inNew York City; they are social animals and like to travel in packs.
In particular, if theword Kennedy appears once in a unit of text (e.g., a paragraph, a discourse, or a genre),then it is much more likely than chance to appear a second time in the same unit oftext.
Function words also deviate from the binomial, though for different reasons (e.g.,stylistic factors mentioned in Biber's paper).These bursts might serve a useful purpose.
People seem to be able to use thesebursts to speed up reaction times in various tasks.
Psycholinguists use the term primingto refer to this effect.
Bursts might also be useful in a number of practical applicationssuch as Information Retrieval (IR) (Salton 1989; Frakes and Baeza-Yates 1992).
Therehave been a number of attempts over the years to model these bursts.
The negative10Kenneth W. Church and Robert L. Mercer Introductionbinomial distribution, for example, was explored in considerable detail in the classicstudy of the authorship of the Federalist Papers, Mosteller and Wallace (1964), a must-read for anyone interested in statistical analyses of large corpora.We can show that the distribution of Kennedy isvery bursty in the Brown Corpus bydividing the corpus into k segments and showing that the probability varies radicallyfrom one segment to another.
For example, if we divide the Brown Corpus into 10segments of 100,000 words each, we find that the frequency of Kennedy is: 58, 57, 2, 12,6, 1, 4, 0, 0, 0.
The variance of these 10 numbers is 539.
Under the binomial assumption,we obtain a very different estimate of the variance.
In a sample of n = 10G 000 words,with \]~ = 140 per million, we would expect a variance of n\]~(1 - \]~) ~ 14.
(The varianceof the binomial is approximately the same as the expected value when p is small.)
Thelarge discrepancy between the empirically derived estimate of the variance (539) andthe one based on the binomial assumption (14) indicates that the binomial assumptiondoes not fit the data very well.3.
Preference-Based ParsersWhen the data don't fit the model very well, we may wish to look for alternativemodels.
Four articles in this special issue propose mpirical alternatives to traditionalparsing methods based on the competence model.
As we have seen, the competencemodel doesn't fit the part-of-speech application very well because of the model's fail-ure to capture certain lexical preferences.
The model also runs into trouble in a numberof other NLP applications.
Consider, for example, the problem of deciding betweenthe words form and farm in the OCR application (mentioned in Section 1.4) when theyappear in the context:pure ( farm form ) ofMost people would have little difficulty deciding that form was the intended word.Neither does an OCR system that employs a trigram language model, because prefer-ences, such as collocations, fall naturally within the scope of the n-gram approximation.Traditional NLP techniques, on the other hand, fail here because the competence ap-proximation does not capture the crucial collocational constraints.Lexicographers u e the terms collocation, co-occurrence, and lexis to describe variousconstraints on pairs of words.
The words strong and powerful are perhaps the canonicalexample.
Halliday (1966; p. 150) noted that although strong and powerful have similarsyntax and semantics, there are contexts where one is much more appropriate thanthe other (e.g., strong tea vs. powerful computers).Psycholinguists have a similar concept, which they call word associations.
Two fre-quently cited examples of highly associated words are: bread~butter and doctor~nurse.See Palermo and Jenkins (1964) for tables of associations, measured for 200 words,factored by grade level and sex.
In general, subjects respond more quickly to a wordsuch as butter when it follows a highly associated word such as bread.Some results and implications are summarized from reaction-time ex-periments in which subjects either (a) classified successive strings ofletters as words and nonwords, or (b) pronounced the strings.
Bothtypes of response to words (e.g., BUTTER) were consistently fasterwhen preceded by associated words (e.g., BREAD) rather than unas-sociated words (e.g, NURSE).
(Meyer, Schvaneveldt, and Ruddy 1975;p. 98)11Computational Linguistics Volume 19, Number 1Table 4The trigram approximation i action (Jelinek 1985).Word Rank More likely alternativesWe 9 The This One Two A Three Please Inneed 7 are will the would also doto 1resolve 85all 9of 2the 1important 657issues 14have know do ...the this these problems ...thedocument question first ...thing point to ...These constraints are rarely discussed in computational linguistics because they arenot captured very well with traditional NLP techniques, especially those based on thecompetence approximation.
Of course, it isn't hard to build computational models thatcapture at least some of these preferences.
Even the trigram model, despite all of itsobvious shortcomings, does better than many traditional methods in this regard.
Thepower of the trigram approximation is illustrated in Table 4 for the sentence fragment,We need to resolve all of the important issues..., selected from a 90 million-word corpus ofIBM office correspondences.
Each row shows the correct word, the rank of the correctword as predicted by the trigram model, and then the list of words judged by thetrigram model to be more probable than the correct word.
Thus, We is the 9 th mostprobable word to begin a sentence.
At this point in the sentence, in the absence of anyother context, he trigram model is as good as any model we could have.
Following Weat the beginning of the sentence, need is the 7 th most probable word, ranking behindare, will, the, would, also, and do.
Here, again, the trigram model still accounts for allof the context here is and so should be doing as well as any model can.
FollowingWe need, to is the most probable word.
Although by now, the trigram model has losttrack of the complete context (it no longer realizes that we are at the beginning of asentence), it is still doing very well.Table 4 shows that the trigram model captures a number of important frequency-based constraints hat would be missed by most traditional parsers.
For example, thetrigram model captures the fact that issues is particularly predictable in the collocation:important issues.
In general, high-frequency function words like to and the, which areacoustically short, are more predictable than content words like resolve and important,which are longer.
This is convenient for speech recognition because it means that thelanguage model provides more powerful constraints just when the acoustic model ishaving the toughest time.
One suspects that this is not an accident, but rather a naturalresult of the evolution of speech to fill the human needs for reliable communicationin the presence of noise.?
The ideal NLP model would combine the strengths of both the competence ap-proximation and the n-gram approximation.
One possible solution might be the Inside-Outside algorithm (Baker 1979; Lari and Young 1991), a generalization f the Forward-Backward algorithm that estimates the parameters of a hidden stochastic context-freegrammar, ather than a hidden Markov model.
Four alternatives are proposed in thesespecial issues: (1) Brent (1993), (2) Briscoe and Carroll (this issue), (3) Hindle and Rooth(this issue), and (4) Weischedel t al.
(1993).
Briscoe and Carroll's contribution is very12Kenneth W. Church and Robert L. Mercer Introductionmuch in the spirit of the Inside-Outside algorithm, whereas Hindle and Rooth's con-tribution, for example, takes an approach that is much closer to the concerns of lexi-cography, and makes use of preferences involving words, rather than preferences thatignore words and focus exclusively on syntactic structures.
Hindle and Rooth showhow co-occurrence statistics can be used to improve the performance of the parser onsentences such as:(wanted)She placed the dress on the rack.putwhere lexical preferences are crucial to resolving the ambiguity of prepositional phraseattachment (Ford, Bresnan, and Kaplan 1982).
Hindle and Rooth show that a parsercan enforce these preferences by comparing the statistical association of the verb-preposition (want... on) with the association ofthe object-preposition (dress... on), whenattaching the prepositional phrase.
This work is just a first step toward preference-based parsing, an empirically motivated alternative to traditional rational approachessuch as ATNs, unification parsers, and principle-based parsers.4.
EntropyHow do we decide if one language model is better than another?
In the 1940s, Shan-non defined entropy, a measure of the information content of a probabilistic source,and used it to quantify such concepts as noise, redundancy, the capacity of a commu-nication channel (e.g., a telephone), and the efficiency of a code.
The standard unit ofentropy is the bit or binary digit.
See Bell, Cleary, and Witten (1990) for a more dis-cussion on entropy; Section 2.2.5 shows how to compute the entropy of a model, andSection 4 discusses how Shannon and others have estimated the entropy of English.From the point of view of speech recognition or OCR, we would like to be ableto characterize the size of the search space, the number of binary questions that therecognizer will have to answer on average in order to decode a message.
Cross entropyis a useful yardstick for measuring the ability of a language model to predict a sourceof data.
If the language model is very good at predicting the future output of thesource, then the cross entropy will be small.
No matter how good the language modelis, though, the cross entropy cannot be reduced below a lower bound, known as theentropy of the source, the cross entropy of the source with itself.One can also think of the cross entropy between a language model and a proba-bilistic source as the number of bits that will be needed on average to encode a symbolfrom the source when it is assumed, albeit mistakenly, that the language model is aperfect probabilistic haracterization f the source.
Thus, there is a close connectionbetween a language model and a coding scheme.
Table 5 below lists a number ofcoding schemes along with estimates of their cross entropies with English text.The standard ASCII code requires 8 bits per character.
It would be a perfect codeif the source produced each of the 2 s = 256 symbols equally often and independentlyof context.
However, English is not like this.
For an English source, it is possible toreduce the average length of the code by assigning shorter codes to more frequentsymbols (e.g., e, n, s) and longer codes to less frequent symbols (e.g., j, q, z), using acoding scheme such as a Huffman code (Bell, Cleary, and Witten 1990; Section 5.1.2).Other codes, such as Lempel-Ziv (Welch 1984; Bell, Cleary, and Witten, Chapters 8-9)and n-gram models on words, achieve ven better compression by taking advantage ofcontext, hough none of these codes seem to perform as well as people do in predictingthe next letter (Shannon 1951).13Computational Linguistics Volume 19, Number 1Table 5Cross entropy of various language models.Model Bits / CharacterASCIIHuffman code each charLempel-Ziv (Unix TM compress)Unigram (Huffman code each word)TrigramHuman Performance854.432.1 (Brown, personal communication)1.76 (Brown et al 1992)1.25 (Shannon 1951)The cross entropy, H, of a code and a source is given by:H(source, code) = - ~ ~ Pr(s, h I source) log 2 Pr(s I h, code)s hwhere Pr(s, h I source) is the joint probability of a symbol s following a history h giventhe source.
Pr(s I h, code) is the conditional probability of s given the history (context)h and the code.
In the special case of ASCII, where Pr(s I h, ASCII) = 1/256, we canactually carry out the indicated sum, and find, not surprisingly, that ASCII requires 8bits per character:256 1 1H(source, ASCII = - ~ 256 1Og2 ~ = 8S~IIn more difficult cases, cross entropy is estimated by a sampling procedure.
Two in-dependent samples of the source are collected: $1 and $2.
The first sample, $1, is usedto fit the values of the parameters of the code, and second sample, $2, is used to testthe fit.
For example, to determine the value of 5 bits per character for the Huffmancode in Table 5, we counted the number of times that each of the 256 ASCII charactersappeared in $1, a sample of N1 characters elected from the Wall Street Journal textdistributed by the ACL/DCI.
These counts were used to determine Pr(s I h, code) (orrather Pr(s I code), since the Huffman code doesn't depend on h).
Then we collected asecond sample, $2, of N2 characters, and tested the fit with the formula:N21 i~=llOg2Pr(S2\[i\]lcode ) H(source, code) ~ -where  S 2 \[i\] is the/th character in the second sample.
It is important in this procedure touse two different samples of text.
If we were to use the same sample for both testingand training, we would obtain an overly optimistic estimate of how well the codeperforms.The other codes in Table 5 make better use of context (h), and therefore, theyachieve better compression.
For example, Huffman coding on words (a unigram model)is more than twice as compact as Huffman coding on characters (2.1 vs. 5 bits/char.
).The unigram model is also more than twice as good as Lempel-Ziv (2.1 vs. 4.43 bits/char.
), demonstrating that compress, a popular Unix TM tool for compressing files, could14Kenneth W. Church and Robert L. Mercer IntroductionTable 6Summary of two approaches toNLP.Rationalism EmpiricismWell-known Advocates:Model:Contexts of Interest:Goals:Linguistic Generalizations:Parsing Strategies:Applications:Chomsky, MinskyCompetence ModelPhrase StructureAll and OnlyExplanatoryTheoreticalAgreement andWh-movementPrinciple-BasedCKY (Chart), ATNs,UnificationUnderstandingWho did what to whomShannon, Skinner, Firth, HarrisNoisy Channel ModelN-gramsMinimize Prediction Error (Entropy)DescriptiveAppliedCollocations and Word AssociationsPreference-BasedForward-Backward, Inside--OutsideRecognitionNoisy Channel Applicationsbe improved by a factor of two (when the files are in English).
The trigram model, themethod of choice in speech recognition, achieves 1.76 bits per character, outperformingthe practical alternatives in Table 5, but falling half a bit shy of Shannon's estimate ofhuman performance.
2Someday parsers might help squeeze out some of this remaining half bit betweenthe trigram model and Shannon's bound, but thus far, parsing has had little impact.Lari and Young (1991; p. 255), for example, conducted a number of experiments withstochastic ontext-free grammars (SCFGs), and concluded that "\[t\]he experiments onword recognition showed that although SCFGs are effective, their complex trainingroutine prohibits them from directly replacing the simpler HMM-based recognizers.
"They then proceeded to argue, quite sensibly, that parsers are probably more appropri-ate for tasks where phrase structure is more directly relevant than in word recognition.In general, phrase structure is probably more important for understanding who didwhat to whom, than recognizing what was said.
3 Some tasks are probably more ap-propriate for Chomsky's rational approach to language and other tasks are probablymore appropriate for Shannon's empirical approach to language.
Table 6 summarizessome of the differences between the two approaches.5.
Machine Translation and Bilingual LexicographyIS machine translation (MT) more suitable for rationalism or empiricism?
Both ap-proaches have been investigated.
Weaver (1949) was the first to propose an informationtheoretic approach to MT.
The empirical approach was also practiced at Georgetownduring the 1950s and 1960s (Henisz-Dostert, Ross Macdonald, and Zarechnak 1979)in a system that eventually became known as SYSTRAN.
Recently, most work in MT2 In fact, the trigram model might be even better than suggested in Table 5, since the estimate for thetrigram model in Brown et al (1992) is computed over a 256-character alphabet, whereas the estimatefor human performance in Shannan (1951) is computed over a 27-character alphabet.3 Lari and Young actually looked at another task involving phonotactic structure where there is alsogood reason to believe that SCFGs might be able to capture crucial linguistic onstraints hat might bemissed by simpler HMMs.15Computational Linguistics Volume 19, Number 1has tended to favor rationalism, though there are some important exceptions, uch asexample-based MT (Sato and Nagao 1990).
The issue remains as controversial s ever,as evidenced by the lively debate on rationalism versus empiricism at TMI-92, a recentconference on MT.
4The paper by Brown et al (1990) revives Weaver's information theoretic approachto MT.
It requires a bit more squeezing and twisting to fit machine translation i to thenoisy channel mold: to translate, for example, from French to English, one imaginesthat the native speaker of French has thought up what he or she wants to say inEnglish and then translates mentally into French before actually saying it.
The task ofthe translation system is to recover the original English, E, from the observed French,F.
While this may seem a bit far-fetched, it differs little in principle from using Englishas an interlingua or as a meaning representation language.E --* Noisy Channel --* FAs before, one minimizes one's chance of error by choosing E according to the formula:= argmaxPr(E) Pr(F\]E)EAs before, the parameters of the model are estimated by computing various statisticsover large samples of text.
The prior probability, Pr(E), is estimated in exactly the sameway as discussed above for the speech recognition application.
The parameters of thechannel model, Pr(F \[ E), are estimated from a parallel text that has been aligned byan automatic procedure that figures out which parts of the source text correspond towhich parts of the target ext.
See Brown et al (1993) for more details on the estimationof the parameters.The information theoretic approach to MT may fail for reasons advanced by Chom-sky and others in the 1950s.
But regardless of its ultimate success or failure, there is agrowing community of researchers in corpus-based linguistics who believe that it willproduce a number of lexical resources that may be of great value.
In particular, therehas been quite a bit of discussion of bilingual concordances recently (e.g., Klavansand Tzoukermann 1990a, 1990b; Church and Gale 1991), including the 1990 and 1991lexicography conferences sponsored by Oxford University Press and Waterloo Univer-sity.
A bilingual concordance is like a monolingual concordance except hat each linein the concordance is followed by a line of text in a second language.
There are alsosome hopes that the approach might produce tools that could be useful for humantranslators (Isabelle 1992).There are three papers in these special issues on aligning bilingual texts such asthe Canadian Hansards (parliamentary debates) that are available in both English andFrench: Brown et al (1993), Gale and Church (this issue), and Kay and R6senschein(this issue).
Warwick-Armstrong and Russell have also been interested in the alignmentproblem (Warwick-Armstrong and Russell 1990).
Except for Brown et al, this work isfocused on the less controversial pplications in lexicography and human translation,rather than MT.4 Requests for a tape of the debate should be sent to the attention of Pierre Isabelle, CCRIT, TMI-92, 1575boul.
Chomedey, Laval (Quebec), H7V 2X2, Canada.
Copies of the TMI proceedings can be obtained bywriting to CCRIT or sending e-mail to tmi@ccrit.doc.ca.16Kenneth W. Church and Robert L. Mercer Introduction6.
Monolingual Lexicography, Machine-Readable Dictionaries (MRDs),and Computational LexiconsThere has been a long tradition of empiricist approaches in lexicography, both bilingualand monolingual, dating back to Johnson and Murray.
As corpus data and machine-readable dictionaries (MRDs) become more and more available, it is becoming easierto compile lexicons for computers and dictionaries for people.
This is a particularlyexciting area in computational linguistics as evidenced by the large number of con-tributions in these special issues: Biber (1993), Brent (1993), Hindle and Rooth (thisissue), Pustejovsky et al (1993), and Smadja (this issue).
Starting with the COBUILDdictionary (Sinclair et al 1987), it is now becoming more and more common to findlexicographers working directly with corpus data.
Sinclair makes an excellent case forthe use of corpus evidence in the preface to the COBUILD dictionary:For the first time, a dictionary has been compiled by the thoroughexamination of a representative group of English texts, spoken andwritten, running to many millions of words.
This means that in ad-dition to all the tools of the conventional dictionary makerswwidereading and experience of English, other dictionaries and of courseeyes and ears--this dictionary is based on hard, measurable evidence.
(Sinclair et al 1987; p. xv)The experience of writing the COBUILD dictionary is documented in Sinclair (1987),a collection of articles from the COBUILD project; see Boguraev (1990) for a strongpositive review of this collection.
At the time, the corpus-based approach to lexicogra-phy was considered pioneering, even somewhat controversial; today, quite a numberof the major lexicography houses are collecting large amounts of corpus data.The traditional alternative to corpora are citation indexes, boxes of interestingcitations collected on index cards by large numbers of human readers.
Unfortunately,citation indexes tend to be a bit like butterfly collections, full of rare and unusualspecimens, but severely lacking in ordinary, garden-variety moths.
Murray, the editorof the Oxford English Dictionary, complained:The editor or his assistants have to search for precious hours for exam-ples of common words, which readers passed by...
Thus, of Abusionwe found in the slips about 50 instances; of Abuse not five.
(James Au-gustus Henry Murray, Presidential Address, Philological Society Trans-actions 1877-9, pp.
571-2, quoted by Murray 1977, p. 178)He then went on to say, "There was not a single quotation for imaginable, a word usedby Chaucer, Sir Thomas More, and Milton."
From a statistical point of view, citationindexes have serious ampling problems; they tend to produce a sample that is heavilyskewed away from the "central and typical" facts of the language that every speakeris expected to know.
Large corpus studies, such as the COBUILD dictionary, offerthe hope that it might be possible to base a dictionary on a large and representativesample of the language as it is actually used.6.1 Should a Corpus Be Balanced?Ideally, we would like to use a large and representative sample of general anguage,but if we have to choose between large and representative, which is more important?There was a debate on a similar question between Prof. John Sinclair and Sir Randolf17Computational Linguistics Volume 19, Number 1Table 7Coverage of imaginable in various corpora.Size (in millions) Corpus raw freq freq/million1 Brown Corpus 0 01 Bible 0 02 Shakespeare 0 07 WSJ 41 5.910 Groliers 5 0.518 Hansard 15 0.829 DOE 5 0.246 AP 1988 36 0.850 AP 1989 39 0.856 AP 1990 21 0.447 AP 1991 19 0.4Quirk at the 1991 lexicography conference sponsored by Oxford University Press andWaterloo University, where the house voted, perhaps urprisingly, that a corpus doesnot need to be balanced.
Although the house was probably predisposed to side withQuirk's position, Sinclair was able to point out a number of serious problems with thebalancing position.
It may not be possible to properly balance a corpus.
And moreover,if we insist on throwing out idiosyncratic data, we may find it very difficult to collectany data at all, since all corpora have their quirks.In some sense, the question comes down to a tradeoff between quality and quan-tity.
American industrial laboratories (e.g., IBM, AT&T) tend to favor quantity, whereasthe BNC, NERC, and many dictionary publishers, especially in Europe, tend to favorquality.
The paper by Biber (1993) argues for quality, suggesting that we ought to usethe same kinds of sampling methods that statisticians use when studying the econ-omy or predicting the results of an election.
Poor sampling methods, inappropriateassumptions, and other statistical errors can produce misleading results: "There arelies, damn lies, and statistics.
"Unfortunately, sampling methods can be expensive; it is not clear whether we canjustify the expense for the kinds of applications that we have in mind.
Table 7 mightlend some support for the quantity position for Murray's example of imaginable.
Notethat there is plenty of evidence in the larger corpora, but not in the smaller ones.
Thus,it would appear that "more data are better data," at least for the purpose of findingexemplars of words like imaginable.Similar comments hold for collocation studies, as illustrated in Table 8, whichshows mutual information values (Fano 1961; p. 28)Pr(x~ y)I(x;y) = log 2 Pr(x) Pr(y)for several collocations in a number of different corpora.
Mutual information comparesthe probability of observing word x and word y together (the joint probability) to theprobability of observing x and y independently (chance).
Most of the mutual informa-tion values in Table 8 are much larger than zero, indicating, as we would hope, thatthe collocations appear much more often in these corpora than one would expect bychance.
The probabilities, Pr(x) and Pr(y), are estimated by counting the number ofobservations of x and y in a corpus, f(x) and f(y), respectively, and normalizing by N,18Kenneth W. Church and Robert L. Mercer IntroductionTable 8Collocations in different corpora.Size Corpus strong strong strong strong strong(in millions) support economy winds man enough1 Brown Corpus 5.1 (1) - -  8.3 (1) - -  7.3,1 Bible - -  - -  - -  3.4 (7) - -2 Shakespeare .
.
.
.
6.5 (4)7 WSJ 5.5 4.9 6.5 (7) 4.7 6.510 Groliers 5.8 3.8 (3) 8.3 4.2 (3) 8.318 Hansard 6.2 6.4 - -  - -  7.029 DOE 4.5 4.3 (4) 7.7 - -  7.446 AP 1988 6.3 6.3 8.5 4.0 7.050 AP 1989 6.3 4.7 8.4 1.8 (7) 7.356 AP 1990 6.5 3.7 8.3 2.4 (9) 7.547 AP 1991 7.0 3.4 8.7 2.0 (6) 7.3the size of the corpus.
The joint probability, Pr(x, y), is estimated by counting the num-ber of times that x is immediately followed by y in the corpus, f(x, y), and normalizingby N. Unfortunately, mutual  information values become unstable if the counts are toosmall.
For this reason, small counts (less than 10) are shown in parentheses.
A dash isused when there is no evidence for the collocation.Like Table 7, Table 8 also shows that "more data are better data."
That is, thereis plenty of evidence in the larger corpora, but not in the smaller ones.
"Only a largecorpus of natural language enables us to identify recurring patterns in the languageand to observe collocational and lexical restrictions accurately .
.
.
.  "
(Hanks 1990; p. 36)However, in order to make use of this evidence we have to find ways to com-pensate for the obvious problems of working with unbalanced ata.
For example, inthe Canadian Hansards, there are a number  of unwanted phrases such as: "House ofCommons,"  free trade agreement," "honour and duty to present," and "Hear!
Hear!
"Fortunately, though, it is extremely unlikely that these unwanted phrases will appearmuch more often than chance across a range of other corpora such as Departmentof Energy (DOE) abstracts or the Associated Press (AP) news.
If such a phrase wereto appear relatively often across a range of such diverse corpora, then it is probablyworthy of further investigation.
Thus, it is not required that the corpora be balanced,but rather that their quirks be uncorrelated across a range of different corpora.
Thisis a much weaker and more realistic requirement than the more standard (and moreidealistic) practice of balancing and purging quirks.6.2 Lexicography and Exploratory Data Analysis (EDA)Statistics can be used for many different purposes.
Traditionally, statistics such asStudent's t-tests were developed to test a particular hypothesis.
For example, supposethat we were concerned that strong enough shouldn't  be considered a collocation.
A t-test could be used to compare the hypothesis that strong enough appears too often to bea fluke against he null hypothesis that the observations can be attributed to chance.
Thet-score compares the two hypotheses, by taking the difference of the means of the twoprobabil ity distributions, and normalizing appropriately by the variances, so that theresult can be interpreted as a number  of standard eviations.
Theoretically, if the t-scoreis larger than 1.65 standard eviations, then we ought to believe that the co-occurrences19Computational Linguistics Volume 19, Number 1are significant and we can reject the null hypothesis with 95% confidence, though inpractice we might look for a t-score of 2 or more standard eviations, ince t-scoresare often inflated (due to certain violations of the assumptions behind the model).
SeeDunning (this issue) for a critique of the assumption that the probabilities are normallydistributed, and an alternative parameterization of the probability distributions.t = mean (Pr(strong, enough)) - mean(Pr(strong)) mean (Pr(enough))x/~r2(pr(strong, enough)) + cr2(pr(strong)Pr(enough))In the Brown Corpus, it happens that f(strong, enough) = 11, f(strong) = 194, f(enough)= 426, and N = 1,181,041.
Using these values, we estimate t ~ 3.3, which is largerthan 1.65, and therefore we can confidently reject he null hypothesis, and conclude thatthe co-occurrence is significantly larger than chance.
The estimation uses the approx-imation, cr2(pr(strong, enough)) ~ f(strong, enough)/N 2, which can be justified underappropriate binomial assumptions.
It is also assumed that cra(pr(strong) Pr(enough)) isvery small and can be omitted.f(stTonx,enouS h ) f(strong) f(~,ough)t~  N N N ~3.3Although statistics are often used to test a particular hypothesis as we have just seen,statistics can also be used to explore the space of possible hypotheses, or to discovernew hypotheses ( upervised/unsupervised learning/training).
See Tukey (1977) andMosteller and Tukey (1977) for two textbooks on Exploratory Data Analysis (EDA),and Jelinek (1985) for a very nice review paper on self-organizing statistics.
Both theexploratory and self-organizing views are represented in these special issues.
Puste-jovsky et al (1993) use an EDA approach to investigate certain questions in lexicalsemantics.
Brent (1993), in contrast, adopts a self-organizing approach to identify sub-categorization features.Table 9 shows how the t-score can be used in an exploratory mode to extract largenumbers of words from the Associated Press (AP) news that co-occur more often withstrong than with powerful, and vice versa.
It is an interesting question whether collo-cations are simply idiosyncratic as Halliday and many others have generally assumed(see Smadja \[this issue\]), or whether there might be some general principles that couldaccount for many of the cases.
After looking at Table 9, Hanks, a lexicographer andone of the authors of Church et al (1991), hypothesized that strong is an intrinsicquality whereas powerful is an extrinsic one.
Thus, for example, any worthwhile politi-cian or cause can expect strong supporters, who are enthusiastic, onvinced, vociferous,etc., but far more valuable are powerful supporters, who will bring others with them.They are also, according to the AP news, much rarer--or at any rate, much less oftenmentioned.
This is a fascinating hypothesis that deserves further investigation.Summary statistics uch as mutual information and t-scores may have an impor-tant role to play in helping lexicographers to discover significant patterns of collo-cations, though the position remains somewhat controversial.
Some lexicographersprefer mutual information, some prefer t-scores, and some are unconvinced that ei-ther of them is any good.
Church et al (1991) argued that different statistics havedifferent strengths and weaknesses, and that it requires human judgment and explo-ration to decide which statistic is best for a particular problem.
Others, such as Jelinek(1985), would prefer a self-organizing approach, where there is no need for humanjudgment.20Kenneth W. Church and Robert L. Mercer IntroductionTable 9An example of the t-score (Church et al 1991).Strong w Powerful wt strong w powerful w w t strong w powerfulw w12.42 161 0 showing -7.44 1 56 than11.94 175 2 support -5.60 1 32 figure10.08 550 68 , -5.37 3 31 minority9.97 106 0 defense -5.23 1 28 of9.76 102 0 economy -4.91 0 24 post9.50 97 0 demand -4.63 5 25 new9.40 95 0 gains -4.35 27 36 military9.18 91 0 growth -3.89 0 15 figures8.84 137 5 winds -3.59 6 17 presidency8.02 83 1 opposition -3.57 27 29 political7.78 67 0 sales -3.33 0 11 computers7.
Conc lus ionThe flourishing renaissance ofempiricism in computational linguistics grew out of theexperience of the speech recognition community during the 1970s and 1980s.
Many ofthe same statistical techniques (e.g., Shannon's Noisy Channel Model, n-gram mod-els, hidden Markov models (HMMs), entropy (H), mutual information (I), Student'st-score) have appeared in one form or another, often first in speech, and then soonthereafter in language.
Many of the same researchers have applied these methods toa variety of application areas ranging from language modeling for noisy channel ap-plications (e.g., speech recognition, optical character recognition \[OCR\], and spellingcorrection \[Damerau 1964; Kukich 1992\]), to part-of-speech tagging, parsing, transla-tion, lexicography, text compression (Bell, Cleary, and Witten 1990) and informationretrieval (IR) (Salton 1989; Frakes and Baeza-Yates 1992).Empiricism is, of course, a very old tradition.
Back in the 1950s and 1960s, longbefore the speech work of the 1970s and 1980s, there was Skinner's Behaviorism inPsychology, Shannon's Information Theory in Electrical Engineering, and Harris' Dis-tributional Hypothesis in American Linguistics and the Firthian approach in BritishLinguistics ("You shall know a word by the company it keeps").
It is possible thatmuch of this work was actually inspired by Turing's code-breaking efforts duringWorld War II, but we may never know for sure given the necessity for secrecy.The recent revival in empiricism has been fueled by three developments.
Firstcomputers are much more powerful and more available than they were in the 1950swhen empiricist ideas were first applied to problems in language, or in the 1970s and1980s, when data-intensive methods were too expensive for researchers working inuniversities.
Second, data have become much more available than ever before.
As aresult of a number of data collection and related efforts uch as ACL/DCI, BNC, CLR,ECI, EDR, LDC, ICAME, NERC, and TEI, most researchers should now be able tomake use of a number of very respectable machine-readable dictionaries (MRDs) andtext corpora.
(See footnote 1 for information on contacting many of these organiza-tions.)
Data-intensive methods are no longer restricted to those working in affluentindustrial laboratories.
Third, and perhaps most importantly, due to various politicaland economic hanges around the world, there is a greater emphasis these days on21Computational Linguistics Volume 19, Number 1deliverables and evaluation.
Data collection efforts have been relatively successful inresponding to these pressures by delivering massive quantities of data.
Text Analysishas also prospered because of its tradition of evaluating performance with theoreticallymotivated numerical measures uch as entropy.ReferencesAyuso, D.; Bobrow, R.; MacLaughlin, D.;Meteer, M.; Ramshaw, L.; Schwartz, R.;and Weischedel, R. (1990).
"Towardunderstanding text with a very largevocabulary."
DARPA Speech and NaturalLanguage (,Vorkshop.
San Mateo, CA,354-358.
Morgan Kaufmann.Bahl, L.; Baker, J.; Cohen, P.; Dixon, N.;Jelinek, E; Mercer, R.; and Silverman, H.(1975).
"Preliminary results in theperformance of a system for theautomatic recognition of continuousspeech."
IBM Technical Report #RC 5654.Bahl, L.; Baker, J.; Cohen, P.; Jelinek, E;Lewis, B.; and Mercer, R.
(1978).
"Recognition of a continuously readnatural corpus."
In Proceedings, IEEEInternational Conference on Acoustics, Speechand Signal Processing (ICASSP).Bahl, L., and Mercer, R. (1976).
"Part ofspeech assignment by a statisticaldecision algorithm."
In Abstracts of Papersfrom the International Symposium onInformation Theory.Baker, J.
(1979).
"Trainable grammars forspeech recognition."
In SpeechCommunication Papers for the 97th Meeting ofthe Acoustical Society of America, edited byKlatt and Wolf, 547-550.Bakis, R. (1976).
"Continuous peechrecognition via centisecond acousticstates."
In Proceedings of91st Meeting of theAcoustic Society of America.Baum, L. (1972).
"An inequality andassociated maximization technique instatistical estimation of probabilisticfunctions of a Markov process.
"Inequalities, 3 1-8.Bell, T.; Cleary, J.; and Witten, I.
(1990).
TextCompression.
Prentice Hall.Biber, D. (1993).
"Representativeness incorpus design."
Computational Linguistics,19(2).
In press.Boggess, L.; Agarwal, R.; and Davis, R.(1991).
"Disambiguation f prepositionalphrases in automatically labelledtechnical text."
AAAI, 155-159.Boguraev, B.
(1990).
"Looking Up: anaccount of the COBUILD project in lexicalcomputing."
Computational Linguistics,16(3), 184-185.Brent, M. (1993).
"Robust acquisition ofsubcategorization features from arbitrarytext: syntactic knowledge meetsunsupervised learning."
ComputationalLinguistics, 19(2).
In press.Brown, P.; Cocke, J.; Della Pietra, S.; DellaPietra, V.; Jelinek, F.; Lafferty, J.; Mercer,R.
; and Rossin, P. (1990).
"A statisticalapproach to machine translation.
"Computational Linguistics, 16(2), 79-85.Brown, P.; Della Pietra, S.; Della Pietra, V.;Lai, J.; and Mercer, R. (1992).
"Anestimate of an upper bound for theentropy of English."
ComputationalLinguistics, 18(1), 31--40.Brown, P.; Della Pietra, S.; Della Pietra, V.;Mercer, R. (1993).
"The mathematics ofmachine translation."
ComputationalLinguistics, 19(2).
In press.Chomsky, N. (1957).
Syntactic Structures.Mouton.Church, K. (1988).
"A stochastic partsprogram and noun phrase parser forunrestricted text."
In Proceedings, SecondConference on Applied Natural LanguageProcessing.
(ACL), Austin, TX, 136-143.Church, K.; Hanks, P.; Hindle, D.; and Gale,W.
(1991).
"Using statistics in lexicalanalysis."
In Lexical Acquisition: UsingOn-Line Resources toBuild a Lexicon, editedby Zernik.
Lawrence Erlbaum, 115-164.Church, K., and Gale, W.
(1991).
"Concordances for parallel text."
InProceedings, Seventh Annual Conference oftheUW Centre for the New Oxford EnglishDictionary and Text Research.Cohen, P., and Mercer, R. (1974).
"Thephonological component of an automaticspeech-recognition system."
In SpeechRecognition, Invited Papers Presented atthe 1974 IEEE Symposium, edited byR.
Reddy, 275-320.Cover, T., and Thomas, J.
(1991).
Elements ofInformation Theory.
John Wiley & Sons.Croft, W.; Turtle, H.; and Lewis, D.
(1991).
"The use of phrases and structuredqueries in information retrieval."
In SIGIRForum, edited by A. Bookstein,Y.
Chiaramella, G. Salton, andV.
Raghavan, 32-45.Damerau, F. (1964).
"A technique forcomputer detection and correction ofspelling errors."
Communications of theACM, 7(3), 171-176.22Kenneth W. Church and Robert L. Mercer IntroductiondeMarcken, C. (1990).
"Parsing the LOBcorpus."
Association for ComputationalLinguistics, 243-251.DeRose, S. (1988).
"Grammatical categorydisambiguation by statisticaloptimization."
Computational Linguistics,14(1), 31-39.Deroualt, A., and Merialdo, B.
(1986).
"Natural anguage modeling forphoneme-to-text transcription."
IEEETransactions on Pattern Analysis and MachineIntelligence PAMI-8(6), 742-749.Fano, R. (1961).
Transmission ofInformation.MIT.
Press.Firth, J.
(1957).
"A synopsis of linguistictheory 1930-1955."
In Studies in LinguisticAnalysis, Philological Society, Oxford;reprinted in Selected Papers of J. R. Firth,edited by E Palmer.
Longman.
1968.Ford, M.; Bresnan, J.; and Kaplan, R.
(1982).
"A competence based theory of syntacticclosure."
In The Mental Representation fGrammatical Relations, edited byJ.
Bresnan.
MIT Press, 727-796.Frakes, W., and Baeza-Yates, R., eds.
(1992).Information Retrieval: Data Structures andAlgorithms.
Prentice Hall.Francis, W., and Ku~era, H. (1982).Frequency Analysis of English Usage,Houghton Mifflin.Furui, S. (1989).
Digital Speech Processing,Synthesis, and Recognition.
Marcel Dekker.Garside, R.; Leech, G.; and Sampson, G.(1987).
The Computational Analysis ofEnglish.
Longman.Good, I.
(1953).
"The population frequenciesof species and the estimation ofpopulation parameters."
Biometrika, 40,237-264.Halliday, M. (1966).
"Lexis as a linguisticlevel," In In Memory of J. R. Firth, edited byC.
Bazell, J. Catford, M. Halliday, andR.
Robins.
Longman.Hanks, P. (1990).
"Evidence a'nd intuition inlexicography."
In Meaning andLexicography, edited by J. Tomaszczyk andB.
Lewandowska-Tomaszczyk, 31-41.John Benjamins Publishing Company.Hearst, M. (1991).
"Toward noun homonymdisambiguation using local context inlarge text corpora."
In Proceedings, SeventhAnnual Conference ofthe UW Centre for theNew OED and Text Research.
University ofWaterloo, Waterloo, Ontario, 1-22.Henisz-Dostert, B.; Ross Macdonald, R.; andZarechnak, M., eds.
(1979).
MachineTranslation.
Mouton.Hindle, D. (1989).
"Acquiringdisambiguation rules from text."
ACL,118-125.Isabelle, P. (1992).
"Bi-textual aids fortranslators."
In Proceedings, Eighth AnnualConference ofthe UW Centre for the NewOED and Text Research.
University ofWaterloo, Waterloo, Ontario, 76-89.Jelinek, E (1985).
"Self-organized languagemodeling for speech recognition."
IBMReport.
Reprinted in W & L, 450-506.Jelinek, E, and Mercer, R.
(1980).
"Interpolated estimation of Markovsource parameters from sparse data."
InProceedings, Workshop on Pattern Recognitionin Practice.
North-Holland.Jelinek, F., and Mercer, R.
(1985).
"Probability distribution estimation fromsparse data."
IBM Technical DisclosureBulletin, 28, 2591-2594.Jelinek, E; Mercer, R.; and Roukos, S.
(1991).
"Principles of lexical anguage modelingfor speech recognition."
In Advances inSpeech Signal Processing, edited by S. Furuiand M. Mohan.
Marcel Dekker, 651-700.Karlsson, E (1990).
"Constraint grammar asa framework for parsing running text."
InProceedings, 15th International Conference onComputational Linguistics (COLING-90),168-173.Klavans, J., and Tzoukermann, E.
(1990a).
"The BICORD system."
In Proceedings,15th International Conference onComputational Linguistics (COLING-90),Helsinki, Finland, 174-179.Klavans, J., and Tzoukermann, E.
(1990b).
"Linking bilingual corpora nd machinereadable dictionaries with the BICORDsystem."
In Proceedings, Sixth AnnualConference ofthe UW Centre for the NewOxford English Dictionary and Text Research,19-30.Klatt, D. (1977).
"Review of the ARPAspeech understanding project."
Journal ofthe Acoustical Society of America.
Reprintedin W & L, 554-575.Klatt, D. (1980).
"Scriber and lafs: Two newapproaches tospeech analysis."
In Trendsin Speech Recognition, edited by W. Lea.Prentice-Hall.Kukich, K. (1992).
"Techniques forautomatically correcting words in text.
"ACM Computing Surveys, 24(4), 377-439.Kupiec, J.
(1989).
"Augmenting a hiddenMarkov model for phrase-dependentword tagging."
DARPA Speech and NaturalLanguage Workshop, San Mateo, CA, 92-98.Morgan Kaufmann.Kupiec, J.
(1992).
"Robust part-of-speechtagging using a hidden Markov model.
"Computer Speech and Language, 6, 225-242.Lari, K., and Young, S. (1991).
"Applicationsof stochastic context-free grammars usingthe inside-outside algorithm."
ComputerSpeech and Language, 237-258.23Computational Linguistics Volume 19, Number 1Leech, G.; Garside, R.; and Atwell, E.
(1983).
"The automatic grammatical tagging ofthe LOB corpus."
ICAME News 7, 13-33.Leonard, R. (1984).
"A database forspeaker-independent digit recognition.
"Proceedings ofthe IEEE InternationalConference on Acoustics, Speech and SignalProcessing (ICASSP), 3, 1-4.Liberman, M., and Church, K. (1991).
"Textanalysis and word pronunciation itext-to-speech synthesis."
In Advances inSpeech Signal Processing, edited by S. Furuiand M. Mohan.
Marcel Dekker, 791-832.Merialdo, B.
(1991).
"Tagging text with aprobabilistic model."
IEEE InternationalConference on Acoustics, Speech and SignalProcessing (ICASSP), 809-812.Meyer, D.; Schvaneveldt, R.; and Ruddy, M.(1975).
"Loci of contextual effects onvisual word-recognition."
In Attention andPerformance V, edited by P. Rabbitt andS.
Dornie.
Academic Press, 98-116.Minsky, M., and Papert, S. (1969).Perceptrons; An Introduction toComputational Geometry.
MIT Press.Mish, F., ed.
(1983).
Webster's Ninth NewCollegiate Dictionary.
Merriam, Webster.Mosteller, E, and Tukey, J.
(1977).
DataAnalysis and Regression.
Addison-Wesley.Mosteller, Fredrick, and Wallace, David(1964).
Inference and Disputed Authorship:The Federalist.
Addison-Wesley.Murray, K. (1977).
Caught in the Web of Words:James Murray and the Oxford EnglishDictionary.
Yale University Press.Palermo, D., and Jenkins, J.
(1964).
WordAssociation Norms.
University ofMinnesota Press.Price, P.; Fisher, W.; Bernstein, J.; and Pallett,D.
(1988).
"The DARPA 1000-wordresource management database forcontinuous speech recognition."
InProceedings, IEEE International Conference onAcoustics, Speech and Signal Processing(ICASSP), 1, 651-654.Pustejovsky, J.; Berger, S.; and Anick, P.(1993).
"Lexical semantic techniques forcorpus analysis."
ComputationalLinguistics, 19(2).
In press.Rabiner, L. (1989).
"A tutorial on hiddenMarkov models and selected applicationsin speech recognition."
In Proceedings,IEEE, 77(2), 257-286.
Reprinted in W & L,267-296.Salton, G. (1989).
Automatic Text Processing.Addison-Wesley.Salton, G.; Zhao, Z.; and Buckley, C.
(1990).
"A simple syntactic approach for thegeneration of indexing phrases.
"Technical Report 90-1137, Department ofComputer Science, Cornell University.Sato, S., and Nagao, M. (1990).
"Towardsmemory based translation."
InProceedings, 15th International Conference onComputational Linguistics (COLING-90),247-252.Shannon, C. (1948).
"The mathematicaltheory of communication."
Bell SystemTechnical Journal, 27, 398-403.Shannon, C. (1951).
"Prediction and entropyof printed English."
Bell Systems TechnicalJournal, 30, 50-64.Sinclair, J.; Hanks, P.; Fox, G.; Moon, R.; andStock, P., eds.
(1987).
Collins COBUILDEnglish Language Dictionary.
Collins.Sinclair, J., ed.
(1987).
Looking Up: An Accountof the COBUILD Project in LexicalComputing.
Collins.Tukey, J.
(1977).
Exploratory Data Analysis.Addison-Wesley.Voutilainen, A.; Heikkila, J.; and Anttila, A.(1992).
"Constraint grammar of English:A performance-oriented introduction.
"Publication No.
21, University ofHelsinki, Department of Linguistics,Helsinki, Finland.Waibel, A., and Lee, K., eds.
(1990).
Readingsin Speech Recognition.
Morgan Kaufmann.Warwick-Armstrong, S. and Russell, G.(1990).
"Bilingual concordancing andbilingual exicography."
Euralex 1990.Weaver, W. (1949).
"Translation.
"Reproduced in Machine Translation ofLanguages, edited in 1955 by W. Lockeand A. Booth.
MIT Press, 15-23.Welch, T. (1984).
"A technique for highperformance data compression.
"Computer, 17(6), 8-19.Woods, W. (1970).
"Transition etworks fornatural anguage analysis."
CACM,13(10), 591-606.24
