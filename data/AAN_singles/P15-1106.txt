Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1095?1105,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDriving ROVER with Segment-based ASR Quality EstimationShahab Jalalvand(1,2), Matteo Negri(1), Daniele Falavigna(1), Marco Turchi(1)(1)FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy(2)University of Trento, Italy{jalalvand,negri,falavi,turchi}@fbk.euAbstractROVER is a widely used method tocombine the output of multiple auto-matic speech recognition (ASR) systems.Though effective, the basic approach andits variants suffer from potential draw-backs: i) their results depend on the orderin which the hypotheses are used to feedthe combination process, ii) when appliedto combine long hypotheses, they disre-gard possible differences in transcriptionquality at local level, iii) they often rely onword confidence information.
We addressthese issues by proposing a segment-basedROVER in which hypothesis ranking isobtained from a confidence-independentASR quality estimation method.
Our re-sults on English data from the IWSLT2012and IWSLT2013 evaluation campaignssignificantly outperform standard ROVERand approximate two strong oracles.1 IntroductionIn automatic speech recognition (ASR), the com-bination of transcription hypotheses produced bymultiple systems usually leads to significant worderror rate (WER) reductions compared to the out-put of each individual system.
Systems?
diversityand complementarity have been exploited in dif-ferent ways to synthetically obtain more accuratetranscriptions.
Recognizer output voting error re-duction ?
ROVER (Fiscus, 1997), the most widelyused method, performs hypothesis fusion in twosteps.
First, the 1-best transcriptions from multi-ple systems are aligned by means of dynamic pro-gramming to build a single, minimal word tran-sition network.
Then, the resulting network issearched to select the best scoring word at eachnode.
The final hypothesis is constructed via a ma-jority voting mechanism and, if available, by usingword confidence measures.This general strategy has been improved in sev-eral ways but, despite their proven effectiveness,ROVER and its variants have three potential draw-backs.
The first one is intrinsic to their implemen-tation: the fusion process starts from one of theinput hypotheses, which is used as ?skeleton?
forthe greedy alignment of the others.
The order inwhich the hypotheses are used to feed the processcan hence determine significant variations in theWER of the resulting combination.
This calls forautomatic methods for ranking the hypothesesto initialise and carry on the fusion process.The second drawback is inherent to the wayROVER is usually run: the fusion process is typi-cally fed with transcriptions of entire audio record-ings (lasting up to hours).
With this level of granu-larity, the skeleton used as basis for the alignmentmay consist of long transcriptions whose qualitycan considerably vary at local level.
For instance,the worst transcription of an entire audio recording(globally) could be the best one for some passages(locally).
This calls for solutions capable to op-erate at higher granularity levels (e.g.
segmentslasting up to few seconds) to better exploit thelocal diversity of the combined transcriptions.The third drawback relates to the applicabilityof ROVER-like fusion methods: their commontrait is the reliance on information about the in-ner workings of the combined systems.
Indeed,the standard voting scheme with confidence scoresis usually much more reliable than the simplerfrequency-based voting.
The access to confidencescores, however, is a too rigid constraint in ap-plication scenarios where the hypotheses to becombined come from unknown (?black-box?)
sys-tems.1This calls for confidence-independent fu-sion methods.1One example, among the many possible ones, is the sce-nario in which an array of microphones (e.g.
in a room or avehicle) sends input to one or more commercial ASR systemswhich do not provide confidence information.1095L3 L4 L5 L6 L7 L8SysO 12.2 11.7 11.8 11.9 12.1 12.1InSysO 19.8 16.6 15.1 13.9 13.4 13.3SegO 10.5 11.0 11.4 11.6 11.7 11.7InSegO 22.9 19.6 17.4 15.8 14.4 13.0Table 1: Motivation: the influence of hypothesisorder and granularity on standard ROVER results.The impact of the first two issues is evident fromthe figures provided in Table 1.
The results refer tothe WER achieved by different ?oracles?
obtainedfrom the output of eight ASR systems that partici-pated in the IWSLT2013 campaign (Cettolo et al,2013).2Such oracles combine:?
Different numbers of transcriptions (fromthree ?
L3 to eight ?
L8);?
At different granularity levels (whole utter-ance ?
SysO and segment ?
SegO);?
In different orders (best to worst ?
SysO,SegO and inverse ?
InSysO, InSegO).As shown in the table, the gap betweenutterance-based (SysO) and segment-based(SegO) is evident at all levels: WER differencesvary from 0.3 (11.9 vs. 11.6 at L6) to 1.7 points(12.2 vs. 10.5 at L3).
Another gap is evidentbetween best-to-worst and inverse rankings,with WER differences up to 7.6 points at wholeutterance level (SysO vs. InSysO at L3) and12.4 points at segment level (SegO vs. InSegOat L3).
Another interesting observation is thattop results (i.e.
lower WER) are obtained whencombining a subset of the outputs (respectivelyfour at utterance level and three at segment level).Referring to this analysis, the goal of computingROVER based on hypothesis ranking at highergranularity levels is well motivated.A crucial need to achieve this goal is the avail-ability of a confidence-independent method to pre-dict the quality of ASR transcriptions at segmentlevel.
This ?quality estimation?
(QE) task hasbeen recently addressed in (Negri et al, 2014;C. de Souza et al, 2015) as a supervised regres-sion problem in which transcriptions?
WER is pre-dicted without having access to reference tran-scripts.3Different feature sets have been evalu-ated, showing that even with those extracted only2Details about this dataset will be provided in Section 6.1.3This formulation is very similar to the machine transla-tion counterpart of the task (Specia et al, 2009; Mehdad etal., 2012; Turchi et al, 2014; C. de Souza et al, 2014).from the signal and the transcription (i.e.
disre-garding information about the decoding process)the prediction error is sufficiently low to open toreal applications.
However, though promising, ex-perimental results stem from an intrinsic evalua-tion in which QE is only addressed in isolation.By applying it to inform ROVER, we pro-pose for the first time an application-oriented ex-trinsic evaluation of ASR QE (our first contri-bution).
To this aim, we extend previous ASRQE methods with new features (second contribu-tion), and report significant improvements overstandard ROVER on a shared dataset (third con-tribution).
For the sake of brevity, our compar-ison is performed only against standard ROVERand in ?black-box?
conditions.
However it?s worthremarking that our approach can be straightfor-wardly applied to any ROVER-like variant and, ifavailable, by exploiting confidence features.2 Related workThis paper gathers three main research strands to-gether: ASR system combination, ASR quality es-timation and machine-learned ranking.Fiscus (1997) proposed ROVER as an approachto produce a composite ASR output.
The basic ap-proach has been extended in several ways.
N-BestROVER (Stolcke et al, 2000) improves the orig-inal method by combining multiple alternativesfrom each combined system.
Schwenk and Gau-vain (2000) exploit a secondary language modelto rescore the final n-best hypotheses generated byROVER.
iROVER (Hillard et al, 2007) exploits aclassifier to choose the system that is most likely tobe correct at each word location.
cROVER (Abidaet al, 2011) integrates a semantic pre-filtering stepin which the word transition network is scanned toflag and eliminate erroneous words to facilitate thevoting.
Other approaches to ASR system combi-nation make use of word lattices or confusion net-works (Mangu, 2000; Li et al, 2002; Evermannand Woodland, 2000; Hoffmeister et al, 2006;Bougares et al, 2013, inter alia).
Note that allthese combination methods require to have accessto the inner structure of the ASR decoder, whileASR systems, especially the commercial ones, of-ten do not provide this information.ASR quality estimation allows us to overcomethis problem and obtain confidence-independentestimates of ASR output quality.
Based on thepositive intrinsic evaluation results reported in1096(Negri et al, 2014; C. de Souza et al, 2015), herewe extend the approach with new features and per-form an extrinsic evaluation in a real applicationscenario.
Our new features are inspired by re-search on ASR error detection at word level (Gold-water et al, 2010; Pellegrini and Trancoso, 2010).Machine-learned ranking (MLR) or learning torank (Hang, 2011) is widely used in informationretrieval to order the answers to a user?s query(Cao et al, 2007; McFee and Lanckriet, 2010;McSherry and Najork, 2008).
We use it to orderthe transcription hypotheses produced by multipleASR systems and feed ROVER with the resultingranked lists.3 MethodGiven an utterance and a set of M transcriptionhypotheses produced by M different (possibly un-known) ASR systems, our goal is to:1.
Split the utterance into segments (ideally atsentence level);2.
For each segment, automatically estimate thequality (e.g.
in terms of WER) of the corre-sponding M (segment-level) hypotheses;3.
Use the estimates to rank the hypotheses andfeed ROVER based on the ranking;4.
Reconstruct the entire utterance transcrip-tion by concatenating the combined segment-level transcriptions produced by ROVER;5.
Measure the overall WER differences againststandard ROVER and other oracles.Step 1 is performed by a start-end point detectionmodule based on signal energy, which is followedby a segment classification module based on Gaus-sian Mixture Models similar to (Cettolo and Fed-erico, 2000).
Although the comparison with al-ternative splitting methods might lead to differentresults, this is not the main focus of the paper andis left as future work.
Steps 2?4, instead, repre-sent the core of our contribution and are describedin the following sections.4 Segment-based QE-informed ROVERROVER uses iterative dynamic programming tobuild a word transition network (WTN) from mul-tiple ASR output hypotheses.
The resulting WTNcan be seen as a confusion network with an equalnumber of word arc hypotheses (one for each ASRsystem entering the combination) in each corre-spondence slot.
The best word sequence is deter-mined from the WTN via majority voting amongthe words in each slot.
Most of the extensions ofROVER, such as iROVER (Hillard et al, 2007),cROVER (Abida et al, 2011) and the one de-scribed in (Zhang and Rudnicky, 2006), aim tolearn a scoring function that allows improving thereordering of words inside each slot.
In particu-lar, iROVER reorders the words in each slot bymeans of a classifier trained with features thatcharacterize the individual ASR systems.
This ap-proach, however, needs first to properly normal-ize the word lattices generated by each system, inorder to exhibit the same vocabulary and similardensities, and to generate a unified segmentationfor joining the lattices.In a similar way, motivated by the analysisshown in Table 1, our method applies reorderingof the ASR hypotheses at segment level.
However,differently from iROVER, it does not require toaccess the inner components of the decoders (e.g.word lattices or word confidences), nor to applypre-processing steps that can distort the outputs ofindividual ASR components.this is my mobile A mobile phone can change Thank you???????????????????????????????
?concat(SRV)T1A T2A TnAT1B T2B TnBT1C T2C TnCSRV(T1A, T1B, T1C) SRV(T2A, T2C, T2B) SRV(TnB, TnA, TnC)RV(T 1..nA ,T 1..nB ,T 1..nC )Figure 1: Segment-based ROVERFigure 1 illustrates the difference between stan-dard ROVER (RV, shown at the rightmost verti-cal) which works at the utterance level (lastingup to few hours) and the segment-based ROVER(SRV, shown at the bottom horizontal) that worksat the segment level (lasting up to few seconds).RV keeps the order of the systems static alongthe whole utterance (A  B  C, i.e.
systemA has generated a better transcription than sys-tem B which, in turn is better than system C) forall the segments RV (TA1..n, TB1..n, TC1..n).
SRV, in-stead, dynamically changes the system order from1097one segment to the other.
For example, the systemorder for the first segment is A  B  C, whilefor the next segment it is A  C  B.
Our hy-pothesis is that, with a proper segment-based rank-ing, SRV will result in lower WER scores than RV.Note that, as depicted in Figure 1, segment-based ROVER requires that all the ASR systemsshare a common segmentation.
This is easy to ob-tain by force-aligning the transcriptions of eachsystem with a given segmentation (e.g.
one ran-domly chosen among those employed by eachASR system).In this paper we approach segment-level ASRQE as a supervised learning task, by comparingtwo alternative strategies: ranking by regression(Section 4.1) and machine-learned ranking (Sec-tion 4.2).
Both methods rely on the features usedin (Negri et al, 2014), extended with a new set ofword-level features described in Section 5.4.1 Ranking by regression (RR)The first ranking strategy is based on training a re-gressor on a set of (signal, transcription, WER)triples, and use it to predict the WER score fornew, unseen (signal, transcription) test instances.Then, based on the predicted WERs, a ranked listis produced for each segment to feed ROVER.To train the regressor, we are given N seg-ments (Si, 1 ?
i ?
N ), their automatic transcrip-tions ({T1i.
.
.
TMi}i=Ni=1) produced by M ASR sys-tems, and manual references from which the trueWERs ({TW1i.
.
.
TWMi}i=Ni=1) can be computedfor each segment i.
The whole set of train-ing data is hence represented by instances: I ={(Si, Tji, TWji), 1 ?
j ?
M, 1 ?
i ?
N }.Training is performed with two alternative strate-gies, which differ in the amount of training dataused.
The first one, RR1, employs the whole train-ing set I .
The second one, RR2, uses only onetranscription for each segment, randomly chosenfrom the M available.
In this case, the training setbecomes: I?= {(Si, Tji, TWji), 1 ?
i ?
N, j =rnd(M)} where rnd(M) is a random number be-tween 1 to M .
In practice, RR2 learns from asmaller but more diverse training set compared toRR1.
On the one side, in fact, RR1 deals witha larger number of training instances (M timesmore), but the feature vectors will share the samevalues for the features extracted from the signal ofeach utterance.
On the other side, RR2 reduces thesize of the training set I?down to1Mof I , but onlyone feature vector is extracted for each utterance.The unpredictable effect of such differences on QEresults motivates experiments with both methods.4.2 Machine-learned ranking (MLR)The second strategy relies on directly traininga ranking model from a set of instances I ={(Si, Tji, TRji), 1 ?
i ?
N, 1 ?
j ?
M }, whereSiand Tjirespectively represent segments andtranscriptions, and TRjirepresents ?true ranks?computed from the corresponding reference WERvalues TWji.
That is, given two transcriptions, Tjiand Tkiand the true WERs, then TRji TRki, ifTWji?
TWki.It is worth to note that MLR, differently fromthe two regression methods described above, per-forms a pairwise comparison between the seg-ment candidates.
That is, for each pair of seg-ment transcriptions, the algorithm processes theircorresponding feature vectors against each otherand decides to place one transcription ahead of theother, as long as returning a score for this decision.Based on this score, the algorithm is then able torank more than two candidates.5 FeaturesWe use two sets of features.
One consists of thebasic features described in (Negri et al, 2014); theother includes several word-based features specif-ically introduced for our ranking task.5.1 Basic featuresBasic features can be further divided in threegroups:Signal features (16 in total) aim to capture thedifficulty to transcribe a given input by lookingat the signal as a whole.
They are obtained byanalyzing the audio waveform with a window of20ms at a frame rate of 10ms.
For each analysedwindow, 12 Mel Frequency Cepstral Coefficients(MFCCs) are evaluated (MFCC of order 0 is dis-carded) plus log energy.
Then, to form the signalfeature vector for each given segment, we com-pute the mean/min/max values of raw energy, aswell as the mean MFCCs values and total segmentduration.Hybrid features (26) provide a more fine-grained way to capture the difficulty of transcrib-ing the signal.
They are computed based on1098the forced alignment between the M given auto-matic transcriptions of each segment and the cor-responding acoustic observations obtained fromraw features.
For each transcription hypothesishybrid features are: signal to noise ratio (SNR),mean/min/max noise energy, mean/min/max wordenergy, (max word - min noise) energy, number ofsilences (#sil), #sil per second, number of words(#wrd) per second,#sil#wrd, total duration of words(Dwrd), total duration of silences (Dsil), mean du-ration of words, mean duration of silences,DsilDwrd,Dwrd?Dsil, standard deviation (std) of word du-ration, std of silence duration, mean/std/min/maxof pitch4, number of hesitations, frequency of hes-itations.Textual features (10) aim to capture the plausi-bility (i.e.
the fluency) of a transcription.
For eachhypothesis textual features are: number of words,LM log probability, LM log probability of part ofspeech (POS), log perplexity, LM log perplexityof POS, percentage (%) of numbers, % of tokenswhich do not contain only ?
[a-z]?, % of contentwords, % of nouns, % of verbs.5.2 Word-based featuresTo compensate the absence of ASR confidence in-formation, we also designed a set of ?word-based?features inspired by previous approaches to ASRerror detection (Chieu and Ng, 2002; Pellegriniand Trancoso, 2010; Goldwater et al, 2010; Tamet al, 2014).
They aim to capture words?
pronun-ciation difficulty, which is determined by the num-ber of lexical neighbors (similar pronunciations)and the types of phonemes that form the words.From the ASR error detection field we also borrowadditional language model features based on re-current neural network language model (RNNLM)probability (Mikolov et al, 2010).Word-based features (22) are: POS tag andscore of the previous/current/next words (6),RNNLM probabilities (2) given by modelstrained on in-domain and out-of-domain data, in-domain/out-of-domain 4-gram LM probability (2),number of phoneme classes (including fricatives,liquids, nasals, stops and vowels) (5), number ofhomophones (1), number of lexical neighbors (1)and binary features answering the three questions:?is stop word??
(1), ?is before/after repetition?
?4Pitch features have been computed with the Praat soft-ware tool (Boersma and Weenink, 2005).Dataset duration sent token voc talkstst2012 1h45m 1,124 19.2k 2.8k 11tst2013 4h50m 2,246 41.6k 5.6k 28Table 2: Dataset statistics: duration, number ofsentences, number of tokens, vocabulary size,number of talks.System tst2012 tst2013FBK 16.8 23.2KIT 12.7 14.4MITLL 13.3 15.9NAIST ?
16.2NICT 12.4 13.5PRKE ?
27.2RWTH 13.6 16.0UEDIN 14.4 22.1Table 3: Official WER[%] scores of the partic-ipants in the IWSLT2012 and IWSLT2013 ASRevaluations.
(2), ?is before/after silence??
(2).
Since the ASRhypotheses of a given segment might contain dif-ferent numbers of words, we average the values ofthe word-based features for each hypothesis.6 Experimental setupIn this section we illustrate the audio data used inour experiments, the methods used to inform andrun ROVER, the evaluation metric and the signifi-cance testing method applied.6.1 DataWe experiment with two sets of speech recordingscollected from English TED talks and used forthe 2012 (IWSLT2012) and 2013 (IWSLT2013)editions of the International Workshop on Spo-ken Language Translation (Federico et al, 2012;Cettolo et al, 2013).
Statistics for both datasetsare shown in Table 2.
Six teams participatedin the 2012 evaluation: FBK, KIT, MITLL,NICT, RWTH and UEDIN.
Two more competi-tors, NAIST and PRKE, took part in the 2013 edi-tion of the campaign.
The related WERs are re-ported in Table 3.
For detailed system descrip-tions we refer the reader to the IWSLT20125andIWSLT20136proceedings.In the experiments, we used tst2012 for train-ing with 4-fold cross-validation, and tst2013 fortesting purposes.
Note that cross-validation wasapplied ensuring that a given speaker does not ap-5http://workshop2012.iwslt.org6http://workshop2013.iwslt.org1099pear simultaneously both in the training and vali-dation sets.
The same condition holds for the testset: speakers in tst2012 do not occur in tst2013.These conditions, and the use of two differentsets of talks (acquired in different IWSLT editionsand transcribed by different sets of ASR systems),make our task particularly difficult and guaranteethe congruence with real-life scenarios in whichtraining and test data are totally independent.As previously mentioned, a common segmenta-tion needs to be shared among the various ASRcomponents.
To do this we decided to use theone provided by our internal ASR system, and toforce-align to it all the other ones.6.2 Terms of comparisonWe compare our segment-based QE-informedROVER against three methods that differ in thegranularity of the combined hypotheses and in theway they are ranked:Random ROVER.
It is obtained by averagingthe results of 100 runs of standard, system-levelROVER (i.e.
the WTN is obtained by com-bining transcriptions of the whole utterance) inwhich the systems to be combined are ranked ran-domly.
Note that this is the only possible wayto run ROVER in absence of information aboutthe reliability of the combined systems.
RandomROVER is the standard fusion method adoptedin IWSLT2013 to produce the final transcriptionsthat are sent to the machine translation phase.System-based Oracle (SysO).
It is obtainedby computing the standard, system-level ROVERbased on the true system ranking (i.e.
the actualranking of the IWSLT2013 participants).
We con-sider it as an oracle since the true ranking repre-sents prior knowledge about systems?
reliabilitywhich is not available in real testing conditions.Segment-based Oracle (SegO).
It is obtainedby computing ROVER at segment-level, using thetrue system ranking for each segment.
Also thisoracle relies on information about systems?
rank-ing (at a higher granularity level), which is notavailable in real testing conditions.
As shown inTable 1, this is the strongest term of comparisonand actually represents out upper bound.6.3 Evaluation metric and significance testAs usually done in ASR evaluation, performanceresults are measured in terms of WER.7Oursegment-based, QE-informed ROVER is hencecompared against the other methods based on theWER computed on the test set (tst2013).To measure if two methods produce statisticallydifferent results, we run the matched-pairs signif-icance test (Gillick and Cox, 1989).
It is basedon averaging the differences between the numberof errors (insertions, deletions and substitutions)produced by the two approaches for the individualsegments.
If the average falls in the [-0.05,+0.05]interval, then the global WER difference betweenthe two methods is not statistically significant.In terms of results?
significance tests, our suc-cess criteria are: i) a statistically significant im-provement over random ROVER, and ii) non-significant differences with respect to the twostrong oracles.
For the sake of comparison, wedefine three symbols for the evaluation results re-ported in Table 4:1.
???
indicates that the corresponding WERscore is not significantly different from ran-dom ROVER (a negative result);2.
???
indicates that the WER score is not sig-nificantly different from the system-basedROVER oracle (a positive result);3.
???
indicates that the WER score is not sig-nificantly different from the segment-basedROVER oracle (the best result).6.4 Ranking ModelsRanking by regression (see Section 4.1) is per-formed using the implementation of the extremelyrandomized trees algorithm (Geurts et al, 2006)provided by the Scikit-learn package (Pedregosaet al, 2011).
Extra-trees are a tree-based ensemblemethod for supervised classification and regres-sion, which we successfully used in the past bothfor MT (de Souza et al, 2013) and ASR qualityestimation (Negri et al, 2014).
The model usedfor machine learned ranking (see Section 4.2) isbased on the implementation of the random forest7The word error rate is the minimum edit distance be-tween an hypothesis and the reference transcription.
Edit dis-tance is calculated as the number of edits (word insertions,deletions, substitutions) divided by the number of words inthe reference.
Lower WERs (?)
indicate better transcriptions.1100method-number of combined systems L3 L4 L5 L6 L7 L8Random ROVER 14.6 13.7 13.2 12.8 12.7 12.4SegO 10.5 11.0 11.4 11.6 11.7 11.7SysO 12.2 11.7 11.8 11.9 12.1 12.1RR1 +Basic 13.9 13.1 12.6 12.4 12.4 12.3 ?
?RR1 +WordBased 14.0 13.0 12.5 12.2 12.3 ?
12.3 ?
?RR1 +Basic+WordBased 14.0 13.0 12.5 12.2 12.3 ?
12.3 ?
?RR2 +Basic 13.8 13.0 12.6 12.4 12.3 ?
12.3 ?
?RR2 +WordBased 14.2 13.1 12.7 12.4 12.5 ?
12.4 ?
?RR2 +Basic+WordBased 13.7 12.8 12.4 12.2 12.2 ?
12.2 ?
?MLR +Basic 12.9 12.4 12.3 12.1 ?
12.3 12.2 ?
?MLR +WordBased 12.4 ?
12.1 12.0 12.0 ?
12.2 ?
12.2 ?
?MLR +Basic+WordBased 12.4 ?
12.1 12.0 ?
11.9 ?
?
12.2 ?
12.2 ?
?Table 4: WER[%] (?)
of random, oracle and QE-informed ROVERs.
The symbols assigned to somescores indicate their statistical significance (p ?
0.05 computed with the matched-pairs test).
In particu-lar: ???
= the result is not statistically different from random ROVER; ???
= the result is not statisticallydifferent from SysO; ???
the result is not statistically different from SegO.ensemble method (Breiman, 2001) provided in theRankLib library.8As mentioned in Section 6.1, all the rankingmodels are trained in 4-fold cross validation.
RR1uses all the instances in tst2012 (i.e.
1,124 seg-ments transcribed by 6 ASR systems, which re-sults in a total of 6,744 training instances).
RR2uses only one instance per segment, which is ran-domly selected among the 6 automatic transcrip-tions available in tst2012 (resulting in a total of1,124 training instances).
Similar to RR1, MLRuses all the instances in tst2012 (6,744 in total).The learning parameters of each model (numberof bags, number of trees per bag, number of leavesper tree and minimum number of instances perleaf) are tuned by maximising Mean Average Pre-cision as the objective function (Hang, 2011).All the models are trained using the ba-sic features (+Basic), the word-based ones(+WordBased) and their combination (+Ba-sic+WordBased).7 Results and discussionTable 4 reports the WER results obtained ontst2013 by ROVER methods fed with: differentnumbers of hypotheses (from 3 to 8), at differentgranularity levels (whole utterance vs. segment),ranked with different models (random, RR1, RR2and MLR) trained with different sets of features8http://sourceforge.net/p/lemur/wiki/RankLib/(Basic, WordBased, Basic+WordBased).The first three rows present the results achievedby our terms of comparison: random ROVER,the segment-based oracle (SegO) and the system-based oracle (SysO).
As anticipated when moti-vating our work (see Table 1), the WER achievedby SegO is always lower than the scores achievedby SysO.
Note also that the performance of SegOdecreases as the number of combined hypothesesincreases, due to the introduction in the input ofprogressively worse transcripts.
Instead, SysO ex-hibits a less coherent behaviour, with close WERvalues at all levels, and a minimum in correspon-dence of column L4 (the combination of fourtranscriptions of the whole utterance).
We inter-pret these results as a further motivation for ourwork: feeding ROVER with a good ranking thatexploits local (segment-level) differences betweenthe combined hypotheses seems to be more reli-able than relying on system-level ranks based onglobal WER scores.
A theoretical analysis of therelation between the diversity of the combined hy-potheses and ROVER results is presented in (Au-dhkhasi et al, 2014).
In light of this analysis,our results open an interesting issue concerningthe trade-offs between optimal hypothesis rankingand their (local) diversity.
We initially explore thisproblem in Section 7.1, but leave for future worka more systematic investigation.Rows 4-6 show the results achieved by RR1(ranking by regression, trained with all the tran-1101scriptions for each input segment).
When trainedonly with basic features, it always outperformsrandom ROVER.
At L8 the gain is not statisti-cally significant but, at the same time, also theWER difference with SysO is not significant.
Notethat, proceeding from L3 to L8, the WER differ-ence between RR1+Basic and random ROVER de-creases from 0.7 to 0.1.
This can be explained bythe fact that when the number of candidates in-creases, then the role of majority voting dominatesthe role of hypothesis ranking.
Similar trends areshown by all other approaches, including the or-acles.
RR1+WordBased slightly improves overRR1+Basic, indicating the possible usefulness ofthis new set of features.
However, when usedin combination (RR1+Basic+WordBased), the twofeature sets do not yield further WER reductions.Nevertheless, what is worth to remark is that at L7and L8 the distance from Sys0 is not statisticallysignificant (a positive result).As shown in rows 7-9, the situation changeswith RR2 (ranking by regression, trained with onetranscription per segment).
When trained with thecombined feature sets (RR2+Basic+WordBased),the model always leads to slight WER reductionsover RR2+Basic.
Also in this case, the gains overrandom ROVER are consistent (they range from0.9 at L3 to 0.2 at L8), and the difference with re-spect to SysO is not statistically significant at L7and L8 (a positive result).As shown in rows 10-12, results are further im-proved by MLR.
Except for L8, the improvementover random ROVER is statistically significant,large and consistent with all feature sets.
TheWER reduction obtained by MLR+Basic variesfrom 1.7 to 0.2 WER points, indicating a higher ef-fectiveness of machine-learned ranking comparedto ranking by regression.
MLR+WordBased pro-duces further WER reductions, with differenceswith SysO that become statistically not-significantat four levels (L3, L6, L7 and L8).
Finally, whentrained with the combined feature sets, the rankingmodel leads to the lowest WER scores.
Notice-ably, such results are not only on par with SysO(the difference is statistically significant only atL4), but in one case (L6) they even reach thoseof SegO, the strongest competitor (best result).Overall, as evidenced by the L8 column, whenthe number of input components becomes largeour QE-informed approaches are not significantlybetter than random ROVER and SysO.
This raisesthe need of a stopping criterion to avoid enteringuseless inputs into the ROVER combination.
To-gether with the trade-off between ranking perfor-mance and hypotheses?
diversity, this representsan interesting topic for future work.7.1 The role of hypotheses?
diversityTo gain further insights on our results, and asa first step along the research directions previ-ously outlined, we analysed the relation betweenROVER results and hypotheses?
diversity.
To thisaim, Figure 2 plots the WER of our best method(MLR+Basic+WordBased) and the two oraclesas a function of hypotheses?
diversity at L6, forwhich we obtain the best results.10 20 30 40 50 60 70 80 90 1000051015202530Level of diversity (MAX WER[%] ?
MIN WER[%])WER[%]051015202530Occurences[%]occurences[%]SegOSysOMLR+Basic+WordBasedFigure 2: Results on tst2013 of the oracles and ourbest model, as functions of hypotheses?
diversity.Diversity is measured by computing the differencebetween the maximum and the minimum WERsof the input transcriptions.
All the segments arethen grouped with regard to this difference.
Forexample 10 on the x-axis refers to the group ofsegments whose diversities lay in the interval of[0,10); 20 refers to the segments whose diversitiesare in [10,20) and consequently, 100 represents thesegments whose diversities lay in [90,100].
Thislatter means that for each segment there is at leastone transcription that is perfect or close to perfec-tion, and one that is (almost completely) wrong.For segments with diversity smaller than 70, theperformance of the system-based oracle (line withcircle marks) and our segment-level QE-informedROVER (line with triangle marks) is almost iden-tical.
Instead, for segments with a ?high?
levelof diversity (in the interval [70,100]), our methodsignificantly outperforms the system-based oracle.With a maximum gain larger than 3 WER points, itapproaches the strong segment-based oracle (line1102with asterisk marks).
Remarkably, for diversityvalues in the interval [90,100], our method is ableto halve the distance that separates the two oracles.The considerable WER reductions observed fordiversity values larger than 70 shed new light onthe global results reported in Table 4.
The fact thatsuch performance gains are hidden in the globalscores can be explained by looking at the dashedline in Figure 2, which shows the percentage ofsegments belonging to each diversity level.
Asit can be observed, the vast majority of the seg-ments (?95%) falls in diversity bins in the inter-val [10,70).
The large WER reductions obtainedon the few remaining segments are definitely notenough to boost global results.
Overall, this find-ing suggests that our segment-level QE-informedROVER can fully unfold its potential in applica-tion scenarios featuring high diversity among thetranscriptions.7.2 Prediction of overall ranksSince our results strongly depend on the reliabil-ity of hypothesis ranking, our final analysis fo-cuses on the correlation between QE-based rank-ing methods and the ?true?
ranks used as priorknowledge by the system-based oracle (the officialranking of the IWSLT2013 participants).
In orderto predict the overall IWSLT2013 ranking, we firstrun our QE models on each segment.
Systems arethen ordered based on the average ranking scorereceived by their transcriptions.
Finally, the alter-native QE-based methods (RR1, RR2 and MLR)are compared by measuring their Spearman corre-lation with the TRUE systems?
order.Table 5 reports the resulting rankings and thecorresponding correlation with the true, officialone.
Among all the possible combinations (8 fac-torial), our two best methods (RR2 and MLR) re-sult in a systems?
ordering with high correlationwith the official IWSLT2013 ranking.
In particu-lar, MLR achieves correlation of 0.905 with threeout of eight systems (1, 2 and 8) that are correctlypositioned.
The correlation values of the differ-ent approaches reflect the performance reportedin Table 4, in which the WER achieved by us-ing MLR is usually better than the ones obtainedfrom RR1 and RR2.
It is interesting to note inthe last column of Table 5 that the ranking errorsare represented by switches between systems withsimilar WERs, while it seems easier to discrimi-nate between systems with more distant WER val-ues.
This consideration is in line with the findingsof Section 7.1 concerning the higher potential ofsegment-level QE-informed ROVER in scenariosfeaturing a higher diversity between the combinedsystems.tst2013 WER TRUE RR1 RR2 MLRNICT 13.5 1 6 2 1KIT 14.4 2 3 4 2MITLL 15.9 3 1 1 4RWTH 16.0 4 2 3 5NAIST 16.2 5 5 5 3UEDIN 22.1 6 8 8 7FBK 23.2 7 4 6 6PRKE 27.2 8 7 7 8Spearman correlation 0.429 0.809 0.905Table 5: True and predicted IWSLT2013 systemranks (correct predictions are shown in bold).8 ConclusionsWe presented a novel approach to improve thecombination of multiple automatic transcriptionhypotheses using ROVER.
Our method is based oninforming the fusion process with accurate worderror rate predictions obtained from ASR qualityestimation models.
First, to exploit the possiblelocal diversity among the combined hypotheses,it performs quality prediction and ranking at seg-ment level.
Then, the predicted ranks for eachsegment are used to feed ROVER.
Finally, thecombined hypotheses are concatenated to recon-struct the entire utterance transcription.
To rankpredictions, we compared two different regressionmodels with a machine-learned ranking method.We carried out experiments on a set of EnglishTED talks collected for two editions of the IWSLTASR evaluation campaign.
Results show that oursegment-level QE-informed ROVER outperformsthe standard random ROVER and performs on par(differences are not statistically significant) witha system-based ROVER oracle that exploits priorknowledge about systems?
reliability.
Moreover,compared to a very strong segment-based ROVERoracle, in one case the performance of our methodis not statistically different.
These results are par-ticularly encouraging, especially in light of thefact that our approach does not exploit confidenceinformation related to the internal behaviour of theASR decoders.
Overall, this represents the firstconfirmation, obtained in an extrinsic evaluationsetting, of the good potential of reference-free andsystem-agnostic ASR quality estimation.1103ReferencesKacem Abida, Fakhri Karray, and Wafa Abida.
2011.cROVER: Improving ROVER using Automatic Er-ror Detection.
In Proceedings of the IEEE Inter-national Conference on Acoustics, Speech, and Sig-nal Processing, (ICASSP 2011), pages 1753?1756,Prague, Czech Republic, May.Kartik Audhkhasi, Andreas M Zavou, Panayiotis GGeorgiou, and Shrikanth S Narayanan.
2014.
The-oretical analysis of diversity in an ensemble of au-tomatic speech recognition systems.
IEEE/ACMTransactions on Audio, Speech & Language Pro-cessing, 22(3):711?726.Paul Boersma and David Weenink.
2005.
Praat: DoingPhonetics by Computer (Version 4.3.01).
Retrievedfrom http://www.praat.org/.Fethi Bougares, Del?eglise, Est`eve Paul, Yannick, andMickael Rouvier.
2013.
LIUM ASR System forEtape French Evaluation Campaign: Experimentson System Combination using Open-source Recog-nizers.
In Proceedings of the 16th InternationalConference on Text, Speech, and Dialogue, pages319?326, Pilsen, Czech Republic, September.Leo Breiman.
2001.
Random forests.
Machine learn-ing, 45(1):5?32.Jos?e G. C. de Souza, Marco Turchi, and Matteo Ne-gri.
2014.
Machine Translation Quality EstimationAcross Domains.
In Proceedings of the 25th Inter-national Conference on Computational Linguistics(COLING 2014): Technical Papers, pages 409?420,Dublin, Ireland, August.Jos?e G. C. de Souza, Hamed Zamani, Matteo Negri,Marco Turchi, and Daniele Falavigna.
2015.
Mul-titask Learning for Adaptive Quality Estimation ofAutomatically Transcribed Utterances.
In Proceed-ings of the 2015 Conference of the North AmericanChapter of the Association for Computational Lin-guistics - Human Language Technologies (NAACLHLT 2015), Denver, Colorado, USA.Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, andHang Li.
2007.
Learning to Rank: from PairwiseApproach to Listwise Approach.
In Proceedingsof the 24th International Conference on Machinelearning (ICML-07), pages 129?136, Corvalis, Ore-gon, USA.Mauro Cettolo and Marcello Federico.
2000.
ModelSelection Criteria for Acoustic Segmentation.
InASR2000-Automatic Speech Recognition: Chal-lenges for the new Millenium ISCA Tutorial and Re-search Workshop (ITRW).Mauro Cettolo, Jan Niehues, Sebastian St?uker, LuisaBentivogli, and Marcello Federico.
2013.
Report onthe 10th IWSLT Evaluation Campaign.
In Proceed-ings of the International Workshop on Spoken Lan-guage Translation (IWSLT 2013), Heidelberg, Ger-many, December.Hai Leong Chieu and Hwee Tou Ng.
2002.
NamedEntity Recognition: A Maximum Entropy ApproachUsing Global Information.
In Proceedings of the19th International Conference on ComputationalLinguistics - Volume 1, COLING ?02, pages 1?7,Taipei, Taiwan.Jos?e G. C. de Souza, Christian Buck, Marco Turchi,and Matteo Negri.
2013.
FBK-UEdin participationto the WMT13 quality estimation shared task.
InProceedings of the Eighth Workshop on StatisticalMachine Translation, pages 352?358, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.Gunnar Evermann and PC Woodland.
2000.
Pos-terior Probability Decoding, Confidence Estimationand System Combination.
In Proceedings of NISTSpeech Transcription Workshop, volume 27, CollegePark, MD, USA.Marcello Federico, Luisa Bentivogli, Michael Paul,and Sebastian St?uker.
2012.
Overview of theIWSLT 2012 Evaluation Campaign.
In Proceed-ings of the International Workshop on Spoken Lan-guage Translation (IWSLT 2012), pages 11?27,Hong Kong, December.Jonathan G Fiscus.
1997.
A Post-processing Sys-tem to Yield Reduced Word Error Rates: Recog-nizer Output Voting Error Reduction (ROVER).
InProceedings of the IEEE Workshop on AutomaticSpeech Recognition and Understanding, pages 347?354, Santa Barbara, CA, USA.
IEEE.Pierre Geurts, Damien Ernst, and Louis Wehenkel.2006.
Extremely randomized trees.
Machine learn-ing, 63(1):3?42.Laurence Gillick and Stephen J Cox.
1989.
Some Sta-tistical Issues in the Comparison of Speech Recog-nition Algorithms.
In Proceedings of the IEEE In-ternational Conference on Acoustics, Speech, andSignal Processing, (ICASSP 1989), pages 532?535,Glasgow, Scotland.Sharon Goldwater, Dan Jurafsky, and Christopher DManning.
2010.
Which words are hard to rec-ognize?
prosodic, lexical, and disfluency factorsthat increase speech recognition error rates.
SpeechCommunication, 52(3):181?200.LI Hang.
2011.
A short introduction to learning torank.
IEICE TRANSACTIONS on Information andSystems, 94(10):1854?1862.Dustin Hillard, Bjoern Hoffmeister, Mari Ostendorf,Ralf Schlueter, and Hermann Ney.
2007. iROVER:Improving System Combination with Classification.In Human Language Technologies 2007: The Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics; CompanionVolume, Short Papers, pages 65?68, Rochester, NewYork, April.1104Bj?orn Hoffmeister, Tobias Klein, Ralf Schl?uter, andHermann Ney.
2006.
Frame Based System Com-bination and a Comparison with Weighted ROVERand CNC.
In Proceedings of the InternationalConference on Spoken Language Processing (Inter-speech 2006 ?
ICSLP), pages 537?540, Pittsburgh,PA, USA.Xiang Li, Rita Singh, and Richard M. Stern.
2002.Lattice Combination for Improved Speech Recogni-tion.
In Proceedings of the International Conferenceof Spoken Language Processing, Denver, CO, USA,September.Lidia Mangu.
2000.
Finding Consensus in SpeechRecognition.
John Hopkins University.
PhD The-sis.Brian McFee and Gert R Lanckriet.
2010.
MetricLearning to Rank.
In Proceedings of the 27th Inter-national Conference on Machine Learning (ICML-10), pages 775?782, Haifa, Israel, June.Frank McSherry and Marc Najork.
2008.
Comput-ing information retrieval performance measures ef-ficiently in the presence of tied scores.
In Advancesin information retrieval, pages 414?421.
Springer.Yashar Mehdad, Matteo Negri, and Marcello Fed-erico.
2012.
Match without a Referee: EvaluatingMT Adequacy without Reference Translations.
InProceedings of the Machine Translation Workshop(WMT2012), pages 171?180, June.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent Neural Network Based Language Model.
InProceedings of INTERSPEECH 2010, 11th AnnualConference of the International Speech Communi-cation Association, pages 1045?1048, Makuhari,Chiba, Japan, September.Matteo Negri, Marco Turchi, Jos?e G. C. de Souza,and Falavigna Daniele.
2014.
Quality Estimationfor Automatic Speech Recognition.
In Proceedingsof COLING 2014, the 25th International Confer-ence on Computational Linguistics: Technical Pa-pers, pages 1813?1823, Dublin, Ireland, August.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, et al 2011.
Scikit-learn:Machine learning in python.
The Journal of Ma-chine Learning Research, 12:2825?2830.Thomas Pellegrini and Isabel Trancoso.
2010.
Im-proving ASR Error Detection with Non-decoderBased Features.
In Proceedings of INTERSPEECH2010, 11th Annual Conference of the InternationalSpeech Communication Association, pages 1950?1953, Makuhari, Chiba, Japan, September.Holger Schwenk and Jean-Luc Gauvain.
2000.
Im-proved ROVER using Language Model Information.In ASR2000-Automatic Speech Recognition: Chal-lenges for the new Millenium ISCA Tutorial and Re-search Workshop (ITRW).Lucia Specia, Nicola Cancedda, Marc Dymetman,Marco Turchi, and Nello Cristianini.
2009.
Es-timating the Sentence-Level Quality of MachineTranslation Systems.
In Proceedings of the 13thAnnual Conference of the European Associationfor Machine Translation (EAMT?09), pages 28?35,Barcelona, Spain.Andreas Stolcke, Harry Bratt, John Butzberger, Ho-racio Franco, Venkata Ramana Gadde, MadelainePlauche, Colleen Richey, Elizabeth Shriberg, KemalSonmez, F Weng, and Jing Zheng.
2000.
The SRImarch 2000 HUB5 conversational speech transcrip-tion system.Yik-Cheung Tam, Yun Lei, Jing Zheng, and WenWang.
2014.
ASR Error Detection using Recur-rent Neural Network Language Model and Comple-mentary ASR.
In Proceedings of the IEEE Interna-tional Conference on Acoustics, Speech, and SignalProcessing, (ICASSP 2014), pages 2312?2316, Flo-rence, Italy, May.Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. deSouza, and Matteo Negri.
2014.
Adaptive Qual-ity Estimation for Machine Translation.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 710?720, Baltimore, Maryland, June.Rong Zhang and Alexander I. Rudnicky.
2006.
In-vestigations of Issues for Using Multiple AcousticModels to Improve Continuous Speech Recognition.In Proceedings of INTERSPEECH, Pittsburgh, PA,USA, September.1105
