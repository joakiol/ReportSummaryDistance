Learning Semantic-Level Information Extraction Rules byType-Oriented ILPYutaka Sasaki and Yoshihiro MatsuoNTT Communicat ion  Science Laboratories2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan{sasaki, yosihiro} ~cslab.kecl .ntt .co.
jpAbstractThis paper describes an approach to using se-mantic rcprcsentations for learning informationextraction (IE) rules by a type-oriented induc-tire logic programming (ILl)) system.
NLPcomponents of a lnachine translation system areused to automatically generate semantic repre-sentations of text corpus that can be given di-rectly to an ILP system.
The latest experimen-tal results show high precision and recall of thelearned rules.1 Int roduct ionInformation extraction (IE) tasks in this paperinvolve the MUC-3 style IE.
The input for theinformation extraction task is an empty tem-plate and a set of natural anguage texts that de-scribe a restricted target domain, such as corpo-rate mergers or terrorist atta.cks in South Amer-ica.
Templates have a record-like data struc-ture with slots that have names, e.g., "companyname" and "merger d~te", and v~lues.
The out-put is a set of filled templates.
IE tasks arehighly domain-dependent, so rules and dictio-naries for filling values in the telnp\]ate slots de-pend on the domain.it is a heavy burden for IE system develop-ers that such systems depend on hand-maderules, which cannot be easily constructed andchanged.
For example, Umass/MUC-3 neededabout 1,500 person-hours of highly skilled laborto build the IE rules and represent them as adictionary (Lehnert, 1992).
All the rules mustbe reconstructed i'rom scratch when the targetdomain is changed.To cope with this problem, some pioneershave studied methods for learning informationextraction rules (Riloff,1996; Soderland ctal.,1.995; Kim et el., 1995; Huffman, 1996; Califfand Mooney, 1997).
Along these lines, our ap-preach is to a.pply an inductive logic program-ruing (ILP) (Muggleton, 1991)system to thelearning of IE rules, where information is ex-tracted from semantic representations of newsarticles.
The ILP system that we employed isa type-oriented ILP system I{\]\]B + (Sasaki andHaruno, 1997), which can efficiently and effec-tively h~mdle type (or sort) information in train-ing data.2 Our Approach to IE TasksThis section describes our approach to IE tasks.Figure 1. is an overview of our approach to learn-ing IE rules using an II, P system from seman-tic representations.
First, training articles areanalyzed and converted into semantic represen-tations, which are filled case fl'ames representedas atomic formulae.
Training templates are pre-pared by hand as well.
The ILP system learns\]!!
; rules in the tbrm of logic l)rograms with typeinformation.
To extract key inlbrmation from anew ~rticle, semantic representation s au tomat-ically generated from the article is matched bythe IE rules.
Extracted intbrmation is filled intothe template slots.3 NLP  Resources and Tools3.1 The Semantic Attribute SystemWe used the semantic attribute system of "Ge lTaikei - -  A Japanese Lexicon" (lkehara el el.,1997a; Kurohashi and Sakai, 1.999) compiled bythe NTT Communication Science Laboratoriesfor a Japanese-to-English machine translationsystem, ALT- J /E  (Ikehm:a et al, 1994).
The se-mantic attribute system is a sort of hierarchicalconcept thesaurus represented as a tree struc-ture in which each node is called a semanticcateqory.
An edge in the tree represents an is_aor has_a relation between two categories.
Thesemantic attribute system is 11.2 levels deep and698semantic representation new article' ' '  \[\]\]\] s?~chy yze ~ Analyze I rolease(cl,pl) articles sentences announce(cl,dl)i'~kackgrou n d Anal.... nzwledge / \[E rules ~ F representatiOn sentences I semantic.ooitive ..... lI re,oa o x. , IIanswertemplates filled Company: c2 ~7"A;p'-'"iyrule~='~" "~by hand Draotauotd..~2 to semantic I -  I ,opreseot t,onFigure l: l/lock diagram of IE using IM )contains about 3,000 sema.ntic ategory nodes.More than 300,000 Japanese words a.re linked tothe category nodes.3.2 Verb Case Frame Dict ionaryThe Japanese-to-li;nglish valency 1)a.ttern dic-t ionary of "(\]oi-Taikei" ( lkehara et al, 1997b;Kurohash.i and Saka.i, 1999) was also originallydeveloped for ALT-,I/IB.
The.
wde:ncy dictionaryconta.ins about 15,000 case frames with sema.n-tic restrictions on their arguments lbr 6,000a apanese verbs.
Each ca.se frame consists of onepredicate a.nd one or more case elements tha.th ave a list; of sere an tic categories.3.3 Natural  Language Processing ToolsWe used the N I,P COml)onents of kl/ l ' - .
I /F,  fortext a, nalysis.
These inclu<le the morphologica,lamdyzer, the syntactic analyzer, and the caseaDalyzer for Japanese.
The components a.re ro-bust a:nd generic tools, mainly ta:rgeted to news-paper articles.3.3.1 Generic Case Analyzerl,et us examine the case a.nalysis in more de-tail.
The <'as(; analyzer eads a set of parse treecandidates produced by the J a.panese syntacticanalyzer.
The parse tree is :represented as a de-penden cy of ph rases (i. e., .\] al>anese bu'nsctmt).First, it divides the parse tree into unit sen-tences, where a unit sentence consists of onepredicate and its noun and adverb dependentphrases.
Second, it compares each unit sen-tence.with a verb case fl'alne dictionary, l!
;achframe consists a predicate condition and severalcast elements conditions.
The predicate con-dition specifies a verb that matches the framea.
:nd each case-role has a. case element conditionwhi ch sl>ecifie.s particles an d sere an tic categoriesof" noun phrases.
The preference va.lue is de-lined as the summation of noun phrase \])refer-ences which are calculated from the distancesbetween the categories of the input sentencesm~d the categories written in the f i ' amcs .
Thecase a.na.lyzer then chooses the most preferablepa.rse tree and the most preferable combinationof case frames.The valency dictionary also has case<roles(Table \] ) for :noun phrase conditions.
The case-roles of adjuncts are determined by using theparticles of adjuncts and the sema.ntic a.tegoriesof n ou n ph ra.ses.As a result, the OUtl)ut O\[' the case a.nalysis isa set; el" (;ase fl:ames for ca.oh unit se:ntence.
Thenoun phra.ses in \['tames are la.beled by case-roh;sin Tal)le 1.l!
'or siml)\]icity , we use case-role codes, such a.sN 1 and N2, a.s the labels (or slot ha.rues) to rep-resent case li:ames.
The relation between sen-tences and case-roles is described in detail in( Ikehara el el., 1993).3.3.2 Logical Form TranslatorWe developed a logical form translator li'E1 ~that generates semantic representations ex-pressed a,s atomic Ibrmulae from the cast; fi:a.mesand parse trees.
For later use, document II)and tense inlbrmation a.re also added to the caseframes.For example, tile case fl:ame in 'l.
'able 2 is ob-tained a:l'ter analyzing the following sentence ofdocument l) 1:"Jalctcu(.lack) h,a suts,tkesu(suitca.se) we699Table 1: Case-RolesName Code Description l~xampl.eSubject N1 the agent/experiencer of I throw a ball.an event/situationObjectl  N2 the object of an eventObject2 N3 another object of an eventLoc-Source N4 source location of a movementLoc-Goal N5 goal location of a movementPurpose N6 the purpose of an actionResult N7 the result of an eventLocative N8 the location of an eventComitative N9 co-experiencerQuotative N10 quoted expressionMaterial N 11 material/ ingredientCause N12 the reason for an eventInstrument N13 a concrete instrumentMeans N14 an abstract instrumentTime-Position TN1 the time of an eventTime-Source TN2 the starting time of an eventTime-Goal TN3 the end time of ~n eventAmount QUANT quantity of somethingI throw a ball.I compare it with them.I start fl'om Japan.I go to Japan.I go shopping.It results in failure.it occurs at the station.I share a room with him.I say that ....I fill the glass with water.It collapsed fr'om the weight.I speak with a microphone.I speak in Japanese.I go to bed at i0:00.I work from Monday.It continues until Monday.I spend $10.hok,,ba(the omce) kava(from)   o(the airport) ,),i(to)ha~obu(carry)"("Jack carries a suitcase from the office to theairport.
")Table 2: Case Frame of the Sample Sentencepredicate: hakobu (carry)article: 1) 1tense: presentNI: Jakhu (Jack)N2: sutsukesu (suitcase)N4: sl, okuba (the office)N5: kuko (the airport)4 Induct ive Learning ToolConventional ILP systems take a set of positiveand negative xamples, and background knowl-edge.
The output is a set of hypotheses in theform of logic programs that covers positives anddo not cover negatives.
We employed the type-oriented ILP system RHB +.4.1 Features of Type-orlented ILPSystem RHB +The type-oriented I\],P system has the tbllowingfeatures that match the needs for learning l\]"~rules.?
A type-oriented ILP system can efficientlyand effectively handle type (or seman-tic category) information in training data..This feature is adwmtageous in controllingthe generality and accuracy of learned IErules.?
It can directly use semantic representationsof the text as background knowledge., It can learn from only positive examples.?
Predicates are allowed to have labels (orkeywords) for readability and expressibil-ity.4.2 Summary of Type-oriented ILPSystem RHB +This section summarizes tile employed type-oriented ILP system RHB +.
The input ofRHB + is a set of positive examples and back-ground knowledge including type hierarchy (or700the semantic attribute system).
The output isa set of I\[orn clauses (Lloyd, 11.987) having vari-;tl~les with tyl)e intbrmation.
That is, the termis extended to the v-term.4.3 v-termsv-terms are the restricted form of 0-terms (Ai't-K~tci and Nasr, 1986; Ait-Kaci et al, 11994).
In-l'ormttlly, v-terms are Prolog terms whose vari-ables a.re replaced with variable Var of type T,which is denoted as Var:T. Predicttte ~tnd tim(:-tion symbols ~tre allowed to h;we features (orlabels).
For examl)\]e,speak( agent~ X :human,objcct~ Y :language)is a clause based on r-terms which ha.s labelsagent and object, and types human andlanguage.4.,4 A lgor i thmThe algorithm of lHllI + is basically ~t greedycovering algorithm.
It constructs clauses one-by-one by calling inner_loop (Algorithm \])which returns a hypothesis clause.
A hypoth-esis clause is tel)resented in the form of head :--body.
Covered examples are removed from 1 ) ineach cycle.The inner loop consists of two phases: thehead construction phase and the body construc-tion I)hase.
It constrncts heads in a bottom-upmanner and constructs the body in a top-downlna.nner, following the result described in (Zelleel al., 1994).
"\['he search heuristic PWI  is weighted infor-m~tivity eml)loying the l,a.place estimate.
Let7' = {Head : -Body } U B K.rwz( r ,T )_  l I f ' l+ J- - I .
f ' - - /?
1?g2 IQ-~\]'i\[ _12 2'where IPl denotes the number of positive ex-amples covered by T and Q(T) is the empiricalcontent.
The smaller the value of PWI, the can-didate clause is better.
Q(T) is defined as theset of atoms (1) that are derivable from T ~md(2) whose predicate is the target I)redicate, i.e.,the predicate name of the head.The dynamic type restriction, by positivc ex-amples uses positive examples currently coveredin order to determine appropriate types to wtri-~bles for the current clause.A lgor i thm 1 inner_loop1.
Given positives P, original positives 1~o, back-ground knowledge 1Hr.2.
Decidc typcs of variables in a head by comput-ing the lyped least general generalizations (lgg)of N pairs of clcmcnts in P, and select he mostgeneral head as H cad.3.
If the stopping condition is satisfied, returnHead.It.
Let Body bc empty.5, Create a set of all possible literals L using vari-ables in Head and Body.6.
Let BEAM be top If litcrals l~, of L wilhrespect to the positive weighted informalivilyPWI.7.
Do later steps, assuming that l~ is added toBody for each literal lk in BEAM.8.
Dynamically restrict types in Body by callin, gthe dynamic type restriction by positive exam-pies.9.
If the slopping condition is satisfied, rct'aru(Head :- Body).lO.
Goto 5.5 I l l us t ra t ion  o f  a Learn ing  ProcessNow, we examine tile two short notices of' newproducts release in Table 3.
The following tableshows a sample te:ml)late tbr articles reportinga new product relea.se.Tom pl ate1.
article id:2. coml)any:3. product:4. release date:5.1 Preparat ionSuppose that the following semantic represen-tations is obtained from Article 1.
(cl) announce( article => I,tense => past,tnl => "this week",nl => "ABC Corp.",nlO => (c2) ) .
(c2) release( article => I,tense => future,tni => "Jan. 20",nl => "ABC Corp.",n2 => "a color printer" ).701Table 3: Sample SentencesArticle id Sentence#1 "ABC Corp. this week zmnounced that it will release a color printer on Jan.
20.
"#2 "XYZ Corp. released a color scanner last month.
"The filled template for Article 1 is as follows.Template \]\].
article id: 12. colnpany: ABC Corp.3.
product: a color printer4.
release date: Jan. 20Suppose that the following semantic represen-tation is obtained from Article 2.
(c3) release( article => 2,tense => past,tnl => "last month",nl => "XYZ Corp.",n2 => "a color scanner" ).The filled template for Article 2 is as follows.Template 21. article id: 22. company: XYZ Corp.3.
product: a color scanner4.
release date: last month5.2 Head Const ruct ionTwo positive examples are selected for the tem-plate slot "company".company(ar t i c le -number  => iname => "ABe Corp") .company(ar t i c le -number  => 2name => "XYZ Corp") .By computing a least general generalization(lgg)sasaki97, the following head is obtained:company( article-number => Art: numbername => Co: organization).5.3 Body  ConstructionGenerate possible literals 1 by combining predi-cate names and variables, then check the PWI1,1iterals,, here means atomic formulae or negatedones .values of clauses to which one of the literaladded.
In this case, suppose that adding the fol-lowing literal with predicate release is the bestone.
After the dynamic type restriction, thecurrent clause satisfies the stopping condition.Finally, the rule for extracting "company name"is returned.
Extraction rules for other slots"product" and "release date" can be learned inthe sanle manner.
Note that several literals maybe needed in the body of the clause to satisfythe stopping condition.company(article-number => Art:number,name => Co: organization )?
- release( article => Art,tense => T: tense,tnl => D: time,nl => Co,n2 => P: product ).5.4 Ext rac t ionNow, we have tile following sen\]antic represen-tation extracted from the new article:Article 3: " JPN Corp. has released a new CI)player.
''2(c4) release( article => 3,tense => perfect_present,tnl => nil,n l  => "JPN Corp.",n2 => "a new CD player" ).Applying the learned IE rules and other rules,we can obtain the filled template for Article 3.Template 31. article id: 32. company: JPN Corp.3.
product: C I )p layer4.
release date:2\;Ve always assume nil for the case that is not in-cluded in the sentence.702Table d: Learning results of new product release(a) Without data correctioncompany product release datePrecision 89.6%Recall 82.1%Average time (set.)
15.8l)recision 911 .1%Recall 85.7%Average time (sec.)
22.980.5%66.7%22.J90.6%66.7%ld.dannounce date \[ pricelOO.O% 58.4%82.4:% 60.8%2.2 I 1.0(b) With data.
correctioncompany product release date80.o%69.7%25.292.3%82.8%33.55annotmce date \[ price100.0% 87.1%88.2% 82.4%5.1.5 11.96 Experimental Results6.1.
Setting of ExperhnentsWe extracted articles related to the release ofnew products from a one-year newspaper cor-pus written in Japanese 3.
One-hundred arti-cles were randomly selected fi'om 362 relevantarticles.
The template we used consisted oftive slots: company name, prod'uct name, re-lease date, a~tnomzcc date, and price.
We alsofilled one template for each a.rticle.
After an-a.lyzing sentences, case fi'ames were convertedinto atomic tbrmulae representing semantic rep-re,,~entationx a.  described in Section 2 and 3.
Allthe semantic representations were given to thelea.rner as background \]?nowledge, ~md the tilledtemplates were given as positive examples.
Tospeed-up the leCturing process, we selected pred-icate names that are relevant o the word s in thetemplates as the target predicates to be used bythe ILl ~ system, and we also restricted the num-ber of literals in the body of hypotheses to one.Precision and recM1, the standard metrics \['orIF, tasks, are counted by using the remove-one-out cross validation on tile e, xamples for eachitem.
We used a VArStation with tlie PentiumH Xeon (450 MHz):for this experiment.6.2 Results'l?M)le 4 shows the results of our experiment.
Inthe experiment of learning from semantic repre-sentations, including errors in case-role selectionand semantic ategory selection, precision was3We used ~rticles from the Mainichi Newspaimrs of1994 with permission.very high.
'l'he precision of the learned ruleslot price was low beta.use the seman tic categoryname automatieaJly given to the price expres-sions in the dat~ were not quite a.ppropriate.For the tire items, 6?-82% recall was achieved.With the background knowledge having sere an-tic representations corrected by hand, precisionwas very high mid 70-88% recMl was achieved.The precision of price was markedly improved.It ix important that the extraction of liveditthrent pieces o1' information showed good re-sults.
This indica.tex that the \]LI' system RIII~ +has a high potential in IE tasks.7 Related Workl)revious researches on generating lli; rulesfrom texts with templates include AutoSlog-TS (Riloff,1996), (',I{YS'FAL (Soderland et al,1995), I'AIAKA (l(im et al, 1995), MlgP (Iluff-man, 11.996) and RAPII~;I~ (Califl' and Mooney,1997).
In our approach, we use the type-oriented H,P system RItlJ +, which ix indepen-dent of natural language analysis.
This pointdifferentiates our ~pproach from the others.Learning semantic-level IE rules using an II,Psystem from semantic representations is also anew challenge in II'; studies.Sasald (Sasaki and Itaruno, 11997) appliedRI{B + to the extraction of the number of deathsand injuries fi'om twenty five articles.
Thatexperiment was sufficient o assess the perfor-mance of the learner, but not to evaJuate itsfeasibility in IE tasks.7038 Conc lus ions  and RemarksThis paper described a use of semantic repre-sentations for generating information extractionrules by applying a type-oriented ILP system.Experiments were conducted on the data gen-erated fi'om 100 news articles in the domain ofnew product release.
The results showed veryhigh precision, recall of 67-82% without datacorrection and 70-88% recall with correct se-mantic representations.
The extraction of fivedifferent pieces of information showed good re-sults.
This indicates that our learner RHB + hasa high potential in IE tasks.ReferencesH.
Ai't-Kaci and R. Nasr, LOGIN: A logic pro-gramming language with built-in inheritance,Journal oJ' Logic Programming, 3, pp.185-215, 1986.lt.
Ai't-Kaci, B. Dumant, R. Meyer, A. Podel-ski, and P. Van Roy, The Wild Life Itandbook,1994.M.
E. Califf and R. J. Mooney, RelationalLearning of Pattern-Match Rules for Informa-tion Extraction, Proc.
of ACL-97 Workshopin Natural Language Learning, 1997.S.
B. Huffman, Learning Information Extrac-tion Patterns from Examples, Statistical andSymbolic Approaches to Learning for NaturalLanguage Processing, pp.246 260, 1996.S.
ikehara, M. Miyazaki, and A. Yokoo, Clas-si:fication of language knowledge for mean-ing analysis in machine translations, Trans-actions of Information Processing Societyof Japan, Vol.34, pp.1692-1704, 1993.
(in.Japanese)S. Ikehara, S. Shirai, K. Ogura, A. Yokoo,H.
Nakaiwa and T. Kawaoka, ALT-J/E: AJapanese to English Machine Translation Sys-tem tbr Communication with Translation,Proc.
of The 13th IFIP World ComputerCongress, pp.80-85, 1994.S.
Ikehara, M. Miyazaki, S. Shirai, A. Yokoo,H.
Nakaiwa, K. Ogura, Y. Oyama andY.
Hayashi (eds.
), The Semantic AttributeSystem, Goi-lktikci -- A Japanese Lexi-con, Vol.1, Iwanami Publishing, 1997.
(inJapanese)S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo,H.
Nakaiwa, K. Ogura, Y. Oyama andY.
Hayashi (eds.
), The Valency Dictionary,Goi-Taikei -- A Japanese Lcxicon, Vol.5,Iwa.nami Publishing, 1997.
(in Japanese)J.-T. Kim and D. I. Moldovan, Acquisitionof Linguistic Patterns for Knowledge-BasedInformation Extraction, \[EEE Transactionon Knowledge and Data Engineering (IEEETKDE), Vol.7, No.5, pp.713 724, 1995.S.
Kurohashi and Y. Sakai, Semantic Analysisof Japanese Noun Phrases: A New Approachto Dictionary-Based Understanding Thc 37thAnnual Meeting of the Association for Com-putational Linguistics (A CL-99), pp.481-488,1999.W.
Lehnert, C. Cardie, D. Fisher, J. McCarthy,E.
Riloff and S. Soderland, University of Mas-sachusetts: MUC-4 Test Results and Analy-sis, Proc.
of The 1;burth Message Understand-ing Conference (MUC-4), pp.151-158, 1992.J.
Lloyd, Foundations of Logic Prog'mmming,Springer, 1987.S.
Muggleton, Inductive logic programming,New Generation Computing, Vol.8, No.4,pp.295-318, 1991.E.
Riloff, Automatically Generating Extrac-tion Pattern from Untagged Text, Proc.ofAmerican Association for Artificial IntcIli-gcnce (AAAI-96), pp.1044-1049, 1996.Y.
Sasaki and M. IIaruno, RHB+: A Type-Oriented 1LP System Learning from PositiveData, Proc.
of The l/jth International JointConference on Artificial Intelligence (LJCA l-9"/), pp.894-899, 1997.S.
Soderland, 1).
Fisher, J. Aseltine, W. Lenert,CRYSTAL: Inducing a Conceptual Dictio-n~ry, Proc.
of The 13th International JointConJ'crcnce on Artificial Intelligence (IJCAI-95), pp.1314 1319, 1995.J.
M. Zelle and R. J. Mooney, J.
B. Konvisser,Combining Top-down and Bottom-up Meth-ods in Inductive Logic Programming, Procof The 11th Tntcrnational Conference on Ma-chine Learning (ML-94), pp.343-351, 1994.J704
