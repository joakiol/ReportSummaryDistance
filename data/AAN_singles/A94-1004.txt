Modeling Content Identification from Document ImagesTakehiro NakayamaFuji Xerox Palo Alto Laboratory3400 HiUview AvenuePalo Alto, CA 94304 USAnakayama~pal.xerox.comAbstractA new technique to locate content-represent-ing words for a given document image usingabstract representation f character shapes isdescribed.
A character shape code representa-tion defined by the location of a character in atext line has been developed.
Character shapecode generation avoids the computationalexpense of conventional optical character rec-ognition (OCR).
Because character shapecodes are an abstraction of standard charactercode (e.g., ASCII), the mapping is ambiguous.In this paper, the ambiguity is shown to bepractically limited to an acceptable vel.
It isillustrated that: first, punctuation marks areclearly distinguished from the other charac-ters; second, stop words are generally distin-guishable from other words, because thepermutations of character shape codes in func-tion words are characteristically different fromthose in content words; and third, numeralsand acronyms in capital letters are distinguish-able from other words.
With these clAssifica-tions, potential content-representing words areidentified, and an analysis of their distributionyields their rank.
Consequently, introducingcharacter shape codes makes it possible toinexpensively and robustly bridge the gapbetween electronic documents and hard-copydocuments for the purpose of content identifi-cation.1 IntroductionDocuments are becoming increasingly available inmachine-readable form.
As they are stored automati-cally and transferred on networks, many natural an-guAge processing techniques that identify their contenthave been developed to assist users with informationretrieval and document classification.
Conventionally,stored records axe identified by sets of keywords orphrases, known as index terms (Salton, 1991).Although documents are increasingly being com-puter generated, they are still printed on paper for read-ing, dissemination, and markup.
As it is believed thatpaper will remain a comfortable medium for readingand modification (O'Gorman and Kasturi, 1992), devel-opment of content identifiCAtion techniques from a doc-ument image is still important.
OCR is often used toconvert adocument image into machine-readable form,but processing performance is limited by the overheadof OCR (Mori et al, 1992; Nagy, 1992; Rice et al,1993).
Because of the inaccuracy and expense of OCR,we decided to avoid using it.Instead, we have developed a method that firstmakes generalizations about images of characters, thenperforms gross classification of the isolated charactersand agglomerates these character shape codes into spa-tially isolated (word shape) tokens (Nakayama ndSpitz, 1993; Sibun and Spitz, this volume).
Generatingword shape tokens is inexpensive, fast, and robust.Word shape tokens are a potential alternative to charac-ter coded words when they are used for language deter-mination and part-of-speech tagging (Nakayama ndSpitz, 1993; Sibun and Spitz, this volume; Sibun andFarrar, 1994).
In this paper, we describe an extension ofour approach to content identification.2 Word shape token generation from imageIn this section, we introduce word shape tokens andtheir generation from document images.First, we classify characters by determining thecharacteristics of the text line.
We identify the positionsof the baseline and the x-height as shown in figure 1(Spitz, 1993).Next, we count the number of connected compo-nents in each character cell and note the position ofthose connected components with respect o the base-line and x-height (Nakayama nd Spitz, 1993; Sibunand Spitz, this volume).
The basic character classes { Ax i g j U ' - , .
: = ! }
and the members which constitutethose classes are shown in Table 1.
In this paper, they22are represented in bold-face type (e.g., Aigxx).
Notethat a character shape code subset { - , .
: !}
includesonly punctuation marks.
This is important for our clean-ing process which will be described later.~ ~ I ~  t?p x-heightbaseline bottomFigure 1: Text line parameter positionsCharacter shape codes are grouped by word bound-ary into word shape tokens (see Sibun and Spitz, thisvolume).
The correspondence between the scannedword image and the word shape token is one-to-one;that is, when a certain word shape token is selected, itsoriginal word image can be immediately located.Recocnizing word shape tokens from images is twoor three orders of magnitude faster than conventionalOCR (Spitz, 1994), and is robust for real-world ocu-ments which are sometimes degraded by poor printingand which sometimes use more than a single font.Table 1: Shape class code membershipcharactermembers shape codeA A-Z lxlfllkltB 0-9 +#$&0/<>\[\]@{ }1x acemnorsuvwxzg gPqY~J jU ii~auO0'-,.:=!
"-,.:;=!
?The use of ouly 13 character shape codes instead ofapproximately 100 standard character codes results in aone-to-many correspondence between word shapetokens and words.Figure 2 shows how much character shape codesreduce word variation using the most frequent 10,000English words (Carroll et al, 1971) in order of fre-quency.
A word is defined as a string of graphic harac-ters bounded on the left and right by spaces.
Words aredistinguished by their graphic haracters.
For example,"apple", "Apple", and "apples" are three differentwords, while "will" (modal) and "will" (noun) are thesame.
For the purpose of comparing the character shapecode representation with the standard character coderepresentation, the x axis represents he number of tim-quent words, and the y axis represents the number ofdistinct words represented in both ASCII and charactershape codes.
The number of words in ASCII naturallycorresponds to the number of original words one-to-one.
On the other hand, the number of words in charac-ter shape codes (the number of word shape tokens) isless than half of the number of original words.
This gapis a constraint on the accuracy of our approach, but weshow it is not a serious limitation in the following sec-tion.O'lO"UOO"6n E" It--10000ASCII /80006000 - - char~.
.
.
.
.
.
.40002000 .
~'~'~" .
.
.
.
.
.
I0 r " ,  , I , , , I , , , I , , , I , , ,0 2000 4000 6000 8000 10000number of words (frequency order)Figure 2: ASCII and character shape code3 Content identificationText characterization s an important domain for naturallanguage processing.
Many published techniques utilizeword frequencies of a text for information retrieval andtext categorization (Jacobs, 1992; Cutting et al, 1993).We also characterize the content of the documentimage by finding words that seem to specify the topic ofthe document.
Briefly, our strategy is to identify the fre-quently occurring word shape tokens.In this section, we first describe aprocess of clean-ing the input sequence which precedes the main proce-dures.
Then, we illustrate how to collect the importanttokens, introducing a stop list of common word shapetokens which is used to remove the tokens that areinsufficiently specific to represent the content of thedocuments.3.1 Cleaning input sequenceGiven a sequence of word shape tokens, the systemremoves the specific character shape codes '-', ',', '.
',':', and '!'
that do not contribute important linguisticinformation to the words to which they adhere, but thatchange the shape of the tokens.
Otherwise, word shapewould vary according to position and punctuation,which would interfere with token distribution analysisdownstream.
We ignore possible sentence initial wordshape alteration by capitalization simply because it isalmost impossible to presume the original shape.
In this23paper, capitalized words are counted differently fromtheir uncapitalized counterparts.Our cleaning process concatenates word shapetokens before and after the hyphen at the end of line.The process also deletes intended hyphens (e.g.,AxxxA-xixAxA \[broad-minded\] --> AxxxAxixAxA).Eliminating hyphens reduces the variation of wordshape tokens.
We measured this effect using the afore-mentioned frequent 10,000 words.
Forty-two words of10,000 are hyphenated.
In character shape code repre-sentation, 10,000 words map into 3,679 word shapetokens (figure 2).
When hyphens are eliminated, the10,000 words fall into 3,670 word shape tokens.
Thissmall reduction implies that eliminating hyphens doesnot practically affect he following process.3.2 Introducing a word shape token stop listAfter cleaning is done, the system analyzes word shapetoken distribution.
Word shape tokens are counted onthe hypothesis that frequent ones correspond to wordsthat represent content; however, tokens that correspondto function words are also very frequent.
One problemawaiting solution is that of developing a technique toseparate these two classes.In tiffs paper, we define function words as the set of{prepositions, determiners, conjunctions, pronouns,modals, be and have surface forms }, and content wordsas the set of {nouns, verbs (excluding modals and beand have surface forms), adjectives }.Words that belongin both categories are defined as function words.
Weexclude adverbs from both, because they sometimesbehave as function words and sometimes as contentwords.
Words that can be adverbs but also can be eithera function or a context word are not counted as adverbs.In English, function words tend to be short whereascontent words tend to be long.
For the purpose of inves-tigating characteristics of function and content words incharacter shape code representation, wecompiled a lex-icon of 71,372 distinct word shape token entries froman ASCII-represented lexicon of 245,085 word entrieswhich was provided by Xerox PARC and was modifiedin our laboratory.
254 word shape token entries of thelexicon correspond to 515 function words, 63,356entries correspond to 226,648 content words, and 209entries correspond to both function and content words.Finally, 8,921 word shape token entries correspond to17,922 adverbs.
Figure 3 shows the distribution of wordshape token length.
Frequency of occurrence of wordshape tokens was not taken into account; that is, wesimply counted the length of each entry and computedthe population ratio.
The distribution of content wordsis apparendy different from that of function words.
Inthe figure, we also record the distribution of word shapetokens corresponding to the 100 most frequent words(75 function words, 16 content words, and 9 adverbs)from the source (Carroll et al, 1971).
It illustrates thatvery common words are short.00.4g.0 .3t -  ~ 0.2==0.1~0.0mA -" "- function wordsI I  `7 "7 content words0 5 10 15 20  25The length of word shape tokenFigure 3: Word shape token length distributionA stop list of the most common function wordshape tokens was constructed so that they could beremoved from sequences of word shape tokens.
It isimportant to select he right word shape tokens for thislist, which must selectively remove more functionwords than content words.
In general, the larger the list,the more it removes both function and content words.Thinking back to our goal of finding frequent contentwords, we don't need to try to remove all functionwords.
We need only to remove the function words thatare nsually more frequent than content-representing fre-quent words in the text on the assumption that the fre-quency of individual function words is almostindependent of topic.
Infrequent function words thatremain after using the word shape token stop list aredistinguishable from frequent content words by com-paring their frequencies.We generated a word shape token stop list usingCarroll's list of frequent function words.
We selectedseveral sets of the most freq~ent function words, bylimiting the minimum frequency of words in the set to1%, 0.5%, 0.1%, 0.09%, 0.08% ..... 0%, then convertedthem into word shape tokens.
We tested these wordshape tokens on the aforementioned lexicon to countthe number of matching entries.
Table 2 gives part ofthe results, where Freq.FW stands for frequencies ofthe selected function words, # FW for the number ofthem, # stop-tokens for the number of word shapetokens derived from them, FW.Match for a ratio of thenumber of matching function words to the total numberof function words in the lexicon (515), and CW.Matchfor a ratio of the number of matching content words tothe total number of content words (226,648).
A wordshape token stop list, for instance, from function wordswhose frequencies are more than 0.5% removes 0.4%of content words and 18% of function words from thelexicon; a word shape token stop list from functionwords with frequencies more than 0.01% removes 4.2%24of content and 56% of function words; and a wordshape token stop list from all function words in the lexi-con removes 9.5% of content words.Table 2: Application of word shape token stop list tolexiconFW.
CW.Freq.FW # FW # stop- Match  Match tokens (%) (%) (%)> 1 7 6 6.2 0.1> 0.5 19 15 18 0.4> O.
1 77 44 39 1.8> 0.09 81 44 39 1.8> 0.07 87 47 41 2.4iiiiiiiiii ii iiiiiiiiiiii i ii!
l i  iiiiiiill iiiiiiiiiiiiii!
iiiiiiiiiiiiiiii !
iiiiii !
!iiiiiiii iiiiiiiiiiii!
iiiiiiiiiii> 0.03 115 68 49 3.7> 0.01 153 95 56 4.2> 0 515 254 100 9.5Function words (frequency > 0.05%)the of and a to in is you that it he for was on are as withhis they at be this from I have or by one had but whatall were when we there can an your which their ff willeach about up out them she many some so these wouldother into has her like him could no than been its whonow my over down only may after where most hroughbefore our me any same around another must becausesuch off every between should under us along whilemight next below something both few thoseWord shape token stop list f rem above wordsAAx xA xxA x Ax ix gxx AAxA iA Axx xxx xxxiAA Aix AAxg AAix Axxx A Ag AxA xAxAxAA xxxx xAxx AAxxx gxxx xAixA AAxixxxxA xAxxA xg AAxx xAx xxxg xxxAA xAAxxixAx AiAx lAx xxAg xxg xAxxx AAxxxgAAxAxxx xxxxxA xxxAAxx Axxxxxx xxxxgAxAxxxx xAxxAA xxAxx xAxxg xAiAx xigAAAxAxx xxxxAAixg AxAAFigure 4: Selected function words and word shapetoken stop listWe also tested these word shape token stop lists onASCII encoded documents, and discovered that goodresults are obtained with the lists derived from functionwords with frequencies of more than 0.05%.
This listidentifies all words that occur more than 5 times per10,000 in the document.
Figure 4 shows the selectedfunction words and the corresponding word shape tokenstop list.
The number of stop tokens is 57 for 101 ftme-lion words.
Table 2 shows that the list removes 2.9% ofcontent words and 44% of function words from the lex-icon.3.3 Augmentation of the word shape token stop listIn our character classification, all numeric charactersare represented by the character shape code A (Table 1).Therefore, after cleaning is done, all numerals in a textfall into word shape tokens A*, where * means zero ormore repetitions of A.
This sometimes makes the fre-quency of A* unreasonably high though numerals areoften of little importance in content identification.A* matches all numerals, but since it matches fewcontent words except for acronyms in capital etters, wedecided to add A* to the word shape token stop list.Table 3: Testing the word shape token stop list onsample documentsCW.1 CW.R FW.I FW.Rsample --' CW.2 (%) --, FW.2 (%)doe.1 347 -* 321 7.5 74 --~ 18 76doe.2 246 --* 221 10 63 --~ 7 89doe.3 245 --* 225 8.2 61 --* 10 85doe.4 292 --* 272 6.8 61 --* 7 89doe.5 279 -* 265 5.0 71 --* 16 78doe.6 255 ~ 236 7.5 56 --* 12 79doe.7 177 --* 164 7.3 53 --* 14 74doe.8 253 --* 231 8.7 71 -~ 17 76doe.9 227 --* 214 5.7 64 --* 11 83doe.10 239 --* 218 8.8 63 --* 14 78doe.ll 233 ~ 212 9.0 62 --* 10 84doe.12 294 -* 265 9.9 58 --* 12 79doe.13 233-~ 212 9.0 57 ~ 12 79doe.14 271 --* 248 8.5 59 --* 13 78doe.15 130 -* 115 12 42 --* 5 88doe.16 1582--* 1513 4.4 150--* 45 70doe.17 453 --* 409 9.7 99 --* 17 83doe.18 292 --~ 249 15 75 --* 8 89doe.19 1189 --* 1046 12 157-~ 35 78doc.20 309 -* 286 7.4 73 -~ 6 923.4 Testing the word shape token stop listOur word shape token stop list was tested on 20 ASCIIencoded sample documents, ranged in length from 571to 13,756 words, from a variety of sources includingbusiness reports, travel guides, advertisements, echni-25cal reports, and private letters.
First, we generated wordshape tokens, and cleaned as described earlier.
Next, weremoved tokens that were on the word shape token stoplist.
Table 3 shows the number of content and functionwords which the documents consist of before and afterusing the list.
In the table, CW.1 and CW.2 stand for thenumber of distinct content words in the original docu-ment and the number after using the word shape tokenstop list, respectively.
CW.R stands for a ratio of(CW.1 - CW.2) to CW.1.
Similarly, FW.1 and FW.2stand for the number of distinct function words beforeand after using the list, and FW.R is a ratio of (FW.1 -FW.2) to FW.1.
FW.R is much larger than CW.R inall sample documents, which shows the good perfor-mance of the word shape token stop list.
We should notethat the values of CW.R are larger than the 2.9% thatwe get from testing the list on the lexicon.
This isbecause the lexicon includes many uncommon wordsand these tend to be longer than the function wordsselected to make the word shape token stop list.
Thisimplies that our list removes more ordinary contentwords than uncommon ones.
We believe that removingsuch words affects content identification little sinceordinary content words in many topics usually don'tOriginal document:(count)67 AAx55Ax35 AAAA33 xA32xxA31 ix29Axx28 xxxx23 AA22 AxiAAixg22 A19 xxxA18 AAA16x16 gxxx16 Ag14xxx12 xxxxAxxxAixx10 xxgxxAxA9xxword shape token ranking and corresponding words{the, The}{ to, be, Fr, In, As, On, An, (e}{ 1988, 1989, 2000, 1990, 1987, +7%), +5%), +28%, +27%, +18%, +14%}{of, at}{and, out, not, end, act}{in, is}{ for, due, ten, low, For, Rnz }{some, over, were, more, same, rose, ease }{6%, 5%, At, 9%, 7%, 4%, 3%, 1%, 8%, 2%, 11, 10, +6}Ibuilding, Building}{4, 9, 8, 6, 3, 2, 1, 0, R, A, 5}{ work, real, cost, such, much, most }{90%, 83%, 847, 80%, 5%), 49%, 29%, 27%, 26%, 23%, 21%, 19%, 175, 14%, 13%}la, s}{ year, pace, grew }{by, By}!
was, are, new, can, saw, own, one }{ construction }{ expanded, expected, reported }{on, as, or}After using the word shape token stop list: word sha22 AxiAAixg {building, Building}12 xxxxAxxxAixx {construction}10 xxgxxAxA {expanded, expected, reported}8 xxxAxx { sector, number }7 xxgixxxxixg { engineering }7 xxAxxx { volume, orders, return }7 ixAxxAxixA { industrial }7 gxxxAA {growth}7 grdxxx {prices}7 AxAxAxx { October }6 xxxxix~ learnings}6 AxxAx { trade }6 AxxAAxg { backlog }token ranking and corresponding words6 Aixxx {firms, Since}6 AixxA {first, fixed}5 ixxxxxxx {increase}5 AxxxxA {demand, traced, lowest}5 Axxxixg {housing}5 AiAAixx {billion}4 xxxAxxxAx {contracts}4 xxAx {rate }4 ixAxxixx { interior }4 gxxxxAixg {preceding}4 gxixA {point}4 AxxxxAxx {branches}4 Axixx { Swiss }Figure 5: Most frequent word shape tokens and corresponding words26specify the content of the document well (Salton et al,1975).
Likewise, the values of FW.R are larger Chart44% for the same reason.After using the word shape token stop list, wecounted the remaining tokens to obtain content-repre-senting words in their frequency order.
All samples uc-cessfully indicated their content by appropriate words.Data for a sample document reporting the growthof the building industry in Switzerland in 1988 and itsoutlook for 1989, consisting of 1013 words, are shownin figure 5.
It shows top frequent word shape tokenranking of the original document and the new rankingafter using the word shape token stop list.
The numberof removed tokens was 544.
Most of them representedcommon words and numerals.
The top ranking afterusing the word shape token stop list consists of contentwords and represents he content of the document muchbetter than the ranking before using it.Figure 5 also suggests that we can inexpensivelylocate key words by performing OCR on the few fre-quent word shape tokens.4 Conclusions and further directionsGenerating word shape tokens from images is inexpen-sive and robust for real-world ocuments.
Word shapetokens do not carry alphabetical information, but theyare potentially usable for content identification by locat-ing content-representing word images.
Our method usesa word shape token stop list and analyzes the dislribu-tion of tokens.
This technique depends on the observa-tion that, in English, the characteristics of word shapediffer between function and content words, and betweenfrequent and infrequent words.We expect o be able to extend the technique tomany other European languages that have similar char-acteristics.
For example, German function words tendto be shorter than nouns, which are always capitalized.In addition, by drawing on our language determinationtechnique, which uses the same word shape tokens(Nakayama nd Spitz, 1993; Sibun and Spitz, this vol-ume), we could enhance the technique described herefor multilingual sources.Other future work involves examining automaticdocument categorization i which an input documentimage is assigned to some pre-existing subject category(Cavnar and Trenlde, 1994).
With reliable training data,we feel we can identify the configuration ofword shapetokens across subjects.
Using a statistical method tocompute the distance between input and configurationsof categories would be a good approach.
This might beuseful for document sorting service for fax machines.AcknowledgmentsThe author gratefully acknowledges helpful suggestionsby Larry Spitz and Penni Sibun.ReferencesJohn B. Carroll, Peter Davies, and Barry Richman, TheAmerican Heritage word frequency book, Boston,Houghton-Mifflin, 1971.William B. Cavnar and John M. Trenkle, N-Gram-Based TextCategorization, Proceedings of the Third AnnualSymposium on Document Analysis and InformationRetrieval, Las Vegas, U.S.A., 1994.Douglass R. Cutting, David R. Karger, and Jan O. Pedersen,Constant Interaction-Tune Scatter/Gather B owsing ofVery Large Document Collections, Proceedings of the16th Annual International ACM SIGIR Conference,Pittsburgh, U.S.A., 1993.Paul S. Jacob, Joining Statistics with NLP for Text Categori-zation, Proceedings of the Third Conference on AppliedNatural Language Processing, Trento, Italy, 1992.Shunji Mori, Ching Y. Suen, and Kazuhiko Yamamoto, His-torical Review of OCR Research and Development, Pro-ceedings of IEEE, Vol.
80, No.
7, 1992.George Nagy, At the Frontiers of OCR, Proceedings of IEEE,Vol.
80, No.
7, 1992.Takehiro Nakayama nd A. Lawrence Spitz, European Lan-guage Determination from Image, Proceedings of theSecond International Conference on Document Analysisand Recognition, Tsukuba Science City, Japan, 1993.Lawrence O'Gorman and Rangachar Kasturi, DocumentImage Analysis Systems, Computer July 1992 Vol.
25.Stephen V. Rice, Junichi Kanai and Thomas A. Nartker, AnEvaluation of OCR Accuracy, Information ScienceResearch Institute 1993 Annual Research Report, Univer-sity of Nevada, Las Vegas.Gerard Salton, Developments in Automatic Text Retrieval,Science Vol.
253, No.
5023, Aug. 30, 1991.Gerard Salton, A. Wong, and C. S. Young, A Vector SpaceModel for Automatic Indexing, Communications of theACM November 1975 Vol.
18.Penelope Sibun and David S. Farrar, Content CharacterizationUsing Word Shape Tokens, Proceedings of the 15th Inter-national Conference on Computational Linguistics,Kyoto, Japan, 1994.Penelope Sibun and A. Lawrence Spitz, Language Determina-tion: Natural Language Processing from Scanned Docu-ment Images, this volume.A.
Lawrence Spitz, Generalized Line Word and CharacterFinding, Proceedings of the International Conference onImage Analysis and Processing, Bad, Italy, 1993.A.
Lawrence Spitz, Using Character Shape Codes for WordSpotting in Document Images, Proceedings of the ThirdInternational Workshop on Syntactic and StructuralPattern Recognition, Haifa, Israel, 1994.27
