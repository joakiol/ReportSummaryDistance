I n tegrat ion  Of  V i sua l  In ter -word  Const ra in ts  AndL ingu is t i c  Knowledge  In  Degraded Text  Recogn i t ionTao HongCenter  of  Exce l lence  for Document  Ana lys i s  and  Recogn i t ionDepar tment  of Computer  Sc ienceSta te  Un ivers i ty  of  New York  at  Buffa lo,  Buf fa lo,  NY  14260t aohong@cs ,  bu f fa lo ,  eduAbst rac t  1 2 3 4Degraded text recognition is a difficult task.
Given a Please f in  in t i renoisy text image, a word recognizer can be applied to 0.90 0.33 0.30 0.80Fleece fill In toegenerate several candidates for each word image.
High- o. o5 o.
30 o.
28 o. iolevel knowledge sources can then be used to select a Pierce flu io liredecision from the candidate set for each word image.
0.02 0.21 0.25 0.05In this paper, we propose that visual inter-word con- Fierce f l i t  i l l  thestraints can be used to facilitate candidate selection.
0.02 o.
10 o.
13 0.03Visual inter-word constraints provide a way to link word Pieces t i l l  Io Ikeimages inside the text page, and to interpret hem sys- 0.01 0.o6 0.04 0.02tematically.In t roduct ionThe objective of visual text recognition is to transforman arbitrary image of text into its symbolic equivalentcorrectly.
Recent technical advances in the area of doc-ument recognition have made automatic text recogni-tion a viable alternative to manual key entry.
Given ahigh quality text page, a commercial document recog-nition system can recognize the words on the page ata high correct rate.
However, given a degraded textpage, such as a multiple-generation photocopy or fac-simile, performance usually drops abruptly(\[1\]).Given a degraded text image, word images can be ex-tracted after layout analysis.
A word image from a de-graded text page may have touching characters, brokencharacters, distorted or blurred characters, which maymake the word image difficult to recognize accurately.After character recognition and correction based on dic-tionary look-up, a word recognizer will provide one ormore word candidates for each word image.
Figure 1lists the word candidate sets for the sentence, "Pleasefill in the application form."
Each word candidate hasa confidence score, but the score may not be reliablebecause of noise in the image.
The correct word candi-date is usually in the candidate set, but may not be thecandidate with the highest confidence score.
Instead ofsimply picking up the word candidate with the high-est recognition score, which may make the correct ratequite low, we need to find a method which can select acandidate for each word image so that the correct ratecan be as high as possible.Contextual information and high-level knowledge canbe used to select a decision word for each word image5 6 7application farm !0.90 0.35applicators form0.05 0.30acquisition forth0.03 0.20duplication foam0.01 0.11implication force0.01 0.04Figure 1: Candidate Sets for the Sentence:in the application form/""Please fillin its context.
Currently, there are two approaches,the statistical approach and the structural approach,towards the problem of candidate selection.
In the sta-tistical approach, language models, such as a HiddenMarker Model and word collocation can be utilized forcandidate selection (\[2, 4, 5\]).
In the structural ap-proach, lattice parsing techniques have been developedfor candidate selection(\[3, 7\]).The contextual constraints considered in a statisti-cal language model, such as word collocation, are localconstraints.
For a word image, a candidate will be se-lected according to the candidate information from itsneighboring word images in a fixed window size.
Thewindow size is usually set as one or two.
In the latticeparsing method, a grammar is used to select a candi-date for each word image inside a sentence so that thesequence of those selected candidates form a grammat-ical and meaningful sentence.
For example, considerthe sentence "Please fill in the application form".
Weassume all words except the word "form" have beenrecognized correctly and the candidate set for the word"form" is { farm, form, forth, foam, forth } (see thesecond sentence in Figure 2).
The candidate "form"can be selected easily because the collocation between"application" and "form" is strong and the resultingsentence is grammatical.The contextual information inside a small window orinside a sentence sometimes may not be enough to selecta candidate correctly.
For example, consider the sen-328Sentence 11This2farmformforthfoamforce11 12Please f i l l3 4 5 6 7 8 9 10is almost the same as that oneSentence 213 14 15 16 17in the application farm !formforthfoamforceFigure 2: Word candidates of two example sen-tences(word images 2 and 16 are similar)?skill; it  iologica-t)ly based.
LanKua zeis  ometh ing-G'K  bornhow to,'g_,,_zo_v?.
Yet hypofl\esis that%h re   t9Io:gicat unde.
innings tohuman linguistic abili W does not ex-plain eve,-:ything.
There may indeedversal elements.
All ,k((dK'f tan,ma -,es zs@cer tam orgamz a tional principles.Figure 3: Part of text page with three sentencestence "This form is almost the same as that one"(seethe first sentence in Figure 2).
Word image 16 has fivecandidates: { farm, form, forth, foam, forth }.
Afterlattice parsing, the candidate "forth" will be removedbecause it does not fit the context.
But it is difficultto select a candidate from "farm", "form" "foam" and"force" because ach of them makes the sentence gram-matical and meaningful.
In such a case, more contex-tual constraints are needed to distinguish the remainingcandidates and to select the correct one.Let's further assume that the sentences in Figure 2are from the same text.
By image matching, we knowword images 2 and 16 are visually similar.
If two wordimages are almost the same, they must be the sameword.
Therefore, same candidates must be selected forword image 2 and word image 16, After "form" is chosenfor image 16 it can also be chosen as the decision forimage 2.Possible Relations between W1.
and W2type at symbolic level at image levelW1--W2W2=XeWleYprefix_of(W1) =prefix_of(W2)suf yiz_oy(W1) =~u y yix_o y(W2 )suyyiz_of(WQ =prefiz_of(W~)Note 1: "~" means approximatelyNote 2: "e" means concatenation.VV-~ ~ W2W1 ~ subimage_of(W2)left_part_of(W1) ,~left_part_of(W2)right_part_of(W1)right_part_of(W2)right_part_of(W1) ,.~left_part_of(W2)image matching;Table 1: Possible Inter-word RelationsV isua l  In ter -Word  Re la t ionsA visual inter-word relation can be defined between twoword images if they share the same pattern at the imagelevel.
There are five types of visual inter-word relationslisted in the right part of Table 1.
Figure 3 is a part ofa scanned text image in which a small number of wordrelations are circled to demonstrate the abundance ofinter-word relations defined above even in such a smallfragment of a real text page.
Word images 2 and 8 arealmost the same.
Word image 9 matches the left partof word image 1 quite well.
Word image 5 matches apart of the image 6, and so on.Visual inter-word relations can be computed by ap-plying simple image matching techniques.
They can becalculated in clean text images, as well as in highly de-graded text fmages, because the word images, due totheir relatively large size, are tolerant o noise (\[6\]).Visual inter-word relations can be used as constraintsin the process of word image interpretation, especiallyfor candidate selection.
It is not surprising that wordrelations at the image level are highly consistent withword relations at the symbolic level(see Table 1).
If twowords hold a relation at the symbolic level and they arewritten in the same font and size, their word imagesshould keep the same relation at the image level.
Andalso, if two word images hold a relation at the imagelevel, the truth values of the word images should havethe same relation at the symbolic level.
In Figure 3,word images 2 and 8 must be recognized as the sameword because they can match each other; the identityof word image 5 must be a sub-string of the identity ofword image 6 because word image 5 can match with apart of word image 6; and so on.Visual inter-word constraints provide us a way to linkword images inside a text page, and to interpret hemsystematically.
The research discussed in this paper in-tegrates visual inter-word constraints with a statisticallanguage model and a lattice parser to improve the per-formance of candidate selection.329Current  S ta tus  o f  WorkA word-collocation-based relaxation algorithm anda probabilistic lattice chart parser have been de-signed for word candidate selection in degraded textrecognition(\[3, 4\]).
The relaxation algorithm runs iter-atively.
In each iteration, the confidence score of eachcandidate is adjusted based on its current confidenceand its collocation scores with the currently most pre-ferred candidates for its neighboring word images.
Re-laxation ends when all candidates reach a stable state.For each word image, those candidates with a low con-fidence score will be removed from the candidate sets.Then, the probabilistic lattice chart parser will be ap-plied to the reduced candidate sets to select the can-didates that appear in the most preferred parse treesbuilt by the parser.
There can be different strategies touse visual inter-word constraints inside the relaxationalgorithm and the lattice parser.
One of the strategieswe are exploiting is to re-evaluate the top candidatesfor the related word images after each iteration of re-laxation or after lattice parsing.
If they hold the samerelation at the symbolic level, the confidence scores ofthe candidates will be increased.
Otherwise, the imageswith a low confidence score will follow the decision ofthe images with a high confidence score.Five articles from the Brown Corpus were chosen ran-domly as testing samples.
They are A06, GO2, J42, NO1and ROT, each with about %000 words.
Given a wordimage, our word recognizer generates its top10 candi-dates from a dictionary with 70,000 different entries.In preliminary experiments, we exploit only the type-1relation listed in Table 1.
After clustering word im-ages by image matching, similar images will be in thesame cluster.
Any two images from the same clusterhold the type-1 relation.
Word collocation data weretrained from the Penn Treebank and the Brown Cor-pus except for the five testing samples.
Table 2 showsresults of candidate selection with and without usingvisual inter-word constraints.
The top1 correct rate forcandidate lists generated by a word recognizer is as lowas 57.1%, Without using visual inter-word constraints,the correct rate of candidate selection by relaxation andlattice parsing is 83.1%.
After using visual inter-wordconstraints, the correct rate becomes 88.2%.ArticleNumberOfWordsA06 2213G02 2267J42 2269N01 2313R07 2340Total 11402WordRecognitionResult53.8%67.7%54.5%57.3%52.2%57.1%Candidate SelectionUsing No UsingConstraints Constraints83.1% 88.5%83.8% 87.8%83.6% 89.5%82.7% 87.1%82.6% 88.1%83.1% 88.2%Table 2: Comparison Of Candidate Selection ResultsConc lus ions  and  Future  D i rec t ionsIntegration of natural language processing and imageprocessing is a new area of interest in document anal-ysis.
Word candidate selection is a problem we arefaced with in degraded text recognition, as well as inhandwriting recognition.
Statistical anguage modelsand lattice parsers have been designed for the prob-lem.
Visual inter-word constraints in a text page canbe used with linguistic knowledge sources to facilitatecandidate selection.
Preliminary experimental resultsshow that the performance of candidate selection is im-proved significantly although only one inter-word rela-tion was used.
The next step is to fully integrate visualinter-word constraints and linguistic knowledge sourcesin the relaxation algorithm and the lattice parser.AcknowledgmentsI would like to thank Jonathan J.
Hull for his supportand his helpful comments on drafts of this paper.Re ferences\[1\] Henry S. Baird, "Document Image Defect Modelsand Their Uses," in Proceedings of the Second In-ternational Conference on Document Analysis andRecognition ICDAR-93, Tsukuba, Japan, October20-22, 1993, pp.
62-67.\[2\] Kenneth Ward Church and Patrick Hanks, "WordAssociation Norms, Mutual Information, and Lexi-cography," Computational Linguistics, Vol.
16, No.1, pp.
22-29, 1990.\[3\] Tao Hong and Jonathan J.
Hull, "Text RecognitionEnhancement with a Probabilistic Lattice ChartParser," in Proceedings of the Second InternationalConference on Document Analysis and RecognitionICDAR-93, Tsukuba, Japan, October 20-22, 1993.\[4\] Tao Hong and Jonathan J.
Hull, "Degraded TextRecognition Using Word Collocation," in Pro-ceedings of IS~T/SPIE Symposium on DocumentRecognition, San Jose, CA, February 6-10, 1994.\[5\] Jonathan J.
Hull, "A Hidden Markov Model forLanguage Syntax in Text Recognition," in Pro-ceedings of l lth IAPR International Conference onPattern Recognition, The Hague, The Netherlands,pp.124-127, 1992.\[6\] Siamak Khoubyari and Jonathan J.
Hull, "KeywordLocation in Noisy Document Image," in Proceed-ings of the Second Annual Symposium on Docu-ment Analysis and Information Retrieval, Las Ve-gas, Nevada, pp.
217-231, April 26-28, 1993.\[7\] Masaru Tomita, "An Efficient Word Lattice Pars-ing Algorithm for Continuous Speech Recognition,"in Proceedings of the International Conference onAcoustic, Speech and Signal Processing, 1986.330
