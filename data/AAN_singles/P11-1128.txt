Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1278?1287,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAdjoining Tree-to-String TranslationYang Liu, Qun Liu, and Yajuan Lu?Key Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{yliu,liuqun,lvyajuan}@ict.ac.cnAbstractWe introduce synchronous tree adjoininggrammars (TAG) into tree-to-string transla-tion, which converts a source tree to a targetstring.
Without reconstructing TAG deriva-tions explicitly, our rule extraction algo-rithm directly learns tree-to-string rules fromaligned Treebank-style trees.
As tree-to-stringtranslation casts decoding as a tree parsingproblem rather than parsing, the decoder stillruns fast when adjoining is included.
Lessthan 2 times slower, the adjoining tree-to-string system improves translation quality by+0.7 BLEU over the baseline system only al-lowing for tree substitution on NIST Chinese-English test sets.1 IntroductionSyntax-based translation models, which exploit hi-erarchical structures of natural languages to guidemachine translation, have become increasingly pop-ular in recent years.
So far, most of them havebeen based on synchronous context-free grammars(CFG) (Chiang, 2007), tree substitution grammars(TSG) (Eisner, 2003; Galley et al, 2006; Liu etal., 2006; Huang et al, 2006; Zhang et al, 2008),and inversion transduction grammars (ITG) (Wu,1997; Xiong et al, 2006).
Although these for-malisms present simple and precise mechanisms fordescribing the basic recursive structure of sentences,they are not powerful enough to model some impor-tant features of natural language syntax.
For ex-ample, Chiang (2006) points out that the transla-tion of languages that can stack an unbounded num-ber of clauses in an ?inside-out?
way (Wu, 1997)provably goes beyond the expressive power of syn-chronous CFG and TSG.
Therefore, it is necessaryto find ways to take advantage of more powerful syn-chronous grammars to improve machine translation.Synchronous tree adjoining grammars (TAG)(Shieber and Schabes, 1990) are a good candidate.As a formal tree rewriting system, TAG (Joshi et al,1975; Joshi, 1985) provides a larger domain of lo-cality than CFG to state linguistic dependencies thatare far apart since the formalism treats trees as basicbuilding blocks.
As a mildly context-sensitive gram-mar, TAG is conjectured to be powerful enough tomodel natural languages.
Synchronous TAG gener-alizes TAG by allowing the construction of a pairof trees using the TAG operations of substitutionand adjoining on tree pairs.
The idea of using syn-chronous TAG in machine translation has been pur-sued by several researchers (Abeille et al, 1990;Prigent, 1994; Dras, 1999), but only recently inits probabilistic form (Nesson et al, 2006; De-Neefe and Knight, 2009).
Shieber (2007) argues thatprobabilistic synchronous TAG possesses appealingproperties such as expressivity and trainability forbuilding a machine translation system.However, one major challenge for applying syn-chronous TAG to machine translation is computa-tional complexity.
While TAG requires O(n6) timefor monolingual parsing, synchronous TAG requiresO(n12) for bilingual parsing.
One solution is to usetree insertion grammars (TIG) introduced by Sch-abes and Waters (1995).
As a restricted form ofTAG, TIG still allows for adjoining of unboundedtrees but only requires O(n3) time for monolingualparsing.
Nesson et al (2006) firstly demonstrate1278o?zo?ngto?ngNNNPPresidentX,?1{Ime?iguo?NRNPUSX,?2NP?
NP?NPX?
X?X,?1NPNP?
NPNNo?zo?ngto?ngXX?
XPresident,?2NPNPNR{Ime?iguo?NPNNo?zo?ngto?ngXXUSXPresident,?3Figure 1: Initial and auxiliary tree pairs.
The source side (Chinese) is a Treebank-style linguistic tree.
The target side(English) is a purely structural tree using a single non-terminal (X).
By convention, substitution and foot nodes aremarked with a down arrow (?)
and an asterisk (?
), respectively.
The dashed lines link substitution sites (e.g., NP?
andX?
in ?1) and adjoining sites (e.g., NP and X in ?2) in tree pairs.
Substituting the initial tree pair ?1 at the NP?-X?node pair in the auxiliary tree pair ?1 yields a derived tree pair ?2, which can be adjoined at NN-X in ?2 to generate?3.the use of synchronous TIG for machine translationand report promising results.
DeNeefe and Knight(2009) prove that adjoining can improve translationquality significantly over a state-of-the-art string-to-tree system (Galley et al, 2006) that uses syn-chronous TSG with tractable computational com-plexity.In this paper, we introduce synchronous TAG intotree-to-string translation (Liu et al, 2006; Huang etal., 2006), which is the simplest and fastest amongsyntax-based approaches (Section 2).
We proposea new rule extraction algorithm based on GHKM(Galley et al, 2004) that directly induces a syn-chronous TAG from an aligned and parsed bilingualcorpus without converting Treebank-style trees toTAG derivations explicitly (Section 3).
As tree-to-string translation takes a source parse tree as input,the decoding can be cast as a tree parsing problem(Eisner, 2003): reconstructing TAG derivations froma derived tree using tree-to-string rules that allow forboth substitution and adjoining.
We describe how toconvert TAG derivations to translation forest (Sec-tion 4).
We evaluated the new tree-to-string systemon NIST Chinese-English tests and obtained con-sistent improvements (+0.7 BLEU) over the STSG-based baseline system without significant loss in ef-ficiency (1.6 times slower) (Section 5).2 ModelA synchronous TAG consists of a set of linked ele-mentary tree pairs: initial and auxiliary.
An initialtree is a tree of which the interior nodes are all la-beled with non-terminal symbols, and the nodes onthe frontier are either words or non-terminal sym-bols marked with a down arrow (?).
An auxiliarytree is defined as an initial tree, except that exactlyone of its frontier nodes must be marked as footnode (?).
The foot node must be labeled with a non-terminal symbol that is the same as the label of theroot node.Synchronous TAG defines two operations to buildderived tree pairs from elementary tree pairs: substi-tution and adjoining.
Nodes in initial and auxiliarytree pairs are linked to indicate the correspondencebetween substitution and adjoining sites.
Figure 1shows three initial tree pairs (i.e., ?1, ?2, and ?3)and two auxiliary tree pairs (i.e., ?1 and ?2).
Thedashed lines link substitution nodes (e.g., NP?
andX?
in ?1) and adjoining sites (e.g., NP and X in ?2)in tree pairs.
Substituting the initial tree pair ?1 at1279{Ime?iguo?o?zo?ngto?ngn?a`oba?ma??du`?l?qia?ngj????sh`?jia`n??yu?y?
?gIqia?nze?0 1 2 3 4 5 6 7 8NR NN NR P NN NN VV NNNP NP NP NP NPNP PP VPNP VPIPUS President Obama has condemned the shooting incidentFigure 2: A training example.
Tree-to-string rules can be extracted from shaded nodes.node minimal initial rule minimal auxiliary ruleNR0,1 [1] ( NR me?iguo? )
?
USNP0,1 [2] ( NP ( x1:NR? )
) ?
x1NN1,2 [3] ( NN zo?ngto?ng ) ?
PresidentNP1,2 [4] ( NP ( x1:NN? )
) ?
x1[5] ( NP ( x1:NP? )
( x2:NP? )
) ?
x1 x2[6] ( NP0:1 ( x1:NR? )
) ?
x1 [7] ( NP ( x1:NP? )
( x2:NP? )
) ?
x1 x2NP0,2 [8] ( NP0:2 ( x1:NP? )
( x2:NP? )
) ?
x1 x2[9] ( NP0:1 ( x1:NN? )
) ?
x1 [10] ( NP ( x1:NP? )
( x2:NP? )
) ?
x1 x2[11] ( NP0:2 ( x1:NP? )
( x2:NP? )
) ?
x1 x2NR2,3 [12] ( NR a`oba?ma? )
?
ObamaNP2,3 [13] ( NP ( x1:NR? )
) ?
x1[14] ( NP ( x1:NP? )
( x2:NP? )
) ?
x1 x2[15] ( NP0:2 ( x1:NP? )
( x2:NP? )
) ?
x1 x2 [16] ( NP ( x1:NP? )
( x2:NP? )
) ?
x1 x2NP0,3 [17] ( NP0:1 ( x1:NR? )
) ?
x1 [18] ( NP ( x1:NP? )
( x2:NP? )
) ?
x1 x2[19] ( NP0:1 ( x1:NN? )
) ?
x1[20] ( NP0:1 ( x1:NR? )
) ?
x1NN4,5 [21] ( NN qia?ngj?? )
?
shootingNN5,6 [22] ( NN sh?`jia`n ) ?
incidentNP4,6 [23] ( NP ( x1:NN? )
( x2:NN? )
) ?
x1 x2PP3,6 [24] ( PP ( du?` ) ( x1:NP? )
) ?
x1NN7,8 [25] ( NN qia?nze? )
?
condemnedNP7,8 [26] ( NP ( x1:NN? )
) ?
x1VP6,8 [27] ( VP ( VV yu?y?? )
( x1:NP? )
) ?
x1[28] ( VP ( x1:PP? )
( x2:VP? )
) ?
x2 the x1VP3,8 [29] ( VP0:1 ( VV yu?y?? )
( x1:NP? )
) ?
x1 [30] ( VP ( x1:PP? )
( x2:VP? )
) ?
x2 the x1IP0,8 [31] ( IP ( x1:NP? )
( x2:VP? )
) ?
x1 has x2Table 1: Minimal initial and auxiliary rules extracted from Figure 2.
Note that an adjoining site has a span as subscript.For example, NP0:1 in rule 6 indicates that the node is an adjoining site linked to a target node dominating the targetstring spanning from position 0 to position 1 (i.e., x1).
The target tree is hidden because tree-to-string translation onlyconsiders the target surface string.1280the NP?-X?
node pair in the auxiliary tree pair ?1yields a derived tree pair ?2, which can be adjoinedat NN-X in ?2 to generate ?3.For simplicity, we represent ?2 as a tree-to-stringrule:( NP0:1 ( NR me?iguo? )
) ?
USwhere NP0:1 indicates that the node is an adjoin-ing site linked to a target node dominating the tar-get string spanning from position 0 to position 1(i.e., ?US?).
The target tree is hidden because tree-to-string translation only considers the target surfacestring.
Similarly, ?1 can be written as( NP ( x1:NP? )
( x2:NP? )
) ?
x1 x2where x denotes a non-terminal and the subscriptsindicate the correspondence between source and tar-get non-terminals.The parameters of a probabilistic synchronousTAG are??Pi(?)
= 1 (1)??Ps(?|?)
= 1 (2)??Pa(?|?)
+ Pa(NONE|?)
= 1 (3)where ?
ranges over initial tree pairs, ?
over aux-iliary tree pairs, and ?
over node pairs.
Pi(?)
isthe probability of beginning a derivation with ?;Ps(?|?)
is the probability of substituting ?
at ?;Pa(?|?)
is the probability of adjoining ?
at ?
; fi-nally, Pa(NONE|?)
is the probability of nothing ad-joining at ?.For tree-to-string translation, these parameterscan be treated as feature functions of a discrimi-native framework (Och, 2003) combined with otherconventional features such as relative frequency, lex-ical weight, rule count, language model, and wordcount (Liu et al, 2006).3 Rule ExtractionInducing a synchronous TAG from training dataoften begins with converting Treebank-style parsetrees to TAG derivations (Xia, 1999; Chen andVijay-Shanker, 2000; Chiang, 2003).
DeNeefe andKnight (2009) propose an algorithm to extract syn-chronous TIG rules from an aligned and parsedbilingual corpus.
They first classify tree nodesinto heads, arguments, and adjuncts using heuristics(Collins, 2003), then transform a Treebank-style treeinto a TIG derivation, and finally extract minimally-sized rules from the derivation tree and the string onthe other side, constrained by the alignments.
Proba-bilistic models can be estimated by collecting countsover the derivation trees.However, one challenge is that there are manyTAG derivations that can yield the same derived tree,even with respect to a single grammar.
It is difficultto choose appropriate single derivations that enablethe resulting grammar to translate unseen data well.DeNeefe and Knight (2009) indicate that the way toreconstruct TIG derivations has a direct effect on fi-nal translation quality.
They suggest that one possi-ble solution is to use derivation forest rather than asingle derivation tree for rule extraction.Alternatively, we extend the GHKM algorithm(Galley et al, 2004) to directly extract tree-to-stringrules that allow for both substitution and adjoiningfrom aligned and parsed data.
There is no need fortransforming a parse tree into a TAG derivation ex-plicitly before rule extraction and all derivations canbe easily reconstructed using extracted rules.
1 Ourrule extraction algorithm involves two steps: (1) ex-tracting minimal rules and (2) composition.3.1 Extracting Minimal RulesFigure 2 shows a training example, which consists ofa Chinese parse tree, an English string, and the wordalignment between them.
By convention, shadednodes are called frontier nodes from which tree-to-string rules can be extracted.
Note that the sourcephrase dominated by a frontier node and its corre-sponding target phrase are consistent with the wordalignment: all words in the source phrase are alignedto all words in the corresponding target phrase andvice versa.We distinguish between three categories of tree-1Note that our algorithm does not take heads, complements,and adjuncts into consideration and extracts all possible ruleswith respect to word alignment.
Our hope is that this treatmentwould make our system more robust in the presence of noisydata.
It is possible to use the linguistic preferences as features.We leave this for future work.1281to-string rules:1. substitution rules, in which the source tree isan initial tree without adjoining sites.2.
adjoining rules, in which the source tree is aninitial tree with at least one adjoining site.3.
auxiliary rules, in which the source tree is anauxiliary tree.For example, in Figure 1, ?1 is a substitution rule,?2 is an adjoining rule, and ?1 is an auxiliary rule.Minimal substitution rules are the same with thosein STSG (Galley et al, 2004; Liu et al, 2006) andtherefore can be extracted directly using GHKM.
Byminimal, we mean that the interior nodes are notfrontier and cannot be decomposed.
For example,in Table 2, rule 1 (for short r1) is a minimal substi-tution rule extracted from NR0,1.Minimal adjoining rules are defined as minimalsubstitution rules, except that each root node mustbe an adjoining site.
In Table 2, r2 is a minimalsubstitution rule extracted from NP0,1.
As NP0,1 isa descendant of NP0,2 with the same label, NP0,1is a possible adjoining site.
Therefore, r6 can bederived from r2 and licensed as a minimal adjoiningrule extracted from NP0,2.
Similarly, four minimaladjoining rules are extracted from NP0,3 because ithas four frontier descendants labeled with NP.Minimal auxiliary rules are derived from minimalsubstitution and adjoining rules.
For example, in Ta-ble 2, r7 and r10 are derived from the minimal sub-stitution rule r5 while r8 and r11 are derived fromr15.
Note that a minimal auxiliary rule can have ad-joining sites (e.g., r8).Table 1 lists 17 minimal substitution rules, 7 min-imal adjoining rules, and 7 minimal auxiliary rulesextracted from Figure 2.3.2 CompositionWe can obtain composed rules that capture rich con-texts by substituting and adjoining minimal initialand auxiliary rules.
For example, the compositionof r12, r17, r25, r26, r29, and r31 yields an initialrule with two adjoining sites:( IP ( NP0:1 ( NR a`oba?ma? )
) ( VP2:3 ( VV yu?y??
)( NP ( NN qia?nze? )
) ) ) ?
Obama has condemnedNote that the source phrase ?a`oba?ma?
.
.
.
yu?y??
qia?nze?
?is discontinuous.
Our model allows both the sourceand target phrases of an initial rule with adjoiningsites to be discontinuous, which goes beyond the ex-pressive power of synchronous CFG and TSG.Similarly, the composition of two auxiliary rulesr8 and r16 yields a new auxiliary rule:( NP ( NP ( x1:NP? )
( x2:NP? )
) ( x3:NP? )
) ?
x1x2x3We first compose initial rules and then com-pose auxiliary rules, both in a bottom-up way.
Tomaintain a reasonable grammar size, we follow Liu(2006) to restrict that the tree height of a rule is nogreater than 3 and the source surface string is nolonger than 7.To learn the probability models Pi(?
), Ps(?|?),Pa(?|?
), and Pa(NONE|?
), we collect and normal-ize counts over these extracted rules following De-Neefe and Knight (2009).4 DecodingGiven a synchronous TAG and a derived source treepi, a tree-to-string decoder finds the English yieldof the best derivation of which the Chinese yieldmatches pi:e?
= e(arg maxD s.t.
f(D)=piP (D))(4)This is called tree parsing (Eisner, 2003) as the de-coder finds ways of decomposing pi into elementarytrees.Tree-to-string decoding with STSG is usuallytreated as forest rescoring (Huang and Chiang,2007) that involves two steps.
The decoder first con-verts the input tree into a translation forest using atranslation rule set by pattern matching.
Huang etal.
(2006) show that this step is a depth-first searchwith memorization in O(n) time.
Then, the decodersearches for the best derivation in the translation for-est intersected with n-gram language models andoutputs the target string.
2Decoding with STAG, however, poses one majorchallenge to forest rescoring.
As translation forestonly supports substitution, it is difficult to constructa translation forest for STAG derivations because of2Mi et al (2008) give a detailed description of the two-stepdecoding process.
Huang and Mi (2010) systematically analyzethe decoding complexity of tree-to-string translation.1282?1IP0,8NP2,3 VP3,8?NR2,3??2NR2,3n?a`oba?ma?
?1NP0,3NP1,2 NP2,3?NN1,2??2NP0,3NP0,2?
NP2,3?
?3NP0,2NP0,1 NP1,2?NR0,1?
?3NN2,3o?zo?ngto?ngelementary tree translation rule?1 r1 ( IP ( NP0:1 ( x1:NR? )
) ( x2:VP? )
) ?
x1 x2?2 r2 ( NR a`oba?ma? )
?
Obama?1 r3 ( NP ( NP0:1 ( x1:NN? )
) ( x2:NP? )
) ?
x1 x2?2 r4 ( NP ( x1:NP? )
( x2:NP? )
) ?
x1 x2?3 r5 ( NP ( NP ( x1:NR? )
) ( x2:NP? )
) ?
x1 x2?3 r6 ( NN zo?ngto?ng ) ?
PresidentFigure 3: Matched trees and corresponding rules.
Each node in a matched tree is annotated with a span as superscriptto facilitate identification.
For example, IP0,8 in ?1 indicates that IP0,8 in Figure 2 is matched.
Note that its left childNP2,3 is not its direct descendant in Figure 2, suggesting that adjoining is required at this site.
?1?2(1.1) ?1(1) ?2(1)?3(1) ?3(1.1)IP0,8NP0,2 VP3,8NR0,1 NN1,2 NR2,3e1 e2e3 e4hyperedge translation rulee1 r1 + r4 ( IP ( NP ( x1:NP? )
( NP ( x2:NR? )
) ) ( x3:VP? )
?
x1 x2 x3e2 r1 + r3 + r5 ( IP ( NP ( NP ( x1:NP? )
( x2:NP? )
) ( NP ( x3:NR? )
) ) ( x4:VP? )
) ?
x1 x2 x3 x4e3 r6 ( NN zo?ngto?ng ) ?
Presidente4 r2 ( NR a`oba?ma? )
?
ObamaFigure 4: Converting a derivation forest to a translation forest.
In a derivation forest, a node in a derivation forest is amatched elementary tree.
A hyperedge corresponds to operations on related trees: substitution (dashed) or adjoining(solid).
We use Gorn addresses as tree addresses.
?2(1.1) denotes that ?2 is substituted in the tree ?1 at the node NR2,3?of address 1.1 (i.e., the first child of the first child of the root node).
As translation forest only supports substitution, wecombine trees with adjoining sites to form an equivalent tree without adjoining sites.
Rules are composed accordingly(e.g., r1 + r4).1283adjoining.
Therefore, we divide forest rescoring forSTAG into three steps:1. matching, matching STAG rules against the in-put tree to obtain a TAG derivation forest;2. conversion, converting the TAG derivation for-est into a translation forest;3. intersection, intersecting the translation forestwith an n-gram language model.Given a tree-to-string rule, rule matching is to finda subtree of the input tree that is identical to thesource side of the rule.
While matching STSG rulesagainst a derived tree is straightforward, it is some-what non-trivial for STAG rules that move beyondnodes of a local tree.
We follow Liu et al (2006) toenumerate all elementary subtrees and match STAGrules against these subtrees.
This can be done by firstenumerating all minimal initial and auxiliary treesand then combining them to obtain composed trees,assuming that every node in the input tree is fron-tier (see Section 3).
We impose the same restrictionson the tree height and length as in rule extraction.Figure 3 shows some matched trees and correspond-ing rules.
Each node in a matched tree is annotatedwith a span as superscript to facilitate identification.For example, IP0,8 in ?1 means that IP0,8 in Figure2 is matched.
Note that its left child NP2,3 is notits direct descendant in Figure 2, suggesting that ad-joining is required at this site.A TAG derivation tree specifies uniquely howa derived tree is constructed using elementary trees(Joshi, 1985).
A node in a derivation tree is an ele-mentary tree and an edge corresponds to operationson related elementary trees: substitution or adjoin-ing.
We introduce TAG derivation forest, a com-pact representation of multiple TAG derivation trees,to encodes all matched TAG derivation trees of theinput derived tree.Figure 4 shows part of a TAG derivation forest.The six matched elementary trees are nodes in thederivation forest.
Dashed and solid lines representsubstitution and adjoining, respectively.
We useGorn addresses as tree addresses: 0 is the addressof the root node, p is the address of the pth child ofthe root node, and p ?
q is the address of the qth childof the node at the address p. The derivation forestshould be interpreted as follows: ?2 is substituted inthe tree ?1 at the node NR2,3?
of address 1.1 (i.e., thefirst child of the first child of the root node) and ?1 isadjoined in the tree ?1 at the node NP2,3 of address1.To take advantage of existing decoding tech-niques, it is necessary to convert a derivation forestto a translation forest.
A hyperedge in a transla-tion forest corresponds to a translation rule.
Mi etal.
(2008) describe how to convert a derived treeto a translation forest using tree-to-string rules onlyallowing for substitution.
Unfortunately, it is notstraightforward to convert a derivation forest includ-ing adjoining to a translation forest.
To alleviate thisproblem, we combine initial rules with adjoiningsites and associated auxiliary rules to form equiv-alent initial rules without adjoining sites on the flyduring decoding.Consider ?1 in Figure 3.
It has an adjoining siteNP2,3.
Adjoining ?2 in ?1 at the node NP2,3 pro-duces an equivalent initial tree with only substitutionsites:( IP0,8 ( NP0,3 ( NP0,2? )
( NP2,3 ( NR2,3? )
) ) ( VP3,8? )
)The corresponding composed rule r1 + r4 has noadjoining sites and can be added to translation forest.We define that the elementary trees needed to becomposed (e.g., ?1 and ?2) form a composition treein a derivation forest.
A node in a composition tree isa matched elementary tree and an edge correspondsto adjoining operations.
The root node must be aninitial tree with at least one adjoining site.
The de-scendants of the root node must all be auxiliary trees.For example, ( ?1 ( ?2 ) ) and ( ?1 ( ?1 ( ?3 ) ) ) aretwo composition trees in Figure 4.
The number ofchildren of a node in a composition tree depends onthe number of adjoining sites in the node.
We usecomposition forest to encode all possible composi-tion trees.Often, a node in a composition tree may have mul-tiple matched rules.
As a large amount of composi-tion trees and composed rules can be identified andconstructed on the fly during forest conversion, weused cube pruning (Chiang, 2007; Huang and Chi-ang, 2007) to achieve a balance between translationquality and decoding efficiency.1284category description numberVP verb phrase 12.40NP noun phrase 7.69IP simple clause 7.26QP quantifier phrase 0.14CP clause headed by C 0.10PP preposition phrase 0.09CLP classifier phrase 0.02ADJP adjective phrase 0.02LCP phrase formed by ?XP+LC?
0.02DNP phrase formed by ?XP+DEG?
0.01Table 2: Top-10 phrase categories of foot nodes and theiraverage occurrences in training corpus.5 EvaluationWe evaluated our adjoining tree-to-string translationsystem on Chinese-English translation.
The bilin-gual corpus consists of 1.5M sentences with 42.1MChinese words and 48.3M English words.
The Chi-nese sentences in the bilingual corpus were parsedby an in-house parser.
To maintain a reasonablegrammar size, we follow Liu et al (2006) to re-strict that the height of a rule tree is no greater than3 and the surface string?s length is no greater than 7.After running GIZA++ (Och and Ney, 2003) to ob-tain word alignment, our rule extraction algorithmextracted 23.0M initial rules without adjoining sites,6.6M initial rules with adjoining sites, and 5.3Mauxiliary rules.
We used the SRILM toolkit (Stol-cke, 2002) to train a 4-gram language model on theXinhua portion of the GIGAWORD corpus, whichcontains 238M English words.
We used the 2002NIST MT Chinese-English test set as the develop-ment set and the 2003-2005 NIST test sets as thetest sets.
We evaluated translation quality using theBLEU metric, as calculated by mteval-v11b.pl withcase-insensitive matching of n-grams.Table 2 shows top-10 phrase categories of footnodes and their average occurrences in training cor-pus.
We find that VP (verb phrase) is most likelyto be the label of a foot node in an auxiliary rule.On average, there are 12.4 nodes labeled with VPare identical to one of its ancestors per tree.
NP andIP are also found to be foot node labels frequently.Figure 4 shows the average occurrences of foot nodelabels VP, NP, and IP over various distances.
A dis-tance is the difference of levels between a foot node0.00.51.01.52.02.53.03.54.04.50  1  2  3  4  5  6  7  8  9  10  11averageoccurrencedistanceVPIPNPFigure 5: Average occurrences of foot node labels VP,NP, and IP over various distances.system grammar MT03 MT04 MT05Moses - 33.10 33.96 32.17hierarchical SCFG 33.40 34.65 32.88STSG 33.13 34.55 31.94tree-to-string STAG 33.64 35.28 32.71Table 3: BLEU scores on NIST Chinese-English test sets.Scores marked in bold are significantly better that thoseof STSG at pl.01 level.and the root node.
For example, in Figure 2, the dis-tance between NP0,1 and NP0,3 is 2 and the distancebetween VP6,8 and VP3,8 is 1.
As most foot nodesare usually very close to the root nodes, we restrictthat a foot node must be the direct descendant of theroot node in our experiments.Table 3 shows the BLEU scores on the NISTChinese-English test sets.
Our baseline system is thetree-to-string system using STSG (Liu et al, 2006;Huang et al, 2006).
The STAG system outper-forms the STSG system significantly on the MT04and MT05 test sets at pl.01 level.
Table 3 alsogives the results of Moses (Koehn et al, 2007) andan in-house hierarchical phrase-based system (Chi-ang, 2007).
Our STAG system achieves compara-ble performance with the hierarchical system.
Theabsolute improvement of +0.7 BLEU over STSG isclose to the finding of DeNeefe and Knight (2009)on string-to-tree translation.
We feel that one majorobstacle for achieving further improvement is thatcomposed rules generated on the fly during decod-ing (e.g., r1 + r3 + r5 in Figure 4) usually have toomany non-terminals, making cube pruning in the in-1285STSG STAGmatching 0.086 0.109conversion 0.000 0.562intersection 0.946 1.064other 0.012 0.028total 1.044 1.763Table 4: Comparison of average decoding time.tersection phase suffering from severe search errors(only a tiny fraction of the search space can be ex-plored).
To produce the 1-best translations on theMT05 test set that contains 1,082 sentences, whilethe STSG system used 40,169 initial rules withoutadjoining sites, the STAG system used 28,046 initialrules without adjoining sites, 1,057 initial rules withadjoining sites, and 1,527 auxiliary rules.Table 4 shows the average decoding time on theMT05 test set.
While rule matching for STSG needs0.086 second per sentence, the matching time forSTAG only increases to 0.109 second.
For STAG,the conversion of derivation forests to translationforests takes 0.562 second when we restrict that atmost 200 rules can be generated on the fly for eachnode.
As we use cube pruning, although the trans-lation forest of STAG is bigger than that of STSG,the intersection time barely increases.
In total, theSTAG system runs in 1.763 seconds per sentence,only 1.6 times slower than the baseline system.6 ConclusionWe have presented a new tree-to-string translationsystem based on synchronous TAG.
With translationrules learned from Treebank-style trees, the adjoin-ing tree-to-string system outperforms the baselinesystem using STSG without significant loss in effi-ciency.
We plan to introduce left-to-right target gen-eration (Huang and Mi, 2010) into the STAG tree-to-string system.
Our work can also be extended toforest-based rule extraction and decoding (Mi et al,2008; Mi and Huang, 2008).
It is also interesting tointroduce STAG into tree-to-tree translation (Zhanget al, 2008; Liu et al, 2009; Chiang, 2010).AcknowledgementsThe authors were supported by National NaturalScience Foundation of China Contracts 60736014,60873167, and 60903138.
We thank the anonymousreviewers for their insightful comments.ReferencesAnne Abeille, Yves Schabes, and Aravind Joshi.
1990.Using lexicalized tags for machine translation.
InProc.
of COLING 1990.John Chen and K. Vijay-Shanker.
2000.
Automated ex-traction of tags from the penn treebank.
In Proc.
ofIWPT 2000.David Chiang.
2003.
Statistical parsing with an au-tomatically extracted tree adjoining grammar.
Data-Oriented Parsing.David Chiang.
2006.
An introduction to synchronousgrammars.
ACL Tutorial.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proc.
of ACL 2010.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29(4).Steve DeNeefe and Kevin Knight.
2009.
Synchronoustree adjoining machine translation.
In Proc.
ofEMNLP 2009.Mark Dras.
1999.
A meta-level grammar: Redefiningsynchronous tag for translation and paraphrase.
InProc.
of ACL 1999.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proc.
of ACL 2003.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.of NAACL 2004.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.
ofACL 2006.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProc.
of ACL 2007.Liang Huang and Haitao Mi.
2010.
Efficient incremen-tal decoding for tree-to-string translation.
In Proc.
ofEMNLP 2010.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proc.
of AMTA 2006.Aravind Joshi, L. Levy, and M. Takahashi.
1975.
Treeadjunct grammars.
Journal of Computer and SystemSciences, 10(1).Aravind Joshi.
1985.
How much contextsensitiv-ity is necessary for characterizing structural descrip-tions)tree adjoining grammars.
Natural Language1286Processing)Theoretical, Computational, and Psy-chological Perspectives.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of ACL 2007 (poster), pages 77?80, Prague,Czech Republic, June.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proc.
of ACL 2006.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Proc.
ofACL 2009.Haitao Mi and Liang Huang.
2008.
Forest-based transla-tion rule extraction.
In Proceedings of EMNLP 2008.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL/HLT 2008,pages 192?199, Columbus, Ohio, USA, June.Rebecca Nesson, Stuart Shieber, and Alexander Rush.2006.
Induction of probabilistic synchronous tree-insertion grammars for machine translation.
In Proc.of AMTA 2006.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proc.
of ACL 2003.Gilles Prigent.
1994.
Synchronous tags and machinetranslation.
In Proc.
of TAG+3.Yves Schabes and Richard Waters.
1995.
A cubic-time,parsable formalism that lexicalizes context-free gram-mar without changing the trees produced.
Computa-tional Linguistics, 21(4).Stuart M. Shieber and Yves Schabes.
1990.
Synchronoustree-adjoining grammars.
In Proc.
of COLING 1990.Stuart M. Shieber.
2007.
Probabilistic synchronous tree-adjoining grammars for machine translation: The ar-gument from bilingual dictionaries.
In Proc.
of SSST2007.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proceedings of ICSLP 2002,pages 901?904.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Fei Xia.
1999.
Extracting tree adjoining grammars frombracketed corpora.
In Proc.
of the Fifth Natural Lan-guage Processing Pacific Rim Symposium.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In Proc.
of ACL 2006.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A tree se-quence alignment-based tree-to-tree translation model.In Proc.
of ACL 2008.1287
