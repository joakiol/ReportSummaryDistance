Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 9?16, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Statistical Semantic Parser that Integrates Syntax and SemanticsRuifang Ge Raymond J. MooneyDepartment of Computer SciencesUniversity of Texas, AustinTX 78712, USAfgrf,mooneyg@cs.utexas.eduAbstractWe introduce a learning semantic parser,SCISSOR, that maps natural-language sen-tences to a detailed, formal, meaning-representation language.
It first usesan integrated statistical parser to pro-duce a semantically augmented parse tree,in which each non-terminal node hasboth a syntactic and a semantic label.A compositional-semantics procedure isthen used to map the augmented parsetree into a final meaning representation.We evaluate the system in two domains,a natural-language database interface andan interpreter for coaching instructions inrobotic soccer.
We present experimentalresults demonstrating that SCISSOR pro-duces more accurate semantic representa-tions than several previous approaches.1 IntroductionMost recent work in learning for semantic parsinghas focused on ?shallow?
analysis such as seman-tic role labeling (Gildea and Jurafsky, 2002).
In thispaper, we address the more ambitious task of learn-ing to map sentences to a complete formal meaning-representation language (MRL).
We consider twoMRL?s that can be directly used to perform useful,complex tasks.
The first is a Prolog-based languageused in a previously-developed corpus of queries toa database on U.S. geography (Zelle and Mooney,1996).
The second MRL is a coaching language forrobotic soccer developed for the RoboCup CoachCompetition, in which AI researchers compete toprovide effective instructions to a coachable team ofagents in a simulated soccer domain (et al, 2003).We present an approach based on a statisti-cal parser that generates a semantically augmentedparse tree (SAPT), in which each internal node in-cludes both a syntactic and semantic label.
We aug-ment Collins?
head-driven model 2 (Collins, 1997)to incorporate a semantic label on each internalnode.
By integrating syntactic and semantic inter-pretation into a single statistical model and findingthe globally most likely parse, an accurate combinedsyntactic/semantic analysis can be obtained.
Once aSAPT is generated, an additional step is required totranslate it into a final formal meaning representa-tion (MR).Our approach is implemented in a system calledSCISSOR (Semantic Composition that IntegratesSyntax and Semantics to get Optimal Representa-tions).
Training the system requires sentences an-notated with both gold-standard SAPT?s and MR?s.We present experimental results on corpora for bothgeography-database querying and Robocup coach-ing demonstrating that SCISSOR produces more ac-curate semantic representations than several previ-ous approaches based on symbolic learning (Tangand Mooney, 2001; Kate et al, 2005).2 Target MRL?sWe used two MRLs in our experiments: CLANG andGEOQUERY.
They capture the meaning of linguisticutterances in their domain in a formal language.92.1 CLANG: the RoboCup Coach LanguageRoboCup (www.robocup.org) is an interna-tional AI research initiative using robotic socceras its primary domain.
In the Coach Competition,teams of agents compete on a simulated soccer fieldand receive advice from a team coach in a formallanguage called CLANG.
In CLANG, tactics andbehaviors are expressed in terms of if-then rules.As described in (et al, 2003), its grammar consistsof 37 non-terminal symbols and 133 productions.Below is a sample rule with its English gloss:((bpos (penalty-area our))(do (player-except our {4})(pos (half our))))?If the ball is in our penalty area, all our playersexcept player 4 should stay in our half.
?2.2 GEOQUERY: a DB Query LanguageGEOQUERY is a logical query language for a smalldatabase of U.S. geography containing about 800facts.
This domain was originally chosen to testcorpus-based semantic parsing due to the avail-ability of a hand-built natural-language interface,GEOBASE, supplied with Turbo Prolog 2.0 (BorlandInternational, 1988).
The GEOQUERY languageconsists of Prolog queries augmented with severalmeta-predicates (Zelle and Mooney, 1996).
Belowis a sample query with its English gloss:answer(A,count(B,(city(B),loc(B,C),const(C,countryid(usa))),A))?How many cities are there in the US?
?3 Semantic Parsing FrameworkThis section describes our basic framework for se-mantic parsing, which is based on a fairly stan-dard approach to compositional semantics (Juraf-sky and Martin, 2000).
First, a statistical parseris used to construct a SAPT that captures the se-mantic interpretation of individual words and thebasic predicate-argument structure of the sentence.Next, a recursive procedure is used to composition-ally construct an MR for each node in the SAPTfrom the semantic label of the node and the MR?shas2VP?bownerplayer the ballNN?player CD?unum NP?nullNN?nullVB?bownerS?bownerNP?playerDT?nullPRP$?teamourFigure 1: An SAPT for a simple CLANG sentence.Function:BUILDMR(N;K)Input: The root node N of a SAPT;predicate-argument knowledge, K, for the MRL.Notation: XMRis the MR of node X .Output: NMRCi:= the ith child node of N; 1  i  nCh= GETSEMANTICHEAD(N ) // see Section 3ChMR= BUILDMR(Ch; K)for each other child Ciwhere i 6= hCiMR= BUILDMR(Ci; K)COMPOSEMR(ChMR, CiMR; K) // see Section 3NMR= ChMRFigure 2: Computing an MR from a SAPT.of its children.
Syntactic structure provides infor-mation of how the parts should be composed.
Am-biguities arise in both syntactic structure and the se-mantic interpretation of words and phrases.
By in-tegrating syntax and semantics in a single statisticalparser that produces an SAPT, we can use both se-mantic information to resolve syntactic ambiguitiesand syntactic information to resolve semantic ambi-guities.In a SAPT, each internal node in the parse treeis annotated with a semantic label.
Figure 1 showsthe SAPT for a simple sentence in the CLANG do-main.
The semantic labels which are shown afterdashes are concepts in the domain.
Some type con-cepts do not take arguments, like team and unum(uniform number).
Some concepts, which we referto as predicates, take an ordered list of arguments,like player and bowner (ball owner).
The predicate-argument knowledge, K , specifies, for each predi-cate, the semantic constraints on its arguments.
Con-straints are specified in terms of the concepts thatcan fill each argument, such as player(team, unum)and bowner(player).
A special semantic label nullis used for nodes that do not correspond to any con-cept in the domain.Figure 2 shows the basic algorithm for build-ing an MR from an SAPT.
Figure 3 illustrates the10player the ballN3?bowner(_)N7?player(our,2)N2?nullnull      nullN4?player(_,_)    N5?teamourN6?unum2N1?bowner(_)hasN8?bowner(player(our,2))Figure 3: MR?s constructed for each SAPT Node.construction of the MR for the SAPT in Figure 1.Nodes are numbered in the order in which the con-struction of their MR?s are completed.
The firststep, GETSEMANTICHEAD , determines which of anode?s children is its semantic head based on hav-ing a matching semantic label.
In the example, nodeN3 is determined to be the semantic head of thesentence, since its semantic label, bowner, matchesN8?s semantic label.
Next, the MR of the seman-tic head is constructed recursively.
The semantichead of N3 is clearly N1.
Since N1 is a part-of-speech (POS) node, its semantic label directly de-termines its MR, which becomes bowner( ).
Oncethe MR for the head is constructed, the MR of allother (non-head) children are computed recursively,and COMPOSEMR assigns their MR?s to fill the ar-guments in the head?s MR to construct the com-plete MR for the node.
Argument constraints areused to determine the appropriate filler for each ar-gument.
Since, N2 has a null label, the MR of N3also becomes bowner( ).
When computing the MRfor N7, N4 is determined to be the head with theMR: player( , ).
COMPOSEMR then assigns N5?sMR to fill the team argument and N6?s MR to fillthe unum argument to construct N7?s complete MR:player(our, 2).
This MR in turn is composed withthe MR for N3 to yield the final MR for the sen-tence: bowner(player(our,2)).For MRL?s, such as CLANG, whose syntax doesnot strictly follow a nested set of predicates and ar-guments, some final minor syntactic adjustment ofthe final MR may be needed.
In the example, thefinal MR is (bowner (player our f2g)).
In the fol-lowing discussion, we ignore the difference betweenthese two.There are a few complications left which re-quire special handling when generating MR?s,like coordination, anaphora resolution and non-compositionality exceptions.
Due to space limita-tions, we do not present the straightforward tech-niques we used to handle them.4 Corpus AnnotationThis section discusses how sentences for trainingSCISSOR were manually annotated with SAPT?s.Sentences were parsed by Collins?
head-drivenmodel 2 (Bikel, 2004) (trained on sections 02-21of the WSJ Penn Treebank) to generate an initialsyntactic parse tree.
The trees were then manuallycorrected and each node augmented with a semanticlabel.First, semantic labels for individual words, calledsemantic tags, are added to the POS nodes in thetree.
The tag null is used for words that have no cor-responding concept.
Some concepts are conveyedby phrases, like ?has the ball?
for bowner in the pre-vious example.
Only one word is labeled with theconcept; the syntactic head word (Collins, 1997) ispreferred.
During parsing, the other words in thephrase will provide context for determining the se-mantic label of the head word.Labels are added to the remaining nodes in abottom-up manner.
For each node, one of its chil-dren is chosen as the semantic head, from which itwill inherit its label.
The semantic head is chosenas the child whose semantic label can take the MR?sof the other children as arguments.
This step wasdone mostly automatically, but required some man-ual corrections to account for unusual cases.In order for COMPOSEMR to be able to constructthe MR for a node, the argument constraints forits semantic head must identify a unique conceptto fill each argument.
However, some predicatestake multiple arguments of the same type, such aspoint.num(num,num), which is a kind of point thatrepresents a field coordinate in CLANG.In this case, extra nodes are inserted in the treewith new type concepts that are unique for each ar-gument.
An example is shown in Figure 4 in whichthe additional type concepts num1 and num2 are in-troduced.
Again, during parsing, context will beused to determine the correct type for a given word.The point label of the root node of Figure 4 is theconcept that includes all kinds of points in CLANG.Once a predicate has all of its arguments filled, we11,0.5 , ?RRB??RRB??null?LRB?
0.1CD?num CD?num?LRB?
?point.numPRN?pointCD?num1 CD?num2Figure 4: Adding new types to disambiguate argu-ments.use the most general CLANG label for its concept(e.g.
point instead of point.num).
This generalityavoids sparse data problems during training.5 Integrated Parsing Model5.1 Collins Head-Driven Model 2Collins?
head-driven model 2 is a generative, lexi-calized model of statistical parsing.
In the followingsection, we follow the notation in (Collins, 1997).Each non-terminal X in the tree is a syntactic label,which is lexicalized by annotating it with a word,w, and a POS tag, tsyn.
Thus, we write a non-terminal as X(x), where X is a syntactic label andx = hw; tsyni.
X(x) is then what is generated bythe generative model.Each production LHS ) RHS in the PCFG isin the form:P (h)!Ln(ln):::L1(l1)H(h)R1(r1):::Rm(rm)where H is the head-child of the phrase, which in-herits the head-word h from its parent P .
L1:::Lnand R1:::Rmare left and right modifiers of H .Sparse data makes the direct estimation ofP(RHSjLHS) infeasible.
Therefore, it is decom-posed into several steps ?
first generating the head,then the right modifiers from the head outward,then the left modifiers in the same way.
Syntacticsubcategorization frames, LC and RC, for the leftand right modifiers respectively, are generated be-fore the generation of the modifiers.
Subcat framesrepresent knowledge about subcategorization prefer-ences.
The final probability of a production is com-posed from the following probabilities:1.
The probability of choosing a head constituentlabel H: Ph(HjP; h).2.
The probabilities of choosing the left and rightsubcat frames LC and RC: Pl(LCjP;H; h)and Pr(RCjP;H; h).has2our player thePRP$?team NN?player CD?unumNN?nullDT?nullNP?player(player) VP?bowner(has)NP?null(ball)ballS?bowner(has)VB?bownerFigure 5: A lexicalized SAPT.3.
The probabilities of generat-ing the left and right modifiers:Qi=1::m+1Pr(Ri(ri)jH;P; h;i 1; RC) Qi=1::n+1Pl(Li(li)jH;P; h;i 1; LC).Where  is the measure of the distance fromthe head word to the edge of the constituent,and Ln+1(ln+1) and Rm+1(rm+1) are STOP .The model stops generating more modifierswhen STOP is generated.5.2 Integrating Semantics into the ModelWe extend Collins?
model to include the genera-tion of semantic labels in the derivation tree.
Un-less otherwise stated, notation has the same mean-ing as in Section 5.1.
The subscript syn refers tothe syntactic part, and sem refers to the semanticpart.
We redefine X and x to include semantics,each non-terminal X is now a pair of a syntactic la-bel Xsynand a semantic label Xsem.
Besides be-ing annotated with the word, w, and the POS tag,tsyn, X is also annotated with the semantic tag,tsem, of the head child.
Thus, X(x) now consists ofX = hXsyn;Xsemi, and x = hw; tsyn; tsemi.
Fig-ure 5 shows a lexicalized SAPT (but omitting tsynand tsem).Similar to the syntactic subcat frames, we alsocondition the generation of modifiers on semanticsubcat frames.
Semantic subcat frames give se-mantic subcategorization preferences; for example,player takes a team and a unum.
Thus LC and RCare now: hLCsyn; LCsemi and hRCsyn; RCsemi.X(x) is generated as in Section 5.1, but using thenew definitions of X(x), LC and RC .
The imple-mentation of semantic subcat frames is similar tosyntactic subcat frames.
They are multisets speci-fying the semantic labels which the head requires inits left or right modifiers.As an example, the probability of generating thephrase ?our player 2?
using NP-[player](player) !12PRP$-[team](our) NN-[player](player) CD-[unum](2)is (omitting only the distance measure):Ph(NN-[player]jNP-[player],player)Pl(hfg,fteamgijNP-[player],player)Pr(hfg,funumgijNP-[player],player)Pl(PRP$-[team](our)jNP-[player],player,hfg,fteamgi)Pr(CD-[unum](2)jNP-[player],player,hfg,funumgi)Pl(STOPjNP-[player],player,hfg,fgi)Pr(STOPjNP-[player],player,hfg,fgi)5.3 SmoothingSince the left and right modifiers are independentlygenerated in the same way, we only discuss smooth-ing for the left side.
Each probability estimation inthe above generation steps is called a parameter.
Toreduce the risk of sparse data problems, the parame-ters are decomposed as follows:Ph(HjC) = Phsyn(HsynjC)Phsem(HsemjC;Hsyn)Pl(LCjC) = Plsyn(LCsynjC)Plsem(LCsemjC;LCsyn)Pl(Li(li)jC) = Plsyn(Lisyn(ltisyn; lwi)jC)Plsem(Lisem(ltisem; lwi)jC;Lisyn(ltisyn))For brevity, C is used to represent the context onwhich each parameter is conditioned; lwi, ltisyn, andltisemare the word, POS tag and semantic tag gener-ated for the non-terminal Li.
The word is generatedseparately in the syntactic and semantic outputs.We make the independence assumption that thesyntactic output is only conditioned on syntactic fea-tures, and semantic output on semantic ones.
Notethat the syntactic and semantic parameters are stillintegrated in the model to find the globally mostlikely parse.
The syntactic parameters are the sameas in Section 5.1 and are smoothed as in (Collins,1997).
We?ve also tried different ways of condition-ing syntactic output on semantic features and viceversa, but they didn?t help.
Our explanation is theintegrated syntactic and semantic parameters havealready captured the benefit of this integrated ap-proach in our experimental domains.Since the semantic parameters do not depend onany syntactic features, we omit the sem subscriptsin the following discussion.
As in (Collins, 1997),the parameter Pl(Li(lti; lwi)jP;H;w; t;; LC) isfurther smoothed as follows:Pl1(LijP;H;w; t;; LC) Pl2(ltijP;H;w; t;; LC;Li)Pl3(lwijP;H;w; t;; LC;Li(lti))Note this smoothing is different from the syntacticcounterpart.
This is due to the difference betweenPOS tags and semantic tags; namely, semantic tagsare generally more specific.Table 1 shows the various levels of back-off foreach semantic parameter.
The probabilities fromthese back-off levels are interpolated using the tech-niques in (Collins, 1997).
All words occurring lessthan 3 times in the training data, and words in testdata that were not seen in training, are unknownwords and are replaced with the ?UNKNOWN?
to-ken.
Note this threshold is smaller than the one usedin (Collins, 1997) since the corpora used in our ex-periments are smaller.5.4 POS Tagging and Semantic TaggingFor unknown words, the POS tags allowed are lim-ited to those seen with any unknown words duringtraining.
Otherwise they are generated along withthe words using the same approach as in (Collins,1997).
When parsing, semantic tags for each knownword are limited to those seen with that word dur-ing training data.
The semantic tags allowed for anunknown word are limited to those seen with its as-sociated POS tags during training.6 Experimental Evaluation6.1 MethodologyTwo corpora of NL sentences paired with MR?swere used to evaluate SCISSOR.
For CLANG, 300pieces of coaching advice were randomly selectedfrom the log files of the 2003 RoboCup Coach Com-petition.
Each formal instruction was translatedinto English by one of four annotators (Kate et al,2005).
The average length of an NL sentence inthis corpus is 22.52 words.
For GEOQUERY, 250questions were collected by asking undergraduatestudents to generate English queries for the givendatabase.
Queries were then manually translated13BACK-OFFLEVEL Ph(Hj:::) PLC(LCj:::) PL1(Lij:::) PL2(ltij:::) PL3(lwij:::)1 P,w,t P,H,w,t P,H,w,t,,LC P,H,w,t,,LC, LiP,H,w,t,,LC, Li, lti2 P,t P,H,t P,H,t,,LC P,H,t,,LC, LiP,H,t,,LC, Li, lti3 P P,H P,H,,LC P,H,,LC, LiLi, lti4 ?
?
?
LiltiTable 1: Conditioning variables for each back-off level for semantic parameters (sem subscripts omitted).into logical form (Zelle and Mooney, 1996).
Theaverage length of an NL sentence in this corpus is6.87 words.
The queries in this corpus are morecomplex than those in the ATIS database-query cor-pus used in the speech community (Zue and Glass,2000) which makes the GEOQUERY problem harder,as also shown by the results in (Popescu et al, 2004).The average number of possible semantic tags foreach word which can represent meanings in CLANGis 1.59 and that in GEOQUERY is 1.46.SCISSOR was evaluated using standard 10-foldcross validation.
NL test sentences are first parsedto generate their SAPT?s, then their MR?s were builtfrom the trees.
We measured the number of test sen-tences that produced complete MR?s, and the num-ber of these MR?s that were correct.
For CLANG,an MR is correct if it exactly matches the correctrepresentation, up to reordering of the arguments ofcommutative operators like and.
For GEOQUERY,an MR is correct if the resulting query retrievedthe same answer as the correct representation whensubmitted to the database.
The performance of theparser was then measured in terms of precision (thepercentage of completed MR?s that were correct)and recall (the percentage of all sentences whoseMR?s were correctly generated).We compared SCISSOR?s performance to severalprevious systems that learn semantic parsers that canmap sentences into formal MRL?s.
CHILL (Zelle andMooney, 1996) is a system based on Inductive LogicProgramming (ILP).
We compare to the versionof CHILL presented in (Tang and Mooney, 2001),which uses the improved COCKTAIL ILP system andproduces more accurate parsers than the original ver-sion presented in (Zelle and Mooney, 1996).
SILT isa system that learns symbolic, pattern-based, trans-formation rules for mapping NL sentences to formallanguages (Kate et al, 2005).
It comes in two ver-sions, SILT-string, which maps NL strings directlyto an MRL, and SILT-tree, which maps syntactic01020304050607080901000  50  100  150  200  250Precision(%)Training sentencesSCISSORSILT-stringSILT-treeCHILLGEOBASEFigure 6: Precision learning curves for GEOQUERY.010203040506070800  50  100  150  200  250Recall (%)Training sentencesSCISSORSILT-stringSILT-treeCHILLGEOBASEFigure 7: Recall learning curves for GEOQUERY.parse trees (generated by the Collins parser) to anMRL.
In the GEOQUERY domain, we also compareto the original hand-built parser GEOBASE.6.2 ResultsFigures 6 and 7 show the precision and recall learn-ing curves for GEOQUERY, and Figures 8 and 9 forCLANG.
Since CHILL is very memory intensive,it could not be run with larger training sets of theCLANG corpus.Overall, SCISSOR gives the best precision and re-call results in both domains.
The only exceptionis with recall for GEOQUERY, for which CHILL isslightly higher.
However, SCISSOR has significantlyhigher precision (see discussion in Section 7).1401020304050607080901000  50  100  150  200  250  300Precision(%)Training sentencesSCISSORSILT-stringSILT-treeCHILLFigure 8: Precision learning curves for CLANG.010203040506070800  50  100  150  200  250  300Recall (%)Training sentencesSCISSORSILT-stringSILT-treeCHILLFigure 9: Recall learning curves for CLANG.Results on a larger GEOQUERY corpus with 880queries have been reported for PRECISE (Popescu etal., 2003): 100% precision and 77.5% recall.
Onthe same corpus, SCISSOR obtains 91.5% precisionand 72.3% recall.
However, the figures are not com-parable.
PRECISE can return multiple distinct SQLqueries when it judges a question to be ambigu-ous and it is considered correct when any of theseSQL queries is correct.
Our measure only considersthe top result.
Due to space limitations, we do notpresent complete learning curves for this corpus.7 Related WorkWe first discuss the systems introduced in Section6.
CHILL uses computationally-complex ILP meth-ods, which are slow and memory intensive.
Thestring-based version of SILT uses no syntactic in-formation while the tree-based version generates asyntactic parse first and then transforms it into anMR.
In contrast, SCISSOR integrates syntactic andsemantic processing, allowing each to constrain andinform the other.
It uses a successful approach to sta-tistical parsing that attempts to find the SAPT withmaximum likelihood, which improves robustnesscompared to purely rule-based approaches.
How-ever, SCISSOR requires an extra training input, gold-standard SAPT?s, not required by these other sys-tems.
Further automating the construction of train-ing SAPT?s from sentences paired with MR?s is asubject of on-going research.PRECISE is designed to work only for the spe-cific task of NL database interfaces.
By comparison,SCISSOR is more general and can work with otherMRL?s as well (e.g.
CLANG).
Also, PRECISE is nota learning system and can fail to parse a query it con-siders ambiguous, even though it may not be consid-ered ambiguous by a human and could potentially beresolved by learning regularities in the training data.In (Lev et al, 2004), a syntax-driven approachis used to map logic puzzles described in NL toan MRL.
The syntactic structures are paired withhand-written rules.
A statistical parser is used togenerate syntactic parse trees, and then MR?s arebuilt using compositional semantics.
The meaningof open-category words (with only a few exceptions)is considered irrelevant to solving the puzzle andtheir meanings are not resolved.
Further steps wouldbe needed to generate MR?s in other domains likeCLANG and GEOQUERY.
No empirical results arereported for their approach.Several machine translation systems also attemptto generate MR?s for sentences.
In (et al, 2002),an English-Chinese speech translation system forlimited domains is described.
They train a statisti-cal parser on trees with only semantic labels on thenodes; however, they do not integrate syntactic andsemantic parsing.History-based models of parsing were first in-troduced in (Black et al, 1993).
Their originalmodel also included semantic labels on parse-treenodes, but they were not used to generate a formalMR.
Also, their parsing model is impoverished com-pared to the history included in Collins?
more recentmodel.
SCISSOR explores incorporating semanticlabels into Collins?
model in order to produce a com-plete SAPT which is then used to generate a formalMR.The systems introduced in (Miller et al, 1996;Miller et al, 2000) also integrate semantic labelsinto parsing; however, their SAPT?s are used to pro-15duce a much simpler MR, i.e., a single semanticframe.
A sample frame is AIRTRANSPORTATIONwhich has three slots ?
the arrival time, origin anddestination.
Only one frame needs to be extractedfrom each sentence, which is an easier task thanour problem in which multiple nested frames (pred-icates) must be extracted.
The syntactic model in(Miller et al, 2000) is similar to Collins?, but doesnot use features like subcat frames and distance mea-sures.
Also, the non-terminal label X is not furtherdecomposed into separately-generated semantic andsyntactic components.
Since it used much more spe-cific labels (the cross-product of the syntactic andsemantic labels), its parameter estimates are poten-tially subject to much greater sparse-data problems.8 ConclusionSCISSOR learns statistical parsers that integrate syn-tax and semantics in order to produce a semanti-cally augmented parse tree that is then used to com-positionally generate a formal meaning representa-tion.
Experimental results in two domains, a natural-language database interface and an interpreter forcoaching instructions in robotic soccer, have demon-strated that SCISSOR generally produces more accu-rate semantic representations than several previousapproaches.
By augmenting a state-of-the-art statis-tical parsing model to include semantic information,it is able to integrate syntactic and semantic cluesto produce a robust interpretation that supports thegeneration of complete formal meaning representa-tions.9 AcknowledgementsWe would like to thank Rohit J. Kate , Yuk WahWong and Gregory Kuhlmann for their help in an-notating the CLANG corpus and providing the eval-uation tools.
This research was supported by De-fense Advanced Research Projects Agency undergrant HR0011-04-1-0007.ReferencesDaniel M. Bikel.
2004.
Intricacies of Collins?
parsing model.Computational Linguistics, 30(4):479?511.Ezra Black, Frederick Jelineck, John Lafferty, David M. Mager-man, Robert L. Mercer, and Salim Roukos.
1993.
Towardshistory-based grammars: Using richer models for probabilis-tic parsing.
In Proc.
of ACL-93, pages 31?37, Columbus,Ohio.Borland International.
1988.
Turbo Prolog 2.0 ReferenceGuide.
Borland International, Scotts Valley, CA.Mao Chen et al 2003.
Users manual: RoboCupsoccer server manual for soccer server version 7.07and later.
Available at http://sourceforge.net/projects/sserver/.Michael J. Collins.
1997.
Three generative, lexicalised mod-els for statistical parsing.
In Proc.
of ACL-97, pages 16?23,Madrid, Spain.Yuqing Gao et al 2002.
Mars: A statistical semantic parsingand generation-based multilingual automatic translation sys-tem.
Machine Translation, 17:185?212.Daniel Gildea and Daniel Jurafsky.
2002.
Automated labelingof semantic roles.
Computational Linguistics, 28(3):245?288.Daniel Jurafsky and James H. Martin.
2000.
Speech and Lan-guage Processing: An Introduction to Natural LanguageProcessing, Computational Linguistics, and Speech Recog-nition.
Prentice Hall, Upper Saddle River, NJ.Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney.
2005.Learning to transform natural to formal languages.
To ap-pear in Proc.
of AAAI-05, Pittsburgh, PA.Iddo Lev, Bill MacCartney, Christopher D. Manning, and RogerLevy.
2004.
Solving logic puzzles: From robust process-ing to precise semantics.
In Proc.
of 2nd Workshop on TextMeaning and Interpretation, ACL-04, Barcelona, Spain.Scott Miller, David Stallard, Robert Bobrow, and RichardSchwartz.
1996.
A fully statistical approach to natural lan-guage interfaces.
In ACL-96, pages 55?61, Santa Cruz, CA.Scott Miller, Heidi Fox, Lance A. Ramshaw, and Ralph M.Weischedel.
2000.
A novel use of statistical parsing to ex-tract information from text.
In Proc.
of NAACL-00, pages226?233, Seattle, Washington.Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003.
To-wards a theory of natural language interfaces to databases.
InProc.
of IUI-2003, pages 149?157, Miami, FL.
ACM.Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko,and Alexander Yates.
2004.
Modern natural language in-terfaces to databases: Composing statistical parsing with se-mantic tractability.
In COLING-04, Geneva, Switzerland.Lappoon R. Tang and Raymond J. Mooney.
2001.
Using multi-ple clause constructors in inductive logic programming forsemantic parsing.
In Proc.
of ECML-01, pages 466?477,Freiburg, Germany.John M. Zelle and Raymond J. Mooney.
1996.
Learning toparse database queries using inductive logic programming.In Proc.
of AAAI-96, pages 1050?1055, Portland, OR.Victor W. Zue and James R. Glass.
2000.
Conversational in-terfaces: Advances and challenges.
In Proc.
of the IEEE,volume 88(8), pages 1166?1180.16
