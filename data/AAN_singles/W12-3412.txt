Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 89?99,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsGenerative Constituent Parsing and Discriminative Dependency Reranking:Experiments on English and FrenchJoseph Le Roux Beno?t Favre?
Alexis Nasr?
Seyed Abolghasem Mirroshandel?,?LIPN, Universit?
Paris Nord ?
CNRS UMR 7030, Villetaneuse, France?LIF, Universit?
Aix-Marseille ?
CNRS UMR 7279, Marseille, France?Computer Engineering Department, Sharif university of Technology, Tehran, Iranleroux@univ-paris13.fr, benoit.favre@lif.univ-mrs.fr,alexis.nasr@lif.univ-mrs.fr, ghasem.mirroshandel@lif.univ-mrs.frAbstractWe present an architecture for parsing in twosteps.
A phrase-structure parser builds foreach sentence an n-best list of analyses whichare converted to dependency trees.
These de-pendency structures are then rescored by a dis-criminative reranker.
Our method is languageagnostic and enables the incorporation of ad-ditional information which are useful for thechoice of the best parse candidate.
We testour approach on the the Penn Treebank andthe French Treebank.
Evaluation shows a sig-nificative improvement on different parse met-rics.1 IntroductionTwo competing approaches exist for parsing naturallanguage.
The first one, called generative, is basedon the theory of formal languages and rewriting sys-tems.
Parsing is defined here as a process that trans-forms a string into a tree or a tree forest.
It is of-ten grounded on phrase-based grammars ?
althoughthere are generative dependency parsers ?
in partic-ular context-free grammars or one of their numer-ous variants, that can be parsed in polynomial time.However, the independence hypothesis that under-lies this kind of formal system does not allow forprecise analyses of some linguistic phenomena, suchas long distance and lexical dependencies.In the second approach, known as discriminative,the grammar is viewed as a system of constraintsover the correct syntactic structures, the words of thesentence themselves being seen as constraints overthe position they occupy in the sentence.
Parsingboils down to finding a solution that is compatiblewith the different constraints.
The major problem ofthis approach lies in its complexity.
The constraintscan, theoretically, range over any aspect of the finalstructures, which prevents from using efficient dy-namic programming techniques when searching fora global solution.
In the worst case, final structuresmust be enumerated in order to be evaluated.
There-fore, only a subset of constraints is used in imple-mentations for complexity reasons.
This approachcan itself be divided into formalisms relying on logicto describe constraints, as the model-theoretic syn-tax (Pullum and Scholz, 2001), or numerical for-malisms that associate weights to lexico-syntacticsubstructures.
The latter has been the object of somerecent work thanks to progresses achieved in thefield of Machine Learning.
A parse tree is repre-sented as a vector of features and its accuracy ismeasured as the distance between this vector and thereference.One way to take advantage of both approachesis to combine them sequentially, as initially pro-posed by Collins (2000).
A generative parser pro-duces a set of candidates structures that constitutethe input of a second, discriminative module, whosesearch space is limited to this set of candidates.Such an approach, parsing followed by reranking,is used in the Brown parser (Charniak and Johnson,2005).
The approach can be extended in order tofeed the reranker with the output of different parsers,as shown by (Johnson and Ural, 2010; Zhang et al,2009).In this paper we are interested in applying rerank-ing to dependency structures.
The main reason isthat many linguistic constraints are straightforwardto implement on dependency structures, as, for ex-ample, subcategorization frames or selectional con-straints that are closely linked to the notion of de-89pendents of a predicate.
On the other hand, depen-dencies extracted from constituent parses are knownto be more accurate than dependencies obtainedfrom dependency parsers.
Therefore the solution wechoose is an indirect one: we use a phrase-basedparser to generate n-best lists and convert them tolists of dependency structures that are reranked.
Thisapproach can be seen as trade-off between phrase-based reranking experiments (Collins, 2000) and theapproach of Carreras et al (2008) where a discrimi-native model is used to score lexical features repre-senting unlabelled dependencies in the Tree Adjoin-ing Grammar formalism.Our architecture, illustrated in Figure 1, is basedon two steps.
During the first step, a syntagmaticparser processes the input sentence and produces n-best parses as well as their probabilities.
They areannotated with a functional tagger which tags syn-tagms with standard syntactic functions subject, ob-ject, indirect object .
.
.
and converted to dependencystructures by application of percolation rules.
In thesecond step, we extract a set of features from thedependency parses and the associated probabilities.These features are used to reorder the n-best listand select a potentially more accurate parse.
Syn-tagmatic parses are produced by the implementationof a PCFG-LA parser of (Attia et al, 2010), simi-lar to (Petrov et al, 2006), a functional tagger anddependency converter for the target language.
Thereranking model is a linear model trained with animplementation of the MIRA algorithm (Crammer etal., 2006)1.Charniak and Johnson (2005) and Collins (2000)rerank phrase-structure parses and they also includehead-dependent information, in other words unla-belled dependencies.
In our approach we take intoaccount grammatical functions or labelled depen-dencies.It should be noted that the features we use are verygeneric and do not depend on the linguistic knowl-edge of the authors.
We applied our method to En-glish, the de facto standard for testing parsing tech-nologies, and French which exhibits many aspects ofa morphologically rich language.
But our approachcould be applied to other languages, provided that1This implementation is available at https://github.com/jihelhere/adMIRAble.the resources ?
treebanks and conversion tools ?
ex-ist.
(1) PCFG-LA n-best constituency parses(2) Function annotation(3) Conversion to dependency parses(4) Feature extraction(5) MIRA rerankingwFinal constituency & dependency parseInput textFigure 1: The parsing architecture: production of the n-best syntagmatic trees (1) tagged with functional labels(2), conversion to a dependency structure (3) and featureextraction (4), scoring with a linear model (5).
The parsewith the best score is considered as final.The structure of the paper is the following: inSection 2 we describe the details of our generativeparser and in Section 3 our reranking model togetherwith the features templates.
Section 4 reports the re-sults of the experiments conducted on the Penn Tree-bank (Marcus et al, 1994) as well as on the Paris 7Treebank (Abeill?
et al, 2003) and Section 5 con-cludes the paper.2 Generative ModelThe first part of our system, the syntactic analysisitself, generates surface dependency structures in asequential fashion (Candito et al, 2010b; Canditoet al, 2010a).
A phrase structure parser based onLatent Variable PCFGs (PCFG-LAs) produces treestructures that are enriched with functions and thenconverted to labelled dependency structures, whichwill be processed by the parse reranker.902.1 PCFG-LAsProbabilistic Context Free Grammars with LatentAnnotations, introduced in (Matsuzaki et al, 2005)can be seen as automatically specialised PCFGslearnt from treebanks.
Each symbol of the gram-mar is enriched with annotation symbols behavingas subclasses of this symbol.
More formally, theprobability of an unannotated tree is the sum of theprobabilities of its annotated counterparts.
For aPCFG-LA G, R is the set of annotated rules, D(t)is the set of (annotated) derivations of an unanno-tated tree t, and R(d) is the set of rules used in aderivation d. Then the probability assigned by G tot is:PG(t) =?d?D(t)PG(d) =?d?D(t)?r?R(d)PG(r) (1)Because of this alternation of sums and productsthat cannot be optimally factorised, there is no ex-act polynomial dynamic programming algorithm forparsing.
Matsuzaki et al (2005) and Petrov andKlein (2007) discuss approximations of the decod-ing step based on a Bayesian variational approach.This enables cubic time decoding that can be fur-ther enhanced with coarse-to-fine methods (Char-niak and Johnson, 2005).This type of grammars has already been testedon a variety of languages, in particular Englishand French, giving state-of-the-art results.
Let usstress that this phrase-structure formalism is not lex-icalised as opposed to grammars previously used inreranking experiments (Collins, 2000; Charniak andJohnson, 2005).
The notion of lexical head is there-fore absent at parsing time and will become avail-able only at the reranking step.2.2 Dependency StructuresA syntactic theory can either be expressed withphrase structures or dependencies, as advocated forin (Rambow, 2010).
However, some informationmay be simpler to describe in one of the representa-tions.
This equivalence between the modes of repre-sentations only stands if the informational contentsare the same.
Unfortunately, this is not the casehere because the phrase structures that we use donot contain functional annotations and lexical heads,whereas labelled dependencies do.This implies that, in order to be convertedinto labelled dependency structures, phrase struc-ture parses must first be annotated with functions.Previous experiments for English and French aswell (Candito et al, 2010b) showed that a sequentialapproach is better than an integrated one for context-free grammars, because the strong independence hy-pothesis of this formalism implies a restricted do-main of locality which cannot express the contextneeded to properly assign functions.
Most func-tional taggers, such as the ones used in the followingexperiments, rely on classifiers whose feature setscan describe the whole context of a node in order tomake a decision.3 Discriminative modelOur discriminative model is a linear modeltrained with the Margin-Infused Relaxed Algorithm(MIRA) (Crammer et al, 2006).
This model com-putes the score of a parse tree as the inner productof a feature vector and a weight vector represent-ing model parameters.
The training procedure ofMIRA is very close to that of a perceptron (Rosen-blatt, 1958), benefiting from its speed and relativelylow requirements while achieving better accuracy.Recall that parsing under this model consists in(1) generating a n-best list of constituency parsesusing the generative model, (2) annotating each ofthem with function tags, (3) converting them to de-pendency parses, (4) extracting features, (5) scoringeach feature vector against the model, (6) selectingthe highest scoring parse as output.For training, we collect the output of feature ex-traction (4) for a large set of training sentences andassociate each parse tree with a loss function that de-notes the number of erroneous dependencies com-pared to the reference parse tree.
Then, modelweights are adjusted using MIRA training so that theparse with the lowest loss gets the highest score.
Ex-amples are processed in sequence, and for each ofthem, we compute the score of each parse accordingto the current model and find an updated weight vec-tor that assigns the first rank to the best parse (calledoracle).
Details of the algorithm are given in the fol-lowing sections.913.1 DefinitionsLet us consider a vector space of dimensionmwhereeach component corresponds to a feature: a parsetree p is represented as a sparse vector ?(p).
Themodel is a weight vector w in the same space whereeach weight corresponds to the importance of thefeatures for characterizing good (or bad) parse trees.The score s(p) of a parse tree p is the scalar productof its feature vector ?
(p) and the weight vector w.s(p) =m?i=1wi?i(p) (2)Let L be the n-best list of parses produced by thegenerative parser for a given sentence.
The highestscoring parse p?
is selected as output of the reranker:p?
= argmaxp?Ls(p) (3)MIRA learning consists in using training sen-tences and their reference parses to determine theweight vector w. It starts with w = 0 and modifiesit incrementally so that parses closest to the refer-ence get higher scores.
Let l(p), loss of parse p,be the number of erroneous dependencies (governor,dependent, label) compared to the reference parse.We define o, the oracle parse, as the parse with thelowest loss in L.Training examples are processed in sequence asan instance of online learning.
For each sentence,we compute the score of each parse in the n-bestlist.
If the highest scoring parse differs from the or-acle (p?
6= o), the weight vector can be improved.In this case, we seek a modification of w ensuringthat o gets a better score than p?
with a differenceat least proportional to the difference between theirloss.
This way, very bad parses get pushed deeperthan average parses.
Finding such weight vectorcan be formulated as the following constrained opti-mization problem:minimize: ||w||2 (4)subject to: s(o)?
s(p?)
?
l(o)?
l(p?)
(5)Since there is an infinity of weight vectors thatsatisfy constraint 5, we settle on the one with thesmallest magnitude.
Classical constrained quadraticoptimization methods can be applied to solve thisproblem: first, Lagrange multipliers are used to in-troduce the constraint in the objective function, thenthe Hildreth algorithm yields the following analyticsolution to the non-constrained problem:w?
= w + ?
(?(o)?
?(p?))
(6)?
= max[0,l(o)?
l(p?)?
[s(o)?
s(p?)]||?(o)?
?(p?
)||2](7)Here, w?
is the new weight vector, ?
is an up-date magnitude and [?(o)?
?(p?)]
is the differencebetween the feature vector of the oracle and that ofthe highest scoring parse.
This update, similar tothe perceptron update, draws the weight vector to-wards o while pushing it away from p?.
Usual tricksthat apply to the perceptron also apply here: (a) per-forming multiple passes on the training data, and (b)averaging the weight vector over each update2.
Al-gorithm 1 details the instructions for MIRA training.Algorithm 1 MIRA trainingfor i = 1 to t dofor all sentences in training set doGenerate n-best list L from generative parserfor all p ?
L doExtract feature vector ?
(p)Compute score s(p) (eq.
2)end forGet oracle o = argminp l(p)Get best parse p?
= argmaxp s(p)if p?
6= o thenCompute ?
(eq.
7)Update weight vector (eq.
6)end ifend forend forReturn average weight vector over updates.3.2 FeaturesThe quality of the reranker depends on the learningalgorithm as much as on the feature set.
These fea-tures can span over any subset of a parse tree, up tothe whole tree.
Therefore, there are a very large setof possible features to choose from.
Relevant fea-tures must be general enough to appear in as many2This can be implemented efficiently using two weight vec-tors as for the averaged perceptron.92parses as possible, but specific enough to character-ize good and bad configurations in the parse tree.We extended the feature set from (McDonald,2006) which showed to be effective for a range oflanguages.
Our feature templates can be categorizedin 5 classes according to their domain of locality.In the following, we describe and exemplify thesetemplates on the following sentence from the Penntreebank, in which we target the PMOD dependencybetween ?at?
and ?watch.
?Probability Three features are derived from thePCFG-LA parser, namely the posterior proba-bility of the parse (eq.
1), its normalized prob-ability relative to the 1-best, and its rank in then-best list.Unigram Unigram features are the most simple asthey only involve one word.
Given a depen-dency between position i and position j of typel, governed by xi, denoted xil?
xj , two fea-tures are created: one for the governor xi andone for the dependent xj .
They are describedas 6-tuples (word, lemma, pos-tag, is-governor,direction, type of dependency).
Variants withwildcards at each subset of tuple slots are alsogenerated in order to handle sparsity.In our example, the dependency between?looked?
and ?at?
generates two features:[at, at, IN, G, R, PMOD] and[looked, look, NN, D, L, PMOD]And also wildcard features such as:[-, at, IN, G, R, PMOD], [at,-, IN, G, R, PMOD] ...[at, -, -, -, -, PMOD]This wildcard feature generation is applied toall types of features.
We will omit it in the re-mainder of the description.Bigram Unlike the previous template, bigram fea-tures model the conjunction of the governorand the dependent of a dependency relation,like bilexical dependencies in (Collins, 1997).Given dependency xil?
xj , the feature cre-ated is (word xi, lemma xi, pos-tag xi, wordxj , lemma xj , pos-tag xj , distance3 from i toj, direction, type).The previous example generates the followingfeature:[at, at, IN, watch, watch, NN,2, R, PMOD]Where 2 is the distance between ?at?
and?watch?.Linear context This feature models the linear con-text between the governor and the dependentof a relation by looking at the words betweenthem.
Given dependency xil?
xj , for eachword from i + 1 to j ?
1, a feature is createdwith the pos-tags of xi and xj , and the pos tagof the word between them (no feature is createif j = i + 1).
An additional feature is createdwith pos-tags at positions i?
1, i, i+ 1, j ?
1,j, j +1.
Our example yields the following fea-tures:[IN, PRP$, NN], and [VBD, IN,PRP$, PRP$, NN, .
].Syntactic context: siblings This template and thenext one look at two dependencies in two con-figurations.
Given two dependencies xil?
xjand xim?
xk, we create the feature (word,lemma, pos-tag for xi, xj and xk, distance fromi to j, distance from i to k, direction and type ofeach of the two dependencies).
In our example:[looked, look, VBD, I, I, PRP,at, at, IN, 1, 1, L, SBJ, R,ADV]Syntactic context: chains Given two dependenciesxil?
xjm?
xk, we create the feature (word,lemma, pos-tag of xi, xj and xk, distance fromi to j, distance from i to k, direction and type ofeach of the two dependencies).
In our example:[looked, look, VBD, at, at, IN,watch, watch, NN, 1, 2, R, ADV,3In every template, distance features are quantified in 7classes: 1, 2, 3, 4, 5, 5 to 10, more.93R, PMOD]It is worth noting that our feature templates onlyrely on information available in the training set, anddo not use any external linguistic knowledge.4 ExperimentsIn this section, we evaluate our architecture ontwo corpora, namely the Penn Treebank (Marcus etal., 1994) and the French Treebank (Abeill?
et al,2003).
We first present the corpora and the toolsused for annotating and converting structures, thenthe performances of the phrase structure parser aloneand with the discriminative reranker.4.1 Treebanks and ToolsFor English, we use the Wall Street Journal sectionsof the Penn Treebank.
We learn the PCFG-LA fromsections 02-214.
We then use FUNTAG (Chrupa?aet al, 2007) to add functions back to the PCFG-LAanalyses.
For the conversion to dependency struc-tures we use the LTH tool (Johansson and Nugues,2007).
In order to get the gold dependencies, we runLTH directly on the gold parse trees.
We use sec-tion 22 for development and section 23 for the finalevaluation.For French, we use the Paris 7 Treebank (orFrench Treebank, FTB).
As in several previous ex-periments we decided to divide the 12,350 phrasestructure trees in three sets: train (80%), develop-ment (10%) and test (10%).
The syntactic tag set forFrench is not fixed and we decided to use the onedescribed in (Candito and Crabb?, 2009) to be ableto compare this system with recent parsing resultson French.
As for English, we learn the PCFG-LAwithout functional annotations which are added af-terwards.
We use the dependency structures devel-oped in (Candito et al, 2010b) and the conversiontoolkit BONSA?.
Furthermore, to test our approachagainst state of the art parsing results for Frenchwe use word clusters in the phrase-based parser asin (Candito and Crabb?, 2009).For both languages we constructed 10-fold train-ing data from train sets in order to avoid overfittingthe training data.
The trees from training sets weredivided into 10 subsets and the parses for each sub-set were generated by a parser trained on the other4Functions are omitted.9 subsets.
Development and test parses are given bya parser using the whole training set.
Developmentsets were used to choose the best reranking model.For lemmatisation, we use the MATE lemmatiserfor English and a home-made lemmatiser for Frenchbased on the lefff lexicon (Sagot, 2010).4.2 Generative ModelThe performances of our parser are summarised inFigure 2, (a) and (b), where F-score denotes the Par-seval F-score5, and LAS and UAS are respectivelythe Labelled and Unlabelled Attachment Score ofthe converted dependency structures6.
We give or-acle scores (the score that our system would get ifit selected the best parse from the n-best lists) whenthe parser generates n-best lists of depth 10, 20, 50and 100 in order to get an idea of the effectivenessof the reranking process.One of the issues we face with this approach isthe use of an imperfect functional annotator.
ForFrench we evaluate the loss of accuracy on the re-sulting dependency structure from the gold develop-ment set where functions have been omitted.
TheUAS is 100% but the LAS is 96.04%.
For Englishthe LAS from section 22 where functions are omit-ted is 95.35%.From the results presented in this section we canmake two observations.
First, the results of ourparser are at the state of the art on English (90.7%F-score) and on French (85.7% F-score).
So thereranker will be confronted with the difficult task ofimproving on these scores.
Second, the progressionmargin is sensible with a potential LAS error reduc-tion of 41% for English and 40.2% for French.4.3 Adding the Reranker4.3.1 Learning Feature WeightsThe discriminative model, i.e.
template instancesand their weights, is learnt on the training set parsesobtained via 10-fold cross-validation.
The genera-tive parser generates 100-best lists that are used aslearning example for the MIRA algorithm.
Featureextraction produces an enormous number of fea-tures: about 571 millions for English and 179 mil-5We use a modified version of evalb that gives the ora-cle score when the parser outputs a list of candidates for eachsentence.6All scores are measured without punctuation.94(a) Oracle Scores on PTB dev set (b) Oracle Scores on FTB dev set8990919293949596971 10 20 50 100Size of n-best listUASLASF-scoreOracle score868890929486889092941 10 20 50 100Size of n?best listOracle scoreUASLASF-score(c) Reranker scores on PTB dev set (d) Reranker scores on FTB dev set8990919293941 10 20 50 100Size of n-best listUASLASF-scoreReranked score8687888990911 10 20 50 100Size of n?best listUASLASF-scoreReranker scoreFigure 2: Oracle and reranker scores on PTB and FTB data on the dev.
set, according to the depth of the n-best.lions for French.
Let us remark that this large set offeatures is not an issue because our discriminativelearning algorithm is online, that is to say it consid-ers only one example at a time, and it only givesnon-null weights to useful features.4.3.2 EvaluationIn order to test our system we first tried to eval-uate the impact of the length of the n-best list overthe reranking predictions7.
The results are shown inFigure 2, parts (c) and (d).For French, we can see that even though the LASand UAS are consistently improving with the num-ber of candidates, the F-score is maximal with 50candidates.
However the difference between 50 can-didates and 100 candidates is not statistically signifi-cant.
For English, the situation is simpler and scoresimprove continuously on the three metrics.Finally we run our system on the test sets for bothtreebanks.
Results are shown8 in Table 1 for En-glish, and Table 2 for French.
For English the im-provement is 0.9% LAS, 0.7% Parseval F-score and7The model is always trained with 100 candidates.8F < 40 is the parseval F-score for sentences with less than40 words.0.8% UAS.Baseline RerankerF 90.4 91.1F < 40 91.0 91.7LAS 88.9 89.8UAS 93.1 93.9Table 1: System results on PTB Test setFor French we have improvements of 0.3/0.7/0.9.If we add a template feature indicating the agree-ment between part-of-speech provided by the PCFG-LA parser and a part-of-speech tagger (Denisand Sagot, 2009), we obtain better improvements:0.5/0.8/1.1.Baseline Reranker Rerank + MEltF 86.6 87.3 87.4F < 40 88.7 89.0 89.2LAS 87.9 89.0 89.2UAS 91.0 91.9 92.1Table 2: System results on FTB Test set954.3.3 Comparison with Related WorkWe compare our results with related parsing re-sults on English and French.For English, the main results are shown in Ta-ble 3.
From the presented data, we can see thatindirect reranking on LAS may not seem as goodas direct reranking on phrase-structures compared toF-scores obtained in (Charniak and Johnson, 2005)and (Huang, 2008) with one parser or (Zhang etal., 2009) with several parsers.
However, our sys-tem does not rely on any language specific featureand can be applied to other languages/treebanks.
Itis difficult to compare our system for LAS becausemost systems evaluate on gold data (part-of-speech,lemmas and morphological information) like Bohnet(2010).Our system also compares favourably with thesystem of Carreras et al (2008) that relies on a morecomplex generative model, namely Tree AdjoiningGrammars, and the system of Suzuki et al (2009)that makes use of external data (unannotated text).F LAS UASHuang, 2008 91.7 ?
?Bohnet, 2010 ?
90.3 ?Zhang et al 2008 91.4 ?
93.2Huang and Sagae, 2010 ?
?
92.1Charniak et al 2005 91.5 90.0 94.0Carreras et al 2008 ?
?
93.5Suzuki et al 2009 ?
?
93.8This work 91.1 89.8 93.9Table 3: Comparison on PTB Test setFor French, see Table 4, we compare our systemwith the MATE parser (Bohnet, 2010), an improve-ment over the MST parser (McDonald et al, 2005)with hash kernels, using the MELT part-of-speechtagger (Denis and Sagot, 2009) and our own lemma-tiser.We also compare the French system with resultsdrawn from the benchmark performed by Candito etal.
(2010a).
The first system (BKY-FR) is close toours without the reranking module, using the Berke-ley parser adapted to French.
The second (MST-FR) is based on MSTParser (McDonald et al, 2005).These two system use word clusters as well.The next section takes a close look at the modelsof the reranker and its impact on performance.F < 40 LAS UASThis work 89.2 89.2 92.1MATE + MELT ?
89.2 91.8BKY-FR 88.2 86.8 91.0MST-FR ?
88.2 90.9Table 4: Comparison on FTB Test set4.3.4 Model AnalysisIt is interesting to note that in the test sets, theone-best of the syntagmatic parser is selected 52.0%of the time by the reranker for English and 34.3% ofthe time for French.
This can be explained by thedifference in the quantity of training data in the twotreebanks (four times more parses are available forEnglish) resulting in an improvement of the qualityof the probabilistic grammar.We also looked at the reranking models, specifi-cally at the weight given to each of the features.
Itshows that 19.8% of the 571 million features havea non-zero weight for English as well as 25.7% ofthe 179 million features for French.
This can be ex-plained by the fact that for a given sentence, featuresthat are common to all the candidates in the n-bestlist are not discriminative to select one of these can-didates (they add the same constant weight to thescore of all candidates), and therefore ignored by themodel.
It also shows the importance of feature engi-neering: designing relevant features is an art (Char-niak and Johnson, 2005).We took a closer look at the 1,000 features ofhighest weight and the 1,000 features of lowestweight (negative) for both languages that representthe most important features for discriminating be-tween correct and incorrect parses.
For English,62.0% of the positive features are backoff featureswhich involve at least one wildcard while they are85.9% for French.
Interestingly, similar results holdfor negative features.
The difference between thetwo languages is hard to interpret and might be duein part to lexical properties and to the fact that thesefeatures may play a balancing role against towardsnon-backoff features that promote overfitting.Expectedly, posterior probability features havethe highest weight and the n-best rank feature has thehighest negative weight.
As evidenced by Table 5,96en (+) en (-) fr (+) fr (-)Linear 30.4 36.1 44.8 44.0Unigram 20.7 16.3 9.7 8.2Bigram 27.4 29.1 20.8 24.4Chain 15.4 15.3 13.7 19.4Siblings 5.8 3.0 10.8 3.6Table 5: Repartition of weight (in percentage) in the1,000 highest (+) and lowest (-) weighted features for En-glish and French.among the other feature templates, linear context oc-cupies most of the weight mass of the 1,000 highestweighted features.
It is interesting to note that theunigram and bigram templates are less present forFrench than for English while the converse seems tobe true for the linear template.
Sibling features areconsistently less relevant.In terms of LAS performance, on the PTB testset the reranked output is better than the baselineon 22.4% of the sentences while the opposite is truefor 10.4% of the sentences.
In 67.0% of the sen-tences, they have the same LAS (but not necessar-ily the same errors).
This emphasises the difficultyof reranking an already good system and also ex-plains why oracle performance is not reached.
Boththe baseline and reranker output are completely cor-rect on 21.3% of the sentences, while PCFG-LA cor-rectly parses 23% of the sentences and the MIRAbrings that number to 26%.Figures 3 and 4 show hand-picked sentences forwhich the reranker selected the correct parse.
TheFrench sentence is a typical difficult example forPCFGs because it involves a complex rewriting rulewhich might not be well covered in the trainingdata (SENT ?
NP VP PP PONCT PP PONCT PPPONCT).
The English example is tied to a wrongattachment of the prepositional phrase to the verbinstead of the date, which lexicalized features of thereranker handle easily.5 ConclusionWe showed that using a discriminative reranker, ontop of a phrase structure parser, based on converteddependency structures could lead to significant im-provements over dependency and phrase structureparse results.
We experimented on two treebanksfor two languages, English and French and we mea-sured the improvement of parse quality on three dif-ferent metrics: Parseval F-score, LAS and UAS,with the biggest error reduction on the latter.
How-ever the gain is not as high as expected by lookingat oracle scores, and we can suggest several possibleimprovements on this method.First, the sequential approach is vulnerable to cas-cading errors.
Whereas the generative parser pro-duces several candidates, this is not the case of thefunctional annotators: these errors are not amend-able.
It should be possible to have a functional tag-ger with ambiguous output upon which the rerankercould discriminate.
It remains an open question ashow to integrate ambiguous output from the parserand from the functional tagger.
The combinationof n-best lists would not scale up and working onthe ambiguous structure itself, the packed forest asin (Huang, 2008), might be necessary.
Another pos-sibility for future work is to let the phrase-basedparser itself perform function annotation, but somepreliminary tests on French showed disappointingresults.Second, designing good features, sufficiently gen-eral but precise enough, is, as already coinedby Charniak and Johnson (2005), an art.
More for-mally, we can see several alternatives.
Dependencystructures could be exploited more thoroughly using,for example, tree kernels.
The restricted number ofcandidates enables the use of more global features.Also, we haven?t used any language-specific syntac-tic features.
This could be another way to improvethis system, relying on external linguistic knowledge(lexical preferences, subcategorisation frames, cop-ula verbs, coordination symmetry .
.
.
).
Integratingfeatures from the phrase-structure trees is also an op-tion that needs to be explored.Third this architecture enables the integration ofseveral systems.
We experimented on French using apart-of-speech tagger but we could also use anotherparser and either use the methodology of (Johnsonand Ural, 2010) or (Zhang et al, 2009) which fu-sion n-best lists form different parsers, or use stack-ing methods where an additional parser is used asa guide for the main parser (Nivre and McDonald,2008).Finally it should be noted that this system does notrely on any language specific feature, and thus canbe applied to languages other that French or English97NNSStocksNPVBDwere CD698 CDmillionQPNNSbushelsNPINon NNPMay CD31NPINof DTthis NNyearNPPPNPPPVP..SNNSStocksNPVBDwere CD698 CDmillionQPNNSbushelsNPINon NNPMay CD31NPPPINof DTthis NNyearNPPPVP..Sdependency parsessyntagmaticparsesBefore reranking After rerankingFigure 3: English sentence from the WSJ test set for which the reranker selected the correct tree while the firstcandidate of the n-best list suffered from an incorrect attachment.SENTNP VN PPPONCTNPPONCTNPP NPP V VPP P AP DET ADJ PONCT P ADJADJSENTNP VN PPPONCT NP PONCTNPP NPP V VPP P AP P NC PONCT P ADJADJ NPPP PPNPP NPP V VPP P ADJ PONCT DET ADJ PONCT P ADJ PONCT NPP NPP V VPP P ADJ PONCT P NC PONCT P ADJ PONCTdependency parsessyntagmatic parsesBefore reranking After rerankingFigure 4: Sentence from the FTB for which the best parse according to baseline was incorrect, probably due to thetendency of the PCFG-LA model to prefer rules with more support.
The reranker selected the correct parse.without re-engineering new reranking features.
Thismakes this architecture suitable for morphologicallyrich languages.AcknowledgmentsThis work has been funded by the French AgenceNationale pour la Recherche, through the projectSEQUOIA (ANR-08-EMER-013).ReferencesAnne Abeill?, Lionel Cl?ment, and Toussenel Fran?ois,2003.
Treebanks, chapter Building a treebank forFrench.
Kluwer, Dordrecht.M.
Attia, J.
Foster, D. Hogan, J.
Le Roux, L. Tounsi, andJ.
van Genabith.
2010.
Handling Unknown Words inStatistical Latent-Variable Parsing Models for Arabic,English and French.
In Proceedings of SPMRL.Bernd Bohnet.
2010.
Top Accuracy and Fast Depen-dency Parsing is not a Contradiction.
In Proceedingsof COLING.M.-H. Candito and B. Crabb?.
2009.
Improving Gen-erative Statistical Parsing with Semi-Supervised WordClustering.
In Proceedings of IWPT 2009.M.-H. Candito, J. Nivre, P. Denis, and E. Henestroza An-guiano.
2010a.
Benchmarking of Statistical Depen-dency Parsers for French.
In Proceedings of COL-ING?2010.Marie Candito, Beno?t Crabb?, and Pascal Denis.
2010b.Statistical French Dependency Parsing : TreebankConversion and First Results.
In Proceedings ofLREC2010.Xavier Carreras, Michael Collins, and Terry Koo.
2008.TAG, Dynamic Programming and the Perceptron forEfficient, Feature-rich Parsing.
In CONLL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt DiscriminativeReranking.
In Proceedings of ACL.98Grzegorz Chrupa?a, Nicolas Stroppa, Josef van Genabith,and Georgiana Dinu.
2007.
Better training for func-tion labeling.
In Proceedings of RANLP, Borovets,Bulgaria.Michael Collins.
1997.
Three Generative, LexicalisedModels for Statistical Parsing.
In Proceedings of the35th Annual Meeting of the ACL.Michael Collins.
2000.
Discriminative Reranking forNatural Language Parsing.
In Proceedings of ICML.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalevShwartz, and Yoram Singer.
2006.
OnlinePassive-Aggressive Algorithm.
Journal of MachineLearning Research.Pascal Denis and Beno?t Sagot.
2009.
Coupling an anno-tated corpus and a morphosyntactic lexicon for state-of-the-art pos tagging with less human effort.
In Pro-ceedings PACLIC 23, Hong Kong, China.Liang Huang.
2008.
Forest Reranking: DiscriminativeParsing with Non-Local Features.
In Proceedings ofACL.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProceedings of NODALIDA 2007, pages 105?112,Tartu, Estonia, May 25-26.Mark Johnson and Ahmet Engin Ural.
2010.
Rerank-ing the Berkeley and Brown Parsers.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 665?668, Los An-geles, California, June.
Association for ComputationalLinguistics.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The penn tree-bank: Annotating predicate argument structure.
InProceedings of the ARPA Speech and Natural Lan-guage Workshop.Takuya Matsuzaki, Yusuke Miyao, and Jun ichi Tsujii.2005.
Probabilistic CFG with Latent Annotations.
InProceedings of ACL.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online Large-Margin Training of DependencyParsers.
In Association for Computational Linguistics(ACL).Ryan McDonald.
2006.
Discriminative Training andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InProceedings of ACL, pages 950?958.Slav Petrov and Dan Klein.
2007.
Improved Infer-ence for Unlexicalized Parsing.
In HLT-NAACL, pages404?411.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, and In-terpretable Tree Annotation.
In ACL.Geoffrey K. Pullum and Barbara C. Scholz.
2001.
On thedistinction between model-theoretic and generative-enumerative syntactic frameworks.
In Logical Aspectsof Computational Linguistics.Owen Rambow.
2010.
The Simple Truth about Depen-dency and Phrase Structure Representations: An Opin-ion Piece.
In NAACL HLT.Frank Rosenblatt.
1958.
The Perceptron: A ProbabilisticModel for Information Storage and Organization in theBrain.
Psychological Review.Beno?t Sagot.
2010.
The lefff, a freely available andlarge-coverage lexicon for french.
In Proceedings ofLREC 2010, La Valette, Malta.J.
Suzuki, H. Isozaki, X. Carreras, and M. Collins.
2009.An empirical study of semi-supervised structured con-ditional models for dependency parsing.
In Proceed-ings of the 2009 Conference on Empirical Methods inNatural Language Processing: Volume 2-Volume 2,pages 551?560.
Association for Computational Lin-guistics.Hui Zhang, Min Zhang, Chew Lim Tan, and HaizhouLi.
2009.
K-best combination of syntactic parsers.In Proceedings of EMNLP.99
