Improving Domain-Specific Word Alignment for Computer AssistedTranslationWU Hua, WANG HaifengToshiba (China) Research and Development Center5/F., Tower W2, Oriental PlazaNo.1, East Chang An Ave., Dong Cheng DistrictBeijing, China, 100738{wuhua, wanghaifeng}@rdc.toshiba.com.cnAbstractThis paper proposes an approach to improveword alignment in a specific domain, in whichonly a small-scale domain-specific corpus isavailable, by adapting the word alignmentinformation in the general domain to thespecific domain.
This approach first trains twostatistical word alignment models with thelarge-scale corpus in the general domain and thesmall-scale corpus in the specific domainrespectively, and then improves thedomain-specific word alignment with these twomodels.
Experimental results show a significantimprovement in terms of both alignmentprecision and recall.
And the alignment resultsare applied in a computer assisted translationsystem to improve human translation efficiency.1 IntroductionBilingual word alignment is first introduced as anintermediate result in statistical machine translation(SMT) (Brown et al, 1993).
In previous alignmentmethods, some researchers modeled the alignmentswith different statistical models (Wu, 1997; Och andNey, 2000; Cherry and Lin, 2003).
Some researchersuse similarity and association measures to buildalignment links (Ahrenberg et al, 1998; Tufis andBarbu, 2002).
However, All of these methodsrequire a large-scale bilingual corpus for training.When the large-scale bilingual corpus is notavailable, some researchers use existing dictionariesto improve word alignment (Ker and Chang, 1997).However, few works address the problem ofdomain-specific word alignment when neither thelarge-scale domain-specific bilingual corpus nor thedomain-specific translation dictionary is available.This paper addresses the problem of wordalignment in a specific domain, where only a smalldomain-specific corpus is available.
In thedomain-specific corpus, there are two kinds ofwords.
Some are general words, which are alsofrequently used in the general domain.
Others aredomain-specific words, which only occur in thespecific domain.
In general, it is not quite hard toobtain a large-scale general bilingual corpus whilethe available domain-specific bilingual corpus isusually quite small.
Thus, we use the bilingualcorpus in the general domain to improve wordalignments for general words and the corpus in thespecific domain for domain-specific words.
In otherwords, we will adapt the word alignmentinformation in the general domain to the specificdomain.In this paper, we perform word alignmentadaptation from the general domain to a specificdomain (in this study, a user manual for a medicalsystem) with four steps.
(1) We train a wordalignment model using the large-scale bilingualcorpus in the general domain; (2) We train anotherword alignment model using the small-scalebilingual corpus in the specific domain; (3) We buildtwo translation dictionaries according to thealignment results in (1) and (2) respectively; (4) Foreach sentence pair in the specific domain, we use thetwo models to get different word alignment resultsand improve the results according to the translationdictionaries.
Experimental results show that ourmethod improves domain-specific word alignment interms of both precision and recall, achieving a21.96% relative error rate reduction.The acquired alignment results are used in ageneralized translation memory system (GTMS, akind of computer assisted translation systems)(Simard and Langlais, 2001).
This kind of systemfacilitates the re-use of existing translation pairs totranslate documents.
When translating a newsentence, the system tries to provide thepre-translated examples matched with the input andrecommends a translation to the human translator,and then the translator edits the suggestion to get afinal translation.
The conventional TMS can onlyrecommend translation examples on the sententiallevel while GTMS can work on both sentential andsub-sentential levels by using word alignment results.These GTMS are usually employed to translatevarious documents such as user manuals, computeroperation guides, and mechanical operation manuals.22.1Word Alignment AdaptationBi-directional Word AlignmentIn statistical translation models (Brown et al, 1993),only one-to-one and more-to-one word alignmentlinks can be found.
Thus, some multi-word unitscannot be correctly aligned.
In order to deal with thisproblem, we perform translation in two directions(English to Chinese, and Chinese to English) asdescribed in (Och and Ney, 2000).
The GIZA++toolkit 1  is used to perform statistical wordalignment.For the general domain, we use  andto represent the alignment sets obtained with Englishas the source language and Chinese as the targetlanguage or vice versa.
For alignment links in bothsets, we use i for English words and j for Chinesewords.1SG 2SG}0 },{|),{(1 ?== jjjj aaAjASG}0  },{|),{(2 ?== iiii aaAAiSGWhere, is the position of the sourceword aligned to the target word in position k. The setindicates the words aligned to the samesource word k. For example, if a Chinese word inposition j is connect to an English word in position i,then .
And if a Chinese word in position j isconnect to English words in position i and k, then.
),( jikak =),( jikAk =ia j =},{ kiA j =Based on the above two alignment sets, weobtain their intersection set, union set 2  andsubtraction set.Intersection:  21 SGSGSG ?=Union:  21 SGSGPG ?=Subtraction:  SGMG ?= PGFor the specific domain, we use  andto represent the word alignment sets in the twodirections.
The symbols ,1SF 2SFSF PF  and MFrepresents the intersection set, union set and thesubtraction set, respectively.2.2Translation Dictionary AcquisitionWhen we train the statistical word alignment modelwith a large-scale bilingual corpus in the generaldomain, we can get two word alignment results forthe training data.
By taking the intersection of thetwo word alignment results, we build a newalignment set.
The alignment links in thisintersection set are extended by iteratively addingword alignment links into it as described in (Och andNey, 2000).1 It is located at http://www.isi.edu/~och/GIZA++.html2  In this paper, the union operation does not remove thereplicated elements.
For example, if set one includes twoelements {1, 2} and set two includes two elements {1, 3}, thenthe union of these two sets becomes {1, 1, 2, 3}.Based on the extended alignment links, we buildan English to Chinese translation dictionarywith translation probabilities.
In order to filter somenoise caused by the error alignment links, we onlyretain those translation pairs whose translationprobabilities are above a threshold1D1?
orco-occurring frequencies are above a threshold 2?
.When we train the IBM statistical wordalignment model with a limited bilingual corpus inthe specific domain, we build another translationdictionary  with the same method as for thedictionary .
But we adopt a different filteringstrategy for the translation dictionary .
We uselog-likelihood ratio to estimate the associationstrength of each translation pair because Dunning(1993) proved that log-likelihood ratio performedvery well on small-scale data.
Thus, we get thetranslation dictionary  by keeping those entrieswhose log-likelihood ratio scores are greater than athreshold2D1D32D2D?
.2.3 Word Alignment Adaptation AlgorithmBased on the bi-directional word alignment, wedefine  as SI SFSGSI ?= and as UGSIPFPGUG ?
?= .
The word alignment links inthe set SI  are very reliable.
Thus, we directlyaccept them as correct links and add them into thefinal alignment set .
WAInput: Alignment set and  SI UG(1) For alignment links in , we directly addthem into the final alignment set .SIWA(2) For each English word i in the , we firstfind its different alignment links, and then dothe following:UGa) If there are alignment links found indictionary , add the link with the largestprobability to .1DWAb) Otherwise, if there are alignment links foundin dictionary , add the link with thelargest log-likelihood ratio score to .2DWAc) If both a) and b) fail, but three links select thesame target words for the English word i, weadd this link into .
WAd) Otherwise, if there are two different links forthis word: one target is a single word, andthe other target is a multi-word unit and thewords in the multi-word unit have no link in, add this multi-word alignment link to.WAWAOutput: Updated alignment set  WAFigure 1.
Word Alignment Adaptation AlgorithmFor each source word in the set , there aretwo to four different alignment links.
We first usetranslation dictionaries to select one link amongthem.
We first examine the dictionary  and thento see whether there is at least an alignment linkof this word included in these two dictionaries.
If itis successful, we add the link with the largestprobability or the largest log-likelihood ratio score tothe final set .
Otherwise, we use two heuristicrules to select word alignment links.
The detailedalgorithm is described in Figure 1.UG1D2DWAFigure 2.
Alignment ExampleFigure 2 shows an alignment result obtained withthe word alignment adaptation algorithm.
Forexample, for the English word ?x-ray?, we have twodifferent links in UG .
One is (x-ray, X) and theother is (x-ray, X ??).
And the single Chinesewords ???
and ???
have no alignment links in theset .
According to the rule d), we select the link(x-ray, X??
).WA3 Evaluation3.13.2We compare our method with three other methods.The first method ?Gen+Spec?
directly combines thecorpus in the general domain and in the specificdomain as training data.
The second method ?Gen?only uses the corpus in the general domain astraining data.
The third method ?Spec?
only uses thedomain-specific corpus as training data.
With thesetraining data, the three methods can get their owntranslation dictionaries.
However, each of them canonly get one translation dictionary.
Thus, only oneof the two steps a) and b) in Figure 1 can be appliedto these methods.
The difference between these threemethods and our method is that, for each word, ourmethod has four candidate alignment links while theother three methods only has two candidatealignment links.
Thus, the steps c) and d) in Figure 1should not be applied to these three methods.Training and Testing DataWe have a sentence aligned English-Chinesebilingual corpus in the general domain, whichincludes 320,000 bilingual sentence pairs, and asentence aligned English-Chinese bilingual corpus inthe specific domain (a medical system manual),which includes 546 bilingual sentence pairs.
Fromthis domain-specific corpus, we randomly select 180pairs as testing data.
The remained 366 pairs areused as domain-specific training data.The Chinese sentences in both the training setand the testing set are automatically segmented intowords.
In order to exclude the effect of thesegmentation errors on our alignment results, wecorrect the segmentation errors in our testing set.The alignments in the testing set are manuallyannotated, which includes 1,478 alignment links.Overall PerformanceWe use evaluation metrics similar to those in (Ochand Ney, 2000).
However, we do not classifyalignment links into sure links and possible links.We consider each alignment as a sure link.
If we useto represent the alignments identified by theproposed methods and  to denote the referencealignments, the methods to calculate the precision,recall, and f-measure are shown in Equation (1), (2)and (3).
According to the definition of the alignmenterror rate (AER) in (Och and Ney, 2000), AER canbe calculated with Equation (4).
Thus, the higher thef-measure is, the lower the alignment error rate is.Thus, we will only give precision, recall and AERvalues in the experimental results.GSCS|S||SS|GCG ?=precision       (1)|S||SS|CCG ?=recall   (2)||||||*2CGCGSSSSfmeasure +?=  (3)fmeasureSSSSAERCGCG ?=+?
?= 1||||||*21  (4)Method Precision Recall AEROurs 0.8363 0.7673 0.1997Gen+Spec 0.8276 0.6758 0.2559Gen 0.8668 0.6428 0.2618Spec 0.8178 0.4769 0.3974Table 1.
Word Alignment Adaptation ResultsWe get the alignment results shown in Table 1 bysetting the translation probability threshold to1.01 =?
, the co-occurring frequency threshold to52 =?
and log-likelihood ratio score to 503 =?
.From the results, it can be seen that our approachperforms the best among others, achieving muchhigher recall and comparable precision.
It alsoachieves a 21.96% relative error rate reductioncompared to the method ?Gen+Spec?.
This indicatesthat separately modeling the general words anddomain-specific words can effectively improve theword alignment in a specific domain.4 Computer Assisted Translation SystemA direct application of the word alignment result tothe GTMS is to get translations for sub-sequences inthe input sentence using the pre-translated examples.For each sentence, there are many sub-sequences.GTMS tries to find translation examples that matchthe longest sub-sequences so as to cover as much ofthe input sentence as possible without overlapping.Figure 3 shows a sentence translated on thesub-sentential level.
The three panels display theinput sentence, the example translations and thetranslation suggestion provided by the system,respectively.
The input sentence is segmented tothree parts.
For each part, the GTMS finds oneexample to get a translation fragment according tothe word alignment result.
By combining the threetranslation fragments, the GTMS produces a correcttranslation suggestion ???????
CT ????
?Without the word alignment information, theconventional TMS cannot find translations for theinput sentence because there are no examples closelymatched with it.
Thus, word alignment informationcan improve the translation accuracy of the GTMS,which in turn reduces editing time of the translatorsand improves translation efficiency.Figure 3.
A Snapshot of the Translation System5 ConclusionThis paper proposes an approach to improvedomain-specific word alignment through alignmentadaptation.
Our contribution is that our approachimproves domain-specific word alignment byadapting word alignment information from thegeneral domain to the specific domain.
Ourapproach achieves it by training two alignmentmodels with a large-scale general bilingual corpusand a small-scale domain-specific corpus.
Moreover,with the training data, two translation dictionariesare built to select or modify the word alignmentlinks and further improve the alignment results.Experimental results indicate that our approachachieves a precision of 83.63% and a recall of76.73% for word alignment on a user manual of amedical system, resulting in a relative error ratereduction of 21.96%.
Furthermore, the alignmentresults are applied to a computer assisted translationsystem to improve translation efficiency.Our future work includes two aspects.
First, wewill seek other adaptation methods to furtherimprove the domain-specific word alignment results.Second, we will use the alignment adaptation resultsin other applications.ReferencesLars Ahrenberg, Magnus Merkel and MikaelAndersson.
1998.
A Simple Hybrid Aligner forGenerating Lexical Correspondences in ParallelTests.
In Proc.
of the 36th Annual Meeting of theAssociation for Computational Linguistics and the17th International Conference on ComputationalLinguistics, pages 29-35.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics,19(2): 263-311.Colin Cherry and Dekang Lin.
2003.
A ProbabilityModel to Improve Word Alignment.
In Proc.
ofthe 41st Annual Meeting of the Association forComputational Linguistics, pages 88-95.Ted Dunning.
1993.
Accurate Methods for theStatistics of Surprise and Coincidence.Computational Linguistics, 19(1): 61-74.Sue J. Ker, Jason S. Chang.
1997.
A Class-basedApproach to Word Alignment.
ComputationalLinguistics, 23(2): 313-343.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In Proc.
of the 38thAnnual Meeting of the Association forComputational Linguistics, pages 440-447.Michel Simard and Philippe Langlais.
2001.Sub-sentential Exploitation of TranslationMemories.
In Proc.
of MT Summit VIII, pages335-339.Dan Tufis and Ana Maria Barbu.
2002.
LexicalToken Alignment: Experiments, Results andApplication.
In Proc.
of the Third InternationalConference on Language Resources andEvaluation, pages 458-465.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of ParallelCorpora.
Computational Linguistics, 23(3):377-403.
