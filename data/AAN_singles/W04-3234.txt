Trained Named Entity Recognition Using Distributional ClustersDayne FreitagHNC Software, LLC3661 Valley Centre DriveSan Diego, CA 92130DayneFreitag@fairisaac.comAbstractThis work applies boosted wrapper induction(BWI), a machine learning algorithm for informa-tion extraction from semi-structured documents, tothe problem of named entity recognition.
The de-fault feature set of BWI is augmented with featuresbased on distributional term clusters induced from alarge unlabeled text corpus.
Using no traditional lin-guistic resources, such as syntactic tags or special-purpose gazetteers, this approach yields results nearthe state of the art in the MUC 6 named entity do-main.
Supervised learning using features derivedthrough unsupervised corpus analysis may be re-garded as an alternative to bootstrapping methods.1 IntroductionThe problem of named entity recognition (NER) hasrecently received increasing attention.
Identifica-tion of generic semantic categories in text?suchas mentions of people, organizations, locations, andtemporal and numeric expressions?is a necessaryfirst step in many applications of information ex-traction, information retrieval, and question answer-ing.
To a large extent, knowledge-poor methods suf-fice to yield good recognition performance.
In par-ticular, supervised learning can be used to producea system with performance at or near the state of theart (Bikel et al, 1997).In the supervised learning framework, a corpusof (typically) a few hundred documents is annotatedby hand to identify the entities of interest.
Featuresof local context are then used to train a system todistinguish instances from non-instances in noveltexts.
Such features may include literal word tests,patterns of orthography, parts of speech, seman-tic categories, or membership in special-purposegazetteers.While supervised training greatly facilitates thedevelopment of a robust NER system, the re-quirement of a substantial training corpus remainsan impediment to the rapid deployment of NERin new domains or new languages.
A numberbush peters reagan noriega ...john robert james david ...president chairman head owner ...japan california london chicago ...Table 1: Sample members of four clusters from theWall Street Journal corpus.of researchers have therefore sought to exploitthe availability of unlabeled documents, typicallyby bootstrapping a classifier using automatic la-bellings (Collins and Singer, 1999; Cucerzan andYarowsky, 1999; Thelen and Riloff, 2002).Here, we investigate a different approach.
Us-ing a distributional clustering technique called co-clustering, we produce clusters which, intuitively,should be useful for NER.
Table 1 shows exam-ple terms from several sample clusters induced us-ing a collection of documents from the Wall StreetJournal (WSJ).
Several papers have shown that dis-tributional clustering yields categories that havehigh agreement with part of speech (Schu?tze, 1995;Clark, 2000).
As the table illustrates, these clus-ters also tend to have a useful semantic dimension.Clustering on the WSJ portion of the North Ameri-can News corpus yields two clusters that clearly cor-respond to personal names, one for first names andone for last names.
As an experiment, we scannedthe MUC6 NER data set for token sequences con-sisting of zero or more members of the first namecluster (or an initial followed by a period), followedby one or more members of the last name cluster.This simple procedure identified 64% of personalnames with 77% precision.In this paper, we attempt to improve on thisresult by converting the clusters into features tobe exploited by a general-purpose machine learn-ing algorithm for information extraction.
In Sec-tion 2, we provide a brief description of BoostedWrapper Induction (BWI), a pattern learner that hasyielded promising results on semi-structured infor-mation extraction problems (Freitag and Kushmer-ick, 2000).
In Section 3, we describe our clusteringapproach and its particular application.
Section 4presents the results of our experiments.
Finally, inSection 5, we assess the significance of our contri-bution and attempt to identify promising future di-rections.2 BWIBWI decomposes the problem of recognizing fieldinstances into two Boolean classification problems:recognizing field-initial and field-terminal tokens.Given a target field, a separate classifier is learnedfor each of these problems, and the distribution offield lengths is modeled as a frequency histogram.At application time, tokens that test positive for ini-tial are paired with those testing positive for termi-nal.
If the length of a candidate instance, as definedby such a pair, is determined to have non-zero like-lihood using the length histogram, a prediction isreturned.Each of the three parts of a full prediction?initialboundary, terminal boundary, and length?is as-signed a real-valued confidence.
The confidence ofa boundary detection is its strength as determined byAdaBoost, while that of the length assessment is theempirical length probability, which is determinedusing the length histogram.
The confidence of thefull prediction is the product of these three individ-ual confidence scores.
In the event that overlappingpredictions are found in this way (a rare event, em-pirically), the predictions with lower confidence arediscarded.In this section, we sketch those aspects of BWIrelevant to the current application.
More detailsare available in the paper in which BWI was de-fined (Freitag and Kushmerick, 2000).2.1 BoostingBWI uses generalized AdaBoost to produce eachboundary classifier (Schapire and Singer, 1998).Boosting is a procedure for improving the perfor-mance of a ?weak learner?
by repeatedly applyingit to a training set, at each step modifying exam-ple weights to emphasize those examples on whichthe learner has done poorly in previous steps.
Theoutput is a weighted collection of weak learner hy-potheses.
Classification involves having the individ-ual hypotheses ?vote,?
with strengths proportionalto their weights, and summing overlapping votes.Although this is the first application of BWI toNER, boosting has previously been shown to workwell on this problem.
Differing from BWI in the de-tails of the application, two recent papers neverthe-less demonstrate the effectiveness of the boostingCap Initial capitalAllCap All capitalsUncap Initial lower caseAlpha Entirely alphabetic charactersANum Entirely alpha-numeric charactersPunc PunctuationNum Entirely numeric charactersSchar Single alphabetic characterAny AnythingTable 2: Default wildcards used in these experi-ments.paradigm for NER in several languages (Carreraset al, 2002; Wu et al, 2002), one of them achiev-ing the best overall performance in a comparison ofseveral systems (Sang, 2002).2.2 Boundary DetectorsThe output of a single invocation of the weak learnerin BWI is always an individual pattern, called aboundary detector.
A detector has two parts, oneto match the text leading up to a boundary, theother for trailing text.
Each part is a list of zeroor more elements.
In order for a boundary tomatch a detector, the tokens preceding the bound-ary (or following it) must match the correspondingelements in sequence.
For example, the detector[ms .
][jones] matches boundaries precededby the (case-normalized) two-token sequence ?ms.?
and followed by the single token ?jones?.Detectors are grown iteratively, beginning withan empty detector and repeatedly adding the ele-ment that best increases the ability of the currentdetector to discriminate true boundaries from falseones, using a cost function sensitive to the exam-ple weighting.
A look-ahead parameter allows thisdecision to be based on several additional contexttokens.
The process terminates when no extensionsyield a higher score than the current detector.2.3 WildcardsThe elements of the detector [ms .
][jones] areliteral elements, which match tokens using case-normalized string comparison.
More interesting el-ements can be introduced by defining token wild-cards.
Each wildcard defines some Boolean func-tion over the space of tokens.Table 2 lists the baseline wildcards.
Using wild-cards from this list, the example detector can be gen-eralized to match a much broader range of bound-aries (e.g., [ms <Any>][<Cap>]).
By defin-ing new wildcards, we can inject useful domainknowledge into the inference process, potentiallyimproving the performance of the resulting extrac-tor.
For example, we might define a wildcard called?Honorific?
that matches any of ?ms?, ?mr?,?mrs?, and ?dr?.2.4 Boundary WildcardsIn the original formulation of BWI, boundaries areidentified without reference to the location of theopposing boundary.
However, we might expect thatthe end of a name, say, would be easier to identify ifwe know where it begins.
We can build detectorsthat exploit this knowledge by introducing a spe-cial wildcard (called Begin) that matches the be-ginnings of names.In these experiments, therefore, we modifyboundary detection in the following way.
Insteadof two detector lists, we learn four?the two listsas in the original formulation (call them    and 	 ), and two more lists (    and  	 ).
Ingenerating the latter two lists, we give the learneraccess to these special wildcards (e.g., the wildcardEnd in generating   ).At extraction time,  and   are firstused to detect boundaries, as before.
These de-tections are then used to determine which tokensmatch the ?special?
wildcards used by   and .
Then, instead of pairing     predictionswith those of , they are paired with thosemade by   (and    with   ).
In in-formal experiments, we found that this proceduretended to increase F1 performance by several pointson a range of tasks.
We adopt it uniformly in theexperiments reported here.3 Co-ClusteringAs in Brown, et al(1992), we seek a partition ofthe vocabulary that maximizes the mutual infor-mation between term categories and their contexts.To achieve this, we use information theoretic co-clustering (Dhillon et al, 2003), in which a spaceof entities, on the one hand, and their contexts, onthe other, are alternately clustered to maximize mu-tual information between the two spaces.3.1 BackgroundThe input to our algorithm is two finite sets of sym-bols, say ffflfiffi!
#"$ffi&%"('('(')"$ffi!
*,+.- (e.g., terms) and/ fi01 #"$02%("('('(')"$03*546- (e.g., term contexts), to-gether with a set of co-occurrence count data con-sisting of a non-negative integer 798:;=< for everypair of symbols >ffi@?A"$0)B)C from  and / .
The out-put is two partitions:  fiffi"(DDD"$ffi*+FE- and/Gfi0"(DDD"$0*4HE- , where each ffi?is a subset of (a ?cluster?
), and each 0?a subset of / .
Theco-clustering algorithm chooses the partitions and /to (locally) maximize the mutual informa-tion between them, under a constraint limiting thetotal number of clusters in each partition.Recall that the entropy or Shannon information ofa discrete distribution is:IKJMLON8QP>ffi,CSRUTP>ffi5CVD (1)This quantifies average improvement in one?sknowledge upon learning the specific value of anevent drawn from  .
It is large or small dependingon whether  has many or few probable values.The mutual information between random vari-ables  and / can be written:WXJZY[N8(; P>ffiF"$0\CSR]TP>ffiF"$0\CP>ffi,CP>0\C(2)This quantifies the amount that one expects to learnindirectly about  upon learning the value of / , orvice versa.3.2 The AlgorithmLet  be a random variable over vocabulary termsas found in some text corpus.
We define / torange over immediately adjacent tokens, encodingco-occurrences in such a way as to distinguish leftfrom right occurrences.Given co-occurrence matrices tabulated in thisway, we perform an approximate maximization of^_JEYE using a simulated annealing procedure inwhich each trial move takes a symbol ffi or 0 outof the cluster to which it is tentatively assigned andplaces it into another.
Candidate moves are chosenby selecting a non-empty cluster uniformly at ran-dom, randomly selecting one of its members, thenrandomly selecting a destination cluster other thanthe source cluster.
When temperature 0 is reached,all possible moves are repeatedly attempted until nofurther improvements are possible.For efficiency and noise reduction, we first clus-ter only the 5000 most frequent terms and contextterms.
The remaining terms in the corpus vocabu-lary are then added by assigning each term to thecluster that maximizes the mutual information ob-jective function.4 EvaluationWe experimented with the MUC 6 named entitydata set, which consists of a training set of 318 doc-uments, a validation set of 30 documents, and a testset of 30 documents.All documents are annotated to identify threetypes of name (PERSON, ORGANIZATION,[][september]DATE[in <Num>][<Punc>][][5 <ANum> .
<ANum>]TIME[midnight][][][$]MONEY[$ <Any> billion][][<Alph> <Punc>][<Any> %]PCT.
[<Num> percentage <Alph>][][mr <Any>][<Cap>]PERSON[<Cap>][, vice][][nissan]ORG.
[inc <Any>][][in][<Cap> , <Alph> <Punc>]LOC.
[germany][]Table 3: Sample boundary detectors for the sevenMUC 6 fields produced by BWI using the baselinefeature set.
An initial and terminal detector is shownfor each field.LOCATION), two types of temporal expression(DATE, TIME), and two types of numeric expres-sion (MONEY, PERCENT).
It is common to re-port performance in terms of precision, recall, andtheir harmonic mean (F1), a convention to which weadhere.4.1 BaselineUsing the wildcards listed in Table 2, we trainedBWI for 500 boosting iterations on each of the sevenentity fields.
The output out each of these trainingruns consists of   	 boundary detectors.Look-ahead was set to 3.Table 3 shows a few of the boundary detectorsinduced by this procedure.
These detectors wereselected manually to illustrate the kinds of patternsgenerated.
Note how some of the detectors amountto field-specific gazetteer entries.
Others have moreinteresting (and typically intuitive) structure.
Wedefer quantitative evaluation to the next section,where a comparison with the cluster-enhanced ex-tractors will be made.4.2 Adding Cluster FeaturesThe MUC 6 dataset was produced using articlesfrom the Wall Street Journal.
In order to pro-duce maximally relevant clusters, we used docu-ments from the WSJ portion of the North Ameri-can News corpus as input to co-clustering?some119,000 documents in total.
Note that there is a tem-poral disparity between the MUC 6 corpus and thisclustering corpus, which has an undetermined im-pact on performance.[<C95>][<C73>]PERS.
[<C144> <Any> <C106>][<Uncap>][][<C178> express]ORG.
[bank <ANum> <C146>][][][<C72> korea]LOC.
[<C160>][<Punc>]Table 4: Sample boundary detectors for the sevenMUC 6 fields produced by BWI using the expandedfeature set.72 general south north poor ...73 john robert james david ...95 says adds asks recalls ...106 clinton dole johnson gingrich ...144 mr ms dr sen ...146 japan american china congress ...160 washington texas california ...178 american foreign local ...Table 5: Most frequent members of clusters refer-enced by detectors in Table 4.We used this data to produce 200 clusters, as de-scribed in Section 3.
Treating each of these clus-ters as an unlabeled gazetteer, we then defined cor-responding wildcards.
For example, the value ofwildcard <C35> only matches a term belonging toCluster 35.
In order to reduce the training time of agiven boundary learning problem, we tabulated thefrequency of wildcard occurrence within three to-kens of any occurrences of the target boundary andomitted from training wildcards testing true fewerthan ten times.1Table 4, which lists sample detectors from theseruns, includes some that are clearly impossible toexpress using the baseline feature set.
An exam-ple is the first row, which matches a third-personpresent-tense verb used in quote attribution, fol-lowed by a first name (see Table 5).
At the sametime, some of the new wildcards are employed triv-ially, such as the use of <C178> in the field-initialdetector for the ORGANIZATION field.Table 6 shows performance of the two variantson the individual MUC 6 fields, tested over the?dryrun?
and ?formal?
test sets combined.
In thistable, we scored each field individually using ourown evaluation software.
An entity instance wasjudged to be correctly extracted if a prediction pre-cisely identified its boundaries (ignoring ?ALT?
at-1For the TIME field, which occurs a total of six times in thetraining set, this cut-off was a single occurrence.Field F1 Prec RecBase 0.766 0.765 0.768DATE Clust 0.782 0.776 0.789Base 0.667 1.000 0.500TIME Clust 0.667 1.000 0.500Base 0.938 0.926 0.949MONEY Clust 0.943 0.938 0.949Base 0.922 0.855 1.000PERCENT Clust 0.930 0.869 1.000Base 0.827 0.810 0.844PERSON Clust 0.892 0.859 0.927Base 0.587 0.811 0.460ORG.
Clust 0.733 0.796 0.680Base 0.726 0.675 0.785LOCATION Clust 0.724 0.648 0.821Table 6: Performance on the seven MUC 6 fields,without (Base) and with (Clust) cluster-based fea-tures.
Significantly better precision or recall scores,at the 95% confidence level, are in boldface.tributes).
Non-matching predictions and missed en-tities were counted as false positives and false neg-atives, respectively.
We assessed the statistical sig-nificance of precision and recall scores by comput-ing beta confidence intervals at the 95% level.
In thetable, the higher precision or recall is in boldface ifits separation from the lower score is significant.Except for TIME and LOCATION, all fields ben-efit from inclusion of the cluster features.
TIME,which is scarce in the training and test sets, is insen-sitive to their inclusion.
The effect on LOCATIONis more interesting.
It shares in the general tendencyof cluster features to increase recall, but loses preci-sion as a result.2 Although the increase in recall isapproximately the same as the loss in precision, theF1 score, which is more heavily influenced by thelower of precision and recall, drops slightly.While the effect of the cluster features on pre-cision is inconsistent, they typically benefit recall.This effect is most dramatic in the case of ORGA-NIZATION, where, at the expense of a small drop inprecision, recall increases by more than 20 points.The somewhat counter-intuitive improvements inprecision on some fields (particularly the significantimprovement on PERSON) is attributable to ourlearning framework.
Boosting for a sufficient num-ber of iterations forces a learner to account for allboundary tokens through one or more detectors.
Tothe exent that the baseline?s features are unable to2Note, however, that none of the differences observed forLOCATION are significant at the 95% level.account for as many of the boundary tokens, it isforced to learn a larger number of over-specializeddetectors that rely on questionable patterns in thedata.
Depending on the task, these detectors canlead to a larger proportion of false positives.The relatively weak result for DATE comes asa surprise.
Inspection of the data leads us to at-tribute this to two factors.
On the one hand, thereis considerable temporal drift between the trainingand test sets.
Many of the dates are specific tocontemporaneous events; patterns based on specificyears, therefore, generalize in only a limited way.At the same time, the notion of date, as understoodin the MUC 6 corpus, is reasonably subtle.
Mean-ing roughly ?non-TIME temporal expression,?
it in-cludes everything from shorthand date expressionsto more interesting phrases, such as, ?the first sixmonths of fiscal 1994.?In passing we note a few potentially relevant id-iosyncrasies in these experiments.
Most significantis a representational choice we made in tokenizingthe cluster corpus.
In tallying frequencies we treatedall numeric expressions as occurrences of a specialterm, ?*num*?.
Consequently, the tokens ?1989?and ?10,000?
are treated as instances of the sameterm, and clustering has no opportunity to distin-guish, say, years from monetary amounts.The (perhaps) disappointing performance on therelatively simple fields, TIME and PERCENT,somewhat under-reports the strength of the learner.As noted above, TIME occurs only very infre-quently.
Consequently, little training data is avail-able for this field and mistakes (BWI missed one ofthe three instances in the test set) have a large effecton the TIME-specific scores.
In the case of PER-CENT, we ignored MUC instructions not to attemptto recognize instances in tabular regions.
One ofthe documents contains a significant number of un-labeled percentages in such a table.
BWI duly rec-ognized these?to the detriment of the reported pre-cision.4.3 MUC EvaluationFor comparison with numbers reported in the lit-erature, we used the learned extractors to producemark-up and evaluated the result using the MUC 6scorer.
The MUC 6 evaluation framework differsfrom ours in two key ways.
Most importantly, allentity types are to be processed simultaneously.
Webenefit from this framework, since spurious predic-tions for one entity type may be superseded by cor-rect predictions for a related type.
The opportunityis greatest for the three name types; in inspectingthe false positives, we observed a number of confu-Field F1 Prec RecBase 0.91 0.91 0.91DATE Clust 0.92 0.90 0.94Base 0 0 0TIME Clust 0 0 0Base 0.95 0.94 0.96MONEY Clust 0.95 0.95 0.96Base 0.97 0.94 1.0PERCENT Clust 1.0 1.0 1.0Base 0.88 0.91 0.86PERSON Clust 0.94 0.94 0.95Base 0.62 0.78 0.52ORG.
Clust 0.79 0.84 0.74Base 0.86 0.86 0.87LOCATION Clust 0.86 0.80 0.92Base 0.79 0.85 0.73ALL Clust 0.87 0.88 0.86Table 7: Performance on the markup task, as scoredby the MUC 6 scorer.sions among these fields.3 The MUC scorer is alsomore lenient than ours, awarding points for extrac-tion of alternative strings and forgiving the inclusionof certain functional tokens in the extracted text.In moving to the multi-entity extraction setting,the obvious approach is to collect predictions fromall extractors simultaneously.
However, this re-quires a strategy for dealing with overlapping pre-dictions (e.g., a single text fragment labeled as botha person and organization).
We resolve such con-flicts by preferring in each case the extraction withthe highest confidence.
In order to render confi-dence scores more comparable, we normalized theweights of detectors making up each boundary clas-sifier so they sum to one.A comparison of Table 7 with Table 6 suggeststhe extent to which BWI benefits from the multi-field mark-up setting.
Note that, here, we used onlythe ?formal?
test set for evaluation, in contrast withthe numbers in Table 6, which combine the two testsets.
The lift we observe from cluster features is alsoin evidence here, and is most evident as an increasein recall, particularly of PERSON and ORGANI-ZATION.
There is now also an increase in globalprecision, attributable in large part to the benefit ofextracting multiple fields simultaneously.The F1 score produced by BWI is compara-ble to the best machine-learning-based results re-3For example, companies are occasionally named after peo-ple (e.g., Liz Claiborne).ported elsewhere.
Bikel, at al (1997), reports sum-mary F1 of 0.93 on the same test set, but usinga model trained on 450,000 words.
We count ap-proximately 130,000 words in the experiments re-ported here.
The numbers reported by Bennett, etal (1997), for PERSON, ORGANIZATION, andLOCATION (F1 of 0.947, 0.815, and 0.925, respec-tively), are slightly better than the numbers BWIreaches on the same fields.
Note, however, that thefeatures provided to their learner include syntacticlabels and carefully engineered semantic categories,whereas we eschew knowledge- and labor-intensiveresources.
This has important implications for theportability of the approaches to new domains andlanguages.By taking a few post-processing steps, it is pos-sible to realize further improvements.
For example,the learner occasionally identifies terms and phraseswhich some simple rules can reliably reject.
By sup-pressing any prediction that consists entirely of astopword, we increase the precision of both ORGA-NIZATION and LOCATION to 0.86 (from 0.84 and0.80) and overall F1 to 0.88.We can also exploit what Cucerzan andYarowsky (1999) call the one sense per discoursephenomenon, the tendency of terms to have a fixedmeaning within a single document.
By mark-ing up unmarked strings that match extracted en-tity instances in the same document, we can im-prove the recall of some fields.
We added this post-processing step for the PERSON and ORGANI-ZATION fields.
This increased recall of PERSONfrom 0.95 to 0.98 and of ORGANIZATION from0.74 to 0.79 with minimal changes to precision anda slight improvement in summary F1.4.4 Analysis and Related WorkThe promise of this general method?supervisedlearning on small training set using features derivedfrom a larger unlabeled set?lies in the support itprovides for rapid deployment in novel domains andlanguages.
Without relying on any linguistic re-sources more advanced than a tokenizer and someorthographic features, we can produce a NER mod-ule using only a few annotated documents.How few depends ultimately on the difficulty ofthe domain.
We might also expect the benefit ofdistributional features to decrease with increasingtraining set size.
Figure 1 displays the F1 learning-curve performance of BWI, both with and withoutcluster features, on the two fields that benefit thegreatest from these features, PERSON and ORGA-NIZATION.
As expected, the difference appears tobe greatest on the low end of the horizontal axis (al-0.10.20.30.40.50.60.70.80.90  50  100  150  200  250  300  350FormaltestF1Training documentsPERSON (clust.
)PERSON (base)ORG.
(clust.)ORG.
(base)Figure 1: F1 as a function of training set size innumber of documents.though overfitting complicates the comparison).
Atthe same time, the improvement is fairly consistentat all training set sizes.
Either the baseline featureset is ultimately too impoverished for this task, or,more likely, the complete MUC 6 training set (318documents) is small for this class of learner.Techniques to lessen the need for annotation forNER have received a fair amount of attention re-cently.
The prevailing approach to this problem isa bootstrapping technique, in which, starting with afew hand-labeled examples, the system iterativelyadds automatic labels to a corpus, training itself,as it were.
Examples of this are Cucerzan andYarowsky (1999), Thelen and Riloff (2002), andCollins and Singer (1999).These techniques address the same problem asthis paper, but are otherwise quite different from thework described here.
The labeling method (seed-ing) is an indirect form of corpus annotation.
Thepromise of all such approaches is that, by startingwith a small number of seeds, reasonable results canbe achieved at low expense.
However, it is difficultto tell how much labeling corresponds to a givennumber of seeds, since this depends on the cover-age of the seeds.
Note, too, that any bootstrappingapproach must confront the problem of instability;poor initial decisions by a bootstrapping algorithmcan lead to large eventual performance degrada-tions.
We might expect a lightly supervised learnerwith access to features based on a full-corpus anal-ysis to yield more consistently strong results.Of the three approaches mentioned above, onlyCucerzan and Yarowsky do not presuppose a syn-tactic analysis of the corpus, so their work is perhapsmost comparable to this one.
Of course, compar-isons must be strongly qualified, given the differentlabeling methods and data sets.
Nevertheless, per-formance of cluster-enhanced BWI at the low endof the horizontal axis compares favorably with theEnglish F1 performance of 0.543 they report using190 seed words.
And, arguably, annotating 10-20documents is no more labor intensive than assem-bling a list of 190 seed words.Strong corroboration for the approach advocatedin this paper is provided by Miller, et al(2004),in which cluster-based features are combined witha sequential maximum entropy model proposed inCollins (2002) to advance the state of the art.
In ad-dition, using active learning, the authors are able toreduce human labeling effort by an order of magni-tude.Miller, et al use a proprietary data set for train-ing and testing, so it is difficult to make a closecomparison of outcomes.
At roughly comparabletraining set sizes, they appear to achieve a scoreof about 0.89 (F1) with a ?conventional?
HMM,versus 0.93 using the discriminative learner trainedwith cluster features (compared with 0.86 reachedby BWI).
Both the HMM and Collins model areconstrained to account for an entire sentence in tag-ging it, making determinations for all fields simulta-neously, in contrast to the individual, local boundarydetections made by BWI.
This characteristic proba-bly accounts for the accuracy advantage they appearto enjoy.An interesting distinguishing feature of Miller,et al is their use of hierarchical clustering.
Whilemuch is made of the ability of their approach toaccomodate different levels of granularity automat-ically, no evidence is provided that the hierarchyprovides real benefit.
At the same time, our workshows that significant gains can be realized with asingle, sufficiently granular partition of terms.
It isknown, moreover, that greedy agglomerative clus-tering leads to partitions that are sub-optimal interms of a mutual information objective function(see, for example, Brown, et al(1992)).
Ultimately,it is left to future research to determine how sensi-tive, if at all, the NER gains are to the details of theclustering.5 ConclusionThere are several ways in which this work might beextended and improved, both in its particular formand in general:  BWI models initial and terminal boundaries,but ignores characteristics of the extractedphrase other than its length.
We are explor-ing mechanisms for modeling relevant phrasalstructure.  While global statistical approaches, such as se-quential averaged perceptrons or CRFs (Mc-Callum and Li, 2003), appear better suited tothe NER problem than local symbolic learners,the two approaches search different hypothesisspaces.
Based on the surmise that, by combin-ing them, we can realize improvements over ei-ther in isolation, we are exploring mechanismsfor integration.  The distributional clusters we find are indepen-dent of the problem to which we want to applythem and may sometimes be inappropriate orhave the wrong granularity.
We are exploringways to produce groupings that are sensitive tothe task at hand.Our results clearly establish that an unsuperviseddistributional analysis of a text corpus can producefeatures that lead to enhanced precision and, espe-cially, recall in information extraction.
We havesuccessfully used these features in lieu of domain-specific, labor-intensive resources, such as syntac-tic analysis and special-purpose gazetteers.
Distri-butional analysis, combined with light supervision,is an effective, stable alternative to bootstrappingmethods.AcknowledgmentsThis material is based on work funded in whole orin part by the U.S. Government.
Any opinions, find-ings, conclusions, or recommendations expressed inthis material are those of the authors, and do notnecessarily reflect the views of the U.S. Govern-ment.ReferencesS.W.
Bennett and C. Aone.
1997.
Learning to tagmultilingual texts through observation.
In Proc.2nd Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-2), August.D.M.
Bikel, S. Miller, R. Schwartz, andR.
Weischedel.
1997.
Nymble: a high-performance learning name-finder.
In Proc.5th Conference on Applied Natural LanguageProcessing (ANLP-97), April.P.F.
Brown, V.J.
Della Pietra, P.V.
deSouza, J.C.Lai, and R.L.
Mercer.
1992.
Class-based n-grammodels of natural language.
Computational Lin-guistics, 18(4):467?479.X.
Carreras, L. Ma`rquez, and L. Padro?.
2002.Named entity extraction using AdaBoost.
In Pro-ceedings of CoNLL-2002, Taipei, Taiwan.A.
Clark.
2000.
Inducing syntactic categories bycontext distribution clustering.
In CoNLL 2000,September.M.
Collins and Y.
Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proc.1999 Joint SIGDAT Conference on EmpiricalMethods in NLP and Very Large Corpora.M.
Collins.
2002.
Discriminative training methodsfor hidden Markov models: Theory and exper-iments with perceptron algorithms.
In Proceed-ings of EMNLP-2002.S.
Cucerzan and D. Yarowsky.
1999.
Languageindependent named entity recognition combin-ing morphological and contextual evidence.
InProc.
1999 Joint SIGDAT Conference on Empir-ical Methods in NLP and Very Large Corpora,pages 90?99.I.
S. Dhillon, S. Mallela, and D. S. Modha.
2003.Information-theoretic co-clustering.
TechnicalReport TR-03-12, Dept.
of Computer Science, U.Texas at Austin.D.
Freitag and N. Kushmerick.
2000.
Boostedwrapper induction.
In Proc.
17th National Con-ference on Artificial Intelligence (AAAI-2000),August.A.
McCallum and W. Li.
2003.
Early results fornamed entity recognition with conditional ran-dom fields, feature induction and web-enhancedlexicons.
In Proc.
7th Conference on NaturalLanguage Learning (CoNLL-03).S.
Miller, J. Guinness, and A. Zamanian.
2004.Name tagging with word clusters and discrimi-native training.
In Proceedings of HLT/NAACL04.E.
F. Tjong Kim Sang.
2002.
Introductionto the CoNLL-2002 shared task: Language-independent named entity recognition.
In Pro-ceedings of CoNLL-2002.R.E.
Schapire and Y.
Singer.
1998.
Improvedboosting algorithms using confidence-rated pre-dictions.
In Proc.
11th Annual Conferenceon Computational Learning Theory (COLT-98),pages 80?91, July.H.
Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proc.
7th EACL Conference (EACL-95), March.M.
Thelen and E. Riloff.
2002.
A bootstrappingmethod for learning semantic lexicons using ex-traction pattern contexts.
In Proc.
2002 Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP 2002).D.
Wu, G. Ngai, M. Carpuat, J. Larsen, and Y. Yang.2002.
Boosting for named entity recognition.
InProceedings of CoNLL-2002, Taipei, Taiwan.
