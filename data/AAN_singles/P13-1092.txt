Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 933?943,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsGrounded Unsupervised Semantic ParsingHoifung PoonOne Microsoft WayMicrosoft ResearchRedmond, WA 98052, USAhoifung@microsoft.comAbstractWe present the first unsupervised ap-proach for semantic parsing that rivalsthe accuracy of supervised approachesin translating natural-language questionsto database queries.
Our GUSP systemproduces a semantic parse by annotat-ing the dependency-tree nodes and edgeswith latent states, and learns a proba-bilistic grammar using EM.
To compen-sate for the lack of example annotationsor question-answer pairs, GUSP adoptsa novel grounded-learning approach toleverage database for indirect supervision.On the challenging ATIS dataset, GUSPattained an accuracy of 84%, effectivelytying with the best published results by su-pervised approaches.1 IntroductionSemantic parsing maps text to a formal mean-ing representation such as logical forms or struc-tured queries.
Recently, there has been a bur-geoning interest in developing machine-learningapproaches for semantic parsing (Zettlemoyer andCollins, 2005; Zettlemoyer and Collins, 2007;Mooney, 2007; Kwiatkowski et al, 2011), butthe predominant paradigm uses supervised learn-ing, which requires example annotations that arecostly to obtain.
More recently, several grounded-learning approaches have been proposed to alle-viate the annotation burden (Chen and Mooney,2008; Kim and Mooney, 2010; Bo?rschinger et al,2011; Clarke et al, 2010; Liang et al, 2011).
Inparticular, Clarke et al (2010) and Liang et al(2011) proposed methods to learn from question-answer pairs alone, which represents a significantadvance.
However, although these methods exon-erate annotators from mastering specialized logi-cal forms, finding the answers for complex ques-tions still requires non-trivial effort.
1Poon & Domingos (2009, 2010) proposed theUSP system for unsupervised semantic parsing,which learns a parser by recursively clusteringand composing synonymous expressions.
Whiletheir approach completely obviates the need for di-rect supervision, their target logic forms are self-induced clusters, which do not align with existingdatabase or ontology.
As a result, USP can not beused directly to answer complex questions againstan existing database.
More importantly, it missesthe opportunity to leverage database for indirectsupervision.In this paper, we present the GUSP system,which combines unsupervised semantic parsingwith grounded learning from a database.
GUSPstarts with the dependency tree of a sentence andproduces a semantic parse by annotating the nodesand edges with latent semantic states derived fromthe database.
Given a set of natural-languagequestions and a database, GUSP learns a prob-abilistic semantic grammar using EM.
To com-pensate for the lack of direct supervision, GUSPconstrains the search space using the databaseschema, and bootstraps learning using lexicalscores computed from the names and values ofdatabase elements.Unlike previous grounded-learning approaches,GUSP does not require ambiguous annotationsor oracle answers, but rather focuses on lever-aging database contents that are readily avail-able.
Unlike USP, GUSP predetermines the tar-get logical forms based on the database schema,which alleviates the difficulty in learning and en-sures that the output semantic parses can be di-rectly used in querying the database.
To handlesyntax-semantics mismatch, GUSP introduces anovel dependency-based meaning representation1Clarke et al (2010) and Liang et al (2011) used theannotated logical forms to compute answers for their experi-ments.933by augmenting the state space to represent seman-tic relations beyond immediate dependency neigh-borhood.
This representation also factorizes overnodes and edges, enabling linear-time exact infer-ence in GUSP.We evaluated GUSP on end-to-end questionanswering using the ATIS dataset for semanticparsing (Zettlemoyer and Collins, 2007).
Com-pared to other standard datasets such as GEO andJOBS, ATIS features a database that is an orderof magnitude larger in the numbers of relationsand instances, as well as a more irregular lan-guage (ATIS questions were derived from spo-ken dialogs).
Despite these challenges, GUSPattains an accuracy of 84% in end-to-end ques-tion answering, effectively tying with the state-of-the-art supervised approaches (85% by Zettle-moyer & Collins (2007), 83% by Kwiatkowski etal.
(2011)).2 Background2.1 Semantic ParsingThe goal of semantic parsing is to map text toa complete and detailed meaning representation(Mooney, 2007).
This is in contrast with semanticrole labeling (Carreras and Marquez, 2004) and in-formation extraction (Banko et al, 2007; Poon andDomingos, 2007), which have a more restrictedgoal of identifying local semantic roles or extract-ing selected information slots.The standard language for meaning representa-tion is first-order logic or a sublanguage, such asFunQL (Kate et al, 2005; Clarke et al, 2010) andlambda calculus (Zettlemoyer and Collins, 2005;Zettlemoyer and Collins, 2007).
Poon & Domin-gos (2009, 2010) induce a meaning representa-tion by clustering synonymous lambda-calculusforms stemming from partitions of dependencytrees.
More recently, Liang et al (2011) proposedDCS for dependency-based compositional seman-tics, which represents a semantic parse as a treewith nodes representing database elements and op-erations, and edges representing relational joins.In this paper, we focus on semantic parsingfor natural-language interface to database (Groszet al, 1987).
In this problem setting, a natural-language question is first translated into a mean-ing representation by semantic parsing, and thenconverted into a structured query such as SQL toobtain answer from the database.2.2 Unsupervised Semantic ParsingUnsupervised semantic parsing was first proposedby Poon & Domingos (2009, 2010) with theirUSP system.
USP defines a probabilistic modelover the dependency tree and semantic parse us-ing Markov logic (Domingos and Lowd, 2009),and recursively clusters and composes synony-mous dependency treelets using a hard EM-likeprocedure.
Since USP uses nonlocal features (e.g.,the argument-number feature) and operates overpartitions, exact inference is intractable, and USPresorts to a greedy approach to find the MAP parseby searching over partitions.
Titov & Klementiev(2011) proposed a Bayesian version of USP andTitov & Klementiev (2012) adapted it for seman-tic role induction.
In USP, the meaning is repre-sented by self-induced clusters.
Therefore, to an-swer complex questions against a database, it re-quires an additional ontology matching step to re-solve USP clusters with database elements.Popescu et al (2003, 2004) proposed the PRE-CISE system, which does not require labeled ex-amples and can be directly applied to questionanswering with a database.
The PRECISE sys-tem, however, requires substantial amount of engi-neering, including a domain-specific lexicon thatspecifies the synonyms for names and values ofdatabase elements, a restricted set of potential in-terpretations for domain verbs and prepositions, aswell as a set of domain questions with manually la-beled POS tags for retraining the tagger and parser.It also focuses on the subset of easy questions (?se-mantically tractable?
questions), and sidesteps theproblem of dealing with complex and nested struc-tures, as well as ambiguous interpretations.
Re-markably, while PRECISE can be very accurateon easy questions, it does not try to learn fromthese interpretations.
In contrast, Goldwasser etal.
(2011) proposed a self-supervised approach,which iteratively chose high-confidence parses toretrain the parser.
Their system, however, stillrequired a lexicon manually constructed for thegiven domain.
Moreover, it was only applied toa small domain (a subset of GEO), and the resultstill trailed supervised systems by a wide margin.2.3 Grounded Learning for Semantic ParsingGrounded learning is motivated by alleviating theburden of direct supervision via interaction withthe world, where the indirect supervision maytake the form as ambiguous annotations (Chen934gettorontoflight from todiegoinsan stoppingdtwE:flight:RE:flightV:city.nameV:city.name:CE:flight_stopV:airport.codeV:city.name + E:flightFigure 1: End-to-end question answering byGUSP for sentence get flight from toronto to sandiego stopping in dtw.
Top: the dependency treeof the sentence is annotated with latent semanticstates by GUSP.
For brevity, we omit the edgestates.
Raising occurs from flight to get and sink-ing occurs from get to diego.
Bottom: the seman-tic tree is deterministically converted into SQL toobtain answer from the database.and Mooney, 2008; Kim and Mooney, 2010;Bo?rschinger et al, 2011) or example question-answer pairs (Clarke et al, 2010; Liang et al,2011).
In general, however, such supervision isnot always available or easy to obtain.
In con-trast, databases are often abundantly available, es-pecially for important domains.The database community has considerableamount of work on leveraging databases in varioustasks such as entity resolution, schema matching,and others.
To the best of our knowledge, this ap-proach is still underexplored in the NLP commu-nity.
One notable exception is distant supervision(Mintz et al, 2009; Riedel et al, 2010; Hoffmannet al, 2011; Krishnamurthy and Mitchell, 2012;Heck et al, 2013), which used database instancesto derive training examples for relation extraction.This approach, however, still has considerable lim-itations.
For example, it only handles binary rela-tions, and the quality of the training examples isinherently noisy and hard to control.
Moreover,this approach is not applicable to the question-answering setting considered in this paper, sinceentity pairs in questions need not correspond tovalid relational instances in the database.3 Grounded Unsupervised SemanticParsingIn this section, we present the GUSP system forgrounded unsupervised semantic parsing.
GUSPis unsupervised and does not require example log-ical forms or question-answer pairs.
Figure 1shows an example of end-to-end question answer-ing using GUSP.
GUSP produces a semantic parseof the question by annotating its dependency treewith latent semantic states.
The semantic treecan then be deterministically converted into SQLto obtain answer from the database.
Given aset of natural-language questions and a database,GUSP learns a probabilistic semantic grammar us-ing EM.To compensate for the lack of annotated ex-amples, GUSP derives indirect supervision froma novel combination of three key sources.
First,GUSP leverages the target database to constrainthe search space.
Specifically, it defines the se-mantic states based on the database schema, andderives lexical-trigger scores from database ele-ments to bootstrap learning.Second, in contrast to most existing approachesfor semantic parsing, GUSP starts directly fromdependency trees and focuses on translating theminto semantic parses.
While syntax may not al-ways align perfectly with semantics, it is stillhighly informative about the latter.
In particular,dependency edges are often indicative of semanticrelations.
On the other hand, syntax and semanticoften diverge, and synactic parsing errors abound.To combat this problem, GUSP introduces a noveldependency-based meaning representation with anaugmented state space to account for semantic re-lations that are nonlocal in the dependency tree.GUSP?s approach of starting directly from de-pendency tree is inspired by USP.
However, GUSPuses a different meaning representation definedover individual nodes and edges, rather than par-titions, which enables linear-time exact inference.GUSP also handles complex linguistic phenomenaand syntax-semantics mismatch by explicitly aug-menting the state space, whereas USP?s capabilityin handling such phenomena is indirect and morelimited.GUSP represents meaning by a semantic tree,which is similar to DCS (Liang et al, 2011).
Theirapproach to semantic parsing, however, differsfrom GUSP in that it induced the semantic tree di-rectly from a sentence, rather than starting from935a dependency tree and annotating it.
Their ap-proach alleviates some complexity in the mean-ing representation for handling syntax-semanticsmismatch, but it has to search over a much largersearch space involving exponentially many candi-date trees.
This might partially explain why it hasnot yet been scaled up to the ATIS dataset.Finally, GUSP recognizes that certain aspectsin semantic parsing may not be worth learn-ing using precious annotated examples.
Theseare domain-independent and closed-class expres-sions, such as times and dates (e.g., before 5pmand July seventeenth), logical connectives (e.g.,and, or, not), and numerics (e.g., 200 dol-lars).
GUSP preprocesses the text to detect suchexpressions and restricts their interpretation todatabase elements of compatible types (e.g., be-fore 5pm vs. flight.departure time orflight.arrival time).
Short of training ex-amples, GUSP also resolves quantifier scopingambiguities deterministically by a fixed ordering.For example, in the phrase cheapest flight to Seat-tle, the scope of cheapest can be either flight orflight to seattle.
GUSP always chooses to applythe superlative at last, amounting to choosing themost restricted scope (flight to seattle), which isusually the correct interpretation.In the remainder of this section, we first formal-ize the problem setting and introduce the GUSPmeaning representation.
We then present theGUSP model and learning and inference algo-rithms.
Finally, we describe how to convert aGUSP semantic parse into SQL.3.1 Problem FormulationLet d be a dependency tree, N(d) and E(d) beits nodes and edges.
In GUSP, a semantic parseof d is an assignment z : N(d) ?
E(d) ?
Sthat maps its nodes and edges to semantic statesin S. For example, in the example in Figure 1,z(flight) = E : flight.
At the core of GUSPis a joint probability distribution P?
(d, z) over thedependency tree and the semantic parse.
Seman-tic parsing in GUSP amounts to finding the mostprobable parse z?
= argmaxz P?
(d, z).
Givena set of sentences and their dependency trees D,learning in GUSP maximizes the log-likelihood ofD while summing out the latent parses z:??
= argmax logP?
(D)= argmax?d?Dlog?zP?
(d, z)3.2 Simple Semantic StatesNode states GUSP creates a state E:X (E shortfor entity) for each database entity X (i.e., adatabase table), a state P:Y (P short for prop-erty) and V:Y (V short for value) for each databaseattribute Y (i.e., a database column).
Nodestates are assigned to dependency nodes.
Intu-itively, they represent database entities, proper-ties, and values.
For example, the ATIS do-main contains entities such as flight and fare,which may contain properties such as the depar-ture time flight.departure time or ticketprice fare.one direction cost.
The men-tions of entities and properties are representedby entity and property states, whereas constantssuch as 9:25am or 120 dollars are repre-sented by value states.
In the semantic parse inFigure 1, for example, flight is assigned to en-tity state E:flight, where toronto is assignedto value state V:city.name.
There is a specialnode state NULL, which signifies that the subtreeheaded by the word contributes no meaning to thesemantic parse (e.g., an auxilliary verb).Edge states GUSP creates an edge state foreach valid relational join paths connecting twonode states.
Edge states are assigned to de-pendency edges.
GUSP enforces the constraintsthat the node states of the dependency par-ent and child must agree with the node statesin the edge state.
For example, E:flight--V:flight.departure time represents anatural join between the flight entity and the prop-erty value departure time.
For a dependency edgee : a ?
b, the assignment to E:flight--V:flight.departure time signifies thata represents a flight entity, and b represents thevalue of its departure time.
An edge state mayalso represent a relational path consisting of aserial of joins.
For example, Zettlemoyer andCollins (2007) used a predicate from(f,c) tosignify that flight f starts from city c. In the ATISdatabase, however, this amounts to a path of threejoins:flight.from airport-airportairport-airport serviceairport service-cityIn GUSP, this is represented by the edgestate flight-flight.from airport--airport-airport service-city.936GUSP only creates edge states for relational joinpaths up to length four, as longer paths rarelycorrespond to meaningful semantic relations.Composition To handle compositions such asAmerican Airlines and New York City, it helpsto distinguish the head words (Airlines and City)from the rest.
In GUSP, this is handled by intro-ducing, for each node state such as E:airline,a new node state such as E:airline:C, whereC signifies composition.
For example, in Figure1, diego is assigned to V:city.name, whereassan is assigned to V:city.name:C, since sandiego forms a single meaning unit, and should betranslated into SQL as a whole.3.3 Domain-Independent StatesThese are for handling special linguistic phenom-ena that are not domain-specific, such as negation,superlatives, and quantifiers.Operator states GUSP create node states forthe logical and comparison operators (OR, AND,NOT, MORE, LESS, EQ).
Additionally, to han-dle the cases when prepositions and logicalconnectives are collapsed into the label of adependency edge, as in Stanford dependency,GUSP introduces an edge state for each tripleof an operator and two node states, such asE:flight-AND-E:fare.Quantifier states GUSP creates a node state foreach of the standard SQL functions: argmin,argmax, count, sum.
Additionally, it cre-ates a node state for each pair of compatible func-tion and property.
For example, argmin canbe applied to any numeric property, in particularflight.departure time, and so the nodestate P:flight.departure time:argminis created and can be assigned to superlatives suchas earliest.3.4 Complex Semantic StatesFor sentences with a correct dependency tree andwell-aligned syntax and semantics, the simple se-mantic states suffice for annotating the correct se-mantic parse.
However, in complex sentences,syntax and semantic often diverge, either due totheir differing goals or simply stemming from syn-tactic parsing errors.
In Figure 1, the dependencytree contains multiple errors: from toronto and tosan diego are mistakenly attached to get, whichhas no literal meaning here; stopping in dtw is alsowrongly attached to diego rather than flight.
An-notating such a tree with only simple states willlead to incorrect semantic parses, e.g., by joiningV:city:san diego with V:airport:dtwvia E:airport service, rather than join-ing E:flight with V:airport:dtw viaE:flight stop.To overcome these challenges, GUSP intro-duces three types of complex states to handlesyntax-semantics divergence.
Figure 1 shows thecorrect semantic parse for the above sentence us-ing the complex states.Raising For each simple node state N, GUSPcreates a ?raised?
state N:R (R short for raised).
Araised state signifies a word that has little or noneof its own meaning, but effectively takes one of itschild states to be its own (?raises?).
Correspond-ingly, GUSP creates a ?raising?
edge state N-R-N,which signifies that the parent is a raised state andits meaning is derived from the dependency childof state N. For all other children, the parent be-haves just as state N. For example, in Figure 1, getis assigned to the raised state E:flight:R, andthe edge between get and flight is assigned to theraising edge state E:flight-R-E:flight.Sinking For simple node states A, B and anedge state E connecting the two, GUSP createsa ?sinking?
node state A+E+B:S (S for sinking).When a node n is assigned to such a sinking state,n can behave as either A or B for its children(i.e., the edge states can connect to either one),and n?s parent must be of state B.
In Figure 1,for example, diego is assigned to a sinking stateV:city.name + E:flight (the edge state isomitted for brevity).
E:flight comes from itsparent get.
For child san, diego behaves as in stateV:city.name, and their edge state is a simplecompositional join.
For the other child stopping,diego behaves as in state E:flight, and theiredge state is a relational join connecting flightwith flight stop.
Effectively, this connectsstopping with get and eventually with flight (due toraising), virtually correcting the syntax-semanticsmismatch stemming from attachment errors.Implicit For simple node states A, B and anedge state E connecting the two, GUSP also cre-ates a node state A+E+B:I (I for implicit) withthe ?implicit?
state B.
In natural languages, an en-tity is often introduced implicitly, which the readerinfers from shared world knowledge.
For example,937to obtain the correct semantic parse for Give methe fare from Seattle to Boston, one needs to inferthe existence of a flight entity, as in Give me thefare (of a flight) from Seattle to Boston.
Implicitstates offer candidates for addressing such needs.As in sinking, child nodes have access to either ofthe two simple states, but the implicit state is notvisible to the parent node.3.5 Lexical-Trigger ScoresGUSP uses the database elements to automaticallyderive a simple scoring scheme for lexical triggers.If a database element has a name of k words, eachword is assigned score 1/k for the correspondingnode state.
Similarly for property values and valuenode states.
In a sentence, if a word w triggers anode state with score s, its dependency childrenand left and right neighbors all get a trigger scoreof 0.1?s for the same state.
To score relevant wordsnot appearing in the database (due to incomplete-ness of the database or lexical variations), GUSPuses DASH (Pantel et al, 2009) to provide addi-tional word-pair scoring based on lexical distribu-tional similarity computed over general text cor-pora (Wikipedia in this case).
In the case of multi-ple score assignments for the same word, the max-imum score is used.For multi-word values of property Y , and fora dependency edge connecting two collocatedwords, GUSP assigns a score 1.0 to the edge statejoining the value node state V:Y to its composi-tion state V:Y:C, as well as the edge state joiningtwo composition states V:Y:C.GUSP also uses a domain-independent list ofsuperlatives with the corresponding data types andpolarity (e.g., first, last, earliest, latest, cheapest)and assigns a trigger score of 1.0 for each prop-erty of a compatible data type (e.g., cheapest forproperties of type MONEY).3.6 The GUSP ModelIn a nutshell, the GUSP model resembles a tree-HMM, which models the emission of words anddependencies by node and edge states, as well astransition between an edge state and the parentand child node states.
In preliminary experimentson the development set, we found that the na?
?vemodel (with multinomials as conditional probabil-ities) did not perform well in EM.
We thus choseto apply feature-rich EM (Berg-Kirkpatrick et al,2010) in GUSP, which enabled the use of moregeneralizable features.
Specifically, GUSP definesa probability distribution over dependency tree dand semantic parse z byP?
(d, z) =1Z exp?ifi(d, z) ?
wi(d, z)where fi andwi are features and their weights, andZ is the normalization constant that sums over allpossible d, z (over the same unlabeled tree).
Thefeatures of GUSP are as follows:Lexical-trigger scores These are implementedas emission features with fixed weights.
For ex-ample, given a token t that triggers node stateN with score s, there is a corresponding features1(lemma = t, state = N) with weight ?
?s, where?
is a parameter.Emission features for node states GUSP usestwo templates for emission of node states: forraised states, 1(token = ?
), i.e., the emissionweights for all raised states are tied; for non-raisedstates, 1(lemma = ?, state = N).Emission features for edge states GUSP usesthe following templates for emission of edgestates:Child node state is NULL, dependency= ?
;Edge state is RAISING, dependency= ?
;Parent node state is same as the child node state,dependency= ?
;Otherwise, parent node state= ?, child nodestate= ?, edge state type= ?, dependency= ?.Transition features GUSP uses the followingtemplates for transition features, which are similarto the edge emission features except for the depen-dency label:Child node state is NULL;Edge state is RAISING;Parent node state is same as the child node state;Otherwise, parent node state= ?, child nodestate= ?, edge state type= ?.Complexity Prior To favor simple semanticparses, GUSP imposes an exponential prior withweight ?
on nodes states that are not null or raised,and on each relational join in an edge state.3.7 Learning and InferenceSince the GUSP model factors over nodes andedges, learning and inference can be done ef-ficiently using EM and dynamic programming.Specifically, the MAP parse and expectations can938be computed by tree-Viterbi and inside-outside(Petrov and Klein, 2008).
The parameters can beestimated by feature-rich EM (Berg-Kirkpatrick etal., 2010).Because the Viterbi and inside-outside are ap-plied to a fixed tree (i.e., the input dependencytree), their running times are only linear in the sen-tence length in GUSP.3.8 Query GenerationGiven a semantic parse, GUSP generates the SQLby a depth-first traversal that recursively computesthe denotation of a node from the denotations of itschildren and its node state and edge states.
Eachdenotation is a structured query that contains: alist of entities for projection (corresponding tothe FROM statement in SQL); a computation treewhere the leaves are simple joins or value compar-isons, and the internal nodes are logical or quan-tifier operators (the WHERE statement); the salientdatabase elements (the SELECT statement).
Be-low, we illustrate this procedure using the seman-tic parse in Figure 1 as a running example.Value node state GUSP creates a semantic ob-ject of the given type with a unique index andthe word constant.
For example, the denotationfor node toronto is a city.name object with aunique index and constant ?toronto?.
The uniqueindex is necessary in case the SQL involves mul-tiple instances of the same entity.
For example,the SQL in Figure 1 involves two instances of theentity city, corresponding to the departure andarrival cities, respectively.
By default, such a se-mantic object will be translated into an equalityconstraint, such as city.name = toronto.Entity or property node state GUSP creates asemantic object of the given type with a unique re-lation index.
For example, the denotation for nodeflight is simply a flight object with a unique in-dex.
By default, such an object will contribute tothe list of entities in SQL projection (the FROMstatement), but not any constraints.NULL state GUSP returns an empty denotation.Simple edge state GUSP appends the child de-notation to that of the parent, and appends equal-ity constraints corresponding to the relational joinpath.
In the case of composition, such as the joinbetween diego and san, GUSP simply keeps theparent object, while adding to it the words fromthe child.
In the case of a more complex join,such as that between stopping and dtw, GUSP addsthe relational constraints that join flight stopwith airport:flight stop.stop airport = airport.airport id.Raising edge state GUSP simply takes the childdenotation and sets that to the parent.Implicit and sinking states GUSP maintainstwo separate denotations for the two simple statesin the complex state, and processes their respec-tive edge states accordingly.
For example, thenode diego contains two denotations, one forV:city.name, and one for E:flight, withthe corresponding child being san and stopping,respectively.Domain-independent states For comparatorstates such as MORE or LESS, GUSP changes thedefault equality constraints to an inequality one,such as flight.depart time < 600 for before6am.
For logical connectives, GUSP combines theprojection and constraints accordingly.
For quan-tifier states, GUSP applies the given function tothe query.Resolve scoping ambiguities GUSP delays ap-plying quantifiers until the child semantic objectdiffers from the parent one or when reaching theroot.
GUSP employs the following fixed orderingin evaluating quantifiers and operators: superla-tives and other quantifiers are evaluated at last(i.e., after evaluating all other joins or operatorsfor the given object), whereas negation is evalu-ated first, conjunctions and disjunctions are evalu-ated in their order of appearance.4 Experiments4.1 TaskWe evaluated GUSP on the ATIS travel planningdomain, which has been studied in He & Young(2005, 2006) and adapted for evaluating semanticparsing by Zettlemoyer & Collins (2007) (hence-forth ZC07).
The ZC07 dataset contains annotatedlogical forms for each sentence, which we do notuse.
Since our goal is not to produce a specific log-ical form, we directly evaluate on the end-to-endtask of translating questions into database queriesand measure question-answering accuracy.
TheATIS distrbution contains the original SQL anno-tations, which we used to compute gold answers939for evaluation only.
The dataset is split into train-ing, development, and test, containing 4500, 478,and 449 sentences, respectively.
We used the de-velopment set for initial development and tuninghyperparameters.
At test time, we ran GUSP overthe test set to learn a semantic parser and outputthe MAP parses.24.2 PreprocessingThe ATIS sentences were originally derived fromspoken dialog and were therefore in lower cases.Since case information is important for parsersand taggers, we first truecased the sentences us-ing DASH (Pantel et al, 2009), which stores thecase for each phrase in Wikipedia.We then ran the sentences through SPLAT, astate-of-the-art NLP toolkit (Quirk et al, 2012), toconduct tokenization, part-of-speech tagging, andconstituency parsing.
Since SPLAT does not out-put dependency trees, we ran the Stanford parserover SPLAT parses to generate the dependencytrees in Stanford dependency (de Marneffe et al,2006).4.3 SystemsFor the GUSP system, we set the hyperparame-ters from initial experiments on the developmentset, and used them in all subsequent experiments.Specifically, we set ?
= 50 and ?
= ?0.1, andran three iterations of feature-rich EM with an L2prior of 10 over the feature weights.To evaluate the importance of complex states,we considered two versions of GUSP : GUSP-SIMPLE and GUSP-FULL, where GUSP-SIMPLE only admits simple states, whereasGUSP-FULL admits all states.During development, we found that somequestions are inherently ambiguous that can-not be solved except with some domainknowledge or labeled examples.
In Sec-tion 3.2, we discuss an edge state that joinsa flight with its starting city: flight--flight.from airport-airport--airport service-city.
The ATISdatabase also contains another path of the samelength: flight-flight.from airport--airport-ground service-city.
Theonly difference is that air service is replacedby ground service.
In some occasions, the2This doesn?t lead to overfitting since we did not use anylabeled information in the test set.Table 1: Comparison of semantic parsing accu-racy on the ATIS test dataset.
Both ZC07 andFUBL used annotated logical forms in training,whereas GUSP-FULL and GUSP++ did not.
Thenumbers for GUSP-FULL and GUSP++ are end-to-end question answering accuracy, whereas thenumbers for ZC07 and FUBL are recall on exactmatch in logical forms.AccuracyZC07 84.6FUBL 82.8GUSP-FULL 74.8GUSP++ 83.5answers are identical whereas in others they aredifferent.
Without other information, neither thecomplexity prior nor EM can properly discrimi-nate one against another.
(Note that this ambiguityis not present in the ZC07 logical forms, whichuse a single predicate from(f,c) for the entirerelation paths.
In other words, to translate ZC07logical forms into SQL, one also needs to decideon which path to use.
)Another type of domain-specific ambigui-ties involves sentences such as give me in-formation on flights after 4pm on wednesday.There is no obvious information to disam-biguate between flight.departure timeand flight.arrival time for 4pm.Such ambiguities suggest opportunities for in-teractive learning,3 but this is clearly out ofthe scope of this paper.
Instead, we incor-porated a simple disambiguation feature with asmall weight of 0.01 that fires over the sim-ple states of flight.departure time andairport service.
We named the resultingsystem GUSP++.To gauge the difficulty of the task and the qual-ity of lexical-trigger scores, we also considereda deterministic baseline LEXICAL, which com-puted semantic parses using lexical-trigger scoresalone.3For example, after eliminating other much less likelyalternatives, the system can present to the user with bothchoices and let the user to choose the correct one.
The im-plicit feedback signal can then be used to train the system forfuture disambiguation.940Table 2: Comparison of question answering accu-racy in ablation experiments.AccuracyLEXICAL 33.9GUSP-SIMPLE 66.5GUSP-FULL 74.8GUSP++ 83.5?
RAISING 75.7?
SINKING 77.5?
IMPLICIT 76.24.4 ResultsWe first compared the results of GUSP-FULL andGUSP++ with ZC07 and FUBL (Kwiatkowski etal., 2011).4 Note that ZC07 and FUBL were eval-uated on exact match in logical forms.
We usedtheir recall numbers which are the percentages ofsentences with fully correct logical forms.
Giventhat the questions are quite specific and generallyadmit nonzero number of answers, the question-answer accuracy should be quite comparable withthese numbers.Table 1 shows the comparison.
Surprisingly,even without the additional disambiguation fea-ture, GUSP-FULL already attained an accuracybroadly in range with supervised results.
With thefeature, GUSP++ effectively tied with the bestsupervised approach.To evaluate the importance of various compo-nents in GUSP, we conducted ablation test to com-pare the variants of GUSP.
Table 2 shows the re-sults.
LEXICAL can parse more than one thirdof the sentences correctly, which is quite remark-able in itself, considering that it only used the lex-ical scores.
On the other hand, roughly two-thirdof the sentences cannot be correctly parsed in thisway, suggesting that the lexical scores are noisyand ambiguous.
In comparison, all GUSP variantsachieved significant gains over LEXICAL.
Addi-tionally, GUSP-FULL substantially outperformedGUSP-SIMPLE, highlighting the challenges ofsyntax-semantics mismatch in ATIS, and demon-strating the importance and effectiveness of com-plex states for handling such mismatch.
All threetypes of complex states produced significant con-tributions.
For example, compared to GUSP++,4We should note that while the more recent system ofFUBL slightly trails ZC07, it is language-independent andcan parse questions in multiple languages.removing RAISING dropped accuracy by almost8 points.4.5 DiscussionUpon manual inspection, many of the remainingerrors are due to syntactic parsing errors that aretoo severe to fix.
This is partly due to the fact thatATIS sentences are out of domain compared tothe newswired text on which the syntactic parserswere trained.
For example, show, list were regu-larly parsed as nouns, whereas round (as in roundtrip) were often parsed as a verb and northwestwere parsed as an auxilliary verb.
Another reasonis that ATIS sentences are typically less formal orgrammatical, which exacerbates the difficulty inparsing.
In this paper, we used the 1-best depen-dency tree to produce semantic parse.
An interest-ing future direction is to consider joint syntactic-semantic parsing, using k-best trees or even theparse forest as input and reranking the top parseusing semantic information.55 ConclusionThis paper introduces grounded unsupervisedsemantic parsing, which leverages availabledatabase for indirect supervision and uses agrounded meaning representation to account forsyntax-semantics mismatch in dependency-basedsemantic parsing.
The resulting GUSP system isthe first unsupervised approach to attain an accu-racy comparable to the best supervised systems intranslating complex natural-language questions todatabase queries.Directions for future work include: jointsyntactic-semantic parsing, developing better fea-tures for learning; interactive learning in a dialogsetting; generalizing distant supervision; applica-tion to knowledge extraction from database-richdomains such as biomedical sciences.AcknowledgmentsWe would like to thank Kristina Toutanova, ChrisQuirk, Luke Zettlemoyer, and Yoav Artzi for use-ful discussions, and Patrick Pantel and MichaelGammon for help with the datasets.5Note that this is still different from the currently predom-inant approaches in semantic parsing, which learn to parseboth syntax and semantics by training from the semanticparsing datasets alone, which are considerably smaller com-pared to resources available for syntactic parsing.941ReferencesMichele Banko, Michael J. Cafarella, Stephen Soder-land, Matt Broadhead, and Oren Etzioni.
2007.Open information extraction from the web.
In Pro-ceedings of the Twentieth International Joint Con-ference on Artificial Intelligence, pages 2670?2676,Hyderabad, India.
AAAI Press.Taylor Berg-Kirkpatrick, John DeNero, and Dan Klein.2010.
Painless unsupervised learning with features.In Proceedings of Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics.Benjamin Bo?rschinger, Bevan K. Jones, and MarkJohnson.
2011.
Reducing grounded learning tasksto grammatical inference.
In Proceedings of the2011 Conference on Empirical Methods in NaturalLanguage Processing.Xavier Carreras and Luis Marquez.
2004.
Introductionto the CoNLL-2004 shared task: Semantic role la-beling.
In Proceedings of the Eighth Conference onComputational Natural Language Learning, pages89?97, Boston, MA.
ACL.David L. Chen and Raymond J. Mooney.
2008.
Learn-ing to sportscast: A test of grounded language ac-quisition.
In ICML-08.James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing fromworld?s response.
In Proceedings of the 2010 Con-ference on Natural Language Learning.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the Fifth International Conferenceon Language Resources and Evaluation, pages 449?454, Genoa, Italy.
ELRA.Pedro Domingos and Daniel Lowd.
2009.
MarkovLogic: An Interface Layer for Artificial Intelligence.Morgan & Claypool, San Rafael, CA.Dan Goldwasser, Roi Reichart, James Clarke, and DanRoth.
2011.
Confidence driven unsupervised se-mantic parsing.
In Proceedings of the Forty NinthAnnual Meeting of the Association for Computa-tional Linguistics.B.J.
Grosz, D. Appelt, P. Martin, and F. Pereira.
1987.Team: An experiment in the design of transportablenatural language interfaces.
Artificial Intelligence,32:173?243.Yulan He and Steve Young.
2005.
Semantic process-ing using the hidden vector state model.
In Com-puter Speech and Language.Yulan He and Steve Young.
2006.
Spoken lan-guage understanding using the hidden vector statemodel.
In Speech Communication Special Issue onSpoken Language understanding for ConversationalSystems.Larry Heck, Dilek Hakkani-Tur, and Gokhan Tur.2013.
Leveraging knowledge graphs for web-scaleunsupervised semantic parsing.
In Proceedings ofthe Interspeech 2013.Raphael Hoffmann, Congle Zhang, Xiao Ling,Luke Zettlemoyer, and Daniel S. Weld.
2011.Knowledge-based weak supervision for informationextraction of overlapping relations.
In Proceedingsof the Forty Ninth Annual Meeting of the Associationfor Computational Linguistics.R.
J. Kate, Y. W. Wong, and R. J. Mooney.
2005.Learning to transform natural to formal languages.In Proceedings of the Twentieth National Confer-ence on Artificial Intelligence.Joohyun Kim and Raymond J. Mooney.
2010.
Gen-erative alignment and semantic parsing for learningfrom ambiguous supervision.
In COLING10.Jayant Krishnamurthy and Tom M. Mitchell.
2012.Weakly supervised training of semantic parsers.
InEMNLP-12.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2011.
Lexical generaliza-tion in ccg grammar induction for semantic parsing.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing.Percy Liang, Michael Jordan, and Dan Klein.
2011.Learning dependency-based compositional seman-tics.
In Proceedings of the Forty Ninth Annual Meet-ing of the Association for Computational Linguis-tics.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theForty Seventh Annual Meeting of the Association forComputational Linguistics.Raymond J. Mooney.
2007.
Learning for semanticparsing.
In Proceedings of the Eighth InternationalConference on Computational Linguistics and Intel-ligent Text Processing, pages 311?324, Mexico City,Mexico.
Springer.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing.Slav Petrov and Dan Klein.
2008.
Discriminative log-linear grammars with latent variables.
In NIPS-08.Hoifung Poon and Pedro Domingos.
2007.
Joint in-ference in information extraction.
In Proceedings ofthe Twenty Second National Conference on ArtificialIntelligence, pages 913?918, Vancouver, Canada.AAAI Press.942Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1?10, Singapore.
ACL.Hoifung Poon and Pedro Domingos.
2010.
Unsuper-vised ontological induction from text.
In Proceed-ings of the Forty Eighth Annual Meeting of the As-sociation for Computational Linguistics, pages 296?305, Uppsala, Sweden.
ACL.Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.2003.
Towards a theory of natural language inter-faces to databases.
In IUI-03.Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,David Ko, and Alexander Yates.
2004.
Modernnatural language interfaces to databases: Compos-ing statistical parsing with semantic tractability.
InCOLING-04.Chris Quirk, Pallavi Choudhury, Jianfeng Gao, HisamiSuzuki, Kristina Toutanova, Michael Gamon, Wen-tau Yih, and Lucy Vanderwende.
2012.
MSRSPLAT, a language analysis toolkit.
In Proceedingsof NAACL HLT 2012 Demonstration Session.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions with-out labeled text.
In Proceedings of the Sixteen Euro-pean Conference on Machine Learning.Ivan Titov and Alexandre Klementiev.
2011.
Abayesian model for unsupervised semantic parsing.In Proceedings of the Forty Ninth Annual Meeting ofthe Association for Computational Linguistics.Ivan Titov and Alexandre Klementiev.
2012.
Abayesian approach to unsupervised semantic role in-duction.
In Proceedings of the Conference of theEuropean Chapter of the Association for Computa-tional Linguistics.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammers.
In Proceedings of the Twenty FirstConference on Uncertainty in Artificial Intelligence,pages 658?666, Edinburgh, Scotland.
AUAI Press.Luke S. Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed ccg grammars for parsingto logical form.
In Proceedings of the Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning.943
