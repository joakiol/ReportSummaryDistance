Edit Detection and Parsing for Transcribed SpeechEugene Charniak and Mark JohnsonDeparments of Computer Science and Cognitive and Linguistic SciencesBrown Laboratory for Linguistic Information Processing (BLLIP)Brown University, Providence, RI 02912ec,mj@cs.brown.edu AbstractWe present a simple architecture for parsingtranscribed speech in which an edited-word de-tector rst removes such words from the sen-tence string, and then a standard statisticalparser trained on transcribed speech parses theremaining words.
The edit detector achieves amisclassication rate on edited words of 2.2%.
(The NULL-model, which marks everything asnot edited, has an error rate of 5.9%.)
To evalu-ate our parsing results we introduce a new eval-uation metric, the purpose of which is to makeevaluation of a parse tree relatively indierentto the exact tree position of EDITED nodes.
Bythis metric the parser achieves 85.3% precisionand 86.5% recall.1 IntroductionWhile signicant eort has been expended onthe parsing of written text, parsing speechhas received relatively little attention.
Thecomparative neglect of speech (or transcribedspeech) is understandable, since parsing tran-scribed speech presents several problems absentin regular text: \um"s and \ah"s (or moreformally, lled pauses), frequent use of par-entheticals (e.g., \you know"), ungrammaticalconstructions, and speech repairs (e.g., \Whydidn?t he, why didn?t she stay home?
").In this paper we present and evaluate a simpletwo-pass architecture for handling the problemsof parsing transcribed speech.
The rst passtries to identify which of the words in the stringare edited (\why didn?t he," in the above exam-ple).
These words are removed from the stringgiven to the second pass, an already existing sta-tistical parser trained on a transcribed speech?
This research was supported in part by NSF grant LISSBR 9720368 and by NSF ITR grant 20100203.corpus.
(In particular, all of the research in thispaper was performed on the parsed \Switch-board" corpus as provided by the LinguisticData Consortium.
)This architecture is based upon a fundamen-tal assumption: that the semantic and prag-matic content of an utterance is based solelyon the unedited words in the word sequence.This assumption is not completely true.
Forexample, Core and Schubert [8] point to coun-terexamples such as \have the engine take theoranges to Elmira, um, I mean, take them toCorning" where the antecedent of \them" isfound in the EDITED words.
However, we be-lieve that the assumption is so close to true thatthe number of errors introduced by this assump-tion is small compared to the total number oferrors made by the system.In order to evaluate the parser?s output wecompare it with the gold-standard parse trees.For this purpose a very simple third pass isadded to the architecture: the hypothesizededited words are inserted into the parser output(see Section 3 for details).
To the degree thatour fundamental assumption holds, a \real" ap-plication would ignore this last step.This architecture has several things to recom-mend it.
First, it allows us to treat the editingproblem as a pre-process, keeping the parser un-changed.
Second, the major clues in detectingedited words in transcribed speech seem to berelatively shallow phenomena, such as repeatedword and part-of-speech sequences.
The kindof information that a parser would add, e.g.,the node dominating the EDITED node, seemsmuch less critical.Note that of the major problems associatedwith transcribed speech, we choose to deal withonly one of them, speech repairs, in a specialfashion.
Our reasoning here is based upon whatone might and might not expect from a second-pass statistical parser.
For example, ungram-maticality in some sense is relative, so if thetraining corpus contains the same kind of un-grammatical examples as the testing corpus,one would not expect ungrammaticality itselfto be a show stopper.
Furthermore, the beststatistical parsers [3,5] do not use grammaticalrules, but rather dene probability distributionsover all possible rules.Similarly, parentheticals and lled pauses ex-ist in the newspaper text these parsers currentlyhandle, albeit at a much lower rate.
Thus thereis no particular reason to expect these construc-tions to have a major impact.1 This leavesspeech repairs as the one major phenomenonnot present in written text that might pose amajor problem for our parser.
It is for that rea-son that we have chosen to handle it separately.The organization of this paper follows the ar-chitecture just described.
Section 2 describesthe rst pass.
We present therein a boostingmodel for learning to detect edited nodes (Sec-tions 2.1 { 2.2) and an evaluation of the modelas a stand-alone edit detector (Section 2.3).Section 3 describes the parser.
Since the parseris that already reported in [3], this section sim-ply describes the parsing metrics used (Section3.1), the details of the experimental setup (Sec-tion 3.2), and the results (Section 3.3).2 Identifying EDITED wordsThe Switchboard corpus annotates disfluenciessuch as restarts and repairs using the terminol-ogy of Shriberg [15].
The disfluencies includerepetitions and substitutions, italicized in (1a)and (1b) respectively.
(1) a. I really, I really like pizza.b.
Why didn?t he, why didn?t she stayhome?Restarts and repairs are indicated by disfluencytags ?
[?, ?+?
and ?]?
in the disfluency POS-taggedSwitchboard corpus, and by EDITED nodes inthe tree-tagged corpus.
This section describesa procedure for automatically identifying wordscorrected by a restart or repair, i.e., words that1Indeed, [17] suggests that filled pauses tend to indi-cate clause boundaries, and thus may be a help in pars-ing.are dominated by an EDITED node in the tree-tagged corpus.This method treats the problem of identify-ing EDITED nodes as a word-token classicationproblem, where each word token is classied aseither edited or not.
The classier applies towords only; punctuation inherits the classica-tion of the preceding word.
A linear classiertrained by a greedy boosting algorithm [16] isused to predict whether a word token is edited.Our boosting classier is directly based on thegreedy boosting algorithm described by Collins[7].
This paper contains important implemen-tation details that are not repeated here.
Wechose Collins?
algorithm because it oers goodperformance and scales to hundreds of thou-sands of possible feature combinations.2.1 Boosting estimates of linearclassifiersThis section describes the kinds of linear clas-siers that the boosting algorithm infers.
Ab-stractly, we regard each word token as an eventcharacterized by a nite tuple of random vari-ables(Y;X1; : : : ;Xm):Y is the the conditioned variable and rangesover f?1;+1g, with Y = +1 indicating thatthe word is not edited.
X1; : : : ;Xm are the con-ditioning variables; each Xj ranges over a niteset Xj .
For example, X1 is the orthographicform of the word and X1 is the set of all wordsobserved in the training section of the corpus.Our classiers use m = 18 conditioning vari-ables.
The following subsection describes theconditioning variables in more detail; they in-clude variables indicating the POS tag of thepreceding word, the tag of the following word,whether or not the word token appears in a\rough copy" as explained below, etc.The goal of the classier is to predict thevalue of Y given values for X1; : : : ;Xm.
Theclassier makes its predictions based on the oc-curence of combinations of conditioning vari-able/value pairs called features.
A feature Fis a set of variable-value pairs hXj ; xji, withxj 2 Xj.
Our classier is dened in terms ofa nite number n of features F1; : : : ;Fn, wheren  106 in our classiers.2 Each feature Fi de-2It turns out that many pairs of features are exten-sionally equivalent, i.e., take the same values on eachnes an associated random boolean variableFi =?hXj,xji2Fi(Xj=xj);where (X=x) takes the value 1 if X = x and 0otherwise.
That is, Fi = 1 i Xj = xj for allhXj ; xji 2 Fi.Our classier estimates a feature weight i foreach feature Fi, that is used to dene the pre-diction variable Z:Z =n?i=1iFi:The prediction made by the classier issign(Z) = Z=jZj, i.e., ?1 or +1 depending onthe sign of Z.Intuitively, our goal is to adjust the vectorof feature weights ~ = (1; : : : ; n) to minimizethe expected misclassification rate E[(sign(Z) 6=Y )].
This function is dicult to minimize,so our boosting classier minimizes the ex-pected Boost loss E[exp(?Y Z)].
As Singer andSchapire [16] point out, the misclassicationrate is bounded above by the Boost loss, so alow value for the Boost loss implies a low mis-classication rate.Our classier estimates the Boost loss as?Et[exp(?Y Z)], where ?Et[] is the expectationon the empirical training corpus distribution.The feature weights are adjusted iteratively;one weight is changed per iteration.
The fea-ture whose weight is to be changed is selectedgreedily to minimize the Boost loss using thealgorithm described in [7].
Training contin-ues for 25,000 iterations.
After each iterationthe misclassication rate on the developmentcorpus ?Ed[(sign(Z) 6= Y )] is estimated, where?Ed[] is the expectation on empirical develop-ment corpus distribution.
While each iterationlowers the Boost loss on the training corpus, agraph of the misclassication rate on the de-velopment corpus versus iteration number is anoisy U-shaped curve, rising at later iterationsdue to overlearning.
The value of ~ returnedword token in our training data.
We developed a methodfor quickly identifying such extensionally equivalent fea-ture pairs based on hashing XORed random bitmaps,and deleted all but one of each set of extensionally equiv-alent features (we kept a feature with the smallest num-ber of conditioning variables).by the estimator is the one that minimizes themisclassciation rate on the development cor-pus; typically the minimum is obtained afterabout 12,000 iterations, and the feature weightvector ~ contains around 8000 nonzero featureweights (since some weights are adjusted morethan once).32.2 Conditioning variables and featuresThis subsection describes the conditioning vari-ables used in the EDITED classier.
Many of thevariables are dened in terms of what we calla rough copy.
Intuitively, a rough copy iden-ties repeated sequences of words that mightbe restarts or repairs.
Punctuation is ignoredfor the purposes of dening a rough copy, al-though conditioning variables indicate whetherthe rough copy includes punctuation.
A roughcopy in a tagged string of words is a substringof the form 1?
2, where:1.
1 (the source) and 2 (the copy) both be-gin with non-punctuation,2.
the strings of non-punctuation POS tags of1 and 2 are identical,3.
(the free final) consists of zero or moresequences of a free nal word (see below)followed by optional punctuation, and4.
?
(the interregnum) consists of sequences ofan interregnum string (see below) followedby optional punctuation.The set of free-final words includes all partialwords (i.e., ending in a hyphen) and a small setof conjunctions, adverbs and miscellanea, suchas and, or, actually, so, etc.
The set of interreg-num strings consists of a small set of expressionssuch as uh, you know, I guess, I mean, etc.
Wesearch for rough copies in each sentence start-ing from left to right, searching for longer copiesrst.
After we nd a rough copy, we restartsearching for additional rough copies followingthe free nal string of the previous copy.
Wesay that a word token is in a rough copy i itappears in either the source or the free nal.4(2) is an example of a rough copy.3We used a smoothing parameter  as described in[7], which we estimate by using a line-minimization rou-tine to minimize the classifier?s minimum misclassifica-tion rate on the development corpus.4In fact, our definition of rough copy is more complex.For example, if a word token appears in an interregnum(2) I thought I????1cou-,?
??
?I mean,?
??
??I???
?2would n-ish the workTable 1 lists the conditioning variables usedin our classier.
In that table, subscript inte-gers refer to the relative position of word to-kens relative to the current word; e.g.
T1 isthe POS tag of the following word.
The sub-script f refers to the tag of the rst word of thefree nal match.
If a variable is not dened fora particular word it is given the special value?NULL?
; e.g., if a word is not in a rough copythen variables such as Nm, Nu, Ni, Nl, Nr andTf all take the value NULL.
Flags are boolean-valued variables, while numeric-valued variablesare bounded to a value between 0 and 4 (as wellas NULL, if appropriate).
The three variablesCt, Cw and Ti are intended to help the classiercapture very short restarts or repairs that maynot involve a rough copy.
The flags Ct and Ciindicate whether the orthographic form and/ortag of the next word (ignoring punctuation) arethe same as those of the current word.
Ti hasa non-NULL value only if the current word isfollowed by an interregnum string; in that caseTi is the POS tag of the word following thatinterregnum.As described above, the classier?s featuresare sets of variable-value pairs.
Given a tuple ofvariables, we generate a feature for each tupleof values that the variable tuple assumes in thetraining data.
In order to keep the feature setmanagable, the tuples of variables we considerare restricted in various ways.
The most impor-tant of these are constraints of the form ?if Xjis included among feature?s variables, then sois Xk?.
For example, we require that if a fea-ture contains Pi+1 then it also contains Pi fori  0, and we impose a similiar constraint onPOS tags.2.3 Empirical evaluationFor the purposes of this research the Switch-board corpus, as distributed by the LinguisticData Consortium, was divided into four sectionsand the word immediately following the interregnum alsoappears in a (different) rough copy, then we say that theinterregnum word token appears in a rough copy.
Thispermits us to approximate the Switchboard annotationconvention of annotating interregna as EDITED if theyappear in iterated edits.
(or subcorpora).
The training subcorpus con-sists of all les in the directories 2 and 3 of theparsed/merged Switchboard corpus.
Directory4 is split into three approximately equal-size sec-tions.
(Note that the les are not consecutivelynumbered.)
The rst of these (les sw4004.mrgto sw4153.mrg) is the testing corpus.
All editdetection and parsing results reported hereinare from this subcorpus.
The les sw4154.mrgto sw4483.mrg are reserved for future use.
Theles sw4519.mrg to sw4936.mrg are the devel-opment corpus.
In the complete corpus threeparse trees were suciently ill formed in thatour tree-reader failed to read them.
These treesreceived trivial modications to allow them tobe read, e.g., adding the missing extra set ofparentheses around the complete tree.We trained our classier on the parsed datales in the training and development sections,and evaluated the classifer on the test section.Section 3 evaluates the parser?s output in con-junction with this classier; this section focuseson the classier?s performance at the individualword token level.
In our complete application,the classier uses a bitag tagger to assign eachword a POS tag.
Like all such taggers, our tag-ger has a nonnegligible error rate, and these tag-ging could conceivably aect the performance ofthe classier.
To determine if this is the case,we report classier performance when trainedboth on \Gold Tags" (the tags assigned by thehuman annotators of the Switchboard corpus)and on \Machine Tags" (the tags assigned byour bitag tagger).
We compare these results toa baseline \null" classier, which never identi-es a word as EDITED.
Our basic measure ofperformance is the word misclassication rate(see Section 2.1).
However, we also report pre-cision and recall scores for EDITED words alone.All words are assigned one of the two possiblelabels, EDITED or not.
However, in our evalua-tion we report the accuracy of only words otherthan punctuation and lled pauses.
Our logichere is much the same as that in the statisticalparsing community which ignores the locationof punctuation for purposes of evaluation [3,5,6] on the grounds that its placement is entirelyconventional.
The same can be said for lledpauses in the switchboard corpus.Our results are given in Table 2.
They showthat our classier makes only approximately 1/3W0 Orthographic wordP0; P1; P2; Pf Partial word flagsT?1; T0; T1; T2; Tf POS tagsNm Number of words in common in source and copyNu Number of words in source that do not appear in copyNi Number of words in interregnumNl Number of words to left edge of sourceNr Number of words to right edge of sourceCt Followed by identical tag flagCw Followed by identical word flagTi Post-interregnum tag flagTable 1: Conditioning variables used in the EDITED classier.of the misclassication errors made by the nullclassier (0.022 vs. 0.059), and that using thePOS tags produced by the bitag tagger doesnot have much eect on the classier?s perfor-mance (e.g., EDITED recall decreases from 0.678to 0.668).3 Parsing transcribed speechWe now turn to the second pass of our two-passarchitecture, using an \o-the-shelf" statisticalparser to parse the transcribed speech after hav-ing removed the words identied as edited bythe rst pass.
We rst dene the evaluationmetric we use and then describe the results ofour experiments.3.1 Parsing metricsIn this section we describe the metric we useto grade the parser output.
As a rst desider-atum we want a metric that is a logical exten-sion of that used to grade previous statisticalparsing work.
We have taken as our startingpoint what we call the \relaxed labeled preci-sion/recall" metric from previous research (e.g.[3,5]).
This metric is characterized as follows.For a particular test corpus let N be the totalnumber of nonterminal (and non-preterminal)constituents in the gold standard parses.
LetM be the number of such constituents returnedby the parser, and let C be the number of thesethat are correct (as dened below).
Then pre-cision = C=M and recall = C=N .A constituent c is correct if there exists a con-stituent d in the gold standard such that:1. label(c) = label(d)55For some reason, starting with [12] the labels ADVP2.
begin(c) r begin(d)3. end(c) r end(d)In 2 and 3 above we introduce an equivalencerelation r between string positions.
We dener to be the smallest equivalence relation sat-isfying a r b for all pairs of string positions aand b separated solely by punctuation symbols.The parsing literature uses r rather than =because it is felt that two constituents shouldbe considered equal if they disagree only in theplacement of, say, a comma (or any other se-quence of punctuation), where one constituentincludes the punctuation and the other excludesit.Our new metric, \relaxed edited labeled preci-sion/recall" is identical to relaxed labeled preci-sion/recall except for two modications.
First,in the gold standard all non-terminal subcon-stituents of an EDITED node are removed andthe terminal constituents are made immediatechildren of a single EDITED node.
Furthermore,two or more EDITED nodes with no separatingnon-edited material between them are mergedinto a single EDITED node.
We call this versiona \simplied gold standard parse."
All precisionrecall measurements are taken with respected tothe simplied gold standard.Second, we replace r with a new equiva-lence relation e which we dene as the smallestequivalence relation containing r and satisfy-ing begin(c) e end(c) for each EDITED node cin the gold standard parse.6and PRT are considered to be identical as well.6We considered but ultimately rejected defining ?eusing the EDITED nodes in the returned parse ratherClassiferNull Gold Tags Machine TagsMisclassication rate 0.059 0.021 0.022EDITED precision { 0.952 0.944EDITED recall 0 0.678 0.668Table 2: Performance of the \null" classier (which never marks a word as EDITED) and boostingclassiers trained on \Gold Tags" and \Machine Tags".1 2 3 4 5 6 7 8E E E Ethe , bagel with uh , doughnut1 2 2 4 5 2 2 8Figure 1: Equivalent string positions as dened by e.We give a concrete example in Figure 1.
Therst row indicates string position (as usual inparsing work, position indicators are betweenwords).
The second row gives the words of thesentence.
Words that are edited out have an\E" above them.
The third row indicates theequivalence relation by labeling each string posi-tion with the smallest such position with whichit is equivalent.There are two basic ideas behind this deni-tion.
First, we do not care where the EDITEDnodes appear in the tree structure produced bythe parser.
Second, we are not interested in thene structure of EDITED sections of the string,just the fact that they are EDITED.
That wedo care which words are EDITED comes intoour gure of merit in two ways.
First, (non-contiguous) EDITED nodes remain, even thoughtheir substructure does not, and thus they arecounted in the precision and recall numbers.Secondly (and probably more importantly), fail-ure to decide on the correct positions of editednodes can cause collateral damage to neighbor-ing constituents by causing them to start or stopin the wrong place.
This is particularly rele-vant because according to our denition, whilethe positions at the beginning and ending of anedit node are equivalent, the interior positionsare not (unless related by the punctuation rule).than the simplified gold standard.
We rejected this be-cause the ?erelation would then itself be dependenton the parser?s output, a state of affairs that might al-low complicated schemes to improve the parser?s perfor-mance as measured by the metric.See Figure 1.3.2 Parsing experimentsThe parser described in [3] was trained on theSwitchboard training corpus as specied in sec-tion 2.1.
The input to the training algorithmwas the gold standard parses minus all EDITEDnodes and their children.We tested on the Switchboard testing sub-corpus (again as specied in Section 2.1).
Allparsing results reported herein are from all sen-tences of length less than or equal to 100 wordsand punctuation.
When parsing the test corpuswe carried out the following operations:1. create the simplied gold standard parseby removing non-terminal children of anEDITED node and merging consecutiveEDITED nodes.2.
remove from the sentence to be fed to theparser all words marked as edited by anedit detector (see below).3. parse the resulting sentence.4.
add to the resulting parse EDITED nodescontaining the non-terminal symbols re-moved in step 2.
The nodes are added ashigh as possible (though the denition ofequivalence from Section 3.1 should makethe placement of this node largely irrele-vant).5. evaluate the parse from step 4 against thesimplied gold standard parse from step 1.We ran the parser in three experimental sit-uations, each using a dierent edit detector instep 2.
In the rst of the experiments (labeled\Gold Edits") the \edit detector" was simplythe simplied gold standard itself.
This was tosee how well the parser would do it if had perfectinformation about the edit locations.In the second experiment (labeled \GoldTags"), the edit detector was the one describedin Section 2 trained and tested on the part-of-speech tags as specied in the gold standardtrees.
Note that the parser was not given thegold standard part-of-speech tags.
We were in-terested in contrasting the results of this experi-ment with that of the third experiment to gaugewhat improvement one could expect from usinga more sophisticated tagger as input to the editdetector.In the third experiment (\Machine Tags") weused the edit detector based upon the machinegenerated tags.The results of the experiments are given inTable 3.
The last line in the gure indicatesthe performance of this parser when trained andtested on Wall Street Journal text [3].
It isthe \Machine Tags" results that we consider the\true" capability of the detector/parser combi-nation: 85.3% precision and 86.5% recall.3.3 DiscussionThe general trends of Table 3 are much as onemight expect.
Parsing the Switchboard data ismuch easier given the correct positions of theEDITED nodes than without this information.The dierence between the Gold-tags and theMachine-tags parses is small, as would be ex-pected from the relatively small dierence inthe performance of the edit detector reported inSection 2.
This suggests that putting signicanteort into a tagger for use by the edit detec-tor is unlikely to produce much improvement.Also, as one might expect, parsing conversa-tional speech is harder than Wall Street Jour-nal text, even given the gold-standard EDITEDnodes.Probably the only aspect of the above num-bers likely to raise any comment in the pars-ing community is the degree to which pre-cision numbers are lower than recall.
Withthe exception of the single pair reported in [3]and repeated above, no precision values in therecent statistical-parsing literature [2,3,4,5,14]have ever been lower than recall values.
Eventhis one exception is by only 0.1% and not sta-tistically signicant.We attribute the dominance of recall over pre-cision primarily to the influence of edit-detectormistakes.
First, note that when given thegold standard edits the dierence is quite small(0.3%).
When using the edit detector edits thedierence increases to 1.2%.
Our best guess isthat because the edit detector has high preci-sion, and lower recall, many more words are leftin the sentence to be parsed.
Thus one ndsmore nonterminal constituents in the machineparses than in the gold parses and the precisionis lower than the recall.4 Previous researchWhile there is a signicant body of work on nd-ing edit positions [1,9,10,13,17,18], it is dicultto make meaningful comparisons between thevarious research eorts as they dier in (a) thecorpora used for training and testing, (b) theinformation available to the edit detector, and(c) the evaluation metrics used.
For example,[13] uses a subsection of the ATIS corpus, takesas input the actual speech signal (and thus hasaccess to silence duration but not to words), anduses as its evaluation metric the percentage oftime the program identies the start of the in-terregnum (see Section 2.2).
On the other hand,[9,10] use an internally developed corpus of sen-tences, work from a transcript enhanced withinformation from the speech signal (and thususe words), but do use a metric that seems to besimilar to ours.
Undoubtedly the work closestto ours is that of Stolcke et al [18], which alsouses the transcribed Switchboard corpus.
(How-ever, they use information on pause length, etc.,that goes beyond the transcript.)
They cate-gorize the transitions between words into morecategories than we do.
At rst glance theremight be a mapping between their six categoriesand our two, with three of theirs correspondingto EDITED words and three to not edited.
Ifone accepts this mapping they achieve an er-ror rate of 2.6%, down from their NULL rate of4.5%, as contrasted with our error rate of 2.2%down from our NULL rate of 5.9%.
The dier-ence in NULL rates, however, raises some doubtsthat the numbers are truly measuring the samething.Experiment Labeled Precision Labeled Recall F-measureGold Edits 87.8 88.1 88.0Gold Tags 85.4 86.6 86.0Machine Tags 85.3 86.5 85.9WSJ 89.5 89.6Table 3: Results of Switchboard parsing, sentence length  100.There is also a small body of work on parsingdisfluent sentences [8,11].
Hindle?s early work[11] does not give a formal evaluation of theparser?s accuracy.
The recent work of Schubertand Core [8] does give such an evaluation, buton a dierent corpus (from Rochester Trainsproject).
Also, their parser is not statisticaland returns parses on only 62% of the strings,and 32% of the strings that constitute sentences.Our statistical parser naturally parses all of ourcorpus.
Thus it does not seem possible to makea meaningful comparison between the two sys-tems.5 ConclusionWe have presented a simple architecture forparsing transcribed speech in which an editedword detector is rst used to remove such wordsfrom the sentence string, and then a statisticalparser trained on edited speech (with the editednodes removed) is used to parse the text.
Theedit detector reduces the misclassication rateon edited words from the null-model (markingeverything as not edited) rate of 5.9% to 2.2%.To evaluate our parsing results we have intro-duced a new evaluation metric, relaxed editedlabeled precision/recall.
The purpose of thismetric is to make evaluation of a parse treerelatively indierent to the exact tree posi-tion of EDITED nodes, in much the same waythat the previous metric, relaxed labeled pre-cision/recall, make it indierent to the attach-ment of punctuation.
By this metric the parserachieved 85.3% precision and 86.5% recall.There is, of course, great room for improve-ment, both in stand-alone edit detectors, andtheir combination with parsers.
Also of interestare models that compute the joint probabilitiesof the edit detection and parsing decisions |that is, do both in a single integrated statisticalprocess.References1.
Bear, J., Dowding, J. and Shriberg, E.Integrating multiple knowledge sources fordetection and correction of repairs in human-computer dialog.
In Proceedings of the 30thAnnual Meeting of the Association for Com-putational Linguistics.
56{63.2.
Charniak, E. Statistical parsing with acontext-free grammar and word statistics.In Proceedings of the Fourteenth NationalConference on Articial Intelligence.
AAAIPress/MIT Press, Menlo Park, CA, 1997,598{603.3.
Charniak, E. A maximum-entropy-inspired parser.
In Proceedings of the 2000Conference of the North American Chap-ter of the Association for ComputationalLinguistics.
ACL, New Brunswick NJ, 2000.4.
Collins, M. J.
A new statistical parserbased on bigram lexical dependencies.
In Pro-ceedings of the 34th Annual Meeting of theACL.
1996.5.
Collins, M. J.
Three generative lexical-ized models for statistical parsing.
In Pro-ceedings of the 35th Annual Meeting of theACL.
1997, 16{23.6.
Collins, M. J. Head-Driven StatisticalModels for Natural Language Parsing.
Uni-versity of Pennsylvania, Ph.D. Dissertation,1999.7.
Collins, M. J. Discriminative rerankingfor natural language parsing.
In Proceedingsof the International Conference on MachineLearning (ICML 2000).
2000.8.
Core, M. G. and Schubert, L. K. A syn-tactic framework for speech repairs and otherdisruptions.
In Proceedings of the 37th An-nual Meeting of the Association for Compu-tational Linguistics.
1999, 413{420.9.
Heeman, P. A. and Allen, J. F. Into-national boundaries, speech repairs and dis-course markers: modeling spoken dialog.
In35th Annual Meeting of the Association forComputational Linguistics and 17th Interna-tional Conference on Computational Linguis-tics.
1997, 254{261.10.
Heeman, P. A. and Allen, J. F. Speechrepairs, intonational phrases and discoursemarkers: modeling speakers?
utterances inspoken dialogue.
Computational Linguistics254 (1999).11.
Hindle, D. Deterministic parsing of syn-tactic non-fluencies.
In Proceedings of the21st Annual Meeting of the Association forComputational Linguistics.
1983, 123{128.12.
Magerman, D. M. Statistical decision-treemodels for parsing.
In Proceedings of the 33rdAnnual Meeting of the Association for Com-putational Linguistics.
1995, 276{283.13.
Nakatani, C. H. and Hirschberg, J. Acorpus-based study of repair cues in sponta-neous speech.
Journal of the Acoustical Soci-ety of America 953 (1994), 1603{1616.14.
Ratnaparkhi, A.
Learning to parse natu-ral language with maximum entropy models.Machine Learning 34 1/2/3 (1999), 151{176.15.
Shriberg, E. E. Preliminaries to a The-ory of Speech Disfluencies.
In PhD Disserta-tion.
Department of Psychology, Universityof California-Berkeley, 1994.16.
Singer, Y. and Schapire, R. E. Im-proved boosting algorithms using condence-based predictions.
In Proceedings of theEleventh Annual Conference on Computa-tional Learning Theory.
1998, 80{91.17.
Stolcke, A. and Shriberg, E. Auto-matic linguistic segmantation of conversa-tional speech.
In Proceedings of the 4th In-ternational Conference on Spoken LanguageProcessing (ICSLP-96).
1996.18.
Stolcke, A., Shriberg, E., Bates, R.,Ostendorf, M., Hakkani, D., Plauche,M., Tu?r, G. and Lu, Y.
Automatic detec-tion of sentence boundaries and disfluenciesbased on recognized words.
Proceedings ofthe International Conference on Spoken Lan-guage Processing 5 (1998), 2247{2250.
