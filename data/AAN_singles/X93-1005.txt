DOCUMENT DETECTION OVERVIEWDonna HarmanNational Institute of Standards and TechnologyGaithersburg, MD.
208991.
INTRODUCTIONThe goal of the document detection half of the TIPSTERproject wasto significantly advance the state of the art ineffective document detection from large, real-world ocu-ment collections.
This document detection eeded to beused in both the routing environment (static queriesagainst aconstant stream of new data) and the adhoc envi-ronment (new queries against archival data).
An addition-al requirement was that the algorithms for these tasks beas domain and language independent as possible.
Todemonstrate language independence, the project was doneboth in Japanese and English.
To demonstrate domain in-dependence, the test collection was selected to covermany different subject areas and different document s ruc-tures.The document detection task mirrors the general taskknown as information retrieval.
This area of research asseen over 30 years of experimentation \[1\], leaving alegacy of proven evaluation methodologies.
The mostprominent of these methodologies is the use of a test col-lection.
A test collection for information retrieval consistsof a set of documents, a set of test queries or questions,and a set of relevance judgments that are considered to bethe "right" answers to the questions.
The first test collec-tions, such as the Cranfield collection, were built in theearly 1960's.
The Cranfield collection contains 1400 doc-uments (all abstracts), 225 queries (several sentence natu-ral language statements), and 1827 relevance judgments,or an average of about 6 relevant documents per query.Since the early 1960's several other test collections havebeen built, but none contain the extremely large numbersof documents necessary to reflect he environment to bemodeled in TIPSTER.The first step of this project, therefore, was to create avery large test collection and to design the test methodolo-gy and evaluation measures needed for TIPSTER.
Thetest design was based on traditional information retrievalmodels, and is detailed in the next section.
Evaluationwas done using recall, precision and fallout measures.These measures are discussed in the section on evaluationmetrics.The test design and test collection used for TIPSTER wasalso used for both the TREC conferences \[2,3\].
The onlydifference between the evaluation done for the TIPSTERcontractors and the TREC participants was in the evalua-tion schedule and in the number of results submitted forevaluation.
The first TREC conference took place 2months after the 12-month TIPSTER evaluation and thesecond TREC conference coincided with the 24-monthTIPSTER evaluation.
The TIPSTER contractors had anadditional evaluation at 18 months.
TREC participantswere limited to submitting only 2 sets of results for adhocor routing evaluation, whereas the TIPSTER contractorswere allowed to submit an unlimited number of runs forevaluation.2.
TEST DESIGNThe test design called for the creation of a set of trainingdata and a set of test data.
The training data consisted oflarge numbers of documents (between 1 and 2 gigabytesof text), 50 training topics, and lists of documents for eachof the topics that were known to be relevant (the "fight an-swers").
The test data consisted of 50 new topics andabout a gigabyte of new documents.A slight departure from traditional information retrievalmethodology was needed to better handle the TIPSTERenvironment.
All previous test collections have assumedthat the test questions or topics are closely related to theactual queries submitted to the retrieval systems, as thetest questions are generally transformed automatically in-to the structure of terms ubmitted to the retrieval systemsas input.
This input structure is called the query in theTIPSTER environment, with the test question itself re-ferred to as the topic.
Since most previous research asinvolved simple automatic generation ofqueries from top-ics, there was no need for a distinction to be made be-tween topics and queries.
In TIPSTER this distinction be-came important because the topics needed to carry a largeamount of highly specific information, and the methods ofquery construction therefore became more complex.TrainingTopics(T-Train)TestTopics(T-Test)Q11Q3TrainingDocuments(D-Train)TestDocuments(D-Test)Figure 1 -- The TIPSTER Document Detection TaskFigure 1 shows a schematic of the test design, includingthe various components of the test methodology.
The dia-gram reflects the four data sets (2 sets of topics and 2 setsof documents) that were provided to contractors.
The firstset of topics and documents (T-Train and D-Train) wereprovided to allow system training and to serve as the basefor routing and adhoc experiments.
The roudng taskassumes a static set of topics (T-Train), with evaluation ofrouting done by providing new test documents (D-Test).The adhoc task assumes a static set of documents (D-Train), with evaluation of adhoc retrieval done by provid-ing new topics (T-Test).Three different sets of queries were generated from thedata sets.
Q1 is the set of queries (probably multiple sets)created to help in adjusting a retrieval system to this task.The results of this research were used to create Q2, therouting queries to be used against he new test documents(D-Test).
Q3 is the set of queries created from the newtest topics (T-Test) as adhoc queries for .~earching againstthe old documents (D-Train).
The results from searchesusing Q2 and Q3 were the official evaluation results entto NIST for both TIPSTER and TREC.The Japanese language test design paralleled exactly theEnglish language test design.3.
EVALUATION SCHEDULEFor the English language document detection task therewere three evaluations conducted uring the 2-year phaseI program.12-month evaluation?
D-Train--disk 1 (about I gigabyte of documents)?
T-Train -- topics 1-50?
D-Test -- disk 2 (about 1 gigabyte of documents)?
T-Test -- topics 51-10010?
routing test -- topics 1-50 against disk 2Because of the lateness of data availability, and thescarcity of sample relevance assessments for training, theemphasis was put on doing adhoc evaluation and only halfof the routing test was done.18-month evaluation?
D-Train -- disks 1 & 2 (about 2 gigabytes of docu-ments)?
T-Train -- topics 51-100?
D-Test -- subset of future disk 3 (about 500megabytes of documents)?
T-Test -- revised topics 1-50?
adhoc test -- topics 1-50 against disks 1 & 2?
routing test -- topics 51-100 against subset of disk 3By the 18-month evaluation point, large numbers of rele-vance judgments were available for training (due to themany TREC-1 participants).
This second evaluationtherefore concentrated onthe routing task, although adhocevaluation was also done.24-month evaluation?
D-Train -- disks 1 & 2 (about 2 gigabytes of docu-ments)?
T-Train -- topics 1-100?
D-Test -- disk 3 (about 1 gigabyte of documents)?
T-Test -- topics 101-150?
adhoc test -- topics 101-150 against disks 1 & 2?
routing test -- topics 51-100 against all of disk 3This data point corresponded directly to the TREC-2 dataand therefore allows comparison between the 24-monthTIPSTER results and the TREC-2 results.4.
SPECIF IC  TASK GUIDEL INESBecause the TIPSTER contractors and TREC participantsused a wide variety of indexing/knowledge base buildingtechniques, and a wide variety of approaches to generatesearch queries, it was important to establish clear guide-lines for the evaluation task.
The guidelines deal with themethods of indexing/knowledge base construction, andwith the methods of generating the queries from the sup-plied topics.
In general they were constructed toreflect anactual operational environment, and to allow .as fair aspossible a separation among the diverse query construc-tion approaches.There were guidelines for constructing and manipulatingthe system data structures.
These structures were definedto consist of the original documents, any new structuresbuilt automatically from the documents ( uch as invertedfiles, thesauri, conceptual networks, etc.)
and any newstructures built manually from the documents (such asthesauri, synonym lists, knowledge bases, rules, etc.
).The following guidelines were developed for the TIP-STER task.1.
System data structures hould be built using theinitial training set (documents D-Train, trainingtopics T-Train, and the relevance judgments).They may be modified based on the test docu-ments D-Test, but not based on the test topics.
Inparticular, the processing of one test topic shouldnot affect he processing of another test topic.
Forexample, it is not allowed to update a systemknowledge base based on the analysis of one testtopic in such a way that the interpretation f sub-sequent test topics was changed in any fashion.2.
There are several parts of the Wall Street Journaland the Ziff material that contain manuallyassigned controlled or uncontrolled index terms.These fields are delimited by SGML tags, as spec-ified in the documentation files included with theda~ Since the primary focus is on retrieval androuting of naturally occurring text, these manuallyindexed terms should not be used.3.
Special care should be used in handling the rout-ing topics.
In a true routing situation, a singledocument would be indexed and compared againstthe routing topics.
Since the test documents aregenerally indexed as a complete set, routingshould be simulated by not using any test docu-ment information (such as IDF based on the testcollection, total frequency based on the test collec-tion, etc.)
in the searching.
It is permissible to usetraining-set collection information however.Additionally there were guidelines for constructing thequeries from the provided topics.
These guidelines wereconsidered of great importance for fair system compari-son and were therefore carefully constructed.
Threegeneric ategories were defined, based on the amount andkind of manual intervention used.1.
Method 1 -- completely automatic initial queryconstruction.adhoc queries -- The system will automaticallyextract information from the topic (the topic fieldsused should be identified) to construct the query.The query will then be submitted to the system(with no manual modifications) and the results11..from the system will be the results submitted toNIST.
There should be no manual interventionthat would affect he results.routing queries -- The queries should be con-structed automatically using the training topics,the training relevance judgments and the trainingdocuments.
The queries hould then be submittedto NIST before the test documents are releasedand should not be modified after that point.
Theunmodified queries should be run against he testdocuments and the results ubmitted to NIST.Method 2 -- manual initial query construction.adhoc queries -- The query is constructed in somemanner from the topic, either manually or usingmachine assistance.
The methods used should beidentified, along with the human expertise (bothdomain expertise and computer expertise) neededto construct a query.
Once the query has beenconstructed, it will be submitted to the system?
(with no manual intervention), and the resultsfrom the system will be the results submitted toNIST.
There should be no manual interventionafter initial query conslrucfion that would affectthe results.
(Manual intervention is covered byMethod 3.
)routing queries -- The queries should be con-structed in the same manner as the adhoc queriesfor method 2, but using the training topics, rele-vance judgments, and training documents.
Theyshould then be submitted to NIST before the testdocuments are released and should not be modi-fied after that point.
The unmodified queriesshould be run against he test documents and theresults ubmitted to NIST.Method 3 -- automatic or manual query construc-tion with feedback.adhoc queries -- The initial query can be con-structed using either Method 1 or Method 2.
Thequery is submitted to the system, and a subset ofthe retrieved ocuments i used for manual feed-back, i.e.
a human makes judgments about he rel-evance of the documents in this subset.
Thesejudgments may be communicated to the system,which may automatically modify the query, or thehuman may simply choose to modify the queryhimself.
At some point, feedback should end, andthe query should be accepted as final.
Systemsthat submit runs using this method must submitseveral different sets of results to allow tracking o fthe time/cost benefit of doing relevance f edback.routing queries -- Method 3 cannot be used forrouting queries as routing systems have typicallynot supported feedback.5.
EVALUATION METRICS5.1 Recall/Precision CurvesStandard recall/precision figures were calculated for eachTIPSTER and TREC system and the tables and graphs forthe results were provided.
Figure 2 shows typicalrecall/precision curves.
The x axis plots the recall valuesat fixed levels of recall, whereRecall =number of relevant items retrievedwtal number of relevant items in collectionThe y axis plots the average precision values at thosegiven recall values, where precision is calculated bynumber of relevant items retrievedPrecision' =wtal number of items retrievedThese curves represent averages over the 50 topics.
Theaveraging method was developed many years ago \[2\] andis well accepted by the information retrieval community.It was therefore used unchanged for the TIPSTER evalua-tion.
The curves how system performance across the fullrange of retrieval, i.e.
at the early stage of retrieval wherethe highly-ranked documents give high accuracy or preci-sion, and at the final stage of retrieval where there is usu-ally a low accuracy, but more complete retrieval.
Notethat the use of these curves assumes a ranked output froma system.
Systems that provide an unranked set of docu-ments are known to be less effective and therefore werenot tested in the TIPSTER/TREC programs.The curves in figure 2 show that system A has a muchhigher precision at the low recall end of the graph andtherefore is more accurate.
System B however has higherprecision at the higher recall end of the curve and there-fore will give a more complete set of relevant documents,assuming that the user is willing to look further in theranked list.5.2 Reca l l / Fa l lout  CurvesA second set of curves were calculated using therecall/fallout measures, where recall is defined as beforeand fallout is defined asnumber of nonrelevant i ems retrievedfallout =total number of nonrelevant i ems in collection120.80Sample  Reca l l /P rec i s ion  Curves0.60o ~o ~0.400.200.000.00  0.20 0.40 0.60 0.80  1 .00Reca l l4-  Sys tem A_~_ System B0.75Sample Recall/Fallout Curves0.70.650.6o 0.550.50.450.40.35 I l I I l I I I I0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16Fa l lout4 -  Sys tem A ~ System B1.00I I0.18 0.20Figure 2 -- A Sample Recall/Precision CurveFigure 3 -- A Sample i~ecall/Fallout Curve13Note that recall has the same definition as the probabilityof detection and that fallout has the same definition as theprobability of false alarm, so that the recall/fallout curvesare also the ROC (Relative Operating Characteristic)curves used in signal processing.
A sample set of curvescorresponding to the recall/precision curves are shown infigure 3.
These curves show the same order of perfor-mance as do the recall/precision curves and are providedas an alternative method of viewing the results.
The pre-sent version of the curves is experimental sthe curve cre-ation is particularly sensitive to scaling (what range isused for calculating fallout).
The high precision perfor-mance does not show well in figure 3; the high recall per-formance dominates the curves.Whereas the recall/precision curves show the retrievalsystem results as they might be seen by a user (since pre-cision measures the accuracy of each retrieved ocumentas it is retrieved), the recall/fallout curves emphasize theability of these systems to screen out non-relevant mate-rial.
In particular the fallout measure shows the discrima-tion powers of these systems on a large document collec-tion.
Since recall/precision measures do not include anyindication of the collection size, the recall and precisionof a system based on a 1400 document collection could bethe same as that of a system based on a million documentcollection, but obviously the discrimation powers on amillion document collection would be much greater.
Thiswas not have been a problem on the smaller collections,but the discrimination power of systems on TIPSTER-sized collections i  very important.5.3 Single-Value Evaluation MeasuresIn addition to these recall/precision and recall/falloutcurves, there were 3 single-value measures often used inTIPSTER.
The first two measures are precision averagesacross the curves, and the third measure is precision at aparticular cutoff of documents retrieved.One of the averages, the non-interpolated average rreci-sion, combines the average precision for each topic, withthat topic average computed by taking the precision afterevery retrieved relevant document.
The final average cor-responds to the area under an ideal (non-interpolated)recall/precision curve.The second precision average (the l 1-point precisionaverage) averages across interpolated precision values(which makes it somewhat less accurate).
It is calculatedby averaging the precision at each of the 11 standardrecall points on the curve (0.0, 0.1 .
.
.
.
.
1.0) for each topic.Often this average is stated as an improvement over somebaseline average 11-point precision.The third measure used is an average of the precision ateach topic after 100 documents have been retrieved forthat topic.
This measure is useful because it contains nointerpolation, and reflects a clearly comprehendedretrieval point.
It took on added importance in the TIP-STER environment because only the top I00 documentsretrieved for each topic were actually assessed.
For thisreason it produces a guaranteed evaluation point for eachsystem.5.4 Problems with EvaluationSince this was the first time that such a large collection oftext has been used in evaluation, there were some prob-lems using the existing methods of evaluation.
The majorproblem concerned a thresholding effect caused by aninability to evaluate ALL documents retrieved by a givensystem.For the TIPSTER 12-month evaluation and TREC-1 thegroups were asked to send in only the top 200 documentsretrieved by their systems.
This artificial document cutoffis relatively low and systems did not retrieve all the rele-vant documents for most topics within the cutoff.
Alldocumen~ retrieved beyond the 200 were considered non-relevant by default and therefore the recall/precisioncurves became inaccurate after about 40% recall on aver-age.
The 18-month TIPSTER evaluation used a cutoff of500 documents, and the TIPSTER 24-month and TREC-2used the top 1000 documents.
Figure 4 shows the differ-ence in the curves produced by these evaluation thresh-olds, including a curve for no threshold (similar to theway evaluation has been done on the smaller collections.
).These curves how that the use of a 1000-document cutoffhas mostly resolved the thresholding problem.Two more issues in evaluation have become important.The first issue involves the need for more statistical evalu-ation.
As will be seen in the results, the recall/precisioncurves are often close, and there is a need to check if thereis truly any statistically significant differences betweentwo systems' results or two sets of results from the samesystem.
This problem is currently under investigation icollaboration with statistical groups experienced in the- evaluation of information retrieval systems.The second issue involves getting beyond the averages tobetter understand system performance.
Because of thehuge number of documents and the long topics, it is verydifficult o perform failure analysis, or any type of analy-sis on the results to better understand the retrieval pro-cesses being tested.
Without better understanding ofunderlying system performance, it will be hard to consoli-date research progress.
Some preliminary analysis of pertopic performance was provided for the TIPSTER24-month evaluation and TREC-2, and more attention willbe given to this problem in the future.14.
mgMEf fec ts  o f  Cuto f f  on  Eva luat ion0.400 .800 .600 .20w mO.OO O.20  0 .40  0 .60  0 .80  1 .OOReca l lat  200  +at5OO ~.
at  1000 o fu l l0.OO1.OOFigure 4: Effect of evaluation cutoffs on recall/precision curves6.
REFERENCES\[1\] Belkin N.J, and Croft W.B.
Retrieval Techniques.
InWilliams, M.
(Ed.
), Annual Review of lnformation Scienceand Technology (pp.
109-145).
New York, NY: ElsevierScience Publishers, 1987.\[2\] Harman D.
(Ed.
).The First Text REtrieval Conference(TREC-1).
National Institute of Standards and Technol-ogy Special Publication 500-207, 1993.\[3\] Harman D.
(Ed.
).The Second Text REtrieval Confer-ence (TREC-2).
National Institute of Standards and Tech-nology Special Publication 500-215, in press.\[4\] Salton G. and McGill M. (1983).
Introduction to Mod-ern Information Retrieval.
New York, NY.
: McGraw-HiUBook Company.1@
