Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 56?64,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsExploring Individual Differences in Student Writing with aNarrative Composition Support EnvironmentJulius Goth and Alok Baikadi and Eun Ha and Jonathan Rowe and Bradford Mottand James LesterDepartment of Computer ScienceNorth Carolina State UniversityRaleigh, NC, USA{jgoth, abaikad, eha, jprowe, bwmott, lester}@ncsu.eduAbstractNovice writers face significant challenges asthey learn to master the broad range of skillsthat contribute to composition.
Novice andexpert writers differ considerably, and devis-ing effective composition support tools fornovice writers requires a clear understandingof the process and products of writing.
Thispaper reports on a study conducted with morethan one hundred middle grade students inter-acting with a narrative composition supportenvironment.
The texts are found to pose im-portant challenges for state-of-the-art naturallanguage processing techniques.
Furthermore,the study investigates the language usage ofmiddle grade students, the cohesion and co-herence of the resulting texts, and the relation-ship between students?
language arts skillsand their writing processes.
The findings sug-gest that composition support environmentsrequire robust NLP tools that can account forthe variations in students?
writing in order toeffectively support each phase of the writingprocess.1 IntroductionWriting is fundamentally complex.
Writers mustsimultaneously consider a constellation of factorsduring composition, including writing task re-quirements, knowledge of audience, domainknowledge, language usage, and tone (Hayes andFlower, 1981).
Furthermore, effective writinginvolves sophisticated higher-order cognitiveskills, such as synthesis of ideas, critical thinking,and self-regulation.
Text genres, such as narrativeor expository texts, also introduce distinct re-quirements and conventions (Hayes and Flower,1981).Because writing itself is complex, learning towrite poses significant challenges for students.The central role of writing in communication,knowledge organization, and sensemaking pointsto the need to devise methods and tools with whichwriting skills can be effectively taught and learned(Graham, 2006).
Intelligent tutoring systems(VanLehn, 2006) offer a promising means for de-livering tailored writing support to students.
How-ever, developing intelligent tutors to scaffoldstudent writing poses a number of technical andpedagogical hurdles.
Texts written by novice writ-ers are likely to exhibit significant variation ingrammar, cohesion, coherence, and content qual-ity; these characteristics are likely to be problem-atic for analysis by current natural languageprocessing tools.
Furthermore, students?
individ-ual differences in language arts skills, writing self-efficacy, domain knowledge, and motivation canhave pedagogical implications.
An effective intel-ligent writing tutor must do more than just parseand understand student texts; it must also providetailored feedback that fosters effective writingprocesses and enhances student motivation forwriting.This paper explores several key questions forthe design of intelligent composition support toolsfor novice writers.
First, it investigates the per-formance of current syntactic parsing tools on acorpus of narrative texts written by middle gradestudents during interactions in a narrative composi-56tion support environment.
A narrative composi-tion support environment aims to support the prin-cipal processes of writing, such as planning,revision, and text production.
The second questionthe paper explores is how middle school students?language art skills affect the cohesion and coher-ence of texts produced during interactions with anarrative composition support environment.
Third,the paper investigates how middle school students?language art skills affect their writing processesduring interactions in a narrative composition sup-port environment.
Studying the interactions be-tween the environment?s support mechanisms andstudents?
individual differences provides insightsinto the affordances and limitations of novices?writing abilities, as well as implications for thedesign of intelligent tutors for narrative writing.The study presented here investigates novicewriters?
composition processes during interactionswith a narrative composition support environment.In the study, 127 middle grade students interactedwith the NARRATIVE THEATRE fable compositionsupport environment.
The NARRATIVE THEATREuses a multimedia interface to guide students asthey select key elements of their fable (e.g., moral,setting, characters), prompts students through anexplicit, timed story planning process, and allowsstudents to review their earlier planning decisionsat any point during writing of the main text.
Stu-dents?
literacy ratings and log data from interac-tions with the NARRATIVE THEATRE environmentare analyzed to investigate the differences betweenhigh- and low-skill students and their practice ofkey composition processes in the NARRATIVETHEATRE environment, including planning, textproduction, and revision.
Coh-Metrix (Graesser etal., 2004) was also used to analyze the cohesionand coherence characteristics of the students?
fa-bles.
The observations from this study offer im-portant implications for the design of intelligentcomposition support tools for novice writers.2 Related WorkSince Hayes and Flower first proposed their semi-nal model of writing nearly thirty years ago (1981),a rich body of work has investigated the cognitivefunctions supporting written composition.
Founda-tional results are now in place on the core proc-esses of writing, including idea generation(Galbraith et al, 2009), text production (Berningeret al, 2002), and revision (McCutchen et al,1997).
Furthermore, a detailed account of thecomposition process has begun to emerge across arange of writing experience levels (Graham et al,2002) and text genres (Langer, 1985).Particularly important for the design of compo-sition support tools for novices is the emergence ofa consensus account of the characteristics of nov-ice writers?
narrative composition processes.
Em-pirical studies have suggested that notabledifferences exist between novice and expert writ-ers, such as novices?
use of knowledge-tellingpractices versus experts?
use of knowledge-transformation practices during text production(Bereiter and Scardamalia, 1987).
However, it hasbeen argued that even novice writers can employhigh-level knowledge-transformation processeswhen situated within an appropriate task environ-ment with effective writing scaffolds (Cameronand Moshenko, 1996).
Other work has found thatstudents?
domain and linguistic knowledge influ-ences the coherence and quality of their expositorywritings (DeGroff, 1987).
These findings under-score the importance of investigating methods foreffective and engaging writing instruction targetedat novice writers, as well as automated tools totailor feedback and scaffolding to individual stu-dents.In addition to grounding their work in the writ-ing research literature, designers of compositionsupport tools will likely need to avail themselvesof the full gamut of natural language processingtechniques to analyze students?
texts with regard tosyntax, semantics, and discourse.
However, intexts produced by novice writers, grammaticalerrors and incoherent discourse abound, whichmay present serious challenges for natural lan-guage processing since the majority of currentNLP tools have been developed for well-formedtexts.
While existing NLP tools have been suc-cessfully used in writing support systems designedfor expert writers (Mahlow and Piotrowski, 2009),common structural issues in novice compositionsare likely to prove problematic for current tools.However, recent work has begun to explore tech-niques for handling ill-formed texts that are similarto those produced by novice writers.
For example,Gamon et al conducted a word-level analysis oftexts written by non-native English speakers(2008).
Focusing on two types of errors (deter-miners and prepositions), they use decision-tree57classifiers in combination with a language modeltrained on a large English corpus to detect andcorrect erroneous selection of words.
Wagner etal.
investigated the detection of grammatical mal-formedness of individual sentences (2007).
Theyfound it effective to combine a shallow approachthat uses n-grams and a deep approach that usessyntactic parse results.
Higgins et al explored theoverall coherence of texts written by students(2004).
Using support vector machines, their sys-tem identified the portions of text that resulted incoherence breakdowns with regard to relatednessto the essay question and relatedness between dis-course elements.To date, a relatively small number of intelligenttutoring systems have been developed to supportstudent learning in the language arts, and evenfewer have sought to specifically address writing.Sourcer?s Apprentice is a web-based learning envi-ronment to help high school students gather, evalu-ate, and integrate information for writing essaysabout history topics (Britt et al, 2004), althoughSourcer?s Apprentice did not seek to apply NLPtools to understand or scaffold students?
composi-tions directly.
Other work on intelligent tutoringfor language arts, such as Project LISTEN (Mo-stow and Aist, 2001) and REAP (Heilman et al,2007), has addressed vocabulary learning and read-ing comprehension.3 Narrative Corpus AcquisitionTo investigate narrative composition in novicewriters, a study was conducted with more than onehundred middle grade students using a narrativecomposition support environment.
TheNARRATIVE THEATRE (Figure 1) is an interactiveenvironment designed to capture both the processand products of writing.1 Targeting a user popula-tion of sixth grade students (age typically 12 years)and the genre of fables, the NARRATIVE THEATREenables students to create stories in an environmentthat was specifically designed to scaffold novices?composition activities during a timed story plan-1 The version of the NARRATIVE THEATRE used in the studyreported in this paper is the forerunner of a more generalcreativity support environment.
It is under development in ourlaboratory that will employ NLP techniques and intelligentgraphics generation.
The study reported here was conducted toinform the design of the creativity enhancement environmentand intelligent tutoring systems to support composition.Figure 1.
Narrative Theatre fable composition support environment.58ning and writing process.
The NARRATIVETHEATRE employs a multimedia interface createdwith Adobe's Flash?
development platform andAIR runtime environment.
Its design was inspiredby a worksheet that is widely used as part of theGrade 6 writing curriculum.During the planning phase, students select amoral, a setting, a cast of characters, and a set ofobjects for the story they will create.
The systemprovides nine different morals, four settings, tencharacters, and twenty objects from which studentsmay choose.
Each setting is accompanied by avisual representation, which can be enlarged byclicking on the image to highlight salient featuresof the setting.
Characters and objects are also visu-ally represented by static graphics, which weredesigned to be neutral in gender and expression inorder to allow students creative choice when fillingnarrative roles with the characters.Once the choices have been made, students arepresented with a screen that allows them to viewtheir planning decisions and begin structuring theirfable.
The planning area allows students to makenotes about what they would like to have happenduring the beginning, middle, and ending.
The topof the page contains windows that display the set-ting, characters, and objects that were chosen ear-lier, and that can provide more information via amouseover.
Students craft a plan for the beginning(setting and characters are introduced), middle(conflict and problem), and end (conflict resolu-tion) of their stories.
For each of the three majorsegments of the story, they formulate a textualplan.
After the planning information is entered, thestudents may begin writing (Figure 1).
They thencreate the actual prose, which is entered as rawtext.
The writing and revision phase are supportedwith a spell-correction facility.
All student activi-ties including interface selections and the textstreams from planning and writing are logged andtime-stamped.During the study, a total of 127 sixth-grademiddle school students (67 males, 60 females)participated in the study.
The students ranged inage from 10 to 13.
Approximately 38% of thestudents were Caucasian, 27% African-American,17% Hispanic or Latino, 6% Asian, 2% AmericanIndian, and the remaining 10% were of mixed orother descent.
Students participated as part of theirLanguage Arts class.
The study spanned two daysfor each student involved.
On the first day, thestudents were seated at a computer and asked to fillout a pre-experiment questionnaire, which requiredapproximately twenty minutes.
On the second day,the students were again assigned to a computer.They were presented with the NARRATIVETHEATRE interface, which asked them to enter aunique identification number.
Once correctly en-tered, the students were presented with a shortinstructional video that described the features andoperation of the interface.
They were given fifteenminutes to complete the planning activity, whichincluded choosing a setting, main characters,props, and deciding the beginning, middle, and endof their story.
Once planning was completed, ortime ran out, the students were given anotherthirty-five minutes to write their fable.
After theirfable was completed, the students were asked tocomplete a post-experiment questionnaire.
Thissurvey was also allotted twenty minutes for com-pletion.
In total, the study lasted ninety minutes.4 FindingsThree categories of analyses were performed onthe NARRATIVE THEATRE corpus: an analysis ofnatural language processing tool performance(specifically, an analysis of syntactic parsers), ananalysis of coherence and cohesion in the writtentexts using the automated cohesion metric toolCoh-Metrix (Graesser et al, 2004), and an analysisof students?
writing processes.As part of an investigation of students?
individ-ual differences in writing, students?
language artsskills were measured by their scores from the prioryear?s End-of-Grade reading test.
Subjective rat-ings of writing ability were also obtained for eachstudent from their teachers.
The reading scoreswere used in the presented analyses because theywere obtained through systematic testing, but it isinteresting to note that the objective reading scoresand subjective writing scores were found to bestrongly correlated by calculating the Spearman?scorrelation coefficient2, rho = .798, p < .0001.
Thehigh correlation suggests that reading scores canserve as a reasonable indicator of language artsskills.2 Spearman?s correlation coefficient was used because of theordinal nature of the reading and writing measures (Myers andWell, 2003).594.1 Natural Language ProcessingTwo syntactic parsing tools were used to analyzestudents?
fables and develop an initial account ofthe performance of current natural language proc-essing tools on a corpus of novice-generated narra-tive texts.
The Link Grammar Parser (Temperley,1995) and Stanford Parser (Klein and Manning,2003) were run on the entire corpus, and their per-formance recorded.Link parsing provides insight into the number ofgrammatically malformed sentences observed ineach fable.
Link grammars center on the notion oflinkable entities directly combined with one an-other, as opposed to tree-like structures.
Link pars-ers attempt to identify one or more syntacticallyvalid representations, where each entity is pairedwith another.
Passages were split into sentencesusing OpenNLP, and then run against the LinkGrammar Parser (Temperley, 1995).
If a sentencehad no suitable link based on the parser (e.g., ?Lastdog I saw a great movie?
), it was considered ?bro-ken?
because it lacked an appropriate linkage.
Aratio of sentences without appropriate linkage tototal sentence count was used to characterize thelink parser?s performance on each student?s fable.On average, the Link Grammar Parser foundlinkages for 41% of sentences (SD=.22).
Interest-ingly, reading level was shown to have a marginaleffect on the link parser?s success rate,F(2,110) = 5.78, p = .06.
Post hoc Tukey?s testsrevealed that above-grade level readers were mar-ginally more likely to write linkable sentences thanat-grade level readers, p = .07.
The effect wasstrongly significant between above-grade levelreaders and below-grade level readers, p = .003.The Stanford parser was used to investigate thefrequency with which sentences could be success-fully parsed.
A parsing failure was noted any timethe tool was forced to fall back to a PCFG parse.On average, the Stanford parser produced a parsefor 91% of students?
sentences.
A significant ef-fect of reading grade-level on Stanford parser suc-cess rate was observed, F(2,110) = 4.41, p = .015.Post hoc tests showed that above-grade level read-ers wrote significantly more sentences that couldbe parsed than below-grade level readers, p = .02.There was also a marginal difference observedbetween below-grade level readers and at-gradelevel readers, p = .08.Gender was not found to have an effect on thepercentage of linkable sentences, nor the numberof Stanford parser failures.4.2 Individual Differences and Written TextsSeveral analyses were conducted to investigateindividual differences in students?
written texts.Analyses focused on writing length, cohesioncharacteristics, coherence characteristics, andspelling errors.
Fable lengths were measured incharacters (M = 1346, SD = 601).A marginal effect of reading grade-level on fa-ble length was observed, F(2,110) = 2.89, p = .06.Post hoc tests showed that at-grade level readerstended to write longer fables than below-gradelevel readers, p = .10.
Gender was also found tohave a significant effect on writing length.
Spe-cifically, females tended to write longer fables thanmales, F(1,110) = 4.41, p = .04.Table 1.
The effects of reading grade-level on select Coh-Metrix features.
* denotes p < .1 and ** denotes p < .05Coh-Metrix feature Below-Grade  At-Grade  Above-Grade  F(2, 110) =Hypernym, nouns 5.9 (0.93)  6.17 (0.81)  6.53 (0.43)  1.24 Below-Above**Hypernym, verbs 1.44 (0.18)  1.49 (0.18)  1.48 (0.17)  3.72Causal cohesion 0.83 (0.09)  0.87 (0.1)  0.39 (0.16)  3.70 Below-Above**At-Above**LSA, paragraph to paragraph 0.34 (0.19)  0.45 (0.22)  0.49 (0.18)  3.89 Below-Above**Below-At*LSA, sentence to sentence 0.21 (0.14)  0.24 (0.11)  0.22 (0.09)  0.48Personal pronoun usage 107 (35.88)  101 (29.98)  89 (19.37)  2.25 Below-Above*Pronoun to noun phrase ratio 0.36 (0.12)  0.35 (0.10)  0.30 (0.06)  2.2160To investigate cohesion and coherence in stu-dents?
fables, the corpus was analyzed with Coh-Metrix, a tool for analyzing the cohesion,language, and readability of texts (Graesser et al,2004).
At the core of Coh-Metrix is a lexical ana-lyzer, syntactic parser, part-of-speech tagger, andreference corpora (for LSA) that processes text andreturns linguistic and discourse related features.Coh-Metrix measures several types of cohesion, aswell as concreteness, connectives, diversity in lan-guage, and syntactic complexity.
Concreteness ismeasured using the hypernym depth values re-trieved from the WordNet lexical taxonomy, andaveraged across noun and verb categories.Results from an analysis of reading grade-levelsand Coh-Metrix features are presented in Table 1.Interestingly, above-grade level students were ob-served to have lower causal cohesion scores thanat-grade level or below-grade level students.
Theconverse is found in an examination of paragraph-to-paragraph LSA scores, which are often used tomeasure semantic cohesion.
Below-grade levelreaders tended to have lower semantic cohesionscores than at-grade level readers.
LSA scores onadjacent sentences and all combinations of sen-tences were not significant across any of thegroups.
Sentence-to-sentence LSA scores werealso not significant across groups.Gender did not have a significant effect oncausal cohesion, hypernym depth of verbs, orparagraph-to-paragraph LSA values.
However,gender was found to have a significant effect onhypernym depth of nouns, F(1,110) = 15.96,p = .0001.
Males tended to use more concretenouns in their writing passages, with an averagedifference of .6 in hypernym depth.
The ratio ofpronouns to noun phrases was also significant be-tween genders, F(1,110) = 10.19, p = .002.
Fe-males had a 38% pronoun to NP ratio whereasmales were at 32%.
Gender had a significant ef-fect on sentence-to-sentence LSA scores, F(1,110)= 19.9, p = .0001.
Males tended to have a higherLSA score across adjacent sentences (M = .27, SD= .11) than females (M = .18, SD = .1).
Finally,gender had a significant effect on personal pronounincidence score, F(1,110) = 9.12, p = .003.
Fe-males used personal pronouns as 11.1% of theircontent whereas males used them as 9.3% of theircontent.An examination of the number of spelling errorsremaining in student fables, as well as students?usage of the built-in spelling corrector, was con-ducted.
However, no significant effects were ob-served across reading level or gender.4.3 Individual Differences and WritingProcessesSeveral features in the student interaction logswere chosen to investigate key aspects of students?writing processes.
Specifically, these featuresinclude planning length, planning and writing time,revision behavior, pauses in text production, andreviews of prior planning decisions.On average, students spent 665 seconds plan-ning their fables and 2199 seconds writing theirfables (SD = 535).
Students also typed 537 charac-ters on average while planning their fables (SD =254).
No significant effect of reading level wasobserved on planning length, but reading level didhave a significant effect on time spent in the plan-ning phase, F(2,110) = 12.76, p < .0001.
Below-grade level readers spent significantly more timeTable 2.
The effects of reading grade-level on writing process characteristics.
* denotes p < .1, ** denotes p < .05, and *** denotes p < .01.Writing process feature Below-Grade  At-Grade  Above-Grade  F(2, 110) =Avg length of deletion, planning 21.30 (8.31)  28.18 (11.14)  30.99 (12.37)  8.34 Below-Above***Below-At***Avg length of deletion, writing 23.42 (9.26)  27.74 (10.28)  33.06 (16.80)  5.18 Below-Above***Mouseovers/min 0.19 (0.15)  0.09 (0.07)  0.07 (0.05)  11.81 Below-Above***Below-At***5+ second revision count, planning 9.14 (6.62)  5.43 (4.43)  2.37 (2.36)  12.70 Below-Above***Below-At***At-Above*5+ second revision count, writing 18.04 (7.84)  14.21 (7.81)  13.37 (7.52)  3.83 Below-Above*Below-At*61on planning than at-grade level readers, p = .001,as well as above-grade level readers, p < .0001.There were also significant differences in writingtime across reading level groups, F(2, 110) = 6.47,p = .002.
Below-grade level readers took signifi-cantly more time composing their fables than at-grade level readers, p = .05.
Also, below-gradelevel readers took significantly more time to writetheir fables than above-grade level readers,p = .003.Females tended to write longer passages in theplanning section than males F(1,110) = 4.68,p = .03.
Time spent on the planning section waslower among females than males, F(1,110) = 3.92,p = .05.
Females also spent less time on the writ-ing section than males, F(1,110) = 3.87, p = .05.Students?
revision behaviors were gauged usinga heuristic that measures edit distances betweensuccessive snapshots of fables collected at one-minute intervals during composition.
Each minute,a static snapshot of student?s fable progress wastaken and logged.
Edit distances between succes-sive snapshots of students?
fables were measuredusing the Google Diff, Match and Patch tools tomake ?before?
and ?after?
comparisons (Google,2009).
Comparing two successive snapshots of asingle fable, a revision was defined as any inser-tion of text that occurred before the tail end of thefable.The effects of reading level on revision in boththe planning and writing stages is presented inTable 2.
During the writing stage, a significanteffect of grade-level was observed on average revi-sion length between below-grade level readers andat-grade level readers, as well as between below-grade level readers and above-grade level readers.Within the planning section, at-grade level readersrevised more text than below-grade level readers,and above-grade level readers revised more textthan at-grade level readers.
Self-efficacy for writ-ing was found to be significantly correlated withaverage revision length in both the planning, r =.21, p = .03, and writing, r = .31, p = .001, stages.Pauses between successive keystrokes wereinvestigated during both the planning and writingstages of NARRATIVE THEATRE interactions.
Forthe purpose of this work, a pause is defined as akeystroke made five or more seconds after thepreceding keystroke.
Keystroke pauses were cate-gorized as either an appendage or a revision, de-pending on whether they occurred before the tailend of the passage (revision) or after the tail end(appendage).
For the planning section, below-grade readers paused significantly more often thanat-grade readers.
Also, at-grade readers pausedbefore revising significantly more often thanabove-grade readers.
The effects of reading levelon a number of writing process subscores areshown in Table 2.Gender had a significant effect on pauses priorto revision in the writing phase, F(1,110) = 3.26,p = .07.
Females paused on more occasions thanmales.
However, no gender effect was found forpause behavior during the planning phase.During the planning and writing stages ofNARRATIVE THEATRE interactions, students couldreview their prior planning selections?includingcharacters, objects, and settings?by hovering themouse over the respective region near the top ofthe screen (mouseover).
Upon hovering the mouseover the appropriate region, a graphical illustrationof the student?s planning selection was presented.Mouseover instances were recorded to obtain in-sight into idea generation, or instances where thestudent was contemplating what to write next.Mouseovers were calculated in terms of averagemouseovers over time (in minutes).
The effects ofreading ability on mouseover behaviors are shownin Table 2.For the mouseover metric, reading level had asignificant effect on the mouseover rate.
Below-grade level learners tended to use the mouseoverfeature on a more frequent basis than both at-gradelevel and above-grade level readers.
There was nota significant difference between at-grade level andabove-grade level groups.The effect of gender on mouseover rate was sig-nificant, F(1,110) = 9.93, p = .002.
Males used themouseover feature on fewer occasions than fe-males.5 DiscussionThe performance of the two parsers differedwidely.
The Stanford Parser was able to parse over90% of fables, but the Link Grammar Parser wasonly successful for about 40% of the fables.
Whileparser failure is not always indicative of poorgrammaticality, every sentence that failed on theStanford Parser contained either misspelled wordsor run-on sentences.
Many of these were indicatedby errors in the sentence segmentation as well.62There were also indications that students?
languagearts skills may influence the grammaticality oftheir written sentences; significant effects of read-ing level were found on both the Stanford Parser?ssuccess rate and the Link parser?s success rate.The fact that below-grade level students consti-tuted a considerable proportion of the study?s stu-dent population suggests that pedagogical writingsupport tools should be capable of handling thevariations inherent in students?
writings, and lever-age natural language processing results to informtutorial feedback.Paragraph-to-paragraph LSA scores tended toincrease with reading level.
This has implicationsfor semantic cohesion (Graessar, 2004) and indi-cates that students with a higher reading assess-ment score produce stories that satisfy thisparticular dimension of cohesion.
However, theconverse was true for the Coh-Metrix measure ofcausal cohesion, where above-grade level studentsactually produced the lowest cohesion scores.
Onepossible explanation could stem from differencesin vocabulary skills between above- and below-grade level students; students who exercise a largervocabulary may be penalized by Coh-Metrix?scohesion metric.
Alternatively, the result may berelated to the fact that below-grade level studentstend to produce less text (Graessar, 2004).
Clearly,students?
individual differences in language artsability affect the cohesiveness of the texts theywrite, but additional investigation is necessary todevelop a clear understanding of the relationshipbetween cohesion and language arts ability, as wellas the implications for tailoring tutoring.With regard to the writing process, the averagelength per revision was significantly greater forstudents of higher reading skill-levels.
There is apossibility that this may be associated with moreelaborate revision processes, which requires furtherinvestigation.
It should be noted that the revisionfinding was more salient for the planning stage ofNARRATIVE THEATRE interactions.
This resultmay also indicate that below-grade level readerswere somewhat less thorough when planning theirfables.
Further, differences in mouseover behaviorwere found across reading levels, apparently indi-cating a decline in the rate of mouseovers as read-ing level increased.
This finding may be the resultof below-grade level students experiencing diffi-culties in idea generation, or a lack of motivation.Finally, the number of pauses prior to revision wasfound to decrease as reading level increased.
Thisresult may point to difficulties with text productionfor lower language arts skill students.
Difficultytranslating ideas into text may point to a need forintelligent writing tutors to help reduce lower read-ing level students?
cognitive load during writing.6 Conclusions and Future WorkWe have presented a study conducted with middlegrade students to investigate the process and prod-ucts of writing in a narrative composition supportenvironment.
The study found significant varia-tions in syntactic parser performance associatedwith students?
language arts abilities, as well asrelationships between students?
reading level andthe grammaticality of their writing.
For example,the stories of below-grade readers had a lowerlevel of semantic cohesion than at-grade levelreaders, but surprisingly, above-grade level stu-dents?
writings exhibited lower causal cohesionthan both at-grade and below-grade level students.Reading level had a significant effect on time spentin the planning phase, and below-grade level read-ers spent more time composing fables than at-grade level readers.
There were also gender differ-ences, with females spending less time in both theplanning and writing phases.
There were also dif-ferences with respect to revision, with above-gradereaders revising more than below-grade readers.The study highlights important issues about howto design composition support tools.
Compositionsupport tools that are sensitive to students?
indi-vidual writing abilities seem likely to be most ef-fective.
Natural language processing is critical foranalyzing students?
texts and informing the contentof adaptive tutorial feedback.
Intelligent writingtutors should utilize natural language processingtechniques that can robustly handle the variationsin students?
writings, and deliver tailored scaffold-ing informed by analyses of students?
texts andwriting processes.The findings suggest that several directions existfor future work.
Additional analysis is necessaryto investigate the correctness of syntactic parses.Further investigation of students?
individual differ-ences in writing at the discourse and narrative lev-els is also necessary.
Results from these analysesshould then be used to inform the design of tech-niques for adaptive tutorial feedback in narrativecomposition support environments.63AcknowledgementsThe authors wish to thank members of the NorthCarolina State University IntelliMedia Group fortheir assistance with the NARRATIVE THEATRE.This research was supported by the National Sci-ence Foundation under Grant IIS-0757535.
Anyopinions, findings, and conclusions or recommen-dations expressed in this material are those of theauthors and do not necessarily reflect the views ofthe National Science Foundation.ReferencesC.
Bereiter and M. Scardamalia.
1987.
The Psychologyof Written Composition.
Lawrence Erlbaum Associ-ates, Hillsdale, NJ.V.
W. Berninger, K. Vaughan, R. D. Abbott, K. Begay,K.
B. Coleman, G. Curtain, J. M. Hawkins, and S.Graham.
2002.
Teaching spelling and composition a-lone and together: Implications for the simple viewof writing.
Journal of Educational Psychology,94(2):291?304.M.
A. Britt, P. Wiemer-Hasting, A. Larson, and C. Per-fetti.
2004.
Automated feedback on source citation inessay writing.
International Journal of Artificial In-telligence in Education, 14(3?4):359?374.C.
A. Cameron and B. Moshenko.
1996.
Elicitation ofknowledge transformational reports while childrenwrite narratives.
Canadian Journal of BehaviouralScience, 28(4):271?280.L.
J. C. DeGroff.
1987.
The influence of priorknowledge on writing, conferencing, and revising.Elementary School Journal, 88(2):105?118.L.
Flower and J. Hayes.
1981.
A cognitive processtheory of writing.
College Composition and Commu-nication, 32(4):365?387.D.
Galbraith, J. Hallam, T. Olive, and N. Le Bigot.2009.
The role of different components of workingmemory in writing.
In Proceedings of Annual Con-ference of the Cognitive Science Society, Amsterdam,The Netherlands.M.
Gamon, J. Gao, C. Brockett, A. Klementiev, W. B.Dolan, D. Belenko, and L. Vanderwende.
2008.
Us-ing contextual speller techniques and languagemodeling for ESL error correction.
In Proceedings ofthe International Joint Conference on Natural Lan-guage Processing, pages 449?456, Hyderabad, India.Google Diff Match and Patch [Software].
Availablefrom http://code.google.com/p/google-diff-match-patch/A.
C. Graesser, D. S. McNamara, M. M. Louwerse, andZ.
Cai.
2004.
Coh-Metrix: Analysis of text on cohe-sion and language.
Behavior Research Methods In-struments and Computers, 36(2):193?202.S.
Graham.
2006.
Strategy instruction and the teachingof writing: A meta-analysis.
In C. A. MacArthur, S.Graham, and J. Fitzgerald, editors, Handbook of writ-ing research.
Guilford Press, New York, NY, pages187?207.S.
Graham, K. R. Harris, and B. F. Chorzempa.
2002.Contribution of spelling instruction to the spelling,writing, and reading of poor spellers.
Journal of Edu-cational Psychology, 94(4):669?686.J.
Hayes.
1996.
A new framework for understandingcognition and affect in writing.
In C. M. Levy and S.Ransdell, editors, The Science of Writing: Theories,Methods, Individual Differences, and Applications.Lawrence Erbaum Associates, Mahwah, NJ, pages 1?28.M.
Heilman, K. Collins-Thompson, J. Callan, and M.Eskenazi.
2007.
Combining lexical and grammticalfeatures to improve readability measures for first andsecond language texts.
In Proceedings of HumanLanguage Technology Conference, pages 460?467,Rochester, NY.D.
Higgins, J. Bustein, D. Marcu, and C. Gentile.
2004.Evaluating multiple aspects of coherence in studentessays.
In Proceedings of Human Language Tech-nology conference/North American chapter of theAssociation for Computational Linguistics, pages185?192, Boston, MA.J.
A. Langer.
1985.
Children's sense of genre: A studyof performance on parallel reading and writingTasks.
Written Communication, 2(2):157?187.C.
Mahlow and M. Piotrowski.
2009.
LingURed: Lan-guage-aware editing functions based on NLP re-sources.
In Proceedings of InternationalMulticonference on Computer Science and Informa-tion Technology, pages 243?250, Mragowo, Poland.D.
McCutchen, M. Francis, and S. Kerr.
1997.
Revisingfor meaning: Effects of knowledge and strategy.Journal of Educational Psychology, 89(4):667?676.J.
Mostow and G. Aist.
2001.
Evaluating tutors thatlisten: an overview of project LISTEN.
In K. Forbusand P. Feltovich, editors, Smart Machines in Educa-tion.
MIT Press, Cambridge, MA, USA, pages 169?234.J.
Myers and A.
Well.
2003.
Research Design and Sta-tistical Analysis.
Erlbaum, Mahwah, NJ.K.
VanLehn.
2006.
The behavior of tutoring systems.International Journal of Artificial Intelligence inEducation, 16(3):227?265.J.
Wagner, J.
Foster, and J.
Van Genabith.
2007.
Acomparative evaluation of deep and shallowapproaches to the automatic detection of commongrammatical errors.
In Proceedings of 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 112?121, Prague, CzechRepublic.64
