Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 19?26,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAssessing the benefits of partial automatic pre-labeling for frame-semanticannotationInes Rehbein and Josef Ruppenhofer and Caroline SporlederComputational LinguisticsSaarland University{rehbein,josefr,csporled}@coli.uni-sb.deAbstractIn this paper, we present the results of anexperiment in which we assess the useful-ness of partial semi-automatic annotationfor frame labeling.
While we found no con-clusive evidence that it can speed up humanannotation, automatic pre-annotation doesincrease its overall quality.1 IntroductionLinguistically annotated resources play a crucialrole in natural language processing.
Many recentadvances in areas such as part-of-speech tagging,parsing, co-reference resolution, and semantic rolelabeling have only been possible because of the cre-ation of manually annotated corpora, which thenserve as training data for machine-learning basedNLP tools.
However, human annotation of linguis-tic categories is time-consuming and expensive.While this is already a problem for major languageslike English, it is an even bigger problem for less-used languages.This data acquisition bottleneck is a well-knownproblem and there have been numerous efforts toaddress it on the algorithmic side.
Examples in-clude the development of weakly supervised learn-ing methods such as co-training and active learning.However, addressing only the algorithmic side isnot always possible and not always desirable in allscenarios.
First, some machine learning solutionsare not as generally applicable or widely re-usableas one might think.
It has been shown, for example,that co-training does not work well for problemswhich cannot easily be factorized into two indepen-dent views (Mueller et al, 2002; Ng and Cardie,2003).
Some active learning studies suggest boththat the utility of the selected examples stronglydepends on the model used for classification andthat the example pool selected for one model canturn out to be sub-optimal when another model istrained on it at a later stage (Baldridge and Os-borne, 2004).
Furthermore, there are a number ofscenarios for which there is simply no alternativeto high-quality, manually annotated data; for exam-ple, if the annotated corpus is used for empiricalresearch in linguistics (Meurers and Mu?ller, 2007;Meurers, 2005).In this paper, we look at this problem from thedata creation side.
Specifically we explore whethera semi-automatic annotation set-up in which a hu-man expert corrects the output of an automatic sys-tem can help to speed up the annotation processwithout sacrificing annotation quality.For our study, we explore the task of frame-semantic argument structure annotation (Baker etal., 1998).
We chose this particular task because itis a rather complex ?
and therefore time-consuming?
undertaking, and it involves making a number ofdifferent but interdependent annotation decisionsfor each instance to be labeled (e.g.
frame as-signment and labeling of frame elements, see Sec-tion 3.1).
Semi-automatic support would thus be ofreal benefit.More specifically, we explore the usefulness ofautomatic pre-annotation for the first step in the an-notation process, namely frame assignment (wordsense disambiguation).
Since the available inven-tory of frame elements is dependent on the cho-sen frame, this step is crucial for the whole anno-tation process.
Furthermore, semi-automatic an-notation is more feasible for the frame labelingsub-task.
Most automatic semantic role labelingsystems (ASRL), including ours, tend to performmuch better on frame assignment than on framerole labeling and correcting an erroneously chosen19frame typically also requires fewer physical opera-tions from the annotator than correcting a numberof wrongly assigned frame elements.We aim to answer three research questions in ourstudy: First, we explore whether pre-annotation offrame labels can indeed speed up the annotationprocess.
This question is important because frameassignment, in terms of physical operations of theannotator, is a relatively minor effort compared toframe role assignment and because checking a pre-annotated frame still involves all the usual men-tal operations that annotation from scratch does.Our second major question is whether annotationquality would remain acceptably high.
Here theconcern is that annotators might tend to simply goalong with the pre-annotation, which would lead toan overall lower annotation quality than they couldproduce by annotating from scratch.1 Dependingon the purpose for which the annotations are to beused, trading off accuracy for speed may or maynot be acceptable.
Our third research question con-cerns the required quality of pre-annotation for itto have any positive effect.
If the quality is too low,the annotation process might actually be sloweddown because annotations by the automatic systemwould have to be deleted before the new correctone could be made.
In fact, annotators might ig-nore the pre-annotations completely.
To determinethe effect of the pre-annotation quality, we not onlycompared a null condition of providing no priorannotation to one where we did, but we in fact com-pared the null condition to two different qualitylevels of pre-annotation, one that reflects the per-formance of a state-of-the-art ASRL system andan enhanced one that we artificially produced fromthe gold standard.2 Related WorkWhile semi-automatic annotation is frequently em-ployed to create labeled data more quickly (see,e.g., Brants and Plaehn (2000)), there are compar-atively few studies which systematically look atthe benefits or limitations of this approach.
Oneof the earliest studies that investigated the advan-tages of manually correcting automatic annotationsfor linguistic data was carried out by Marcus etal.
(1993) in the context of the construction of thePenn Treebank.
Marcus et al (1993) employed1This problem is also known in the context of resourcesthat are collaboratively constructed via the web (Kruschwitzet al, 2009)a post-correction set-up for both part-of-speechand syntactic structure annotation.
For pos-taggingthey compared the semi-automatic approach to afully manual annotation.
They found that the semi-automatic method resulted both in a significantreduction of annotation time, effectively doublingthe word annotation rate, and in increased inter-annotator agreement and accuracy.Chiou et al (2001) explored the effect of au-tomatic pre-annotation for treebank construction.For the automatic step, they experimented with twodifferent parsers and found that both reduce over-all annotation time significantly while preservingaccuracy.
Later experiments by Xue et al (2002)confirmed these findings.Ganchev et al (2007) looked at semi-automaticgene identification in the biomedical domain.
They,too, experimented with correcting the output of anautomatic annotation system.
However, rather thanemploying an off-the-shelf named entity tagger,they trained a tagger maximized for recall.
Thehuman annotators were then instructed to filter theannotation, rejecting falsely labeled expressions.Ganchev et al (2007) report a noticeable increasein speed compared to a fully manual set-up.The approach that is closest to ours is that ofChou et al (2006) who investigate the effect of au-tomatic pre-annotation for Propbank-style semanticargument structure labeling.
However that studyonly looks into the properties of the semi-automaticset-up; the authors did not carry out a control studywith a fully manual approach.
Nevertheless Chouet al (2006) provide an upper bound of the savingsobtained by the semi-automatic process in termsof annotator operations.
They report a reduction inannotation effort of up to 46%.3 Experimental setup3.1 Frame-Semantic AnnotationThe annotation scheme we use is that of FrameNet(FN), a lexicographic project that produces adatabase of frame-semantic descriptions of Englishvocabulary.
Frames are representations of proto-typical events or states and their participants in thesense of Fillmore (1982).
In the FN database, bothframes and their participant roles are arranged invarious hierarchical relations (most prominently,the is-a relation).FrameNet links these descriptions of frames withthe words and multi-words (lexical units, LUs) thatevoke these conceptual structures.
It also docu-20ments all the ways in which the semantic roles(frame elements, FEs) can be realized as syntacticarguments of each frame-evoking word by labelingcorpus attestations.
As a small example, considerthe Collaboration frame, evoked in English by lexi-cal units such as collaborate.v, conspire.v, collabo-rator.n and others.
The core set of frame-specificroles that apply include Partner1, Partner2, Partnersand Undertaking.
A labeled example sentence is(1) [The two researchers Partners] COLLAB-ORATED [on many papers Undertaking].FrameNet uses two modes of annotation: full-text, where the goal is to exhaustively annotatethe running text of a document with all the differ-ent frames and roles that occur, and lexicographic,where only instances of particular target words usedin particular frames are labeled.3.2 Pilot StudyPrior to the present study we carried out a pilotexperiment comparing manual and semi-automaticannotation of different segments of running text.In this experiment we saw no significant effectfrom pre-annotation.
Instead we found that theannotation speed and accuracy depended largelyon the order in which the texts were annotated andon the difficulty of the segments.
The influenceof order is due to the fact that FrameNet has morethan 825 frames and each frame has around two tofive core frame elements plus a number of non-coreelements.
Therefore even experienced annotatorscan benefit from the re-occuring of frames duringthe ongoing annotation process.Drawing on our experiences with the first exper-iment, we chose a different experimental set-up forthe present study.
To reduce the training effect, weopted for annotation in lexicographic mode, restrict-ing the number of lemmas (and thereby frames)to annotate, and we started the experiment witha training phase (see Section 3.5).
Annotating inlexicographic mode also gave us better control overthe difficulty of the different batches of data.
Sincethese now consist of unrelated sentences, we cancontrol the distribution of lemmas across the seg-ments (see Section 3.4).Furthermore, since the annotators in our pi-lot study had often ignored the error-prone pre-annotation, in particular for frame elements, we de-cided not to pre-annotate frame elements and to ex-periment with an enhanced level of pre-annotationto explore the effect of pre-annotation quality.3.3 Annotation Set-UpThe annotators included the authors and three com-putational linguistics undergraduates who havebeen performing frame-semantic annotation for atleast one year.
While we use FrameNet data, ourannotation set-up is different.
The annotation con-sists of decorating automatically derived syntacticconstituency trees with semantic role labels usingthe Salto tool (Burchardt et al, 2006) (see Figure 1).By contrast, in FrameNet annotation a chunk parseris used to provide phrase type and grammatical rela-tions for the arguments of the target words.
Further,FrameNet annotators need to correct mistakes ofthe automatic grammatical analysis, unlike in ourexperiment.
The first annotation step, frame as-signment, involves choosing the correct frame forthe target lemma from a pull down menu; the sec-ond step, role assignment, requires the annotatorsto draw the available frame element links to theappropriate syntactic constituent(s).The annotators performed their annotation oncomputers where access to the FrameNet website,where gold annotations could have been found, wasblocked.
They did, however, have access to localcopies of the frame descriptions needed for thelexical units in our experiment.
As the overall timeneeded for the annotation was too long to do inone sitting, the annotators did it over several days.They were instructed to record the time (in minutes)that they took for the annotation of each annotationsession.Our ASRL system for state-of-the-art pre-annotation was Shalmaneser (Erk and Pado, 2006).The enhanced pre-annotation was created by man-ually inserting errors into the gold standard.3.4 DataWe annotated 360 sentences exemplifying all thesenses that were defined for six different lemmas inFrameNet release 1.3.
The lemmas were the verbsrush, look, follow, throw, feel and scream.
Theseverbs were chosen for three reasons.
First, theyhave enough annotated instances in the FN releasethat we could use some instances for testing andstill be left with a set of instances sufficiently largeto train our ASRL system.
Second,we knew fromprior work with our automatic role labeler that ithad a reasonably good performance on these lem-mas.
Third, these LUs exhibit a range of difficultyin terms of the number of senses they have in FN(see Table 1) and the subtlety of the sense distinc-21Figure 1: The Salto Annotation ToolInstances Sensesfeel 134 6follow 113 3look 185 4rush 168 2scream 148 2throw 155 2Table 1: Lemmas usedtions ?
e.g.
the FrameNet senses of look are harderto distinguish than those of rush.
We randomlygrouped our sentences into three batches of equalsize and for each batch we produced three versionscorresponding to our three levels of annotation.3.5 Study designIn line with the research questions that we wantto address and the annotators that we have avail-able, we choose an experimental design that isamenable to an analysis of variance.
Specifically,we randomly assign our 6 annotators (1-6) to threegroups of two (Groups I-III).
Each annotator expe-riences all three annotation conditions, namely nopre-annotation (N), state-of-the-art pre-annotation(S), and enhanced pre-annotation (E).
This is thewithin-subjects factor in our design, all other fac-tors are between subjects.
Namely, each group wasrandomly matched to one of three different ordersin which the conditions can be experienced (seeTable 2).
The orderings are designed to controlfor the effects that increasing experience may haveon speed and quality.
While all annotators end uplabeling all the same data, the groups also differas to which batch of data is presented in whichcondition.
This is intended as a check on any inher-1st 2nd 3rd AnnotatorsGroup I E S N 5, 6Group II S N E 2, 4Group III N E S 1, 3Table 2: Annotation condition by order and groupent differences in annotation difficulty that mightexist between the data sets.
Finally, to rule outdifficulties with unfamiliar frames and frame el-ements needed for the lexical units used in thisstudy, we provided some training to the annota-tors.
In the week prior to the experiment, they weregiven 240 sentences exemplifying all 6 verbs in alltheir senses to annotate and then met to discuss anyquestions they might have about frame or FE dis-tinctions etc.
These 240 sentences were also usedto train the ASRL system.4 ResultsIn addition to time, we measured precision, recalland f-score for frame assignment and semantic roleassignment for each annotator.
We then performedan analysis of variance (ANOVA) on the outcomesof our experiment.
Our basic results are presentedin Table 3.
As can be seen and as we expected,our annotators differed in their performance bothwith regard to annotation quality and speed.
Belowwe discuss our results with respect to the researchquestions named above.4.1 Can pre-annotation of frame assignmentspeed up the annotation process?Not surprisingly, there are considerable differencesin speed between the six annotators (Table 3),22Precision Recall F t pAnnotator 194/103 91.3 94/109 86.2 88.68 75 N99/107 92.5 99/112 88.4 90.40 61 E105/111 94.6 105/109 96.3 95.44 65 SAnnotator 293/105 88.6 93/112 83.0 85.71 135 S86/98 87.8 86/112 76.8 81.93 103 N98/106 92.5 98/113 86.7 89.51 69 EAnnotator 395/107 88.8 95/112 84.8 86.75 168 N103/110 93.6 103/112 92.0 92.79 94 E99/113 87.6 99/113 87.6 87.60 117 SAnnotator 4106/111 95.5 106/112 94.6 95.05 80 S99/108 91.7 99/113 87.6 89.60 59 N105/112 93.8 105/113 92.9 93.35 52 EAnnotator 5104/110 94.5 (104/112) 92.9 93.69 170 E91/103 88.3 (91/113) 80.5 84.22 105 S96/100 96.0 (96/113) 85.0 90.17 105 NAnnotator 6102/106 96.2 102/112 91.1 93.58 124 E94/105 89.5 94/112 83.9 86.61 125 S93/100 93.0 93/113 82.3 87.32 135 NTable 3: Results for frame assignment: precision,recall, f-score (F), time (t) (frame and role as-signment), pre-annotation (p): Non, Enhanced,Shalmaneserwhich are statistically significant with p ?
0.05.Focussing on the order in which the text segmentswere given to the annotators, we observe a sig-nificant difference (p ?
0.05) in annotation timeneeded for each of the segments.
With one ex-ception, all annotators took the most time on thetext segment given to them first, which hints at anongoing training effect.The different conditions of pre-annotation (none,state-of-the-art, enhanced) did not have a signifi-cant effect on annotation time.
However, all anno-tators except one were in fact faster under the en-hanced condition than under the unannotated con-dition.
The one annotator who was not faster anno-tated the segment with the enhanced pre-annotationbefore the other two segments; hence there mighthave been an interaction between time savings frompre-annotation and time savings due to a trainingeffect.
This interaction between training effect anddegree of pre-annotation might be one reason whywe do not find a significant effect between anno-tation time and pre-annotation condition.
Anotherreason might be that the pre-annotation only re-duces the physical effort needed to annotate thecorrect frame which is relatively minor comparedto the cognitive effort of determining (or verifying)the right frame, which is required for all degrees ofpre-annotation.4.2 Is annotation quality influenced byautomatic pre-annotation?To answer the second question, we looked at therelation between pre-annotation condition and f-score.
Even though the results in f-score for thedifferent annotators vary in extent (Table 4), there isno significant difference between annotation qual-ity for the six annotators.Anot1 Anot2 Anot3 Anot4 Anot5 Anot691.5 85.7 89.0 92.7 89.4 89.2Table 4: Average f-score for the 6 annotatorsNext we performed a two-way ANOVA (Within-Subjects design), and crossed the dependent vari-able (f-score) with the two independent vari-ables (order of text segments, condition of pre-annotation).
Here we found a significant effect(p ?
0.05) for the impact of pre-annotation on an-notation quality.
All annotators achieved higherf-scores for frame assignment on the enhanced pre-annotated text segments than on the ones with nopre-annotation.
With one exception, all annotatorsalso improved on the already high baseline for theenhanced pre-annotation (Table 5).Seg.
Precision Recall f-scoreShalmaneserA (70/112) 62.5 (70/96) 72.9 67.30B (75/113) 66.4 (75/101) 74.3 70.13C (66/113) 58.4 (66/98) 67.3 62.53Enhanced Pre-AnnotationA (104/112) 92.9 (104/111) 93.7 93.30B (103/112) 92.0 (103/112) 92.0 92.00C (99/113) 87.6 (99/113) 87.6 87.60Table 5: Baselines for automatic pre-annotation(Shalmaneser) and enhanced pre-annotationThe next issue concerns the question of whetherannotators make different types of errors when pro-vided with the different styles of pre-annotation.We would like to know if erroneous frame assign-ment, as done by a state-of-the-art ASRL will temptannotators to accept errors they would not make inthe first place.
To investigate this issue, we com-pared f-scores for each of the frames for all threepre-annotation conditions with f-scores for frameassignment achieved by Shalmaneser.
The boxplotin Figure 2 shows the distribution of f-scores foreach frame for the different pre-annotation stylesand for Shalmaneser.
We can see that the same23Figure 2: F-Scores per frame for human annotators on different levels of pre-annotation and for Shal-manesererror types are made by human annotators through-out all three annotation trials, and that these errorsare different from the ones made by the ASRL.Indicated by f-score, the most difficult framesin our data set are Scrutiny, Fluidic motion, Seek-ing, Make noise and Communication noise.
Thisshows that automatic pre-annotation, even if noisyand of low quality, does not corrupt human anno-tators on a grand scale.
Furthermore, if the pre-annotation is good it can even improve the overallannotation quality.
This is in line with previousstudies for other annotation tasks (Marcus et al,1993).4.3 How good does pre-annotation need to beto have a positive effect?Comparing annotation quality on the automaticallypre-annotated texts using Shalmaneser, four out ofsix annotators achieved a higher f-score than on thenon-annotated sentences.
The effect, however, isnot statistically significant.
This means that pre-annotation produced by a state-of-the-art ASRLsystem is not yet good enough a) to significantlyspeed up the annotation process, and b) to improvethe quality of the annotation itself.
On the positiveside, we also found no evidence that the error-pronepre-annotation decreases annotation quality.Most interestingly, the two annotators whoshowed a decrease in f-score on the text segmentspre-annotated by Shalmaneser (compared to thetext segments with no pre-annotation provided)had been assigned to the same group (Group I).Both had first annotated the enhanced, high-qualitypre-annotation, in the second trial the sentencespre-annotated by Shalmaneser, and finally the textswith no pre-annotation.
It might be possible thatthey benefitted from the ongoing training, resultingin a higher f-score for the third text segment (nopre-annotation).
For this reason, we excluded theirannotation results from the data set and performedanother ANOVA, considering the remaining fourannotators only.Figure 3 illustrates a noticeable trend for the in-teraction between pre-annotation and annotationquality: all four annotators show a decrease inannotation quality on the text segments withoutpre-annotation, while both types of pre-annotation(Shalmaneser, Enhanced) increase f-scores for hu-man annotation.
There are, however, differencesbetween the impact of the two pre-annotation typeson human annotation quality: two annotators showbetter results on the enhanced, high-quality pre-24Figure 3: Interaction between pre-annotation andf-scoreannotation, the other two perform better on thetexts pre-annotated by the state-of-the-art ASRL.The interaction between pre-annotation and f-scorecomputed for the four annotators is weakly signifi-cant with p ?
0.1.Next we investigated the influence of pre-annotation style on annotation time for the fourannotators.
Again we can see an interesting pat-tern: The two annotators (A1, A3) who annotatedin the order N-E-S, both take most time for thetexts without pre-annotation, getting faster on thetext pre-processed by Shalmaneser, while the leastamount of time was needed for the enhanced pre-annotated texts (Figure 4).
The two annotators (A2,A4) who processed the texts in the order S-N-E,showed a continuous reduction in annotation time,probably caused by the interaction of training anddata quality.
These observations, however, shouldbe taken with a grain of salt, as they outline trends,but due to the low number of annotators, could notbe substantiated by statistical tests.4.4 Semantic Role AssignmentAs described in Section 3.5, we provided pre-annotation for frame assignment only, thereforewe did not expect any significant effects of the dif-ferent conditions of pre-annotation on the task ofsemantic role labeling.
To allow for a meaningfulFigure 4: Interaction between pre-annotation andtimecomparison, the evaluation of semantic role assign-ment was done on the subset of frames annotatedcorrectly by all annotators.As with frame assignment, there are consid-erable differences in annotation quality betweenthe annotators.
In contrast to frame assignment,here the differences are statistically significant(p ?
0.05).
Table 6 shows the average f-scorefor each annotator on the semantic role assignmenttask.Anot1 Anot2 Anot3 Anot4 Anot5 Anot685.2 80.1 87.7 89.2 82.5 84.3Table 6: Average f-scores for the 6 annotatorsAs expected, neither the condition of pre-annotation nor the order of text segments had anysignificant effect on the quality of semantic roleassignment.25 Conclusion and future workIn the paper we presented experiments to assessthe benefits of partial automatic pre-annotation ona frame assignment (word sense disambiguation)task.
We compared the impact of a) pre-annotations2The annotation of frame and role assignment was done asa combined task, therefore we do not report separate resultsfor annotation time for semantic role assignment.25provided by a state-of-the-art ASRL, and b) en-hanced, high-quality pre-annotation on the annota-tion process.
We showed that pre-annotation hasa positive effect on the quality of human annota-tion: the enhanced pre-annotation clearly increasedf-scores for all annotators, and even the noisy, error-prone pre-annotations provided by the ASRL sys-tem did not lower the quality of human annotation.We suspect that there is a strong interactionbetween the order in which the text segmentsare given to the annotators and the three annota-tion conditions, resulting in lower f-scores for thegroup of annotators who processed the ASRL pre-annotations in the first trial, where they could notyet profit from the same amount of training as theother two groups.The same problem occurs with annotation time.We have not been able to show that automaticpre-annotation speeds up the annotation process.However, we suspect that here, too, the interactionbetween training effect and annotation conditionmade it difficult to reach a significant improve-ment.
One way to avoid the problem would be afurther split of the test data, so that the differenttypes of pre-annotation could be presented to theannotators at different stages of the annotation pro-cess.
This would allow us to control for the strongbias through incremental training, which we can-not avoid if one group of annotators is assigneddata of a given pre-annotation type in the first trial,while another group encounters the same type ofdata in the last trial.
Due to the limited numberof annotators we had at our disposal as well asthe amount of time needed for the experiments wecould not sort out the interaction between orderand annotation conditions.
We will take this issueup in future work, which also needs to address thequestion of how good the automatic pre-annotationshould be to support human annotation.
F-scoresfor the enhanced pre-annotation provided in ourexperiments were quite high, but it is possible thata similar effect could be reached with automaticpre-annotations of somewhat lower quality.The outcome of our experiments provides strongmotivation to improve ASRL systems, as automaticpre-annotation of acceptable quality does increasethe quality of human annotation.ReferencesC.
F. Baker, C. J. Fillmore, J.
B. Lowe.
1998.
Theberkeley framenet project.
In Proceedings of the17th international conference on Computational lin-guistics, 86?90, Morristown, NJ, USA.
Associationfor Computational Linguistics.J.
Baldridge, M. Osborne.
2004.
Active learningand the total cost of annotation.
In Proceedings ofEMNLP.T.
Brants, O. Plaehn.
2000.
Interactive corpus annota-tion.
In Proceedings of LREC-2000.A.
Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?.2006.
SALTO ?
a versatile multi-level annotationtool.
In Proceedings of LREC.F.-D. Chiou, D. Chiang, M. Palmer.
2001.
Facilitat-ing treebank annotation using a statistical parser.
InProceedings of HLT-2001.W.-C. Chou, R. T.-H. Tsai, Y.-S. Su, W. Ku, T.-Y.
Sung,W.-L. Hsu.
2006.
A semi-automatic method for an-notation a biomedical proposition bank.
In Proceed-ings of FLAC-2006.K.
Erk, S. Pado.
2006.
Shalmaneser - a flexible tool-box for semantic role assignment.
In Proceedings ofLREC, Genoa, Italy.C.
J. F. C. J. Fillmore, 1982.
Linguistics in the MorningCalm, chapter Frame Semantics, 111?137.
HanshinPublishing, Seoul, 1982.K.
Ganchev, F. Pereira, M. Mandel, S. Carroll, P. White.2007.
Semi-automated named entity annotation.In Proceedings of the Linguistic Annotation Work-shop, 53?56, Prague, Czech Republic.
Associationfor Computational Linguistics.U.
Kruschwitz, J. Chamberlain, M. Poesio.
2009.
(lin-guistic) science through web collaboration in theANAWIKI project.
In Proceedings of WebSci?09.M.
P. Marcus, B. Santorini, M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.W.
D. Meurers, S. Mu?ller.
2007.
Corpora and syntax(article 44).
In A. Lu?deling, M.
Kyto?, eds., Corpuslinguistics.
Mouton de Gruyter, Berlin.W.
D. Meurers.
2005.
On the use of electronic cor-pora for theoretical linguistics.
case studies fromthe syntax of german.
Lingua, 115(11):1619?1639.
http://purl.org/net/dm/papers/meurers-03.html.C.
Mueller, S. Rapp, M. Strube.
2002.
Applying co-training to reference resolution.
In Proceedings of40th Annual Meeting of the Association for Com-putational Linguistics, 352?359, Philadelphia, Penn-sylvania, USA.
Association for Computational Lin-guistics.V.
Ng, C. Cardie.
2003.
Bootstrapping corefer-ence classifiers with multiple machine learning algo-rithms.
In Proceedings of the 2003 Conference onEmpirical Methods in Natural Language Processing(EMNLP-2003).N.
Xue, F.-D. Chiou, M. Palmer.
2002.
Building alarge-scale annotated chinese corpus.
In Proceed-ings of Coling-2002.26
