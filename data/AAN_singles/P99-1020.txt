A Method for Word Sense Disambiguation of Unrestr icted TextRada Miha lcea  and Dan I. Mo ldovanDepartment of Computer Science and EngineeringSouthern Methodist UniversityDallas, Texas, 75275-0122(rada,moldovan}@seas.smu.eduAbst rac tSelecting the most appropriate sense for an am-biguous word in a sentence is a central prob-lem in Natural Language Processing.
In thispaper, we present a method that attemptsto disambiguate all the nouns, verbs, adverbsand adjectives in a text, using the senses pro-vided in WordNet.
The senses are ranked us-ing two sources of information: (1) the Inter-net for gathering statistics for word-word co-occurrences and (2)WordNet for measuring thesemantic density for a pair of words.
We reportan average accuracy of 80% for the first rankedsense, and 91% for the first two ranked senses.Extensions of this method for larger windows ofmore than two words are considered.1 In t roduct ionWord Sense Disambiguation (WSD) is an openproblem in Natural Language Processing.
Itssolution impacts other tasks such as discourse,reference resolution, coherence, inference andothers.
WSD methods can be broadly classifiedinto three types:1.
WSD that make use of the informationprovided by machine readable dictionaries(Cowie et al, 1992), (Miller et al, 1994),(Agirre and Rigau, 1995), (Li et al, 1995),(McRoy, 1992);2.
WSD that use information gathered fromtraining on a corpus that has alreadybeen semantically disambiguated (super-vised training methods) (Gale et al, 1992),(Ng and Lee, 1996);3.
WSD that use information gathered fromraw corpora (unsupervised training meth-ods) (Yarowsky, 1995) (Resnik, 1997).There are also hybrid methods that combineseveral sources of knowledge such as lexicon in-formation, heuristics, collocations and others(McRoy, 1992) (Bruce and Wiebe, 1994) (Ngand Lee, 1996) (Rigau et al, 1997).Statistical methods produce high accuracy re-sults for small number of preselected words.
Alack of widely available semantically tagged cor-pora almost excludes upervised learning meth-ods.
A possible solution for automatic acqui-sition of sense tagged corpora has been pre-sented in (Mihalcea and Moldovan, 1999), butthe corpora acquired with this method has notbeen yet tested for statistical disambiguation fwords.
On the other hand, the disambiguationusing unsupervised methods has the disadvan-tage that the senses are not well defined.
Noneof the statistical methods disambiguate adjec-tives or adverbs o far.In this paper, we introduce a method that at-tempts to disambiguate all the nouns, verbs, ad-jectives and adverbs in a text, using the sensesprovided in WordNet (Fellbaum, 1998).
Toour knowledge, there is only one other method,recently reported, that disambiguates unre-stricted words in texts (Stetina et al, 1998).2 A word-word  dependencyapproachThe method presented here takes advantage ofthe sentence context.
The words are paired andan attempt is made to disambiguate one wordwithin the context of the other word.
Thisis done by searching on Internet with queriesformed using different senses of one word, whilekeeping the other word fixed.
The senses areranked simply by the order provided by thenumber of hits.
A good accuracy is obtained,perhaps because the number of texts on the In-ternet is so large.
In this way, all the words are152processed and the senses axe ranked.
We usethe ranking of senses to curb the computationalcomplexity in the step that follows.
Only themost promising senses are kept.The next step is to refine the ordering ofsenses by using a completely different method,namely the semantic density.
This is measuredby the number of common words that are withina semantic distance of two or more words.
Thecloser the semantic relationship between twowords the higher the semantic density betweenthem.
We introduce the semantic density be-cause it is relatively easy to measure it on aMRD like WordNet.
A metric is introduced inthis sense which when applied to all possiblecombinations of the senses of two or more wordsit ranks them.An essential aspect of the WSD method pre-sented here is that it provides a raking of pos-sible associations between words instead of abinary yes/no decision for each possible sensecombination.
This allows for a controllable pre-cision as other modules may be able to distin-guish later the correct sense association fromsuch a small pool.3 Contextua l  rank ing  o f  word  sensesSince the Internet contains the largest collectionof texts electronically stored, we use the Inter-net as a source of corpora for ranking the sensesof the words.3.1 A lgor i thm 1For a better explanation of this algorithm, weprovide the steps below with an example.
Weconsidered the verb-noun pair "investigate re-port"; in order to make easier the understand-ing of these examples, we took into considera-tion only the first two senses of the noun re-port.
These two senses, as defined in WordNet,appear in the synsets: (report#l, study} and{report#2, news report, story, account, writeup}.INPUT: semantically untagged word1 - word2pair (W1 - W2)OUTPUT: ranking the senses of one wordPROCEDURE:STEP 1.
Form a similarity list \]or each senseof one of the words.
Pick one of the words,say W2, and using WordNet, form a similaritylist for each sense of that word.
For this, usethe words from the synset of each sense and thewords from the hypernym synsets.
Consider,for example, that W2 has m senses, thus W2appears in m similarity lists:...,(wL( ' ,  ...,where W 1, Wff, ..., W~ n are the senses of W2,and W2 (s) represents the synonym number s ofthe sense W~ as defined in WordNet.Example The similarity lists for the first twosenses of the noun report are:(report, study)(report, news report, story, account, write up)STEP 2.
Form W1 - W2 (s) pairs.
The pairs thatmay be formed are:- w ,  - (1), - ..., w l  -(Wl  -- W 2, Wl - W2 2(1), Wi  - W2(2), ..., Wl  - W:  (k2))(Wl  - W2 n, Wl  - W2 n(1), Wl  - W2 m(2), ..., Wi - W~ (kin))Example The pairs formed with the verb inves-tigate and the words in the similarity lists of thenoun report are:(investigate-report, investigate-study)(investigate-report, investigate-news report, investigate-story, investigate-account, i vestigate-write up)STEP 3.
Search the Internet and rank the sensesW~ (s).
A search performed on the Internet foreach set of pairs as defined above, results in avalue indicating the frequency of occurrences forWl and the sense of W2.
In our experiments weused (Altavista, 1996) since it is one of the mostpowerful search engines currently available.
Us-ing the operators provided by AltaVista, query-forms are defined for each W1 - W2 (s) set above:(a)  ( "w,  oR  "w l  oR  oR  .
.
.OR "W1 W~ (k~)')(b) ((W~ NEAR W~) OR (W1 NEAR W~ (1)) OR (W1NEAR W~ (2)) OR ... OR (W~ NEAR W~(k')))for all 1 < i < m. Using one of these queries,we get the number of hits for each sense i of W2and this provides a ranking of the m senses ofW2 as they relate with 1411.Example The types of query that can be formedusing the verb investigate and the similarity listsof the noun report, are shown below.
After eachquery, we indicate the number of hits obtainedby a search on the Internet, using AltaVista.
(a) ("investigate r port" OR "investigate study") (478)("investigate r port" OR "investigate news report" OR"investigate story" OR "investigate account" OR "inves-tigate write up") (~81)(b) ((investigate NEAR report) OR (investigate NEARstudy)) (34880)((investigate NEAR report) OR (investigate NEAR newsreport) OR (investigate NEAR story) OR (investigateNEAR account) OR (investigate NEAR write up))(15ss4)A similar algorithm is used to rank thesenses of W1 while keeping W2 constant (un-disambiguated).
Since these two procedures aredone over a large corpora (the Internet), andwith the help of similarity lists, there is littlecorrelation between the results produced by thetwo procedures.3.1.1 P rocedure  Eva luat ionThis method was tested on 384 pairs: 200 verb-noun (file br-a01, br-a02), 127 adjective-noun(file br-a01), and 57 adverb-verb (file br-a01),extracted from SemCor 1.6 of the Brown corpus.Using query form (a) on AltaVista, we obtainedthe results shown in Table 1.
The table indi-cates the percentages of correct senses (as givenby SemCor) ranked by us in top 1, top 2, top3, and top 4 of our list.
We concluded that bykeeping the top four choices for verbs and nounsand the top two choices for adjectives and ad-verbs, we cover with high percentage (mid andupper 90's) all relevant senses.
Looking from adifferent point of view, the meaning of the pro-cedure so far is that it excludes the senses thatdo not apply, and this can save a considerableamount of computation time as many words arehighly polysemous.top 1 top 2 top 3 top 4noun 76% 83~ 86~ 98%verb 60% 68% 86% 87%adjective 79.8% 93%adverb 87% 97%Table 1: Statistics gather from the Internet for384 word pairs.We also used the query form (b), but the re-sults obtained were similar; using, the operatorNEAR,  a larger number of hits is reported, butthe sense ranking remains more or less the same.3.2 Conceptua l  dens i ty  a lgor i thmA measure of the relatedness between words canbe a knowledge source for several decisions inNLP applications.
The approach we take hereis to construct a linguistic ontext for each senseof the verb and noun, and to measure the num-ber of the common nouns shared by the verband the noun contexts.
In WordNet each con-cept has a gloss that acts as a micro-context forthat concept.
This is a rich source of linguisticinformation that we found useful in determiningconceptual density between words.3.2.1 A lgor i thm 2INPUT: semantically untagged verb - noun pairand a ranking of noun senses (as determined byAlgorithm 1)OUTPUT: sense tagged verb - noun pairP aOCEDURE:STEP 1.
Given a verb-noun pair V - N, denotewith < vl,v2, ...,Vh > and < nl,n2, ...,nt > thepossible senses of the verb and the noun usingWordNet.STEP 2.
Using Algorithm 1, the senses of thenoun are ranked.
Only the first t possible sensesindicated by this ranking will be considered.The rest are dropped to reduce the computa-tional complexity.STEP 3.
For each possible pair vi - nj, the con-ceptual density is computed as follows:(a) Extract all the glosses from the sub-hierarchy including vi (the rationale for select-ing the sub-hierarchy is explained below)(b) Determine the nouns from these glosses.These constitute the noun-context of the verb.Each such noun is stored together with a weightw that indicates the level in the sub-hierarchyof the verb concept in whose gloss the noun wasfound.
(c) Determine the nouns from the noun sub-hierarchy including nj.
(d) Determine the conceptual density Cij ofcommon concepts between the nouns obtainedat (b) and the nouns obtained at (c) using themetric:IcdijlkCij = log (descendents j) (1)where:?
Icdljl is the number of common concepts betweenthe hierarchies of vl and nj154?
wk are the levels of the nouns in the hierarchy ofverb vi?
descendentsj is the total number of words withinthe hierarchy of noun njSTEP 4.
Vii ranks each pair vi -n j ,  for all i andj.Rat iona le1.
In WordNet, a gloss explains a concept andprovides one or more examples with typical us-age of that concept.
In order to determine themost appropriate noun and verb hierarchies, weperformed some experiments using SemCor andconcluded that the noun sub-hierarchy shouldinclude all the nouns in the class of nj.
Thesub-hierarchy of verb vi is taken as the hierar-chy of the highest hypernym hi of the verb vi.
Itis necessary to consider a larger hierarchy thenjust the one provided by synonyms and directhyponyms.
As we replaced the role of a corporawith glosses, better results are achieved if moreglosses are considered.
Still, we do not want toenlarge the context oo much.2.
As the nouns with a big hierarchy tendto have a larger value for Icdij\[, the weightedsum of common concepts is normalized with re-spect to the dimension of the noun hierarchy.Since the size of a hierarchy grows exponentiallywith its depth, we used the logarithm of the to-tal number of descendants in the hierarchy, i.e.log(descendents j).3.
We also took into consideration and haveexperimented with a few other metrics.
But af-ter running the program on several examples,the formula from Algorithm 2 provided the bestresults.4 An  ExampleAs an example, let us consider the verb-nouncollocation revise law.
The verb revise has twopossible senses in WordNet 1.6 and the noun law?
has seven senses.
Figure 1 presents the synsetsin which the different meanings of this verb andnoun appear.First, Algorithm 1 was applied and searchthe Internet using AltaVista, for all possi-ble pairs V-N that may be created using re-vise and the words from the similarity lists oflaw.
The following ranking of senses was ob-tained: Iaw#2(2829), law#3(648), law#4(640),law#6(397), law#1(224), law#5(37), law#7(O),"REVISE1.
{revise#l}=> { rewrite}2.
{retool, revise#2}=> { reorganize, shake up}LAW1.
{ law#I, jurisprudence}=> {collection, aggregation,accumulation, assemblage}2.
{law#2}= > {rule, prescript\] ...3.
{law#3, natural aw}= > \[ concept, conception, abstract\]4.
{law#4, law of nature}= > \[ concept, conception, abstract\]5.
{jurisprudence, law#5, legal philosophy}=> \[ philosophy}6.
{law#6, practice of law}=> \[ learned profession}7.
{police, police force, constabulary, law#7}= > {force, personnel}Figure 1: Synsets and hypernyms for the differ-ent meanings,  as defined in WordNetwhere the numbers in parentheses indicate thenumber of hits.
By setting the threshold att = 2, we keep only sense #2 and #3.Next, Algorithm 2 is applied to rank the fourpossible combinations (two for the verb timestwo for the noun).
The results are summarizedin Table 2: (1) \[cdij\[ - the number of commonconcepts between the verb and noun hierarchies;(2) descendantsj  the total number of nounswithin the hierarchy of each sense nj; and (3)the conceptual density Cij for each pair ni - vjderived using the formula presented above.ladij I descendantsj Cijn2 n3  1"$2 I"$3 n2 1"$35 4 975 1265 0.30 0.280 0 975 1265 0 0Table 2: Values used in computing the concep-tual density and the conceptual density CijThe largest conceptual density C12 = 0.30corresponds to V 1 --  n2 : rev ise#l~2 - l aw#2/5(the notation #i /n  means sense i out of n pos-155sibletionCor,senses given by WordNet).
This combina-of verb-noun senses also appears in Sem-file br-a01.5 Eva luat ion  and  compar i son  w i tho ther  methods5.1 Tests against SemCorThe method was tested on 384 pairs selectedfrom the first two tagged files of SemCor 1.6(file br-a01, br-a02).
From these, there are 200verb-noun pairs, 127 adjective-noun pairs and57 adverb-verb pairs.In Table 3, we present a summary of the results.top 1 top 2 top 3 top 4noun 86.5% 96% 97% 98%verb 67% 79% 86% 87%adjective 79.8% 93%adverb 87% 97%Table 3: Final results obtained for 384 wordpairs using both algorithms.Table 3 shows the results obtained using bothalgorithms; for nouns and verbs, these resultsare improved with respect o those shown inTable 1, where only the first algorithm was ap-plied.
The results for adjectives and adverbs arethe same in both these tables; this is because thesecond algorithm is not used with adjectives andadverbs, as words having this part of speech arenot structured in hierarchies in WordNet, butin clusters; the small size of the clusters limitsthe applicability of the second algorithm.Discussion of results When evaluating theseresults, one should take into consideration that:1.
Using the glosses as a base for calculat-ing the conceptual density has the advantage ofeliminating the use of a large corpus.
But a dis-advantage that comes from the use of glossesis that they are not part-of-speech tagged, likesome corpora are (i.e.
Treebank).
For this rea-son, when determining the nouns from the verbglosses, an error rate is introduced, as someverbs (like make, have, go, do) are lexically am-biguous having a noun representation in Word-Net as well.
We believe that future work onpart-of-speech tagging the glosses of WordNetwill improve our results.2.
The determination of senses in SemCorwas done of course within a larger context, thecontext of sentence and discourse.
By workingonly with a pair of words we do not take advan-tage of such a broader context.
For example,when disambiguating the pair protect court ourmethod picked the court meaning "a room inwhich a law court sits" which seems reasonablegiven only two words, whereas SemCor gives thecourt meaning "an assembly to conduct judicialbusiness" which results from the sentence con-text (this was our second choice).
In the nextsection we extend our method to more than twowords disambiguated at the same time.5.2 Compar i son  wi th  o ther  methodsAs indicated in (Resnik and Yarowsky, 1997),it is difficult to compare the WSD methods,as long as distinctions reside in the approachconsidered (MRD based methods, supervisedor unsupervised statistical methods), and inthe words that are disambiguated.
A methodthat disambiguates unrestricted nouns, verbs,adverbs and adjectives in texts is presented in(Stetina et al, 1998); it attempts to exploit sen-tential and discourse contexts and is based onthe idea of semantic distance between words,and lexical relations.
It uses WordNet and itwas tested on SemCor.Table 4 presents the accuracy obtained byother WSD methods.
The baseline of this com-parison is considered to be the simplest methodfor WSD, in which each word is tagged withits most common sense, i.e.
the first sense asdefined in WordNet.Base Stetina Yarowsky Ourline methodnoun 80.3% 85.7% 93.9% 86.5%verb 62.5% 63.9% 67%adjective 81.8% 83.6% 79.8adverb 84.3% 86.5% 87%AVERAOE I 77% I 80% I 180 .1%1Table 4: A comparison with other WSD meth-ods.As it can be seen from this table, (Stetina etal., 1998) reported an average accuracy of 85.7%for nouns, 63.9% for verbs, 83.6% for adjectivesand 86.5% for adverbs, slightly less than our re-sults.
Moreover, for applications such as infor-mation retrieval we can use more than one sensecombination; if we take the top 2 ranked com-binations our average accuracy is 91.5% (fromTable 3).Other methods that were reported in the lit-156erature disambiguate either one part of speechword (i.e.
nouns), or in the case of purely statis-tical methods focus on very limited number ofwords.
Some of the best results were reportedin (Yarowsky, 1995) who uses a large trainingcorpus.
For the noun drug Yarowsky obtains91.4% correct performance and when consider-ing the restriction "one sense per discourse" theaccuracy increases to 93.9%, result representedin the third column in Table 4.6 Extens ions6.1 Noun-noun and verb-verb pairsThe method presented here can be applied in asimilar way to determine the conceptual densitywithin noun-noun pairs, or verb-verb pairs (inthese cases, the NEAR operator should be usedfor the first step of this algorithm).6.2 Larger  window sizeWe have extended the disambiguation methodto more than two words co-occurrences.
Con-sider for example:The bombs caused amage but no injuries.The senses pecified in SemCor, are:la.
bomb(#1~3) cause(#1//2) damage(#1~5)iujury (#1/4 )For each word X, we considered all possiblecombinations with the other words Y from thesentence, two at a time.
The conceptual densityC was computed for the combinations X -Yas a summation of the conceptual densities be-tween the sense i of the word X and all thesenses of the words Y.
The results are shownin the tables below where the conceptual den-sity calculated for the sense #i  of word X ispresented in the column denoted by C#i:X - Y C#1 0#2 C#3bomb-cause 0.57 0 0bomb-damage 5.09 0.13 0bomb-injury 2.69 0.15 0SCORE 8.35 0.28 0By selecting the largest values for the con-ceptual density, the words are tagged with theirsenses as follows:lb.
bomb(#1/3) cause(#1/2) damage(#1~5)iuju, (#e/4)X-Ycause-bombcause-damagecause-injurySCOREc#I5.1612.8312.6330.62C#21.342.641.755.73X - Y C#1damage-bomb 5.60damage-cause 1.73damage-injury 9.87SCORE 17.20c#22.142.632.577.34C#3 C#4 C#51.95 0.88 2.160.17 0.16 3.803.24 1.56 7.595.36 2.60 13.55Note that the senses for word injury differ fromla.
to lb.
; the one determined by our method(#2/4) is described in WordNet as "an acci-dent that results in physical damage or hurt"(hypernym: accident), and the sense providedin SemCor (#1/4) is defined as "any physicaldamage'(hypernym: health problem).This is a typical example of a mismatchcaused by the fine granularity of senses in Word-Net which translates into a human judgmentthat is not a clear cut.
We think that thesense selection provided by our method is jus-tified, as both damage and injury are objectsof the same verb cause; the relatedness of dam-age(#1/5) and injury(#2/~) is larger, as bothare of the same class noun.event as opposed toinjury(#1~4) which is of class noun.state.Some other randomly selected examples con-sidered were:2a.
The te,~orists(#l/1) bombed(#l/S) theembassies(#1~1).2b.
terrorist(#1~1) bomb(#1~3)embassy(#1~1)3a.
A car-bomb(#1~1) exploded(#2/lO) in\]rout of PRC(#I/1) embassy(#1/1).3b.
car-bomb(#1/1) explode(#2/lO)PRC(#I/1) embassy(#1~1)4a.
The bombs(#1~3) broke(#23~27)windows(#l/4) and destroyed(#2~4) thetwovehicles(#1~2).4b.
bomb(#1/3) break(#3/27) window(#1/4)destroy(#2/4) vehicle(# l/2)where sentences 2a, 3a and 4a are extractedfrom SemCor, with the associated senses foreach word, and sentences 2b, 3b and 4b showthe verbs and the nouns tagged with their sensesby our method.
The only discrepancy is for the157X - Y C#I  C#2 C#3 C#4injury-bomb 2.35 5.35 0.41 2.28injury-cause 0 4.48 0.05 0.01injury-damage 5.05 10.40 0.81 9.69SCORE 7.40 20.23 1.27 11.98word broke and perhaps this is due to the largenumber of its senses.
The other word with alarge number of senses explode was tagged cor-rectly, which was encouraging.7 Conc lus ionWordNet is a fine grain MRD and this makes itmore difficult to pinpoint he correct sense com-bination since there are many to choose fromand many are semantically close.
For appli-cations such as machine translation, fine graindisambiguation works well but for informationextraction and some other applications this isan overkill, and some senses may be lumped to-gether.
The ranking of senses is useful for manyapplications.Re ferencesE.
Agirre and G. Rigau.
1995.
A proposal forword sense disambiguation using conceptualdistance.
In Proceedings of the First Inter-national Conference on Recent Advances inNatural Language Processing, Velingrad.Altavista.
1996.
Digital equipment corpora-tion.
"http://www.altavista.com".R.
Bruce and J. Wiebe.
1994.
Word sensedisambiguation using decomposable models.In Proceedings of the Thirty Second An-nual Meeting of the Association for Computa-tional Linguistics (ACL-9~), pages 139-146,LasCruces, NM, June.J.
Cowie, L. Guthrie, and J. Guthrie.
1992.Lexical disambiguation using simulated an-nealing.
In Proceedings of the Fifth Interna-tional Conference on Computational Linguis-tics COLING-92, pages 157-161.C.
Fellbaum.
1998.
WordNet, An ElectronicLexical Database.
The MIT Press.W.
Gale, K. Church, and D. Yarowsky.
1992.One sense per discourse.
In Proceedings oftheDARPA Speech and Natural Language Work-shop, Harriman, New York.X.
Li, S. Szpakowicz, and M. Matwin.
1995.A wordnet-based algorithm for word seman-tic sense disambiguation.
In Proceedings ofthe Forteen International Joint Conferenceon Artificial Intelligence IJCAI-95, Montreal,Canada.S.
McRoy.
1992.
Using multiple knowledgesources for word sense disambiguation.
Com-putational Linguistics, 18(1):1-30.R.
Mihalcea and D.I.
Moldovan.
1999.
An au-tomatic method for generating sense taggedcorpora.
In Proceedings of AAAI-99, Or-lando, FL, July.
(to appear).G.
Miller, M. Chodorow, S. Landes, C. Leacock,and R. Thomas.
1994.
Using a semantic on-cordance for sense identification.
In Proceed-ings of the ARPA Human Language Technol-ogy Workshop, pages 240-243.H.T.
Ng and H.B.
Lee.
1996.
Integrating multi-ple knowledge sources to disambiguate wordsense: An examplar-based approach.
In Pro-ceedings of the Thirtyfour Annual Meeting ofthe Association for Computational Linguis-tics (A CL-96), Santa Cruz.P.
Resnik and D. Yarowsky.
1997.
A perspec-tive on word sense disambiguation methodsand their evaluation.
In Proceedings of A CLSiglex Workshop on Tagging Text with LexicalSemantics, Why, What and How?, Washing-ton DC, April.P.
Resnik.
1997.
Selectional preference andsense disambiguation.
In Proceedings of A CLSiglex Workshop on Tagging Text with LexicalSemantics, Why, What and How?, Washing-ton DC, April.G.
Rigau, J. Atserias, and E. Agirre.
1997.Combining unsupervised lexical knowledgemethods for word sense disambiguation.Computational Linguistics.J.
Stetina, S. Kurohashi, and M. Nagao.
1998.General word sense disambiguation methodbased on a full sentential context.
In Us-age of WordNet in Natural Language Process-ing, Proceedings ofCOLING-A CL Workshop,Montreal, Canada, July.D.
Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.In Proceedings of the Thirtythird Associationof Computational Linguistics.158
