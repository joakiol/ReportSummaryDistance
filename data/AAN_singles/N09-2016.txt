Proceedings of NAACL HLT 2009: Short Papers, pages 61?64,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsLearning Bayesian Networks for Semantic Frame Compositionin a Spoken Dialog SystemMarie-Jean Meurs, Fabrice Lefe`vre and Renato de MoriUniversite?
d?Avignon et des Pays de VaucluseLaboratoire Informatique d?Avignon (EA 931), F-84911 Avignon, France.
{marie-jean.meurs,fabrice.lefevre,renato.demori}@univ-avignon.frAbstractA stochastic approach based on DynamicBayesian Networks (DBNs) is introduced forspoken language understanding.
DBN-basedmodels allow to infer and then to composesemantic frame-based tree structures fromspeech transcriptions.
Experimental results onthe French MEDIA dialog corpus show theappropriateness of the technique which bothlead to good tree identification results and canprovide the dialog system with n-best lists ofscored hypotheses.1 IntroductionRecent developments in Spoken Dialog Systems(SDSs) have renewed the interest for the extrac-tion of rich and high-level semantics from users?utterances.
Shifting every SDS component fromhand-crafted to stochastic is foreseen as a good op-tion to improve their overall performance by an in-creased robustness to speech variabilities.
For in-stance stochastic methods are now efficient alter-natives to rule-based techniques for Spoken Lan-guage Understanding (SLU) (He and Young, 2005;Lefe`vre, 2007).The SLU module links up the automatic speechrecognition (ASR) module and the dialog manager.From the user?s utterance analysis, it derives a repre-sentation of its semantic content upon which the di-alog manager can decide the next best action to per-form, taking into account the current dialog context.In this work, the overall objective is to increase therelevancy of the semantic information used by thesystem.
Generally the internal meaning representa-tion is based on flat concept sets obtained by eitherkeyword spotting or conceptual decoding.
In somecases a dialog act can be added on top of the conceptset.
Here we intend to consider an additional se-mantic composition step which will capture the ab-stract semantic structures conveyed by the basic con-cept representation.
A frame formalism is applied tospecify these nested structures.
As such structuresdo not rely on sequential constraints, pure left-rightbranching semantic parser (such as (He and Young,2005)) will not apply in this case.To derive automatically such frame meaning rep-resentations we propose a system based on a twodecoding step process using dynamic Bayesian net-works (DBNs) (Bilmes and Zweig, 2002): first ba-sic concepts are derived from the user?s utterancetranscriptions, then inferences are made on sequen-tial semantic frame structures, considering all theavailable previous annotation levels (words and con-cepts).
The inference process extracts all possiblesub-trees (branches) according to lower level infor-mation (generation) and composes the hypothesizedbranches into a single utterance-span tree (composi-tion).
A hand-craft rule-based approach is used toderive the seed annotated training data.
So both ap-proaches are not competing and the stochastic ap-proach is justified as only the DBN system is ableto provide n-best lists of tree hypotheses with confi-dence scores to a stochastic dialog manager (such asthe very promising POMDP-based approaches).The paper is organized as follows.
The next sec-tion presents the semantic frame annotation on theMEDIA corpus.
Then Section 3 introduces the DBN-based models for semantic composition and finallySection 4 reports on the experiments.61HOTEL LOCATIONlocation_eventLODGINGlodging_hotel lodging_locationframeframe frameFE FEFEFigure 1: Frames, FEs and relations associated to the se-quence ?staying in a hotel near the Festival de Cannes?2 Semantic Frames on the MEDIA corpusMEDIA is a French corpus of negotiation di-alogs among users and a tourist information phoneserver (Bonneau-Maynard et al, 2005).
The corpuscontains 1,257 dialogs recorded using a Wizard ofOz system.
The semantic corpus is annotated withconcept-value pairs corresponding to word segmentswith the addition of specifier tags representing somerelations between concepts.
The annotation utilizes83 basic concepts and 19 specifiers.Amongst the available semantic representations,the semantic frames (Lowe et al, 1997) are probablythe most suited to the task, mostly because of theirability to represent negotiation dialogs.
Semanticframes are computational models describing com-mon or abstract situations involving roles, the frameelements (FEs).
The FrameNet project (Fillmore etal., 2003) provides a large frame database for En-glish.
As no such resource exists for French, weelaborated a frame ontology to describe the semanticknowledge of the MEDIA domain.
The MEDIA on-tology is composed of 21 frames and 86 FEs.
All aredescribed by a set of manually defined patterns madeof lexical units and conceptual units (frame and FEevoking words and concepts).
Figure 1 gives the an-notation of word sequence ?staying in a hotel nearthe Festival de Cannes?.
The training data are auto-matically annotated by a rule-based process.
Patternmatching triggers the instantiation of frames andFEs which are composed using a set of logical rules.Composition may involve creation, modification ordeletion of frame and FE instances.
About 70 rulesare currently used.
This process is task-oriented andis progressively enriched with new rules to improveits accuracy.
A reference frame annotation for thetraining corpus is established in this way and usedfor learning the parameters of the stochastic modelsintroduced in the next section.concept conceptconeconeconcepttrans concepttransFrameFEFrameFEFrame-FE transFrame-FE transFrame trans Frame transconcept conceptpntrapntraconeconeconcepttrans concepttranspspsFE transFE transFigure 2: Frames, FEs as one or 2 unobserved variablesconcept conceptconeconeconceptrtasnF concepttasnFmasEemasEemasEertasnFmasEertasnFconcept conceptpntrapntraconeconeconcepttasnFconcepttasnFmasEertasnF masEertasnFpspsm-rtasnFm-rtasnFFigure 3: 2-level decoding of frames and FEs3 DBN-based Frame ModelsThe generative DBN models used in the system aredepicted on two time slices (two words) in figures 2and 3.
In practice, a regular pattern is repeated suffi-ciently to fit the entire word sequence.
Shaded nodesare observed variables whereas empty nodes are hid-den.
Plain lines represent conditional dependenciesbetween variables and dashed lines indicate switch-ing parents (variables modifying the conditional re-lationship between others).
An example of a switch-ing parent is given by the trans nodes which in-fluence the frame and FE nodes: when trans nodeis null the frame or FE stays the same from slice toslice, when trans is 1 a new frame or FE value ispredicted based on the values of its parent nodes inthe word sequence using frame (or FE) n-grams.In the left DBN model of Figure 2 frames and FEsare merged in a single compound variable.
Theyare factorized in the right model using two variablesjointly decoded.
Figure 3 shows the 2-level modelwhere frames are first decoded then used as observedvalues in the FE decoding step.
Merging frames andFEs into a variable reduces the decoding complex-ity but leads to deterministic links between frames62and FEs.
With their factorization, on the contrary, itis possible to deal with the ambiguities in the frameand FE links.
During the decoding step, every com-bination is tested, even not encountered in the train-ing data, by means of a back-off technique.
Dueto the increase in model complexity, a sub-optimalbeam search is applied for decoding.
In this way,the 2-level approach reduces the complexity of thefactored approach while preserving model general-ization.Because all variables are observed at trainingtime, the edge?s conditional probability tables aredirectly derived from observation counts.
To im-prove their estimates, factored language models(FLMs) are used along with generalized parallelbackoff (Bilmes and Kirchhoff, 2003).
Several FLMimplementations of the joint distributions are usedin the DBN models, corresponding to the arrows inFigures 2 and 3.
In the FLMs given below, n is thehistory length (n = 1 for bigrams), the uppercaseand lowercase letters FFE, F , FE, C and W re-spectively stand for frame/FE (one variable), frame,FE, concept and word variables:?
Frame/FE compound variable:P (FFE) '?nk=0 P (ffek|ffek?1);P (C|FFE) '?nk=0 P (ck|ck?1, ffek);P (W |C,FFE) '?nk=0 P (wk|wk?1, ck, ffek).?
Frame and FE variables, joint decoding:P (F ) '?nk=0 P (fk|fk?1);P (FE|F ) '?nk=0 P (fek|fek?1, fk);P (C|FE,F ) '?nk=0 P (ck|ck?1, fek, fk);P (W |C,FE, F ) '?nk=0 P (wk|wk?1, ck, fek, fk).?
Frame and FE variables, 2-level decoding:?
First stage: same as frame/FE compound variablesbut only decoding frames?
Second stage: same as joint decodind but frames areobservedP (F? )
'?nk=0 P (f?k|f?k?1);P (FE|F? )
'?nk=0 P (fek|fek?1, f?k);P (C|F?
, FE) '?nk=0 P (ck|ck?1, f?k, fek);P (W |C, F?
, FE) '?nk=0 P (wk|wk?1, ck, f?k, fek).Variables with hat have observed values.Due to the frame hierarchical representation,some overlapping situations can occurred when de-termining the frame and FE associated to a concept.To address this difficulty, a tree-projection algorithmis performed on the utterance tree-structured frameannotation and allows to derive sub-branches associ-ated to a concept (possibly more than one).
Startingfrom a leaf of the tree, a compound frame/FE classis obtained by aggregating the father vertices (eitherframes or FEs) as long as they are associated to thesame concept (or none).
The edges are defined bothby the frame?FE attachments and the FE?framesub-frame relations.Thereafter, either the branches are considered di-rectly as compound classes or the frame and FE in-terleaved components are separated to produce twoclass sets.
These compound classes are consideredin the decoding process then projected back after-wards to recover the two types of frame?FE con-nections.
However, some links are lost because de-coding is sequential.
A set of manually defined rulesis used to retrieve the missing connections from theset of hypothesized branches.
Theses rules are sim-ilar to those used in the semi-automatic annotationof the training data but differ mostly because theavailable information is different.
For instance, theframes cannot anymore be associated to a particularword inside a concept but rather to the whole seg-ment.
The training corpus provides the set of frameand FE class sequences on which the DBN parame-ters are estimated.4 Experiments and ResultsThe DBN-based composition systems were evalu-ated on a test set of 225 speakers?
turns manuallyannotated in terms of frames and FEs.
The rule-based system was used to perform a frame annota-tion of the MEDIA data.
On the test set, an aver-age F-measure of 0.95 for frame identification con-firms the good reliability of the process.
The DBNmodel parameters were trained on the training datausing jointly the manual transcriptions, the manualconcept annotations and the rule-based frame anno-tations.Experiments were carried out on the test set underthree conditions varying the input noise level:?
REF (reference): speaker turns manually tran-scribed and annotated;?
SLU: concepts decoded from manual transcrip-tions using a DBN-based SLU model comparableto (Lefe`vre, 2007) (10.6% concept error rate);?
ASR+SLU: 1-best hypotheses of transcriptions63Inputs REF SLU ASR + SLUDBN models Frames FE Links Frames FE Links Frames FE Linksframe/FEs p?/r?
0.91/0.93 0.91/0.86 0.93/0.98 0.87/0.82 0.91/0.83 0.93/0.98 0.86/0.80 0.90/0.86 0.92/0.98(compound) F?-m 0.89 0.86 0.92 0.81 0.82 0.92 0.78 0.84 0.92frames and FEs p?/r?
0.92/0.92 0.92/0.85 0.94/0.98 0.88/0.81 0.92/0.83 0.93/0.97 0.87/0.79 0.90/0.86 0.94/0.97(2 variables) F?-m 0.90 0.86 0.94 0.80 0.83 0.91 0.78 0.84 0.93frames then FEs p?/r?
0.92/0.94 0.91/0.82 0.92/0.98 0.88/0.86 0.91/0.80 0.92/0.97 0.87/0.81 0.89/0.82 0.93/0.98(2-level) F?-m 0.91 0.83 0.93 0.83 0.80 0.90 0.79 0.80 0.92Table 1: Precision (p?
), Recall (r?)
and F-measure (F?-m) on the MEDIA test set for the DBN-based frame compositionsystems.generated by an ASR system and concepts decodedusing them (14.8% word error rate, 24.3% concepterror rate).All the experiments reported in the paper were per-formed using GMTK (Bilmes and Zweig, 2002),a general purpose graphical model toolkit andSRILM (Stolcke, 2002), a language modelingtoolkit.Table 1 is populated with the results on the testset for the DBN-based frame composition systemsin terms of precision, recall and F-measure.
For theFE figures, only the reference FEs corresponding tocorrectly identified frames are considered.
Only theframe and FE names are considered, neither theirconstituents nor their order matter.
Finally, resultsare given for the sub-frame links between framesand FEs.
Table 1 shows that the performances of the3 DBN-based systems are quite comparable.
Any-how the 2-level system can be considered the bestas besides its good F-measure results, it is also themost efficient model in terms of decoding complex-ity.
The good results obtained for the sub-framelinks confirm that the DBN models combined with asmall rule set can be used to generate consistent hi-erarchical structures.
Moreover, as they can providehypotheses with confidence scores they can be usedin a multiple input/output context (lattices and n-bestlists) or in a validation process (evaluating and rank-ing hypotheses from other systems).5 ConclusionThis work investigates a stochastic process for gen-erating and composing semantic frames using dy-namic Bayesian networks.
The proposed approachoffers a convenient way to automatically derive se-mantic annotations of speech utterances based ona complete frame and frame element hierarchicalstructure.
Experimental results, obtained on the ME-DIA dialog corpus, show that the performance of theDBN-based models are definitely good enough to beused in a dialog system in order to supply the dialogmanager with a rich and thorough representation ofthe user?s request semantics.
Though this can alsobe obtained using a rule-based approach, the DBNmodels alone are able to derive n-best lists of se-mantic tree hypotheses with confidence scores.
Theincidence of such outputs on the dialog manager de-cision accuracy needs to be asserted.AcknowledgmentThis work is supported by the 6th Framework Re-search Program of the European Union (EU), LUNAProject, IST contract no 33549,www.ist-luna.euReferencesJ.
Bilmes and K. Kirchhoff.
2003.
Factored languagemodels and generalized parallel backoff.
In NAACLHLT.J.
Bilmes and G. Zweig.
2002.
The graphical modelstoolkit: An open source software system for speechand time-series processing.
In IEEE ICASSP.H.
Bonneau-Maynard, S. Rosset, C. Ayache, A. Kuhn,D.
Mostefa, and the Media consortium.
2005.
Seman-tic annotation of the MEDIA corpus for spoken dialog.In ISCA Eurospeech.C.J.
Fillmore, C.R.
Johnson, and M.R.L.
Petruck.
2003.Background to framenet.
International Journal ofLexicography, 16.3:235?250.Y.
He and S. Young.
2005.
Spoken language understand-ing using the hidden vector state model.
Speech Com-munication, 48(3-4):262?275.F.
Lefe`vre.
2007.
Dynamic bayesian networks and dis-criminative classifiers for multi-stage semantic inter-pretation.
In IEEE ICASSP.J.B.
Lowe, C.F.
Baker, and C.J.
Fillmore.
1997.
A frame-semantic approach to semantic annotation.
In SIGLEXWorkshop: Why, What, and How?A.
Stolcke.
2002.
Srilm an extensible language model-ing toolkit.
In IEEE ICASSP.64
