A Plethora of Methods for Learning English CountabilityTimothy BaldwinCSLIStanford UniversityStanford, CA 94305 USAtbaldwin@csli.stanford.eduFrancis BondNTT Communication Science LaboratoriesNippon Telegraph and Telephone CorporationKyoto, Japanbond@cslab.kecl.ntt.co.jpAbstractThis paper compares a range of methodsfor classifying words based on linguis-tic diagnostics, focusing on the task oflearning countabilities for English nouns.We propose two basic approaches tofeature representation: distribution-basedrepresentation, which simply looks atthe distribution of features in the cor-pus data, and agreement-based represen-tation which analyses the level of token-wise agreement between multiple pre-processor systems.
We additionally com-pare a single multiclass classifier archi-tecture with a suite of binary classifiers,and combine analyses from multiple pre-processors.
Finally, we present and evalu-ate a feature selection method.1 IntroductionLexical acquisition can be described as the processof populating a grammar skeleton with lexical items,through a process of mapping word lemmata ontolexical types described in the grammar.
Dependingon the linguistic precision of the base grammar, lex-ical acquisition can range in complexity from sim-ple part-of-speech tagging (shallow lexical acquisi-tion) to the acquisition of selectionally-constrainedsubcategorisation frame clusters or constructionalcompatibilities (deep lexical acquisition).
Our par-ticular interest is in the latter task of deep lexicalacquisition with respect to English nouns.We are interested in developing learning tech-niques for deep lexical acquisition which take a fixedset of linguistic diagnostics, and classify words ac-cording to corpus data.
We propose a range of gen-eral techniques for this task, as exemplified over thetask of English countability acquisition.
Countabil-ity is the syntactic property that determines whethera noun can take singular and plural forms, and af-fects the range of permissible modifiers.
Manynouns have both countable and uncountable lemmas,with differences in meaning: I submitted two papers?documents?
(countable) vs.
Please use white paper?substance to be written on?
(uncountable).This research complements that described inBaldwin and Bond (2003), where we present the lin-guistic foundations and features drawn upon in thecountability classification task, and motivate theclaim that countability preferences can be learnedfrom corpus evidence.
In this paper, we focus onthe methods used to tackle the task of countabilityclassification based on this fixed feature set.The remainder of this paper is structured as fol-lows.
Section 2 outlines the countability classes,resources and pre-processors.
Section 3 presentstwo methods of representing the feature space.
Sec-tion 4 details the different classifier designs and thedataset, which are then evaluated in Section 5.
Fi-nally, we conclude the paper with a discussion inSection 6.2 PreliminariesIn this section, we describe the countability classes,the resources used in this research, and the featureextraction method.
These are described in greaterdetail in Baldwin and Bond (2003).2.1 Countability classesNouns are classified as belonging to one or more offour possible classes: countable, uncountable, pluralonly and bipartite.
Countable nouns can be modi-fied by denumerators, prototypically numbers, andhave a morphologically marked plural form: onedog, two dogs.
Uncountable nouns cannot be mod-ified by denumerators, but can be modified by un-specific quantifiers such as much; they do not showany number distinction (prototypically being singu-lar): *one equipment, some equipment, *two equip-ments.
Plural only nouns only have a plural form,such as goods, and cannot be either denumerated ormodified by much; many plural only nouns, suchas clothes, use the plural form even as modifiers: aclothes horse.
Bipartite nouns are plural when theyhead a noun phrase (trousers), but generally singu-lar when used as a modifier (trouser leg); they canbe denumerated with the classifier pair: a pair ofscissors.2.2 Gold standard dataInformation about noun countability was obtainedfrom two sources: COMLEX 3.0 (Grishman etal., 1998) and the common noun part of ALT-J/E?s Japanese-to-English semantic transfer dictio-nary (Ikehara et al, 1991).
Of the approximately22,000 noun entries in COMLEX, 13,622 are markedas countable, 710 as uncountable and the remainderare unmarked for countability.
ALT-J/E has 56,245English noun types with distinct countability.2.3 Feature spaceFeatures used in this research are divided up intofeature clusters, each of which is conditioned onthe occurrence of a target noun in a given construc-tion.
Feature clusters are either one-dimensional(describing a single multivariate feature) or two-dimensional (describing the interaction between twomultivariate features), with each dimension describ-ing a lexical or syntactic property of the construc-tion in question.
An example of a one-dimensionalfeature cluster is head noun number, i.e.
the num-ber (singular or plural) of the target noun when it oc-curs as the head of an NP; an example of a two-dimensional feature cluster in subject?verb agree-ment, i.e.
the number (singular or plural) of the tar-get noun when it occurs as head of a subject NPvs.
number agreement on the verb (singular or plu-ral).
Below, we provide a basic description of the10 feature clusters used in this research and their di-mensionality ([x]=1-dimensional feature cluster withx unit features, [x?y]=2-dimensional feature clusterwith x ?
y unit features).
These represent a total of206 unit features.Head noun number:[?]
the number of the targetnoun when it heads an NPModifier noun number:[?]
the number of the targetnoun when a modifier in an NPSubject?verb agreement:[???]
the number of thetarget noun in a subject position vs. numberagreement on the governing verbCoordinate noun number:[???]
the number of thetarget noun vs. the number of the head nouns ofconjunctsN of N constructions:[????]
the type of the N?
(e.g.COLLECTIVE, TEMPORAL) vs. the number of thetarget noun (N?)
in an N?
of N?
constructionOccurrence in PPs:[????]
the preposition type vs.the presence or absence of a determiner whenthe target noun occurs in singular form in a PPPronoun co-occurrence:[????]
what personal, pos-sessive and reflexive pronouns (e.g.
he, their,itself ) occur in the same sentence as singularand plural instances of the target nounSingular determiners:[??]
what singular-selectingdeterminers (e.g.
a, much) occur in NPs headedby the target noun in singular formPlural determiners:[??]
what plural-selecting de-terminers (e.g.
many, various) occur in NPsheaded by the target noun in plural formNon-bounded determiners:[????]
what non-bounded determiners (e.g.
more, sufficient)occur in NPs headed by the target noun, andwhat is the number of the target noun for each2.4 Feature extractionThe values for the features described above were ex-tracted from the written component of the BritishNational Corpus (BNC, Burnard (2000)) using threedifferent pre-processors: (a) a POS tagger, (b) a full-text chunker and (c) a dependency parser.
These areused independently to test the efficacy of the differ-ent systems at capturing features used in the clas-sification process, and in tandem to consolidate thestrengths of the individual methods.With the POS extraction method, we first taggedthe BNC using an fnTBL-based tagger (Ngai andFlorian, 2001) trained over the Brown and WSJ cor-pora and based on the Penn POS tagset.
We thenlemmatised this data using a Penn tagset-customisedversion of morph (Minnen et al, 2001).
Finally, weimplemented a range of high-precision, low-recallPOS-based templates to extract out the features fromthe processed data.For the chunker, we ran fnTBL over the lem-matised tagged data, training over CoNLL 2000-style (Tjong Kim Sang and Buchholz, 2000) chunk-converted versions of the full Brown and WSJ cor-pora.
For the NP-internal features (e.g.
determin-ers, head number), we used the noun chunks directly,or applied POS-based templates locally within nounchunks.
For inter-chunk features (e.g.
subject?verbagreement), we looked at only adjacent chunk pairsso as to maintain a high level of precision.We read dependency tuples directly off the outputof RASP (Briscoe and Carroll, 2002b) in grammati-cal relation mode.1 RASP has the advantage that re-call is high, although precision is potentially lower1We used the first parse in the experiments reported here.An alternative method would be to use weighted dependencytuples, as described in Briscoe and Carroll (2002a).than chunking or tagging as the parser is forced intoresolving phrase attachment ambiguities and com-mitting to a single phrase structure analysis.After generating the different feature vectors foreach noun based on the above configurations, we fil-tered out all nouns which did not occur at least 10times in NP head position in the output of all threesystems.
This resulted in a total of 20,530 nouns,of which 9,031 are contained in the combined COM-LEX and ALT-J/E lexicons.
The evaluation is basedon these 9,031 nouns.3 Feature representationWe test two basic feature representations in this re-search: distribution-based, which simply looks atthe relative occurrence of different features in thecorpus data, and agreement-based, which analysesthe level of token-wise agreement between multiplesystems.3.1 Distribution-based feature representationIn the distribution-based feature representation, wetake each target noun in turn and compare its amal-gamated value for each unit feature with (a) the val-ues for other target nouns, and (b) the value of otherunit features within that same feature cluster.
Thatis, we focus on the relative prominence of featuresglobally within the corpus and locally within eachfeature cluster.In the case of a one-dimensional feature cluster(e.g.
singular determiners), each unit feature f s fortarget noun w is translated into 3 separate featurevalues:corpfreq(f s,w) =freq(f s|w)freq(?)
(1)wordfreq(f s,w) =freq(f s|w)freq(w) (2)featfreq(f s,w) =freq(f s|w)?ifreq(f i|w)?
(3)where freq(?)
is the frequency of all words in the cor-pus.
That is, for each unit feature we capture the rel-ative corpus frequency, frequency relative to the tar-get word frequency, and frequency relative to otherfeatures in the same feature cluster.
Thus, for an n-valued one-dimensional feature cluster, we generate3n independent feature values.In the case of a two-dimensional feature ma-trix (e.g.
subject-position noun number vs. verbnumber agreement), each unit feature f s,t for tar-get noun w is translated into corpfreq(f s,t,w),wordfreq(f s,t,w) and featfreq(f s,t,w) as above,and 2 additional feature values:featdimfreq?
(f s,t,w) =freq(f s,t|w)?ifreq(f i,t|w)(4)featdimfreq?
(f s,t,w) =freq(f s,t|w)?j freq(f s,j |w)(5)which represent the featfreq values calculated alongeach of the two feature dimensions.
Additionally,we calculate cumulative totals for each row andcolumn of the feature matrix and describe each asfor the one-dimensional features above (in the formof 3 values).
Thus, for an m ?
n-valued two-dimensional feature cluster, we generate a total of5mn+ 3(m+ n) independent feature values.The feature clusters produce a combined total of1284 individual feature values.3.2 Agreement-based feature representationThe agreement-based feature representation con-siders the degree of token agreement between thefeatures extracted using the three different pre-processors.
This allows us to pinpoint the reliable di-agnostics within the corpus data and filter out noisegenerated by the individual pre-processors.It is possible to identify the features whichare positively-correlated with a unique countabilityclass (e.g.
occurrence of a singular noun with thedeterminer a occurs only for countable nouns), andfor each to determine the token-level agreement be-tween the different systems.
The number of diagnos-tics considered for each of the countability classesis: 32 for countable nouns, 19 for uncountable nounsand 1 for each of plural only and bipartite nouns.The total number of diagnostics we test agreementacross is thus 53.The token-level correlation for each feature f s iscalculated fourfold according to relative agreement,the ?
statistic, correlated frequency and correlatedweight.
The relative agreement between systemssys?
and sys?
wrt f s for target noun w is defined tobe:agr(f s,w)(sys?, sys?)
=|tok(f s,w)(sys?)
?
tok(f s,w)(sys?
)||tok(f s,w)(sys?)
?
tok(f s,w)(sys?
)|where tok (f s,w)(sys i) returns the set of token in-stances of (f s,w).
The ?
statistic (Carletta, 1996)is recast as:?
(f s,w)(sys?, sys?)
=agr(f s,w)(sys?, sys?)?
?agr(f s,?)(sys?,sys?)N??
?agr(f s,?)(sys?,sys?
)NIn this modified form, ?
(f s,w) represents the diver-gence in relative agreement wrt f s for target noun w ,relative to the mean relative agreement wrt f s overall words.
Correlated frequency is defined to be:cfreq(f s,w)(sys?, sys?)
=|tok(f s,w)(sys?)
?
tok(f s,w)(sys?
)|freq(w)It describes the occurrence of tokens in agreementfor (f s,w) relative to the total occurrence of the tar-get word.The metrics are used to derive three separate fea-ture values for each diagnostic over the three pre-processor system pairings.
We additionally calcu-late the mean value of each metric across the systempairings and the overall correlated weight for eachcountability class C as:cw(C ,w)(sys?, sys?)
=?f s?C |tok(f s,w)(sys?)
?
tok(f s,w)(sys?
)|?i|tok(f i,w)(sys?)
?
tok(f i,w)(sys?
)|Correlated weight describes the occurrence of corre-lated features in the given countability class relativeto other correlated features.We test agreement: (a) for each of these diag-nostics individually and within each countabilityclass (Agree(Token,?
)), and (b) across the amalgamof diagnostics for each of the countability classes(Agree(Class,?)).
For Agree(Token,?
), we calculateagr , ?
and cfreq values for each of the 53 diag-nostics across the 3 system pairings, and addition-ally calculate the mean value for each value.
Weadditionally calculate the overall cw value for eachcountability class.
This results in a total of 640 fea-ture values (3?
53?
3 + 53?
3 + 4).
In the caseof Agree(Class,?
), we average the agr , ?
and cfreqvalues across each countability class for each of thethree system pairings, and also calculate the meanvalue in each case.
We further calculate the overallcw value for each countability class, culminating in52 feature values (3?
4?
3 + 4?
3 + 4).4 Classifier Set-up and EvaluationBelow, we outline the different classifiers testedand describe the process used to generate the gold-standard data.4.1 Classifier architecturesWe propose a variety of unsupervised and super-vised classifier architectures for the task of learningcountability, and also a feature selection method.
Inall cases, our classifiers are built using TiMBL ver-sion 4.2 (Daelemans et al, 2002), a memory-basedclassification system based on the k-nearest neigh-bour algorithm.
As a result of extensive parame-ter optimisation, we settled on the default configu-ration2 for TiMBL with k set to 9.32IB1 with weighted overlap, gain ratio-based featureweighting and equal weighting of neighbours.3We additionally experimented with the kernel-basedTinySVM system, but found TiMBL to be the marginally supe-rior performer in all cases, a somewhat surprising result giventhe high-dimensionality of the feature space.Full-feature supervised classifiersThe simplest system architecture applies the su-pervised learning paradigm to the distribution-basedfeature vectors for each of the POS tagger, chun-ker and RASP (Dist(POS,?
), Dist(chunk,?)
andDist(RASP,?
), respectively).
For the distribution-based feature representation, we additionallycombine the outputs of the three pre-processors by:(a) concatenating the individual distribution-basedfeature vectors for the three systems (resulting ina 3852-element feature vector: Dist(AllCON,?
));and (b) taking the mean over the three systems foreach distribution-based feature value (resulting ina 1284-element feature vector: Dist(AllMEAN,?
)).The agreement-based feature representationprovides two additional system configurations:Agree(Class,?)
and Agree(Token,?)
(see Section3.2).Orthogonal to the issue of how to generate thefeature values is the question of how to classifya given noun according to the different countabil-ity classes.
The two basic options here are to ei-ther have a single classifier and define multiclassesaccording to all observed combinations of count-ability classes (Dist(?,SINGLE)), or have a suite ofbinary classifiers, one for each countability class(Dist(?,SUITE)).
The SINGLE classifier architec-ture has advantages in terms of speed (a 4?
speed-up over the classifier suite) and simplicity, but runsinto problems with data sparseness for the less-commonly attested multi-classes given that a singlenoun can occur with multiple countabilities.
TheSUITE classifier architecture delineates the differentcountability classes more directly, but runs the riskof a noun not being classified according to any of thefour classes.Feature-selecting supervised classifiersWe improve the performance of the basic classi-fiers by way of best-N filter-based feature selection.Feature selection has been shown to improve clas-sification accuracy over a variety of tasks (Liu andMotoda, 1988), but in the case of memory-basedlearners such as TiMBL, has the additional advan-tage of accelerating the classification process and re-ducing memory overhead.
The computational com-plexity of memory-based learners is proportional tothe number of features, so any reduction in the fea-ture space leads to a proportionate reduction in com-putational time.
For tasks such as countability clas-sification with a large number of both feature valuesand test instances (particularly if we are to classifyall noun types in a given corpus), this speed-up isvital.Our feature selection method uses a combinedfeature relevance metric to estimate the best-N fea-tures for each countability class, and then restrictsthe classifier to operate over only those N features.Feature relevance is estimated through analysis ofthe correspondence between class and feature val-ues for a given feature, through metrics includingshared variance and information gain.
These indi-vidual metrics tend to be biased toward particularfeatures: information gain and gain ratio, e.g., tendto favour features of higher cardinality (White andLiu, 1994).
In order to minimise such bias, wegenerate a feature ranking for each feature selec-tion metric (based on the relative feature relevancescores), and simply add the absolute ranks for eachfeature together.
By re-ranking the features in in-creasing order of summed rank, we can generate ageneralised feature relevance ranking.
We are nowin a position to prune the feature space to a pre-determined size, by taking the best-N features in thefeature ranking.The feature selection metrics we combine arethose implemented in TiMBL, namely: shared vari-ance, chi-square, information gain and gain ratio.Unsupervised classifierIn order to derive a common baseline for the dif-ferent systems, we built an unsupervised classifierwhich, for each target noun, simply checks to seeif any diagnostic (as used in the agreement-basedfeature representation) was detected for each of thecountability classes; even a single occurrence ofa diagnostic is taken to be sufficient evidence formembership in that countability class.
Elementarysystem combination is achieved by voting betweenthe three pre-processor outputs as to whether the tar-get noun belongs to a given countability class.
Thatis, the target noun is classified as belonging to agiven countability class iff at least two of the pre-processors furnish linguistic evidence for member-ship in that class.4.2 Training dataTraining data was generated independently for theSINGLE and SUITE classifiers.
In each case, we firstextracted all countability-annotated nouns from eachof the ALT-J/E and COMLEX lexicons which are at-tested at least 10 times in the BNC, and composedthe training data from these pre-filtered sets.
In thecase of the SINGLE classifier, we simply classifiedwords according to the union of all countabilitiesfrom ALT-J/E and COMLEX, resulting in the follow-ing dataset:Count Uncount Plural Bipart No.
Freq1 0 0 0 4068 .6850 1 0 0 1134 .1910 0 1 0 35 .0060 0 0 1 10 .0021 1 0 0 650 .1101 0 1 0 13 .0020 1 1 0 13 .0020 0 1 1 5 .0011 1 1 0 8 .001From this, it is evident that some class combinations(e.g.
plural only+bipartite) are highly infrequent, hint-ing at a problem with data sparseness.For the SUITE classifier, we generate the positiveexemplars for the countable and uncountable classesfrom the intersection of the COMLEX and ALT-J/Edata for that class; negative exemplars, on the otherhand, are those not annotated as belonging to thatclass in either lexicon.
With the plural only andbipartite data, COMLEX cannot be used as it doesnot describe these two classes.
We thus took allmembers of each class listed in ALT-J/E as our pos-itive exemplars, and all remaining nouns with non-identical singular and plural forms as negative ex-emplars.
This resulted in the following datasets:Class Positive data Negative dataCountable 4,342 1,476Uncountable 1,519 5,471Plural only 84 5,639Bipartite 35 5,6395 EvaluationEvaluation of the supervised classifiers was carriedout based on 10-fold stratified cross-validation overthe relevant dataset, and results presented here areaveraged over the 10 iterations.
Classifier perfor-mance is rated according to classification accuracy(the proportion of instances classified correctly) andF-score (?
= 1).
In the case of the SINGLE classifier,the class-wise F-score is calculated by decomposingthe multiclass labels into their components.
A count-able+uncountable instance misclassified as countable,for example, would count as a misclassification interms of classification accuracy, a correct classifica-tion in the calculation of the countable F-score, and amisclassification in the calculation of the uncountableF-score.
Note that the SINGLE classifier is run over adifferent dataset to each member of the SUITE clas-sifier, and cross-comparison of the classification ac-curacies is not representative of the relative systemperformance (classification accuracies for the SIN-GLE classifier are given in parentheses to reinforcethis point).
Classification accuracies are thus simplyused for classifier comparison within a basic classi-fier architecture (SINGLE or SUITE), and F-score isClassifier Accuracy F-scoreMajority class .746 .855Unsupervised .798 .879Dist(POS,SUITE) .928 .953Dist(POS,SINGLE) (.850) .940Dist(chunk,SUITE) .933 .956Dist(chunk,SINGLE) (.853) .942Dist(RASP,SUITE) .923 .950Dist(RASP,SINGLE) (.847) .940Dist(AllCON,SUITE) .939 .960Dist(AllCON,SINGLE) (.857) .944Dist(AllMEAN,SUITE) .937 .959Agree(Token,SUITE) .902 .936Agree(Class,SUITE) .911 .941Table 1: Basic results for countable nounsClassifier Accuracy F-scoreMajority class .783 (.357)Unsupervised .342 .391Dist(POS,SUITE) .945 .876Dist(POS,SINGLE) (.850) .861Dist(chunk,SUITE) .945 .876Dist(chunk,SINGLE) (.853) .861Dist(RASP,SUITE) .944 .872Dist(RASP,SINGLE) (.847) .851Dist(AllCON,SUITE) .952 .892Dist(AllCON,SINGLE) (.857) .873Dist(AllMEAN,SUITE) .954 .895Agree(Token,SUITE) .923 .825Agree(Class,SUITE) .923 .824Table 2: Basic results for uncountable nounsthe evaluation metric of choice for overall evalua-tion.We present the results for two baseline systemsfor each countability class: a majority-class clas-sifier and the unsupervised method.
The Majorityclass system is run over the binary data used bythe SUITE classifier for the given class, and sim-ply classifies all instances according to the mostcommonly-attested class in that dataset.
Irrespectiveof the majority class, we calculate the F-score basedon a positive-class classifier, i.e.
a classifier whichnaively classifies each instance as belonging to thegiven class; in the case that the positive class is notthe majority class, the F-score is given in parenthe-ses.The results for the different system configurationsover the four countability classes are presented inTables 1?4, in which the highest classification accu-racy and F-score values for each class are presentedin boldface.
The classifier Dist(AllCON,SUITE), forexample, applies the distribution-based feature rep-resentation in a SUITE classifier configuration (i.e.it tests for binary membership in each countabilityclass), using the concatenated feature vectors fromeach of the tagger, chunker and RASP.Items of note in the results are:Classifier Accuracy F-scoreMajority class .985 (.023)Unsupervised .411 .033Dist(POS,SUITE) .989 .558Dist(POS,SINGLE) (.850) .479Dist(chunk,SUITE) .990 .568Dist(chunk,SINGLE) (.853) .495Dist(RASP,SUITE) .989 .415Dist(RASP,SINGLE) (.847) .360Dist(AllCON,SUITE) .990 .582Dist(AllCON,SINGLE) (.857) .500Dist(AllMEAN,SUITE) .990 .575Agree(Token,SUITE) .988 .409Agree(Class,SUITE) .988 .401Table 3: Basic results for plural only nounsClassifier Accuracy F-scoreMajority class .994 (.012)Unsupervised .931 .137Dist(POS,SUITE) .997 .752Dist(POS,SINGLE) (.850) .857Dist(chunk,SUITE) .997 .704Dist(chunk,SINGLE) (.853) .865Dist(RASP,SUITE) .997 .700Dist(RASP,SINGLE) (.847) .798Dist(AllCON,SUITE) .996 .723Dist(AllCON,SINGLE) (.857) .730Dist(AllMEAN,SUITE) .997 .710Agree(Token,SUITE) .997 .710Agree(Class,SUITE) .997 .695Table 4: Basic results for bipartite nouns?
all system configurations surpass both themajority-class baseline and unsupervised clas-sifier in terms of F-score?
for all other than bipartite nouns, the SUITEclassifier outperforms the SINGLE classifier interms of F-score?
the best of the distribution-based classifierswas, without exception, superior to the best ofthe agreement-based classifiers?
chunk-based feature extraction generally pro-duced superior performance to POS tag-basedfeature extraction, which was in turn gener-ally better than RASP-based feature extraction;statistically significant differences in F-score(based on the two-tailed t-test, p < .05) wereobserved for both chunking and tagging overRASP for the plural only class, and chunkingover RASP for the countable class?
for the SUITE classifier, system combinationby either concatenation (Dist(AllCON,SUITE))or averaging over the individual feature val-ues (Dist(AllMEAN,SUITE)) generally led to astatistically significant improvement over eachof the individual systems for the countable0.750.80.850.90.951100  10000.1110100F-scoreInstances/secNo.
Features (N)rand-N (countable)best-N (countable)best-N (uncountable)rand-N (uncountable)best-N (countable)best-N (uncountable)Figure 1: Effects of feature selectionand uncountable classes,4 but there was nostatistical difference between these two archi-tectures for any of the 4 countability classes;for the SINGLE classifier, system combination(Dist(AllCON,SUITE)) did not lead to a signifi-cant performance gainTo evaluate the effects of feature selection, wegraphed the F-score value and processing time (ininstances processed per second5) over values ofN from 25 to the full feature set.
We targetedthe Dist(AllCON,SUITE) system for evaluation (3852features), and ran it over both the countable and un-countable classes.6 We additionally carried out ran-dom feature selection as a baseline to compare thefeature selection results against.
Note that the x-axis(N ) and right y-axis (instances/sec) are both log-arithmic, such that the linear right-decreasing timecurves are indicative of the direct proportionality be-tween the number of features and processing time.The differential in F-score for the best-N configura-tion as compared to the full feature set is statisticallyinsignificant for N > 100 for countable nouns andN > 50 for uncountable nouns.
That is, feature se-lection facilitates a relative speed-up of around 30?without a significant drop in F-score.
Comparing theresults for the best-N and rand-N features, the dif-ference in F-score was statistically significant for allvalues of N < 1000.
The proposed method of fea-ture selection thus allows us to maintain the full clas-sification potential of the feature set while enabling4No significant performance difference was observed for:Dist(ChunkMEAN,SUITE) vs. Dist(All?,SUITE) for countablenouns, and Dist(POSCON,SUITE) vs. Dist(AllCON,SUITE) foruncountable nouns.5As evaluated on an AMD Athlon 2100+ CPU with 3GB ofmemory.6We focus exclusively on countable and uncountable nounshere and in the remainder of supplementary evaluation as theseare by far the most populous countability classes.Feature COUNTABLE UNCOUNTABLEspace Acc F-score Acc F-scoreAll features .937 .959 .954 .895Best-200 .934 .956 .949 .884Binary .904?
.931?
.930?
.833?Corpus freq .929 .954 .952 .889Word freq .933 .956 .954 .896Feature freq .928 .952?
.934?
.869?Table 5: Results for restricted feature setsa speedup greater than an order of magnitude, po-tentially making the difference in practical utility forthe proposed method.To determine the relative impact of the com-ponent feature values on the performance of thedistribution-based feature representation, we usedthe Dist(AllMEAN,SUITE) configuration to build: (a)a classifier using a single binary value for eachunit feature, based on simple corpus occurrence (Bi-nary); and (b) 3 separate classifiers based on each ofthe corpfreq , wordfreq and featfreq features valuesonly (without the 2D feature cluster totals).
In eachcase, the total number of feature values is 206.The results for each of these classifiers overcountable and uncountable nouns are pre-sented in Table 5, as compared to the basicDist(AllMEAN,SUITE) classifier with all 1,284features (All features) and also the best-200 features(Best-200).
Results which differ from those forAll features to a level of statistical significance areasterisked.
The binary classifiers performed signif-icantly worse than All features for both countableand uncountable nouns, underlining the utility of thedistribution-based feature representation.
wordfreqis marginally superior to corpfreq as a standalonefeature representation, and both of these were onthe whole slightly below the full feature set inperformance (although no significant difference wasobserved).
featfreq performed slightly worse again,significantly below the level of the full feature set.Results for the best-200 classifier were marginallyhigher than those for each of the individual featurerepresentations in the case of the countable class,but marginally below the results for corpfreq andwordfreq in the case of the uncountable class.
Thedifferences here are not statistically significant, andadditional evaluation is required to determine therelative success of feature selection over simplyusing wordfreq values, for example.6 DiscussionThere have been at least three earlier approachesto the automatic determination of countability:two using semantic cues and one using cor-pora.
Bond and Vatikiotis-Bateson (2002) deter-mine a noun?s countability preferences?as de-fined in a 5-way classification?from its se-mantic class in the ALT-J/E lexicon, and showthat semantics predicts countability 78% of thetime.
O?Hara et al (2003) implemented a sim-ilar approach using the much larger Cyc on-tology and achieved 89.5% accuracy, mappingonto the 2 classes of countable and uncount-able.
Schwartz (2002) learned noun countabilitiesby looking at determiner occurrence in singularnoun chunks and was able to tag 11.7% of BNCnoun tokens as countable and 39.5% as uncountable,achieving a noun type agreement of 88% and 44%,respectively, with the ALT-J/E lexicon.
Our resultscompare favourably with each of these.In a separate evaluation, we took the best-performing classifier (Dist(AllCON,SUITE)) and ranit over open data, using best-500 feature selection(Baldwin and Bond, 2003).
The output of theclassifier was evaluated relative to hand-annotateddata, and the level of agreement found to be around92.4%, which is approximately equivalent to theagreement between COMLEX and ALT-J/E of 93.8%.In conclusion, we have presented a plethora oflearning techniques for deep lexical acquisition fromcorpus data, and applied each to the task of classify-ing English nouns for countability.
We specificallycompared two feature representations, based on rel-ative feature occurrence and token-level classifica-tion, and two basic classifier architectures, using asuite of binary classifiers and a single multi-classclassifier.
We also analysed the effects of comb-ing the output of multiple pre-processors, and pre-sented a simple feature selection method.
Overall,the best results were obtained using a distribution-based suite of binary classifiers combining the out-put of multiple pre-processors.AcknowledgementsThis material is based upon work supported by the NationalScience Foundation under Grant No.
BCS-0094638 and alsothe Research Collaboration between NTT Communication Sci-ence Laboratories, Nippon Telegraph and Telephone Corpora-tion and CSLI, Stanford University.
We would like to thankLeonoor van der Beek, Slaven Bilac, Ann Copestake, Ivan Sagand the three anonymous reviewers for their valuable input onthis research, and John Carroll for providing access to RASP.ReferencesTimothy Baldwin and Francis Bond.
2003.
Learning the count-ability of English nouns from corpus data.
In Proc.
of the41st Annual Meeting of the ACL, Sapporo, Japan.
(to ap-pear).Francis Bond and Caitlin Vatikiotis-Bateson.
2002.
Using anontology to determine English countability.
In Proc.
of the19th International Conference on Computational Linguistics(COLING 2002), Taipei, Taiwan.Ted Briscoe and John Carroll.
2002a.
High precision extractionof grammatical relations.
In Proc.
of the 19th InternationalConference on Computational Linguistics (COLING 2002),pages 134?140, Taipei, Taiwan.Ted Briscoe and John Carroll.
2002b.
Robust accurate sta-tistical annotation of general text.
In Proc.
of the 3rd In-ternational Conference on Language Resources and Evalu-ation (LREC 2002), pages 1499?1504, Las Palmas, CanaryIslands.Lou Burnard.
2000.
User Reference Guide for the British Na-tional Corpus.
Technical report, Oxford University Comput-ing Services.Jean Carletta.
1996.
Assessing agreement on classifica-tion tasks: the kappa statistic.
Computational Linguistics,22(2):249?254.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-tal van den Bosch.
2002.
TiMBL: Tilburg memory basedlearner, version 4.2, reference guide.
ILK technical report02-01.Ralph Grishman, Catherine Macleod, and Adam Myers, 1998.COMLEX Syntax Reference Manual.
Proteus Project, NYU.
(http://nlp.cs.nyu.edu/comlex/refman.ps).Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and HiromiNakaiwa.
1991.
Toward an MT system without pre-editing?
effects of new methods in ALT-J/E?.
In Proc.
of the ThirdMachine Translation Summit (MT Summit III), pages 101?106, Washington DC, USA.Huan Liu and Hiroshi Motoda.
1988.
Feature Extraction, Con-struction and Selection: A Data Mining Perspective.
KluwerAcademic Publishers.Guido Minnen, John Carroll, and Darren Pearce.
2001.
Ap-plied morphological processing of English.
Natural Lan-guage Engineering, 7(3):207?23.Grace Ngai and Radu Florian.
2001.
Transformation-basedlearning in the fast lane.
In Proc.
of the 2nd Annual Meetingof the North American Chapter of Association for Compu-tational Linguistics (NAACL2001), pages 40?7, Pittsburgh,USA.Tom O?Hara, Nancy Salay, Michael Witbrock, Dave Schnei-der, Bjoern Aldag, Stefano Bertolo, Kathy Panton, FritzLehmann, Matt Smith, David Baxter, Jon Curtis, and PeterWagner.
2003.
Inducing criteria for mass noun lexical map-pings using the Cyc KB and its extension to WordNet.
InProc.
of the Fifth International Workshop on ComputationalSemantics (IWCS-5), Tilburg, the Netherlands.Lane O.B.
Schwartz.
2002.
Corpus-based acquisition of headnoun countability features.
Master?s thesis, Cambridge Uni-versity, Cambridge, UK.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.
Introduc-tion to the CoNLL-2000 shared task: Chunking.
In Proc.of the 4th Conference on Computational Natural LanguageLearning (CoNLL-2000), Lisbon, Portugal.Allan P. White and Wei Zhong Liu.
1994.
Bias in information-based measures in decision tree induction.
Machine Learn-ing, 15(3):321?9.
