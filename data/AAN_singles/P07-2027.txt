Proceedings of the ACL 2007 Demo and Poster Sessions, pages 105?108,Prague, June 2007. c?2007 Association for Computational LinguisticsDisambiguating Between Generic and Referential ?You?
in Dialog?Surabhi GuptaDepartment of Computer ScienceStanford UniversityStanford, CA 94305, USsurabhi@cs.stanford.eduMatthew PurverCenter for the Studyof Language and InformationStanford UniversityStanford, CA 94305, USmpurver@stanford.eduDan JurafskyDepartment of LinguisticsStanford UniversityStanford, CA 94305, USjurafsky@stanford.eduAbstractWe describe an algorithm for a novel task: disam-biguating the pronoun you in conversation.
You canbe generic or referential; finding referential you is im-portant for tasks such as addressee identification orextracting ?owners?
of action items.
Our classifierachieves 84% accuracy in two-person conversations;an initial study shows promising performance even onmore complex multi-party meetings.1 Introduction and BackgroundThis paper describes an algorithm for disambiguat-ing the generic and referential senses of the pronounyou.Our overall aim is the extraction of action itemsfrom multi-party human-human conversations, con-crete decisions in which one (or more) individualstake on a group commitment to perform a given task(Purver et al, 2006).
Besides identifying the task it-self, it is crucial to determine the owner, or personresponsible.
Occasionally, the name of the responsi-ble party is mentioned explicitly.
More usually, theowner is addressed directly and therefore referred tousing a second-person pronoun, as in example (1).1(1)A: and um if you can get that binding point alsomaybe with a nice example that would be helpfulfor Johno and me.B: Oh yeah uh O K.It can also be important to distinguish betweensingular and plural reference, as in example (2)where the task is assigned to more than one person:(2)A: So y- so you guys will send to the rest of us um aversion of um, this, and - the - uh, description -B: With sugge- yeah, suggested improvements and -Use of ?you?
might therefore help us both in de-?This work was supported by the CALO project(DARPA grant NBCH-D-03-0010) and ONR (MURI awardN000140510388).
The authors also thank John Niekrasz forannotating our test data.1(1,2) are taken from the ICSI Meeting Corpus (Shriberg etal., 2004); (3,4) from Switchboard (Godfrey et al, 1992).tecting the fact that a task is being assigned, and inidentifying the owner.
While there is an increas-ing body of work concerning addressee identifica-tion (Katzenmaier et al, 2004; Jovanovic et al,2006), there is very little investigating the problemof second-person pronoun resolution, and it is thisthat we address here.
Most cases of ?you?
do not infact refer to the addressee but are generic, as in ex-ample (3); automatic referentiality classification istherefore very important.
(3)B: Well, usually what you do is just wait until youthink it?s stopped,and then you patch them up.2 Related WorkPrevious linguistic work has recognized that ?you?is not always addressee-referring, differentiating be-tween generic and referential uses (Holmes, 1998;Meyers, 1990) as well as idiomatic cases of ?youknow?.
For example, (Jurafsky et al, 2002) foundthat ?you know?
covered 47% of cases, the referen-tial class 22%, and the generic class 27%, with nosignificant differences in surface form (duration orvowel reduction) between the different cases.While there seems to be no previous work investi-gating automatic classification, there is related workon classifying ?it?, which also takes various referen-tial and non-referential readings: (Mu?ller, 2006) uselexical and syntactic features in a rule-based clas-sifier to detect non-referential uses, achieving rawaccuracies around 74-80% and F-scores 63-69%.3 DataWe used the Switchboard corpus of two-party tele-phone conversations (Godfrey et al, 1992), and an-notated the data with four classes: generic, referen-tial singular, referential plural and a reported refer-ential class, for mention in reported speech of an105Training TestingGeneric 360 79Referential singular 287 92Referential plural 17 3Reported referential 5 1Ambiguous 4 1Total 673 176Table 1: Number of cases found.originally referential use (as the original addresseemay not be the current addressee ?
see example (4)).We allowed a separate class for genuinely ambigu-ous cases.
Switchboard explicitly tags ?you know?when used as a discourse marker; as this (generic)case is common and seems trivial we removed itfrom our data.
(4)B: Well, uh, I guess probably the last one I went to Imet so many people that I had not seen in proba-bly ten, over ten years.It was like, don?t you remember me.And I am like no.A: Am I related to you?To test inter-annotator agreement, two people an-notated 4 conversations, yielding 85 utterances con-taining ?you?
; the task was reported to be easy, andthe kappa was 100%.We then annotated a total of 42 conversations fortraining and 13 for testing.
Different labelers an-notated the training and test sets; none of the au-thors were involved in labeling the test set.
Table 1presents information about the number of instancesof each of these classes found.4 FeaturesAll features used for classifier experiments wereextracted from the Switchboard LDC Treebank 3release, which includes transcripts, part of speechinformation using the Penn tagset (Marcus et al,1994) and dialog act tags (Jurafsky et al, 1997).Features fell into four main categories:2 senten-tial features which capture lexical features of theutterance itself; part-of-speech features which cap-ture shallow syntactic patterns; dialog act featurescapturing the discourse function of the current ut-terance and surrounding context; and context fea-tures which give oracle information (i.e., the cor-rect generic/referential label) about preceding uses2Currently, features are all based on perfect transcriptions.of ?you?.
We also investigated using the presenceof a question mark in the transcription as a feature,as a possible replacement for some dialog act fea-tures.
Table 2 presents our features in detail.N FeaturesSentential Features (Sent)2 you, you know, you guysN number of you, your, yourself2 you (say|said|tell|told|mention(ed)|mean(t)|sound(ed))2 you (hear|heard)2 (do|does|did|have|has|had|are|could|should|n?t) you2 ?if you?2 (which|what|where|when|how) youPart of Speech Features (POS)2 Comparative JJR tag2 you (VB*)2 (I|we) (VB*)2 (PRP*) youDialog Act Features (DA)46 DA tag of current utterance i46 DA tag of previous utterance i ?
146 DA tag of utterance i ?
22 Presence of any question DA tag (Q DA)2 Presence of elaboration DA tagOracle Context Features (Ctxt)3 Class of utterance i ?
13 Class of utterance i ?
23 Class of previous utterance by same speaker3 Class of previous labeled utteranceOther Features (QM)2 Question markTable 2: Features investigated.
N indicates the num-ber of possible values (there are 46 DA tags; contextfeatures can be generic, referential or N/A).5 Experiments and ResultsAs Table 1 shows, there are very few occurrencesof the referential plural, reported referential and am-biguous classes.
We therefore decided to model ourproblem as a two way classification task, predictinggeneric versus referential (collapsing referential sin-gular and plural as one category).
Note that we ex-pect this to be the major useful distinction for ouroverall action-item detection task.Baseline A simple baseline involves predicting thedominant class (in the test set, referential).
Thisgives 54.59% accuracy (see Table 1).3SVM Results We used LIBSVM (Chang and Lin,2001), a support vector machine classifier trainedusing an RBF kernel.
Table 3 presents results for3Precision and recall are of course 54.59% and 100%.106Features Accuracy F-ScoreCtxt 45.66% 0%Baseline 54.59% 70.63%Sent 67.05% 57.14%Sent + Ctxt + POS 67.05% 57.14%Sent + Ctxt + POS + QM 76.30% 72.84%Sent + Ctxt + POS + Q DA 79.19% 77.50%DA 80.92% 79.75%Sent + Ctxt + POS +QM + DA 84.39% 84.21%Table 3: SVM results: generic versus referentialvarious selected sets of features.
The best set of fea-tures gave accuracy of 84.39% and f-score 84.21%.Discussion Overall performance is respectable;precision was consistently high (94% for thehighest-accuracy result).
Perhaps surprisingly, noneof the context or part-of-speech features were foundto be useful; however, dialog act features provedvery useful ?
using these features alone give usan accuracy of 80.92% ?
with the referential classstrongly associated with question dialog acts.We used manually produced dialog act tags, andautomatic labeling accuracy with this fine-grainedtagset will be low; we would therefore prefer touse more robust features if possible.
We found thatone such heuristic feature, the presence of ques-tion mark, cannot entirely substitute: accuracy isreduced to 76.3%.
However, using only the binaryQ DA feature (which clusters together all the dif-ferent kinds of question DAs) does better (79.19%).Although worse than performance with a full tagset,this gives hope that using a coarse-grained set oftags might allow reasonable results.
As (Stolcke etal., 2000) report good accuracy (87%) for statementvs.
question classification on manual Switchboardtranscripts, such coarse-grained information mightbe reliably available.Surprisingly, using the oracle context features (thecorrect classification for the previous you) alone per-forms worse than the baseline; and adding these fea-tures to sentential features gives no improvement.This suggests that the generic/referential status ofeach you may be independent of previous yous.Features Accuracy F-ScoreProsodic only 46.66% 44.31%Baseline 54.59% 70.63%Sent + Ctxt + POS +QM + DA + Prosodic 84.39% 84.21%Table 4: SVM results: prosodic featuresCategory Referential GenericCount 294 340Pitch (Hz) 156.18 143.98Intensity (dB) 60.06 59.41Duration (msec) 139.50 136.84Table 5: Prosodic feature analysis6 Prosodic FeaturesWe next checked a set of prosodic features, test-ing the hypothesis that generics are prosodically re-duced.
Mean pitch, intensity and duration were ex-tracted using Praat, both averaged over the entireutterance and just for the word ?you?.
Classifi-cation results are shown in Table 4.
Using onlyprosodic features performs below the baseline; in-cluding prosodic features with the best-performingfeature set from Table 3 gives identical performanceto that with lexical and contextual features alone.To see why the prosodic features did not help, weexamined the difference between the average pitch,intensity and duration for referential versus genericcases (Table 5).
A one-sided t-test shows no signif-icant differences between the average intensity andduration (confirming the results of (Jurafsky et al,2002), who found no significant change in duration).The difference in the average pitch was found to besignificant (p=0.2) ?
but not enough for this featurealone to cause an increase in overall accuracy.7 Error AnalysisWe performed an error analysis on our best classi-fier output on the training set; accuracy was 94.53%,giving a total of 36 errors.Half of the errors (18 of 36) were ambiguous evenfor humans (the authors), if looking at the sentencealone without the neighboring context from the ac-tual conversation ?
see (5a).
Treating these exam-ples thus needs a detailed model of dialog context.The other major class of errors requires detailed107knowledge about sentential semantics and/or theworld ?
see e.g.
(5b,c), which we can tell are ref-erential because they predicate inter-personal com-parison or communication.In addition, as questions are such a useful feature(see above), the classifier tends to label all questioncases as referential.
However, generic uses do occurwithin questions (5d), especially if rhetorical (5e):(5) a. so uh and if you don?t have the money then use acredit cardb.
I?m probably older than youc.
although uh I will personally tell you I used to workat a bankd.
Do they survive longer if you plant them in the wintertime?e.
my question I guess are they really your peers?8 Initial Multi-Party ExperimentsThe experiments above used two-person dialog data:we expect that multi-party data is more complex.
Weperformed an initial exploratory study, applying thesame classes and features to multi-party meetings.Two annotators labeled one meeting from theAMI corpus (Carletta et al, 2006), giving a total of52 utterances containing ?you?
on which to assessagreement: kappa was 87.18% for two way clas-sification of generic versus referential.
One of theauthors then labeled a testing set of 203 utterances;104 are generic and 99 referential, giving a baselineaccuracy of 51.23% (and F-score of 67.65%).We performed experiments for the same task: de-tecting generic versus referential uses.
Due to thesmall amount of data, we trained the classifier on theSwitchboard training set from section 3 (i.e.
on two-party rather than multi-party data).
Lacking part-of-speech or dialog act features (since the dialog acttagset differs from the Switchboard tagset), we usedonly the sentential, context and question mark fea-tures described in Table 2.However, the classifier still achieves an accuracyof 73.89% and F-score of 74.15%, comparable to theresults on Switchboard without dialog act features(accuracy 76.30%).
Precision is lower, though (bothprecision and recall are 73-75%).9 ConclusionsWe have presented results on two person and multi-party data for the task of generic versus referential?you?
detection.
We have seen that the problem isa real one: in both datasets the distribution of theclasses is approximately 50/50, and baseline accu-racy is low.
Classifier accuracy on two-party data isreasonable, and we see promising results on multi-party data with a basic set of features.
We expect theaccuracy to go up once we train and test on same-genre data and also add features that are more spe-cific to multi-party data.ReferencesJ.
Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot,T.
Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal,G.
Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post,D.
Reidsma, and P. Wellner.
2006.
The AMI meeting cor-pus.
In MLMI 2005, Revised Selected Papers.C.-C. Chang and C.-J.
Lin, 2001.
LIBSVM: a library forSupport Vector Machines.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.J.
J. Godfrey, E. Holliman, and J. McDaniel.
1992.
SWITCH-BOARD: Telephone speech corpus for research and devel-opment.
In Proceedings of IEEE ICASSP-92.J.
Holmes.
1998.
Generic pronouns in the Wellington corpusof spoken New Zealand English.
Ko?tare, 1(1).N.
Jovanovic, R. op den Akker, and A. Nijholt.
2006.
Ad-dressee identification in face-to-face meetings.
In Proceed-ings of the 11th Conference of the EACL.D.
Jurafsky, E. Shriberg, and D. Biasca.
1997.
Switch-board SWBD-DAMSL shallow-discourse-function annota-tion coders manual, draft 13.
Technical Report 97-02, Uni-versity of Colorado, Boulder.D.
Jurafsky, A.
Bell, and C. Girand.
2002.
The role of thelemma in form variation.
In C. Gussenhoven and N. Warner,editors, Papers in Laboratory Phonology VII, pages 1?34.M.
Katzenmaier, R. Stiefelhagen, and T. Schultz.
2004.
Iden-tifying the addressee in human-human-robot interactionsbased on head pose and speech.
In Proceedings of the 6thInternational Conference on Multimodal Interfaces.M.
Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies,M.
Ferguson, K. Katz, and B. Schasberger.
1994.
The Penntreebank: Annotating predicate argument structure.
In ARPAHuman Language Technology Workshop.M.
W. Meyers.
1990.
Current generic pronoun usage.
Ameri-can Speech, 65(3):228?237.C.
Mu?ller.
2006.
Automatic detection of nonreferential It inspoken multi-party dialog.
In Proceedings of the 11th Con-ference of the EACL.M.
Purver, P. Ehlen, and J. Niekrasz.
2006.
Detecting actionitems in multi-party meetings: Annotation and initial exper-iments.
In MLMI 2006, Revised Selected Papers.E.
Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004.The ICSI Meeting Recorder Dialog Act (MRDA) Corpus.
InProceedings of the 5th SIGdial Workshop.A.
Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Juraf-sky, P. Taylor, C. V. Ess-Dykema, R. Martin, and M. Meteer.2000.
Dialogue act modeling for automatic tagging andrecognition of conversational speech.
Computational Lin-guistics, 26(3):339?373.108
