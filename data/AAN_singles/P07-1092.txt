Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 728?735,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsMachine Translation by Triangulation:Making Effective Use of Multi-Parallel CorporaTrevor Cohn and Mirella LapataHuman Computer Research Centre, School of InformaticsUniversity of Edinburgh{tcohn,mlap}@inf.ed.ac.ukAbstractCurrent phrase-based SMT systems performpoorly when using small training sets.
Thisis a consequence of unreliable translation es-timates and low coverage over source andtarget phrases.
This paper presents a methodwhich alleviates this problem by exploit-ing multiple translations of the same sourcephrase.
Central to our approach is triangula-tion, the process of translating from a sourceto a target language via an intermediate thirdlanguage.
This allows the use of a muchwider range of parallel corpora for train-ing, and can be combined with a standardphrase-table using conventional smoothingmethods.
Experimental results demonstrateBLEU improvements for triangulated mod-els over a standard phrase-based system.1 IntroductionStatistical machine translation (Brown et al, 1993)has seen many improvements in recent years, mostnotably the transition from word- to phrase-basedmodels (Koehn et al, 2003).
Modern SMT sys-tems are capable of producing high quality transla-tions when provided with large quantities of trainingdata.
With only a small training sample, the trans-lation output is often inferior to the output from us-ing larger corpora because the translation algorithmmust rely on more sparse estimates of phrase fre-quencies and must also ?back-off?
to smaller sizedphrases.
This often leads to poor choices of targetphrases and reduces the coherence of the output.
Un-fortunately, parallel corpora are not readily availablein large quantities, except for a small subset of theworld?s languages (see Resnik and Smith (2003) fordiscussion), therefore limiting the potential use ofcurrent SMT systems.In this paper we provide a means for obtainingmore reliable translation frequency estimates fromsmall datasets.
We make use of multi-parallel cor-pora (sentence aligned parallel texts over three ormore languages).
Such corpora are often createdby international organisations, the United Nations(UN) being a prime example.
They present a chal-lenge for current SMT systems due to their rela-tively moderate size and domain variability (exam-ples of UN texts include policy documents, proceed-ings of meetings, letters, etc.).
Our method translateseach target phrase, t, first to an intermediate lan-guage, i, and then into the source language, s. Wecall this two-stage translation process triangulation(Kay, 1997).
We present a probabilistic formulationthrough which we can estimate the desired phrasetranslation distribution (phrase-table) by marginali-sation, p(s|t) =?i p(s, i|t).As with conventional smoothing methods (Koehnet al, 2003; Foster et al, 2006), triangulation in-creases the robustness of phrase translation esti-mates.
In contrast to smoothing, our method allevi-ates data sparseness by exploring additional multi-parallel data rather than adjusting the probabilities ofexisting data.
Importantly, triangulation provides uswith separately estimated phrase-tables which couldbe further smoothed to provide more reliable dis-tributions.
Moreover, the triangulated phrase-tablescan be easily combined with the standard source-target phrase-table, thereby improving the coverageover unseen source phrases.As an example, consider Figure 1 which showsthe coverage of unigrams and larger n-gram phraseswhen using a standard source target phrase-table, atriangulated phrase-table with one (it) and nine lan-guages (all), and a combination of standard and tri-angulated phrase-tables (all+standard).
The phraseswere harvested from a small French-English bitext728and evaluated against a test set.
Although very fewsmall phrases are unknown, the majority of largerphrases are unseen.
The Italian and all results showthat triangulation alone can provide similar or im-proved coverage compared to the standard source-target model; further improvement is achieved bycombining the triangulated and standard models(all+standard).
These models and datasets will bedescribed in detail in Section 3.We also demonstrate that triangulation can beused on its own, that is without a source-target dis-tribution, and still yield acceptable translation out-put.
This is particularly heartening, as it provides ameans of translating between the many ?low den-sity?
language pairs for which we don?t yet have asource-target bitext.
This allows SMT to be appliedto a much larger set of language pairs than was pre-viously possible.In the following section we provide an overviewof related work.
Section 3 introduces a generativeformulation of triangulation.
We present our evalua-tion framework in Section 4 and results in Section 5.2 Related WorkThe idea of using multiple source languages forimproving the translation quality of the target lan-guage dates back at least to Kay (1997), who ob-served that ambiguities in translating from one lan-guage onto another may be resolved if a transla-tion into some third language is available.
Systemswhich have used this notion of triangulation typi-cally create several candidate sentential target trans-lations for source sentences via different languages.A single translation is then selected by finding thecandidate that yields the best overall score (Och andNey, 2001; Utiyama and Isahara, 2007) or by co-training (Callison-Burch and Osborne, 2003).
Thisties in with recent work on ensemble combinationsof SMT systems, which have used alignment tech-niques (Matusov et al, 2006) or simple heuristics(Eisele, 2005) to guide target sentence selection andgeneration.
Beyond SMT, the use of an intermediatelanguage as a translation aid has also found appli-cation in cross-lingual information retrieval (Gollinsand Sanderson, 2001).Callison-Burch et al (2006) propose the use ofparaphrases as a means of dealing with unseensource phrases.
Their method acquires paraphrasesby identifying candidate phrases in the source lan-1 2 3 4 5 6phrase lengthproportionof test events in phrasetable0.0050.010.020.050.10.20.51 standardItalianallall + standardFigure 1: Coverage of fr ?
en test phrases using a 10,000 sen-tence bitext.
The standard model is shown alongside triangu-lated models using one (Italian) or nine other languages (all).guage, translating them into multiple target lan-guages, and then back to the source.
Unknownsource phrases are substituted by the back-translatedparaphrases and translation proceeds on the para-phrases.In line with previous work, we exploit multi-ple source corpora to alleviate data sparseness andincrease translation coverage.
However, we differin several important respects.
Our method oper-ates over phrases rather than sentences.
We proposea generative formulation which treats triangulationnot as a post-processing step but as part of the trans-lation model itself.
The induced phrase-table entriesare fed directly into the decoder, thus avoiding theadditional inefficiencies of merging the output ofseveral translation systems.Although related to Callison-Burch et al (2006)our method is conceptually simpler and more gen-eral.
Phrase-table entries are created via multiplesource languages without the intermediate step ofparaphrase extraction, thereby reducing the expo-sure to compounding errors.
Our phrase-tables maywell contain paraphrases but these are naturally in-duced as part of our model, without extra processingeffort.
Furthermore, we improve the translation esti-mates for both seen and unseen phrase-table entries,whereas Callison-Burch et al concentrate solely onunknown phrases.
In contrast to Utiyama and Isa-hara (2007), we employ a large number of inter-mediate languages and demonstrate how triangu-lated phrase-tables can be combined with standardphrase-tables to improve translation output.729en varm kartoffeleen hete aardappel uma batata quenteune patate une patate chaudd?licate une question d?licatea hot potatosourceintermediatetargetFigure 2: Triangulation between English (source) and French (target), showing three phrases in Dutch, Danish and Portuguese,respectively.
Arrows denote phrases aligned in a language pair and also the generative translation process.3 TriangulationWe start with a motivating example before formalis-ing the mechanics of triangulation.
Consider trans-lating the English phrase a hot potato1 into French,as shown in Figure 2.
In our corpus this Englishphrase occurs only three times.
Due to errors inthe word alignment the phrase was not included inthe English-French phrase-table.
Triangulation firsttranslates hot potato into a set of intermediate lan-guages (Dutch, Danish and Portuguese are shown inthe figure), and then these phrases are further trans-lated into the target language (French).
In the ex-ample, four different target phrases are obtained, allof which are useful phrase-table entries.
We arguethat the redundancy introduced by a large suite ofother languages can correct for errors in the wordalignments and also provide greater generalisation,since the translation distribution is estimated from aricher set of data-points.
For example, instances ofthe Danish en varm kartoffel may be used to trans-late several English phrases, not only a hot potato.In general we expect that a wider range of pos-sible translations are found for any source phrase,simply due to the extra layer of indirection.
So, if asource phrase tends to align with two different tar-get phrases, then we would also expect it to alignwith two phrases in the ?intermediate?
language.These intermediate phrases should then each alignwith two target phrases, yielding up to four targetphrases.
Consequently, triangulation will often pro-duce more varied translation distributions than thestandard source-target approach.3.1 FormalisationWe now formalise triangulation as a generativeprobabilistic process operating independently onphrase pairs.
We start with the conditional distri-bution over three languages, p(s, i|t), where the ar-guments denote phrases in the source, intermediate1An idiom meaning a situation for which no one wants toclaim responsibility.and target language, respectively.
From this distri-bution, we can find the desired conditional over thesource-target pair by marginalising out the interme-diate phrases:2p(s|t) =?ip(s|i, t)p(i|t)?
?ip(s|i)p(i|t) (1)where (1) imposes a simplifying conditional inde-pendence assumption: the intermediate phrase fullyrepresents the information (semantics, syntax, etc.
)in the source phrase, rendering the target phrase re-dundant in p(s|i, t).Equation (1) requires that all phrases in theintermediate-target bitext must also be found in thesource-intermediate bitext, such that p(s|i) is de-fined.
Clearly this will often not be the case.
In thesesituations we could back-off to another distribution(by discarding part, or all, of the conditioning con-text), however we take a more pragmatic approachand ignore the missing phrases.
This problem ofmissing contexts is uncommon in multi-parallel cor-pora, but is more common when the two bitexts aredrawn from different sources.While triangulation is intuitively appealing, itmay suffer from a few problems.
Firstly, as with anySMT approach, the translation estimates are basedon noisy automatic word alignments.
This leads tomany errors and omissions in the phrase-table.
Witha standard source-target phrase-table these errors areonly encountered once, however with triangulationthey are encountered twice, and therefore the errorswill compound.
This leads to more noisy estimatesthan in the source-target phrase-table.Secondly, the increased exposure to noise meansthat triangulation will omit a greater proportion oflarge or rare phrases than the standard method.
An2Equation (1) is used with the source and target argumentsreversed to give p(t|s).730alignment error in either of the source-intermediateor intermediate-target bitexts can prevent the extrac-tion of a source-target phrase pair.
This effect can beseen in Figure 1, where the coverage of the Italiantriangulated phrase-table is worse than the standardsource-target model, despite the two models usingthe same sized bitexts.
As we explain in the nextsection, these problems can be ameliorated by us-ing the triangulated phrase-table in conjunction witha standard phrase-table.Finally, another potential problem stems from theindependence assumption in (1), which may be anoversimplification and lead to a loss of information.The experiments in Section 5 show that this effect isonly mild.3.2 Merging the phrase-tablesOnce induced, the triangulated phrase-table can beusefully combined with the standard source-targetphrase-table.
The simplest approach is to use linearinterpolation to combine the two (or more) distribu-tions, as follows:p(s, t) =?j?jpj(s, t) (2)where each joint distribution, pj , has a non-negativeweight, ?j , and the weights sum to one.
The jointdistribution for triangulated phrase-tables is definedin an analogous way to Equation (1).
We expectthat the standard phrase-table should be allocateda higher weight than triangulated phrase-tables, asit will be less noisy.
The joint distribution is nowconditionalised to yield p(s|t) and p(t|s), which areboth used as features in the decoder.
Note that the re-sulting conditional distribution will be drawn solelyfrom one input distribution when the conditioningcontext is unseen in the remaining distributions.
Thismay lead to an over-reliance on unreliable distribu-tions, which can be ameliorated by smoothing (e.g.,Foster et al (2006)).As an alternative to linear interpolation, we alsoemploy a weighted product for phrase-table combi-nation:p(s|t) ?
?jpj(s|t)?j (3)This has the same form used for log-linear trainingof SMT decoders (Och, 2003), which allows us totreat each distribution as a feature, and learn the mix-ing weights automatically.
Note that we must indi-vidually smooth the component distributions in (3)to stop zeros from propagating.
For this we useSimple Good-Turing smoothing (Gale and Samp-son, 1995) for each distribution, which provides es-timates for zero count events.4 Experimental DesignCorpora We used the Europarl corpus (Koehn,2005) for experimentation.
This corpus consists ofabout 700,000 sentences of parliamentary proceed-ings from the European Union in eleven Europeanlanguages.
We present results on the full corpus for arange of language pairs.
In addition, we have createdsmaller parallel corpora by sub-sampling 10,000sentence bitexts for each language pair.
These cor-pora are likely to have minimal overlap ?
about1.5% of the sentences will be shared between eachpair.
However, the phrasal overlap is much greater(10 to 20%), which allows for triangulation usingthese common phrases.
This training setting waschosen to simulate translating to or from a ?lowdensity?
language, where only a few small indepen-dently sourced parallel corpora are available.
Thesebitexts were used for direct translation and triangula-tion.
All experimental results were evaluated on theACL/WMT 20053 set of 2,000 sentences, and arereported in BLEU percentage-points.Decoding Pharaoh (Koehn, 2003), a beam-search decoder, was used to maximise:T?
= argmaxT?jfj(T,S)?j (4)where T and S denote a target and source sentencerespectively.
The parameters, ?j , were trained usingminimum error rate training (Och, 2003) to max-imise the BLEU score (Papineni et al, 2002) ona 150 sentence development set.
We used a stan-dard set of features, comprising a 4-gram languagemodel, distance based distortion model, forwardand backward translation probabilities, forward andbackward lexical translation scores and the phrase-and word-counts.
The translation models and lex-ical scores were estimated on the training corpuswhich was automatically aligned using Giza++ (Ochet al, 1999) in both directions between source andtarget and symmetrised using the growing heuristic(Koehn et al, 2003).3For details see http://www.statmt.org/wpt05/mt-shared-task.731Lexical weights The lexical translation score isused for smoothing the phrase-table translation esti-mate.
This represents the translation probability of aphrase when it is decomposed into a series of inde-pendent word-for-word translation steps (Koehn etal., 2003), and has proven a very effective feature(Zens and Ney, 2004; Foster et al, 2006).
Pharaoh?slexical weights require access to word-alignments;calculating these alignments between the source andtarget words in a phrase would prove difficult fora triangulated model.
Therefore we use a modifiedlexical score, corresponding to the maximum IBMmodel 1 score for the phrase pair:lex(t|s) =1Zmaxa?kp(tk|sak) (5)where the maximisation4 ranges over all one-to-many alignments and Z normalises the score by thenumber of possible alignments.The lexical probability is obtained by interpo-lating a relative frequency estimate on the source-target bitext with estimates from triangulation, inthe same manner used for phrase translations in (1)and (2).
The addition of the lexical probability fea-ture yielded a substantial gain of up to two BLEUpoints over a basic feature set.5 Experimental ResultsThe evaluation of our method was motivated bythree questions: (1) How do different training re-quirements affect the performance of the triangu-lated models presented in this paper?
We expectperformance gains with triangulation on small andmoderate datasets.
(2) Is machine translation out-put influenced by the choice of the intermediate lan-guage/s?
Here, we would like to evaluate whetherthe number and choice of intermediate languagesmatters.
(3) What is the quality of the triangulatedphrase-table?
In particular, we are interested in theresulting distribution and whether it is sufficientlydistinct from the standard phrase-table.5.1 Training requirementsBefore reporting our results, we briefly discuss thespecific choice of model for our experiments.
Asmentioned in Section 3, our method combines the4The maximisation in (5) can be replaced with a sum withsimilar experimental results.standard interp +indic separateen ?
de 12.03 12.66 12.95 12.25fr ?
en 23.02 24.63 23.86 23.43Table 1: Different feature sets used with the 10K trainingcorpora, using a single language (es) for triangulation.
Thecolumns refer to standard, uniform interpolation, interpolationwith 0-1 indicator features, and separate phrase-tables, respec-tively.triangulated phrase-table with the standard source-target one.
This is desired in order to compensate forthe noise incurred by the triangulation process.
Weused two combination methods, namely linear inter-polation (see (2)) and a weighted geometric mean(see (3)).Table 1 reports the results for two translation taskswhen triangulating with a single language (es) us-ing three different feature sets, each with differenttranslation features.
The interpolation model usesuniform linear interpolation to merge the standardand triangulated phrase-tables.
Non-uniform mix-tures did not provide consistent gains, although,as expected, biasing towards the standard phrase-table was more effective than against.
The indicatormodel uses the same interpolated distribution alongwith a series of 0-1 indicator features to identify thesource of each event, i.e., if each (s, t) pair is presentin phrase-table j.
We also tried per-context featureswith similar results.
The separate model has a sepa-rate feature for each phrase-table.All three feature sets improve over the standardsource-target system, while the interpolated featuresprovided the best overall performance.
The rela-tively poorer performance of the separate modelis perhaps surprising, as it is able to differentiallyweight the component distributions; this is probablydue to MERT not properly handling the larger fea-ture sets.
In all subsequent experiments we reportresults using linear interpolation.As a proof of concept, we first assessed the ef-fect of triangulation on corpora consisting of 10,000sentence bitexts.
We expect triangulation to de-liver performance gains on small corpora, since alarge number of phrase-table entries will be un-seen.
In Table 2 each entry shows the BLEU scorewhen using the standard phrase-table and the ab-solute improvement when using triangulation.
Herewe have used three languages for triangulation(it ?
{de, en, es, fr}\{s, t}).
The source-target lan-guages were chosen so as to mirror the evaluationsetup of NAACL/WMT.
The translation tasks range732s ?
t ?
de en es frde - 17.58 16.84 18.06- +1.20 +1.99 +1.94en 12.45 - 23.83 24.05+1.22 - +1.04 +1.48es 12.31 23.83 - 32.69+2.24 +1.35 - +0.85fr 11.76 23.02 31.22 -+2.41 +2.24 +1.30 -Table 2: BLEU improvements over the standard phrase-table(top) when interpolating with three triangulated phrase-tables(bottom) on the small training sample.from easy (es ?
fr) to very hard (de ?
en).
In allcases triangulation resulted in an improvement intranslation quality, with the highest gains observedfor the most difficult tasks (to and from German).For these tasks the standard systems have poor cov-erage (due in part to the sizeable vocabulary of Ger-man phrases) and therefore the gain can be largelyexplained by the additional coverage afforded by thetriangulated phrase-tables.To test whether triangulation can also improveperformance of larger corpora we ran six separatetranslation tasks on the full Europarl corpus.
Theresults are presented in Table 3, for a single trian-gulation language used alone (triang) or uniformlyinterpolated with the standard phrase-table (interp).These results show that triangulation can producehigh quality translations on its own, which is note-worthy, as it allows for SMT between a much largerset of language pairs.
Using triangulation in con-junction with the standard phrase-table improvedover the standard system in most instances, andonly degraded performance once.
The improvementis largest for the German tasks which can be ex-plained by triangulation providing better robustnessto noisy alignments (which are often quite poor forGerman) and better estimates of low-count events.The difficulty of aligning German with the other lan-guages is apparent from the Giza++ perplexity: thefinal Model 4 perplexities for German are quite high,as much as double the perplexity for more easilyaligned language pairs (e.g., Spanish-French).Figure 3 shows the effect of triangulation on dif-ferent sized corpora for the language pair fr ?
en.It presents learning curves for the standard systemand a triangulated system using one language (es).As can be seen, gains from triangulation only di-minish slightly for larger training corpora, and thattask standard interm triang interpde ?
en 23.85 es 23.48 24.36en ?
de 17.24 es 16.28 17.42es ?
en 30.48 fr 29.06 30.52en ?
es 29.09 fr 28.19 29.09fr ?
en 29.66 es 29.59 30.36en ?
fr 30.07 es 28.94 29.62Table 3: Results on the full training set showing triangulationwith a single language, both alone (triang) and alongside a stan-dard model (interp).llllsize of training bitext(s)BLEU score10K 40K 160K 700K2224262830l standardtrianginterpFigure 3: Learning curve for fr ?
en translation for the standardsource-target model and a triangulated model using Spanish asan intermediate language.the purely triangulated models have very competi-tive performance.
The gain from interpolation witha triangulated model is roughly equivalent to havingtwice as much training data.Finally, notice that triangulation may benefitwhen the sentences in each bitext are drawn from thesame source, in that there are no unseen ?intermedi-ate?
phrases, and therefore (1) can be easily evalu-ated.
We investigate this by examining the robust-ness of our method in the face of disjoint bitexts.The concepts contained in each bitext will be morevaried, potentially leading to better coverage of thetarget language.
In lieu of a study on different do-main bitexts which we plan for the future, we bi-sected the Europarl corpus for fr ?
en, triangulat-ing with Spanish.
The triangulated models were pre-sented with fr-es and es-en bitexts drawn from eitherthe same half of the corpus or from different halves,resulting in scores of 28.37 and 28.13, respectively.5These results indicate that triangulation is effective5The baseline source-target system on one half has a scoreof 28.85.733triang interpBLEU score19202122232425fi (?14.26)dadadedeelelesesfiititnlnlptptsvsvFigure 4: Comparison of different triangulation languages forfr ?
en translation, relative to the standard model (10K trainingsample).
The bar for fi has been truncated to fit on the graph.for disjoint bitexts, although ideally we would testthis with independently sourced parallel texts.5.2 The choice of intermediate languagesThe previous experiments used an ad-hoc choiceof ?intermediate?
language/s for triangulation, andwe now examine which languages are most effec-tive.
Figure 4 shows the efficacy of the remainingnine languages when translating fr ?
en.
Minimumerror-rate training was not used for this experiment,or the next shown in Figure 5, in order to highlightthe effect of the changing translation estimates.
Ro-mance languages (es, it, pt) give the best results,both on their own and when used together with thestandard phrase-table (using uniform interpolation);Germanic languages (de, nl, da, sv) are a distant sec-ond, with the less related Greek and Finnish the leastuseful.
Interpolation yields an improvement for all?intermediate?
languages, even Finnish, which has avery low score when used alone.The same experiment was repeated for en ?
detranslation with similar trends, except that theGermanic languages out-scored the Romance lan-guages.
These findings suggest that ?intermediate?languages which exhibit a high degree of similaritywith the source or target language are desirable.
Weconjecture that this is a consequence of better auto-matic word alignments and a generally easier trans-lation task, as well as a better preservation of infor-mation between aligned phrases.Using a single language for triangulation clearlyimproves performance, but can we realise furtherimprovements by using additional languages?
Fig-1 2 3 4 5 6 7 8 9# intermediate languagesBLEUscore2223242526 trianginterpFigure 5: Increasing the number of intermediate languages usedfor triangulation increases performance for fr ?
en (10K train-ing sample).
The dashed line shows the BLEU score for thestandard phrase-table.ure 5 shows the performance profile for fr ?
enwhen adding languages in a fixed order.
The lan-guages were ordered by family, with Romance be-fore Germanic before Greek and Finnish.
Each ad-dition results in an increase in performance, even forthe final languages, from which we expect little in-formation.
The purely triangulated (triang) and in-terpolated scores (interp) are converging, suggestingthat the source-target bitext is redundant given suf-ficient triangulated data.
We obtained similar resultsfor en ?
de.5.3 Evaluating the quality of the phrase-tableOur experimental results so far have shown thattriangulation is not a mere approximation of thesource-target phrase-table, but that it extracts addi-tional useful translation information.
We now as-sess the phrase-table quality more directly.
Com-parative statistics of a standard and a triangulatedphrase-table are given in Table 4.
The coverage oversource and target phrases is much higher in the stan-dard table than the triangulated tables, which reflectsthe reduced ability of triangulation to extract largephrases ?
despite the large increase in the num-ber of events.
The table also shows the overlappingprobability mass which measures the sum of prob-ability in one table for which the events are presentin the other.
This shows that the majority of massis shared by both tables (as joint distributions), al-though there are significant differences.
The Jensen-Shannon divergence is perhaps more appropriate forthe comparison, giving a relatively high divergence734standard triangsource phrases (M) 8 2.5target phrases (M) 7 2.5events (M) 12 70overlapping mass 0.646 0.750Table 4: Comparative statistics of the standard triangulated tableon fr ?
en using the full training set and Spanish as an inter-mediate language.of 0.3937.
This augurs well for the combination ofstandard and triangulated phrase-tables, where di-versity is valued.
The decoding results (shown inTable 3 for fr ?
en) indicate that the two meth-ods have similar efficacy, and that their interpolatedcombination provides the best overall performance.6 ConclusionIn this paper we have presented a novel method forobtaining more reliable translation estimates fromsmall datasets.
The key premise of our work is thatmulti-parallel data can be usefully exploited for im-proving the coverage and quality of phrase-basedSMT.
Our triangulation method translates from asource to a target via one or many intermediate lan-guages.
We present a generative formulation of thisprocess and show how it can be used together withthe entries of a standard source-target phrase-table.We observe large performance gains when trans-lating with triangulated models trained on smalldatasets.
Furthermore, when combined with a stan-dard phrase-table, our models also yield perfor-mance improvements on larger datasets.
Our exper-iments revealed that triangulation benefits from alarge set of intermediate languages and that perfor-mance is increased when languages of the same fam-ily to the source or target are used as intermediates.We have just scratched the surface of the possi-bilities for the framework discussed here.
Importantfuture directions lie in combining triangulation withricher means of conventional smoothing and usingtriangulation to translate between low-density lan-guage pairs.Acknowledgements The authors acknowledgethe support of EPSRC (grants GR/T04540/01 andGR/T04557/01).
Special thanks to Markus Becker, ChrisCallison-Burch, David Talbot and Miles Osborne for theirhelpful comments.ReferencesP.
F. Brown, V. J. D. Pietra, S. A. D. Pietra, R. L. Mercer.
1993.The mathematics of statistical machine translation: Parame-ter estimation.
Computational Linguistics, 19(2):263?311.C.
Callison-Burch, M. Osborne.
2003.
Bootstrapping parallelcorpora.
In Proceedings of the NAACL Workshop on Build-ing and Using Parallel Texts: Data Driven Machine Trans-lation and Beyond, Edmonton, Canada.C.
Callison-Burch, P. Koehn, M. Osborne.
2006.
Improved sta-tistical machine translation using paraphrases.
In Proceed-ings of the HLT/NAACL, 17?24, New York, NY.A.
Eisele.
2005.
First steps towards multi-engine machinetranslation.
In Proceedings of the ACL Workshop on Build-ing and Using Parallel Texts, 155?158, Ann Arbor, MI.G.
Foster, R. Kuhn, H. Johnson.
2006.
Phrase-table smooth-ing for statistical machine translation.
In Proceedings of theEMNLP, 53?61, Sydney, Australia.W.
A. Gale, G. Sampson.
1995.
Good-turing frequency esti-mation without tears.
Journal of Quantitative Linguistics,2(3):217?237.T.
Gollins, M. Sanderson.
2001.
Improving cross languageretrieval with triangulated translation.
In Proceedings of theSIGIR, 90?95, New Orleans, LA.M.
Kay.
1997.
The proper place of men and machines in lan-guage translation.
Machine Translation, 12(1?2):3?23.P.
Koehn, F. J. Och, D. Marcu.
2003.
Statistical phrase-based translation.
In Proceedings of the HLT/NAACL, 48?54, Edomonton, Canada.P.
Koehn.
2003.
Noun Phrase Translation.
Ph.D. thesis, Uni-versity of Southern California, Los Angeles, California.P.
Koehn.
2005.
Europarl: A parallel corpus for evaluation ofmachine translation.
In Proceedings of MT Summit, Phuket,Thailand.E.
Matusov, N. Ueffing, H. Ney.
2006.
Computing consesustranslation from multiple machine translation systems us-ing enhanced hypotheses alignment.
In Proceedings of theEACL, 33?40, Trento, Italy.F.
J. Och, H. Ney.
2001.
Statistical multi-source translation.
InProceedings of the MT Summit, 253?258, Santiago de Com-postela, Spain.F.
J. Och, C. Tillmann, H. Ney.
1999.
Improved alignmentmodels for statistical machine translation.
In Proceedings ofthe EMNLP and VLC, 20?28, University of Maryland, Col-lege Park, MD.F.
J. Och.
2003.
Minimum error rate training in statistical ma-chine translation.
In Proceedings of the ACL, 160?167, Sap-poro, Japan.K.
Papineni, S. Roukos, T. Ward, W.-J.
Zhu.
2002.
BLEU: Amethod for automatic evaluation of machine translation.
InProceedings of the ACL, 311?318, Philadelphia, PA.P.
Resnik, N. A. Smith.
2003.
The Web as a parallel corpus.Computational Linguistics, 29(3):349?380.M.
Utiyama, H. Isahara.
2007.
A comparison of pivot methodsfor phrase-based statistical machine translation.
In Proceed-ings of the HLT/NAACL, 484?491, Rochester, NY.R.
Zens, H. Ney.
2004.
Improvements in phrase-based statisti-cal machine translation.
In D. M. Susan Dumais, S.
Roukos,eds., Proceedings of the HLT/NAACL, 257?264, Boston,MA.735
