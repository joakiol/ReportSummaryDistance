Anaphor resolution in unrestricted texts with partial parsingA.
Ferr~indez; M. PalomarDept.
Languages and Information SystemsAlicante University - Apt.
9903080 - Alicante - Spainantonio@dlsi.ua.es mpalomar@dlsi.ua.esL.
MorenoDept.
Information Systems andComputationValencia University of Technologylmoreno@dsic.upv.esAbstractIn this paper we deal with several kinds ofanaphora in unrestricted texts.
These kinds ofanaphora re pronominal references, urface-count anaphora nd one-anaphora.
In order tosolve these anaphors we work on the outputof a part-of-speech tagger, on which weautomatically apply a partial parsing from theformalism: Slot Unification Grammar, whichhas been implemented in Prolog.
We only usethe following kinds of information: lexical(the lemma of each word), morphologic(person, number, gender) and syntactic.Finally we show the experimental results, andthe restrictions and preferences that we haveused for anaphor resolution with partialparsing.IntroductionNowadays there are two different approaches toanaphor resolution: integrated and alternative.The former is based on the integration of differentkinds of knowledge (e.g.
syntactic or semanticinformation) whereas the latter is based onstatistical, neural networks or the principles ofreasoning with uncertainty: e.g.
Connoly (1994)and Mitkov (1997).Our system can be included into the firstapproach.
In these integrated approaches thesemantic and domain knowledge information isvery expensive in relation to computationalprocessing.
As a consequence, current anaphorresolution implementations mainly rely onconstraints and preference heuristics whichemploy information originated frommorphosyntactic or shallow semantic analysis,e.g.
in Baldwin (1997).
These approaches,however, perform remarkably well.
In Lappin andLeass (1994) it is described an algorithm forpronominal naphor resolution with a high rate ofcorrect analyses: 85%.
This one operatesprimarily on syntactic information only.
InKennedy and Boguraev (1996) it is proposed analgorithm for anaphor resolution which is amodified and extended version of that developedby Lappin and Leass (1994).
In contrast o thatwork, this algorithm does not require in-depth,full, syntactic parsing of text.
The modificationsenable the resolution process to work from theoutput of a POS tagger, enriched only withannotations of grammatical function of lexicalitems in the input text stream.
The advantage ofthis algorithm is that anaphor esolution can berealized within NLP frameworks which do not -orcannot- employ robust and reliable parsingcomponents.
Quantitative valuation shows theanaphor esolution algorithm described here torun at a rate of 75% accuracy.
Our frameworkwill allow us a similar approach to that ofKennedy and Boguraev (1996), but we willautomatically get syntactic information frompartial parsing.
Moreover, our proposal will alsobe applied to other kinds of anaphors uch assurface-count anaphora or one-anaphora.There are some other approaches that work onthe output of a POS tagger, e.g.
that of Mitkovand Stys (1997), in which it is proposed anotherknowledge-poor approach to resolving pronounsin technical manuals in both English and Polish.This approach is a modification of the reported inMitkov (1997).
Here, the knowledge is limited toa small noun phrase grammar, a list of terms andThis paper has been supported by the CICYT number TIC97-0671-C02-01 / 02385a set of antecedent indicators (definiteness,giveness, term preference, lexical reiteration, ...).We will work in a similar way to this approach,since we use some of its antecedent indicators,but we automatically apply a partial parsing thatallows us to deal with other kinds of anaphors aswell as pronouns.In this work we are going to apply a partialparsing on the output of a POS tagger in order tosolve anaphora problem.
We will work over thecorpus used within CRATER z.
This corpuscontains the International TelecommunicationsUnion CCITT handbook, also known as The BlueBook, in English, French and Spanish versions.This corpus is the most important collection oftelecommunication texts and contains 5M words,automatically tagged by the Spanish version ofthe Xerox tagger.
We will use the system SlotUnification Grammar (SUG) in order to get apartial parsing on the output of this tagger.SUG is a logical formalism based onunification, which is an extension of DefiniteClause Grammars (DCG).
It is called SlotUnification Grammar due to the slot structuresgenerated by the parser.
SUG has been developedwith the aim of extending DCG in order tofacilitate the resolution of several NaturalLanguage Processing (NLP) problems in amodular way.
This system has been firstlyproposed in Ferr~ndez (1997a), and it has beenpreviously applied to anaphor resolution inFerr~indez (1997b).We have used SUG instead of other wellknown formalisms uch as Head Driven PhraseStructure Grammar (HPSG), Lexical FunctionalGrammar (LFG) or Slot Grammars (SG), becauseSUG allows a modular and computationaltreatment of  NLP problems, and it facilitates itsintegration with a POS tagger.In the following section we will brieflydescribe SUG formalism in order to facilitate theundertanding of  this paper.
In section 2 we willpropose a SUG grammar to accomplish the partialparsing of the unrestricted text and the interfaceto work with the output of the POS tagger.
Insection 3 we will explain the algorithm used toanaphor resolution and its constraints and2 http://138.87.135.33/-mdavies/roanoke.htmpreferences.
And, finally, in section 4 we willoffer some figures of the evaluation of thesystem.1 Slot Unification GrammarIn this section we will briefly describe SUGformalism.
We will only show some of thecapabilities of SUG in order to undertand thispaper.
For further details on SUG it is necessaryto consult Femindez (1997a).SUG can be defined as this quadruple:(NT, T,P,H), where NT and T are a finite set ofnonterminal and terminal symbols respectively;moreover NT~ T = fD.
P is a finite set of pairs++> 13 where ot~NT, 13~(TuNT)*u {procedurescalls}, and these pairs are called production rules.Finally H is a set of production rules which onlyhas the first member of the production rule, i.e.
a,and ot's name is either coordinated, juxtaposition,fusion, basicWord or isWord.SUG's production rules adds to those of DCGthat each subconstituent of 13 could be omitted inthe sentence if it is noted between the optionaloperator: << constituent >>.
It is a well-knownfact that we can get optional constituents in DCGfrom making use of a nonterminal symbol (e.g.optA, with optA--->A and optA-~\[\]).
However thisskill obliges us to add new nonterminal symbols,whereas SUG allows us to get it without addingany new one.
We can get an example from Figure1, in which we can see the reduction ofgrammatical rules in SUG.DCG Grammar:np -> subst.
\[ SUG Grammar:np -> det, subst.
IlnP ++> <<det>>, <<adj>>, np->det, adj, subst.np->det, subst, adj.
L subst, <<adj>>, <<pp>>.np ->det, subst, pp,~CG Grammar with optional constituents:np -> optDet, optAdj, subst, optAdj, optPP.optDet -> det.
optAdj-> adj.
.....optDet -> \[\].
optAdj -> \[\].Figure 1.
Comparison between DCG and SUGwith reference to optional constituents.Furthermore, this optional operator has thepossibility of reminding whether the optionalconstituent has been parsed in the sentence or not.This information will be very useful in theresolution of NLP problems such as ellipsis or386extraposition.
This fact is carried out by adding alabel to the optional constituent, e.g.
<< SSNP"np >>.
This label will be an uninstantiated Prologvariable if constituent np is missing, so Prologpredicate var (SSNP) would success.We have developed a translator which turnsSUG rules into Prolog clauses.
This translator hasbeen run under SICStus Prolog 2.1 and ArityProlog 5.1, and it will translate into Prolog eachSUG production rule.
This translator will providewhat we call slot structure (henceforth SS).This SS stores the syntactic, morphologic andsemantic information of every constituent of thegrammar.
Each SS consists of a structure withfunctor the name of the constituent (np, vp .
.
.
.
).Its first argument corresponds to another structurewith functor conc which includes all thearguments of the constituent (Number, Gender,SemanticType).
The second one corresponds tothe 3.p of the final logical formula of theconstituent.
And the remaining argumentscorrespond to the SS of its subconstituents.
In thisSS the parser leaves as uninstantiated Prologvariables ("_") the slots corresponding to theoptional constituents that do not appear in thesentence, in this way, we know what has beenparsed and what has not.
From now on we willshow each SS with 3.p and conc only if it isnecessary, in order to get simplicity.Se,t .ce ) _ .
f  1' .roo, o oc .... tothe DictionaryI lo, st ctu,,II Processof~solutionofNLPproblems: ~anaphora, ellipsis, PP-atachraent, ...bTnal Slot Structure without these NLP problems \[Figure 2Now we would like to make clear the process inwhich we obtain the final logical formula.
First ofall we parse the sentence, and then we get its SS.After that, it would be the moment in which wecould try to solve NLP problems such asextraposition, ellipsis, PP-attachment andanaphora.
The solution will consist of a new SSwhich will be used to obtain the final logicalformula.
This process has been summed up inFigure 2.
We would like to emphasize that thisskill of resolution allows us to produce modularNLP systems in which grammatical rules, logicalformulas and the module of resolution of NLPproblems are quite independent from each other.Our SUG parser will access the dictionary onlyonce during the whole process of parsing in orderto avoid repeated access to the same word fromthe dictionary.
It stores the information of  eachword on a list before starting the parse and it willwork with this structure instead of the list ofwords of a DCG parser in Prolog; e.g.
DCG list:\[this, book, is, mine\], SUG list: \[word (this, \[adj(sing, dem), pron (sing, dem)\] ), word (book,\[noun (...)\]) .... \].
Each element from the SUG listis a structure with name word and with twoarguments.
The first one corresponds to the sameword of the sentence like a Prolog atom.
Thesecond one corresponds to a structure list whichrefers to the lexical entries of the word.
That is tosay that every time the parser has to access alexical entry of a word, it will look it up in thislist; it will not access the dictionary ever again.2 Partial parsing with SUGIn Abney (1997) it is considered necessary tocarry out a partial parsing on the unrestricted textinstead of a complete parsing, both due to errorsand the unavoidable incompleteness of lexiconand grammar.
It is also difficult to do a globalsearch efficiently with unrestricted text, due tothe length of sentences and the ambiguity ofgrammars.
Partial parsing is considered aresponse to these difficulties.
Partial parsingtechniques aim to recover syntactic informationefficiently and reliably from unrestricted text, bysacrificing completeness and depth of analysis.In this section we will show the application ofSUG in partial parsing.
We are going to take theoutput of a POS tagger as input, and after apply apartial parsing with SUG.
The previouslymentioned corpus The blue book is going to beworked on, which has been automatically taggedby the Spanish version of the Xerox tagger.
Eachword in a tagged sentence has the followingsyntax: (surfaceForm, lemma, TAG).387- -  i (cormcetions, connection, NCFP) .. .
.
:( f  lnterface in order to~ i\[ ";'o;~i,";,.ib;g~',ii~o'.
'\]'): g ;,~'(i/'g: " "\]\[ map each tag into the \ [~: \ [ar t (  fem, pl,det)\]), word (?onnection, :\[ aproprlate labehnto\[ :\[noun (common,fem,pl)\]), ... \] :the SUG grammar J "i~ Partial parsing ~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
with SUG J ~ ISUG grammar in Figure 5 that willionly parse certain constituents?
Slot Structure that will beused in anaphora resolution )Figure 3.
Interface between the tagger and SUG.We will proceed in the way that is described inFigure 3.
Firstly the tagged sentence is turned intothe SUG list format, where each Xerox tag ismapped into the apropriate label into the SUGgrammar, e.g.
the Xerox tag (connections,connection, NCFP) is mapped into the SUG tagword (connection, \[noun (common, fem, pl)\]).Finally, this SUG list of words will be taken asinput for the grammar described in Figure 4.
Thisgrammar will carry out the partial parsing of thetext, and the SUG parser will produce the SS thatwill be used in the algorithm, which is proposedfor anaphor resolution.
This simple interfacebetween the tagger and SUG is one of theadvantages of the modularity that presents SUG.It will allow us to work with different dictionariesor taggers with the same SUG grammar.
This isdue to the fact that in this system there is a greatindependence b tween the grammar, the lexicon,the process of dealing with NLP problems and theprocess of obtaining the final logical formula.sentence ++><< PP:pp >>, << NP:np >>, <<P:pronoun>>,<< V:verb >>, <<C:conj>>,<#\[1,remainingSentence(PP, NP, P, V, C) #>.remainingSentence(PP, NP, P, V, C) ++><t## ( {(var(PP), var(NP), vat(P), var(V), var(C))}, IVV\]),(_ _ )~/f>,sentence.% .
Grammatical rules for each constituent o parsecoordinated( pp, simplePP ).simplePP ++> preposition, np.coordinated( np, simpleNP 0 ).simpleNP (substantive Type) ++> <<determiner>>,<<adjective>>, noun, <<pp>>.simpleNP (adjective Type) ++> <<determiner>>, adjective,<<pp>>.Figure 4.
Partial parsing with SUG.The grammar in Figure 4 will only parsecoordinated prepositional phrases (pp),coordinated noun phrases (np), pronouns (p),conjunctions (conj) and verbs (verb) in whateverorder that they appear in the text and it will allowus to work in a similar way that the algorithmmentioned in Kennedy and Boguraev (1996).
Butin our approach we will automatically get thesyntactic information from this grammar.
The SSreturned by the parser will consist of a sequenceof these constituents: pp, np, p, conj, verb andfree words.
The attachments (e.g.
of the pp) willbe postponed to the module of resolution of NLPproblems, which could work jointly with thealgorithm for anaphor resolution (in a similar wayto the approach proposed in Azzam (1995)).
Thefree words will consist of constituents that are notcovered by the grammar (e.g.
adverbs) or wordsthat are not important for the anaphor esolution.The output of the whole system will consist of asequence of the logical formulas of eachconstituent.Here sentence will be the initial symbol of thegrammar and the partial parsing will be appliedwith the rules shown in Figure 4.
If we want acomplete parsing, we just have to substitute theserules for the following: sentence ++> np, vp, andobviously we will have to add the grammaticalrule for a verbal phrase (vp).3 The algorithmIn this section we are going to propose analgorithm which can deal with discourseanaphora in unrestricted texts with partialparsing.
It is based on the process of parsingdescribed in Figure 3.
So this process will takethe output of a POS tagging as input, and it willbe applied after the partial parsing of a sentence(using the grammar described in Figure 4) andbefore obtaining its logical formula.This algorithm is shown in Figure 5 and it willdeal with pronominal references, surface-countanaphora nd one-anaphora.
This algorithm willtake a slot structure (SS) that consists of asequence of the following constituents: np, pp, p,conj and verbs and it will return a new onewithout anaphors.
Every possible antecedent(noun phrases) will be stored in a list of388antecedents, that will be used to solve theanaphors.
Another structure will be stored in thislist for each antecedent: paral (Sent, Clause,PosVerb, NumConst, NumCoord).
This structurewill be used to deduce the parallelism with partialparsing between an anaphor and its antecedent.Its first argument, Sent, is the sentence in whichthe antecedent appears.
The second one is theclause in which it appears.
Consider that thebeginning of a new clause has been found whenwe parse a free conjuction (we do not refer to theconjunctions that join the coordinated noun andprepositional phrases).
The third one is theposition of the antecedent with reference to theverb of the clause: before (bv) or after (av).
Thefourth one is the number of constituent in thesentence and the fifth one is the number ofcoordinated constituent if it is included in acoordinated np or pp.
For example in: He saidthat Peter and John bought a book, we have thefollowing: paralm (S, 1, bv, 1, 1), paraljoh, (S, 2,bv, 4, 2) and paralbook (S,2,av,6,1).Parse a sentence.
We obtain its slot structure (SS1).For each anaphor in SSI:Select the antecedents of the previous X sentencesdepending on the kind of anaphor in LOApply constraints (depending on the kind of anaphor) to LOwith a result of L I :Case of:IL l l  = I Then:This one will be the antecedent of the anaphorIL I I  ?
1 Then:Apply preferences (depending on the kind of enaphor) toL 1, with a result of L2:The first one of L2 will be the selected antecedentUpdate SSf with each antecedent of each anaphor with aresult of SS2.Figure 5.
Algorithm for anaphor esolution.At the same time that we are searching forantecedents, we will also search for anaphors andwhenever we found an anaphor this algorithmwill be applied.
The kind of anaphors we aregoing to search are the following: pronouns (he,she .
.
.
.
), pronominal noun phrases formed by:determiner + pronoun (the second, the former,...), noun phrases with the structure: determiner +adjective + "one" (the red one, this anaphors inSpanish 3are noun phrases in which the noun has3 We are going to work with Spanish unrestrictedtexts, but whenever it is possible, all the examples willbe translated into English in order to facilitate itsunderstanding.been omitted: el rojo).
We will identify suchanaphors from its SS (its functor and its numberand type of arguments).
For example, the one-anaphor in Spanish will have the following SUGrule: np ++> <<determiner>>, adjective,<<pp>>, and the following SS: np (determiner(...), adjective (...), pp (...)).The number of previous sentences consideredin the resolution of an anaphor will be determinedby the kind of anaphor itself.
For pronominalreferences will be considered the antecedents inthe same sentence or in the previous entence if itis in the same paragraph, unlike to one-anaphorawhich have more lexical information, so we willconsider the antecedents in the same paragraph.We will be able to know the number of sentencebecause this information will be stored jointlywith the SS of every antecedent: for eachsentence will be assigned a different Prologvariable and all the antecedents in this sentencewill have this variable in itsparal structure.The algorithm will apply a set of constraints othe list of possible antecedents in order todiscount candidates.
If there is only onecandidate, this one will be the antecedent of theanaphor.
Otherwise, if there are still more thanone candidates left, a set of preferences will beapplied that will sort the list of remainingantecedents, and the selected antecedent will bethe first one.
It is important to remark that theseconstraints and preferences could be different foreach kind of anaphor.Next the constraints and preferences are goingto be briefly explained.
Morphosyntacticagreement (person, gender and number) will bechecked by unification of the structure concdescribed in section 1.
It is a strong constraint onreference, but it is not absolute: At the zoo, amonkey scampered between two elephants.
Onesnorted at it 4, or in: John and Bill~ went into theshop.
They~ bought a book.
To solve the secondexample we will store a new antecedent withplural number which includes all the coordinatednoun phrases (in this case John and Bill).
We willdetect he coordination of noun phrases from theSS returned by the SUG fact coordinated.
In one-4 In this paper we will not deal with problemscaused by quantification.389anaphora we have considered the numberagreement as a preference instead of a constraintin order to solve sentences like this: Wendy didn'tgive either boy a green shirti, but she gave Suetwo red onesj, where the anaphor and itsantecedent do not agree in number (so they do notco-refer to the same entity of the discourse).The c-command constraints will be applied onthe syntactic information stored in the SS of eachconstituent and its structure paral.
For examplethe following constraint: "A pronominal NP mustbe interpreted as non-coreferential with any NPthat c-commands it", e.g.
Zeldai bores herj.
It isaccomplished by the information stored in theirstructures: paral~ (Sent1, Clause1 .
.
.
.  )
and paralj(Sent1, Clause1 .
.
.
.  )
which means that they are inthe same sentence and clause.
However in Johnjwas late for work, because he~ slept in, here Johnand he can be coreferential because they are indifferent clauses separated by the conjunctionbecause: paraljoh, (Senti, Clausel .
.
.
.
), paralh~(Sent1, Clause2 .
.
.
.
).
But in John~ and hej boughta book, the pronoun will not corefer with Johnalthough there is a conjunction between thembecause they are in the same coordinated nounphrase, which is known from: parali ($1, C1, by,1_, 1) and paralj ($1, C1, by, 1, 2).
In sentenceslike (John~ 's portrait of himj)ue is interesting andThis is (the mani who hej saW)N P the coreference isnot permitted because the pronoun and theantecedent are in the same constituent NP (theyare in the same slot structure: np (det (the), noun(man), relSent (...)).
As well in John bought abook for Peteri and for a friend of him~, thepronoun can corefer with Peter although theybelong to the same coordinated constituentbecause the pronoun is an adjunct of the secondcoordinated constituent.
From the reflexivityconstraints in Maryj loves herse~, we canconclude the antecedent of herself is Marybecause they are in the same clause.In relation to preferences, they will be differentfor each kind of anaphor: the non-reflexivepronouns will prefer the antecedent in the samesentence and clause, and if there are still morethan one antecedent left, those in the sameposition with reference to the verb: syntacticparallelism.
Moreover we have added some otherpreferences, e.g.
a non-reflexive pronoun wouldnot be allowed to have an antecedent that appearin the same clause due to reflexivity constraints:Jacki saw Samj at the party.
Samj gave himi adrink.
If after applying these preferences, thereare more than one antecedent left, we will choosethe antecedent most recently mentioned.In order to solve surface-count anaphora wewill use the SS returned by the SUG factcoordinated.
This fact allows the coordination ofconstituents with the same or different form:Peter, your daughter and she and it will allow usto access whatever coordinated constituent in theorder we wish.
That is to say, its SS: np(simpleNP (Peter), conj(', '), np (simpleNP (det(your), noun (daughter)), conj (and), np(simpleNP (pron (she)), , _))), and theirstructures paral with their fifth argument will tellus the number of coordinated constituent:paralp,,er (S, C, V, P, 1), paraldaugh,e, (S  C, V, P, 2),....
In this way the anaphor: the second one willchoose an antecedent with a structure paral witha value of 2 in its fifth argument.To solve one-anaphora we will apply thefollowing preference: we will choose theantecedents with a similar structure.
For example,in Wendy didn't give either boy a green tie-dyedT-shirti, but she gave Sue a blue onej, theantecedent a green tie-dyed T-shirt would bechosen instead of  Wendy or Sue because theyhave similar SS (a determiner, a common nounand an adjective): np (noun(Wendy)), npi (~, det(a), adj (\[green, tie-dyed\]5), noun (T-shirt)) andnpj ~,  det (a), adj (\[blue\]), pron (one)).
This SSwill allow decomposition of the description (i.e.green can be broken off) and the solution of theanaphora will be: np (Y, det (a), adj (\[blue\]),noun (T-shirt)).
It is important o remark that thesolution will have a different variable 6(Y) than itsantecedent (X).
It means the anaphor and itsantecedent do not co-refer, so the anaphor efersto a new entity in the discourse.
However in Johnbought a red dark apple~ and a green pear.
He atethe red one~, the anaphor will co-refer with a reddark apple.
We will distinguish both casess This list of adjectives i provided by the SUG factjuxtaposition.6 This variable corresponds tothe ~.p of the finallogical formula of the constituent (see section 1).390because in the second one the anaphor and itsantecedent share the same modifiers 7 (red) andthey agree in number.4 Evaluation of the systemWe have run our system on part of the previouslymentioned corpus (9600 words), and we have gotthe following figures.
Our system has detected100% of the anaphors described in this paper, andthe partial parsing described in Figure 4, hasparsed 81% of words with a very simplegrammar 8.
The medium length of the sentenceswith anaphors is 48 words.
For pronominalreferences we have a 83% accuracy in detectingthe position of the antecedent.
For one-anaphoraand surface-count anaphora, we have not gotsignificant figures since there were not so manyanaphors as we wished (only 5 anaphors with a80% accuracy).
The reason why some of thereferences have failed is mainly due to the lack ofsemantic information and due to the problem ofattachments between different parsedconstituents 9.ConclusionsIn this paper we have proposed a computationalapproach to the resolution of pronominalreferences, surface-count anaphora and one-anaphora.
This approach works on the output of aPOS tagger, on which we will automaticallyapply a partial parsing from the formalism: SlotUnification Grammar.
We have only used lexical,morphologic and syntactic information.
We haveslightly '?
improved the accuracy (83%) inpronominal references to the work of Kennedyand Boguraev (1996) (75%), but we have alsoimproved that approach since we automatically7 It is obvious that we will probably need moresemantic information i order to solve these anaphors,but in this paper we are not going to consider thisinformation since the tagger does not provide it.s We could easily improve this percentage fromadding more constituents othe grammar (e.g.
adverbsor punctuation marks).9 To solve this problem is also necessary semanticinformation.,o It is difficult o compare both measures becausewe have worked on different texts (Spanish texts).apply a partial parsing and we deal with otherkinds of anaphors.As a future aim we will include semanticinformation in our algorithm in order to check theimprovement that we get with it.
This informationwill be stored in a dictionary which could beautomatically consulted (since this semanticinformation is not provided by the tagger).ReferencesAbney S. (1997) Part*of-Speech Tagging and PartialParsing.
In Steve Young and Gerrit Bloothooft (eds)Corpus-based methods in language and speechprocessing.
Kluwer Academic PublishersAzzam S. (1995) An Algorithm to Co-OrdinateAnaphor resolution and PPS DisambiguationProcess.
EACLBaldwin B.
(1997) CogNIAC: high precisioncoreference with limited knowledge and linguisticresources.
ACL/EACL workshop on Operationalfactors in practical, robust anaphor resolutionConnoly D., Burger J. and Day D. (1994) A Machinelearning approach to anaphoric reference.International Conference on New Methods inLanguage Processing, UMISTFerdmdez A., Palomar M. and Moreno L. (1997a) SlotUnification Grammar.
Joint Conference onDeclarative Programming.
APPIA-GULP-PRODEFerr6ndez A., Palomar M. and Moreno L. (1997b) SlotUnificacion Grammar and anaphor resolution.Recent Advances in Natural Language ProcessingKennedy C. and Boguraev B.
(1996) Anaphora forEveryone: Pronominal Anaphor esolution without aParser.
COLINGLappin S. and Leass H. (1994) An algorithm forpronominal anaphor resolution.
ComputationalLinguistics, 20(4)Mitkov R. (1997) Pronoun resolution: the practicalalternative".
In S. Botley, T. McEnery (eds)Discourse Anaphora nd Anaphor Resolution, Univ.College London PressMitkov R. (1995) An uncertainty reasoning approachto anaphor resolution.
Natural Language Pacific RimSymposium.
Seoul.
KoreaMitkov R. and Stys M. (1997) Robust referenceresolution with limited knowledge: high precisiongenre-specific approach for English and Polish.Recent Advances in Natural Language Processing391
