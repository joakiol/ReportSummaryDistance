Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 1?8,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsTeaching Applied Natural Language Processing: Triumphs and TribulationsMarti HearstSchool of Information Management & SystemsUniversity of California, BerkeleyBerkeley, CA 94720hearst@sims.berkeley.eduAbstractIn Fall 2004 I introduced a new coursecalled Applied Natural Language Process-ing, in which students acquire an under-standing of which text analysis techniquesare currently feasible for practical appli-cations.
The class was intended for in-terdisciplinary students with a somewhattechnical background.
This paper de-scribes the topics covered and the pro-gramming exercises, emphasizing whichaspects were successful and which prob-lematic, and makes recommendations forfuture versions of the course.1 IntroductionIn Fall 2005 I introduced a new graduate level coursecalled Applied Natural Language Processing.1 Thegoal of this course was to acquaint students with thestate-of-the-art of the field of NLP with an empha-sis on applications.
The intention was for studentsto leave the class with an understanding of what iscurrently feasible (and just on the horizon) to ex-pect from content analysis, and how to use and ex-tend existing NLP tools and technology.
The coursedid not emphasize the theoretical underpinnings ofNLP, although we did cover the most important al-gorithms.
A companion graduate course on Statis-tical NLP was taught by Dan Klein in the Com-puter Science department.
Dan?s course focused on1Lecture notes, assignments, and other resources can befound at http://www.sims.berkeley.edu/courses/is290-2/f04/ .foundations and core NLP algorithms.
Several com-puter science students took both courses, and thuslearned both the theoretical and the applied sides ofNLP.
Dan and I discussed the goals and content ofour respective courses in advance, but developed thecourses independently.2 Course Role within the SIMS ProgramThe primary target audience of the Applied NLPcourse were masters students, and to a lesser ex-tent, PhD students, in the School of InformationManagement and Systems.
(Nevertheless, PhD stu-dents in computer science and other fields also tookthe course.)
MIMS students (as the SIMS mas-ters students are known) pursue a professional de-gree studying information at the intersection of tech-nology and social sciences.
The students?
techni-cal backgrounds vary widely; each year a signifi-cant fraction have Computer Science undergraduatedegrees, and another significant fraction have so-cial science or humanities backgrounds.
All stu-dents have an interest in technology and are re-quired to take some challenging technical courses,but most non-CS background students are uncom-fortable with advanced mathematics and are not ascomfortable with coding as CS students are.A key aspect of the program is the capstone fi-nal project, completed in the last semester, that (ide-ally) combines knowledge and skills obtained fromthroughout the program.
Most students form a teamof 3-4 students and build a system, usually to meetthe requirements of an outside client or customer(although some students write policy papers andothers get involved in research with faculty mem-1bers).
Often the execution of these projects makesuse of user-centered design, including a needs as-sessment, and iterative design and testing of the arti-fact.
These projects often also have a backend de-sign component using database design principles,document engineering modeling, or information ar-chitecture and organization principles, with sensitiv-ity to legal considerations for privacy and intellec-tual property.
Students are required to present theirwork to an audience of students, faculty, and pro-fessionals, produce a written report, and produce awebsite that describes and demonstrates their work.In many cases these projects would benefit greatlyfrom content analysis.
Past projects have includeda system to query on and monitor news topics asthey occur across time and sources, a system to ana-lyze when and where company names are mentionedin text and graph interconnections among them, asystem to allow customization of news channels bytopic, and systems to search and analyze blogs.
Ourpast course offerings in this space focused on infor-mation retrieval with very little emphasis on contentanalysis, so students were using only IR-type tech-niques for these projects.The state of the art in NLP had advanced suffi-ciently that the available tools can be employed for anumber of projects like these.
Furthermore, it is im-portant for students attempting such projects to havean understanding of what is currently feasible andwhat is too ambitious.
In fact, I find that this is a keyaspect of teaching an applied class: learning whatis possible with existing tools, what is feasible butrequires more expertise than can be engineered in asemester with existing tools, and what is beyond thescope of current techniques.3 Choosing Tools and ReadingsThe main challenges for a hands-on course as I?denvisioned surrounded finding usable interoperabletools, and defining feasible assignments that makeuse of programming without letting it interfere withlearning.There is of course the inevitable decision of whichprogramming language(s) to work with.
Scriptingtools such as python are fast and easy to prototypewith, but require the students to learn a new pro-gramming language.
Java is attractive because manytools are written in it and the MIMS students werefamiliar with java ?
they are required to use it fortwo of their required courses but still tend to strug-gle with it.
I did not consider perl since python is amore principled language and is growing in accep-tance and in tool availability.In the end I decided to require the students to learnpython because I wanted to use NLTK, the NaturalLanguage Toolkit (Loper and Bird, 2002).
One goalof NLTK is to remove the emphasis on programmingto enable students to achieve results quickly; andthis aligned with my primary goal.
NLTK seemedpromising because it contained some well-writtentutorials on n-grams, POS tagging and chunking,and contained text categorization modules.
(I alsowanted support for entity extraction, which NLTKdoes not supply.)
NLTK is written in python, andso I decided to try it and have the students learn anew programming language.
As will be described indetail below, our use of NLTK was somewhat suc-cessful, but we experienced numerous problems aswell.I made a rather large mistake early on by notspending time introducing python, since I wantedthe assignments to correspond to the lectures and didnot want to spend lecture time on the programminglanguage itself.
I instructed students who had regis-tered for the course to learn python during the sum-mer, but (not surprisingly) many of did not and hadto struggle in the first few weeks.
In retrospect, I re-alize I should have allowed time for people to learnpython, perhaps via a lab session that met only dur-ing the first few weeks of class.Another sticking point was student exposure toregular expressions.
Regex?s were very importantand useful practical tools both for tokenization as-signments and for shallow parsing.
I assumed thatthe MIMS students had gotten practice with regu-lar expressions because they are required to take acomputer concepts foundations course which I de-signed several years ago.
Unfortunately, the lecturerwho took over the class from me had decided toomit regex?s and related topics.
I realized that I hadto do some remedial coverage of the topic, whichof course bored the CS students and which was notcomplete enough for the MIMS students.
Again thissuggests that perhaps some kind of lab is needed forgetting people caught up in topics, or that perhaps2the first few weeks of the class should be optionalfor more advanced students.I was also unable to find an appropriate textbook.Neither Schu?tze & Manning nor Jurafsky & Mar-tin focus on the right topics.
The closest in termsof topic is Natural Language Processing for OnlineApplications by Peter Jackson & Isabelle Moulinier,but much of this book focuses on Information Re-trieval (which we teach in two other courses) and didnot go into depth on the topics I most cared about.Instead of a text, students read a small selection ofresearch papers and the NLTK tutorials.4 TopicsThe course met twice weekly for 80 minute periods.The topic coverage is shown below; topics followedby (2) indicate two lecture periods were needed.Course IntroductionUsing Large Collections (intro to NLTK)Tokenization, Morphological AnalysisPart-of-Speech TaggingConditional ProbabilitiesShallow Parsing (2)Text Classification: IntroductionText Classification: Feature SelectionText Classification: AlgorithmsText Classification: Using WekaInformation Extraction (2)Email and Anti-Spam AnalysisText Data MiningLexicons and OntologiesFrameNet (guest lecture by Chuck Fillmore)Enron email dataset (in-class work) (2)Spelling Correction / ClusteringSummarization (guest lecture by Drago Radev)Question Answering (2)Machine Translation (slides by Kevin Knight)Topic Segmentation / Discourse ProcessingClass PresentationsNote the lack of coverage of full syntactic parsing,which is covered extensively in Dan Klein?s course.I touched on it briefly in the second shallow pars-ing lecture and felt this level of coverage was ac-ceptable because shallow parsing is often as usefulif not more so than full parsing for most applica-tions.
Note also the lack of coverage of word sensedisambiguation.
This topic is rich in algorithms, butwas omitted primarily due to time constraints, but inpart because of the lack of well-known applications.Based on the kinds of capstone projects the MIMSstudents have done in the past, I knew that the mostimportant techniques for their needs surroundedtext categorization and information extraction/entityrecognition.
There are terrific software resources fortext categorization and the field is fairly mature, soI had my PhD students Preslav Nakov and BarbaraRosario gave the lectures on this topic, in order toprovide them with teaching experience.The functionality provided by named entityrecognition is very important for a wide range ofreal-world applications.
Unfortunately, none of thefree tools that we tried were particularly successful.Those that are available are difficult to configure andget running in a short amount of time, and have vir-tually no documentation.
Furthermore, the state-of-the-art in algorithms is not present in the availabletools in the way that more mature technologies suchas POS tagging, parsing, and categorization are.5 Using NLTK5.1 BenefitsWe used the latest version of NLTK, which at thetime was version 1.4.2 NLTK supplies some pre-processed text collections, which are quite useful.
(Unfortunately, the different corpora have differenttypes of preprocessing applied to them, which of-ten lead to confusion and extra work for the class.
)The NLTK tokenizer, POS taggers and the shallowparser (chunker) have terrific functionality once theyare understood; some students were able to get quiteaccurate results using these and the supplied train-ing sets.
The ability to combine different n-gramtaggers within the structure of a backoff tagger alsosupported an excellent exercise.
However, a some-what minor problem with the taggers is that there isno compact way to store the model resulting fromtagging for later use.
A serialized object could becreated and stored, but the size of such object wasso large that it takes about as long to load it intomemory as it does to retrain the tagger.2http://nltk.sourceforge.org35.2 DrawbacksThere were four major problems with NLTK fromthe perspective of this course.
The first major prob-lem was the inconsistency in the different releasesof code, both in terms of incompatibilities betweenthe data structures in the different versions, andincompatibility of the documentation and tutorialswithin the different versions.
It was tricky to de-termine which documentation was associated withwhich code version.
And much of the contributedcode did not work with the current version.The second major problem was related to the first,but threw a major wrench into our plans: some of theadvertised functionality simply was not available inthe current version of the software.
Notably, NLTKadvertised a text categorization module; without thisI would not have adopted NLTK as the coding plat-form for the class.
Unfortunately, the most currentversion did not in fact support categorization, andwe discovered this just days before we were to be-gin covering this topic.The third major problem was the incompletenessof the documentation for much of the code.
Thisto some degree undermined the goal of reducing theamount of work for students, since they (and I) hadto struggle to figure out what was going on in thecode and data structures.One of these documentation problems centeredaround the data structure for conditional probabil-ities.
NLTK creates a FreqDist class which is ex-plained well in the documentation (it records a countfor each occurrence of some phenomenon, muchlike a hash table) and provides methods for retriev-ing the max, the count and frequency of each oc-currence, and so on.
It also provides a class calleda CondFreqDist, but does not document its meth-ods nor explain its implementation.
Users have toscrutinize the examples given and try to reverse en-gineer the data structure.
Eventually I realized thatit is simply a list of objects of type FreqDist, butthis was difficult to determine at first, and causedmuch wasting of time and confusion among the stu-dents.
There is also confusion surrounding the useof the method names count and frequency for Fre-qDist.
Count refers to number of occurrences andfrequency to a probability distribution across items,but this distinction is never stated explicitly althoughit can be inferred from a table of methods in the tu-torial.A less dramatic but still hampering problem waswith the design of the core data structures, whichmake use of attribute tags rather than classes.
Thisleads to rather awkward code structures.
For exam-ple, after a sentence is tokenized, the results of tok-enization are appended to the sentence data structureand are accessed via use of a subtoken keyword suchas ?TOKENS?.
To then run a POS tagger over thetokenized results, the ?TOKENS?
keyword has to bespecified as the value for a SUBTOKENS attribute,and another keyword must be supplied to act as thename of the tagged results.
In my opinion it wouldbe better to use the class system and define objectsof different types and operations on those objects.6 AssignmentsOne of the major goals of the class was for the stu-dents to obtain hands-on experience using and ex-tending existing NLP tools.
This was accomplishedthrough a series of homework assignments and a fi-nal project.
My pedagogical philosophy surround-ing assignments is to supply as much as the function-ality as necessary so that the coding that students doleads directly to learning.
Thus, I try to avoid mak-ing students deal with details of formatting files andso on.
I also try to give students a starting point tobuild up on.The first assignment made use of some exercisesfrom the NLTK tutorials.
Students completed to-kenizing exercises which required the use of theNLTK corpus tool accessors and the FreqDist andCondFreqDist classes.
They also did POS taggingexercises which exposed them to the idea of n-grams, backoff algorithms, and to the process oftraining and testing.
This assignment was challeng-ing (especially because of some misleading text inthe tagging tutorial, which has since been fixed) butthe students learned a great deal.
As mentionedabove, I should have begun with a preliminary as-signment which got students familiar with pythonbasics before attempting this assignment.For assignment 2, I provided a simple set of regu-lar expression grammar rules for the shallow parserclass, and asked the students to improve on these.After building the chunker, students were asked to4choose a verb and then analyze verb-argument struc-ture (they were provided with two relevant papers(Church and Hanks, 1990; Chklovski and Pantel,2004)).
As mentioned above, most of the MIMS stu-dents were not familiar with regular expressions, soI should have done a longer unit on this topic, at theexpense of boring the CS students.The students learned a great deal from working toimprove the grammar rules, but the verb-argumentanalysis portion was not particularly successful, inpart because the corpus analyzed was too small toyield many sentences for a given verb and becausewe did not have code to automatically find regu-larities about the semantics of the arguments of theverbs.
Other causes of difficulty were the students?lack of linguistic background, and the fact that thechunking part took longer than I expected, leavingstudents little time for the analysis portion of the as-signment.Assignments 3 and 4 are described in the follow-ing subsections.6.1 Text Categorization AssignmentAs mentioned above, text categorization is useful fora wide range SIMS applications, and we made it acenterpiece of the course.
Unfortunately, we had tomake a mid-course correction when I suddenly real-ized that text categorization was no longer availablein NLTK.After looking at a number of tools, we decidedto use the Weka toolkit for categorization (Wittenand Frank, 2000).
We did not want the students tofeel they had wasted their time learning python andNLTK, so we decided to make it easy for the stu-dents to reuse their python code by providing an in-terface between it and Weka.My PhD student Preslav Nakov provided greathelp by writing code to translate the output of ourpython code into the input format expected by Weka.
(Weka is written in java but has command line andGUI interfaces, and can read in input files and storemodels as output files.)
As time went on we addedincreasingly more functionality to this code, tying itin with the NLTK modules so that the students coulduse the NLTK corpora for training and testing.33Available at http://www.sims.berkeley.edu/courses/is290-2/f04/assignments/assignment3.htmlBoth Preslav and I had used Weka in the past butmainly with the command-line interface, and nottaking advantage of its rich functionality.
As withNLTK, the documentation for Weka was incompleteand out of date, and it was difficult to determine howto use the more advanced features.
We performedextended experimentation with the system and de-veloped a detailed tutorial on how to use the system;this tutorial should be of general use.4For the categorization task, we used the ?twentynewsgroups?
collection that was supplied withNLTK.
Unfortunately, it was not preprocessed intosentences, so I also had to write some sentence split-ting code (based on Palmer and Hearst (1997)) sostudents could make use of their tokenizer and tag-ger code.We selected one pair of newsgroups which con-tained very different content (rec.motorcyclesvs.
sci.space).
We called this the diverseset.
We then created two groups of news-groups with more homogeneous content (a)rec.autos, rec.motorcycles, rec.sport.baseball,rec.sport.hockey, and (b) sci.crypt, sci.electronics,sci.med.original, sci.space.
The intention was toshow the students that it is easier to automaticallydistinguish the heterogeneous groups than thehomogeneous ones.We set up the code to allow students to adjust thesize of their training and development sets, and toseparate out a reserved test set that would be usedfor comparing students?
solutions.We challenged the students to get the best scorespossible on the held out test set, telling them not touse this test set until they were completely finishedtraining and testing on the development set.
(We re-lied on the honor system for this.)
We made it knownthat we would announce which were the top-scoringassignments.
As a general rule I avoid competitionin my classes, but this was kept very low-key; onlythe top-scoring results would be named.
Further-more, innovative approaches that perhaps did not doas well as some others were also highlighted.
Stu-dents were required to try at least 2 different typesof features and 3 different classifiers.This assignment was quite successful, as the stu-4Available at http://www.sims.berkeley.edu/courses/is290-2/f04/lectures/lecture11.ppt5dents were creative about building their features,and it was possible to achieve very strong results(much stronger than I expected) on both sets ofnewsgroups.
The best scoring approaches got 99%accuracy on the 2-way diverse distinction and 97%accuracy on the 4-way homogeneous distinction.6.2 Enron Email AssignmentMany of the SIMS students are interested in socialnetworking and related topics.
I decided as part ofthe class that we would analyze a relatively new textcollection that had become available and that con-tained the potential for interesting text mining andanalysis.
I was also interested in having the classhelp produce a resource that would be of use to otherclasses and researchers.
Thus we decided to take onthe Enron email corpus,5 on which limited analysishad been done.My PhD student Andrew Fiore wrote code to pre-process this text, removing redundancies, normal-izing email addresses, labeling quoted text, and soon.
He and I designed a database schema for repre-senting much of the structure of the collection andloaded in the parsed text.
I created a Lucene6 in-dex for doing free text queries while Andrew built ahighly functional web interface for searching fieldedcomponents.
Andrew?s system eventually allowedfor individual students to login and register annota-tions on the email messages.This collection consists of approximately 200,000messages after the duplicates have been removed.We wanted to identify a subset of emails that mightbe interesting for analysis while at the same timeavoiding highly personal messages, messages con-sisting mainly of jokes, and so on.
After doing nu-merous searches, we decided to try to focus primar-ily on documents relating to the California energycrisis, trading discrepancies, and messages occur-ring near the end of the time range (just before thecompany?s stock crashed).After selecting about 1500 messages, I devised aninitial set of categories.
In class we refined these.One student had the interesting idea of trying toidentify change in emotional tone as the scandalssurrounding the company came to light, so we addedemotional tone as a category type.
Each message5http://www-2.cs.cmu.edu/ enron/6http://lucene.apache.orgwas then read and annotated by two students usingthe pre-defined categories.
Students were asked toreconcile their differences when they had them.Despite these safeguards, my impression is thatthe resulting assignments are far from consistent andthe categories themselves are still rather ad hoc andoftentimes overlapping.
There were many difficultcuration issues, such as how to categorize a messagewith forwarded content when that content differedin kind from the new material.
If we?d spent moretime on this we could have done a better job, but asthis was not an information organization course, Ifelt we could not spend more time on perfecting thelabels.
Thus, I do not recommend the category la-bels be used for serious analysis.
Nevertheless, anumber of researchers have asked for the cleanedup database and categories, and we have made thempublicly available, along with the search interface.7The students were then given two weeks to pro-cess the collection in some manner.
I made sev-eral suggestions, including trying to automaticallyassign the hand-assigned categories, extending someautomatic acronym recognition work that we?d donein our research (Schwartz and Hearst, 2003), usingnamed entity recognition code to identify various ac-tors, clustering the collection, or doing some kind ofsocial network analysis.
Students were told that theycould extend this assignment into their final projectsif they chose.For most students it was difficult to obtain a strongresult using this collection.
The significant excep-tion was for those students who worked on ex-tending our acronym recognition algorithm; theseprojects were quite successful.
(In fact, one studentmanaged to improve on our results with a rather sim-ple modification to our code.)
Students often hadcreative ideas that were stymied by the poor qualityof the available tools.
Two groups used the MAL-LET named entity recognizer toolkit8 in order to dovarious kinds of social network analysis, but the re-sults were poor.
(Students managed to make up forthis deficiency in creative ways.
)I was a bit worried about students trying to useclustering to analyze the results, given the generaldifficulty of making sense of the results of cluster-7http://bailando.sims.berkeley.edu/enron email.html8http://mallet.cs.umass.edu6ing, and this concern was justified.
Clustering basedon Weka and other tools is of course memory- andcompute-intensive, but more problematically, the re-sults are difficult to interpret.
I would recommendagainst allowing students to do a text clustering exer-cise unless within a more constrained environment.In summary, students were excited about build-ing a resource based on relatively untapped and veryinteresting data.
The resulting analysis on this un-tamed text was somewhat disappointing, but giventhat only two weeks were spent on this part of theassignment, I believe it was a good learning experi-ence.
Furthermore, the resulting resource seems tobe of interest to a number of researchers, as was ourintention.6.3 Final ProjectsI deliberately kept the time for the final projectsshort (about 3 weeks) so students would not go over-board or feel pressure to do something hugely time-consuming.
The goal was to allow students to tietogether some of the different ideas and skills they?dacquired in the class (and elsewhere), and to learnthem in more depth by applying them to a topic ofpersonal interest.Students were encouraged to work in pairs, andI suggested a list of project ideas.
Students whoadopted suggested projects tended to be more suc-cessful than those who developed their own.
Thosewho tried other topics were often too ambitious andhad trouble getting meaningful results.
However,several of those students were trying ideas that theyplanned to apply to their capstone projects, and soit was highly valuable for them to get a preview ofwhat worked and what did not.One suggestion I made was to create a back-of-the-book indexer, specifically for a recipe book, andone team did a good job with this project.
Anotherwas to improve on or apply an automatic hierarchygeneration tool that we have developed in our re-search (Stoica and Hearst, 2004).
Students workingon a project to collect metadata for camera phoneimages successfully applied this tool to this prob-lem.
Again, social networking analysis topics werepopular but not particularly successful; NLP toolsare not advanced enough yet to meet the needs ofthis intriguing topic area.
Not surprisingly, whenstudents started with a new (interesting) text collec-tion, they were bogged down in the preprocessingstage before they could get much interesting workdone.6.4 Reflecting on AssignmentsAlthough students were excited about the Enron col-lection and we created a resource that is actively be-ing used by other researchers, I think in future ver-sions of the class I will omit this kind of assignmentand have the students start their final projects sooner.This will allow them time to do any preprocessingnecessary to get the text into shape for doing theinteresting work.
I will also exercise more controlover what they are allowed to attempt (which is notmy usual style) in order to ensure more successfuloutcomes.I am not sure if I will use NLTK again or not.
Ifthe designers make significant improvements on thecode and documentation, then I probably will.
Thestyle and intent of the tutorials are quite appropriatefor the goals of the class.
Students with strongercoding background tended to use java for their finalprojects, whereas the others tended to build on thepython code we developed in the class assignments,which suggests that this kind of toolkit approach isuseful for them.7 ConclusionsOverall, I feel the main goals of the course were met.Although I am emphasizing how the course could beimproved, most students were quite positive aboutthe class, giving it an overall score of 5.8 out of 7with a mode of 6 in their anonymous course reviews.
(This is on the low side for my courses; most whogave it low scores found the programming too diffi-cult.
)Most students found the material highly stimulat-ing and the work challenging but not overwhelming.Several students mentioned that a lab session witha dedicated TA would have been desirable.
Sev-eral suggested covering less material in more depthand several commented that the Enron exercise wasa neat idea although not entirely successful in execu-tion.
Students remarked on liking reading researchpapers rather than a textbook (they also liked the rel-atively light reading load, which I feel was appropri-ate given the heavy assignment load).
Some students7wanted more emphasis on real-world applications; Ithink it would be useful to have guest speakers fromindustry talk about this if possible.I would like to see more research tools devel-oped to a point to which they can be applied moresuccessfully, especially in the area of informationextraction.
I would also recommend to colleaguesthat careful control be retained over assignments andprojects to ensure feasibility in the outcome.
It ismore difficult to get good results on class projects inNLP than in other areas I?ve taught.
As we so oftensee in text analysis work, it can often be difficult todo better than simple word counts for many projects.I am interested in hearing ideas about how to ac-commodate both the somewhat technical and thehighly technical students, especially in the earlyparts of the course.
Perhaps the best solution is tooffer an optional laboratory section, at least for thefirst few weeks, but perhaps for the entire term, butthis solution obviously requires more resources.When designing this course I did a fairly extensiveweb search looking for courses that offered what Iwas interested in, but didn?t find much.
I used theproceedings of the ACL-02 workshop on teachingNLP (where I learned about NLTK) as well as theNLP Universe.
I think it would be a good idea tostart an archive of teaching resources; ACM SIGCHIis in the midst of creating such an educational digitallibrary and this example is worth studying.9AcknowledgementsThanks to Preslav Nakov, Andrew Fiore, and Bar-bara Rosario for their help with the class, and forall the students who took the class.
Thanks also toSteven Bird and Edward Loper for developing andsharing NLTK, and for their generous time and helpwith the system during the course of the class.
Thiswork was supported in part by NSF DBI-0317510.ReferencesTimothy Chklovski and Patrick Pantel.
2004.
Verbo-cean: Mining the web for fine-grained semantic verbrelations.
In Proceedings of EMNLP, Barcelona.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicogra-9http://hcc.cc.gatech.edu/phy.
American Journal of Computational Linguistics,16(1):22?29.Edward Loper and Steven Bird.
2002.
Nltk: The naturallanguage toolkit.
In Proceedings of the ACL Work-shop on Effective Tools and Methodologies for Teach-ing Natural Language Processing and ComputationalLinguistics, Philadelphia.David Palmer and Marti A. Hearst.
1997.
Adaptive mul-tilingual sentence boundary disambiguation.
Compu-tational Lingiustics, 23(2).Ariel Schwartz and Marti Hearst.
2003.
A simplealgorithm for identifying abbreviation definitions inbiomedical text.
In Proceedings of the Pacific Sym-posium on Biocomputing (PSB 2003), Kauai, Hawaii.Emilia Stoica and Marti Hearst.
2004.
Nearly-automatedmetadata hierarchy creation.
In Proceedings of HLT-NAACL Companion Volume, Boston.Ian H. Witten and Eibe Frank.
2000.
Data Mining:Practical machine learning tools with Java implemen-tations.
Morgan Kaufmann, San Francisco.8
