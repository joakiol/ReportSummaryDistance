Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 24?33,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsLarge Scale Relation Detection?Chris Welty and James Fan and David Gondek and Andrew SchlaikjerIBM Watson Research Center ?
19 Skyline Drive ?
Hawthorne, NY 10532, USA{welty, fanj, dgondek, ahschlai}@us.ibm.comAbstractWe present a technique for reading sentencesand producing sets of hypothetical relationsthat the sentence may be expressing.
Thetechnique uses large amounts of instance-levelbackground knowledge about the relations inorder to gather statistics on the various waysthe relation may be expressed in language, andwas inspired by the observation that half of thelinguistic forms used to express relations oc-cur very infrequently and are simply not con-sidered by systems that use too few seed ex-amples.
Some very early experiments are pre-sented that show promising results.1 IntroductionWe are building a system that learns to read in a newdomain by applying a novel combination of naturallanguage processing, machine learning, knowledgerepresentation and reasoning, information retrieval,data mining, etc.
techniques in an integrated way.Central to our approach is the view that all parts ofthe system should be able to interact during any levelof processing, rather than a pipeline view in whichcertain parts of the system only take as input the re-sults of other parts, and thus cannot influence thoseresults.
In this paper we discuss a particular caseof that idea, using large knowledge bases hand inhand with natural language processing to improvethe quality of relation detection.
Ultimately we de-fine reading as representing natural language text in?
Research supported in part by DARPA MRP GrantFA8750-09-C0172a way that integrates background knowledge and in-ference, and thus are doing the relation detectionto better integrate text with pre-existing knowledge,however that should not (and does not) prevent usfrom using what knowledge we have to influencethat integration along the way.2 BackgroundThe most obvious points of interaction between NLPand KR systems are named entity tagging and otherforms of type instance extraction.
The second ma-jor point of interaction is relation extraction, andwhile there are many kinds of relations that maybe detected (e.g.
syntactic relations such as modi-fiers and verb subject/object, equivalence relationslike coreference or nicknames, event frame relationssuch as participants, etc.
), the kind of relations thatreading systems need to extract to support domain-specific reasoning tasks are relations that are knownto be expressed in supporting knowledge-bases.
Wecall these relations semantic relations in this paper.Compared to entity and type detection, extractionof semantic relations is significantly harder.
In ourwork on bridging the NLP-KR gap, we have ob-served several aspects of what makes this task dif-ficult, which we discuss below.2.1 Keep readingHumans do not read and understand text by first rec-ognizing named entities, giving them types, and thenfinding a small fixed set of relations between them.Rather, humans start with the first sentence and buildup a representation of what they read that expandsand is refined during reading.
Furthermore, humans24do not ?populate databases?
by reading; knowledgeis not only a product of reading, it is an integral partof it.
We require knowledge during reading in orderto understand what we read.One of the central tenets of our machine readingsystem is the notion that reading is not performed onsentences in isolation.
Often, problems in NLP canbe resolved by simply waiting for the next sentence,or remembering the results from the previous, andincorporating background or domain specific knowl-edge.
This includes parse ambiguity, coreference,typing of named entities, etc.
We call this the KeepReading principle.Keep reading applies to relation extraction aswell.
Most relation extraction systems are imple-mented such that a single interpretation is forcedon a sentence, based only on features of the sen-tence itself.
In fact, this has been a shortcomingof many NLP systems in the past.
However, whenyou apply the Keep Reading principle, multiple hy-potheses from different parts of the NLP pipeline aremaintained, and decisions are deferred until there isenough evidence to make a high confidence choicebetween competing hypotheses.
Knowledge, suchas those entities already known to participate in arelation and how that relation was expressed, canand should be part of that evidence.
We will presentmany examples of the principle in subsequent sec-tions.2.2 Expressing relations in languageDue to the flexibility and expressive power of nat-ural language, a specific type of semantic relationcan usually be expressed in language in a myriadof ways.
In addition, semantic relations are of-ten implied by the expression of other relations.For example, all of the following sentences moreor less express the same relation between an actorand a movie: (1) ?Elijah wood starred in Lord ofthe Rings: The Fellowship of the Ring?, (2) ?Lordof the Rings: The Fellowship of the Ring?s ElijahWood, ...?, and(3) ?Elijah Wood?s coming of agewas clearly his portrayal of the dedicated and noblehobbit that led the eponymous fellowship from thefirst episode of the Lord of the Rings trilogy.?
Nohuman reader would have any trouble recognizingthe relation, but clearly this variability of expressionpresents a major problem for machine reading sys-tems.To get an empirical sense of the variability of nat-ural language used to express a relation, we stud-ied a few semantic relations and found sentencesthat expressed that relation, extracted simple pat-terns to account for how the relation is expressedbetween two arguments, mainly by removing the re-lation arguments (e.g.
?Elijah Wood?
and ?Lord ofthe Rings: The Fellowship of the Ring?
above) andreplacing them with variables.
We then counted thenumber of times each pattern was used to expressthe relation, producing a recognizable very long tailshown in Figure 1 for the top 50 patterns expressingthe acted-in-movie relation in 17k sentences.
Moresophisticated pattern generalization (as discussed inlater sections) would significantly fatten the head,bringing it closer to the traditional 50% of the areaunder the curve, but no amount of generalizationwill eliminate the tail.
The patterns become increas-ingly esoteric, such as ?The movie Death BecomesHer features a brief sequence in which Bruce Willisand Goldie Hawn?s characters plan Meryl Streep?scharacter?s death by sending her car off of a cliffon Mulholland Drive,?
or ?The best known Hawk-sian woman is probably Lauren Bacall, who iconi-cally played the type opposite Humphrey Bogart inTo Have and Have Not and The Big Sleep.
?2.3 What relations matterWe do not consider relation extraction to be an endin and of itself, but rather as a component in largersystems that perform some task requiring interoper-ation between language- and knowledge-based com-ponents.
Such larger tasks can include questionanswering, medical diagnosis, intelligence analysis,museum curation, etc.
These tasks have evaluationcriteria that go beyond measuring relation extractionresults.
The first step in applying relation detectionto these larger tasks is analysis to determine whatrelations matter for the task and domain.There are a number of manual and semi-automaticways to perform such analysis.
Repeating thetheme of this paper, which is to use pre-existingknowledge-bases as resources, we performed thisanalysis using freebase and a set of 20k question-answer pairs representing our task domain.
For eachquestion, we formed tuples of each entity name inthe question (QNE) with the answer, and found all2501002003004005006007008009001000Figure 1: Pattern frequency for acted-in-movie relation for 17k sentences.01020304050607080Figure 2: Relative frequency for top 50 relations in 20K question-answer pairs.the relations in the KB connecting the entities.
Wekept a count for each relation of how often it con-nected a QNE to an answer.
Of course we don?t ac-tually know for sure that the relation is the one beingasked, but the intuition is that if the amount of datais big enough, you will have at least a ranked list ofwhich relations are the most frequent.Figure 2 shows the ranking for the top 50 rela-tions.
Note that, even when restricted to the top 50relations, the graph has no head, it is basically alltail; The top 50 relations cover about 15% of the do-main.
In smaller, manual attempts to determine themost frequent relations in our domain, we had a sim-ilar result.
What this means is that supporting eventhe top 50 relations with perfect recall covers about15% of the questions.
It is possible, of course, tonarrow the domain and restrict the relations that canbe queried?this is what database systems do.
Forreading, however, the results are the same.
A read-ing system requires the ability to recognize hundredsof relations to have any significant impact on under-standing.2.4 Multi-relation learning on many seedsThe results shown in Figure 1 and Figure 2 con-firmed much of the analysis and experiences we?dhad in the past trying to apply relation extraction inthe traditional way to natural language problems like26question answering, building concept graphs fromintelligence reports, semantic search, etc.
Either bytraining machine learning algorithms on manuallyannotated data or by manually crafting finite-statetransducers, relation detection is faced by this two-fold problem: the per-relation extraction hits a wallaround 50% recall, and each relation itself occursinfrequently in the data.This apparent futility of relation extraction led usto rethink our approach.
First of all, the very longtail for relation patterns led us to consider how topick up the tail.
We concluded that to do so wouldrequire many more examples of the relation, butwhere can we get them?
In the world of linked-data,huge instance-centered knowledge-bases are rapidlygrowing and spreading on the semantic web1.
Re-sources like DBPedia, Freebase, IMDB, Geonames,the Gene Ontology, etc., are making available RDF-based data about a number of domains.
Thesesources of structured knowledge can provide a largenumber of seed tuples for many different relations.This is discussed further below.Furthermore, the all-tail nature of relation cover-age led us to consider performing relation extractionon multiple relations at once.
Some promising re-sults on multi-relation learning have already been re-ported in (Carlson et al, 2009), and the data sourcesmentioned above give us many more than just thehandful of seed instances used in those experiments.The idea of learning multiple relations at once alsofits with our keep reading principle - multiple rela-tion hypotheses may be annotated between the samearguments, with further evidence helping to disam-biguate them.3 ApproachOne common approach to relation extraction is tostart with seed tuples and find sentences that con-tain mentions of both elements of the tuple.
Fromeach such sentence a pattern is generated using atminimum universal generalization (replace the tupleelements with variables), though adding any form ofgeneralization here can significantly improve recall.Finally, evaluate the patterns by applying them totext and evaluating the precision and recall of the tu-ples extracted by the patterns.
Our approach, called1http://linkeddata.org/Large Scale Relation Detection (LSRD), differs inthree important ways:1.
We start with a knowledge-base containing alarge number (thousands to millions) of tuplesencoding relation instances of various types.Our hypothesis is that only a large number ofexamples can possibly account for the long tail.2.
We do not learn one relation at a time, butrather, associate a pattern with a set of relationswhose tuples appear in that pattern.
Thus, whena pattern is matched to a sentence during read-ing, each relation in its set of associated rela-tions is posited as a hypothetical interpretationof the sentence, to be supported or refuted byfurther reading.3.
We use the knowledge-base as an oracle to de-termine negative examples of a relation.
Asa result the technique is semi-supervised; itrequires no human intervention but does re-quire reliable knowledge-bases as input?theseknowledge-bases are readily available today.Many relation extraction techniques depend on aprior step of named entity recognition (NER) andtyping, in order to identify potential arguments.However, this limits recall to the recall of the NERstep.
In our approach patterns can match on anynoun phrase, and typing of these NPs is simply an-other form of evidence.All this means our approach is not relation extrac-tion per se, it typically does not make conclusionsabout a relation in a sentence, but extracts hypothe-ses to be resolved by other parts of our reading sys-tem.In the following sections, we elaborate on thetechnique and some details of the current implemen-tation.3.1 Basic pipelineThe two principle inputs are a corpus and aknowledge-base (KB).
For the experiments below,we used the English Gigaword corpus2 extendedwith Wikipedia and other news sources, and IMDB,DBPedia, and Freebase KBs, as shown.
The intent is2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2003T0527to run against a web-scale corpus and larger linked-data sets.Input documents are sentence delimited, tok-enized and parsed.
The technique can benefit dra-matically from coreference resolution, however inthe experiments shown, this was not present.
Foreach pair of proper names in a sentence, the namesare looked up in the KB, and if they are related,a pattern is extracted from the sentence.
At min-imum, pattern extraction should replace the nameswith variables.
Depending on how patterns are ex-tracted, one pattern may be extracted per sentence,or one pattern may be extracted per pair of propernames in the sentence.
Each pattern is associatedwith all the relations known in the KB between thetwo proper names.
If the pattern has been extractedbefore, the two are merged by incrementing the as-sociated relation counts.
This phase, called patterninduction, is repeated for the entire corpus, resultingin a large set of patterns, each pattern associated withrelations.
For each ?pattern, relation?
pair, there is acount of the number of times that pattern appearedin the corpus with names that are in the relation ac-cording to the KB.The pattern induction phase results in positivecounts, i.e.
the number of times a pattern appearedin the corpus with named entities known to be re-lated in the KB.
However, the induction phase doesnot exhaustively count the number of times each pat-tern appears in the corpus, as a pattern may appearwith entities that are not known in the KB, or are notknown to be related.
The second phase, called pat-tern training, goes through the entire corpus again,trying to match induced patterns to sentences, bind-ing any noun phrase to the pattern variables.
Someattempt is made to resolve the noun phrase to some-thing (most obviously, a name) that can be lookedup in the KB, and for each relation associated withthe pattern, if the two names are not in the relationaccording to the KB, the negative count for that re-lation in the matched pattern is incremented.
Theresult of the pattern training phase is an updated setof ?pattern, relation?
pairs with negative counts.The following example illustrates the basic pro-cessing.
During induction, this sentence is encoun-tered:Tom Cruise and co-star Nicole Kidmanappeared together at the premier.The proper names ?Tom Cruise?
and ?Nicole Kid-man?
are recognized and looked up in the KB.
Wefind instances in the KB with those names, and thefollowing relations: coStar(Tom Cruise,Nicole Kidman); marriedTo(TomCruise, Nicole Kidman).
We extract apattern p1: ?x and co-star ?y appearedtogether at the premier in which all thenames have been replace by variables, and theassociations <p1, costar, 1, 0> and <p1,marriedTo, 1, 0> with positive counts andzero negative counts.
Over the entire corpus, we?dexpect the pattern to appear a few times and endup with final positive counts like <p1, coStar,14, 0> and <p1, marriedTo, 2, 0>, in-dicating the pattern p1 appeared 14 times in thecorpus between names known to participate in thecoStar relation, and twice between names knownto participate in the marriedTo relation.
Duringtraining, the following sentence is encountered thatmatches p1:Tom Hanks and co-star Daryl Hannah ap-peared together at the premier.The names ?Tom Hanks?
and ?Daryl Hannah?are looked up in the KB and in this case onlythe relation coStar is found between them, so themarriedTo association is updated with a negativecount: <p1, marriedTo, 2, -1>.
Over theentire corpus, we?d expect the counts to be some-thing like <p1, costar, 14, -6> and <p1,marriedTo, 2, -18>.This is a very simple example and it is difficult tosee the value of the pattern training phase, as it mayappear the negative counts could be collected duringthe induction phase.
There are several reasons whythis is not so.
First of all, since the first phase onlyinduces patterns between proper names that appearand are related within the KB, a sentence in the cor-pus matching the pattern would be missed if it didnot meet that criteria but was encountered before thepattern was induced.
Secondly, for reasons that arebeyond the scope of this paper, having to do withour Keep Reading principle, the second phase doesslightly more general matching: note that it matchesnoun phrases instead of proper nouns.283.2 Candidate-instance matchingAn obvious part of the process in both phases istaking strings from text and matching them againstnames or labels in the KB.
We refer to the strings inthe sentences as candidate arguments or simply can-didates, and refer to instances in the KB as entitieswith associated attributes.
For simplicity of discus-sion we will assume all KBs are in RDF, and thusall KB instances are nodes in a graph with uniqueidentifiers (URIs) and arcs connecting them to otherinstances or primitive values (strings, numbers, etc.
).A set of specially designated arcs, called labels, con-nect instances to strings that are understood to namethe instances.
The reverse lookup of entity identi-fiers via names referred to in the previous sectionrequires searching for the labels that match a stringfound in a sentence and returning the instance iden-tifier.This step is so obvious it belies the difficultly ofthe matching process and is often overlooked, how-ever in our experiments we have found candidate-instance matching to be a significant source of error.Problems include having many instances with thesame or lexically similar names, slight variations inspelling especially with non-English names, inflex-ibility or inefficiency in string matching in KB im-plementations, etc.
In some of our sources, namesare also encoded as URLs.
In the case of movieand book titles-two of the domains we experimentedwith-the titles seem almost as if they were designedspecifically to befuddle attempts to automaticallyrecognize them.
Just about every English word is abook or movie title, including ?It?, ?Them?, ?And?,etc., many years are titles, and just about every num-ber under 1000.
Longer titles are difficult as well,since simple lexical variations can prevent matchingfrom succeeding, e.g.
the Shakespeare play, A Mid-summer Night?s Dream appears often as MidsummerNight?s Dream, A Midsummer Night Dream, and oc-casionally, in context, just Dream.
When titles arenot distinguished or delimited somehow, they canconfuse parsing which may fail to recognize them asnoun phrases.
We eventually had to build dictionar-ies of multi-word titles to help parsing, but of coursethat was imperfect as well.The problems go beyond the analogous ones incoreference resolution as the sources and technologythemselves are different.
The problems are severeenough that the candidate-instance matching prob-lem contributes the most, of all components in thispipeline, to precision and recall failures.
We haveobserved recall drops of as much as 15% and preci-sion drops of 10% due to candidate-instance match-ing.This problem has been studied somewhat in theliterature, especially in the area of database recordmatching and coreference resolution (Michelson andKnoblock, 2007), but the experiments presented be-low use rudimentary solutions and would benefitsignificantly from improvements; it is important toacknowledge that the problem exists and is not astrivial as it appears at first glance.3.3 Pattern representationThe basic approach accommodates any pattern rep-resentation, and in fact we can accommodate nonpattern-based learning approaches, such as CRFs, asthe primary hypothesis is principally concerned withthe number of seed examples (scaling up initial setof examples is important).
Thus far we have onlyexperimented with two pattern representations: sim-ple lexical patterns in which the known argumentsare replaced in the sentence by variables (as shownin the example above), and patterns based on thespanning tree between the two arguments in a de-pendency parse, again with the known arguments re-placed by variables.
In our initial design we down-played the importance of the pattern representationand especially generalization, with the belief thatvery large scale would remove the need to general-ize.
However, our initial experiments suggest thatgood pattern generalization would have a signifi-cant impact on recall, without negative impact onprecision, which agrees with findings in the litera-ture (Pantel and Pennacchiotti, 2006).
Thus, theseearly results only employ rudimentary pattern gen-eralization techniques, though this is an area we in-tend to improve.
We discuss some more details ofthe lack of generalization below.4 ExperimentIn this section we present a set of very early proof ofconcept experiments performed using drastic simpli-fications of the LSRD design.
We began, in fact, by29Relation Prec Rec F1 Tuples Seedsimdb:actedIn 46.3 45.8 0.46 9M 30Kfrb:authorOf 23.4 27.5 0.25 2M 2Mimdb:directorOf 22.8 22.4 0.22 700K 700Kfrb:parentOf 68.2 8.6 0.16 10K 10KTable 1: Precision and recall vs. number of tuples usedfor 4 freebase relations.using single-relation experiments, despite the cen-trality of multiple hypotheses to our reading system,in order to facilitate evaluation and understanding ofthe technique.
Our main focus was to gather datato support (or refute) the hypothesis that more re-lation examples would matter during pattern induc-tion, and that using the KB as an oracle for trainingwould work.
Clearly, no KB is complete to beginwith, and candidate-instance matching errors dropapparent coverage further, so we intended to explorethe degree to which the KB?s coverage of the relationimpacted performance.
To accomplish this, we ex-amined four relations with different coverage char-acteristics in the KB.4.1 Setup and resultsThe first relation we tried was the acted-in-showrelation from IMDB; for convenience we refer toit as imdb:actedIn.
An IMDB show is a movie,TV episode, or series.
This relation has over 9M<actor, show> tuples, and its coverage wascomplete as far as we were able to determine.
How-ever, the version we used did not have a lot of namevariations for actors.
The second relation was theauthor-of relation from Freebase (frb:authorOf ),with roughly 2M <author, written-work>tuples.
The third relation was the director-of-movie relation from IMDB (imdb:directorOf ), with700k <director,movie> tuples.
The fourthrelation was the parent-of relation from Free-base (frb:parentOf ), with roughly 10K <parent,child> tuples (mostly biblical and entertainment).Results are shown in Table 1.The imdb:actedIn experiment was performed onthe first version of the system that ran on 1 CPU and,due to resource constraints, was not able to use morethan 30K seed tuples for the rule induction phase.However, the full KB (9M relation instances) wasavailable for the training phase.
With some man-ual effort, we selected tuples (actor-movie pairs) ofpopular actors and movies that we expected to ap-pear most frequently in the corpus.
In the other ex-periments, the full tuple set was available for bothphases, but 2M tuples was the limit for the size ofthe KB in the implementation.
With these promisingpreliminary results, we expect a full implementationto accommodate up to 1B tuples or more.The evaluation was performed in decreasing de-grees of rigor.
The imdb:actedIn experiment was runagainst 20K sentences with roughly 1000 actor inmovie relations and checked by hand.
For the otherthree, the same sentences were used, but the groundtruth was generated in a semi-automatic way by re-using the LSRD assumption that a sentence con-taining tuples in the relation expresses the relation,and then spot-checked manually.
Thus the evalua-tion for these three experiments favors the LSRD ap-proach, though spot checking revealed it is the pre-cision and not the recall that benefits most from this,and all the recall problems in the ground truth (i.e.sentences that did express the relation but were notin the ground truth) were due to candidate-instancematching problems.
An additional idiosyncrasy inthe evaluation is that the sentences in the groundtruth were actually questions, in which one of thearguments to the relation was the answer.
Sincethe patterns were induced and trained on statements,there is a mismatch in style which also significantlyimpacts recall.
Thus the precision and recall num-bers should not be taken as general performance, butare useful only relative to each other.4.2 DiscussionThe results are promising, and we are continuing thework with a scalable implementation.
Overall, theresults seem to show a clear correlation between thenumber of seed tuples and relation extraction recall.However, the results do not as clearly support themany examples hypothesis as it may seem.
Whenan actor and a film that actor starred in are men-tioned in a sentence, it is very often the case that thesentence expresses that relation.
However, this wasless likely in the case of the parent-of relation, andas we considered other relations, we found a widedegree of variation.
The borders relation betweentwo countries, for example, is on the other extremefrom actor-in-movie.
Bordering nations often wage30war, trade, suspend relations, deport refugees, sup-port, oppose, etc.
each other, so finding the two na-tions in a sentence together is not highly indicativeof one relation or another.
The director-of-movie re-lation was closer to acted-in-movie in this regard,and author-of a bit below that.
The obvious next stepto gather more data on the many examples hypoth-esis is to run the experiments with one relation, in-creasing the number of tuples with each experimentand observing the change in precision and recall.The recall results do not seem particularly strik-ing, though these experiments do not include pat-tern generalization (other than what a dependencyparse provides) or coreference, use a small corpus,and poor candidate-instance matching.
Further, asnoted above there were other idiosyncrasies in theevaluation that make them only useful for relativecomparison, not as general results.Many of the patterns induced, especially forthe acted-in-movie relation, were highly lexical,using e.g.
parenthesis or other punctuation tosignal the relation.
For example, a commonpattern was actor-name (movie-name), ormovie-name: actor-name, e.g.
?LeonardoDiCaprio (Titanic) was considering accepting therole as Anakin Skywalker,?
or ?Titanic: LeonardoDiCaprio and Kate Blanchett steam up the silverscreen against the backdrop of the infamous disas-ter.?
Clearly patterns like this rely heavily on thecontext and typing to work.
In general the pattern?x (?y) is not reliable for the actor-in-movie re-lation unless you know ?x is an actor and ?y is amovie.
However, some patterns, like ?x appearsin the screen epic ?y is highly indicativeof the relation without the types at all - in fact it isso high precision it could be used to infer the typesof ?x and ?y if they were not known.
This seemsto fit extremely well in our larger reading system,in which the pattern itself provides one form of evi-dence to be combined with others, but was not a partof our evaluation.One of the most important things to general-ize in the patterns we observed was dates.
Ifpatterns like, actor-name appears in the1994 screen epic movie-name could havebeen generalized to actor-name appears inthe date screen epic movie-name, re-call would have been boosted significantly.
As itstood in these experiments, everything but the argu-ments had to match.
Similarly, many relations oftenappear in lists, and our patterns were not able to gen-eralize that away.
For example the sentence, ?MarkHamill appeared in Star Wars, Star Wars: The Em-pire Strikes Back, and Star Wars: The Return of theJedi,?
causes three patterns to be induced; in each,one of the movies is replaced by a variable in thepattern and the other two are required to be present.Then of course all this needs to be combined, so thatthe sentence, ?Indiana Jones and the Last Crusade isa 1989 adventure film directed by Steven Spielbergand starring Harrison Ford, Sean Connery, DenholmElliott and Julian Glover,?
would generate a patternthat would get the right arguments out of ?Titanicis a 1997 epic film directed by James Cameron andstarring Leonardo DiCaprio, Kate Winslett, KathyBates and Bill Paxon.?
At the moment the formersentence generates four patterns that require the di-rector and dates to be exactly the same.Some articles in the corpus were biographieswhich were rich with relation content but also withpervasive anaphora, name abbreviations, and othercoreference manifestations that severely hamperedinduction and evaluation.5 Related workEarly work in semi-supervised learning techniquessuch as co-training and multi-view learning (Blumand Mitchell, 1998) laid much of the ground workfor subsequent experiments in bootstrapped learn-ing for various NLP tasks, including named entitydetection (Craven et al, 2000; Etzioni et al, 2005)and document classification (Nigam et al, 2006).This work?s pattern induction technique also repre-sents a semi-supervised approach, here applied torelation learning, and at face value is similar in mo-tivation to many of the other reported experimentsin large scale relation learning (Banko and Etzioni,2008; Yates and Etzioni, 2009; Carlson et al, 2009;Carlson et al, 2010).
However, previous techniquesgenerally rely on a small set of example relation in-stances and/or patterns, whereas here we explicitlyrequire a larger source of relation instances for pat-tern induction and training.
This allows us to betterevaluate the precision of all learned patterns acrossmultiple relation types, as well as improve coverage31of the pattern space for any given relation.Another fundamental aspect of our approach liesin the fact that we attempt to learn many relationssimultaneously.
Previously, (Whitelaw et al, 2008)found that such a joint learning approach was use-ful for large-scale named entity detection, and weexpect to see this result carry over to the relation ex-traction task.
(Carlson et al, 2010) also describesrelation learning in a multi-task learning framework,and attempts to optimize various constraints positedacross all relation classes.Examples of the use of negative evidencefor learning the strength of associations betweenlearned patterns and relation classes as proposedhere has not been reported in prior work to ourknowledge.
A number of multi-class learning tech-niques require negative examples in order to prop-erly learn discriminative features of positive classinstances.
To address this requirement, a number ofapproaches have been suggested in the literature forselection or generation of negative class instances.For example, sampling from the positive instancesof other classes, randomly perturbing known pos-itive instances, or breaking known semantic con-straints of the positive class (e.g.
positing multiplestate capitols for the same state).
With this work,we treat our existing RDF store as an oracle, and as-sume it is sufficiently comprehensive that it allowsestimation of negative evidence for all target relationclasses simultaneously.The first (induction) phase of LSRD is very simi-lar to PORE (Wang et al, 2007) (Dolby et al, 2009;Gabrilovich and Markovitch, 2007) and (Nguyenet al, 2007), in which positive examples were ex-tracted from Wikipedia infoboxes.
These also bearstriking similarity to (Agichtein and Gravano, 2000),and all suffer from a significantly smaller number ofseed examples.
Indeed, its not using a database ofspecific tuples that distinguishes LSRD, but that ituses so many; the scale of the induction in LSRDis designed to capture far less frequent patterns byusing significantly more seedsIn (Ramakrishnan et al, 2006) the same intu-ition is captured that knowledge of the structure ofa database should be employed when trying to inter-pret text, though again the three basic hypotheses ofLSRD are not supported.In (Huang et al, 2004), a similar phenomenon towhat we observed with the acted-in-movie relationwas reported in which the chances of a protein in-teraction relation being expressed in a sentence arealready quite high if two proteins are mentioned inthat sentence.6 ConclusionWe have presented an approach for Large Scale Re-lation Detection (LSRD) that is intended to be usedwithin a machine reading system as a source of hy-pothetical interpretations of input sentences in natu-ral language.
The interpretations produced are se-mantic relations between named arguments in thesentences, and they are produced by using a largeknowledge source to generate many possible pat-terns for expressing the relations known by thatsource.We have specifically targeted the technique at theproblem that the frequency of patterns occurring intext that express a particular relation has a very longtail (see Figure 1), and without enough seed exam-ples the extremely infrequent expressions of the re-lation will never be found and learned.
Further, wedo not commit to any learning strategy at this stageof processing, rather we simply produce counts, foreach relation, of how often a particular pattern pro-duces tuples that are in that relation, and how of-ten it doesn?t.
These counts are simply used as ev-idence for different possible interpretations, whichcan be supported or refuted by other components inthe reading system, such as type detection.We presented some very early results which whilepromising are not conclusive.
There were manyidiosyncrasies in the evaluation that made the re-sults meaningful only with respect to other experi-ments that were evaluated the same way.
In addi-tion, the evaluation was done at a component level,as if the technique were a traditional relation extrac-tion component, which ignores one of its primarydifferentiators?that it produces sets of hypotheticalinterpretations.
Instead, the evaluation was doneonly on the top hypothesis independent of other evi-dence.Despite these problems, the intuitions behindLSRD still seem to us valid, and we are investing in atruly large scale implementation that will overcomethe problems discussed here and can provide more32valid evidence to support or refute the hypothesesLSRD is based on:1.
A large number of examples can account for thelong tail in relation expression;2.
Producing sets of hypothetical interpretationsof the sentence, to be supported or refuted byfurther reading, works better than producingone;3.
Using existing, large, linked-data knowledge-bases as oracles can be effective in relation de-tection.References[Agichtein and Gravano2000] E. Agichtein and L. Gra-vano.
2000.
Snowball: extracting relations from largeplain-text collections.
In Proceedings of the 5th ACMConference on Digital Libraries, pages 85?94, SanAntonio, Texas, United States, June.
ACM.
[Banko and Etzioni2008] Michele Banko and Oren Et-zioni.
2008.
The tradeoffs between open and tradi-tional relation extraction.
In Proceedings of the 46thAnnual Meeting of the Association for ComputationalLinguistics.
[Blum and Mitchell1998] A. Blum and T. Mitchell.
1998.Combining labeled and unlabeled data with co-training.
In Proceedings of the 1998 Conference onComputational Learning Theory.
[Carlson et al2009] A. Carlson, J. Betteridge, E. R. Hr-uschka Jr., and T. M. Mitchell.
2009.
Coupling semi-supervised learning of categories and relations.
InProceedings of the NAACL HLT 2009 Workshop onSemi-supervised Learning for Natural Language Pro-cessing.
[Carlson et al2010] A. Carlson, J. Betteridge, R. C.Wang, E. R. Hruschka Jr., and T. M. Mitchell.
2010.Coupled semi-supervised learning for information ex-traction.
In Proceedings of the 3rd ACM InternationalConference on Web Search and Data Mining.
[Craven et al2000] Mark Craven, Dan DiPasquo, DayneFreitag, Andrew McCallum, Tom Mitchell, KamalNigam, and Sean Slattery.
2000.
Learning to constructknowledge bases from the World Wide Web.
ArtificialIntelligence, 118(1?2):69?113.
[Dolby et al2009] Julian Dolby, Achille Fokoue, AdityaKalyanpur, Edith Schonberg, and Kavitha Srinivas.2009.
Extracting enterprise vocabularies using linkedopen data.
In Proceedings of the 8th International Se-mantic Web Conference.
[Etzioni et al2005] Oren Etzioni, Michael Cafarella,Doug Downey, Ana-Maria Popescu, Tal Shaked,Stephen Soderland, Daniel S. Weld, and AlexanderYates.
2005.
Unsupervised named-entity extractionfrom the web: An experimental study.
Artificial Intel-ligence, 165(1):91?134, June.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovichand Shaul Markovitch.
2007.
Computing seman-tic relatedness using wikipedia-based explicit seman-tic analysis.
In IJCAI.
[Huang et al2004] Minlie Huang, Xiaoyan Zhu, Yu Hao,Donald G. Payan, Kunbin Qu, and Ming Li.
2004.Discovering patterns to extract protein-protein interac-tions from full texts.
Bioinformatics, 20(18).
[Michelson and Knoblock2007] Matthew Michelson andCraig A. Knoblock.
2007.
Mining heterogeneoustransformations for record linkage.
In Proceedings ofthe 6th International Workshop on Information Inte-gration on the Web, pages 68?73.
[Nguyen et al2007] Dat P. Nguyen, Yutaka Matsuo, ,and Mitsuru Ishizuka.
2007.
Exploiting syntacticand semantic information for relation extraction fromwikipedia.
In IJCAI.
[Nigam et al2006] K. Nigam, A. McCallum, , andT.
Mitchell, 2006.
Semi-Supervised Learning, chapterSemi-Supervised Text Classification Using EM.
MITPress.
[Pantel and Pennacchiotti2006] Patrick Pantel and MarcoPennacchiotti.
2006.
Espresso: Leveraging genericpatterns for automatically harvesting semantic rela-tions.
In Proceedings of the 21st international Confer-ence on Computational Linguistics and the 44th An-nual Meeting of the Association For ComputationalLinguistics, Sydney, Australia, July.
[Ramakrishnan et al2006] Cartic Ramakrishnan, Krys J.Kochut, and Amit P. Sheth.
2006.
A framework forschema-driven relationship discovery from unstruc-tured text.
In ISWC.
[Wang et al2007] Gang Wang, Yong Yu, and HaipingZhu.
2007.
PORE: Positive-only relation extractionfrom wikipedia text.
In ISWC.
[Whitelaw et al2008] C. Whitelaw, A. Kehlenbeck,N.
Petrovic, , and L. Ungar.
2008.
Web-scale namedentity recognition.
In Proceeding of the 17th ACMConference on information and Knowledge Manage-ment, pages 123?132, Napa Valley, California, USA,October.
ACM.
[Yates and Etzioni2009] Alexander Yates and Oren Et-zioni.
2009.
Unsupervised methods for determiningobject and relation synonyms on the web.
ArtificialIntelligence, 34:255?296.33
