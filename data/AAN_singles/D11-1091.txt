Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 980?990,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsUnsupervised Learning of Selectional Restrictions and Detection ofArgument CoercionsKirk Roberts and Sanda M. HarabagiuHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083, USA{kirk,sanda}@hlt.utdallas.eduAbstractMetonymic language is a pervasive phe-nomenon.
Metonymic type shifting, or ar-gument type coercion, results in a selectionalrestriction violation where the argument?s se-mantic class differs from the class the predi-cate expects.
In this paper we present an un-supervised method that learns the selectionalrestriction of arguments and enables the de-tection of argument coercion.
This methodalso generates an enhanced probabilistic reso-lution of logical metonymies.
The experimen-tal results indicate substantial improvementsthe detection of coercions and the ranking ofmetonymic interpretations.1 IntroductionMetonymic language is pervasive in today?s socialinteractions.
For example, it is typical to find ques-tions that require metonymic resolution:(Q1) Did you enjoy War and Peace?
(Q2) Does anyone have any advice on how to starta bowling team?1In order to process such questions and capture theintention of the person that posed them, coercionsare needed.
Question (Q1) is interpreted as whetheryou enjoyed reading ?War and Peace?, while (Q2)is interpreted as asking for advice on organizing,forming, or registering a bowling team.
The qual-ity of the answers therefore depends on the abilityto (1) recognize when metonymic language is used,and (2) to produce coercions that capture the user?sintention.
One important step in this direction was1Both questions taken from Yahoo Answers.taken by SemEval-2010 Task 7, which focused onthe ability to recognize (a) an argument?s selectionalrestriction for predicates such as arrive at, cancel,or hear, and (b) the type of coercion that licenseda correct interpretation of the metonymy.
Details ofthe task are reported in (Pustejovsky et al, 2010).Approaches to metonymy based on this task are lim-ited, however, because (a) the task is focused only onsemantically non-ambiguous predicates and (b) theselectional restrictions of the arguments were cho-sen from a pre-defined set of six semantic classes(artifact, document, event, location, proposition, andsound).
However, metonymy coercion systems ca-pable of providing the interpretations of questions(Q1) and (Q2) clearly cannot operate with the sim-plifications designed for this task.Inspired by recent advances in modeling selec-tional preferences with latent-variable models (Rit-ter et al, 2010; ?O Se?aghdha, 2010), we proposean unsupervised model for learning selectional re-strictions.
The model assumes that (1) argumentshave a single selected class exemplified by the se-lectional restriction, and (2) the selected class canbe inferred from the data, in part by modeling howcoercive each predicate is.
The model is capable ofoperating with both ambiguous and disambiguatedpredicates, producing superior results for predicatesthat have been disambiguated.
The selectional re-strictions and coercions detected by the model re-ported in this paper can be used to enhance the logi-cal metonymy approach reported in Lapata and Las-carides (2003).
The experimental results show a sig-nificant improvement in the ranking of interpreta-tions.980The rest of this paper is organized as follows.Section 2 discusses related work.
Section 3 de-tails unsupervised models that inform detection ofmetonymies.
Section 4 outlines a method for disam-biguating ambiguous predicates.
Section 5 describesthe enhanced interpretation of logical metonymieswhen conventional constraints are known.
Section 6outlines our implementation and experimental de-sign.
Section 7 presents our experimental results inthree broad tasks: (i) semantic class induction, (ii)coercion detection, and (iii) logical metonymy inter-pretation.
Section 8 summarizes the conclusions.2 Previous WorkLapata and Lascarides (2003) propose a probabilis-tic ranking model for logical metonymies.
They es-timate these probabilities using co-occurrence fre-quencies of predicate-argument pairs in a corpus.Shutova (2009) extends this approach to providesense-disambiguated interpretations from WordNet(Fellbaum, 1998) by using the alternative interpre-tations to disambiguate polysemous words.
Shutovaand Teufel (2009) extend this approach further byclustering these sense-disambiguated interpretationsinto distinct groups of meaning (e.g., {read, browse,look through} and {write, produce, work on} for?enjoy book?).
Not only do these approaches as-sume logical metonymies have already been iden-tified, but they are susceptible to providing interpre-tations that are themselves logical metonymies (e.g.,finish book).
In this paper, we propose an enhance-ment to resolving logical metonymies by ruling outevent-invoking predicates in order to provide moresemantically valid interpretations.Recently, the resolution of several linguistic prob-lems has benefited from Latent Dirichlet Alloca-tion (LDA) (Blei et al, 2003) models.
?O Se?aghdha(2010) examines several selectional preference mod-els based on LDA in predicting human judgementson predicate-argument plausibility.
Both LDA andan extension, ROOTH-LDA (based on Rooth et al(1999)), perform well at predicting plausibility onunseen predicate-argument pairs.
Inspired by theseresults, we propose to extend selectional preferencemodels in order to learn selectional restrictions.Alternatively, unsupervised algorithms exist thatboth induce semantic classes (Rooth et al, 1999;Lin and Pantel, 2001) and cluster predicates by theirselectional restrictions (Rumshisky et al, 2007) butnone of these provide a sufficient framework for de-termining if a specific argument violates its predi-cate?s selectional restriction.3 Unsupervised Learning of SelectionalRestrictionsIn predicate-argument structures, predicates imposeselectional restrictions in the form of semantic ex-pectations on their arguments.
Whenever the seman-tic class of the argument meets these constraints aselection occurs.
For example, the predicate ?hear?imposes the semantics related to sound on the ar-gument ?voice?.
Because the semantic class for?voice?
conforms to these constraints, we call its se-mantic class the selected class.
However, when thesemantic class of the argument violates these con-straints, we follow Pustejovsky et al (2010) and re-fer to this as a coercion.
In this case, we call theargument?s semantic class the coerced class.
For ex-ample, ?hear speaker?
is a coercion where the ar-gument class, person, is implicitly coerced into thevoice of the speaker, a sound.3.1 A Baseline ModelWe consider the LDA-based selectional prefer-ence model reported in ?O Se?aghdha (2010) as abaseline for modeling selectional restrictions.
For-mally, we define our LDA baseline model as follows.Let V be the predicate vocabulary size, let A be theargument vocabulary size, and let K be the numberof argument classes.
Let avi be the ith (non-unique)argument realized by predicate v. Let cvi be the classfor avi .
Let ?v be the class distribution for predicatev and ?k be the argument distribution for class k.The graphical model for this LDA is shown in Fig-ure 1(a).
The generative process for LDA is:For each argument class k = 1..K:1.
Choose ?k ?
Dirichlet(?
)For each unique predicate v = 1..V :2.
Choose ?v ?
Dirichlet(?
)For every argument i = 1..nv:3.
Choose cvi ?
Multinomial(?v)4.
Choose avi ?
Multinomial(?cvi )Following Griffiths and Steyvers (2004), we col-lapse ?
and ?
and estimate the model using Gibbs981?
?
c a?
?VNK(a)??0?1?s?cxa?
?VNK(b)Figure 1: Graphical models for (a) LDA, and (b) coercion LDA (cLDA).Sampling.
This yields the update equation:p(cvi = k|av;?, ?)
?fvk + ?fv + K?fak + ?fk + A?
(1)Where fak is the frequency of argument a being as-signed class k; fk is the frequency of class k beingassigned to any argument; fvk is the frequency ofpredicate v having an argument of class k; and fv isthe total number of non-unique arguments for pred-icate v.3.2 A Coercion ModelWe now incorporate our assumptions for selec-tional restriction modeling.
Namely: (1) there isone selected class per predicate, and (2) the predi-cate?s selected class can be chosen from the classesof its arguments.
To accomplish this, we must alsoaccount for the coerciveness of each predicate.
Weassign a latent variable ?v for each predicate v thatcontrols how coercive v should be.
The additionalhyper-parameters ?0 and ?1 act as priors on ?v.
Thegenerative process for this coercion LDA model,which we denote cLDA, is:For each argument class k = 1..K:1.
Choose ?k ?
Dirichlet(?
)For each unique predicate v = 1..V :2.
Choose sv ?
Uniform(1,K)3.
Choose ?v ?
Dirichlet(?)24.
Choose ?v ?
Beta(?0, ?1)For every argument i = 1..nv:5.
Choose cvi ?
Multinomial(?v)6.
Choose xvi ?
Bernoulli(?v)7.
If xvi = 1, Choose avi ?
Multinomial(?cvi )Else Choose avi ?
Multinomial(?sv)The model variable sv represents the selected classfor predicate v. The coerced class is represented2With the exception that the probability of drawing the se-lected class sv is zero.
This can be seen as drawing the multi-nomial ?v from a Dirichlet distribution with K-1 components.for each argument i by cvi , where xvi chooses be-tween the selected and coerced class.
The variablexvi is similar to switching variables in other graph-ical models such as Chemudugunta et al (2007)and Reisinger and Mooney (2010), where switch-ing variables are used to choose between a back-ground distribution and a document-specific distri-bution.
In this case, the switching variable choosesbetween a specific class and a predicate-specific dis-tribution.
The graphical model for cLDA is shownin Figure 1(b).
Note that cLDA is virtually equiva-lent to LDA when ?v is 1 and ?1 is small because theselected class will be ignored.
In this way, highly co-ercive predicates have less of an impact on the argu-ment clustering because they are more reliant on themultinomial ?.
We use Gibbs sampling to performmodel inference and collapse ?, ?, and ?
, integrat-ing them out using multinomial-Dirichlet conjugacy(the Beta distribution used by ?
is just a special caseof the Dirichlet with only two parameters).The update formula for the selected class sv is:p(sv = k|av, cv ,xv;?, ?
)?nv?iP (avi |sv = k;?)?
?i?Svfavi k + ?fk + A?
(2)Where nv is the number of argument observationsfor predicate v; Sv is the set of arguments of v thatare selections; and favi k is the frequency of word avibeing assigned to class k for any predicate.
We thensample cvi and xvi jointly:p(cvi = k, xvi = q|sv, cvi?
,xvi?
,av;?, ?, ?)?
p(cvi =k;?
)p(xvi =q; ?
)p(avi |sv, cvi?
,xvi?
,av;?)?
fvk + ?fv + K?fvq + ?qfv0 + ?0 + fv1 + ?1faz + ?fz + A?
(3)982Where fvq, fv0, and fv1 is the frequency of x valuesthat equal q, 0, and 1, respectively, for predicate v;faz is the frequency of word a being in class z andfz is the frequency all words being in class z, wherez is defined as being equal to k when xvi = 1, or svwhen xvi = 0.Note that Equation (2) results in a sampling ofthe selected class for v proportional to the numberof arguments in each class for v, fulfilling our sec-ond assumption.
Also note from Equation (3), thesecond term corresponds to the coerciveness of thepredicate.
When the predicate is very coercive, themarginal probability associated with xvi = 0 will bevery low.
If all predicates become entirely coercive,most x values will become 1 and the cLDA will be-come almost equivalent to an LDA model.3.3 Coercion DetectionAfter the latent parameters have been estimated,we still require a method to determine if a givenpredicate-argument pair is a coercion or not.
Weassign a score in [0, 1] instead of a binary value.Higher scores (near 1) indicate high likelihood ofselection, while lower scores (near 0) indicate coer-cion.
The LDA model must rely on a scoring methodusing the predicate-class and argument-class mix-tures:C1(v, a) =K?kP (k|v)P (a|k)=K?k?vk?ka (4)Where ?vk represents the probability of any argumentof v being in the class k and ?ka represents the prob-ability of the argument a being in class k for anypredicate.
C1 is also available as a scoring methodfor cLDA by including the proportion of the selectedclass sv in ?.
Note that since ?
and ?
are integratedout for both LDA and cLDA, we instead use theirfrequencies smoothed with ?
and ?, respectively,which is their maximum likelihood estimate.The cLDA model contains two useful parametersthat can identify selections and coercions: the se-lected class s and the coercion indicator x. Thisyields two more coercion scoring metrics:C2(v, a) = P (a|sv)= ?sva (5)C3(v, a) = P (xva = 0|v, a)= 1.0 ?
?i?Iva xvi|Iva |(6)Where sv is the selected class for predicate v; Ivais the set of predicate-argument instances for pred-icate v and argument a; and xvi is 0 for a selectionand 1 for a coercion.
Of the three metrics, C3 is themost direct measure of a coercion as it representsthe average decision the model learned on the samepredicate-argument pair.
However, C3 requires alarge sample of instances for a particular predicateand argument, and so may be quite sparse.
In prac-tice, these different metrics have their own strengthsand weaknesses and the best performing method of-ten depends on the final task.4 Predicate Sense InductionOur assumption of a single selected class per predi-cate ignores predicate polysemy.
However, the samelexical item may have multiple meanings, each witha separate selected class.
We therefore propose amethod of partitioning a predicate?s arguments bythe induced senses of the predicate.
This allows sep-arate induced predicates to each select a separate ar-gument class.
Consider the verb fire, which has atleast two distinct common senses: (1) to shoot orpropel an object (e.g., to fire a gun), and (2) to laysomeone off (e.g., to fire an employee).
The firstsense selects a weapon (e.g., gun, bullet, rocket),while the second sense selects a person (e.g., em-ployee, coach, apprentice).Specifically, we employ tiered clustering(Reisinger and Mooney, 2010) using the wordsin the predicate?s context.
Tiered clustering is adiscrete clustering method, as opposed to methodssuch as (Brody and Lapata, 2009) that assign adistribution of word senses to each word instance.Tiered clustering has several advantages overother discrete clustering approaches.
First, tieredclustering learns a background word distribution inaddition to the clusters.
This reduces the impact thatwords common to most senses have on the cluster-ing process and allow clusters to form around onlythe most salient words.
Second, tiered clustering983Cluster 1 Cluster 2 Cluster 3 Cluster 4(18,391) (16,651) (18,749) (11,833)shots ball hire gungun puck letter imaginationIsraeli hired Yeltsin grillmissiles owner minister laserrockets shots workforce cellsofficers coaches executives enginesoldiers net employee brainrounds circle managers !bullets Johnson hired enginesweapons Williams union fireTable 1: Context word clusters resulting from tiered clus-tering for the verb fire (includes the number of uniquewords belonging to each cluster).uses a Chinese Restaurant Process (CRP) prior tocontrol both the formation of new clusters (senses)and the bias toward larger clusters (more commonsenses).
This conforms with our intuition of howword senses are distributed: a few common senseswith a gradual transition to a long tail of rare senses.When deciding which cluster to use for a givenpredicate-argument pair, we use the cluster mostassociated with the argument.We use a 10-token window around the predicateas features.
The result of predicate induction on theverb fire is shown in Table 4.
The first three clusterscan be interpreted to be about (1) firing weapons, (2)sport-related shots (e.g., ?fired the puck?
), and (3)lay-offs.
One must be careful in choosing the param-eters for induction, however, as it is possible to par-tition a unique word sense such that coercions andselections are placed in a separate clusters.
Section 6discusses our parameter selection experiments.5 Logical Metonymy InterpretationLogical metonymies are a unique class of coercionsdue to the fact that their eventive interpretation canbe derived from verbal predicates.
For instance, forthe logical metonymy ?enjoy book?, we know thatread is a good candidate interpretation because (1)books are objects whose purpose is to be read and(2) reading is an event that may be enjoyed.
Wetherefore expect to see many instances of both ?readbook?
and ?enjoy reading?
(Lapata and Lascarides,2003).
Conversely, for coercions with non-eventiveinterpretations, such as ?arrive at meeting?, the in-terpretation (location of) is more dependent on thepredicate (arrive) than the function of its argument(meeting).In this section, we limit our discussion of logicalmetonymy to the verb-object case, its correspond-ing baseline for ranking interpretations, and our pro-posed enhancements.
However, similar baselinesexist for other types of logical metonymy, such asadjective-noun and noun-noun.
Since our enhance-ment does not depend on any syntactic informationbeyond the predicate-argument instances needed forSection 3.2, it could easily be applied to those aswell.Lapata and Lascarides (2003) propose a proba-bilistic ranking model where the probability of aninterpretation e for a verb-object pair (v, o) is pro-portional to the probability of all three in a verb-interpretation-object pattern.3 For example, theprobability that read is the correct interpretation of?enjoy book?
is proportional to the likelihood of see-ing ?enjoy reading book?
expressed as a syntacticdependency in a sufficiently large corpus.
Due todata sparsity, they approximate this likelihood ofseeing the object given the verb and interpretationto simply the likelihood of seeing the object giventhe interpretation.
We denote this logical metonymyranking method as LMLL, formally defined as:LMLL(e; v, o) = Pc(v, e, o)= Pc(e)Pc(v|e)Pc(o|e, v)?
Pc(e)Pc(v|e)Pc(o|e)?
fc(v, e)fc(o, e)Nfc(e)(7)Where Pc and fc indicate probability and frequency,respectively, derived from corpus counts.
See Lap-ata and Lascarides (2003) for a detailed explanationof how these frequencies are obtained.This model, which we consider our baseline, isonly partially correct as the corpus will contain co-ercions that form invalid interpretations.
Considerthe phrases ?enjoy finishing a book?
and ?enjoydiscussing a book?.
Both ?finish book?
and ?dis-cuss book?
are coercions (and logical metonymies)themselves, and do not form a valid interpretation.43They use two patterns: ?v e-ing o?
and ?v to e o?, where eis tagged as a verb.4For evidence of the frequency of these phrases, at the timeof this writing, ?enjoy finishing a book?
and ?enjoy finishingthe book?
have a combined 728 Google hits, while ?enjoy dis-cussing a book?
and ?enjoy discussing the book?
have a com-984Thus, when discovering interpretations for logicalmetonymies, we must be aware of the selectional re-strictions of candidate interpretations.We propose to incorporate the coercion probabil-ity learned by our cLDA model in order to rank onlythose interpretations that are considered selections:LM ?
(e; v, o) = P (v, e, o, xeo = 0) (8)However, due to the approximations made to esti-mate Pc(v, e, o), this probability cannot be directlycalculated as not all the frequencies reflect verb-object counts.
Instead, we can combine the corpusprobability Pc(v, e, o) with the probability that theverb-object pair (e, o) is a coercion in our model.We denote this probability Px(e, o), and it may bederived from the scoring metrics in Equations (4),(5), or (6) above.
We further propose three methodsfor enhancing the LMLL baseline using Px(e, o) toapproximate Equation 8.A naive method for including information fromour cLDA model is to consider the corpus prob-ability, Pc(v, e, o) and the coercion probability,Px(e, o), to be independent:LMIND(e; v, o) = Pc(v, e, o)Px(e, o) (9)In other words, the rank of an interpretation is dic-tated by the unweighted combination of its corpusprobability Pc and its coercion probability Px.
How-ever, these two quantities are not likely to be inde-pendent.
Most instances where e is used with eitherv or o are in fact selective.5 We therefore experimentwith two shallow learning methods for combiningthese two quantities.The first method is a filtering approach where athreshold is learned for Px:LMTH(e; v, o) ={Pc(v, e, o) if Px(e, o) ?
?0 otherwise (10)Where the threshold ?
is learned from a developmentset.
We expect this model could suffer from noisy Pxvalues or to simply choose a threshold of zero due tothe prominence of Pc.Finally, we include a weighted linear model tobined 7,040 Google hits.5For comparison, ?enjoy reading a book?
and ?enjoy read-ing the book?
have a combined 6.5 million Google hitsdiscover the relative value of Pc and Px:LMWT (e; v, o) = w1Pc(v, e, o)+w2Px(e, o) (11)Where w1 and w2 are learned weights.
We dis-cuss how the parameters for LMTH and LMWT arelearned in the experimental setup below.6 Experimental SetupWe use the NYT subsection of the English GigawordFourth Edition (Parker et al, 2009) for a total of1.8M newswire articles.
The Stanford DependencyParser (de Marneffe et al, 2006) is used to extractverb-object relations (dobj) that form the input to ourmodel.
To reduce noise, we keep only verbs listed inVerbNet (Kipper et al, 1998) with at least 100 ar-gument instances, discarding have and say, whichare too semantically flexible to select from clear se-mantic classes and so common they distort the classdistributions.
This results in 4,145 unique verbs with51M argument instances (388K unique arguments).Additionally we use the dependency parser to ex-tract open clausal complements of verbs (e.g., ?liketo swim?)
for use in logical metonymy interpreta-tion.
We believe this to be a more reliable alter-native to the phrase chunk extraction patterns usedin Lapata and Lascarides (2003).
We keep clausalcomplements (xcomp) where the dependent is eithera gerund or infinitive in order to estimate Pc(v|e) inEquation (7).For tiered clustering we use the same implemen-tation as Reisinger and Mooney (2010)6 to partitionthe surface form of the verb into one or more in-duced forms.
Instead of using a fixed number ofiterations, the clustering was run for 100 iterationspast the best recorded log-likelihood in order to findthe best possible fit to the data.
We tuned the hyper-parameters by maximizing the log-likelihood on asmall held-out set of 20 predicate-argument pairs(10 selections, 10 coercions).
The resulting parti-tions were fairly conservative, yielding 12,332 in-duced verbs or about 3 induced verb forms for everysurface form, with 305 verbs not being partitioned atall.We implemented both LDA and cLDA as de-scribed in Sections 3.1 and 3.2.
For the ?
and ?6Available at http://github.com/joeraii/UTML-Latent-Variable-Modeling-Toolkit985hyper-parameters, we used the MALLET (McCal-lum, 2002) defaults of 1.0 and 0.1, respectively, forboth LDA and cLDA.
We used the 20 predicate-argument pairs mentioned above to tune the ?
hyper-parameters as well as the number of iterations.
Both?0 and ?1 were set to 100.
We observed that forboth LDA and cLDA, longer runs (in iterations) re-sulted in improved model log-likelihood but infe-rior results in terms of detecting coercions.
It isnot uncommon in topic modeling for model likeli-hood to not be completely correlated with the scoreon the task for which the topic model was intended(see Chang et al (2009)).
Both LDA and cLDAwere found to perform best at 50 iterations on thisdata, after which their class distributions were less?smooth?
and became rigidly associated with just afew classes, thus having a negative impact on coer-cion detection.
While further iterations hurt coer-cion detection, only minor gains in model likelihoodare seen.
We believe the small number of iterationsnecessary for the model to converge is therefore afunction of the data.
In traditional topic modeling,documents are generally of similar size (i.e., withinan order of magnitude).
But in our data, many pred-icates have 10,000 times more instances than others.We have not yet empirically explored the impact ofusing a more uniform number of arguments for eachpredicate.
This issue also makes it difficult to takemultiple samples, which we experimented with un-successfully.Our a priori intuition was that as the numberof classes was increased, LDA would improve andcLDA would degrade due to its assumption of a sin-gle selected class.
However, this did not always bearout in the results for every task described below.As such, instead of choosing a specific number ofclasses for each model, we describe results for eachmodel with K = 10, 25, and 50.For logical metonymy, both LMTH and LMWTrequire learned parameters.
LMTH needs a learnedthreshold while LMWT needs two learned weights.For both, we split the data set into two partitions,learn the optimal threshold/weights on one partition,and use it as the parameters for the other partition.Both methods are trained on the final scoring metric,described in Section 7.3.
For threshold learning, thisinvolves finding the optimal cut-off to maximize thescore.
For weight learning, we use an exhaustiveinduced predicates?
N Y# classes 10 25 50 10 25 50LDANMI .382 .448 .389 .435 .391 .383Rand .717 .731 .721 .760 .723 .730F1 .425 .319 .192 .543 .311 .205B3 (C) .553 .513 .444 .525 .476 .341B3 (E) .453 .351 .223 .521 .324 .234MUC .545 .545 .531 .500 .532 .544cLDANMI .446 .403 .360 .510 .430 .366Rand .736 .719 .716 .788 .734 .711F1 .448 .291 .183 .567 .329 .184B3 (C) .575 .484 .312 .593 .495 .313B3 (E) .473 .321 .205 .556 .346 .205MUC .500 .521 .507 .595 .541 .571Table 2: Clustering scores for induced classes.search over the range {1.0, 0.9, .
.
.
, 0.2, 0.1, 10?2,10?3, .
.
.
, 10?14} for both w1 and w2.7 Results and Discussion7.1 Semantic Class InductionFor the evaluation of the argument classes in-duced by our method, we use a subset of the Word-Net lexicographer files, which correspond to coarse-grained semantic classes.
We chose this form ofevaluation because, unlike a named entity corpus,no sentential context is required and is thereforemore consistent with the information available toour model.
We use six of the larger, more seman-tically coherent WordNet classes: artifact, person,plant, animal, location, and food.
We consider eachof these a cluster and compare them to clusters com-posed of the top ten non-polysemous words (accord-ing to WordNet) in each of the classes generatedby both the baseline (LDA) and our model (cLDA).Words not in both sets of clusters are removed.
Theresult of this evaluation, compared with six cluster-ing metrics, is shown in Table 2.
For descriptions ofNMI, Rand, and cluster F-measure, see Manning etal.
(2008); for the B3 metrics (Cluster and Element),see Bagga and Baldwin (1998); for the MUC met-ric, see Vilain et al (1995).
Each metric has differ-ent strengths and biases in regards to the number anddistribution of clusters, so all are provided to give ageneral picture of class induction performance.The best performing model on all metrics is cLDAwith induced predicates using 10 classes.
However,as the number of classes is increased and the gran-ularity of the induced classes becomes more fine-grained, LDA (predictably) outperforms cLDA onmost metrics.
This is consistent with our intuition986induced predicates?
N Y# classes 10 25 50 10 25 50LDA C1 74.4 78.7 80.5 69.7 70.1 73.4cLDAC1 80.6 81.2 80.9 76.2 78.4 77.5C2 75.4 75.9 78.9 73.5 68.3 80.8C3 67.8 70.8 67.4 70.9 67.4 74.1Table 3: Accuracy on SemEval-2010 Task 7 data.that a single-class assumption degrades as the num-ber of classes increases.For this evaluation, predicate induction also im-proved LDA for smaller numbers of classes, but notto the degree that it improved cLDA.
Without pred-icate induction, LDA outperforms cLDA on all sixmetrics for 25 and 50 classes.
With predicate in-duction, LDA outperforms cLDA on only one metricfor 25 classes and five metrics for 50 classes.
Thusthe induced predicates do reduce the negative im-pact caused by the single selected class assumptionfor semantic class induction.7.2 Coercion DetectionFor the evaluation of coercion detection, we usethe SemEval-2010 Task 7 data (Pustejovsky et al,2010).
This data uses the most common sense foreach of five predicates (arrive, cancel, deny, fin-ish, and hear) with a total of 2,070 sentences an-notated with the argument?s source type (the argu-ment?s semantic class) and target type (the predi-cate?s selected class for that argument).
We ignorethe actual argument classes and evaluate on the coer-cion type, which is a selection when the source andtarget type match, and a coercion otherwise.In order to evaluate unsupervised systems on thisdata, we use the corresponding training set (1,031examples) to learn a threshold for coercion detec-tion.
At test time, if the model output is below thethreshold, a coercion is inferred.
Otherwise it is con-sidered a selection.
Therefore, the better a modelcan rank selections over coercions, the more accu-rate threshold it will learn.
The results for this eval-uation are shown in Table 3.
The baseline for thistask (threshold = 0, or all selections) is 67.4.The best overall model on this data is cLDA us-ing the C1 coercion scoring method (Equation (4)).This method consistently outperforms the baselineLDA, especially for smaller numbers of classes, per-forming best with K = 25.
The second metric, C2,was not as reliable.
The third metric, C3, performedpoorly on the task.
As discussed in Section 3.3, C3is a direct result of the sampling for the predicate-argument pair in question and can thus be expectedto perform poorly on rare predicate-argument pairs.Given that many of the arguments in this data arerare or unseen in the Gigaword data (e.g., ?cancelRenault?
), C3?s poor performance is understandable.The use of predicate sense induction based ontiered clustering to overcome the single-class as-sumption caused significant degradation in perfor-mance on this task.
Using automatically inducedpredicates instead of the surface form caused an av-erage degradation of 2.6 points across the twelvetests.
A potential explanation for this is thatthe evaluated predicates have a single dominantsense, meaning the single class assumption may bevalid for these predicates (the task-defined selectedclasses are: location for arrive, event for cancel andfinish, proposition for deny, and sound for hear).Therefore it would be interesting to evaluate it ona set of highly polysemous predicates with multi-ple dominant senses.
Furthermore, the introductionof predicate sense induction was designed to helpcLDA, and the performance degradation for thesenine tests was not as large as it was for LDA.
ForcLDA, C1 had an average degradation of 3.5 pointscompared to LDA?s C1 average degradation of 6.5points.
cLDA?s C2 had an average degradation ofonly 2.5 points and C3 was actually improved by 2.1points.
This suggests that there is value in assign-ing different selected classes via sense induction, butthat the two-step approach is not beneficial for thesecommon predicates.
This could be overcome by ajoint approach of inducing predicate classes whilesimultaneously detecting coercions, as the presenceof many coercions would be an indicator that moreinduced predicates are necessary.7.3 Logical Metonymy InterpretationFor the evaluation of logical metonymy, we useboth an existing data set and a newly created dataset.
Shutova and Teufel (2009) annotated 10 verb-object logical metonymies from Lapata and Las-carides (2003) with sense-disambiguated interpreta-tions and organized the interpretations into clustersrepresenting different possible meanings.
For evalu-ation purposes we ignore the sense annotations andclusters and consider all lexical matchings of oneof the annotated interpretations to be correct.
The987induced predicates?
N Y# classes 10 25 50 10 25 50LMLL 0.381 0.365LMINDLDA C1 0.415 0.406 0.383 0.386 0.412 0.395cLDAC1 0.408 0.412 0.412 0.407 0.468 0.439C2 0.415 0.447 0.419 0.414 0.415 0.434C3 0.416 0.453 0.455 0.395 0.416 0.402LMTHLDA C1 0.599 0.568 0.588 0.479 0.520 0.551cLDAC1 0.571 0.644 0.751 0.497 0.620 0.708C2 0.544 0.496 0.633 0.457 0.635 0.660C3 0.601 0.677 0.767 0.472 0.622 0.571LMWTLDA C1 0.383 0.381 0.379 0.365 0.356 0.361cLDAC1 0.380 0.387 0.381 0.386 0.377 0.321C2 0.317 0.342 0.350 0.338 0.340 0.345C3 0.378 0.370 0.350 0.387 0.382 0.384Table 4: Mean average precision (MAP) scores on the Shutova and Teufel (2009) data set.
The bold items indicate thebest scores with/without induced predicates as well as using/not using a threshold-based interpretation method.induced predicates?
N Y# classes 10 25 50 10 25 50LMLL 0.274 0.248LMINDLDA C1 0.291 0.286 0.294 0.263 0.267 0.255cLDAC1 0.296 0.298 0.285 0.280 0.274 0.288C2 0.291 0.287 0.288 0.283 0.271 0.285C3 0.318 0.317 0.333 0.298 0.285 0.307LMTHLDA C1 0.478 0.534 0.534 0.414 0.495 0.479cLDAC1 0.449 0.504 0.541 0.391 0.495 0.513C2 0.505 0.478 0.456 0.398 0.429 0.440C3 0.449 0.496 0.577 0.382 0.439 0.446LMWTLDA C1 0.276 0.270 0.271 0.248 0.251 0.249cLDAC1 0.271 0.272 0.270 0.257 0.259 0.265C2 0.274 0.274 0.266 0.250 0.259 0.261C3 0.271 0.273 0.274 0.253 0.262 0.259Table 5: Mean average precision (MAP) scores on 100 logical metonymies manually annotated with interpretations.The bold items indicate the best scores with/without induced predicates as well as using/not using a threshold-basedinterpretation method.data contains an average of 11 interpretations permetonymy and has a reported 70% recall.In order to create a larger data set, we identified100 verb-object logical metonymies, including thoseused in Lapata and Lascarides (2003).
Three anno-tators were asked to provide up to five interpreta-tions for each metonymy (they were not providedwith any verbs from which to choose, only the verb-object pair).
The annotators provided an average of4.6 interpretations per metonymy.
Because our goalwas recall, inter-annotator agreement was necessar-ily low, and each logical metonymy had an averageof 11.7 unique interpretations.
All annotators agreedon at least one interpretation for 40 metonymies,while for 14 they had no interpretations in common.7Since logical metonymy interpretation is usuallyevaluated as a ranking task, we score our methods7 Data available athttp://www.hlt.utdallas.edu/?kirk/data/lmet.zipusing mean average precision (MAP):MAP = 1QQ?q=1?Nn=1 prec(n)?
rel(n)interps(q) (12)Where Q is the number of metonymies evaluated;N is the number of interpretations ranked; prec(n)is the precision at rank n; rel(n) = 1 if interpreta-tion n is valid, 0 otherwise; and interps(q) is thenumber of valid interpretations for the metonymy q.We rank all 4,145 verbs as interpretations except forthose removed by the threshold technique, as theyhave a score of zero.
This can give LMTH artifi-cially high MAP scores since it may remove somevalid interpretations that are low-ranking.
However,since a smaller, higher precision list may be usefulfor many applications we still consider MAP a validmetric and indicate both the highest scoring methodand the highest scoring non-threshold method.
Theresults on the Shutova and Teufel (2009) data are988shown in Table 4.
The results on our own data areshown in Table 5.The scores reported in the Shutova and Teufel(2009) data are noticeably higher than the data weannotated.
Since the metonymies in our data are asuper-set of those in their data, and since for thosemetonymies our annotators provided approximatelythe same number of interpretations (110 versus 120),this likely indicates the remaining metonymies inour data are more difficult.In all cases the best reported scores use cLDA.Unlike coercion detection on the SemEval data, C3performs very well, achieving the highest scoreswhen no predicate sense induction is used.
Also un-like coercion detection, LDA scores do not increaseas the number of classes increase.
We suspect boththese differences have to do with the fact that the ar-guments in this data are far more common.
SinceLDA is a selectional preference model and its co-ercion scores correspond roughly to the plausibilityof seeing a predicate-argument pair, it is less able todistinguish coercions in common arguments.Of the logical metonymy ranking methods,LMTH consistently produces the highest MAPscores.
However, as stated before, by using a cut-offand removing low-ranking valid interpretations, theMAP score is increased, which might not be applica-ble to some applications.
The best non-thresholdedranking method is LMIND, which naively combinesthe LMLL score with the coercion probability.
Inalmost every case this beats out LMWT .
Upon in-spection, we observed that the range of scale for theLMLL scores are very inconsistent.
This can makeit difficult to learn a linear model using these scoresas features, and as a result the learned weights wereforced to ignore the coercion score and rely entirelyon LMLL.
We attempted other scaling methods,such as a rank-based method, but these had poor re-sults as well, so we leave the problem of the super-vised learning these weights to future work.Using induced senses did not result in the dras-tic and consistent degradation in performance seenon the SemEval data, and the highest non-thresholdresult for the Shutova and Teufel (2009) data usedpredicate induction.
Both metonymy data sets werelimited to the verbs found in Lapata and Lascarides(2003), which are still quite common (attempt, be-gin, enjoy, expect, finish, prefer, start, survive, try,want).
However, the verbs used in our data set had agreater number of WordNet senses attested in a cor-pus than the SemEval data (an average of 4.4 sensesfor our data versus 3.0 senses for the SemEval data).This suggests the potential value of sense inductionfor highly polysemous predicates and further moti-vates the integration of sense induction within a se-lectional restriction model.8 ConclusionWe have presented a novel topic model that ex-tends an unsupervised selectional preference model(LDA) to an unsupervised selectional restrictionmodel (cLDA) using two assumptions.
For the firstassumption, that each predicate has a single selectedclass, we proposed a predicate induction method toovercome predicate polysemy.
This improved re-sults for semantic class induction but proved harmfulfor detecting coercions on common predicates witha single, dominant sense.
For the second assump-tion, that the selected class can be inferred from thedata, we proposed a sampling method based on theclasses of the predicate?s arguments.
Superior per-formance on coercion detection shows the merit ofthis assumption.Additionally, we proposed methods for improvingan existing task, logical metonymy interpretation,using the learned parameters of our model, showingpositive results.It is clear that our model may be improved bymore accurate predicate sense induction.
To thisend, we plan to develop a model that simultane-ously induces predicates and learns coercions, usingknowledge of a predicate?s coerciveness to informthe induction mechanism.AcknowledgementsWe would like to thank Diarmuid ?O Se?aghdha,Bryan Rink, and Anna Rumshisky for several help-ful conversations during the course of this work.We thank Mirella Lapata and Ekaterina Shutova formaking the data from their experiments availableas well as the organizers of SemEval-2010 Task 7for the associated data set.
Additionally, we thankSrikanth Gullapalli, Aileen McDermott, and BryanRink for annotating the data set used in our exper-iment.
Finally, we thank the anonymous reviewersfor their suggestions on improving this work.989ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In In Proceedings ofthe First International Conference on Language Re-sources and Evaluation Workshop on Linguistic Coref-erence.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Samuel Brody and Mirella Lapata.
2009.
Bayesian wordsense induction.
In Proceedings of the 12th Confer-ence of the European Chapter of the Association forComputational Linguistics, pages 103?111.Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,Chong Wang, and David M. Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In NeuralInformation Processing Systems, pages 1?9.Chaitanya Chemudugunta, Padhraic Smyth, and MarkSteyvers.
2007.
Modeling General and Specific As-pects of Documents with a Probabilistic Topic Model.In Advances in Neural Information Processing Sys-tems 19.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the Fifth International Language Re-sources and Evaluation.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences of the United States of America,101(Suppl 1):5228.Karin Kipper, Hoa Trang Dang, and Martha Palmer.1998.
Class-based construction of a verb lexicon.
InProceedings of AAAI/IAAI.Maria Lapata and Alex Lascarides.
2003.
A ProbabilisticAccount of Logical Metonymy.
Computational Lin-guistics, 21(2):261?315.Dekang Lin and Patrick Pantel.
2001.
Induction of Se-mantic Classes from Natural Language Text.
In Pro-ceedings of ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, pages 317?322.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press.Andrew Kachites McCallum.
2002.
Mallet: A machinelearning for language toolkit.Diarmuid ?O Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 435?444.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English Gigaword Fourth Edi-tion.
The LDC Corpus Catalog., LDC2009T13.James Pustejovsky, Anna Rumshisky, Alex Plotnick,Elisabetta Jezek, Olga Batiukova, and Valeria Quochi.2010.
SemEval-2010 Task 7: Argument Selectionand Coercion.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 27?32.Joseph Reisinger and Raymond J. Mooney.
2010.
AMixture Model with Sharing for Lexical Semantics.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 1173?1182.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A La-tent Dirichlet Allocation method for Selectional Pref-erences.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages424?434.Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-roll, and Franz Beil.
1999.
Inducing a semanticallyannotated lexicon via EM-based clustering.
In Pro-ceedings of the 37th Annual Meeting of the Associationfor Computational Linguistics.Anna Rumshisky, Victor A. Grinberg, and James Puste-jovsky.
2007.
Detecting selectional behavior of com-plex types in text.
In Fourth International Workshopon Generative Approaches to the Lexicon.Ekaterina Shutova and Simone Teufel.
2009.
LogicalMetonymy: Discovering Classes of Meaning.
In Pro-ceedings of the CogSci 2009 Workshop on SemanticSpace Models.Ekaterina Shutova.
2009.
Sense-based interpretation oflogical metonymy using a statistical method.
In Pro-ceedings of the ACL 2009 Student Workshop.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsfo the 6th Message Understanding Conference, pages45?52.990
