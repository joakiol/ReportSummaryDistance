Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 260?265,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsLIMSI?s Participation in the 2013 Shared Taskon Native Language IdentificationThomas Lavergne, Gabriel Illouz, Aure?lien MaxLIMSI-CNRSUniv.
Paris SudOrsay, France{firstname.lastname}@limsi.frRyo NagataLIMSI-CNRS & Konan University8-9-1 OkamotoKobe 658-0072 Japanrnagata@konan-u.ac.jpAbstractThis paper describes LIMSI?s participation tothe first shared task on Native Language Iden-tification.
Our submission uses a MaximumEntropy classifier, using as features characterand chunk n-grams, spelling and grammati-cal mistakes, and lexical preferences.
Perfor-mance was slightly improved by using a two-step classifier to better distinguish otherwiseeasily confused native languages.1 IntroductionThis paper describes the submission from LIMSI tothe 2013 shared task on Native Language Identifica-tion (Tetreault et al 2013).
The creation of this newchallenge provided us with a dataset (12,100 TOEFLessays by learners of English of eleven native lan-guages (Blanchard et al 2013)) that was necessaryto us to develop an initial framework for studyingNative Language Identification in text.
We expectthat this challenge will draw conclusions that willprovide the community with new insights into theimpact of native language in foreign language writ-ing.
We believe that such a research domain iscrucial, not only for improving our understandingof language learning and language production pro-cesses, but also for developing Natural LanguageProcessing applications to support text improve-ment.This article is organized as follows.
We first de-scribe in Section 2 our maximum entropy systemused for the classification of a given text in Englishinto the native languages of the shared task.
We thenintroduce the various sets of features that we have in-cluded in our submission, comprising basic n-gramfeatures (3.1) and features to capture spelling mis-takes (3.2), grammatical mistakes (3.3), and lexicalpreference (3.4).
We next report the performance ofeach of our sets of features (4.1) and our attempt toperform a two-step classification to reduce frequentmisclassifications (4.2).
We finally conclude with ashort discussion (section 5).2 A Maximum Entropy modelOur system is based on a classical maximum entropymodel (Berger et al 1996):p?
(y|x) =1Z?
(x)exp(?>F (x, y))whereF is a vector of feature functions, ?
a vector ofassociated parameter values, and Z?
(x) the partitionfunction.Given N independent samples (xi, yi), the modelis trained by minimizing, with respect to ?, the neg-ative conditional log-likelihood of the observations:L(?)
= ?N?i=1log p(yi|xi).This term is complemented with an additional regu-larization term so as to avoid overfitting.
In our case,an `1 regularization is used, with the additional ef-fect to produce a sparse model.The model is trained with a gradient descent algo-rithm (L-BFGS) using the Wapiti toolkit (Lavergneet al 2010).
Convergence is determined either byerror rate stability on an held-out dataset or whenlimits of numerical precision are reached.2603 FeaturesOur submission makes use of basic features, includ-ing n-grams of characters and part-of-speech tags.We further experimented with several sets of fea-tures that will be described and compared in the fol-lowing sections.3.1 Basic featuresWe used n-grams of characters up to length 4 as fea-tures.
In order to reduce the size of the feature spaceand the sparsity of these features, we used a hashkernel (Shi et al 2009) of size 216 with a hash fam-ily of size 4.
This allowed us to significantly reducethe training time with no noticeable impact on themodel?s performance.Our set of basic features also includes n-grams ofpart-of-speech (POS) tags and chunks up to length 3.Both were computed using an in-house CRF-basedtagger trained on PennTreeBank (Marcus et al1993).
The POS tags sequences were post-processedso that word tokens were used in lieu of their cor-responding POS tags for the following: coordinat-ing conjunctions, determiners, prepositions, modals,predeterminers, possessives, pronouns, and questionadverbs (Nagata, 2013).For instance, from this sentence excerpt:[NP Some/DT people/NNS] [VPmight/MD think/VB] [SBAR that/IN][VP traveling/VBG] [PP in/IN].
.
.we extract n-grams from the pseudo POS-tag se-quence:Some NNS MD VB that VBG in.
.
.and n-grams from the chunk sequence:NP VP SBAR VP PP.
.
.The length of chunks is encoded as separate fea-tures that correspond to mean length of each type ofchunks.
As shown in (Nagata, 2013), length of nounsequences is also informative and thus was encodedas a feature.3.2 Capturing spelling mistakesWe added a set of features to capture informationabout spelling mistakes in the model, following theintuition that some spelling mistakes may be at-tributed to the influence of the writer?s native lan-guage.To extract these features, each document is pro-cessed using the ispell1 spell checker.
This re-sults in a list of incorrectly written word forms anda set of potential corrections.
For each word, thebest correction is next selected using a set of rules,which were built manually after a careful study ofthe training dataset.When a corrected word is found, the incorrectfragment of the word is isolated by striping fromthe original and corrected words common prefix andsuffix, keeping only the inner-most substring differ-ence.
For example, given the following mistake andcorrection:appartment?
apartmentthis procedure generates the following feature:pp?
pSuch a feature may for instance help to identify na-tive languages (using latin scripts) where doublingof letters is frequent.3.3 Capturing grammatical mistakesErrors at the grammatical level are captured usingthe ?language tool?
toolkit (Milkowski, 2010), arule-based grammar and style checker.
Each rule fir-ing in a document is mapped to an individual feature.This triggers features such asBEEN PART AGREEMENT, corresponding tocases where the auxiliary be is not followed by apast participle, or EN A VS AN, corresponding toconfusions between the correct form the articles aand an.3.4 Capturing lexical preferencesLearners of a foreign language may have some pref-erence for lexical choice given some semantic con-tent that they want to convey2.
We made the follow-ing assumption: the lexical variant chosen for eachword may correspond to the less ambiguous choiceif mapping from the native language to English3.1http://www.gnu.org/software/ispell/2We assumed that we should not expect thematic differencesin the contents of the essays across original languages, as theprompts for the essays were evenly distributed.3This assumption of course could not hold for advancedlearners of English, who should make their lexical choices in-dependently of their native language.261Thus, for each word in an English essay, if weknew a corresponding word (or sense) that a writermay have thought of in her native language, wewould like to consider the most likely translationinto English, according to some reliable probabilis-tic model of lexical translation into English, as thelexical choice most likely to be made by a learner ofthis native language.As we obviously do not have access to the wordin the native language of the writer, we approximatethis information by searching for the word that max-imizes the translation probability of translating backfrom the native language after translating from theoriginal English word.
This in fact corresponds to awidely used way of computing paraphrase probabili-ties from bilingual translation distributions (Bannardand Callison-Burch, 2005):e?l ?
argmaxe?fpl(f |e).pl(e|f)where f ranges over all possible translations of En-glish word e in a given native language l.Preferably, we would like to obtain candidatetranslations into the native language in context,that is, by translating complete sentences and us-ing a posteriori translation probabilities.
We couldnot do this for a number of reasons, the main onebeing that we did not have the possibility of usingor building Statistical Machine Translation systemsfor all the language pairs involving English and thenative languages of the shared task.
We thereforeresorted to simply finding, for each English word,the most likely back-translation into English via agiven native language.
Using the Google Transla-tion online Statistical Machine Translation service4,which proposed translations from and to English andall the native languages of the shared task, a furtherapproximation had to be made as, in practice, wewere only able to access the most likely translationsfor words in isolation: we considered only the besttranslation of the original English word in the nativelanguage, and then kept its best back-translation intoEnglish.
We here note some common intuitions withthe use of roundtrip translation as a Machine Trans-lation evaluation metrics (Rapp, 2009).4http://translate.google.comTable 1 provides various examples of back-translations for English adjectives obtained via eachnative language.
The samples from the Table showthat our procedure produces a significant number ofnon identical back-translations.
They also illustratesome types of undesirable results obtained, whichled us to only consider as features for our classi-fier the proportion of words in essays for whichthe above-defined back-translation yielded the sameword, considering all possible native languages.
Weonly considered content words, as out-of-contextback-translation for function words would be too un-reliable.
Table 2 shows values for some documentsof the training set.
As can be seen, there are impor-tant differences across languages, some languagesobtaining high scores on average (e.g.
French andJapanese) and others obtaining low scores on aver-age (e.g.
Korean, Turkish).
Furthermore, the high-est score is only rarely obtained for the actual nativelanguage of each document, showing that keepingthe most probable language according to this valuealone would not allow to obtain a good classificationperformance.4 Experiments4.1 Results per set of featuresFor all our experiments reported here, we used thefull training data provided using cross-validation totune the regularization parameter.
Our results arepresented in the top part of Table 3.
Using our com-plete set of features yields our best performance onaccuracy, corresponding to a 0.75% absolute im-provement over using our basic n-gram featuresonly.
No type of features allows a significant im-provement over the n-gram features when added in-dividually.4.2 Two-step classificationTable 4 contains the confusion matrix for our systemacross languages.
It clearly stands out that two lan-guage pairs were harder to distinguish: Hindi (hin)and Telugu (tel) on the one hand, and Korean (kor)and Japanese (jpn) on the other.In order to improve the performance of our model,we performed a two-step classification focused onthese difficult pairs.
For this, we built additionalclassifiers for each difficult pairs.
Both are built262eng abrupt affirmative amazing ambiguous anarchic atrocious attentive awkwardara sudden positive amazing mysterious messy terrible heedful inappropriatechi sudden sure amazing ambiguous anarchic atrocious careful awkwardfre sudden affirmative amazing ambiguous anarchic atrocious careful awkwardger abrupt affirmative incredible ambiguous anarchical gruesome attentively awkwardhin suddenly positive amazing vague chaotic brutal observant clumsyita abrupt affirmative amazing ambiguous anarchist atrocious careful uncomfortablejap sudden positive surprising ambiguous anarchy heinous cautious awkwardkor fortuitous positive amazing ambiguous anarchic severe kind awkwardspa abrupt affirmative surprising ambiguous anarchic atrocious attentive clumsytel abrupt affirmative amazing ambiguous anarchic formidable attentive awkwardtur sudden positive amazing uncertain anarchic brutal attentive strangeTable 1: Examples of back translations for English adjectives from the training set via each of the eleven nativelanguages of the shared task.
Back-translations that differ from the original word are indicated using a bold face.Doc id.
Native l. ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR976 ARA 0.80 0.88 0.91 0.95 0.75 0.91 0.87 0.73 0.89 0.79 0.7129905 CHI 0.84 0.81 0.93 0.87 0.79 0.89 0.89 0.56 0.93 0.62 0.7561765 FRE 0.73 0.84 0.90 0.71 0.73 0.83 0.86 0.50 0.91 0.58 0.66100416 GER 0.78 0.80 0.86 0.83 0.72 0.89 0.86 0.70 0.90 0.67 0.6726649 HIN 0.68 0.75 0.88 0.89 0.67 0.85 0.86 0.69 0.86 0.75 0.7739189 ITA 0.68 0.85 0.92 0.94 0.74 0.93 0.89 0.69 0.92 0.72 0.723044 JPN 0.83 0.81 0.89 0.83 0.68 0.94 0.91 0.71 0.94 0.83 0.703150 KOR 0.75 0.86 0.91 0.84 0.76 0.88 0.87 0.55 0.88 0.67 0.736614 SPA 0.79 0.90 0.86 0.85 0.78 0.85 0.92 0.67 0.90 0.70 0.6812600 TEL 0.65 0.74 0.84 0.73 0.71 0.92 0.90 0.76 0.95 0.82 0.585565 TUR 0.70 0.77 0.88 0.78 0.70 0.84 0.86 0.72 0.84 0.74 0.71Table 2: Values corresponding to the proportion of content words in a random essay for each native language for whichback-translation yielded the same word.FRE GER ITA SPA TUR ARA HIN TEL KOR JPN CHIFRE 79 4 4 3 2 3 0 0 2 2 1GER 0 89 2 4 1 0 1 0 2 1 0ITA 6 1 83 6 1 1 0 0 0 1 1SPA 4 4 5 72 2 3 3 2 1 1 3TUR 3 2 1 3 81 1 3 2 0 3 1ARA 3 0 1 3 3 81 5 2 1 0 1HIN 1 1 1 3 2 1 64 26 1 0 0TEL 0 0 1 0 0 1 17 81 0 0 0KOR 1 1 0 0 3 1 0 0 80 12 2JPN 1 0 2 2 0 3 0 1 13 73 5CHI 0 1 0 0 2 2 0 2 3 3 87Table 4: Confusion matrix on the Test set.263Features X-Val Testngm 74.83% 75.27%ngm+ort 74.98% 75.29%ngm+grm 75.18% 75.63%ngm+lex 74.85% 75.47%all 75.57% 75.81%2-step (a) 75.46% 75.69%2-step (b) 75.89% 75.98%Table 3: Accuracy results obtained by cross-validationand using the provided Test set for various combina-tions of features and our two 2-step strategies.
The fea-ture sets are: character and part-of-speech n-grams fea-tures (ngm), spelling features (ort), grammatical features(grm), and lexical preference features (lex).from the same feature sets as for the first-step modelbut with only three labels: one for each language ofthe pair and one for any other language.The training data used for these new models in-clude all documents from both languages as well asdocument misclassified as one of them by the first-step classifier (using cross-validation to label the fulltraining set).
The formers keep their original labelswhile the later are relabeled as other.Document classified in one of the difficult pairsby the first-step classifier were post-processed withthese new models.
When the new label predicted isother, the second best choice of the first step is used.We investigated two setups for the first classifier:(a) using the original 11 native languages classi-fier, and (b) using a new classifier with languagesof the difficult pairs merged, resulting in 9 native?languages?.Our results, shown in Figure 3 for easy com-parison, improve over our system using all fea-tures only when the first-pass classifier uses the setof 9 merged pseudo-languages (b).
We obtain amoderate 0.32% absolute improvement in accuracyover one-step classification on cross-validation, and0.17% improvement on the Test set.5 Discussion and conclusionWe have submitted on maximum entropy system tothe shared task on Native Language Identification,for which our basic set of n-gram features alreadyobtained a level of performance, around 75% in ac-curacy, close to the best performance reported in oursubmission.
The additional feature sets that we haveincluded in our system, while improving the model,did not allow us to capture a deeper influence of thenative language.A first analysis reveals that the model fails to fullyuse the additional feature sets due to lack of context.Future experiments will need to link more closelythese features to the documents for which they pro-vide useful information.Due to time constraints and engineering issues,the two-pass system was not ready by the time ofsubmission.
The results that we have included inthis report show that it is a promising approach thatwe should continue to explore.
We also plan to con-duct experiments that exploit the information aboutthe level of English available in the essays, some-thing that we did not consider for this submission.While this information is not directly available, itmay be infered from the data as a first-step classifi-cation.
We believe that studying its influence on themistakes make learners of different native languageis a promising direction.The approach that we have described in this sub-mission, as most of previously published approachesfor this task, attempts to find mistakes in the text ofthe documents.
The most typical mistakes are thenused by the classifier to detect the native language.This does not take into consideration the fact that na-tive English writers also make errors.
It would be in-teresting to explore the divergence between varioussets of writers/learners, not from the mean of non-native writers, but from the mean of native writers.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL?05), pages 597?604,Ann Arbor, Michigan.Adam Berger, Stephen Della Pietra, and VincentDella Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Linguis-tics, 22(1), March.Daniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: ACorpus of Non-Native English.
Technical report, Ed-ucational Testing Service.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2642010.
Practical very large scale CRFs.
In Proceed-ings the 48th Annual Meeting of the Association forComputational Linguistics (ACL), pages 504?513.
As-sociation for Computational Linguistics, July.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: The Penn treebank.
ComputationalLinguistics, 19(2):313?330.Marcin Milkowski.
2010.
Developing an open-source,rule-based proofreading tool.
Software - Practice andExperience, 40(7):543?566.Ryo Nagata.
2013.
Generating a language family treefrom indo-european non-native english texts (to ap-pear).
In Proceedings the 51th Annual Meeting of theAssociation for Computational Linguistics (ACL).
As-sociation for Computational Linguistics.Reinhard Rapp.
2009.
The backtranslation score: Auto-matic mt evalution at the sentence level without refer-ence translations.
In Proceedings of the ACL-IJCNLP2009 Conference Short Papers, pages 133?136, Sun-tec, Singapore.Qinfeng Shi, James Petterson, Gideon Dror, John Lang-ford, Alex Smola, and S.V.N.
Vishwanathan.
2009.Hash kernels for structured data.
Journal of MachineLearning Research, 10:2615?2637, December.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A Report on the First Native Language IdentificationShared Task.
In Proceedings of the Eighth Workshopon Building Educational Applications Using NLP, At-lanta, GA, USA, June.
Association for ComputationalLinguistics.265
