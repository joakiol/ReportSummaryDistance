Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1414?1422, Dublin, Ireland, August 23-29 2014.Modeling Newswire Events using Neural Networks for Anomaly DetectionPradeep DasigiLanguage Technologies Institute5000 Forbes AvenuePittsburgh, PA 15213USApdasigi@cs.cmu.eduEduard HovyLanguage Technologies Institute5000 Forbes AvenuePittsburgh, PA 15213USAhovy@cmu.eduAbstractAutomatically identifying anomalous newswire events is a hard problem.
We discuss the com-plexity of the problem and introduce a novel technique to model events based on recursive neuralnetworks to represent events as composition of their semantic arguments.
Our model learns todifferentiate between normal and anomalous events.
We model anomaly detection as a binaryclassification problem and show that the model learns useful features to classify anomaly.
Weuse headlines from the weird news category publicly available on newswire websites to extractanomalous training examples and those from Gigaword as normal examples.
We evaluate theclassifier on human annotated data and obtain an accuracy of 65.44%.
We also show that ourmodel is at least as competent as the least competent human annotator in anomaly detection.1 IntroductionUnderstanding events is a fundamental prerequisite for deeper semantic analysis of language.
We intro-duce the problem of automatic anomalous event detection in this paper and propose a novel event modelthat can learn to differentiate between normal and anomalous events.
We generally define anomalousevents as those that are unusual compared to the general state of affairs and might invoke surprise whenreported.
For example, given the event mention in the following sentenceMan recovering after being shot by his dog.one might think it is strange because dogs are not expected to shoot men.
But the mentionsMan recovering after being shot by cops.Man recovering after being bitten by a dog.are not as unusual as the previous one.
While all three sentences are equally valid syntactically, and itis not unclear what any of them means, it is our knowledge about the role fillers ?both individuallyand specifically in combination?
that enables us to differentiate between normal and anomalous events.Hence we hypothesize that anomaly is a result of unexpected or unusual combination of semantic rolefillers.
Given this idea, an automatic anomaly detection algorithm has to encode the goodness of semanticrole filler coherence.It has to be noted that event level anomaly is not the same as semantic incoherence.
An event con-structed by randomly choosing words to form each of the semantic arguments is not anomalous since wecannot argue whether the event is normal or anomalous when it is unclear what the event means.
Hence,we define anomalous events to be the sub class of those that are semantically coherent, but are unusualonly based on real world knowledge.Automatic anomalous event detection is a hard problem since determining what a good combinationof role fillers requires deep semantic and pragmatic knowledge.
Moreover, manual judgment of anomalyThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1414itself may be difficult and people often may not agree with each other in this regard.
We describe thedifficulty in human judgment in greater detail in Section 4.4.
Automatic detection of anomaly requiresencoding complex information, which has to be composed from the semantics of the individual wordsin the sentence.
A fundamental problem in doing so is the sparsity in semantic space due to the discreterepresentations of meaning of words.In this paper, we describe an attempt to model newswire events as a composition of the predicate withits semantic arguments.
Our approach is based on the recent models used for semantic composition usingrecursive neural networks (RNN).
It has been previously shown by Socher et al.
(2010) and Socher etal.
(2013b) among others that RNN can effectively deal with sparsity in semantic space by represent-ing meaning at a higher level of abstraction than the surface forms of words, and thus being able tolearn more general patterns.
These models are very relevant to modeling event semantics because thesparsity problem ranges from polysemy and synonymy at the lexical semantic level to entity and eventco-reference at the discourse level.2 Background2.1 Selectional Preference and Thematic FitSelectional preference, a notion introduced by Wilks (1973), refers to the phenomenon of the predicateand the fillers of its arguments affecting the likelihood of fillers of other arguments.
Thus the idea is thatpredicate and the role fillers ?prefer?
some fillers for other roles.
For example, given that the predicate iswrites, the agent author prefers the patient book, while the agent programmer prefers the patient code.This idea is used by Elman (2009), and is very similar to the role-filler composition that we use foranomaly detection.Erk et al.
(2010) also model selectional preferences using vector spaces.
They measure the goodnessof the fit of a noun with a verb in terms of the similarity between the vector of the noun and some?exemplar?
nouns taken by the verb in the same argument role.
Baroni and Lenci (2010) also measureselectional preference similarly, but instead of exemplar nouns, they calculate a prototype vector for thatrole based on the vectors of the most common nouns occurring in that role for the given verb.
Lenci(2011) builds on this work and models the phenomenon that the expectations of the verb or its role-fillerschange dynamically given other role fillers.2.2 Recursive Neural NetworksRecursive Neural Networks (RNN), first introduced by Goller and Kuchler (1996), are multilayer neuralnetwork models used for efficient processing of structured objects of arbitrary shape.
These have beensuccessfully used for modeling semantics of sentences of arbitrary length by Socher et al.
(2010), forsentiment analysis by Socher et al.
(2013b), for syntactic parsing by Socher et al.
(2013a) and for learn-ing morphologically aware word representations by Luong et al.
(2013).
RNN are attractive because theycan encode compositions of meaning guided by syntax or some other linguistic structure known a priori.Moreover, they provide flexibility in terms of learning composition weights based on supervised or un-supervised objectives.
Consequently RNN learn feature representations depending on the task.
Hence,this is a good choice for modeling event composition.In its simplest form, an RNN processes information backed by a Directed Acyclic Graph (DAG),where each node represents a neural network with the same parameters.
The output produced at eachintermediate step of encoding usually has the same dimensionality as each of the inputs, hence RNNprojects the representation of a structure of arbitrary length into the same space as the inputs.
Thisproperty is what makes RNN recursive.
An example RNN with a binary DAG (tree) structure is shownin Figure 1.
The activation from each neural network node isc = g(y1?y2) = Sg(W (y1?y2) + b)where ?
represents concatenation of vector representations of the inputs, y1, y2?
Rn?1are the inputs,W ?
Rn?2nis the composition weight matrix and b ?
Rn?1is the bias.
Sg is a element wise sigmoid1415Figure 1: Example of a Recursive Neural Network backed by a binary treeFigure 2: Example of an event treefunction.
Apart from encoding the composition, RNN also produce a score of compositions = S?cwhere S ?
Rn?1is a scoring operator and s is a score that shows how good the composition is.
(Col-lobert et al., 2011) take an unsupervised approach to training RNN for semantic composition based onthe contrastive estimation technique proposed by (Smith and Eisner, 2005) and assuming that any wordand its context is a positive example and a random word in the same context is a negative training ex-ample.
(Socher et al., 2013b) among others use a supervised objective that is based on the label error atthe topmost node in the RNN.
The parameters of the simplest model are W , b and S. For representationlearning, the inputs xiare also made parameters.
Goller and Kuchler (1996) propose Backpropagationthrough structure (BPTS), that respects the underlying DAG structure during backpropagation of gradi-ents.3 Neural Event ModelWe define an event as the pair (V,A), where V is the predicate or a semantic verb1, and A is the set of itssemantic arguments like agent, patient, time, location, so on.
Our aim is to obtain a vector representationof the event that is composed from representations of individual words, while explicitly guided by thesemantic role structure.
This representation can be understood as an embedding of the event in an eventspace.Neural Event Model (NEM) is a kind of RNN that is guided by a tree representation of events like theone shown in Figure 2.
The edges connected to the root of the tree correspond to the predicate and itssemantic roles (arguments).
All the other edges form binary sub-trees of arguments.
NEM is a super-vised model that learns to differentiate between anomalous and normal events by classifying the eventembeddings.
The inputs to NEM are the semantic arguments, and the representations of words in eachargument.
We recursively compose the words in each argument to obtain argument level representations,which are then composed to obtain an event embedding.Intra-argument composition (called argument composition henceforth) is unsupervised, and we usecontrastive estimation to learn the parameters.
The structure of the binary tree backing argument compo-sition is determined dynamically, composing at each stage the two nodes which give the best composition1By semantic verb, we mean an action word whose syntactic category is not necessarily a verb.
For example, in Terroristattacks on the World Trade Center.., attacks is not a verb but is still an action word.1416Figure 3: Neural Event Model: Encodingscore.
Inter-argument composition (called event composition henceforth) is supervised and we use labelerror to learn the parameters.
Figure 3 shows how NEM encodes the event shown in Figure 2.
The blueboxes show argument composition and the red box shows event composition.3.1 TrainingNEM is trained in two phases.
The first, argument composition, is unsupervised while the second, eventcomposition, is supervised.3.1.1 Argument CompositionAn argument composition node takes inputs of dimensionality 2n and produces an composed outputrepresentation of dimensionality n and a composition score.
Accordingly, we define the node in termsof the parameters ?arg= {Warg?
Rn?2n; barg, Sarg?
Rn?1;V } where Warg, bargand Sargare thecomposition weight, bias and the scoring operators respectively as described previously, and V is the setof representations of all the words in the vocabulary.
All nodes performing argument composition usethe same parameters.
Training is done in contrastive estimation fashion and the objective isargmin?argJarg= argmin?argmax(0, 1?
s+ sc)where s is the score of the composition of the entire argument produced by the root node of the argument,and scis the score produced by randomly replacing one of the words in the argument at a time.
Thestructure of the binary tree backing each argument is determined dynamically.
This is done by startingwith leaf nodes in the tree for each of the words in the argument, comparing the composition scores ofevery pair of adjacent leaf nodes, and actually composing the pair that gives the highest score, whichgives a new node.
The process is repeated until we build a complete binary tree for each argument.3.1.2 Event CompositionEvent composition takes argument representations and produces the event representation and label in-dicating whether the event is normal or anomalous.
We define the event composition node in terms ofthe parameters ?event= {Wevent?
Rn?kn; bevent, Levent?
Rn?1} where k is the number of semanticarguments per event.
Leventis the label operator.
The objective of this phase isargmin?eventJevent= argmin?event(?l log h(e) + (1?
l) log(1?
h(e)))where l is the reference binary label indicating whether the event is normal or anomalous, e is the eventrepresentation and h(e) is the output of the logistic function.
Concretely,h(e) =11 + e?L?eventeWe implement the functions and perform stochastic gradient descent using Theano (Bergstra et al., 2010).14174 Experiments4.1 Event ExtractionWe extract events by running the Semantic Role Labeling (SRL) tool in SENNA (Collobert et al., 2011).SENNA uses PropBank (Palmer et al., 2005) style semantic tags.
We consider only the roles A0, A1,AM-TMP and AM-LOC as the arguments of our events2.
For example, the event in the tree shown inFigure 2 is extracted from the sentenceTwo Israeli helicopters killed 70 soldiers in Gaza strip.and SENNA identifies the following as the semantic rolesverb:killed A0:Two Israeli helicopters A1:70 soldiers AM-LOC:in Gaza strip4.2 DataSince the second phase of training NEM is supervised, we need newswire events that are normal andthose that are anomalous.
We crawl 3684 ?weird news?
headlines available publicly on the website ofNBC news3, such as the following:?
India weaponizes world?s hottest chili.?
Man recovering after being shot by his dog.?
Thai snake charmer puckers up to 19 cobras.We assume that the events extracted from this source, called NBC Weird Events (NWE) henceforth, areanomalous for training.
NWE contains 4271 events extracted using SENNA?s SRL.
We use 3771 of thoseevents as our negative training data, and the remaining for testing.
Similarly, we extract events also fromheadlines in the AFE section of Gigaword, called Gigaword Events (GWE) henceforth.
We assume theseevents are normal.
To use as positive examples for training event composition, we sample roughly thesame number of events from GWE as our negative examples from NWE.
It has to be noted that eachheadline may contain multiple events and some may not contain events at all.For argument composition, we use about 100k whole sentences from AFE headlines and the weirdnews headlines from which NWE are extracted.
Since we are training argument composition, we do notuse the event structure in the first phase.
It has to be noted that all our training data are easily availableand do not require any human annotation.We test the performance of NEM on 1003 events which are not part of the training dataset.
Theseevents are sampled with equal probabilities from NWE and GWE and are human annotated for anomaly.Section 4.4 has details of the annotation task.4.3 Word Vector InitializationWe initialize the vector representations of the words in our vocabulary using the embeddings available inSENNA 3.0 (Collobert et al., 2011) if available, and randomly if not.
For event composition, if the eventdoes not have a specific role filler, we input a zero vector for the role.4.4 AnnotationWe post the annotation of the test set containing 1003 events as Human Intelligence Tasks (HIT) onAmazon Mechanical Turk (AMT).
We break the task into 20 HITs and ask the workers to select oneof the four options - highly unusual, strange, normal and cannot say for each event.
We ask them toselect highly unusual when the event seems too strange to be true, strange if it seems unusual but stillplausible, and cannot say only if the information present in the event is not sufficient to make a decision.We present each event along with the original headline and the semantic arguments.
Along with marking2These four types cover about 85% of all arguments in our training and test datasets.3http://www.nbcnews.com/html/msnbc/3027113/3032524/4429950/4429950_1.html1418Total number of annotators 22Normal annotations 56.3%Strange annotations 28.6%Highly unusual annotations 10.3%Cannot Say annotations 4.8%Avg.
events annotated per worker 3444-way Inter annotator agreement (?)
0.343-way Inter annotator agreement (?)
0.56Table 1: Annotation Statisticsone of the four options above, if an event is strange or highly unusual, we ask the annotators to select theparts of the headline that make it so.
Since there can be multiple events in the headline, the annotatorsdecision regarding the parts of the sentence that cause anomaly help us identify which particular event inthe headline is anomalous.Table 1 shows some statistics of the annotation task.
We compute the Inter Annotator Agreement(IAA) in terms of Kripendorff?s alpha (Krippendorff, 1980).
The advantage of using this measure insteadof the more popular Kappa is that the former can deal with missing information, which is the case withour task since annotators work on different overlapping subsets of the test set.
The 4-way IAA shownin the table corresponds to agreement over the original 4-way decision (including cannot say) while the3-way IAA is measured after merging the highly unusual and strange decisions.Additionally we use MACE (Hovy et al., 2013) to assess the quality of annotation.
MACE models theannotation task as a generative process of producing the observed labels conditioned on the true labelsand the competence of the annotators, and predicts both the latent variables.
The average of competenceof annotators, a value that ranges from 0 to 1, for our task is 0.49 for the 4-way decision and 0.59 for the3-way decision.We generate true label predictions produced by MACE, discard the events for which the predictionremains to be cannot say, and use the rest as reference for evaluating NEM, which is described in Sec-tion 4.5.
This leaves 949 events as our reference dataset, of which only 41% of the labels are strange orhighly unusual.
It has to be noted that even though our test set has equal size samples from both NWEand GWE, the true distribution is not uniform.Language Model Separability Given the annotations, we test to see if the sentences correspondingto anomalous events can be separated from normal events by simpler features.
We build a n-gram lan-guage model from the training data set used for argument composition and measure the perplexity ofthe sentences in the test set.
Figure 4 shows a comparison of the perplexity scores for different labels.If the n-gram features are enough to separate different classes of sentences, one would expect the sen-tences corresponding to strange and highly unusual labels to have higher perplexity ranges than normalsentences, because the language model is built from a dataset that is expected to have a distribution ofsentences where majority of them contain normal events.
As it can be seen in Figure 4, except for a fewoutliers, most data points in all the categories are in similar perplexity ranges.
Hence, sentences withdifferent labels cannot be separated based on an n-gram language model features.4.5 EvaluationWe evaluate the performance of event composition by comparing the predicted labels from the classifieragainst the ones given by MACE.
We merge the two anomaly classes and calculate accuracy of the binaryclassifier, and the precision and recall of anomaly detection.Baseline We compare the performance of our model against a baseline that is based on how wellthe semantic arguments in the event match the selectional preferences of the predicate.
We measureselectional preference using Point-wise Mutual Information (PMI) (Church and Hanks, 1990) of the headwords of each semantic argument with the predicate.
The baseline model is built as follows.
We perform1419Figure 4: Comparison of perplexity scores for different labelsNEM BaselineAccuracy 65.44% 45.22%AnomalousPrecision 56.55% 36.30%Recall 48.22% 59.50%NormalPrecision 64.62% 42.08%Recall 77.66% 33.60 %Table 2: Classification Performance and Comparison with Baselinedependency parsing using MaltParser (Nivre et al., 2007) on the sentences in the training data used inthe first phase of training to obtain the head words of the semantic arguments.
We then calculate the PMIvalues of all the pairs < hA, p > where h is the head word of argument A and p is the predicate of theevent.
For training our baseline classifier, we use the labeled training data from the event compositionphase.
The features to this classifier are the PMI measures of the < hA, p > pairs estimated from thelarger dataset.
The classifier thus trained to distinguish between anomalous and normal events is appliedto the test set.Table 2 shows the results and a comparison with the PMI based baseline.
The accuracy of the baselineclassifier is lower than 50%, which is the expected accuracy of a classifier that assigns labels randomly.The precision of that random classifier in predicting anomalous events is expected to be 41%, since that isthe percentage of anomaly labels in our reference set as described in Section 4.4.
The accuracy of NEMis higher than the baseline model.
One possible reason for the PMI based baseline having higher recallin predicting anomaly and lower precision is that the statistics estimated from larger training data cannotbe generalized to the test set due to sparsity issues.
This indicates the advantage of using continuousrepresentations at a higher level of abstraction as features for classification.To further compare NEM with human annotators, we give to MACE, the binary labels produced byNEM along with the annotations and measure the competence.
For the sake of comparison, we alsogive to MACE, a list of random binary labels as one of the annotations to measure the competence of ahypothetical worker that made random choices.
These results are reported in Table 3.
It can be seen thatthe performance of NEM is comparable at least to the least competent human.5 Discussion and Future WorkThe two evaluation experiments show that the neural network does learn to distinguish between normaland anomalous events.
Future improvements to this model will include better event extraction techniques.Since the current approach is supervised, the training data size for learning event composition is lim-ited.
We plan to develop unsupervised approaches that can learn good models of normal events, anddetect anomalies based on how well new events fit in the model.
One possible approach is to do learning1420Human average 0.59Human highest 0.70Human lowest 0.26Random 0.02NEM 0.26Table 3: Anomaly Detection Competencebased on contrastive estimation in the second phase as well.
The assumption behind taking this approachfor learning is that a randomly generated data point is likely to be a negative example, which is not neces-sarily true for learning event composition.
Generating malformed events that are syntactically valid butanomalous without much human effort can greatly help in developing such an unsupervised algorithm.One important aspect of anomaly that is currently not handled by NEM is the level of generality of theconcepts the events contain.
Usually more general concepts cause events to be more normal since theyconvey lesser information.
For example, an American soldier shooting another American soldier may beconsidered unusual, while a soldier shooting another soldier may not be as unusual, and at the highestlevel of generalization, a person shooting another person is normal.
This information of generality hasto be incorporated into the event model.
This can be achieved by integrating real world knowledgefrom knowledge bases like Wordnet (Miller, 1995) or from corpus statistics like the work by Lin (1998)into the event model.
Bordes et al.
(2011) learn continuous representations of entities and relations inknowledge bases.
More recently, an alternative approach for doing the same was proposed by Chen etal.
(2013).
These representations can greatly help modeling events.Finally, the idea of modeling event composition can help processing event data in general and can beapplied to other tasks like finding co-referent events.6 ConclusionWe introduced the problem of anomalous newswire event detection and illustrated its difficulty.
Ourapproach is similar to the ones successfully used for modeling semantic composition.
We showed thatwhile our event composition model does learn to distinguish between normal and anomalous events,there is scope for improved models that can effectively incorporate real world information and can betrained in an unsupervised fashion.
We note that in general event composition is more difficult thantraditional semantic composition since the former also deals with pragmatics.
Consequently the set ofnonsensical events is different from the set of anomalous sentences, and while meaningless events andwell composed normal events are two ends of the semantic spectrum, semantically valid anomalousevents lie somewhere between them.AcknowledgementsThis research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT pro-gram.ReferencesMarco Baroni and Alessandro Lenci.
2010.
Distributional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?721.James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins,Joseph Turian, David Warde-Farley, and Yoshua Bengio.
2010.
Theano: a cpu and gpu math expressioncompiler.
In Proceedings of the Python for Scientific Computing Conference (SciPy), volume 4.Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, et al.
2011.
Learning structured embeddings ofknowledge bases.
In AAAI.1421Danqi Chen, Richard Socher, Christopher D Manning, and Andrew Y Ng.
2013.
Learning new facts from knowl-edge bases with neural tensor networks and semantic word vectors.
arXiv preprint arXiv:1301.3618.Kenneth Ward Church and Patrick Hanks.
1990.
Word association norms, mutual information, and lexicography.Computational linguistics, 16(1):22?29.Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493?2537.Jeffrey L Elman.
2009.
On the meaning of words and dinosaur bones: Lexical knowledge without a lexicon.Cognitive science, 33(4):547?582.Katrin Erk, Sebastian Pad?o, and Ulrike Pad?o.
2010.
A flexible, corpus-driven model of regular and inverseselectional preferences.
Computational Linguistics, 36(4):723?763.Christoph Goller and Andreas Kuchler.
1996.
Learning task-dependent distributed representations by backprop-agation through structure.
In Neural Networks, 1996., IEEE International Conference on, volume 1, pages347?352.
IEEE.Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy.
2013.
Learning whom to trust withmace.
In Proceedings of NAACL-HLT, pages 1120?1130.Klaus Krippendorff.
1980.
Content analysis: An introduction to its methodology.
Sage Publications (BeverlyHills).Alessandro Lenci.
2011.
Composing and updating verb argument expectations: A distributional semantic model.In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 58?66.
As-sociation for Computational Linguistics.Dekang Lin.
1998.
Automatic retrieval and clustering of similar words.
In Proceedings of the 17th internationalconference on Computational linguistics-Volume 2, pages 768?774.
Association for Computational Linguistics.Minh-Thang Luong, Richard Socher, and Christopher D Manning.
2013.
Better word representations with recur-sive neural networks for morphology.
CoNLL-2013, 104.George A Miller.
1995.
Wordnet: a lexical database for english.
Communications of the ACM, 38(11):39?41.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov, andErwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency parsing.
NaturalLanguage Engineering, 13(2):95?135.Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005.
The proposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.Noah A Smith and Jason Eisner.
2005.
Contrastive estimation: Training log-linear models on unlabeled data.In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 354?362.Association for Computational Linguistics.Richard Socher, Christopher D Manning, and Andrew Y Ng.
2010.
Learning continuous phrase representationsand syntactic parsing with recursive neural networks.
In Proceedings of the NIPS-2010 Deep Learning andUnsupervised Feature Learning Workshop, pages 1?9.Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng.
2013a.
Parsing with compositionalvector grammars.
In In Proceedings of the ACL conference.
Citeseer.Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christo-pher Potts.
2013b.
Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceed-ings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631?1642.Yorick Wilks.
1973.
Preference semantics.
Technical report, DTIC Document.1422
