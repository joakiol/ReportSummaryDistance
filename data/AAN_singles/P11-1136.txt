Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1356?1364,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUnsupervised Decomposition of a Document into Authorial ComponentsMoshe Koppel      Navot Akiva Idan Dershowitz Nachum DershowitzDept.
of Computer Science  Dept.
of Bible School of Computer ScienceBar-Ilan University Hebrew University Tel Aviv UniversityRamat Gan, Israel Jerusalem, Israel Ramat Aviv, Israel{moishk,navot.akiva}@gmail.com dershowitz@gmail.com nachumd@tau.ac.ilAbstractWe propose a novel unsupervised methodfor separating out distinct authorial compo-nents of a document.
In particular, we showthat, given a book artificially ?munged?from two thematically similar biblicalbooks, we can separate out the two consti-tuent books almost perfectly.
This allowsus to automatically recapitulate many con-clusions reached by Bible scholars overcenturies of research.
One of the key ele-ments of our method is exploitation of dif-ferences in synonym choice by differentauthors.1 IntroductionWe propose a novel unsupervised method forseparating out distinct authorial components of adocument.There are many instances in which one is facedwith a multi-author document and wishes to deli-neate the contributions of each author.
Perhaps themost salient example is that of documents of his-torical significance that appear to be composites ofmultiple earlier texts.
The challenge for literaryscholars is to tease apart the document?s variouscomponents.
More contemporary examples includeanalysis of collaborative online works in whichone might wish to identify the contribution of aparticular author for commercial or forensic pur-poses.We treat two versions of the problem.
In thefirst, easier, version, the document to be decom-posed is given to us segmented into units, each ofwhich is the work of a single author.
The challengeis only to cluster the units according to author.
Inthe second version, we are given an unsegmenteddocument and the challenge includes segmentingthe document as well as clustering the resultingunits.We assume here that no information about theauthors of the document is available and that inparticular we are not supplied with any identifiedsamples of any author?s writing.
Thus, our me-thods must be entirely unsupervised.There is surprisingly little literature on thisproblem, despite its importance.
Some work in thisdirection has been done on intrinsic plagiarism de-tection (e.g., Meyer zu Eisen 2006) and documentoutlier detection (e.g., Guthrie et al 2008), but thiswork makes the simplifying assumption that thereis a single dominant author, so that outlier unitscan be identified as those that deviate from thedocument as a whole.
We don?t make this simpli-fying assumption.
Some work on a problem that ismore similar to ours was done by Graham et al(2005).
However, they assume that examples ofpairs of paragraphs labeled as same-author/different-author are available for use as thebasis of supervised learning.
We make no suchassumption.The obvious approach to our unsupervised ver-sion of the problem would be to segment the text(if necessary), represent each of the resulting unitsof text as a bag-of-words, and then use clusteringalgorithms to find natural clusters.
We will see,however, that this na?ve method is quite inade-quate.
Instead, we exploit a method favored by theliterary scholar, namely, the use of synonymchoice.
Synonym choice proves to be far more use-ful for authorial decomposition than ordinary lexi-cal features.
However, synonyms are relatively1356sparse and hence, though reliable, they are notcomprehensive; that is, they are useful for separat-ing out some units but not all.
Thus, we use a two-stage process: first find a reliable partial clusteringbased on synonym usage and then use these as thebasis for supervised learning using a different fea-ture set, such as bag-of-words.We use biblical books as our testbed.
We dothis for two reasons.
First, this testbed is well mo-tivated, since scholars have been doing authorialanalysis of biblical literature for centuries.
Second,precisely because it is of great interest, the Biblehas been manually tagged in a variety of ways thatare extremely useful for our method.Our main result is that given artificial booksconstructed by randomly ?munging?
together ac-tual biblical books, we are able to separate out au-thorial components with extremely high accuracy,even when the components are thematically simi-lar.
Moreover, our automated methods recapitulatemany of the results of extensive manual research inauthorial analysis of biblical literature.The structure of the paper is as follows.
In thenext section, we briefly review essential informa-tion regarding our biblical testbed.
In Section 3, weintroduce a na?ve method for separating compo-nents and demonstrate its inadequacy.
In Section 4,we introduce the synonym method, in Section 5 weextend it to the two-stage method, and in Section 6,we offer systematic empirical results to validatethe method.
In Section 7, we extend our method tohandle documents that have not been pre-segmented and present more empirical results.
InSection 8, we suggest conclusions, including someimplications for Bible scholarship.2 The Bible as TestbedWhile the biblical canon differs across religionsand denominations, the common denominator con-sists of twenty-odd books and several shorterworks, ranging in length from tens to thousands ofverses.
These works vary significantly in genre,and include historical narrative, law, prophecy, andwisdom literature.
Some of these books are re-garded by scholars as largely the product of a sin-gle author?s work, while others are thought to becomposites in which multiple authors are well-represented ?
authors who in some cases lived inwidely disparate periods.
In this paper, we willfocus exclusively on the Hebrew books of the Bi-ble, and we will work with the original untran-slated texts.The first five books of the Bible, collectivelyknown as the Pentateuch, are the subject of muchcontroversy.
According to the predominant Jewishand Christian traditions, the five books were writ-ten by a single author ?
Moses.
Nevertheless, scho-lars have found in the Pentateuch what they believeare distinct narrative and stylistic threads corres-ponding to multiple authors.Until now, the work of analyzing compositetexts has been done in mostly impressionistic fa-shion, whereby each scholar attempts to detect thetelltale signs of multiple authorship and compila-tion.
Some work on biblical authorship problemswithin a computational framework has been at-tempted, but does not handle our problem.
Muchearlier work (for example, Radday 1970; Bee1971; Holmes 1994) uses multivariate analysis totest whether the clusters in a given clustering ofsome biblical text are sufficiently distinct to beregarded as probably a composite text.
By contrast,our aim is to find the optimal clustering of a docu-ment, given that it is composite.
Crucially, unlikethat earlier work, we empirically prove the efficacyof our methods by testing it against known groundtruth.
Other computational work on biblical au-thorship problems (Mealand 1995; Berryman et al2003) involves supervised learning problemswhere some disputed text is to be attributed to oneof a set of known authors.
The supervised author-ship attribution problem has been well-researched(for surveys, see Juola (2008), Koppel et al (2009)and Stamatatos (2009)), but it is quite distinct fromthe unsupervised problem we consider here.Since our problem has been dealt with almostexclusively using heuristic methods, the subjectivenature of such research has left much room for de-bate.
We propose to set this work on a firm algo-rithmic basis by identifying an optimal stylisticsubdivision of the text.
We do not concern our-selves with how or why such distinct threads exist.Those for whom it is a matter of faith that the Pen-tateuch is not a composition of multiple writers canview the distinction investigated here as that ofmultiple styles.3 A Na?ve AlgorithmFor expository purposes, we will use a canoni-cal example to motivate and illustrate each of a1357sequence of increasingly sophisticated algorithmsfor solving the decomposition problem.
Jeremiahand Ezekiel are two roughly contemporaneousbooks belonging to the same biblical sub-genre(prophetic works), and each is widely thought toconsist primarily of the work of a single distinctauthor.
Jeremiah consists of 52 chapters and Eze-kiel consists of 48 chapters.
For our first challenge,we are given all 100 unlabeled chapters and ourtask is to separate them out into the two constituentbooks.
(For simplicity, let?s assume that it isknown that there are exactly two natural clusters.
)Note that this is a pre-segmented version of theproblem since we know that each chapter belongsto only one of the books.As a first try, the basics of which will serve as afoundation for more sophisticated attempts, we dothe following:1.
Represent each chapter as a bag-of-words (us-ing all words that appear at least k times in thecorpus).2.
Compute the similarity of every pair of chaptersin the corpus.3.
Use a clustering algorithm to cluster the chap-ters into two clusters.We use k=2, cosine similarity and ncut cluster-ing (Dhillon et al 2004).
Comparing the Jeremiah-Ezekiel split to the clusters thus obtained, we havethe following matrix:Book Cluster I Cluster IIJerEze29282320As can be seen, the clusters are essentially or-thogonal to the Jeremiah-Ezekiel split.
Ideally,100% of the chapters would lie on the majoritydiagonal, but in fact only 51% do.
Formally, ourmeasure of correspondence between the desiredclustering and the actual one is computed by firstnormalizing rows and then computing the weightof the majority diagonal relative to the whole.
Thismeasure, which we call normalized majority di-agonal (NMD), runs from 50% (when the clustersare completely orthogonal to the desired split) to100% (where the clusters are identical with thedesired split).
NMD is equivalent to maximal ma-cro-averaged recall where the maximum is takenover the (two) possible assignments of books toclusters.
In this case, we obtain an NMD of 51.5%,barely above the theoretical minimum.This negative result is not especially surprisingsince there are many ways for the chapters to split(e.g., according to thematic elements, sub-genre,etc.)
and we can?t expect an unsupervised methodto read our minds.
Thus, to guide the method in thedirection of stylistic elements that might distin-guish between Jeremiah and Ezekiel, we define aclass of generic biblical words consisting of all 223words that appear at least five times in each of tendifferent books of the Bible.Repeating our experiment of above, though li-miting our feature set to generic biblical words, weobtain the following matrix:Book Cluster I Cluster IIJerEze32282020As can be seen, using generic words yieldsNMD of 51.3%, which does not improve matters atall.
Thus, we need to try a different approach.4 Exploiting Synonym UsageOne of the key features used by Bible scholarsto classify different components of biblical litera-ture is synonym choice.
The underlying hypothesisis that different authorial components are likely todiffer in the proportions with which alternativewords from a set of synonyms (synset) are used.This hypothesis played a part in the pioneeringwork of Astruc (1753) on the book of Genesis ?using a single synset: divine names ?
and has beenrefined by many others using broader feature sets,such as that of Carpenter and Hartford-Battersby(1900).
More recently, the synonym hypothesis hasbeen used in computational work on authorshipattribution of English texts in the work of Clarkand Hannon (2007) and Koppel et al (2006).This approach presents several technical chal-lenges.
First, ideally ?
in the absence of a suffi-ciently comprehensive thesaurus ?
we would wishto identify synonyms in an automated fashion.Second, we need to adapt our similarity measurefor reasons that will be made clear below.4.1 (Almost) Automatic Synset IdentificationOne of the advantages of using biblical litera-ture is the availability of a great deal of manualannotation.
In particular, we are able to identifysynsets by exploiting the availability of the stan-dard King James translation of the Bible into Eng-1358lish (KJV).
Conveniently, and unlike most moderntranslations, KJV almost invariably translates syn-onyms identically.
Thus, we can generally identifysynonyms by considering the translated version ofthe text.
There are two points we need to be preciseabout.
First, it is not actually words that we regardas synonymous, but rather word roots.
Second, tobe even more precise, it is not quite roots that aresynonymous, but rather senses of roots.
Conve-niently, Strong?s (1890 [2010]) Concordance listsevery occurrence of each sense of each root thatappears in the Bible separately (where senses aredistinguished in accordance with the KJV transla-tion).
Thus, we can exploit KJV and the concor-dance to automatically identify synsets as well asoccurrences of the respective synonyms in a syn-set.1 (The above notwithstanding, there is still aneed for a bit of manual intervention: due to poly-semy in English, false synsets are occasionallycreated when two non-synonymous Hebrew wordsare translated into two senses of the same Englishword.
Although this could probably be handledautomatically, we found it more convenient to do amanual pass over the raw synsets and eliminate theproblems.
)The above procedure yields a set of 529 synsetsincluding a total of 1595 individual synonyms.Most synsets consist of only two synonyms, butsome include many more.
For example, there are 7Hebrew synonyms corresponding to ?fear?.4.2 Adapting the Similarity MeasureLet?s now represent a unit of text as a vector inthe following way.
Each entry represents a syn-onym in one of the synsets.
If none of the syn-onyms in a synset appear in the unit, all their cor-responding entries are 0.
If j different synonyms ina synset appear in the unit, then each correspond-ing entry is 1/j and the rest are 0.
Thus, in the typi-cal case where exactly one of the synonyms in asynset appears, its corresponding entry in the vec-tor is 1 and the rest are 0.Now we wish to measure the similarity of twosuch vectors.
The usual cosine measure doesn?tcapture what we want for the following reason.
Ifthe two units use different members of a synset,cosine is diminished; if they use the same membersof a synset, cosine is increased.
So far, so good.But suppose one unit uses a particular synonym1Thanks to Avi Shmidman for his assistance with this.and the other doesn?t use any member of that syn-set.
This should teach us nothing about the similar-ity of the two units, since it reflects only on therelevance of the synset to the content of that unit; itsays nothing about which synonym is chosen whenthe synset is relevant.
Nevertheless, in this case,cosine would be diminished.The required adaptation is as follows: we firsteliminate from the representation any synsets thatdo not appear in both units (where a synset is saidto appear in a unit if any of its constituent syn-onyms appear in the unit).
We then compute cosineof the truncated vectors.
Formally, for a unit xrepresented in terms of synonyms, our new similar-ity measure is cos'(x,y) = cos(x|S(x ?y),y|S(x ?y)),where x|S(x ?y) is the projection of x onto the syn-sets that appear in both x and y.4.3  Clustering Jeremiah-Ezekiel Using Syn-onymsWe now apply ncut clustering to the similaritymatrix computed as described above.
We obtainthe following split:Book Cluster I Cluster IIJerEze485443Clearly, this is quite a bit better than results ob-tained using simple lexical features as describedabove.
Intuition for why this works can be pur-chased by considering concrete examples.
Thereare two Hebrew synonyms ?
p??
?h and miq?
?a?corresponding to the word ?corner?, two (min?
?hand t?r?m?h) corresponding to the word ?obla-tion?, and two (n??a?
and ??
?al) corresponding tothe word ?planted?.
We find that p??
?h, min?
?hand n??a?
tend to be located in the same units and,concomitantly, miq?
?a?, t?r?m?h and ??
?al are lo-cated in the same units.
Conveniently, the formerare all Jeremiah and the latter are all Ezekiel.While the above result is far better than thoseobtained using more na?ve feature sets, it is, never-theless, far from perfect.
We have, however, onemore trick at our disposal that will improve theseresults further.5 Combining Partial Clustering and Su-pervised LearningAnalysis of the above clustering results leads totwo observations.
First, some of the units belong1359firmly to one cluster or the other.
The rest have tobe assigned to one cluster or the other becausethat?s the nature of the clustering algorithm, but infact are not part of what we might think of as thecore of either cluster.
Informally, we say that a unitis in the core of its cluster if it is sufficiently simi-lar to the centroid of its cluster and it is sufficientlymore similar to the centroid of its cluster than toany other centroid.
Formally, let S be a set of syn-sets, let B be a set of units, and let C be a cluster-ing of B where the units in B are represented interms of the synsets in S. For a unit x in clusterC(x) with centroid c(x), we say that x is in the coreof C(x) if cos'(x,c(x))>?1 and cos'(x,c(x))-cos'(x,c)>?2for every centroid c?c(x).
In our experiments be-low, we use ?1=1/?2 (corresponding to an angle ofless than 45 degrees between x and the centroid ofits cluster) and ?2=0.1.Second, the clusters that we obtain are based ona subset of the full collection of synsets that doesthe heavy lifting.
Formally, we say that a synonymn in synset s is over-represented in cluster C ifp(x?C|n?x) > p(x?C|s?x) and p(x?C|n?x) > p(x?C).That is, n is over-represented in C if knowing thatn appears in a unit increases the likelihood that theunit is in C, relative to knowing only that somemember of n?s synset appears in the unit and rela-tive to knowing nothing.
We say that a synset s is aseparating synset for a clustering {C1,C2} if somesynonym in s is over-represented in C1 and a dif-ferent synonym in s is over-represented in C2.5.1 Defining the Core of a ClusterWe leverage these two observations to formallydefine the cores of the respective clusters using thefollowing iterative algorithm.1.
Initially, let S be the collection of all synsets, letB be the set of all units in the corpusrepresented in terms of S, and let {C1,C2} bean initial clustering of the units in B.2.
Reduce B to the cores of C1 and C2.3.
Reduce S to the separating synsets for {C1,C2}.4.
Redefine C1 and C2 to be the clusters obtainedfrom clustering the units in the reduced Brepresented in terms of the synsets in reduced S.5.
Repeat Steps 2-4 until convergence (no furtherchanges to the retained units and synsets).At the end of this process, we are left with twowell-separated cluster cores and a set of separatingsynsets.
When we compute cores of clusters in ourJeremiah-Ezekiel experiment, 26 of the initial 100units are eliminated.
Of the 154 synsets that appearin the Jeremiah-Ezekiel corpus, 118 are separatingsynsets for the resulting clustering.
The resultingcluster cores split with Jeremiah and Ezekiel asfollows:Book Cluster I Cluster IIJerEze362036We find that all but two of the misplaced unitsare not part of the core.
Thus, we have a betterclustering but it is only a partial one.5.2 Using Cores for Supervised LearningNow that we have what we believe are strongrepresentatives of each cluster, we can use them ina supervised way to classify the remaining unclus-tered units.
The interesting question is which fea-ture set we should use.
Using synonyms would justget us back to where we began.
Instead we use theset of generic Bible words introduced earlier.
Thepoint to recall is that while this feature set provedinadequate in an unsupervised setting, this does notmean that it is inadequate for separating Jeremiahand Ezekiel, given a few good training examples.Thus, we use a bag-of-words representation re-stricted to generic Bible words for the 74 units inour cluster cores and label them according to thecluster to which they were assigned.
We now applySVM to learn a classifier for the two clusters.
Weassign each unit, including those in the training set,to the class assigned to it by the SVM classifier.The resulting split is as follows:Book Cluster I Cluster IIJerEze510148Remarkably, even the two Ezekiel chapters thatwere in the Jeremiah cluster (and hence were es-sentially misleading training examples) end up onthe Ezekiel side of the SVM boundary.It should be noted that our two-stage approachto clustering is a generic method not specific to ourparticular application.
The point is that there aresome feature sets that are very well suited to a par-ticular unsupervised problem but are sparse, sothey give only a partial clustering.
At the sametime, there are other feature sets that are denserand, possibly for that reason, adequate for super-1360vised separation of the intended classes but inade-quate for unsupervised separation of the intendedclasses.
This suggests an obvious two-stage me-thod for clustering, which we use here to good ad-vantage.This method is somewhat reminiscent of semi-supervised methods sometimes used in text catego-rization where few training examples are available(Nigam et al 2000).
However, those methods typi-cally begin with some information, either in theform of a small number of labeled documents or inthe form of keywords, while we are not suppliedwith these.
Furthermore, the semi-supervised workbootstraps iteratively, at each stage using featuresdrawn from within the same feature set, while weuse exactly two stages, the second of which uses adifferent type of feature set than the first.For the reader?s convenience, we summarize theentire two-stage method:1.
Represent units in terms of synonyms.2.
Compute similarities of pairs of units usingcos'.3.
Use ncut to obtain an initial clustering.4.
Use the iterative method to find cluster cores.5.
Represent units in cluster cores in terms of ge-neric words.6.
Use units in cluster cores as training for learn-ing an SVM classifier.7.
Classify all units according to the learned SVMclassifier.6 Empirical ResultsWe now test our method on other pairs of bibli-cal books to see if we obtain comparable results tothose seen above.
We need, therefore, to identify aset of biblical books such that (i) each book is suf-ficiently long (say, at least 20 chapters), (ii) each iswritten by one primary author, and (iii) the authorsare distinct.
Since we wish to use these books as agold standard, it is important that there be a broadconsensus regarding the latter two, potentially con-troversial, criteria.
Our choice is thus limited to thefollowing five books that belong to two biblicalsub-genres: Isaiah, Jeremiah, Ezekiel (propheticliterature), Job and Proverbs (wisdom literature).
(Due to controversies regarding authorship (Pope1952, 1965), we include only Chapters 1-33 ofIsaiah and only Chapters 3-41 of Job.
)Recall that our experiment is as follows: Foreach pair of books, we are given all the chapters inthe union of the two books and are given no infor-mation regarding labels.
The object is to sort outthe chapters belonging to the respective two books.
(The fact that there are precisely two constituentbooks is given.
)We will use the three algorithms seen above:1. generic biblical words representation and ncutclustering;2. synonym representation and ncut clustering;3. our two-stage algorithm.We display the results in two separate figures.In Figure 1, we see results for the six pairs ofbooks that belong to different sub-genres.
In Figure2, we see results for the four pairs of books that arein the same genre.
(For completeness, we includeJeremiah-Ezekiel, although it served above as adevelopment corpus.)
All results are normalizedmajority diagonal.Figure 1.
Results of three clustering methods for differ-ent-genre pairsFigure 2.
Results of three clustering methods for same-genre pairsAs is evident, for different-genre pairs, even thesimplest method works quite well, though not aswell as the two-stage method, which is perfect forfive of six such pairs.
The real advantage of thetwo-stage method is for same-genre pairs.
For1361these the simple method is quite erratic, while thetwo-stage method is near perfect.
We note that thesynonym method without the second stage isslightly worse than generic words for different-genre pairs (probably because these pairs sharerelatively few synsets) but is much more consistentfor same-genre pairs, giving results in the area of90% for each such pair.
The second stage reducesthe errors considerably over the synonym methodfor both same-genre and different-genre pairs.7  Decomposing Unsegmented DocumentsUp to now, we have considered the case wherewe are given text that has been pre-segmented intopure authorial units.
This does not capture the kindof decomposition problems we face in real life.
Forexample, in the Pentateuch problem, the text isdivided up according to chapter, but there is noindication that the chapter breaks are correlatedwith crossovers between authorial units.
Thus, wewish now to generalize our two-stage method tohandle unsegmented text.7.1 Generating Composite DocumentsTo make the problem precise, let?s considerhow we might create the kind of document that wewish to decompose.
For concreteness, let?s thinkabout Jeremiah and Ezekiel.
We create a compositedocument, called Jer-iel, as follows:1.
Choose the first k1 available verses of Jeremiah,where k1 is a random integer drawn from theuniform distribution over the integers 1 to m.2.
Choose the first k2 available verses of Ezekiel,where k2 is a new random integer drawn fromthe above distribution.3.
Repeat until one of the books is exhausted; thenchoose the remaining verses of the other book.For the experiments discussed below, we usem=100 (though further experiments, omitted forlack of space, show that results shown are essen-tially unchanged for any m?60).
Furthermore, tosimulate the Pentateuch problem, we break Jer-ielinto initial units by beginning a new unit wheneverwe reach the first verse of one of the original chap-ters of Jeremiah or Ezekiel.
(This does not leak anyinformation since there is no inherent connectionbetween these verses and actual crossover points.
)7.2 Applying the Two-Stage MethodOur method works as follows.
First, we refinethe initial units (each of which might be a mix ofverses from Jeremiah and Ezekiel) by splittingthem into smaller units that we hope will be pure(wholly from Jeremiah or from Ezekiel).
We saythat a synset is doubly-represented in a unit if theunit includes two different synonyms of that syn-set.
Doubly-represented synsets are an indicationthat the unit might include verses from two differ-ent books.
Our object is thus to split the unit in away that minimizes doubly-represented synonyms.Formally, let M(x) represent the number of synsetsfor which more than one synonym appear in x.
Call?x1,x2?
a split of x if x=x1x2.
A split ?x1',x2'?
is optim-al if ?x1',x2'?= argmax M(x)-max(M(x1),M(x2)) wherethe maximum is taken over all splits of x.
If for aninitial unit, there is some split for which M(x)-max(M(x1),M(x2)) is greater than 0, we split the unitoptimally; if there is more than one optimal split,we choose the one closest to the middle verse ofthe unit.
(In principle, we could apply this proce-dure iteratively; in the experiments reported here,we split only the initial units but not split units.
)Next, we run the first six steps of the two-stagemethod on the units of Jer-iel obtained from thesplitting process, as described above, until thepoint where the SVM classifier has been learned.Now, instead of classifying chapters as in Step 7 ofthe algorithm, we classify individual verses.The problem with classifying individual versesis that verses are short and may contain few or norelevant features.
In order to remedy this, and alsoto take advantage of the stickiness of classes acrossconsecutive verses (if a given verse is from a cer-tain book, there is a good chance that the nextverse is from the same book), we use two smooth-ing tactics.Initially, each verse is assigned a raw score bythe SVM classifier, representing its signed distancefrom the SVM boundary.
We smooth these scoresby computing for each verse a refined score that isa weighted average of the verse?s raw score andthe raw scores of the two verses preceding andsucceeding it.
(In our scheme, the verse itself isgiven 1.5 times as much weight as its immediateneighbors and three times as much weight as sec-ondary neighbors.
)Moreover, if the refined score is less than 1.0(the width of the SVM margin), we do not initially1362assign the verse to either class.
Rather, we checkthe class of the last assigned verse before it and thefirst assigned verse after it.
If these are the same,the verse is assigned to that class (an operation wecall ?filling the gaps?).
If they are not, the verseremains unassigned.To illustrate on the case of Jer-iel, our original?munged?
book has 96 units.
After pre-splitting,we have 143 units.
Of these, 105 are pure units.Our two cluster cores, include 33 and 39 units, re-spectively; 27 of the former are pure Jeremiah and30 of the latter are pure Ezekiel; no pure units arein the ?wrong?
cluster core.
Applying the SVMclassifier learned on the cluster cores to individualverses, 992 of the 2637 verses in Jer-iel lie outsidethe SVM margin and are assigned to some class.All but four of these are assigned correctly.
Fillingthe gaps assigns a class to 1186 more verses, allbut ten of them correctly.
Of the remaining 459unassigned verses, most lie along transition points(where smoothing tends to flatten scores and wherepreceding and succeeding assigned verses tend tobelong to opposite classes).7.3 Empirical ResultsWe randomly generated composite books foreach of the book pairs considered above.
In Fig-ures 3 and 4, we show for each book pair the per-centage of all verses in the munged document thatare ?correctly?
classed (that is, in the majority di-agonal), the percentage incorrectly classed (minori-ty diagonal) and the percentage not assigned toeither class.
As is evident, in each case the vastmajority of verses are correctly assigned and only asmall fraction are incorrectly assigned.
That is, wecan tease apart the components almost perfectly.Figure 3.
Percentage of verses in each munged differ-ent-genre pair of books that are correctly and incorrectlyassigned or remain unassigned.Figure 4.
Percentage of verses in each munged same-genre pair of books that are correctly and incorrectlyassigned or remain unassigned.8 Conclusions and Future WorkWe have shown that documents can be decom-posed into authorial components with very highaccuracy by using a two-stage process.
First, weestablish a reliable partial clustering of units byusing synonym choice and then we use these par-tial clusters as training texts for supervised learn-ing using generic words as features.We have considered only decompositions intotwo components, although our method generalizestrivially to more than two components, for exampleby applying it iteratively.
The real challenge is todetermine the correct number of components,where this information is not given.
We leave thisfor future work.Despite this limitation, our success on mungedbiblical books suggests that our method can befruitfully applied to the Pentateuch, since the broadconsensus in the field is that the Pentateuch can bedivided into two main authorial categories: Priestly(P) and non-Priestly (Driver 1909).
(Both catego-ries are often divided further, but these subdivi-sions are more controversial.)
We find that oursplit corresponds to the expert consensus regardingP and non-P for over 90% of the verses in the Pen-tateuch for which such consensus exists.
We havethus been able to largely recapitulate several centu-ries of painstaking manual labor with our auto-mated method.
We offer those instances in whichwe disagree with the consensus for the considera-tion of scholars in the field.In this work, we have exploited the availabilityof tools for identifying synonyms in biblical litera-ture.
In future work, we intend to extend our me-thods to texts for which such tools are unavailable.1363ReferencesJ.
Astruc.
1753.
Conjectures sur les m?moires originauxdont il paroit que Moyse s?est servi pour composer lelivre de la Gen?se.
Brussels.R.
E. Bee.
1971.
Statistical methods in the study of theMasoretic text of the Old Testament.
J. of the RoyalStatistical Society, 134(1):611-622.M.
J. Berryman, A. Allison, and D. Abbott.
2003.
Sta-tistical techniques for text classification based onword recurrence intervals.
Fluctuation and Noise Let-ters, 3(1):L1-L10.J.
E. Carpenter, G. Hartford-Battersby.
1900.
The Hex-ateuch: According to the Revised Version.
London.J.
Clark and C. Hannon.
2007.
A classifier system forauthor recognition using synonym-based features.Proc.
Sixth Mexican International Conference on Ar-tificial Intelligence, Lecture Notes in Artificial Intel-ligence, vol.
4827, pp.
839-849.I.
S. Dhillon, Y. Guan, and B. Kulis.
2004.
Kernel k-means: spectral clustering and normalized cuts.
Proc.ACM International Conference on Knowledge Dis-covery and Data Mining (KDD), pp.
551-556.S.
R. Driver.
1909.
An Introduction to the Literature ofthe Old Testament (8th ed.).
Clark, Edinburgh.N.
Graham, G. Hirst, and B. Marthi.
2005.
Segmentingdocuments by stylistic character.
Natural LanguageEngineering, 11(4):397-415.D.
Guthrie, L. Guthrie, and Y. Wilks.
2008.
An unsu-pervised probabilistic approach for the detection ofoutliers in corpora.
Proc.
Sixth International Lan-guage Resources and Evaluation (LREC'08), pp.
28-30.D.
Holmes.
1994.
Authorship attribution, Computersand the Humanities, 28(2):87-106.P.
Juola.
2008.
Author Attribution.
Series title:Foundations and Trends in Information Retriev-al.
Now Publishing, Delft.M.
Koppel, N. Akiva, and I. Dagan.
2006.
Feature in-stability as a criterion for selecting potential stylemarkers.
J. of the American Society for InformationScience and Technology, 57(11):1519-1525.M.
Koppel, J.  Schler, and S. Argamon.
2009.
Compu-tational methods in authorship attribution.
J. of theAmerican Society for Information Science and Tech-nology, 60(1):9-26.D.
L. Mealand.
1995.
Correspondence analysis of Luke.Lit.
Linguist Computing, 10(3):171-182.S.
Meyer zu Eisen and B. Stein.
2006.
Intrinsic plagiar-ism detection.
Proc.
European Conference on Infor-mation Retrieval (ECIR 2006), Lecture Notes inComputer Science, vol.
3936, pp.
565?569.K.
Nigam, A. K. McCallum, S. Thrun, and T. M. Mit-chell.
2000.
Text classification from labeled and un-labeled documents using EM, Machine Learning,39(2/3):103-134.M.
H. Pope.
1965.
Job (The Anchor Bible, Vol.
XV).Doubleday, New York, NY.M.
H. Pope.
1952.
Isaiah 34 in relation to Isaiah 35, 40-66.
Journal of Biblical Literature, 71(4):235-243.Y.
Radday.
1970.
Isaiah and the computer: A prelimi-nary report, Computers and the Humanities, 5(2):65-73.E.
Stamatatos.
2009.
A survey of modern authorshipattribution methods.
J. of the American Society forInformation Science and Technology, 60(3):538-556.J.
Strong.
1890.
The Exhaustive Concordance of theBible.
Nashville, TN.
(Online edition:http://www.htmlbible.com/sacrednamebiblecom/kjvstrongs/STRINDEX.htm; accessed 14 November2010.
)1364
