Proceedings of the 4th International Workshop on Computational Terminology, pages 22?31,Dublin, Ireland, August 23 2014.Identification of Bilingual Terms from Monolingual Documents forStatistical Machine TranslationMihael Arcan1 Claudio Giuliano2 Marco Turchi2 Paul Buitelaar11 Unit for Natural Language Processing, Insight @ NUI Galway, Ireland{mihael.arcan , paul.buitelaar}@insight-centre.org2 FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy{giuliano, turchi}@fbk.euAbstractThe automatic translation of domain-specific documents is often a hard task for generic Sta-tistical Machine Translation (SMT) systems, which are not able to correctly translate the largenumber of terms encountered in the text.
In this paper, we address the problems of automaticidentification of bilingual terminology using Wikipedia as a lexical resource, and its integrationinto an SMT system.
The correct translation equivalent of the disambiguated term identified inthe monolingual text is obtained by taking advantage of the multilingual versions of Wikipedia.This approach is compared to the bilingual terminology provided by the Terminology as a Ser-vice (TaaS) platform.
The small amount of high quality domain-specific terms is passed to theSMT system using the XML markup and the Fill-Up model methods, which produced a relativetranslation improvement up to 13% BLEU score points1 IntroductionTranslation tasks often need to deal with domain-specific terms in technical documents, which requirespecific lexical knowledge of the domain.
Nowadays, SMT systems are suitable to translate very frequentexpressions but fail in translating domain-specific terms.
This mostly depends on a lack of domain-specific parallel data from which the SMT systems can learn.
Translation tools such as Google Translateor open source phrase-based SMT systems, trained on generic data, are the most common solutions andthey are often used to translate manuals or very specific texts, resulting in unsatisfactory translations.This problem is particular relevant for professional translators that work with documents coming fromdifferent domains and are supported by generic SMT systems.
A valuable solution to help them in han-dling domain-specific terms is represented by online terminology resources, e.g.
IATE - Inter-ActiveTerminology for Europe,1 which are continuously updated and can be easily queried.
However, the man-ual use of these services can be very time demanding.
For this reason, the identification and embeddingof domain-specific terms in an SMT system is a crucial step towards increasing translator productivityand translation quality in highly specific domains.In this paper, we propose an approach to automatically detect monolingual domain-specific terms froma source language document and identify their equivalents using Wikipedia cross-lingual links.
For thispurpose we extend The Wiki Machine API,2 a tool for linking terms in text to Wikipedia pages, addingtwo more components able to first identify domain-specific terms, and to find their translations in a targetlanguage.
The identified bilingual terms are then compared with those obtained by TaaS (Skadins?
et al.,2013).
The embedding of the domain-specific terms into an SMT system is performed by use of theXML markup approach, which uses the terms as preferred translation candidates at run time, and theFill-Up model (Bisazza et al., 2011), which emphasizes phrase pairs extracted from the bilingual terms.Our results show that the performance of our technique and TaaS are comparable in the identificationof monolingual and bilingual domain-specific terms.
From the machine translation point of view, ourexperiments highlight the benefit of integrating bilingual terms into the SMT system, and the relativeimprovement in BLEU score of the Fill-Up model over the baseline and the XML markup approach.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceed-ings footer are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1 http://iate.europa.eu/ 2 https://bitbucket.org/fbk/thewikimachine/Terminology questions in texts authored by patientsNoemie ElhadadDepartment of Biomedical InformaticsColumbia University, USAnoemie@dbmi.columbia.eduThis work is c er a Creative Com ons Attribution 4.0 Internatio l License.
Page numb rs and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/222 MethodologyGiven a source document, it is processed by our pipeline that: (i) with the help of The Wiki Machine, itidentifies, disambiguates and links all terms in the document to the Wikipedia pages; (ii) the terms andtheir links are used to identify the domain of the document and filter out the terms that are not domain-specific; (iii) the translation of such terms is obtained following the Wikipedia cross-lingual links; (iv)the bilingual domain-specific terms are embedded into the SMT system using different strategies.
In therest of this section, each step is described in detail.2.1 Bilingual Term IdentificationTerm Detection and Linking The Wiki Machine is a tool for linking terms in text to Wikipedia pagesand enriching them with information extracted from Wikipedia and Linked Open Data (LOD) resourcessuch as DBPedia or Freebase.
The Wiki Machine has been preferred among other approaches because itachieves the best performance in term disambiguation and linking (Mendes et al., 2011), and facilitatesthe extraction of structured information from Wikipedia.The annotation process consists of a three-step pipeline based on statistical and machine learningmethods that exclusively uses Wikipedia to train the models.
No linguistic processing, such as stemming,morphology analysis, POS tagging, or parsing, is performed.
This choice facilitates the portability of thesystem as the only requirement is the existence of a Wikipedia version with a sufficient coverage for thespecific language and domain.
The first step identifies and ranks the terms by relevance using a simplestatistical approach based on tf-idf weighting, where all the n-grams, for n from 1 to 10, are generatedand the idf is directly calculated on Wikipedia pages.
The second step links the terms to Wikipedia pages.The linking problem is cast as a supervised word sense disambiguation problem, in which the terms mustbe disambiguated using Wikipedia to provide the sense inventory and the training data (for each sense,a list of phrases where the term appears) as first introduced in (Mihalcea, 2007).
The application usesan ensemble of word-expert classifiers that are implemented using the kernel-based approach (Giulianoet al., 2009).
Specifically, domain and syntagmatic aspects of sense distinction are modelled by meansof a combination of the latent semantic and string kernels (Shawe-Taylor and Cristianini, 2004).
Thethird step enriches the linked terms using information extracted from Wikipedia and LOD resources.The additional information relative to the pair term/Wikipedia page consists of alternative terms (i.e.,orthographical and morphological variants, synonyms, and related terms), images, topic, type, crosslanguage links, etc.
For example, in the text ?click right mouse key to pop up menu and Gnome panel?,The Wiki Machine identifies the terms mouse, key, pop up menu and Gnome panel.
For the ambiguousterm mouse, the linking algorithm returns the Wikipedia page ?Mouse (computing)?, and the other termsused to link that page in Wikipedia with their frequency, i.e., computer mouse, mice, and Mouse.In the context of the experiments reported here, we were specifically interested in the identification ofdomain-specific bilingual terminology to be embedded into the SMT system.
For this reason, we extendThe Wiki Machine adding the functionality of filtering out terms that do not belong to the documentdomain, and of automatically retrieving term translations.Domain Detection To identify specific terms, we assign a domain to each linked term in a text, afterthat we obtain the most frequent domain and filter out the terms that are out of scope.
In the exampleabove, the term mouse is accepted because it belongs to the domain computer science, as the majority ofterms (mouse, pop up menu and Gnome panel), while the term key in the domain music is rejected.The large number of languages and domains to cover prevents us from using standard text classificationtechniques to categorize the document.
For this reason, we implemented an approach based on themapping of the Wikipedia categories into the WordNet domains (Bentivogli et al., 2004).
The Wikipediacategories are created and assigned by different human editors, and are therefore less rigorous, coherentand consistent than usual ontologies.
In addition, the Wikipedia?s category hierarchy forms a cyclic graph(Zesch and Gurevych, 2007) that limits its usability.
Instead, the WordNet domains are organized in ahierarchy that contains only 164 items with a degree of granularity that makes them suitable for NaturalLanguage Processing tasks.
The approach we are proposing overcomes the Wikipedia category sparsity,allows us reducing the number of domains to few tens instead of some hundred thousands (800,00023categories in the English Wikipedia) and does not require any language-specific training data.
Wikipediacategories that contain more pages (?1,000) have been manually mapped to WordNet domains.
Thedomain for a term is obtained as follows.
First, for each term, we extract its set of categories, C, fromthe Wikipedia page linked to it.
Second, by means of a recursive procedure, all possible outgoing paths(usually in a large number) from each category in C are followed in the graph of Wikipedia categories.When one of the mapped categories to a WordNet domain is found, the approach stops and associates therelative WordNet domain to the term.
In this way, more and more domains are assigned to a single term.Third, to isolate the most relevant one, these domains are ranked according the number of times they havebeen found following all the paths.
The most frequent domain is assigned to the terms.
Although thisprocess needs the human intervention for the manual mapping, it is done once and it is less demandingthan annotating large amounts of training documents for text classification, because it does not requirethe reading of the document for topic identification.Bilingual Term Extraction The last phase consists in finding the translation of the domain terminol-ogy.
We exploit the Wikipedia cross-language links, which, however, provide an alignment at page levelnot at term level.
To deal with this issue we introduced the following procedure.
If the term is equal tothe source page title (ignoring case) we return the target page; otherwise, we return the most frequent al-ternative form of the term in the target language.
From the previous example, the system is able to returnthe Italian page Mouse and all terms used in the Italian Wikipedia to express this concept of Mouse incomputer science.
Using this information, the term mouse is paired with its translation into Italian.2.2 Integration of Bilingual Terms into SMTA straightforward approach for adding bilingual terms to the SMT system consists of concatenating thetraining data and the terms.
Although it has been shown to perform better than more complex techniques(Bouamor et al., 2012), it is still affected by major disadvantages that limits its use in real applications.In particular, when small amounts of bilingual terms are concatenated with a large training dataset, termswith ambiguous translations are penalised, because the most frequent and general translations oftenreceive the highest probability, which drives the SMT system to ignore specific translations.In this paper, we focus on two techniques that give more priority to specific translations than genericones: the Fill-Up model and the XML markup approach.
The Fill-Up model has been developed toaddress a common scenario where a large generic background model exists, and only a small quantityof in-domain data can be used to build an in-domain model.
Its goal is to leverage the large coverageof the background model, while preserving the domain-specific knowledge coming from the in-domaindata.
Given the generic and the in-domain phrase tables, they are merged.
For those phrase pairs thatappear in both tables, only one instance is reported in the Fill-Up model with the largest probabilitiesaccording to the tables.
To keep track of a phrase pair?s provenance, a binary feature that penalises ifthe phrase pair comes from the background table is added.
The same strategy is used for reorderingtables.
In our experiments, we use the bilingual terms identified from the source data as in-domaindata.
Word alignments are computed on the concatenation of the data.
Phrase extraction and scoringare carried out separately on each corpus.
The XML markup approach makes it possible to directly passexternal knowledge to the decoder, specifying translations for particular spans of the source sentence.
Inour scenario, the source term is used to identify a span in the source sentence, while the target term isdirectly passed to the decoder.
With the setting exclusive, the decoder uses only the specified translationsignoring other possible translations in the translation model.3 Experimental SettingIn our experiments, we used different English-Italian and Italian-English test sets from two domains: (i)a small subset of the GNOME project data3 (4,3K tokens) and KDE4 Data4 (9,5K) for the IT domainand (ii) a subset of the EMEA corpus (11K) for the medical domain.In order to assess the quality of the monolingual and bilingual terms, we create a terminological goldstandard.
Two annotators with a linguistic background and English and Italian proficiency were asked3 https://l10n.gnome.org/ 4 http://i18n.kde.org/24to mark all domain-specific terms in a set of 66 English and Italian documents of the GNOME corpus,and a set of 100 paragraphs (4,3K tokens) from the KDE4 corpus.5 Domain-specificity was defined asall (multi-)words that are typically used in the IT domain and that may have different Italian translationsin other domains.
The average Cohen?s Kappa of GNOME and KDE anno computed at token level was0.66 for English and 0.53 for Italian.
Following Landis and Koch (1977), this corresponds to a substantialand moderate agreement between the annotators.Finally the gold standard dataset was generated by the intersection of the annotations of the two an-notators.
In detail, for the GNOME dataset the annotators marked 93 single-word and 134 multi-wordexpressions (MWEs), resulting 227 terms in overall.
For the KDE anno dataset, 321 monolingual termsfor the GNOME dataset were annotated, whereby 192 of them were multi-word expressions.
This resultsin 190 unique bilingual terms for the GNOME corpus and 355 for the KDE anno dataset.We compare the monolingual and bilingual terms identified by our approach to the terms obtainedby the online service TaaS,6 which is a cloud-based platform for terminology services based on thestate-of-the-art terminology extraction and bilingual terminology alignment methods.
TaaS providesseveral options in term identification, of which we selected TWSC, Tilde wrapper system for CollTerm,(Pinnis et al., 2012).
TWSC is based on linguistic analysis, i.e.
part of speech tagging and morpho-syntactic patterns, enriched with statistical features.
TaaS allows for lookup in several manually andautomatically built monolingual and bilingual terminological resources and for our experiment we useEuroTermBank (ETB), Taus Data and Web Data.
Accessing several resources, TaaS may provide severaltranslations for a unique source term, but not an indicator of their translation quality.
To avoid assigningthe same probability to all the translations of the same source term, we prioritise a translation by theresource it was provided.
In our case, we favour first the translation provided by ETB.
If no translationis available, we use the translation provided by Taus Data or eventually from Web Data.
Before startingthe term extraction approach, TaaS requires manual specification of the source and target languages, thedomain, and the source document.
Since we focused on the IT and medical domains we set the optionsto ?Information and communication technology?
and ?Medicine and pharmacy?, respectively.For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), wherethe word alignments were built with the GIZA++ toolkit (Och and Ney, 2003).
The IRSTLM toolkit(Federico et al., 2008) was used to build the 5-gram language model.
For a broader domain coverage,we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl(Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of?37M tokens and a development set of ?10K tokens.In our experiments, an instance of Moses trained on the generic parallel dataset was used in threedifferent scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markupapproach for translating remaining parts that were not covered by the embedded terminology; (iii) in theFill-Up method as background translation model.4 EvaluationIn this Section, we report the performance of the different term identification tools and term embeddingmethods for the two domains: IT and the medical domain.
For evaluating the extracted monolingualand bilingual terms, we calculate precision, recall and f-measure using the manually labelled KDE annoand GNOME datasets.
In addition, we perform a manual inspection of a subset of the bilingual identi-fied terms.
The BLEU metric (Papineni et al., 2002) was used to automatically evaluate the quality ofthe translations.
The metric calculates the overlap of n-grams between the SMT system output and areference translation, provided by a professional translator.4.1 Monolingual Term IdentificationIn Table 1, the column ?Ident.?
represents the number of identified terms for each tool, whereby weobserved TaaS always extracts more terms than The Wiki Machine.
While extracting Italian terms,TaaS extracts twice as more terms as The Wiki Machine, which can be explained by the overall lower5 In the rest of the paper, we refer to the annotated part of KDE4 as KDE anno6 https://demo.taas-project.eu/25English ItalianKDE anno Ident.
unigram MWE Precision Recall F1 Ident.
unigram MWE Precision Recall F1TaaS 431 144 287 0.442 0.594 0.507 518 147 371 0.326 0.511 0.398The Wiki Machine 327 247 80 0.400 0.406 0.403 207 184 23 0.429 0.268 0.330GNOME Ident.
unigram MWE Precision Recall F1 Ident.
unigram MWE Precision Recall F1TaaS 311 119 192 0.260 0.355 0.301 359 110 249 0.272 0.415 0.329The Wiki Machine 275 199 76 0.303 0.364 0.330 196 167 29 0.331 0.275 0.301Table 1: Evaluation of monolingual term identification for the KDE anno and GNOME dataset.amount of Italian pages in Wikipedia compared to the English version.
Focusing on the amount ofidentified single-word and multi-word expressions, it is interesting to notice that TaaS, independently ofthe language, extracts around twice as more MWEs than single words.
Differently, The Wiki Machineidentifies mostly single-word terms, whereby they represent around three-fourth of all identified termsfor English and around 12% for Italian.For the KDE anno dataset, TaaS in most cases (except in precision for the Italian KDE anno dataset)outperforms The Wiki Machine approach in all metrics.
Especially we observed a higher recall producedby the TaaS approach, which can be deduced from the higher number of extracted MWEs compared toThe Wiki Machine approach.
On the English GNOME dataset, The Wiki Machine performs comparableresults to TaaS, with a slightly higher recall and F1.
On the Italian side, The Wiki Machine identifies lessMWEs than TaaS, which results in a low recall and F1.In summary, we observe that TaaS performs best on the KDE anno dataset, whereas The Wiki Machineand TaaS perform comparable results on the GNOME dataset.
Analysing the overall results, we noticethat precision, recall and F1 are generally better in English than in Italian.
This is due to the fact thatItalian tends to use more words to express the same concept compared to English.4.2 Bilingual Term IdentificationTable 2 reports the performance of The Wiki Machine and TaaS in the identification of bilingual termsevaluated against the manually produced list of terms.
In both language pairs and datasets, TaaS and TheWiki Machine mostly identify similar amounts of bilingual terms (column ?Ident.?)
and match with thegold standard (column ?Mat.?).
Only for KDE anno, It?En, TaaS identifies almost 50% more bilingualterms than The Wiki Machine.It is worth noticing that, although TaaS is accessing high quality manually-produced termbases, e.g.ETB in our results, there is no evidence that it works significantly better than The Wiki Machine access-ing Wikipedia.
In fact, in terms of F1, The Wiki Machine performs best on the GNOME annotated testset, while it is outperformed by TaaS on KDE anno.
In both cases, differences in performance are mini-mal.
According to the precision measure, The Wiki Machine seems to be able to produce more accuratebilingual terms.The automatic evaluation shows difficulties (low F1 scores) for The Wiki Machine and TaaS in iden-tifying bilingual terms that perfectly match the gold standard.
To better understand the quality of termtranslations, we asked one of the annotators involved in the creation of the gold standard to perform amanual evaluation of a subset of fifty bilingual terms randomly selected from each list.
We used thefour error categories proposed in (Aker et al., 2013): 1) The terms are exact translations of each otherin the domain; 2) Inclusion: Not an exact translation, but an exact translation of one term is entirelycontained within the term in the other language; 3) Overlap: Not category 1 or 2, but the terms share atleast one translated word; 4) Unrelated: No word in either term is a translation of a word in the other.The percentages of bilingual terms assigned to each class are shown in Table 3.In terms of comparison between the two tools, the manual evaluation confirms that there is no evidencethat a tool produces better term translations than the other in all the test sets.
In fact, except for KDE annoEn?It where TaaS outperforms The Wiki Machine, the percentage of bilingual terms assigned to class1 for both the tools is almost similar.
In terms of absolute scores, the manual evaluation shows thatthe quality of the identified bilingual terms is relatively high (merging the terms assigned to classes 126GNOME En?It Ident.
Mat.
Precision Recall F1TaaS 145 20 0.138 0.105 0.119The Wiki Machine 156 25 0.160 0.130 0.144GNOME It?En Ident.
Mat.
Precision Recall F1TaaS 139 21 0.151 0.110 0.127The Wiki Machine 140 23 0.164 0.121 0.139KDE anno En?It Ident.
Mat.
Precision Recall F1TaaS 249 65 0.261 0.183 0.215The Wiki Machine 229 49 0.202 0.138 0.164KDE anno It?En Ident.
Mat.
Precision Recall F1TaaS 228 58 0.254 0.163 0.199The Wiki Machine 155 48 0.292 0.135 0.185Table 2: Automatic evaluation of bilingual terms ex-tracted from GNOME and KDE anno.GNOME En?It 1 2 3 4TaaS 0.66 0.08 0.00 0.26The Wiki Machine 0.70 0.08 0.06 0.16GNOME It?En 1 2 3 4TaaS 0.78 0.08 0.02 0.12The Wiki Machine 0.68 0.12 0.04 0.16KDE anno En?It 1 2 3 4TaaS 0.90 0.00 0.06 0.04The Wiki Machine 0.70 0.10 0.06 0.14KDE anno It?En 1 2 3 4TaaS 0.70 0.10 0.10 0.10The Wiki Machine 0.64 0.22 0.08 0.06Table 3: Manual evaluation of bilingual termsbased on four error categories (1-4).and 2, we reach a score, in most of the cases, larger than 80%).
This is in contrast with the automaticevaluation, which reports limited performances (F1 ?
0.2) for both methods.
The main reason is thatthe automatic evaluation requires a perfect match between the identified and the gold standard bilingualterms to measure an improvement in F1, while the manual evaluation can reward bilingual terms that donot perfectly match any gold standard terms but are correct translations of each other.
An example isthe multi-word bilingual term ?settings of the network connection?
impostazioni della connessione direte?
that is present in the gold standard as a single multi-word term, while it is identified by The WikiMachine as two distinct bilingual terms, i.e.
?network connection?
connessione di rete?
and ?settings?
impostazioni?.
From the translation point of view, both the distinct terms are correct and they areassigned to class 1 during the manual evaluation, but they are ignored by the automatic evaluation.The analysis of terms assigned to error class four shows that both methods are affected by similarproblems.
The main source of error is the correct detection of the source term domain, which results ina translated term that does not belong to the correct domain.
For instance, in the bilingual term ?stringhe?
shoe and boot laces?, the term ?stringhe?
(?strings?
in the IT domain) is translated into ?laces?.
Simi-larly, the English term ?launchers?
(?lanciatori?
in Italian in the IT domain) is translated into ?lanciarazzimultiplo?
(?multiple rocket launchers?
in English), which is clearly not an IT term.
Furthermore, TheWiki Machine seems to have more problems in identifying the right morphological variation, e.g.
?in-dirizzi ip?
ip address?, where ?indirizzi?
is a plural noun and needs to be translated into ?addresses?.This is expected because page titles in Wikipedia are not always inflected.
An interesting example high-lighted by the annotator in the TaaS translations is: ?percorso di ricerca?
?
?how do i access refreshgrid texture?
?, where the Italian term (?search path?
in English) is translated with a completely wrongtranslation.
In the next Section we evaluate whether the automatic identified bilingual terms can improvethe performance of an SMT system and if it is robust to the aforementioned errors.4.3 Embedding Terminology into SMTOur further experiments focused on the automatic evaluation of the translation quality of the EMEA,GNOME and KDE test sets (Table 4).
The obtained bilingual terminology from TaaS and The Wiki Ma-chine was embedded through the Fill-Up and XML markup approaches.
The approximate randomizationapproach in MultEval (Clark et al., 2011) is used to test whether differences among system performancesare statistically significant with a p-value < 0.05.
The parameters of the baseline method and the Fill-Upmodels were optimized on the development set.Injecting the obtained TaaS bilingual terms improves the BLEU score in several cases.
XML markupoutperforms the general baseline approach in three (out of eight) datasets, whereby three of them arestatistically significant (GNOME En?It, KDE anno En?It).
Embedding the same bilingual terminol-ogy into the Fill-Up model helped to outperform the baseline approach for all test sets, whereby only theresult for EMEA En?It is not statistically significant.27GNOME KDE anno EMEA KDE4En?It It?En En?It It?En En?It It?En En?It It?Engeneral baseline 15.39 21.62 15.58 22.64 25.88 25.75 19.22 23.54XML Mark-up (TaaS) 15.87 22.45* 17.62* 23.88* 25.84 25.74 18.97 24.27*Fill-Up Model (TaaS) 16.22* 22.73* 17.61* 23.45* 25.95 26.02* 19.69* 24.56*XML Mark-up (The Wiki Machine) 15.49 20.57 17.19* 23.44* 25.59 24.97 17.74 22.16Fill-Up Model (The Wiki Machine) 15.82 21.70 16.48* 23.28* 26.35* 26.44* 19.61* 24.14*Table 4: Automatic BLEU Evaluation on GNOME, KDE and EMEA datasets with different term em-bedding strategies (bold results = best performance ; * statistically significant compared to baseline).Finally, we investigate the impact of embedding the identified terms provided by The Wiki Machine.When we suggest translation candidates with the XML markup, it only slightly outperforms the baselineapproach for GNOME En?It, but statistically significant improves the translations for the KDE annotest set for both language directions.
Similarly to previous observations, the Fill-Up model improvesfurther the translations, i.e.
the translations are statistically significant better than the baseline for bothlanguage pairs of both KDE test sets as well as for EMEA.To better understand our translation results, we manually inspected the EMEA En?It sentences, whichhave the best translation performance.
For each of the source sentence and the translation method,we analyse the translated sentences and the bilingual terms that match at least one word in the sourcesentence.
Both translation strategies tried to encapsulate the bilingual terms, but there is clear evidencethat the Fill-Up model better embeds the target terms in the context of the translation.
For instance inthe following example, the target sentence produced by the XML markup (XML trg) does not containthe article ?la?, uses a wrong conjunction (?di?
instead of ?per?)
and wrongly orders the adjective withthe noun (?adulti pazienti?
instead of ?pazienti adulti?).
All these issues are correctly addressed by theFill-Up model (Fill-Up trg) which produces a smoother translation.source sentence: adult patients receive therapy for tumoursreference sentence: pazienti adulti ricevono la terapia per i tumoribilingual terms: therapy?
terapia, patients?
pazienti, adult?
adultiXML trg: adulti pazienti ricevono terapia di tumoriFill-Up trg: pazienti adulti ricevono la terapia per i tumoriAnalysing the number of suggested bilingual terms per sentence, we notice that The Wiki Machinetends to propose more terms than TaaS (on average, The Wiki Machine 3.1, TaaS 2.5 per sentence).Of these terms, TaaS provides on average more translations for each unique source term than The WikiMachine (on average, TaaS 1.51, The Wiki Machine 1).In addition to evaluating the performance of TaaS and The Wiki Machine separately, for the EMEAdataset we concatenate the terminological lists provided by the tools and supply it to the XML markupand the Fill-Up approach.
Embedding the combined terminology with the XML markup produces aBLEU score of 25.59 for En?It and 24.92 for It?En.
This performance is similar to the scores obtainedusing the terminology provided by The Wiki Machine, but worse compared to TaaS.
Passing the wholeterminology to the Fill-Up model, the BLEU score increases up to 26.57 for En?It and 27.02 for It?En,which are the best BLEU scores for the EMEA test set.
This experiment shows the complementarity ofthe two term identification methods and suggests a novel research direction.5 Related WorkThe main focus of our research is on bilingual term identification and the embedding of this knowledgeinto an SMT system.
Since previous research (Wu et al.
(2008); Haddow and Koehn (2012)) showed thatan SMT system built by using a large general resource cannot be used to translate domain-specific terms,we have to provide the system domain-specific lexical knowledge.Wikipedia with its rich lexical and semantic knowledge was used as a resource for bilingual termidentification in the context of SMT.
Tyers and Pieanaar (2008) describe method for extracting bilingualdictionary entries from Wikipedia to support the machine translation system.
Based on exact string28matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingualdictionary.
Besides the interwiki link system, Erdmann et al.
(2009) enhances their bilingual dictionaryby using redirection page titles and anchor text within Wikipedia.
To filter out incorrect term translationpairs, the authors use the backward link information to prove if a redirect page title or an anchor textrepresents a synonymous expression.
Niehues and Waibel (2011) analyse different methods to integratethe extracted Wikipedia titles into their system, whereby they explore methods to disambiguate betweendifferent translations by using the text in the articles.
In addition, the authors use morphological formsof terms to enhance the extracted bilingual dictionary.
The results show that the number of out-of-vocabulary words could be reduced by 50% on computer science lectures, which improved the translationquality by more than 1 BLEU point.
Arcan et al.
(2013) restrict term identification to the observeddomain by using the frequency information of Wikipedia categories.
Different from these approacheswe focus on domain-specific dictionary generation, ignoring identified terms which do not belong to thedomain to be observed.
Furthermore, we take advantage of the Wikipedia category graph representationand its linking to WordNet domain, which allowed us to identify the domain we were interested in.Furthermore, research has been done on the integration of domain-specific parallel data into SMT,either by retraining small domain-specific and large general resources as one concatenated parallel data(Koehn and Schroeder, 2007), adding new phrase pairs directly into the phrase table (Langlais, 2002;Ren et al., 2009; Haddow and Koehn, 2012) or assigning adequate weights to the in- and out-of-domaintranslation models (Foster and Kuhn (2007); La?ubli et al.
(2013)).
Bouamor et al.
(2012) address theproblem of finding the best approach to integrate new obtained knowledge in an SMT system, and showthat they should be used as additional parallel sentences to train the translation model.
In our approach,we use the XML markup and the Fill-Up approach, which handles the in-domain parallel data equallyto the out-domain data.
Furthermore, Okita and Way (2010) investigate the effect of integrating bilin-gual terminology in the training step of an SMT system, and analyse in particular the performance andsensitivity of the word aligner.
As opposed to their approach, we do not have prior knowledge about thebilingual terminology, since we extract it from the document to be translated.6 ConclusionIn this paper we presented an approach to identify bilingual domain-specific terms starting from a mono-lingual text and to integrate these into an SMT system.
With the help of terminological and lexicalresources, we are able to discover a small amount (?200) of high-quality domain-specific terms andenhanced the performance of an SMT system trained on large amounts (1.8M) of parallel sentences.Monolingual and bilingual term evaluation showed no evidence that one of the tested tools (The WikiMachine or TaaS) produces better terms than the other in all the test sets.
Depending on the manual map-ping between the Wikipedia categories and WordNet domains and the existence of a Wikipedia version,our approach is language and domain independent, does not need training data and is able to overcomethe sparseness and coherence problems of the Wikipedia categories.
Evaluation of the two systems ondifferent language directions and domains shows significant improvements over the baseline in termsof two BLEU scores (up to 13%) and confirms the applicability of such techniques in a real scenario.It is interesting to notice that the Fill-Up technique regularly outperforms the XML markup approach,taking advantage of all terms and not only the overlapping terms in the text to be translated.
Our contri-bution shows a different context of using Fill-Up and extends the usability of it in terms of embeddingterminological knowledge into SMT.
In future work, we plan to focus on exploiting morphological termvariations taking advantage of the alternative terms (i.e., orthographical and morphological variants,synonyms, and related terms) provided by The Wiki Machine.
This will make it possible to increase thecoverage adding new terms and the accuracy of the proposed method for bilingual term identification.AcknowledgmentsThis publication has emanated from research supported in part by a research grant from Science Founda-tion Ireland (SFI) under Grant Number SFI/12/RC/2289 and by the European Union supported projectsEuroSentiment (Grant No.
296277), LIDER (Grant No.
610782) and MateCat (ICT-2011.4.2-287688).29ReferencesAhmet Aker, Monica Paramita, and Robert Gaizauskas.
2013.
Extracting bilingual terminologies from comparablecorpora.
In Proceedings of ACL, Sofia, Bulgaria.Mihael Arcan, Susan Marie Thomas, Derek De Brandt, and Paul Buitelaar.
2013.
Translating the FINREP taxon-omy using a domain-specific corpus.
In Machine Translation Summit XIV, pages 199?206.Luisa Bentivogli, Pamela Forner, Bernardo Magnini, and Emanuele Pianta.
2004.
Revising the wordnet domainshierarchy: semantics, coverage and balancing.
In Proceedings of the Workshop on Multilingual LinguisticRessources, pages 101?108.
Association for Computational Linguistics.Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011.
Fill-up versus Interpolation Methods for Phrase-basedSMT Adaptation.
In Proceedings of IWSLT.Dhouha Bouamor, Nasredine Semmar, and Pierre Zweigenbaum.
2012.
Identifying bilingual multi-word expres-sions for statistical machine translation.
In Proceedings of the Eight International Conference on Language Re-sources and Evaluation (LREC?12), Istanbul, Turkey, may.
European Language Resources Association (ELRA).Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.
2011.
Better Hypothesis Testing for Statistical Ma-chine Translation: Controlling for Optimizer Instability .
In Proceedings of the Association for ComputationalLingustics.Maike Erdmann, Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio.
2009.
Improving the extraction of bilin-gual terminology from wikipedia.
ACM Trans.
Multimedia Comput.
Commun.
Appl., 5(4):31:1?31:17, Novem-ber.Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008.
Irstlm: an open source toolkit for handling largescale language models.
In INTERSPEECH, pages 1618?1621.
ISCA.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for smt.
In Proceedings of the Second Work-shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA.
Association forComputational Linguistics.Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo Strapparava.
2009.
Kernel methods for minimallysupervised wsd.
Computational Linguistics, 35(4):513?528.Barry Haddow and Philipp Koehn.
2012.
Analysing the Effect of Out-of-Domain Data on SMT Systems.
InProceedings of the Seventh Workshop on Statistical Machine Translation, Montre?al, Canada.
Association forComputational Linguistics.Philipp Koehn and Josh Schroeder.
2007.
Experiments in domain adaptation for statistical machine translation.
InProceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227, Strouds-burg, PA, USA.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Constantin, and EvanHerbst.
2007.
Moses: open source toolkit for statistical machine translation.
In Proceedings of the 45th AnnualMeeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ?07, pages 177?180, Stroudsburg,PA, USA.
Association for Computational Linguistics.Philipp Koehn.
2005.
Europarl: A Parallel Corpus for Statistical Machine Translation.
In Conference Proceed-ings: the tenth Machine Translation Summit, pages 79?86.
AAMT.J.
Richard Landis and Gary G. Koch.
1977.
Measurement of Observer Agreement for Categorical Data.
InBiometrics, volume 33, pages 159?174.Philippe Langlais.
2002.
Improving a general-purpose statistical translation engine by terminological lexicons.
InProceedings of the 2nd International Workshop on Computational Terminology (COMPUTERM) ?2002, Taipei,Taiwan, pages 1?7.Samuel La?ubli, Mark Fishel, Martin Volk, and Manuela Weibel.
2013.
Combining statistical machine translationand translation memories with domain adaptation.
In Stephan Oepen, Kristin Hagen, and Janne Bondi Johan-nesse, editors, Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013),May 22?24, 2013, Oslo University, Norway, Linko?ping Electronic Conference Proceedings, pages 331?341,Oslo, May.
Linko?pings universitet Electronic Press.30Pablo N Mendes, Max Jakob, Andre?s Garc?
?a-Silva, and Christian Bizer.
2011.
Dbpedia spotlight: shedding lighton the web of documents.
In Proceedings of the 7th International Conference on Semantic Systems, pages 1?8.ACM.Rada Mihalcea.
2007.
Using Wikipedia for Automatic Word Sense Disambiguation.
In Proceedings of NAACL-HLT, pages 196?203.Jan Niehues and Alex Waibel.
2011.
Using Wikipedia to Translate Domain-specific Terms in SMT.
In nterna-tional Workshop on Spoken Language Translation, San Francisco, CA, USA.Franz Josef Och and Hermann Ney.
2003.
A systematic comparison of various statistical alignment models.Computational Linguistics, 29.Tsuyoshi Okita and Andy Way.
2010.
Statistical Machine Translation with Terminology.
In Proceedings of theFirst Symposium on Patent Information Processing (SPIP), Tokyo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evaluation ofmachine translation.
In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,ACL ?02, pages 311?318.Ma?rcis Pinnis, Nikola Ljubes?ic?, Dan S?tefa?nescu, Inguna Skadin?a, Marko Tadic?, and Tatiana Gornostay.
2012.Term extraction, tagging, and mapping tools for under-resourced languages.
In Proceedings of the Terminologyand Knowledge Engineering (TKE2012) Conference.Zhixiang Ren, Yajuan Lu?, Jie Cao, Qun Liu, and Yun Huang.
2009.
Improving statistical machine translationusing domain bilingual multiword expressions.
In Proceedings of the Workshop on Multiword Expressions:Identification, Interpretation, Disambiguation and Applications, MWE ?09, pages 47?54, Stroudsburg, PA,USA.
Association for Computational Linguistics.John Shawe-Taylor and Nello Cristianini.
2004.
Kernel Methods for Pattern Analysis.
Cambridge UniversityPress, New York, NY, USA.Raivis Skadins?, Marcis Pinnis, Tatiana Gornostay, and Andrejs Vasiljevs.
2013.
Application of online terminologyservices in statistical machine translation.
In Proceedings of the XIV Machine Translation Summit, Nice, France.Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Da?niel Varga.2006.
The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages.
In Proceedings of the 5thInternational Conference on Language Resources and Evaluation (LREC?2006).Jo?rg Tiedemann.
2012.
Parallel data, tools and interfaces in opus.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Thierry Declerck, Mehmet Ug?ur Dog?an, Bente Maegaard, Joseph Mariani, Jan Odijk, andStelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Eval-uation (LREC?12), Istanbul, Turkey, may.
European Language Resources Association (ELRA).Francis M. Tyers and Jacques A. Pieanaar.
2008.
Extracting bilingual word pairs from wikipedia.
In Collabo-ration: interoperability between people in the creation of language resources for less-resourced languages (ASALTMIL workshop).Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.
Domain adaptation for statistical machine translationwith domain dictionary and monolingual corpora.
In Proceedings of the 22nd International Conference onComputational Linguistics - Volume 1, COLING ?08, pages 993?1000.Torsten Zesch and Iryna Gurevych.
2007.
Analysis of the wikipedia category graph for nlp applications.
InProceedings of the TextGraphs-2 Workshop (NAACL-HLT), pages 1?8, Rochester, April.
Association for Com-putational Linguistics.31
