Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 17?24Manchester, August 2008On Robustness and Domain Adaptation using SVDfor Word Sense DisambiguationEneko Agirre and Oier Lopez de LacalleInformatika Fakultatea, University of the Basque Country20018, Donostia, Basque Country{e.agirre,oier.lopezdelacalle}@ehu.esAbstractIn this paper we explore robustness anddomain adaptation issues for Word SenseDisambiguation (WSD) using SingularValue Decomposition (SVD) and unlabeleddata.
We focus on the semi-supervised do-main adaptation scenario, where we trainon the source corpus and test on the tar-get corpus, and try to improve results us-ing unlabeled data.
Our method yieldsup to 16.3% error reduction compared tostate-of-the-art systems, being the first toreport successful semi-supervised domainadaptation.
Surprisingly the improvementcomes from the use of unlabeled data fromthe source corpus, and not from the targetcorpora, meaning that we get robustnessrather than domain adaptation.
In addition,we study the behavior of our system on thetarget domain.1 IntroductionIn many Natural Language Processing (NLP)tasks we find that a large collection of manually-annotated text is used to train and test supervisedmachine learning models.
While these modelshave been shown to perform very well when testedon the text collection related to the training data(what we call the source domain), the performancedrops considerably when testing on text from otherdomains (called target domains).In order to build models that perform well innew (target) domains we usually find two settings(Daum?e III, 2007): In the semi-supervised settingthe goal is to improve the system trained on thesource domain using unlabeled data from the tar-get domain, and the baseline is that of the systemc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.trained on the source domain.
In the supervisedsetting, training data from both source and tar-get domains are used, and the baseline is providedby the system trained on the target domain.
Thesemi-supervised setting is the most attractive, as itwould save developers the need to hand-annotatetarget corpora every time a new domain is to beprocessed.The main goal of this paper is to use unlabeleddata in order to get better domain-adaptation re-sults for Word Sense Disambiguation (WSD) inthe semi-supervised setting.
Singular Value De-composition (SVD) has been shown to find corre-lations between terms which are helpful to over-come the scarcity of training data in WSD (Gliozzoet al, 2005).
This paper explores how this abilityof SVD can be applied to the domain-adaptation ofWSD systems, and we show that SVD and unla-beled data improve the results of two state-of-the-art WSD systems (k-NN and SVM).
For the sake ofthis paper we call this set of experiments the do-main adaptation scenario.In addition, we also perform some related exper-iments on just the target domain.
We use unlabeleddata in order to improve the results of a systemtrained and tested in the target domain.
These re-sults are complementary to the domain adaptationexperiments, and also provide an upperbound forsemi-supervised domain adaptation.
We call theseexperiments the target domain scenario.
Notethat both scenarios are semi-supervised, in that ourfocus is on the use of unlabeled data in addition tothe available labeled data.The experiments were performed on a publiclyavailable corpus which was designed to study theeffect of domain in WSD (Koeling et al, 2005).
Itcomprises 41 nouns closely related to the SPORTSand FINANCES domains with 300 examples foreach.
The 300 examples were drawn from theBritish National Corpus (Leech, 1992) (BNC), theSPORTS section of the Reuters corpus (Leech,171992), and the FINANCES section of Reuters inequal number.The paper is structured as follows.
Section 2 re-views prior work in the area.
Section 3 presents thedatasets used, and Section 4 the learning methods,including the application of SVD.
The experimen-tal results are presented in Section 5, for the semi-supervised domain adaptation scenario, and Sec-tion 6, for the target scenario.
Section 7 presentsthe discussion and Section 8 the conclusions andfuture work.2 Prior WorkDomain adaptation is a subject attracting moreand more attention.
In the semi-supervised set-ting, Blitzer et al (2006) use Structural Corre-spondence Learning and unlabeled data to adapta Part-of-Speech tagger.
They carefully select so-called ?pivot features?
to learn linear predictors,perform SVD on the weights learned by the pre-dictor, and thus learn correspondences among fea-tures in both source and target domains.
Our tech-nique also uses SVD, but we directly apply it to allfeatures, and thus avoid the need to define pivotfeatures.
In preliminary work we unsuccessfullytried to carry along the idea of pivot features toWSD.
Zelikovitz and Hirsh (2001) use unlabeleddata (so-called background knowledge) with La-tent Semantic Indexing (also based on SVD) on aText Classification task with positive results.
Theyuse related unlabeled text and include it in theterm-by-document matrix to expand it and capturebetter the interesting properties of the data.
Theirapproach is similar to our SMA method in Section4.2).In the supervised setting, a recent paper byDaum?e III (2007) shows that, using a very simplefeature augmentation method coupled with Sup-port Vector Machines, he is able to effectivelyuse both labeled target and source data to pro-vide the best results in a number of NLP tasks.His method improves or equals over previously ex-plored more sophisticated methods (Daum?e III andMarcu, 2006; Chelba and Acero, 2004).Regarding WSD, some initial works made ba-sic analysis of the particular issues.
Escudero etal.
(2000) tested the supervised adaptation set-ting on the DSO corpus, which had examples fromthe Brown corpus and Wall Street Journal cor-pus.
They found that the source corpus did nothelp when tagging the target corpus, showing thattagged corpora from each domain would suffice,and concluding that hand tagging a large generalcorpus would not guarantee robust broad-coverageWSD.
Agirre and Mart?
?nez (2000) also used theDSO corpus in the supervised setting to show thattraining on a subset of the source corpora that istopically related to the target corpus does allowfor some domain adaptation.
Their work used thefact that the genre tags of Brown allowed to detectwhich parts of the corpus were related to the targetcorpus.More recently, Koeling et al (2005) presentedan unsupervised system to learn the predominantsenses of particular domains.
Their system wasbased on the use of a similarity thesaurus inducedfrom the domain corpus and WordNet.
They usedthe same dataset as in this paper for evaluation.Chan and Ng (2007) performed supervised domainadaptation on a manually selected subset of 21nouns from the DSO corpus.
They used activelearning, count-merging, and predominant senseestimation in order to save target annotation ef-fort.
They showed that adding just 30% of the tar-get data to the source examples the same precisionas the full combination of target and source datacould be achieved.
They also showed that usingthe source corpus allowed to significantly improveresults when only 10%-30% of the target corpuswas used for training.
No data was given about theuse of both tagged corpora.Though not addressing domain adaptation, otherworks on WSD also used SVD and are closely re-lated to the present paper.
Gliozzo et al (2005)used SVD to reduce the space of the term-to-document matrix, and then computed the similaritybetween train and test instances using a mappingto the reduced space (similar to our SMA methodin Section 4.2).
They combined other knowledgesources into a complex kernel using SVM.
Theyreport improved performance on a number of lan-guages in the Senseval-3 lexical sample dataset.Our present paper differs from theirs in that wepropose an additional method to use SVD (the OMTmethod, Section 4.2), and that we evaluate the con-tribution of unlabeled data and SVD in isolation,leaving combination for future work.Ando (2006) used Alternative Structured Op-timization, which is closely related to StructuralLearning (cited above).
He first trained one lin-ear predictor for each target word, and then per-formed SVD on 7 carefully selected submatrices18of the feature-to-predictor matrix of weights.
Thesystem attained small but consistent improvements(no significance data was given) on the Senseval-3 lexical sample datasets using SVD and unlabeleddata.We have previously shown (Agirre et al, 2005;Agirre and Lopez de Lacalle, 2007) that perform-ing SVD on the feature-to-documents matrix is asimple technique that allows to improve perfor-mance with and without unlabeled data.
The useof several k-NN classifiers trained on a number ofreduced and original spaces was shown to rank firstin the Senseval-3 dataset and second in the Se-mEval 2007 competition.
The present work ex-tends our own in that we present a comprehensivestudy on a domain adaptation dataset, producingadditional insight on our method and the relationbetween SVD, features and unlabeled data.3 Data setsThe dataset we use was designed for domain-related WSD experiments by Koeling et al (2005),and is publicly available.
The examples comefrom the BNC (Leech, 1992) and the SPORTS andFINANCES sections of the Reuters corpus (Roseet al, 2002), comprising around 300 examples(roughly 100 from each of those corpora) for eachof the 41 nouns.
The nouns were selected be-cause they were salient in either the SPORTS orFINANCES domains, or because they had senseslinked to those domains.
The occurrences werehand-tagged with the senses from WordNet (WN)version 1.7.1 (Fellbaum, 1998).Compared to the DSO corpus used in prior work(cf.
Section 2) this corpus has been explicitly cre-ated for domain adaptation studies.
DSO con-tains texts coming from the Brown corpus and theWall Street Journal, but the texts are not classi-fied according to specific domains (e.g.
Sports, Fi-nances), which make DSO less suitable to studydomain adaptation.In addition to the labeled data, we also useunlabeled data coming from the three sourcesused in the labeled corpus: the ?written?
part ofthe BNC (89.7M words), the FINANCES part ofReuters (117,734 documents, 32.5M words), andthe SPORTS part (35,317 documents, 9.1M words).4 Learning features and methodsIn this section, we review the learning features, thetwo methods to apply SVD, and the two learningalgorithms used in the experiments.4.1 Learning featuresWe relied on the usual features used in previousWSD work, grouped in three main sets.
Localcollocations comprise the bigrams and trigramsformed around the target word (using either lem-mas, word-forms, and PoS tags1), those formedwith the previous/posterior lemma/word-form inthe sentence, and the content words in a ?4-wordwindow around the target.
Syntactic dependen-cies2use the object, subject, noun-modifier, prepo-sition, and sibling lemmas, when available.
Fi-nally, Bag-of-words features are the lemmas ofthe content words in the whole context, plus thesalient bigrams in the context (Pedersen, 2001).4.2 Features from the reduced spaceApart from the original space of features, we havethe so called SVD features, obtained from theprojection of the feature vectors into the reducedspace (Deerwester et al, 1990).
Basically, we seta term-by-document or feature-by-example matrixM from the corpus (see section below for moredetails).
SVD decomposes it into three matrices,M = U?VT.
If the desired number of dimensionsin the reduced space is p, we select p rows from ?and V , yielding ?pand Vprespectively.
We canmap any feature vector~t (which represents either atrain or test example) into the p-dimensional spaceas follows:~tp=~tTVp??1p.
Those mapped vectorshave p dimensions, and each of the dimensions iswhat we call a SVD feature.
We can now use themapped vectors (~tp) to train and test any learningmethod, as usual.
We have explored two differentvariants in order to build the reduced matrix andobtain the SVD features, as follows.Single Matrix for All target words (SVD-SMA).
The method comprises the following steps:(i) extract bag-of-word features (terms in this case)from unlabeled corpora, (ii) build the term-by-document matrix, (iii) decompose it with SVD, and(iv) project the labeled data (train/test).
This tech-nique is very similar to previous work on SVD(Gliozzo et al, 2005; Zelikovitz and Hirsh, 2001).The dimensionality reduction is performed once,over the whole unlabeled corpus, and it is then ap-plied to the labeled data of each word.
The reduced1The PoS tagging was performed with the fnTBL toolkit(Ngai and Florian, 2001)2This software was kindly provided by David Yarowsky?sgroup, from Johns Hopkins University.19space is constructed only with terms, which corre-spond to bag-of-words features, and thus discardsthe rest of the features.
Given that the WSD litera-ture has shown that all features, including local andsyntactic features, are necessary for optimal per-formance (Pradhan et al, 2007), we propose thefollowing alternative to construct the matrix.One Matrix per Target word (SVD-OMT).
Foreach word: (i) construct a corpus with its occur-rences in the labeled and, if desired, unlabeled cor-pora, (ii) extract all features, (iii) build the feature-by-example matrix, (iv) decompose it with SVD,and (v) project all the labeled training and test datafor the word.
Note that this variant performs oneSVD process for each target word separately, henceits name.
We proposed this technique in (Agirre etal., 2005).An important parameter when doing SVD is thenumber of dimensions in the reduced space (p).We tried two different values for p (25 and 200) inthe BNC domain, and the results were consistentin that 25 performed better for SVD-OMT and 200better for SVD-SMA.
Those values were chosen fortesting in the SPORTS and FINANCES domains, i.e.25 for SVD-OMT and 200 for SVD-SMA.4.3 Building MatricesThe methods in the previous section can be appliedto the following matrices M :?
TRAIN: The matrix comprises features fromlabeled train examples alone.
This matrix canonly be used to obtain OMT features.?
TRAIN ?
BNC: In addition to TRAIN, wematrix also includes unlabeled examples fromthe source corpus (BNC).
Both OMT and SMAfeatures can be obtained.?
TRAIN ?
{SPORTS,FINANCES}: Like theprevious, but using unlabeled examples fromone of the target corpora (FINANCES orSPORTS) instead.
Both OMT and SMA featurecan be obtained.Based on previous work (Agirre et al, 2005), weused 50% of the respective unlabeled corpora forOMT features, and the whole corpora for SMA.4.4 Learning methodsWe used two well known classifiers, Support Vec-tor Machines (SVM) and k-Nearest Neighbors (k-NN).
Regarding SVM we used linear kernels imple-mented in SVM-Light (Joachims, 1999).
We esti-mated the soft margin (C) for each feature spaceand each word using a greedy process in a prelim-inary experiment on the source training data usingcross-validation.
The same C value was used in therest of the settings.k-NN is a memory based learning method,where the neighbors are the k most similar la-beled examples to the test example.
The similarityamong instances is measured by the cosine of theirvectors.
The test instance is labeled with the senseobtaining the maximum the sum of the weightedvote of the k most similar contexts.
We set k to5 based on previous results (Agirre and Lopez deLacalle, 2007).5 Domain adaptation scenarioIn this scenario we try to adapt a general purposesupervised WSD system trained on the source cor-pus (BNC) to a target corpus (either SPORTS or FI-NANCES) using unlabeled corpora only.5.1 Experimental resultsTable 1 shows the precision results for this sce-nario.
Note that all methods have full coverage,i.e.
they return a sense for all test examples, andtherefore precision suffices to compare among sys-tems.
We have computed significance ranges forall results in this paper using bootstrap resam-pling (Noreen, 1989).
F1scores outside of theseintervals are assumed to be significantly differentfrom the related F1score (p < 0.05).The table has two main parts, each regardingto one of the target domains, SPORTS and FI-NANCES.
The use of two target domains allows totest whether the methods behave similarly in bothdomains.
The columns denote the classifier andSVD method used: the MFS column correspondsto the most frequent sense, k-NN-ORIG (SVM-ORIG) corresponds to performing k-NN (SVM) onthe original feature space, k-NN-OMT (SVM-OMT)corresponds to k-NN (SVM) on the reduced dimen-sions of the OMT strategy, and k-NN-SMA (SVM-SMA) corresponds to k-NN (SVM) on the reduceddimensions of the SMA strategy (cf.
Section 4.2).The rows correspond to the matrix used for SVD(cf.
Section 4.3).
Note that some of the cells haveno result, because that combination is not applica-ble, e.g.
using the TRAIN ?
BNC in the originalspace.In the first row (TRAIN) of Table 1 we cansee that in both domains SVM on the originalspace outperforms k-NN with statistical signifi-20BNC?
SPORTSmatrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMATRAIN 39.0?1.3 51.7?1.3 53.0?1.6 - 53.9?1.3 47.4?1.5 -TRAIN ?
SPORTS - - 47.8?1.5 49.7?1.5 - 51.8?1.5 53.8?1.5TRAIN ?
BNC - - 61.4?1.4 57.1?1.5 - 57.1?1.6 57.2?1.5BNC?
FINANCESmatrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMATRAIN 51.2?1.6 60.4?1.6 62.5?1.4 - 62.9?1.6 59.4?1.5 -TRAIN ?
FINANCES - - 57.4?1.9 60.6?1.5 - 60.4?1.4 62.7?1.4TRAIN ?
BNC - - 65.9?1.5 68.3?1.4 - 67.0?1.3 66.8?1.5Table 1: Precision for the domain adaptation scenario: training on labeled source corpus, plus unlabeledcorpora.cance.
Those are the baseline systems.
On thesame row, working on the reduced space of theTRAIN matrix with OMT allows to improve the re-sults of k-NN, but not for SVM.Contrary to our expectations, adding target unla-beled corpora (TRAIN ?
SPORTS and TRAIN ?
FI-NANCES rows respectively) does not improve theresults over the baseline.
But using the source un-labeled data (TRAIN ?
BNC), we find that for bothdomains and in all four columns the results are sig-nificantly better than for the best baseline in bothSPORTS and FINANCES corpora.The best results on the TRAIN ?
BNC row de-pend on the domain corpus.
While k-NN-OMT ob-tains the best results for SPORTS, in FINANCESk-NN-SMA is best.
k-NN, in principle a weakermethod that SVM, is able to attain the same orsuperior performance than SVM on the reducedspaces.Table 3 summarizes the main results, and alsoshows the error reduction figures, which range be-tween 6.9% and 16.3%.
As the most importantconclusion, we want to stress that, in this sce-nario, we are able to build a very robust systemjust adding unlabeled source material, and that wefail to adapt to the domain using the target cor-pus.
These results are relevant to improve a genericWSD system to be more robust when ported to newdomains.5.2 Controlling sizeIn the original experiments reported in the previ-ous sections, the size of the unlabeled corpora wasnot balanced.
Due to the importance of the amountof unlabeled data, we performed two control ex-periments for the OMT and SMA matrices on thedomain adaptation scenario, focusing on the k-NNmethod.
Regarding OMT, we used the minimumnumber of instances per word between BNC andeach of the target domains.
The system obtained60.0 of precision using unlabeled data from BNCand 49.5 for SPORTS data (compared to 61.4 and47.8 in table 1, respectively).
We did the same inthe FINANCES domain, and we obtained 65.6 ofprecision for BNC and 54.4 for FINANCES (com-pared to 65.7 and 57.4 in table 1, respectively).
Al-though the contribution of BNC unlabeled data isslightly lower in this experiment, due to the smalleramount of data, it still outperforms the target unla-beled data by a large margin.In the case of the SMA matrix, we used 25% ofthe BNC, which is comparable to the SPORTS andFINANCES sizes.
The results, 56.9 of precision inSPORTS domain and 68.1 in FINANCES (comparedto 57.1 and 68.3 in table 1, respectively), confirmthat the size is not an important factor for SMA ei-ther.6 Target scenarioIn this second scenario we focus on the target do-main.
We train and test on the target domain, anduse unlabeled data in order to improve the result.The goal of these experiments is to check the be-havior of our method when applied to the targetdomain, in order to better understand the results onthe domain adaptation scenario.
They also providean upperbound for semi-supervised domain adap-tation.6.1 Experimental resultsThe results are presented in table 2.
All experi-ments in this section have been performed using3-fold cross-validation.
Again, we have full cover-age in all cases, and the significance ranges corre-spond to the 95% confidence level.
The table hastwo main parts, each regarding to one of the targetdomains, SPORTS and FINANCES.
As in Table 1,the columns specify the classifier and SVD methodused, and the rows correspond to the matrices used21SPORTS?
SPORTS (xval)matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMATRAIN 77.8?1.2 84.5?1.0 85.0?1.1 - 85.1?1.0 81.0?1.5 -TRAIN ?
SPORTS - - 86.1?0.9 82.7?1.1 - 85.1?1.1 80.3?1.5TRAIN ?
BNC - - 84.4?1.0 80.4?1.5 - 84.3?0.9 79.8?1.2FINANCES?
FINANCES (xval)matrix configuration MFS k-NN-ORIG k-NN-OMT k-NN-SMA SVM-ORIG SVM-OMT SVM-SMATRAIN 82.3?1.3 87.1?1.0 87.4?1.0 - 87.0?1.0 85.5?1.1 -TRAIN ?
SPORTS - - 87.8?0.8 84.3?1.4 - 86.4?0.9 82.9?1.1TRAIN ?
BNC - - 87.4?1.2 83.5?1.2 - 85.7?0.9 84.3?1.1Table 2: Precision for the target scenario: training on labeled target corpora, plus unlabeled corpora.to obtain the features.Table 2 shows that k-NN-OMT using the tar-get corpus (SPORTS and FINANCES, respectively)slightly improves over the k-NN-ORIG and SVM-ORIG classifiers, with significant difference in theSPORTS domain.
Contrary to the results on theprevious section, the source unlabeled corpus de-grades performance, but the target corpus does al-low for small improvements.
Note that, in this sce-nario, both SVM and k-NN perform similarly in theoriginal space, but only k-NN is able to profit fromthe reduced space.
Table 3 summarizes the bestresult, alongside the error reduction.The results of these experiments allow to con-trast both scenarios, and to get deeper insight aboutthe relation between the labeled and unlabeled datawhen performing SVD, as we will examine in thenext section.7 DiscussionThe main contribution of this paper is to showthat we obtain robustness when faced with do-main shifts using a semi-supervised strategy.
Weshow that we can obtain it using a large, general,unlabeled corpus.
Note that our semi-supervisedmethod to attain robustness for domain shifts isvery cost-effective, as it does not require costlyhand-tagged material nor even large numbers ofunlabeled data from each target domain.
Theseresults are more valuable given the lack of sub-stantial positive results on the literature on semi-supervised or supervised domain adaptation forWSD (Escudero et al, 2000; Mart?
?nez and Agirre,2000; Chan and Ng, 2007).Compared to other settings, our semi-supervisedresults improve over the completely unsupervisedsystem in (Koeling et al, 2005), which had 43.7%and 49.9% precision for the SPORTS and FI-NANCES domains respectively, but lag well behindthe target domain scenario, showing that there isstill room for improvement in the semi-supervisedsetting.While these results are based on a lexical sam-ple, and thus not directly generalizable to an all-words corpus, we think that they reflect the maintrends for nouns, as the 41 nouns where selectedamong those exhibiting domain dependence (Koel-ing et al, 2005).
We can assume, though it wouldbe needed to be explored empirically, that othernouns exhibiting domain independence would de-grade less when moving to other domains, and thuscorroborate the robustness effect we have discov-ered.The fact that we attain robustness rather than do-main adaptation proper deserves some analysis.
Inthe domain adaptation scenario only source unla-beled data helped, but the results on the target sce-nario show that it is the target unlabeled data whichis helping, and not the source one.
Given thatSVD basically finds correlations among features,it seems that constructing the term-by-document(or feature-by-example) matrix with the trainingdata and the unlabeled corpus related to the train-ing data is the key factor in play here.The reasons for this can be traced back as fol-lows.
Our source corpus is the BNC, which is abalanced corpus containing a variety of genres anddomains.
The 100 examples for each word thathave been hand-tagged were gathered at random,and thus cover several domains.
For instance, theOMT strategy for building the matrix extracts hun-dreds of other examples from the BNC, and whenSVD collapses the features into a reduced space,it effectively captures the most important corre-lations in the feature-by-example matrix.
Whenfaced with examples from a new domain, the re-duced matrix is able to map some of the featuresfound in the test example to those in the train ex-ample.
Such overlap is more difficult if only 100examples from the source domain are available.22SPORTS FINANCES sign.
E.R (%) method53.9?1.3 62.9?1.6 - - labeled source (SVM-ORIG: baseline )57.1?1.5 68.3?1.4 ++ 6.9/14.5 labeled source + SVD on unlabeled source (k-NN-SMA)61.4?1.4 65.9?1.5 ++ 16.3/8.1 labeled source + SVD on unlabeled source (k-NN-OMT)85.1?1.0 87.0?1.0 - - labeled target (SVM-ORIG: baseline)86.1?0.9 87.8?0.8 + 6.7/6.1 labeled target + SVD on unlabeled target (k-NN-OMT)Table 3: Summary with the most important results for the two scenarios (best results for each in bold).The significance column shows significance over baselines: ++ (significant in both target domains),+ (significant in a single domain).
The E.R column shows the error reduction in percentages over thebaseline methods.The unlabeled data and SVD process allow to cap-ture correlations among the features occurring inthe test data and those in the training data.On the other hand, we are discarding all originalfeatures, as we focus on the features from the re-duced space alone.
The newly found correlationscome at the price of possibly ignoring effectiveoriginal features, causing information loss.
Onlywhen the correlations found in the reduced spaceoutweigh this information loss do we get betterperformance on the reduced space than in the orig-inal space.
The experiment in Section 6 is impor-tant in that it shows that the improvement is muchsmaller and only significant in the target domainscenario, which is in accordance with the hypothe-sis above.
This information loss is a motivation forthe combination of the features from the reducedspace with the original features, which will be thefocus of our future work.Regarding the learning method and the twostrategies to apply SVD, the results show that k-NN profits from the reduced spaces more thanSVM, even if its baseline performance is lowerthan SVM.
Regarding the matrix building system,in the domain adaptation scenario, k-NN-OMT ob-tains the best results (with statistical significance)in the SPORTS corpus, and k-NN-SMA yields thebest results (with statistical significance) in the FI-NANCES domain.
Averaging over both domains,k-NN-OMT is best.
The target scenario results con-firm this trend, as k-NN-OMT is superior to k-NN-SMA in both domains.
These results are in ac-cordance with our previous experience on WSD(Agirre et al, 2005), where our OMT method gotbetter results than SMA and those of (Gliozzo etal., 2005) (who also use a method similar to SMA)on the Senseval-3 lexical sample.
While OMT re-duces the feature-by-example matrix of each tar-get word, SMA reduces a single term-by-documentmatrix.
SMA is able to find important correlationsamong similar terms in the corpus, but it misses therich feature set used by WSD systems, as it focuseson bag-of-words alone.
OMT on the other hand isable to find correlations between all features whichare relevant to the target word only.8 Conclusions and Future WorkIn this paper we explore robustness and domainadaptation issues for Word Sense Disambiguationusing SVD and unlabeled data.
We focus on thesemi-supervised scenario, where we train on thesource corpus (BNC), test on two target corpora(SPORTS and FINANCES sections of Reuters), andimprove the results using unlabeled data.Our method yields up to 16.3% error reductioncompared to SVM and k-NN on the labeled dataalone, showing the first positive results on domainadaptation for WSD.
In fact, we show that our re-sults are due to the use of a large, general, unla-beled corpus, and rather than domain-adaptationproper we show robustness in face of a domainshift.
This kind of robustness is even more cost-effective than semi-supervised domain adaptation,as it does not require large unlabeled corpora andrepeating the computations for each new target do-main.This paper shows that the OMT technique to ap-ply SVD that we proposed in (Agirre et al, 2005)compares favorably to SMA, which has been previ-ously used in (Gliozzo et al, 2005), and that k-NNexcels SVM on the features from the reduced space.We also show that the unlabeled data needs to berelated to the training data, and that the benefits ofour method are larger when faced with a domainshift (compared to test data coming from the samedomain as the training data).In the future, we plan to combine the featuresfrom the reduced space with the rest of features,either using a combination of k-NN classifiers(Agirre et al, 2005; Agirre and Lopez de Lacalle,2007) or a complex kernel (Gliozzo et al, 2005).23A natural extension of our work would be to applyour techniques to the supervised domain adapta-tion scenario.AcknowledgmentsWe wish to thank Diana McCarthy and Rob Koel-ing for kindly providing us the Reuters tagged cor-pora, David Mart?
?nez for helping us with the learn-ing features, and Walter Daelemans for his ad-vice on domain adaptation.
Oier Lopez de La-calle has a PhD grant from the Basque Govern-ment.
This work is partially funded by the Educa-tion Ministry (KNOW TIN2006-15049, OpenMTTIN2006-15307-C03-02) and the Basque CountryUniversity (IT-397-07).ReferencesAgirre, E. and O. Lopez de Lacalle.
2007.
UBC-ALM:Combining k-NN with SVD for WSD.
In Proceed-ings of the Fourth International Workshop on Se-mantic Evaluations (SemEval-2007).
Association forComputational Linguistics.Agirre, E., O. Lopez de Lacalle, and D.
Mart??nez.
2005.Exploring feature spaces with svd and unlabeled datafor Word Sense Disambiguation.
In Proceedings ofthe Conference on Recent Advances on Natural Lan-guage Processing (RANLP?05).Ando, R. Kubota.
2006.
Applying alternating structureoptimization to word sense disambiguation.
In Pro-ceedings of the 10th Conference on ComputationalNatural Language Learning (CoNLL).Blitzer, J., R. McDonald, and F. Pereira.
2006.
Domainadaptation with structural correspondence learning.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing.Chan, Yee Seng and Hwee Tou Ng.
2007.
Domainadaptation with active learning for word sense dis-ambiguation.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics.Chelba, C. and A. Acero.
2004.
Adaptation of maxi-mum entropy classifier: Little data can help a lot.
InProceedings of of th Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).Daum?e III, H. and D. Marcu.
2006.
Domain adaptationfor statistical classifiers.
Journal of Artificial Intelli-gence Research, 26:101?126.Daum?e III, H. 2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics.Deerwester, S., S. Dumais, G. Furnas, T. Landauer, andR.
Harshman.
1990.
Indexing by Latent SemanticAnalysis.
Journal of the American Society for Infor-mation Science.Escudero, G., L. M?arquez, and G. Rigau.
2000.
AnEmpirical Study of the Domain Dependence of Su-pervised Word Sense Didanbiguation Systems.
Pro-ceedings of the joint SIGDAT Conference on Empir-ical Methods in Natural Language Processing andVery Large Corpora, EMNLP/VLC.Fellbaum, C. 1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Gliozzo, A. M., C. Giuliano, and C. Strapparava.
2005.Domain Kernels for Word Sense Disambiguation.43nd Annual Meeting of the Association for Com-putational Linguistics.
(ACL-05).Joachims, T. 1999.
Making Large?Scale SVM Learn-ing Practical.
Advances in Kernel Methods ?
Sup-port Vector Learning, Cambridge, MA.
MIT Press.Koeling, R., D. McCarthy, and J. Carroll.
2005.Domain-specific sense distributions and predomi-nant sense acquisition.
In Proceedings of the HumanLanguage Technology Conference and Conferenceon Empirical Methods in Natural Language Process-ing.
HLT/EMNLP.Leech, G. 1992.
100 million words of English: theBritish National Corpus.
Language Research.Mart?
?nez, D. and E. Agirre.
2000.
One Sense per Col-location and Genre/Topic Variations.
Conference onEmpirical Method in Natural Language.Ngai, G. and R. Florian.
2001.
Transformation-BasedLearning in the Fast Lane.
Proceedings of the Sec-ond Conference of the North American Chapter ofthe Association for Computational Linguistics.Noreen, E. W. 1989.
Computer-Intensive Methods forTesting Hypotheses.
John Wiley & Sons.Pedersen, T. 2001.
A Decision Tree of Bigrams is anAccurate Predictor of Word Sense.
In Proceedingsof the Second Meeting of the North American Chap-ter of the Association for Computational Linguistics(NAACL-01).Pradhan, S., E. Loper, D. Dligach, and M. Palmer.2007.
Semeval-2007 task-17: English lexical sam-ple, srl and all words.
In Proceedings of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007).Rose, T. G., M. Stevenson, and M. Whitehead.
2002.The reuters corpus volumen 1 from yesterday?s newsto tomorrow?s language resources.
In Proceedingsof the Third International Conference on LanguageResources and Evaluation (LREC-2002).Zelikovitz, S. and H. Hirsh.
2001.
Using LSI for textclassification in the presence of background text.
InProceedings of CIKM-01, 10th ACM InternationalConference on Information and Knowledge Manage-ment.
US.24
