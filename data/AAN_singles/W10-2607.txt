Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 45?52,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsDomain Adaptation with Unlabeled Data for Dialog Act TaggingAnna Margolis1,2 Karen Livescu21Department of Electrical Engineering, University of Washington, Seattle, WA, USA.2TTI-Chicago, Chicago, IL, USA.amargoli@ee.washington.edu, klivescu@ttic.edu, mo@ee.washington.eduMari Ostendorf1AbstractWe investigate the classification of utter-ances into high-level dialog act categoriesusing word-based features, under condi-tions where the train and test data dif-fer by genre and/or language.
We han-dle the cross-language cases with ma-chine translation of the test utterances.We analyze and compare two feature-based approaches to using unlabeled datain adaptation: restriction to a shared fea-ture set, and an implementation of Blitzeret al?s Structural Correspondence Learn-ing.
Both methods lead to increased detec-tion of backchannels in the cross-languagecases by utilizing correlations betweenbackchannel words and utterance length.1 IntroductionDialog act (or speech act) tagging aims to labelabstract functions of utterances in conversations,such as Request, Floorgrab, or Statement; poten-tial applications include automatic conversationanalysis, punctuation transcription, and human-computer dialog systems.
Although some appli-cations require domain-specific tag sets, it is oftenuseful to label utterances based on generic tags,and several tag sets have been developed for thispurpose, e.g.
DAMSL (Core and Allen, 1997).Many approaches to automatic dialog act (DA)tagging assume hand-labeled training data.
How-ever, when building a new system it may be diffi-cult to find a labeled corpus that matches the tar-get domain, or even the language.
Even within thesame language, speech from different domains candiffer linguistically, and the same DA categoriesmight be characterized by different cues.
The do-main characteristics (face-to-face vs. telephone,two-party vs. multi-party, informal vs. agenda-driven, familiar vs. stranger) can influence boththe distribution of tags and word choice.This work attempts to use unlabeled target do-main data in order to improve cross-domain train-ing performance, an approach referred to as bothunsupervised and semi-supervised domain adapta-tion in the literature.
We refer to the labeled train-ing domain as the source domain.
We comparetwo adaptation approaches: a simple one basedon forcing the classifier to learn only on ?shared?features that appear in both domains, and a morecomplex one based on Structural CorrespondenceLearning (SCL) from Blitzer et al (2007).
Theshared feature approach has been investigated foradaptation in other tasks, e.g.
Aue and Gamon(2005) for sentiment classification and Dredze etal.
(2007) for parsing.
SCL has been used suc-cessfully for sentiment classification and part-of-speech tagging (Blitzer et al, 2006); here we in-vestigate its applicability to the DA classificationtask, using a multi-view learning implementationas suggested by Blitzer et al (2009).
In addition toanalyzing these two methods on a novel task, weshow an interesting comparison between them: inthis setting, both methods turn out to have a simi-lar effect caused by correlating cues for a particu-lar DA class (Backchannel) with length.We classify pre-segmented utterances based ontheir transcripts, and we consider only four high-level classes: Statement, Question, Backchannel,and Incomplete.
Experiments are performed us-ing all train/test pairs among three conversationalspeech corpora : the Meeting Recorder Dialog Actcorpus (MRDA) (Shriberg et al, 2004), Switch-board DAMSL (Swbd) (Jurafsky et al, 1997), andthe Spanish Callhome dialog act corpus (SpCH)(Levin et al, 1998).
The first is multi-party,face-to-face meeting speech; the second is topic-prompted telephone speech between strangers;and the third is informal telephone speech betweenfriends and family members.
The first two are inEnglish, while the third is in Spanish.
When thesource and target domains differ in language, we45apply machine translation to the target domain toconvert it to the language of the source domain.2 Related WorkAutomatic DA tagging across domain has beeninvestigated by a handful of researchers.
Webband Liu (2008) investigated cross-corpus train-ing between Swbd and another corpus consist-ing of task-oriented calls, although no adaptationwas attempted.
Similarly, Rosset et al (2008)reported on recognition of task-oriented DA tagsacross domain and language (French to English)by using utterances that had been pre-processedto extract entities.
Tur (2005) applied supervisedmodel adaptation to intent classification acrosscustomer dialog systems, and Guz et al (2010)applied supervised model adaptation methods forDA segmentation and classification on MRDA us-ing labeled data from both MRDA and Swbd.Most similar to our work is that of Jeong et al(2009), who compared two methods for semi-supervised adaptation, using Swbd/MRDA as thesource training set and email or forums corpora asthe target domains.
Both methods were based onincorporating unlabeled target domain examplesinto training.
Success has also been reported forself-training approaches on same-domain semi-supervised learning (Venkataraman et al, 2003;Tur et al, 2005).
We are not aware of prior workon cross-lingual DA tagging via machine transla-tion, although a translation approach has been em-ployed for cross-lingual text classification and in-formation retrieval, e.g.
Bel et al (2003).In recent years there has been increasing in-terest in domain adaptation methods based onunlabeled target domain data.
Several kinds ofapproaches have been proposed, including self-training (Roark and Bacchiani, 2003), instanceweighting (Huang et al, 2007), change of featurerepresentation (Pan et al, 2008), and clusteringmethods (Xing et al, 2007).
SCL (Blitzer et al,2006) is one feature representation approach thathas been effective on certain high-dimensionalNLP problems, including part-of-speech taggingand sentiment classification.
SCL uses unlabeleddata to learn feature projections that tie togethersource and target features via their correlationswith features shared between domains.
It first se-lects ?pivot features?
that are common in both do-mains; next, linear predictors for those features arelearned on all the other features.
Finally, singularvalue decomposition (SVD) is performed on thecollection of learned linear predictors correspond-ing to different pivot features.
Features that tendto get similar weights in predicting pivot featureswill be tied together in the SVD.
By learning onthe SVD dimensions, the source-trained classifiercan put weight on target-only features.3 MethodsOur four-class DA problem is similar to problemsstudied in other work, such as Tur et al (2007)who used five classes (ours plus Floorgrab/hold).When defining a mapping from each corpus?
tagset to the four high-level classes, our goal was totry to make the classes similarly defined acrosscorpora.
Note that the Incomplete category is de-fined in Swbd-DAMSL to include only utterancestoo short to determine their DA label (e.g., just afiller word).
Thus, for our work the MRDA In-complete category excludes utterances also taggedas Statement or Question; it includes those con-sisting of just a floor-grab, hold or filler word.For classification we used an SVM with linearkernel, with L2 regularization and L1 loss, as im-plemented in the Liblinear package (Fan et al,2008) which uses the one-vs.-rest configurationfor multiclass classification.
SVMs have been suc-cessful for supervised learning of DAs based onwords and other features (Surendran and Levow,2006; Liu, 2006).
Features are derived from thehand transcripts, which are hand-segmented intoDA units.
Punctuation and capitalization are re-moved so that our setting corresponds to classifi-cation based on (perfect) speech recognition out-put.
The features are counts of unigrams, bi-grams, and trigrams that occur at least twice inthe train set, including beginning/end-of-utterancetags (?s?, ?/s?
), and a length feature (total num-ber of words, z-normalized across the trainingset).
Note that some previous work on DA tag-ging has used contextual features from surround-ing utterances, or Markov models for the DA se-quence.
In addition, some work has used prosodicor other acoustic features.
The work of Stolckeet al (2000) found benefits to using Markov se-quence models and prosodic features in additionto word features, but those benefits were relativelysmall, so for simplicity our experiments here useonly word features and classify utterances in iso-lation.We used Google Translate to derive English46translations of the Spanish SpCH utterances, andto derive Spanish translations of the English Swbdand MRDA utterances.
Of course, translations arefar from perfect; DA classification performancecould likely be improved by using a translationsystem trained on spoken dialog.
For instance,Google Translate often failed on certain words like?i?
that are usually capitalized in text.
Even so,when training and testing on translated utterances,the results with the generic system are surprisinglygood.The results reported below used the standardtrain/test splits provided with the corpora: MRDAhad 51 train meetings/11 test; Swbd had 1115 trainconversations/19 test; SpCH had 80 train conver-sations/20 test.
The SpCH train set is the smallestat 29k utterances.
To avoid issues of differing trainset size when comparing performance of differentmodels, we reduced the Swbd and MRDA trainsets to the same size as SpCH using randomly se-lected examples from the full train sets.
For eachadaptation experiment, we used the target domaintraining set as the unlabeled data, and report per-formance on the target domain test set.
The testsets contain 4525, 15180, and 3715 utterances forSwbd, MRDA, and SpCH respectively.4 ResultsTable 1 shows the class proportions in the trainingsets for each domain.
MRDA has fewer Backchan-nels than the the others, which is expected sincethe meetings are face-to-face.
SpCH has fewer In-completes and more Questions than the others; thereasons for this are unclear.
Backchannels havethe shortest mean length (less than 2 words) in alldomains.
Incompletes are also short, while State-ments have the longest mean length.
The meanlengths of Statements and Questions are similarin the English corpora, but are shorter in SpCH.
(This may point to differences in how the utter-ances were segmented; for instance Swbd utter-ances can span multiple turns, although 90% areonly one turn long.
)Because of the high class skew, we consider twodifferent schemes for training the classifiers, andreport different performance measures for each.To optimize overall accuracy, we use basic un-weighted training.
To optimize average per-classrecall (weighted equally across all classes), we useweighted training, where each training example isweighted inversely to its class proportion.
We op-timize the regularization parameter using a sourcedomain development set corresponding to eachtraining set.
Since the optimum values are closefor all three domains, we choose a single value forall the accuracy classifiers and a single value forall the per-class recall classifiers.
(Different valuesare chosen for different feature types correspond-ing to the different adaptation methods.)Inc.
Stat.
Quest.
Back.Swbd 8.1% 67.1% 5.8% 19.1%MRDA 10.7% 67.9% 7.5% 14.0%SpCH 5.7% 60.6% 12.1% 21.7%Table 1: Proportion of utterances in eachDA category (Incomplete, Statement, Question,Backchannel) in each domain?s training set.Table 2 gives baseline performance for all train-test pairs, using translated versions of the test setwhen the train set differs in language.
It also liststhe in-domain results using translated (train andtest) data, and results using the adaptation methods(which we discuss below).
Figure 1 shows detailsof the contribution of each class to the average per-class recall; bar height corresponds to the secondcolumn in Table 2.4.1 Baseline performance and analysisWe observe first that translation does not have alarge effect on in-domain performance; degrada-tion occurs primarily in Incompletes and Ques-tions, which depend most on word order and there-fore might be most sensitive to ordering differ-ences in the translations.
We conclude that it ispossible to perform well on the translated test setswhen the training data is well matched.
However,cross-domain performance degradation is muchworse between pairs that differ in language thanbetween the two English corpora.We now describe three kinds of issues contribut-ing to cross-domain domain degradation, whichwe observed anecdotally.
First, some highly im-portant words in one domain are sometimes miss-ing entirely from another domain.
This issue ap-pears to have a dramatic effect on Backchanneldetection across languages: when optimizing foraverage per-class recall, the English-trained clas-sifiers detect about 20% of the Spanish translatedBackchannels and the Spanish classifier detectsa little over half of the English ones, while theyeach detect more than 80% in their own domain.47train set Acc (%) Avg.
Rec.
(%)Test on SwbdSwbd 89.2 84.9Swbd translated 86.7 80.4MRDA baseline 86.4 78.0MRDA shared only 85.7* 77.7MRDA SCL 81.8* 69.6MRDA length only 78.3* 51.4SpCH baseline 74.5 57.2SpCH shared only 77.4* 64.2SpCH SCL 76.8* 64.8SpCH length only 77.7* 48.2majority 67.7 25.0Test on MRDAMRDA 83.8 80.5MRDA translated 80.5 74.7Swbd baseline 81.0 71.6Swbd shared only 80.1* 72.1Swbd SCL 75.6* 68.1Swbd length only 68.6* 44.9SpCH baseline 66.9 50.5SpCH shared only 66.8 52.1SpCH SCL 66.1* 58.4SpCH length only 68.3* 44.6majority 65.2 25.0Test on SpCHSpCH 83.1 72.8SpCH translated 82.4 71.3Swbd baseline 63.8 41.1Swbd shared only 66.2* 50.9Swbd SCL 68.2* 47.2Swbd length only 72.6* 43.6MRDA baseline 65.1 42.9MRDA shared only 65.5 51.2MRDA SCL 67.6* 50.9MRDA length only 72.6* 44.7majority 65.3 25.0Table 2: Overall accuracy and average per-classrecall on each test set, using in-domain, in-domaintranslated, and cross-domain training.
Starred re-sults under the accuracy column are significantlydifferent from the corresponding cross-domainbaseline under McNemar?s test (p < 0.05).
(Sig-nificance is not calculated for the average per-classrecall column.)
?Majority?
classifies everything asStatement.The reason for the cross-domain drop is that manybackchannel words in the English corpora (uhhuh,right, yeah) do not overlap with those in the Span-train Swbd train MRDA train SpCH020406080Test on Swbdave.per?classrecall(%)in?domainin?domain trans.baselnsharedSCLlengthbaselnsharedSCLlengthISQBtrain MRDA train Swbd train SpCH020406080Test on MRDAin?domainin?domain trans.baselnsharedSCLlengthbaselnsharedSCLlengthtrain SpCH train Swbd train MRDA020406080Test on SpCHin?domainin?domain trans.baselnsharedSCLlengthbaselnsharedSCLlengthFigure 1: Per-class recall of weighted classifiersin column 2 of Table 2.
Bar height representsaverage per-class recall; colors indicate contribu-tion of each class: I=incomplete, S=statement,Q=question, B=backchannel.
(Maximum possiblebar height is 100%, each color 25%).ish corpora (mmm, s?
?, ya) even after translation?for example, ?ya?
becomes ?already?, ?s???
be-comes ?yes?, ?right?
becomes ?derecho?, and ?uh-huh?, ?mmm?
are unchanged.A second issue has to do with different kindsof utterances found in each domain, which some-times lead to different relationships between fea-tures and class label.
This is sometimes causedby the translation system; for example, utterancesstarting with ?es que .
.
.?
are usually statementsin SpCH, but without capitalization the translatoroften gives ?is that .
.
.?.
Since ??s??is?that?
isa cue feature for Question in English, these utter-ances are usually labeled as Question by the En-glish domain classifiers.
The existence of differ-ent types of utterances can result in sets of featuresthat are more highly correlated in one domain thanthe other.
In both Swbd and translated SpCH, ut-terances containing the trigram ??s??but??/s??
aremost likely to be in the Incomplete class.
In Swbd,the bigram ?but??/s??
rarely occurs outside of thattrigram, but in SpCH it sometimes occurs at the48end of long (syntactically-incomplete) Statements,so it corresponds to much lower likelihood for theIncomplete class.The last issue concerns utterances whose truelabel probabilities given the word sequence arenot the same across domains.
We distinguish twosuch kinds utterances.
The first are due to classdefinition differences across domains and anno-tators, e.g., long statements or questions that arealso incomplete are more often labeled Incompletein SpCH and Swbd than in MRDA.
The secondkind are utterances whose class labels are not com-pletely determined by their word sequence.
Tominimize error rate the classifier should label anutterance with its most frequent class, but that maydiffer across domains.
For example, ?yes?
can beeither a Statement of Backchannel; in the Englishcorpora, it is most likely to be a Statement (?yeah?is more commonly used for Backchannels).
How-ever, ?s???
is most likely to be a Backchannel inSpCH.
To measure the effect of differing labelprobabilities across domains, we trained ?domain-general?
classifiers using concatenated trainingsets for each pair of domains.
We found that theyperformed about the same or only slightly worsethan domain-specific models, so we conclude thatthis issue is likely only a minor effect.4.2 Adaptation using shared features onlyIn the cross-language domain pairs, some dis-criminative features in one domain are missingin the other.
By removing all features from thesource domain training utterances that are not ob-served (twice) in the target domain training data,we force the classifier to learn only on featuresthat are present in both domains.
As seen inFigure 1, this had the effect of improving re-call of Backchannels in the four cross-languagecases.
Backchannels are the second-most frequentclass after Statements, and are typically short inall domains.
Many typical Backchannel wordsare domain-specific; by removing them from thesource data, we force the classifier to attempt todetect Backchannels based on length alone.
Theresulting classifier has a better chance of recog-nizing target domain Backchannels that lack thesource-only Backchannel words.
At the sametime, it mistakes many other short utterances forBackchannels, and does particularly worse on In-completes, for which length is also strong cue.Although average per-class recall improved in allfour cross-language cases, total accuracy only im-proved significantly in two of those cases, andfor the Swbd/MRDA pair, accuracy got signifi-cantly worse.
The effect on the one-vs.-rest com-ponent classifiers was mixed: for some (State-ment and some Backchannel classifiers in thecross-language cases), accuracy improved, whilein other cases it decreased.As noted above, the shared feature approachwas investigated by Aue and Gamon (2005), whoargued that its success depends on the assump-tion that class/feature relationships be the sameacross domains.
However, we argue here that thesuccess of this method requires stronger assump-tions about both the relationship between domainsand the correlations between domain-specific andshared features.
Consider learning a linear modelon either the full source domain feature set or thereduced shared feature set.
In general, the co-efficients for a given feature will be different ineach model?in the reduced case, the coefficientsincorporate correlation information and label pre-dictive information for the removed (source-only)features.
This is potentially useful on the tar-get domain, provided that there exist analogous,target-only features that have similar correlationswith the shared features, and similar predictive co-efficients.For example, consider the discriminative sourceand target features ?uhhuh?
and ?mmm,?
whichare both are correlated with a shared, noisier, fea-ture (length).
Forcing the model to learn only onthe shared, noisy feature incorporates correlationinformation about ?uhhuh?, which is similar tothat of ?mmm?.
Thus, the reduced model is poten-tially more useful on the target domain, comparedto the full source domain model which might notput weight on the noisy feature.
On the other hand,the approach is inappropriate in several other sce-narios.
For one, if the target domain utterancesactually represent samples from a subspace of thesource domain, the absence of features is informa-tive: the fact that an utterance does not contain??s??verdad??/s?
?, for instance, might mean thatit is less likely to be a Question, even if none ofthe target domain utterances contain this feature.4.3 Adaptation using SCLThe original formulation of SCL proposed predict-ing pivot features using the entire feature set, ex-cept for those features perfectly correlated with49the pivots (e.g., the pivots themselves).
Our ex-periments with this approach found it unsuitablefor our task, since even after removing the pivotsthere are many features which remain highly cor-related with the pivots due to overlapping n-grams(i-love vs. love).
The number of features that over-lap with pivots is large, so removing these wouldlead to few features being included in the projec-tions.
Therefore, we adopted the multi-view learn-ing approach suggested by Blitzer et al (2009).We split the utterances into two parts; pivot fea-tures in the first part were predicted with all thefeatures in the second, and vice versa.
We experi-mented with splitting the utterances in the middle,but found that since the number of words in thefirst part (nearly) predicts the number in the sec-ond part, all of the features in the first part werepositively predictive of pivots in the second partso the main dimension learned was length.
In theresults presented here, the first part consists of thefirst word only, and the second part is the rest ofthe utterance.
(All utterances in our experimentshave at least one word.)
Pivot features are selectedin each part and predicted using a least-squareslinear regression on all features in the other part.We used the SCL-MI method of Blitzer et al(2007) to select pivot features, which requires thatthey be common in both domains and have highmutual information (MI) with the class (accordingto the source labels.)
We selected features that oc-curred at least 10 times in each domain and werein the top 500 ranked MI features for any of thefour classes; this resulted in 78-99 first-part piv-ots and 787-910 second-part pivots (depending onthe source-target pair).
We performed SVD onthe learned prediction weights for each part sep-arately, and the top (at most) 100 dimensions wereused to project utterances on each side.In all train-test pairs, the first dimension of thefirst part appeared to distinguish short utterancewords from long ones.
Such short-utterance wordsincluded backchannels from both domains, in ad-dition to acknowledgments, exclamations, swearwords and greetings.
An analogous dimension ex-isted in the second part, which captured words cor-related with short utterances greater than one word(right, really, interesting).
The other dimensions ofboth domains were difficult to interpret.We experimented with using the SCL fea-tures together with the raw features (n-grams andlength), as suggested by (Blitzer et al, 2006).
Asin (Blitzer et al, 2006), we found it necessary toscale up the SCL features to increase their utiliza-tion in the presence of the raw features; however,it was difficult to guess the optimal scaling factorwithout having access to labeled target data.
Theresults here use SCL features only, which also al-lows us to more clearly investigate the utility ofthose features and to compare them with the otherfeature sets.The most notable effect was an improvementin Backchannel recall, which occurred under bothweighted and unweighted training.
In addition,there was high confusability between Statementsand the other classes, and more false detectionsof Backchannels.
When optimizing for accuracy,SCL led to an improvement in accuracy in threeof the four cross-language cases.
When optimiz-ing for average per-class recall, it led to improve-ment in all cross-language cases; however, re-call of Statements went down dramatically in allcases.
In addition, while there was no clear ben-efit of the SCL vs. the shared-feature method onthe cross-language cases, the SCL approach didmuch worse than the shared-feature approach onthe Swbd/MRDA pair, causing large degradationfrom the baseline.As we have noted, utterance length appearsto underlie the improvement seen in the cross-language performance for both the SCL andshared-feature approaches.
Therefore, we includeresults for a classifier based only on the lengthfeature.
Optimizing for accuracy, this methodachieves the highest accuracy of all methods inthe cross-language pairs.
(It does so by classifyingeverything as Statement or Backchannel, althoughwith weighted training, as shown in Figure 1, itgets some Incompletes.)
However, under weightedclass training, the average per-class recall of thismethod is much worse than the shared-feature andSCL approaches.Comparison with other SCL tasks Althoughwe basically take a text classification approach tothe problem of dialog act tagging, our problem dif-fers in several ways from the sentiment classifi-cation task in Blitzer et al (2007).
In particular,utterances are much shorter than documents, andwe use position information via the start/end-of-sentence tags.
Some important DA cue features(such as the value of the first word) are mutuallyexclusive rather than correlated.
In this way ourproblem resembles the part-of-speech tagging task50(Blitzer et al, 2006), where the category of eachword is predicted using values of the left, right,and current word token.
In fact, that work useda kind of multi-view learning for the SCL projec-tion, with three views corresponding to the threeword categories.
However, our problem essen-tially uses a mix of bag-of-words and position-based features, which poses a greater challengesince there is no natural multi-view split.
The ap-proach described here suffers from the fact that itcannot use all the features available to the base-line classifier?bigrams and trigrams spanning thefirst and second words are left out.
It also suffersfrom the fact that the first-word pivot feature set isextremely small?a consequence of the small setof first words that occur at least 10 times in the29k-utterance corpora.5 ConclusionsWe have considered two approaches for domainadaptation for DA tagging, and analyzed theirperformance for source/target pairs drawn fromthree different domains.
For the English domains,the baseline cross-domain performance was quitegood, and both adaptation methods generally ledto degradation over the baseline.
For the cross-language cases, both methods were effective at im-proving average per-class recall, and particularlyBackchannel recall.
SCL led to significant accu-racy improvement in three cases, while the sharedfeature approach did so in two cases.
On theother hand, SCL showed poor discrimination be-tween Statements and other classes, and did worseon the same-language pair that had little cross-domain degradation.
Both methods work by tak-ing advantage of correlations between shared anddomain-specific class-discriminative features.
Un-fortunately in our task, membership in the rareclasses is often cued by features that are mutuallyexclusive, e.g., the starting n-gram for Questions.Both methods might therefore benefit from addi-tional shared features that are correlated with thesen-grams, e.g., sentence-final intonation for Ques-tions.
(Indeed, other work on semi-supervisedDA tagging has used a richer feature set: Jeonget al (2009) included parse, part-of-speech, andspeaker sequence information, and Venkataramanet al (2003) used prosodic information, plus asequence-modeling framework.)
From the taskperspective, an interesting result is that machinetranslation appears to preserve most of the dialog-act information, in that in-domain performance issimilar on original and translated text.AcknowledgmentsWe thank Sham Kakade for suggesting the multi-view SCL method based on utterance splits andfor many other helpful discussions, as well as JohnBlitzer for helpful discussions.
We thank the threereviewers for their useful comments.This research was funded by the Office ofthe Director of National Intelligence (ODNI), In-telligence Advanced Research Projects Activity(IARPA).
All statements of fact, opinion or con-clusions contained herein are those of the authorsand should not be construed as representing the of-ficial views or policies of IARPA, the ODNI or theU.S.
Government.ReferencesAnthony Aue and Michael Gamon.
2005.
Customiz-ing sentiment classifiers to new domains: a casestudy.
In Proc.
International Conference on RecentAdvances in NLP.Nuria Bel, Cornelis H. A. Koster, and Marta Ville-gas.
2003.
Cross-lingual text categorization.
InResearch and Advanced Technology for Digital Li-braries, pages 126?139.
Springer Berlin / Heidel-berg.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proc.
of the 2006 Conference onEmpirical Methods in Natural Language Process-ing, pages 120?128.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, Bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 440?447.John Blitzer, Dean P. Foster, and Sham M. Kakade.2009.
Zero-shot domain adaptation: A multi-viewapproach.
Technical report, Toyota TechnologicalInstitute TTI-TR-2009-1.Mark G. Core and James F. Allen.
1997.
Coding di-alogs with the DAMSL annotation scheme.
In Proc.of the Working Notes of the AAAI Fall Symposium onCommunicative Action in Humans and Machines.Mark Dredze, John Blitzer, Partha Pratim Taluk-dar, Kuzman Ganchev, Joa?o Graca, and FernandoPereira.
2007.
Frustratingly hard domain adapta-tion for dependency parsing.
In Proc.
of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages1051?1055.51Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Umit Guz, Gokhan Tur, Dilek Hakkani-Tu?r, andSe?bastien Cuendet.
2010.
Cascaded model adapta-tion for dialog act segmentation and tagging.
Com-puter Speech & Language, 24(2):289?306.Jiayuan Huang, Alexander J. Smola, Arthur Gretton,Karsten M. Borgwardt, and Bernhard Scho?lkopf.2007.
Correcting sample selection bias by unlabeleddata.
In Advances in Neural Information ProcessingSystems 19, pages 601?608.Minwoo Jeong, Chin Y. Lin, and Gary G. Lee.
2009.Semi-supervised speech act recognition in emailsand forums.
In Proc.
of the 2009 Conference onEmpirical Methods in Natural Language Process-ing, pages 1250?1259.Dan Jurafsky, Liz Shriberg, and Debra Biasca.
1997.Switchboard SWBD-DAMSL shallow-discourse-function annotation coders manual, draft 13.
Tech-nical report, University of Colorado at BoulderTechnical Report 97-02.Lori Levin, Ann Thyme?-Gobbel, Alon Lavie, KlausRies, and Klaus Zechner.
1998.
A discourse cod-ing scheme for conversational Spanish.
In Proc.
The5th International Conference on Spoken LanguageProcessing, pages 2335?2338.Yang Liu.
2006.
Using SVM and error-correctingcodes for multiclass dialog act classification in meet-ing corpus.
In Proc.
Interspeech, pages 1938?1941.Sinno J. Pan, James T. Kwok, and Qiang Yang.
2008.Transfer learning via dimensionality reduction.
InProc.
of the Twenty-Third AAAI Conference on Arti-ficial Intelligence.Brian Roark and Michiel Bacchiani.
2003.
Supervisedand unsupervised PCFG adaptation to novel do-mains.
In Proc.
of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology,pages 126?133.Sophie Rosset, Delphine Tribout, and Lori Lamel.2008.
Multi-level information and automatic dia-log act detection in human?human spoken dialogs.Speech Communication, 50(1):1?13.Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, JeremyAng, and Hannah Carvey.
2004.
The ICSI meetingrecorder dialog act (MRDA) corpus.
In Proc.
of the5th SIGdial Workshop on Discourse and Dialogue,pages 97?100.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26:339?373.Dinoj Surendran and Gina-Anne Levow.
2006.
Dialogact tagging with support vector machines and hiddenMarkov models.
In Proc.
Interspeech, pages 1950?1953.Gokhan Tur, Dilek Hakkani-Tu?r, and Robert E.Schapire.
2005.
Combining active and semi-supervised learning for spoken language under-standing.
Speech Communication, 45(2):171?186.Gokhan Tur, Umit Guz, and Dilek Hakkani-Tu?r.2007.
Model adaptation for dialog act tagging.In Proc.
IEEE Spoken Language Technology Work-shop, pages 94?97.Gokhan Tur.
2005.
Model adaptation for spoken lan-guage understanding.
In Proc.
IEEE InternationalConference on Acoustics, Speech, and Signal Pro-cessing, pages 41?44.Anand Venkataraman, Luciana Ferrer, Andreas Stol-cke, and Elizabeth Shriberg.
2003.
Traininga prosody-based dialog act tagger from unlabeleddata.
In Proc.
IEEE International Conference onAcoustics, Speech, and Signal Processing, volume I,pages 272?275.Nick Webb and Ting Liu.
2008.
Investigating theportability of corpus-derived cue phrases for dia-logue act classification.
In Proc.
of the 22nd Inter-national Conference on Computational Linguistics(Coling 2008), pages 977?984.Dikan Xing, Wenyuan Dai, Gui-Rong Xue, and YongYu.
2007.
Bridged refinement for transfer learn-ing.
In Knowledge Discovery in Databases: PKDD2007, pages 324?335.
Springer Berlin / Heidelberg.52
