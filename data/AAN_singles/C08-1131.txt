Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1041?1048Manchester, August 2008Measuring and Predicting Orthographic Associations:Modelling the Similarity of Japanese KanjiLars Yencken and Timothy Baldwin{lljy,tim}@csse.unimelb.edu.auNICTA Research LabUniversity of MelbourneAbstractAs human beings, our  mental  processesfor  recognising  linguistic  symbols  gen-erate  perceptual  neighbourhoods  aroundsuch symbols where confusion errors oc-cur.
Such  neighbourhoods  also  pro-vide  us  with  conscious  mental  associa-tions between symbols.
This  paper  for-malises orthographic models for similarityof Japanese kanji, and provides a proof-of-concept dictionary extension leveragingthe mental associations provided by ortho-graphic proximity.1 IntroductionElectronic dictionary interfaces have evolved frommere digitised forms of their paper ancestors.
Theynow enhance accessibility by addressing the sep-arate needs of language consumers and languageproducers, of learners from non-speakers to nativespeakers, and by targeting the specific difficultiespresented by individual languages.For languages with logographic orthographies,such  as  Japanese  and  Chinese, accessibility  re-mains poor due to the difficulties in looking upan unknown character in the dictionary.
The tra-ditional method of character lookup in these lan-guages involves identifying the primary compo-nent (or ?radical?
), counting its strokes, looking itup in the index, counting the remainder of strokesin the original character, then finding the characterin a sub-index.
This presents several opportunitiesfor error, but fortunately improvements have beenmade, as we discuss in Section 2.We are interested in the perceptual process ofidentifying characters, in particular the behaviourof perception within dense visual neighbourhoods.Within the dictionary accessibility space, we are?2008.
Licensed  under  the Creative  CommonsAttribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.motivated by the potential to correct confusion er-rors, but also to leverage the mental associationsprovided by visual proximity to allow advancedlearners  to  find  unknown characters  faster.
Asproof-of-concept, we propose a method for look-ing up unknown words with unfamiliar characters,based on similarity with known characters.In  essence, our  method  is  based  on  the  userplausibly ?mistyping?
the word based on closely-matching kanji they are familiar with (and hencecan readily access via a standard input method ed-itor), from which we predict the correct kanji com-bination based on kanji similarity and word fre-quency.
For example, given the input ?
?, thesystem could suggest the word??
[hosa] ?help?based on similarity between the high-frequency?and the graphically-similar but low-frequency?.The  proposed  method  is  combined  with  theFOKS lookup strategy proposed by Bilac (2002)for looking up unknown words via plausibly incor-rect readings.The contributions of this paper are the proposalof a range of character similarity models for logo-graphic scripts, a novel evaluation method for lo-gographic character confusability, and the incorpo-ration of kanji similarity into a word-level lookupmodel.The remainder of this paper is structured as fol-lows.
Firstly, we review related lookup systems(Section 2), and go on to discuss how we measureand model kanji similarity, including an evalua-tion of the methods (Section 3).
We then focus onthe conversion of similarity models into confusionmodels, and their integration into a search interface(Section 4).
Examining both our models and theinterface itself, we discuss our findings (Section 5)before finally concluding (Section 6).2 A review of related systems2.1 Associative lookup systemsAssociative  lookup  systems  are  based  on  thepremise that characters and words form a highly1041connected lexical network.
They focus on findingand making accessible the mental links providedby proximity within this network.
In contrast, sys-tems which correct for confusion model plausibleerrors in order to recover from them.
Examples ofassociative systems are as follows:Semantic Orthographic(for producers) (for consumers)MonolingualVisual WordNetthis paperBilingualstandard  bilingualdictionariesPinyomi  dictio-nary interfaceFerret and Zock (2006) introduce the distinctionbetween language producers as encoders of seman-tic  information, and language consumers  as  de-coders of orthographic (or phonetic) information.We first consider systems to aid production of lan-guage.Systems for production give form and sound toknown semantics.
The most common such systemsare bilingual dictionaries which associate words inone language with their near-synonyms in a secondlanguage.
Even within a monolingual context, theproblem of selecting the right word can be difficult,whether the difficulty is one of limited knowledgeor simply one of access, as in the case of the tip-of-the-tongue problem.
Work in this area (Zock,2002; Zock and Bilac, 2004) has more recently fo-cused on extending WordNet with syntagmatic re-lationships (Ferret and Zock, 2006).
Access couldtake the form of the Visual WordNet Project.1For language consumers, the challenge is to findthe meaning or sound of a word with known form.For logographic languages, where characters areentered phonetically2using an input method ed-itor, computer  input  of  an  unknown word  withknown form remains difficult, since the reading isunknown.In  a  bilingual  context, the  Pinyomi  Chinese-Japanese dictionary interface overcomes this ob-stacle by allowing Japanese speakers to look upa Chinese word via the Japanese-equivalent char-acters based on orthographic associations betweensimilar characters (Yencken et al, 2007).Our proposed extension to the FOKS dictionaryis functionally similar to Pinyomi, but in a mono-lingual Japanese context.
In our case, a Japaneseword containing unknown characters is found by1http://kylescholz.com/projects/wordnet/2A notable exception is the Wubizixing lookup method forChinese.querying with known characters that are visuallysimilar.
Unlike Pinyomi, which uses an ideogramtransliteration table to determine associations, weuse direct models of character similarity to deter-mine associations.2.2 Kanji lookup systemsWe next provide a brief review of five kanji lookupsystems in order to situate our proposed interfaceappropriately.The SKIP (System of Kanji Indexing by Pat-terns)  system  of  lookup  provides  an  indexingscheme based on a kanji?s overall shape rather thanits primary radical (Halpern, 1999).
For example,?
[aka] ?bright?
has skip code 1-4-4, with the firstnumber indicating it is horizontally split into twoparts, and the second and third numbers represent-ing the respective stroke counts of the two parts.The Kansuke dictionary simplifies the method ofcounting strokes, to form a three-number code rep-resenting the horizontal, vertical and other strokesthat make up a character (Tanaka-Ishii and Godon,2006).
Characters can also be looked up from theircomponents.
For our earlier example ?
consistsof?
with code 3-2-0 and?
with code 3-1-1.The  Kanjiru  dictionary  (Winstead, 2006)  at-tempts  to  interactively  assemble  a  character  byshape and stroke via mouse movements, providingthe user with structural ways of building up com-ponents until the desired character is found.Finally, hand-writing interfaces attempt to cir-cumvent the computer input problem altogether,but still suffer from several issues: the awkward-ness of mouse input for drawing characters; sensi-tivity to both stroke order and connectivity of com-ponents; and the difference in hand-writing stylesbetween learners and native speakers.These lookup methods contrast  with our pro-posed similarity-based search in several ways.Firstly, our  method  combines  word-  andcharacter-level information directly, yet providesthe means to lookup words with unknown charac-ters without the use of wildcards.
The downside tothis is that the user needs to use kanji in the searchquery, limiting potential users to intermediate andadvanced learners with some knowledge of kanji.Secondly, we are able to cater to both intentionalsimilarity-based searches, and unintentional inputerrors, increasing the accessibility of the base dic-tionary.
This approach shares much with the FOKSdictionary interface (Bilac, 2002), which provides1042error-correcting lookup for reading-based dictio-nary queries.
Suppose, for example, a user wishesto look up the word ??
?festival float?, but isunsure of its pronunciation.
FOKS allows themto guess the pronunciation based on readings theyknow for each character in other contexts.
In thiscase, they might combine ?
[yama] ?mountain?and ?
[kuruma] ?car?
and guess the word read-ing as [yamakuruma].
The correct reading [dashi]cannot be guessed from the word?s parts, but oureducated guess would lead the user to the wordand provide access to both the correct reading andmeaning.This  approach  is  complementary  to  our  pro-posed method.
Suppose, analogously, that the userwishes to look up the word??
but is unfamiliarwith the first kanji.
A query for ??
would trig-ger an inference based on the similarity between?and?, and provide the desired word in the results,allowing the user to determine both its pronuncia-tion [h?moN] and its meaning ?visit?.3 Modelling similarity3.1 Metric space modelsThere has been little work on methods for measur-ing or predicting the similarity between two kanji.While there have been many psycholinguistic stud-ies  on  various  specific  aspects  of  perception  ofChinese and Japanese logographic characters, fewtouch directly on orthographic confusion.
For abrief discussion, see Yencken and Baldwin (2006).Broadly, current  literature  suggests  that  kanjirecognition may be hierarchical, building radicalsfrom strokes, and whole characters from radicals.Each point of recognition and combination sug-gests a potential site for misrecognition or confu-sion with an orthographic or semantic neighbour.The most directly relevant study involved twoexperiments  by Yeh  and  Li  (2002).
In  a  sort-ing task, subjects tended to categorise charactersby their structure, rather than their shared compo-nents.
In a subsequent search task, presence ofshared structure between target and distractors wasthe dominant factor in subjects?
response times.We previously proposed two naive kanji similar-ity measures: a cosine similarity metric operatingon boolean radical vectors, and the l1norm (Man-hattan distance) between rendered images of kanji(Yencken and Baldwin, 2006).
Evaluating on aset of human similarity judgements, we determinedthat the cosine similarity method outperformed thedstrokedtreedradical??????????l1???
?3, 11a, 2a, 2a3, 11a, 2a, 2a, 2a??
??
??
?
??
?
?
?
?
???
??
??
?
??
?
?
?
?
??
?Figure 1: A summary of our kanji distance metricsl1norm, although it had lower precision for high-similarity pairs.3.1.1 Bag of radicals with shapeWhen learners of Japanese study a new charac-ter, they do not study its strokes in isolation, butinstead build on prior knowledge of its componentradicals.
For example, ?
[aka] ?bright?
could beanalysed as being made up of the?
[sun] ?hi?
and?
[moon] ?tsuki?
radicals.Radicals are useful in several ways.
The num-ber of radicals in any kanji is much smaller thanthe number of strokes for any kanji, making suchkanji easier to chunk and recall in memory.
Fur-thermore, radicals can provide cues to the mean-ing and pronunciation of characters which containthem.3The original metric used in Yencken and Bald-win (2006) simply calculates the cosine similaritybetween radical vectors.
This ignores the positionof radicals, which is known to be important in sim-ilarity judgements, and also the number of timeseach radical occurs within a kanji.
Hence, ?, ?and ?
are all considered identical (radical = ?
),as are?
and?
(radical =?).
The metric is cal-3For example, kanji containing the radical ?, such as?
[mune] ?chest?
and?
[ude] ?arm?, are reliably body parts.Kanji containing the radical ?, as in ?
[d?]
?copper?
and?
[d?]
?body?, often have the Chinese or on reading [d?
]amongst their valid pronunciations.1043culated by:dradical(x,y) = 1?
rx ?
ry|rx||ry|(1)To address radical multiplicity, and the findingsof Yeh and Li?s study, we set the above metric tounit distance whenever the two characters differin their basic shape.
To approximate shape, weuse the first part of each kanji?s 3-part SKIP code.which can take values horizontal, vertical, contain-ment or other.
SKIP codes for each kanji are pro-vided in Kanjidic,4and radical membership in theRadkfile.5This change allows the metric to distinguish be-tween examples with repeated components.
Thealtered metric aims to capture the visual and se-mantic salience of radicals in kanji perception, andto also take into account some basic shape similar-ity.3.1.2 Distance of rendered imagesIn contrast to the previous approach, we can con-sider kanji as arbitrary symbols rendered in print oron screen, and then attempt to measure their sim-ilarity.
The simplest way to do this is to simplyrender each kanji to an image of fixed size, and tothen use some distance metric over images.A common and simple distance metric is the l1norm, which simply sums the difference in lumi-nance between pixels of the two images for somealignment.
Fortunately, all kanji are intended tooccupy an identically sized block, so alignment isvia a grid, constant across all kanji.
Consideringpx(i, j) to be the luminance of the pixel at position(i, j) of rendered kanji x, we evaluate the l1normas follows:l1(x,y) =?i,j|px(i, j)?py(i, j)| (2)This calculation depends on the image representa-tion chosen, and could differ slightly across fonts,image sizes and rasterisation methods.
We used theMS Gothic font, rendering to 80x80 images, withanti-aliasing.This metric is  aimed at  capturing the generaloverlap  of  strokes  between  the  two  characters,along with the overlap of whitespace, which givesuseful structure information.
This metric is knownto be noisy for low-to-medium similarity pairs, butis very useful in distinguishing near neighbours.4http://www.csse.monash.edu.au/~jwb/kanjidic.html5http://www.csse.monash.edu.au/~jwb/kradinf.html3.1.3 Stroke edit distanceA third possibility is to reduce kanji to the verystrokes used to write them.
Two features of the or-thography make this possible: (1) kanji are not ar-bitrary symbols, but configurations of strokes cho-sen from within a finite and limited set; and (2)each kanji has a precise stroke order which is con-sistent for reused kanji components, such that iftwo or more arbitrary components were combinedto form a new pseudo-character, native speakerswould largely agree on the stroke order.To define a metric based on strokes, we needboth  a  source  of  stroke  data  and  a  comparisonmethod.
For stroke data, we look to a hierarchi-cal data set for Japanese kanji created by Apel andQuint (2004).
Each kanji is specified by its strokes,grouped into common stroke groups (components),and broken down in a hierarchical manner into rel-ative positions within the kanji (for example: leftand right, top and bottom).
The strokes themselvesare based on a taxonomy of some 26 stroke types(46 including sub-variants).For any given kanji, we can flatten its hierarchyto generate an ordered sequence of strokes: a sig-nature for that character.
The natural distance met-ric across such sequences is the string edit distance.This forms our dstrokemetric.Much  useful  information  is  preserved  withinstroke signatures.
Since radicals are written in se-quence, they form contiguous blocks in the signa-ture.
The edit distance will thus align shared radi-cals when their position is similar enough.
Sincecomponents are usually drawn in a left-to-right,top-to-bottom order, the order of components ina signature also reflects their position as part ofthe larger character.
Finally, it provides a smoothblending from stroke similarity to radical similar-ity, and can recognise the similarity between pairslike?
[hi] ?sun?
and?
[me] ?eye?.3.1.4 Tree edit distanceIn our previous approach, we discarded much ofthe hierarchical information available, relying onstroke order to approximate it.
We can instead usethe full data, and calculate the ordered tree edit dis-tance between kanji XML representations.
Treeedit distance is defined as the length of the short-est sequence of inserts, deletions and relabellingsrequired to convert  one tree into another (Bille,2005).
Though a cost function between labels canbe specified, we gave inserts/deletions and rela-bellings unit cost.1044Figure 1 provides an overview of the structureof each kanji?s representation.
Actual trees alsocontain  phonetic  elements, radicals, and  strokegroups  whose  strokes  are  spread  across  severalnon-contiguous blocks.
Another motivation for in-cluding tree edit distance is to determine if this ad-ditional information is useful in determining kanjisimilarity.3.2 EvaluationWe evaluate our distance metrics over three datasets.The first data set is the human similarity judge-ments from Yencken and Baldwin (2006).
Thisdata set is overly broad in that it weights the abil-ity to distinguish low and medium similarity pairsequally with distinguishing medium and high sim-ilarity pairs.
It is clear that for most applications,determining the high similarity pairs with high pre-cision is most important.
Nevertheless, this data setis useful for comparing our metrics with those pro-posed in previous research.In order to better measure performance on high-similarity pairs, which we expect to form the basisof incorrect kanji inputs, we need a set of human-selected confusion data.
The second data set isdrawn from the White Rabbit JLPT Level 36kanjiflashcards.
Each flashcard contains either one ortwo highly-similar neighbours which might be con-fused with a given kanji.
We use this set to deter-mine our likely performance in a search task.Our third data set is based on human confusabil-ity judgements for kanji pairings.3.2.1 Similarity experiment dataThe first data consists of human similarity judge-ments to pairs of kanji, scored on a 5 point scale(Yencken and Baldwin, 2006).
The experimenthad 179 participants, covering a broad range ofJapanese proficiency.
The key participant group-ings are: (1) non-speakers of Chinese, Japanese orKorean (Non-CJK); (2) Japanese second-languagelearners  (JSL);  and  (3)  Japanese  first-languagespeakers (JFL).
Figure 2 gives the rank correlation?
between each metric and a rater, averaged overall raters in each proficiency group.For  each  metric, the  mean  rank  correlationincreased  with  the  participants?
knowledge  of6Japanese Language Proficiency Test: the standard gov-ernment test for foreigners learning Japanese.Non-CJK JSL JFL00.1750.3500.5250.700Metric agreement within rater groupsdradical?SHAPEdradical+SHAPEl1dtreedstrokeFigure 2: Mean value of Spearman?s rank correlation ?
overrater groups for each metric (dradical(?shape) is the originalmetric, and dradical(+shape) is our augmented version)Japanese (from Non-CJK to JSL to JFL), indicat-ing that the raters made more motivated and consis-tent similarity judgements.
The dradical(+shape)metric dominates the other metrics, including theoriginal dradical(?shape), at all levels of knowl-edge.
This confirms the salience of radicals and thetendency for individuals to classify kanji by theirbroad shape, as suggested by Yeh and Li (2002).l1, dstrokeand dtreeperform poorly in comparison.Interestingly, these three metrics have large per-formance differences for non-speakers, but not fornative-speakers.Despite overall poor performance from our newmetrics, we were able to improve on the originaldradical(?shape).
We now evaluate over the flash-card data set for comparison.3.2.2 Flashcard data setThe flashcard data differs greatly from the previ-ous experimental data, as it consists of only human-selected  high-similarity  pairs.
Accordingly, wetook two approaches to evaluation.Firstly, for  each  high-similarity  pair  (a pivotkanji and its distractor), we randomly select a thirdkanji from the j?y?
character set7and combine itwith the pivot to form a second pair which is highlylikely to be low similarity.
We then compare howwell each metric can classify the two pairs by im-posing the correct ordering on them, in the form ofclassification accuracy.
The results of this evalua-tion are shown in Table 1.
We include a theoreticalrandom baseline of 0.500, since any decision has a7The  ?common  use?
government  kanji  set, containing1945 characters.1045Metric Accuracydtree0.979dstroke0.968l10.957dradical0.648random baseline 0.500Table 1: Accuracy at detecting which of two pairs (flashcardvs.
random) has high similarityMetric MAP p@1 p@5 p@10dstroke0.594 0.313 0.151 0.100dtree0.560 0.313 0.149 0.094l10.503 0.257 0.139 0.089dradical0.356 0.197 0.087 0.063Table 2: The mean average precision (MAP), and precision atN ?
{1,5,10} over the flashcard data50% a priori chance of being successful.The performance of dradical?
close to our ran-dom baseline, despite performing best in the pre-vious task ?
is suggestive of the different charac-teristics of this task.
In particular, a metric whichorders well across the broad spectrum of similaritypairs may not be well suited to identifying high-similarity pairs, and vice-versa.The other  three  metrics  have  accuracy above0.95 on this task, indicating the ease with whichthey can identify high-similarity pairs.
However,this does not guarantee that the neighbourhoodsthey generate will  be free from noise, since thereal-world prevalence of highly similar charactersis likely to be very low.To better determine what dictionary search re-sults  might  be  like, we consider  each flashcardkanji as a query, and its high-similarity distractorsas relevant documents (and implicitly all remainingkanji as irrelevant documents, i.e.
dissimilar char-acters).
We can then calculate the Mean AveragePrecision (MAP, i.e.
the mean area under the preci-sion?recall curve for a query set) and the precisionat N neighbours, for varied N .
The results of thisapproach are presented in Table 2.The precision statistics confirm the ranking ofmetrics found in the earlier classification task.
Thedstrokemetric outperforms l1by a greater margin inthe MAP statistic and precision at N = 1, but nar-rows again for greater N .
This suggests that it ismore reliable in the upper similarity ranking.3.2.3 Distractor pool experimentThe flashcard data provides good examples ofhigh-similarity pairs, but suffers from several prob-lems.
Firstly, the constraints of the flashcard for-mat limit the number of high-similarity neighboursthat can be presented on each flashcard to at mosttwo; in some cases we might expect more.
Sec-ondly, the  methodology behind  the  selection  ofthese high-similarity neighbours is unclear.For these reasons, we conducted an experimentto attempt to replicate the flashcard data.
100 kanjiwere randomly chosen from the JLPT 3 set (here-after pivots).
For each pivot kanji, we generated apool of possible high-similarity neighbours in thefollowing way.
Firstly, we seeded the pool withthe neighbours from the flashcard data set.
We thenadded the highest similarity neighbour as given byeach of our similarity metrics.
Since these couldoverlap, we iteratively continued adding an addi-tional neighbour from all of our metrics until ourpool contained at least four neighbours.Native or native-like speakers of Japanese weresolicited as participants.
After a dry run, each par-ticipant was presented with a series of pivot kanji.For each pivot kanji, they were asked to select fromits pool of neighbours which (if any) might be con-fused for that kanji based on their graphical simi-larity.
The order of pivots was randomised for eachrater, as was the order of neighbours for each pivot.Kanji were provided as images using MS Gothicfont for visual consistency across browsers.Three  participants  completed  the  experiment,selecting 1.32 neighbours per pivot on average, lessthan 1.86 per pivot provided by the flashcard data.Inter-rater agreement was quite low, with a mean?
of 0.34 across rater pairings, suggesting that par-ticipants found the task difficult.
This is unsurpris-ing, since as native speakers the participants areexperts at discriminating between characters, andare unlikely to make the same mistakes as learners.Comparing their judgements to the flashcard dataset yields a mean ?
of 0.37.Ideally, this data generates a frequency distribu-tion over potential neighbours based on the num-ber of times they were rated as similar.
However,since the number of participants was small, we sim-ply combined the neighbours with high-similarityjudgements for each pivot, yielding an average of2.45 neighbours per pivot.
Re-evaluating our met-rics on this data gives the figures in Table 3.1046Metric MAP p@1 p@5 p@10dstroke1.046 0.530 0.228 0.146dtree1.028 0.540 0.228 0.136l10.855 0.480 0.200 0.117dradical0.548 0.270 0.122 0.095Table 3: The mean average precision (MAP), and precision atN ?
{1,5,10} over the distractor dataCompared to the flashcard data set, the orderingand relative performance of metrics is similar, withdstrokemarginally improving on dtree, but both sig-nificantly outperforming l1and dradical.
The near-doubling of high similarity neighbours from 1.32to 2.45 is reflected by a corresponding increase inMAP and precision@N scores, though the effectis somewhat reduced as N increases.4 From similarity to searchHaving examined several character distance met-rics, and evaluated them over our three data sets,we now consider  their  application  to  dictionaryword search.4.1 Overall modelOur broad probability model for looking up wordsbased on similar kanji  is  identical  to the FOKSmodel for search based on readings, save that wesubstitute readings for kanji in our query.
A uni-gram approximation leads us to Equation 3 below,where q = q0.
.
.
qn is the query given by the user,w = w0.
.
.wn is the desired word, and each qi andwi is a kanji character:Pr(w|q) ?
Pr(w)Pr(q|w)= Pr(w)?iPr(qi|w,q0 .
.
.
qi?1)?
Pr(w)?iPr(qi|wi) (3)The final line of Equation 3 requires two modelsto be supplied.
The first, Pr(w), is the probabilitythat a word will be looked up.
Here we approxi-mate using corpus frequency over the Nikkei news-paper data, acknowledging that a newspaper cor-pus is skewed differently to learner data.
The sec-ond model is our confusion model Pr(qi|wi), inter-preted either as the probability of confusing kanjiwi with kanji qi, or of the user intentionally select-ing qi to query for wi.
It is this model that we nowfocus on.4.2 Confusion modelAlthough we can construct a confusion model us-ing our distance metric alone, it is clear that fre-quency effects will occur.
For example, the like-lihood of confusion is increased if the target wi israre and unknown, but qi is a highly-similar high-frequency neighbour; certainly this is a typical usecase for intentional similarity-based querying.
Wethus propose a generic confusion model based asimilarity measure between kanji:Pr(qi|wi)?Pr(qi)s(qi,wi)?j Pr(qi,j)s(qi,j ,wi)(4)The  confusion  model  uses  a  similarity  functions(qi,wi) and a kanji frequency model Pr(qi) to de-termine the relative probability  of  confusing wiwith qi amongst  other  candidates.
We convertthe desired distance metric d into s according tos(x,y) = 1?
d(x,y) if the range of d is [0,1], ors(x,y) = 11+d(x,y) if the range of d is [0,?
).To maximise the accessibility of this form ofsearch, we must find the appropriate trade-off be-tween providing sufficient candidates and limitingthe noise.
We use a thresholding method borrowedfrom Clark and Curran (2004), where our thresh-old is set as a proportion of the first candidate?sscore.
For example, using 0.9 as our threshold, ifthe first candidate has a similarity score of 0.7 withthe target kanji, we would then accept any neigh-bours with a similarity greater than 0.63.
Usingthe dstrokemetric with a ratio of 0.9, there are onaverage 2.65 neighbours for each kanji in the j?y?character set.4.3 Evaluating searchSearch by similar grapheme has an advantage tosearch by word reading: reading results are natu-rally ambiguous due to homophony in Japanese,and attempts to perform error correction may in-terfere with exact matches in the results ranking.Grapheme-based search may have only one exactmatch, so additional secondary candidates are notin direct competition with existing search practices.We can estimate the accessibility improvementgiven by this form of search as follows.
Let usassume that learners study kanji in frequency or-der.
For each kanji learned, one or more high-similarity neighbours also become accessible.
Tak-ing all pairings of kanji within the JIS X 0208-1990 character set, using the dstrokemetric with acutoff ratio of 0.9, and assuming full precision onthe neighbour graph this generates, we get the ac-cessibility curve found in Figure 3.
Our baselineis a single kanji accessible for each kanji learned.10470150030004500600050 400 800 12501750225027503250375042504750Accessibility of similarity search#kanjiaccessible# kanji knownbaseline accessible by searchFigure 3: The accessibility improvement of kanji similaritysearchOur actual precision makes the proportion of us-able neighbours smaller; we will thus need to ex-pose the user to a larger set of candidates to get thislevel of improvement.
Improvements in precisionand recall are still needed to reduce noise.5 Discussion and future workA current  difficulty  in  evaluating  this  form  ofsearch is the lack of available query data to objec-tively evaluate the search before deployment.
Thisrestricts evaluation to longer-term post-hoc analy-sis based on query logs.
Such logs will also provideadditional real-world similarity and confusion datato improve our metrics.This form of search is directly extensible to Chi-nese, and is limited only by the availability of char-acter data.
Indeed, preliminary similarity modelsfor Chinese already exist (Liu and Lin, 2008).
Oursimilarity modelling may also suggest approachesfor more general symbol systems that lack ade-quate indexing schemes, for example heraldry.There is much potential in the adaption of dic-tionaries as drill tutors in the context of languagelearning (Zock and Quint, 2004).
The models pre-sented in this paper could provide dynamic kanjidrills, to aid early learners to distinguish similarkanji and provide challenge more advanced learn-ers.6 ConclusionWe have proposed a method of searching the dictio-nary for Japanese words containing unknown kanji,based on their visual similarity to familiar kanji.In order to achieve this, we have considered sev-eral metrics over characters, improved on existingbaselines and evaluated further over a flashcard set.Of these metrics, the edit distance taken over strokedescriptions performed the best for high-similaritycases, and was used to construct similarity-basedsearch at the word level.References[Apel and Quint2004] Apel, Ulrich and Julien Quint.
2004.Building a graphetic dictionary for Japanese kanji ?
char-acter look up based on brush strokes or stroke groups, andthe display of kanji as path data.
In Proc.
COLING 2004,Geneva, Switzerland.
[Bilac2002] Bilac, Slaven.
2002.
Intelligent dictionary inter-face for learners of Japanese.
Master?s thesis, Tokyo Insti-tute of Technology.
[Bille2005] Bille, Philip.
2005.
A survey on tree edit dis-tance and related problems.
Theoretical Computer Sci-ence, 337(1-3):217?239.
[Clark and Curran2004] Clark, Stephen and James R. Curran.2004.
The importance of supertagging for wide-coverageCCG parsing.
In Proc.
COLING 2004, page 282?288,Geneva, Switzerland.
[Ferret and Zock2006] Ferret, Olivier  and  Michael  Zock.2006.
Enhancing electronic dictionaries with an indexbased on associations.
In Proc.
COLING/ACL 2006, pages281?288, Sydney, Australia.
[Halpern1999] Halpern, Jack, editor.
1999.
The  Kodan-sha Kanji Learner?s Dictionary.
Kodansha International,Tokyo.
[Liu and Lin2008] Liu, Chao-Lin and Jen-Hsiang Lin.
2008.Using structural information for identifying similar Chi-nese characters.
In Proc.
ACL 2008: HLT, Short Papers(Companion Volume), pages 93?96, Columbus, Ohio.
[Tanaka-Ishii and Godon2006] Tanaka-Ishii, Kumiko and Ju-lian Godon.
2006.
Kansuke: A kanji look-up system basedon a few stroke prototype.
In Proc.
ICCPOL 2006, Singa-pore.
[Winstead2006] Winstead, Chris.
2006.
Electronic kanji dic-tionary based on ?Dasher?.
Proc.
IEEE SMCals 2006 ,pages 144?148, Logan, USA.
[Yeh and Li2002] Yeh, Su-Ling  and  Jing-Ling  Li.
2002.Role  of  structure  and  component  in  judgements  of  vi-sual similarity of Chinese characters.
Journal of Experi-mental Psychology: Human Perception and Performance,28(4):933?947.
[Yencken and Baldwin2006] Yencken, Lars  and  TimothyBaldwin.
2006.
Modelling  the  orthographic  neigh-bourhood for  Japanese kanji.
In Proc.
ICCPOL 2006,Singapore.
[Yencken et al2007] Yencken, Lars, Zhihui Jin, and KumikoTanaka-Ishii.
2007.
Pinyomi - dictionary lookup via ortho-graphic associations.
In Proc.
PACLING 2007, Melbourne,Australia.
[Zock and Bilac2004] Zock, Michael and Slaven Bilac.
2004.Word lookup on the basis of associations: from an idea toa roadmap.
In Proc.
COLING 2004, pages 89?95, Geneva,Switzerland.
[Zock and Quint2004] Zock, Michael and Julien Quint.
2004.Why have them work for peanuts, when it is so easy to pro-vide reward?
Motivations for converting a dictionary intoa drill tutor.
In Proc.
PAPILLON 2004, Grenoble, France.
[Zock2002] Zock, Michael.
2002.
Sorry, what was your nameagain, or how to overcome the tip-of-the tongue problemwith the help of a computer?
In Proc.
COLING 2002,pages 1?6, Taipei, Taiwan.1048
