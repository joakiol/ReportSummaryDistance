Proceedings of the 12th European Workshop on Natural Language Generation, pages 74?81,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsLearning Lexical Alignment Policies for GeneratingReferring Expressions in Spoken Dialogue SystemsSrinivasan JanarthanamSchool of InformaticsUniversity of EdinburghEdinburgh EH8 9ABs.janarthanam@ed.ac.ukOliver LemonSchool of InformaticsUniversity of EdinburghEdinburgh EH8 9ABolemon@inf.ed.ac.ukAbstractWe address the problem that differentusers have different lexical knowledgeabout problem domains, so that automateddialogue systems need to adapt their gen-eration choices online to the users?
domainknowledge as it encounters them.
We ap-proach this problem using policy learningin Markov Decision Processes (MDP).
Incontrast to related work we propose a newstatistical user model which incorporatesthe lexical knowledge of different users.We evaluate this user model by showingthat it allows us to learn dialogue poli-cies that automatically adapt their choiceof referring expressions online to differ-ent users, and that these policies are sig-nificantly better than adaptive hand-codedpolicies for this problem.
The learnedpolicies are consistently between 2 and8 turns shorter than a range of differenthand-coded but adaptive baseline lexicalalignment policies.1 IntroductionIn current ?troubleshooting?
spoken dialogue sys-tems (SDS)(Williams, 2007), the major part of theconversation is directed by the system, while theuser follows the system?s instructions.
Once thesystem decides what instruction to give the user(at the dialogue management level), it faces sev-eral decisions to be made at the natural languagegeneration (NLG) level.
These include, decidingwhich concepts to include in the utterance, decid-ing the referring expressions (RE) to use in the ut-terance and so on.
A little-studied problem is towhat extent a system could automatically align tothe user?s lexical knowledge by adapting its REchoices, in particular based on his domain exper-tise, and how this can be modelled and optimisedcomputationally.
(Issacs and Clark, 1987) show how two in-terlocutors adapt their language in a conversa-tion by assessing each other?s domain expertiseduring dialogue, by observing how they react toeach other?s RE choices.
This is called align-ment through Audience Design (Clark, 1996; Bell,1984).
Using inappropriate REs in instructions hasbeen identified as a serious problem affecting asystem?s usability (Molich and Nielsen, 1990).In this paper, we treat NLG within a com-putational learning paradigm (Lemon, 2008;Rieser and Lemon, 2009; Janarthanam andLemon, 2008).
We examine whether a SDS canautomatically learn a lexical alignment policyfor audience design, which enables it to chooseappropriate REs by predicting the user?s lexicalknowledge dynamically during the course of thedialogue.
This can avoid clarification requestsfrom the users and keep the dialogues short.
Theexample given below describes the kind of lexicalalignment behaviour that we want the system tolearn.
The system chooses ?small white box?instead of ?ADSL filter?
and ?monitor symbol?instead of ?network icon?, because it learnt thatthe user is a novice based on their clarificationrequests.
However, it switches to using technicalterms like ?browser?, when it learns that theuser is not a complete novice (as he verifies thedescription for the network icon in Usr 4).Example 1.Sys 1>> Is your router connectedto the computer?Usr 1>> Uh.
What?s a router?Sys 2>> It?s the big black box.Usr 2>> Ok.. yes.Sys 3>> Do you see a small whitebox connected to the router?Usr 3>> Yes.Sys 4>> Ok. Is there a flashingmonitor symbol at the bottom74right of the screen?Usr 4>> the network icon?Sys 5>> Yes.
Is it flashing?Usr 5>> Yes.
It is flashing.Sys 6>> Ok.
Please open yourbrowser.We study lexical alignment in the context oftroubleshooting dialogues ?
where users wish tomend their broken internet connection.
This taskdemands a new type of User Simulation.In this paper, after a review of related work(section 2), we first present (section 3) a statisti-cal User Simulation which supports different do-main knowledge profiles and reacts accordingly toquestions or instructions from an SDS.
We thenpresent a reinforcement learning model of lexicalalignment due to audience design (in sections 4 &5).
We then evaluate the User Simulation (section6), testing whether a simulation that is sensitive toa system?s RE choices can be used to learn goodlexical alignment policies.
Finally, we comparepolicies learned in interaction with the User Sim-ulation with hand-coded policies, and present theresults in section 7.2 Related workSeveral statistical user simulation models thatmodel a user?s behaviour in a conversation havebeen proposed (Georgila et al, 2005; Schatzmannet al, 2006; Schatzmann et al, 2007).
These mod-els issue task specific dialogue acts like inform-ing their search constraints, confirming values, re-jecting misrecognised values, etc.
However, theydo not model a user population with varying do-main expertise.
Also, none of these models seekclarification at conceptual or lexical levels that oc-cur naturally in conversations between real users.
(Komatani et al, 2003) proposed using user mod-els with features like skills, domain knowledgeand hastiness as a part of the dialogue managerto produce adaptive responses.
(Janarthanam andLemon, 2008) presented a user simulation modelthat simulates a variety of users with different do-main knowledge profiles.
Although this modelincorporated clarification acts at the conceptuallevel, these users ignore the issues concerning theuser?s understanding of the REs used by the sys-tem.
In this work, in contrast to the above, wepresent a User Simulation model which explicitlyencodes the user?s lexical knowledge of the do-main, understands descriptive expressions, and is-sues clarification requests at the lexical level.3 User SimulationOur User Simulation module simulates dialoguebehaviour of different users, and interacts with thedialogue system by exchanging both dialogue actsand REs.
It produces users with different knowl-edge profiles.
The user population produced bythe simulation comprises a spectrum from com-plete novices to experts in the domain.
Simulatedusers behave differently from one another becauseof differences in their knowledge profiles.
Simu-lated users are also able to learn new REs duringinteraction with the SDS.
These new expressionsare held in the user simulation?s short term mem-ory for later use in the conversation.
Simulatedusers interact with the environment using an in-teractive mechanism that allows them to observeand manipulate the states of various domain ob-jects.
The interaction between the user and theother components is given in figure 1 (notationsexplained in later sections).Figure 1: Experimental setup3.1 Domain knowledge modelDomain experts know most of the technical termsthat are used to refer to domain objects whereasnovice users can only reliably identify them whendescriptive expressions are used.
While in themodel of (Janarthanam and Lemon, 2008) knowl-edge profiles were presented only at conceptuallevels (e.g.
does the user know what a modem is?
),we present them in a more granular fashion.
Inthis model, the user?s domain knowledge profileis factored into lexical (LKu,t), factual (FKu,t)and procedural knowledge (PKu,t) components.75Lexical knowledge LKu,tvocab([modem, router], dobj1)vocab([wireless, WiFi], dobj3)vocab([modem power light], dobj7)Factual knowledge FKu,tlocation(dobj1)location(dobj7)Procedural knowledge PKu,tprocedure(replace filter)procedure(refresh page)Table 1: Knowledge profile - Intermediate user.A user?s lexical knowledge is encoded in the for-mat:vocab(referring expressions, domain object)where referring expressions can be a list of ex-pressions that the user knows can be used to talkabout each domain object.Whether the user knows facts like the locationof the domain objects (location(domain object)) isencoded in the factual component.
Similarly, theprocedural component encodes the user?s knowl-edge of how to find or manipulate domain objects(procedure(domain action)).
Table 1 shows an ex-ample user knowledge profile.In order to create a knowledge spectrum, aBayesian knowledge model is used.
The currentmodel incorporates patterns of only the lexicalknowledge among the users.
For instance, peo-ple who know the word ?router?
most likely alsoknow ?DSL light?
and ?modem?
and so on.
Thesedependencies between REs are encoded as condi-tional probabilities in the Bayesian model.
Figure2 shows the dependencies between knowledge ofREs.Figure 2: Bayes Net for User Lexical KnowledgeUsing this Bayesian model, we instantiate dif-ferent knowledge profiles for different users.
Thecurrent conditional probabilities were set by handbased on intuition.
In future work, these valueswill be populated based on simple knowledge sur-veys performed on real users (Janarthanam andLemon, 2009).
This method creates a spectrum ofusers from ones who have no knowledge of tech-nical terms to ones who know all the technicaljargon, though every profile will have a differentfrequency of occurrence.
This difference in fre-quency reflects that expert users are less commonthan novice users.The user?s domain knowledge can be dynami-cally updated.
The new REs, both technical anddescriptive, presented by the system through clar-ification moves are stored in the user?s short termmemory.
Exactly how long (in terms of dialogueturns) to retain the newly acquired knowledge isgiven by a retention index RIu.
At the end of RIuturns, the lexical item is removed from user?s shortterm memory.3.2 User Dialogue Action setApart from environment-directed acts, simulatedusers issue a number of dialogue acts.
The list ofdialogue actions that the user can perform in thismodel is given in Table 2.
It consists of defaultmoves like provide info and acknowledge as wellas some clarification moves.
Request descriptionis issued when the SDS uses technical terms thatthe simulated user does not know, e.g.
?What isa router??.
Request verification is issued whenthe SDS uses descriptive lexical items for do-main objects that the user knows more techni-cal terms for, e.g.
System: ?Is the black boxplugged in??
User: ?Do you mean the router?
?.Request disambiguation is issued when the userfaces an underspecified and ambiguous descrip-tive expression, e.g.
?User: I have two black boxeshere - one with lights and one without.
Whichone is it??.
These clarification strategies havebeen modeled based on (Schlangen, 2004).
Theuser simulation also issues request location andrequest procedure dialogue acts, when it does notknow the location of domain objects or how to ma-nipulate them, respectively.3.3 Environment simulationThe environment simulation includes both physi-cal objects, such as the computer, modem, ADSLfilter, etc and virtual objects, such as the browser,control panel, etc in the user?s environment.
Phys-ical and virtual connections between these objects76report problemprovide info(dobj, info)acknowledgerequest verification(x, y)request description(x)request disambiguation(x, [y1,y2])request location(dobj)request procedure(daction)thank systemTable 2: User Dialogue Acts.are also simulated.
At the start of every dialogue,the environment is initiated to a faulty condition.Following a system instruction or question, theuser issues two kinds of environment acts.
It is-sues an observation act Ou,t to observe the statusof a domain object and a manipulation act Mu,tto change the state of the environment (Se,t).
Thesimulation also includes task irrelevant objects inorder to confuse the users with underspecified de-scriptive expressions.
For instance, we simulatetwo domain objects that are black in colour - anexternal hard disk and a router.
So, the users mayget confused when the system uses the expression,?black box?.3.4 User Action SelectionUser Action selection has several steps.
The user?sdialogue behaviour is described in the action se-lection algorithm (Table 3).
Firstly, the user mustidentify all the RE choices (RECs,t) that are usedto refer to different domain objects (dobj) anddomain actions (daction) in the system instruc-tion (step 1).
Secondly, the user?s knowledge ofthe prerequisite factual (FKprereq) and procedural(PKprereq) knowledge components connected tothe observation or manipulation action is checked.If the user does not satisfy the knowledge re-quirements, the user simulation issues an appro-priate clarification request (steps 2 & 3).
Afterthe knowledge requirements are satisfied, the userissues environment directed actions and respondsto system instruction As,t (steps 4 & 5).
Whenthe system provides the user specific information,they are added to the user?s short term memory(steps 6-8).
Although, the action selection processis deterministic at this level, it is dependent onthe users?
diverse knowledge profiles, which en-sures stochastic dialogue behaviour amongst dif-ferent users created by the module.greet the userrequest status(x)request action(x)give description(x)accept verification(x,y)give location(dobj)give procedure(daction)close dialogueTable 4: System Dialogue acts.4 Dialogue System ModelThe dialogue system is modeled as a reinforce-ment learning agent in a Markov Decision Pro-cess framework (Levin et al, 1997).
At everyturn, it interacts with the Simulated User by issu-ing a System Dialogue Act (As,t) along with a setof REs, called the System RE Choices (RECs,t).RECs,t contains the REs that refer to various do-main objects in the dialogue act As,t.
First, thesystem decides the dialogue act to issue using ahand-coded dialogue strategy.
Troubleshooting in-structions are coded in the troubleshooting deci-sion tree1.
Dialogue repair moves include select-ing clarification moves in response to user?s re-quest.
The list of system dialogue acts is givenTable 4.The system issues various repair moves whenthe users are unable to carry out the system?s in-structions due to ignorance, non-understanding orthe ambiguous nature of the instructions.
Thegive description act is used to give the user a de-scription of the domain object previously referredto using a technical term.
It is also used whenthe user requests disambiguation.
Similarly, ac-cept verification is given when the user wants toverify whether the system is referring to a certaindomain object y using the expression x.After selecting the dialogue act As,t, a setof REs must be chosen to refer to each ofthe domain objects/actions used in the dia-logue act.
For instance, the dialogue act re-quest status(router dsl light) requires referencesto be made to domain objects ?router?
and ?DSLlight?.
For each of these references, the systemchooses a RE, creating the System RE ChoiceRECs,t.
In this study, we have 7 domain objectsand they can either be referred to using technical1The Troubleshooting decision tree was hand-built usingguidelines from www.orange.co.uk and is similar to the oneused by their Customer Support personnel77Input: System Dialogue Act As,t, System Referring Expressions Choice RECs,tand User State Su,t: LKu,t, FKu,t, PKu,tStep 1. ?
x ?
RECs,tStep 1a.
if (vocab(x, dobj)?
LKu,t) then next x.Step 1b.
else if (description(x, dobj) & ?
j ((is jargon(j) & vocab(j, dobj) /?
LKu,t))) then next x.Step 1c.
else if (is jargon(x) & (vocab(x, dobj) /?
LKu,t)) then return request description(x).Step 1d.
else if (is ambiguous(x)) then return request disambiguation(x).Step 1e.
else if (description(x, dobj) & ?
j ((is jargon(j) & vocab(j, dobj) ?
LKu,t)))then return request verification(x, j).Step 2. if (?dobj location(dobj) ?
FKprereq & location(dobj) /?
FKu,t)then return request location(dobj).Step 3. else if (?daction procedure(daction) ?
PKprereq & procedure(daction) /?
PKu,t)then return request procedure(daction).Step 4. else if (As,t = request status(dobj))then observe env(dobj, status), return provide info(dobj, status)Step 5. else if (As,t = request action(daction))then manipulate env(daction), return acknowledge.Step 6. else if (As,t = give description(j, d) & description(d, dobj))then add to short term memory(vocab(j, dobj)), return acknowledge.Step 7. else if (As,t = give location(dobj))then add to short term memory(location(dobj)), return acknowledge.Step 8. else if (As,t = give procedure(daction))then add to short term memory(procedure(daction)), return acknowledge.Table 3: Algorithm: Simulated User Action Selectionterms or descriptive expressions.
For instance, theDSL light on the router can be descriptively re-ferred to as the ?second light on the panel?
or us-ing the technical term, ?DSL light?.
Sometimesthe system has to choose between a lesser knowntechnical term and a well-known one.
Some de-scriptive expressions may be underspecified andtherefore can be ambiguous to the user (for ex-ample, ?the black box?).
Choosing inappropri-ate expressions can make the conversation longerwith lots of clarification and repair episodes.
Thiscan lead to long frustrating dialogues, affecting thetask success rate.
Therefore, the dialogue systemmust learn to use appropriate REs in its utterances.The RE choices available to the system are givenin Table 5.The system?s RE choices are based on a partof the dialogue state that records which of thetechnical terms the user knows.
These variablesare initially set to unknown (u).
During the di-alogue, they are updated to user knows (y) oruser doesnot know (n) states.
We therefore recordthe user?s lexical knowledge during the course ofthe dialogue and let the system learn the statisticalusage patterns by itself.
Part of the dialogue state1.
router / black box / black box with lights2.
power light / first light on the panel3.
DSL light / second light on the panel4.
online light / third light on the panel5.
network icon / flashing computer symbol6.
network connections / earth with plug7.
WiFi / wirelessTable 5: System RE choices.relevant to system?s RE choices is given in Table 6.The state can be extended to include other rele-vant information like the usage of various REs bythe user as well to enable alignment with the userthrough priming (Pickering and Garrod, 2004) andpersonal experience (Clark, 1996).
However theyare not yet implemented in the present work.5 Reward functionThe reward function calculates the rewardawarded to the reinforcement learning agent atthe end of each dialogue session.
Successfultask completion is rewarded with 1000 points.Dialogues running beyond 50 turns are deemed78Feature Valuesuser knows router y/n/uuser knows power light y/n/uuser knows dsl light y/n/uuser knows online light y/n/uuser knows network icon y/n/uuser knows network connections y/n/uuser knows wifi y/n/uTable 6: (Part of) Dialogue state for Lexical Align-ment.unsuccessful and are awarded 0 points.
Thenumber of turns in each dialogue varies accordingto the system?s RE choices and the simulateduser?s response moves.
Each turn costs 10 points.The final reward is calculated as follows:TaskCompletionReward(TCR) = 1000TurnCost(TC) = 10TotalTurnCost(TTC) = #(Turns) ?
TCFinalReward = TCR?
TTCThe reward function therefore gives high re-wards when the system produces shorter dia-logues, which is possible by adaptively using ap-propriate REs for each user.6 TrainingThe system was trained to produce an adaptivelexical alignment policy, which can adapt to userswith different lexical knowledge profiles.
Ideally,the system must interact with a number of dif-ferent users in order to learn to align with them.However, with a large number of distinct Bayesianuser profiles (there are 90 possible user profiles),the time taken for learning to converge is exorbi-tantly high.
Hence the system was trained withselected profiles from the distribution.
It wasinitially trained using two user profiles from thevery extremes of the knowledge spectrum pro-duced by the Bayesian model - complete expertsand complete novices.
In this study, we cali-brated all users to know all the factual and proce-dural knowledge components, because the learn-ing exercise was targeted only at the lexical level.With respect to the lexical knowledge, completeexperts knew all the technical terms in the do-main.
Complete novices, on the other hand, knewonly one: power light.
We set the RIu to 10,so that the users do not forget newly learned lexi-cal items for 10 subsequent turns.
Ideally, we ex-pected the system to learn to use technical termswith experts and to use descriptive expressionswith novices and a mixture for intermediates.
Thesystem was trained using SARSA reinforcementlearning algorithm (Sutton and Barto, 1998), withlinear function approximation, for 50000 cycles.It produced around 1500 dialogues and producedan alignment policy (RL1) that adapted to usersafter the first turn which provides evidence aboutthe kind of user the system is dealing with.The system learns to get high reward by pro-ducing shorter dialogues.
By learning to chooseREs by adapting to the lexical knowledge of theuser, it avoids unnecessary clarification and repairepisodes.
It learns to choose descriptive expres-sions for novice users and jargon for expert users.It also learns to use technical terms when all usersknow them (for instance, ?power light?).
Due tothe user?s high retention (10 turns), the systemlearned to use newly learned items later in the di-alogue.We also trained another alignment policy (RL2)with two other intermediate high frequency userlexical profiles.
These profiles (Int1 and Int2)were chosen from either ends of the knowledgespectrum close to the extremes.
Int1 is a knowl-edge profile that is close to the novice end.
Itonly knows two technical terms: ?power light?and ?WiFi?.
On the other hand, Int2 is profilethat is close to the expert end and knows all tech-nical terms except: ?dsl light?
and ?online light?
(which are the least well-known technical termsin the user population).
With respect to the otherknowledge components - factual and procedural,both users know every component equally.
Wetrained the system for 50000 cycles following thesame procedure as above.
This produced an align-ment policy (RL2) that learned to optimize themoves, similar to RL1, but with respect to thegiven distinct intermediate users.Figure 3 shows the overall dialogue reward forthe 2 policies during training.Both policies RL1 and RL2, apart from learn-ing to adapt to the users, also learned not to useambiguous expressions.
Ambiguous expressionslead to confusion and the system has to spend ex-tra turns for clarification.
Therefore both policieslearnt to avoid using ambiguous expressions.Figure 4 shows the dialogue length variation forthe 2 policies during training.797 Evaluation and baselinesWe evaluated both the learned policies using a test-ing simulation and compared the results to otherbaseline hand-coded policies.
Unlike the train-ing simulation, the testing simulation used theBayesian knowledge model to produce all differ-ent kinds of user knowledge profiles.
It producedaround 90 different profiles in varying distribution,resembling a realistic user population.
The testswere run over 250 simulated dialogues each.Several rule-based baseline policies were man-ually created for the sake of comparison:1.
Random - Choose REs at random.2.
Descriptive only - Only choose descriptiveexpressions.
If there is more than one de-scriptive expression it picks one randomly.3.
Jargon only - Chooses the technical terms.4.
Adaptive 1 - It starts with a descriptive ex-pression.
If the user asks for verification, itFigure 3: Final reward for RL1 & RL2.Figure 4: Dialogue length for RL1 & RL2.Policy Avg.
Reward Avg.
LengthRL2 830.4 16.98RL1 812.3 18.77Adaptive 1 809.6 19.04Adaptive 2 792.1 20.79Adaptive 3 780.2 21.98Random 749.8 25.02Desc only 796.6 20.34Jargon only 762.0 23.8Table 7: Rewards and Dialogue Length.switches to technical terms for the rest of thedialogue.5.
Adaptive 2 - It starts with a technical termand switches to descriptive expressions if theuser does not understand in the first turn.6.
Adaptive 3 - This rule-based policy adaptscontinuously based on the previous expres-sion.
For instance, if the user did not un-derstand the technical reference to the currentobject, it uses a descriptive expression for thenext object in the dialogue.The first three policies (random, descriptiveonly and jargon only) are equivalent to policieslearned using user simulations that are not sensi-tive to system?s RE choices.
In such cases, thelearned policies will not have a well-defined strat-egy to choose REs based on user?s lexical knowl-edge.
Table 7 shows the comparative results forthe different policies.
RL (1 & 2) are significantlybetter than all the hand-coded policies.
Also, RL2is significantly better than RL1 (p < 0.05).Ideally the system with complete knowledge ofthe user would be able to finish the dialogue in13 turns.
Similarly, if it got it wrong every timeit would take 28 turns.
From table 7 we see thatRL2 performs better than other policies, with anaverage dialogue length of around 17 turns.
Thelearned policies were able to discover the hid-den dependencies between lexical items that wereencoded in the Bayesian knowledge model.
Al-though trained only on two knowledge profiles, thelearned policies adapt well to unseen users, due tothe generalisation properties of the linear functionapproximation method.
Many unseen states arisewhen interacting with users with new profiles andboth the learned policies generalise very well insuch situations, whereas the baseline policies donot.808 ConclusionIn this paper, we have shown that by using a sta-tistical User Simulation that is sensitive to REchoices we are able to learn NLG policies thatadaptively decide which REs to use based on audi-ence design.
We have shown that the lexical align-ment policies learned with this type of simulationare better than a range of hand-coded policies.Although lexical alignment policies could behand-coded, the designers would need to investsignificant resources every time the list of referringexpressions is revised or the conditions of the dia-logue change.
Using reinforcement learning, near-optimal lexical alignment policies can be learnedquickly and automatically.
This model can be usedin any task where interactions need to be tailoredto different users?
lexical knowledge of the do-main.8.1 Future workLexical alignment in dialogue also happens dueto priming (Pickering and Garrod, 2004) and per-sonal experience (Clark, 1996).
We will examinetrade-offs in various conditions, like ?instruct?
ver-sus ?teach?
and low versus high retention users.Using Wizard-of-Oz studies and knowledge sur-veys, we plan to make the model more data-drivenand realistic (Janarthanam and Lemon, 2009).
Wewill also evaluate the learned policies with realusers.AcknowledgementsThe research leading to these results has re-ceived funding from the European Community?sSeventh Framework (FP7) under grant agree-ment no.
216594 (CLASSiC Project www.classic-project.org), EPSRC project no.
EP/E019501/1,and the British Council (UKIERI PhD Scholar-ships 2007-08).ReferencesA.
Bell.
1984.
Language style as audience design.Language in Society, 13(2):145?204.H.
H. Clark.
1996.
Using Language.
Cambridge Uni-versity Press, Cambridge.K.
Georgila, J. Henderson, and O.
Lemon.
2005.Learning User Simulations for Information StateUpdate Dialogue Systems.
In Proceedings of Eu-rospeech/Interspeech.E.
A. Issacs and H. H. Clark.
1987.
References inconversations between experts and novices.
Journalof Experimental Psychology: General, 116:26?37.S.
Janarthanam and O.
Lemon.
2008.
User simulationsfor online adaptation and knowledge-alignment inTroubleshooting dialogue systems.
In Proc.
SEM-dial?08.S.
Janarthanam and O.
Lemon.
2009.
A Wizard-of-Ozenvironment to study Referring Expression Genera-tion in a Situated Spoken Dialogue Task.
In Proc.ENLG?09.K.
Komatani, S. Ueno, T. Kawahara, and H. G. Okuno.2003.
Flexible Guidance Generation using UserModel in Spoken Dialogue Systems.
In Proc.ACL?03.O.
Lemon.
2008.
Adaptive Natural Language Genera-tion in Dialogue using Reinforcement Learning.
InProc.
SEMdial?08.E.
Levin, R. Pieraccini, and W. Eckert.
1997.
Learn-ing Dialogue Strategies within the Markov DecisionProcess Framework.
In Proceedings of ASRU97.R.
Molich and J. Nielsen.
1990.
Improving a Human-Computer Dialogue.
Communications of the ACM,33-3:338?348.M.
J. Pickering and S. Garrod.
2004.
Toward a mech-anistic psychology of dialogue.
Behavioral andBrain Sciences, 27:169?225.V.
Rieser and O.
Lemon.
2009.
Natural LanguageGeneration as Planning Under Uncertainty for Spo-ken Dialogue Systems.
In Proc.
EACL?09.J.
Schatzmann, K. WeilHammer, M. N. Stuttle, andS.
J.
Young.
2006.
A Survey of Statistical User Sim-ulation Techniques for Reinforcement Learning ofDialogue Management Strategies.
Knowledge Engi-neering Review, pages 97?126.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye,and S. J.
Young.
2007.
Agenda-based User Simula-tion for Bootstrapping a POMDP Dialogue System.In Proceedings of HLT/NAACL 2007.D.
Schlangen.
2004.
Causes and strategies for request-ing clarification in dialogue.
Proceedings of the 5thSIGdial Workshop on Discourse and Dialogue (SIG-DIAL 04), Boston.R.
Sutton and A. Barto.
1998.
Reinforcement Learn-ing.
MIT Press.J.
Williams.
2007.
Applying POMDPs to DialogSystems in the Troubleshooting Domain.
In ProcHLT/NAACL Workshop on Bridging the Gap: Aca-demic and Industrial Research in Dialog Technol-ogy.81
