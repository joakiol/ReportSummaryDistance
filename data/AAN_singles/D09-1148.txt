Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1428?1436,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPUsing Word-Sense Disambiguation Methods to Classify Web Queries byIntentEmily PitlerComputer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104, USAepitler@seas.upenn.eduKen ChurchJohns Hopkins UniversityHuman Language Technology Center of ExcellenceBaltimore, MD 21211Kenneth.Church@jhu.eduAbstractThree methods are proposed to classifyqueries by intent (CQI), e.g., navigational,informational, commercial, etc.
Follow-ing mixed-initiative dialog systems, searchengines should distinguish navigationalqueries where the user is taking the ini-tiative from other queries where there aremore opportunities for system initiatives(e.g., suggestions, ads).
The query in-tent problem has a number of useful appli-cations for search engines, affecting howmany (if any) advertisements to display,which results to return, and how to ar-range the results page.
Click logs areused as a substitute for annotation.
Clickson ads are evidence for commercial in-tent; other types of clicks are evidence forother intents.
We start with a simple Na?
?veBayes baseline that works well when thereis plenty of training data.
When train-ing data is less plentiful, we back offto nearby URLs in a click graph, usinga method similar to Word-Sense Disam-biguation.
Thus, we can infer that de-signer trench is commercial because it isclose to www.saksfifthavenue.com, whichis known to be commercial.
The baselinemethod was designed for precision andthe backoff method was designed for re-call.
Both methods are fast and do not re-quire crawling webpages.
We recommenda third method, a hybrid of the two, thatdoes no harm when there is plenty of train-ing data, and generalizes better when thereisn?t, as a strong baseline for the CQI task.1 Classify Queries By Intent (CQI)Determining query intent is an important prob-lem for today?s search engines.
Queries are short(consisting of 2.2 terms on average (Beitzel et al,2004)) and contain ambiguous terms.
Search en-gines need to derive what users want from this lim-ited source of information.
Users may be search-ing for a specific page, browsing for information,or trying to buy something.
Guessing the correctintent is important for returning relevant items.Someone searching for designer trench is likelyto be interested in results or ads for trench coats,while someone searching for world war I trenchmight be irritated by irrelevant clothing advertise-ments.Broder (2002) and Rose and Levinson (2004)categorized queries into those with navigational,informational, and transactional or resource-seeking intent.
Navigational queries are queriesfor which a user has a particular web page in mindthat they are trying to navigate to, such as grey-hound bus.
Informational queries are those likeSan Francisco, in which the user is trying to gatherinformation about a topic.
Transactional queriesare those like digital camera or download adobereader, where the user is seeking to make a trans-action or access an online resource.Knowing the intent of a query greatly affects thetype of results that are relevant.
For many queries,Wikipedia articles are returned on the first pageof results.
For informational queries, this is usu-ally appropriate, as a Wikipedia article containssummaries of topics and links to explore further.However, for navigational or transactional queries,Wikipedia is not as appropriate.
A user lookingfor the greyhound bus homepage is probably notinterested in facts about the company.
Similarly,someone looking to download adobe reader willnot be interested in Wikipedia?s description of theproduct?s history.
Conversely, for informationalqueries, Wikipedia articles tend to be appropriatewhile advertisements are not.
The user searchingfor world war I trench might find the Wikipediaarticle on trench warfare useful, while he is prob-1428(a) The advertisements and related searches are probably more likely to be clicked on thanthe top result for designer trench.
(b) The top result will receive more clicks than the spelling suggestion.
Wikipedia oftenreceives lots of clicks, but not for commercial queries like bestbuy.Figure 1: Results pages from two major search engines.
A search results page has limited real estate thatmust be divided between search results, spelling suggestions, query suggestions, and ads.ably not interested in purchasing clothing, or evenWorld War I related products.
We noticed empiri-cally that queries in the logs tend to have a highproportion of clicks on the Wikipedia article orthe ads, but almost never both.
The Wikipediapage for Best Buy in Figure 1(b) is probably awaste of space.
Knowing whether a particularquery is navigational, informational, or transac-tional would improve search and advertising rel-evance.After a query is issued, search engines returna list of results, and possibly also advertisements,suggestions of related searches, and spelling sug-gestions.
For different queries, these alternativeshave varying utilities to the users.
Consider thequeries in Figures 1(a) and 1(b).
For designertrench, the advertisements may well be more use-ful to the user than the standard set of results.
Thequery suggestions for designer trench all wouldhelp refine the query, whereas the suggestions forbestbuy are less useful, as they would either re-turn the same set of results or take the user to BestBuy?s competitors?
sites.
The spelling suggestionfor best buy instead of bestbuy is also unnecessary.Devoting more page space to the content that islikely to be clicked on could help improve the userexperience.In this paper we consider the task of: given aclass of queries, which types of answer (standardsearch, ads, query suggestions, or spelling sug-1429gestions) are likely to be clicked on?
Typos willtend to have more clicks on the spelling sugges-tions, informational queries will have more clickson Wikipedia pages, and commercial queries willhave more clicks on the ads.
The observed behav-ior of where users click tells us something aboutthe hidden intentions of the users when they issuethat query.We focus on commercial intent (Dai et al,2006), the intent to purchase a product or service,to illustrate our method of predicting query intent.The business model of web search today is heav-ily dependent on advertising.
Advertisers bid onqueries, and then the search results page also con-tains ?sponsored?
sites by the advertisers who wonthe auction for that query.
It is thus advantageousfor the advertisers to bid on queries which are mostlikely to result in a commercial transaction.
Ifa query is classified as likely implying commer-cial intent, but the advertisers have overlooked thisquery, then the search engine may want to sug-gest that advertisers bid on that query.
The searchengine may also want to treat queries classifiedas having commercial intent differently, by rear-ranging the appearance of the page, or by showingmore or fewer advertisements.This paper starts with a simple Na?
?ve Bayesbaseline to classify queries by intent (CQI).
Super-vised methods work well, especially when there isplenty of annotated data for testing and training.Unfortunately, since we don?t have as much anno-tated data as we might like, we propose two work-arounds:1.
Use click logs as a substitute for annotateddata.
Clicks on ads are evidence for commer-cial intent; other types of clicks are evidencefor other intents.2.
We propose a method similar to Yarowsky(1995) to generalize beyond the training set.2 Related WorkClick logs have been used for a variety of tasksinvolved in information retrieval, including pre-dicting which pages are the best results for queries(Piwowarski and Zaragoza, 2007; Joachims, 2002;Xue et al, 2004), choosing relevant advertise-ments (Chakrabarti et al, 2008), suggesting re-lated queries (Beeferman and Berger, 2000), andpersonalizing results (Tan et al, 2006).
Queriesthat have a navigational intent tended to havea highly skewed click distribution, while usersclicked on a wider range of results after issuinginformational queries.
Lee et al (2005) used theclick distributions to classify navigational versusinformational intents.While navigational, informational, andresource-seeking are very broad intentions, otherresearchers have looked at personalization andintent on a per user basis.
Downey et al (2008)use the last URL visited in a session or the lastsearch engine result visited as a proxy for theuser?s information goal, and then looked at thecorrespondence between information needs andqueries (how the goals are expressed).We are interested in a granularity of intentin between navigational/informational/resource-seeking and personalized intents.
For these sortsof intents, the web pages associated with queriesprovide useful information.
To classify queriesinto an ontology of commercial queries, Broderet al (2007) found that a classifier that used thetext of the top result pages performed much bet-ter than a classifier that used only the query string.While the results are quite good on their hierarchyof 6000 types of commercial intents, they manu-ally constructed about 150 hand-picked exampleseach for each of the 6000 intents.
Beitzel et al(2005) do semi-supervised learning over the querylogs to classify queries into topics, but also trainwith hundreds of thousands of manually annotatedqueries.
Thus, while we also use the query logsand the identities of web pages of associated witheach query, we are interested in finding methodsthat can be applied when that much annotation isprohibitive.Semi-supervised methods over the click graphmake it possible to train classifiers after startingfrom a much smaller set of seed queries.
Li et al(2008) used the semi-supervised learning methoddescribed in Zhou et al (2004) to gain a muchlarger training set of examples, and then trainedclassifiers for product search or job search on theexpanded set.
Random walk methods over theclick graph have also been used to propagate re-lations between URLs, for tasks such as finding?adult?
content (Craswell and Szummer, 2007)and suggesting related queries (Antonellis et al,2008) and content (Baluja et al, 2008).
In ourwork we also seek to classify query intent us-ing the click graph, but we demonstrate the ef-fectiveness of a simple method by building deci-1430sion lists of URLs.
In addition, we evaluate ourmethod automatically by using user click rates,rather than assembling hand-labeled examples fortraining and testing.Dai et al (2006) also classified queries by com-mercial intent, but their method involved crawlingthe top landing pages for each query, which canbe quite time-consuming.
In this paper we investi-gate the commercial intent problem when crawlingpages is not feasible, and use only the identities ofthe top URLs.3 Using Click Logs as a Substitute forAnnotationPrior work has used click logs in lieu of manualannotations of relevance ratings, either of web-pages (Joachims, 2002) or of sponsored search ad-vertisements (Ciaramita et al, 2008).
Here we usethe click logs as a large-scale source of intents.Logs from Microsoft?s Live Search are used fortraining and test purposes.
Logs from May 2008were used for training, and logs from June 2008were used for testing.The logs distinguish four types of clicks: (a)search results, (b) ads, (c) spelling suggestions and(d) query suggestions.
Some prototypical queriesof each type are shown in Table 1.
As mentionedabove, clicks on ads are evidence for commercialintent; other types of clicks are evidence for otherintents.
The query, ebay official, is assumed to becommercial intent, because a large fraction of theclicks are on ads.
In contrast, typos tend to haverelatively more clicks on ?did-you-mean?
spellingsuggestions.The query logs contain over a terabyte ofdata for each day, and our experiments weredone using months of logs at a time.
Weused SCOPE (Chaiken et al, 2008), a script-ing programming language designed for doingMap-Reduce (Dean and Ghemawat, 2004) stylecomputations, to distribute the task of aggre-gating the counts of each query over thousandsof servers.
As the same query is often issuedseveral times by multiple users across an en-tire month of search logs, we summarize eachquery with four ratios?search results clicks:overallclicks, ad clicks:overall clicks, spelling sugges-tion clicks:overall clicks, and query suggestionclicks:overall clicks.A couple of steps were taken to ensure reliableratios.
We are classifying types, not tokens, andso limited ourselves to those queries with 100 ormore clicks.
This still leaves us with over half amillion distinct queries for training and for test-ing, yet alows us to use click ratios as a substitutefor annotating these huge data sets.
If a query wasonly issued once and the user clicked on an ad,that may be more a reflection of the user, ratherthan reflecting that the query is 100% commer-cial.
In addition, the ratios compare clicks of onetype with clicks of another, rather than compar-ing clicks with impressions.
There is less risk of afailure to find fallacy if we count events (clicks) in-stead of non-events (non-clicks).
There are manyreasons for non-clicks, only some of which tell usabout the meaning of the query.
There are bots thatcrawl pages and never click.
Many links can?t beseen (e.g., if they are below the fold).Queries are labeled as positive examples ofcommercial intent if their ratio is in the top half ofthe training set, and negative otherwise.
A similarprocedure is used to label queries with the threeother intents.Our task is to predict future click patterns basedon past click patterns.
Note that a query may ap-pear in both the test set and the training set, al-though not necessarily with the same label.
In fact,because of the robustness requirement of 100+clicks, many queries appear in both sets; 506,369out of 591,122 of the test queries were also presentin the training month.
The overlap reflects naturalprocesses on the web, with a long tail (of queriesthat will never be seen again) and a big fat head (ofqueries that come up again and again).
Throwingaway the overlap would both drastically reduce thesize of the data and make the problem less realisticfor a commercial application.We therefore report results on various trainingset sizes so that we can show both: (a) the abil-ity of the proposed method to generalize to unseenqueries, and (b) the high performance of the base-lines in a realistic setting.
We vary the number ofnew queries by training the methods on subsets of20%, 40%, 60%, 80%, and 100% of the positiveexamples (along with all the negative examples)in the training set.
This led to the test set having17%, 34%, 52%, 67%, and 86% actual overlap ofthese queries, respectively, with the training sets.1431Click Type Query Type Example(Area on Results Page) (Intent)Spelling Suggestion Typo www.lastmintue.com.auAd Commercial Intent ebay officialQuery Suggestion Suggestible sears employees (where there are some popular query suggestionsindicating how current employees can navigate to the benefits site,as well as how others can apply for employment)Search Result Standard Search craigslist, denver, coTable 1: Queries with a high percentage of clicks in each category4 Three CQI Methods4.1 Method 1: Look-up BaselineThe baseline method checks if a query was presentin the training set, and if so, outputs the label fromthe training set.
If the query was not present, itbacks off to the appropriate default label: ?non-commercial?
for the commercial intent task (and?non-suggestible?, ?not a typo?, etc.
for the otherCQI tasks).
This very simple baseline methodis effective because the ratios tend to be fairlystable from one month to the next.
The query,ebay official, for example, has relatively high adclicks in both the training month as well as thetest month.
The next section will propose an al-ternative method to address the main weakness ofthe baseline method, the inability to generalize be-yond the queries in the training set.Figure 2: saks and bluefly trench coats are knownto be commercial, while world war I trench isknown to be non-commercial.
What about de-signer trench?
We can classify it as commercialbecause it shares URLs with the known commer-cial queries.4.2 Method 2: Using Click Graph Context toGeneralize Beyond the Queries in theTraining SetTo address the generalization concern, we proposea method inspired by Yarowsky (1994).
Wordsense disambiguation is a classic problem in nat-ural language processing.
Some words have mul-tiple senses; for instance, bank can either meana riverbank or a financial institution, and for var-ious tasks such as information retrieval, parsing,or information extraction, it is useful to be able todifferentiate between the possible meanings.When a word is being used in each sense, ittends to appear in a different context.
For exam-ple, if the word muddy is nearby bank, the authoris probably using the riverbank sense of the term,while if the word deposit is nearby, the word isprobably being used with the financial sense.Yarowksy (1995) thus creates a list of each pos-sible context, sorted by how strong the evidence isfor a particular sense.
To classify a new example,Yarowsky (1994) finds the most informative collo-cation pattern that applies to the test example.In this work, rather than using the surroundingwords as context as in text classification, we con-sider the surrounding URLs in the click graph ascontext.
A sample portion of the click graph isshown in figure 2.
The figure shows queries onthe left and URLs on the right.
The click graphwas computed on a very large sample of logs com-puted well before the training period.
There is anedge from a query q to a URL u if at least 10 usersissued q and then clicked on u.For each URL, we look at its neighboringqueries and calculate the log likelihood ratio oftheir labels in the training set.
We classify a newquery q according to URL?, the neighboring URLwith the strongest opinion (highest absolute valueof the log likelihood ratio).
That is, we computeURL?with:1432argmaxUi?Nbr(q)????logPr(Intent|Ui)Pr(?Intent|Ui)???
?If the neighboring opinion is positive (that is,Pr(Intent|URL?)
> Pr(?Intent|URL?
)), thenthe query q is assigned a positive label.
Otherwise,q is assigned a negative label.In Figure 2, we classify designer trench as acommercial query based on the neighbor withthe strongest opinion.
In this case, therewas a tie between two neighbors with equallystrong opinions: www.saksfifthavenue.com andwww.bluefly.com/Designer-Trench-Coats.
Bothneighbors are associated with queries that werelabeled commercial in the training set: saks andbluefly trench coats, respectively.This method allows the labels of training setqueries to propagate through the URLs to new testset queries.4.3 Method 3: Hybrid (?Better Together?
)We recommend a hybrid of the two methods:?
Method 1: the look-up baseline?
Method 2: use click graph context to gener-alize beyond the queries in the training setMethod 1 is designed for precision and method 2is designed for recall.
The hybrid uses method1 when applicable, and otherwise, backs off tomethod 2.5 Results5.1 Commercial IntentTable 2 and Figures 3(a) and 3(b) compare the per-formance on the proposed hybrid method with thebaseline.
When there is plenty of training mate-rial, both methods perform about equally well (thelook-up baseline has an F-score of 84.1%, com-pared with the hybrid method?s F-score of 85.3%),but generalization becomes important when train-ing data is severely limited.
Figure 3(a) showsthat the proposed method does no harm and mighteven help a little when there is plenty of trainingdata.
The hybrid?s main benefit is generalizationto queries beyond the training set.
If we severelylimit the size of the training set to just 20% of themonth, as in Figure 3(b), then the proposed hybridmethod is substantially better than the baseline.
Inthis case, the proposed hybrid method?s F-scoreis 65.8%, compared with the look-up method?s F-score of 28.4%.5.2 Other types of clicksTable 3 and Figures 4(a) and 4(b) show a similarpattern for the query suggestion task.
In fact, thepattern is perhaps even stronger for the query sug-gestion task than commercial intent.
When the fulltraining set is used, the hybrid method achievesan F-score of 91.9% (precision = 91.5%, recall =92.3%).
When only 20% of the training data isused, the hybrid method has an F-score of 73.9%,compared with the baseline?s F-score of 29.6%.
Asimilar pattern was observed for clicks on searchresults.The one exception is the spelling suggestiontask, where the context heuristic proved ineffec-tive, for reasons that should not be surprising inretrospect.
Click graph distance is an effectiveheuristic for many intents, but not for typos.
Userswho issue misspelled the query have the samegoals as users who correctly spell the query, sowe shouldn?t expect URLs to be able to differ-entiate them.
For misspelled queries, for exam-ple, yuotube, there are correctly spelled queries,like youtube, with the same intent that will tend tobe associated with the same set of URLs (such aswww.youtube.com).6 Conclusion and Future WorkWe would like to be able to distinguish webqueries by intent.
Unfortunately, we don?t haveannotated data for query intent, but we do haveaccess to large quantities of click logs.
The logsdistinguish four types of clicks: (a) search results,(b) ads, (c) spelling suggestions and (d) query sug-gestions.
Clicks on ads are evidence for commer-cial intent; other types of clicks are evidence forother intents.
Click logs are huge sources of data,and while there are privacy concerns, anonymizedlogs are beginning to be released for research pur-poses (Craswell et al, 2009).Besides commercial intent, queries can also bedivided into two broader classes: queries in whichthe user is browsing and queries for which the useris navigating.
Clicks on the ads and query sug-gestions indicate that users are browsing and will-ing to look at these alternative suggestions, whileclicks on the search results indicate that the userswere navigating to what they were searching for.Clicks on typos indicate neither, as presumably theusers are not entering typos on purpose.Just as dialogue management systems learnpolicies for when to allow user initiative (the user1433(a) (b)Figure 3: Better together: proposed hybrid is no worse than baseline (left) and generalizes better tounseen tail queries (right).
The two panels are the same, except that the training set was reduced on theright to test generalization error.
(a) (b)Figure 4: Similar to Figures 3(a) and 3(b), adding the decision list method generalizes over the look-upmethod for the ?suggestible?
task.can respond in an open way) versus system ini-tiative (the system asks the user questions with arestricted set of possible answers) (Rela?no et al,1999; Scheffler and Young, 2002; Singh et al,2002), search engines may want to learn policiesfor when the user just wants the search results orwhen the user is open to suggestions.
When userswant help (they want the search engine to suggestresults), more space on the page should be devotedto the ads and the query suggestions.
When theusers know what it is they want, more of the pageshould be given to the search results they askedfor.We started with a simple baseline for predictingclick location that had great precision, but didn?tgeneralize well beyond the queries in the train-ing set.
To improve recall, we proposed a con-text heuristic that backs off in the click graph.The backoff method is similar to Yarowsky?s WordSense Disambiguation method, except that contextis defined in terms of URLs nearby in click graphdistance, as opposed to words nearby in the text.Our third method, a hybrid of the baselinemethod and the backoff method, is the strongestbaseline we have come up with.
The evaluationshowed that the hybrid does no harm when thereis plenty of training data, and generalizes betterwhen there isn?t.A direction for further research would be to seeif propagating query intent through URLs that arenot direct neighbors but are further away, perhapsthrough random walk methods (Baluja et al, 2008;1434Training Size F-score Precision / RecallBaseline Method 2 Hybrid Baseline Method 2 Hybrid100% 84.1 75.6 85.3 88.2 / 80.4 76.6 / 74.6 85.7 / 85.080% 74.4 74.8 83.5 88.2 / 64.3 79.3 / 70.7 86.7 / 80.660% 62.4 72.9 80.7 88.3 / 48.2 82.5 / 65.3 87.9 / 74.640% 47.9 70.1 76.0 77.5 / 34.7 78.5 / 63.3 80.7 / 66.020% 28.4 62.5 65.8 77.6 / 17.4 75.9 / 53.1 74.3 / 59.1Table 2: The baseline and hybrid methods have comparable F-scores when there is plenty of trainingdata, but generalization becomes important when training data is severely limited.
The proposed hybridmethod generalizes better as indicated by the widening gap in F-scores with smaller and smaller trainingsets.Training Size F-score Precision / RecallBaseline Method 2 Hybrid Baseline Method 2 Hybrid100% 91.0 86.2 91.9 94.9 / 87.4 90.7 / 82.3 91.5 / 92.380% 80.5 85.2 90.6 94.9 / 69.9 91.6 / 79.7 91.9 / 89.460% 67.6 83.3 88.6 94.9 / 52.4 92.6 / 75.8 92.3 / 85.140% 51.0 79.5 84.7 94.9 / 34.9 87.6 / 72.7 93.0 / 77.820% 29.6 69.8 73.9 81.5 / 18.1 90.6 / 56.8 94.0 / 60.8Table 3: F-scores on the query suggestion task.
As in the commercial intent task, the proposed hybridmethod does no harm when there is plenty of training data, but generalizes better when training data isseverely limited.Antonellis et al, 2008) improves classification.Similar methods could be applied in future workto many other applications such labeling queriesand URLs by: language, market, location, time,intended for a search vertical (such as medicine,recipes), intended for a type of answer (maps, pic-tures), as well as inappropriate intent (porn, spam).In addition to click type, there are many otherfeatures in the logs that could prove useful forclassifying queries by intent, e.g., who issued thequery, when and where.
Similar methods couldalso be used to personalize search (Teevan et al,2008); for queries that mean different things to dif-ferent people, the Yarowsky method could be ap-plied to variables such as user, time and place, sothe results reflect what a particular user intendedin a particular context.7 AcknowledgmentsWe thank Sue Dumais for her helpful commentson an early draft of this work.
We would also liketo thank the members of the Text Mining, Search,and Navigation (TMSN) group at Microsoft Re-search for useful discussions and the anonymousreviewers for their helpful comments.ReferencesI.
Antonellis, H. Garcia-Molina, and C.C.
Chang.2008.
Simrank++: query rewriting through linkanalysis of the clickgraph (poster).
WWW.S.
Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,S.
Kumar, D. Ravichandran, and M. Aly.
2008.Video suggestion and discovery for youtube: takingrandom walks through the view graph.
WWW.D.
Beeferman and A. Berger.
2000.
Agglomerativeclustering of a search engine query log.
In SIGKDD,pages 407?416.S.M.
Beitzel, E.C.
Jensen, A. Chowdhury, D. Gross-man, and O. Frieder.
2004.
Hourly analysis of avery large topically categorized web query log.
SI-GIR, pages 321?328.S.M.
Beitzel, E.C.
Jensen, O. Frieder, D.D.
Lewis,A.
Chowdhury, and A. Kolcz.
2005.
Improvingautomatic query classification via semi-supervisedlearning.
ICDM, pages 42?49.A.Z.
Broder, M. Fontoura, E. Gabrilovich, A. Joshi,V.
Josifovski, and T. Zhang.
2007.
Robust classifi-cation of rare queries using web knowledge.
SIGIR,pages 231?238.A.
Broder.
2002.
A taxonomy of web search.
SIGIR,36(2).R.
Chaiken, B. Jenkins, P.?A.
Larson, B. Ramsey,D.
Shakib, S. Weaver, and J. Zhou.
2008.
SCOPE:1435Easy and efficient parallel processing of massivedata sets.
Proceedings of the VLDB Endowmentarchive, 1(2):1265?1276.D.
Chakrabarti, D. Agarwal, and V. Josifovski.
2008.Contextual advertising by combining relevance withclick feedback.
WWW.M.
Ciaramita, V. Murdock, and V. Plachouras.
2008.Online learning from click data for sponsoredsearch.N.
Craswell and M. Szummer.
2007.
Random walkson the click graph.
In Proceedings of the 30th an-nual international ACM SIGIR conference on Re-search and development in information retrieval,pages 239?246.N.
Craswell, R. Jones, G. Dupret, and E. Viegas (Con-ference Chairs).
2009.
Wscd ?09: Proceedings ofthe 2009 workshop on web search click data.H.K.
Dai, L. Zhao, Z. Nie, J.R. Wen, L. Wang, andY.
Li.
2006.
Detecting online commercial intention(OCI).
WWW, pages 829?837.J.
Dean and S. Ghemawat.
2004.
MapReduce: Sim-plified Data Processing on Large Clusters.
OSDI,pages 137?149.D.
Downey, S. Dumais, D. Liebling, and E. Horvitz.2008.
Understanding the relationship betweensearchers?
queries and information goals.
In CIKM.T.
Joachims.
2002.
Optimizing search engines usingclickthrough data.
In Proceedings of the ACM Con-ference on Knowledge Discovery and Data Mining(KDD), ACM.U.
Lee, Z. Liu, and J. Cho.
2005.
Automatic identifi-cation of user goals in Web search.
In WWW, pages391?400.X.
Li, Y.Y.
Wang, and A. Acero.
2008.
Learningquery intent from regularized click graphs.
In SI-GIR, pages 339?346.B.
Piwowarski and H. Zaragoza.
2007.
Predictive userclick models based on click-through history.
In Pro-ceedings of the sixteenth ACM conference on Con-ference on information and knowledge management,pages 175?182.J.
Rela?no, D. Tapias, M.
Rodr?
?guez, M. Charfuel?an,and L. Hern?andez.
1999.
Robust and flexiblemixed-initiative dialogue for telephone services.
InProceedings of EACL.D.E.
Rose and D. Levinson.
2004.
Understanding usergoals in web search.
WWW, pages 13?19.K.
Scheffler and S. Young.
2002.
Automatic learn-ing of dialogue strategy using dialogue simulationand reinforcement learning.
In Proceedings of HLT,pages 12?19.S.
Singh, D. Litman, M. Kearns, and M. Walker.
2002.Optimizing dialogue management with reinforce-ment learning: Experiments with the NJFun sys-tem.
Journal of Artificial Intelligence Research,16(1):105?133.Bin Tan, Xuehua Shen, and ChengXiang Zhai.
2006.Mining long-term search history to improve searchaccuracy.
pages 718?723.
KDD.J.
Teevan, S.T.
Dumais, and D.J.
Liebling.
2008.
Topersonalize or not to personalize: modeling querieswith variation in user intent.
SIGIR, pages 163?170.Gui-Rong Xue, Hua-Jun Zeng, Zheng Chen, YongYu, Wei-Ying Ma, WenSi Xi, and WeiGuo Fan.2004.
Optimizing web search using web click-through data.
In CIKM ?04: Proceedings of the thir-teenth ACM international conference on Informa-tion and knowledge management, pages 118?126.D.
Yarowsky.
1994.
Decision lists for lexical ambigu-ity resolution: Application to accent restoration inSpanish and French.
ACL, pages 88?95.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
ACL, pages189?196.D.
Zhou, O. Bousquet, T.N.
Lal, J. Weston, andB.
Scholkopf.
2004.
Learning with Local andGlobal Consistency.
In NIPS.1436
