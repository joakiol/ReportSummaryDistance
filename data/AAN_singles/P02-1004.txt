Machine-learned contexts for linguistic operationsin German sentence realizationMichael GAMON, Eric RINGGER, Simon CORSTON-OLIVER, Robert MOOREMicrosoft ResearchMicrosoft CorporationRedmond, WA 98052{mgamon, ringger, simonco, bobmoore}@microsoft.comAbstractWe show that it is possible to learn thecontexts for linguistic operations whichmap a semantic representation to asurface syntactic tree in sentencerealization with high accuracy.
We castthe problem of learning the contexts forthe linguistic operations asclassification tasks, and applystraightforward machine learningtechniques, such as decision treelearning.
The training data consist oflinguistic features extracted fromsyntactic and semantic representationsproduced by a linguistic analysissystem.
The target features are extractedfrom links to surface syntax trees.
Ourevidence consists of four examples fromthe German sentence realization systemcode-named Amalgam: caseassignment, assignment of verb positionfeatures, extraposition, and syntacticaggregation1 IntroductionThe last stage of natural language generation,sentence realization, creates the surface stringfrom an abstract (typically semantic)representation.
This mapping from abstractrepresentation to surface string can be direct, or itcan employ intermediate syntactic representationswhich significantly constrain the output.Furthermore, the mapping can be performedpurely by rules, by application of statisticalmodels, or by a combination of both techniques.Among the systems that use statistical ormachine learned techniques in sentencerealization, there are various degrees ofintermediate syntactic structure.
Nitrogen(Langkilde and Knight, 1998a, 1998b) produces alarge set of alternative surface realizations of aninput structure (which can vary in abstractness).This set of candidate surface strings, representedas a word lattice, is then rescored by a word-bigram language model, to produce the best-ranked output sentence.
FERGUS (Bangalore andRambow, 2000), on the other hand, employs amodel of syntactic structure during sentencerealization.
In simple terms, it adds a tree-basedstochastic model to the approach taken by theNitrogen system.
This tree-based model chooses abest-ranked XTAG representation for a givendependency structure.
Possible linearizations ofthe XTAG representation are generated and thenevaluated by a language model to pick the bestpossible linearization, as in Nitrogen.In contrast, the sentence realization systemcode-named Amalgam (A Machine LearnedGeneration Module) (Corston-Oliver et al, 2002;Gamon et al, 2002b) employs a series oflinguistic operations which map a semanticrepresentation to a surface syntactic tree viaintermediate syntactic representations.
Thecontexts for most of these operations in Amalgamare machine learned.
The resulting syntactic treecontains all the necessary information on its leafnodes from which a surface string can be read.The goal of this paper is to show that it ispossible to learn accurately the contexts forlinguistically complex operations in sentencerealization.
We propose that learning the contextsfor the application of these linguistic operationscan be viewed as per-operation classificationproblems.
This approach combines advantages ofa linguistically informed approach to sentencerealization with the advantages of a machineComputational Linguistics (ACL), Philadelphia, July 2002, pp.
25-32.Proceedings of the 40th Annual Meeting of the Association forlearning approach.
The linguistically informedapproach allows us to deal with complex linguisticphenomena, while machine learning automates thediscovery of contexts that are linguisticallyrelevant and relevant for the domain of the data.The machine learning approach also facilitatesadaptation of the system to a new domain orlanguage.
Furthermore, the quantitative nature ofthe machine learned models permits finerdistinctions and ranking among possible solutions.To substantiate our claim, we provide fourexamples from Amalgam: assignment of case,assignment of verb position features,extraposition, and syntactic aggregation.2 Overview of AmalgamAmalgam takes as its input a sentence-levelsemantic graph representation with fixed lexicalchoices for content words (the logical form graphof the NLPWin system ?
see (Heidorn, 2000)).This representation is first degraphed into a tree,and then gradually augmented by the insertion offunction words, assignment of case and verbposition features, syntactic labels, etc., andtransformed into a syntactic surface tree.
Agenerative statistical language model establisheslinear order in the surface tree (Ringger et al, inpreparation), and a surface string is generatedfrom the leaf nodes.
Amalgam consists of eightstages.
We label these ML (machine-learnedcontext) or RB (rule-based).Stage 1 Pre-processing (RB):?
degraphing of the semantic representation?
retrieval of lexical informationStage 2 Flesh-out (ML):?
assignment of syntactic labels?
insertion of function words?
assignment of case and verb positionfeaturesStage 3 Conversion to syntactic tree (RB):?
introduction of syntactic representationfor coordination?
splitting of separable prefix verbs basedon both lexical information andpreviously assigned verb position features?
reversal of heads (e.g., in quantitativeexpressions) (ML)Stage 4 Movement:?
extraposition (ML)?
raising, wh movement (RB)Stage 5 Ordering (ML):?
ordering of constituents and leaf nodes inthe treeStage 6 Surface cleanup (ML):?
lexical choice of determiners and relativepronouns?
syntactic aggregationStage 7 Punctuation (ML)Stage 8 Inflectional generation (RB)All machine learned components, with theexception of the generative language model forordering of constituents (stage 5), are decision treeclassifiers built with the WinMine toolkit(Chickering et al, 1997; Chickering, nd.).
Thereare a total of eighteen decision tree classifiers inthe system.
The complexity of the decision treesvaries with the complexity of the modeled task.The number of branching nodes in the decisiontree models in Amalgam ranges from 3 to 447.3 Data and feature extractionThe data for all of the models were drawn from aset of 100,000 sentences from technical softwaremanuals and help files.
The sentences areanalyzed by the NLPWin system, which providesa syntactic and logical form analysis.
Nodes in thelogical form representation are linked to thecorresponding syntactic nodes, allowing us tolearn contexts for the mapping from the semanticrepresentation to a surface syntax tree.
The data issplit 70/30 for training versus model parametertuning.
For each set of data we built decision treesat several different levels of granularity (bymanipulating the prior probability of treestructures to favor simpler structures) and selectedthe model with the maximal accuracy asdetermined on the parameter tuning set.
Allmodels are then tested on data extracted from aseparate blind set of 10,000 sentences from thesame domain.
For both training and test, we onlyextract features from sentences that have receiveda complete, spanning parse: 85.14% of thesentences in the training and parameter tuning set,and 84.59% in the blind test set fall into thatcategory.
Most sentences yield more than onetraining case.We attempt to standardize as much as possiblethe set of features to be extracted.
We exploit thefull set of features and attributes available in theanalysis, instead of pre-determining a small set ofpotentially relevant features (Gamon et al,2002b).
This allows us to share the majority ofcode between the individual feature extractiontasks.
More importantly, it enables us to discovernew linguistically interesting and/or domain-specific generalizations from the data.
Typically,we extract the full set of available analysisfeatures of the node under investigation, its parentand its grandparent, with the only restriction beingthat these features need to be available at the stagewhere the model is consulted at generation run-time.
This provides us with a sufficiently largestructural context for the operations.
In addition,for some of the models we add a small set offeatures that we believe to be important for thetask at hand, and that cannot easily be expressedas a combination of analysis features/attributes onconstituents.
Most features, such as lexicalsubcategorization features and semantic featuressuch as [Definite] are binary.
Other features, suchas syntactic label or semantic relation, have asmany as 25 values.
Training time on a standard500MHz PC ranges from one hour to six hours.4 Assignment of caseIn German sentence realization, properassignment of morphological case is essential forfluent and comprehensible output.
German is alanguage with fairly free constituent order, and theidentification of functional roles, such as subjectversus object, is not determined by position in thesentence, as in English, but by morphologicalmarking of one of the four cases: nominative,accusative, genitive or dative.
In Amalgam, caseassignment is one of the last steps in the Flesh-outstage (stage 2).
Morphological realization of casecan be ambiguous in German (for example, afeminine singular NP is ambiguous betweenaccusative and nominative case).
Since themorphological realization of case depends on thegender, number and morphological paradigm of agiven NP, we chose to only consider NP nodeswith unambiguous case as training data for themodel1.
As the target feature for this model is1Ideally, we should train the case assignment model ona corpus that is hand-disambiguated for case.
In theabsence of such a corpus, though, we believe that ourapproach is linguistically justified.
The case of an NPdepends solely on the syntactic context it appears in.morphological case, it has four possible values forthe four cases in German.4.1 Features in the case assignmentmodelFor each data point, a total of 712 features wasextracted.
Of the 712 features available to thedecision tree building tools, 72 were selected ashaving predictive value in the model.
The selectedfeatures fall into the following categories:?
syntactic label of the node, its parent andgrandparent?
lemma (i.e., citation form) of the parent,and lemma of the governing preposition?
subcategorization information, includingcase governing properties of governingpreposition and parent?
semantic relation of the node itself to itsparent, of the parent to its grandparent,and of the grandparent to its great-grandparent?
number information on the parent andgrandparent?
tense and mood on the parent andgrandparent?
definiteness on the node, its parent andgrandparent?
the presence of various semanticdependents such as subject, direct andindirect objects, operators, attributiveadjuncts and unspecified modifiers on thenode and its parent and grandparent?
quantification, negation, coordination onthe node, the parent and grandparent?
part of speech of the node, the parent andthe grandparent?
miscellaneous semantic features on thenode itself and the parent4.2 The case assignment modelThe decision tree model for case assignmenthas 226 branching nodes, making it one of themost complex models in Amalgam.
For eachnominal node in the 10,000 sentence test set, wecompared the prediction of the model to theSince we want to learn the syntactically determiningfactors for case, using unambiguously case marked NPsfor training seems justified.morphological case compatible with that node.The previously mentioned example of a singularfeminine NP, for example, would yield a ?correct?if the model had predicted nominative oraccusative case (because the NP ismorphologically ambiguous between accusativeand nominative), and it would yield an ?incorrect?if the model had predicted genitive or dative.
Thisparticular evaluation setup was a necessarycompromise because of the absence of a hand-annotated corpus with disambiguated case in ourdomain.
The caveat here is that downstreammodels in the Amalgam pipeline that pick up oncase as one of their features rely on the absoluteaccuracy of the assigned case, not the relativeaccuracy with respect to morphologicalambiguity.
Accuracy numbers for each of the fourcase assignments are given in Table 1.
Note that itis impossible to give precision/recall numbers,without a hand-disambiguated test set.
Thebaseline for this task is 0.7049 (accuracy if themost frequent case (nominative) had beenassigned to all NPs).Table 1.
Accuracy of the case assignment model.Value AccuracyDat 0.8705Acc 0.9707Gen 0.9457Nom 0.9654overall 0.93525 Assignment of verb positionfeaturesOne of the most striking properties of German isthe distributional pattern of verbs in main andsubordinate clauses.
Most descriptive accounts ofGerman syntax are based on a topology of theGerman sentence that treats the position of theverb as the fixed frame around which othersyntactic constituents are organized in relativelyfree order (cf.
Eisenberg, 1999; Engel, 1996).
Theposition of the verb in German is non-negotiable;errors in the positioning of the verb result ingibberish, whereas most permutations of otherconstituents only result in less fluent output.Depending on the position of the finite verb,German sentences and verb phrases are classifiedas being ?verb-initial?, ?verb-second?
or ?verb-final?.
In verb-initial clauses (e.g., in imperatives),the finite verb is in initial position.
Verb-secondsentences contain one constituent preceding thefinite verb, in the so-called ?pre-field?.
The finiteverb is followed by any number of constituents inthe ?middle-field?, and any non-finite verbs arepositioned at the right periphery of the clause,possibly followed by extraposed material orcomplement clauses (the ?post-field?).
Verb-finalclauses contain no verbal element in the verb-second position: all verbs are clustered at the rightperiphery, preceded by any number of constituentsand followed only by complement clauses andextraposed material.During the Flesh-out stage in Amalgam, adecision tree classifier is consulted to make aclassification decision among the four verbpositions: ?verb-initial?, ?verb-second?, ?verb-final?, and ?undefined?.
The value ?undefined?for the target feature of verb position is extractedfor those verbal constituents where the localsyntactic context is too limited to make a cleardistinction between initial, second, or finalposition of the verb.
The number of ?undefined?verb positions is small compared to the number ofclearly established verb positions: in the test set,there were only 690 observed cases of?undefined?
verb position out of a total of 15,492data points.
At runtime in Amalgam, verb positionfeatures are assigned based on the classificationprovided by the decision tree model.5.1 Features in the verb position modelFor each data point, 713 features were extracted.Of those features, 41 were selected by the decisiontree algorithm.
The selected features fall into thefollowing categories:?
syntactic label of the node and the parent?
subcategorization features?
semantic relations of the node to its parentand of the parent node to its parent?
tense and mood features?
presence of empty, uncontrolled subject?
semantic features on the node and theparent5.2 The verb position modelThe decision tree model for verb position has115 branching nodes.
Precision, recall and F-measure for the model are given in Table 2.
As apoint of reference for the verb position classifier,assigning the most frequent value (second) of thetarget feature yields a baseline score of 0.4240.Table 2.
Precision, recall, and F-measure for the verbposition model.Value Precision Recall F-measureInitial 0.9650 0.9809 0.9729Second 0.9754 0.9740 0.9743Final 0.9420 0.9749 0.9581Undefined 0.5868 0.3869 0.4663Overallaccuracy0.94916 ExtrapositionIn both German and English it is possible toextrapose clausal material to the right periphery ofthe sentence (extraposed clauses underlined in theexamples below):Relative clause extraposition:English: A man just left who had come toask a question.German: Der Mann ist geradeweggegangen, der gekommen war, umeine Frage zu stellen.Infinitival clause extraposition:English: A decision was made to leave thecountry.German: Eine Entscheidung wurdegetroffen, das Land zu verlassen.Complement clause extraposition:English: A rumour has been circulatingthat he is ill.German: Ein Ger?cht ging um, dass erkrank ist.Extraposition is not obligatory like other typesof movement (such as Wh-movement).
Bothextraposed and non-extraposed versions of asentence are acceptable, with varying degrees offluency.The interesting difference between English andGerman is the frequency of this phenomenon.While it can easily be argued that Englishsentence realization may ignore extraposition andstill result in very fluent output, the fluency ofsentence realization for German will suffer muchmore from the lack of a good extrapositionmechanism.
We profiled data from variousdomains (Gamon et al 2002a) to substantiate thislinguistic claim (see Uszkoreit et al 1998 forsimilar results).
In the technical domain, morethan one third of German relative clauses areextraposed, as compared to a meagre 0.22% ofEnglish relative clauses.
In encyclopaedia text(Microsoft Encarta), approximately every fifthGerman relative clause is extraposed, compared toonly 0.3% of English relative clauses.
Forcomplement clauses and infinitival clauses, thedifferences are not as striking, but still significant:in the technical and encyclopaedia domains,extraposition of infinitival and complementclauses in German ranges from 1.5% to 3.2%,whereas English only shows a range from 0% to0.53%.We chose to model extraposition as an iterativemovement process from the original attachmentsite to the next higher node in the tree (for analternative one-step solution and a comparison ofthe two approaches see (Gamon et al, 2002a)).The target feature of the model is the answer tothe yes/no question ?Should the clause move fromnode X to the parent of node X?
?.6.1 Features in the extraposition modelThe tendency of a clause to be extraposed dependson properties of both the clause itself (e.g., somenotion of ?heaviness?)
and the current attachmentsite.
Very coarse linguistic generalizations are thata relative clause tends to be extraposed if it issufficiently ?heavy?
and if it is followed by verbalmaterial in the same clause.
Feature extraction forthis model reflects that fact by taking intoconsideration features on the extrapositioncandidate, the current attachment site, andpotential next higher landing site.
This results in atotal of 1168 features.
Each node in the parentchain of an extraposable clause, up to the actualattachment node, constitutes a single data pointDuring the decision tree building process, 60features were selected as predictive.
They can beclassified as follows:General feature:?
overall sentence lengthFeatures on the extraposable clause:?
presence of verb-final and verb-secondancestor nodes?
?heaviness?
both in number of charactersand number of tokens?
various linguistic features in the localcontext (parent node and grandparentnode): number and person, definiteness,voice, mood, transitivity, presence oflogical subject and object, presence ofcertain semantic attributes, coordination,prepositional relations?
syntactic label?
presence of modal verbs?
prepositional relations?
transitivityFeatures on the attachment site?
presence of logical subject?
status of the parent and grandparent as aseparable prefix verb?
voice and presence of modal verbs on theparent and grandparent?
presence of arguments and transitivityfeatures on the parent and grandparent?
number, person and definiteness; the sameon parent and grandparent?
syntactic label; the same on the parent andgrandparent?
verb position; the same on the parent?
prepositional relation on parent andgrandparent?
semantic relation that parent andgrandparent have to their respectiveparent node6.2 The extraposition modelDuring testing of the extraposition model, themodel was consulted for each extraposable clauseto find the highest node to which that clause couldbe extraposed.
In other words, the target node forextraposition is the highest node in the parentchain for which the answer to the classificationtask ?Should the clause move from node X to theparent of node X??
is ?yes?
with no interceding?no?
answer.
The prediction of the model wascompared with the actual observed attachment siteof the extraposable clause to yield the accuracyfigures shown in Table 3.
The model has 116branching nodes.
The baseline for this task iscalculated by applying the most frequent value forthe target feature (?don't move?)
to all nodes.
Thebaseline for extraposition of infinitival andcomplement clauses is very high.
The number ofextraposed clauses of both types in the test set(fifteen extraposed infinitival clauses and twelveextraposed complement clauses) is very small, soit comes as no surprise that the model accuracyranges around the baseline for these two types ofextraposed clauses.Table 3.
Accuracy of the extraposition model.Extraposable clause Accuracy BaselineRELCL 0.8387 0.6093INFCL 0.9202 0.9370COMPCL 0.9857 0.9429Overall 0.8612 0.67587 Syntactic aggregationAny sentence realization component thatgenerates from an abstract semantic representationand strives to produce fluent output beyond simpletemplates will have to deal with coordination andthe problem of duplicated material incoordination.
This is generally viewed as a sub-area of aggregation in the generation literature(Wilkinson, 1995; Shaw, 1998; Reape andMellish, 1999; Dalianis and Hovy, 1993).
InAmalgam, the approach we take is strictly intra-sentential, along the lines of what has been calledconjunction reduction in the linguistic literature(McCawley, 1988).
While this may seem a fairlystraightforward task compared to inter-sentential,semantic and lexical aggregation, it should benoted that the cross-linguistic complexity of thephenomenon makes it much less trivial than a firstglance at English would suggest.
In German, forexample, position of the verb in the coordinatedVPs plays an important role in determining whichduplicated constituent can be omitted.The target feature for the classification task isformulated as follows: ?In which coordinatedconstituent is the duplicated constituent to berealized??.
There are three values for the targetfeature: ?first?, ?last?, and ?middle?.
The thirdvalue (?middle?)
is a default value for cases whereneither the first, nor the last coordinatedconstituent can be identified as the location for therealization of duplicated constituents.
Atgeneration runtime, multiple realizations of aconstituent in coordination are collected and theaggregation model is consulted to decide on theoptimal position in which to realize thatconstituent.
The constituent in that position isretained, while all other duplicates are removedfrom the tree.7.1 Features in the syntactic aggregationmodelA total of 714 features were extracted for thesyntactic aggregation model.
Each instance ofcoordination which exhibits duplicated material atthe semantic level without correspondingduplication at the syntactic level constitutes a datapoint.Of these features, 15 were selected aspredictive in the process of building the decisiontree model:?
syntactic label and syntactic label of theparent node?
semantic relation to the parent of theduplicated node, its parent and grandparent?
part of speech of the duplicated node?
verb position across the coordinated node?
position of the duplicated node inpremodifiers or postmodifiers of the parent?
coordination of the duplicated node andthe grandparent of the duplicated node?
status of parent and grandparent as aproposition?
number feature on the parent?
transitivity and presence of a direct objecton the parent7.2 The syntactic aggregation modelThe syntactic aggregation model has 21 branchingnodes.
Precision, recall and F-measure for themodel are given in Table 4.
As was to be expectedon the basis of linguistic intuition, the value?middle?
for the target feature did not play anyrole.
In the test set there were only 2 observedinstances of that value.
The baseline for this taskis 0.8566 (assuming ?first?
as the default value).Table 4.
Precision, recall, and F-measure for thesyntactic aggregation model.Value Precision Recall F-measurelast 0.9191 0.9082 0.9136first 0.9837 0.9867 0.9851middle 0.0000 0.0000 0.0000overallaccuracy0.97468 Conclusion and future researchWe have demonstrated on the basis of fourexamples that it is possible to learn the contextsfor complex linguistic operations in sentencerealization with high accuracy.
We proposed tostandardize most of the feature extraction for themachine learning tasks to all available linguisticfeatures on the node, and its parent andgrandparent node.
This generalized set of featuresallows us to rapidly train on new sets of data andto experiment with new machine learning tasks.Furthermore, it prevents us from focusing on asmall set of hand-selected features for a givenphenomenon; hence, it allows us to learn new (andunexpected) generalizations from new data.We have found decision trees to be useful forour classification problems, but other classifiersare certainly applicable.
Decision trees providedan easily accessible inventory of the selectedfeatures and some indication of their relativeimportance in predicting the target features inquestion.
Although our exposition has focused onthe preferred value (the mode) predicted by themodels, decision trees built by WinMine predict aprobability distribution over all possible targetvalues.
For a system such as Amalgam, built as apipeline of stages, this point is critical, sincefinding the best final hypothesis requires theconsideration of multiple hypotheses and theconcomitant combination of probabilities assignedby the various models in the pipeline to allpossible target values.
For example, ourextraposition model presented above dependsupon the value of the verb-position feature, whichis predicted upstream in the pipeline.
Currently,we greedily pursue the best hypothesis, whichincludes only the mode of the verb-positionmodel?s prediction.
However, work in progressinvolves a search that constructs multiplehypotheses incorporating each of the predictionsof the verb-position model and their scores, andlikewise for all other models.We have found the combination of knowledge-engineered linguistic operations with machine-learned contexts to be advantageous.
Theknowledge-engineered choice of linguisticoperations, allows us to deal with complexlinguistic phenomena.
Machine learning, on theother hand, automates the discovery of generaland domain-specific contexts.
This facilitatesadaptation of the system to a new domain or evento a new language.It should also be noted that none of the learnedmodels can be easily replaced by a rule.
Whilecase assignment, for example, depends to a highdegree on the lexical properties of the governingpreposition or governing verb, other factors suchas semantic relations, etc., play a significant role,so that any rule approaching the accuracy of themodel would have to be quite complex.We are currently adapting Amalgam to the taskof French sentence realization, as a test of thelinguistic generality of the system.
Initial resultsare encouraging.
It appears that much of thefeature extraction and many of the linguisticoperations are reusable.AcknowledgementsOur thanks go to Max Chickering for assistancewith the WinMine decision tree tools and to ZhuZhang who made significant contributions to thedevelopment of the extraposition models.ReferencesS.
Bangalore and O. Rambow 2000.
Exploiting aprobabilistic hierarchical model for generation.Proceedings of the 18th International Conference onComputational Linguistics (COLING 2000).Saarbr?cken, Germany.
42-48.D.
M. Chickering.
nd.
WinMine Toolkit Home Page.http://research.microsoft.com/~dmax/WinMine/Tooldoc.htmD.
M. Chickering, D. Heckerman and C. Meek.
1997.A Bayesian approach to learning Bayesian networkswith local structure.
In ?Uncertainty in ArtificialIntelligence: Proceedings of the ThirteenthConference?, D. Geiger and P. Punadlik Shenoy,ed., Morgan Kaufman, San Francisco, California,pp.
80-89.S.
Corston-Oliver, M. Gamon, E. Ringger, and R.Moore.
2002.
An overview of Amalgam: A machine-learned generation module.
To be presented atINLG 2002.H.
Dalianis and E. Hovy 1993 Aggregation in naturallanguage generation.
Proceedings of the 4thEuropean Workshop on Natural LanguageGeneration, Pisa, Italy.P.
Eisenberg 1999.
Grundriss der deutschenGrammatik.
Band2: Der Satz.
Metzler,Stuttgart/Weimar.U.
Engel.
1996.
Deutsche Grammatik.
Groos,Heidelberg.M.
Gamon, E. Ringger, Z. Zhang, R. Moore and S.Corston-Oliver.
2002a.
Extraposition: A case studyin German sentence realization.
To be presented atthe 19th International Conference on ComputationalLinguistics (COLING) 2002.M.
Gamon, E. Ringger, S. Corston-Oliver.
2002b.Amalgam: A machine-learned generation module.Microsoft Research Technical Report, to appear.G.
E. Heidorn.
2002.
Intelligent Writing Assistance.
In?A Handbook of Natural Language Processing:Techniques and Applications for the Processing ofLanguage as Text?, R. Dale, H. Moisl, and H.Somers (ed.
), Marce Dekker, New York.I.
Langkilde.
and K. Knight.
1998a.
The practical valueof n-grams in generation.
Proceedings of the 9thInternational Workshop on Natural LanguageGeneration, Niagara-on-the-Lake, Canada.
pp.
248-255.I.
Langkilde and K. Knight.
1998b.
Generation thatexploits corpus-based statistical knowledge.Proceedings of the 36th ACL and 17th COLING(COLING-ACL 1998).
Montr?al, Qu?bec, Canada.704-710.J.
D. McCawley.
1988 The Syntactic Phenomena ofEnglish.
The University of Chicago Press, Chicagoand London.M.
Reape.
and C. Mellish.
1999.
Just what isaggregation anyway?
Proceedings of the 7thEuropean Workshop on Natural LanguageGeneration, Toulouse, France.E.
Ringger, R. Moore, M. Gamon, and S. Corston-Oliver.
In preparation.
A Linguistically InformedGenerative Language Model for Intra-ConstituentOrdering during Sentence Realization.J.
Shaw.
1998 Segregatory Coordination and Ellipsis inText Generation.
Proceedings of COLING-ACL,1998, pp 1220-1226.H.
Uszkoreit, T. Brants, D. Duchier, B. Krenn, L.Konieczny, S. Oepen and W. Skut.
1998.
Aspekteder Relativsatzextraposition im Deutschen.
Claus-Report Nr.99, Sonderforschungsbereich 378,Universit?t des Saarlandes, Saarbr?cken, Germany.J.
Wilkinson 1995 Aggregation in Natural LanguageGeneration: Another Look.
Co-op work term report,Department of Computer Science, University ofWaterloo.
