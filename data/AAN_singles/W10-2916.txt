Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 126?134,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA Semi-Supervised Batch-Mode Active Learning Strategy for ImprovedStatistical Machine TranslationSankaranarayanan Ananthakrishnan, Rohit Prasad, David Stallard and Prem NatarajanBBN Technologies10 Moulton StreetCambridge, MA, U.S.A.{sanantha,rprasad,stallard,prem}@bbn.comAbstractThe availability of substantial, in-domainparallel corpora is critical for the develop-ment of high-performance statistical ma-chine translation (SMT) systems.
Suchcorpora, however, are expensive to pro-duce due to the labor intensive nature ofmanual translation.
We propose to al-leviate this problem with a novel, semi-supervised, batch-mode active learningstrategy that attempts to maximize in-domain coverage by selecting sentences,which represent a balance between domainmatch, translation difficulty, and batch di-versity.
Simulation experiments on anEnglish-to-Pashto translation task showthat the proposed strategy not only outper-forms the random selection baseline, butalso traditional active learning techniquesbased on dissimilarity to existing trainingdata.
Our approach achieves a relative im-provement of 45.9% in BLEU over theseed baseline, while the closest competitorgained only 24.8% with the same numberof selected sentences.1 IntroductionRapid development of statistical machine transla-tion (SMT) systems for resource-poor languagepairs is a problem of significant interest to theresearch community in academia, industry, andgovernment.
Tight turn-around schedules, bud-get restrictions, and scarcity of human translatorspreclude the production of large parallel corpora,which form the backbone of SMT systems.Given these constraints, the focus is on makingthe best possible use of available resources.
Thisusually involves some form of prioritized data col-lection.
In other words, one would like to con-struct the smallest possible parallel training corpusthat achieves a desired level of performance on un-seen test data.Within an active learning framework, this canbe cast as a data selection problem.
The goal isto choose, for manual translation, the most infor-mative instances from a large pool of source lan-guage sentences.
The resulting sentence pairs, incombination with any existing in-domain seed par-allel corpus, are expected to provide a significantlyhigher performance gain than a na?
?ve random se-lection strategy.
This process is repeated until acertain level of performance is attained.Previous work on active learning for SMT hasfocused on unsupervised dissimilarity measuresfor sentence selection.
Eck et al (2005) describe aselection strategy that attempts to maximize cov-erage by choosing sentences with the highest pro-portion of previously unseen n-grams.
However,if the pool is not completely in-domain, this strat-egy may select irrelevant sentences, whose trans-lations are unlikely to improve performance on anin-domain test set.
They also propose a technique,based on TF-IDF, to de-emphasize sentences sim-ilar to those that have already been selected.
How-ever, this strategy is bootstrapped by random ini-tial choices that do not necessarily favor sentencesthat are difficult to translate.
Finally, they workexclusively with the source language and do notuse any SMT-derived features to guide selection.Haffari et al (2009) propose a number of fea-tures, such as similarity to the seed corpus, trans-lation probability, relative frequencies of n-gramsand ?phrases?
in the seed vs. pool data, etc., foractive learning.
While many of their experimentsuse the above features independently to comparetheir relative efficacy, one of their experimentsattempts to predict a rank, as a linear combina-tion of these features, for each candidate sentence.The top-ranked sentences are chosen for manualtranslation.
The latter strategy is particularly rel-evant to this paper, because the goal of our active126learning strategy is not to compare features, but tolearn the trade-off between various characteristicsof the candidate sentences that potentially maxi-mizes translation improvement.The parameters of the linear ranking model pro-posed by Haffari et al (2009) are trained usingtwo held-out development sets D1 and D2 - themodel attempts to learn the ordering of D1 thatincrementally maximizes translation performanceon D2.
Besides the need for multiple parallelcorpora and the computationally intensive natureof incrementally retraining an SMT system, theirapproach suffers from another major deficiency.It requires that the pool have the same distribu-tional characteristics as the development sets usedto train the ranking model.
Additionally, they se-lect all sentences that constitute a batch in a singleoperation following the ranking procedure.
Sincesimilar or identical sentences in the pool will typ-ically meet the selection criteria simultaneously,this can have the undesired effect of choosing re-dundant batches with low diversity.
This results inunder-utilization of human translation resources.In this paper, we propose a novel batch-modeactive learning strategy that ameliorates the aboveissues.
Our semi-supervised learning approachcombines a parallel ranking strategy with sev-eral features, including domain representativeness,translation confidence, and batch diversity.
Theproposed approach includes a greedy, incrementalbatch selection strategy, which encourages diver-sity and reduces redundancy.
The following sec-tions detail our active learning approach, includ-ing the experimental setup and simulation resultsthat clearly demonstrate its effectiveness.2 Active Learning ParadigmActive learning has been studied extensively in thecontext of multi-class labeling problems, and the-oretically optimal selection strategies have beenidentified for simple classification tasks with met-ric features (Freund et al, 1997).
However, nat-ural language applications such as SMT present asignificantly higher level of complexity.
For in-stance, SMT model parameters (translation rules,language model n-grams, etc.)
are not fixed innumber or type, and vary depending on the train-ing instances.
This gives rise to the concept ofdomain.
Even large quantities of out-of-domaintraining data usually do not improve translationperformance.
As we will see, this causes simpleactive selection techniques based on dissimilarityor translation difficulty to be ineffective, becausethey tend to favor out-of-domain sentences.Our proposed active learning strategy is moti-vated by the idea that the chosen sentences shouldmaximize coverage, and by extension, translationperformance on an unseen test set.
It shouldpick sentences that represent the target domain,while simultaneously enriching the training datawith hitherto unseen, difficult-to-translate con-structs that are likely to improve performance on atest set.
We refer to the former as representative-ness and to the latter as difficulty.Since it is computationally prohibitive to re-train an SMT system for individual translationpairs, a batch of sentences is usually selected ateach iteration.
We desire that each batch be suffi-ciently diverse; this increases the number of con-cepts (phrase pairs, translation rules, etc.)
that canbe learned from manual translations of a selectedbatch.
Thus, our active learning strategy attempts,at each iteration, to select a batch of mutually di-verse source sentences, which, while introducingnew concepts, shares at least some commonalitywith the target domain.
This is done in a com-pletely statistical, data-driven fashion.In designing this active learning paradigm, wemake the following assumptions.?
A small seed parallel corpus S is availablefor training an initial SMT system.
This mayrange from a few hundred to a few thousandsentence pairs.?
Sentences must be selected from a large poolP.
This may be an arbitrary collection of in-and out-of-domain source language sentences.Some measure of redundancy is permitted andexpected, i.e.
some sentences may be identicalor very similar to others.?
A development set D is available to tune theSMT system and train the selection algorithm.An unseen test set T is used to evaluate it.?
The seed, development, and test sets are de-rived from the target domain distribution.To re-iterate, we do not assume or require thepool to have the same domain distribution as theseed, development, and test sets.
This reflects areal-world scenario, where the pool may be drawnfrom multiple sources (e.g.
targeted collections,newswire text, web, etc.).
This is a key departurefrom existing work on active learning for SMT.127S e e d  c o r p u sS M T  s y s t e mM o n o l i n g u a l  p o o lP o o l  t r a i n i n g D e v e l .
s e tD o m a i n  m a t c hT r a n s .
d i f f i c u l t yD i v e r s i t yP r e f e r r e d  o r d e rC 1C 2Input features MLP Classifiers Classifier targetsFigure 1: Flow-diagram of the active learner.3 Active Learning ArchitectureFigure 1 illustrates the proposed active learningarchitecture in the form of a high-level flow-diagram.
We begin by randomly sampling a smallfraction of the large monolingual pool P to cre-ate a pool training set PT, which is used to trainthe learner.
The remainder, which we call the poolevaluation set PE, is set aside for active selection.We also train an initial phrase-based SMT system(Koehn et al, 2003) with the available seed cor-pus.
The pool training set PT, in conjunction withthe seed corpus S, initial SMT system, and held-out development set D, is used to derive a numberof input features as well as target labels for train-ing two parallel classifiers.3.1 Preferred OrderingThe learner must be able to map input featuresto an ordering of the pool sentences that attemptsto maximize coverage on an unseen test set.
Weteach it to do this by providing it with an orderingof PT that incrementally maximizes source cov-erage on D. This preferred ordering algorithm in-crementally maps sentences in PT to a ordered setOT by picking, at each iteration, the sentence withthe highest coverage criterion with respect to D,and inserting it at the current position within OT.The coverage criterion is based on content-wordn-gram overlap with D, discounted by constructsalready observed in S and higher-ranked sentencesin OT, as illustrated in Algorithm 1.
Our hypoth-esis is that sentences, which maximally improvecoverage, likely lead to bigger gains in translationperformance as well.The O(|PT|2) complexity of this algorithm isone reason we restrict PT to a few thousand sen-tences.
Another reason not to order the entire pooland simply select the top-ranked sentences, is thatbatches thus constructed would overfit the devel-opment set on which the ordering is based, andnot generalize well to an unseen test set.3.2 Ranker FeaturesEach candidate sentence in the pool is representedby a vector of features, which fall under one ofthe three categories, viz.
representativeness, dif-ficulty, and diversity.
We refer to the first twoas context-independent, because they can be com-puted independently for each sentence.
Diversityis a context-dependent feature and must be evalu-ated in the context of an ordering of sentences.128Algorithm 1 Preferred orderingOT ?
()Sg ?
count(g) ?g ?
ngr(S)Dg ?
count(g) ?g ?
ngr(D)for k = 1 to |PT| doPU ?
PT ?OTy?
?
argmaxy?PU?g?ngr(y)yg ?Dg ?
nSg + 1OT (k)?
y?Sg ?
Sg + y?g ?g ?
ngr(y?
)end forreturn OT3.2.1 Domain RepresentativenessDomain representativeness features gauge the de-gree of similarity between a candidate pool sen-tence and the seed training data.
We quantify thisusing an n-gram overlap measure between candi-date sentence x and the seed corpus S defined byEquation 1.sim(x,S) =?g?ngr(x)xg ?min(Sng , Cn)Cn?g?ngr(x)xg(1)xg is the number of times n-gram g occurs in x,Sg the number of times it occurs in the seed cor-pus, n its length in words, and Cn the count ofn-grams of length n in S. Longer n-grams thatoccur frequently in the seed receive high similar-ity scores, and vice-versa.
In evaluating this fea-ture, we only consider n-grams up to length fivethat contain least one content word.Another simple domain similarity feature weuse is sentence length.
Sentences in conversationaldomains are typically short, while those in weband newswire domains run longer.3.2.2 Translation DifficultyAll else being equal, the selection strategy shouldfavor sentences that the existing SMT system findsdifficult to translate.
To this end, we estimate aconfidence score for each SMT hypothesis, usinga discriminative classification framework reminis-cent of Blatz et al (2004).
Confidence estima-tion is treated as a binary classification problem,where each hypothesized word is labeled ?cor-rect?
or ?incorrect?.
Word-level reference labelsfor training the classifier are obtained from Trans-lation Edit Rate (TER) analysis, which producesthe lowest-cost alignment between the hypothe-ses and the gold-standard references (Snover etal., 2006).
A hypothesized word is ?correct?
ifit aligns to itself in this alignment, and ?incorrect?otherwise.We derive features for confidence estimationfrom the phrase derivations used by the decoder ingenerating the hypotheses.
For each target word,we look up the corresponding source phrase thatproduced it, and use this information to computea number of features from the translation phrasetable and target language model (LM).
These in-clude the in-context LM probability of the targetword, the forward and reverse phrase translationprobabilities, the maximum forward and reverseword-level lexical translation probabilities, num-ber of competing target phrases in which the tar-get word occurs, etc.
In all, we use 11 word-levelfeatures (independent of the active learning fea-tures) to train the classifier in conjunction with theabovementioned binary reference labels.A logistic regression model is used to directlyestimate the posterior probability of the binaryword label.
Thus, our confidence score is es-sentially the probability of the word being ?in-correct?.
Sentence-level confidence is computedas the geometric average of word-level posteriors.Confidence estimation models are trained on theheld-out development set.We employ two additional measures of transla-tion difficulty for active learning: (a) the numberof ?unknown?
words in target hypotheses causedby untranslatable source words, and (b) the aver-age length of source phrases in the 1-best SMTdecoder derivations.3.2.3 Batch DiversityBatch diversity is evaluated in the context of anexplicit ordering of the candidate sentences.
Ingeneral, sentences that are substantially similar tothose above them in a ranked list have low diver-sity, and vice-versa.
We use content-word n-gramoverlap to measure similarity with previous sen-tences, per Equation 2.d(b | B) = 1.0?
?g?ngr(b)n?Bg?g?ngr(b)n?max(Bg, 1.0)(2)B represents the set of sentences ranked higherthan the candidate b, for which we wish to evalu-ate diversity.
Bg is the number of times n-gram g129occurs in B.
Longer, previously unseen n-gramsserve to boost diversity.
The first sentence in agiven ordering is always assigned unit diversity.The coverage criterion used by the preferred or-dering algorithm in Section 3.1 ensures good cor-respondence between the rank of a sentence and itsdiversity, i.e.
higher-ranked in-domain sentenceshave higher diversity, and vice-versa.3.3 Training the LearnerThe active learner is trained on the pool trainingset PT.
The seed training corpus S serves as thebasis for extracting domain similarity features foreach sentence in this set.
Translation difficulty fea-tures are evaluated by decoding sentences in PTwith the seed SMT system.
Finally, we computediversity for each sentence in PT based on its pre-ferred order OT according to Equation 2.
Learn-ing is semi-supervised as it does not require trans-lation references for either PT or D.Traditional ranking algorithms such as PRank(Crammer and Singer, 2001) work best when thenumber of ranks is much smaller than the samplesize; more than one sample can be assigned thesame rank.
In the active learning problem, how-ever, each sample is associated with a unique rank.Moreover, the dynamic range of ranks in OT issignificantly smaller than that in PE, to which theranking model is applied, resulting in a mismatchbetween training and evaluation conditions.We overcome these issues by re-casting theranking problem as a binary classification task.The top 10% sentences in OT are assigned a ?se-lect?
label, while the remaining are assigned acontrary ?do-not-select?
label.
The input featuresare mapped to class posterior probabilities usingmulti-layer perceptron (MLP) classifiers.
The useof posteriors allows us to assign a unique rank toeach candidate sentence.
The best candidate sen-tence is the one to which the classifier assigns thehighest posterior probability for the ?select?
la-bel.
We use one hidden layer with eight sigmoid-activated nodes in this implementation.Note that we actually train two MLP classi-fiers with different sets of input features as shownin Figure 1.
Classifier C1 is trained using onlythe context-independent features, whereas C2 istrained with the full set of features including batchdiversity.
These classifiers are used to implementan incremental, greedy selection algorithm withparallel ranking, as explained below.Algorithm 2 Incremental greedy selectionB?
()for k = 1 to N doPci ?
{x ?
PE | d(x | B) = 1.0}Pcd ?
{x ?
PE | d(x | B) < 1.0}C?
C1(fci(Pci)) ?
C2(fcd(Pcd,B))bk ?
argmaxx?PEC(x)PE ?
PE ?
{bk}end forreturn B4 Incremental Greedy SelectionTraditional rank-and-select batch construction ap-proaches choose constituent sentences indepen-dently, and therefore cannot ensure that the cho-sen sentences are sufficiently diverse.
Our strat-egy implements a greedy selection algorithm thatconstructs each batch iteratively; the decision bk(the sentence to fill the kth position in a batch)depends on all previous decisions b1, ?
?
?
, bk?1.This allows de-emphasizing sentences similar tothose that have already been placed in the batch,while favoring samples containing previously un-seen constructs.4.1 Parallel RankingWe begin with an empty batch B, to which sen-tences from the pool evaluation set PE must beadded.
We then partition the sentences in PE intwo mutually-exclusive groups Pcd and Pci.
Theformer contains candidates that share at least onecontent-word n-gram with any existing sentencesin B, while the latter consists of sentences thatdo not share any overlap with them.
Note thatB is empty to start with; thus, Pcd is empty andPci = PE at the beginning of the first iterationof selection.
The diversity feature is computed foreach sentence in Pcd based on existing selectionsin B, while the context-independent features areevaluated for sentences in both partitions.Next, we apply C1 to Pci and C2 to Pcd and in-dependently obtain posterior probabilities for the?select?
label for both partitions.
We take theunion of class posteriors from both partitions andselect the sentence with the highest probability ofthe ?select?
label to fill the next slot bk, corre-sponding to iteration k, in the batch.
The selectedsentence is subsequently removed from PE.The above parallel ranking technique (Algo-rithm 2) is applied iteratively until the batch130reaches a pre-determined size N .
At itera-tion k, the remaining sentences in PE are par-titioned based on overlap with previous selec-tions b1, ?
?
?
, bk?1 and ranked based on the unionof posterior probabilities generated by the corre-sponding classifiers.
This ensures that sentencessubstantially similar to those that have alreadybeen selected receive a low diversity score, and aresuitably de-emphasized.
Depending on the char-acteristics of the pool, batches constructed by thisalgorithm are likely more diverse than a simplerank-and-select approach.5 Experimental Setup and ResultsWe demonstrate the effectiveness of the proposedsentence selection algorithm by performing a setof simulation experiments in the context of anEnglish-to-Pashto (E2P) translation task.
We sim-ulate a low-resource condition by using a verysmall number of training sentence pairs, sampledfrom the collection, to bootstrap a phrase-basedSMT system.
The remainder of this parallel cor-pus is set aside as the pool.At each iteration, the selection algorithm picks afixed-size batch of source sentences from the pool.The seed training data are augmented with thechosen source sentences and their translations.
Anew set of translation models is then estimated andused to decode the test set.
We track SMT perfor-mance across several iterations and compare theproposed algorithm to a random selection baselineas well as other common selection strategies.5.1 Data ConfigurationOur English-Pashto data originates from a two-way collection of spoken dialogues, and thus con-sists of two parallel sub-corpora: a directional E2Pcorpus and a directional Pashto-to-English (P2E)corpus.
Each sub-corpus has its own independenttraining, development, and test partitions.
The di-rectional E2P training, development, and test setsconsist of 33.9k, 2.4k, and 1.1k sentence pairs, re-spectively.
The directional P2E training set con-sists of 76.5k sentence pairs.We obtain a seed training corpus for the simula-tion experiments by randomly sampling 1,000 sen-tence pairs from the directional E2P training par-tition.
The remainder of this set, and the entire re-versed directional P2E training partition are com-bined to create the pool (109.4k sentence pairs).
Inthe past, we have observed that the reversed direc-tional P2E data gives very little performance gainin the E2P direction even though its vocabulary issimilar, and can be considered ?out-of-domain?
asfar as the E2P translation task is concerned.
Thus,our pool consists of 30% in-domain and 70% out-of-domain sentence pairs, making for a challeng-ing active learning problem.
A pool training set of10k source sentences is sampled from this collec-tion, leaving us with 99.4k candidate sentences.5.2 Selection StrategiesWe implement the following strategies for sen-tence selection.
In all cases, we use a fixed-sizebatch of 200 sentences per iteration.?
Random selection, in which source sentencesare uniformly sampled from PE.?
Similarity selection, where we choose sen-tences that exhibit the highest content-word n-gram overlap with S.?
Dissimilarity selection, which selects sen-tences having the lowest degree of content-word n-gram overlap with S.?
Active learning with greedy incremental selec-tion, using a learner to maximize coverage bycombining various input features.We simulate a total of 30 iterations, with theoriginal 1,000 sample seed corpus growing to7,000 sentence pairs.5.3 Simulation ResultsWe track SMT performance at each iteration intwo ways.
The first and most effective method isto simply use an objective measure of translationquality, such as BLEU (Papineni et al, 2001).
Fig-ure 2(a) illustrates the variation in BLEU scoresacross iterations for each selection strategy.
Wenote that the proposed active learning strategy per-forms significantly better at every iteration thanrandom, similarity, and dissimilarity-based selec-tion.
At the end of 30 iterations, the BLEUscore gained 2.46 points, a relative improvementof 45.9%.
By contrast, the nearest competitor wasthe random selection baseline, whose performancegained only 1.33 points in BLEU, a 24.8% im-provement.
Note that we tune the phrase-basedSMT feature weights using MERT (Och, 2003)once in the beginning, and use the same weightsacross all iterations.
This allowed us to compareselection methods without variations introducedby fluctuation of the weights.131(a) Trajectory of BLEU(b) Trajectory of untranslated word ratio(c) Directionality match (d) Diversity/UniquenessFigure 2: Simulation results for data selection.
Batch size at each iteration is 200 sentences.132The second method measures test set coveragein terms of the proportion of untranslated wordsin the SMT hypotheses, which arise due to theabsence of appropriate in-context phrase pairs inthe training data.
Figure 2(b) shows the varia-tion in this measure for the four selection tech-niques.
Again, the proposed active learning algo-rithm outperforms its competitors across nearly alliterations, with very large improvements in the ini-tial stages.
Overall, the proportion of untranslatedwords dropped from 8.74% to 2.28% after 30 iter-ations, while the closest competitor (dissimilarityselection) dropped to 2.59%.It is also instructive to compare the distribu-tion of the 6,000 sentences selected by each strat-egy at the end of the simulation to determinewhether they came from the ?in-domain?
E2Pset or the ?out-of-domain?
P2E collection.
Fig-ure 2(c) demonstrates that only 1.3% of sentenceswere selected from the reversed P2E set by theproposed active learning strategy.
On the otherhand, 70.9% of the sentences selected by thedissimilarity-based technique came from the P2Ecollection, explaining its low BLEU scores on theE2P test set.
Surprisingly, similarity selection alsochose a large fraction of sentences from the P2Ecollection; this was traced to a uniform distribu-tion of very common sentences (e.g.
?thank you?,?okay?, etc.)
across the E2P and P2E sets.Figure 2(d) compares the uniqueness and over-all n-gram diversity of the 6,000 sentences chosenby each strategy.
The similarity selector receivedthe lowest score on this scale, explaining the lackof improvement in coverage as measured by theproportion of untranslated words in the SMT hy-potheses.
Again, the proposed approach exhibitsthe highest degree of uniqueness, underscoring itsvalue in lowering batch redundancy.It is interesting to note that dissimilarity selec-tion is closest to the proposed active learning strat-egy in terms of coverage, and yet exhibits theworst BLEU scores.
This confirms that, whilethere is overlap in their vocabularies, the E2P andP2E sets differ significantly in terms of longer-span constructs that influence SMT performance.These results clearly demonstrate the powerof the proposed strategy in choosing diverse, in-domain sentences that not only provide superiorperformance in terms of BLEU, but also improvecoverage, leading to fewer untranslated conceptsin the SMT hypotheses.6 Conclusion and Future DirectionsRapid development of SMT systems for resource-poor language pairs requires judicious use of hu-man translation capital.
We described a novel ac-tive learning strategy that automatically learns topick, from a large monolingual pool, sentencesthat maximize in-domain coverage.
In conjunc-tion with their translations, they are expected toimprove SMT performance at a significantly fasterrate than existing selection techniques.We introduced two key ideas that distinguishour approach from previous work.
First, we uti-lize a sample of the candidate pool, rather than anadditional in-domain development set, to learn themapping between the features and the sentencesthat maximize coverage.
This removes the restric-tion that the pool be derived from the target do-main distribution; it can be an arbitrary collectionof in- and out-of-domain sentences.Second, we construct batches using an incre-mental, greedy selection strategy with parallelranking, instead of a traditional batch rank-and-select approach.
This reduces redundancy, allow-ing more concepts to be covered in a given batch,and making better use of available resources.We showed through simulation experiments thatthe proposed strategy selects diverse batches ofhigh-impact, in-domain sentences that result in amuch more rapid improvement in translation per-formance than random and dissimilarity-based se-lection.
This is reflected in objective indicators oftranslation quality (BLEU), and in terms of cover-age as measured by the proportion of untranslatedwords in SMT hypotheses.
We plan to evaluatethe scalability of our approach by running simu-lations on a number of additional language pairs,domains, and corpus sizes.An issue with iterative active learning in gen-eral is the cost of re-training the SMT system foreach batch.
Small batches provide for smooth per-formance trajectories and better error recovery atan increased computational cost.
We are currentlyinvestigating incremental approaches that allowSMT models to be updated online with minimalperformance loss compared to full re-training.Finally, there is no inherent limitation in theproposed framework that ties it to a phrase-basedSMT system.
With suitable modifications to theinput feature set, it can be adapted to work withvarious SMT architectures, including hierarchicaland syntax-based systems.133ReferencesJohn Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2004.
Confidence es-timation for machine translation.
In COLING ?04:Proceedings of the 20th international conference onComputational Linguistics, page 315, Morristown,NJ, USA.
Association for Computational Linguis-tics.Koby Crammer and Yoram Singer.
2001.
Prankingwith ranking.
In Advances in Neural InformationProcessing Systems 14, pages 641?647.
MIT Press.Matthias Eck, Stephan Vogel, and Alex Waibel.
2005.Low cost portability for statistical machine transla-tion based in N-gram frequency and TF-IDF.
InProceedings of IWSLT, Pittsburgh, PA, October.Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-tali Tishby.
1997.
Selective sampling using thequery by committee algorithm.
Machine Learning,28(2-3):133?168.Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.2009.
Active learning for statistical phrase-basedmachine translation.
In NAACL ?09: Proceedingsof Human Language Technologies: The 2009 An-nual Conference of the North American Chapterof the Association for Computational Linguistics,pages 415?423, Morristown, NJ, USA.
Associationfor Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL ?03: Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, pages 48?54, Morristown, NJ, USA.Association for Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL ?03:Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, pages 160?167, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: A method for automaticevaluation of machine translation.
In ACL ?02: Pro-ceedings of the 40th Annual Meeting on Associa-tion for Computational Linguistics, pages 311?318,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings AMTA, pages 223?231, August.134
