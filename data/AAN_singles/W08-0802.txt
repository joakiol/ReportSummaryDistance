Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 10?12,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Wearable Headset Speech-to-Speech Translation SystemKriste Krstovski, Michael Decerbo, Rohit Prasad, David Stallard, Shirin Saleem,Premkumar NatarajanSpeech and Language Processing DepartmentBBN Technologies10 Moulton Street, Cambridge, MA, 02138{krstovski, mdecerbo, rprasad, stallard, ssaleem, prem}@bbn.comAbstractIn this paper we present a wearable, headsetintegrated eyes- and hands-free speech-to-speech (S2S) translation system.
The S2S sys-tem described here is configured for translin-gual communication between English andcolloquial Iraqi Arabic.
It employs an n-gramspeech recognition engine, a rudimentaryphrase-based translator for translating recog-nized Iraqi text, and a rudimentary text-to-speech (TTS) synthesis engine for playingback the English translation.
This paper de-scribes the system architecture, the functional-ity of its components, and the configurationsof the speech recognition and machine transla-tion engines.1 BackgroundHumanitarian personnel, military personnel, andvisitors in foreign countries often need to commu-nicate with residents of a host country.
Human in-terpreters are inevitably in short supply, andtraining personnel to speak a new language is diffi-cult.
Under the DARPA TRANSTAC and Babylonprograms, various teams have developed systemsthat enable two-way communication over a lan-guage barrier (Waibel et al, 2003; Zhou et al,2004; Stallard et al, 2006).
The two-way speech-to-speech (S2S) translation systems seek, in prin-ciple, to translate any utterance, by using generalstatistical models trained on large amounts ofspeech and text data.The performance and usability of such two-wayspeech-to-speech (S2S) translation systems isheavily dependent on the computational resources,such as processing power and memory, of the plat-form they are running on.
To enable open-endedconversation these S2S systems employ powerfulbut highly memory- and computation-intensivestatistical speech recognition and machine transla-tion models.
Thus, at the very minimum they re-quire the processing and memory configuration ofcommon-of-the-shelf (COTS) laptops.Unfortunately, most laptops do not have a formfactor that is suitable for mobile users.
The size,weight, and shape of laptops render them unsuit-able for handheld use.
Moreover, simply carryingthe laptop can be infeasible for users, such as mili-tary personnel, who are already overburdened withother equipment.
Embedded platforms, on theother hand, offer a more suitable form factor interms of size and weight, but lack the computa-tional resources required to run more open-ended2-way S2S systems.In previous work, Prasad et al (2007) reportedon the development of a S2S system for WindowsMobile based handheld computers.
To overcomethe challenges posed by the limited resources ofthat platform, the PDA version of the S2S systemwas designed to be more constrained in terms ofthe ASR and MT vocabulary.
As described in de-tail in (Prasad et al, 2007), the PDA based S2Ssystem configured for English/Iraqi S2S translationdelivers fairly accurate translation at faster thanreal-time.In this paper, we present ongoing developmentwork on an S2S system that runs on an even moreconstrained hardware platform; namely, a proces-sor embedded in a wearable headset with just 32MB of memory.
Compared to the PDA based sys-10tem described in (Prasad et al, 2007), the wearablesystem is designed for both eyes- and hands-freeoperation.
The headset-integrated translation de-vice described in this paper is configured for two-way conversation in English/Iraqi.
The target do-main is the force protection, which includes sce-narios of checkpoints, house searches, civil affairs,medical, etc.In what follows, we discuss the hardware andsoftware details of the headset-integrated transla-tion device.2 Hardware PlatformThe wearable S2S system described in this paperruns on a headset-integrated computational plat-form developed by Integrated Wave Technologies,Inc.
(IWT).
The headset-integrated platform em-ploys a 200 MHz StrongARM integer processorwith a total of just 32MB RAM available for boththe operating system and the translation software.The operating system currently running on theplatform is Embedded Linux.There are two audio cards on the headset plat-form for two-way communication through separateaudio input and output channels.
The default soundcard uses the headset integrated close-talking mi-crophone as an audio input and the second audiocard can be used with an ambient microphonemounted on the device or an external microphone.In addition, each headset earpiece contains innerand outer set of speakers.
The inner earpiecespeakers are for the English speaking user whowears the headset, whereas the outer speakers arefor the foreign language speaker who is not re-quired to wear the headset.3 Software ArchitectureDepicted in Figure 1 is the software system archi-tecture for the headset-integrated wearable S2Ssystem.
We are currently using a fixed-phrase Eng-lish-to-Iraqi speech translation module from IWTfor translating from English to Iraqi.
In the Iraqi-to-English (I2E) direction, we use an n-gram ASRengine to recognize Iraqi speech, a custom, phrase-based ?micro translator?
for translating Iraqi text toEnglish text, and finally a TTS module for convert-ing the English text into speech.
The rest of thispaper focuses on the components of the Iraqi-to-English translation module.Fixed point ASR Engine: The ASR engine usesphonetic hidden Markov models (HMM) with oneor more forms of the following parameter tying:Phonetic-Tied Mixture (PTM), State-Tied Mixture(STM), and State-Clustered-Tied Mixture (SCTM)models.For the headset-integrated platform, we use afixed-point ASR engine described in (Prasad et al,2007).
As in (Prasad et al, 2007) for real-time per-formance we use the compact PTM models in bothrecognition passes of our two-pass ASR decoder.Phrase-based Micro Translator: Phrase-basedstatistical machine translation (SMT) has beenwidely adopted as the translation engine in S2Ssystems.
Such SMT engines require only a largecorpus of bilingual sentence pairs to deliver robustperformance on the domain of that corpus.
How-ever, phrase-based SMT engines require significantamount of memory, even when configured for me-dium vocabulary tasks.
Given the limited memoryon the headset platform, we chose to develop in-stead a phrase-based ?micro translator?
module,which acts like a bottom-up parser.
The micro-translator uses translation rules derived from ourphrase-based SMT engine.
Rules are created auto-matically by running the SMT engine on a smalltraining corpus and recording the phrase pairs itused in decoding it.
These phrase pairs then be-come rules which are treated just as though theyhad been written by hand.
The micro translatorcurrently makes no use of probabilities.
Instead, asshown in Figure 2, for any given Arabic utterance,the translator greedily chooses the longest match-ing source phrase that does not overlap a sourcephrase already chosen.
The target phrases for thesesource phrases are then output as the translation.These target phrases come out in source-languageFigure 1.
Software architecture of the S2S system.11order, as no language model is currently used forreordering.The micro translator currently consists of 1300rules and 2000 words.
Its memory footprint is just32KB.
This small memory footprint is achieved byrepresenting the rules in binary format rather thantext format.English Playback using TTS: To play the Eng-lish translation to the headset user we developed arudimentary TTS module.
The TTS module parsesthe output of the I2E translator to extract eachtranslated word.
It then uses the list of extractedwords to read the appropriate pre-recorded (or syn-thesized) audio.
Once the word pronunciations au-dio files are read we splice the beginning and theend of the audio files to reduce the amount of si-lence and concatenate them into a single file whichis then played to the user on the inner earphonespeakers.The total memory footprint of our current Iraqito English translation module running on the head-set-integrated platform is just 9MB.
The currentconfiguration of the translation module?s IraqiASR engine yields word error rate (WER) of 20%on test-set utterances without out-of-vocabulary(OOV) words.4 Conclusions and Future WorkIn this paper we have presented the initial setup ofa speech-to-speech translation system configuredfor the headset platform.
Our current work is fo-cused on expanding the vocabulary of the Iraqi-to-English translation module by exploiting the richmorphology of Iraqi Arabic.
In particular, we areinvestigating the use of morphemes (prefix, stems,and suffixes) for expanding the effective vocabu-lary of the headset translator.
We are also develop-ing use cases for performing a formal evaluation ofboth the usability and performance of the headsettranslator.ReferencesAlex Waibel, Ahmed Badran, Alan W Black, RobertFrederking, Donna Gates ,Alon Lavie, Lori Levin,Kevin Lenzo, Laura Mayfield Tomokiyo, J?urgenReichert, Tanja Schultz, Dorcas Wallace, MonikaWoszczyna and Jing Zhang.
2003.
?Speechalator:Two-way Speech-to-Speech Translation on a Con-sumer PDA,?
Proc.
8th European Conference onSpeech Communication and Technology(EUROSPEECH 2003), Geneva, Switzerland.Bowen Zhou, Daniel D?echelotte and Yuqing Gao.2004.
?Two-way Speech-to-Speech Translation onHandheld Devices,?
Proc.
8th International Confer-ence on Spoken Language Processing, Jeju Island,Korea.David Stallard, Frederick Choi, Kriste Krstovski, PremNatarajan and Shirin Saleem.
2006.
?A HybridPhrase-based/Statistical Speech Translation System,?Proc.
The 9th International Conference on SpokenLanguage Processing (Interspeech 2006 - ICSLP),Pittsburg, PA.David Stallard, John Makhoul, Frederick Choi, EhryMacrostie, Premkumar Natarajan, Richard Schwartzand Bushra Zawaydeh.
2003.
?Design and Evaluationof a Limited two-way  Speech Translator,?
Proc.
8thEuropean Conference on Speech Communication andTechnology (EUROSPEECH 2003), Geneva, Swit-zerland.Rohit Prasad, Kriste Krstovski, Frederick Choi, ShirinSaleem, Prem Natarajan, Michael Decerbo and DavidStallard.
2007.
?Real-Time Speech-to-Speech Trans-lation for PDAs,?
Proc.
IEEE International Confer-ence on Portable Information Devices (IEEE Portable2007), Orlando, FL.Figure 2.
Decoding in micro translator.12
