Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 430?440,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsUsing Summarizationto Discover Argument Facets in Online Idealogical DialogAmita Misra, Pranav Anand, Jean Fox Tree, and Marilyn WalkerUC Santa CruzNatural Language and Dialogue Systems Lab1156 N. High.
SOE-3Santa Cruz, California, 95064, USAamisra2|panand|foxtree|mawalker@ucsc.eduAbstractMore and more of the information availableon the web is dialogic, and a significant por-tion of it takes place in online forum conver-sations about current social and political top-ics.
We aim to develop tools to summarizewhat these conversations are about.
What arethe CENTRAL PROPOSITIONS associated withdifferent stances on an issue; what are the ab-stract objects under discussion that are centralto a speaker?s argument?
How can we recog-nize that two CENTRAL PROPOSITIONS real-ize the same FACET of the argument?
We hy-pothesize that the CENTRAL PROPOSITIONSare exactly those arguments that people findmost salient, and use human summarizationas a probe for discovering them.
We describeour corpus of human summaries of opinion-ated dialogs, then show how we can identifysimilar repeated arguments, and group theminto FACETS across many discussions of atopic.
We define a new task, ARGUMENTFACET SIMILARITY (AFS), and show that wecan predict AFS with a .54 correlation score,versus an ngram system baseline of .39 anda semantic textual similarity system baselineof .45.1 IntroductionIn the wake of the Penn TreeBank, much progresshas been achieved in processing the monologic,informational language characteristic of newswiretext.
But an increasing share of the text data onthe web is unlike newswire in a variety of ways: itis dialogic, opinionated, argumentative.
And whilesome of these dialogs may be a little more than flamewars, a significant portion involve contentful, rea-PostID:TurnS1:1 Agreed She is ignoring my religious freedom and trying toinstitute her religion into law.
The law that will bar my familyfrom legal protections.
It won?t protect her marriage but willbar me and my people from from being full citizens.
She isn?tprotecting marriage but perserving her heterosexual privledge.S2:1 How on earth is she impeding on you religious freedom?
Sheisn?t trying to take away your right to any religious ceremony.
Withsuch a wide-open standard of what constitutes religious freedom thatyou seem to have, any legislation could be construed as imposing onreligious freedom.S1:2 Because it is her religious belief that marriage is between a manand a woman.
My religious belief is that marriage is between twopeople that love each other regardless of sex.
She is tying to placeher religious belif into law over mine.
Who gets hurt here?
If myreligious belief is put into law she can still marry the person of herchoice.
If her religious belief gets put into law she can still marry theperson of her choice but I do not get to.
So I and my people are hurtby codifing her religious belief into law.
She is trying to keep gaypeople out of marriage and thus preserve her heterosexual privledge.S2:2 But by that definition, either one could be viewed as imped-ing on religious freedom, including your view impeding on hers !We don?t define imposing on religious freedom on the basis of hav-ing different ideals.
It doesn?t effect your religion or religiousfreedom if you don?t get benefits under gay marriages.
You canargue in other ways, on other basis, but the idea that not giving gaysmarriage benefits is imposing on religious freedom is an empty ?argument ?.Figure 1: Gay Marriage Dialog-1.soned disputes on important social and political top-ics, as exemplified by the forum snippets in Figs.
1and 3.
Studying data like this will undoubtedlyhelp us to understand dialogic and informal argu-mentative language in general.
And, indeed, pre-vious work (Abbott et al, 2011; Somasundaran andWiebe, 2010) has examined the structure of thesediscussions ?
e.g., the argumentative discourse rela-tion a post bears to its parent (agreeing or disagree-ing), or the stance that a person takes on an issue.Our goal here is to develop techniques to recog-nize the specific arguments and counterargumentspeople tend to advance, and group them across dis-cussions into the FACETS on which that issue is ar-430(a) (b) (c)Figure 2: The overall engineering architecture of our approach.
(a) Basic engineering approach for ex-tracting CENTRAL PROPOSITIONS and clustering them into argument FACETS across several dialogs; (b)Workflow for ?detecting?
central propositions via pyramid evaluation of multiple summaries; (c) Workflowfor obtaining gold-standard labels for AFS task.gued across the population at large.
Recognizing theFACETS of an argument automatically entails at leasttwo subtasks, as schematized in Fig.
2a.PostID:TurnS1:1 Certainly not yours.
You should know that I am for no marriagein government.
It should be left to a religious institution where it willactually mean something.
The states should then go back to doingsomething that actually makes sense and doesn?t reward people likeBritney Spears for being white trash.S2:1 That is all well and good, but it is not the religious ceremonyand sanction that gays are looking for.
They already have that; thereare churches that perform same-sex marriages.
It is the civil ben-efits that are at issue.
Are you saying you would be in favor offoregoing ALL the legal rights and benefits you are afforded bymarriage?
For example: *Assumption of Spouse?s Pension *Au-tomatic Inheritance *Automatic Housing Lease Transfer *Be-reavement Leave.... What do you say?S1:2 yeah I know.
I?m saying that there should be a better system.For example, if you had a best friend who you are roommates with...both hetero for the sake of argument... and never wish to get marriedthen could they get some of the benefits you described?Figure 3: Gay Marriage Dialog-2.First, there must be a system, the CENTRALPROPOSITION detector, that can extract the most es-sential arguments in a particular conversation.
Ex-ample CENTRAL PROPOSITIONS in Figs.
1 and 3are provided in bold.
Second, there must be anothersystem, the ARGUMENT FACET inducer, that relatesthese conversation-specific arguments to each otherin terms of FACETS, e.g.
that identifies the two spe-cific central propositions in Figs.
1 and 3 about?legal protections?
and ?civil benefits?
as the same(abstract) FACET, namely that same-sex marriage isabout getting the civil rights benefits of marriage.We first focus on the question of extracting re-liable data for central propositions.
See Fig.
2b.We propose that the CENTRAL PROPOSITIONS ofa dialog are exactly those arguments that peoplefind most salient, which is naturally reflected bytheir summarization behavior.
We then apply thePyramid method, by which the CENTRAL PROPO-SITIONS bubble up to the highest tiers of the pyra-mid, thereby allowing us to identify them.
With thecentral propositions in hand, we proceed to build theargument facet inducer.
We introduce a new task ofARGUMENT FACET SIMILARITY (AFS).
We discusshow AFS is similar to, but different than SEMANTICTEXTUAL SIMILARITY (STS) (Agirre et al, 2012;Jurgens et al, 2014; Agirre et al, 2013; Beltagy etal., 2014; Han et al, 2013).Sec.
2 provides a more detailed overview and de-scription of our method, and the data that it pro-duces.
Sec.
3 describes our experimental setup forthe AFS task and then presents our results.
We de-scribe a learning approach that achieves correlationsof .54 on the AFS task, as compared to a baselinecorrelation of .45 using off-the-shelf modules that431are competitive in STS tasks.
We delay a detaileddiscussion of related work to Sec.
4 when we cancompare it to our own approach.
Sec.
5 summarizesthe paper and discusses future work.2 Experimental MethodFig.
2 summarizes our overall method for producingthe summary corpus and then extracting argumentsand clustering them into FACETS.
Our method con-sists of the following steps:S1: Dialog Selection.S2: MT summarization of dialogs selected in S1.S3: Pyramid annotation of summaries produced byS2 and selection of top-tier pyramid labelsas CENTRAL PROPOSITIONS for individual di-alogs.S4: Clustering of CENTRAL PROPOSITIONS fromS3.S5: MT ARGUMENT FACET SIMILARITY task, us-ing clusters from S4.S6: Train and test a predictor for ARGUMENTFACET SIMILARITY (Sec.
3).We explain these steps in more detail below.S1: Dialog Selection.
We use the publicly avail-able Internet Argument Corpus (IAC) (Walker et al,2012).
We use the links in the meta-data to extracta sequence of turns to build two-party dialog chainslike those in Figs.
1 and 3.
We extracted 85 dialogsfor the topic gay marriage from an original corpusof 1292 discussion threads using these criteria:?
Number of turns per contributor: We want di-alogs in which substantive issues were discussed,so we extract dialogs with at least 3 turns per con-versant that present at least 2 different perspectiveson an issue.?
Author: Some authors post frequently and woulddominate the corpus if we use random selection.
Toget richer, more diverse dialogs expressing differ-ent perspectives, we only select a single dialog be-tween any particular pair of authors from a discus-sion thread.?
Word Count in a post: Some posts are long.
Tomake it practical to collect dialog summaries, weextract dialogs where the number of words per turnis less than 250.S2: MT Summarization Task.
The summarizationtask was run on Mechanical Turk.
To get goodS1 thinks that the government should stay out of mar-riage and that it should be left to religious institu-tions.
He thinks there needs to be a better systemand that single people are the ones that are harmedthe most by marriage laws because they are unable toget any of the benefits that married people do evenif they want them, or it is important to their situa-tion.
S2 says religious ceremonies aren?t what gaypeople want because they already can have them viachurches.
They want the rights and to keep the gov-ernment out would be to give up those rights.
If singlepeople want those rights they should get married, buthe thinks you should be free to marry who you wish.The issue here is whether government or religionshould decides the principles of marriage, and whois allowed to get married.Speaker one believes that leaving it up to religionsgroups does not satisfy what gays are looking for.They are searching for the civil benefits that comewith a marriage and would like to be treated equally inthat respect.
The speaker believes gay should be ableto marry a person of their choice and get equal rights.Speaker two opinions that there should indeed be abetter system for marriage benefits and that it is all?single?
people that get screwed over by marriage?scurrent stature.
Speaker two believes that gay peopleshould marry a woman if they want the same rights.Figure 4: Two of the 5 Summaries for Dialog-2.quality summaries, workers completed a qualifica-tion test involving summarizing a sample dialog.Workers were instructed to summarize accordingto dialog length: dialogs under 750 words in 125words, and those above 750 in 175 words.
Weuse 45 dialogs in this study and save the other 40for future work.
We collect 5 summaries for eachdialog resulting in a dataset of 225 summaries.Fig.
4 provides 2 of the 5 summaries collected forthe dialog in Fig.
3.S3: Pyramid Annotation.
We trained three under-graduates to annotate summaries to produce pyra-mids.
We hypothesize that we can use the Pyramidmethod to induce the FACETS of a topic across a setof dialogs (Nenkova and Passonneau, 2004).
Theannotation of Pyramids seeks to uncover the com-mon elements, or summary content units (SCUs),across several summaries (in our case, 5).
EachSCU identifies a set of spans that are semanticallyequivalent.
Each SCU also has a unique annotator-generated label that reflects the semantic meaning ofthe contributions.
Because our aim here is to focuson argument propositional content, the annotators432were instructed to keep only the main propositionin the SCU as the label, ignoring any attributions orother types of content.
See Table 1.
Once annota-tion is complete, the SCUs are ranked based on theirfrequency across all of the summaries, as shown bythe Tier in Fig.
5, which includes data from the twosummaries in Fig.
4.Contributor S1 points to the trend to legalize gaymarriage in western countries such asNetherlands, Belgium, and most ofCanadaContributor S1 refutes this assertion, citing a numberof countries which recognize same-sexmarriage.Contributor He states the US is more similar to An-glo nations and in many of those gaymarriage is legal.Label A number of countries recognize same-sex marriage.Table 1: A sample label after removing the attribu-tions from the SCU contributors.S4: SCUs to clusters.
The pyramid structure di-rectly reflects the content that the annotators deemmost important in the original dialog.
We are inter-ested in the content that bubbles to the top across allthe dialogs.
We take the Tier 3 and above SCUs asour CENTRAL PROPOSITIONS, and extract the labelsof those SCUs.
This gives a total of 329 SCU la-bels.
In what follows we treat a cluster of CENTRALPROPOSITIONS as a FACET label, just as a synsetconcept in WordNet is labeled by its members.The purpose of AFS, then, is to provide a simi-larity metric on these SCU labels.
As described be-low (and sketched in Fig.
2c), we used MechanicalTurk to provide similarity scores between pairs ofSCU central propositions.
Although, in principle,we could have asked about all possible pairs of the329 CENTRAL PROPOSITIONS, most pairs are likelyto be unrelated, and so we used an initial clusteringalgorithm to help reduce the work and cost.To group similar arguments, we performed clus-tering across our 329 labels.
We performed Agglom-erative Clustering using Scikit-learn (Agg Cluster-ing in Fig.
2c).
(Pedregosa et al, 2011).
It recur-sively merges the pair of clusters that minimally in-creases a given linkage distance.
We used cosinesimilarity as the distance measure with average link-age criteria.
To focus on topic-specific cues, theclustering was performed using only nouns, verbsand adjectives.
After generating all pairwise combi-nations within a cluster, this approach yielded 1131argument pairs used in the Mechanical Turk AFStask.
See Fig.
2c.InstructionsWe would like you to classify each of the followingsets of pairs based on your perception of how SIM-ILAR the arguments are, on the following scale, ex-amples follow.
(5) Completely equivalent, mean pretty much exactlythe same thing, using different words.
(4) Mostly equivalent, but some unimportant detailsdiffer.
One argument may be more specific than an-other or include a relatively unimportant extra fact.
(3) Roughly equivalent, but some important informa-tion differs or is missing.
This includes cases wherethe argument is about the same FACET but the au-thors have different stances on that facet.
(2) Not equivalent, but share some details.
For exam-ple, talking about the same entities but making differ-ent arguments (different facets)(1) Not equivalent, but are on same topic(0) On a different topicFacet: A facet is a low level issue that often reoccursin many arguments in support of the author?s stanceor in attacking the other author?s position.
There aremany ways to argue for your stance on a topic.
Forexample, in a discussion about the death penalty youmay argue in favor of it by claiming that it deterscrime.
Alternatively, you may argue in favor of thedeath penalty because it gives victims of the crimesclosure.
On the other hand you may argue against thedeath penalty because some innocent people will bewrongfully executed or because it is a cruel and un-usual punishment.
Each of these specific points is afacet.For two utterances to be about the same facet, it is notnecessary that the authors have the same belief towardthe facet.
For example, one author may believe thatthe death penalty is a cruel and unusual punishmentwhile the other one attacks that position.
However, inorder to attack that position they must be discussingthe same facet.Figure 6: Instructions for AFS MT HIT.S5: MT Argument Facet Similarity HIT.
Fig.
6shows the instructions defining AFS for the MT HIT.Inspired by the scale used for STS, we collected an-notations on a 6 point scale.
One crucial differencein our formulation was a desire to capture similarityin FACET and argument simultaneously.
The use ofthe value 3 for ?same FACET, contradictory stance?was a well-thought decision in the definition of AFS.433SCU Label Used by summarizer?
Tier1 2 3 4 5Gay couples are interested in the rights and benefits associated with marriage.
X X X X X 5Gay people should be able to marry a person of their choice and get equal rights.
X X X X X 5Government should not be involved in marriage and marriage should be left toreligious institutions.X X X X X 5Discussion on the civil benefits of marriage and the rights of marriage.
X X X X 4Gay couples are unable to get any benefits that married people do.
X X X X 4There should be a better system for marriage benefits.
X X X X 4Religious ceremonies are not what gay people want.
X X X 3Single people are the ones that are harmed the most by marriage laws.
X X X 3Gay people should marry the opposite sex if they want the same rights.
X X 2Gays have religious ceremonies already can have them via churches X 1Relation to the issues by consideration of the case of a life-long bachelor uncle X 1Figure 5: Pyramid for Dialog-2.
SCU labels in Tiers 3-5 are assumed to be the CENTRAL PROPOSITIONS.Just as two words can only be antonyms if they arein the same semantic field, two arguments can onlybe contradictory if they are about the same FACET.Thus, we instruct annotators to give a score of 3 toopposing arguments on the same FACET.The task was put on Mechanical Turk using twoseparate batches.
For the first batch we randomlyselected 500 pairs from our pairs dataset of 1131pairs.
However, our subsequent impression was thatthe clustering had not filtered out enough of the un-related pairs (score 0-1).
For the second batch weselected the top 500 pairs according to the UMBCsimilarity score (Han et al, 2013).
This gave us afinal pair dataset of 1000 pairs.
Since AFS is a noveland subjective task, workers took a qualification test.Then each pair was annotated by 5 workers, and oneof the authors provided gold standard labels.
TheHIT allowed 5 AFS judgements per hit, thus thenumber of pairs annotated by a worker varies from5 to 1000.To increase reliability, we removed the annota-tions from those workers who had attempted lessthan 4 hits (20 pairs) and had the lowest pairwisecorrelations with our gold standard annotation.
Ourfinal AFS score was the average score across all theannotators.
The final AFS score correlated at .7 withour gold standard annotation, showing that the AFSsimilarity task is well-defined, and understandableby minimally trained annotators on MT.
Table 4 pro-vides typical examples of argument pairs and theirMT AFS score, along with the predicted scores fromsome of our models.
We discuss the AFS values andinteresting cases in Sec.
3 below.3 Machine Learning Experiments andResultsGiven the data collected above, we defined a super-vised machine learning experiment with AFS as ourdependent variable and different collections of fea-tures inspired from previous work as our indepen-dent variables.3.1 FeaturesNGRAM overlap.
This is our primary baseline.For each argument, we extracted all the unigrams,bigrams and trigrams, and then counted how manywere in overlap across the two arguments.
For un-igrams we did not include stop words.
StemmedNgrams were used to get better overlap.UMBC.
This is our secondary baseline.
This featureis the Semantic Textual Similarity obtained usingUMBC Semantic Similarity tool (Han et al, 2013)DISCO Distributionally Similar Category.
Weused the distributional similarity tool DISCO withthe pre-computed English Wikipedia word space(Kolb, 2008).
We extract the top 5 distributionallysimilar nouns, verbs, and adjectives for each argu-ment.
For each argument pair, three vector pairs(over nouns, verbs, and adjectives) are created withthis extended vocabulary.
Stemming was performedand cosine similarity between these vector pairs wascalculated.LIWC Category.
This feature set is based on theLinguistics Inquiry Word Count tool (Pennebaker etal., 2001).
To tune these features, we first used a setof gay marriage posts from websites such as Creat-eDebate and ConvinceMe to extract relevant LIWC434categories.
We supplemented this data with gay mar-riage posts from 4forums, but excluded the discus-sion threads in our dialog corpus.
From this data, weextracted the LIWC categories most frequent nouns,verbs and adjectives.
For the verbs category, we ex-cluded the verbs present in the NLTK stop word list.We retained only semantically rich categories suchas Biological Processes, Causation, Cognitive Pro-cesses, Humans, Negative Emotion, Positive Emo-tion, Religion, Sexual, and Social Processes.
Thescore for this set was the LIWC category overlapcount across pairs for each category.ROUGE Scores.
ROUGE is a family of metrics todetermine the quality of a summary by comparing itto other ideal summaries (Lin, 2004).
It is based ona number of overlapping units such as n-gram, wordsequences, and word pairs.
This feature includesall of the rouge f-scores available via the packageat https://pypi.python.org/pypi/pyrouge/0.1.0.3.2 ResultsOur aim is to predict the similarity among repeatedarguments across many discussions in online socialand political debate forums, a task we have dubbedARGUMENT FACET SIMILARITY (AFS).
Given theCENTRAL PROPOSITIONS from the CP detector (seeFig.
2a), we need to train an argument FACET in-ducer.
We define AFS as a regression problem andevaluate support vector regression and linear regres-sion for 10-fold cross validation using the Weka ma-chine learning toolkit (Hall et al, 2005).Classifier RMS MAE RSMO 1.0208 0.8019 0.532Linear Regression 0.9996 0.8003 0.540Table 2: Support Vector and Linear Regression.RMS: Root Mean Squared Error, MAE: Mean AbsoluteError, R: Correlation Coefficient.Table 2 shows that the results for support vec-tor regression are worse than the linear regressionmodel using our proposed features combined withUMBC, hence we focus hereon on linear regres-sion.
Table 3 provides the correlations, MAE, andRMS values for models produced using various setsof features.
We considered two baselines, simpleNgram overlap and the off-the-shelf UMBC STSmetric (Han et al, 2013).
In general, we foundthat Ngram overlap (Row 1) performed best aloneof our features, but falls short of the UMBC base-Row Feature Set R MAE RMS1 NGRAM (N) 0.39 0.90 1.092 UMBC (U) 0.46 0.86 1.063 LIWC (L) 0.32 0.92 1.134 DISCO (D) 0.33 0.93 1.125 ROUGE (R) 0.34 0.91 1.126 N-U 0.47 0.85 1.057 N-L 0.45 0.86 1.068 N-R 0.42 0.88 1.089 N-D 0.41 0.89 1.0810 U-R 0.48 0.84 1.0411 U-L 0.51 0.83 1.0212 U-D 0.45 0.86 1.0613 N-L-R 0.48 0.84 1.0414 U-L-R 0.53 0.81 1.0015 N-L-R-D 0.50 0.83 1.0316 N-L-R-U 0.54 0.80 1.0017 N-L-R-D-U 0.54 0.80 1.00Table 3: Results for Different Individual Featuresand Feature Combinations.line (Row 2).
It is interesting that Ngram alone out-performs distributional measures (which Conrad &Wiebe found most helpful) as well as Rouge (whichcontains metrics insensitive to linear adjacency).Table 3, Row 15, shows that the best correlationthat is achievable without UMBC is the combinationof Ngram, LIWC, ROUGE and DISCO (NLRD).This combination significantly improves over theUMBC baseline of 0.46 to 0.50 (paired t-test, p <.05).We then tested combinations of of features todetermine which feature sets are complementary.LIWC + NGRAM is significantly different thanNGRAM alone ( p< 0.01), and ROUGE + NGRAMis significantly different than NGRAM alone ( p =0.03), but DISCO does not add anything ( p = 0.2).This shows that LIWC and ROUGE features com-plement Ngram features.
Other combinations of in-terest are NGRAM + LIWC (Row 7) which amaz-ingly performs as well as UMBC while UMBC in-cludes sentence alignment, a model of negation, anddistributional measures (Han et al, 2013).
This sug-gests that AFS is clearly a different task that STS.Additionally we also combined our proposed set offeatures with UMBC.
A comparison of Row 15 (ourfeature set) with Rows 16 and 17 of Table 3 wherewe combine our features with UMBC shows that thisimproves the correlation further, from the UMBCbaseline of 0.46 to 0.54 (p < 0.01.
)435Row N L U NLRD NLRDU MTAFSArg1 Arg21 1.38 1.50 0.37 1.31 0.40 0.00 everyone has the freedom ofspeechservice in the military2 2.00 2.02 1.55 2.33 1.86 1.14 gay people should be able tomarry a person of their choiceand get equal rightsreferring to namecalling and vi-olence from the original postthat was opposing gay rights3 2.00 1.29 2.52 1.37 1.54 1.33 Constitutional right to be opposedto gay marriage as well as gaypeople themselvesarguing about marriage benefitsbetween single people and mar-ried4 2.00 1.70 2.74 1.77 1.98 1.80 people should not pick and choosewhat they want equal rights on.people did not want gay marriage5 1.38 1.92 0.88 1.94 1.64 2.50 the Republicans creating anotherHolocaustNo republican in leadershipwould call for the exterminationof gays6 1.69 2.02 2.58 1.89 2.49 2.60 homosexuals have all the samerights as heterosexualsOpposition to equal rights forgay couples.7 1.83 2.40 1.46 2.81 2.51 3.00 There was prejudice against gaysin 1909 just as there is nowit is prejudice as opposed to reli-gious or moral beliefs which fuelthe anti-gay agenda;8 2.00 1.70 3.16 1.73 2.41 3.40 homosexual relationshipsshould not compare to het-erosexual marriages becauseonly heterosexuals are legallyallowed to marrymarriage should be between aheterosexual couple9 2.00 2.70 2.09 2.83 3.03 3.50 it is prejudice as opposed to reli-gious or moral beliefs which fuelthe anti-gay agenda;when people claim religion in do-ing prejudice they are actuallyabandoning their morals10 2.94 2.02 2.93 2.18 2.70 3.50 gay people should be able tomarry a person of their choiceand get equal rights.Gay couples are unable to getany benefits that married peo-ple do.11 2.14 1.50 2.91 2.08 2.62 3.60 Paul Cameron is the voice of theRepublicansConversation about PaulCameron12 2.63 3.63 2.60 3.75 3.57 4.17 in opening this opportunity forgay marriage, the definition ofmarriage will changeopponents of homosexual mar-riage tend to argue that a changeto marriage law would make it tooopen ended13 4.23 2.72 2.26 4.82 4.12 4.50 AIDs was initially spread in theUnited States primarily by homo-sexuals.No one argues the point that AIDswas spread in the United States byhomosexuals.Table 4: Predicted Scores for each model and the Mechanical Turk AFS gold standard for selected argumentpairs from the pairs dataset.
Best performing model for each pair is shown in bold.
The table is sorted bythe AFS score (gold standard).
The argument pairs shown in bold are cases where UMBC by itself beats ourproposed model.
KEY: Feature sets model.
N = NGRAM, U = UMBC STS tool, L = Linguistic Inquiry andWord Count; R = Rouge, D = DISCO, AFS= Mean of Mechanical Turker AFS scores, our gold standard.For example, NLRD means a combination of NGRAM, LIWC, ROUGE and DISCO.It is also interesting to examine the differences inmodel scores for particular argument pairs as shownin Table 4.
The best performing model for each rowis in bold in Table 4.
As described in the HIT in-structions in Fig.
6, values of AFS near 0 (Row 1)indicate different topics and no similarity.
Valuesnear 1 indicate same topic but different arguments(Rows 2,3).
Values of 3 and above indicate sameFACET (Rows 7,8), and values near 5 are same facetand very similar argument (Rows 12 and 13).
BothArg1 and Arg2 in Row 10 makes the same argumentbut Arg1 includes additional argumentation.
In Row12, there is very low Ngram overlap, but strong AFSand NLRD performs better than the other models,and LIWC performs well by itself.In Row 1, UMBC performs the best with a pre-dicted score of 0.37 as opposed to an AFS scoreof 0.00.
Other rows where UMBC on its own pro-vides the best performance are highlighted in the ta-ble with Arg1 and Arg2 in bold.
The top perfor-mance of NLRD in Row 5 without UMBC perhapsarises from the semantic information that extermi-nation and holocau are somehow related.
NGRAMoverlap does the best in Row 13 despite the fact thatthe phrase No one argues the point that does not par-ticipate in the NGRAM overlap.4364 Related WorkOur approach draws on three different strands of re-lated work: (1) argument mining; (2) semantic tex-tual similarity; and (3) dialog summarization, whichwe discuss and compare with our work below.Argument Mining.
The study of the structure of ar-guments has a long tradition in logic, rhetoric andpsychology (Walton et al, 2008; Reed and Rowe,2004; Walton, 2009; Gilbert, 1997; Jackson and Ja-cobs, 1980; Madnani et al, 2012).
Much of thiswork has been on formal (legal or political) argu-mentation, and the small computational literaturethat has applied the rhetorical categories of this re-search has likewise focused on formal, monologictext (Feng and Hirst, 2011; Palau and Moens, 2009;Goudas et al, 2014).
More recent work (Ghosh etal., 2014) has attempted to apply these theories todialogic text in online forums.
Ghosh et al labelspans in conversations with attacking moves (CALL-OUTS) and their corresponding argumentative TAR-GETS in another speaker?s utterance, and they at-tempt to learn these callout-target pairs in a super-vised framework.
Other work attempts to identifygeneral categories of speech-acts such as disagree-ments or justifications (Misra and Walker, 2015; Bi-ran and Rambow, 2011).What unites all of the above approaches is an in-terest in understanding the detailed rheotrical struc-ture of a particular linguistic interaction (monologicor dialogic).
Our present work is focused instead oninducing the recurring FACETS in a particular topicdomain via weakly supervised learning over severaldialogic interactions.
Several different threads of re-cent research on argument mining have strong paral-lels with this goal (Conrad et al, 2012; Boltuzic and?Snajder, 2014; Hasan and Ng, 2014).Conrad & Wiebe construct an argument miningsystem on monologic weblog and news data aboutuniversal healthcare.
One component of their sys-tem identifies ARGUING SEGMENTS and the sec-ond component labels the segments with the rele-vant stance-specific ARGUMENT TAGS.
They showthat distributional similarity features help identifyarguments that belong to the same tag set (notably,we did not find distributional similarity helpful forAFS.)
Boltuzic & Snajder pursue argument miningon comment streams.
Instead of hand-generating ar-gument tags like Conrad & Wiebe, they select shortsentential summaries of the key arguments for agiven topic from a debate website, and then labelcomments on the same topic from a different web-site with the most closely matching summary.
Thesame problem on debate posts is tackled as a ?reasonclassification?
problem (Hasan and Ng, 2014), witha probabilistic framework for argument recognition(reason classification) that operates jointly with therelated task of stance classification.All of these approaches differ from ours in threerespects.
First, they all assume a finite set of topic-specific labels that are determined in some form bythe researchers themselves.
In contrast, we seekto uncover popular facets via clustering the centralpropositions across the dialogs.
After our own ini-tial categorical efforts, we feel that the argument?topics?
have such nuance that they resist clear la-bels or category membership.
Instead, we feel thata scale such as AFS is a better fit, both for the di-versity of the data itself and for the idea of inducingFACETS bottom up.
Second, these approaches as-sume the labels are dependent on a particular stancetowards an issue, whereas our facets are deliberatelydesigned to unify across stance disagreement.
Fi-nally, all other approaches in argument mining workfrom the source text itself.
We instead (to our knowl-edge, for the first time) work from human summariesof dialogs because it is an open question whetherthe CENTRAL PROPOSITIONS for a dialog are reallyidentifiable as continuous spans of text in the dialogitself.
(Indeed, our corpus will allow us to determinehow true that assumption is.
)Semantic Textual Similarity.
There appears tobe similarity between FACET induction and aspectlearning in sentiment analysis (Brody and Elhadad,2010), but FACETS are propositional abstract ob-jects, while aspects can usually be described asnouns or properties.
Facet induction is more similarto work on STS (Mihalcea et al, 2006; Yeh et al,2009; Agirre et al, 2012; Han et al, 2013; Jurgenset al, 2014).
Calculating similarity is a central as-pect of AFS.
Our scale and MT task for AFS was in-spired by the STS task and definition.
In addition, asa baseline we apply an off-the-shelf system that cal-culates STS (UMBC) and compare it with our ownsystem (Han et al, 2013).
In order to avoid askingfor judgements for many unrelated arguments (CEN-TRAL PROPOSITIONS), and to make the AFS taskmore doable for Turkers, we also use UMBC as afilter on pairs of CENTRAL PROPOSITIONS as partof making our HIT.
This biases the distribution of437the training set to having a much larger set of moresimilar pairs, which has been a problem for previouswork (Boltuzic and?Snajder, 2014), where the vastmajority of pairs that were labelled were unrelated.However the AFS task is clearly different than STS,partly because the data is dialogic and partly becauseit is argumentative.
Our results show that we can im-prove on STS systems for the AFS task.Dialog Summarization.
Much previous work on di-alog summarization focused on extracting phenom-ena specific to meetings, such as action items ordecisions (Murray et al, 2006; Hsueh and Moore,2008; Whittaker et al, 2012; Janin et al, 2004;Carletta, 2007).
Other approaches, like our work,use semantic similarity metrics to identify the mostcentral or important utterances of a spoken dialog(Gurevych and Strube, 2004), but do not attempt tofind the FACETS of a set of arguments across mul-tiple dialogs.
Another parallel may exist betweenwork on nuclearity in RST and its use in summa-rization (Marcu, 1999).
However our notion of aCENTRAL PROPOSITION is different than nuclear-ity in RST, since FACETS are derived from CEN-TRAL PROPOSITIONS that rise to the top of the pyra-mid across summarizers, and then (via AFS) acrossmany dialogs on a topic, while RST nuclearity isonly defined for a span of text by a single speaker.Other work examines how social phenomena af-fect summarization, such as a study of how thepoliteness level in computer-generated dialogs im-pacted summaries (Roman et al, 2006).
Emotionnaturally occurs in the IAC, and summarizers?
orien-tation to emotion is intriguing.
Emotional informa-tion has been observed even in summaries of profes-sional chats discussing technology (Zhou and Hovy,2005).
However the instructions to our Pyramid an-notators were to not include information of this typein the pyramids.
We are currently collecting an ad-ditional summary corpus using a method that we ex-pect to result in more evaluative and emotional as-sessments in summaries.5 ConclusionThis paper presents a method and results for ex-tracting FACETS of a topic, across multiple infor-mal arguments on the same topic.
We first use hu-man summarization of dialogs as a probe to deter-mine the CENTRAL PROPOSITIONS of each dialog.Then we use clustering in combination with mea-sures of SEMANTIC SIMILARITY to group the CEN-TRAL PROPOSITIONS into the important FACETS ofan argument across many different dialogs.
Impor-tantly, we do not attempt to enumerate the possibleFACETS for an argument in advance, believing thatbottom-up discovery of FACETS is a better fit to theproblem.This paper contributes to the current state ofknowledge in three ways: (1) we collected sum-maries of spontaneously-produced written dialog ofhigh social and political importance (available fromhttp://nlds.soe.ucsc.edu/summarycorpus).
(2) we proposed a novel application of the pyramidsummarization scheme to the task of FACET induc-tion; and (3) we introduce a new task of ARGUMENTFACET SIMILARITY (AFS) aimed at identifyingFACETS across opinionated dialogs and show thatwe can identify AFS with a correlation of .54 asopposed to a baseline of .46 provided by a systemdesigned for a similar task.
We suspect that thesummarize-and-collate approach used here couldbe promisingly applied to produce annotations on arange of subjective, holistic properties of dialog.In future work, we aim to expand on this work inseveral ways.
First, we hope to expand summaries,similarity judgments, and systems to several topicsbeyond gay marriage.
We believe, for example, thatthe features and the system we have trained for AFSwill apply to other domains without retraining, sincenone of the features are topic specific, but we havenot shown that.
In addition, we aim to develop addi-tional features and improve on the results reportedhere.
For example, we believe that it is possiblethat other off-the-shelf systems, such as for exam-ple one for sentence specificity (Louis and Nenkova,2011; Louis and Nenkova, 2012), might possiblyhelp with aspects of this task.
In addition, in future,we aim to automatically identify CENTRAL PROPO-SITIONS without the mediation of human summa-rizers and evaluators.
Given the summaries that wehave collected for each dialog, we plan to examinethe relationship between the contributors to the re-lated pyramid and the original source text, to deter-mine whether indeed there are surface features ofthe source that would allow us to treat CENTRALPROPOSITION detection as an extractive task.Acknowledgments This work was funded by NSFGRANT IIS-1302668, Grant NPS-BAA-03, and anIARPA Grant on Persuasion in Dialogue to UCSCby subcontract from the University of Maryland.438ReferencesRob Abbott, Marilyn Walker, Jean E. Fox Tree, PranavAnand, Robeson Bowmani, and Joseph King.
2011.How can you say such things?!?
: Recognizing Dis-agreement in Informal Political Argument.
In Pro-ceedings of the ACL Workshop on Language and So-cial Media.Eneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A piloton semantic textual similarity.
In Proceedings of theFirst Joint Conference on Lexical and ComputationalSemantics-Volume 1: Proceedings of the main confer-ence and the shared task, and Volume 2: Proceedingsof the Sixth International Workshop on Semantic Eval-uation, pages 385?393.
Association for ComputationalLinguistics.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
Sem 2013 sharedtask: Semantic textual similarity, including a pilot ontyped-similarity.
In In* SEM 2013: The Second JointConference on Lexical and Computational Semantics.Association for Computational Linguistics.
Citeseer.Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014.Probabilistic soft logic for semantic textual similar-ity.
Proceedings of Association for ComputationalLinguistics (ACL-14).O.
Biran and O. Rambow.
2011.
Identifying justifica-tions in written dialogs.
In 2011 Fifth IEEE Inter-national Conference on Semantic Computing (ICSC),pages 162?168.
IEEE.Filip Boltuzic and Jan?Snajder.
2014.
Back up yourstance: Recognizing arguments in online discussions.In Proceedings of the First Workshop on Argumenta-tion Mining, pages 49?58.Samuel Brody and Noemie Elhadad.
2010.
An unsu-pervised aspect-sentiment model for online reviews.In Human Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 804?812.
Association for Computational Linguistics.Jean Carletta.
2007.
Unleashing the killer corpus: ex-periences in creating the multi-everything ami meet-ing corpus.
Language Resources and Evaluation,41(2):181?190.Alexander Conrad, Janyce Wiebe, et al 2012.
Recog-nizing arguing subjectivity and argument tags.
In Pro-ceedings of the Workshop on Extra-Propositional As-pects of Meaning in Computational Linguistics, pages80?88.
Association for Computational Linguistics.Vanessa Wei Feng and Graeme Hirst.
2011.
Classify-ing arguments by scheme.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies-Volume1, pages 987?996.
Association for Computational Lin-guistics.Debanjan Ghosh, Smaranda Muresan, Nina Wacholder,Mark Aakhus, and Matthew Mitsui.
2014.
Analyzingargumentative discourse units in online interactions.ACL 2014, page 39.Michael A. Gilbert.
1997.
Coalescent argumentation.Theodosis Goudas, Christos Louizos, Georgios Petasis,and Vangelis Karkaletsis.
2014.
Argument extractionfrom news, blogs, and social media.
In Artificial In-telligence: Methods and Applications, pages 287?299.Springer.I.
Gurevych and M. Strube.
2004.
Semantic similarityapplied to spoken dialogue summarization.
In Pro-ceedings of the 20th international conference on Com-putational Linguistics, pages 764?771.
ACL.M.
Hall, F. Eibe, G. Holms, B. Pfahringer, P. Reutemann,and I. Witten.
2005.
The weka data mining software:An update.
SIGKDD Explorations, 11(1).Lushan Han, Abhay Kashyap, Tim Finin, James May-field, and Jonathan Weese.
2013.
Umbc ebiquity-core: Semantic textual similarity systems.
Atlanta,Georgia, USA, page 44.Kazi Saidul Hasan and Vincent Ng.
2014.
Why are youtaking this stance?
identifying and classifying reasonsin ideological debates.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing.P.Y.
Hsueh and J. Moore.
2008.
Automatic decision de-tection in meeting speech.
Machine Learning for Mul-timodal Interaction, pages 168?179.Sally Jackson and Scott Jacobs.
1980.
Structure ofconversational argument: Pragmatic bases for the en-thymeme.
Quarterly Journal of Speech, 66(3):251?265.A.
Janin, J. Ang, S. Bhagat, R. Dhillon, J. Edwards,J.
Macias-Guarasa, N. Morgan, B. Peskin, E. Shriberg,A.
Stolcke, et al 2004.
The icsi meeting project:Resources and research.
In Proceedings of the 2004ICASSP NIST Meeting Recognition Workshop.David Jurgens, Mohammad Taher Pilehvar, and RobertoNavigli.
2014.
Semeval-2014 task 3: Cross-level se-mantic similarity.
SemEval 2014, page 17.P.
Kolb.
2008.
Disco: A multilingual database ofdistributionally similar words.
In Proceedings ofKONVENS-2008.C.-Y.
Lin.
2004.
Rouge: A package for automaticevaluation of summaries rouge: A package for auto-matic evaluation of summaries.
In Proceedings of theWorkshop on Text Summarization Branches Out (WAS2004).Annie Louis and Ani Nenkova.
2011.
Automatic identi-fication of general and specific sentences by leveragingdiscourse annotations.
In IJCNLP, pages 605?613.439Annie Louis and Ani Nenkova.
2012.
A corpus of gen-eral and specific sentences from news.
In LREC, pages1818?1821.Nitin Madnani, Michael Heilman, Joel Tetreault, andMartin Chodorow.
2012.
Identifying high-level or-ganizational elements in argumentative discourse.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL HLT ?12, pages 20?28, Stroudsburg, PA,USA.
Association for Computational Linguistics.Daniel Marcu.
1999.
Discourse trees are good indica-tors of importance in text.
Advances in automatic textsummarization, pages 123?136.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In AAAI, volume 6, pages775?780.Amita Misra and Marilyn A Walker.
2015.
Topic inde-pendent identification of agreement and disagreementin social media dialogue.
In Proceedings of the SIG-DIAL 2013 Conference: The 15th Annual Meeting ofthe Special Interest Group on Discourse and Dialogue.G.
Murray, S. Renals, J. Carletta, and J. Moore.
2006.
In-corporating speaker and discourse features into speechsummarization.
In Proceedings of the main conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics, pages 367?374.
Association forComputational Linguistics.A.
Nenkova and R. Passonneau.
2004.
Evaluating con-tent selection in summarization: The pyramid method.In Proceedings of HLT-NAACL, volume 2004.Raquel Mochales Palau and Marie-Francine Moens.2009.
Argumentation mining: the detection, classifi-cation and structure of a rguments in text.
In Proceed-ings of the 12th international conference on artificialint elligence and law, pages 98?107.
ACM.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.J.
W. Pennebaker, L. E. Francis, and R. J. Booth, 2001.LIWC: Linguistic Inquiry and Word Count.Chris Reed and Glenn Rowe.
2004.
Araucaria: Softwarefor argument analysis, diagramming and representa-tion.
International Journal on Artificial IntelligenceTools, 13(04):961?979.N.
Roman, P. Piwek, P. Carvalho, and M. B. R. Ariadne.2006.
Politeness and bias in dialogue summarization:two exploratory studies.
In J. Shanahan, Y. Qu, andJ.
Wiebe, editors, Computing attitude and affect intext: theory and applications, volume 20 of The In-formation Retrieval Series.
Springer.S.
Somasundaran and J. Wiebe.
2010.
Recognizingstances in ideological on-line debates.
In Proceedingsof the NAACL HLT 2010 Workshop on ComputationalApproaches to Analysis and Generation of Emotion inText, pages 116?124.
Association for ComputationalLinguistics.Marilyn Walker, Pranav Anand, Rob Abbott, Jean E. FoxTree, Craig Martell, and Joseph King.
2012.
That?syour evidence?
: Classifying stance in online politicaldebate.
Decision Support Sciences.Douglas Walton, Chris Reed, and Fabrizio Macagno.2008.
Argumentation Schemes.
Cambridge UniversityPress.Douglas Walton.
2009.
Argumentation theory: A veryshort introduction.
In Guillermo Simari and Iyad Rah-wan, editors, Argumentation in Artificial Intelligence,pages 1?22.
Springer US.S.
Whittaker, V. Kalnikait?e, and P. Ehlen.
2012.
Markupas you talk: establishing effective memory cues whilestill contributing to a meeting.
In Proceedings of theACM 2012 conference on Computer Supported Coop-erative Work, pages 349?358.
ACM.Eric Yeh, Daniel Ramage, Christopher D Manning,Eneko Agirre, and Aitor Soroa.
2009.
Wikiwalk:random walks on wikipedia for semantic relatedness.In Proceedings of the 2009 Workshop on Graph-basedMethods for Natural Language Processing, pages 41?49.
Association for Computational Linguistics.L.
Zhou and E. Hovy.
2005.
Digesting virtual geekculture: The summarization of technical internet re-lay chats.
In Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, pages298?305.
ACL.440
