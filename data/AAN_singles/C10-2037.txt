Coling 2010: Poster Volume, pages 320?328,Beijing, August 2010Monolingual Distributional Profiles for Word Substitution in MachineTranslationRashmi Gangadharaiahrgangadh@cs.cmu.eduRalf D. Brownralf@cs.cmu.eduLanguage Technologies Institute,Carnegie Mellon UniversityJaime Carbonelljgc@cs.cmu.eduAbstractOut-of-vocabulary (OOV) words present asignificant challenge for Machine Trans-lation.
For low-resource languages, lim-ited training data increases the frequencyof OOV words and this degrades the qual-ity of the translations.
Past approacheshave suggested using stems or synonymsfor OOV words.
Unlike the previousmethods, we show how to handle not justthe OOV words but rare words as wellin an Example-based Machine Transla-tion (EBMT) paradigm.
Presence of OOVwords and rare words in the input sentenceprevents the system from finding longerphrasal matches and produces low qual-ity translations due to less reliable lan-guage model estimates.
The proposedmethod requires only a monolingual cor-pus of the source language to find can-didate replacements.
A new frameworkis introduced to score and rank the re-placements by efficiently combining fea-tures extracted for the candidate replace-ments.
A lattice representation scheme al-lows the decoder to select from a beamof possible replacement candidates.
Thenew framework gives statistically signif-icant improvements in English-Chineseand English-Haitian translation systems.1 IntroductionAn EBMT system makes use of a parallel corpusto translate new sentences.
Each input sentenceis matched against the source side of a trainingcorpus.
When matches are found, the correspond-ing translations in the target language are obtainedthrough sub-sentential alignment.
In our EBMTsystem, the final translation is obtained by com-bining the partial target translations using a sta-tistical target Language Model.
EBMT systems,like other data-driven approaches, require largeamounts of data to function well (Brown, 2000).Having more training data is beneficial re-sulting in log-linear improvement in translationquality for corpus-based methods (EBMT, SMT).Koehn (2002) shows translation scores for a num-ber of language pairs with different training sizestranslated using the Pharaoh SMT toolkit (Koehnet al, 2003).
However, obtaining sizable paral-lel corpora for many languages is time-consumingand expensive.
For rare languages, finding bilin-gual speakers becomes especially difficult.One of the main reasons for low quality transla-tions is the presence of large number of OOV andrare words (low frequency words in the trainingcorpus).
Variation in domain and errors in spellingincrease the number of OOV words.
Many of thepresent translation systems either ignore these un-known words or leave them untranslated in the fi-nal target translation.
When data is limited, thenumber of OOV words increases, leading to thepoor performance of the translation models andthe language models due to the absence of longersequences of source word matches and less reli-able language model estimates.Approaches in the past have suggested usingstems or synonyms for OOV words as replace-ments (Yang and Kirchhoff, 2006).
Similaritymeasures have been used to find words that areclosely related (Marton et al, 2009).
For morpho-320logically rich languages, the OOV word is mor-phologically analyzed and the stem is used as itsreplacement (Popovic?
and Ney, 2004).This paper presents a simpler method inspiredby the Context-based MT approach (Carbonell etal., 2006) to improve translation quality.
Themethod requires a large source language mono-lingual corpus and does not require any otherlanguage dependent resources to obtain replace-ments.
Approaches suggested in the past onlyconcentrated on finding replacements for the OOVwords and not the rare words.
This paper pro-poses a unified method to find possible replace-ments for OOV words as well as rare words basedon the context in which these words appear.
Inthe case of rare words, the translated sentence istraced back to find the origin of the translationsand the target translations of the replacements arereplaced with the translations of the rare words.
Inthe case of OOV words, the target translations arereplaced by the OOV word itself.
The main ideafor adopting this approach is the belief that theEBMT system will be able to find longer phrasalmatches and that the language model will be ableto give better probability estimates while decod-ing if it is not forced to fragment text at OOV andrare-word boundaries.
This method is highly ben-eficial for low-resource languages that do not havemorphological analysers or Part-of-Speech (POS)taggers and in cases where the similarity measuresproposed in the past do not find closely relatedwords for certain OOV words.The rest of the paper is organized as follows.The next section (Section 2) discusses relatedwork in handling OOV words.
Section 3 describesthe method adopted in this paper.
Section 4 de-scribes the experimental setup.
Section 5 reportsthe results obtained with the new framework forEnglish-Chinese and English-Haitian translationsystems.
Section 6 concludes and suggests pos-sible future work.2 Related WorkOrthographic and morpho-syntactic techniquesfor preprocessing training and test data have beenshown to reduce OOV word rates.
Popovic?and Ney (2004) demonstrated this on rich mor-phological languages in an SMT system.
Theyintroduced different types of transformations tothe verbs to reduce the number of unseen wordforms.
Habash (2008) addresses spelling, name-transliteration OOVs and morphological OOVs inan Arabic-English Machine Translation system.Phrases with the OOV replacements in the phrasetable of a phrase-based SMT system were ?recy-cled?
to create new phrases in which the replace-ments were replaced by the OOV words.Yang and Kirchhoff (2006) proposed a back-off model for phrase-based SMT that translatedword forms in the source language by hierarchi-cal morphological phrase level abstractions.
Ifan unknown word was found, the word was firststemmed and the phrase table entries for wordssharing the same stem were modified by replacingthe words with their stems.
If a phrase entry or asingle word phrase was found, the correspondingtranslation was used, otherwise the model backedoff to the next level and applied compound split-ting to the unknown word.
The phrase table in-cluded phrasal entries based on full word forms aswell as stemmed and split counterparts.Vilar et al (2007) performed the translationprocess treating both the source and target sen-tences as a string of letters.
Hence, there areno unknown words when carrying out the actualtranslation of a test corpus.
The word-based sys-tem did most of the translation work and the letter-based system translated the OOV words.The method proposed in this work to han-dle OOV and rare words is very similar to themethod adopted by Carbonell et al (2006) to gen-erate word and phrasal synonyms in their Context-based MT system.
Context-based MT does notrequire parallel text but requires a large monolin-gual target language corpus and a fullform bilin-gual dictionary.
The main principle is to find thosen-gram candidate translations from a large targetcorpus that contain as many potential word andphrase translations of the source text from the dic-tionary and fewer spurious content words.
Theoverlap decoder combines the target n-gram trans-lation candidates by finding maximal left and rightoverlaps with the translation candidates of the pre-vious and following n-grams.
When the overlapdecoder does not find coherent sequences of over-lapping target n-grams, more candidate transla-321tions are obtained by substituting words or phrasesin the target n-grams by their synonyms.Barzilay and McKeown (2001) and Callison-Burch et al (2006) extracted paraphrases frommonolingual parallel corpus where multiple trans-lations were present for the same source.
The syn-onym generation in Carbonell et al (2006) differsfrom the above in that it does not require paral-lel resources containing multiple translations forthe same source language.
In Carbonell et al(2006), a list of paired left and right contexts thatcontain the desired word or phrase are extractedfrom the monolingual corpus.
The same corpusis used to find other words and phrases that fit thepaired contexts in the list.
The idea is based on thedistributional hypothesis which states that wordswith similar meanings tend to appear in similarcontexts (Harris, 1954).
Hence, their approachperformed synonym generation on the target lan-guage to find translation candidates that wouldprovide maximal overlap during decoding.Marton et al (2009) proposed an approach sim-ilar to Carbonell et al (2006) to obtain replace-ments for OOV words, where monolingual dis-tributional profiles for OOV words were con-structed.
Hence, the approach was applied on thesource language side as opposed to Carbonell etal.
(2006) which worked on the target language.Only similarity scores and no other features wereused to rank the paraphrases (or replacements)that occured in similar contexts.
The high rank-ing paraphrases were used to augment the phrasetable of phrase-based SMT.All of the previously suggested methods onlyhandle OOV words (except Carbonell et al (2006)which handles low frequency target phrases) andno attempt is made to handle rare words.
Many ofthe methods explained above directly modify thetraining corpus (or phrase table in phrase-basedSMT) increasing the size of the corpus.
Ourmethod clusters words and phrases based on theircontext as described by Carbonell et al (2006) butuses the clustered words as replacements for notjust the OOV words but also for the rare wordson the source language side.
Our method doesnot make use of any morphological analysers,POS taggers or manually created dictionariesas they may not be available for many rare orlow-resource languages.
The translation of thereplacements in the final decoded target sentenceis replaced by the translation of the original word(or the source word itself in the OOV case),hence, we do not specifically look for synonyms.The only condition for a word to be a candidatereplacement is that its left and right context needto match with that of the OOV/rare-word.
Hence,the clustered words could have different semanticrelations.
For example,(cluster1):?laugh, giggle, chuckle, cry, weep?where ?laugh, giggle, chuckle?
are synonyms and?cry, weep?
are antonyms of ?laugh?.Clusters can also contain hypernyms (or hy-ponyms), meronyms (or holonyms), troponymsand coordinate terms along with synonyms andantonyms.
For example,(cluster2):?country, region, place, area, dis-trict, state, zone, United States, Canada, Korea,Malaysia?.where ?country?
is a hypernym of ?UnitedStates/Canada/Korea/Malaysia?.
?district?
is ameronym of ?state?.
?United States, Canada,Korea, Malaysia?
are coordinate terms sharing?country?
as their hypernym.The contributions made by the paper are three-fold: first, replacements are found for not just theOOV words but for the rare words as well.
Sec-ond, the framework used allows scoring replace-ments based on multiple features to permit op-timization.
Third, instead of directly modifyingthe training corpus by replacing the candidate re-placements by the OOV words, a new representa-tion scheme is used for the test sentences to effi-ciently handle a beam of possible replacements.3 Proposed MethodLike Marton et al (2009), only a large monolin-gual corpus is required to extract candidate re-placements.
To retrieve more replacements, themonolingual corpus is pre-processed by first gen-eralizing numbers, months and years by NUM-BER, MONTH and YEAR tags, respectively.3223.1 OOV and Rare wordsWords in the test sentence (new source sentenceto be translated) that do not appear in the trainingcorpus are called OOV words.
Words in the testsentence that appear less thanK times in the train-ing corpus are considered as rare words (in thispaper K = 3).
The method presented in the fol-lowing sections holds for both OOV as well as rarewords.
In the case of rare words, the final transla-tion is postprocessed (Section 3.7) to include thetranslation of the rare word.The procedure adopted will be explained witha real example T (the rest of the sentence isremoved for the sake of clarity) encountered inthe test data with ?hawks?
as the OOV word,T :a mobile base , hitting three hawks withone arrow over the past few years ...3.2 ContextAs the goal is to obtain longer target phrasal trans-lations for the test sentence before decoding,only words that fit the left and right context of theOOV/rare-word in the test sentence are extracted.Unlike Marton et al (2009) where a context listfor each OOV is generated from the contextsof their replacements, this paper uses only theleft and right context of the OOV/rare-word.The default window size for the context is fivewords (two words to the left and two words to theright of the OOV/rare-word).
If the windowedwords contain only function words, the windowis incremented until at least one content word ispresent in the resulting context.
This enables oneto find sensible replacements that fit the contextwell.
The contexts for T are:Left-context (L): hitting threeRight-context (R): with one arrowThe above contexts are further processed togeneralize the numbers by a NUMBER tagto produce more candidate replacements.
Theresulting contexts are now:Left-context (L): hitting NUMBERRight-context (R): with NUMBER arrowAs a single L ?
R context is used, a farsmaller number of replacements are extracted.3.3 Finding Candidate replacementsThe monolingual corpus (ML) of the source lan-guage is used to find words and phrases (Xk) thatfitLXkR i.e., withL as its left context and/orR asits right context.
The maximum length for Xk isset to 3 currently.
The replacements are further fil-tered to obtain only those replacements that con-tain at least one content word.
As illustrated ear-lier, the resulting replacement candidates are notnecessarily synonyms.3.4 FeaturesA local context of two to three words to the leftof an OOV/rare-word (wordi) and two to threewords to the right of wordi contain sufficientclues for the word,wordi.
Hence, local contextualfeatures are used to score each of the replacementcandidates (Xi,k) of wordi.
Each Xi,k extractedin the previous step is converted to a feature vectorcontaining 11 contextual features.
Certainly morefeatures can be extracted with additional knowl-edge sources.
The framework allows adding morefeatures, but for the present results, only these 11features were used.As our aim is to assist the translation system infinding longer target phrasal matches, the featuresare constructed from the occurrence statistics ofXi,k from the bilingual training corpus (BL).
If acandidate replacement does not occur in the BL,then it is removed from the list of possible replace-ment candidates.Frequency counts for the features of a partic-ular replacement, Xi,k, extracted in the contextof Li,?2Li,?1 (two preceding words of wordi)and Ri,+1Ri,+2 (two following words of wordi)(the remaining words in the left and right contextof wordi are not used for feature extraction) areobtained as follows:f1: frequency of Xi,kRi,+1f2: frequency of Li,?1Xi,kf3: frequency of Li,?1Xi,kRi,+1f4: frequency of Li,?2Li,?1Xi,kf5: frequency of Xi,kRi,+1Ri,+2f6: frequency of Li,?2Li,?1Xi,kRi,+1323f7: frequency of Li,?1Xi,kRi,+1Ri,+2f8: frequency of Li,?2Li,?1Xi,kRi,+1Ri,+2f9: frequency of Xi,k in MLf10: frequency of Xi,k in BLf11: number of feature values (f1, ..f10) > 0f11 is a vote feature which counts the num-ber of features (f1 ... f10) that have a valuegreater than zero.
The features are normalizedto fall within [0, 1].
The sentences in ML, BLand test data are padded with two begin markersand two end markers for obtaining counts forOOV/rare-words that appear at the beginning orend of a test sentence.3.5 RepresentationBefore we go on to explaining the lattice repre-sentation, we would like to make a small clarifica-tion in the terminalogy used.
In the MT commu-nity, a lattice usually refers to the list of possiblepartially-overlapping target translations for eachpossible source n?gram phrase in the input sen-tence.
Since we are using the term lattice to alsorefer to the possible paths through the input sen-tence, we will call the lattice used by the decoder,the ?decoding lattice?.
The lattice obtained fromthe input sentence representing possible replace-ment candidates will be called the ?input lattice?.An input lattice (Figure 1) is constructed witha beam of replacements for the OOV and rarewords.
Each replacement candidate is given ascore (Eqn 1) indicating the confidence that a suit-able replacement is found.
The numbers in Fig-ure 1 indicate the start and end indices (basedon character counts) of the words in the test sen-tence.
In T , two replacements were found for theword ?hawks?
: ?homers?
and ?birds?.
However,?homers?
was not found in the BL and hence, itwas removed from the replacement list.The input lattice also includes the OOV wordwith a low score (Eqn 2).
This allows the EBMTsystem to also include the OOV/rare-word dur-ing decoding.
In the Translation Model of theEBMT system, this test lattice is matched againstthe source sentences in the bilingual training cor-pus.
The matching process would now also lookfor phrases with ?birds?
and not just ?hawks?.When a match is found, the corresponding trans-T?:?????a?mobile?base?,?hitting?three??
hawks?with?one?arrow?.....input?lattice:0?
0?
(???a???)1?
6?
(???mobile???)7?
10?
(???base???)11?
11?
(??,??)12?
18?
(???hitting???)13?
17?
(???three???)18?
22?
(???hawks?????0.0026)18?
22?
(???birds???????0.9974)23?
26?
(???with???)27?
29?
(???one???)30?
34?
(???arrow???)?????????????
?Figure 1: Lattice of the input sentence T contain-ing replacements for OOV words.OOV/Rare word Candidate ReplacementsSpelling errorskrygyzstan kyrgyzstan,...yusukuni yasukuni,..kilomaters kilometers, miles, km, ...Coordinate termssomoa india, turkey, germany, russia, japan,...ear body, arms, hands, feet, mind, car, ...buyers dealer, inspector, the experts, smuggler,.Synonymsplummet drop, dropped, fell, ....Synonyms and Antonymsoptimal worse, better, minimal,....Figure 2: Sample English candidate replacementsobtained.lation in the target language is obtained throughsub-sentential alignment (Section 3.7).
The scoreson the input lattice are later used by the decoder(Section 3.7).
Each replacement Xi,k for theOOV/rare-word (wordi) is scored with a logisticfunction (Bishop, 2006) to convert the dot productof the features and weights (~?
?
~fi,k) to a score be-tween 0 and 1 (Eqn 1 and Eqn 2).p?
(Xi,k|wordi) =exp(~??
~fi,k)1+?j=1...S exp(~??
~fi,j)(1)324p?
(wordi) =11 +?j=1...S exp(~?
?
~fi,j)(2)where, ~fi,j is the feature vector for the jth replace-ment candidate of wordi, S is the number of re-placements, ~?
is the weight vector indicating theimportance of the corresponding features.3.6 Tuning feature weightsWe would like to select those feature weights (~?
)which would lead to the least expected loss intranslation quality (Eqn 3).
?log(BLEU) (Pap-ineni et al, 2002) is used to calculate the expectedloss over a development set.
As this objectivefunction has many local minima and is piecewiseconstant, the surface is smoothed using the L2-norm regularization.
Powell?s algorithm (Powell,1964) with grid-based line optimization is used tofind the best weights.
7 different random guessesare used to initialize the algorithm.min?E?
[L(ttune)] + ?
?
||?||2 (3)The algorithm assumes that partial derivates ofthe function are not available.
Approximations ofthe weights (?1, ..?N ) are generated successivelyalong each of the N standard base vectors.
Theprocedure is iterated with a stopping criteria basedon the amount of change in the weights and thechange in the loss.
A cross-validation set (in ad-dition to the regularization term) is used to pre-vent overfitting at the end of each iteration of thePowell?s algorithm.
This process is repeated withdifferent values of ?
, as in Deterministic Anneal-ing (Rose, 1998).
?
is initialized with a high valueand is halved after each process.3.7 System DescriptionThe EBMT system finds phrasal matches for thetest (or input) sentence from the source side ofthe bilingual corpus.
The corresponding tar-get phrasal translations are obtained through sub-sentential alignment.
When an input lattice isgiven instead of an input sentence, the system per-forms the same matching process for all possiblephrases obtained from the input lattice.
Hence,the system also finds matches for source phrasesthat contain the replacements for the OOV/rare-word.
Only the top C ranking replacement candi-?
???????a????mobile??base???,??hitting???three???hawks????with???????one???????arrow??....?????????????????????
?????birds??
??
?
?
?
??
??
?,??
????
hawks???three?birds????Decoding?Lattice?
?
?????three?birds?with?one?arrow?
?Figure 3: Lattice containing possible phrasal tar-get translations for the test sentence T .dates for every OOV/rare word are used in build-ing the input lattice.
The optimal value of C wasempirically found to be 2.
On examining the ob-tained input lattices, the proposed method foundreplacements for at the most 3 OOV/rare words ineach test sentence (Section 4).
Hence, the numberof possible paths through the input lattice is notsubstantially large.The target translations of all the source phrasesare placed on a common decoding lattice.
Anexample of a decoding lattice for example T isgiven in Figure 3.
The system is now able to findlonger matches (?
three birds with one arrow ?and ?
three birds ?)
which was not possible earlierwith the OOV word, ?hawks?.
The local order-ing information between the translations of ?threebirds?
and ?with one arrow?
is well captured dueto the retrieval of the longer source phrasal match,?three birds with one arrow?.
Our ultimate goalis to obtain translations for such longer n?gramsource phrases boosting the confidence of both thetranslation model and the language model.The decoder used in this paper (Brown, 2003)works on this decoding lattice of possiblephrasal target translations (or fragments) forsource phrases present in the input lattice to gen-erate the target translation.
Similar to Pharaoh(Koehn et al, 2003), the decoder uses multi-level beam search with a priority queue formedbased on the number of source words translated.Bonuses are given for paths that have overlappingfragments.
The total score (TS) for a path (Eqn4) through the translation lattice is the arithmeticaverage of the scores for each target word in the325path.
The EBMT engine assigns each candidatephrasal translation a quality score computed asa log-linear combination of alignment score andtranslation probability.
The alignment score indi-cates the engine?s confidence that the right targettranslation has been chosen for a source phrase.The translation probability is the proportion oftimes each distinct alternative translation was en-countered out of all the translations.
If the pathincludes a candidate replacement, the log of thescore, p?
(wi), given for a candidate replacementis incorporated into TS as an additional term witha weight wt5.TS = 1tt?i=1[wt1 log(bi) + wt2 log(peni)+wt3 log(qi) + wt4 log(P (wi|wi?2, wi?1))+1I(wi=replacement)wt5 log(p?
(wi)) ] (4)where, t is the number of target words in the path,wtj indicates the importance of each score, bi isthe bonus factor given for long phrasal matches,peni is the penalty factor for source and targetphrasal-length mismatches, qi is the quality scoreand P (wi|wi?2, wi?1) is the LM score.
The pa-rameters of the EBMT system (wtj) are tuned ona development set.The target translation is postprocessed to in-clude the translation of the OOV/rare-word withthe help of the best path information from thedecoder.
In the case of OOV words, since thetranslation is not available, the OOV word is putback into the final output translation in place ofthe translation of its replacement.
In the outputtranslation of the test example T , the translationof ?birds?
is replaced by the word, ?hawks?.
Forrare words, knowing that the translation of the rareword may not be correct (due to poor alignmentstatistics), the target translation of the replacementis replaced by the translation of the rare wordobtained from the dictionary.
If the rare wordhas multiple translations, the translation with thehighest score is chosen.4 Experimental SetupAs we are interested in improving the per-formance of low-resource EBMT, the English-Haitian (Eng-Hai) newswire data (Haitian Cre-ole, CMU, 2010) containing 15,136 sentence-pairs was used.
To test the performance in otherlanguages, we simulated sparsity by choosing lesstraining data for English-Chinese (Eng-Chi).
Forthe Eng-Chi experiments, we extracted 30k train-ing sentence pairs from the FBIS (NIST, 2003)corpus.
The data was segmented using the Stan-ford segmenter (Tseng et al, 2005).
Althoughwe are only interested in small data sets, we alsoperformed experiments with a larger data set of200k.
5-gram Language Models were built fromthe target half of the training data with Kneser-Ney smoothing.
For the monolingual English cor-pus, 9 million sentences were collected from theHansard Corpus (LDC, 1997) and FBIS data.EBMT system without OOV/rare-word han-dling is chosen as the Baseline system.
The pa-rameters of the EBMT system are tuned with 200sentence pairs for both Eng-Chi and Eng-Hai.
Thetuned EBMT parameters are used for the Base-line system and the system with OOV/rare-wordhandling.
The feature weights for the proposedmethod are then tuned on a seperate developmentset of 200 sentence-pairs with source sentencescontaining at least 1 OOV/rare-word.
The cross-validation set for this purpose is made up of 100sentence-pairs.
In the OOV case, 500 sentencepairs containing at least 1 OOV word are used fortesting.
For the rare word handling experiments,500 sentence pairs containing at least 1 rare wordare used for testing.To assess the translation quality, 4-gram word-based BLEU is used for Eng-Hai and 3-gramword-based BLEU is used for Eng-Chi.
SinceBLEU scores have a few limitations, the NIST andTER metrics are also used.
The test data used forcomparing the system handling OOV words andthe Baseline (without OOV word handling) is dif-ferent from the test data used for comparing thesystem handling rare words and the Baseline sys-tem (without rare word handling).
In the formercase, the test data handles only OOV words andin the latter, the test data only handles rare words.Hence, the test data for both the cases do not com-pletely overlap.
As we are interested in determin-ing whether handling rare words in test sentencesis useful, we keep both the test data sets seper-ate and assess the improvements obtained by only326OOV/Rare system TER BLEU NISTOOV Baseline 77.89 18.61 4.8525Handling OOV 76.95 19.32 4.9664Rare Baseline 74.23 22.84 5.3803Handling Rare 74.02 23.12 5.4406Table 1: Comparison of translation scores of theBaseline system and system handling OOV andRare words for Eng-Hai.handling OOV words and by only handling rarewords over their corresponding Baselines.
As fu-ture work, it would be interesting to create one testdata set to handle both OOV and rare words to seethe overall gain.The test set is further split into 5 files and theWilcoxon (Wilcoxon, 1945) Signed-Rank test isused to find the statistical significance.5 ResultsSample replacements found are given in Figure 2.For both Eng-Chi and Eng-Hai experiments, onlythe top C ranking replacement candidates wereused.
The value of C was tuned on the develop-ment set and the optimal value was found to be2.
Translation quality scores obtained on the testdata with 30k and 200k Eng-Chi training data setsare given in Table 2.
Table 1 shows the resultsobtained on Eng-Hai.
Statistically significant im-provements (p < 0.0001) were seen by handlingOOV words as well as rare words over their cor-responding baselines.As the goal of the approach was to obtain longertarget phrasal matches, we counted the number ofn-grams for each value of n present on the de-coding lattice in the 30k Eng-Chi case.
The sub-plots: A and B in Figure 4, shows the frequencyof n-grams for higher values of n (for n > 5)when handling OOV and rare words.
The plotsclearly show the increase in number of longer tar-get phrases when compared to the phrases ob-tained by the baseline systems.Since the BLEU and NIST scores were com-puted only up to 3-grams, we further found thenumber of n-gram matches (for n > 3) in thefinal translation of the test data with respect tothe reference translations (subplots: C and D).As expected, a larger number of longer n?grammatches were found.
For the OOV case, matches6 7 8 9 10 11 12 13 14 15020004000600080001000012000n?gram#n?grams on thedecoding latticeBaselineHandling OOV words6 7 8 9 10 11 12 13 14 15050001000015000n?gramBaselineHandling Rare words4 5 6 7 8 9 10 11050100150n?gramBaselineHandling OOV words4 5 6 7 8 9 10 11010203040n?gram#correctly translated n?gramsBaselineHandling Rare wordsA CB DFigure 4: A, B: number of n-grams found for in-creasing values of n on the decoding lattice.
C, D:number of target n-gram matches for increasingvalues of n with respect to the reference transla-tions.OOV/Rare Training system TER BLEU NISTdata sizeOOV 30k Baseline 82.03 14.12 4.118630k Handling OOV 80.97 14.78 4.1798200k Baseline 79.41 19.90 4.6822200k Handling OOV 77.66 20.50 4.7654Rare 30k Baseline 82.09 15.36 4.362630k Handling Rare 80.02 16.03 4.4314200k Baseline 78.04 20.96 4.9647200k Handling Rare 77.35 21.17 5.0122Table 2: Comparison of translation scores of theBaseline system and system handling OOV andRare words for Eng-Chi.up to 9-grams were found where the baseline onlyfound matches up to 8-grams.6 Conclusion and Future WorkA simple approach to improve translation qualityby handling both OOV and rare words was pro-posed.
The framework allowed scoring and rank-ing each replacement candidate efficiently.The method was tested on two language pairsand statistically significant improvements wereseen in both cases.
The results showed that rarewords also need to be handled to see improve-ments in translation quality.In this paper, the proposed method was only ap-plied on words, as future work we would like toextend it to OOV and rare-phrases as well.327ReferencesR.
Barzilay and K. McKeown 2001.
Extracting para-phrases from a parallel corpus.
In Proceedingsof the 39th Annual Meeting of the Association forComputaional Linguistics, pp.
50-57.C.
M. Bishop 2006.
Pattern Recognition and MachineLearning, Springer.R.
D. Brown, R. Hutchinson, P. N. Bennett, J. G. Car-bonell, P. Jansen.
2003.
Reducing Boundary Fric-tion Using Translation-Fragment Overlap.
In Pro-ceedings of The Ninth Machine Translation Summit,pp.
24-31.R.
D. Brown.
2000.
Automated Generalization ofTranslation Examples.
In Proceedings of The Inter-national Conference on Computational Linguistics,pp.
125-131.C.
Callison-Burch, P. Koehn and M. Osborne.
2006.Improved Statistical Machine Translation UsingParaphrases.
In Proceedings of The North Ameri-can Chapter of the Association for ComputationalLinguistics, pp.
17-24.J.
Carbonell, S. Klien, D. Miller, M. Steinbaum, T.Grassiany and J. Frey.
2006.
Context-Based Ma-chine Translation Using Paraphrases.
In Proceed-ings of The Association for Machine Translation inthe Americas, pp.
8-12.N.
Habash.
2008.
Four Techniques for On-line Handling of Out-of-Vocabulary Words inArabic-English Statistical Machine Translation.
InProceedings of Association for ComputationalLinguistics-08: HLT, pp.
57-60.Public release of Haitian Creole lan-guage data by Carnegie Mellon, 2010.http://www.speech.cs.cmu.edu/haitian/Z.
Harris.
1954.
Distributional structure.
Word,10(23): 146-162.P.
Koehn.
2004.
Pharaoh: a Beam Search Decoder forPhrase-Based Statistical Machine Translation Mod-els.
The Association for Machine Translation.P.
Koehn, F. J. Och and D. Marcu.
2003.
Statis-tical Phrase-Based Translation.
In Proceedings ofHLT:The North American Chapter of the Associa-tion for Computational Linguistics.P.
Koehn 2002 Europarl: A multilingual corpusfor evaluation of machine translation.
Unpublished,http://www.isi.edu/koehn/publications/europarl/Linguistic Data Consortium.
1997 Hansard Corpus ofParallel English and French.
Linguistic Data Con-sortium, December.
http://www.ldc.upenn.edu/Y.
Marton, C. Callison-Burch and P. Resnik.
2009.Improved Statistical Machine Translation UsingMonolingually-derived Paraphrases.
In Proceed-ing of The Empirical Methods in Natural LanguageProcessing, pp.
381-390.NIST.
2003.
Machine translation evaluation.http://nist.gov/speech/tests/mt/K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: A Method for Automatic Evaluation of Ma-chine Translation.
In Proceedings of The Associa-tion for Computational Linguistics.
pp.
311-318.M.
Popovic?
and H. Ney.
2004.
Towards the use ofWord Stems and Suffixes for Statistical MachineTranslation.
In Proceedings of The InternationalConference on Language Resources and Evalua-tion.M.
J. D. Powell.
1964.
An efficient method for find-ing the minimum of a function of several variableswithout calculating derivatives Computer Journal.Volume 7, pp.
152-162.K.
Rose.
1998.
Deterministic annealing for clustering,compression, classification, regression, and relatedoptimization problems.
In Proceedings of The In-stitute of Electrical and Electronics Engineers, pp.2210-2239.H.
Tseng, P. Chang, G. Andrew, D. Jurafsky and C.Manning.
2005.
A Conditional Random FieldWord Segmenter.
Fourth SIGHAN Workshop onChinese Language Processing.D.
Vilar, J. Peter, and H. Ney.
2007.
Can we translateletters?
In Proceedings of Association Computa-tional Linguistics Workshop on SMT, pp.
33-39.M.
Yang and K. Kirchhoff.
2006.
Phrase-basedback-off models for machine translation of highlyinflected languages.
In Proceedings of EuropeanChapter of the ACL, 41-48.F.
Wilcoxon.
1945.
Individual comparisons byranking methods.
Biometrics, 1, 80-83. tool:http://faculty.vassar.edu/lowry/wilcoxon.html328
