Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 50?59,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsAn Unsupervised Approach to User Simulation: toward Self-ImprovingDialog SystemsSungjin Lee1,2 and Maxine Eskenazi11Language Technologies Institute, Carnegie Mellon University, Pittsburgh, Pennsylvania2Computer Science and Engineering, Pohang University of Science and Technology, South Korea{sungjin.lee, max}@cs.cmu.edu1, junion@postech.ac.kr2AbstractThis paper proposes an unsupervised ap-proach to user simulation in order to automati-cally furnish updates and assessments of a de-ployed spoken dialog system.
The proposedmethod adopts a dynamic Bayesian networkto infer the unobservable true user action fromwhich the parameters of other components arenaturally derived.
To verify the quality of thesimulation, the proposed method was appliedto the Let?s Go domain (Raux et al, 2005)and a set of measures was used to analyze thesimulated data at several levels.
The resultsshowed a very close correspondence betweenthe real and simulated data, implying that it ispossible to create a realistic user simulator thatdoes not necessitate human intervention.1 IntroductionFor the past decade statistical approaches to dialogmodeling have shown positive results for optimizinga dialog strategy with real data by applying well-understood machine learning methods such as rein-forcement learning (Henderson et al, 2008; Thom-son and Young, 2010; Williams and Young, 2007b).User simulation is becoming an essential componentin developing and evaluating such systems.
In thispaper we describe an unsupervised process to au-tomatically develop user simulators.
The motiva-tion for this comes from the fact that many systemsare presently moving from being simple lab simu-lations to actual deployed systems with real users.These systems furnish a constant flow of new datathat needs to be processed in some way.
Our goal isto minimize human intervention in processing thisdata.
Previously, data had to be hand-annotated, aslow and costly process.
Recently crowdsourcinghas made annotation faster and less expensive, butall of the data still has to be processed and timemust be spent in creating the annotation interfaceand tasks, and in quality control.
Our goal is to pro-cess the metadata (e.g.
user actions, goals, error ty-pology) in an unsupervised manner.
And our methodeliminates the need for human transcription and an-notation by inferring the user goal from groundinginformation.
We also consider user actions as la-tent variables which are inferred based on observa-tions from Automatic Speech Recognition (ASR).We used the above inferred user actions paired withthe observed actions to build an error model.
Sincethe focus of this work is placed on improving andevaluating the dialog strategy, error simulation canbe carried out at the semantic level.
This eliminatesthe need for transcription, which would have neces-sitated an error simulation at the surface level.
Theend result here will be a system that has as little hu-man intervention as possible.This paper is structured as follows.
Section 2 de-scribes previous research and the novelty of our ap-proach.
Section 3 elaborates on our proposed un-supervised approach to user simulation.
Section 4explains the experimental setup.
Section 5 presentsand discusses the results.
Finally, Section 6 con-cludes with a brief summary and suggestions for fu-ture research.2 Related WorkPrevious user simulation studies can be roughly cat-egorized into rule-based methods (Chung, 2005;50Lopez-Cozar et al, 2006; Schatzmann et al, 2007a)and data-driven methods (Cuayahuitl et al, 2005;Eckert et al, 1997; Jung et al, 2009; Levin et al,2000; Georgila et al, 2006; Pietquin, 2004).
Rule-based methods generally allow for more control overtheir designs for the target domain while data-drivenmethods afford more portability from one domain toanother and are attractive for modeling user behav-ior based on real data.
Although development costsfor data-driven methods are typically lower thanthose of rule-based methods, previous data-drivenapproaches have still required a certain amount ofhuman effort.
Most intention-level models take asemantically annotated corpus to produce user in-tention without introducing errors (Cuayahuitl et al,2005; Jung et al, 2009).
Surface-level approachesneed transcribed data to train their surface form anderror generating models (Jung et al, 2009; Schatz-mann et al, 2007b).
A few studies have attempted todirectly simulate the intention, surface, and error byapplying their statistical methods on the recognizeddata rather than on the transcribed data (Georgila etal., 2006; Schatzmann et al, 2005).
Although suchapproaches can avoid human intervention, the soleincorporation of erroneous user action can propa-gate those errors to the higher-level discourse fea-tures which are computed from them, and thus couldresult in less realistic user behavior.
In this work, thetrue user action is treated as a hidden variable and,further, its associated dialog history is also viewed aslatent so that the uncertainty of the true user actionis properly controlled in a principled manner.
Syedand Williams (2008) adopted the Expectation Max-imization algorithm for parameter learning for a la-tent variable model.
But their method still requires asmall amount of transcribed data to learn the obser-vation confusability, and it suffers from overfittingas a general property of maximum likelihood.
Toaddress this problem, we propose a Bayesian learn-ing method, which requires no transcribed data.3 Unsupervised Approach to UserSimulationBefore describing each component in detail, wepresent the overall process of user simulation withan example in the Let?s Go domain in Figure 1.
Tobegin a dialog, the user simulator first sets the userFigure 1: The overall process of user simulation in theLet?s Go domain, where users call the spoken dialog sys-tem to get bus schedule information for Pittsburghgoal by sampling the goal model.
Then the user sim-ulator engages in a conversation with the dialog sys-tem until the termination model ends it.
At eachturn, the termination model randomly determineswhether the dialog will continue or not.
If the dia-log continues, the user model generates user actionsat the predicate level with respect to the given usergoal and system action.
Having the user actions, theerror template model transforms some user actionsinto other actions if necessary and determines whichaction will receive an incorrect value.
After that, theerror value model substantiates the values by draw-ing a confusable value if specified to be incorrect orby using the goal value.
Finally, a confidence scorewill be attached to the user action by sampling theconfidence score model which conditions on the cor-rectness of the final user action.3.1 Goal ModelThe goal model is the first component to be de-fined in terms of the working flow of the user sim-ulator.
In order to generate a plausible user goalin accordance with the frequency at which it ap-pears in a real situation, the dialog logs are parsedto look for the grounding information1 that the usershave provided.
Since the representation of a usergoal in this study is a vector of constraints requiredby a user, for example [Route:61C, Source:CMU,1Specifically, we used explicitly confirmed information bythe system for this study51Destination:AIRPORT, Time:6 PM], each time weencounter grounding information that includes theconstraints used in the backend queries, this is addedto the user goal.
If two actions contradict each other,the later action overwrites the earlier one.
Once allof the user goals in the data have been gathered,a discrete distribution over the user goal is learnedusing a maximum likelihood estimation.
Becausemany variables later in this paper are discrete, a gen-eral notation of a conditional discrete distribution isexpressed as follows:p(xi|xpa(i),?)
=?k,k???(pa(i),k)?(xi,k?)k,k?
(1)where k represents the joint configuration of all theparents of i and ?
(?, ?)
denotes Kronecker delta.
Notethat ?k?
?k,k?
= 1.
Given this notation, the goalmodel ?
can be written in the following form:g ?
p(g|?)
=?k??
(g,k)k (2)3.2 User ModelHaving generated a user goal, the next task is to inferan appropriate user action for the given goal and sys-tem action.
This is what the user model does.
Sinceone of key properties of our unsupervised approachis that the true user actions are not observable, theuser model should maintain a belief over the dia-log state by taking into consideration the observeduser actions.
Inspired by (Williams et al, 2005),to keep the complexity of the user model tractable,a dynamic Bayesian network is adopted with sev-eral conditional independence assumptions, givingrise to the graphical structure which is shown in Fig-ure 2.
Unlike belief tracking in a dialog system, theuser goal in a user simulation is pre-determined be-fore the beginning of the dialog.
As with most pre-vious studies, this property allows the user modelto deal with a predicate-level action consisting of aspeech act and a concept (e.g.
[Inform(Source), In-form(Time)]) and is only concerned about whether agiven field is specified or not in the user goal (e.g.Bus:Unspecified, Source:Specified).
This abstract-level handling enables the user model to employ ex-act inference algorithms such as the junction treealgorithm (Lauritzen and Spiegelhalter, 1988) formore efficient reasoning over the graphical structure.Figure 2: The graphical structure of the dynamicBayesian network for the user model.
g denotes the usergoal and st,ut,ht,ot represents the system action, theuser action, the dialog history, and the observed user ac-tion for each time slice, respectively.
The shaded itemsare observable and the transparent ones are latent.The joint distribution for this model is given byp(g,S,H,U,O|?
)= p(h0|pi)?tp(ut|g, st,ht?1,?)?
p(ht|ht?1,ut,?
)p(ot|ut, ?
)(3)where a capital letter stands for the set ofcorresponding random variables, e.g., U ={u1, .
.
.
,uN}, and ?
= {pi,?,?, ?}
denotes theset of parameters governing the model2.For a given user goal, the user model basicallyperforms an inference to obtain a marginal distribu-tion over ut for each time step from which it cansample the probability of a user action in a givencontext:ut ?
p(ut|g, st1,ut?11 ,?)
(4)where st1 denotes the set of system actions from time1 to time t and ut?11 is the set of previously sampleduser actions from time 1 to time t?
1.3.2.1 Parameter EstimationAs far as parameters are concerned, ?
is a determin-istic function that yields a fraction of an observedconfidence score in accordance with the degree ofagreement between ut and ot:p(ot|ut) = CS(ot) ?
( |ot ?
ut||ot ?
ut|)p+  (5)2Here, uniform prior distributions are assigned on g and S52where CS(?)
returns the confidence score of the as-sociated observation and p is a control variable overthe strength of disagreement penalty3.
In addition, piand ?
are deterministically set by simple discourserules, for example:p(ht = Informed|ht?1,ut) ={1 if ht?1 = Informed or ut = Inform(?
),0 otherwise.
(6)The only parameter that needs to be learned in theuser model, therefore, is ?
and it can be estimatedby maximizing the likelihood function (Equation 7).The likelihood function is obtained from the jointdistribution (Equation 3) by marginalizing over thelatent variables.p(g,S,O|?)
=?H,Up(g,S,H,U,O|?)
(7)Since direct maximization of the likelihood func-tion will lead to complex expressions with noclosed-form solutions due to the latent variables, theExpectation-Maximization (EM) algorithm is an ef-ficient framework for finding maximum likelihoodestimates.As it is well acknowledged, however, that over-fitting can arise as a general property of maximumlikelihood, especially when only a small amount ofdata is available, a Bayesian approach needs to beadopted.
In a Bayesian model, any unknown pa-rameter is given a prior distribution and is absorbedinto the set of latent variables, thus it is infeasibleto directly evaluate the posterior distribution of thelatent variables and the expectations with respect tothis distribution.
Therefore a deterministic approx-imation, called mean field theory (Parisi, 1988), isapplied.In mean field theory, the family of posterior distri-butions of the latent variables is assumed to be par-titioned into disjoint groups:q(Z) =M?i=1qi(Zi) (8)where Z = {z1, .
.
.
, zN} denotes all latent variablesincluding parameters and Zi is a disjoint group.3For this study, p was set to 1.0Amongst all distributions q(Z) having the form ofEquation 8, we then seek the member of this familyfor which the divergence from the true posterior dis-tribution is minimized.
To achieve this, the follow-ing optimization with respect to each of the qi(Zi)factors is to be performed in turn (Bishop, 2006):ln q?j (Zj) = Ei 6=j[ln p(X,Z)]+ const (9)where X = {x1, .
.
.
,xN} denotes all observed vari-ables and Ei 6=j means an expectation with respect tothe q distributions over all groups Zi for i 6= j.Now, we apply the mean field theory to the usermodel.
Before doing so, we need to introduce theprior over the parameter ?
which is a product ofDirichlet distributions4.p(?)
=?kDir(?k|?0k)=?kC(?0k)?l?
?0k?1k,l(10)where k represents the joint configuration of all ofthe parents and C(?0k) is the normalization constantfor the Dirichlet distribution.
Note that for symme-try we have chosen the same parameter ?0k for eachof the components.Next we approximate the posterior distribution,q(H,U,?)
using a factorized form, q(H,U)q(?
).Then we first apply Equation 9 to find an expressionfor the optimal factor q?(?
):ln q?(?)
= EH,U[ln p(g,S,H,U,O,?
)]+ const= EH,U[?tln p(ut|g, st,ht?1,?
)]+ ln p(?)
+ const=?t?i,j,k,l(EH,U[?i,j,k,l]ln?i,j,k,l)+?i,j,k,l(?oi,j,k,l ?
1) ln?i,j,k,l + const=?i,j,k,l((EH,U[ni,j,k,l] + (?oi,j,k,l ?
1))?
ln?i,j,k,l)+ const(11)4Note that priors over parameters for deterministic distribu-tions (e.i., pi,?,and ?)
are not necessary.53where ?i,j,k,l denotes ?
(g, i)?
(st, j)?
(ht?1, k)?
(ut, l) and ni,j,k,l is the number of times whereg = i, st = j,ht?1 = k, and ut = l. This leadsto a product of Dirichlet distributions by taking theexponential of both sides of the equation:q?(?)
=?i,j,kDir(?i,j,k|?i,j,k),?i,j,k,l = ?0i,j,k,l + EH,U[ni,j,k,l](12)To evaluate the quantity EH,U[ni,j,k,l], Equation 9needs to be applied once again to obtain an op-timal approximation of the posterior distributionq?
(H,U).ln q?
(H,U) = E?
[ln p(g,S,H,U,O,?
)]+ const= E?
[?tln p(ut|g, st,ht?1,?
)+ ln p(ht|ht?1,ut)+ ln p(ot|ut)]+ const=?t(E?
[ln p(ut|g, st,ht?1,?
)]+ ln p(ht|ht?1,ut)+ ln p(ot|ut))+ const(13)where E?
[ln p(ut|g, st,ht?1,?)]
can be obtainedusing Equation 12 and properties of the Dirichletdistribution:E?
[ln p(ut|g, st,ht?1,?)]=?i,j,k,l?i,j,k,lE?[ln?i,j,k,l]=?i,j,k,l?i,j,k,l(?(?i,j,k,l)?
?(?
?i,j,k))(14)where ?(?)
is the digamma function with ?
?i,j,k =?l ?i,j,k,l.
Because computing EH,U[ni,j,k,l] isequivalent to summing each of the marginal poste-rior probabilities q?
(ht?1,ut) with the same con-figuration of conditioning variables, this can bedone efficiently by using the junction tree algorithm.Note that the expression on the right-hand side forboth q?(?)
and q?
(H,U) depends on expectationscomputed with respect to the other factors.
Wewill therefore seek a consistent solution by cyclingthrough the factors and replacing each in turn with arevised estimate.3.3 Error ModelThe purpose of the error model is to alter the useraction to reflect the prevalent speech recognition andunderstanding errors.
The error generation processconsists of three steps: the error model first gen-erates an error template then fills it with erroneousvalues, and finally attaches a confidence score.Given a user action, the error model maps it into adistorted form according to the probability distribu-tion of the error template model ?
:T (u) ?
p(T (u)|u) =?k,k???(u,k)?
(T (u),k?)k,k?
(15)where T (?)
is a random function that maps a pred-icate of the user action to an error template, e.g.T (Inform(Time)) ?
Inform(Route:incorrect).
Tolearn the parameters, the hidden variable ut is sam-pled using Equation 4 for each observation ot in thetraining data and the value part of each observationis replaced with a binary value representing its cor-rectness with respect to the user goal.
This results ina set of complete data on which the maximum like-lihood estimates of ?
are learned.With the error template provided, next, the errormodel fills it with incorrect values if necessary fol-lowing the distribution of the error value model ?which is separately defined for each concept, other-wise it will keep the correct value:C(v) ?
p(C(v)|v) =?k,k???(v,k)?(C(v),k?)
(16)where C(?)
is a random function which maps a cor-rect value to a confusable value, e.g.
C(Forbes) ?Forward.
As with the error template model, the pa-rameters of the error value model are also easilytrained on the dataset of all pairs of a user goal valueand the associated observed value.
Because no er-ror values can be observed for a given goal value, anunconditional probability distribution is also trainedas a backoff.Finally, the error model assigns a confidencescore by sampling the confidence score model ?54which is separately defined for each concept:s ?
p(s|c) =?k,k???(c,k)?(s,k?)
(17)where s denotes the confidence score and c repre-sents the correctness of the value of the user actionwhich is previously determined by the error tem-plate model.
Since two decimal places are used todescribe the confidence score, the confidence scoremodel is represented with a discrete distribution.This lends itself to trivial parameter learning similarto other models by computing maximum likelihoodestimates on the set of observed confidence scoresconditioned on the correctness of the relevant val-ues.In sum, for example, having a user action[Inform(Source:Forbes), Inform(Time:6 PM)] gothrough the sequence of aforementioned modelspossibly leads to [Inform(Source:Forward), In-form(Route:6C)].3.4 Termination ModelFew studies have been conducted to estimate theprobability that a dialog will terminate at a certainturn in the user simulation.
Most existing workattempts to treat a termination initiated by a useras one of the dialog actions in their user models.These models usually have a limited dialog historythat they can use to determine the next user action.This Markov assumption is well-suited to ordinarydialog actions, each generally showing a correspon-dence with previous dialog actions.
It is not diffi-cult, however, to see that more global contexts (e.g.,cumulative number of incorrect confirmations) willhelp lead a user to terminate a failed dialog.
In ad-dition, the termination action occurs only once atthe end of a dialog unlike the other actions.
Thus,we do not need to put the termination action intothe user model.
In order to easily incorporate manyglobal features involving an entire dialog (Table 1)into the termination model, the logistic regressionmodel is adapted.
At every turn, before getting intothe user model, we randomly determine whether adialog will stop according to the posterior probabil-ity of the termination model given the current dialogcontext.Feature DescriptionNT Number of turnsRIC Ratio of incorrect confirmationsRICW Ratio of incorrect confirmationswithin a windowRNONU Ratio of non-understandingRNONUW Ratio of non-understandingwithin a windowACS Averaged confidence scoreACSW Averaged confidence scorewithin a windowRCOP Ratio of cooperative turnsRCOPW Ratio of cooperative turnswithin a windowRRT C Ratio of relevant system turnsfor each conceptRRTW C Ratio of relevant system turnsfor each concept within a windowNV C Number of values appeared foreach conceptTable 1: A description of features used for a logisticregression model to capture the termination probability.The window size was set to 5 for this study.4 Experimental Setup4.1 DataTo verify the proposed method, three months of datafrom the Let?s Go domain were split into two monthsof training data and one month of test data.
Also,to take the error level into consideration, we classi-fied the data into four groups according to the aver-aged confidence score and used each group of datato build a different error model for each error level.For comparison purposes, simulated data was gen-erated for both training and test data by feeding thesame context of each piece of data to the proposedmethod.
Due to the characteristics of the bus sched-ule information domain, there are a number of caseswhere no bus schedule is available, such as requestsfor uncovered routes and places.
Such cases wereexcluded for clearer interpretation of the result, giv-ing us the data sets described in Table 2.4.2 MeasuresTo date, a variety of evaluation methods have beenproposed in the literature (Cuayahuitl et al, 2005;Jung et al, 2009; Georgila et al, 2006; Pietquin and55Training data Test dataNumber of dialogs 1,275 669Number of turns 9,645 5,103Table 2: A description of experimental data sets.Hastie, 2011; Schatzmann et al, 2005; Williams,2007a).
Nevertheless, it remains difficult to finda suitable set of evaluation measures to assess thequality of the user simulation.
We have chosento adopt a set of the most commonly used mea-sures.
Firstly, expected precision (EP), expected re-call (ER) and F-Score offer a reliable method forcomparing real and simulated data even though itis not possible to specify the levels that need to besatisfied to conclude that the simulation is realistic.These are computed by comparison of the simulatedand real user action for each turn in the corpus:EP = 100 ?
Number of identical actionsNumber of simulated actions (18)ER = 100 ?
Number of identical actionsNumber of real actions (19)F-Score = 100 ?
2 ?
EP ?
EREP + ER (20)Next, several descriptive statistics are employed toshow the closeness of the real and simulated datain a statistical sense.
The distribution of differentuser action types, turn length and confidence scorecan show constitutional similarity.
It is still possible,however, to be greatly different in their interdepen-dence and cause quite different behavior at the dia-log level even though there is a constitutional sim-ilarity.
Therefore, the dialog-level statistics such asdialog completion rate and averaged dialog lengthwere also computed by running the user simulatorwith the Let?s Go dialog system.5 ResultsAs mentioned in Section 4.2, expected precision andrecall were measured.
Whereas previous studiesonly reported the scores computed in the predicatelevel, i.e.
speech act and concept, we also measuredthe scores based on the output of the error templatemodel which is the predicate-level action with anindicator of the correctness of the associated value(Figure 1).
The result (Table 3) shows a moderateTraining data Test dataError Mark w/o w/ w/o w/EP 58.13 45.12 54.44 41.86ER 58.40 45.33 54.61 41.99F-Score 58.27 45.22 54.52 41.93Table 3: Expected precision, expected recall and F-Scorebalance between agreement and variation which isa very desirable characteristic of a user simulatorsince a simulated user is expected not only to resem-ble real data but also to cover diverse unseen behav-ior to a reasonable extent.
As a natural consequenceof the increased degree of freedom, the scores con-sidering error marking are consistently lower.
In ad-dition, the results of test data are slightly lower thanthose of training data, as expected, yet a suitable bal-ance remains.Next, the comparative distributions of differentactions between real and simulated data are pre-sented for both training and test data (Figure 3).The results are also based on the output of the er-ror template model to further show how errors aredistributed over different actions.
The distributionsof simulated data either from training or test datashow a close match to the corresponding real dis-tributions.
Interestingly, even though the error ratioof the test data is noticeably different from that ofthe training data, the proposed method is still ableto generate similar results.
This means the vari-ables and their conditional probabilities of the pro-posed method were designed and estimated properlyenough to capture the tendency of user behavior withrespect to various dialog contexts.
Moreover, thecomparison of the turn length distribution (Figure 4)indicates that the simulated data successfully repli-cated the real data for both training and test data.The results of confidence score simulation are pre-sented in Figure 55.
For both training and test data,the simulated confidence score displays forms thatare very similar to the real ones.Finally, to confirm the resemblance on the dialoglevel, the comparative results of dialog completionrate and averaged dialog length are summarized inTable 4.
As shown in the dialog completion result,the simulated user is a little harder than the real user5Due to the space limitation, the detailed illustrations foreach action type are put in Appendix A.56Figure 3: A comparison of the distribution of differentactions between real and simulated data for both trainingand test dataFigure 4: A comparison of the distribution of turn lengthbetween real and simulated data for both training and testdatato accomplish the purpose.
Also, the variation of thesimulated data as far as turn length is concerned wasgreater than that of the real data, although the aver-aged lengths were similar to each other.
This mightindicate the need to improve the termination model.The proposed method for the termination model isconfined to incorporating only semantic-level fea-tures but a variety of different features would, ofcourse, cause the end of a dialog, e.g.
system de-lay, acoustic features, spatial and temporal context,weather and user groups.6 ConclusionIn this paper, we presented a novel unsupervised ap-proach for user simulation which is especially de-sirable for real deployed systems.
The proposedFigure 5: A comparison of the distribution of confidencescore between real and simulated data for both trainingand test dataReal SimulatedDCR (%) 59.68 55.04ADL mean std.
mean std.Success 10.62 4.59 11.08 5.10Fail 7.75 6.20 7.75 8.64Total 9.46 5.48 9.50 7.12Table 4: A comparison of dialog completion rate (DCR)and averaged dialog length (ADL) which is presented ac-cording to the dialog result.method can cover the whole pipeline of user sim-ulation on the semantic level without human inter-vention.
Also the quality of simulated data has beendemonstrated to be similar to the real data over anumber of commonly employed metrics.
Althoughthe proposed method does not deal with simulat-ing N-best ASR results, the extension to supportN-best results will be one of our future efforts, assoon as the Let?s Go system uses N-best results.Our future work also includes evaluation on improv-ing and evaluating dialog strategies.
Furthermore, itwould be scientifically more interesting to comparethe proposed method with a supervised approach us-ing a corpus with semantic transcriptions.
On theother hand, as an interesting application, the pro-posed user model could be exploited as a part of be-lief tracking in a spoken dialog system since it alsoconsiders a user action to be hidden.57AcknowledgmentsWe would like to thank Alan Black for helpful com-ments and discussion.
This work was supported bythe second Brain Korea 21 project.ReferencesC.
Bishop, 2006.
Pattern Recognition and MachineLearning.
Springer.G.
Chung, 2004.
Developing a Flexible Spoken DialogSystem Using Simulation.
In Proceedings of ACL.H.
Cuayahuitl, S. Renals, O.
Lemon, H. Shimodaira,2005.
Humancomputer dialogue simulation using hid-den Markov models.
In Proceedings of ASRU.W.
Eckert, E. Levin, R. Pieraccini, 1997.
User modelingfor spoken dialogue system evaluation.
In Proceed-ings of ASRU.K.
Georgila, J. Henderson, O.
Lemon, 2006.
User simu-lation for spoken dialogue systems: Learning and eval-uation.
In Proceedings of Interspeech.J.
Henderson, O.
Lemon, K. Georgila, 2008.
Hybrid Re-inforcement / Supervised Learning of Dialogue Poli-cies from Fixed Datasets.
Computational Linguistics,34(4):487-511S.
Jung, C. Lee, K. Kim, M. Jeong, G. Lee, 2009.Data-driven user simulation for automated evaluationof spoken dialog systems.
Computer Speech and Lan-guage, 23(4):479?509.S.
Lauritzen and D. J. Spiegelhalter, 1988.
Local Com-putation and Probabilities on Graphical Structures andtheir Applications to Expert Systems.
Journal ofRoyal Statistical Society, 50(2):157?224.E.
Levin, R. Pieraccini, W. Eckert, 2000.
A stochasticmodel of humanmachine interaction for learning di-alogstrategies.
IEEE Transactions on Speech and Au-dio Processing, 8(1):11-23.R.
Lopez-Cozar, Z. Callejas, and M. McTear, 2006.
Test-ing the performance of spoken dialogue systems bymeans of an articially simulated user.
Articial Intel-ligence Review, 26(4):291-323.G.
Parisi, 1988.
Statistical Field Theory.
Addison-Wesley.O.
Pietquin, 2004.
A Framework for UnsupervisedLearning of Dialogue Strategies.
Ph.D. thesis, Facultyof Engineering.O.
Pietquin and H. Hastie, 2011.
A survey on metricsfor the evaluation of user simulations.
The KnowledgeEngineering Review.A.
Raux, B. Langner, D. Bohus, A. W Black, and M.Eskenazi, 2005.
Let?s Go Public!
Taking a SpokenDialog System to the Real World.
In Proceedings ofInterspeech.J.
Schatzmann, K. Georgila, S. Young, 2005.
Quantita-tive evaluation of user simulation techniques for spo-ken dialogue systems.
In Proceedings of SIGdial.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye, S.Young, 2007.
Agenda-based user simulation for boot-strapping a POMDP dialogue system.
In Proceedingsof HLT/NAACL.J.
Schatzmann, B. Thomson, S. Young, 2007.
Errorsimulation for training statistical dialogue systems.
InProceedings of ASRU.U.
Syed and J. Williams, 2008.
Using automaticallytranscribed dialogs to learn user models in a spokendialog system.
In Proceedings of ACL.B.
Thomson and S. Young, 2010.
Bayesian updateof dialogue state: A POMDP framework for spokendialogue systems.
Computer Speech & Language,24(4):562-588.J.
Williams, P. Poupart, and S. Young, 2005.
FactoredPartially Observable Markov Decision Processes forDialogue Management.
In Proceedings of Knowledgeand Reasoning in Practical Dialogue Systems.J.
Williams, 2007.
A Method for Evaluating and Com-paring User Simulations: The Cramer-von Mises Di-vergence.
In Proceedings of ASRU.J.
Williams and S. Young, 2007.
Partially observableMarkov decision processes for spoken dialog systems.Computer Speech & Language, 21(2):393-422.58AppendicesAppendix A.
Distribution of confidence score for each conceptFigure 6: A comparison of the distribution of confidence score between real and simulated data for the training dataFigure 7: A comparison of the distribution of confidence score between real and simulated data for the test data59
