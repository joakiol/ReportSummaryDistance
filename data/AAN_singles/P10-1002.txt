Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 12?20,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsDependency Parsing and Projection Based on Word-Pair ClassificationWenbin Jiang and Qun LiuKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{jiangwenbin, liuqun}@ict.ac.cnAbstractIn this paper we describe an intuitionisticmethod for dependency parsing, where aclassifier is used to determine whether apair of words forms a dependency edge.And we also propose an effective strategyfor dependency projection, where the de-pendency relationships of the word pairsin the source language are projected to theword pairs of the target language, leadingto a set of classification instances ratherthan a complete tree.
Experiments showthat, the classifier trained on the projectedclassification instances significantly out-performs previous projected dependencyparsers.
More importantly, when this clas-sifier is integrated into a maximum span-ning tree (MST) dependency parser, ob-vious improvement is obtained over theMST baseline.1 IntroductionSupervised dependency parsing achieves the state-of-the-art in recent years (McDonald et al, 2005a;McDonald and Pereira, 2006; Nivre et al, 2006).Since it is costly and difficult to build human-annotated treebanks, a lot of works have also beendevoted to the utilization of unannotated text.
Forexample, the unsupervised dependency parsing(Klein and Manning, 2004) which is totally basedon unannotated data, and the semisupervised de-pendency parsing (Koo et al, 2008) which isbased on both annotated and unannotated data.Considering the higher complexity and lower per-formance in unsupervised parsing, and the need ofreliable priori knowledge in semisupervised pars-ing, it is a promising strategy to project the de-pendency structures from a resource-rich languageto a resource-scarce one across a bilingual corpus(Hwa et al, 2002; Hwa et al, 2005; Ganchev et al,2009; Smith and Eisner, 2009; Jiang et al, 2009).For dependency projection, the relationship be-tween words in the parsed sentences can be sim-ply projected across the word alignment to wordsin the unparsed sentences, according to the DCAassumption (Hwa et al, 2005).
Such a projec-tion procedure suffers much from the word align-ment errors and syntactic isomerism between lan-guages, which usually lead to relationship projec-tion conflict and incomplete projected dependencystructures.
To tackle this problem, Hwa et al(2005) use some filtering rules to reduce noise,and some hand-designed rules to handle languageheterogeneity.
Smith and Eisner (2009) performdependency projection and annotation adaptationwith quasi-synchronous grammar features.
Jiangand Liu (2009) resort to a dynamic programmingprocedure to search for a completed projected tree.However, these strategies are all confined to thesame category that dependency projection mustproduce completed projected trees.
Because of thefree translation, the syntactic isomerism betweenlanguages and word alignment errors, it wouldbe strained to completely project the dependencystructure from one language to another.We propose an effective method for depen-dency projection, which does not have to pro-duce complete projected trees.
Given a word-aligned bilingual corpus with source language sen-tences parsed, the dependency relationships of theword pairs in the source language are projected tothe word pairs of the target language.
A depen-dency relationship is a boolean value that repre-sents whether this word pair forms a dependencyedge.
Thus a set of classification instances are ob-tained.
Meanwhile, we propose an intuitionisticmodel for dependency parsing, which uses a clas-sifier to determine whether a pair of words forma dependency edge.
The classifier can then betrained on the projected classification instance set,so as to build a projected dependency parser with-out the need of complete projected trees.12ij jiFigure 1: Illegal (a) and incomplete (b) dependency tree produced by the simple-collection method.Experimental results show that, the classifiertrained on the projected classification instancessignificantly outperforms the projected depen-dency parsers in previous works.
The classifiertrained on the Chinese projected classification in-stances achieves a precision of 58.59% on the CTBstandard test set.
More importantly, when thisclassifier is integrated into a 2nd-ordered max-imum spanning tree (MST) dependency parser(McDonald and Pereira, 2006) in a weighted aver-age manner, significant improvement is obtainedover the MST baselines.
For the 2nd-order MSTparser trained on Penn Chinese Treebank (CTB)5.0, the classifier give an precision increment of0.5 points.
Especially for the parser trained on thesmaller CTB 1.0, more than 1 points precision in-crement is obtained.In the rest of this paper, we first describethe word-pair classification model for dependencyparsing (section 2) and the generation methodof projected classification instances (section 3).Then we describe an application of the projectedparser: boosting a state-of-the-art 2nd-orderedMST parser (section 4).
After the comparisonswith previous works on dependency parsing andprojection, we finally five the experimental results.2 Word-Pair Classification Model2.1 Model DefinitionFollowing (McDonald et al, 2005a), x is used todenote the sentence to be parsed, and xi to denotethe i-th word in the sentence.
y denotes the de-pendency tree for sentence x, and (i, j) ?
y rep-resents a dependency edge from word xi to wordxj , where xi is the parent of xj .The task of the word-pair classification modelis to determine whether any candidate word pair,xi and xj s.t.
1 ?
i, j ?
|x| and i 6= j, forms adependency edge.
The classification result C(i, j)can be a boolean value:C(i, j) = p p ?
{0, 1} (1)as produced by a support vector machine (SVM)classifier (Vapnik, 1998).
p = 1 indicates that theclassifier supports the candidate edge (i, j), andp = 0 the contrary.
C(i, j) can also be a real-valued probability:C(i, j) = p 0 ?
p ?
1 (2)as produced by an maximum entropy (ME) classi-fier (Berger et al, 1996).
p is a probability whichindicates the degree the classifier support the can-didate edge (i, j).
Ideally, given the classifica-tion results for all candidate word pairs, the depen-dency parse tree can be composed of the candidateedges with higher score (1 for the boolean-valuedclassifier, and large p for the real-valued classi-fier).
However, more robust strategies should beinvestigated since the ambiguity of the languagesyntax and the classification errors usually lead toillegal or incomplete parsing result, as shown inFigure 1.Follow the edge based factorization method(Eisner, 1996), we factorize the score of a de-pendency tree s(x,y) into its dependency edges,and design a dynamic programming algorithmto search for the candidate parse with maximumscore.
This strategy alleviate the classification er-rors to some degree and ensure a valid, completedependency parsing tree.
If a boolean-valued clas-sifier is used, the search algorithm can be formal-ized as:y?
= argmaxys(x,y)= argmaxy?
(i,j)?yC(i, j) (3)And if a probability-valued classifier is used in-stead, we replace the accumulation with cumula-13Type FeaturesUnigram wordi ?
posi wordi posiwordj ?
posj wordj posjBigram wordi ?
posi ?
wordj ?
posj posi ?
wordj ?
posj wordi ?
wordj ?
posjwordi ?
posi ?
posj wordi ?
posi ?
wordj wordi ?
wordjposi ?
posj wordi ?
posj posi ?
wordjSurrounding posi ?
posi+1 ?
posj?1 ?
posj posi?1 ?
posi ?
posj?1 ?
posj posi ?
posi+1 ?
posj ?
posj+1posi?1 ?
posi ?
posj ?
posj+1 posi?1 ?
posi ?
posj?1 posi?1 ?
posi ?
posj+1posi ?
posi+1 ?
posj?1 posi ?
posi+1 ?
posj+1 posi?1 ?
posj?1 ?
posjposi?1 ?
posj ?
posj+1 posi+1 ?
posj?1 ?
posj posi+1 ?
posj ?
posj+1posi ?
posj?1 ?
posj posi ?
posj ?
posj+1 posi?1 ?
posi ?
posjposi ?
posi+1 ?
posjTable 1: Feature templates for the word-pair classification model.tive product:y?
= argmaxys(x,y)= argmaxy?
(i,j)?yC(i, j) (4)Where y is searched from the set of well-formeddependency trees.In our work we choose a real-valued ME clas-sifier.
Here we give the calculation of dependencyprobability C(i, j).
We use w to denote the param-eter vector of the ME model, and f(i, j, r) to de-note the feature vector for the assumption that theword pair i and j has a dependency relationship r.The symbol r indicates the supposed classificationresult, where r = + means we suppose it as a de-pendency edge and r = ?
means the contrary.
Afeature fk(i, j, r) ?
f(i, j, r) equals 1 if it is ac-tivated by the assumption and equals 0 otherwise.The dependency probability can then be definedas:C(i, j) = exp(w ?
f(i, j,+))?r exp(w ?
f(i, j, r))=exp(?k wk ?
fk(i, j,+))?r exp(?k wk ?
fk(i, j, r))(5)2.2 Features for ClassificationThe feature templates for the classifier are simi-lar to those of 1st-ordered MST model (McDon-ald et al, 2005a).
1 Each feature is composedof some words and POS tags surrounded word iand/or word j, as well as an optional distance rep-resentations between this two words.
Table showsthe feature templates we use.Previous graph-based dependency models usu-ally use the index distance of word i and word j1We exclude the in between features of McDonald et al(2005a) since preliminary experiments show that these fea-tures bring no improvement to the word-pair classificationmodel.to enrich the features with word distance infor-mation.
However, in order to utilize some syntaxinformation between the pair of words, we adoptthe syntactic distance representation of (Collins,1996), named Collins distance for convenience.
ACollins distance comprises the answers of 6 ques-tions:?
Does word i precede or follow word j??
Are word i and word j adjacent??
Is there a verb between word i and word j??
Are there 0, 1, 2 or more than 2 commas be-tween word i and word j??
Is there a comma immediately following thefirst of word i and word j??
Is there a comma immediately preceding thesecond of word i and word j?Besides the original features generated accordingto the templates in Table 1, the enhanced featureswith Collins distance as postfixes are also used intraining and decoding of the word-pair classifier.2.3 Parsing AlgorithmWe adopt logarithmic dependency probabilitiesin decoding, therefore the cumulative product ofprobabilities in formula 6 can be replaced by ac-cumulation of logarithmic probabilities:y?
= argmaxys(x,y)= argmaxy?
(i,j)?yC(i, j)= argmaxy?
(i,j)?ylog(C(i, j))(6)Thus, the decoding algorithm for 1st-ordered MSTmodel, such as the Chu-Liu-Edmonds algorithm14Algorithm 1 Dependency Parsing Algorithm.1: Input: sentence x to be parsed2: for ?i, j?
?
?1, |x|?
in topological order do3: buf ?
?4: for k ?
i..j ?
1 do ?
all partitions5: for l ?
V[i, k] and r ?
V[k + 1, j] do6: insert DERIV(l, r) into buf7: insert DERIV(r, l) into buf8: V[i, j]?
top K derivations of buf9: Output: the best derivation of V[1, |x|]10: function DERIV(p, c)11: d?
p ?
c ?
{(p ?
root, c ?
root)} ?
new derivation12: d ?
evl?
EVAL(d) ?
evaluation function13: return dused in McDonald et al (2005b), is also appli-cable here.
In this work, however, we still adoptthe more general, bottom-up dynamic program-ming algorithm Algorithm 1 in order to facilitatethe possible expansions.
Here, V[i, j] contains thecandidate parsing segments of the span [i, j], andthe function EVAL(d) accumulates the scores ofall the edges in dependency segment d. In prac-tice, the cube-pruning strategy (Huang and Chi-ang, 2005) is used to speed up the enumeration ofderivations (loops started by line 4 and 5).3 Projected Classification InstanceAfter the introduction of the word-pair classifica-tion model, we now describe the extraction of pro-jected dependency instances.
In order to allevi-ate the effect of word alignment errors, we basethe projection on the alignment matrix, a compactrepresentation of multiple GIZA++ (Och and Ney,2000) results, rather than a single word alignmentin previous dependency projection works.
Figure2 shows an example.Suppose a bilingual sentence pair, composed ofa source sentence e and its target translation f .
yeis the parse tree of the source sentence.
A is thealignment matrix between them, and each elementAi,j denotes the degree of the alignment betweenword ei and word fj .
We define a boolean-valuedfunction ?
(y, i, j, r) to investigate the dependencyrelationship of word i and word j in parse tree y:?
(y, i, j, r) =??????????
?1(i, j) ?
y and r = +or(i, j) /?
y and r = ?0 otherwise(7)Then the score that word i and word j in the targetsentence y forms a projected dependency edge,Figure 2: The word alignment matrix between aChinese sentence and its English translation.
Notethat probabilities need not to be normalized acrossrows or columns.s+(i, j), can be defined as:s+(i, j) =?i?,j?Ai,i?
?
Aj,j?
?
?
(ye, i?, j?,+) (8)The score that they do not form a projected depen-dency edge can be defined similarly:s?
(i, j) =?i?,j?Ai,i?
?
Aj,j?
?
?
(ye, i?, j?,?)
(9)Note that for simplicity, the condition factors yeand A are omitted from these two formulas.
Wefinally define the probability of the supposed pro-jected dependency edge as:Cp(i, j) =exp(s+(i, j))exp(s+(i, j)) + exp(s?
(i, j))(10)The probability Cp(i, j) is a real value between0 and 1.
Obviously, Cp(i, j) = 0.5 indicates themost ambiguous case, where we can not distin-guish between positive and negative at all.
On theother hand, there are as many as 2|f |(|f |?1) candi-date projected dependency instances for the targetsentence f .
Therefore, we need choose a thresholdb for Cp(i, j) to filter out the ambiguous instances:the instances with Cp(i, j) > b are selected as thepositive, and the instances with Cp(i, j) < 1 ?
bare selected as the negative.4 Boosting an MST ParserThe classifier can be used to boost a existing parsertrained on human-annotated trees.
We first estab-lish a unified framework for the enhanced parser.For a sentence to be parsed, x, the enhanced parserselects the best parse y?
according to both the base-line model B and the projected classifier C.y?
= argmaxy[sB(x,y) + ?sC(x,y)] (11)15Here, sB and sC denote the evaluation functionsof the baseline model and the projected classi-fier, respectively.
The parameter ?
is the relativeweight of the projected classifier against the base-line model.There are several strategies to integrate the twoevaluation functions.
For example, they can be in-tegrated deeply at each decoding step (Carreras etal., 2008; Zhang and Clark, 2008; Huang, 2008),or can be integrated shallowly in a reranking man-ner (Collins, 2000; Charniak and Johnson, 2005).As described previously, the score of a depen-dency tree given by a word-pair classifier can befactored into each candidate dependency edge inthis tree.
Therefore, the projected classifier canbe integrated with a baseline model deeply at eachdependency edge, if the evaluation score given bythe baseline model can also be factored into de-pendency edges.We choose the 2nd-ordered MST model (Mc-Donald and Pereira, 2006) as the baseline.
Es-pecially, the effect of the Collins distance in thebaseline model is also investigated.
The relativeweight ?
is adjusted to maximize the performanceon the development set, using an algorithm similarto minimum error-rate training (Och, 2003).5 Related Works5.1 Dependency ParsingBoth the graph-based (McDonald et al, 2005a;McDonald and Pereira, 2006; Carreras et al,2006) and the transition-based (Yamada and Mat-sumoto, 2003; Nivre et al, 2006) parsing algo-rithms are related to our word-pair classificationmodel.Similar to the graph-based method, our modelis factored on dependency edges, and its decod-ing procedure also aims to find a maximum span-ning tree in a fully connected directed graph.
Fromthis point, our model can be classified into thegraph-based category.
On the training method,however, our model obviously differs from othergraph-based models, that we only need a set ofword-pair dependency instances rather than a reg-ular dependency treebank.
Therefore, our model ismore suitable for the partially bracketed or noisytraining corpus.The most apparent similarity between ourmodel and the transition-based category is thatthey all need a classifier to perform classificationconditioned on a certain configuration.
However,they differ from each other in the classification re-sults.
The classifier in our model predicates a de-pendency probability for each pair of words, whilethe classifier in a transition-based model gives apossible next transition operation such as shift orreduce.
Another difference lies in the factoriza-tion strategy.
For our method, the evaluation scoreof a candidate parse is factorized into each depen-dency edge, while for the transition-based models,the score is factorized into each transition opera-tion.Thanks to the reminding of the third reviewerof our paper, we find that the pairwise classifica-tion schema has also been used in Japanese de-pendency parsing (Uchimoto et al, 1999; Kudoand Matsumoto, 2000).
However, our work showsmore advantage in feature engineering, modeltraining and decoding algorithm.5.2 Dependency ProjectionMany works try to learn parsing knowledge frombilingual corpora.
Lu?
et al (2002) aims toobtain Chinese bracketing knowledge via ITG(Wu, 1997) alignment.
Hwa et al (2005) andGanchev et al (2009) induce dependency gram-mar via projection from aligned bilingual cor-pora, and use some thresholds to filter out noiseand some hand-written rules to handle heterogene-ity.
Smith and Eisner (2009) perform depen-dency projection and annotation adaptation withQuasi-Synchronous Grammar features.
Jiang andLiu (2009) refer to alignment matrix and a dy-namic programming search algorithm to obtainbetter projected dependency trees.All previous works for dependency projection(Hwa et al, 2005; Ganchev et al, 2009; Smith andEisner, 2009; Jiang and Liu, 2009) need completeprojected trees to train the projected parsers.
Be-cause of the free translation, the word alignmenterrors, and the heterogeneity between two lan-guages, it is reluctant and less effective to projectthe dependency tree completely to the target lan-guage sentence.
On the contrary, our dependencyprojection strategy prefer to extract a set of depen-dency instances, which coincides our model?s de-mand for training corpus.
An obvious advantageof this strategy is that, we can select an appropriatefiltering threshold to obtain dependency instancesof good quality.In addition, our word-pair classification modelcan be integrated deeply into a state-of-the-artMST dependency model.
Since both of them are16Corpus Train Dev TestWSJ (section) 2-21 22 23CTB 5.0 (chapter) others 301-325 271-300Table 2: The corpus partition for WSJ and CTB5.0.factorized into dependency edges, the integrationcan be conducted at each dependency edge, byweightedly averaging their evaluation scores forthis dependency edge.
This strategy makes betteruse of the projected parser while with faster de-coding, compared with the cascaded approach ofJiang and Liu (2009).6 ExperimentsIn this section, we first validate the word-pairclassification model by experimenting on human-annotated treebanks.
Then we investigate the ef-fectiveness of the dependency projection by eval-uating the projected classifiers trained on the pro-jected classification instances.
Finally, we re-port the performance of the integrated dependencyparser which integrates the projected classifier andthe 2nd-ordered MST dependency parser.
Weevaluate the parsing accuracy by the precision oflexical heads, which is the percentage of the wordsthat have found their correct parents.6.1 Word-Pair Classification ModelWe experiment on two popular treebanks, the WallStreet Journal (WSJ) portion of the Penn EnglishTreebank (Marcus et al, 1993), and the Penn Chi-nese Treebank (CTB) 5.0 (Xue et al, 2005).
Theconstituent trees in the two treebanks are trans-formed to dependency trees according to the head-finding rules of Yamada and Matsumoto (2003).For English, we use the automatically-assignedPOS tags produced by an implementation of thePOS tagger of Collins (2002).
While for Chinese,we just use the gold-standard POS tags followingthe tradition.
Each treebank is splitted into threepartitions, for training, development and testing,respectively, as shown in Table 2.For a dependency tree with n words, only n ?1 positive dependency instances can be extracted.They account for only a small proportion of all thedependency instances.
As we know, it is importantto balance the proportions of the positive and thenegative instances for a batched-trained classifier.We define a new parameter r to denote the ratio ofthe negative instances relative to the positive ones.8484.58585.58686.5871  1.5  2  2.5  3DependencyPrecision(%)Ratio r (#negative/#positive)WSJCTB 5.0Figure 3: Performance curves of the word-pairclassification model on the development sets ofWSJ and CTB 5.0, with respect to a series of ratior.Corpus System P %WSJ Yamada and Matsumoto (2003) 90.3Nivre and Scholz (2004) 87.31st-ordered MST 90.72nd-ordered MST 91.5our model 86.8CTB 5.0 1st-ordered MST 86.532nd-ordered MST 87.15our model 82.06Table 3: Performance of the word-pair classifica-tion model on WSJ and CTB 5.0, compared withthe current state-of-the-art models.For example, r = 2 means we reserve negativeinstances two times as many as the positive ones.The MaxEnt toolkit by Zhang 2 is adopted totrain the ME classifier on extracted instances.
Weset the gaussian prior as 1.0 and the iteration limitas 100, leaving other parameters as default values.We first investigate the impact of the ratio r onthe performance of the classifier.
Curves in Fig-ure 3 show the performance of the English andChinese parsers, each of which is trained on an in-stance set corresponding to a certain r. We findthat for both English and Chinese, maximum per-formance is achieved at about r = 2.5.
3 TheEnglish and Chinese classifiers trained on the in-stance sets with r = 2.5 are used in the final eval-uation phase.
Table 3 shows the performances onthe test sets of WSJ and CTB 5.0.We also compare them with previous works onthe same test sets.
On both English and Chinese,the word-pair classification model falls behind ofthe state-of-the-art.
We think that it is probably2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.3We did not investigate more fine-grained ratios, since theperformance curves show no dramatic fluctuation along withthe alteration of r.175454.55555.5560.65  0.7  0.75  0.8  0.85  0.9  0.95DependencyPrecision(%)Threshold bFigure 4: The performance curve of the word-pair classification model on the development setof CTB 5.0, with respect to a series of threshold b.due to the local optimization of the training pro-cedure.
Given complete trees as training data, itis easy for previous models to utilize structural,global and linguistical information in order to ob-tain more powerful parameters.
The main advan-tage of our model is that it doesn?t need completetrees to tune its parameters.
Therefore, if trainedon instances extracted from human-annotated tree-banks, the word-pair classification model wouldnot demonstrate its advantage over existed state-of-the-art dependency parsing methods.6.2 Dependency ProjectionIn this work we focus on the dependency projec-tion from English to Chinese.
We use the FBISChinese-English bitext as the bilingual corpus fordependency projection.
It contains 239K sen-tence pairs with about 6.9M/8.9M words in Chi-nese/English.
Both English and Chinese sentencesare tagged by the implementations of the POS tag-ger of Collins (2002), which trained on WSJ andCTB 5.0 respectively.
The English sentences arethen parsed by an implementation of 2nd-orderedMST model of McDonald and Pereira (2006),which is trained on dependency trees extractedfrom WSJ.
The alignment matrixes for sentencepairs are generated according to (Liu et al, 2009).Similar to the ratio r, the threshold b need alsobe assigned an appropriate value to achieve a bet-ter performance.
Larger thresholds result in betterbut less classification instances, the lower cover-age of the instances would hurt the performance ofthe classifier.
On the other hand, smaller thresh-olds lead to worse but more instances, and toomuch noisy instances will bring down the classi-fier?s discriminating power.We extract a series of classification instance setsCorpus System P %CTB 2.0 Hwa et al (2005) 53.9our model 56.9CTB 5.0 Jiang and Liu (2009) 53.28our model 58.59Table 4: The performance of the projected classi-fier on the test sets of CTB 2.0 and CTB 5.0, com-pared with the performance of previous works onthe corresponding test sets.Corpus Baseline P% Integrated P%CTB 1.0 82.23 83.70CTB 5.0 87.15 87.65Table 5: Performance improvement brought bythe projected classifier to the baseline 2nd-orderedMST parsers trained on CTB 1.0 and CTB 5.0, re-spectively.with different thresholds.
Then, on each instanceset we train a classifier and test it on the develop-ment set of CTB 5.0.
Figure 4 presents the ex-perimental results.
The curve shows that the max-imum performance is achieved at the threshold ofabout 0.85.
The classifier corresponding to thisthreshold is evaluated on the test set of CTB 5.0,and the test set of CTB 2.0 determined by Hwa etal.
(2005).
Table 4 shows the performance of theprojected classifier, as well as the performance ofprevious works on the corresponding test sets.
Theprojected classifier significantly outperforms pre-vious works on both test sets, which demonstratesthat the word-pair classification model, althoughfalling behind of the state-of-the-art on human-annotated treebanks, performs well in projecteddependency parsing.
We give the credit to its goodcollaboration with the word-pair classification in-stance extraction for dependency projection.6.3 Integrated Dependency ParserWe integrate the word-pair classification modelinto the state-of-the-art 2nd-ordered MST model.First, we implement a chart-based dynamic pro-gramming parser for the 2nd-ordered MST model,and develop a training procedure based on theperceptron algorithm with averaged parameters(Collins, 2002).
On the WSJ corpus, this parserachieves the same performance as that of McDon-ald and Pereira (2006).
Then, at each derivationstep of this 2nd-ordered MST parser, we weight-edly add the evaluation score given by the pro-jected classifier to the original MST evaluationscore.
Such a weighted summation of two eval-18uation scores provides better evaluation for can-didate parses.
The weight parameter ?
is tunedby a minimum error-rate training algorithm (Och,2003).Given a 2nd-ordered MST parser trained onCTB 5.0 as the baseline, the projected classi-fier brings an accuracy improvement of about 0.5points.
For the baseline trained on the smallerCTB 1.0, whose training set is chapters 1-270 ofCTB 5.0, the accuracy improvement is much sig-nificant, about 1.5 points over the baseline.
Itindicates that, the smaller the human-annotatedtreebank we have, the more significant improve-ment we can achieve by integrating the project-ing classifier.
This provides a promising strategyfor boosting the parsing performance of resource-scarce languages.
Table 5 summarizes the experi-mental results.7 Conclusion and Future WorksIn this paper, we first describe an intuitionis-tic method for dependency parsing, which re-sorts to a classifier to determine whether a wordpair forms a dependency edge, and then proposean effective strategy for dependency projection,which produces a set of projected classification in-stances rather than complete projected trees.
Al-though this parsing method falls behind of pre-vious models, it can collaborate well with theword-pair classification instance extraction strat-egy for dependency projection, and achieves thestate-of-the-art in projected dependency parsing.In addition, when integrated into a 2nd-orderedMST parser, the projected parser brings signifi-cant improvement to the baseline, especially forthe baseline trained on smaller treebanks.
Thisprovides a new strategy for resource-scarce lan-guages to train high-precision dependency parsers.However, considering its lower performance onhuman-annotated treebanks, the dependency pars-ing method itself still need a lot of investigations,especially on the training method of the classifier.AcknowledgementThis project was supported by National NaturalScience Foundation of China, Contract 60736014,and 863 State Key Project No.
2006AA010108.We are grateful to the anonymous reviewers fortheir thorough reviewing and valuable sugges-tions.
We show special thanks to Dr. RebeccaHwa for generous help of sharing the experimen-tal data.
We also thank Dr. Yang Liu for sharingthe codes of alignment matrix generation, and Dr.Liang Huang for helpful discussions.ReferencesAdam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
Compu-tational Linguistics.Xavier Carreras, Mihai Surdeanu, and Lluis Marquez.2006.
Projective dependency parsing with percep-tron.
In Proceedings of the CoNLL.Xavier Carreras, Michael Collins, and Terry Koo.2008.
Tag, dynamic programming, and the percep-tron for efficient, feature-rich parsing.
In Proceed-ings of the CoNLL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine-grained n-best parsing and discriminativereranking.
In Proceedings of the ACL.Michael Collins.
1996.
A new statistical parser basedon bigram lexical dependencies.
In Proceedings ofACL.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proceedings of theICML, pages 175?182.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the EMNLP, pages 1?8, Philadelphia, USA.Jason M. Eisner.
1996.
Three new probabilistic mod-els for dependency parsing: An exploration.
In Pro-ceedings of COLING, pages 340?345.Kuzman Ganchev, Jennifer Gillenwater, and BenTaskar.
2009.
Dependency grammar induction viabitext projection constraints.
In Proceedings of the47th ACL.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the IWPT, pages 53?64.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofthe ACL.Rebecca Hwa, Philip Resnik, Amy Weinberg, andOkan Kolak.
2002.
Evaluating translational corre-spondence using annotation projection.
In Proceed-ings of the ACL.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.In Natural Language Engineering, volume 11, pages311?325.19Wenbin Jiang and Qun Liu.
2009.
Automatic adapta-tion of annotation standards for dependency parsingusing projected treebank as source corpus.
In Pro-ceedings of IWPT.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic adaptation of annotation standards: Chineseword segmentation and pos tagging?a case study.
InProceedings of the 47th ACL.Dan Klein and Christopher D. Manning.
2004.
Cor-pusbased induction of syntactic structure: Models ofdependency and constituency.
In Proceedings of theACL.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of the ACL.Taku Kudo and Yuji Matsumoto.
2000.
Japanese de-pendency structure analysis based on support vectormachines.
In Proceedings of the EMNLP.Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu.
2009.Weighted alignment matrices for statistical machinetranslation.
In Proceedings of the EMNLP.Yajuan Lu?, Sheng Li, Tiejun Zhao, and Muyun Yang.2002.
Learning chinese bracketing knowledgebased on a bilingual language model.
In Proceed-ings of the COLING.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
In Computa-tional Linguistics.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of EACL, pages 81?88.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005a.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL, pages 91?98.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedingsof HLT-EMNLP.J.
Nivre and M. Scholz.
2004.
Deterministic depen-dency parsing of english text.
In Proceedings of theCOLING.Joakim Nivre, Johan Hall, Jens Nilsson, GulsenEryigit, and Svetoslav Marinov.
2006.
Labeledpseudoprojective dependency parsing with supportvector machines.
In Proceedings of CoNLL, pages221?225.Franz J. Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of theACL.Franz Joseph Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe ACL, pages 160?167.David Smith and Jason Eisner.
2009.
Parser adap-tation and projection with quasi-synchronous gram-mar features.
In Proceedings of EMNLP.Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-hara.
1999.
Japanese dependency structure analysisbased on maximum entropy models.
In Proceedingsof the EACL.Vladimir N. Vapnik.
1998.
Statistical learning theory.In A Wiley-Interscience Publication.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
In NaturalLanguage Engineering.H Yamada and Y Matsumoto.
2003.
Statistical depen-dency analysis using support vector machines.
InProceedings of IWPT.Yue Zhang and Stephen Clark.
2008.
Joint word seg-mentation and pos tagging using a single perceptron.In Proceedings of the ACL.20
