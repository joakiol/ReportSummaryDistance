Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 132?141,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUsing Multiple Sources to Construct a Sentiment Sensitive Thesaurusfor Cross-Domain Sentiment ClassificationDanushka BollegalaThe University of Tokyo7-3-1, Hongo, Tokyo,113-8656, Japandanushka@iba.t.u-tokyo.ac.jpDavid WeirSchool of InformaticsUniversity of SussexFalmer, Brighton,BN1 9QJ, UKd.j.weir@sussex.ac.ukJohn CarrollSchool of InformaticsUniversity of SussexFalmer, Brighton,BN1 9QJ, UKj.a.carroll@sussex.ac.ukAbstractWe describe a sentiment classification methodthat is applicable when we do not have any la-beled data for a target domain but have somelabeled data for multiple other domains, des-ignated as the source domains.
We automat-ically create a sentiment sensitive thesaurususing both labeled and unlabeled data frommultiple source domains to find the associa-tion between words that express similar senti-ments in different domains.
The created the-saurus is then used to expand feature vectorsto train a binary classifier.
Unlike previouscross-domain sentiment classification meth-ods, our method can efficiently learn frommultiple source domains.
Our method signif-icantly outperforms numerous baselines andreturns results that are better than or com-parable to previous cross-domain sentimentclassification methods on a benchmark datasetcontaining Amazon user reviews for differenttypes of products.1 IntroductionUsers express opinions about products or servicesthey consume in blog posts, shopping sites, or re-view sites.
It is useful for both consumers as wellas for producers to know what general public thinkabout a particular product or service.
Automaticdocument level sentiment classification (Pang et al,2002; Turney, 2002) is the task of classifying a givenreview with respect to the sentiment expressed bythe author of the review.
For example, a sentimentclassifier might classify a user review about a movieas positive or negative depending on the sentimentexpressed in the review.
Sentiment classificationhas been applied in numerous tasks such as opinionmining (Pang and Lee, 2008), opinion summariza-tion (Lu et al, 2009), contextual advertising (Fanand Chang, 2010), and market analysis (Hu and Liu,2004).Supervised learning algorithms that require la-beled data have been successfully used to build sen-timent classifiers for a specific domain (Pang et al,2002).
However, sentiment is expressed differentlyin different domains, and it is costly to annotatedata for each new domain in which we would liketo apply a sentiment classifier.
For example, in thedomain of reviews about electronics products, thewords ?durable?
and ?light?
are used to express pos-itive sentiment, whereas ?expensive?
and ?short bat-tery life?
often indicate negative sentiment.
On theother hand, if we consider the books domain thewords ?exciting?
and ?thriller?
express positive sen-timent, whereas the words ?boring?
and ?lengthy?usually express negative sentiment.
A classifiertrained on one domain might not perform well ona different domain because it would fail to learn thesentiment of the unseen words.Work in cross-domain sentiment classification(Blitzer et al, 2007) focuses on the challenge oftraining a classifier from one or more domains(source domains) and applying the trained classi-fier in a different domain (target domain).
A cross-domain sentiment classification system must over-come two main challenges.
First, it must identifywhich source domain features are related to whichtarget domain features.
Second, it requires a learn-ing framework to incorporate the information re-132garding the relatedness of source and target domainfeatures.
Following previous work, we define cross-domain sentiment classification as the problem oflearning a binary classifier (i.e.
positive or negativesentiment) given a small set of labeled data for thesource domain, and unlabeled data for both sourceand target domains.
In particular, no labeled data isprovided for the target domain.In this paper, we describe a cross-domain senti-ment classification method using an automaticallycreated sentiment sensitive thesaurus.
We use la-beled data from multiple source domains and unla-beled data from source and target domains to rep-resent the distribution of features.
We represent alexical element (i.e.
a unigram or a bigram of wordlemma) in a review using a feature vector.
Next, foreach lexical element we measure its relatedness toother lexical elements and group related lexical ele-ments to create a thesaurus.
The thesaurus capturesthe relatedness among lexical elements that appearin source and target domains based on the contextsin which the lexical elements appear (their distribu-tional context).
A distinctive aspect of our approachis that, in addition to the usual co-occurrence fea-tures typically used in characterizing a word?s dis-tributional context, we make use, where possible, ofthe sentiment label of a document: i.e.
sentiment la-bels form part of our context features.
This is whatmakes the distributional thesaurus sensitive to senti-ment.
Unlabeled data is cheaper to collect comparedto labeled data and is often available in large quan-tities.
The use of unlabeled data enables us to ac-curately estimate the distribution of words in sourceand target domains.
Our method can learn from alarge amount of unlabeled data to leverage a robustcross-domain sentiment classifier.We model the cross-domain sentiment classifica-tion problem as one of feature expansion, where weappend additional related features to feature vectorsthat represent source and target domain reviews inorder to reduce the mismatch of features between thetwo domains.
Methods that use related features havebeen successfully used in numerous tasks such asquery expansion (Fang, 2008), and document classi-fication (Shen et al, 2009).
However, feature expan-sion techniques have not previously been applied tothe task of cross-domain sentiment classification.In our method, we use the automatically createdthesaurus to expand feature vectors in a binary clas-sifier at train and test times by introducing relatedlexical elements from the thesaurus.
We use L1 reg-ularized logistic regression as the classification al-gorithm.
(However, the method is agnostic to theproperties of the classifier and can be used to expandfeature vectors for any binary classifier).
L1 regular-ization enables us to select a small subset of featuresfor the classifier.
Unlike previous work which at-tempts to learn a cross-domain classifier using a sin-gle source domain, we leverage data from multiplesource domains to learn a robust classifier that gen-eralizes across multiple domains.
Our contributionscan be summarized as follows.?
We describe a fully automatic method to createa thesaurus that is sensitive to the sentiment ofwords expressed in different domains.?
We describe a method to use the created the-saurus to expand feature vectors at train and testtimes in a binary classifier.2 A Motivating ExampleTo explain the problem of cross-domain sentimentclassification, consider the reviews shown in Ta-ble 1 for the domains books and kitchen appliances.Table 1 shows two positive and one negative re-view from each domain.
We have emphasized inboldface the words that express the sentiment ofthe authors of the reviews.
We see that the wordsexcellent, broad, high quality, interesting, andwell researched are used to express positive senti-ment in the books domain, whereas the word disap-pointed indicates negative sentiment.
On the otherhand, in the kitchen appliances domain the wordsthrilled, high quality, professional, energy sav-ing, lean, and delicious express positive sentiment,whereas the words rust and disappointed expressnegative sentiment.
Although high quality wouldexpress positive sentiment in both domains, and dis-appointed negative sentiment, it is unlikely that wewould encounter well researched in kitchen appli-ances reviews, or rust or delicious in book reviews.Therefore, a model that is trained only using bookreviews might not have any weights learnt for deli-cious or rust, which would make it difficult for thismodel to accurately classify reviews of kitchen ap-pliances.133books kitchen appliances+ Excellent and broad survey of the development ofcivilization with all the punch of high quality fiction.I was so thrilled when I unpack my processor.
It isso high quality and professional in both looks andperformance.+ This is an interesting and well researched book.
Energy saving grill.
My husband loves the burgersthat I make from this grill.
They are lean and deli-cious.- Whenever a new book by Philippa Gregory comesout, I buy it hoping to have the same experience, andlately have been sorely disappointed.These knives are already showing spots of rust de-spite washing by hand and drying.
Very disap-pointed.Table 1: Positive (+) and negative (-) sentiment reviews in two different domains.sentence Excellent and broad survey ofthe development of civilization.POS tags Excellent/JJ and/CC broad/JJsurvey/NN1 of/IO the/ATdevelopment/NN1 of/IO civi-lization/NN1lexical elements(unigrams)excellent, broad, survey, devel-opment, civilizationlexical elements(bigrams)excellent+broad, broad+survey,survey+development, develop-ment+civilizationsentiment fea-tures (lemma)excellent*P, broad*P, sur-vey*P, excellent+broad*P,broad+survey*Psentiment fea-tures (POS)JJ*P, NN1*P, JJ+NN1*PTable 2: Generating lexical elements and sentiment fea-tures from a positive review sentence.3 Sentiment Sensitive ThesaurusOne solution to the feature mismatch problem out-lined above is to use a thesaurus that groups differ-ent words that express the same sentiment.
For ex-ample, if we know that both excellent and deliciousare positive sentiment words, then we can use thisknowledge to expand a feature vector that containsthe word delicious using the word excellent, therebyreducing the mismatch between features in a test in-stance and a trained model.
Below we describe amethod to construct a sentiment sensitive thesaurusfor feature expansion.Given a labeled or an unlabeled review, we firstsplit the review into individual sentences.
We carryout part-of-speech (POS) tagging and lemmatiza-tion on each review sentence using the RASP sys-tem (Briscoe et al, 2006).
Lemmatization reducesthe data sparseness and has been shown to be effec-tive in text classification tasks (Joachims, 1998).
Wethen apply a simple word filter based on POS tags toselect content words (nouns, verbs, adjectives, andadverbs).
In particular, previous work has identifiedadjectives as good indicators of sentiment (Hatzi-vassiloglou and McKeown, 1997; Wiebe, 2000).Following previous work in cross-domain sentimentclassification, we model a review as a bag of words.We select unigrams and bigrams from each sentence.For the remainder of this paper, we will refer to un-igrams and bigrams collectively as lexical elements.Previous work on sentiment classification has shownthat both unigrams and bigrams are useful for train-ing a sentiment classifier (Blitzer et al, 2007).
Wenote that it is possible to create lexical elements bothfrom source domain labeled reviews as well as fromunlabeled reviews in source and target domains.Next, we represent each lexical element u using aset of features as follows.
First, we select other lex-ical elements that co-occur with u in a review sen-tence as features.
Second, from each source domainlabeled review sentence in which u occurs, we cre-ate sentiment features by appending the label of thereview to each lexical element we generate from thatreview.
For example, consider the sentence selectedfrom a positive review of a book shown in Table 2.In Table 2, we use the notation ?*P?
to indicate posi-tive sentiment features and ?*N?
to indicate negativesentiment features.
The example sentence shown inTable 2 is selected from a positively labeled review,and generates positive sentiment features as shownin Table 2.
In addition to word-level sentiment fea-tures, we replace words with their POS tags to create134POS-level sentiment features.
POS tags generalizethe word-level sentiment features, thereby reducingfeature sparseness.Let us denote the value of a feature w in the fea-ture vector u representing a lexical element u byf(u, w).
The vector u can be seen as a compact rep-resentation of the distribution of a lexical element uover the set of features that co-occur with u in the re-views.
From the construction of the feature vector udescribed in the previous paragraph, it follows thatw can be either a sentiment feature or another lexicalelement that co-occurs with u in some review sen-tence.
The distributional hypothesis (Harris, 1954)states that words that have similar distributions aresemantically similar.
We compute f(u, w) as thepointwise mutual information between a lexical ele-ment u and a feature w as follows:f(u, w) = log(c(u,w)N?ni=1 c(i,w)N ?
?mj=1 c(u,j)N)(1)Here, c(u,w) denotes the number of review sen-tences in which a lexical element u and a featurew co-occur, n and m respectively denote the totalnumber of lexical elements and the total number offeatures, and N =?ni=1?mj=1 c(i, j).
Pointwisemutual information is known to be biased towardsinfrequent elements and features.
We follow the dis-counting approach of Pantel & Ravichandran (2004)to overcome this bias.Next, for two lexical elements u and v (repre-sented by feature vectors u and v, respectively), wecompute the relatedness ?
(v, u) of the feature v tothe feature u as follows,?
(v, u) =?w?
{x|f(v,x)>0} f(u, w)?w?
{x|f(u,x)>0} f(u, w).
(2)Here, we use the set notation {x|f(v, x) > 0} todenote the set of features that co-occur with v. Re-latedness of a lexical element u to another lexicalelement v is the fraction of feature weights in thefeature vector for the element u that also co-occurwith the features in the feature vector for the ele-ment v. If there are no features that co-occur withboth u and v, then the relatedness reaches its min-imum value of 0.
On the other hand if all featuresthat co-occur with u also co-occur with v, then therelatedness , ?
(v, u), reaches its maximum value of1.
Note that relatedness is an asymmetric measureby the definition given in Equation 2, and the relat-edness ?
(v, u) of an element v to another element uis not necessarily equal to ?
(u, v), the relatedness ofu to v.We use the relatedness measure defined in Equa-tion 2 to construct a sentiment sensitive thesaurus inwhich, for each lexical element u we list lexical el-ements v that co-occur with u (i.e.
f(u, v) > 0) indescending order of relatedness values ?
(v, u).
Inthe remainder of the paper, we use the term base en-try to refer to a lexical element u for which its relatedlexical elements v (referred to as the neighbors of u)are listed in the thesaurus.
Note that relatedness val-ues computed according to Equation 2 are sensitiveto sentiment labels assigned to reviews in the sourcedomain, because co-occurrences are computed overboth lexical and sentiment elements extracted fromreviews.
In other words, the relatedness of an ele-ment u to another element v depends upon the sen-timent labels assigned to the reviews that generate uand v. This is an important fact that differentiatesour sentiment-sensitive thesaurus from other distri-butional thesauri which do not consider sentimentinformation.Moreover, we only need to retain lexical elementsin the sentiment sensitive thesaurus because whenpredicting the sentiment label for target reviews (attest time) we cannot generate sentiment elementsfrom those (unlabeled) reviews, therefore we arenot required to find expansion candidates for senti-ment elements.
However, we emphasize the fact thatthe relatedness values between the lexical elementslisted in the sentiment-sensitive thesaurus are com-puted using co-occurrences with both lexical andsentiment features, and therefore the expansion can-didates selected for the lexical elements in the tar-get domain reviews are sensitive to sentiment labelsassigned to reviews in the source domain.
Usinga sparse matrix format and approximate similaritymatching techniques (Sarawagi and Kirpal, 2004),we can efficiently create a thesaurus from a large setof reviews.4 Feature ExpansionOur feature expansion phase augments a feature vec-tor with additional related features selected from the135sentiment-sensitive thesaurus created in Section 3 toovercome the feature mismatch problem.
First, fol-lowing the bag-of-words model, we model a reviewd using the set {w1, .
.
.
, wN}, where the elementswi are either unigrams or bigrams that appear in thereview d. We then represent a review d by a real-valued term-frequency vector d ?
RN , where thevalue of the j-th element dj is set to the total numberof occurrences of the unigram or bigram wj in thereview d. To find the suitable candidates to expand avector d for the review d, we define a ranking scorescore(ui,d) for each base entry in the thesaurus asfollows:score(ui,d) =?Nj=1 dj?
(wj , ui)?Nl=1 dl(3)According to this definition, given a review d, a baseentry ui will have a high ranking score if there aremany words wj in the review d that are also listedas neighbors for the base entry ui in the sentiment-sensitive thesaurus.
Moreover, we weight the re-latedness scores for each word wj by its normal-ized term-frequency to emphasize the salient uni-grams and bigrams in a review.
Recall that related-ness is defined as an asymmetric measure in Equa-tion 2, and we use ?
(wj , ui) in the computation ofscore(ui,d) in Equation 3.
This is particularly im-portant because we would like to score base entriesui considering all the unigrams and bigrams that ap-pear in a review d, instead of considering each uni-gram or bigram individually.To expand a vector, d, for a review d, we firstrank the base entries, ui using the ranking scorein Equation 3 and select the top k ranked base en-tries.
Let us denote the r-th ranked (1 ?
r ?
k)base entry for a review d by vrd.
We then extend theoriginal set of unigrams and bigrams {w1, .
.
.
, wN}by the base entries v1d, .
.
.
, vkd to create a new vec-tor d?
?
R(N+k) with dimensions corresponding tow1, .
.
.
, wN , v1d, .
.
.
, vkd for a review d. The valuesof the extended vector d?
are set as follows.
Thevalues of the first N dimensions that correspond tounigrams and bigrams wi that occur in the review dare set to di, their frequency in d. The subsequent kdimensions that correspond to the top ranked basedentries for the review d are weighted according totheir ranking score.
Specifically, we set the value ofthe r-th ranked base entry vrd to 1/r.
Alternatively,one could use the ranking score, score(vrd, d), itselfas the value of the appended base entries.
However,both relatedness scores as well as normalized term-frequencies can be small in practice, which leads tovery small absolute ranking scores.
By using theinverse rank, we only take into account the rela-tive ranking of base entries and ignore their absolutescores.Note that the score of a base entry depends on areview d. Therefore, we select different base en-tries as additional features for expanding differentreviews.
Furthermore, we do not expand each wiindividually when expanding a vector d for a re-view.
Instead, we consider all unigrams and bi-grams in d when selecting the base entries for ex-pansion.
One can think of the feature expansion pro-cess as a lower dimensional latent mapping of fea-tures onto the space spanned by the base entries inthe sentiment-sensitive thesaurus.
The asymmetricproperty of the relatedness (Equation 2) implicitlyprefers common words that co-occur with numerousother words as expansion candidates.
Such wordsact as domain independent pivots and enable us totransfer the information regarding sentiment fromone domain to another.Using the extended vectors d?
to represent re-views, we train a binary classifier from the sourcedomain labeled reviews to predict positive and neg-ative sentiment in reviews.
We differentiate the ap-pended base entries vrd from wi that existed in theoriginal vector d (prior to expansion) by assigningdifferent feature identifiers to the appended base en-tries.
For example, a unigram excellent in a featurevector is differentiated from the base entry excellentby assigning the feature id, ?BASE=excellent?
to thelatter.
This enables us to learn different weights forbase entries depending on whether they are usefulfor expanding a feature vector.
We use L1 regu-larized logistic regression as the classification algo-rithm (Ng, 2004), which produces a sparse model inwhich most irrelevant features are assigned a zeroweight.
This enables us to select useful features forclassification in a systematic way without having topreselect features using heuristic approaches.
Theregularization parameter is set to its default valueof 1 for all the experiments described in this paper.1365 Experiments5.1 DatasetTo evaluate our method we use the cross-domainsentiment classification dataset prepared by Blitzeret al (2007).
This dataset consists of Amazon prod-uct reviews for four different product types: books(B), DVDs (D), electronics (E) and kitchen appli-ances (K).
There are 1000 positive and 1000 neg-ative labeled reviews for each domain.
Moreover,the dataset contains some unlabeled reviews (on av-erage 17, 547) for each domain.
This benchmarkdataset has been used in much previous work oncross-domain sentiment classification and by eval-uating on it we can directly compare our methodagainst existing approaches.Following previous work, we randomly select 800positive and 800 negative labeled reviews from eachdomain as training instances (i.e.
1600?4 = 6400);the remainder is used for testing (i.e.
400 ?
4 =1600).
In our experiments, we select each domain inturn as the target domain, with one or more other do-mains as sources.
Note that when we combine morethan one source domain we limit the total numberof source domain labeled reviews to 1600, balancedbetween the domains.
For example, if we combinetwo source domains, then we select 400 positive and400 negative labeled reviews from each domain giv-ing (400 + 400) ?
2 = 1600.
This enables us toperform a fair evaluation when combining multiplesource domains.
The evaluation metric is classifica-tion accuracy on a target domain, computed as thepercentage of correctly classified target domain re-views out of the total number of reviews in the targetdomain.5.2 Effect of Feature ExpansionTo study the effect of feature expansion at train timecompared to test time, we used Amazon reviews fortwo further domains, music and video, which werealso collected by Blitzer et al (2007) but are notpart of the benchmark dataset.
Each validation do-main has 1000 positive and 1000 negative labeledreviews, and 15000 unlabeled reviews.
Using thevalidation domains as targets, we vary the numberof top k ranked base entries (Equation 3) used forfeature expansion during training (Traink) and test-ing (Testk), and measure the average classification0 200 400 600 800 100002004006008001000TrainkTest k0.7760.7780.780.7820.7840.786Figure 1: Feature expansion at train vs. test times.B D K B+D B+K D+K B+D+K5055606570758085Source DomainsAccuracyon electronics domainFigure 2: Effect of using multiple source domains.accuracy.
Figure 1 illustrates the results using a heatmap, where dark colors indicate low accuracy val-ues and light colors indicate high accuracy values.We see that expanding features only at test time (theleft-most column) does not work well because wehave not learned proper weights for the additionalfeatures.
Similarly, expanding features only at traintime (the bottom-most row) also does not performwell because the expanded features are not used dur-ing testing.
The maximum classification accuracy isobtained when Testk = 400 and Traink = 800, andwe use these values for the remainder of the experi-ments described in the paper.5.3 Combining Multiple SourcesFigure 2 shows the effect of combining multiplesource domains to build a sentiment classifier forthe electronics domain.
We see that the kitchen do-main is the single best source domain when adapt-ing to the electronics target domain.
This behavior1370 200 400 600 80040455055606570758085Positive/Negative instancesAccuracyB E K B+E B+K E+K B+E+KFigure 3: Effect of source domain labeled data.0 0.2 0.4 0.6 0.8 15055606570Source unlabeled dataset sizeAccuracyB E K B+E B+K E+K B+E+KFigure 4: Effect of source domain unlabeled data.is explained by the fact that in general kitchen appli-ances and electronic items have similar aspects.
Buta more interesting observation is that the accuracythat we obtain when we use two source domains isalways greater than the accuracy if we use those do-mains individually.
The highest accuracy is achievedwhen we use all three source domains.
Althoughnot shown here for space limitations, we observedsimilar trends with other domains in the benchmarkdataset.To investigate the impact of the quantity of sourcedomain labeled data on our method, we vary theamount of data from zero to 800 reviews, with equalamounts of positive and negative labeled data.
Fig-ure 3 shows the accuracy with the DVD domain asthe target.
Note that source domain labeled data isused both to create the sentiment sensitive thesaurusas well as to train the sentiment classifier.
Whenthere are multiple source domains we limit and bal-ance the number of labeled instances as outlined inSection 5.1.
The amount of unlabeled data is heldconstant, so that any change in classification accu-0 0.2 0.4 0.6 0.8 15055606570Target unlabeled dataset sizeAccuracyB E K B+E B+K E+K B+E+KFigure 5: Effect of target domain unlabeled data.racy is directly attributable to the source domain la-beled instances.
Because this is a binary classifica-tion task (i.e.
positive vs. negative sentiment), a ran-dom classifier that does not utilize any labeled datawould report a 50% classification accuracy.
FromFigure 3, we see that when we increase the amountof source domain labeled data the accuracy increasesquickly.
In fact, by selecting only 400 (i.e.
50% ofthe total 800) labeled instances per class, we achievethe maximum performance in most of the cases.To study the effect of source and target domainunlabeled data on the performance of our method,we create sentiment sensitive thesauri using differ-ent proportions of unlabeled data.
The amount oflabeled data is held constant and is balanced acrossmultiple domains as outlined in Section 5.1, so anychanges in classification accuracy can be directly at-tributed to the contribution of unlabeled data.
Figure4 shows classification accuracy on the DVD targetdomain when we vary the proportion of source do-main unlabeled data (target domain?s unlabeled datais fixed).Likewise, Figure 5 shows the classification ac-curacy on the DVD target domain when we varythe proportion of the target domain?s unlabeled data(source domains?
unlabeled data is fixed).
From Fig-ures 4 and 5, we see that irrespective of the amountbeing used, there is a clear performance gain whenwe use unlabeled data from multiple source domainscompared to using a single source domain.
How-ever, we could not observe a clear gain in perfor-mance when we increase the amount of the unla-beled data used to create the sentiment sensitive the-saurus.138Method K D E BNo Thesaurus 72.61 68.97 70.53 62.72SCL 80.83 74.56 78.43 72.76SCL-MI 82.06 76.30 78.93 74.56SFA 81.48 76.31 75.30 77.73LSA 79.00 73.50 77.66 70.83FALSA 80.83 76.33 77.33 73.33NSS 77.50 73.50 75.50 71.46Proposed 85.18 78.77 83.63 76.32Within-Domain 87.70 82.40 84.40 80.40Table 3: Cross-domain sentiment classification accuracy.5.4 Cross-Domain Sentiment ClassificationTable 3 compares our method against a number ofbaselines and previous cross-domain sentiment clas-sification techniques using the benchmark dataset.For all previous techniques we give the results re-ported in the original papers.
The No Thesaurusbaseline simulates the effect of not performing anyfeature expansion.
We simply train a binary clas-sifier using unigrams and bigrams as features fromthe labeled reviews in the source domains and ap-ply the trained classifier on the target domain.
Thiscan be considered to be a lower bound that doesnot perform domain adaptation.
SCL is the struc-tural correspondence learning technique of Blitzeret al (2006).
In SCL-MI, features are selected us-ing the mutual information between a feature (uni-gram or bigram) and a domain label.
After selectingsalient features, the SCL algorithm is used to train abinary classifier.
SFA is the spectral feature align-ment technique of Pan et al (2010).
Both the LSAand FALSA techniques are based on latent semanticanalysis (Pan et al, 2010).
For the Within-Domainbaseline, we train a binary classifier using the la-beled data from the target domain.
This upper base-line represents the classification accuracy we couldhope to obtain if we were to have labeled data for thetarget domain.
Note that this is not a cross-domainclassification setting.
To evaluate the benefit of us-ing sentiment features on our method, we give a NSS(non-sentiment sensitive) baseline in which we cre-ate a thesaurus without using any sentiment features.Proposed is our method.From Table 3, we see that our proposed methodreturns the best cross-domain sentiment classifica-tion accuracy (shown in boldface) for the three do-mains kitchen appliances, DVDs, and electronics.For the books domain, the best results are returnedby SFA.
The books domain has the lowest numberof unlabeled reviews (around 5000) in the dataset.Because our method relies upon the availability ofunlabeled data for the construction of a sentimentsensitive thesaurus, we believe that this accounts forour lack of performance on the books domain.
How-ever, given that it is much cheaper to obtain unla-beled than labeled data for a target domain, there isstrong potential for improving the performance ofour method in this domain.
The analysis of vari-ance (ANOVA) and Tukey?s honestly significant dif-ferences (HSD) tests on the classification accuraciesfor the four domains show that our method is sta-tistically significantly better than both the No The-saurus and NSS baselines, at confidence level 0.05.We therefore conclude that using the sentiment sen-sitive thesaurus for feature expansion is useful forcross-domain sentiment classification.
The resultsreturned by our method are comparable to state-of-the-art techniques such as SCL-MI and SFA.
In par-ticular, the differences between those techniques andour method are not statistically significant.6 Related WorkCompared to single-domain sentiment classifica-tion, which has been studied extensively in previouswork (Pang and Lee, 2008; Turney, 2002), cross-domain sentiment classification has only recently re-ceived attention in response to advances in the areaof domain adaptation.
Aue and Gammon (2005) re-port a number of empirical tests into domain adap-tation of sentiment classifiers using an ensemble ofclassifiers.
However, most of these tests were un-able to outperform a simple baseline classifier thatis trained using all labeled data for all domains.Blitzer et al (2007) apply the structural corre-spondence learning (SCL) algorithm to train a cross-domain sentiment classifier.
They first chooses a setof pivot features using pointwise mutual informa-tion between a feature and a domain label.
Next,linear predictors are learnt to predict the occur-rences of those pivots.
Finally, they use singularvalue decomposition (SVD) to construct a lower-dimensional feature space in which a binary classi-139fier is trained.
The selection of pivots is vital to theperformance of SCL and heuristically selected pivotfeatures might not guarantee the best performanceon target domains.
In contrast, our method uses allfeatures when creating the thesaurus and selects asubset of features during training using L1 regular-ization.
Moreover, we do not require SVD, whichhas cubic time complexity so can be computation-ally expensive for large datasets.Pan et al (2010) use structural feature alignment(SFA) to find an alignment between domain spe-cific and domain independent features.
The mu-tual information of a feature with domain labels isused to classify domain specific and domain inde-pendent features.
Next, spectral clustering is per-formed on a bipartite graph that represents the re-lationship between the two sets of features.
Fi-nally, the top eigenvectors are selected to constructa lower-dimensional projection.
However, not allwords can be cleanly classified into domain spe-cific or domain independent, and this process is con-ducted prior to training a classifier.
In contrast, ourmethod lets a particular lexical entry to be listed asa neighour for multiple base entries.
Moreover, weexpand each feature vector individually and do notrequire any clustering.
Furthermore, unlike SCL andSFA, which consider a single source domain, ourmethod can efficiently adapt from multiple sourcedomains.7 ConclusionsWe have described and evaluated a method toconstruct a sentiment-sensitive thesaurus to bridgethe gap between source and target domains incross-domain sentiment classification using multi-ple source domains.
Experimental results using abenchmark dataset for cross-domain sentiment clas-sification show that our proposed method can im-prove classification accuracy in a sentiment classi-fier.
In future, we intend to apply the proposedmethod to other domain adaptation tasks.AcknowledgementsThis research was conducted while the first authorwas a visiting research fellow at Sussex universityunder the postdoctoral fellowship of the Japan Soci-ety for the Promotion of Science (JSPS).ReferencesAnthony Aue and Michael Gamon.
2005.
Customiz-ing sentiment classifiers to new domains: a case study.Technical report, Microsoft Research.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP 2006.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InACL 2007, pages 440?447.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the rasp system.
In COL-ING/ACL 2006 Interactive Presentation Sessions.Teng-Kai Fan and Chia-Hui Chang.
2010.
Sentiment-oriented contextual advertising.
Knowledge and Infor-mation Systems, 23(3):321?344.Hui Fang.
2008.
A re-examination of query expansionusing lexical resources.
In ACL 2008, pages 139?147.Z.
Harris.
1954.
Distributional structure.
Word, 10:146?162.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In ACL 1997, pages 174?181.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In KDD 2004, pages 168?177.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many relevantfeatures.
In ECML 1998, pages 137?142.Yue Lu, ChengXiang Zhai, and Neel Sundaresan.
2009.Rated aspect summarization of short comments.
InWWW 2009, pages 131?140.Andrew Y. Ng.
2004.
Feature selection, l1 vs. l2 regular-ization, and rotational invariance.
In ICML 2004.Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, QiangYang, and Zheng Chen.
2010.
Cross-domain senti-ment classification via spectral feature alignment.
InWWW 2010.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification using ma-chine learning techniques.
In EMNLP 2002, pages 79?86.Patrick Pantel and Deepak Ravichandran.
2004.
Au-tomatically labeling semantic classes.
In NAACL-HLT?04, pages 321 ?
328.Sunita Sarawagi and Alok Kirpal.
2004.
Efficient setjoins on similarity predicates.
In SIGMOD ?04, pages743?754.140Dou Shen, Jianmin Wu, Bin Cao, Jian-Tao Sun, QiangYang, Zheng Chen, and Ying Li.
2009.
Exploit-ing term relationship to boost text classification.
InCIKM?09, pages 1637 ?
1640.Peter D. Turney.
2002.
Thumbs up or thumbs down?semantic orientation applied to unsupervised classifi-cation of reviews.
In ACL 2002, pages 417?424.Janyce M. Wiebe.
2000.
Learning subjective adjectivefrom corpora.
In AAAI 2000, pages 735?740.141
