Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 108?116,Avignon, France, April 23 - 24 2012. c?2012 Association for Computational LinguisticsUsing context and phonetic featuresin models of etymological sound changeHannes Wettig1, Kirill Reshetnikov2 and Roman Yangarber11Department of Computer Science 2Institute of LinguisticsUniversity of Helsinki, Finland Academy of SciencesFirst.Last@cs.helsinki.fi Moscow, RussiaAbstractThis paper presents a novel method foraligning etymological data, which mod-els context-sensitive rules governing soundchange, and utilizes phonetic features of thesounds.
The goal is, for a given corpus ofcognate sets, to find the best alignment atthe sound level.
We introduce an imputa-tion procedure to compare the goodness ofthe resulting models, as well as the good-ness of the data sets.
We present evalu-ations to demonstrate that the new modelyields improvements in performance, com-pared to previously reported models.1 IntroductionThis paper introduces a context-sensitive modelfor alignment and analysis of etymological data.Given a raw collection of etymological data (thecorpus)?we first aim to find the ?best?
alignmentat the sound or symbol level.
We take the corpus(or possibly several different corpora) for a lan-guage family as given; different data sets are typ-ically conflicting, which creates the need to deter-mine which is more correct.
Etymological datasets are found in digital etymological databases,such as ones we use for the Uralic language fam-ily.
A database is typically organized into cog-nate sets; all elements within a cognate set areposited (by the database creators) to be derivedfrom a common origin, which is a word-form inthe ancestral proto-language.Etymology encompasses several problems,including: discovery of sets of cognates?genetically related words; determination of ge-netic relations among groups of languages, basedon linguistic data; discovering regular sound cor-respondences across languages in a given lan-guage family; and reconstruction of forms in theproto-languages.Computational methods can provide valuabletools for the etymological community.
The meth-ods can be judged by how well they model certainaspects of etymology, and by whether the auto-matic analysis produces results that match theo-ries established by manual analysis.In this work, we allow all the data?and onlythe data?to determine what rules underly it,rather than relying on external (and possibly bi-ased) rules that try to explain the data.
This ap-proach will provide a means of measuring thequality of the etymological data sets in terms oftheir internal consistency?a dataset that is moreconsistent should receive a higher score.
We seekmethods that analyze the data automatically, inan unsupervised fashion, to determine whether acomplete description of the correspondences canbe discovered automatically, directly from rawetymological data?cognate sets within the lan-guage family.
Another way to state the questionis: what alignment rules are ?inherently encoded?in the given corpus itself.At present, our aim is to analyze given etymo-logical datasets, rather than to construct new onesfrom scratch.
Because our main goal is to de-velop methods that are as objective as possible,the models make no a priori assumptions or ?uni-versal?
principles?e.g., no preference to alignvowel with vowels, or a symbol with itself.
Themodels are not aware of the identity of a symbolacross languages, and do not try to preserve iden-tity, of symbols, or even of features?rather theytry to find maximally regular correspondences.In Section 2 we describe the data used in ourexperiments, and review approaches to etymolog-ical alignment over the last decade.
We formalizethe problem of alignment in Section 3, give the108Uralic treeFigure 1: Finno-Ugric branch of Uralic language fam-ily (the data used in the experiments in this paper)technical details of our models in Section 4.
Wepresent results and discussion in Sections 5 and 6.2 Data and Related WorkWe use two large Uralic etymological resources.The StarLing database of Uralic, (Starostin,2005), based on (Re?dei, 1988 1991), containsover 2500 cognate sets.
Suomen Sanojen Alku-pera?
(SSA), ?The Origin of Finnish Words?, aFinnish etymological dictionary, (Itkonen and Ku-lonen, 2000), has over 5000 cognate sets, (abouthalf of which are only in languages from theBalto-Finnic branch, closest to Finnish).
Mostimportantly, for our models, SSA gives ?dictio-nary?
word-forms, which may contain extraneousmorphological material, whereas StarLing data ismostly stemmed.One traditional arrangement of the Uralic lan-guages1 is shown in Figure 1.
We model etymo-logical processes using these Uralic datasets.The methods in (Kondrak, 2002) learn regularone-to-one sound correspondences between pairsof related languages in the data.
The methodsin (Kondrak, 2003; Wettig et al, 2011) find morecomplex (one-to-many) correspondences.
Thesemodels operate on one language pair at a time;also, they do not model the context of the soundchanges, while most etymological changes areconditioned on context.
The MCMC-based modelproposed in (Bouchard-Co?te?
et al, 2007) explic-itly aims to model the context of changes, and op-1Adapted from Encyclopedia Britannica and (Anttila,1989)erates on more than a pair of languages.2We should note that our models at present op-erate at the phonetic level only, they leave seman-tic judgements of the database creators unques-tioned.
While other work, e.g.
(Kondrak, 2004),has attempted to approach semantics by compu-tational means as well, our model uses the givencognate set as the fundamental unit.
In our work,we do not attempt the problem of discovering cog-nates, addressed, e.g., in, (Bouchard-Co?te?
et al,2007; Kondrak, 2004; Kessler, 2001).
We begininstead with a set of etymological data (or morethan one set) for a language family as given.
Wefocus on the principle of recurrent sound corre-spondence, as in much of the literature, includ-ing (Kondrak, 2002; Kondrak, 2003), and others.As we develop our alignment models at thesound or symbol level, in the process of evalu-ation of these models, we also arrive at model-ing the relationships among groups of languageswithin the family.
Construction of phylogenies isstudied extensively, e.g., by (Nakhleh et al, 2005;Ringe et al, 2002; Barbanc?on et al, 2009).
Thiswork differs from ours in that it operates on manu-ally pre-selected sets of characters, which capturedivergent features of languages within the family,whereas we operate on the raw, complete data.There is extensive work on alignment in themachine-translation (MT) community, and it hasbeen observed that methods from MT alignmentmay be projected onto alignment in etymology.The intuition is that translation sentences in MTcorrespond to cognate words in etymology, whilewords in MT correspond to sounds in etymology.The notion of regularity of sound change in et-ymology, which is what our models try to cap-ture, is loosely similar to contextually conditionedcorrespondence of translation words across lan-guages.
For example, (Kondrak, 2002) employsMT alignment from (Melamed, 1997; Melamed,2000); one might employ the IBM models forMT alignment, (Brown et al, 1993), or the HMMmodel, (Vogel et al, 1996).
Of the MT-relatedmodels, (Bodrumlu et al, 2009) is similar to oursin that it is based on MDL (the Minimum Descrip-tion Length Principle, introduced below).2Using this method, we found that the running time didnot scale well for more than three languages.1093 Aligning Pairs of WordsWe begin with pairwise alignment: aligning pairsof words, from two related languages in ourcorpus of cognates.
For each word pair, thetask of alignment means finding exactly whichsymbols correspond.
Some symbols may alignwith ?themselves?
(i.e., with similar or identi-cal sounds), while others may have undergonechanges during the time when the two related lan-guages have been evolving separately.
The sim-plest form of such alignment at the symbol levelis a pair (?
: ?)
?
?
?
T , a single symbol ?from the source alphabet ?
with a symbol ?
fromthe target alhabet T .
We denote the sizes of thealphabets by |?| and |T |.To model insertions and deletions, we augmentboth alphabets with a special empty symbol?denoted by a dot?and write the augmented al-phabets as ?.
and T.. We can then alignword pairs such as vuosi?al (meaning ?year?
inFinnish and Xanty) , for example as any of:v u o s i| | | | |a l .
.
.v u o s i| | | | |.
a .
l .etc...The alignment on the right then consists of thesymbol pairs: (v:.
), (u:a), (o:.
), (s:l), (i:.
).4 Context Model with Phonetic FeaturesThe context-aware alignment method we presenthere is built upon baseline models published pre-viously, (Wettig et al, 2011), where we presentedseveral models that do not use phonetic featuresor context.
Similarly to the earlier ones, the cur-rent method is based on the Minimum DescriptionLength (MDL) Principle, (Gru?nwald, 2007).We begin with a raw set of (observed) data?the not-yet-aligned word pairs.
We would liketo find an alignment for the data?which wewill call the complete data?complete with align-ments, that make the most sense globally, in termsof embodying regular correspondences.
We areafter the regularity, and the more regularity wecan find, the ?better?
our alignment will be (itsgoodness will be defined formally later).
MDLtells us that the more regularity we can find inthe data, the fewer bits we will need to encodeit (or compress it).
More regularity means lowerentropy in the distribution that describes the data,and lower entropy allows us to construct a moreeconomical code.
That is, if we have no knowl-edge about any regularly of correspondence be-tween symbols, the joint distribution over all pos-sible pairs of symbols will be very flat (high en-tropy).
If we know that certain symbol pairs alignfrequently, the joint distribution will have spikes,and lower entropy.
In (Wettig et al, 2011) weshowed how starting with a random alignment agood joint distribution can be learned using MDL.However the ?rules?
those baseline models wereable to learn were very rudimentary, since theycould not use any information in the context, andwe know that many regular correspondences areconditioned by context.We now introduce models that leverage infor-mation from the context to try to reduce the un-certainty in the distributions further, lowering thecoding cost.
To do that, we will code soundsin terms of their phonetic features: rather thancoding the symbols (sounds) as atomic, we codethem as vectors of phonetic features.
Rather thanaligning symbol pairs, we align the correspond-ing features of the symbols.
While coding eachfeature, the model can make use of features ofother sounds in its context (environment), througha special decision tree built for that feature.4.1 FeaturesWe will code each symbol, to be aligned in thecomplete data, as a feature vector.
First we codethe Type feature, with values: K (consonant), V(vowel), dot, and word boundary, which we de-note as #.
Consonants and vowels have their ownsets of features, with 2?8 values per feature:Consonant articulationM Manner plosive, fricative, glide, ...P Place labial, dental, ..., velarX Voiced ?
, +S Secondary ?
, affricate, aspirate, ...Vowel articulationV Vertical high?lowH Horizontal front?backR Rounding ?
, +L Length 1?54.2 ContextsWhile coding any symbol, the model will be al-lowed to query a fixed, finite set of candidate con-texts.
A context is a triplet (L,P, F ), where Lis the level?either source or target,?and P is110one of the positions that the model may query?relative to the position currently being coded; forexample, we may allow positions as in Fig.
2.
F isone of the possible features found at that position.Therefore, we will have about 2 levels * 8 posi-tions * 2?6 features ?
80 candidate contexts thatcan be queried by the model, as explained below.I itself,?P previous position?S previous non-dot symbol?K previous consonant?V previous vowel+S previous or self non-dot symbol+K previous or self consonant+V previous or self vowelFigure 2: An example of a set of possible positionsin the context?relative to the position currently beingcoded?that can be queried by the context model.4.3 The Two-Part CodeWe code the complete (i.e., aligned) data using atwo-part code, following the MDL Principle.
Wefirst code which particular model instance we se-lect from our class of models, and then code thedata, given the defined model.
Our model classis defined as: a set of decision trees (forest), withone tree to predict each feature on each level.
Themodel instance will define the particular struc-tures for each of the trees.The forest consists of 18 decision trees, one foreach feature on the source and the target level: thetype feature, 4 vowel and 4 consonant features,times 2 levels.
Each node in such tree will ei-ther be a leaf, or will be split by querying one ofthe candidate contexts defined above.
The cost ofcoding the structure of the tree is one bit for everynode?to encode whether this node was split (isan internal node) or is a leaf?plus?
log 80 timesthe number of internal nodes?to encode whichparticular context was chosen to split that node.We will explain how the best context to split on ischosen in Sec.
4.6.Each feature and level define a tree, e.g., the?voiced?
(X) feature of the source symbols cor-responds to the source-X tree.
A node N in thistree holds a distribution over the values of X ofonly those symbol instances in the complete datathat have reached in N by following the contextqueries, starting from the root.
The tree struc-ture tells us precisely which path to follow?completely determined by the context.
For exam-ple, when coding a symbol ?
based on anothersymbol found in the context of ?
?at some level(say, target), some position (say, ?K), and one ofits features (say, M)?the next edge down the treeis determined by that feature?s value; and so on,down to a leaf.
For an example of an actual deci-sion tree learned by the model, see Fig.
5.To compute the code length of the completedata, we only need to take into account the dis-tributions at the leaves.
We could choose from avariety of coding methods; the crucial point is thatthe chosen code will assign a particular number?the cost?to every possible alignment of the data.This code-length, or cost, will then serve as theobjective function?i.e., it will be the value thatthe algorithm will try to optimize.
Each reduc-tion in cost will correspond directly to reductionin the entropy of the probability distribution ofthe symbols, which in turn corresponds to morecertainty (i.e., regularity) in the correspondencesamong the symbols, and to improvement in thealignment.
This is the link to our goal, and thereason for introducing code lengths?it gives usa single number that describes the quality of analignment.We use Normalized Maximum Likelihood(NML), (Rissanen, 1996) as our coding scheme.We choose NML because it has certain optimal-ity properties.
Using NML, we code the distri-bution at each leaf node separately, and summingthe costs of all leaves gives the total cost of thealigned data?the value of our objective function.Suppose n instances end up in a leaf node N ,of the ?-level tree, for feature F having k val-ues (e.g., consonants satisfying N ?s context con-straints in the source-X tree, with k = 2 values:?
and +), and the values are distributed so thatni instances have value i (with i ?
{1, .
.
.
, k}).Then this requires an NML code-length ofLNML(?
;F ;N) = ?
logPNML(?
;F ;N)= ?
log?i(nin)niC(n, k)(1)Here?i(nin)ni is the maximum likelihood of themultinomial data at node N , and the termC(n, k) =?n?1+...+n?k=n?i(n?in)n?i(2)111is a normalizing constant to make PNML a prob-ability distribution.In the MDL literature, e.g., (Gru?nwald, 2007),the term ?
logC(n, k) is called the stochasticcomplexity or the (minimax) regret of the model,(in this case, the multinomial model).
The NMLdistribution provides the unique solution to theminimax problem posed in (Shtarkov, 1987),minP?maxxnlogP (xn|??(xn))P?
(xn)(3)where ??
(xn) = arg max?
P(xn) are the maxi-mum likelihood parameters for the data xn.
Thus,PNML minimizes the worst-case regret, i.e., thenumber of excess bits in the code as compared tothe best model in the model class, with hind-sight.For details on the computation of this code lengthsee (Kontkanen and Myllyma?ki, 2007).Learning the model from the observed data nowmeans aligning the word pairs and building thedecision trees in such a way as to minimize thetwo-part code length: the sum of the model?s codelength?to encode the structure of the trees,?and the data?s code length?to encode the alignedword pairs, using these trees.4.4 Summary of the AlgorithmThe full learning algorithm runs as follows:We start with an initial random alignment foreach pair of words in the corpus, i.e., for eachword pair choose some random path through thematrix depicted in Figure 3.From then on we alternate between two steps:A. re-build the decision trees for all features onsource and target levels, and B. re-align all wordpairs in the corpus.
Both of these operationsmonotonically decrease the two-part cost functionand thus compress the data.We continue until we reach convergence.4.5 Re-alignment ProcedureTo align source word ~?
consisting of symbols~?
= [?1...?n], ~?
?
??
with target word ~?
=[?1...?m] we use dynamic programming.
Thetree structures are considered fixed, as are thealignments of all word pairs, except the one cur-rently being aligned?which is subtracted fromthe counts stored at the leaf nodes.We now fill the matrix V , left-to-right, top-to-bottom.
Every possible alignment of ~?
and ~?
cor-Figure 3: Dynamic programming matrix V, to searchfor the most probable alignmentresponds to exactly one path through this matrix:starting with cost equal to 0 in the top-left cell,moving only downward or rightward, and termi-nating in the bottom-right cell.
In this Viterbi-likematrix, every cell corresponds to a partially com-pleted alignment: reaching cell (i, j) means hav-ing read off i symbols of the source word and jsymbols of the target.
Each cell V (i, j)?markedX in the Figure?stores the cost of the most prob-able path so far: the most probable way to havescanned ~?
through symbol ?i and ~?
through ?j :V (i, j) = min????
?V (i, j ?
1) +L(.
: ?j)V (i?
1, j) +L(?i : .
)V (i?
1, j ?
1) +L(?i : ?j)Each term V (?, ?)
has been computed earlier bythe dynamic programming; the term L(?
)?thecost of aligning the two symbols, inserting ordeleting?is determined by the change in datacode length it induces to add this event to the cor-responding leaf in all the feature trees it concerns.In particular, the cost of the most probable com-plete alignment of the two words will be stored inthe bottom-right cell, V (n,m), marked .4.6 Building Decision TreesGiven a complete alignment of the data, we needto build a decision tree, for each feature on bothlevels, yielding the lowest two-part cost.
The term?decision tree?
is meant in a probabilistic sensehere: instead of a single value, at each node westore a distribution of the corresponding featurevalues, over all instances that reach this node.
Thedistribution at a leaf is then used to code an in-stance when it reaches the leaf in question.
Wecode the features in some fixed, pre-set order, andsource level before target level.112We now describe in detail the process of build-ing the tree for feature X, for the source level, (wewill need do the same for all other features, onboth levels, as well).
We build this tree as follows.First, we collect all instances of consonants on thesource level, and gather the the counts for featureX; and build an initial count vector; suppose it is:value of X: + ?1001 1002This vector is stored at the root of the tree; thecost of this node is computed using NML, eq.
1.Next, we try to split this node, by finding sucha context that if we query the values of the featurein that context, it will help us reduce the entropyin this count vector.
We check in turn all possi-ble candidate contexts, (L,P, F ), and choose thebest one.
Each candidate refers to some symbolfound on the source (?)
or the target (? )
level, atsome relative position P , and to one of that sym-bol?s features F .
We will condition the split onthe possible values of F .
For each candidate, wetry to split on its feature?s values, and collect theresulting alignment counts.Suppose one such candidate is (?, ?V, H),i.e., (source-level, previous vowel, Horizontal fea-ture), and suppose that the H-feature has two val-ues: front/back.
The vector at the root node (re-call, this tree is for the X-feature) would then splitinto two vectors, e.g.
:value of X: + ?X | H=front 1000 1X | H=back 1 1001This would likely be a very good split, sinceit reduces the entropy of the distribution in eachrow almost to zero.
The criterion that guides thechoice of the best candidate to use for splitting anode is the sum of the code lengths of the resultingsplit vectors, and the code length is proportionalto the entropy.We go through all candidates exhaustively, andgreedily choose the one that yields the greatest re-duction in entropy, and drop in cost.
We proceedrecursively down the tree, trying to split nodes,and stop when the total tree cost stops decreasing.This completes the tree for feature X on level ?.We build trees for all features and levels similarly,from the current alignment of the complete data.We augment the set of possible values at ev-ery node with two additional special branches: 6=,meaning the symbol at the queried position is ofthe wrong type and does not have the queried fea-ture, and #, meaning the query ran past the be-ginning of the word.020406080100120140160  0500100015002000250030003500Compressed size x1000 bitsDatasize:number ofwordpairs(average word-length: 5.5 bytes)Gzip Bzip2 1-1 model2-2 modelContext modelFigure 4: Comparison of compression power: Finnish-Estonian data from SSA, using the context model vs.the baseline models and standard compressors.5 Evaluation and ResultsOne way to evaluate the presented models wouldrequire a gold-standard aligned corpus; the mod-els produce alignments which could be comparedto the gold-standard alignments, and we couldmeasure performance quantitatively, e.g., in termsof accuracy.
However, building a gold-standardaligned corpus for the Uralic data proved to beextremely difficult.
In fact, it quickly becomesclear that this problem is at least as difficult asbuilding a full reconstruction for all internal nodesin the family tree (and probably harder), since itrequires full knowledge of all sound correspon-dences within the family.
It is also compoundedby the problem that the word-forms in the corpusmay contain morphological material that is ety-mologically unrelated: some databases give ?dic-tionary?
forms, which contain extraneous affixes,and thereby obscure which parts of a given wordform stand in etymological relationship with othermembers in the cognates set, and which do not.We therefore introduce other methods to evaluatethe models.Compression: In figure 4, we compare thecontext model, and use as baselines the standarddata compressors, Gzip and Bzip, as well as themore basic models presented in (Wettig et al,2011), (labeled ?1x1 and ?2x2?).
We test thecompression of up to 3200 Finnish-Estonian wordpairs, from SSA.
Gzip and Bzip compress data113fin khn kom man mar mrd saa udm ugrest 0.26 0.66 0.64 0.65 0.61 0.57 0.57 0.62 0.62fin 0.63 0.64 0.65 0.59 0.56 0.50 0.62 0.63khn 0.65 0.58 0.69 0.64 0.67 0.66 0.66kom 0.63 0.68 0.66 0.70 0.39 0.66man 0.68 0.65 0.72 0.62 0.62mar 0.65 0.69 0.65 0.66mrd 0.58 0.66 0.63saa 0.67 0.70udm 0.65Table 1: Pairwise normalized edit distances for Finno-Ugric languages, on StarLing data (symmetrized byaveraging over the two directions of imputation).by finding regularities in it (i.e., frequent sub-strings).
The comparison with Gzip is a ?san-ity check?
: we would like to confirm whetherour models find more regularity in the data thanwould an off-the-shelf data compressor, that hasno knowledge that the words in the data are ety-mologically related.
Of course, our models knowthat they should align pairs of consecutive lines.This test shows that learning about the ?vertical?correspondences achieves much better compres-sion rates?allows the models to extract greaterregularity from the data.Figure 5: Part of a tree, showing the rule for voicing ofmedial plosives in Estonian, conditioned on Finnish.Rules of correspondence: One our main goalsis to model rules of correspondence among lan-guages.
We can evaluate the models based on howgood they are at discovering rules.
(Wettig et al,2011) showed that aligning multiple symbols cap-tures some of the context and thereby finds morecomplex rules than their 1-1 alignment model.However, certain alignments, such as t?t/d,p?p/b, and k?k/g between Finnish and Esto-nian, cannot be explained by the multiple-symbolmodel.
This is due to the rule of voicing ofword-medial plosives in Estonian.
This rule couldbe expressed in terms of Two-level Morphol-ogy, (Koskenniemi, 1983) as: a voiceless plosivein Finnish, may correspond to voiced in Esto-nian, if not word-initial.3 The context modelfinds this rule, shown in Fig.
5.
This tree codesthe Target-level (i.e., Estonian) Voiced consonantfeature.
In each node, the counts of correspond-ing feature values are shown in brackets.
Inthe root node?prior to knowing anything aboutthe environment?there is almost complete un-certainty (i.e., high entropy) about the value ofVoiced feature of an Estonian consonant: 821voiceless to 801 voiced in our data.
Redder nodesindicate higher entropy, bluer nodes?lower en-tropy.
The query in the root node tells us to checkthe context Finnish Itself Voiced for the most in-formative clue about whether the current Estonianconsonant is voiced or not.
Tracing the optionsdown left to right from the root, we obtain therules.
The leftmost branch says, if the Finnishis voiced (?
), then the Estonian is almost cer-tainly voiced as well?615 voiced to 2 voicelessin this case.
If the Finnish is voiceless (FinnishItself Voiced = 	), it says voicing may occur, butonly in the red nodes?i.e., only if preceded bya voiced consonant on Estonian level (the branchmarked by ?, 56 cases), or?if previous posi-tion is not a consonant (the 6= branch indicatesthat the candidate?s query does not apply: i.e., thesound found in that position is not a consonant)?it can be voiced only if the corresponding Finnishis a plosive (P, 78 cases).
The blue nodes in thisbranch say that otherwise, the Estonian consonantalmost certainly remains voiceless.The context models discover numerous com-plex rules for different language pairs.
For ex-ample, they learn a rule that initial Finnish k?changes?
(corresponds) to h in Hungarian, if itis followed by a back vowel; the correspondencebetween Komi trills and Udmurt sibilants; etc.Imputation: We introduce a novel test of thequality of the models, by using them to imputeunseen data, as follows.
For a given model,and a language pair (L1, L2)?e.g., (Finnish,Estonian)?hold out one word pair, and train themodel on the remaining data.
Then show themodel the hidden Finnish word and let it guess3In fact, phonetically, in modern spoken Estonian, theconsonants that are written using the symbols b,d,g are nottechnically voiced, but that is a finer point, we use this rulefor illustration of the principle.114the corresponding Estonian.
Imputation can bedone for all models with a simple dynamic pro-gramming algorithm, similar to the Viterbi-likesearch used during training.
Formally, given thehidden Finnish string, the imputation procedureselects from all possible Estonian strings the mostprobable Estonian string, given the model.
Wethen compute an edit distance between the im-puted sting and the true withheld Estonian word(e.g., using the Levenshtein distance).
We repeatthis procedure for all word pairs in the (L1, L2)data set, sum the edit distances and normalize bythe total size of the (true) L2 data?this yields theNormalized Edit Distance NED(L2|L1,M) be-tween L1 and L2, under model M .Imputation is a more intuitive measure of themodel?s quality than code length, with a clearpractical interpretation.
NED is also the ultimatetest of the model?s quality.
If model M im-putes better than M ?
?i.e., NED(L2|L1,M) <NED(L2|L1,M ?
)?then it is difficult to arguethat M could be in any sense ?worse?
than M ?
?it has learned more about the regularities betweenL1 and L2, and it knows more about L2 givenL1.
The context model, which has much lowercost than the baseline, almost always has lowerNED.
This also yields an important insight: itis an encouraging indication that optimizing thecode length is a good approach?the algorithmdoes not optimize NED directly, and yet the costcorrelates strongly with NED, which is a simpleand intuitive measure of the model?s quality.6 DiscussionWe have presented a novel feature-based context-aware MDL model, and a comparison of its per-formance against prior models for the task ofalignment of etymological data.
We have eval-uated the models by examining the the rules ofcorrespondence that they discovers, by comparingcompression cost, imputation power and languagedistances induced by the imputation.
The modelstake only the etymological data set as input, andrequire no further linguistic assumptions.
In thisregard, they is as objective as possible, given thedata.
The data set itself, of course, may be highlysubjective and questionable.The objectivity of models given the data nowopens new possibilities for comparing entire datasets.
For example, we can begin to compare theFinnish and Estonian datasets in SSA vs. Star-Ling, although the data sets have quite differentcharacteristics, e.g., different size?3200 vs. 800word pairs, respectively?and the comparison isdone impartially, relying solely on the data pro-vided.
Another direct consequence of the pre-sented methods is that they enable us to quantifyuncertainty of entries in the corpus of etymologi-cal data.
For example, for a given entry x in lan-guage L1, we can compute exactly the probabil-ity that x would be imputed by any of the models,trained on all the remaining data from L1 plus anyother set of languages in the family.
This can beapplied equally to any entry, in particular to en-tries marked dubious by the database creators.We can use this method to approach the ques-tion of comparison of ?competing?
etymologicaldatasets.
The cost of an optimal alignment ob-tained over a given data set serves as a measure ofits internal consistency.We are currently working to combine the con-text model with 3- and higher-dimensional mod-els, and to extend these models to perform di-achronic imputation, i.e., reconstruction of proto-forms.
We also intend to test the models ondatabases of other language families.AcknowledgmentsWe are very grateful to the anonymous reviewersfor their thoughtful and helpful comments.
Wethank Suvi Hiltunen for the implementation of themodels, and Arto Vihavainen for implementingsome of the earlier models.
This research wassupported by the Uralink Project, funded by theAcademy of Finland and by the Russian Fund forthe Humanities.ReferencesRaimo Anttila.
1989.
Historical and comparative lin-guistics.
John Benjamins.Franc?ois G. Barbanc?on, Tandy Warnow, Don Ringe,Steven N. Evans, and Luay Nakhleh.
2009.
An ex-perimental study comparing linguistic phylogeneticreconstruction methods.
In Proceedings of the Con-ference on Languages and Genes, UC Santa Bar-bara.
Cambridge University Press.Tugba Bodrumlu, Kevin Knight, and Sujith Ravi.2009.
A new objective function for word alignment.In Proc.
NAACL Workshop on Integer Linear Pro-gramming for NLP.Alexandre Bouchard-Co?te?, Percy Liang, Thomas Grif-fiths, and Dan Klein.
2007.
A probabilistic ap-115proach to diachronic phonology.
In Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 887?896, Prague, June.Peter F. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert.
L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.Peter Gru?nwald.
2007.
The Minimum DescriptionLength Principle.
MIT Press.Erkki Itkonen and Ulla-Maija Kulonen.
2000.Suomen Sanojen Alkupera?
(The Origin of FinnishWords).
Suomalaisen Kirjallisuuden Seura,Helsinki, Finland.Brett Kessler.
2001.
The Significance of Word Lists:Statistical Tests for Investigating Historical Con-nections Between Languages.
The University ofChicago Press, Stanford, CA.Grzegorz Kondrak.
2002.
Determining recur-rent sound correspondences by inducing translationmodels.
In Proceedings of COLING 2002: 19th In-ternational Conference on Computational Linguis-tics, pages 488?494, Taipei, August.Grzegorz Kondrak.
2003.
Identifying complex soundcorrespondences in bilingual wordlists.
In A. Gel-bukh, editor, Computational Linguistics and Intel-ligent Text Processing (CICLing-2003), pages 432?443, Mexico City, February.
Springer-Verlag Lec-ture Notes in Computer Science, No.
2588.Grzegorz Kondrak.
2004.
Combining evidence incognate identification.
In Proceedings of the Sev-enteenth Canadian Conference on Artificial Intelli-gence (Canadian AI 2004), pages 44?59, London,Ontario, May.
Lecture Notes in Computer Science3060, Springer-Verlag.Petri Kontkanen and Petri Myllyma?ki.
2007.
Alinear-time algorithm for computing the multino-mial stochastic complexity.
Information ProcessingLetters, 103(6):227?233.Kimmo Koskenniemi.
1983.
Two-level morphol-ogy: A general computational model for word-formrecognition and production.
Ph.D. thesis, Univer-sity of Helsinki, Finland.I.
Dan Melamed.
1997.
Automatic discovery of non-compositional compounds in parallel data.
In TheSecond Conference on Empirical Methods in Nat-ural Language Processing, pages 97?108, Hissar,Bulgaria.I.
Dan Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2):221?249.Luay Nakhleh, Don Ringe, and Tandy Warnow.
2005.Perfect phylogenetic networks: A new methodol-ogy for reconstructing the evolutionary history ofnatural languages.
Language (Journal of the Lin-guistic Society of America), 81(2):382?420.Ka?roly Re?dei.
1988?1991.
Uralisches etymologischesWo?rterbuch.
Harrassowitz, Wiesbaden.Don Ringe, Tandy Warnow, and A. Taylor.
2002.Indo-European and computational cladis-tics.
Transactions of the Philological Society,100(1):59?129.Jorma Rissanen.
1996.
Fisher information andstochastic complexity.
IEEE Transactions on Infor-mation Theory, 42(1):40?47, January.Yuri M. Shtarkov.
1987.
Universal sequential codingof single messages.
Problems of Information Trans-mission, 23:3?17.Sergei A. Starostin.
2005.
Tower of babel: Etymolog-ical databases.
http://newstar.rinet.ru/.Stephan Vogel, Hermann Ney, and Christoph Till-mann.
1996.
HMM-based word alignment in sta-tistical translation.
In Proceedings of 16th Confer-ence on Computational Linguistics (COLING 96),Copenhagen, Denmark, August.Hannes Wettig, Suvi Hiltunen, and Roman Yangarber.2011.
MDL-based Models for Alignment of Et-ymological Data.
In Proceedings of RANLP: the8th Conference on Recent Advances in Natural Lan-guage Processing, Hissar, Bulgaria.116
