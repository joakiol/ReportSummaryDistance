Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 65?72,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Bayesian Model for Discovering Typological ImplicationsHal Daume?
IIISchool of ComputingUniversity of Utahme@hal3.nameLyle CampbellDepartment of LinguisticsUniversity of Utahlcampbel@hum.utah.eduAbstractA standard form of analysis for linguis-tic typology is the universal implication.These implications state facts about therange of extant languages, such as ?if ob-jects come after verbs, then adjectives comeafter nouns.?
Such implications are typi-cally discovered by painstaking hand anal-ysis over a small sample of languages.
Wepropose a computational model for assist-ing at this process.
Our model is able todiscover both well-known implications aswell as some novel implications that deservefurther study.
Moreover, through a carefulapplication of hierarchical analysis, we areable to cope with the well-known samplingproblem: languages are not independent.1 IntroductionLinguistic typology aims to distinguish between log-ically possible languages and actually observed lan-guages.
A fundamental building block for such anunderstanding is the universal implication (Green-berg, 1963).
These are short statements that restrictthe space of languages in a concrete way (for in-stance ?object-verb ordering implies adjective-nounordering?
); Croft (2003), Hawkins (1983) and Song(2001) provide excellent introductions to linguistictypology.
We present a statistical model for auto-matically discovering such implications from a largetypological database (Haspelmath et al, 2005).Analyses of universal implications are typicallyperformed by linguists, inspecting an array of 30-100 languages and a few pairs of features.
Lookingat all pairs of features (typically several hundred) isvirtually impossible by hand.
Moreover, it is insuf-ficient to simply look at counts.
For instance, resultspresented in the form ?verb precedes object impliesprepositions in 16/19 languages?
are nonconclusive.While compelling, this is not enough evidence to de-cide if this is a statistically well-founded implica-tion.
For one, maybe 99% of languages have prepo-sitions: then the fact that we?ve achieved a rate of84% actually seems really bad.
Moreover, if the 16languages are highly related historically or areally(geographically), and the other 3 are not, then wemay have only learned something about geography.In this work, we propose a statistical model thatdeals cleanly with these difficulties.
By building acomputational model, it is possible to apply it toa very large typological database and search overmany thousands of pairs of features.
Our modelhinges on two novel components: a statistical noisemodel a hierarchical inference over language fam-ilies.
To our knowledge, there is no prior workdirectly in this area.
The closest work is repre-sented by the books Possible and Probable Lan-guages (Newmeyer, 2005) and Language Classifica-tion by Numbers (McMahon and McMahon, 2005),but the focus of these books is on automatically dis-covering phylogenetic trees for languages based onIndo-European cognate sets (Dyen et al, 1992).2 DataThe database on which we perform our analysis isthe World Atlas of Language Structures (Haspel-math et al, 2005).
This database contains infor-mation about 2150 languages (sampled from acrossthe world; Figure 1 depicts the locations of lan-65Numeral Glottalized Number ofLanguage Classifiers Rel/N Order O/V Order Consonants Tone GendersEnglish Absent NRel VO None None ThreeHindi Absent RelN OV None None TwoMandarin Obligatory RelN VO None Complex NoneRussian Absent NRel VO None None ThreeTukang Besi Absent ?
Either Implosives None ThreeZulu Absent NRel VO Ejectives Simple Five+Table 1: Example database entries for a selection of diverse languages and features.
?150 ?100 ?50 0 50 100 150?40?200204060Figure 1: Map of the 2150 languages in the database.guages).
There are 139 features in this database,broken down into categories such as ?Nominal Cate-gories,?
?Simple Clauses,?
?Phonology,?
?Word Or-der,?
etc.
The database is sparse: for many lan-guage/feature pairs, the feature value is unknown.
Infact, only about 16% of all possible language/featurepairs are known.
A sample of five languages and sixfeatures from the database are shown in Table 1.Importantly, the density of samples is not random.For certain languages (eg., English, Chinese, Rus-sian), nearly all features are known, whereas otherlanguages (eg., Asturian, Omagua, Frisian) that havefewer than five feature values known.
Furthermore,some features are known for many languages.
Thisis due to the fact that certain features take less effortto identify than others.
Identifying, for instance, ifa language has a particular set of phonological fea-tures (such as glottalized consonants) requires onlylistening to speakers.
Other features, such as deter-mining the order of relative clauses and nouns re-quire understanding much more of the language.3 ModelsIn this section, we propose two models for automat-ically uncovering universal implications from noisy,sparse data.
First, note that even well attested impli-cations are not always exceptionless.
A common ex-ample is that verbs preceding objects (?VO?)
impliesadjectives following nouns (?NA?).
This implication(VO ?
NA) has one glaring exception: English.This is one particular form of noise.
Another sourceof noise stems from transcription.
WALS containsdata about languages documented by field linguistsas early as the 1900s.
Much of this older data wascollected before there was significant agreement indocumentation style.
Different field linguists of-ten had different dimensions along which they seg-mented language features into classes.
This leads tonoise in the properties of individual languages.Another difficulty stems from the sampling prob-lem.
This is a well-documented issue (see, eg.,(Croft, 2003)) stemming from the fact that any set oflanguages is not sampled uniformly from the spaceof all probable languages.
Politically interestinglanguages (eg., Indo-European) and typologicallyunusual languages (eg., Dyirbal) are better docu-mented than others.
Moreover, languages are not in-dependent: German and Dutch are more similar thanGerman and Hindi due to history and geography.The first model, FLAT, treats each language as in-dependent.
It is thus susceptible to sampling prob-lems.
For instance, the WALS database contains ahalf dozen versions of German.
The FLAT modelconsiders these versions of German just as statisti-cally independent as, say, German and Hindi.
Tocope with this problem, we then augment the FLATmodel into a HIERarchical model that takes advan-tage of known hierarchies in linguistic phylogenet-ics.
The HIER model explicitly models the fact thatindividual languages are not independent and exhibitstrong familial dependencies.
In both models, weinitially restrict our attention to pairs of features.
Wewill describe our models as if all features are binary.We expand any multi-valued feature with K valuesinto K binary features in a ?one versus rest?
manner.3.1 The FLAT ModelIn the FLAT model, we consider a 2 ?
N matrix offeature values.
The N corresponds to the number oflanguages, while the 2 corresponds to the two fea-tures currently under consideration (eg., object/verborder and noun/adjective order).
The order of the66two features is important: f1 implies f2 is logicallydifferent from f2 implies f1.
Some of the entries inthe matrix will be unknown.
We may safely removeall languages from consideration for which both areunknown, but we do not remove languages for whichonly one is unknown.
We do so because our modelneeds to capture the fact that if f2 is always true,then f1 ?
f2 is uninteresting.The statistical model is set up as follows.
There isa single variable (we will denote this variable ?m?
)corresponding to whether the implication holds.Thus, m = 1 means that f1 implies f2 and m = 0means that it does not.
Independent of m, we specifytwo feature priors, ?1 and ?2 for f1 and f2 respec-tively.
?1 specifies the prior probability that f1 willbe true, and ?2 specifies the prior probability that f2will be true.
One can then put the model togetherna?
?vely as follows.
If m = 0 (i.e., the implicationdoes not hold), then the entire data matrix is gener-ated by choosing values for f1 (resp., f2) indepen-dently according to the prior probability ?1 (resp.,?2).
On the other hand, if m = 1 (i.e., the impli-cation does hold), then the first column of the datamatrix is generated by choosing values for f1 inde-pendently by ?1, but the second column is generateddifferently.
In particular, if for a particular language,we have that f1 is true, then the fact that the implica-tion holds means that f2 must be true.
On the otherhand, if f1 is false for a particular language, then wemay generate f2 according to the prior probability?2.
Thus, having m = 1 means that the model issignificantly more constrained.
In equations:p(f1 | pi1) = pif11 (1 ?
pi1)1?f1p(f2 | f1, pi2,m) =?f2 m = f1 = 1pif22 (1 ?
pi2)1?f2 otherwiseThe problem with this na?
?ve model is that it doesnot take into account the fact that there is ?noise?in the data.
(By noise, we refer either to mis-annotations, or to ?strange?
languages like English.
)To account for this, we introduce a simple noisemodel.
There are several options for parameteriz-ing the noise, depending on what independence as-sumptions we wish to make.
One could simply spec-ify a noise rate for the entire data set.
One couldalternatively specify a language-specific noise rate.Or one could specify a feature-specific noise rate.We opt for a blend between the first and second op-Figure 2: Graphical model for the FLAT model.tion.
We assume an underlying noise rate for the en-tire data set, but that, conditioned on this underlyingrate, there is a language-specific noise level.
We be-lieve this to be an appropriate noise model because itmodels the fact that the majority of information fora single language is from a single source.
Thus, ifthere is an error in the database, it is more likely thatother errors will be for the same languages.In order to model this statistically, we assume thatthere are latent variables e1,n and e2,n for each lan-guage n. If e1,n = 1, then the first feature for lan-guage n is wrong.
Similarly, if e2,n = 1, then thesecond feature for language n is wrong.
Given thismodel, the probabilities are exactly as in the na?
?vemodel, with the exception that instead of using f1(resp., f2), we use the exclusive-or1 f1 ?
e1 (resp.,f2 ?
e2) so that the feature values are flipped when-ever the noise model suggests an error.The graphical model for the FLAT model is shownin Figure 2.
Circular nodes denote random variablesand arrows denote conditional dependencies.
Therectangular plate denotes the fact that the elementscontained within it are replicated N times (N is thenumber of languages).
In this model, there are four?root?
nodes: the implication value m; the two fea-ture prior probabilities ?1 and ?2; and the language-specific error rate ?.
On all of these nodes we placeBayesian priors.
Since m is a binary random vari-able, we place a Bernoulli prior on it.
The ?s areBernoulli random variables, so they are given inde-pendent Beta priors.
Finally, the noise rate ?
is alsogiven a Beta prior.
For the two Beta parameters gov-erning the error rate (i.e., a?
and b?)
we set these byhand so that the mean expected error rate is 5% andthe probability of the error rate being between 0%and 10% is 50% (this number is based on an expertopinion of the noise-rate in the data).
For the rest of1The exclusive-or of a and b, written a ?
b, is true exactlywhen either a or b is true but not both.67the parameters we use uniform priors.3.2 The HIER ModelA significant difficulty in working with any large ty-pological database is that the languages will be sam-pled nonuniformly.
In our case, this means that im-plications that seem true in the FLAT model mayonly be true for, say, Indo-European, and the remain-ing languages are considered noise.
While this maybe interesting in its own right, we are more interestedin discovering implications that are truly universal.We model this using a hierarchical Bayesianmodel.
In essence, we take the FLAT model andbuild a notion of language relatedness into it.
Inparticular, we enforce a hierarchy on the m impli-cation variables.
For simplicity, suppose that our?hierarchy?
of languages is nearly flat.
Of the Nlanguages, half of them are Indo-European and theother half are Austronesian.
We will use a nearlyidentical model to the FLAT model, but instead ofhaving a single m variable, we have three: one forIE, one for Austronesian and one for ?all languages.
?For a general tree, we assign one implication vari-able for each node (including the root and leaves).The goal of the inference is to infer the value of them variable corresponding to the root of the tree.All that is left to specify the full HIER modelis to specify the probability distribution of the mrandom variables.
We do this as follows.
Weplace a zero mean Gaussian prior with (unknown)variance ?2 on the root m. Then, for a non-rootnode, we use a Gaussian with mean equal to the?m?
value of the parent and tied variance ?2.
Inour three-node example, this means that the root isdistributed Nor(0, ?2) and each child is distributedNor(mroot, ?2), where mroot is the random variablecorresponding to the root.
Finally, the leaves (cor-responding to the languages themselves) are dis-tributed logistic-binomial.
Thus, the m random vari-able corresponding to a leaf (language) is distributedBin(s(mpar)), where mpar is the m value for the par-ent (internal) node and s is the sigmoid functions(x) = [1 + exp(?x)]?1.The intuition behind this model is that the m valueat each node in the tree (where a node is either ?alllanguages?
or a specific language family or an in-dividual language) specifies the extent to which theimplication under consideration holds for that node.A large positive m means that the implication is verylikely to hold.
A large negative value means it isvery likely to not hold.
The normal distributionsacross edges in the tree indicate that we expect them values not to change too much across the tree.
Atthe leaves (i.e., individual languages), the logistic-binomial simply transforms the real-valued ms intothe range [0, 1] so as to make an appropriate input tothe binomial distribution.4 Statistical InferenceIn this section, we describe how we use Markovchain Monte Carlo methods to perform inferencein the statistical models described in the previoussection; Andrieu et al (2003) provide an excel-lent introduction to MCMC techniques.
The keyidea behind MCMC techniques is to approximate in-tractable expectations by drawing random samplesfrom the probability distribution of interest.
The ex-pectation can then be approximated by an empiricalexpectation over these sample.For the FLAT model, we use a combination ofGibbs sampling with rejection sampling as a sub-routine.
Essentially, all sampling steps are standardGibbs steps, except for sampling the error rates e.The Gibbs step is not available analytically for these.Hence, we use rejection sampling (drawing from theBeta prior and accepting according to the posterior).The sampling procedure for the HIER model isonly slightly more complicated.
Instead of perform-ing a simple Gibbs sample for m in Step (4), wefirst sample the m values for the internal nodes us-ing simple Gibbs updates.
For the leaf nodes, weuse rejection sampling.
For this rejection, we drawproposal values from the Gaussian specified by theparent m, and compute acceptance probabilities.In all cases, we run the outer Gibbs sampler for1000 iterations and each rejection sampler for 20 it-erations.
We compute the marginal values for the mimplication variables by averaging the sampled val-ues after dropping 200 ?burn-in?
iterations.5 Data Preprocessing and SearchAfter extracting the raw data from the WALS elec-tronic database (Haspelmath et al, 2005)2, we per-form a minor amount of preprocessing.
Essen-tially, we have manually removed certain feature2This is nontrivial?we are currently exploring the possibil-ity of freely sharing these data.68values from the database because they are underrep-resented.
For instance, the ?Glottalized Consonants?feature has eight possible values (one for ?none?and seven for different varieties of glottalized conso-nants).
We reduce this to simply two values ?has?
or?has not.?
313 languages have no glottalized conso-nants and 139 have some variety of glottalized con-sonant.
We have done something similar with ap-proximately twenty of the features.For the HIER model, we obtain the hierarchy inone of two ways.
The first hierarchy we use is the?linguistic hierarchy?
specified as part of the WALSdata.
This hierarchy divides languages into familiesand subfamilies.
This leads to a tree with the leavesat depth four.
The root has 38 immediate children(corresponding to the major families), and there area total of 314 internal nodes.
The second hierar-chy we use is an areal hierarchy obtained by clus-tering languages according to their latitude and lon-gitude.
For the clustering we first cluster all the lan-guages into 6 ?macro-clusters.?
We then cluster eachmacro-cluster individually into 25 ?micro-clusters.
?These micro-clusters then have the languages at theirleaves.
This yields a tree with 31 internal nodes.Given the database (which contains approxi-mately 140 features), performing a raw search evenover all possible pairs of features would lead to over19, 000 computations.
In order to reduce this spaceto a more manageable number, we filter:?
There must be at least 250 languages for which both fea-tures are known.?
There must be at least 15 languages for which both fea-ture values hold simultaneously.?
Whenever f1 is true, at least half of the languages alsohave f2 true.Performing all these filtration steps reduces thenumber of pairs under consideration to 3442.
Whilethis remains a computationally expensive procedure,we were able to perform all the implication compu-tations for these 3442 possible pairs in about a weekon a single modern machine (in Matlab).6 ResultsThe task of discovering universal implications is, atits heart, a data-mining task.
As such, it is difficultto evaluate, since we often do not know the correctanswers!
If our model only found well-documentedimplications, this would be interesting but uselessfrom the perspective of aiding linguists focus theirenergies on new, plausible implications.
In this sec-tion, we present the results of our method, togetherwith both a quantitative and qualitative evaluation.6.1 Quantitative EvaluationIn this section, we perform a quantitative evaluationof the results based on predictive power.
That is,one generally would prefer a system that finds im-plications that hold with high probability across thedata.
The word ?generally?
is important: this qual-ity is neither necessary nor sufficient for the modelto be good.
For instance, finding 1000 implicationsof the form A1 ?
X,A2 ?
X, .
.
.
, A1000 ?
X iscompletely uninteresting if X is true in 99% of thecases.
Similarly, suppose that a model can find 1000implications of the form X ?
A1, .
.
.
, X ?
A1000,but X is only true in five languages.
In both of thesecases, according to a ?predictive power?
measure,these would be ideal systems.
But they are bothsomewhat uninteresting.Despite these difficulties with a predictive power-based evaluation, we feel that it is a good way to un-derstand the relative merits of our different models.Thus, we compare the following systems: FLAT (ourproposed flat model), LINGHIER (our model usingthe phylogenetic hierarchy), DISTHIER (our modelusing the areal hierarchy) and RANDOM (a modelthat ranks implications?that meet the three qualifi-cations from the previous section?randomly).The models are scored as follows.
We take theentire WALS data set and ?hide?
a random 10%of the entries.
We then perform full inference andask the inferred model to predict the missing val-ues.
The accuracy of the model is the accuracy ofits predictions.
To obtain a sense of the quality ofthe ranking, we perform this computation on thetop k ranked implications provided by each model;k ?
{2, 4, 8, .
.
.
, 512, 1024}.The results of this quantitative evaluation areshown in Figure 3 (on a log-scale for the x-axis).The two best-performing models are the two hier-archical models.
The flat model does significantlyworse and the random model does terribly.
The ver-tical lines are a standard deviation over 100 folds ofthe experiment (hiding a different 10% each time).The difference between the two hierarchical mod-els is typically not statistically significant.
At thetop of the ranking, the model based on phylogenetic690 1 2 3 4 5 6 7 8 9 100.650.70.750.80.850.90.951Number of Implications (log2)PredictionAccuracyLingHier DistHier Flat RandomFigure 3: Results of quantitative (predictive) evalua-tion.
Top curves are the hierarchical models; middleis the flat model; bottom is the random baseline.information performs marginally better; at the bot-tom of the ranking, the order flips.
Comparing thehierarchical models to the flat model, we see thatadequately modeling the a priori similarity betweenlanguages is quite important.6.2 Cross-model ComparisonThe results in the previous section support the con-clusion that the two hierarchical models are doingsomething significantly different (and better) thanthe flat model.
This clearly must be the case.
Theresults, however, do not say whether the two hierar-chies are substantially different.
Moreover, are theresults that they produce substantially different.
Theanswer to these two questions is ?yes.
?We first address the issue of tree similarity.
Weconsider all pairs of languages which are at distance0 in the areal tree (i.e., have the same parent).
Wethen look at the mean tree-distance between thoselanguages in the phylogenetic tree.
We do this for alldistances in the areal tree (because of its construc-tion, there are only three: 0, 2 and 4).
The meandistances in the phylogenetic tree corresponding tothese three distances in the areal tree are: 2.9, 3.5and 4.0, respectively.
This means that languages thatare ?nearby?
in the areal tree are quite often very farapart in the phylogenetic tree.To answer the issue of whether the results ob-tained by the two trees are similar, we employKendall?s ?
statistic.
Given two ordered lists, the?
statistic computes how correlated they are.
?
isalways between 0 and 1, with 1 indicating identicalordering and 0 indicated completely reversed order-ing.
The results are as follows.
Comparing FLATto LINGHIER yield ?
= 0.4144, a very low correla-tion.
Between FLAT and DISTHIER, ?
= 0.5213,also very low.
These two are as expected.
Fi-nally, between LINGHIER and DISTHIER, we ob-tain ?
= 0.5369, a very low correlation, consideringthat both perform well predictively.6.3 Qualitative AnalysisFor the purpose of a qualitative analysis, we re-produce the top 30 implications discovered by theLINGHIER model in Table 2 (see the final page).3Each implication is numbered, then the actual im-plication is presented.
For instance, #7 says thatany language that has adjectives preceding theirgoverning nouns also has numerals preceding theirnouns.
We additionally provide an ?analysis?
ofmany of these discovered implications.
Many ofthem (eg., #7) are well known in the typological lit-erature.
These are simply numbered according towell-known references.
For instance our #7 is im-plication #18 from Greenberg, reproduced by Song(2001).
Those that reference Hawkins (eg., #11) arebased on implications described by Hawkins (1983);those that reference Lehmann are references to theprinciples decided by Lehmann (1981) in Ch 4 & 8.Some of the implications our model discoversare obtained by composition of well-known implica-tions.
For instance, our #3 (namely, OV ?
Genitive-Noun) can be obtained by combining Greenberg #4(OV ?
Postpositions) and Greenberg #2a (Postpo-sitions ?
Genitive-Noun).
It is quite encouragingthat 14 of our top 21 discovered implications arewell-known in the literature (and this, not even con-sidering the tautalogically true implications)!
Thisstrongly suggests that our model is doing somethingreasonable and that there is true structure in the data.In addition to many of the known implicationsfound by our model, there are many that are ?un-known.?
Space precludes attempting explanationsof them all, so we focus on a few.
Some are easy.Consider #8 (Strongly suffixing ?
Tense-aspect suf-fixes): this is quite plausible?if you have a lan-3In truth, our model discovers several tautalogical implica-tions that we have removed by hand before presentation.
Theseare examples like ?SVO ?
VO?
or ?No unusual consonants ?no glottalized consonants.?
It is, of course, good that our modeldiscovers these, since they are obviously true.
However, to savespace, we have withheld them from presentation here.
The 30thimplication presented here is actually the 83rd in our full list.70guage that tends to have suffixes, it will probablyhave suffixes for tense/aspect.
Similarly, #10 statesthat languages with verb morphology for questionslack question particles; again, this can be easily ex-plained by an appeal to economy.Some of the discovered implications require amore involved explanation.
One such example is#20: labial-velars implies no uvulars.4 It turns outthat labial-velars are most common in Africa justnorth of the equator, which is also a place that hasvery few uvulars (there are a handful of other ex-amples, mostly in Papua New Guinea).
While thisimplication has not been previously investigated, itmakes some sense: if a language has one form ofrare consonant, it is unlikely to have another.As another example, consider #28: Obligatorysuffix pronouns implies no possessive affixes.
Thismeans is that in languages (like English) for whichpro-drop is impossible, possession is not markedmorphologically on the head noun (like English,?book?
appears the same regarless of if it is ?hisbook?
or ?the book?).
This also makes sense: if youcannot drop pronouns, then one usually will markpossession on the pronoun, not the head noun.
Thus,you do not need marking on the head noun.Finally, consider #25: High and mid front vowels(i.e., / u/, etc.)
implies large vowel inventory (?
7vowels).
This is supported by typological evidencethat high and mid front vowels are the ?last?
vowelsto be added to a language?s repertoire.
Thus, in orderto get them, you must also have many other types ofvowels already, leading to a large vowel inventory.Not all examples admit a simple explanation andare worthy of further thought.
Some of which (likethe ones predicated on ?SV?)
may just be peculiar-ities of the annotation style: the subject verb orderchanges frequently between transitive and intransi-tive usages in many languages, and the annotationreflects just one.
Some others are bizzarre: why nothaving fricatives should mean that you don?t havetones (#27) is not a priori clear.6.4 Multi-conditional ImplicationsMany implications in the literature have multipleimplicants.
For instance, much research has gone4Labial-velars and uvulars are rare consonants (order 100languages).
Labial-velars are joined sounds like /kp/ and /gb/(to English speakers, sounding like chicken noises); uvularssounds are made in the back of the throat, like snoring.Implicants ImplicandPostpositions ?
Demonstrative-NounAdjective-NounPosessive prefixes ?
Genitive-NounTense-aspect suffixesCase suffixes ?
Genitive-NounPlural suffixAdjective-Noun ?
OVGenitive-NounHigh cons/vowel ratio ?
No tonesNo front-rounded vowelsNegative affix ?
OVGenitive-NounNo front-rounded vowels ?
Large vowel quality inventoryLabial velarsSubordinating suffix ?
PostpositionsTense-aspect suffixesNo case affixes ?
Initial subordinator wordPrepositionsStrongly suffixing ?
Genitive-NounPlural suffixTable 3: Top implications discovered by theLINGHIER multi-conditional model.into looking at which implications hold, consideringonly ?VO?
languages, or considering only languageswith prepositions.
It is straightforward to modifyour model so that it searches over triples of features,conditioning on two and predicting the third.
Spaceprecludes an in-depth discussion of these results, butwe present the top examples in Table 3 (after remov-ing the tautalogically true examples, which are morenumerous in this case, as well as examples that aredirectly obtainable from Table 2).
It is encouragingthat in the top 1000 multi-conditional implicationsfound, the most frequently used were ?OV?
(176times) ?Postpositions?
(157 times) and ?Adjective-Noun?
(89 times).
This result agrees with intuition.7 DiscussionWe have presented a Bayesian model for discoveringuniversal linguistic implications from a typologicaldatabase.
Our model is able to account for noise ina linguistically plausible manner.
Our hierarchicalmodels deal with the sampling issue in a unique way,by using prior knowledge about language families to?group?
related languages.
Quantitatively, the hier-archical information turns out to be quite useful, re-gardless of whether it is phylogenetically- or areally-based.
Qualitatively, our model can recover manywell-known implications as well as many more po-tential implications that can be the object of futurelinguistic study.
We believe that our model is suf-71# Implicant ?
Implicand Analysis1 Postpositions ?
Genitive-Noun Greenberg #2a2 OV ?
Postpositions Greenberg #43 OV ?
Genitive-Noun Greenberg #4 + Greenberg #2a4 Genitive-Noun ?
Postpositions Greenberg #2a (converse)5 Postpositions ?
OV Greenberg #2b (converse)6 SV ?
Genitive-Noun ??
?7 Adjective-Noun ?
Numeral-Noun Greenberg #188 Strongly suffixing ?
Tense-aspect suffixes Clear explanation9 VO ?
Noun-Relative Clause Lehmann10 Interrogative verb morph ?
No question particle Appeal to economy11 Numeral-Noun ?
Demonstrative-Noun Hawkins XVI (for postpositional languages)12 Prepositions ?
VO Greenberg #3 (converse)13 Adjective-Noun ?
Demonstrative-Noun Greenberg #1814 Noun-Adjective ?
Postpositions Lehmann15 SV ?
Postpositions ??
?16 VO ?
Prepositions Greenberg #317 Initial subordinator word ?
Prepositions Operator-operand principle (Lehmann)18 Strong prefixing ?
Prepositions Greenberg #27b19 Little affixation ?
Noun-Adjective ??
?20 Labial-velars ?
No uvular consonants See text21 Negative word ?
No pronominal possessive affixes See text22 Strong prefixing ?
VO Lehmann23 Subordinating suffix ?
Strongly suffixing ??
?24 Final subordinator word ?
Postpositions Operator-operand principle (Lehmann)25 High and mid front vowels ?
Large vowel inventories See text26 Plural prefix ?
Noun-Genitive ??
?27 No fricatives ?
No tones ??
?28 Obligatory subject pronouns ?
No pronominal possessive affixes See text29 Demonstrative-Noun ?
Tense-aspect suffixes Operator-operand principle (Lehmann)30 Prepositions ?
Noun-Relative clause Lehmann, HawkinsTable 2: Top 30 implications discovered by the LINGHIER model.ficiently general that it could be applied to manydifferent typological databases ?
we attempted notto ?overfit?
it to WALS.
Our hope is that the au-tomatic discovery of such implications not onlyaid typologically-inclined linguists, but also othergroups.
For instance, well-attested universal impli-cations have the potential to reduce the amount ofdata field linguists need to collect.
They have alsobeen used computationally to aid in the learning ofunsupervised part of speech taggers (Schone and Ju-rafsky, 2001).
Many extensions are possible to thismodel; for instance attempting to uncover typolog-ically hierarchies and other higher-order structures.We have made the full output of all models availableat http://hal3.name/WALS.Acknowledgments.
We are grateful to Yee WhyeTeh, Eric Xing and three anonymous reviewers fortheir feedback on this work.ReferencesChristophe Andrieu, Nando de Freitas, Arnaud Doucet, andMichael I. Jordan.
2003.
An introduction to MCMC formachine learning.
Machine Learning (ML), 50:5?43.William Croft.
2003.
Typology and Univerals.
CambridgeUniversity Press.Isidore Dyen, Joseph Kurskal, and Paul Black.
1992.
AnIndoeuropean classification: A lexicostatistical experiment.Transactions of the American Philosophical Society, 82(5).American Philosophical Society.Joseph Greenberg, editor.
1963.
Universals of Languages.MIT Press.Martin Haspelmath, Matthew Dryer, David Gil, and BernardComrie, editors.
2005.
The World Atlas of Language Struc-tures.
Oxford University Press.John A. Hawkins.
1983.
Word Order Universals: Quantitativeanalyses of linguistic structure.
Academic Press.Winfred Lehmann, editor.
1981.
Syntactic Typology, volumexiv.
University of Texas Press.April McMahon and Robert McMahon.
2005.
Language Clas-sification by Numbers.
Oxford University Press.Frederick J. Newmeyer.
2005.
Possible and Probable Lan-guages: A Generative Perspective on Linguistic Typology.Oxford University Press.Patrick Schone and Dan Jurafsky.
2001 Language IndependentInduction of Part of Speech Class Labels Using only Lan-guage Universals.
Machine Learning: Beyond Supervision.Jae Jung Song.
2001.
Linguistic Typology: Morphology andSyntax.
Longman Linguistics Library.72
