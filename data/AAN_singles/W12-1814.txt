NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 29?32,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsThe INPROTK 2012 ReleaseTimo BaumannDepartment for InformaticsUniversity of Hamburg, Germanybaumann@informatik.uni-hamburg.deDavid SchlangenFaculty of Linguistics and Literary StudiesBielefeld University, Germanydavid.schlangen@uni-bielefeld.deAbstractWe describe the 2012 release of our ?Incremen-tal Processing Toolkit?
(INPROTK)1, whichcombines a powerful and extensible architec-ture for incremental processing with compo-nents for incremental speech recognition and,new to this release, incremental speech syn-thesis.
These components work fairly domain-independently; we also provide example imple-mentations of higher-level components such asnatural language understanding and dialoguemanagement that are somewhat more tied to aparticular domain.
We offer this release of thetoolkit to foster research in this new and excit-ing area, which promises to help increase thenaturalness of behaviours that can be modelledin such systems.1 IntroductionAs recent work has shown, incremental (or online)processing of user input or generation of systemoutput enables spoken dialogue systems to producebehaviour that is perceived as more natural thanand preferable to that produced by systems that arebound by a turn-based processing mode (Aist etal., 2006; Skantze and Schlangen, 2009; Bu?
et al,2010; Skantze and Hjalmarsson, 2010).
There is stillmuch left to find out about the best ways of mod-elling these behaviours in such systems, however.To foster research in this area, we are releasing anew version of our ?Incremental Processing Toolkit?
(INPROTK), which provides lower-level components(such as speech recognition and speech synthesis,1The code of the toolkit and some example applicationshave been released as open-source at http://inprotk.sourceforge.net.but also a general modular processing architecture)and allows researchers to concentrate on higher-levelmodules (such as natural language understanding anddialogue modelling; for which we provide exampleimplementations).2 We describe these componentsin the following, pointing out the differences andextensions to earlier releases (Baumann et al, 2010).2 An Incremental Processing ArchitectureINPROTK realises the IU-model of incremental pro-cessing (Schlangen and Skantze, 2009; Schlangenand Skantze, 2011), where incremental systems areconceptualised as consisting of a network of pro-cessing modules.
Each module has a left buffer, aprocessor, and a right buffer, where the normal modeof processing is to take input from the left buffer, pro-cess it, and provide output in the right buffer, fromwhere it goes to the next module?s left buffer.
(Top-down, expectation-based processing would work inthe opposite direction.)
Modules exchange incremen-tal units (IUs), which are the smallest ?chunks?
ofinformation that can trigger connected modules intoaction.
IUs typically are part of larger units; e.g.,individual words as parts of an utterance, or frameelements as part of the representation of an utterancemeaning.
This relation of being part of the samelarger unit is recorded through same level links; theunits that were used in creating a given IU are linkedto it via grounded in links.
Modules have to be ableto react to three basic situations: that IUs are addedto a buffer, which triggers processing; that IUs thatwere erroneously hypothesised by an earlier module2An alternative to the toolkit described here is jindigo(Skantze and Hjalmarsson, 2010), http://www.jindigo.net.29are revoked, which may trigger a revision of a mod-ule?s own output; and that modules signal that theycommit to an IU, that is, won?t revoke it anymore (or,respectively, expect it to not be revoked anymore).INPROTK offers flexibility on how tightly orloosely modules are coupled in a system.
It pro-vides mechanisms for sending IU updates betweenprocesses via a light-weight remote procedure callprotocol,3 as well as for using shared memory withinone (Java) process.
INPROTK follows an event-basedmodel, where modules create events, for which othermodules can register as listeners.
Module networksare configured via a system configuration file whichspecifies which modules listen to which.As opposed to our previous release (Baumann etal., 2010), INPROTK module communication is nowcompletely encapsulated in the IUModule class.
Animplementing processor is called into action by amethod which gives access both to the edits to IUsin the left buffer since the last call, and to the list ofIUs directly.
The implementing processor must thennotify its right buffer, either about the edits to theright buffer, or giving the content directly.
Modulescan be fully event-driven, only triggered into actionby being notified of a hypothesis change, or theycan run persistently, in order to create endogenousevents like time-outs.
Event-driven modules can runconcurrently in separate threads or can be called se-quentially by another module (which may seem torun counter the spirit of incremental processing, butcan be advantageous for very quick computationsfor which the overhead of creating threads should beavoided).
In the case of separate threads, which runat different update intervals, the left-buffer view willautomatically be updated to its most recent state.INPROTK also comes with an extensive set of mon-itoring and profiling modules which can be linkedinto the module network at any point and allow tostream data to disk or to visualise it online through aviewing tool (von der Malsburg et al, 2009), as wellas different ways to simulate input (e.g., typed orread from a file) for debugging.
All IUmodules canalso output loggging messages to the viewing tooldirectly (to ease graphic debugging of error cases inmulti-threaded applications).3In an earlier release, we used OAA (Cheyer and Martin,2001), which however turned out to be too slow.3 Incremental Speech RecognitionOur speech recognition module is based on theSphinx-4 (Walker et al, 2004) toolkit and comes withacoustic models for German.4 The module queriesthe ASR?s current best hypothesis after each frame ofaudio and changes its output accordingly, adding orrevoking WordIUs and notifying its listeners.
Addi-tionally, for each of the WordIUs, SyllableIUs andSegmentIUs are created and bound to the word (andto the syllable respectively) via the grounded-in hier-archy.
Later modules in the pipeline are thus able touse this lower-level information (e.g.
to disambiguatemeaning based on prosodic aspects of words).
Forprosodic processing, we inject additional processorsinto Sphinx?
acoustic frontend which provide featuresfor further prosodic processing (pitch, loudness, andspectral tilt).
In this way, IUs are able to access theprecise acoustic data (in raw and processed forms).An ASR?s current best hypothesis frequentlychanges during the recognition process with the ma-jority of the changes not improving the result.
Everysuch change triggers all listening modules (and pos-sibly their listeners), resulting in a lot of unnecessaryprocessing.
Furthermore, changes may actually dete-riorate results, if a ?good?
hypothesis is intermittentlychanged for worse.
Therefore, we developed hypoth-esis smoothing approaches (Baumann et al, 2009)which greatly reduce spurious edits in the output atthe cost of some timeliness: With a lag of 320ms wereduced the amount of spurious edits to 10% from aninitial 90%.
The current implementation of hypothe-sis smoothing is taylored specifically towards ASRoutput, but other input modules (like gesture or facialexpression recognition) could easily be smoothedwith similar methods.4 Incremental NLU and DMAs mentioned above, the more ?higher-level?
com-ponents in our toolkit are more domain-specific thanthe other components, and in any case are proba-bly exactly those modules which users of the toolkitmay want to substitute with their own.
Neverthe-less, we provide example implementations of a sim-ple keyword-spotting ?NLU?, as well as statistically4Models for English, French and other languagesare available from the Sphinx?
distribution and fromhttp://www.voxforge.org.30trained ones (Schlangen et al, 2009; Heintze et al,2010).We have recently built a somewhat more traditionalNLU component which could be more easily portedto other domains (by adapting lexicon and grammar).It consists of a probabilistic, beam-search top-downparser (following (Roark, 2001)), which producesa principled semantic representation in the formal-ism robust minimal recursion semantics (Copestake,2006).
This component is described in more detail in(Peldszus et al, 2012).5 Incremental Speech SynthesisRounding out the toolkit is our new component for in-cremental speech synthesis, which has the followingproperties:(a) It makes possible changes to the as-yet unspokenpart of the ongoing utterance,(b) allows adaptations of delivery parameters suchas speaking rate or pitch with very low latency.
(c) It autonomously makes delivery-related deci-sions (such as producing hesitations), and(d) it provides information about delivery status (e. g.useful in case of barge-ins).
(e) And, last but not least, it runs in real time.Figure 1 provides a look into the internal datastructures of the component, showing a triangularstructure where on successive levels structure is builtjust-in-time (e.g., turning target phoneme sequencesinto vocoding parameters) and hence can be changedwith low cost, if necessary.
We have evaluated thecomponent in an application scenario where it provedto increase perceived naturalness, and have also stud-ied the tradeoff between look-ahead and prosodicquality.
To this end, Figure 2 plots the deviation ofthe prosodic parameters pitch and timing from thatof a non-incremental synthesis of the same utteranceversus the amount of look-ahead, that is, how far intothe current phrase the next phrase becomes known.
Itshows that best results are achieved if the next phrasethat is to be synthesized becomes known no later thanone or two words into the current phrase (w0 or w1).6 Evaluation of Incremental ProcessorsWhile not part of the toolkit proper, we think that itcan only be useful for the field to agree on commonevaluation metrics.
Incremental processing bringsFigure 1: Hierarchic structure of incremental units describ-ing an example utterance as it is being produced duringdelivery, showing the event-based just-in-time processingstrategy.0102030l l ll l lw0 w1 w2 w3 wn?1 wnllpitch dev.timing dev.Figure 2: Deviation of pitch and timing plotted againstlookahead (right context available for incremental synthe-sis).
The more lookahead available, the better the results.new considerations of dynamics into the assessmentof processing quality, and hence requires additionalmetrics compared to non-incremental processing.
In(Baumann et al, 2011) we have proposed a familyof such metrics, and we provide an evaluation frame-work for analysing incremental ASR performance aspart of our distribution.7 ConclusionsWe have sketched the major features of our ?Incre-mental Processing Toolkit?
INPROTK.
While it is farfrom offering ?plug-and-play?
ease of constructingincremental dialogue systems, we hope it will proveuseful for other researchers insofar as it offers solu-tions to the more low-level problems that often arenot one?s main focus, but which need solving any-ways before more interesting things can be done.
Welook forward to what these interesting things may bethat others will build.31AcknowledgmentsMost of the work decribed in this paper was fundedby a grant from DFG in the Emmy Noether Pro-gramme.ReferencesG.S.
Aist, J. Allen, E. Campana, L. Galescu, C.A.Gomez Gallo, S. Stoness, M. Swift, and M Tanen-haus.
2006.
Software architectures for incrementalunderstanding of human speech.
In Proceedings of theInternational Conference on Spoken Language Process-ing (ICSLP), Pittsburgh, PA, USA, September.Timo Baumann, Michaela Atterer, and David Schlangen.2009.
Assessing and improving the performance ofspeech recognition for incremental systems.
In Pro-ceedings of the North American Chapter of the Associa-tion for Computational Linguistics - Human LanguageTechnologies (NAACL HLT) 2009 Conference, Boulder,Colorado, USA, May.Timo Baumann, Okko Bu?, and David Schlangen.
2010.InproTK in action: Open-source software for buildinggerman-speaking incremental spoken dialogue systems.In Proceedings of ESSV 2010, Berlin, Germany.Timo Baumann, Okko Bu?, and David Schlangen.
2011.Evaluation and optimization of incremental processors.Dialogue and Discourse, 2(1):113?141.Okko Bu?, Timo Baumann, and David Schlangen.
2010.Collaborating on utterances with a spoken dialoguesystem using an isu-based approach to incremental dia-logue management.
In Proceedings of the SIGdial 2010Conference, pages 233?236, Tokyo, Japan, September.Adam Cheyer and David Martin.
2001.
The open agentarchitecture.
Journal of Autonomous Agents and Multi-Agent Systems, 4(1):143?148, March.
OAA.Ann Copestake.
2006.
Robust minimal recursion se-mantics.
Technical report, Cambridge Computer Lab.Unpublished draft.Silvan Heintze, Timo Baumann, and David Schlangen.2010.
Comparing local and sequential models for sta-tistical incremental natural language understanding.
InProceedings of the SIGdial 2010 Conference, pages9?16, Tokyo, Japan, September.Andreas Peldszus, Okko Bu?, Timo Baumann, and DavidSchlangen.
2012.
Joint satisfaction of syntactic andpragmatic constraints improves incremental spoken lan-guage understanding.
In Proceedings of the Confer-ence of the European Association for ComputationalLinguistics (EACL 2012), Avignon, France, April.Brian Roark.
2001.
Robust Probabilistic Predictive Syn-tactic Processing: Motivations, Models, and Appli-cations.
Ph.D. thesis, Department of Cognitive andLinguistic Sciences, Brown University.David Schlangen and Gabriel Skantze.
2009.
A general,abstract model of incremental dialogue processing.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL 2009), pages 710?718, Athens, Greece,March.David Schlangen and Gabriel Skantze.
2011.
A gen-eral, abstract model of incremental dialogue processing.Dialogue and Discourse, 2(1):83?111.David Schlangen, Timo Baumann, and Michaela Atterer.2009.
Incremental reference resolution: The task, met-rics for evaluation, and a bayesian filtering model thatis sensitive to disfluencies.
In Proceedings of SIGdial2009, the 10th Annual SIGDIAL Meeting on Discourseand Dialogue, London, UK, September.Gabriel Skantze and Anna Hjalmarsson.
2010.
Towardsincremental speech generation in dialogue systems.
InProceedings of the SIGdial 2010 Conference, pages1?8, Tokyo, Japan, September.Gabriel Skantze and David Schlangen.
2009.
Incrementaldialogue processing in a micro-domain.
In Proceedingsof the 12th Conference of the European Chapter ofthe Association for Computational Linguistics (EACL2009), pages 745?753, Athens, Greece, March.Titus von der Malsburg, Timo Baumann, and DavidSchlangen.
2009.
Telida: A package for manipulationand visualisation of timed linguistic data.
In Proceed-ings of the Poster Session at SIGdial 2009, the 10thAnnual SIGDIAL Meeting on Discourse and Dialogue,London, UK, September.Willie Walker, Paul Lamere, Philip Kwok, Bhiksha Raj,Rita Singh, Evandro Gouvea, Peter Wolf, and JoeWoelfel.
2004.
Sphinx-4: A flexible open sourceframework for speech recognition.
Technical ReportSMLI TR2004-0811, Sun Microsystems Inc.32
