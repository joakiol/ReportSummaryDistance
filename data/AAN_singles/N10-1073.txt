Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 492?500,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsImproving the Multilingual User Experience of Wikipedia UsingCross-Language Name SearchRaghavendra UdupaMicrosoft Research IndiaBangalore, India.Mitesh Khapra ?Indian Institute of Technology BombayPowai, India.AbstractAlthough Wikipedia has emerged as a power-ful collaborative Encyclopedia on the Web, itis only partially multilingual as most of thecontent is in English and a small number ofother languages.
In real-life scenarios, non-English users in general and ESL/EFL 1 usersin particular, have a need to search for rele-vant English Wikipedia articles as no relevantarticles are available in their language.
Themultilingual experience of such users can besignificantly improved if they could expresstheir information need in their native languagewhile searching for English Wikipedia arti-cles.
In this paper, we propose a novel cross-language name search algorithm and employit for searching English Wikipedia articles ina diverse set of languages including Hebrew,Hindi, Russian, Kannada, Bangla and Tamil.Our empirical study shows that the multilin-gual experience of users is significantly im-proved by our approach.1 IntroductionSince its inception in 2001, Wikipedia has emergedas the most famous free, web-based, collaborative,and multilingual encyclopedia with over 13 millionarticles in over 270 languages.
However, Wikipediaexhibits severe asymmetry in the distribution of itscontent in the languages of the world with only asmall number of languages dominating (see Table?This work was done when the author was a summer internat Microsoft Research India.1English as Second Language and English as Foreign Lan-guage.1).
As a consequence, most users of the under-represented languages of the world have no choicebut to consult foreign language Wikipedia articlesfor satisfying their information needs.Table 1: Linguistic asymmetry of WikipediaLanguage Speakers Contributors ArticlesEnglish 1500M 47.1% 3,072,373Russian 278M 5.2% 441,860Hebrew 10M 0.7% 97,987Hindi 550M 0.06% 50,926Bangla 230M 0.02% 20,342Tamil 66M 0.04% 19,472Kannada 47M 0.02% 7,185Although consulting foreign language Wikipediais not a solution for the problem of linguistic asym-metry, in the specific case of ESL/EFL users whoform a sizable fraction of Internet users of the world2, it is arguably the most practical option today.
Typ-ically, ESL/EFL users are reasonably good at read-ing and extracting relevant information from Englishcontent but not so good at expressing their infor-mation needs in English.
In particular, getting thespellings of foreign names in English correctly isvery difficult for most ESL/EFL users due to the dif-ferences in the way a foreign name is pronouncedin the native languages.
For instance, JapaneseEFL speakers often break consonant clusters in for-eign names using vowels (see Table 2) and HindiESL speakers find it difficult to differentiate between?an?, ?en?, and ?on?
in English names (such as ?Clin-2As per some estimates, there are about 1 Billion ESL andEFL speakers in the world today and their number is growing.492ton?)
and will most likely use ?an?
(?Clintan?
).Table 2: Influence of native language on the Englishspelling of names.WikipediaEntity Hindi Japanese KannadaStephenHawkingStefanHokingSuchifunHoukinguSteephanHaakimgPaul Krug-manPol Crugmun PooruKurugumanPaal Kraga-manHarounal-RashidHaroonal-RashidHaruunaru-RasheedoHaroonal-RasheedSubrahmaniyaBharatiSubramaniyaBharatiSuburaamaniyaBahaarachiSubrahmanyaBharathiIn principle, English spell-checkers (Ahmad andKondrak, 2005) can handle the problem of incor-rect spellings in the queries formed by ESL/EFLusers.
But in practice, there are two difficulties.Firstly, most English spell-checkers do not have agood coverage of names which form the bulk of userqueries.
Secondly, spelling correction of names isdifficult because spelling mistakes are markedly in-fluenced by the native language of the user.
Notsurprisingly, Wikipedia?s inbuilt spell-checker sug-gests ?Suchin Housing?
as the only alternative to thequery ?Suchifun Houkingu?
instead of the correctentity ?Stephen Hawking?
(See Table 3 for more ex-amples).The inability of ESL/EFL speakers to expresstheir information needs correctly in English and thepoor performance of spell-checkers highlight theneed for a practical solution for the linguistic asym-metry problem of Wikipedia.
In this work, we arguethe multilingual user experience of ESL/EFL userscan be significantly improved by allowing them toexpress their information need in their native lan-guage.
While it might seem that we would needa fully functional cross-language retrieval systemthat supports translation of non-English queries toEnglish, we note that a good number of the pagesin Wikipedia are on people.
This empirical factallows us to improve the multilingual experienceof ESL/EFL Wikipedia users by means of cross-language name search which is less resource de-manding than a fully functional cross-language re-trieval system.There are several challenges that need to be ad-dressed in order to enable cross-language nameTable 3: Spelling suggestions by Wikipedia.User InputWikipedia?sSuggestionCorrect SpellingSuchifun Houkingu Suchin Housing Stephen HawkingStefan Hoking Stefan Ho king Stephen HawkingPol Crugman Poll Krugman Paul KrugmanPaal Kragaman Paul Krugman Paul KrugmanSuburaamaniya Ba-haarachiSubramaniyaBaracchiSubrahmaniyaBharatisearch in Wikipedia.?
Firstly, name queries are expressed byESL/EFL users in the native languages usingthe orthography of those languages.
Translit-erating the name into Latin script using aMachine Transliteration system is an optionbut state-of-the-art Machine Transliterationtechnologies are still far away from producingthe correct transliteration.
Further, as pointedout by (Udupa et al, 2009a), it is not enoughif a Machine Transliteration system generatesa correct transliteration; it must produce thetransliteration that is present in the Wikipediatitle.?
Secondly, there are about 6 million titles (in-cluding redirects) in English Wikipedia whichrules out the naive approach of comparing thequery with every one of the English Wikipediatitles for transliteration equivalence as is donetypically in transliteration mining tasks.
Apractical cross-language name search systemfor Wikipedia must be able to search millionsof Wikipedia titles in a fraction of a second andreturn the most relevant titles.?
Thirdly, names are typically multi-word andas a consequence there might not be an ex-act match between the query and EnglishWikipedia titles.
Any cross-language namesearch system for Wikipedia must be ableto deal with multi-word names and partialmatches effectively.?
Fourthly, the cross-language name search sys-493tem must be tolerant to spelling variations inthe query as well as the Wikipedia titles.In this work, we propose a novel approach tocross-language name search in Wikipedia that ad-dresses all the challenges described above.
Fur-ther, our approach does not depend on either spell-checkers or Machine Transliteration.
Rather wetransform the problem into a geometric search prob-lem and employ a state-of-the-art geometric algo-rithm for searching a very large database of names.This enables us to accurately search the relevantWikipedia titles for a given user query in a fractionof a second even on a single processor.1.1 Our ContributionsOur contributions can be summarized as follows:1.
We introduce a language and orthography in-dependent geometric representation for single-word names (Section 3.1).2.
We model the problem of learning the geo-metric representation of names as a multi-viewlearning problem and employ the machineryof Canonical Correlation Analysis (CCA) tocompute a low-dimensional Euclidean featurespace.
We map both foreign single-word namesand English single-word names to points in thecommon feature space and the similarity be-tween two single-word names is an exponen-tially decaying function of the squared geomet-ric distance between the corresponding points(Section 3).3.
We model the problem of searching a databaseof names as a geometric nearest neighbor prob-lem in low-dimensional Euclidean space andemploy the well-known ANN algorithm forapproximate nearest neighbors to search forthe equivalent of a query name in the EnglishWikipedia titles (Arya et al, 1998) (Section3.3).4.
We introduce a simple and efficient algorithmfor computing the similarity scores of multi-word names from the single-word similarityscores (Section 3.4).5.
We show experimentally that our approach sig-nificantly improves the multilingual experienceof ESL/EFL users (Section 4).2 Related WorkAlthough approximate similarity search is well-studied, we are not aware of any non-trivial cross-language name search algorithm in the litera-ture.
However, several techniques for mining nametransliterations from monolingual and comparablecorpora have been studied (Pasternack and Roth,2009), (Goldwasser and Roth, 2008), (Klementievand Roth, 2006), (Sproat et al, 2006), (Udupa et al,2009b).
These techniques employ various translit-eration similarity models.
Character unigrams andbigrams were used as features to learn a discrimi-native transliteration model and time series similar-ity was combined with the transliteration similaritymodel (Klementiev and Roth, 2006).
A generativetransliteration model was proposed and used alongwith cross-language information retrieval to minenamed entity transliterations from large comparablecorpora (Udupa et al, 2009b).
However, none ofthese transliteration similarity models are applicablefor searching very large name databases as they relyon brute-force search.
Not surprisingly, (Pasternackand Roth, 2009) report that ?..
testing [727 singleword English names] with fifty thousand [Russian]candidates is a large computational hurdle (it takesour model about seven hours)?.Several algorithms for string similarity searchhave been proposed and applied to various problems(Jin et al, 2005).
None of them are directly applica-ble to cross-language name search as they are basedon the assumption that the query string shares thesame alphabet as the database strings.Machine Transliteration has been studied exten-sively in the context of Machine Translation andCross-Language Information Retrieval (Knight andGraehl, 1998), (Virga and Khudanpur, 2003), (Kuoet al, 2006), (Sherif and Kondrak, 2007), (Ravi andKnight, 2009), (Li et al, 2009), (Khapra and Bhat-tacharyya, 2009).
However, Machine Transliterationfollowed by string similarity search gives less-than-satisfactory solution for the cross-language namesearch problem as we will see later in Section 4.CCA was introduced by Hotelling in 1936 and has494been applied to various problems including CLIR,Text Clustering, and Image Retrieval (Hardoon etal., 2004).
Recently, CCA has gained importancein the Machine Learning community as a techniquefor multi-view learning.
CCA computes a commonsemantic feature space for two-view data and al-lows users to query a database using either of thetwo views.
CCA has been used in bilingual lexi-con extraction from comparable corpora (Gaussieret al, 2004) and monolingual corpora (Haghighi etal., 2008).Nearest neighbor search is a fundamental prob-lem where challenge is to preprocess a set of pointsin some metric space into a geometric data struc-ture so that given a query point, its k-nearest neigh-bors in the set can be reported as fast as possi-ble.
It has applications in many areas including pat-tern recognition and classification, machine learn-ing, data compression, data mining, document re-trieval and statistics.
The brute-force search algo-rithm can find the nearest neighbors in running timeproportional to the product of the number of pointsand the dimension of the metric space.
When the di-mension of the metric space is small, there exist al-gorithms which give better running time than brute-force search.
However, the search time grows expo-nentially with the dimension and none of the algo-rithms do significantly better than brute-force searchfor high-dimensional data.
Fortunately, efficient al-gorithms exist if instead of exact nearest neighbors,we ask for approximate nearest neighbors (Arya etal., 1998).3 Cross-Language Name Search as aGeometric Search ProblemThe key idea behind our approach is the following:if we can embed names as points (or equivalentlyas vectors) in a suitable geometric space, then theproblem of searching a very large database of namescan be casted as a geometric search problem, i.e.
oneof finding the nearest neighbors of the query point inthe database.As illustrative examples, consider the namesStephen and Steven.
A simple geometric represen-tation for these names is the one induced by theircorresponding features: {St, te, ep, ph, he, en} and{St, te, ev, ve, en} 3.
In this representation, eachcharacter bigram constitutes a dimension of the geo-metric feature space whose coordinate value is thenumber of times the bigram appears in the name.It is possible to find a low-dimensional representa-tion for the names by using Principal ComponentsAnalysis or any other dimensionality reduction tech-nique on the bigram feature vectors.
However, thekey point to note is that once we have an appropri-ate geometric representation for names, the similar-ity between two names can be computed asKmono (name1, name2) = e?||?1?
?2||2/22 (1)where ?1 and ?2 are the feature vectors of the twonames and  is a constant.
Armed with the geomet-ric similarity measure, we can leverage geometricsearch techniques for finding names similar to thequery.In the case of cross-language name search, weneed a feature representation of names that is lan-guage/script independent.
Once we map names indifferent languages/scripts to the same feature space,we can essentially treat similarity search as a geo-metric search problem.3.1 Language/Script Independent GeometricRepresentation of NamesTo obtain language/script independent geometricrepresentation of names, we start by forming the lan-guage/script specific feature vectors as described inSection 3.
Given two names, Stephen in Latin scriptand -VFPn in Devanagari script, we form the corre-sponding character bigram feature vectors ?
(usingfeatures {St, te, ep, ph, en}) and ?
(using features{-V, VF, FP, Pn}) respectively.
We then map thesevectors to a common geometric feature space usingtwo linear transformations A and B:??
AT?
= ?s ?
Rd (2)?
?
BT?
= ?s ?
Rd (3)The vectors ?s and ?s can be viewed as lan-guage/script independent representation of thenames Stephen and -VFPn.3Here, we have employed character bigrams as features.
Inprinciple, we can use any suitable set of features including pho-netic features extracted from the strings.4953.1.1 Cross-Language Similarity of NamesIn order to search a database of names in Englishwhen the query is in a native language, say Hindi, weneed to be able to measure the similarity of a name inDevangari script with names in Latin script.
The lan-guage/script independent representation gives a nat-ural way to measure the similarity of names acrosslanguages.
By embedding the language/script spe-cific feature vectors ?
and ?
in a common featurespace via the projections A and B, we can com-pute the similarity of the corresponding names asfollows:Kcross (name1, name2) = e?||?s?
?s||2/22 (4)It is easy to see from Equation 4 that the similarityscore of two names is small when the projections ofthe names are negatively correlated.3.2 Learning Common Feature Space usingCCAIdeally, the transformations A and B should be suchthat similar names in the two languages are mappedto close-by points in the common geometric fea-ture space.
It is possible to learn such transforma-tions from a training set of name transliterations inthe two languages using the well-known multi-viewlearning framework of Canonical Correlation Anal-ysis (Hardoon et al, 2004).
By viewing the lan-guage/script specific feature vectors as two represen-tations/views of the same semantic object, the entitywhose name is written as Stephen in English and as-VFPn in Hindi, we can employ the machinery ofCCA to find the transformations A and B.Given a sample of multivariate data with twoviews, CCA finds a linear transformation for eachview such that the correlation between the projec-tions of the two views is maximized.
Considera sample Z = {(xi, yi)}Ni=1 of multivariate datawhere xi ?
Rm and yi ?
Rn are two views of theobject.
Let X = {xi}Ni=1 and Y = {yi}Ni=1.
As-sume thatX and Y are centered4, i.e., they have zeromean.
Let a and b be two directions.
We can projectX onto the direction a to get U = {ui}Ni=1 whereui = aTxi.
Similarly, we can project Y onto the di-rection b to get the projections V = {vi}ni=1 where4If X and Y are not centered, they can be centered by sub-tracting the respective means.vi = bT yi.
The aim of CCA is to find a pair of di-rections (a, b) such that the projections U and V aremaximally correlated.
This is achieved by solvingthe following optimization problem:?
= max(a,b)< Xa,Xb >||Xa||||Xb||= max(a,b)aTXY T b?aTXXT a?bTY Y T bThe objective function of Equation 5 can be max-imized by solving the following generalized eigenvalue problem (Hardoon et al, 2004):XY T(Y Y T)?1Y XTa = ?2XXTa(Y Y T)?1Y XTa = ?bThe subsequent basis vectors can be foundby adding the orthogonality of bases con-straint to the objective function.
Althoughthe number of basis vectors can be as high asmin{Rank(X), Rank(Y )}, in practice, only thefirst few basis vectors are used since the correlationof the projections is high for these vectors and smallfor the remaining vectors.Let A and B be the first d > 0 basis vectors com-puted by CCA.Figure 1: Projected names (English-Hindi).3.2.1 Common Geometric Feature SpaceAs described in Section 3.1, we represent namesas points in the common geometric feature space de-fined by the projection matrices A and B.
Figure 1496shows a 2-dimensional common feature space com-puted by CCA for English (Latin script) and Hindi(Devanagari script) names.
As can be seen from thefigure, names that are transliterations of each otherare mapped to near-by points in the common featurespace.Figure 2 shows a 2-dimensional common featurespace for English (Latin script) and Russian (Cyrillicscript) names.
As can be seen from the figure, namesthat are transliterations of each other are mapped tonear-by points in the common feature space.Figure 2: Projected names (English-Russian).3.3 Querying the Name DatabaseGiven a database D = {ei}Mi=1 of single-wordnames in English, we first compute their lan-guage/script specific feature vectors ?
(i), i =1, .
.
.
,M .
We then compute the projections ?
(i)s =AT?(i).
Thus, we transform the name database Dinto a set of vectors {?
(1)s , .
.
.
, ?
(M)s } in Rd.Given a query name h in Hindi, we compute itslanguage/script specific feature vector ?
and projectit on to the common feature space to get ?s =BT?
?
Rd.
Names similar to h in the database Dcan be found as solutions of the k-nearest neighborproblem:eik = argmaxei?D?
{eij }k?1j=1 Kcross (ei, h)= argmaxei?D?
{eij }k?1j=1 e?||?
(i)s ?
?s||2/22= argminei?D?
{eij }k?1j=1 ||?
(i)s ?
?s||Unfortunately, computing exact k-nearest neigh-bors in dimensions much higher than 8 is difficultand the best-known methods are only marginallybetter than brute-force search (Arya et al, 1998).Fortunately, there exist very efficient algorithms forcomputing approximate nearest neighbors and inpractice they do nearly as well as the exact near-est neighbors algorithms (Arya et al, 1998).
It isalso possible to control the tradeoff between accu-racy and running time by specifiying a maximumapproximation error bound.
We employ the well-known Approximate Nearest Neighbors (aka ANN)algorithm by Arya and Mount which is known to dowell in practice when d ?
100 (Arya et al, 1998).3.4 Combining Single-Word SimilaritiesThe approach described in the previous sectionsworks only for single-word names.
We need to com-bine the similarities at the level of individual wordsinto a similarity function for multi-word names.
To-wards this end, we form a weighted bipartite graphfrom the two multi-word names as follows:We first tokenize the Hindi query name into sin-gle word tokens and find the nearest English neigh-bors for each of these Hindi tokens using the methodoutlined section 3.3.
We then find out all the En-glish Words which contain one or more of the En-glish neighbors thus fetched.
Let E = e1e2 .
.
.
eIbe one such multi-word English name and H =h1h2 .
.
.
hJ be the multi-word Hindi query.
We forma weighted bipartite graph G = (S ?
T,W ) with anode si for the ith word ei in E and node tj for thejth word hj in H .
The weight of the edge (si, tj) isset as wij = Kcross (ei, hj).Let w be the weight of the maximum weightedbipartite matching in the graph G. We define thesimilarity between E and H as follows:Kcross (E,H) =w|I ?
J |+ 1 .
(5)The numerator of the right hand side of Equation5 favors name pairs which have a good number ofhigh quality matches at the individual word levelwhereas the denominator penalizes pairs that havedisproportionate lengths.Note that, in practice, both I and J are small andhence we can find the maximum weighted bipartitematching very easily.
Further, most edge weights in497Figure 3: Combining Single-Word Similarities.the bipartite graph are negligibly small.
Therefore,even a greedy matching algorithm suffices in prac-tice.4 Experiments and ResultsIn the remainder of this section, we refer to our sys-tem by GEOM-SEARCH.4.1 Experimental SetupWe tested our cross language name search systemusing six native languages, viz., Russian, Hebrew,Hindi, Kannada, Tamil and Bangla.
For each ofthese languages, we created a test set consisting of1000 multi-word name queries and found manuallythe most relevant Wikipedia article for each query inthe test set.
The Wikipedia articles thus found andall the redirect titles that linked to them formed thegold standard for evaluating the performance of oursystem.In order to compare the performance of GEOM-SEARCH with a reasonable baseline, we imple-mented the following baseline: We used a state-of-the art Machine Transliteration system to generatethe best transliteration of each of the queries.
Weused the edit distance between the transliteration andthe single-word English name as the similarity score.We combined single word similarities using the ap-proach described in Section 3.4.
We refer to thisbaseline by TRANS-SEARCH.Note that several English Wikipedia names some-times get the same score for a query.
Therefore,we used a tie-aware mean-reciprocal rank measureto evaluate the performance (McSherry and Najork,2008).4.2 GEOM-SEARCHThe training and search procedure employed byGEOM-SEARCH are described below.4.2.1 CCA TrainingWe learnt the linear transformations A and B thatproject the language/script specific feature vectors tothe common feature space using the approach dis-cussed in Section 3.2.
The learning algorithm re-quires a training set consisting of pairs of single-word names in English and the respective native lan-guage.
We used approximately 15, 000 name pairsfor each native language.A key parameter in CCA training is the number ofdimensions of the common feature space.
We foundthe optimal number of dimensions using a tuning setconsisting of 1, 000 correct name pairs and 1, 000incorrect name pairs for each native language.
Wefound that d = 50 is a very good choice for eachnative language.Another key aspect of training is the choice oflanguage/script specific features.
For the six lan-guages we experimented with and also for English,we found that character bigrams formed a good setof features.
We note that for languages such as Chi-nese, Japanese, and Korean, unigrams are the bestchoice.
Also, for these languages, it may help tosyllabify the English name.4.2.2 SearchAs a pre-processing step, we extracted a list of 1.3million unique words from the Wikipedia titles.
Wecomputed the language/script specific feature vectorfor each word in this list and projected the vector tothe common feature space as described in Section3.1.
The low-dimensional embeddings thus com-puted formed the input to the ANN algorithm.We tokenized each query in the native languageinto constituent words.
For each constituent, we firstcomputed the language/script specific feature vector,projected it to the common feature space, and foundthe k-nearest neighbors using the ANN algorithm.We used k=100 for all our experiments.After finding the nearest neighbors and the corre-sponding similarity scores, we combined the scoresusing the approach described in Section 3.4.4.3 TRANS-SEARCHThe training and search procedure employed byTRANS-SEARCH are described below.498Figure 4: Top scoring English Wikipedia page retrieved by GEOM-SEARCH4.3.1 Transliteration TrainingWe used a state-of-the-art CRF-based translitera-tion technique for transliterating the native languagenames (Khapra and Bhattacharyya, 2009).
We usedCRF++, an open-source CRF training tool, to trainthe transliteration system.
We used exactly thesame features and parameter settings as described in(Khapra and Bhattacharyya, 2009).
As in the case ofCCA, we use around 15, 000 single word name pairsin the training.4.3.2 SearchThe preprocessing step for TRANS-SEARCH isthe same as that for GEOM-SEARCH.
We translit-erated each constituent of the query into English andfind all single-word English names that are at an editdistance of at most 3.
We computed the similarityscore as described in Section 3.4.4.4 EvaluationWe evaluated the performance of GEOM-SEARCHand TRANS-SEARCH using a tie-aware mean re-ciprocal rank (MRR).
Table 4 compares the averagetime per query and the MRR of the two systems.GEOM-SEARCH performed significantly betterthan the transliteration based baseline system for allthe six languages.
On an average, the relevant En-glish Wikipedia page was found in the top 2 re-sults produced by GEOM-SEARCH for all the sixnative languages.
Clearly, this shows that GEOM-SEARCH is highly effective as a cross-langaugename search system.
The good results also validateour claim that cross-language name search can im-Table 4: MRR and average time per query (in seconds)for the two systems.Language GEOM TRANSTime MRR Time MRRHin 0.51 0.686 2.39 0.485Tam 0.23 0.494 2.16 0.291Kan 1.08 0.689 2.17 0.522Ben 1.30 0.495 ?
?Rus 0.15 0.563 1.65 0.476Heb 0.65 0.723 ?
?prove the multi-lingual user experience of ESL/EFLusers.5 ConclusionsGEOM-SEARCH, a geometry-based cross-language name search system for Wikipedia,improves the multilingual experience of ESL/EFLusers of Wikipedia by allowing them to formulatequeries in their native languages.
Further, it is easyto integrate a Machine Translation system withGEOM-SEARCH.
Such a system would find therelevant English Wikipedia page for a query usingGEOM-SEARCH and then translate the relevantWikipedia pages to the native language using theMachine Translation system.6 AcknowledgementWe thank Jagadeesh Jagarlamudi and Shaishav Ku-mar for their help.499ReferencesFarooq Ahmad and Grzegorz Kondrak.
2005.
Learn-ing a spelling error model from search query logs.
InHLT ?05: Proceedings of the conference on HumanLanguage Technology and Empirical Methods in Nat-ural Language Processing, pages 955?962, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Sunil Arya, David M. Mount, Nathan S. Netanyahu, RuthSilverman, and Angela Y. Wu.
1998.
An optimalalgorithm for approximate nearest neighbor searchingfixed dimensions.
J. ACM, 45(6):891?923.
?Eric Gaussier, Jean-Michel Renders, Irina Matveeva,Cyril Goutte, and Herve?
De?jean.
2004.
A geometricview on bilingual lexicon extraction from comparablecorpora.
In ACL, pages 526?533.Dan Goldwasser and Dan Roth.
2008.
Transliteration asconstrained optimization.
In EMNLP, pages 353?362.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of ACL-08: HLT, pages 771?779, Columbus, Ohio, June.
As-sociation for Computational Linguistics.David R. Hardoon, Sa?ndor Szedma?k, and John Shawe-Taylor.
2004.
Canonical correlation analysis: Anoverview with application to learning methods.
Neu-ral Computation, 16(12):2639?2664.Liang Jin, Nick Koudas, Chen Li, and Anthony K. H.Tung.
2005.
Indexing mixed types for approximateretrieval.
In VLDB, pages 793?804.Mitesh Khapra and Pushpak Bhattacharyya.
2009.
Im-proving transliteration accuracy using word-origin de-tection and lexicon lookup.
In Proceedings of the 2009Named Entities Workshop: Shared Task on Translit-eration (NEWS 2009).
Association for ComputationalLinguistics.Alexandre Klementiev and Dan Roth.
2006.
Namedentity transliteration and discovery from multilingualcomparable corpora.
In HLT-NAACL.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4):599?612.Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang.
2006.Learning transliteration lexicons from the web.
InACL.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In ICML, pages 282?289.Haizhou Li, A Kumaran, Vladimir Pervouchine, andMin Zhang.
2009.
Report of news 2009 machinetransliteration shared task.
In Proceedings of the 2009Named Entities Workshop: Shared Task on Translit-eration (NEWS 2009).
Association for ComputationalLinguistics.Frank McSherry and Marc Najork.
2008.
Computinginformation retrieval performance measures efficientlyin the presence of tied scores.
In ECIR, pages 414?421.Jeff Pasternack and Dan Roth.
2009.
Learning bettertransliterations.
In CIKM, pages 177?186.Sujith Ravi and Kevin Knight.
2009.
Learning phonememappings for transliteration without parallel data.
InNAACL-HLT.Hanan Samet.
2006.
Foundations of Multidimensionaland Metric Data Structures (The Morgan KaufmannSeries in Computer Graphics).
Morgan Kaufmann,August.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-based transliteration.
In ACL.Richard Sproat, Tao Tao, and ChengXiang Zhai.
2006.Named entity transliteration with comparable corpora.In ACL.Raghavendra Udupa, K. Saravanan, Anton Bakalov, andAbhijit Bhole.
2009a.
?they are out there, if youknow where to look?
: Mining transliterations of oovquery terms for cross-language information retrieval.In ECIR, pages 437?448.Raghavendra Udupa, K. Saravanan, A. Kumaran, and Ja-gadeesh Jagarlamudi.
2009b.
Mint: A method for ef-fective and scalable mining of named entity transliter-ations from large comparable corpora.
In EACL, pages799?807.Paola Virga and Sanjeev Khudanpur.
2003.
Transliter-ation of proper names in cross-language applications.In SIGIR, pages 365?366.500
