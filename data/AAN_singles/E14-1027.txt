Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 249?258,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsIncremental Bayesian Learning of Semantic CategoriesLea Frermann and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABl.frermann@ed.ac.uk, mlap@inf.ed.ac.ukAbstractModels of category learning have been ex-tensively studied in cognitive science andprimarily tested on perceptual abstractionsor artificial stimuli.
In this paper we focuson categories acquired from natural lan-guage stimuli, that is words (e.g., chair isa member of the FURNITURE category).We present a Bayesian model which, un-like previous work, learns both categoriesand their features in a single process.
Ourmodel employs particle filters, a sequentialMonte Carlo method commonly used forapproximate probabilistic inference in anincremental setting.
Comparison againsta state-of-the-art graph-based approach re-veals that our model learns qualitativelybetter categories and demonstrates cogni-tive plausibility during learning.1 IntroductionConsiderable psychological research has shownthat people reason about novel objects they en-counter by identifying the category to which theseobjects belong and extrapolating from their pastexperiences with other members of that category(Smith and Medin, 1981).
Categorization is a clas-sic problem in cognitive science, underlying a va-riety of common mental tasks including percep-tion, learning, and the use of language.Given its fundamental nature, categorizationhas been extensively studied both experimentallyand in simulations.
Indeed, numerous models ex-ist as to how humans categorize objects rangingfrom strict prototypes (categories are representedby a single idealized member which embodiestheir core properties; e.g., Reed 1972) to full ex-emplar models (categories are represented by a listof previously encountered members; e.g., Nosof-sky 1988) and combinations of the two (e.g., Grif-fiths et al.
2007).
A common feature across dif-ferent studies is the use of stimuli involving real-world objects (e.g., children?s toys; Starkey 1981),perceptual abstractions (e.g., photographs of ani-mals; Quinn and Eimas 1996), or artificial ones(e.g., binary strings, dot patterns or geometricshapes; Medin and Schaffer 1978; Posner andKeele 1968; Bomba and Siqueland 1983).
Mostexisting models focus on adult categorization, inwhich it is assumed that a large number of cate-gories have already been learnt (but see Anderson1991 and Griffiths et al.
2007 for exceptions).In this work we focus on categories acquiredfrom natural language stimuli (i.e., words) andinvestigate how the statistics of the linguistic en-vironment (as approximated by large corpora) in-fluence category formation (e.g., chair and ta-ble are FURNITURE whereas peach and apple areFRUIT1).
The idea of modeling categories usingwords as a stand-in for their referents has beenpreviously used to explore categorization-relatedphenomena such as semantic priming (Cree et al.,1999) and typicality rating (Voorspoels et al.,2008), to evaluate prototype and exemplar mod-els (Storms et al., 2000), and to simulate early lan-guage category acquisition (Fountain and Lapata,2011).
The idea of using naturalistic corpora hasreceived little attention.
Most existing studies usefeature norms as a proxy for people?s representa-tion of semantic concepts.
In a typical procedure,participants are presented with a word and asked togenerate the most relevant features or attributes forits referent concept.
The most notable collectionof feature norms is probably the multi-year projectof McRae et al.
(2005), which obtained featuresfor a set of 541 common English nouns.Our approach replaces feature norms with rep-resentations derived from words?
contexts in cor-pora.
While this is an impoverished view of howcategories are acquired ?
it is clear that they arelearnt through exposure to the linguistic environ-ment and the physical world ?
perceptual infor-1Throughout this paper we will use small caps to denoteCATEGORIES and italics for their members.249mation relevant for extracting semantic categoriesis to a large extent redundantly encoded in linguis-tic experience (Riordan and Jones, 2011).
Besides,there are known difficulties with feature normssuch as the small number of words for which thesecan be obtained, the quality of the attributes, andvariability in the way people generate them (seeZeigenfuse and Lee 2010 for details).
Focusingon natural language categories allows us to buildcategorization models with theoretically unlimitedscope.To this end, we present a probabilistic Bayesianmodel of category acquisition based on the keyidea that learners can adaptively form categoryrepresentations that capture the structure ex-pressed in the observed data.
We model categoryinduction as two interrelated sub-problems: (a) theacquisition of features that discriminate amongcategories, and (b) the grouping of concepts intocategories based on those features.
An importantmodeling question concerns the exact mechanismwith which categories are learned.
To maintaincognitive plausibility, we develop an incrementallearning algorithm.
Incrementality is a central as-pect of human learning which takes place sequen-tially and over time.
Humans are capable of deal-ing with a situation even if only partial informationis available.
They adaptively learn as new infor-mation is presented and locally update their inter-nal knowledge state without systematically revis-ing everything known about the situation at hand.Memory and processing limitations also explainwhy humans must learn incrementally.
It is notpossible to store and have easy access to all the in-formation one has been exposed to.
It seems likelythat people store the most prominent facts and gen-eralizations, which they modify on they fly whennew facts become available.Our model learns categories using a particle fil-ter, a Markov Chain Monte Carlo (MCMC) in-ference mechanism which sequentially integratesnewly observed data and can be thus viewed as aplausible proxy for human learning.
Experimentalresults show that the incremental learner obtainsmeaningful categories which outperform the stateof the art whilst at the same time acquiring seman-tic representations of words and their features.2 Related WorkThe problem of category induction has achievedmuch attention in the cognitive science literature.Incremental category learning was pioneered byAnderson (1991) who develops a non-parametricmodel able to induce categories from abstractstimuli represented by binary features.
Sanbornet al.
(2006) present a fully Bayesian adaptation ofAnderson?s original model, which yields a betterfit with behavioral data.
A separate line of workexamines the cognitive characteristics of categoryacquisition as well as the processes of generalizingand generating new categories and exemplars (Jernand Kemp, 2013; Kemp et al., 2012).
The abovemodels are conceptually similar to ours.
How-ever, they were developed with adult categoriza-tion in mind, and use rather simplistic categoriesrepresenting toy-domains.
It is therefore not clearwhether they generalize to arbitrary stimuli anddata sizes.
We aim to show that it is possible to ac-quire natural language categories on a larger scalepurely from linguistic context.Our model is loosely related to Bayesian mod-els of word sense induction (Brody and Lapata,2009; Yao and Durme, 2011).
We also assumethat local linguistic context can provide importantcues for word meaning and by extension categorymembership.
However, the above models focuson performance optimization and learn in an idealbatch mode, while incorporating various kinds ofadditional features such as part of speech tags ordependencies.
In contrast, we develop a cogni-tively plausible (early) language learning modeland show that categories can be acquired purelyfrom context, as well as in an incremental fashion.From a modeling perspective, we learn cate-gories incrementally using a particle filtering al-gorithm (Doucet et al., 2001).
Particle filters area family of sequential Monte Carlo algorithmswhich update the state space of a probabilisticmodel with newly encountered information.
Theyhave been successfully applied to natural lan-guage acquisition tasks such as word segmentation(Borschinger and Johnson, 2011), or sentence pro-cessing (Levy et al., 2009).
Sanborn et al.
(2006)also use particle filters for small-scale categoriza-tion experiments with artificial stimuli.
To the bestof our knowledge, we present the first particle fil-tering algorithm for large-scale category acquisi-tion from natural text.Our work is closest to Fountain and Lapata(2011) who also develop a model for inducing nat-ural language categories.
Specifically, they pro-pose an incremental version of Chinese Whispers(Biemann, 2006), a randomized graph-clusteringalgorithm.
The latter takes as input a graph whichis constructed from corpus-based co-occurrencestatistics and produces a hard clustering over thenodes in the graph.
Contrary to our model, theytreat the tasks of inferring a semantic representa-250wtwcz?????
?Mult MultMultDir DirDirndkkFigure 1: Plate diagram representation of theBayesCat model.tion for concepts and their class membership astwo separate processes.
This allows to experi-ment with different ways of initializing the co-occurrence matrix (e.g., from bags of words ora dependency parsed corpus), however at the ex-pense of cognitive plausibility.
It is unlikely thathumans have two entirely separate mechanismsfor learning the meaning of words and their cat-egories.
We formulate a more expressive modelwithin a probabilistic framework which capturesthe meaning of words, their similarity, and the pre-dictive power of their linguistic contexts.3 The BayesCat ModelIn this section we present our Bayesian model ofcategory induction (BayesCat for short).
The inputto the model is natural language text, and its finaloutput is a set of clusters representing categoriesof semantic concepts found in the input data.
Likemany other semantic models, BayesCat is inspiredby the distributional hypothesis which states thata word?s meaning is predictable from its context(Harris, 1954).
By extension, we also assume thatcontextual information can be used to character-ize general semantic categories.
Accordingly, theinput to our model is a corpus of documents, eachdefined as a target word t centered in a fixed-lengthcontext window:[c?n... c?1t c1... cn] (1)We assume that there exists one global distribu-tion over categories from which all documents aregenerated.
Each document is assigned a categorylabel, based on two types of features: the docu-ment?s target word and its context words, whichare modeled through separate category-specificdistributions.
We argue that it is important to dis-tinguish between these features, since words be-longing to the same category do not necessarilyco-occur, but tend to occur in the same contexts.For example, the words polar bear and anteaterDraw distribution over categories ??
Dir(?
)for category k doDraw target word distribution ?k?Dir(?
)Draw context word distribution ?k?Dir(?
)for Document d doDraw category zd?Mult(?
)Draw target word wdt?Mult(?zd)for context position n = {1..N} doDraw context word wd,nc?Mult(?zd)Figure 2: The generative process of the BayesCatmodel.are both members of the category ANIMAL.
How-ever, they rarely co-occur (in fact, a cursory searchusing Google yields only three matches for thequery ?polar bear * anteater?).
Nevertheless, wewould expect to observe both words in similarcontexts since both animals eat, sleep, hunt, havefur, four legs, and so on.
This distinction con-trasts our category acquisition task from the clas-sical task of topic inference.Figure 1 presents a plate diagram of theBayesCat model; an overview of the generativeprocess is given in Figure 2.
We first draw a globalcategory distribution ?
from the Dirichlet distribu-tion with parameter ?.
Next, for each category k,we draw a distribution over target words ?kfrom aDirichlet with parameter ?
and a distribution overcontext words ?kfrom a Dirichlet with parame-ter ?.
For each document d, we draw a category zd,then a target word, and N context words from thecategory-specific distributions ?zdand ?zd, respec-tively.4 LearningOur goal is to infer the joint distribution ofall hidden model parameters, and observabledata W .
Since we use conjugate prior distributionsthroughout the model, this joint distribution can besimplified to:P(W,Z,?,?,?;?,?,?)
??k?(Nk+?k)?(?kNk+?k)?K?k=1?r?(Nkr+?r)?(?rNkr+?r)?K?k=1?s?
(Nks+ ?s)?
(?sNks+ ?s), (2)where r and s iterate over the target and contextword vocabulary, respectively, and the distribu-251tions ?,?, and ?
are integrated out and implic-itly captured by the corresponding co-occurrencecounts N??.
?
() denotes the Gamma function, ageneralization of the factorial to real numbers.Since exact inference of the parameters of theBayesCat model is intractable, we use sampling-based approximate inference.
Specifically, wepresent two learning algorithms, namely a Gibbssampler and a particle filter.The Gibbs Sampler Gibbs sampling is a well-established approximate learning algorithm, basedon Markov Chain Monte Carlo methods (Gemanand Geman, 1984).
It operates in batch-mode byrepeatedly iterating through all data points (doc-uments in our case) and assigning the currentlysampled document d a category zdconditioned onthe current labelings of all other documents z?d:zd?
P(zd|z?d,W?d;?,?,?
), (3)using equation (2) but ignoring informationfrom the currently sampled document in all co-occurrence counts.The Gibbs sampler can be seen as an ideallearner, which can view and revise any relevantinformation at any time during learning.
From acognitive perspective, this setting is implausible,since a human language learner encounters train-ing data incrementally and does not systematicallyrevisit previous learning decisions.
Particle filtersare a class of incremental, or sequential, MonteCarlo methods which can be used to model aspectsof the language learning process more naturally.The Particle Filter Intuitively, a particle fil-ter (henceforth PF) entertains a fixed set ofN weighted hypotheses (particles) based on pre-vious training examples.
Figure 3 shows anoverview of the particle filtering learning proce-dure.
At first, every particle of the PF is initializedfrom a base distribution P0(Initialization).
Then asingle iteration over the input data y is performed,during which the posterior distribution of eachdata point ytunder all current particles is com-puted given information from all previously en-countered data points yt?1(Sampling/Prediction).Crucially, each update is conditioned only on theprevious model state zt?1, which results in a con-stant state space despite an increasing amount ofavailable data.
A common problem with PF al-gorithms is weight degeneration, i.e., one particletends to accumulate most of the weight.
To avoidthis problem, at regular intervals the set of parti-cles is resampled in order to discard particles withfor particle p do .
InitializationInitialize randomly or from z0p?
p0(z)for observation t dofor particle n do .
Sampling/PredictionPn(ztn|yt)?
p(ztn|zt?1n,?
)P(yt|ztn,yt?1)zt?Mult({Pn(ztn)}Ni=1) .
ResamplingFigure 3: The particle filtering procedure.low probability and to ensure that the sample isrepresentative of the state space at any time (Re-sampling).This general algorithm can be straightforwardlyadapted to our learning problem (Griffiths et al.,2011; Fearnhead, 2004).
Each observation corre-sponds to a document, which needs to be assigneda category.
To begin with, we assign the first ob-served document to category 0 in all particles (Ini-tialization).
Then, we iterate once over the remain-ing documents.
For each particle n, we computea probability distribution over K categories basedon the simplified posterior distribution as definedin equation (2) (Sampling/Prediction), with co-occurrence counts based on the information fromall previously encountered documents.
Thus, weobtain a distribution over N ?K possible assign-ments.
From this distribution we sample withreplacement N new particles, assign the currentdocument to the corresponding category (Resam-pling), and proceed to the next input document.5 Experimental SetupThe goal of our experimental evaluation is to as-sess the quality of the inferred clusters by compar-ison to a gold standard and an existing graph-basedmodel of category acquisition.
In addition, we areinterested in the incremental version of the model,whether it is able to learn meaningful categoriesand how these change over time.
In the following,we give details on the corpora we used, describehow model parameters were selected, and explainour evaluation procedure.5.1 DataAll our experiments were conducted on a lem-matized version of the British National Corpus(BNC).
The corpus was further preprocessed byremoving stopwords and infrequent words (occur-ring less than 800 times in the BNC).The model output was evaluated against a goldstandard set of categories which was created bycollating the resources developed by Fountain and252Lapata (2010) and Vinson and Vigliocco (2008).Both datasets contain a classification of nouns into(possibly multiple) semantic categories producedby human participants.
We therefore assume thatthey represent psychologically salient categorieswhich the cognitive system is in principle capableof acquiring.
After merging the two resources, andremoving duplicates we obtained 42 semantic cat-egories for 555 nouns.
We split this gold standardinto a development (41 categories, 492 nouns) anda test set (16 categories, 196 nouns).2The input to our model consists of short chunksof text, namely a target word centered in a sym-metric context window of five words (see (1)).In our experiments, the set of target words corre-sponds to the set of nouns in the evaluation dataset.Target word mentions and their context are ex-tracted from the BNC.5.2 Parameters for the BayesCat ModelWe optimized the hyperparameters of theBayesCat model on the development set.For the particle filter, the optimal values are?
= 0.7,?
= 0.1,?
= 0.1.
We used the samevalues for the Gibbs Sampler since it provedinsensitive to hyperparameter variations.
We runthe Gibbs sampler for 200 iterations3and reportresults averaged over 10 runs.
For the PF, we setthe number of particles to 500, and report finalscores averaged over 10 runs.
For evaluation,we take the clustering from the particle with thehighest weight4.5.3 Model ComparisonChinese Whispers We compared our approachwith Fountain and Lapata (2011) who present anon-parametric graph-based model for categoryacquisition.
Their algorithm incrementally con-structs a graph from co-occurrence counts of tar-get words and their contexts (they use a symmetriccontext window of five words).
Target words con-stitute the nodes of the graph, their co-occurrencesare transformed into a vector of positive PMI val-ues, and graph edges correspond to the cosine sim-ilarity between the PMI-vectors representing anytwo nodes.
They use Chinese Whispers (Biemann,2006) to partition a graph into categories.2The dataset is available from www.frermann.de/data.3We checked for convergence on the development set.4While in theory particles should be averaged, we foundthat eventually they became highly similar ?
a commonproblem known as sample impoverishment, which we plan totackle in the future.
Nevertheless, diversity among particlesis present in the initial learning phase, when uncertainty isgreatest, so the model still benefits from multiple hypotheses.We replicated the bag-of-words model pre-sented in Fountain and Lapata (2011) and assessedits performance on our training corpora and testsets.
The scores we report are averaged over 10runs.Chinese Whispers can only make hard cluster-ing decisions, whereas the BayesCat model re-turns a soft clustering of target nouns.
In orderto be able to compare the two models, we con-vert the soft clusters to hard clusters by assign-ing each target word w to category c such thatcat(w) = maxcP(w|c) ?P(c|w).LDA We also compared our model to a standardtopic model, namely Latent Dirichlet Allocation(LDA; Blei et al.
2003).
LDA assumes that a docu-ment is generated from an individual mixture overtopics, and each topic is associated with one worddistribution.
We trained a batch version of LDAusing input identical to our model and the Mallettoolkit (McCallum, 2002).Chinese Whispers is a parameter-free algorithmand thus determines the number of clusters auto-matically.
While the Bayesian models presentedhere are parametric in that an upper bound for thepotential number of categories needs to be speci-fied, the models themselves decide on the specificvalue of this number.
We set the upper bound ofcategories to 100 for LDA as well as the batch andincremental version of the BayesCat model.5.4 Evaluation MetricsOur aim is to learn a set of clusters each of whichcorresponds to one gold category, i.e., it containsall and only members of that gold category.
Wereport evaluation scores based on three metricswhich measure this tradeoff.
Since in unsuper-vised clustering the cluster IDs are meaningless,all evaluation metrics involve a mapping from in-duced clusters to gold categories.
The first twometrics described below perform a cluster-basedmapping and are thus not ideal for assessing theoutput of soft clustering algorithms.
The thirdmetric performs an item-based mapping and canbe directly used to evaluate soft clusters.Purity/Collocation are based on member over-lap between induced clusters and gold classes(Lang and Lapata, 2011).
Purity measures the de-gree to which each cluster contains instances thatshare the same gold class, while collocation mea-sures the degree to which instances with the samegold class are assigned to a single cluster.
We re-port the harmonic mean of purity and collocation253as a single measure of clustering quality.V-Measure is the harmonic mean betweenhomogeneity and collocation (Rosenberg andHirschberg, 2007).
Like purity, V-Measureperforms cluster-based comparisons but is anentropy-based method.
It measures the condi-tional entropy of a cluster given a class, and viceversa.Cluster-F1 is an item-based evaluation metricwhich we propose drawing inspiration from thesupervised metric presented in Agirre and Soroa(2007).
Cluster-F1 maps each target word type toa gold cluster based on its soft class membership,and is thus appropriate for evaluation of soft clus-tering output.
We first create a K?G soft map-ping matrix M from each induced category kitogold classes gjfrom P(gj|ki).
We then map eachtarget word type to a gold class by multiplyingits probability distribution over soft clusters withthe mapping matrix M , and taking the maximumvalue.
Finally, we compute standard precision, re-call and F1 between the mapped system categoriesand the gold classes.6 ResultsOur experiments are designed to answer threequestions: (1) How do the induced categoriesfare against gold standard categories?
(2) Arethere performance differences between BayesCatand Chinese Whispers, given that the two modelsadopt distinct mechanisms for representing lexicalmeaning and learning semantic categories?
(3) Isour incremental learning mechanism cognitivelyplausible?
In other words, does the quality of theinduced clusters improve over time and how do thelearnt categories differ from the output of an idealbatch learner?Clustering performance for the batch BayesCatmodel (BC-Batch), its incremental version(BC-Inc), Chinese Whispers (CW), and LDAis shown in Table 1.
Comparison of the twoincremental models, namely BC-Inc and CW,shows that our model outperforms CW underall evaluation metrics both on the test and thedevelopment set.
Our BC models perform atleast as well as LDA, despite the more complexlearning objective.
Recall that LDA does not learncategory specific features.
BC-Batch performsbest overall, however this is not surprising.
TheBayesCat model learnt in batch mode uses a Gibbssampler which can be viewed as an ideal learnerwith access to the entire training data at any time,and the ability to systematically revise previousdecisions.
This puts the incremental variant at adisadvantage since the particle filter encountersthe data incrementally and never resamplespreviously seen documents.
Nevertheless, asshown in Table 1 BC-Inc?s performance is veryclose to BC-Batch.
BC-Inc outperforms the Gibbssampler in the PC-F1 metric, because it achieveshigher collocation scores.
Inspection of the outputreveals that the Gibbs sampler induces larger clus-ters compared to the particle filter (as well as lessdistinct clusters).
Although the general pattern ofresults is the same on the development and testsets, absolute scores for all systems are higher onthe test set.
This is expected, since the test setcontains less categories with a smaller number ofexemplars and more accurate clusterings can bethus achieved (on average) more easily.Figure 4 displays the learning curves producedby CW and BC-Inc under the PC-F1 (left) andCluster-F1 (right) evaluation metrics.
UnderPC-F1, CW produces a very steep initial learningcurve which quickly flattens off, whereas no learn-ing curve emerges for CW under Cluster-F1.
TheBayesCat model exhibits more discernible learn-ing curves under both metrics.
We also observethat learning curves for CW indicate much morevariance during learning compared to BC-Inc, ir-respectively of the evaluation metric being used.Figure 4b shows learning curves for BC-Inc whenits output classes are interpreted in two ways,i.e., as soft or hard clusters.
Interestingly, the twocurves have a similar shape which points to theusefulness of Cluster-F1 as an evaluation metricfor both types of clusters.In order to better understand the differences inthe learning process between CW and BC-Inc wetracked the evolution of clusterings over time, aswell as the variance across cluster sizes at eachpoint in time.
The results are plotted in Figure 5.The top part of the figure compares the numberof clusters learnt by the two models.
We see thatthe number of clusters inferred by CW drops overtime, but is closer to the number of clusters presentin the gold standard.
The final number of clus-ters inferred by CW is 26, whereas PF-Inc infers90 clusters (there are 41 gold classes).
The mid-dle plot shows the variance in cluster size inducedat any time by CW which is by orders of magni-tude higher than the variance observed in the out-put of BayesCat (bottom plot).
More importantly,the variance in BayesCat resembles the variancepresent in the gold standard much more closely.The clusterings learnt by CW tend to consist of254Development Set Test SetMetric LDA CW BC-Inc BC-Batch LDA CW BC-Inc BC-BatchPC-F1 (Hard) 0.283 0.211 0.283 0.261 0.446 0.380 0.503 0.413V-Measure (Hard) 0.399 0.143 0.383 0.428 0.572 0.220 0.567 0.606Cluster-F1 (Hard) 0.416 0.301 0.386 0.447 0.521 0.443 0.671 0.693Cluster-F1 (Soft) 0.387 ?
0.484 0.523 0.665 ?
0.644 0.689Table 1: Evaluation of model output against a gold standard.
Results are reported for the BayesCat modeltrained incrementally (BC-Inc) and in batch mode (BC-Batch), and Chinese Whispers (CW).
The typeof clusters being evaluated is shown within parentheses.0.050.10.150.20.250.30.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06PC-F1Number of encountered DocumentsCW (Hard)BC-Inc (Hard)(a)0.150.20.250.30.350.40.450.50.550.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06Cluster-F1Number of encountered DocumentsCW (Hard)BC-Inc (Hard)BC-Inc (Soft)(b)Figure 4: Learning curves for BC-Inc and CW based on PC-F1 (left), and Cluster-F1 (right).
The typeof clusters being evaluated is shown within parentheses.
Results are reported on the development set.few very large clusters and a large number of verysmall (mostly singleton) clusters.
Although someof the bigger clusters are meaningful, the overallstructure of clusterings does not faithfully repre-sent the gold standard.Finally, note that in contrast to CW and LDA,the BayesCat model learns not only how to in-duce clusters of target words, but also informa-tion about their category-specific contexts.
Table 2presents examples of the learnt categories togetherwith their most likely contexts.
For example, oneof the categories our model discovers correspondsto BUILDINGS.
Some of the context words or fea-tures relating to buildings refer to their location(e.g., city, road, hill, north, park), architecturalstyle (e.g., modern, period, estate), and material(e.g., stone).7 DiscussionIn this paper we have presented a Bayesian modelof category acquisition.
Our model learns to groupconcepts into categories as well as their features(i.e., context words associated with them).
Cat-egory learning is performed incrementally, usinga particle filtering algorithm which is a naturalchoice for modeling sequential aspects of lan-guage learning.We now return to our initial questions and sum-marize our findings.
Firstly, we observe thatour incremental model learns plausible linguisticcategories when compared against the gold stan-dard.
Secondly, these categories are qualitativelybetter when evaluated against Chinese Whispers,a closely related graph-based incremental algo-rithm.
Thirdly, analysis of the model?s outputshows that it simulates category learning in twoimportant ways, it consistently improves over timeand can additionally acquire category features.Overall, our model has a more cognitively plau-sible learning mechanism compared to CW, andis more expressive, as it can simulate both cat-egory and feature learning.
Although CW ulti-mately yields some meaningful categories, it doesnot acquire any knowledge pertaining to their fea-tures.
This is somewhat unrealistic given that hu-mans are good at inferring missing features for2550204060801000.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06Number of ClustersNumber of encountered DocumentsBC-IncCWGold01002003004000.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06VarianceNumber of encountered DocumentsCWGold3691215180.0e+00 3.5e+05 7.0e+05 1.0e+06 1.4e+06VarianceNumber of encountered DocumentsBC-IncGoldFigure 5: Number of clusters over time (top).Cluster size variance for CW (middle) and BC-Inc(bottom).
Results shown on the development set.unknown categories (Anderson, 1991).
It is alsosymptomatic of the nature of the algorithm whichdoes not have an explicit learning mechanism.Each node in the graph iteratively adopts (in ran-dom order) the strongest class in its neighborhood(i.e., the set of nodes with which it shares an edge).We also showed that LDA is less appropriate forthe category learning task on account of its for-mulation which does not allow to simultaneouslyacquire clusters and their features.There are several options for improving ourmodel.
The learning mechanism presented hereis the most basic of particle methods.
A commonproblem in particle filtering is sample impoverish-ment, i.e., particles become highly similar after afew iterations, and do not optimally represent thesample space.
More involved resampling methodssuch as stratified sampling or residual resampling,have been shown to alleviate this problem (Douc,2005).From a cognitive perspective, the most obviousweakness of our algorithm is its strict incremen-tality.
While our model simulates human mem-BUILDINGSwall, bridge, building, cottage, gate, house, train,bus, stone, chapel, brick, cathedralplan, include, park, city, stone, building, ho-tel, lead, road, hill, north, modern, visit, main,period, cathedral, estate, complete, site, owner,parishWEAPONSshotgun, pistol, knife, crowbar, gun, sledgeham-mer, baton, bullet, motorcycle, van, ambulanceinjure, ira, jail, yesterday, arrest, stolen, fire, of-ficer, gun, police victim, hospital, steal, crash,murder, incident, driver, accident, hitINSTRUMENTStuba, drum, harmonica, bagpipe, harp, violin,saxophone, rock, piano, banjo, guitar, flute, harp-sichord, trumpet, rocker, clarinet, stereo, cello,accordionamp, orchestra, sound, electric, string, sing,song, drum, piano, condition, album, instrument,guitar, band, bass, musicTable 2: Examples of categories induced by the in-cremental BayesCat model (upper row), togetherwith their most likely context words (lower row).ory restrictions and uncertainty by learning basedon a limited number of current knowledge states(i.e., particles), it never reconsiders past catego-rization decisions.
In many linguistic tasks, how-ever, learners revisit past decisions (Frazier andRayner, 1982) and intuitively we would expectcategories to change based on novel evidence, es-pecially in the early learning phase.
In fixed-lagsmoothing, a particle smoothing variant, modelupdates include systematic revision of a fixed setof previous observations in the light of newly en-countered evidence (Briers et al., 2010).
Basedon this framework, we will investigate differentschemes for informed sequential learning.Finally, we would like to compare the model?spredictions against behavioral data, and exam-ine more thoroughly how categories and featuresevolve over time.Acknowledgments We would like to thankCharles Sutton and members of the ILCC at theSchool of Informatics for their valuable feedback.We acknowledge the support of EPSRC throughproject grant EP/I037415/1.256ReferencesAgirre, Eneko and Aitor Soroa.
2007.
Semeval-2007 task 02: Evaluating word sense induc-tion and discrimination systems.
In Proceedingsof the 4th International Workshop on SemanticEvaluations.
Prague, Czech Republic, pages 7?12.Anderson, John R. 1991.
The adaptive nature ofhuman categorization.
Psychological Review98:409?429.Biemann, Chris.
2006.
Chinese Whispers - an effi-cient graph clustering algorithm and its applica-tion to natural language processing problems.
InProceedings of TextGraphs: the 1st Workshopon Graph Based Methods for Natural LanguageProcessing.
New York City, pages 73?80.Blei, David M., Andrew Y. Ng, and Michael I. Jor-dan.
2003.
Latent Dirichlet allocation.
Journalof Machine Learning Research 3:993?1022.Bomba, Paul C. and Eimas R. Siqueland.
1983.The nature and structure of infant form cate-gories.
Journal of Experimental Child Psychol-ogy 35:294?328.Borschinger, Benjamin and Mark Johnson.
2011.A particle filter algorithm for Bayesian wordsegmentation.
In Proceedings of the Aus-tralasian Language Technology AssociationWorkshop.
Canberra, Australia, pages 10?18.Briers, Mark, Arnaud Doucet, and Simon Maskell.2010.
Smoothing algorithms for state-spacemodels.
Annals of the Institute of StatisticalMathematics 62(1):61?89.Brody, Samuel and Mirella Lapata.
2009.Bayesian word sense induction.
In Proceedingsof the 12th Conference of the European Chapterof the ACL.
Athens, Greece, pages 103?111.Cree, George S., Ken McRae, and Chris McNor-gan.
1999.
An attractor model of lexical con-ceptual processing: Simulating semantic prim-ing.
Cognitive Science 23(3):371?414.Douc, Randal.
2005.
Comparison of resamplingschemes for particle filtering.
In 4th Interna-tional Symposium on Image and Signal Pro-cessing and Analysis.
Zagreb, Croatia, pages64?69.Doucet, Arnaud, Nando de Freitas, and Neil Gor-don.
2001.
Sequential Monte Carlo Methods inPractice.
Springer, New York.Fearnhead, Paul.
2004.
Particle filters for mix-ture models with an unknown number of com-ponents.
Statistics and Computing 14(1):11?21.Fountain, Trevor and Mirella Lapata.
2010.
Mean-ing representation in natural language catego-rization.
In Proceedings of the 32nd AnnualConference of the Cognitive Science Society.Portland, Oregon, pages 1916?1921.Fountain, Trevor and Mirella Lapata.
2011.
In-cremental models of natural language categoryacquisition.
In Proceedings of the 33nd An-nual Conference of the Cognitive Science Soci-ety.
Boston, Massachusetts, pages 255?260.Frazier, Lyn and Keith Rayner.
1982.
Making andcorrecting errors during sentence comprehen-sion: Eye movements in the analysis of struc-turally ambiguous sentences.
Cognitive Psy-chology 14(2):178?210.Geman, Stuart and Donald Geman.
1984.
Stochas-tic relaxation, Gibbs distributions and theBayesian restoration of images.
IEEE Trans-actions on Pattern Analysis and Machine Intel-ligence 6(6):721?741.Griffiths, Thomas L., Kevin R. Canini, Adam N.Sanborn, and Daniel J. Navarro.
2007.
Unifyingrational models of categorization via the hierar-chical Dirichlet process.
In Proceedings of the29th Annual Conference of the Cognitive Sci-ence Society.
Nashville, Tennessee, pages 323?328.Griffiths, Thomas L., Adam N. Sanborn, Kevin R.Canini, John D. Navarro, and Joshua B. Tenen-baum.
2011.
Nonparametric Bayesian mod-els of categorization.
In Emmanuel M. Pothosand Andy J. Wills, editors, Formal Approachesin Categorization, Cambridge University Press,pages 173?198.Harris, Zellig.
1954.
Distributional structure.Word 10(23):146?162.Jern, Alan and Charles Kemp.
2013.
A proba-bilistic account of exemplar and category gen-eration.
Cognitive Psychology 66:85?125.Kemp, Charles, Patrick Shafto, and Joshua B.Tenenbaum.
2012.
An integrated account ofgeneralization across objects and features.
Cog-nitive Psychology 64:35?75.Lang, Joel and Mirella Lapata.
2011.
Unsuper-vised semantic role induction with graph par-titioning.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural Lan-guage Processing.
Edinburgh, Scotland, UK.,pages 1320?1331.Levy, Roger P., Florencia Reali, and Thomas L.Griffiths.
2009.
Modeling the effects of mem-257ory on human online sentence processing withparticle filters.
In D. Koller, D. Schuurmans,Y.
Bengio, and L. Bottou, editors, Advancesin Neural Information Processing Systems 21,pages 937?944.McCallum, Andrew Kachites.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.McRae, Ken, George S. Cree, Mark S. Seidenberg,and Chris McNorgan.
2005.
Semantic featureproduction norms for a large set of living andnonliving things.
Behavioral Research Methods37(4):547?59.Medin, Douglas L. and Marguerite M. Schaffer.1978.
Context theory of classification learning.Psychological Review 85(3):207?238.Nosofsky, Robert M. 1988.
Exemplar-basedaccounts of relations between classification,recognition, and typicality.
Journal of Exper-imental Psychology: Learning, Memory, andCognition 14:700?708.Posner, Michael I. and Steven W. Keele.
1968.
Onthe genesis of abstract ideas.
Journal of Exper-imental Psychology 21:367?379.Quinn, Paul C. and Peter D. Eimas.
1996.
Percep-tual cues that permit categorical differentiationof animal species by infants.
Journal of Exper-imental Child Psychology 63:189?211.Reed, Stephen K. 1972.
Pattern recognition andcategorization.
Cognitive psychology 3(3):382?407.Riordan, Brian and Michael N. Jones.
2011.
Re-dundancy in perceptual and linguistic experi-ence: Comparing feature-based and distribu-tional models of semantic representation.
Top-ics in Cognitive Science 3(2):303?345.Rosenberg, Andrew and Julia Hirschberg.
2007.V-measure: A conditional entropy-based ex-ternal cluster evaluation measure.
In Proceed-ings of the 2007 Joint Conference on Empiri-cal Methods in Natural Language Processingand Computational Natural Language Learn-ing.
Prague, Czech Republic, pages 410?420.Sanborn, Adam N., Thomas L. Griffiths, andDaniel J. Navarro.
2006.
A more rational modelof categorization.
In Proceedings of the 28thAnnual Conference of the Cognitive Science So-ciety.
Vancouver, Canada, pages 726?731.Smith, Edward E. and Douglas L. Medin.
1981.Categories and Concepts.
Harvard UniversityPress, Cambridge, MA, USA.Starkey, David.
1981.
The origins of concept for-mation: Object sorting and object preference inearly infancy.
Child Development pages 489?497.Storms, Gert, Paul De Boeck, and Wim Ruts.2000.
Prototype and exemplar-based informa-tion in natural language categories.
Journal ofMemory and Language 42:51?73.Vinson, David and Gabriella Vigliocco.
2008.
Se-mantic feature production norms for a large setof objects and events.
Behavior Research Meth-ods 40(1):183?190.Voorspoels, Wouter, Wolf Vanpaemel, and GertStorms.
2008.
Exemplars and prototypes innatural language concepts: A typicality-basedevaluation.
Psychonomic Bulletin & Review15(3):630?637.Yao, Xuchen and Benjamin Van Durme.
2011.Nonparametric Bayesian word sense induc-tion.
In Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Process-ing.
Portland, Oregon, pages 10?14.Zeigenfuse, Matthew D. and Michael D. Lee.2010.
Finding the features that represent stim-uli.
Acta Psychologica 133(3):283?295.258
