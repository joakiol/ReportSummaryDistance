Learning optimal dialogue management rules by usingreinforcement learning and inductive logic programmingRenaud Lec?ucheMicrosoft CorporationCompass House, 80-82 Newmarket roadCambridge CB5 8DZ, United Kingdomrenaudle@microsoft.comAbstractDeveloping dialogue systems is a complex pro-cess.
In particular, designing efficient dialoguemanagement strategies is often difficult as thereare no precise guidelines to develop them and nosure test to validate them.
Several suggestionshave been made recently to use reinforcementlearning to search for the optimal managementstrategy for specific dialogue situations.
Theseapproaches have produced interesting results,including applications involving real world dia-logue systems.
However, reinforcement learningsuffers from the fact that it is state based.
Inother words, the optimal strategy is expressedas a decision table specifying which action totake in each specific state.
It is therefore diffi-cult to see whether there is any generality acrossstates.
This limits the analysis of the optimalstrategy and its potential for re-use in other di-alogue situations.
In this paper we tackle thisproblem by learning rules that generalize thestate-based strategy.
These rules are more read-able than the underlying strategy and thereforeeasier to explain and re-use.
We also investi-gate the capability of these rules in directingthe search for the optimal strategy by lookingfor generalization whilst the search proceeds.1 IntroductionAs dialogue systems become ubiquitous, dia-logue management strategies are receiving moreand more attention.
They define the system be-havior and mainly determine how well or badlyit is perceived by users.
Generic methodolo-gies exist for developing and testing manage-ment strategies.
Many of these take a user-centric approach based on Wizard of Oz stud-ies and iterative design (Bernsen et al, 1998).However there are still no precise guidelinesabout when to use specific techniques such asmixed-initiative.
Reinforcement learning hasbeen used in several recent approaches to searchfor the optimal dialogue management strategyfor specific dialogue situations (Levin and Pier-accini, 1997; Litman et al, 2000; Singh et al,2000; Walker, 2000).
In these approaches, a dia-logue is seen as a walk through a series of states,from an initial state when the dialogue beginsuntil a terminal state when the dialogue ends.The actions of the dialogue manager as well asthose of the user influence the transitions be-tween states.
Each transition is associated witha reward, which expresses how good or bad itwas to make that transition.
A dialogue strat-egy is then seen as a Markov Decision Process(Levin et al, 1998).
Reinforcement learning canbe used in this framework to search for an op-timal strategy, i.e., a strategy that makes theexpected sum of rewards maximal for all thetraining dialogues.
The main idea behind re-inforcement learning is to explore the space ofpossible dialogues and select the strategy whichoptimizes the expected rewards (Mitchell, 1997,ch.
13).
Once the optimal strategy has beenfound, it can be implemented in the final sys-tem.Reinforcement learning is state-based.
Itfinds out which action to take next given thecurrent state.
This makes the explanation of thestrategy relatively hard and limits its potentialre-use to other dialogue situations.
It is quitedifficult to find out whether generic lessons canbe learned from the optimal strategy.
In this pa-per we use inductive logic programming (ILP)to learn sets of rules that generalize the optimalstrategy.
We show that these can be simpler tointerpret than the decision tables given by re-inforcement learning and can help modify andre-use the strategies.
This is important as hu-man dialogue designers are usually ultimately incharge of writing and changing the strategies.The paper is organized as follows.
We firstdescribe in section 2 a simple dialogue systemwhich we use as an example throughout the pa-per.
In section 3, we present our method andresults on using ILP to generalize the optimaldialogue management strategy found by rein-forcement learning.
We also investigate the useof the rules learned during the search of the op-timal strategy.
We show that, in some cases,the number of dialogues needed to obtain theoptimal strategy can be dramatically reduced.Section 4 presents our current results on thisaspect.
We compare our approach to other cur-rent pieces of work in section 5 and conclude thepaper in section 6.2 Example dialogue systemIn this section we present a simple dialogue sys-tem that we use in the rest of the paper to de-scribe and explain our results.
This system willbe used with automated users in order to sim-ulate dialogues.
The aim of the system is tobe simple enough so that its operation is easyto understand while being complex enough toallow the study of the phenomena we are in-terested in.
This will provide a simple way toexplain our approach.We chose a system whose goal is to find valuesfor three pieces of information, called, unorigi-nally, a, b and c. In a practical system such asan automated travel agent for example, thesevalues could be departure and arrival cities andthe time of a flight.We now describe the system in terms ofstates, transitions, actions and rewards, whichare the basic notions of reinforcement learning.The system has four actions at its disposal: pre-pare to ask (prepAsk) a question about one ofthe pieces of information, prepare to recognize(prepRec) a user?s utterance about a piece ofinformation, ask and recognize (ask&recognize)which outputs all the prepared questions andtries to recognize all the expected utterances,and end (end) which terminates the dialogue.We chose these actions as they are common, inone form or another, in most speech dialoguesystems.
To get a specific piece of information,the system must prepare a question about itand expect a user utterance as an answer be-fore carrying out an ask&recognize action.
Thesystem can try to get more than one piece ofinformation in a single ask&recognize action bypreparing more than one question and prepar-ing to recognize more than one answer.Actions are associated with rewards or penal-ties.
Every system action, except ending, has apenalty of -5 corresponding to some imaginedprocessing cost.
Ending provides a reward of100 times the number of pieces of informationknown when the dialogue ends.
We hope thatthese numbers simulate a realistic reward func-tion.
They could be tuned to reflect user satis-faction for a real dialogue manager.The state of the system represents whichpieces of information are known or unknownand what questions and recognitions have beenprepared.
There is also a special end state.
Forthis example, there are 513 different states.Pieces of information become known whenusers answer the system?s questions.
In our tu-torial example, we used automated users.
Theseusers always give one piece of information ifproperly asked as explained above, and answerpotential further questions with a decreasingprobability (0.5 for a second piece of informa-tion, and 0.25 for a third in our example).
Wecould tune these probabilities to reflect real userbehavior.
Using simulated users enables us toquickly train our system.
It could also allowus to test the usefulness of ILP under differentconditions.3 Learning rules from optimalstrategyIn this section we explain how we obtain andinterpret rules expressing the optimal manage-ment strategy found by reinforcement learningfor the system presented in section 2 as well asa more realistic one.3.1 Example systemWe first search for the optimal strategy of ourexample system by using reinforcement learn-ing.
We do this by having repetitive dialogueswith the automated users and evaluating theaverage reward of the actions taken by the sys-tem.
When deciding what to do in each state,we choose the up-to-now best action with prob-ability 0.8 and other actions with uniform prob-ability totaling 0.2.
This allows the system toexplore the dialogue space while preferably fol-lowing the best strategy found.
The optimalState Action{unknown(c), unknown(b), unknown(a)} prepRec(a){unknown(c), unknown(b), unknown(a), prepRec(a)} prepAsk(a){unknown(c), unknown(b), unknown(a), prepRec(a),prepAsk(a)}ask&recognize{unknown(c), unknown(b), known(a)} prepRec(b){unknown(c), unknown(b), known(a), prepRec(b)} prepAsk(b){unknown(c), unknown(b), known(a), prepRec(b), prepAsk(b)} ask&recognize{unknown(c), known(b), known(a)} prepRec(c){unknown(c), known(b), known(a), prepRec(c)} prepAsk(c){unknown(c), known(b), known(a), prepRec(c), prepAsk(c)} ask&recognize{known(c), known(b), known(a)} endTable 1: Tutorial example optimal strategystrategy for the tutorial example is shown intable 1.
This strategy is very simple: ask onepiece of information at a time until all the pieceshave been collected, and then end the dialogue.A typical dialogue following this strategy wouldsimply go like this, using the travel agent exam-ple:S: Where do you want to leave from?U: Cambridge.S: Where do you want to go to?U: Seattle.S: When do you want to travel?U: Tomorrow.Then, in order to learn rules generalizing theoptimal strategy, we use foidl.
foidl is a pro-gram which learns first-order rules from exam-ples (Mooney and Califf, 1995; Mitchell, 1997,ch.
10).
foidl starts with rules without condi-tions and then adds further terms so that theycover the examples given but not others.
Inour case, rule conditions are about propertiesof states and rule actions are the best actionsto take.
Some advantages of foidl are that itcan learn from a relatively small set of posi-tive examples without the need for explicit neg-ative examples and that it uses intentional back-ground knowledge (Califf and Mooney, 1998).foidl has two main learning modes.
Whenthe examples are functional, i.e., for each statethere is only one best action, foidl learns a setof ordered rules, from the more generic to themore specific.
When applying the rules only thefirst most generic rule whose precondition holdsneeds to be taken into account.
When the ex-amples are not functional, i.e., there is at leastone state where two actions are equally good,foidl learns a bag of rules.
All rules whose pre-conditions hold are applied.
Ordered rules areusually easier to understand.
In this paper, weuse foidl in both modes: functional mode forthe tutorial example and non-functional modefor the other example.The rules learned by foidl from the optimalstrategy are presented in table 2.
Precondi-tions on states express the required and suffi-cient conditions for the action to be taken fora state.
Uppercase letters represent variables(a` la Prolog) which can unify with a, b or c.Rules were learned in functional mode.
Themore generic rules are at the bottom of the ta-ble and the more specific at the top.
It can bequite clearly seen from these rules that the strat-egy is composed of two kinds of rules: orderingrules which indicate in what order the variablesshould be obtained, and generic rules, typesetin italic, which express the strategy of obtainingone piece of information at a time.
The order-ing, which is a, then b, then c, is arbitrary.
Itwas imposed by the reinforcement learning algo-rithm.
The general strategy consists in prepar-ing to ask whatever piece of information the or-dering rules have decided to recognize, and thenasking and recognizing a piece of information assoon as we can.
By expressing the strategy inthe form of rules it is apparent how it operates.It would then be relatively easy for a dialogueengineer to implement a strategy that keeps theoptimal one-at-a-time questioning strategy butdoes not necessarily impose the same order onPreconditions on state Actionunknown(a) prepRec(a)unknown(b), known(X) prepRec(b)known(b), unknown(X) prepRec(c)known(c) endprepAsk(X) ask&recognizeunknown(X), prepRec(X) prepAsk(X)Table 2: Tutorial example optimal rulesthe process.3.2 Real world exampleAlthough the tutorial example showed how rulescould be obtained and interpreted, it does notsay much about the practical use of our ap-proach for real-world dialogues.
In order tostudy this, we applied foidl to the optimalstrategy presented in (Litman et al, 2000),which ?presents a large-scale application of RL[reinforcement learning] to the problem of op-timizing dialogue strategy selection [...]?.
Thissystem is a more realistic application than ourintroductory example.
It has been used by hu-man users over the phone.
The dialogue isabout activities in New Jersey.
A user stateshis/her preferences for a type of activity (mu-seum visit, etc.)
and availability, and the sys-tem retrieves potential activities.
The systemcan vary its dialogue strategy by allowing or notallowing users to give extra information whenanswering a question.
It can also decide to con-firm or not to confirm a piece of information ithas received.The state of the system represents whatpieces of information have been obtained andsome information on how the dialogue hasevolved so far.
This information is repre-sented by variables indicating, in column or-der in table 3, whether the system has greetedthe user (0=no, 1=yes), which piece of infor-mation the system wants (1, 2 or 3), whatwas the confidence in the user?s last an-swer (0=low, 1=medium, 2=high, 3=accept,4=deny), whether the system got a value for thepiece of information (0=no, 1=yes), the numberof times the system asked for that piece of in-formation, whether the last question was open-ended (0=open-ended, 1=restrictive), and howwell the dialogue was going (0=bad, 1=good).Preconditions on state Actiongreetingsinfo.#confidencevalue question#open/closedhistoryB 1 0 1 F G H expconf(1)B 1 1 E F G H expconf(1)B 1 2 E F G H noconf(1)B 1 4 E 0 G H reaskm(1)B 1 D E 1 G H reasks(1)B 2 0 1 F G H expconf(2)1 2 2 1 0 0 0 noconf(2)B 2 0 E F 1 1 noconf(2)B 2 2 E F 1 H noconf(2)B 2 D 0 1 G H reaskm(2)B 3 D 1 F G 1 expconf(3)B 3 D 1 F 0 H expconf(3)1 3 1 1 0 1 0 noconf(3)1 3 1 1 0 0 1 noconf(3)1 3 2 1 0 0 1 noconf(3)B 3 0 E F G 0 noconf(3)B 3 0 E F 0 H noconf(3)1 C 0 0 0 G H asku(C) [2]B C 4 E 0 G H reasks(C) [3]B C 4 0 F G 1 reaskm(C) [2]B C 2 E F 0 1 expconf(C) [2]B C 1 1 F 0 H expconf(C) [5]B C 2 E F 1 0 noconf(C) [3]0 C D E F G H greetu [1]Table 3: NJFun optimal rulesSee Litman et al (2000) for a more detailed ex-planation of the state representation.
The ac-tions the system can take are: greeting users(greetu), asking questions to users (asku), re-asking questions to users with an open or re-strictive question (reaskm/reasks), asking forconfirmation or not (expconf/noconf).
The op-timal strategy is composed of 42 state-actionpairs.
It can be reduced to 24 equivalent rules.We present the rules in table 3.
Some of theserules are very specific to the state they apply to.The more generic ones, which are valid what-ever the exact piece of information being asked,are typeset in italic.
The number of states theygeneralize is indicated in brackets.These rules can be divided into four cate-gories:Asking The first rule simply states that asking(asku) is the best thing to do if we havenever asked the value of a piece of informa-tion before.Re-asking The second rule states that the sys-tem should re-ask for a value with a re-stricted grammar (reasks), i.e., a gram-mar that does not allow mixed-initiative,if the previous attempt was made with anopen-ended grammar and the user deniedthe value obtained.
The third rule statesthat re-asking with an open-ended question(reaskm) is fine when the user denied thevalue obtained but the dialogue was goingwell until now.Confirming The fourth and fifth rules statethat the system should explicitly confirm(expconf) a value if the grammar to getit was open-ended, the confidence in thevalue obtained being medium or even high.No confirmation (noconf) is needed whenthe confidence is high and the answer wasobtained with a restricted grammar evenwhen the dialogue is going badly.Greeting The last rule indicates that the sys-tem should greet the user if it has not doneso already.When preconditions hold for more than onerule, which can for example be the case forreasks and reaskm in some situations, all theactions allowed by the activated rules are pos-sible.The generic rules are more explicit than thestate-based decision table given by reinforce-ment learning.
For example, the rules aboutasking and greeting are obvious and it is reas-suring that the approach suggests them.
Theeffects of open-ended or closed questions on thereasking and confirming policies also becomemuch more apparent.
Restricting the potentialinputs is the best thing to do when re-askingexcept if the dialogue was going well until thatpoint.
In that case the system can risk havingan open-ended grammar.
The rules on confir-mation show the preference to confirm if thevalue was obtained via an open-ended grammarand that no confirmation is required if the sys-tem has high confidence in a value asked viaa closed grammar even if the dialogue is goingbadly.
Because the rules enable us to better un-derstand what the optimal policy does, we mayIterations Best score Best score(without rules) (with rules)5000 154.13 154.1310000 153.78 195.9715000 183.36 220.3420000 193.47 227.44 (*)25000 210.77 231.43 (*)30000 224.32 231.68 (*)35000 224.40 231.71 (*)40000 228.60 231.72 (*)45000 228.95 231.72 (*)50000 230.44 (*) 231.72 (*)Table 4: Effect of using rules during learningbe able to re-use the strategy learned in thisspecific situation in other dialogue situations.It should be noted that the generic rules gen-eralize only a part of the total strategy (18states out of 42 in the example).
Therefore a lotremains to be explained about the less genericrules.
For example, the second piece of infor-mation does not require confirmation even if wegot it with a low confidence value if the gram-mar was restrictive and the dialogue going well.Under the same conditions the first piece of in-formation would require a confirmation.
Theunderlying reasons for these differences are notclear.
Some of the decisions made by the re-inforcement learning algorithm are also hard toexplain, whether in the form of rules or not.
Forexample, the optimal strategy states that thethird piece of information does not require con-firmation if we got it with low confidence andthe dialogue was going badly.
It is difficult toexplain why this is the best action to take.4 Learning optimal strategy usingrulesIn this section, we discuss the use of rules duringlearning.
Since rules can generalize the optimalstrategy as we saw in the previous section, itis interesting to see whether they can general-ize strategies obtained during training.
If therules can generalize the up-to-now best strat-egy, we may then be able to benefit from therules to guide the search for the optimal strat-egy throughout the search space.
In order totest this, we ran the same reinforcement learn-ing algorithm to find out the optimal policy inthe same setting as in the example system of sec-tion 3.
We also ran the same algorithm but thistime we stopped it every 5000 iterations.
Aniteration corresponds to a transition betweenstates in the dialogue.
We then searched forrules summarizing the best policy found untilthen.
We took the generic rules found, i.e., notthe ones that are specific to a particular state,and used these to direct the search.
That is tosay, when a rule applied we chose to take theaction it suggested rather than the action sug-gested by the state?s values (this is still sub-jected to the 0.8 probability selection).
Theidea behind this was that, if the rules gener-alize correctly the best strategy, following therules would guide us more quickly to the bestpolicy than a blind exploration.
It should benoted that the underlying representation is stillstate-based, i.e., we do not generalize the stateevaluation function.
Our method is thereforeguaranteed to find the optimal policy even ifthe actions suggested by the rules are not theright ones.Table 4 summarizes the value of the best pol-icy found after each step of 5000 iterations.
Astar (*) indicates that the optimal strategy hasbeen consistently found.
As can be seen fromthis table, using rules during learning improvedthe value of the best strategy found so far andreduced the number of iterations needed to findthe optimal strategy for this particular example.The main effect of using rules seems to be thestabilization of the search on the optimal policy.The search without rules finds the optimal pol-icy but then goes off track before coming backto it.
This may not be always the case1 sincethe best strategy found at first may not be opti-mal at all (for example, a rather good strategyat first is to end the dialogue immediately sinceit avoids negative rewards), or the dialogue maynot be regular enough for rules to be useful.
Inthese cases using rules may well be detrimen-tal.
Nevertheless it is important to see that1We do not claim any statistical evidence since weran only a limited set of experiments on the effects ofrules and present just one here.
Even if we ran enoughexperiments to get statistically significant results, theywould be of little use as they would depend on a par-ticular type of dialogues.
Much more work needs to bedone to evaluate the influence of rules on reinforcementlearning and, if possible, in which conditions they areuseful.rules can help reduce, in this case by a factor of2, the number of iterations needed to find theoptimal strategy.
Computationally, using rulesmay not be much different than not using themsince the benefits of fewer reinforcement learn-ing cycles are counter-balanced by the inductivelearning costs.
However, requiring fewer train-ing dialogues is still an important advantage ofthis method.
This is especially true for systemsthat train online with real users rather than sim-ulated ones.
In this case, example dialogues arean expensive commodity and reducing the needfor training dialogues is beneficial.5 DiscussionRecent work on reinforcement learning and di-alogue management has mainly focused on howto reduce the search space for the optimal strat-egy.
Because reinforcement learning is statebased and there may potentially be a large num-ber of states, problems may arise when few di-alogues are available and the data too sparse toselect the best strategy.
States can usually becollapsed to make this problem less acute.
Themain idea here is to express the state of thedialogue by a limited number of features whilekeeping enough and the right kind of informa-tion to be able to learn useful strategies (Walkeret al, 1998).
There has also been new researchon how to model the dialogue with partially ob-servable Markov models (Roy et al, 2000).Some work has also been done on finding outrules to select dialogue management strategies.For example, Litman and Pan (2000) use ma-chine learning to learn rules detecting when di-alogues go badly.
The dialogue manager usesa strategy predefined by a dialogue designer.If a rule detects a bad dialogue, the dialoguestrategy is changed to a more restrictive, moresystem guided strategy.
Our approach is dif-ferent from that work since the strategy is notpredefined but based on the optimal strategyfound by reinforcement learning.
Our rules notonly detect in principle when a dialogue is go-ing badly but also indicate which action to take.The efficiency of the rules obviously depends onthe way the optimal strategy search space hasbeen modeled and other conditions influencinglearning.Some pieces of work have been concernedwith natural language processing from an induc-tive logic programming point of view.
Notably,work on morphology (Mooney and Califf, 1995)and parsing (Thompson et al, 1997) has beencarried out.
However, as far as we know, theapplication of inductive logic programming todialogue management is new.6 ConclusionIn this paper, we presented an approach forfinding and expressing optimal dialogue strate-gies.
We suggested using inductive logic pro-gramming to generalize the results given by re-inforcement learning methods.
The resultingrules are more explicit than the decision tablesgiven by reinforcement learning alone.
This al-lows dialogue designers to better understand theeffect of the optimal strategy and improves po-tential re-use of the strategies learned.
We alsoshow that in some situations rules may have abeneficial effect when used during learning.
Byguiding the search based on the best strategyfound so far, they can direct a reinforcementlearning program towards the optimal strategy,thus reducing the amount of training dialoguesneeded.
More work needs to be done to deter-mine, if possible, under which conditions suchimprovements can be obtained.ReferencesNiels Ole Bernsen, Hans Dybkj?r, andLaila Dybkj?r.
1998.
Designing interactivespeech systems.
Springer-Verlag.Mary Elaine Califf and Raymond J. Mooney.1998.
Advantages of decision lists andimplicit negatives in inductive logic pro-gramming.
New Generation Computing,16(3):263?281.Esther Levin and Roberto Pieraccini.
1997.
Astochastic model of computer-human interac-tion for learning dialogue strategies.
Techni-cal Report 97.28.1, AT&T Labs Research.Esther Levin, Roberto Pieraccini, and WielandEckert.
1998.
Using Markov decision processfor learning dialogue strategies.
In Proceed-ings of the IEEE international conference onacoustics, speech and signal processing, Seat-tle, USA, May.Diane J. Litman and Shimei Pan.
2000.
Pre-dicting and adapting to poor speech recogni-tion in a spoken dialogue system.
In Proceed-ings of the 17th national conference on arti-ficial intelligence, Austin, Texas, USA, Au-gust.Diane J. Litman, Michael S. Kearns, Satin-der B. Singh, and Marilyn A. Walker.
2000.Automatic optimization of dialogue manage-ment.
In Proceedings of the 18th interna-tional conference on computational linguis-tics, Saarbru?cken, Luxembourg, Nancy, July.Tom M. Mitchell.
1997.
Machine learning.McGraw-Hill.Raymond J. Mooney and Mary Elaine Califf.1995.
Induction of first-order decision lists:Results on learning the past tense of Englishverbs.
Journal of Artificial Intelligence Re-search, 3:1?24.Nicholas Roy, Joelle Pineau, and SebastianThrun.
2000.
Spoken dialogue managementusing probabilistic reasoning.
In Proceedingsof the 38th annual meeting of the Associationfor Computational Linguistics, Hong-Kong,October.Satinder B. Singh, Michael S. Kearns, Diane J.Litman, and Marylin A. Walker.
2000.
Em-pirical evaluation of a reinforcement learningspoken dialogue system.
In Proceedings of the17th national conference on artificial intelli-gence, Austin, USA, July.Cynthia A. Thompson, Raymond J. Mooney,and Lappoon R. Tang.
1997.
Learning toparse natural language database queries intological forms.
In Proceedings of the work-shop on automata induction, grammatical in-ference, and language acquisition, Nashville,Tennessee, USA, July.Marilyn A. Walker, Jeanne C. Fromer, andShrikanth Narayanan.
1998.
Learning opti-mal dialogue strategies: A case study of aspoken dialogue agent for email.
In Proceed-ings of the 17th international conference oncomputational linguistics and the 36th annualmeeting of the Association for ComputationalLinguistics, Montreal, Quebec, Canada, Au-gust.Marilyn Walker.
2000.
An application of rein-forcement learning to dialogue strategy selec-tion in a spoken dialogue system for email.Journal of Artificial Intelligence Research,12:387?416.
