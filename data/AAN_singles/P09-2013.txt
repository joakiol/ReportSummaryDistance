Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 49?52,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPA Unified Single Scan Algorithmfor Japanese Base Phrase Chunking and Dependency ParsingManabu SassanoYahoo Japan CorporationMidtown Tower,9-7-1 Akasaka, Minato-ku,Tokyo 107-6211, Japanmsassano@yahoo-corp.jpSadao KurohashiGraduate School of Informatics,Kyoto UniversityYoshida-honmachi, Sakyo-ku,Kyoto 606-8501, Japankuro@i.kyoto-u.ac.jpAbstractWe describe an algorithm for Japaneseanalysis that does both base phrase chunk-ing and dependency parsing simultane-ously in linear-time with a single scan of asentence.
In this paper, we show a pseudocode of the algorithm and evaluate its per-formance empirically on the Kyoto Uni-versity Corpus.
Experimental results showthat the proposed algorithm with the votedperceptron yields reasonably good accu-racy.1 IntroductionSingle scan algorithms of parsing are important forinteractive applications of NLP.
For instance, suchalgorithms would be more suitable for robots ac-cepting speech inputs or chatbots handling naturallanguage inputs which should respond quickly insome situations even when human inputs are notclearly ended.Japanese sentence analysis typically consists ofthree major steps, namely morphological analysis,bunsetsu (base phrase) chunking, and dependencyparsing.
In this paper, we describe a novel algo-rithm that combines the last two steps into a sin-gle scan process.
The algorithm, which is an ex-tension of Sassano?s (2004), allows us to chunkmorphemes into base phrases and decide depen-dency relations of the phrases in a strict left-to-right manner.
We show a pseudo code of the al-gorithm and evaluate its performance empiricallywith the voted perceptron on the Kyoto UniversityCorpus (Kurohashi and Nagao, 1998).2 Japanese Sentence StructureIn Japanese NLP, it is often assumed that the struc-ture of a sentence is given by dependency relationsMeg-ga kare-ni ano pen-wo age-ta.Meg-subj to him that pen-acc give-past.ID 0 1 2 3 4Head 4 4 3 4 -Figure 1: Sample sentence (bunsetsu-based)among bunsetsus.
A bunsetsu is a base phrasalunit and consists of one or more content words fol-lowed by zero or more function words.In addition, most of algorithms of Japanese de-pendency parsing, e.g., (Sekine et al, 2000; Sas-sano, 2004), assume the three constraints below.
(1) Each bunsetsu has only one head except therightmost one.
(2) Dependency links between bun-setsus go from left to right.
(3) Dependency linksdo not cross one another.
In other words, depen-dencies are projective.A sample sentence in Japanese is shown in Fig-ure 1.
We can see all the constraints are satisfied.3 Previous WorkAs far as we know, there is no dependency parserthat does simultaneously both bunsetsu chunkingand dependency parsing and, in addition, doesthem with a single scan.
Most of the moderndependency parsers for Japanese require bunsetsuchunking (base phrase chunking) before depen-dency parsing (Sekine et al, 2000; Kudo and Mat-sumoto, 2002; Sassano, 2004).
Although word-based parsers are proposed in (Mori et al, 2000;Mori, 2002), they do not build bunsetsus and arenot compatible with other Japanese dependencyparsers.
Multilingual parsers of participants in theCoNLL 2006 shared task (Buchholz and Marsi,2006) can handle Japanese sentences.
But they arebasically word-based.49Meg ga kare ni ano pen wo age-ta.Meg subj him to that pen acc give-past.ID 0 1 2 3 4 5 6 7Head 1 7 3 7 6 6 7 -Type B D B D D B D -Figure 2: Sample sentence (morpheme-based).?Type?
represents the type of dependency relation.4 Algorithm4.1 Dependency RepresentationIn our proposed algorithm, we use a morpheme-based dependency structure instead of a bunsetsu-based one.
The morpheme-based representationis carefully designed to convey the same informa-tion on dependency structure of a sentence withoutthe loss from the bunsetsu-based one.
The right-most morpheme of the bunsetsu t should modifythe rightmost morpheme of the bunsetsu u whenthe bunsetsu t modifies the bunsetsu u. Everymorpheme except the rightmost one in a bunsetsushould modify its following one.
The sample sen-tence in Figure 1 is converted to the sentence withour proposed morpheme-based representation inFigure 2.Take for instance, the head of the 0-th bunsetsu?Meg-ga?
is the 4-th bunsetsu ?age-ta.?
in Fig-ure 1.
This dependency relation is represented bythat the head of the morpheme ?ga?
is ?age-ta.?
inFigure 2.The morpheme-based representation above can-not explicitly state the boundaries of bunsetsus.Thus we add the type to every dependency rela-tion.
A bunsetsu boundary is represented by thetype associated with every dependency relation.The type ?D?
represents that this relation is a de-pendency of two bunsetsus, while the type ?B?represents a sequence of morphemes inside of agiven bunsetsu.
In addition, the type ?O?, whichrepresents that two morphemes do not have a de-pendency relation, is used in implementations ofour algorithm with a trainable classifier.
Followingthis encoding scheme of the type of dependencyrelations bunsetsu boundaries exist just after themorphemes that have the type ?D?.
Inserting ?|?after every morpheme with ?D?
of the sentence inFigure 2 results in Meg-ga | kare-ni | ano | pen-wo| age-ta.
This is identical to the sentence with thebunsetsu-based representation in Figure 1.Input: wi: morphemes in a given sentence.N : the number of morphemes.Output: hj: the head IDs of morphemes wj.tj: the type of dependency relation.
A possiblevalue is either ?B?, ?D?, or ?O?.Functions: Push(i, s): pushes i on the stack s.Pop(s): pops a value off the stack s.Dep(j, i, w, t): returns true when wjshouldmodify wi.
Otherwise returns false.
Setsalways tj.procedure Analyze(w, N , h, t)var s: a stack for IDs of modifier morphemesbeginPush(?1, s); { ?1 for end-of-sentence }Push(0, s);for i ?
1 to N ?
1 do beginj ?
Pop(s);while (j 6= ?1and (Dep(j, i, w, t) or (i = N ?
1)) ) dobeginhj?
i; j ?
Pop(s)endPush(j, s); Push(i, s)endendFigure 3: Pseudo code for base phrase chunkingand dependency parsing.4.2 Pseudo Code for the Proposed AlgorithmThe algorithm that we propose is based on (Sas-sano, 2004), which is considered to be a simpleform of shift-reduce parsing.
The pseudo code ofour algorithm is presented in Figure 3.
Importantvariables here are hjand tjwhere j is an indexof morphemes.
The variable hjholds the head IDand the variable tjhas the type of dependency re-lation.
For example, the head and the dependencyrelation type of ?Meg?
in Figure 2 are representedas h0= 1 and t0= ?B?
respectively.
The flowof the algorithm, which has the same structure asSassano?s (2004), is controlled with a stack thatholds IDs for modifier morphemes.
Decision ofthe relation between two morphemes is made inDep(), which uses a machine learning-based clas-sifier that supports multiclass prediction.The presented algorithm runs in a left-to-rightmanner and its upper bound of the time complex-ity is O(n).
Due to space limitation, we do notdiscuss its complexity here.
See (Sassano, 2004)50for further details.5 Experiments and Discussion5.1 Experimental Set-upCorpus For evaluation, we used the Kyoto Uni-versity Corpus Version 2 (Kurohashi and Nagao,1998).
The split for training/test/development isthe same as in other papers, e.g., (Uchimoto et al,1999).Selection of a Classifier and its Setting We im-plemented a parser with the voted perceptron (VP)(Freund and Schapire, 1999).
We used a poly-nomial kernel and set its degree to 3 because cu-bic kernels proved to be effective empirically forJapanese parsing (Kudo and Matsumoto, 2002).The number of epoch T of VP was selected usingthe development test set.
For multiclass predic-tion, we used the pairwise method (Kre?el, 1999).Features We have designed rather simple fea-tures based on the common feature set (Uchimotoet al, 1999; Kudo and Matsumoto, 2002; Sassano,2004) for bunsetsu-based parsers.
We use the fol-lowing features for each morpheme:1. major POS, minor POS, conjugation type,conjugation form, surface form (lexicalizedform)2.
Content word or function word3.
Punctuation (periods and commas)4.
Open parentheses and close parentheses5.
Location (at the beginning or end of the sen-tence)Gap features between two morphemes are alsoused since they have proven to be very useful andcontribute to the accuracy (Uchimoto et al, 1999;Kudo and Matsumoto, 2002).
They are repre-sented as a binary feature and include distance (1,2, 3, 4 ?
10, or 11 ?
), particles, parentheses, andpunctuation.In our proposed algorithm basically two mor-phemes are examined to estimate their dependencyrelation.
Context information about the currentmorphemes to be estimated would be very use-ful and we can incorporate such information intoour model.
We assume that we have the j-th mor-pheme and the i-th one in Figure 3.
We also usethe j?n, ..., j?1, j+1, ..., j+n morphemes andthe i ?
n, ..., i ?
1, i + 1, ..., i + n ones, where nMeasure Accuracy (%)Dependency Acc.
93.96Dep.
Type Acc.
99.49Both 93.92Table 1: Performance on the test set.
This result isachieved by the following parameters: The size ofcontext window is 2 and epoch T is 4.Bunsetsu-based Morpheme-basedPrevious 88.48 95.09Ours NA 93.96Table 2: Dependency accuracy.
The system withthe previous method employs the algorithm (Sas-sano, 2004) with the voted perceptron.is the size of the context window.
We examined 0,1, 2 and 3 for n.5.2 Results and DiscussionAccuracy Performances of our parser on the testset is shown in Table 1.
The dependency accuracyis the percentage of the morphemes that have acorrect head.
The dependency type accuracy is thepercentage of the morphemes that have a correctdependency type, i.e., ?B?
or ?D?.
The bottom lineof Table 1 shows the percentage of the morphemesthat have both a correct head and a correct depen-dency type.
In all these measures we excluded thelast morpheme in a sentence, which does not havea head and its associated dependency type.The accuracy of dependency type in Table 1is interpreted to be accuracy of base phrase(bunsetsu) chunking.
Very accurate chunking isachieved.Next we examine the dependency accuracy.
Inorder to recognize how accurate it is, we com-pared the performance of our parser with that ofthe parser that uses one of previous methods.
Weimplemented a parser that employs the algorithmof (Sassano, 2004) with the commonly used fea-tures and runs with VP instead of SVM, whichSassano (2004) originally used.
His parser, whichcannot do bunsetsu chunking, accepts only a chun-ked sentence and then produces a bunsetsu-baseddependency structure.
Thus we cannot directlycompare results with ours.
To enable us to com-pare them we gave bunsetsu chunked sentences byour parser to the parser of (Sassano, 2004) insteadof giving directly the correct chunked sentences51Window Size Dep.
Acc.
Dep.
Type Acc.0 (T = 1) 82.71 99.291 (T = 2) 93.57 99.492 (T = 4) 93.96 99.493 (T = 3) 93.79 99.42Table 3: Performance change depending on thecontext window size00.511.522.530  10  20  30  40  50  60  70  80  90  100SecondsSentence Length (Number of Morphemes)Figure 4: Running time on the test set.
We useda PC (Intel Xeon 2.33 GHz with 8GB memory onFreeBSD 6.3).in the Kyoto University Corpus.
And then we re-ceived results from the parser of (Sassano, 2004),which are bunsetsu-based dependency structures,and converted them to morpheme-based structuresthat follow the scheme we propose in this paper.Finally we have got results that have the compat-ible format and show a comparison with them inTable 2.Although the bunsetsu-based parser outper-formed slightly our morpheme-based parser in thisexperiment, it is still notable that our methodyields comparable performance with even a sin-gle scan of a sentence for dependency parsing inaddition to bunsetsu chunking.
According to theresults in Table 2, we suppose that performance ofour parser roughly corresponds to about 86?87%in terms of bunsetsu-based accuracy.Context Window Size Performance change de-pending on the size of context window is shownin Table 3.
Among them the best size is 2.
Inthis case, we use ten morphemes to determinewhether or not given two morphemes have a de-pendency relation.
That is, to decide the relationof morphemes j and i (j < i), we use morphemesj?2, j?1, j, j+1, j+2 and i?2, i?1, i, i+1, i+2.Running Time and Asymptotic Time Complex-ity We have observed that the running time isproportional to the sentence length (Figure 4).
Thetheoretical time complexity of the proposed algo-rithm is confirmed with this observation.6 Conclusion and Future WorkWe have described a novel algorithm that com-bines Japanese base phrase chunking and depen-dency parsing into a single scan process.
The pro-posed algorithm runs in linear-time with a singlescan of a sentence.In future work we plan to combine morpholog-ical analysis or word segmentation into our pro-posed algorithm.
We also expect that structureanalysis of compound nouns can be incorporatedby extending the dependency relation types.
Fur-thermore, we believe it would be interesting todiscuss linguistically and psycholinguistically thedifferences between Japanese and other Europeanlanguages such as English.
We would like to knowwhat differences lead to easiness of analyzing aJapanese sentence.ReferencesS.
Buchholz and E. Marsi.
2006.
CoNLL-X shared taskon multilingual dependency parsing.
In Proc.
of CoNLL2006, pages 149?164.Y.
Freund and R. E. Schapire.
1999.
Large margin classifi-cation using the perceptron algorithm.
Machine Learning,37(3):277?296.U.
Kre?el.
1999.
Pairwise classification and support vec-tor machines.
In B. Scho?lkopf, C. J. Burges, and A. J.Smola, editors, Advances in Kernel Methods: SupportVector Learning, pages 255?268.
MIT Press.T.
Kudo and Y. Matsumoto.
2002.
Japanese dependencyanalysis using cascaded chunking.
In Proc.
of CoNLL-2002, pages 63?69.S.
Kurohashi and M. Nagao.
1998.
Building a Japaneseparsed corpus while improving the parsing system.
InProc.
of LREC-1998, pages 719?724.S.
Mori, M. Nishimura, N. Itoh, S. Ogino, and H. Watanabe.2000.
A stochastic parser based on a structural word pre-diction model.
In Proc.
of COLING 2000, pages 558?564.S.
Mori.
2002.
A stochastic parser based on an SLM witharboreal context trees.
In Proc.
of COLING 2002.M.
Sassano.
2004.
Linear-time dependency analysis forJapanese.
In Proc.
of COLING 2004, pages 8?14.S.
Sekine, K. Uchimoto, and H. Isahara.
2000.
Back-ward beam search algorithm for dependency analysis ofJapanese.
In Proc.
of COLING-00, pages 754?760.K.
Uchimoto, S. Sekine, and H. Isahara.
1999.
Japanesedependency structure analysis based on maximum entropymodels.
In Proc.
of EACL-99, pages 196?203.52
