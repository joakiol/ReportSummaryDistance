Prediction of Lexicalized Tree Fragments in TextDonald HindleAT&T Bell Laboratories600 Mountain AvenueMurray Hill, NJ 07974ABSTRACTThere is a mismatch between the distribution of information i text,and a variety of grammatical formalisms for describing it, includingngrams, context-free grammars, and dependency grammars.
Ratherthan adding probabilities to existing grammars, it is proposed tocollect he distributions offlexibly sized partial trees.
These can beused to enhance an ngram model, and in analogical parsing.1.
THE PROBLEM WITH PROBABILIZEDGRAMMARSFor a variety of language processing tasks, it is useful to havea predictive language model, a fact which has recently led tothe development probabilistic versions of diverse grammars,including ngram models, context free grammars, various de-pendency grammars, and lexicalized tree grammars.
Theseenterprises share a common problem: there is a mismatch be-tween the distribution of information i  text and the grammarmodel.The problem arises because ach grammar formalism is nat-ural for the expression of only some linguistic relationships,but predictive relationships in text are not so restricted.
Forexample, context-free grammars naturally express relationsamong sisters in a tree, but are less natural for expressingrelations between elements deeper the tree.
In this paper, firstwe discuss the distribution of information in text, and its re-lationship to various grammars.
Then we show how a moreflexible grammatical description of text can be extracted froma corpus, and how such description can enhance a languagemodel.Ngram Models The problem can be seen most simply inngram models, where the basic operation is to guess the prob-ability of a word given n - 1 previous words.
Obviously,there is a deeper structure in text than an n-gram model ad-mits, though thus far, efforts to exploit his information havebeen only marginally successful.
Yet even on its own terms,ngram models typically fail to take into account predictiveinformation.One way that ngram models ignore predictive information isin their strategy for backing off.
Consider, for example, atrigram model where the basic function is to predict a word(wo) given the two previous words (W_l and w-2).
In ourWall Street Journal test corpus, the three word sequence givekittens to appears once, but not at all in the training corpus.Thus, a trigram model will have have difficulty predicting togiven the words give kittens.In this case, the standard move of backing off to a bigrammodel is not very informative.
It is more useful to predictto using the word give than the word kittens, because weknow little about what can follow kittens, but much aboutwhat typically follows give.
We would expect for cases wherethe bigram (w_ 1,w0) does not exist, the alternative bigram(w_2,wo) will be a better predictor (if it exists) than the simpleunigram.Obviously, in this example, the fact that complementation inEnglish is not expressed purely by adjacency explains omeof the power of the w_ 1 predictor.A second problem with ngram models arises because differentword sequences call for a greater or smaller n. For example,while many 6-grams are unique and uninformative, some arepowerful predictors.Table 1 shows the frequencies of the top few words followingthe words New York Stock Exchange in the 60 million wordWall Street Journal corpus.
More than half the time, the wordthat followsNew York Stock Exchange is composite.
However,in the 355 cases where New York Stock Exchange is precededby the word composite (Table 1), composite never occurs asthe following word, and the overwhelming probable choicefor the following word is trading.If we had settled for a 5-gram model here, we would havefailed miserably compared with a 6-gram model.
But ofcourse, this raises the sparse data problem; predicting theparameters of a 6-gram model is daunting.Context Free Grammars It is easy to see that a simple-minded probabilizing of a CFG - that is, taking an existingCFG and assigning probabilities to the rules - is not a verygood predictor.
There several problems.
First, CFG's typi-cally don't include enough lexical information.
Indeed, thenatural use of non-terminal categories i to abstract away from243New York Stock Exchangecomposite New York Stock Exchangecomposite 65971556yesterday 862824trading 480, ,TOTAL 12305trading 349yesterday 4Trading 2composite 0TOTAL 355Table 1: Ngrams with New York Stock Exchangelexical considerations.
Lexical associations are however crit-ical to guessing word probabilities, not only for verb subcat-egorization and selection, but across the vocabulary (see e.g.Church et al 1991).
A context free grammar with a ruleN2-  > ADJ ,  N is not able to naturally express electionalrestrictions between adjectives and nouns, e.g.
the fact thatstrong tea is probable but powerful tea is not.A second problem is that CFG's naturally abstract away fromsyntactic function: for example, in a CFG, a noun phrase isdescribed by the same set of rules whether it occurs as sub-ject, object, object of preposition or whatever.
While thisability to generalize across contexts i  a strength of CFG's, itis disastrous for guessing whether anoun phrase will be a pro-noun or not.
Table 2 shows the probabilities of a noun phrasebeing realized as a pronoun in various contexts, in a sampleof spoken and written texts produced by college students andmatched for content (Hindle 1978).
Clearly, ignoring whethera noun phrase is subject or not reduces the effectiveness ofa predictive model.
(Note too that the differences betweenspoken and written English are not to be ignored.There are of course ways to admit lexical and functional infor-mation into a CFG.
But except for carefully restricted domains(e.g semantic grammars), these typically lead to an explosionof nonterminals and rules, making parameter stimation dif-ficult.spokenwrittenfunction p(PRO)subject .71 (N=2077)non-subject .16 (N=1477)subject .44 (N=1195)non-subject .09 (N=i088)Table 2: Subject and non-subject noun phrasesDependency Grammars Dependency grammars naturallyaddress part of the mismatch between CFG's and predictiveassociations, ince they are expressed in terms of relationsbetween words (Melcuk 1988), Nevertheless, in dependencygrammars as well, certain syntactic relationships are problem-atic.In dependency grammar, there are two competing analysesboth for noun phrases and for verb phrases.
For noun phrases,the head may be taken to be either 1) the head noun (e.g.man in the men) or 2) the determiner (e.g the in the men);analogously, for verb phrases, the head may be taken to beeither 1) the mail verb (e.g.
see in had seen) or 2) the tensedverb of the verb group (e.g have in had seen).
Each anal-ysis has its virtues, and different dependency theorists havepreferred one analysis or the other.
It is not our purposehere to choose a dependency analysis, but to point out thatwhatever the choice, there are consequences for our predic-tive language models.
The two models imply different naturalgeneralizations forestimating probabilities, and thus will leadto different predictions about the language probabilities.
Ifthe determiner is taken to be the head of the noun phrase, thenin guessing the probability of a verb-det-noun structure, theassociation between the verb and the determiner will predom-inate, since when we don't have enough information about averb-det-noun triple, we can back off to pairs.
Conversely, ifthe noun is taken to be the head of the noun phrase, then thepredominant association will be between verb and noun.
(Ofcourse, a more complex relationship between the grammarand the associated predictive language model may be defined,overriding the natural interpretation.
)A ten million word sample of Wall Street Journal text wasparsed, and a set of verb-det-noun triples extracted.
Specif-ically, object noun phrases consisting of a noun preceded bya single determiner p eceded by a verb were tabulated.
Thatis, we consider only verbs with an object, where the objectconsists of a determiner and a noun.
The five most commonsuch triples (preceded by their counts) were:213 have176 be165 be140 raise127 reacha lossthe firstits stakean agreementThree different probability models for predicting the specificverb, determiner, and noun were investigated, and their en-tropies calculated.
Model 0 is the baseline trigram model,assuming no independence among the three terms.
Model 1,the natural model for the determiner=head dependency model,predicts the determiner f om the verb and the noun from thedeterminer (and thus is equivalent to an adjacent word bigrammodel).
Model 2 is the converse, the natural model for thenoun=head dependency model.
Both Model 1 and Model 2244Model for \[V P V \[N P d n \]\] Entropy0 Pr(vdn) = Pr(v)Pr(dn\[v) 15.081 Pr(vdn) = Pr(v)Pr(dlv)Pr(nld ) 20.482 Pr(vdn) = Pr(v)Pr(nlv)Pr(dln ) 17.62Table 3: Three predictive models for verb-det-noun triples inWall Street Journal textignore predictive information, assuming in the first case thatthe choice of noun is independent of the verb, and in the sec-ond case, that the choice of determiner is independent of theverb.
Neither assumption is warranted, as Table 3 shows (bothhave higher entropy than the trigram model), but Model 1, thedeterminer=head model, is considerably inferior.
Model 1 isfor this case like a bigram model, and Table 3 makes it clearthat this is not a particularly good way to model dependen-cies between verb and object: the dominant dependency isbetween verb and noun.In terms of using the distributional information available intext, neither choice is correct, since the answer is lexicaUyspecific.
For example, in predicting the object of verbs, an-swer is a better predictor of its object noun (call, question),while alter is better apredicting its determiner (the, its).In contrast to dependency grammars and context free gram-mars, lexicalized tree adjoining rammars have considerableflexibility in what relations are represented, since the tree isan arbitrary-sized unit (Shabes 1988).
In practice however,lexicalized TAGs have typically been written to reduce thenumber of rules, and thus to assume independence like othergrammars.
In general, for any grammar that is written with-out regard to the distribution of forms in text, simply attachingprobabilities to the grammar will always ignore useful infor-mation.
This does not imply any claim about he descriptivepower of various grammar formalisms; with sufficient inge-nuity, just about any recurrent relation that appears in a corpuscan be encoded in any formalism.
However, different gram-mar formalisms do differ in what they can naturally express.There is a clear linguistic reason for the mismatch betweenreceived grammars and the distribution of structures in text:language provides everal cross cutting ways of organizinginformation (including various kinds of dependencies, paral-lel structures, listing, name-making templates, etc.
), and nosingle model is good for all of these.2.
US ING PART IAL  STRUCTURESThe preceding section has given evidence that adding proba-bilities to existing rammars in several formalisms i less thanoptimal since significant predictive r lationships are necessar-ily ignored.
The obvious solution is to enrich the grammarsto include more information.
To do this, we need variablesized units in our database, with varying terms of description,including adjacency relationships and dependency relation-ships.
That is, given the unpredictable distribution of infor-mation in text, we would like to have a more flexible approachto representing the recurrent relations in a corpus.
To addressthis need, we have been collecting adatabase of partial struc-tures extracted from the Wall Street Journal corpus, in a waydesigned to record recurrent information over a wide range ofsize and terms of the description.Extracting Partial Structures The database of partialstructures i built up from the words in the corpus, by succes-sively adding larger structures, after augmenting the corpuswith the analysis provided by an unsupervised parser.
Thelarger structures found in this way are then entered into thepermanent database of structures only if a relation recurs witha frequency above a given threshold.
When a structure doesnot meet the frequency threshold, it is generalized until itdoes.The descriptive r lationships admitted include:?
basic lexical features- spelling- part-of-speech- lemma- major category (maximal projection)?
dependency relations - depends on?
adjacency relations - precedesConsider an example from the following sentence from the atraining corpus of 20 million words of the Wall Street Journal.
(1) Reserve board rules have put banks between arock and a hard placeThe first order description of a word consists of its basic lexicalfeatures, i.e.
the word spelling, its part of speech, its lemma,and its major category.
Looking at the word banks, we haveas descriptionTERMINALbanks,NN,bank/N,NPAt the first level we add adjacency and dependency informa-tion, specificallyADDED STRUCTURE(precedes (put,VB,put/V, G) (banks,NN,bank/N,NG))(precedes (banks,NN,bank/N,NG) (between,IN,between/I,PG))(depends (put,VB,put/V, G) (banks,NN,bank/N,NG))245Assuming that we require at least two instances for a partialdescription to be entered into the database, none of these threedescriptions qualify for the database.
Therefore we mustabstract away, using an arbitrarily defined abstraction path.First we abstract from the spelling to the lemma.
This moveadmits two relations (since they are now frequent enough)PRUNED STRUCTURES(precedes (put, VB,put/V, VG) (,NN,bank/N,NG))(depends (put,VB,put/V, VG) (,NN,bank/N,NG))units are selected in using language depends on a variety offactors, including meaning, subject matter, speaking situation,style, interlocutor and so on.
Of course, demonstrating thatthis intuition is valid remains for future work.The set of partial trees can be used directly in an analogicalparser, as described in Hindle 1992.
In the parser, we arenot concerned with estimating probabilities, but rather withfinding the structure which best matches the current parserstate, where a match is better the more specific its descriptionis.The third relation is still too infrequent, so we further gener-alize to(precedes (,NN,,NG) (between,IN,between/I,PG))a relation which is amply represented (3802 occurrences).The process is iterated, using the current abstracted descrip-tion of each word, adding a level of description, then gen-eralizing when below the frequency threshold.
Since eachlevel in elaborating the description adds information to eachword, it can only reduce the counts, but never increase them.This process finds a number of recurrent partial structures,including between a rock and a hard place (3 occurrences in20 million words), and \[vpput \[NP distance\] \[pp between\]\] (4occurrences).General Caveats There is of course considerable noise in-troduced by the errors in analysis that the parser makes.There are several arbitrary decisions made in collecting thedatabase.
The level of the threshold is arbitrarily set at 3 forall structures.
The sequence of generalization is arbitrarilydetermined before the training.
And the predicates in thedescription are arbitrarily selected.
We would like to havebetter motivation for all these decisions.It should be emphasized that while the set of descriptive t rmsused in the collection of the partial structure database allowsa more flexible description of the corpus than simple ngrams,CFG's or some dependency descriptions, it nevertheless ialso restrictive.
There are many predictive relationships thatcan not be described.
For example, parallelism, reference,topic-based or speaker-based variation, and so on.Motivation The underlying reason for developing adatabase of partial trees is not primarily for the languagemodeling task of predicting the next word.
Rather the partial-tree database is motivated by the intuition that partial treesare are the locus of other sorts of linguistic information, forexample, semantic or usage information.
Our use of languageseems to involve the composition of variably sized partiallydescribed units expressed in terms of a variety of predicates(only some of which are included in our database).
Which3.
ENHANCING A TRIGRAM MODELThe partial structure database provides more information thanan ngram description, and thus can be used to enhance anngram model.
To explore how to use the best available in-formation in a language model, we turn to a trigram modelof Wall Street Journal text.
The problem is put into relief byfocusing on those cases where the trigram model fails, thatis, where the observed trigram condition (w-2, w_l) does notoccur in the training corpus.In the current est, we randomly assigned each sentence froma 2 million word sample of WSJ text to either the test ortraining set.
This unrealistically minimizes the rate of unseenconditions, since typically the training and test are selectedfrom disjoint documents (see Church and Gale 1991).
Onthe other hand, since the training is only a million words,the trigrams are undertrained.
In general, the rate of unseenconditions will vary with the domain to be modeled and thesize of training corpus, but it will not (in realistic languages) beeliminated.
In this test, 26% (258665/997811) of the bigramsdid not appear in the test, and thus it is necessary to backofffrom the trigram model.We will assume that a trigram model is sufficiently effectiveat preediction i  those cases where the conditioning bigram hasbeen observed in training, and will focus on the problem ofwhat to do when the conditioning bigram has not appearedin the training.
In a standard backoff model, we would lookto estimate Pr(wolw_l).
Here we want to consider a sec-ond predictor derived from our database of partial structures.The particular predictor we use is the lemma of the word thatw-1 depends on, which we will call G(W_l).
In the examplediscussed above, the first (standard) predictor for the wordbetween is the preceding word banks and the second predic-tor for the word between is G(banks), which in this case isput/v.We want to choose among two predictors, w- i  and G(w_l).In general, if we have two conditions, Ca and CCb and we wantto find the probability of the next word given these conditions.Intuitively, we would like to choose the predictor C'i for whichthe predicted istribution of w differs most from the unigramdistribution.
Various measures are possible; here we con-246model logprobunigrambackoff w_ lbackoff G(w_ l)backoff w- t  then G(w_ t)backoff (MAX IS ofw_ l  and G(w_l))9.558.068.207.977.99Table 4: Backoff or unknown trigrams in WSJ text.sider one, which Resnik (1993) calls selectional preference,namely the relative ntropy between the posterior distributionPr(w\]C) and the prior distribution Pr(w).
We'll label thismeasure IS, whereIS(w; C) = E Pr(wlC)loa Pr(wlC)w P (w)In the course of processing sentence (1), we need an estimateof Pr(between\]put banks).
Our training corpus does notinclude the collocation put banks, so no help is available fromtrigrams, therefore we backoff to a bigram model, choosingthe bigram predictor with maximum IS.
The maximum ISis for put/V (G(w_x)) rather than for w-1 (banks) itself, soG(w_l) is used as predictor, giving a logprob estimate of-10.2 rather than -13.1.The choice of G(w_ 1) as predictor here seems to make sense,since we are willing to believe that there is a complementationrelation between put/V and its second complement between.Of course, the choice is not always so intuitively appealing.When we go on to predict he next word, we need an estimateof Pr(albanks between).
Again, our training corpus does notinclude the collocation banks between, so no help is availablefrom trigrams.
In this case, the maximum IS is for banksrather than between, so we use banks to predict a rather thanbetween, giving a logprob estimate of-5.6 rather than -7.10.Overall, however, the two predictors can be combined to im-prove the language model, by always choosing the predictorwith higher IS score.As shown in Table 4, this slightly improves the logprob for ourtest set over either predictor independently.
However, Table4 also shows that a simple strategy of chosing the raw bigramfirst and the G(w_l) bigram when there is no informationavailable is slightly better.
In a more general situation, wherewe have a set of different descriptions of the same condition,the IS score provides away to choose the best predictor.4.
CONCLUSIONRecurrent structures in text vary widely both in size and inthe terms in which they are described.
Existing grammars aretoo restrictive both in the size of structure they admit and intheir terms of description to adequately capture the variationin text.
A method has been described for collecting adatabaseof partial structures from text.
Methods of fully exploiting thedatabase for language modeling are currently being explored.5.
REFERENCES1.
Church, Kenneth W., William A. Gale, Patrick Hanks, andDonald Hindle.
1991.
"Using statistics in lexical analysis:' inUri Zernik (ed.)
Lexical acquisition: using on-line resourcesto build a lexicon, Lawrence Erlbaum, 115-164.2.
Church, Kenneth W. and William A. Gale.
1991.
"A compari-son of the enhanced Good-Turing and deleted estimation meth-ods for estimating probabilities of English bigrams;' ComputerSpeech and Language, 5 19-54.3.
Hindle, Donald.
1992.
"An analogical parser for restricteddomains," In Proceedings ofthe FiSh DARPA Workshop onSpeech &Natural Language, -.4.
Hindle, Donald.
1981.
"A probabilistic grammar of nounphrases in spoken and written English;' In David Sankoff andHenrietta Cedergren (eds.)
Variation Omnibus, Linguistic Re-search, Inc. Edmonton, Alberta.5.
Melchuk, IgorA.
1988.DependencySyntax: TheoryandPrac-tice, State University of New York Press, Albany.6.
Resnik, Philip.
1993.
"Semantic Classes and Syntactic Ambi-guity;' This volume.7.
Schabes, Yves.
1988.
"Parsing strategies with 'lexicalized'grammars: application to tree adjoining grammars", in Pro-ceedings fo the 12th International Conference on Computa-tional Linguistics, COLING88, Budapest, Hungary.247
