Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 645?654,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsDiscovering Implicit Discourse Relations Through Brown Cluster PairRepresentation and Coreference PatternsAttapol T. RutherfordDepartment of Computer ScienceBrandeis UniversityWaltham, MA 02453, USAtet@brandeis.eduNianwen XueDepartment of Computer ScienceBrandeis UniversityWaltham, MA 02453, USAxuen@brandeis.eduAbstractSentences form coherent relations in adiscourse without discourse connectivesmore frequently than with connectives.Senses of these implicit discourse rela-tions that hold between a sentence pair,however, are challenging to infer.
Here,we employ Brown cluster pairs to rep-resent discourse relation and incorporatecoreference patterns to identify senses ofimplicit discourse relations in naturallyoccurring text.
Our system improves thebaseline performance by as much as 25%.Feature analyses suggest that Brown clus-ter pairs and coreference patterns can re-veal many key linguistic characteristics ofeach type of discourse relation.1 IntroductionSentences must be pieced together logically in adiscourse to form coherent text.
Many discourserelations in the text are signaled explicitly througha closed set of discourse connectives.
Simplydisambiguating the meaning of discourse connec-tives can determine whether adjacent clauses aretemporally or causally related (Pitler et al., 2008;Wellner et al., 2009).
Discourse relations and theirsenses, however, can also be inferred by the readereven without discourse connectives.
These im-plicit discourse relations in fact outnumber explicitdiscourse relations in naturally occurring text.
In-ferring types or senses of implicit discourse re-lations remains a key challenge in automatic dis-course analysis.A discourse parser requires many subcompo-nents which form a long pipeline.
The implicitdiscourse relation discovery has been shown to bethe main performance bottleneck of an end-to-endparser (Lin et al., 2010).
It is also central to manyapplications such as automatic summarization andquestion-answering systems.Existing systems, which make heavy use ofword pairs, suffer from data sparsity problem asa word pair in the training data may not appearin the test data.
A better representation of twoadjacent sentences beyond word pairs could havea significant impact on predicting the sense ofthe discourse relation that holds between them.Data-driven theory-independent word classifica-tion such as Brown clustering should be ableto provide a more compact word representation(Brown et al., 1992).
Brown clustering algorithminduces a hierarchy of words in a large unanno-tated corpus based on word co-occurrences withinthe window.
The induced hierarchy might giverise to features that we would otherwise miss.
Inthis paper, we propose to use the cartesian productof Brown cluster assignment of the sentence pairas an alternative abstract word representation forbuilding an implicit discourse relation classifier.Through word-level semantic commonalitiesrevealed by Brown clusters and entity-level rela-tions revealed by coreference resolution, we mightbe able to paint a more complete picture of thediscourse relation in question.
Coreference resolu-tion unveils the patterns of entity realization withinthe discourse, which might provide clues for thetypes of the discourse relations.
The informationabout certain entities or mentions in one sentenceshould be carried over to the next sentence to forma coherent relation.
It is possible that coreferencechains and semantically-related predicates in thelocal context might show some patterns that char-acterize types of discourse relations.
We hypoth-esize that coreferential rates and coreference pat-terns created by Brown clusters should help char-acterize different types of discourse relations.Here, we introduce two novel sets of featuresfor implicit discourse relation classification.
Fur-ther, we investigate the effects of using Brownclusters as an alternative word representation andanalyze the impactful features that arise from645Number of instancesImplicit ExplicitCOMPARISON 2503 (15.11%) 5589 (33.73%)CONTINGENCY 4255 (25.68%) 3741 (22.58%)EXPANSION 8861 (53.48%) 72 (0.43%)TEMPORAL 950 (5.73%) 3684 (33.73%)Total 16569 (100%) 13086 (100%)Table 1: The distribution of senses of implicit dis-course relations is imbalanced.Brown cluster pairs.
We also study coreferentialpatterns in different types of discourse relations inaddition to using them to boost the performanceof our classifier.
These two sets of features alongwith previously used features outperform the base-line systems by approximately 5% absolute acrossall categories and reveal many important charac-teristics of implicit discourse relations.2 Sense annotation in Penn DiscourseTreebankThe Penn Discourse Treebank (PDTB) is thelargest corpus richly annotated with explicitand implicit discourse relations and their senses(Prasad et al., 2008).
PDTB is drawn fromWall Street Journal articles with overlapping an-notations with the Penn Treebank (Marcus et al.,1993).
Each discourse relation contains the infor-mation about the extent of the arguments, whichcan be a sentence, a constituent, or an incontigu-ous span of text.
Each discourse relation is alsoannotated with the sense of the relation that holdsbetween the two arguments.
In the case of implicitdiscourse relations, where the discourse connec-tives are absent, the most appropriate connectiveis annotated.The senses are organized hierarchically.
Our fo-cus is on the top level senses because they are thefour fundamental discourse relations that variousdiscourse analytic theories seem to converge on(Mann and Thompson, 1988).
The top level sensesare COMPARISON, CONTINGENCY, EXPANSION,and TEMPORAL.The explicit and implicit discourse relations al-most orthogonally differ in their distributions ofsenses (Table 1).
This difference has a few im-plications for studying implicit discourse relationsand uses of discourse connectives (Patterson andKehler, 2013).
For example, TEMPORAL relationsconstitute only 5% of the implicit relations but33% of the explicit relations because they mightnot be as natural to create without discourse con-nectives.
On the other hand, EXPANSION rela-tions might be more cleanly achieved without onesas indicated by its dominance in the implicit dis-course relations.
This imbalance in class distri-bution requires greater care in building statisticalclassifiers (Wang et al., 2012).3 Experiment setupWe followed the setup of the previous studiesfor a fair comparison with the two baseline sys-tems by Pitler et al.
(2009) and Park and Cardie(2012).
The task is formulated as four sepa-rate one-against-all binary classification problems:one for each top level sense of implicit discourserelations.
In addition, we add one more classifica-tion task with which to test the system.
We mergeENTREL with EXPANSION relations to follow thesetup used by the two baseline systems.
An argu-ment pair is annotated with ENTREL in PDTB ifan entity-based coherence and no other type of re-lation can be identified between the two argumentsin the pair.
In this study, we assume that the goldstandard argument pairs are provided for each re-lation.
Most argument pairs for implicit discourserelations are a pair of adjacent sentences or adja-cent clauses separated by a semicolon and shouldbe easily extracted.The PDTB corpus is split into a training set, de-velopment set, and test set the same way as in thebaseline systems.
Sections 2 to 20 are used to trainclassifiers.
Sections 0?1 are used for developingfeature sets and tuning models.
Section 21?22 areused for testing the systems.The statistical models in the following exper-iments are from MALLET implementation (Mc-Callum, 2002) and libSVM (Chang and Lin,2011).
For all five binary classification tasks, wetry Balanced Winnow (Littlestone, 1988), Maxi-mum Entropy, Naive Bayes, and Support VectorMachine.
The parameters and the hyperparame-ters of each classifier are set to their default values.The code for our model along with the data ma-trices is available at github.com/attapol/brown_coref_implicit.4 FeaturesUnlike the baseline systems, all of the featuresin the experiments use the output from automaticnatural language processing tools.
We use theStanford CoreNLP suite to lemmatize and part-of-speech tag each word (Toutanova et al., 2003;646Toutanova and Manning, 2000), obtain the phrasestructure and dependency parses for each sentence(De Marneffe et al., 2006; Klein and Manning,2003), identify all named entities (Finkel et al.,2005), and resolve coreference (Raghunathan etal., 2010; Lee et al., 2011; Lee et al., 2013).4.1 Features used in previous workThe baseline features consist of the following:First, last, and first 3 words, numerical ex-pressions, time expressions, average verb phraselength, modality, General Inquirer tags, polarity,Levin verb classes, and production rules.
Thesefeatures are described in greater detail by Pitler etal.
(2009).4.2 Brown cluster pair featuresTo generate Brown cluster assignment pair fea-tures, we replace each word with its hard Browncluster assignment.
We used the Brown wordclusters provided by MetaOptimize (Turian etal., 2010).
3,200 clusters were induced fromRCV1 corpus, which contains about 63 million to-kens from Reuters English newswire.
Then wetake the Cartesian product of the Brown clus-ter assignments of the words in Arg1 and theones of the words in Arg2.
For example, sup-pose Arg1 has two words w1,1, w1,2, Arg2 hasthree words w2,1, w2,2, w2,3, and then B(.)
mapsa word to its Brown cluster assignment.
Aword wijis replaced by its corresponding Browncluster assignment bij= B(wij).
The result-ing word pair features are (b1,1, b2,1), (b1,1, b2,2),(b1,1, b2,3), (b1,2, b2,1), (b1,2, b2,2), and (b1,2, b2,3).Therefore, this feature set can generateO(32002) binary features.
The feature set size isorders of magnitude smaller than using the actualwords, which can generate O(V2) distinct binaryfeatures where V is the size of the vocabulary.4.3 Coreference-based featuresWe want to take advantage of the semantics ofthe sentence pairs even more by considering howcoreferential entities play out in the sentence pairs.We consider various inter-sentential coreferencepatterns to include as features and also to betterdescribe each type of discourse relation with re-spect to its place in the coreference chain.For compactness in explaining the followingfeatures, we define similar words to be the wordsassigned to the same Brown cluster.Number of coreferential pairs: We count thenumber of inter-sentential coreferential pairs.We expect that EXPANSION relations should bemore likely to have coreferential pairs because thedetail or information about an entity mentionedin Arg1 should be expanded in Arg2.
Therefore,entity sharing might be difficult to avoid.Similar nouns and verbs: A binary featureindicating whether similar or coreferential nounsare the arguments of the similar predicates.
Predi-cates and arguments are identified by dependencyparses.
We notice that sometimes the author usessynonyms while trying to expand on the previouspredicates or entities.
The words that indicate thecommon topics might be paraphrased, so exactstring matching cannot detect whether the two ar-guments still focus on the same topic.
This mightbe useful for identifying CONTINGENCY relationsas they usually discuss two causally-related eventsthat involve two seemingly unrelated agentsand/or predicates.Similar subject or main predicates: A binaryfeature indicating whether the main verbs of thetwo arguments have the same subjects or notand another binary feature indicating whether themain verbs are similar or not.
For our purposes,the two subjects are said to be the same if theyare coreferential or assigned to the same Browncluster.
We notice that COMPARISON relationsusually have different subjects for the same mainverbs and that TEMPORAL relations usually havethe same subjects but different main verbs.4.4 Feature selection and training samplereweightingThe nature of the task and the dataset poses atleast two problems in creating a classifier.
First,the classification task requires a large number offeatures, some of which are too rare and incon-ducive to parameter estimation.
Second, the la-bel distribution is highly imbalanced (Table 1) andthis might degrade the performance of the classi-fiers (Japkowicz, 2000).
Recently, Park and Cardie(2012) and Wang et al.
(2012) addressed theseproblems directly by optimally select a subset offeatures and training samples.
Unlike previouswork, we do not discard any of data in the trainingset to balance the label distribution.
Instead, wereweight the training samples in each class duringparameter estimation such that the performance onthe development set is maximized.
In addition, the647Current Park and Cardie (2012) Pitler et al.
(2009)P R F1F1F1COMPARISON vs others 27.34 72.41 39.70 31.32 21.96CONTINGENCY vs others 44.52 69.96 54.42 49.82 47.13EXPANSION vs others 59.59 85.50 70.23 - -EXP+ENTREL vs others 69.26 95.92 80.44 79.22 76.42TEMPORAL vs others 18.52 63.64 28.69 26.57 16.76Table 2: Our classifier outperform the previous systems across all four tasks without the use of gold-standard parses and coreference resolution.COMPARISONFeature set F1% changeAll features 39.70 -All excluding Brown cluster pairs 35.71 -10.05%All excluding Production rules 37.27 -6.80%All excluding First, last, and First 3 39.18 -1.40%All excluding Polarity 39.39 -0.79%CONTINGENCYFeature set F1% changeAll 54.42 -All excluding Brown cluster pairs 51.50 -5.37%All excluding First, last, and First 3 53.56 -1.58%All excluding Polarity 53.82 -1.10%All excluding Coreference 53.92 -0.92%EXPANSIONFeature set F1% changeAll 70.23 -All excluding Brown cluster pairs 67.48 -3.92%All excluding First, last, and First 3 69.43 -1.14%All excluding Inquirer tags 69.73 -0.71%All excluding Polarity 69.92 -0.44%TEMPORALFeature set F1% changeAll 28.69 -All excluding Brown cluster pairs 24.53 -14.50%All excluding Production rules 26.51 -7.60%All excluding First, last, and First 3 26.56 -7.42%All excluding Polarity 27.42 -4.43%Table 3: Ablation study: The four most impact-ful feature classes and their relative percentagechanges are shown.
Brown cluster pair featuresare the most impactful across all relation types.number of occurrences for each feature must begreater than a cut-off, which is also tuned on thedevelopment set to yield the highest performanceon the development set.5 ResultsOur experiments show that the Brown cluster andcoreference features along with the features fromthe baseline systems improve the performance forall discourse relations (Table 2).
Consistent withthe results from previous work, the Naive Bayesclassifier outperforms MaxEnt, Balanced Winnow,and Support Vector Machine across all tasks re-gardless of feature pruning criteria and trainingsample reweighting.
A possible explanation is thatthe small dataset size in comparison with the largenumber of features might favor a generative modellike Naive Bayes (Jordan and Ng, 2002).
So weonly report the performance from the Naive Bayesclassifiers.It is noteworthy that the baseline systems usethe gold standard parses provided by the PennTreebank, but ours does not because we wouldlike to see how our system performs realistically inconjunction with other pre-processing tasks suchas lemmatization, parsing, and coreference reso-lution.
Nevertheless, our system still manages tooutperform the baseline systems in all relations bya sizable margin.Our preliminary results on implicit sense classi-fication suggest that the Brown cluster word rep-resentation and coreference patterns might be in-dicative of the senses of the discourse relations,but we would like to know the extent of the im-pact of these novel feature sets when used in con-junction with other features.
To this aim, we con-duct an ablation study, where we exclude one ofthe feature sets at a time and then test the result-ing classifier on the test set.
We then rank eachfeature set by the relative percentage change inF1score when excluded from the classifier.
Thedata split and experimental setup are identical tothe ones described in the previous section but onlywith Naive Bayes classifiers.The ablation study results imply that Browncluster features are the most impactful feature setacross all four types of implicit discourse rela-tions.
When ablated, Brown cluster features de-grade the performance by the largest percentagecompared to the other feature sets regardless of therelation types(Table 3).
TEMPORAL relations ben-648efit the most from Brown cluster features.
With-out them, the F1score drops by 4.12 absolute or14.50% relative to the system that uses all of thefeatures.6 Feature analysis6.1 Brown cluster featuresThis feature set is inspired by the word pair fea-tures, which are known for its effectiveness in pre-dicting senses of discourse relations between thetwo arguments.
Marcu et al (2002), for instance,artificially generated the implicit discourse rela-tions and used word pair features to perform theclassification tasks.
Those word pair features workwell in this case because their artificially gener-ated dataset is an order of magnitude larger thanPDTB.
Ideally, we would want to use the wordpair features instead of word cluster features ifwe have enough data to fit the parameters.
Con-sequently, other less sparse handcrafted featuresprove to be more effective than word pair featuresfor the PDTB data (Pitler et al., 2009).
We remedythe sparsity problem by clustering the words thatare distributionally similar together and greatly re-duce the number of features.Since the ablation study is not fine-grainedenough to spotlight the effectiveness of the indi-vidual features, we quantify the predictiveness ofeach feature by its mutual information.
UnderNaive Bayes conditional independence assump-tion, the mutual information between the featuresand the labels can be efficiently computed in apairwise fashion.
The mutual information be-tween a binary feature Xiand class label Y is de-fined as:I(Xi, Y ) =?y?x=0,1p?
(x, y) logp?
(x, y)p?(x)p?(y)p?(?)
is the probability distribution function whoseparameters are maximum likelihood estimatesfrom the training set.
We compute mutual infor-mation for all four one-vs-all classification tasks.The computation is done as part of the trainingpipeline in MALLET to ensure consistency in pa-rameter estimation and smoothing techniques.
Wethen rank the cluster pair features by mutual in-formation.
The results are compactly summa-rized in bipartite graphs shown in Figure 1, whereeach edge represents a cluster pair.
Since mu-tual information itself does not indicate whethera feature is favored by one or the other label, wealso verify the direction of the effects of each ofthe features included in the following analysis bycomparing the class conditional parameters in theNaive Bayes model.The most dominant features for COMPARISONclassification are the pairs whose members arefrom the same Brown clusters.
We can distinctlysee this pattern from the bipartite graph becausethe nodes on each side are sorted alphabetically.The graph shows many parallel short edges, whichsuggest that many informative pairs consist of thesame clusters.
Some of the clusters that participatein such pair consist of named-entities from vari-ous categories such as airlines (King, Bell, Virgin,Continental, ...), and companies (Thomson, Volk-swagen, Telstra, Siemens).
Some of the pairs forma broad category such as political agents (citizens,pilots, nationals, taxpayers) and industries (power,insurance, mining).
These parallel patterns in thegraph demonstrate that implicit COMPARISON re-lations might be mainly characterized by juxtapos-ing and explicitly contrasting two different entitiesin two adjacent sentences.Without the use of a named-entity recogni-tion system, these Brown cluster pair features ef-fectively act as features that detect whether thetwo arguments in the relation contain named-entities or nouns from the same categories or not.These more subtle named-entity-related featuresare cleanly discovered through replacing wordswith their data-driven Brown clusters without theneed for additional layers of pre-processing.If the words in one cluster semantically relatesto the words in another cluster, the two clustersare more likely to become informative featuresfor CONTINGENCY classification.
For instance,technical terms in stock and trading (weighted,Nikkei, composite, diffusion) pair up with eco-nomic terms (Trading, Interest, Demand, Produc-tion).
The cluster with analysts and pundits pairsup with the one that predominantly contains quan-tifiers (actual, exact, ultimate, aggregate).
In ad-dition to this pattern, we observed the same par-allel pair pattern we found in COMPARISON clas-sification.
These results suggest that in establish-ing a CONTINGENCY relation implicitly the au-thor might shape the sentences such that they havesemantically related words if they do not mentionnamed-entities of the same category.Through Brown cluster pairs, we obtain featuresthat detect a shift between generality and speci-649Arg 1COMPARISONArg 2?
?American AmericanBankBig,Human,Civil,Greater,...Centre,Bay,Park,Hospital,... Board,CorpsCongress Centre,Bay,Park,Hospital,...EastCongressExchangeEastFed,CWB Fed,CWBIsraelis,Moslems,Jews,terrorists,...GM,Ford,Barrick,Anglo,...JapanIsraelis,Moslems,Jews,terrorists,...King,Bell,Virgin,Continental,... JapanMarchKing,Bell,Virgin,Continental,...Miert,Lumpur,der,Metall,...MarchOlivetti,Eurotunnel,Elf,Lagardere,... Miert,Lumpur,der,Metall,...Power,Insurance,Mining,Engineering,... Olivetti,Eurotunnel,Elf,Lagardere,...Soviet,Homeland,PatrioticPower,Insurance,Mining,Engineering,...Standard,Hurricane,Time,Long,...Soviet,Homeland,PatrioticThomson,Volkswagen,Telstra,Siemens,... Standard,Hurricane,Time,Long,...advertising,ad Thomson,Volkswagen,Telstra,Siemens,...agencyactual,exact,ultimate,aggregate,...analysts,pundits advertising,adauto,semiconductor,automotive,automobile,...agencyaverage averagecash bankchemicals,entertainment,machinery,packaging,... cars,vehicles,tyres,vans,...citizens,pilots,nationals,taxpayers,...cashclosedchemicals,entertainment,machinery,packaging,...common citizens,pilots,nationals,taxpayers,...computer,mainframeclosedsalescommoncomputer,mainframeArg 1CONTINGENCYArg 2:&,undBank?Christopher,Simitis,Perry,Waigel,...10Electric,Motor,Life,Chemical,...;FridayBankHoldings,Industries,Investments,Foods,...Bill,Mrs.If,Unless,Whether,Maybe,... Christopher,Simitis,Perry,Waigel,...King,Bell,Virgin,Continental,...Dow,shuttle,DAX,Ifo,...Major,Howard,Arthuis,Chang,...Electric,Motor,Life,Chemical,...MarchFridayPower,Insurance,Mining,Engineering,... GM,Ford,Barrick,Anglo,...Royal,Port,Cape,Santa,... Holdings,Industries,Investments,Foods,...Senate,senateIage,identity,integrity,identification,... If,Unless,Whether,Maybe,...alsoKing,Bell,Virgin,Continental,...am,?mMajor,Howard,Arthuis,Chang,...analysts,punditsMarchaveragePower,Insurance,Mining,Engineering,...backRoyal,Port,Cape,Santa,...herSenate,senatehisTo,WouldindexTrading,Interest,Demand,Production,...marketactual,exact,ultimate,aggregate,...no age,identity,integrity,identification,...nowagoouralltheyweighted,Nikkei,composite,diffusion,...worldArg 1EXPANSIONArg 2American?Analysts,Economists,Diplomats,Forecasters,...AmericanBut,Saying Boeing,BT,Airbus,Netscape,...Democrats DecemberDow,shuttle,DAX,Ifo,...ExchangeDutroux,Lopez,Morris,Hamanaka,... GM,Ford,Barrick,Anglo,...Electric,Motor,Life,Chemical,...IFor,LikeLynch,Fleming,Reagan,Brandford,...Net,Operating,Primary,Minority,...NoOlivetti,Eurotunnel,Elf,Lagardere,... Olivetti,Eurotunnel,Elf,Lagardere,...Plc,Oy,NV,AB,... Plc,Oy,NV,AB,...S&P,Burns,Tietmeyer,Rifkind,... Republicans,Bonds,Rangers,Greens,...Soviet,Homeland,Patriotic Senate,senateTelecommunications,Broadcasting,Futures,Rail,...Soviet,Homeland,PatrioticTexas,Queensland,Ohio,Illinois,...ThatU.S.Trading,Interest,Demand,Production,...VW,Conrail,Bre-X,Texaco,...U.S.advertising,adVW,Conrail,Bre-X,Texaco,...am,?mWe,Thingsbusinesses alldollar,greenback,ecu analyst,meteorologistfive-year,three-year,two-year,four-year,...becausegetbeenif,whenever,whereverbusinessinvestorscents,pence,p.m.,a.m.,...no compared,coupled,comparesoccupation,Index,Kurdistan,Statements,...couldplanstocksunderArg 1TEMPORALArg 2?ve ?*,@,**,?,...
?re14,13,16?ve20*,@,**,?,...300.5,44,0.2,0.3,...50,1.50,0.50,0.05,...10: 10-year,collective,dual,30-year,...American17,19,21British1991,1989,1949,1979,...Friday200,300,150,120,...Investors,Banks,Companies,Farmers,...26,28,29Prices,Results,Sales,Thousands,... 27,22,23Sept,Nov.,Oct.,Oct,...3,A1,C1,C3,...Trading,Interest,Demand,Production,...30Treasury,mortgage-backed50,1.50,0.50,0.05,...York,York-basedage,identity,integrity,identification,...bond,floating-ratebooks,words,budgets,clothes,...consumerconvertible,bonus,Brady,subordinated,...increaseinterestlossmonthsno-fly,year-ago,corresponding,buffer,...peoplequarterresultsrosethereFigure 1: The bipartite graphs show the top 40 non-stopword Brown cluster pair features for all fourclassification tasks.
Each node on the left and on the right represents word cluster from Arg1 and Arg2respectively.
We only show the clusters that appear fewer than six times in the top 3,000 pairs to excludestopwords.
Although the four tasks are interrelated, some of the highest mutual information features varysubstantially across tasks.ficity within the scope of the relation.
For exam-ple, a cluster with industrial categories (Electric,Motor, Life, Chemical, Automotive) couples withspecific brands or companies (GM, Ford, Barrick,Anglo).
Or such a pair might simply reflects a shiftin plurality e.g.
businesses - business and Analysts-analyst.
EXPANSION relations capture relationsin which one argument provides a specification ofthe previous and relations in which one argumentprovides a generalization of the other.
Thus, theseshift detection features could help distinguish EX-PANSION relations.We found a few common coreference patternsof names in written English to be useful.
First andlast name are used in the first sentence to refer to aperson who just enters the discourse.
That personis referred to just by his/her title and last name inthe following sentence.
This pattern is found to be650All coreference Subject coreference0.00.10.20.30.40.5ComparisonContingencyExpansionTemporalComparisonContingencyExpansionTemporalCoreferential rateFigure 2: The coreferential rate for TEMPORALrelations is significantly higher than the other threerelations (p < 0.05, corrected for multiple com-parison).informative for EXPANSION relations.
For exam-ple, the edges (not shown in the graph due to lackof space) from the first name clusters to the title(Mr, Mother, Judge, Dr) cluster.Time expressions constitutes the majority of thenodes in the bipartite graph for TEMPORAL rela-tions.
More strikingly, the specific dates (e.g.
clus-ters that have positive integers smaller than 31)are more frequently found in Arg2 than Arg1 inimplicit TEMPORAL relations.
It is possible thatTEMPORAL relations are more naturally expressedwithout a discourse connective if a time point isclearly specified in Arg2 but not in Arg1.TEMPORAL relations might also be implicitlyinferred through detecting a shift in quantities.
Wenotice that clusters whose words indicate changese.g.
increase, rose, loss pair with number clusters.Sentences in which such pairs participate might bepart of a narrative or a report where one expects achange over time.
These changes conveyed by thesentences constitute a natural sequence of eventsthat are temporally related but might not need ex-plicit temporal expressions.6.2 Coreference featuresCoreference features are very effective given thatthey constitute a very small set compared to theother feature sets.
In particular, excluding themfrom the model reduces F1scores for TEMPORALand CONTINGENCY relations by approximately1% relative to the system that uses all of thefeatures.
We found that the sentence pairs in thesetwo types of relations have distinctive coreferencepatterns.We count the number of pairs of arguments thatare linked by a coreference chain for each type ofrelation.
The coreference chains used in this studyare detected automatically from the training setthrough Stanford CoreNLP suite (Raghunathan etal., 2010; Lee et al., 2011; Lee et al., 2013).
TEM-PORAL relations have a significantly higher coref-erential rate than the other three relations (p <0.05, pair-wise t-test corrected for multiple com-parisons).
The differences between COMPARI-SON, CONTINGENCY, and EXPANSION, however,are not statistically significant (Figure 2).The choice to use or not to use a discourseconnective is strongly motivated by linguistic fea-tures at the discourse levels (Patterson and Kehler,2013).
Additionally, it is very uncommon tohave temporally-related sentences without usingexplicit discourse connectives.
The difference incoreference patterns might be one of the factorsthat influence the choice of using a discourse con-nective to signal a TEMPORAL relation.
If sen-tences are coreferentially linked, then it might bemore natural to drop a discourse connective be-cause the temporal ordering can be easily inferredwithout it.
For example,(1) Her story is partly one of personal down-fall.
[previously] She was an unstintingteacher who won laurels and inspired stu-dents... (WSJ0044)The coreference chain between the twotemporally-related sentences in (1) can easilybe detected.
Inserting previously as suggestedby the annotation from the PDTB corpus doesnot add to the temporal coherence of the sen-tences and may be deemed unnecessary.
But thepresence of coreferential link alone might biasthe inference toward TEMPORAL relation whileCONTINGENCY might also be inferred.Additionally, we count the number of pairs ofarguments whose grammatical subjects are linkedby a coreference chain to reveal the syntactic-coreferential patterns in different relation types.Although this specific pattern seems rare, morethan eight percent of all relations have coreferen-tial grammatical subjects.
We observe the samestatistically significant differences between TEM-PORAL relations and the other three types of re-lations.
More interestingly, the subject coreferen-tial rate for CONTINGENCY relations is the lowestamong the three categories (p < 0.05, pair-wiset-test corrected for multiple comparisons).651It is possible that coreferential subject patternssuggest temporal coherence between the two sen-tences without using an explicit discourse connec-tive.
CONTINGENCY relations, which can only in-dicate causal relationships when realized implic-itly, impose the temporal ordering of events in thearguments; i.e.
if Arg1 is causally related to Arg2,then the event described in Arg1 must temporallyprecede the one in Arg2.
Therefore, CONTIN-GENCY and TEMPORAL can be highly confusable.To understand why this pattern might help distin-guish these two types of relations, consider theseexamples:(2) He also asserted that exact questions weren?treplicated.
[Then] When referred to the ques-tions that match, he said it was coincidental.
(WSJ0045)(3) He also asserted that exact questions weren?treplicated.
When referred to the questionsthat match, she said it was coincidental.When we switch out the coreferential subjectfor an arbitrary uncoreferential pronoun as we doin (3), we are more inclined to classify the relationas CONTINGENCY.7 Related workWord-pair features are known to work very wellin predicting senses of discourse relations in anartificially generated corpus (Marcu and Echi-habi, 2002).
But when used with a realistic cor-pus, model parameter estimation suffers from datasparsity problem due to the small dataset size.
Bi-ran and McKeown (2013) attempts to solve thisproblem by aggregating word pairs and estimatingweights from an unannotated corpus but only withlimited success.Recent efforts have focused on introducingmeaning abstraction and semantic representationbetween the words in the sentence pair.
Pitler et al.
(2009) uses external lexicons to replace the one-hot word representation with semantic informationsuch as word polarity and various verb classifica-tion based on specific theories (Stone et al., 1968;Levin, 1993).
Park and Cardie (2012) selects anoptimal subset of these features and establishes thestrongest baseline to best of our knowledge.Brown word clusters are hierarchical clustersinduced by frequency of co-occurrences with otherwords (Brown et al., 1992).
The strength of thisword class induction method is that the words thatare classified to the same clusters usually makean interpretable lexical class by the virtue of theirdistributional properties.
This word representationhas been used successfully to augment the perfor-mance of many NLP systems (Ritter et al., 2011;Turian et al., 2010).Louis et al.
(2010) uses multiple aspects ofcoreference as features to classify implicit dis-course relations without much success while sug-gesting many aspects that are worth exploring.
In acorpus study by Louis and Nenkova (2010), coref-erential rates alone cannot explain all of the rela-tions, and more complex coreference patterns haveto be considered.8 ConclusionsWe present statistical classifiers for identifyingsenses of implicit discourse relations and intro-duce novel feature sets that exploit distributionalsimilarity and coreference information.
Our clas-sifiers outperform the classifiers from previouswork in all types of implicit discourse relations.Altogether these results present a stronger base-line for the future research endeavors in implicitdiscourse relations.In addition to enhancing the performance of theclassifier, Brown word cluster pair features dis-close some of the new aspects of implicit dis-course relations.
The feature analysis confirmsour hypothesis that cluster pair features work wellbecause they encapsulate relevant word classeswhich constitute more complex informative fea-tures such as named-entity pairs of the same cat-egories, semantically-related pairs, and pairs thatindicate specificity-generality shift.
At the dis-course level, Brown clustering is superior to aone-hot word representation for identifying inter-sentential patterns and the interactions betweenwords.Coreference chains that traverse through thediscourse in the text shed the light on differ-ent types of relations.
The preliminary analy-sis shows that TEMPORAL relations have muchhigher inter-argument coreferential rates than theother three senses of relations.
Focusing on onlysubject-coreferential rates, we observe that CON-TINGENCY relations show the lowest coreferentialrate.
The coreference patterns differ substantiallyand meaningfully across discourse relations anddeserve further exploration.652ReferencesOr Biran and Kathleen McKeown.
2013.
Aggregatedword pair features for implicit discourse relation dis-ambiguation.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics, pages 69?73.
The Association for Compu-tational Linguistics.Peter F Brown, Peter V deSouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n -gram models of natural language.Computational Linguistics, 18(4):467?479, Decem-ber.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technol-ogy, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al.
2006.
Generat-ing typed dependency parses from phrase structureparses.
In Proceedings of LREC, volume 6, pages449?454.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.Nathalie Japkowicz.
2000.
Learning from imbalanceddata sets: a comparison of various strategies.
InAAAI workshop on learning from imbalanced datasets, volume 68.Michael Jordan and Andrew Ng.
2002.
On discrimi-native vs. generative classifiers: A comparison of lo-gistic regression and naive bayes.
Advances in neu-ral information processing systems, 14:841.Dan Klein and Christopher D Manning.
2003.
Accu-rate unlexicalized parsing.
In the 41st Annual Meet-ing, pages 423?430, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve coref-erence resolution system at the conll-2011 sharedtask.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, pages 28?34.
Association for ComputationalLinguistics.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Computational Linguistics.Beth Levin.
1993.
English verb classes and alter-nations: A preliminary investigation, volume 348.University of Chicago press Chicago.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan.2010.
A PDTB-Styled End-to-End DiscourseParser.
arXiv.org, November.Nick Littlestone.
1988.
Learning quickly when irrele-vant attributes abound: A new linear-threshold algo-rithm.
Machine learning, 2(4):285?318.Annie Louis and Ani Nenkova.
2010.
Creating lo-cal coherence: An empirical assessment.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 313?316.Association for Computational Linguistics.Annie Louis, Aravind Joshi, Rashmi Prasad, and AniNenkova.
2010.
Using entity features to classifyimplicit discourse relations.
In Proceedings of the11th Annual Meeting of the Special Interest Groupon Discourse and Dialogue, pages 59?62.
Associa-tion for Computational Linguistics.William C Mann and Sandra A Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse re-lations.
In Proceedings of the 40th Annual Meet-ing on Association for Computational Linguistics,pages 368?375.
Association for Computational Lin-guistics.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational linguistics, 19(2):313?330.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://www.cs.umass.edu/ mccallum/mallet.Joonsuk Park and Claire Cardie.
2012.
Improving im-plicit discourse relation recognition through featureset optimization.
In Proceedings of the 13th AnnualMeeting of the Special Interest Group on Discourseand Dialogue, pages 108?112.
Association for Com-putational Linguistics.Gary Patterson and Andrew Kehler.
2013.
Predictingthe presence of discourse connectives.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing.
Association for Compu-tational Linguistics.Emily Pitler, Mridhula Raghupathy, Hena Mehta, AniNenkova, Alan Lee, and Aravind K Joshi.
2008.Easily identifiable discourse relations.
TechnicalReports (CIS), page 884.653Emily Pitler, Annie Louis, and Ani Nenkova.
2009.Automatic sense prediction for implicit discourse re-lations in text.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 2-Volume2, pages 683?691.
Association for ComputationalLinguistics.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind K Joshi, and Bon-nie L Webber.
2008.
The penn discourse treebank2.0.
In LREC.
Citeseer.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?10,pages 492?501, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Alan Ritter, Sam Clark, Oren Etzioni, et al.
2011.Named entity recognition in tweets: an experimentalstudy.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1524?1534.
Association for Computational Linguis-tics.Philip Stone, Dexter C Dunphy, Marshall S Smith, andDM Ogilvie.
1968.
The general inquirer: A com-puter approach to content analysis.
Journal of Re-gional Science, 8(1).Kristina Toutanova and Christopher D Manning.
2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger.
In Proceedingsof the 2000 Joint SIGDAT conference on Empiricalmethods in natural language processing and verylarge corpora: held in conjunction with the 38th An-nual Meeting of the Association for ComputationalLinguistics-Volume 13, pages 63?70.
Association forComputational Linguistics.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Association forComputational Linguistics.Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li.
2012.Implicit discourse relation recognition by selectingtypical training examples.
In Proceedings of COL-ING 2012, pages 2757?2772, Mumbai, India, De-cember.
The COLING 2012 Organizing Committee.Ben Wellner, James Pustejovsky, Catherine Havasi,Anna Rumshisky, and Roser Sauri.
2009.
Clas-sification of discourse coherence relations: An ex-ploratory study using multiple knowledge sources.In Proceedings of the 7th SIGdial Workshop on Dis-course and Dialogue, pages 117?125.
Associationfor Computational Linguistics.654
