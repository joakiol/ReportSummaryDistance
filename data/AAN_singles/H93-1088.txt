RESEARCH IN  LARGE VOCABULARYCONTINUOUS SPEECH RECOGNIT ION*Janet Baker, Larry Gillick, and Robert RothDragon Systems, Inc.320 Nevada St.Newton, MA 02160PROJECT GOALSThe primary long term goal of speech research at DragonSystems is to develop algorithms that are capable ofachieving very high performance large vocabulary con-tinuous speech recognition.
At the same time, in thelong run we are also concerned to keep the demands ofthose algorithms for computational power and memoryas modest as possible, so that the results of our researchcan be incorporated into products that will run on mod-erately priced personal computers.RECENT RESULTSMuch of the past year's effort has been devoted towork on speaker independent training, linear discrim-inant analysis, and acoustic modeling, using the WallStreet Journal corpus as our development vehicle, withthe goal of attaining very high accuracy large vocabularySI CSR.
In the past, Dragon had focused primarily onspeaker dependent and speaker adaptive recognition , sothat speaker independent research was a new departurefor us in this past year.
Similarly, in the past Dragonhad confined itself to recognition algorithms that werehighly parsimonious in both computation and memoryusage, but we have now, temporarily, dropped those con-straints in the interest of seeing just what is attainablewith greater computational resources.Our research in the area of speaker independent traininghas focused on the use of tied mixture models with mul-tiparameter streams in representing the effect of contexton the acoustic realization of a phoneme.
These modelsare built using Bayesian smoothing in the context of theEM algorithm.
The prior distribution for a tied mixturemodel for a phoneme in a specific context represents ouropinion about that model based on other more genericmodels for that phoneme that have typically been builtfrom much more data.An important theme of our research as been the ex-ploration of the tradeoff between the greater ability tomodel the dependence among acoustic parameters when*This work was sponsored by the Defense Advanced ResearchProjects Agency under contract number J-FBI-92-060.streams are high dimensional versus the greater acous-tic resolution possible in streams with fewer parameters.Another important thread has been the investigation ofa variety of strategies for building the basis componentsfor the mixtures.The use of |MELDA, a particular form of linear discrim-inant analysis, has played a crucial role in our develop-ment in the last year as a way of eliminating some ofthe interspeaker variability represented in the parame-ters and, perhaps as a consequence, some of the depen-dence among our parameters, thus reducing the need forhigh dimensional streams.
IMELDA also was used for thepurpose of eliminating some of the variability due to themicrophone in the DARPA "stress test".
A further ben-eficial effect of this method lay in the reduction of thenumber of signal processing parameters (from 32 to 16)and thus the size of the acoustic models.The need to retrain our system many times during thecourse of our research led us to re-engineer the training sothat models for different phonemes could simultaneouslybe built on different computers.PLANS FOR THE COMING YEARDragon plans to continue to focus on improving the over-all quality of the acoustic models for large vocabularyspeaker independent continuous speech recognition.
Weintend to enhance the set of acoustic parameters whichare an input to the IMELDA transform, which is likelyto continue to play a key role in our system.
We willagain cluster the output distributions of the states intoPELs, or phonetic elements, as our earlier systems did,both for the purpose of reducing the size of the modelsand for the purpose of smoothing noisy estimates basedon insufficient training data.
Another direction for ex-ploration will be the use of mixture models without iedbasis components.We also intend to investigate cross-word modeling moreintensively, through the use of the position of the wordboundary as part of the context of a triphone model, andpossibly through the use of probabilistic phonologicalrules.394
