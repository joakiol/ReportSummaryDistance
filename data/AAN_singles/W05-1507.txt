Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 65?73,Vancouver, October 2005. c?2005 Association for Computational LinguisticsMachine Translation as Lexicalized Parsing with HooksLiang HuangDept.
of Computer & Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104Hao Zhang and Daniel GildeaComputer Science DepartmentUniversity of RochesterRochester, NY 14627AbstractWe adapt the ?hook?
trick for speeding upbilexical parsing to the decoding problemfor machine translation models that arebased on combining a synchronous con-text free grammar as the translation modelwith an n-gram language model.
Thisdynamic programming technique yieldslower complexity algorithms than havepreviously been described for an impor-tant class of translation models.1 IntroductionIn a number of recently proposed synchronousgrammar formalisms, machine translation of newsentences can be thought of as a form of parsing onthe input sentence.
The parsing process, however,is complicated by the interaction of the context-freetranslation model with an m-gram1 language modelin the output language.
While such formalisms ad-mit dynamic programming solutions having poly-nomial complexity, the degree of the polynomial isprohibitively high.In this paper we explore parallels between transla-tion and monolingual parsing with lexicalized gram-mars.
Chart items in translation must be augmentedwith words from the output language in order to cap-ture language model state.
This can be thought of asa form of lexicalization with some similarity to thatof head-driven lexicalized grammars, despite beingunrelated to any notion of syntactic head.
We show1We speak of m-gram language models to avoid confusionwith n, which here is the length of the input sentence for trans-lation.that techniques for parsing with lexicalized gram-mars can be adapted to the translation problem, re-ducing the complexity of decoding with an inversiontransduction grammar and a bigram language modelfrom O(n7) to O(n6).
We present background onthis translation model as well as the use of the tech-nique in bilexicalized parsing before describing thenew algorithm in detail.
We then extend the al-gorithm to general m-gram language models, andto general synchronous context-free grammars fortranslation.2 Machine Translation using InversionTransduction GrammarThe Inversion Transduction Grammar (ITG) of Wu(1997) is a type of context-free grammar (CFG) forgenerating two languages synchronously.
To modelthe translational equivalence within a sentence pair,ITG employs a synchronous rewriting mechanism torelate two sentences recursively.
To deal with thesyntactic divergence between two languages, ITGallows the inversion of rewriting order going fromone language to another at any recursive level.
ITGin Chomsky normal form consists of unary produc-tion rules that are responsible for generating wordpairs:X ?
e/fX ?
e/X ?
/fwhere e is a source language word, f is a foreign lan-guage word, and  means the null token, and binaryproduction rules in two forms that are responsiblefor generating syntactic subtree pairs:X ?
[Y Z]65andX ?
?Y Z?The rules with square brackets enclosing theright-hand side expand the left-hand side symbolinto the two symbols on the right-hand side in thesame order in the two languages, whereas the ruleswith angled brackets expand the left hand side sym-bol into the two right-hand side symbols in reverseorder in the two languages.
The first class of rulesis called straight rule.
The second class of rules iscalled inverted rule.One special case of 2-normal ITG is the so-calledBracketing Transduction Grammar (BTG) whichhas only one nonterminal A and two binary rulesA ?
[AA]andA ?
?AA?By mixing instances of the inverted rule withthose of the straight rule hierarchically, BTG canmeet the alignment requirements of different lan-guage pairs.
There exists a more elaborate versionof BTG that has 4 nonterminals working togetherto guarantee the property of one-to-one correspon-dence between alignments and synchronous parsetrees.
Table 1 lists the rules of this BTG.
In thediscussion of this paper, we will consider ITG in 2-normal form.By associating probabilities or weights with thebitext production rules, ITG becomes suitable forweighted deduction over bitext.
Given a sentencepair, searching for the Viterbi synchronous parsetree, of which the alignment is a byproduct, turns outto be a two-dimensional extension of PCFG parsing,having time complexity of O(n6), where n is thelength of the English string and the foreign languagestring.
A more interesting variant of parsing over bi-text space is the asymmetrical case in which only theforeign language string is given so that Viterbi pars-ing involves finding the English string ?on the fly?.The process of finding the source string given its tar-get counterpart is decoding.
Using ITG, decoding isa form of parsing.2.1 ITG DecodingWu (1996) presented a polynomial-time algorithmfor decoding ITG combined with an m-gram lan-guage model.
Such language models are commonlyused in noisy channel models of translation, whichfind the best English translation e of a foreign sen-tence f by finding the sentence e that maximizes theproduct of the translation model P (f |e) and the lan-guage model P (e).It is worth noting that since we have specified ITGas a joint model generating both e and f , a languagemodel is not theoretically necessary.
Given a foreignsentence f , one can find the best translation e?:e?
= argmaxeP (e, f)= argmaxe?qP (e, f, q)by approximating the sum over parses q with theprobability of the Viterbi parse:e?
= argmaxemaxqP (e, f, q)This optimal translation can be computed in usingstandard CKY parsing over f by initializing thechart with an item for each possible translation ofeach foreign word in f , and then applying ITG rulesfrom the bottom up.However, ITG?s independence assumptions aretoo strong to use the ITG probability alone for ma-chine translation.
In particular, the context-free as-sumption that each foreign word?s translation is cho-sen independently will lead to simply choosing eachforeign word?s single most probable English trans-lation with no reordering.
In practice it is beneficialto combine the probability given by ITG with a localm-gram language model for English:e?
= argmaxemaxqP (e, f, q)Plm(e)?with some constant language model weight ?.
Thelanguage model will lead to more fluent output byinfluencing both the choice of English words and thereordering, through the choice of straight or invertedrules.
While the use of a language model compli-cates the CKY-based algorithm for finding the besttranslation, a dynamic programming solution is stillpossible.
We extend the algorithm by storing in eachchart item the English boundary words that will af-fect the m-gram probabilities as the item?s Englishstring is concatenated with the string from an adja-cent item.
Due to the locality of m-gram language66Structural Rules Lexical RulesS ?
AS ?
BS ?
CA ?
[AB]A ?
[BB]A ?
[CB]A ?
[AC]A ?
[BC]A ?
[CC]B ?
?AA?B ?
?BA?B ?
?CA?B ?
?AC?B ?
?BC?B ?
?CC?C ?
ei/fjC ?
/fjC ?
ei/Table 1: Unambiguous BTGmodel, only m?1 boundary words need to be storedto compute the new m-grams produced by combin-ing two substrings.
Figure 1 illustrates the combi-nation of two substrings into a larger one in straightorder and inverted order.3 Hook Trick for Bilexical ParsingA traditional CFG generates words at the bottom ofa parse tree and uses nonterminals as abstract rep-resentations of substrings to build higher level treenodes.
Nonterminals can be made more specific tothe actual substrings they are covering by associ-ating a representative word from the nonterminal?syield.
When the maximum number of lexicalizednonterminals in any rule is two, a CFG is bilexical.A typical bilexical CFG in Chomsky normal formhas two types of rule templates:A[h] ?
B[h]C[h?
]orA[h] ?
B[h?
]C[h]depending on which child is the head child thatagrees with the parent on head word selection.Bilexical CFG is at the heart of most modern statisti-cal parsers (Collins, 1997; Charniak, 1997), becausethe statistics associated with word-specific rules aremore informative for disambiguation purposes.
Ifwe use A[i, j, h] to represent a lexicalized con-stituent, ?(?)
to represent the Viterbi score functionapplicable to any constituent, and P (?)
to representthe rule probability function applicable to any rule,Figure 2 shows the equation for the dynamic pro-gramming computation of the Viterbi parse.
The twoterms of the outermost max operator are symmetriccases for heads coming from left and right.
Contain-ing five free variables i,j,k,h?,h, ranging over 1 ton, the length of input sentence, both terms can beinstantiated in n5 possible ways, implying that thecomplexity of the parsing algorithm is O(n5).Eisner and Satta (1999) pointed out we don?t haveto enumerate k and h?
simultaneously.
The trick,shown in mathematical form in Figure 2 (bottom) isvery simple.
When maximizing over h?, j is irrele-vant.
After getting the intermediate result of maxi-mizing over h?, we have one less free variable thanbefore.
Throughout the two steps, the maximumnumber of interacting variables is 4, implying thatthe algorithmic complexity is O(n4) after binarizingthe factors cleverly.
The intermediate resultmaxh?,B[?
(B[i, k, h?])
?
P (A[h] ?
B[h?
]C[h])]can be represented pictorially asC[h]Ai k .
Thesame trick works for the second max term inEquation 1.
The intermediate result coming frombinarizing the second term can be visualized asAkB[h]j.
The shape of the intermediate re-sults gave rise to the nickname of ?hook?.
Melamed(2003) discussed the applicability of the hook trickfor parsing bilexical multitext grammars.
The anal-ysis of the hook trick in this section shows that it isessentially an algebraic manipulation.
We will for-mulate the ITG Viterbi decoding algorithm in a dy-namic programming equation in the following sec-tion and apply the same algebraic manipulation toproduce hooks that are suitable for ITG decoding.4 Hook Trick for ITG DecodingWe start from the bigram case, in which each de-coding constituent keeps a left boundary word and67tu11 u12 v12v11 u21 u22 v22v21XY Z[ ]Ssu21XY ZSs t< >v21 v22 u11 u12 v11 v12u22(a) (b)Figure 1: ITG decoding using 3-gram language model.
Two boundary words need to be kept on the left (u)and right (v) of each constituent.
In (a), two constituents Y and Z spanning substrings s, S and S, t of theinput are combined using a straight rule X ?
[Y Z].
In (b), two constituents are combined using a invertedrule X ?
?Y Z?.
The dashed line boxes enclosing three words are the trigrams produced from combiningtwo substrings.?
(A[i, j, h]) = max?????maxk,h?,B,C[?
(B[i, k, h?])
?
?
(C[k, j, h]) ?
P (A[h] ?
B[h?]C[h])],maxk,h?,B,C[?
(B[i, k, h]) ?
?
(C[k, j, h?])
?
P (A[h] ?
B[h]C[h?])]?????(1)maxk,h?,B,C[?
(B[i, k, h?])
?
?
(C[k, j, h]) ?
P (A[h] ?
B[h?
]C[h])]= maxk,C[maxh?,B[?
(B[i, k, h?])
?
P (A[h] ?
B[h?]C[h])]?
?
(C[k, j, h])]Figure 2: Equation for bilexical parsing (top), with an efficient factorization (bottom)a right boundary word.
The dynamic programmingequation is shown in Figure 3 (top) where i,j,k rangeover 1 to n, the length of input foreign sentence, andu,v,v1,u2 (or u,v,v2,u1) range over 1 to V , the sizeof English vocabulary.
Usually we will constrain thevocabulary to be a subset of words that are probabletranslations of the foreign words in the input sen-tence.
So V is proportional to n. There are sevenfree variables related to input size for doing the max-imization computation.
Hence the algorithmic com-plexity is O(n7).The two terms in Figure 3 (top) within the firstlevel of the max operator, corresponding to straightrules and inverted rules, are analogous to the twoterms in Equation 1.
Figure 3 (bottom) shows how todecompose the first term; the same method appliesto the second term.
Counting the free variables en-closed in the innermost max operator, we get five: i,k, u, v1, and u2.
The decomposition eliminates onefree variable, v1.
In the outermost level, there aresix free variables left.
The maximum number of in-teracting variables is six overall.
So, we reduced thecomplexity of ITG decoding using bigram languagemodel from O(n7) to O(n6).The hooks kXZu u2i that we have built for de-coding with a bigram language model turn out to besimilar to the hooks for bilexical parsing if we focuson the two boundary words v1 and u2 (or v2 and u1)68?
(X[i, j, u, v]) = max?????????maxk,v1,u2,Y,Z[?
(Y [i, k, u, v1]) ?
?
(Z[k, j, u2, v])?
P (X ?
[Y Z]) ?
bigram(v1, u2)],maxk,v2,u1,Y,Z[?
(Y [i, k, u1, v]) ?
?
(Z[k, j, u, v2])?
P (X ?
?Y Z?)
?
bigram(v2, u1)]?????????(2)maxk,v1,u2,Y,Z[?
(Y [i, k, u, v1]) ?
?
(Z[k, j, u2, v]) ?
P (X ?
[Y Z]) ?
bigram(v1, u2)]= maxk,u2,Z[maxv1,Y[?
(Y [i, k, u, v1]) ?
P (X ?
[Y Z]) ?
bigram(v1, u2)]?
?
(Z[k, j, u2, v])]Figure 3: Equation for ITG decoding (top), with an efficient factorization (bottom)that are interacting between two adjacent decodingconstituents and relate them with the h?
and h thatare interacting in bilexical parsing.
In terms of al-gebraic manipulation, we are also rearranging threefactors (ignoring the non-lexical rules), trying to re-duce the maximum number of interacting variablesin any computation step.4.1 Generalization to m-gram CasesIn this section, we will demonstrate how to use thehook trick for trigram decoding which leads us to ageneral hook trick for any m-gram decoding case.We will work only on straight rules and use iconsof constituents and hooks to make the equations eas-ier to interpret.The straightforward dynamic programming equa-tion is:iXu1u2 v1v2j = maxv11,v12,u21,u22,k,Y,Zu22i k jXY Zu1u2 v2v1][v11v12 u21(3)By counting the variables that are dependenton input sentence length on the right hand sideof the equation, we know that the straightfor-ward algorithm?s complexity is O(n11).
The max-imization computation is over four factors thatare dependent on n: ?
(Y [i, k, u1, u2, v11, v12]),?
(Z[k, j, u21, u22, v1, v2]), trigram(v11, v12, u21),and trigram(v12, u21, u22).
As before, our goal isto cleverly bracket the factors.By bracketing trigram(v11, v12, u21) and?
(Y [i, k, u1, u2, v11, v12]) together and maximizingover v11 and Y , we can build the the level-1 hook:u21i kXZu1u2][v12= maxv11,Yu21i kXY Zu1u2][v11v12The complexity is O(n7).Grouping the level-1 hook andtrigram(v12, u21, u22), maximizing over v12,we can build the level-2 hook:u21i kXZu1u2][u22= maxv12u21i kXZu1u2][v12 u22The complexity is O(n7).
Finally,we can use the level-2 hook to com-bine with Z[k, j, u21, u22, v1, v2] to buildX[i, j, u1, u2, v1, v2].
The complexity is O(n9)after reducing v11 and v12 in the first two steps.iXu1u2 v1v2j = maxu21,u22,k,Zu22i k jXZu1u2 v2v1][u21(4)Using the hook trick, we have reduced the com-plexity of ITG decoding using bigrams from O(n7)to O(n6), and from O(n11) to O(n9) for trigram69case.
We conclude that for m-gram decoding ofITG, the hook trick can change the the time com-plexity from O(n3+4(m?1)) to O(n3+3(m?1)).
Toget an intuition of the reduction, we can compareEquation 3 with Equation 4.
The variables v11 andv12 in Equation 3, which are independent of v1 andv2 for maximizing the product have been concealedunder the level-2 hook in Equation 4.
In general,by building m ?
1 intermediate hooks, we can re-duce m ?
1 free variables in the final combinationstep, hence having the reduction from 4(m ?
1) to3(m ?
1).5 Generalization to Non-binary BitextGrammarsAlthough we have presented our algorithm as a de-coder for the binary-branching case of InversionTransduction Grammar, the same factorization tech-nique can be applied to more complex synchronousgrammars.
In this general case, items in the dy-namic programming chart may need to representnon-contiguous span in either the input or outputlanguage.
Because synchronous grammars with in-creasing numbers of children on the right hand sideof each production form an infinite, non-collapsinghierarchy, there is no upper bound on the numberof discontinuous spans that may need to be repre-sented (Aho and Ullman, 1972).
One can, however,choose to factor the grammar into binary branchingrules in one of the two languages, meaning that dis-continuous spans will only be necessary in the otherlanguage.If we assume m is larger than 2, it is likely thatthe language model combinations dominate com-putation.
In this case, it is advantageous to factorthe grammar in order to make it binary in the out-put language, meaning that the subrules will onlyneed to represent adjacent spans in the output lan-guage.
Then the hook technique will work in thesame way, yielding O(n2(m?1)) distinct types ofitems with respect to language model state, and3(m?1) free indices to enumerate when combininga hook with a complete constituent to build a newitem.
However, a larger number of indices point-ing into the input language will be needed now thatitems can cover discontinuous spans.
If the gram-mar factorization yields rules with at most R spansin the input language, there may be O(n2R) dis-tinct types of chart items with respect to the inputlanguage, because each span has an index for itsbeginning and ending points in the input sentence.Now the upper bound of the number of free in-dices with respect to the input language is 2R + 1,because otherwise if one rule needs 2R + 2 in-dices, say i1, ?
?
?
, i2R+2, then there are R + 1 spans(i1, i2), ?
?
?
, (i2R+1, i2R+2), which contradicts theabove assumption.
Thus the time complexity at theinput language side is O(n2R+1), yielding a total al-gorithmic complexity of O(n3(m?1)+(2R+1)).To be more concrete, we will work through a 4-ary translation rule, using a bigram language model.The standard DP equation is:iu vjA= maxv3,u1,v1,u4,v4,u2,k1,k2,k3,B,C,D,EB C D EAv3u u1 v1 u4 v4 u2 vi k1 k2 k3 j (5)This 4-ary rule is a representative difficult case.The underlying alignment pattern for this rule is asfollows:DCEBAIt is a rule that cannot be binarized in the bitextspace using ITG rules.
We can only binarize it inone dimension and leave the other dimension havingdiscontinuous spans.
Without applying binarizationand hook trick, decoding parsing with it accordingto Equation 5 requires time complexity of O(n13).However, we can build the following partial con-stituents and hooks to do the combination gradually.The first step finishes a hook by consuming onebigram.
Its time complexity is O(n5):C D EAu1uk2 k3 = maxv3,BB C D EAu v3 u1k2 k3The second step utilizes the hook we just built andbuilds a partial constituent.
The time complexity isO(n7):70D EAu v1i k1 k2 k3 = maxu1,CC D EAu u1 v1i k1 k2 k3By ?eating?
another bigram, we build the secondhook using O(n7):D EAu u4i k1 k2 k3 = maxv1D EAu v1 u4i k1 k2 k3We use the last hook.
This step has higher com-plexity: O(n8):EAu v4i k1 k2 j = maxu4,k3,Dv4u4k2 k3D EAjk1iuThe last bigram involved in the 4-ary rule is com-pleted and leads to the third hook, with time com-plexity of O(n7):EAjk2k1iu u2= maxv4EAu v4 u2i k1 k2 jThe final combination is O(n7):iu vjA= maxu2,k1,k2,Eui k1 k2EAu2jvThe overall complexity has been reduced toO(n8) after using binarization on the output side andusing the hook trick all the way to the end.
The resultis one instance of our general analysis: here R = 2,m = 2, and 3(m ?
1) + (2R + 1) = 8.6 ImplementationThe implementation of the hook trick in a practi-cal decoder is complicated by the interaction withpruning.
If we build hooks looking for all wordsin the vocabulary whenever a complete constituentis added to the chart, we will build many hooksthat are never used, because partial hypotheses withmany of the boundary words specified by the hooksmay never be constructed due to pruning.
In-stead of actively building hooks, which are inter-mediate results, we can build them only when weneed them and then cache them for future use.
Tomake this idea concrete, we sketch the code for bi-gram integrated decoding using ITG as in Algo-rithm 1.
It is worthy of noting that for clarity weare building hooks in shape ofvk jv?Z, insteadofXY vk jv?as we have been showing in theprevious sections.
That is, the probability for thegrammar rule is multiplied in when a complete con-stituent is built, rather than when a hook is created.If we choose the original representation, we wouldhave to create both straight hooks and inverted hooksbecause the straight rules and inverted rules are to bemerged with the ?core?
hooks, creating more speci-fied hooks.7 ConclusionBy showing the parallels between lexicalization forlanguage model state and lexicalization for syntac-tic heads, we have demonstrated more efficient al-gorithms for previously described models of ma-chine translation.
Decoding for Inversion Transduc-tion Grammar with a bigram language model can bedone in O(n6) time.
This is the same complexityas the ITG alignment algorithm used by Wu (1997)and others, meaning complete Viterbi decoding ispossible without pruning for realistic-length sen-tences.
More generally, ITG with an m-gram lan-guage model is O(n3+3(m?1)), and a synchronouscontext-free grammar with at most R spans in theinput language is O(n3(m?1)+(2R+1)).
While thisimproves on previous algorithms, the degree in nis probably still too high for complete search tobe practical with such models.
The interaction ofthe hook technique with pruning is an interesting71Algorithm 1 ITGDecode(Nt)for all s, t such that 0 ?
s < t ?
Nt dofor all S such that s < S < t do straight rulefor all rules X ?
[Y Z] ?
G dofor all (Y, u1, v1) possible for the span of (s, S) do a hook who is on (S, t), nonterminal as Z, and outside expectation being v1 is requiredif not exist hooks(S, t, Z, v1) thenbuild hooks(S, t, Z, v1)end iffor all v2 possible for the hooks in (S, t, Z, v1) do combining a hook and a hypothesis, using straight rule?
(s, t, X, u1, v2) =max{?
(s, t, X, u1, v2), ?
(s, S, Y, u1, v1) ?
?+(S, t, Z, v1, v2) ?
P (X ?
[Y Z])}end forend forend for inverted rulefor all rules X ?
?Y Z?
?
G dofor all (Z, u2, v2) possible for the span of (S, t) do a hook who is on (s, S), nonterminal as Y , and outside expectation being v2 is requiredif not exist hooks(s, S, Y, v2) thenbuild hooks(s, S, Y, v2)end iffor all v1 possible for the hooks in (s, S, Y, v2) do combining a hook and a hypothesis, using inverted rule?
(s, t, X, u2, v1) =max{?
(s, t, X, u2, v1), ?
(S, t, Z, u2, v2) ?
?+(s, S, Y, v2, v1) ?
P (X ?
?Y Z?
)}end forend forend forend forend forroutine build hooks(s, t, X, v?
)for all (X, u, v) possible for the span of (s, t) do combining a bigram with a hypothesis?+(s, t, X, v?, v) =max{?+(s, t, X, v?, v), bigram(v?, u) ?
?
(s, t, X, u, v)}end for72area for future work.
Building the chart items withhooks may take more time than it saves if many ofthe hooks are never combined with complete con-stituents due to aggressive pruning.
However, it maybe possible to look at the contents of the chart in or-der to build only those hooks which are likely to beuseful.ReferencesAho, Albert V. and Jeffery D. Ullman.
1972.
The The-ory of Parsing, Translation, and Compiling, volume 1.Englewood Cliffs, NJ: Prentice-Hall.Charniak, Eugene.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proceed-ings of the Fourteenth National Conference on Arti-ficial Intelligence (AAAI-97), pages 598?603, MenloPark, August.
AAAI Press.Collins, Michael.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Conference of the Association for Compu-tational Linguistics (ACL-97), pages 16?23, Madrid,Spain.Eisner, Jason and Giorgio Satta.
1999.
Efficient parsingfor bilexical context-free grammars and head automa-ton grammars.
In 37th Annual Meeting of the Associ-ation for Computational Linguistics.Melamed, I. Dan.
2003.
Multitext grammars and syn-chronous parsers.
In Proceedings of the 2003 Meetingof the North American chapter of the Association forComputational Linguistics (NAACL-03), Edmonton.Wu, Dekai.
1996.
A polynomial-time algorithm for sta-tistical machine translation.
In 34th Annual Meetingof the Association for Computational Linguistics.Wu, Dekai.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.73
