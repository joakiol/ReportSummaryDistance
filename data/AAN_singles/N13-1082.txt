Proceedings of NAACL-HLT 2013, pages 697?702,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsAn Examination of Regret in Bullying TweetsJun-Ming Xu, Benjamin Burchfiel, Xiaojin ZhuDepartment of Computer SciencesUniversity of Wisconsin-MadisonMadison, WI 53706, USA{xujm,burchfie,jerryzhu}@cs.wisc.eduAmy BellmoreDepartment of Educational PsychologyUniversity of Wisconsin-MadisonMadison, WI 53706, USAabellmore@wisc.eduAbstractSocial media users who post bullying relatedtweets may later experience regret, potentiallycausing them to delete their posts.
In this pa-per, we construct a corpus of bullying tweetsand periodically check the existence of eachtweet in order to infer if and when it becomesdeleted.
We then conduct exploratory analy-sis in order to isolate factors associated withdeleted posts.
Finally, we propose the con-struction of a regrettable posts predictor towarn users if a tweet might cause regret.1 IntroductionA large body of literature suggests that participantsin bullying events, including victims, bullies, andwitnesses, are likely to report psychological adjust-ment problems (Jimerson, Swearer, and Espelage,2010).
One potential source of therapy for these is-sues can be self-disclosure of the experience to anadult or friend (Mishna and Alaggia, 2005); exist-ing research suggests that victims who seek adviceand help from others report less maladjustment thanvictims who do not (Shelley and Craig, 2010).Disclosure of bullying experiences through so-cial media may be a particularly effective mecha-nism for participants seeking support because so-cial media has the potential to reach large audi-ences and because participants may feel less inhi-bition when sharing private information in an on-line setting (Walther, 1996).
Furthermore, there isevidence that online communication stimulates self-disclosure, which leads to higher quality social rela-tionships and increased well-being (Valkenburg andPeter, 2009).Online disclosure may also present risks forthose involved in bullying however, such as re-victimization, embarrassment, and social ostraciza-tion.
Evidence exists that some individuals may re-act to these risks retroactively, by deleting their so-cial media posts (Child et al 2011; Christofides,Muise, and Desmarais, 2009).
Several relevant mo-tives have been found to be associated with delet-ing posted information, including conflict manage-ment, safety, fear of retribution, impression manage-ment, and emotional regulation (Child, Haridakis,and Petronio, 2012).Our previous work (Xu et al 2012) demonstratesthat social media can be a valuable data source whenstudying bullying, and proposes a text categorizationmethod to recognize social media posts describingbullying episodes, bullying traces.
To better under-stand, and possibly prevent, user regret after postingbullying related tweets, we collect bullying tracesusing the same method and perform regular statuschecks to determine if and when tweets become in-accessible.
While a tweet becoming inaccessibledoes not guarantee it has been deleted, we attempt toleverage http response codes to rule out other com-mon causes of inaccessibility.
Speculating that re-gret may be a major cause of deletion, we first con-duct exploratory analysis on this corpus and then re-port the results of an off-the-shelf regret predictor.2 Data CollectionWe adopt the procedure used in (Xu et al 2012) toobtain bullying traces; each identified trace contains697at least one bullying related keyword and passes abullying-or-not text classifier.Our data was collected in realtime using theTwitter streaming API; once a tweet is collected,we query its url (https://twitter.com/USERID/status/TWEETID) at regular intervalsand infer its status from the resulting http responsecode.
We interpret an HTTP 200 response as an indi-cation a tweet still exists and an HTTP 404 response,which indicates the tweet is unavailable, as indicat-ing deletion.
A user changing their privacy settingscan also result in an HTTP 403 response; we do notconsider this to be a deletion.
Other response codes,which appear quite rarely, are treated as anomaliesand ignored.
All non HTTP 200 responses are re-tried twice to ensure they are not transient oddities.To determine when a tweet is deleted, we at-tempted to access each tweet at time points Ti =5 ?
4i minutes for i = 0, 1 .
.
.
7 after the creationtime.
These roughly correspond to periods of 5 min-utes, 20 minutes, 1.5 hours, 6 hours, 1 day, 4 days,2 weeks, and 2 months.
While we assume that userdeletion is the main cause of a tweet becoming un-available, other causes are possible such as the cen-sorship of illegal contents by Twitter (Twitter, 2012).Our sample data was collected from July 31through October 31, 2012 and contains 522,984 bul-lying traces.
Because of intermittent network andcomputer issues, several multiple day data gaps ex-ist in the data.
To combat this, we filter our data toinclude only tweets of unambiguous status.
If anycheck within the 20480 minutes (about two weeks)interval returns an HTTP 404 code, the tweet isno longer accessible and we consider it deleted.
Ifthe 20480 minute or 81920 minute check returns anHTTP 200 response, that tweet is still accessible andwe consider it surviving.
The union of the survivingand deleted groups formed our cleaned dataset, con-taining 311,237 tweets in total.3 Exploratory Data AnalysisA user?s decision to delete a bullying trace may bethe result of many factors which we would like toisolate and understand.
In this section we will ex-amine several such possible factors.3.1 Word UsageOur dataset contains 331,070 distinct words and weare interested in isolating those with a significantlyhigher presence among either deleted or survivingtweets.
We define the odds ratio of a word wr(w) =P (w | deleted)P (w | surviving),where P (w | deleted) is the probability of word woccurring in a deleted tweet, and P (w | surviving) isthe probability of w appearing in a surviving tweet.In order to ensure stability in the probability estima-tion, we only considered words appearing at least 50times in either the surviving or deleted corpora.Following (Bamman, OConnor, and Smith,2012), we qualitatively analyzed words with ex-treme values of r(w), and found some interestingtrends.
There was a significant tendency for ?jok-ing?
words to occur with r(w) < 0.5; examples in-clude ?xd,?
?haha,?
and ?hahaha.?
Joking words oc-cur less frequently in deleted tweets than survivingones.
On the other end of the spectrum, there wereno joking words with r(w) > 2.
What we foundinstead were words such as ?rip,?
?fat,?
?kill,?
and?suicide.?
While it is relatively clear that joking isless likely to occur in deleted tweets, there was lessof a trend among words appearing more frequentlyin deleted tweets.3.2 Surviving TimeLet N be the total number of tweets in our cor-pus, and D(Ti) be the number of tweets that werefirst detected as deleted at minute Ti after creation.Note that D(Ti) is not cumulative over time: it in-cludes only deletions that occurred in the time inter-val (Ti?1, Ti].
Then we may define the deletion rateat time Ti asRT (Ti) =D(Ti)N(Ti ?
Ti?1).In other words, RT (t) is the fraction of tweets thatare deleted during the one minute period (t, t+ 1).We plot RT vs. t using logarithmic scales on bothaxes in Figure 1 and the result is a quite strong lineartrend.
Fitting the plot with a linear regression, wederive an inverse relationship between RT and t ofthe formRT (t) ?
1/t.6985 minutes 1.5 hours 1 day 2 weeks1E?71E?61E?51E?41E?3tR T(t)Figure 1: Deletion rate decays over time.This result makes sense; the social effects of a par-ticular bullying tweet may decay over time, makingregret less of a factor.
Furthermore, the author mayassume an older tweet has already been seen, render-ing deletion ineffective.
Additionally, because thedrop off in deletion rate is so extreme, we are able tosafely exclude deletions occurring after two weeksfrom our filtered dataset without introducing a sig-nificant amount of noise.
Finally,?
?t=0RT (t) givesthe overall fraction of deletion, which in our case isaround 4%.3.3 Location and Hour of CreationsSome bullying traces contain location meta-data inthe form of GPS coordinates or a user-created profilestring.
We employed a reverse geocoding database(http://www.datasciencetoolkit.org)and a rule-based string matching method to mapthese tweets to their origins (at the state level; onlyfor tweets within the USA).
This also allowed us toconvert creation timestamps from UTC to local timeby mapping user location to timezone.
Becausemany users don?t share their location, we were onlyable to successfully map 85,465 bullying traces to aUS state s, and local hour of day h. Among thesetraces, 3,484 were deleted which translates to anoverall deletion rate of about 4%.Let N(s, h) be the count of bullying traces cre-ated in state s and hour h. Aggregating these countstemporally yields NS(s) =?hN(s, h), while ag-gregating spatially produces NH(h) =?sN(s, h).Similarly, we can defineD(s, h),DS(s) andDH(h)as the corresponding counts of deleted traces.
Wecan now compute the deletion rateRH(h) =DH(h)NH(h), and RS(s) =DS(s)NS(s).The top row of Figure 2 shows NH(h), DH(h),and RH(h).
We find that NH(h) and DH(h) peakin the evening, indicating social media users are gen-erally more active at that time.
The peak of RH(h)appears at late night and, while there are multiplepotential causes for this, we hypothesize that usersmay fail to fully evaluate the consequences of theirposts when tired.
The bottom row of Figure 2 showsNS(s), DS(S), and RS(s).
The plot of NS(s)shows that bullying traces are more likely to origi-nate in California, Texas or New York which is theresult of a population effect.
Importantly however,the deletion rate RS(s) is not affected by populationbias and we see, as expected, that spatial differencesin RS(s) are small.
We performed ?2-test to see ifa state?s deletion rate is significantly different fromthe national average.
We chose the significance levelat 0.05 and used Bonferroni correction for multipletesting.
Only four states have significantly differ-ent deletion rates from the average: Arizona (6.3%,p = 5.9?10?5), California (5.2%, p = 2.7?10?7),Maryland (1.9%, p = 2.3 ?
10?5), and Oklahoma(7.1%, p = 3.5?
10?5).3.4 Author?s RoleParticipants in a bullying episode assume well-defined roles which dramatically affect the view-point of the author describing the event.
We traineda text classifier to determine author role (Xu et al2012), and used it to label each bullying trace in thecleaned corpus by author role: Accuser, Bully, Re-porter, Victim or Other.Table 1 shows that compared to tweets producedby bullies, victims create more bullying traces, pos-sibly due to an increased need for social support onthe part of the victim.
More importantly, P (deleted |victim) is higher than P (deleted | bully), a statis-tically significant difference in a two-proportion z-test.
Possibly, victims are more sensitive to their au-dience?s reaction than bullies.3.5 TeasingMany bullying traces are written jokingly.
We built atext classifier to identify teasing bullying traces (Xuet al 2012) and applied it to the cleaned corpus.Table 2 shows that P (deletion | Teasing) is muchlower than P (deletion | Not Teasing) and the differ-ence is statistically significant in a two-proportion z-699NH(h) DH(h) RH(h)NS(s) DS(s) RS(s)Figure 2: Counts and deletion rates of geo-tagged bullying traces.Deleted Total P (deleted | Role)Accuser 2541 50088 5.07%Bully 1792 30123 5.95%Reporter 11370 147164 7.73%Victim 6497 83412 7.79%Other 41 450 9.11%Table 1: Counts and deletion rate for different roles.Deleted Total P (deleted | Teasing?
)Yes 858 22876 3.75%Not 21383 288361 7.42%Table 2: Counts and deletion rate for teasing or not.test.
It seems plausible that authors are less likely toregret teasing posts because they are less controver-sial and have less potential to generate negative au-dience reactions.
This also corroborates our findingsin word usage that joking words are less frequent indeleted tweets.4 Predicting Regrettable TweetsOnce a bullying tweet is published and seen by oth-ers, the ensuing effects are often impossible to undo.Since ill-thought-out posts may cause unexpectedlynegative consequences to an author?s reputation, re-lationship, and career (Wang et al 2011), it wouldbe helpful if a system could warn users before a po-tentially regrettable tweet is posted.
One straightfor-ward approach is to formulate the task as a binarytext categorization problem.We use the cleaned dataset, in which each tweetis known to be surviving or deleted after 20480 min-utes (about two weeks).
Since this dataset contains22,241 deleted tweets, we randomly sub-sampledthe surviving tweets down to 22,241 to force ourdeleted and surviving datasets to be of equal size.Consequentially, the baseline accuracy of the clas-sifier is 0.5.
While this does make the problem ar-tificially easier, our initial goal was to test for thepresence of a signal in the data.We then followed the preprocessing procedurein (Xu et al 2012), performing case-folding,anonymization, and tokenization, treating URLs,emoticons and hashtags specially.
We also chosethe unigrams+bigrams feature representation, onlykeeping tokens appearing at least 15 times in the cor-pus.We chose to employ a linear SVM implementedin LIBLINEAR (Fan et al 2008) due to its effi-ciency on this large sparse text categorization taskand a 10-fold cross validation was conducted to eval-700uate its performance.
Within the first fold, we usean inner 5-fold cross validation on the training por-tion to tune the regularization parameter on the grid{2?10, 2?9, .
.
.
, 1}; the selected parameter is thenfixed for all the remaining folds.The resulting cross validation accuracy was 0.607with a standard deviation of 0.012.
While it is statis-tically significantly better than the random-guessingbaseline accuracy of 0.5 with a p-value of 5.15 ?10?10, this accuracy is nevertheless too low to beuseful in a practical system.
One possibility is thatthe tweet text contains very limited information forpredicting inaccessibility; a user?s decision to deletea tweet potentially depends on many other factors,such as the conversation context and the characteris-tics of the author and audience.In the spirit of exploring additional informativefeatures for deletion prediction, we also used theteasing and author role classifiers in (Xu et al2012), and appended the predicted teasing, and au-thor role labels to our feature vector.
This aug-mented feature representation achieved a cross val-idation accuracy of 0.606, with standard deviation0.007; not statistically significantly different fromthe text-only feature representation.
While it seemsthat a signal does exist, leveraging it usefully in realworld scenarios may prove challenging due to thehighly-skewed nature of the data.5 DiscussionThere have been several recent works examin-ing causes of deletion in social media.
Wanget al(2011) qualitatively investigated regret associ-ated with users?
posts on social networking sites andidentified several possible causes of regret.
Bammanet al(2012) focused on censorship-related deletionof social media posts, identifying a set of sensitiveterms related to message deletion through a statisti-cal analysis and spatial variation of deletion rate.Assuming that deletion in social media is indica-tive of regret, we studied regret in a bullying con-text by analyzing deletion trends in bullying re-lated tweets.
Through our analysis, we were ableto isolate several factors related to deletion, includ-ing word usage, surviving time, and author role.
Weused these factors to build a regret predictor whichachieved statistically significant results on this verynoisy data.
In the future, we plan to explore morefactors to better understand deletion behavior and re-gret, including users?
recent posts, historical behav-ior, and other statistics related to their specific socialnetwork.AcknowledgmentsWe thank Kwang-Sung Jun, Angie Calvin, andCharles Dyer for helpful discussions.
This workis supported by National Science Foundation grantsIIS-1216758 and IIS-1148012.ReferencesBamman, David, Brendan OConnor, and Noah Smith.2012.
Censorship and deletion practices in chinese so-cial media.
First Monday, 17(3-5).Child, Jeffrey T., Paul M. Haridakis, and Sandra Petro-nio.
2012.
Blogging privacy rule orientations, privacymanagement, and content deletion practices: The vari-ability of online privacy management activity at differ-ent stages of social media use.
Computers in HumanBehavior, 28(5):1859 ?
1872.Child, Jeffrey T, Sandra Petronio, Esther A Agyeman-Budu, and David A Westermann.
2011.
Blog scrub-bing: Exploring triggers that change privacy rules.Computers in Human Behavior, 27(5):2017?2027.Christofides, Emily, Amy Muise, and Serge Desmarais.2009.
Information disclosure and control on facebook:are they two sides of the same coin or two differentprocesses?
CyberPsychology & Behavior, 12(3):341?345.Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Jimerson, Shane R., Susan M. Swearer, and Dorothy L.Espelage.
2010.
Handbook of Bullying in Schools: Aninternational perspective.
Routledge/Taylor & FrancisGroup, New York, NY.Mishna, Faye and Ramona Alaggia.
2005.
Weighing therisks: A child?s decision to disclose peer victimization.Children & Schools, 27(4):217?226.Shelley, Danielle and Wendy M Craig.
2010.
Attri-butions and coping styles in reducing victimization.Canadian Journal of School Psychology, 25(1):84?100.Twitter.
2012.
The twitter rules.
http://support.twitter.com/articles/18311-the-twitter-rules.701Valkenburg, Patti M and Jochen Peter.
2009.
Socialconsequences of the internet for adolescents a decadeof research.
Current Directions in Psychological Sci-ence, 18(1):1?5.Walther, Joseph B.
1996.
Computer-mediated commu-nication impersonal, interpersonal, and hyperpersonalinteraction.
Communication research, 23(1):3?43.Wang, Yang, Gregory Norcie, Saranga Komanduri,Alessandro Acquisti, Pedro Giovanni Leon, and Lor-rie Faith Cranor.
2011.
?I regretted the minute Ipressed share?
: a qualitative study of regrets on face-book.
In Proceedings of the Seventh Symposium onUsable Privacy and Security, SOUPS ?11, pages 10:1?10:16.
ACM.Xu, Jun-Ming, Kwang-Sung Jun, Xiaojin Zhu, and AmyBellmore.
2012.
Learning from bullying traces in so-cial media.
In Proceedings of the 2012 Conferenceof the North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 656?666, Montre?al, Canada, June.
As-sociation for Computational Linguistics.702
