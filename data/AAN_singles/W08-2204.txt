High Precision Analysis of NPswith a Deep ProcessingGrammarAnt?nio BrancoFrancisco CostaUniversidade de Lisboa (Portugal)email: Antonio.Branco@di.fc.ul.ptAbstractIn this paper we present LXGram, a general purpose grammar for thedeep linguistic processing of Portuguese that aims at delivering detailedand high precision meaning representations.
LXGram is grounded on thelinguistic framework of Head-Driven Phrase Structure Grammar (HPSG).HPSG is a declarative formalism resorting to unification and a type sys-tem with multiple inheritance.
The semantic representations that LX-Gram associates with linguistic expressions use the Minimal RecursionSemantics (MRS) format, which allows for the underspecification of scopeeffects.
LXGram is developed in the Linguistic KnowledgeBuilder (LKB)system, a grammar development environment that provides debuggingtools and efficient algorithms for parsing and generation.
The implemen-tation of LXGram has focused on the structure of Noun Phrases, and LX-Gram accounts for many NP related phenomena.
Its coverage continuesto be increased with new phenomena, and there is active work on ex-tending the grammar?s lexicon.
We have already integrated, or plan tointegrate, LXGram in a few applications, namely paraphrasing, treebank-ing and language variant detection.
Grammar coverage has been testedon newspaper text.3132 Branco and Costa1 IntroductionIn this paper we present LXGram, a hand-built, general purpose computational gram-mar for the deep linguistic processing of Portuguese, specially geared to high precisionprocessing of Noun Phrases.
This grammar is based on the framework of Head-DrivenPhrase Structure Grammar (HPSG; Pollard and Sag (1994)), one of the most promi-nent linguistic theories being used in natural language processing.
Like several othercomputational HPSGs, LXGram uses Minimal Recursion Semantics (MRS; Copes-take et al (2005)) for the representation of meaning.LXGram is developed in the Linguistic Knowledge Builder (LKB) system (Copes-take, 2002), a development environment for constraint-based grammars.
This envi-ronment provides a GUI, debugging tools and very efficient algorithms for parsingand generation with the grammars developed there (Malouf et al, 2000; Carroll et al,1999).Several broad-coverage grammars have been developed in the LKB.
Currently, thelargest ones are for English (Copestake and Flickinger, 2000), German (M?ller andKasper, 2000) and Japanese (Siegel and Bender, 2002).
The grammars developedwith the LKB are also supported by the PET parser (Callmeier, 2000), which allowsfor faster parsing times due to the fact that the grammars are compiled into a binaryformat in a first step.
As the LKB grammars for other languages, LXGram is in activedevelopment, and it is intended to be a broad-coverage, open-domain grammar forPortuguese.
At the same time, it produces detailed representations of meaning intandem with syntactic structures, making it useful for a wide range of applications.In Section 2, we describe the framework foundations of the grammar.
The majordesign features of the grammar are introduced in Section 3.
We talk about the coverageof LXGram in Section 4.
Section 5 presents some of the phenomena treated withinthe NP domain and shows examples of implemented analyses relating to NP syntaxand semantics.
In Section 6, results on the performance of the grammar are reported,and in Section 7, we discuss applications where the grammar is or is being integrated.Finally, the paper closes with concluding remarks in Section 8.2 FoundationsLXGram adopts the HPSG framework, a popular linguistic theory with a large body ofliterature covering many natural language phenomena.
These insights can be directlyincorporated in the implementation of a computational grammar.2.1 HPSGHPSG resorts to a declarative formalism to model linguistic data.
It employs a typesystem (supporting multiple inheritance) and typed feature structures (recursive datastructures defining ?has-a?
relations) in order to describe the properties of linguisticobjects (words, phrases, rules).
Unification of types and feature structures is centralto HPSG, used to ensure that the various elements have compatible properties.
Forinstance, the fact that a transitive verb takes an NP as its complement is captured inHPSG by defining a lexical type for transitive verbs, say transitive-verb-lex(eme), withconstraints like the following (among others), presented in the Attribute-Value Matrix(AVM) format widely employed in HPSG:High Precision Analysis of NPs with a Deep Processing Grammar 33????????transitive-verb-lexSYNSEM|LOCAL|CAT|VAL|COMPS?????LOCAL|CAT???
?HEAD nounVAL[SPR ?
?COMPS ??]????????????????
?The NP complement of the verb is represented in this AVM as the value of the at-tribute COMPS.
This attribute takes a list as its value (indicated by the angle brackets).In this case the sole element of this list describes an object with a HEAD feature of thetype noun and empty complements (the attribute COMPS) and specifier (the featureSPR) (i.e.
they have been saturated at the point where the verb combines with thiselement), which is the HPSG description of an NP.2.2 MRSMinimal Recursion Semantics (MRS) is used as the format of semantic representa-tions that LXGram associates with expressions from Portuguese.
MRS has severalproperties that are interesting for applications.
A relevant one is the use of pointers torepresent scope effects that are handled via recursion in traditional formal semantics.This use of pointers (called handles) allows for the underspecification of scope rela-tions, which avoids listing all scope possibilities for ambiguous sentences (althoughthey can still be computed on demand with the LKBmachinery).
This is a useful prop-erty: scope does not need to be resolved in all applications (e.g.
machine translationdoes not require it), but at the same time scoped formulas can be obtained on demandif required (e.g.
for automated inference).We provide an example MRS representation derived for the sentence ?todas asequipas podem vencer?
(all teams can win) in Figure 1.
This MRS describes the???????????????????????
?mrsLTOP h1 hINDEX e2 eRELS???????
?_todo_q_relLBL h3 hARG0 x6 xRSTR h5 hBODY h4 h???????,??
?_equipa_n_relLBL h7 hARG0 x6???,????
?_poder_v_relLBL h8ARG0 e2ARG1 h9 h?????,????
?_vencer_v_relLBL h10ARG0 e11ARG1 x6??????HCONS???
?qeqHARG h1LARG h8???,??
?qeqHARG h5LARG h7???,??
?qeqHARG h9LARG h10???????????????????????????
?Figure 1: MRS for the sentence ?Todas as equipas podem vencer?
(all teams can win)two following scoped formulas, where the predicate _todo_q stands for a universalquantifier:?
_todo_q(x6,_equipa_n(x6),_poder_v(e2,_vencer_v(e11,x6)))?
_poder_v(e2,_todo_q(x6,_equipa_n(x6),_vencer_v(e11,x6)))34 Branco and CostaThe first reading is the one that says that each team has a chance to win, while thesecond reading says that it is possible for there to be a situation in which all teams win(false assuming common sense knowledge).
A single MRS representation is obtainedfor these two readings by instantiating the ARG1 feature of the relation _poder_v (can)with the handle h9, which is related to the handle h10 labeling the relation _vencer_v(win) via a qeq relation (equality modulo intervening quantifiers).
This is the way ofsaying that these two handles are the same (first reading) or that there is an interveninggeneralized quantifier relation (second reading).Semantic representations abstract from many grammatical and superficial detailsof language, like word order, syntactic structure and morphology.
As such, they arevery similar across different natural languages (modulo predicate names).
This is alsotrue of MRS.
Furthermore, semantic representations hide grammar implementation.As such, they are the preferred grammar?s interface for applications, that do not needany knowledge of the grammatical properties of Portuguese and may not need to lookat syntactic analysis.The MRS format is also used with several other computational HPSGs, for otherlanguages.
Several applications (e.g.
Machine Translation) have been used with otherHPSGs that communicate with these grammars via the MRSs (Bond et al, 2004).These applications can be easily integrated with grammars for different languagesthat also use MRS: they are almost completely language independent.3 Design FeaturesGiven the foundational options, LXGram adheres to a number of important designfeatures.Bidirectionality LXGram is bidirectional.
The formalism employed is completelydeclarative.
It can be used for parsing (yielding syntactic analyses and semantic rep-resentations from natural language input) and also for generation (yielding naturallanguage from meaning representations).
As such it can be useful for a wide range ofapplications.Precision LXGram aims at high precision of linguistic processing.
Modulo bugs,the grammar cannot parse ungrammatical input.
Although this feature may have anegative impact on robustness, it is an important aspect of the grammar when it isused for generation, as it means it is not possible to generate ungrammatical strings.1It is indeed possible to mark some rules and some lexical entries to only be used forparsing and not for generation.
In the configuration files for the LKB one can list theserules and lexical items.
We are currently using this feature in order to be able to parseinput that is not ungrammatical but is marked with respect to register, but preventingthe grammar from generating such strings.Importantly, the fact that it cannot parse ungrammatical input also means that thegrammar will not produce impossible analyses for grammatical sentences.Broad Coverage LXGram development is aimed at a broad coverage.
We alsoseek to make LXGram neutral with respect to regional variation as much as possi-ble.
Currently, the grammar accommodates both European Portuguese and Brazilian1We believe that dealing with ill-formed input is best done via other means (rather than let the grammarovergenerate so it can parse more), like partial parsing or the integration with/falling back to other tools.High Precision Analysis of NPs with a Deep Processing Grammar 35Portuguese.
Aspects of variation that are accounted for include lexical differences(merely affecting spelling or more substantial ones) as well as syntactic discrepancies(e.g.
definite articles before possessives, word order between clitic pronouns and theverb).Efficiency The processors on which LXGram runs (LKB, PET) are very efficient.In addition, there are grammar engineering techniques that improve efficiency (e.g.
(Flickinger, 2000)) that are also exploited in our implementation.Robustness The LKB and PET systems provide several ways to combine a grammarwith the output of shallow tools, like part-of-speech taggers.
Such integration canimprove grammar coverage, as the grammar needs information about all words in theinput, and some words may be missing in the grammar?s lexicon.
We have success-fully combined LXGram with a part-of-speech tagger and a morphological analyzer(more in Section 6).
The grammar code includes mappings from the input format(XML) to the feature structures that are manipulated by the grammar.Availability A version of LXGram is publicly available at http://nlxgroup.di.fc.ul.pt/lxgram.
LXGram can be used by applications without any knowledge ofthe grammar?s implementation or internal workings.
The LKB allows for applicationsto communicate with the grammar via sockets, accepting parser input in XML or rawtext and returning semantic representations in XML, for which a DTD is available.
Itis also possible to automatically produce a list of all the predicates known by the gram-mar together with their arity and argument types (from the lexicon and syntax rules),that can be manually annotated with comments and examples.
The predicates corre-sponding to lexical items are however quite transparent once the naming conventionsthat are used are explained.4 Coverage4.1 Lexical CoverageWhen one is using a lexicalist framework like HPSG, lexical coverage is a key issuebecause all tokens in the input should be known by the grammar in order for thegrammar to produce a parse.
Furthermore, the amount of information included inthe lexicon that is used by an HPSG is very large.
Part-of-speech and morphologicalinformation is not sufficient.
For the correct assignment of semantic representations,subcategorization frames as well as other information pertaining to semantics mustbe correctly associated with every lexical item, something that cannot be known withsufficient quality by just using shallower tools, like part-of-speech taggers.In LXGram a hand-crafted lexicon containing several hundreds of nouns, adjectivesand verbs was developed.
However, the manual creation of lexica with this amountof information is time consuming and error prone.
We are exploring methods to al-leviate this problem.
An option is to combine the grammar with shallower tools inorder to have access to some of the information needed and assume default valuesfor the information that cannot be obtained this way.
We have already integratedthe grammar with a set of shallow tools (a part-of-speech tagger, a lemmatizer and amorphological analyzer) in order to guess information about unknown words.
Prelim-inary results indicate an increase in coverage on unrestricted newspaper text from 2%to 13%.
Although this approach cannot guarantee correct semantic representations36 Branco and Costa(or even syntactic trees, since subcategorization frames constrain syntactic structure),it can be useful in applications that only require some restricted amount of linguisticinformation.4.2 Overall Grammatical CoverageIn order to get a quantitative overview of the grammar, it can be characterized asfollows:?
24,484 lines of code, including comments and excluding the lexicon;?
53 syntax rules;?
40 lexical rules, mostly inflectional;?
3,154 total types;?
414 types for lexical items;?
2,718 hand-built lexical entries.For a qualitative overview, these are the linguistic phenomena covered so far:?
Declarative sentences?
Yes-no questions e.g.
: ?Portanto o Estado tem um gosto??
(So does the Statehave preferences?)?
Imperative sentences e.g.
: ?D?-me um desses bolos.?
(Give me one of thosecakes)?
Some subcategorization frames of verbs, nouns and adjectives e.g.
: ?a Pol?-nia empatou com a Fran?a?
(Poland tied with France); ?eu j?
disse que pode serum dos mais baratos?
(I told you already that it can be one of the cheapest);?filho de um professor dos arredores de Viena?
(son of a teacher from the out-skirts of Vienna)?
Comparative constructions e.g.
: ?a vida ?
maior do que o cinema?
(life islarger than cinema)?
Noun phrase structure, including determiners, possessives, cardinal andordinal numerals, prepositional phrases, adjectives, etc.
(examples in thenext section)?
Modification of verbal projections by prepositional and adverbial phrasese.g.
: ?No CAPC termina hoje a exposi??o?
(the exhibit ends today at CAPC);?
Relative clauses e.g.
: ?sete outros suspeitos que a pol?cia ainda procura?
(sevenother suspects that the police are still looking for)?
Null subjects and objects e.g.
: ?Sa?mos depois do jantar.?
((We) left afterdinner); ?Podemos comer l?
perto.?
(We can eat near there)High Precision Analysis of NPs with a Deep Processing Grammar 37?
Floated Quantifiers e.g.
: ?os ?ndices subiram todos?
(the indices have all goneup)The development of the grammar is going on and this grammar is getting its cover-age increased with important phenomena that are missing.
In particular, for the nearfuture, we are working towards including more subcategorization frames for verbs,nouns and adjectives, and implementing wh-questions, coordination and adverbialsubordination.5 Noun PhrasesA special design feature of LXGram is that it includes a comprehensive implementa-tion of Portuguese Noun Phrase structure, covering:?
Bare noun phrases (i.e.
NPs lacking a determiner) e.g.
: ?boa gest?o?
(goodmanagement); ?imagens da bancada?
(images of the seats)?
Determiners and predeterminers e.g.
: ?esta sua ?ltima produ??o?
(this lastproduction of his); ?todos estes problemas?
(all these problems); ?todos os par-tidos pol?ticos?
(all the political parties), ?aquele tempo todo?
(all that time)?
Word order constraints among NP elements e.g.
: ?as duas primeiras insti-tui??es?
(the two first institutions); ?os primeiros sete meses deste ano?
(thefirst seven months of this year); ?sete outros suspeitos que a pol?cia aindaprocura?
(seven other suspects that the police are still looking for); ?os outrostr?s membros do conselho?
(the other three members of the council); ?os seusdois primeiros anos pol?micos na Casa Branca?
(his first two polemic yearsin the White House); ?o primeiro grande conflito que aportava em Bel?m?
(thefirst great conflict that reached Berlin); ?outro lugar qualquer?
(any other place);?um lugar qualquer?
(any place); ?qualquer outra solu??o?
(any other solution)?
Prenominal and postnominal possessives e.g.
: ?o seu terceiro maior parceirocomercial?
(its third major commercial partner); ?um adjunto seu que atendeuali o telefonema?
(an assessor of his who answered the phone call there)?
Modification of adjectives e.g.
: ?os escritores mais importantes?
(the mostimportant writers); ?o discurso razoavelmente optimista?
(the reasonably opti-mistic speech)?
Missing nouns e.g.
: ?dois que s?o g?meos?
(two who are twins)?
Word order between adjectives and complements of nouns e.g.
: ?o conhec-imento essencial das pessoas?
(the essential knowledge about people)?
Adjectives with the semantics of arguments of nouns e.g.
: ?o veto americano?
renova?
?o do mandato?
(the American veto to the renewal of the position)Precision was given a lot of attention.
For instance, many items are constrained notto appear more than once in a given NP (determiners, possessives, cardinals, ordinals,etc.).
Scope phenomena are also handled (motivated by semantics), as well as orderconstraints.
Agreement is enforced.38 Branco and CostaWe present some examples of phenomena for which LXGram provides interestingsemantic representations and that we have not found in the literature pertaining toimplemented grammars.5.1 Floated QuantifiersThe first example relates to floated quantifiers.
For all the sentences in (1), which areall grammatical in Portuguese, LXGram provides the MRS equivalent ofall(x, price(x),will(go_up(x))):(1) a. Todosallosthepre?ospricesv?owillsubir.go upb.
Osthepre?ospricestodosallv?owillsubir.go upc.
Osthepre?ospricesv?owilltodosallsubir.go upd.
Osthepre?ospricesv?owillsubirgo uptodos.allIn all of these cases we associate empty semantics to the definite article (?os?
).Semantic information is percolated around the syntactic trees so that the universalquantifier, which can be realized at several different places, ends up being linked tothe semantics of the NP subject in the semantic representations for all these sentences.We also make sure that definite articles always carry quantifier semantics when nofloated quantifier is present.The implementation revolves around allowing floated quantifiers to attach to verbs,resulting in verb-headed nodes that combine with NP subjects lacking quantifier se-mantics.
Raising verbs, like the form ?v?o?
in this example, constrain their subjectaccording to the constraints of the subject of their VP complement.
For instance, thelast example (1d) receives a syntactic analysis like the one described by the followingtree:[subj-head-phraseSUBJ ??
]HHHHHH1 HHOsthepre?osprices??head-comp-phraseSUBJ?1???HHHHH[SUBJ?1?]v?owill?
?floated-quant-phraseSUBJ?1 NP?e,t????HHH[SUBJ?NP?
]subirgo uptodosallHigh Precision Analysis of NPs with a Deep Processing Grammar 39Here, NP abbreviates a feature structure that describes a noun phrase (of the se-mantic type ??e,t?,t?
), and NP?e,t?
abbreviates the constraints that describe an NPintroduced by a determiner lacking quantifier semantics (i.e.
an NP with an MRSrepresentation that is similar to that of constituents with the the semantic type ?e,t?
).In HPSG, the SUBJ feature encodes the constraints on the subject that a constituentselects.
We use a dedicated syntax rule to combine ?subir?
and ?todos?
(floated-quant-phrase), that creates a node requiring an NP with the semantic type ?e,t?
as its subject.The verb form ?v?o?
is treated as a raising verb: in HPSG the syntactic requirementson the subject of a raising verb are the same as the requirements on the subject of theVP complement that that verb selects for.
This is denoted by the boxed integers in thistree (which represent unification).In this example, the VP complement of ?v?o?
is the phrase ?subir todos?, as thesetwo constituents are combined via the head-comp-phrase rule (selection of comple-ments is represented in a way similar to the selection of subjects, but via the featureCOMPS instead of the feature SUBJ).
The subject of the head-comp-phrase is the sub-ject of its head daughter (?v?o?).
The topmost node is the result of applying a syntacticrule to project subjects to the left of their head (subj-head-phrase).The example in (1c) is processed in a similar fashion:[subj-head-phraseSUBJ ??
]HHHHHHH2 HHOsthepre?osprices??head-comp-phraseSUBJ?2???HHHHH?
?floated-quant-phraseSUBJ?2 NP?e,t????HHH[SUBJ?1?
]v?owilltodosall[SUBJ?1 NP?
]subirgo upIn this example, the complement of ?v?o?
is the node spanning ?subir?, whichselects for a quantified subject.
The subject of the raising verb is accordingly alsoa quantified NP.
Here, the rule to project a floated quantifier applies lower than theconstruction that projects complements, creating a node that requires a non-quantifiedsubject.
The SUBJ feature of head-complement constructions comes from the headdaughter (the floated-quant-phrase node in this example).
Therefore, the node pro-duced by the head-comp-phrase rule also requires a non-quantified subject.Note that the composition of semantics with MRS in based on the concatenation ofthe RELS and HCONS lists associated to the various constituents and passing around40 Branco and Costathe values of the features LTOP and INDEX (see Figure 1).
It is not based on functionapplication.
The composition of semantics with MRS is quite flexible.5.2 Scope of Adjectives and Relative ClausesThe second example that we show here relates to the semantic scope between differentelements of noun phrases.
In particular, we can see a distinction in the interpretationof the two following examples:(2) a. umaposs?velpossiblem?dicodoctorchin?sChinesea possible Chinese doctorb.
umaposs?velpossiblem?dicodoctorquewho?ischin?sChinesea possible doctor who is ChineseIn the first NP an entity is described as possibly being a Chinese doctor.
The secondNP describes an entity as possibly being a doctor and certainly being Chinese.
Ac-cordingly, LXGram delivers slightly different semantic representations for these twoNPs.
The first case produces something similar to?P.
a(x, possible(doctor(x)?
chinese(x)),P(x)).The second NP is treated along the lines of?P.
a(x, possible(doctor(x))?
chinese(x),P(x)).These two different readings are derived simply by constraining the relative syntacticscope of adjectives and relative clauses.
Namely, LXGram forces prenominal adjec-tives to attach higher than postnominal adjectives (2a) but lower than relative clauses(2b).
In this case, the scope differences in the semantic representations are simplyderived from the differences in syntactic scope:HHHumHHHposs?vel HHm?dico chin?sHHHHHumHHHH HHposs?vel m?dicoque ?
chin?sOf course, the following examples receive equivalent semantics:(3) a. umam?dicodoctorchin?schinesea Chinese doctorb.
umam?dicodoctorquewho?ischin?sChinesea doctor who is ChineseHigh Precision Analysis of NPs with a Deep Processing Grammar 416 EvaluationSome evaluation experiments were conducted to test LXGram?s coverage.
In one ofthem, a corpus with newspaper text (self-reference) was used, with 145 sentences.For this experiment, we used a part-of-speech tagger and a morphological analyzer(self-reference) in order to guess some information about out-of-vocabulary words.
Adefault value was assumed for the missing subcategorization information (all unknownverbs were treated as transitive verbs).
The average sentence length was 22 words.
Inthis experiment, 13.1% of all sentences received at least one parse by the grammar.2On the same test corpus, the average time it took for a sentence to parse was 1.1seconds on a P4 machine at 3GHz.
The average amount of memory required to analyzea sentence was 145.5MB.In another experiment, with 180,000 short sentences (5 to 9 words) selected ran-domly from two newspaper corpora (CETEMP?blico and CETENFolha), LXGramhad achieved 26% coverage, using a similar approach to handle unknown words (self-reference).During the development of LXGram we maintain several test suites, consisting ofexample sentences for the implemented phenomena.
The test suites use a controlledvocabulary.
Also, several examples attest several phenomena, in order to test theinteraction of the different modules.
They are very useful to test the syntax rules ofthe grammar and the semantics that LXGram produces, and for regression testing.
Thetest suite for NPs contains 851 sentences (429 of which are negative examples, thatthe grammar should not parse).
The average sentence length is 5.3 words (2?16).
Onthis test suite LXGram has 100% coverage and 0% overgeneration.
The average timeneeded to analyze a sentence is 0.11 seconds, with an average memory requirementof 15.5MB.
Plotting parse time by sentence length, we see an approximately linearincrease in parse time with this test suite.7 Applications and Further WorkWe have used LXGram to automatically discriminate between texts written in Euro-pean Portuguese and Brazilian Portuguese, with encouraging results, which match theresults obtained with other dialect detection methodologies.
(self-reference).3Additionally, we are working towards integrating it with an existing question an-swering system (self-reference).4 This is in part the reason for the special focus onNPs, as these constituents are often short answers to factoid questions.Because the grammar is entirely bidirectional, a paraphraser is gained for free fromthe implementation of LXGram: the grammar can simply be used to generate from thesemantic representations that it derives from an input sentence, thus producing para-phrases of the textual input.
We are also working to integrate the grammar, runningunder this functionality, into the QA system.2Note that one of the HPSGs with the broadest coverage at the moment, the ERG, covers 17% of theBritish National Corpus.
The main cause of parse failure is out-of-vocabulary words.3In particular, the results obtained with LXGram were quite similar to the results obtained with thestandard methods (based on character n-grams) that are used to identify the language in which a given textis written, when used for this purpose.4See (Bobrow et al, 2007) for a similar approach, where an LFG is employed in a question answeringsystem aiming at high precision.42 Branco and CostaOn a par with the above lines of research, we are intensively using the grammar tosemi-automatically produce a treebank that contains syntactic representations and se-mantic descriptions of the sentences in a newspaper corpus.
LXGram has also servedin the past to implement and experiment with novel linguistic analyses of interestingphenomena (self-reference).
By making it freely available, we intend to encouragethis sort of experimentation also by other researchers.
One can reap important bene-fits from computationally implementing linguistic analyses: the debugging tools allowfor fast checking of correctness; the impact on other analyses that are already imple-mented can be immediately assessed via regression testing, making it possible to testthe interaction between linguistic analyses for different phenomena; it is possible toautomatically compare different competing analyses for efficiency, based on test suitesor corpora.8 ConclusionsIn this paper we presented LXGram, a computational grammar for the deep linguisticprocessing of Portuguese.
LXGram is implemented in a declarative formalism.
Itcan be used for analysis as well as generation.
It produces high precision syntacticanalyses and semantic representations.
LXGram supports the two main varieties ofPortuguese: European and Brazilian Portuguese.
It is not dependent on a particulardomain or genre.So far the focus of the implementation was on noun phrases and basic sentencestructure, a coverage that is being extended in ongoingwork.
The outcome of differentevaluation experiments shows scores that are in line with those obtained with similargrammars for other languages.ReferencesBobrow, D. G., B. Cheslow, C. Condoravdi, L. Karttunen, T. H. King, R. Nairn,V.
de Paiva, C. Price, and A. Zaenen (2007).
PARC?s bridge and question answer-ing system.
In T. H. King and E. M. Bender (Eds.
), Proceedings of the GEAF07Workshop, Stanford, CA, pp.
46?66.
CSLI Publications.Bond, F., S. Fujita, C. Hashimoto, K. Kasahara, S. Nariyama, E. Nichols, A. Ohtani,T.
Tanaka, and S. Amano (2004).
The Hinoki treebank: Working toward text un-derstanding.
In S. Hansen-Schirra, S. Oepen, and H. Uszkoreit (Eds.
), COLING2004 5th International Workshop on Linguistically Interpreted Corpora, Geneva,Switzerland, pp.
7?10.
COLING.Callmeier, U.
(2000).
PET ?
A platform for experimentation with efficient HPSGprocessing techniques.
Natural Language Engineering 6(1), 99?108.
(SpecialIssue on Efficient Processing with HPSG).Carroll, J., A. Copestake, D. Flickinger, and V. Poznan?ski (1999).
An efficient chartgenerator for (semi-)lexicalist grammars.
In Proceedings of the 7th EuropeanWorkshop on Natural Language Generation (EWNLG?99), Toulouse, pp.
86?95.Copestake, A.
(2002).
Implementing Typed Feature Structure Grammars.
Stanford:CSLI Publications.High Precision Analysis of NPs with a Deep Processing Grammar 43Copestake, A. and D. Flickinger (2000).
An open-source grammar development envi-ronment and broad-coverage English grammar using HPSG.
In Proceedings of theSecond conference on Language Resources and Evaluation (LREC-2000), Athens,Greece.Copestake, A., D. Flickinger, I.
A.
Sag, and C. Pollard (2005).
Minimal RecursionSemantics: An introduction.
Journal of Research on Language and Computa-tion 3(2?3), 281?332.Flickinger, D. (2000).
On building a more efficient grammar by exploiting types.Natural Language Engineering 6(1), 15?28.
(Special Issue on Efficient Processingwith HPSG).Malouf, R., J. Carrol, and A. Copestake (2000).
Efficient feature structure operationswithout compilation.
Natural Language Engineering 6(1), 29?46.
(Special Issueon Efficient Processing with HPSG).M?ller, S. and W. Kasper (2000).
HPSG analysis of German.
In W. Wahlster (Ed.
),Verbmobil: Foundations of Speech-to-Speech Translation (Artificial Intelligenceed.
)., pp.
238?253.
Berlin Heidelberg New York: Springer-Verlag.Pollard, C. and I.
Sag (1994).
Head-Driven Phrase Structure Grammar.
Stanford:Chicago University Press and CSLI Publications.Siegel, M. and E. M. Bender (2002).
Efficient deep processing of Japanese.
In Pro-ceedings of the 3rd Workshop on Asian Language Resources and InternationalStandardization.
Coling 2002 Post-Conference Workshop, Taipei, Taiwan, pp.
31?38.
