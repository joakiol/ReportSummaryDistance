Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 20?29,Portland, Oregon, June 2011. c?2011 Association for Computational LinguisticsUnsupervised syntactic chunking with acoustic cues:computational models for prosodic bootstrappingJohn K Pate (j.k.pate@sms.ed.ac.uk)Sharon Goldwater (sgwater@inf.ed.ac.uk)School of Informatics, University of Edinburgh10 Crichton St., Edinburgh EH8 9AB, UKAbstractLearning to group words into phrases with-out supervision is a hard task for NLP sys-tems, but infants routinely accomplish it.
Wehypothesize that infants use acoustic cues toprosody, which NLP systems typically ignore.To evaluate the utility of prosodic informationfor phrase discovery, we present an HMM-based unsupervised chunker that learns fromonly transcribed words and raw acoustic cor-relates to prosody.
Unlike previous work onunsupervised parsing and chunking, we useneither gold standard part-of-speech tags norpunctuation in the input.
Evaluated on theSwitchboard corpus, our model outperformsseveral baselines that exploit either lexical orprosodic information alone, and, despite pro-ducing a flat structure, performs competitivelywith a state-of-the-art unsupervised lexical-ized parser, with a substantial advantage inprecision.
Our results support the hypothesisthat acoustic-prosodic cues provide useful ev-idence about syntactic phrases for language-learning infants.1 IntroductionYoung children routinely learn to group words intophrases, yet computational methods have so farstruggled to accomplish this task without supervi-sion.
Previous work on unsupervised grammar in-duction has made progress by exploiting informationsuch as gold-standard part of speech tags (e.g.
Kleinand Manning (2004)) or punctuation (e.g.
Seginer(2007)).
While this information may be availablein some NLP contexts, our focus here is on the com-putational problem facing language-learning infants,who do not have access to either part of speechtags or punctuation.
However, infants do have ac-cess to certain cues that have not been well exploredby NLP researchers focused on grammar inductionfrom text.
In particular, we consider the cues to syn-tactic structure that might be available from prosody(roughly, the structure of speech conveyed throughrhythm and intonation) and its acoustic realization.The idea that prosody provides important ini-tial cues for grammar acquisition is known as theprosodic bootstrapping hypothesis, and is well-established in the field of language acquisition(Gleitman and Wanner, 1982).
Experimental workhas provided strong support for this hypothesis, forexample by showing that infants begin learning ba-sic rhythmic properties of their language prenatally(Mehler et al, 1988) and that 9-month-olds useprosodic cues to distinguish verb phrases from non-constituents (Soderstrom et al, 2003).
However, asfar as we know, there has so far been no direct com-putational evaluation of the prosodic bootstrappinghypothesis.
In this paper, we provide the first suchevaluation by exploring the utility of acoustic cuesfor unsupervised syntactic chunking, i.e., groupingwords into non-hierarchical syntactic phrases.Nearly all previous work on unsupervised gram-mar induction has focused on learning hierarchicalphrase structure (Lari and Young, 1990; Liang et al,2007) or dependency structure (Klein and Manning,2004); we are aware of only one previous paperon unsupervised syntactic chunking (Ponvert et al,2010).
Ponvert et al describe a simple method forchunking that uses only bigram counts and punctu-ation; when the chunks are combined using a right-branching structure, the resulting trees achieve un-labeled bracketing precision and recall that is com-petitive with other unsupervised parsers.
The sys-20tem?s dependence on punctuation renders it inappro-priate for addressing the questions we are interestedin here, but its good performance reccommends syn-tactic chunking as a profitable approach to the prob-lem of grammar induction, especially since chunkscan be learned using much simpler models than areneeded for hierarchical structure.The models used in this paper are all variants ofHMMs.
Our baseline models are standard HMMsthat learn from either lexical or prosodic observa-tions only; we also consider three types of models(including a coupled HMM) that incorporate bothlexical and prosodic observations, but vary the de-gree to which syntactic and prosodic variables aretied together in the latent structure of the models.In addition, we compare the use of hand-annotatedprosodic information (ToBI annotations) to the useof direct acoustic measures (specifically, durationmeasures) as the prosodic observations.
All of ourmodels are unsupervised, receiving no bracketinginformation during training.The results of our experiments strongly supportthe prosodic bootstrapping hypothesis: we findthat using either ToBI annotations or acoustic mea-sures in addition to lexical observations (i.e., wordsequences) vastly improves chunking performanceover any source of information alone.
Interestingly,our best results are achieved using a combinationof words and acoustic information as input, ratherthan words and ToBI annotations.
Our best com-bined model achieves an F-score of 41% when eval-uated on the lowest level of syntactic structure inthe Switchboard corpus1, as compared to 25% fora words-only model and only 3% for an acoustics-only model.
Although the combined model?s scoreis still fairly low, additional results suggest that ourcorpus of transcribed naturalistic speech is signifi-cantly more difficult for unsupervised parsing thanthe written text that is typically used for training.Specifically, we find that a state-of the-art unsuper-vised lexicalized parser, the Common Cover Link1Since our interest is in child language acquisition, wewould prefer to evaluate our system on data from the CHILDESdatabase of child-directed speech (MacWhinney, 2000).
Unfor-tunately, there are no corpora in the database that include phrasestructure annotations.
We are in the process of annotating asmall evaluation corpus with phrase structure trees, and hope touse this for evaluation in future work.
(CCL) parser (Seginer, 2007), achieves only 38%unlabeled bracketing F-score on our corpus, as com-pared to published results of 76% on WSJ10 (En-glish) and 59% on Negra10 (German).
Interestingly,we find that when evaluated against full parse trees,our best chunker achieves an F-score comparable tothat of CCL despite positing only flat structure.Before describing our models and experiments inmore detail, we first present a brief review of rel-evant information about prosody and its relation-ship to syntax, including previous work combiningprosody and syntax in supervised parsing systems.2 Prosody and syntaxProsody is a theoretical linguistic concept posit-ing an abstract organizational structure for speech.2While it is often closely associated with such mea-surable phenomena as movement in fundamen-tal frequency or variation in spectral tilt, theseare merely observable acoustic correlates that pro-vide evidence of varying quality about the hiddenprosodic structure, which specifies such hidden vari-ables as contrastive stress or question intonation.Prosody has been hypothesized to be useful forlearning syntax because it imposes a grouping struc-ture on word sequences that sometimes coincideswith traditional constituency analyses (Ladd, 1996;Shattuck-Hufnagel and Turk, 1996).
Moreover,laboratory experiments have shown that adults useprosody both for syntactic disambiguation (Millotteet al, 2007; Price et al, 1991) and, crucially, inlearning the syntax of an artificial language (Morganet al, 1987).
Accordingly, if prosodic structure issufficiently prominent in the acoustic signal, and co-incides often enough with syntactic structure, then itmay provide children with useful information abouthow to combine words into phrases.Although there are several theories of how to rep-resent and annotate prosodic structure, one of themost influential is the ToBI (Tones and Break In-dices) theory (Beckman et al, 2005), which wewill use in some of our experiments.
ToBI pro-poses, among other things, that the prosodic phras-ing of languages can be represented in terms of se-quences of break indices indicating the strength of2Signed languages also exhibit prosodic phenomena, butthey are not addressed here.21word boundaries.
In Mainstream American EnglishToBI, for example, the boundary between a cliticand its base word (e.g.
?do?
and ?n?t?
of ?don?t?
)is 0, representing a very weak boundary, while theboundary following a word at the end of an intona-tional phrase is 4, indicating a very strong boundary.Below we examine how useful these break indicesare for identifying syntactic boundaries.Finally, we note that our work is not the first com-putational approach to using prosody for identifyingsyntactic structure.
However, previous work (Gre-gory et al, 2004; Kahn et al, 2005; Dreyer andShafran, 2007; No?th et al, 2000) has focused onsupervised parsing rather than unsupervised chunk-ing, and also makes different assumptions aboutprosody.
For example, Gregory et al (2004) assumethat prosody is an acoustically-realized substitute forpunctuation; our own treatment is much less con-strained.
Kahn et al (2005) and Dreyer and Shafran(2007) use ToBI labels to represent prosodic infor-mation, whereas we explore both ToBI and directacoustic measures.
Finally, No?th et al (2000) do notuse ToBI, instead developing a novel prosodic anno-tation system designed specifically to provide cuesto syntax and for annotation efficiency.
However,their system is supervised and focuses on improvingparse speed rather than accuracy.3 ModelsFollowing previous work (e.g.
Molina and Pla(2002) Sha and Pereira (2003)), we formulatechunking as a tagging task.
We use Hidden MarkovModels (HMMs) and their variants to perform thetagging, with carefully specified tags and con-strained transition distributions to allow us to inter-pret the results as a bracketing of the input.
Specif-ically, we use four chunk tags: B (?Begin?)
andE (?End?)
tags are interpreted as the first and lastwords of a chunk, respectively, with I (?Inside?
)corresponding to other words inside a chunk and O(?Outside?)
to all other words.
The transition ma-trices are constrained to afford 0 probability to tran-sitions that violate these definitions.
Additionally,the initial probabilities are constrained to forbid themodels from starting inside or at the end of a phrase.We use this four-tag OBIE tagset rather than themore typical three-tag IOB tagset for two reasons.First, the OBIE set forces all chunks to be at leasttwo words long (the shortest chunk allowed is B E).Imposing this requirement allows us to characterizethe task in concrete terms as ?learning when to groupwords together.?
Second, as we seek to incorporateacoustic correlates of prosody into chunking, we ex-pect edge behavior to merit explicit modeling.3In the following subsections, we describe the var-ious models we use.
Note that input to all mod-els is discrete, consisting of words, ToBI annota-tions, and/or discretized acoustic measures (we de-scribe these measures and their discretization in Sec-tion 3.3).
See Figure 1 for examples of system inputand output; different models will receive differentcombinations of the three kinds of input.3.1 Baseline ModelsOur baseline models are all standard HMMs, withthe graphical structure shown in Figure 2(a).
Thefirst baseline uses lexical information only; the ob-servation at each time step is the phonetic transcrip-tion of the current word in the sentence.
To han-dle unseen words at test time, we use an ?UNK.
?token to replace all words in the training and eval-uation sets that appear less than twice in the train-ing data.
Our second baseline uses prosodic infor-mation only; the observation at each time step isthe hand-annotated ToBI Break Index for the cur-rent word, which takes on one of six values: { 0, 1,2, 3, 4, X, None }.4 Our final baseline uses acous-tic information only.
The observations are one ofsix automatically determined clusters in an acousticspace, as described in Section 3.3.We trained the HMMs using Baum-Welch, andused Viterbi for inference.53Indeed, when we tried using the IOB tag set in prelimi-nary experiments, dev-set performance dropped substantially,supporting this latter intuition.4The numerical break indices indicate breaks of increas-ing strength, ?X?
represents a break of uncertain strength, and?None?
indicates that the preceding word is outside one of thefluent prosodic phrases selected for annotation.
Additional dis-tinctions marked by ?-?
and ?p?
were ignored.5We actually used the junction tree algorithm from MAL-LET, which, in the special case of an HMM, reduces to theForward-Backward algorithm when using Sum-Product mes-sages, and to the Viterbi algorithm when using Max-Productmessages.
Our extension of MALLET to build junction treesefficiently for Dynamic Bayes Nets is available online, and isbeing prepared for submission to the main MALLET project.22(a) Words g.aa dh.ae.t.s dh.ae.t s.aw.n.d.z p.r.ih.t.iy b.ae.d t.ax m.iyAcoustics 4 4 6 4 5 4 5 6ToBI 1 2 1 1 1 1 1 3(b) O O B I I E B E(c) ( ) ( )(d) ( ( ) ( ) )Figure 1: (a) Example input sequences for the three types of input (phonetic word transcriptions, acoustic clusters, andToBI break indices).
(b) Example output tags.
(c) The bracketing corresponding to (b).
(d) The flat tree built from (b).w1C1w2C2w3C3  // //(a) Standard HMM (HMM)w1C1d1w2C2d2w3C3d3OO OO// //OO(b) Two Output HMM (THMM)w1C1P1d1w2C2P2d2w3C3P3d3OOOOOO%%99////%%99////(c) Coupled HMM (CHMM)Figure 2: Graphical structures for our various HMMs.
ci nodes are constrained using the OBIE system, pi nodesare not.
wi nodes represent lexical outputs, and di nodes represent acoustic or ToBI outputs.
(Rectangular nodes areobserved, circular nodes are hidden).3.2 Combined ModelsAs discussed in Section 2, previous theoreticaland experimental work suggests a combined modelwhich models uncertainty both between prosody andacoustics, and between prosody and syntax.
To mea-sure the importance of modeling these kinds of un-certainty, we will evaluate a series of model struc-tures that gradually divorce acoustic-prosodic cuesfrom lexical-syntactic cues.Our first model is the standard HMM from Fig-ure 2(a), but generates a (word, acoustics) or (word,ToBI) pair at each time step.
This model has the sim-plest structure, but includes a separate parameter forevery unique (state, word, acoustics) triple, so maybe too unconstrained to learn anything useful.To reduce the number of parameters, we pro-pose a second model that assumes independence be-tween the acoustic and lexical observations, giventhe syntactic state.
We call this a ?Two-output HMM(THMM)?
and present its graphical structure in Fig-ure 2(b).
It is straightforward to extend Baum-Welchto accommodate the extra outputs of the THMM.Finally, we consider a model that explicitly rep-resents prosodic structure distinctly from syntacticstructure with a second sequence of tags.
We usea Coupled HMM (CHMM) (Nefian et al, 2002),which models a set of observation sequences us-ing a set of hidden variable sequences.
Figure 2(c)presents a two-stream Coupled HMM for three timesteps.
The model consists of an initial state proba-bility distribution pis for each stream s, a transitionmatrix as for each stream s conditioning the distri-bution of stream s at time t + 1 on the state of bothstreams at time t, and an emission matrix bs for eachstream conditioning the observation of stream s attime t on the hidden state of stream s at time t.6Intuitively, the states emitting acoustic measuresoperationalize prosodic structure, and the statesemitting words operationalize syntactic structure.Crucially, Coupled HMMs impose no a priori cor-respondence between variables of different streams,allowing our ?syntactic?
states to vary freely fromour ?prosodic?
states.
As two-stream CHMMsmaintain two emission matrices, two transition ma-6We explored a number of minor variations on this graphicalstructure, but preliminary experiments yielded no improvement.23trices, and two initial state distributions, they aremore complex than the other combined models, butmore closely embody intuitions inspired by previouswork on the prosody-syntax interface.Our Coupled HMMs were also trained using EM.Marginals for the E-step were computed using theimplementation of the junction tree algorithm avail-able in MALLET (McCallum, 2002; Sutton, 2006).During test, the Viterbi tag sequence for each modelis obtained by simply replacing the sum-productmessages with max-product messages.3.3 Acoustic CuesAs explained in Section 2, prosody is an abstract hid-den structure which only correlates with observablefeatures of the acoustic signal, and we seek to selectfeatures which are both easy to measure and likely tocorrelate strongly with the hidden prosodic phrasalstructure.
While there are many possible cues, wehave chosen to use duration cues.
These should pro-vide good evidence about phrases due to the phe-nomenon of pre-boundary lengthening (e.g.
Beck-man and Edwards (1990), Wightman et al (1992)),wherein words, and their final rime, lengthen phrase-finally.
This is likely especially useful for Englishdue to the lack of confounding segmental durationcontrasts (although variation in duration is unpre-dictably distributed (Klatt, 1976)), but should beuseful in varying degrees for other languages.We gather five duration measures:1.
Log total word duration: The annotated wordend time minus the annotated word start time.2.
Log onset duration: The duration from the be-ginning of the word to the end of the first vowel.3.
Log offset duration: The duration from the be-ginning of the last vowel to the end of the word.4.
Onset proportion consonant: The duration ofthe non-vocalic portion of the word onset di-vided by the total onset duration.5.
Offset proportion consonant: The duration ofthe non-vocalic portion of the word offset di-vided by the total offset duration.If a word contains no canonical vowels, then thefirst and last sonorants are counted as vocalic.
If aTrain Dev TestWords 68,533 7,981 8,746Sentences 6,420 778 802Table 1: Data set statisticsword contains no vowels or sonorants, then the on-set and offset are the entire word and the propor-tion consonant for both onset and offset is 1 (thisoccurred for 186 words in our corpus).The potential utility of this acoustic space wasverified by visual inspection of the first few PCAcomponents, which suggested that the position of aword in this acoustic space correlated with bracketcount.
We discretize the raw (i.e.
non-PCA) spacewith k-means with six initially random centers forconsistency with the number of ToBI break indices.4 Experiments4.1 DatasetAll experiments were performed on part of the NiteXML Toolkit edition of the Switchboard corpus(Calhoun et al, 2010).
Specifically, we gathered allconversations which have been annotated for syn-tax, ToBI, and Mississippi State phonetic alignments(which lack punctuation).7 The syntactic parses,word sequences, and ToBI break indices were hand-annotated by trained linguists, while the MississippiState phonetic alignments were automatically pro-duced by a forced alignment of the speech signalto a pronunciation-dictionary based phone sequence,providing an estimate of the beginning and end timeof each phone.
A small number of annotation er-rors (in which the beginning and end times of somephones had been swapped) were corrected by hand.This corpus has 74 conversations with two sideseach.We split this corpus into an 80%/10%/10%train/dev/test 8 partition by dividing the entire cor-pus into ten-sentence chunks, assigning the firsteight to the training partition, and the ninth and tenthto the dev and test partitions, respectively.
We thenremoved all sentences containing only one or two7We threw out a small number of sentences with annotationserrors, e.g.
pointing to missing words.8The dev set was used to explore different model structuresin preliminary experiments; all reported results are on the testset.24words.
Sentences this short have a trivial parse, andare usually formulaic discourse responses (Bell etal., 2009), which may influence their prosody.
Thefinal corpus statistics are presented in Table 1.4.2 EvaluationWe use the Penn Treebank parsed version of Switch-board for evaluation.
This version uses a slightlydifferent tokenization from the Mississippi Statetranscriptions that were used as input to the mod-els, so we transformed the Penn treebank tokeniza-tion to agree with the Mississippi State tokeniza-tion (primarily by concatenating clitics to their basewords?i.e.
?do?
and ??nt?
into ?don?t?
?and split-ting multi-word expressions).
We also removed allgold-standard nodes spanning only Trace or PUNC(recall that the input to the models did not includepunctuation) and collapsed all unary productions.9In all evaluations, we convert our models?
out-put tag sequence to a set of matched brackets by in-serting a left bracket preceding each word tagged Btag and a right bracket following each word taggedE.
This procedure occasionally results in a sentencewith an unmatched opening bracket.
If the un-matched opening bracket is one word from the endof the sentence, we delete it, otherwise we insert aclosing bracket at the end of the sentence.
Figure 1shows example input sequences together with exam-ple output tags and their corresponding bracketings.Previous work on chunking, most notably the2000 CONLL shared task (Tjong et al, 2000), hasdefined gold standard chunks that are useful for find-ing grammatical relations but which do not corre-spond to any particular linguistic notion.
It is notclear that such chunks should play a role in lan-guage acquisition, so instead we evaluate against tra-ditional syntactic constituents from Penn Treebank-style parses in two different ways.Our first evaluation method compares the outputof the chunkers to what Ponvert et al (2010) callclumps, which are just syntactic constituents thatspan only terminals.
We created our clump gold-standard by taking the parse trees resulting from thepreprocessing described above and deleting nodesthat span a non-terminal.
Figure 3 presents an ex-9As we evaluate unlabeled bracketing precision and recall,the label of the resulting nodes is irrelevant.g.aa dh.ae.t.s dh.ae.ts.aw.n.d.zp.r.ih.t.iy b.ae.d t.ax m.iyFigure 3: Example gold-standard with clumps in boxes.ample gold-standard parse tree with the clumps inboxes.
This evaluation avoids penalizing chunkersfor not positing hierarchical structure, but rewardschunkers only for finding very low-level structure.In the interest of making no a priori assumptionsabout the kinds of phrases our unsupervised methodrecovers, we also evaluate our completely flat, non-recursive chunks directly against the fully recursiveparses in the treebank.
To do so, we turn our chun-ked utterance into a flat tree by simply putting brack-ets around the entire utterance as in Figure 1(d).This evaluation penalizes chunkers for never posit-ing hierarchical structure, but makes no assumptionsabout which kinds of phrases ought to be found.4.3 Models and trainingIn all, nine HMM models, two versions of theCCL parser, and a uniform right-branching baselinewere evaluated.
Three of the HMMs were standardHMMs with chunking constraints on the four hiddenstates (as described in Section 3.2) that received asinput either words, ToBI break indices, or word du-ration cluster information, intended as baselines toilluminate the utility of each information source inisolation.
We also ran two each of Coupled HMMand Two-output HMM models that received wordsin one observed chain and either ToBI break index orduration cluster in the other observed chain.
In theCHMM models, chunking constraints were enforcedon the chain generating the words, while variablesgenerating the duration or ToBI information rangedover four discrete states with no constraints.10 Allnon-zero parameters were initialized approximatelyuniformly at random,11 and we ran EM until the log10We also tried imposing chunking constraints on the secondchain, but dev-set performance dropped slightly.11In preliminary dev-set experiments, different random ini-tializations performed within two points of each other.25Condition Prec Rec F-scBaselinesHMMWds 23.5 39.9 26.3BI 7.2 4.8 5.8Ac 4.7 2.5 3.3CombinedModels HMM Wds+BI 24.4 22.2 23.2Wds+Ac 20.7 22.7 21.7THMM Wds+BI 18.2 19.6 18.9Wds+Ac 36.1 47.8 41.2CHMM Wds+BI 25.5 36.3 29.9Wds+Ac 33.6 48.1 39.5CCL Parser 15.4 41.5 22.4Clumper 36.8 37.9 37.3Table 2: Scores for all models, evaluated on clumps.
In-put is words (Wds), break indices (BI), and/or acoustics.corpus probability changed less than 0.001%, typi-cally for 50-150 iterations.The CCL parser was trained on the same word se-quences provided to our models.
We also evaluatedthe CCL parser as a clumper (CCL Clumper) by re-moving internal nodes spanning a non-terminal.
Theright-branching baseline was generated by insertingone opening bracket in front of all but the last word,and closing all brackets at the end of the sentence.4.4 Results and DiscussionTable 2 presents results for our flat chunkers evalu-ated against Ponvert et al (2010)-style clumps.
Sev-eral points are apparent.
First, all three HMM base-lines yield very poor results, especially the prosodicbaselines, whose precision and recall are both be-low 10%.
Although the best combined modelsstill have relatively low performance, it is markedlyhigher than either of the individual baselines, andalso higher than the clumps identified by the CCLparser.
Particularly notable is the fact that lexi-cal and prosodic information appear to be super-additive in some cases, yielding combined perfor-mance that is higher than the sum of the individualscores.
Not all combined models work equally well,however: the poor performance of the HMM com-bined model supports our initial hypothesis that it isover parameterized.
Interestingly, our acoustic clus-ters work better than break indices when combinedwith words.
Finally, we see that the THMM andCHMM obtain similar performance using words +acoustics, suggesting that modeling prosodic struc-% Covered wordschunkchunkuttCondition Words UttsBaselinesHMMWds 81.9 98.4 3.16 2.82BI 68.2 68.1 4.95 1.50Ac 46.3 71.1 4.18 1.21CombinedModels HMM Wds+BI 79.8 98.3 4.30 2.02Wds+Ac 83.3 98.5 3.71 2.45THMM Wds+BI 84.6 99.0 3.84 2.40Wds+Ac 68.0 96.1 2.52 2.94CHMM Wds+BI 83.1 99.0 2.86 3.17Wds+Ac 76.5 97.6 2.62 3.19CCL Clumper 48.3 99.9 2.30 2.29Table 3: % words in a chunk, % utterances with > 0chunks, and mean chunk length and chunks per utterance.Condition Prec Rec F-scBaselinesHMMWds 48.8(32) 26.3(15) 34.2(20)BI 52.4(21) 18.5(5) 27.3(8)Ac 52.5(15) 16.3(3) 24.9(5)CombinedModels HMM Wds+BI 54.4(32) 23.2(11) 32.5(16)Wds+Ac 51.0(32) 24.7(13) 33.3(18)THMM Wds+BI 55.9(38) 26.8(15) 36.2(21)Wds+Ac 55.8(41) 31.0(20) 39.9(27)CHMM Wds+BI 48.4(32) 28.4(17) 35.8(22)Wds+Ac 54.1(40) 31.9(21) 40.1(28)CCL Parser 38.2(28) 37.6(28) 37.9(28)Clumper 58.8(42) 27.3(16) 37.3(23)Table 4: Model performance, evaluated on full trees.Scores in parentheses were computed after removing thefull sentence bracket, which provides a free true positive.ture separately from syntactic structure may be un-necessary (or that the CHMM does so badly).To provide further intuition into the kinds ofchunks recovered by the different models, we listsome relevant statistics in Table 3.
These statis-tics show that the models using lexical informationidentify at least one chunk in virtually all utterances,with the better models averaging 2-3 chunks per ut-terance of around 3 words each.
In contrast, theunlexicalized models find longer chunks (4-5 wordseach) but far fewer of them, with about 30% of ut-terances containing none at all.We turn now to the models?
performance on fullparse trees, shown in Table 4.
Two different scoresare given for each system: the first includes thetop-level bracketing of the full sentence (which is26standard in computing bracketing accuracy, but is afree true positive), while the second does not (for amore accurate picture of the system?s performanceon ambiguous brackets).
Comparing the second setof scores to the clumping evaluation, recall is muchlower for all the chunkers; the relatively small in-crease in precision indicates that the chunkers aremost effective at finding low-level structure.
Forboth sets of scores, the relative F-scores of the chun-kers are similar to the clumping evaluation, withthe words + acoustics versions of the THMM andCHMM scoring best.
Not surprisingly, the CCLparser has much higher recall than the chunkers,though the best chunkers have much higher preci-sion.
The result is that, using standard Parsevalscoring (first column), the best chunkers outperformCCL on F-score; even discounting the free sentence-level bracket (second column) they do about as well.It is worth noting that, although CCL achievesstate-of-the-art performance on the English WSJand German Negra corpora (Seginer (2007) reports75.9% F-score on WSJ10, for example), its perfor-mance on our corpus is far lower.
In fact, on this cor-pus the CCL parser (as well as our chunkers) under-perform a uniform right-branching baseline, whichobtains 42.2% precision and 64.8% recall (includingthe top-level bracket), leading to an overall F-scoreof 51.1%.
This suggests that our corpus is signifi-cantly more difficult than WSJ, probably due to dis-fluencies and/or lack of punctuation.12 Moreover,we stress that the use of a right-branching baseline,while useful as a measure of overall performance,is not plausible as a model of language acquisitionsince it is highly language-specific.5 ConclusionTaken together, our results indicate that a purelylocal model that combines lexical and acoustic-prosodic information in an appropriate way canidentify syntactic phrases far more effectively thana similar model using either source of informationalone.
Our best combined models outperformedthe baseline individual models by a wide marginwhen evaluated against the lowest level of syntac-tic structure, and their performance was compara-12Including punctuation improves CCL little, possibly be-cause the punctuation in this corpus is nearly all sentence-final.ble to CCL, a state-of-the-art unsupervised lexical-ized parser, when evaluated against full parse trees.It is disappointing that all of these systems scoredworse than a right-branching baseline, but this resultunderscores the major differences between parsingspoken utterances (even using transcriptions) andparsing written text (where CCL and other unsu-pervised parsers were developed and tested).
Sincechildren learning language do not (at least initially)know the head direction of their language, the right-branching baseline for English is not available tothem.
Thus, combining lexical and acoustic cuesmay provide them with initial useful informationabout the location of syntactic phrases, as suggestedby the prosodic bootstrapping hypothesis.Nevertheless, we caution against assuming thatthe usefulness of acoustic information must re-sult from its relation to prosody (especially be-cause we found that direct acoustic information wasmore useful than hand-annotated prosodic labels).The ?Smooth Signal Hypothesis?
(Aylett and Turk,2004) posits that talkers modulate their communica-tive effort according to the predictability of theirmessage in order to achieve efficient communica-tion, pronouncing more predictable parts of mes-sages more quickly or less distinctly.
If talkers con-sider syntactic predictability in this process, thenit is possible that acoustic cues help initial gram-mar learning not by serving as cues to prosody butby serving as cues to the talker?s syntax-dependentview of predictability.
In this case, it may makemore sense to discuss ?predictability bootstrapping?rather than ?prosodic bootstrapping.
?Regardless of the underlying reason, we haveshown that acoustic cues can be useful for identi-fying syntactic structure when used in combinationwith lexical information.
In order to further substan-tiate these results, we plan to replicate our experi-ments on a corpus of child-directed speech, whichwe are currently annotating for evaluation purposes.We also hope to extend our findings to a model thatcan identify hierarchical structure, and to analyzemore carefully the reasons for CCL?s poor perfor-mance on the Switchboard corpus, in hopes of devel-oping a model that can reach levels of performancecloser to those typical of unsupervised parsers forwritten text.27ReferencesMatthew Aylett and Alice Turk.
2004.
The smooth sig-nal redundancy hypothesis: A functional explanationfor relationships between redundancy, prosodic promi-nence, and duration in spontaneous speech.
Languageand Speech, 47(1):31 ?
56.Mary E. Beckman and Jan Edwards.
1990.
Lengthen-ings and shortenings and the nature of prosodic con-stituency.
In J. Kingston and Mary E. Beckman, edi-tors, Between the grammar and physics of speech: Pa-pers in laboratory phonology I, pages 152?178.
Cam-bridge: Cambridge University Press.M Beckman, J Hirschberg, and S Shattuck-Hufnagel.2005.
The original tobi system and the evolution ofthe tobi framework.
In S.-A.
Jun, editor, Prosodic Ty-pology ?
The Phonology of Intonation and Phrasing.Oxford University Press.Alan Bell, Jason M. Brenier, Michelle Gregory, CynthiaGirand, and Dan Jurafsky.
2009.
Predictability effectson durations of content and function words in conver-sational english.
Journal of Memory and Language,60:92 ?
111.S Calhoun, J Carletta, J Brenier, N Mayo, D Jurafsky,M Steedman, and D Beaver.
2010.
The nxt-formatswitchboard corpus: A rich resource for investigat-ing the syntax, semantics, pragmatics and prosodyof dialogue.
Language Resources and Evaluation,44(4):387 ?
419.Markus Dreyer and Izhak Shafran.
2007.
Exploitingprosody for pcfgs with latent annotations.
In Proc.
ofInterspeech, Antwerp, Belgium, August.L.
Gleitman and E. Wanner.
1982.
Language acquisition:The state of the art.
In E. Wanner and L. Gleitman, ed-itors, Language acquisition: The state of the art, pages3?48.
Cambridge University Press, Cambridge, UK.Michelle L. Gregory, Mark Johnson, and Eugene Char-niak.
2004.
Sentence-internal prosody does not helpparsing the way punctuation does.
In Proceedingsof the North American Association for ComputationalLinguistics (NAACL), pages 81?88.J.
G. Kahn, M. Lease, E. Charniak, M. Johnson, andM.
Ostendorf.
2005.
Effective use of prosody in pars-ing conversational speech.
In Proc.
of HLT/EMNLP-05.D H Klatt.
1976.
Linguistic uses of segmental durationsin english: Acoustic and perceptual evidence.
JASA,59:1208 ?
1221.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of the42nd Annual Meeting of the Association for Compu-tational Linguistics (ACL 2004), pages 479?486.Bob Ladd.
1996.
Intonational Phonology.
CambridgeUniversity Press.K Lari and S J Young.
1990.
The estimation of stochasticcontext-free grammars using the inside-outside algo-rithm.
Computer Speech and Language, 5:237 ?
257.P.
Liang, S. Petrov, M. I. Jordan, and D. Klein.
2007.
Theinfinite PCFG using hierarchical Dirichlet processes.In Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP/CoNLL).Brian MacWhinney.
2000.
The CHILDES project: Toolsfor analyzing talk.
Lawrence Erlbaum Associates,Mahwah, NJ, third edition.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Jacques Mehler, Peter Juszcyk, Ghislaine Lambertz,Nilofar Halsted, Josiane Bertoncini, and ClaudineAmiele-Tison.
1988.
A precursor to language acqui-sition in young infants.
Cognition, 29:143 ?
178.Se?verine Millotte, Roger Wales, and Anne Christophe.2007.
Phrasal prosody disambiguates syntax.
Lan-guage and Cognitive Processes, 22(6):898 ?
909.Antonio Molina and Feran Pla.
2002.
Shallow parsingusing specialized HMMs.
Journal of Machine Learn-ing Research, 2:595 ?
613.James L. Morgan, Richard P. Meier, and Elissa L. New-port.
1987.
Structural packaging in the input to lan-guage learning: contributions of prosodic and morpho-logical marking of phrases to the acquisition of lan-guage.
Cognitive Psychology, 19:498 ?
550.Ara V. Nefian, Luhong Liang, Xiaobao Pi, Liu Xiaoxi-ang, Crusoe Moe, and Kevin Murphy.
2002.
A cou-pled hmm for audiovisual speech recognition.
In Inter-national Conference on Acoustics, Speech and SignalProcessing.Elmer No?th, Anton Batliner, Andreas Kieling, and RalfeKompe.
2000.
Verbmobil: The use of prosody in thelinguistic components of a speech understanding sys-tem.
IEEE Transactions on Speech and Audio Pro-cessing, 8(5).Elias Ponvert, Jason Baldridge, and Katrin Erk.
2010.Simple unsupervised identification of low-level con-stituents.
In ICSC.P J Price, M Ostendorf, S Shattuck-Hufnagel, andC Fong.
1991.
The use of prosody in syntactic dis-ambiguation.
JASA, pages 2956 ?
2970.Yoav Seginer.
2007.
Fast unsupervised incremental pars-ing.
In Proceedings of the Association of Computa-tional Linguistics.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Proceedings ofHLT-NAACL 03, pages 213?220.28Stefanie Shattuck-Hufnagel and Alice E Turk.
1996.
Aprosody tutorial for investigators of auditory sentenceprocessing.
Journal of Psycholinguistic Research,25(2):193 ?
247.M.
Soderstrom, A. Seidl, D. G. K. Nelson, and P. W.Jusczyk.
2003.
The prosodic bootstrapping ofphrases: Evidence from prelinguistic infants.
Journalof Memory and Language, 49:249?267.Charles Sutton.
2006.
Grmm: Graphical models in mal-let.
http://mallet.cs.umass.edu/grmm/.Erik F. Tjong, Kim Sang, and Sabine Buchholz.
2000.Introduction to the conll-2000 shared task: Chunking.In Proceedings of CoNLL-2000 and LLL-2000, Lis-bon, Portugal.C W Wightman, S Shattuck-Hufnagel, M. Ostendorf, andP J Price.
1992.
Segmental durations in the vicinity ofprosodic phrase boundaries.
Journal of the AcousticalSociety of America, 91(3):1707 ?
1717.29
