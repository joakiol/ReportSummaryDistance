Ordering Among PremodifiersJ ames  Shaw and Vas i le ios  Hatz ivass i log louDepartment of Computer ScienceColumbia UniversityNew York, N.Y. 10027, USA{shaw, vh}@cs, columbia, eduAbst rac tWe present a corpus-based study of the se-quential ordering among premodifiers in nounphrases.
This information is important for thefluency of generated text in practical appli-cations.
We propose and evaluate three ap-proaches to identify sequential order among pre-modifiers: direct evidence, transitive closure,and clustering.
Our implemented system canmake over 94% of such ordering decisions cor-rectly, as evaluated on  a large, previously un-seen test corpus.1 In t roduct ionSequential ordering among premodifiers affectsthe fluency of text, e.g., "large foreign finan-cial firms" or "zero-coupon global bonds" aredesirable, while "foreign large financial firms"or "global zero-coupon bonds" sound odd.
Thedifficulties in specifying a consistent ordering ofadjectives have already been noted by linguists\[Whorf 1956; Vendler 1968\].
During the processof generating complex sentences by combiningmultiple clauses, there are situations where mul-tiple adjectives or nouns modify the same headnoun.
The text generation system must orderthese modifiers in a similar way as domain ex-perts use them to ensure fluency of the text.
Forexample, the description of the age of a patientprecedes his ethnicity and gender in medical do-main as in % 50 year-old white female patient".Yet, general lexicons uch as WordNet \[Miller etal.
1990\] and COMLEX \[Grishman et al 1994\],do not store such information.In this paper, we present automated tech-niques for addressing this problem of determin-ing, given two premodifiers A and B, the pre-ferred ordering between them.
Our methodsrely on and generalize mpirical evidence ob-tained from large corpora, and are evaluatedobjectively on such corpora.
They are informedand motivated by our practical need for order-ing multiple premodifiers in the MAGIC system\[Dalal et al 1996\].
MAGIC utilizes co-ordinatedtext, speech, and graphics to convey informa-tion about a patient's tatus after coronary by-pass surgery; it generates concise but complexdescriptions that frequently involve four or morepremodifiers in the same noun phrase.To demonstrate hat a significant portion ofnoun phrases have multiple premodifiers, weextracted all the noun phrases (NPs, exclud-ing pronouns) in a two million word corpus ofmedical discharge summaries and a 1.5 millionword Wall Street Journal (WSJ) corpus (seeSection 4 for a more detailed description of thecorpora).
In the medical corpus, out of 612,718NPs, 12% have multiple premodifiers and 6%contain solely multiple adjectival premodifiers.In the WSJ corpus, the percentages are a littlelower, 8% and 2%, respectively.
These percent-ages imply that one in ten NPs contains mul-tiple premodifiers while one in 25 contains justmultiple adjectives.Traditionally, linguists study the premodifierordering problem using a class-based approach.Based on a corpus, they propose various se-mantic classes, such as color, size, or national-ity, and specify a sequential order among theclasses.
However, it is not always clear howto map premodifiers to these classes, especiallyin domain-specific applications.
This justifiesthe exploration of empirical, corpus-based al-ternatives, where the ordering between A andB is determined either from direct prior evi-dence in the corpus or indirectly through otherwords whose relative order to A and B has al-ready been established.
The corpus-based ap-proach lacks the ontological knowledge used bylinguists, but uses a much larger amount of di-135rect evidence, provides answers for many morepremodifier orderings, and is portable to differ-ent domains.In the next section, we briefly describe priorlinguistic research on this topic.
Sections 3 and4 describe the methodology and corpus used inour analysis, while the results of our experi-ments are presented in Section 5.
In Section 6,we demonstrate how we incorporated our or-dering results in a general text generation sys-tem.
Finally, Section 7 discusses possible im-provements o our current approach.2 Re la ted  WorkThe order of adjectives (and, by analogy, nom-inal premodifiers) seems to be outside of thegrammar; it is influenced by factors such aspolarity \[Malkiel 1959\], scope, and colloca-tional restrictions \[Bache 1978\].
Linguists \[Goy-vaerts 1968; Vendler 1968; Quirk and Green-baum 1973; Bache 1978; Dixon 1982\] have per-formed manual analyses of (small) corpora andpointed out various tendencies, uch as the factsthat underived adjectives often precede derivedadjectives, and shorter modifiers precede longerones.
Given the difficulty of adequately describ-ing all factors that influence the order of pre-modifiers, most earlier work is based on plac-ing the premodifiers into broad semantic lasses,and specifying an order among these classes.More than ten classes have been proposed, withsome of them further broken down into sub-classes.
Though not all these studies agree onthe details, they demonstrate hat there is fairlyrigid regularity in the ordering of adjectives.For example, Goyvaerts \[1968, p. 27\] proposedthe order quality -< size/ length/shape -<old/new/young -< color -< national ity -<style -< gerund -< denominall; Quirk andGreenbaum \[1973, p. 404\] the order genera l-< age -< color -< participle -< provenance-< noun -< denominal; and Dixon \[1982, p.24\] the order value -< dimension -< physicalproperty -< speed -< human propensity -< age-< color.Researchers have also looked at adjective or-dering across languages \[Dixon 1982; Frawley1992\].
Frawley \[1992\], for example, observedthat English, German, Hungarian, Polish, Turk-ish, Hindi, Persian, Indonesian, and Basque, all1Where A ~ B stands for "A precedes B'.order value before size and both of those beforecolor.As with most manual analyses, the corporaused in these analyses are relatively small com-pared with modern corpora-based studies.
Fur-thermore, different criteria were used to ar-rive at the classes.
To illustrate, the adjec-tive "beautiful" can be classified into at leasttwo different classes because the phrase "beau-tiful dancer" can be transformed from either thephrase "dancer who is beautiful", or "dancerwho dances beautifully".Several deep semantic features have been pro-posed to explain the regularity among the po-sitional behavior of adjectives.
Teyssier \[1968\]first proposed that adjectival functions, i.e.identification, characterization, and classifica-tion, affect adjective order.
Martin \[1970\] car-ried out psycholinguistic studies of adjectiveordering.
Frawley \[1992\] extended the workby Kamp \[1975\] and proposed that intensionalmodifiers precede extensional ones.
However,while these studies offer insights at the complexphenomenon of adjective ordering, they cannotbe directly mapped to a computational proce-dure.On the other hand, recent computationalwork on sentence planning \[Bateman et al1998; Shaw 1998b\] indicates that generation re-search has progressed to a point where hardproblems uch as ellipsis, conjunctions, and or-dering of paradigmatically related constituentsare addressed.
Computational corpus stud-ies related to adjectives were performed by\[Justeson and Katz 1991; Hatzivassiloglou andMcKeown 1993; Hatzivassiloglou and McKeown1997\], but none was directly on the orderingproblem.
\[Knight and Hatzivassiloglou 1995\]and \[Langkilde and Knight 1998\] have proposedmodels for incorporating statistical informationinto a text generation system, an approach thatis similar to our way of using the evidence ob-tained from corpus in our actual generator.3 Methodo logyIn this section, we discuss how we obtain thepremodifier sequences from the corpus for anal-ysis and the three approaches we use for estab-lishing ordering relationships: direct corpus ev-idence, transitive closure, and clustering analy-sis.
The result of our analysis is embodied in a136function, compute_order(A, B) which returnsthe sequential ordering between two premodi-tiers, word A and word B.To identify orderings among premodifiers,premodifier sequences are extracted from sim-plex NPs.
A simplex NP is a maximal nounphrase that includes premodifiers uch as de-terminers and possessives but not post-nominalconstituents uch as prepositional phrases orrelative clauses.
We use a part-of-speech tag-ger \[Brill 1992\] and  a finite-state grammar  toextract s implex NPs .
The  noun phrases we ex-tract start with an optional determiner (DT)  orpossessive pronoun (PRP$) ,  followed by  a se-quence of cardinal numbers  (CDs),  adjectives(JJs), nouns  (NNs),  and  end with a noun.
Weinclude cardinal numbers  in NPs  to capture theordering of numerical  information such as ageand  amounts .
Gerunds  (tagged as VBG)  or pastparticiples (tagged as VBN), such as "heated"in "heated debate", are considered as adjectivesif the word  in front of them is a determiner,possessive pronoun,  or adjective, thus separat-ing adjectival and  verbal forms that are con-flared by  the tagger.
A morpho logy  modu letransforms plural nouns  and  comparat ive andsuperlative adjectives into their base forms toensure max imizat ion  of our frequency counts.There  is a regular expression filter wh ich  re-moves  obvious concatenations of s implex NPssuch as "takeover bid last week"  and  "Tylenol40 milligrams".After s implex NPs  are extracted, sequencesof premodifiers are obtained by dropping deter-miners, genitives, cardinal numbers  and  headnouns.
Our  subsequent  analysis operates on theresulting premodifier sequences, and  involvesthree stages: direct evidence, transitive closure,and  clustering.
We describe each stage in moredetail in the following subsections.3.1 D i rec t  Ev idenceOur  analysis proceeds on the hypothesis thatthe relative order of two premodifiers is fixedand  independent  of context.
G iven  two premod-ifiers A and  B,  there are three possible under-lying orderings, and  our system should striveto find wh ich  is true in this particular case: ei-ther A comes  before B,  B comes  before A, orthe order between A and  B is truly un impor -tant.
Our  first stage relies on  frequency datacollected f rom a training corpus to predict theorder of adjective and noun premodifiers in anunseen test corpus.To collect direct evidence on the order ofpremodifiers, we extract all the premodifiersfrom the corpus as described in the previoussubsection.
We first transform the premodi-tier sequences into ordered pairs.
For example,the phrase "well-known traditional brand-namedrug" has three ordered pairs, "well-known -<traditional", "well-known -~ brand-name", and"traditional -~ brand-name".
A phrase with npremodifiers will have (~) ordered pairs.
Fromthese ordered pairs, we construct a w x w matrixCount, where w the number of distinct modi-fiers.
The cell \[A, B\] in this matrix representsthe number of occurrences of the pair "A -~ B",in that order, in the corpus.Assuming that there is a preferred orderingbetween premodifiers A and B, one of the cellsCount\[A,B\] and Count\[B,A\] should be muchlarger than the other, at least if the corpus be-comes arbitrarily large.
However, given a corpusof a fixed size there will be many cases wherethe frequency counts will both be small.
Thisdata sparseness problem is exacerbated by theinevitable occurrence of errors dur ing the dataextraction process, wh ich  will introduce somespurious pairs (and orderings) of premodifiers.We therefore apply probabilistic reasoning todetermine when the data is strong enough todecide that A -~ B or B -~ A.
Under  the nullhypothesis that the two premoditiers order is ar-bitrary, the number  of t imes we have seen one ofthem follows the binomial  distribution with pa-rameter  p -- 0.5.
The  probability that we  wou ldsee the actually observed number  of cases withA ~ B,  say m,  among n pairs involving A andB isk----mwhich for the special case p = 0.5 becomes(0 (0k=m k=rnIf this probability is low, we reject the null hy-pothesis and conclude that A indeed precedes(or follows, as indicated by the relative frequen-cies) B.1373.2 Trans i t iv i tyAs we mentioned before, sparse data is a seri-ous problem in our analysis.
For example, thematrix of frequencies for adjectives in our train-ing corpus from the medical domain is 99.8%empty--only 9,106 entries in the 2,232 x 2,232matrix contain non-zero values.
To compen-sate for this problem, we explore the transi-tive properties between ordered pairs by com-puting the transitive closure of the ordering re-lation.
Utilizing transitivity information corre-sponds to making the inference that A -< C fol-lows from A -~ B and B -< C, even if we have nodirect evidence for the pair (A, C) but providedthat there is no contradictory evidence to thisinference ither.
This approach allows us to fillfrom 15% (WSJ) to 30% (medical corpus) of theentries in the matrix.To compute the transitive closure of the orderrelation, we map our underlying data to specialcases of commutative semirings \[Pereira and Ri-ley 1997\].
Each word is represented asa node ofa graph, while arcs between odes correspond toordering relationships and are labeled with ele-ments from the chosen semiring.
This formal-ism can be used for a variety of problems, us-ing appropriate definitions of the two binary op-erators (collection and extension) that operateon the semiring's elements.
For example, theall-pairs shortest-paths problem in graph the-ory can be formulated in a rain-plus semiringover the real numbers with the operators rainfor collection and + for extension.
Similarly,finding the transitive closure of a binary relationcan be formulated in a max-rain semi-ring or aor-and semiring over the set {0, 1}.
Once theproper operators have been chosen, the genericFloyd-Warshall algorithm \[Aho et al 1974\] cansolve the corresponding problem without modi-fications.We explored three semirings appropriate toour problem.
First, we apply the statistical de-cision procedure of the previous ubsection andassign to each pair of premodifiers either 0 (ifwe don't have enough information about theirpreferred ordering) or 1 (if we do).
Then we usethe or-and semiring over the {0,1} set; in thetransitive closure, the ordering A -~ B will bepresent if at least one path connecting A and Bvia ordered pairs exists.
Note that it is possiblefor both A -~ B and B -~ A to be present in thetransitive closure.This model involves conversions of the corpusevidence for each pair into hard decisions onwhether one of the words in the pair precedesthe other.
To avoid such early commitments,we use a second, refined model for transitiveclosure where the arc from A to B is labeledwith the probability that A precedes indeed B.The natural extension of the ({0, 1}, or, and)semiring when the set of labels is replaced withthe interval \[0, 1\] is then (\[0, 1\], max, rain).We estimate the probability that A precedes Bas one minus the probability of reaching thatconclusion in error, according to the statisticaltest of the previous ubsection (i.e., one minusthe sum specified in equation (2).
We obtainedsimilar results with this estimator and with themaximal ikelihood estimator (the ratio of thenumber of times A appeared before B to thetotal number of pairs involving A and B).Finally, we consider a third model in whichwe explore an alternative to transitive closure.Rather than treating the number attached toeach arc as a probability, we treat it as a cost,the cost of erroneously assuming that the corre-sponding ordering exists.
We assign to an edge(A, B) the negative logarithm of the probabilitythat A precedes B; probabilities are estimatedas in the previous paragraph.
Then our prob-lem becomes identical to the all-pairs shortest-path problem in graph theory; the correspond-ing semiring is ((0, +c~), rain, +).
We use log-arithms to address computational precision is-sues stemming from the multiplication of smallprobabilities, and negate the logarithms o thatwe cast the problem as a minimization task (i.e.,we find the path in the graph the minimizesthe total sum of negative log probabilities, andtherefore maximizes the product of the originalprobabilities).3.3 Cluster ingAs noted earlier, earlier linguistic work onthe ordering problem puts words into seman-tic classes and generalizes the task from order-ing between specific words to ordering the cor-responding classes.
We follow a similar, butevidence-based, approach for the pairs of wordsthat neither direct evidence nor transitivity canresolve.
We compute an order similarity mea-sure between any two premodifiers, reflectingwhether the two words share the same pat-138tern of relative order with other premodifiersfor which we have sufficient evidence.
For eachpair of premodifiers A and B, we examine v-ery other premodifier in the corpus, X; if bothA -~ X and B -~ X, or both A ~- X and B ~- X,one point is added to the similarity score be-tween A and B.
If on the other hand A -~ X andB ~- X, or A ~- X and B -~ X, one point is sub-tracted.
X does not contribute to the similarityscore if there is not sufficient prior evidence forthe relative order of X and A, or of X and B.This procedure closely parallels non-parametricdistributional tests such as Kendall's T \[Kendall1938\].The similarity scores are then converted intodissimilarities and fed into a non-hierarchicalclustering algorithm \[Sp~th 1985\], which sep-arates the premodifiers in groups.
This isachieved by minimizing an objective function,defined as the sum of within-group dissimilari-ties over all groups.
In this manner, premodi-tiers that are closely similar in terms of sharingthe same relative order with other premodifiersare placed in the same group.Once classes of premodifiers have been in-duced, we examine very pair of classes and de-cide which precedes the other.
For two classesC1 and C2, we extract all pairs of premodifiers(x, y) with x E C1 and y E C2.
If we have evi-dence (either direct or through transitivity) thatx -~ y, one point is added in favor of C1 -~ C2;similarly, one point is subtracted if x ~- y. Afterall such pairs have been considered, we can thenpredict the relative order between words in thetwo clusters which we haven't seen together ear-lier.
This method makes (weak) predictions forany pair (A, B) of words, except if (a) both Aand B axe placed in the same cluster; (b) no or-dered pairs (x, y) with one element in the classof A and one in the class of B have been identi-fied; or (c) the evidence for one class precedingthe other is in the aggregate qually strong inboth directions.4 The  CorpusWe used two corpora for our analysis: hospi-tal discharge summaries from 1991 to 1997 fromthe Columbia-Presbyterian Medical Center, andthe January 1996 part of the Wall Street Jour-nal corpus from the Penn TreeBank \[Marcus etal.
1993\].
To facilitate comparisons across thetwo corpora, we intentionally limited ourselvesto only one month of the WSJ corpus, so thatapproximately the same amount of data wouldbe examined in each case.
The text in each cor-pus is divided into a training part (2.3 millionwords for the medical corpus and 1.5 millionwords for the WSJ) and a test part (1.2 millionwords for the medical corpus and 1.6 millionwords for the WSJ).All domain-specific markup was removed, andthe text was processed by the MXTERMINATORsentence boundary detector \[Reynar and Rat-naparkhi 1997\] and Brill's part-of-speech tag-ger \[Brill 1992\].
Noun phrases and pairs of pre-modifiers were extracted from the tagged corpusaccording to the methods of Section 3.
Fromthe medical corpus, we retrieved 934,823 sim-plex NPs, of which 115,411 have multiple pre-modifiers and 53,235 multiple adjectives only.The corresponding numbers for the WSJ cor-pus were 839,921 NPs, 68,153 NPs with multiplepremodifiers, and 16,325 NPs with just multipleadjectives.We separately analyze two groups of premodi-tiers: adjectives, and adjectives plus nouns mod-ifying the head noun.
Although our techniquesare identical in both cases, the division is moti-vated by our expectation that the task will beeasier when modifiers are limited to adjectives,because nouns tend to be harder to match cor-rectly with our finite-state grammar and the in-put data is sparser for nouns.5 Resu l tsWe applied the three ordering algorithms pro-posed in this paper to the two corpora sepa-rately for adjectives and adjectives plus nouns.For our first technique of directly using evidencefrom a separate training corpus, we filled theCount matrix (see Section 3.1) with the fre-quencies of each ordering for each pair of pre-modifiers using the training corpora.
Then, wecalculated which of those pairs correspond to atrue underlying order relation, i.e., pass the sta-tistical test of Section 3.1 with the probabilitygiven by equation (2) less than or equal to 50%.We then examined each instance of ordered pre-modifiers in the corresponding test corpus, andcounted how many of those the direct evidencemethod could predict correctly.
Note that if Aand B occur sometimes as A -~ B and some-139Corpus Test pairsMedical/adjectives 27,670Financial/adjectives 9,925Medical/adjectives 74,664and nounsFinancial/adjectives 62,383and nounsDirect evidence Transitivity Transitivity(maxomin) (min-plus)92.67% (88.20%-98.47%) 89.60% (94.94%-91.79%) 94.93% (97.20%-96.16%)75.41% (53.85%-98.37%) 79.92% (72.76%-90.79%) 80.77% (76.36%-90.18%)88.79% (80.38%-98.35%) 87.69% (90.86%-91.50%) 90.67% (91.90%-94.27%)65.93% (35.76%-95.27%) 69.61% (56.63%-84.51%) 71.04% (62.48%-83.55%)Table 1: Accuracy of direct-evidence and transitivity methods on different data strata of our testcorpora.
In each case, overall accuracy is listed first in bold, and then, in parentheses, the percentageof the test pairs that the method has an opinion for (rather than randomly assign a decision becauseof lack of evidence) and the accuracy of the method within that subset of test cases.times as B -< A, no prediction method can getall those instances correct.
We elected to followthis evaluation approach, which lowers the ap-parent scores of our method, rather than forcingeach pair in the test corpus to one unambiguouscategory (A -< B, B -< A, or arbitrary).Under this evaluation method, stage one ofour system achieves on adjectives in the medi-cal domain 98.47% correct decisions on pairs forwhich a determination of order could be made.Since 11.80% of the total pairs in the test corpusinvolve previously unseen combinations of ad-jectives and/or new adjectives, the overall accu-racy is 92.67%.
The corresponding accuracy ondata for which we can make a prediction and theoverall accuracy is 98.35% and 88.79% for adjec-tives plus nouns in the medical domain, 98.37%and 75.41% for adjectives in the WSJ data, and95.27% and 65.93% for adjectives plus nouns inthe WSJ data.
Note that the WSJ corpus isconsiderably more sparse, with 64.24% unseencombinations of adjective and noun premodi-tiers in the test part.
Using lower thresholdsin equation (2) results in a lower percentage ofcases for which the system has an opinion but ahigher accuracy for those decisions.
For exam-ple, a threshold of 25% results in the ability topredict 83.72% of the test adjective pairs in themedical corpus with 99.01% accuracy for thesecases .We subsequently applied the transitivitystage, testing the three semiring models dis-cussed in Section 3.2.
Early experimentationindicated that the or-and model performedpoorly, which we attribute to the extensivepropagation of decisions (once a decision in fa-vor of the existence of an ordering relationship ismade, it cannot be revised even in the presenceof conflicting evidence).
Therefore we report re-sults below for the other two semiring models.Of those, the min-plus semiring achieved higherperformance.
That model offers additional pre-dictions for 9.00% of adjective pairs and 11.52%of adjective-plus-noun pairs in the medical cor-pus, raising overall accuracy of our predictionsto 94.93% and 90.67% respectively.
Overall ac-curacy in the WSJ test data was 80.77% for ad-jectives and 71.04% for adjectives plus nouns.Table 1 summarizes the results of these twostages.Finally, we applied our third, clustering ap-proach on each data stratum.
Due to datasparseness and computational complexity is-sues, we clustered the most frequent words ineach set of premodifiers (adjectives or adjectivesplus nouns), selecting those that occurred atleast 50 times in the training part of the cor-pus being analyzed.
We report results for theadjectives elected in this manner (472 frequentadjectives from the medical corpus and 307 ad-jectives from the WSJ corpus).
For these words,the information collected by the first two stagesof the system covers most pairs.
Out of the111,176 (=472.471/2) possible pairs in the med-ical data, the direct evidence and transitivitystages make predictions for 105,335 (94.76%);the corresponding number for the WSJ data is40,476 out of 46,971 possible pairs (86.17%).140The clustering technique makes ordering pre-dictions for a part of the remaining pairs--onaverage, depending on how many clusters arecreated, this method produces answers for 80%of the ordering cases that remained unansweredafter the first two stages in the medical corpus,and for 54% of the unanswered cases in the WSJcorpus.
Its accuracy on these predictions i 56%on the medical corpus, and slightly worse thanthe baseline 50% on the WSJ corpus; this lat-ter, aberrant result is due to a single, very fie-quent pair, chief executive, in which executiveis consistently mistagged as an adjective by thepart-of-speech tagger.Qualitative analysis of the third stage's out-put indicates that it identifies many interest-ing relationships between premodifiers; for ex-ample, the pair of most similar premodifiers onthe basis of positional information is left andright, which clearly fall in a class similar to thesemantic lasses manually constructed by lin-guists.
Other sets of adjectives with stronglysimilar members include {mild, severe, signifi-cant} and {cardiac, pulmonary, respiratory}.We conclude our empirical analysis by test-ing whether a separate model is needed for pre-dicting adjective order in each different domain.We trained the first two stages of our systemon the medical corpus and tested them on theWSJ corpus, obtaining an overall prediction ac-curacy of 54% for adjectives and 52% for adjec-rives plus nouns.
Similar results were obtainedwhen we trained on the financial domain andtested on medical data (58% and 56%).
Theseresults are not much better than what wouldhave been obtained by chance, and are clearlyinferior to those reported in Table 1.
Althoughthe two corpora share a large number of ad-jectives (1,438 out of 5,703 total adjectives inthe medical corpus and 8,240 in the WSJ cor-pus), they share only 2 to 5% of the adjectivepairs.
This empirical evidence indicates that ad-jectives are used differently in the two domains,and hence domain-specific probabilities must beestimated, which increases the value of an au-tomated procedure for the prediction task.6 Us ing  Ordered  Premodi f ie rs  inText  Generat ionExtracting sequential ordering information ofpremodifiers i  an off-line process, the results of(a) "John is a diabetic male white 74-year-old hypertensive patientwith a red swollen mass in theleft groin.
"(b) "John is a 74-year-oldhypertensive diabetic white malepatient with a swollen red massin the left groin.
"Figure 1: (a) Output of the generator withoutour ordering module, containing several errors.
(b) Output of the generator with our orderingmodule.which can be easily incorporated into the over-all generation architecture.
We have integratedthe function compute_order(A, B) into our mul-timedia presentation system MAGIC \[Dalai etal.
1996\] in the medical domain and resolvednumerous premodifier ordering tasks correctly.Example cases where the statistical predictionmodule was helpful in producing a more fluentdescription in MAGIC include placing age infor-mation before thnicity information and the lat-ter before gender information, as well as spe-cific ordering preferences, such as "thick" before"yellow" and "acute" before "severe".
MAGIC'Soutput is being evaluated by medical doctors,who provide us with feedback on different com-ponents of the system, including the fluency ofthe  generated text and its similarity to human-produced reports.Lexicalization is inherently domain depen-dent, so traditional exica cannot be portedacross domains without major modifications.Our approach, in contrast, is based on wordsextracted from a domain corpus and not onconcepts, therefore it can be easily applied tonew domains.
In our MAGIC system, aggre-gation operators, such as conjunction, ellip-sis, and transformations of clauses to adjectivalphrases and relative clauses, are performed tocombine related clauses together and increaseconciseness \[Shaw 1998a; Shaw 1998b\].
Wewrote a function, reorder_premod(... ), which iscalled after the aggregation operators, takes thewhole lexicalized semantic representation, andreorders the premodifiers right before the lin-guistic realizer is invoked.
Figure i shows thedifference in the output produced by our gener-141ator with and without the ordering component.7 Conc lus ions  and  Future  WorkWe have presented three techniques for explor-ing prior corpus evidence in predicting the orderof premodifiers within noun phrases.
Our meth-ods expand on observable data, by inferringnew relationships between premodifiers even forcombinations of premodifiers that do not occurin the training corpus.
We have empirically val-idated our approach, showing that we can pre-dict order with more than 94% accuracy whenenough corpus data is available.
We have alsoimplemented our procedure in a text generator,producing more fluent output sentences.We are currently exploring alternative waysto integrate the classes constructed by the thirdstage of our system into our generator.
Inthe future, we will experiment with semantic(rather than positional) clustering of premodi-tiers, using techniques such as those proposed in\[Hatzivassiloglou and McKeown 1993; Pereira etal.
1993\].
The qualitative analysis of the outputof our clustering module shows that frequentlypositional and semantic lasses overlap, and weare interested in measuring the extent of thisphenomenon quantitatively.
Conditioning thepremodifier ordering on the head noun is an-other promising approach, at least for very fre-quent nouns.8 AcknowledgmentsWe are grateful to Kathy McKeown for numer-ous discussions during the development of thiswork.
The research is supported in part bythe National Library of Medicine under grantR01-LM06593-01 and the Columbia UniversityCenter for Advanced Technology in High Per-formance Computing and Communications inHealthcaxe (funded by the New York State Sci-ence and Technology Foundation).
Any opin-ions, findings, or recommendations expressed inthis paper are those of the authors and do notnecessarily reflect the views of the above agen-cies.Re ferencesAlfred V. Aho, John E. Hopcroft, and Jeffrey D.Ullman.
The Design and Analysis of Com-puter Algorithms.
Addison-Wesley, Reading,Massachusetts, 1974.Carl Bache: The Order of Premodifying Adjec-tives in Present-Day English.
Odense Univer-sity Press, 1978.John A. Bateman; Thomas Kamps, Jorg Kleinz,and Klaus Reichenberger.
CommunicativeGoal-Driven NL Generation and Data-DrivenGraphics Generation: An ArchitecturM Syn-thesis for Multimedia Page Generation.
InProceedings of the 9th International Work-shop on Natural Language Generation., pages8-17, 1998.Eric Brill.
A Simple Rule-Based Part of SpeechTagger.
In Proceedings of the Third Confer-ence on Applied Natural Language Process-ing, Trento, Italy, 1992.
Association for Com-putational Linguistics.Mukesh Dalal, Steven K. Feiner, Kathleen R.McKeown, Desmond A. Jordan, Barry Allen,and Yasser al Safadi.
MAGIC: An Exper-imental System for Generating MultimediaBriefings about Post-Bypass Patient Status.In Proceedings of the 1996 Annual Fall Sym-posium of the American Medical Informat-ics Association (AMIA-96), pages 684-688,Washington, D.C., October 26-30 1996.R.
M. W. Dixon.
Where Have All the AdjectivesGone?
Mouton, New York, 1982.William Frawley.
Linguistic Semantics.Lawrence Erlbaum Associates, Hillsdale, NewJersey, 1992.D.
L. Goyvaerts.
An Introductory Study on theOrdering of a String of Adjectives in Present-Day English.
Philologica Pragensia, 11:12-28, 1968.Ralph Grishman, Catherine Macleod, andAdam Meyers.
COMLEX Syntax: Buildinga Computational Lexicon.
In Proceedings ofthe 15th International Conference on Com-putational Linguistics (COLING-9~), Kyoto,Japan, 1994.Vasileios Hatzivassiloglou and Kathleen McKe-own.
Towards the Automatic Identification ofAdjectival Scales: Clustering Adjectives Ac-cording to Meaning.
In Proceedings of the31st Annual Meeting of the ACL, pages 172-142182, Columbus, Ohio, June 1993.
Associationfor Computational Linguistics.Vasileios Hatzivassiloglou and Kathleen McKe.own.
Predicting the Semantic Orientation ofAdjectives.
In Proceedings of the 35th AnnualMeeting of the A CL, pages 174-181, Madrid,Spain, July 1997.
Association for Computa-tional Linguistics.John S. Justeson and Slava M.  Katz.
Co-occurrences of Antonymous  Adjectives andTheir Contexts.
Computational Linguistics,17(1):1-19, 1991.J.
A. W. Kamp.
Two Theories of Adjectives.In E. L. Keenan, editor, Formal Semanticsof Natural Language.
Cambridge UniversityPress, Cambridge, England, 1975.Maurice G. Kendall.
A New Measure ofRank Correlation.
Biometrika, 30(1-2):81-93, June 1938.Kevin Knight and Vasileios Hatzivassiloglou.Two-Level, Many-Paths Generation.
In Pro-ceedings of the 33rd Annual Meeting of theA CL, pages 252-260, Boston, Massachusetts,June 1995.
Association for ComputationalLinguistics.Irene Langkilde and Kevin Knight.
Genera-tion that Exploits Corpus-Based StatisticalKnowledge.
In Proceedings of the 36th An-nual Meeting of the A CL and the 17th Inter-national Conference on Computational Lin-guistics (ACL//COLING-98), pages 704-710,Montreal, Canada, 1998.Yakov Malkiel.
Studies in Irreversible Bino-mials.
Lingua, 8(2):113-160, May 1959.Reprinted in \[Malkiel 1968\].Yakov Malkiel.
Essays on Linguistic Themes.Blackwell, Oxford, 1968.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
Building a largeannotated corpus of English: The Penn Tree-bank.
Computational Linguistics, 19:313-330, 1993.J.
E. Martin.
Adjective Order and Juncture.Journal of Verbal Learning and Verbal Behav-ior, 9:379-384, 1970.George A. Miller, Richard Beckwith, ChristianeFellbaum, Derek Gross, and Katherine J.Miller.
Introduction to WordNet: An On-Line LexicM Database.
International Journalof Lexicography (special issue), 3(4):235-312,1990.Fernando C. N. Pereira and Michael D. Ri-ley.
Speech Recognition by Composition ofWeighted Finite Automata.
In EmmanuelRoche and Yves Schabes, editors, Finite-State Language Processing, pages 431-453.MIT Press, Cambridge, Massachusetts, 1997.Fernando Pereira, Naftali Tishby, and LillianLee.
Distributional Clustering of EnglishWords.
In Proceedings of the 31st AnnualMeeting of the ACL, pages 183-190, Colum-bus, Ohio, June 1993.
Association for Com-putational Linguistics.Randolph Quirk and Sidney Greenbaum.
AConcise Grammar of Contemporary English.Harcourt Brace Jovanovich, Inc., London,1973.Jeffrey C. Reynar and Adwait Ratnaparkhi.
AMaximum Entropy Approach to IdentifyingSentence Boundaries.
In Proc.
of the 5th Ap-plied Natural Language Conference (ANLP-97), Washington, D.C., April 1997.James Shaw.
Clause Aggregation Using Lin-guistic Knowledge.
In Proceedings of the 9thInternational Workshop on Natural LanguageGeneration., pages 138-147, 1998.James Shaw.
Segregatory Coordination and El-lipsis in Text Generation.
In Proceedings ofthe 36th Annual Meeting of the ACL and the17th International Conference on Computa-tional Linguistics (A CL/COLING-98), pages1220-1226, Montreal, Canada, 1998.Helmuth Sp~th.
Cluster Dissection and Anal-ysis: Theory, FORTRAN Programs, Exam-ples.
Ellis Horwood, Chichester, England,1985.J.
Teyssier.
Notes on the Syntax of the Adjec-tive in Modern English.
Behavioral Science,20:225-249, 1968.Zeno Vendler.
Adjectives and Nominalizations.Mouton and Co., The Netherlands, 1968.Benjamin Lee Whorf.
Language, Thought, andReality; Selected Writings.
MIT Press, Cam-bridge, Massachusetts, 1956.143
