Independence Assumptions Considered HarmfulAlexander  F ranzSony Computer  Science Laboratory  &: D21 LaboratorySony Corporat ion6-7-35 K i tash inagawaSh inagawa-ku,  Tokyo  141, JapanamI?cs l ,  sony .
co .
jpAbst rac tMany current approaches to statistical lan-guage modeling rely on independence a.~-sumptions 1)etween the different explana-tory variables.
This results in modelswhich are computationally simple, butwhich only model the main effects of theexplanatory variables oil the response vari-able.
This paper presents an argmnent infavor of a statistical approach that alsomodels the interactions between the ex-planatory variables.
The argument restson empirical evidence from two series of ex-periments concerning automatic ambiguityresolution.1 In t roduct ionIn this paper, we present an empirical argument infavor of a certain approach to statistical natural an-guage modeling: we advocate statistical natural an-guage models that account for the interactions be-tween the explanatory statistical variables, ratherthan relying on independence a~ssumptions.
Suchmodels are able to perform prediction on the basis ofestimated probability distributions that are properlyconditioned on the combinations of the individualvalues of the explanatory variables.After describing one type of statistical model thatis particularly well-suited to modeling natural lan-guage data, called a loglinear model, we present ein-pirical evidence fi'om a series of experiments on dif-ferent ambiguity resolution tasks that show that theperformance of the loglinear models outranks theperformance of other models described in the lit-erature that a~ssume independence between the ex-planatory variables.2 Stat i s t i ca l  Language Mode l ingBy "statistical language model", we refer to a mathe-matical object that "imitates the properties" of somerespects of naturM language, and in turn makes pre-dictions that are useful from a scientific or engineer-ing point of view.
Much recent work in this flame-work hm~ used written and spoken natural anguagedata to estimate parameters for statisticM modelsthat were characterized by serious limitations: mod-els were either limited to a single explanatory vari-able or.
if more than one explanatory variable wa~sconsidered, the variables were assumed to be inde-pendent.
In this section, we describe a method forstatistical language modeling that transcends theselimitations.2.1 Categor ical  Data  AnalysisCategorical data analysis is the area of statistics thataddresses categorical statistical variable: variableswhose values are one of a set of categories.
An exam-pie of such a linguistic variable is PART-OF-SPEECH,whose possible values might include nou.n, verb, de-terminer, preposition, etc.We distinguish between a set of explanatory vari-ames.
and one response variable.
A statistical modelcan be used to perforin prediction in the followingmanner: Given the values of the explanatory vari-ables, what is the probability distribution for theresponse variable, i.e.. what are the probabilities forthe different possible values of the response variable?2.2 The Cont ingency TableTile ba,sic tool used in categorical data analysis isthe contingency table (sometimes called the "cross-classified table of counts").
A contingency table is amatrix with one dimension for each variable, includ-ing the response variable.
Each cell ill the contin-gency table records the frequency of data with theappropriate characteristics.Since each cell concerns a specific combination offeat.ures, this provides a way to estimate probabil-ities of specific feature combinations from the ob-served frequencies, ms the cell counts can easily beconverted to probabilities.
Prediction is achieved bydetermining the value of the response variable giventhe values of the explanatory variables.1822.3 The  Logl inear ModelA loglinear model is a statistical model of the effectof a set of categorical variables and their combina-tions on the cell counts in a contingency table.
It canbe used to address the problem of sparse data.
sinceit can act a.s a "snmothing device, used to obtaincell estimates for every cell in a sparse array, even ifthe observed count is zero" (Bishop, Fienberg, andHolland.
1975).Marginal totals (sums for all values of some vari-ables) of the observed counts are used to estimatethe parameters of the loglinear model; the model inturn delivers estimated expected cell counts, whichare smoother than the original cell counts.The mathematical form of a loglinear model is a,sfollows.
Let mi5~ be the expected cell count for cell( i .
j .
k .
.
.
.  )
in the contingency table.
The generalform of a loglinear model is ms follows:logm/j~... = u.
-{ - l t l t i ) .
-~ l t2 ( j ) -~-U3(k ) -~ lZ l2 ( i j ) -~- .
.
.
(1)In this formula, u denotes the mean of the logarithmsof all the expected counts, u+ul(1)  denotes the meanof the logarithms of the expected counts with valuei of the first variable, u + u2(j) denotes the mean ofthe logarithms of the expected counts with value j ofthe second variable, u + ux~_(ii) denotes the mean ofthe logarithms of the expected counts with value i ofthe first veriable and value j of the second variable,and so on.Thus.
the term uzii) denotes the deviation of themean of the expected cell counts with value i of thefirst variable from the grand mean u.
Similarly, theterm Ul2(ij) denotes the deviation of the mean of theexpected cell counts with value i of the first variableand value j of the second variable from the grandmean u.
In other words, ttl2(ij) represents the com-bined effect of the values i and j for the first andsecond variables on the logarithms of the expectedcell counts.In this way, a loglinear model provides a way toestimate xpected cell counts that depend not onlyon the main effects of the variables, but also onthe interactions between variables.
This is achievedby adding "interaction terms" such a.s Ul2( i j  ) to  thenmdel.
For further details, see (Fienberg, 1980).2.4 The Iterative Est imation ProcedureFor some loglinear models, it is possible to obtainclosed forms for the expected cell counts.
For morecomplicated models, the iterative proportional f itt ingalgorithm for hierarchical loglinear models (Dentingand Stephan, 1940) can be used.
Briefly, this proce-dure works ms follows.Let the values for the expected cell counts that areestimated by the model be represented by the sym-bol 7hljk ....
The interaction terms in the loglinearnmdels represent constraints on the estimated ex-pected marginal totals.
Each of these marginal con-straints translates into an adjustment scaling factorfor the cell entries.
The iterative procedure has thefollowing steps:1.
Start with initial estimates for the estimated ex-pected cell counts.
For example, set al 7hijal =1.0.2.
Adjust each cell entry by multiplying it by thescaling factors.
This moves the cell entries to-wards satisfaction of the marginal constraintsspecified by the nmdel.3.
Iterate through the adjustment steps until themaximum difference e between the marginaltotals observed in the sample and the esti-mated marginal totals reaches a certain mini-mum threshold, e.g.
e = 0.1.After each cycle, the estimates atisfy the con-straints specified in the model, and the estimatedexpected marginal totals come closer to matchingthe observed totals.
Thus.
the process converges.This results in Maximum Likelihood estimates forboth multinomial and independent Poisson samplingschemes (Agresti, 1990).2.5 Model ing InteractionsFor natural language classification and predictiontasks, the aim is to estimate a conditional proba-bility distribution P(H\ [E )  over the possible valuesof the hypothesis H, where the evidence E consistsof a number of linguistic features el, e2 .... .
Much ofthe previous work in this area assumes independencebetween the linguistic features:P(/-/le~.ej .
.
.
.  )
~ P(H le l )  x P (H le j )  x ... (2)For example, a model to predict Part-of-Speech ofa word on the basis of its morphological ffix and itscapitalization might a.ssume independence betweenthe two explanatory variables a,s follows:P(POSIAFFIX, CAPITALIZATION) ,,~ (3)P(POSIAFFIX ) x P(POSICAPITALIZATION )This results ill a considerable computational sim-plification of the model but, as we shall see below.leads to a considerable oss of information and con-comitant decrease in prediction accuracy.
With aloglinear model, on the other hand.
such indepen-dence assumptions are not necessary.
The loglinearmodel provides a posterior distribution that is prop-erly conditioned on the evidence, and maximizingthe conditional probability P(H IE  ) leads to mini-mum error rate classification (Duda and Hart.
1973).183s3 Pred ic t ing  Par t -o f -SpeechWe will now turn to the empirical evidence support-ing the argument against independence assumptions.
~In this section, we will compare two models for pre- e ~dicting the Part-of-Speech of an unknown word: A ~simple model that treats the various explanatoryvariables ms independent, and a model using log-l inear smoothing of a contingency table that takesinto account the interactions between the explana-tory variables.3.1 Const ruct ing  the  Mode lThe model wa~s constructed in the following way.First,  features that could be used to guess the PUSof a word were determined by examining the trainingport ion of a text corpus.
The initial set of featuresconsisted of the following:?
INCLUDES-NUMBER.
Does the word include anunlber??
CAPITALIZED.
Is the word in sentence-initial po-sition and capitalized, in any other position andcapitalized, or in lower ca~e??
INCLUDES-PERIOD.
Does the word include a pe-riod??
INCLUDES-COMMA.
Does the word include aco ln lna??
FINAL-PERIOD.
Is the last character of the worda period??
INCLUDES-HYPHEN.
Does the word include ahyphen??
ALL-UPPER-CASE.
Is the word in all upper case??
SHORT.
Is the length of the word three charac-ters or less??
INFLECTION.
Does the word carry one of theEnglish inflectional suffixes??
PREFIX.
Does the word carry one of a list offrequently occurring prefixes??
SUFFIX.
Does the word carry one of a list offrequently occurring suffixes?Next, exploratory data analysis was perfornled inorder to determine relevant features and their values,and to approximate which features interact.
Eachword of the training data was then turned into afeature vector, and the feature vectors were cross-classified in a contingency table.
The contingencytable was smoothed using a loglinear models.3.2 DataTraining and evaluation data  was obtained from thePenn Treebank Brown corpus (Marcus, Santorini,and Marcinkiewicz, 1993).
The characteristics of"'rare" words that might show up ms unknown wordsdiffer fi'om the characteristics of words in general.so a two-step procedure wa~ employed a first t imeOverall Accuracyi .
__ , .
.
.
, o_  4 L~hnem?
F~tgf~9 L~llnQ&?
~Oatu?~8.F=0.4 Set Accuracy.
.
.
.
4 maeo,tnaom Flalu,~ \[i 4 LOgL'/~III ~omtur~ j i 9 l.~Jl~ar vu lu ,uFigure 1: Performance of Different Modelsto obtain a set of "'rare" words ms training data, andagain a second time to obtain a separate set of "'rare*"words ms evMuation data.
There were 17,000 wordsin the training data, and 21,000 words in the evalua-tion data.
Ambiguity resolution accuracy was evalu-ated for the "'overall accuracy" (Percentage that themost likely PUS tag is correct), and "'cutoff factoraccuracy" (accuracy of the answer set consisting ofall PUS tags whose probabil ity lies within a factorF of the most likely PUS (de Marcken, 1990)).3.3 Accuracy  Resu l ts(Weischedel et al, 1993) describe a model for un-known words that uses four features, but treats thefeatures ms independent.
We reimplemented thismodel by using four features: POS, INFLECTION,CAPITALIZED, and HYPHENATED, In Figures i 2,the results for this model are labeled 4 Indepen-dent  Features .
For comparison, we created a log-l inear model with the same four features: the resultsfor this model are labeled 4 Log l inear  Features .The highest accuracy was obtained by the log-l inear model that includes all two-way interac-tions and consists of two contingency tM)les withthe following features: POS,  ALL-UPPER-CASE.HYPHENATED, INCLUDES-NUMBER, CAPITALIZED,INFLECTION, SHORT.
PREFIX, and SUFFIX.
The re-sults for this model are lM)eled 9 Log l inear  Fea-tu res .
The parameters for all three unknown wordmodels were estimated from the training data.
andthe models were evaluated on the evaluation data.The accuracy of the different models in a.ssigningthe most likely POSs to words is summarized in Fig-ure 1.
In the left diagram, the two barcharts howtwo different accuracy memsures: Percent correct(Overa l l  Accuracy) ,  and percent correct withinthe F=0.4 cutoff factor answer set (F=0.4  SetAccuracy) .
In both cruses, the loglinear modelwith four features obtains higher accuracy thanthe method that assumes independence between thesame four features.
The loglinear model with nine184ooo o?
.-- .
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
o- .
.
.
.
.
o .
.
.
.
.
.
o .
.
.
.
.?
- -  L?glmea'wlt F~t~e= \]1 2 3 4 5 6 7N~ol  FeaturesFigure 2: Effect of Number of Features on Accuracy$oUregmm Pro~exe~ kog~r  Mce.~Figure 3: Error Rate on Unknown Wordsfeatures further improves this score.3.4 Ef fect  o f  Number  of  Features  onAccuracyThe performance of the loglinear model can be im-proved by adding more features, but this is not pos-sible with the simpler nmdel that assumes indepen-dence between the features.
Figure 2 shows theperformance of the two types of nmdels with fen-ture sets that ranged from a single feature to ninefeatures.As the diagram shows, the accuracies for bothmethods rise with the first few features, but thenthe two methods show a clear divergence.
The ac-curacy of the simpler method levels off around ataround 50-55%, while the loglinear model reachesan accuracy of 70-75%.
This shows that the loglin-ear model is able to tolerate redundant features anduse information from more features than the simplermethod, and therefore achieves better results at am-biguity resolution.3.5 Add ing  Context  to the  Mode lNext, we added of a stochastic POS tagger (Char-niak et al, 1993) to provide a model of context.
Astochastic POS tagger assigns POS labels to wordsin a sentence by using two parameters:?
Lex ica l  P robab i l i t ies :  P(wl t  ) - -  the proba-bil ity of observing word w given that the tag toccurred.?
Contextua l  P robab i l i t ies :  P(t i \ [ t i -1 ,  t~_2) - -the probabi l i ty of observing tag ti given that thetwo previous tags ti-1, t,i--2 occurred.The tagger maximizes the probabil i ty of the tag se-quence T = t.l,t, 2 .
.
.
.
,t.,, given the word sequenceW = wz ,w2, .
.
.
,w, , ,  which is approximated a.s fol-lows:I"LP(TIW) ~ I I  P(wdt~)P(tdt~_~, ti_=) (4)i=  1The accuracy of the combination of the loglinearmodel for local features and the stochastic POS tag-ger for contextual features was evaluated empiricallyby comparing three methods of handling unknownwords:?
Un igram:  Using the prior probabi l i ty distri-bution P( t )  of the POS tags for rare words.?
P robabUis t i c  UWM:  Using the probabil isticmodel that assumes independence between thefeatures.?
C lass i f ie r  UWM:  Using the loglinear modelfor unknown words.Separate sets of training and evaluation data for thetagger were obtained from from the Penn TreebankWall Street corpus.
Evaluation of the combined sys-t.em was performed on different configurations of thePOS tagger on 30-40 different samples containing4,000 words each.Since the tagger displays considerable variance inits accuracy in assigning POS to unknown words incontext, we use boxplots to display the results.
Fig-ure 3 compares the tagging error rate on unknownwords for the unigram method (left) and the log-l inear method with nine features (labeled s ta t i s t i -ca l  c lass i f ier)  at right.
This shows that the Ioglin-ear model significantly improves the Part-of-Speechtagging accuracy of a stochastic tagger on unknownwords.
The median error rate is lowered consider-ably, and samples with error rates over 32% are elim-inated entirely.185o ===?
PmO~?
UWM?
Logli~e= UWMo u , *=*?
?
?
=a?
o ??
08?0 S tO 15 2Q 25 30 35 40 4S 50 SS 60Peeclntage ol Unknown WO~=Figure 4: Effect of Proportion of Unknown Wordson Overall Tagging Error Rate3.6 Effect of P ropor t ion  of UnknownWordsSince most of the lexical ambiguity resolution powerof stochastic PUS tagging comes from the lexicalprobabilities, unknown words represent a significantsource of error.
Therefore, we investigated the effectof different ypes of models for unknown words onthe error rate for tagging text with different propor-tions of unknown words.Samples of text that contained ifferent propor-tions of unknown words were tagged using the threedifferent methods for handling unknown words de-scribed above.
The overall tagging error rate in-creases ignificantly as the proportion of new wordsincreases.
Figure 4 shows a graph of overall taggingaccuracy versus percentage ofunknown words in thetext.
The graph compares the three different meth-ods of handling unknown words.
The diagram showsthat the loglinear model leads to better overall tag-ging performance than the simpler methods, with aclear separation of all samples whose proportion ofnew words is above approximately 10%.4 Predicting PP AttachmentIn the second series of experiments, we compare theperformance of different statistical models on thetask of predicting Prepositional Phrase (PP) attach-ment.4.1 Features for PP  At tachmentFirst, an initial set of linguistic features that couldbe useful for predicting PP attachment was deter-mined.
The initial set included the following fea-tures:?
PREPOSITION.
Possible values of this feature in-clude one of the more frequent prepositions inthe training set, or the value other-prep.
* VERB-LEVEL.
Lexical association strength be-tween the verb and the preposition.?
NOUN-LEVEL.
Lexical association strength be-tween the noun and the preposition.?
NOUN-TAG.
Part-of-Speech of the nominal at-tachment site.
This is included to account forcorrelations between attachment and syntacticcategory of the nominal attachment site, suchas "PPs disfavor attachment to proper nouns."?
NOUN-DEFINITENESS.
Does the nominal attach-ment site include a definite determiner?
Thisfeature is included to account for a possible cor-relation between PP attachment to the nom-inal site and definiteness, which was derivedby (Hirst, 1986) from the principle of presup-position minimization of (Craln and Steedman,1985).?
PP-OBJECT-TAG.
Part-of-speech of the object ofthe PP.
Certain types of PP objects favor at-tachment to the verbal or nominal site.
For ex-ample, temporal PPs, such as "in 1959", wherethe prepositional object is tagged CD (cardi-nal), favor attachment to the VP, because tileVP is more likely to have a temporal dimension.The association strengths for VERB-LEVEL andNOUN-LEVEL were measured using the Mutual In-formation between the noun or verb, and the prepo-sition.
1 The probabilities were derived ms MaximumLikelihood estimates from all PP cases in the train-ing data.
The Mutual Information values were or-dered by rank.
Then, the a~ssociation strengths werecategorized into eight levels (A-H), depending onpercentile in the ranked Mutual Information values.4.2 Exper imenta l  Data  and Eva luat ionTraining and evaluation data was prepared from thePenn treebank.
All 1.1 million words of parsed textin the Brown Corpus, and 2.6 million words of parsedWSJ articles, were used.
All instances of PPs thatare attached to VPs and NPs were extracted.
Thisresulted in 82,000 PP cases from the Brown Corpus,and 89,000 PP cases from the WS.\] articles.
Verbsand nouns were lemmatized to their root forms if theroot forms were attested in the corpus.
If the rootform did not occur in the corpus, then the inflectedform was used.All the PP cases from the Brown Curl)us, and50,000 of the WSJ cases, were reserved ms trainingdata.
The remaining 39,00 WSJ PP cases formed theevaluation pool.
In each experiment, performanceIMutu',d Information provides an estimate of themagnitude of the ratio t)ctw(.
(-n the joint prol)abilityP(verb/noun,1)reposition), and the joint probability a.~-suming indcpendcnce P(verb/noun)P(prcl)osition ) - s(:(,(Church and Hanks, 1990).186o1|uR~m A~jllon Hfr ,3~ & Roolh kog~eaw ~ak~r1 !oo ol?tIio!lloFigure 5: Results for Two Attachment Sites Figure 6: Three Attachment Sites: Right Associa-tion and Lexical Associationwas evaluated oil a series of 25 random samples of100 PP cases fi'om the evaluation pool.
in order toprovide a characterization f the error variance.4.3 Exper imenta l  Results:  TwoAt tachments  SitesPrevious work oll automatic PP attachment disam-biguation has only considered the pattern of a verbphrase containing an object, and a final PP.
Thislends to two possible attachment sites, the verb andthe object of the verb.
The pattern is usually furthersimplified by considering only the heads of the possi-ble attachment sites, corresponding to the sequence"Verb Noun1 Preposition Noun2".The first set of experiments concerns this pattern.There are 53,000 such cases in the training data.
and16,000 such cases in the evaluation pool.
A numberof methods were evaluated on this pattern accord-ing to the 25-sample scheme described above.
Theresults are shown in Figure 5.4.3.1 Basel ine:  Right Associat ionPrepositional phrases exhibit a tendency to attachto the most recent possible attachment site; this isreferred to ms the principle of "'Right Association".For the "V NP PP'" pattern, this means preferringattachment to the noun phra~se.
On the evaluationsamples, a median of 65% of the PP cases were at-tached to the noun.4.3.2 Results of Lexical Associat ion(Hindle and R ooth.
1993) described a method forobtaining estimates of lexical a.ssociation strengthsbetween ouns or verbs and prepositions, and thenusing lexical association strength to predict.
PP at-tachment.
In our reimplementation f this lnethod.the probabilities were estimated fi'om all the PPcases in the training set.
Since our training dataare bracketed, it was possible to estimate tile lexi-cal associations with much less noise than Hindle &R ooth, who were working with unparsed text.
Themedian accuracy for our reimplementation f Hindle& Rooth's method was 81%.
This is labeled "Hindle& Rooth'" in Figure 5.4.3.3 Results of the Loglinear ModelThe loglinear model for this task used the featuresPREPOSITION.
VERB-LEVEL, NOUN-LEVEL, andNOUN-DEFINITENESS,  and it included all second-order interaction terms.
This model achieved a me-dian accuracy of 82%.Hindle & Rooth's lexical association strategy onlyuses one feature (lexical aasociation) to predict PPattachment, but.
ms the boxplot shows, the resultsfrom the loglinear model for the "V NP PP" patterndo not show any significant improvement.4.4 Exper imenta l  Results:  ThreeAt tachment  SitesAs suggested by (Gibson and Pearlmutter.
1994),PP attachment for the "'Verb NP PP" pattern isrelatively easy to predict because the two possibleattachment sites differ in syntactic category, andtherefore have very different kinds of lexical pref-erences.
For example, most PPs with of attach tonouns, and most PPs with f,o and by attach to verbs.In actual texts, there are often more than two possi-ble attachment sites for a PP.
Thus, a second, morerealistic series of experiments was perforlned thatinvestigated ifferent PP attachment strategies forthe pattern "'Verb Noun1 Noun2 Preposition Noun3"'that includes more than two possible attachmentsites that are not syntactically heterogeneous.
Therewere 28,000 such cases in the training data.
and 8000ca,~es in the evaluation pool.187"5 oRIgN AUCCUII~ Split HinOle & Rooln Lo~l~ur M0~elFigure 7: Summary of Results for Three AttachmentSites4.4.1 Baseline: Right Associat ionAs in the first set of experiments, a number ofmethods were evaluated an the three attachment sitepattern with 25 samples of 100 random PP cases.The results are shown in Figures 6-7.
The baselineis again provided by attachment according to theprinciple of "Right Attachment'; to the nmst recentpossible site, i.e.
attaclunent to Noun2.
A medianof 69% of the PP cases were attached to Noun2.4.4.2 Results  of Lexical Assoc iat ionNext, the lexical association method was evalu-ated on this pattern.
First.
the method describedby Hindle & Rooth was reimplemented by using thelexical association strengths estimated from all PPcases.
The results for this strategy are labeled "BasicLexical Association" in Figure 6.
This method onlyachieved a median accuracy of 59%, which is worsethan always choosing the rightmost attachment site.These results suggest that Hindle & R.ooth's coringfunction worked well in the "'Verb Noun1 Preposi-tion Noun2"' case not only because it was an accurateestimator of lexical associations between individualverbs/nouns and prepositions which determine PPattachment, but also because it accurately predictedthe general verb-noun skew of prepositions.4.4.3 Results of Enhanced LexicalAssociat ionIt seems natural that this pattern calls for a com-bination of a structural feature with lexical associa-tion strength.
To implement this, we modified Hin-dle & Rooth's method to estimate attachments othe verb, first noun.
and second noun separately.This resulted in estimates that combine the struc-tural feature directly with the lexical associationstrength.
The modified method performed betterthan the original exical association scoring function,but it still only obtained a median accuracy of 72%.This is labeled "Split Hindle & Rooth" in Figure 7.4.4.4 Results  of Loglinear Mode lTo create a model that combines variousstructural and lexical features without indepen-dence assumptions, we implemented a loglinearmodel that includes the variables VERB-LEVELFIRST-NOUN-LEVEL.
and SECOND-NOUN-LEVEL.
2The loglinear model also includes the variablesPREPOSITION and PP-OBJECT-TAG.
It, wassmoothed with a loglinear model that includes allsecond-order interactions.This method obtained a median accuracy of 79%;this is labeled "Loglinear Model" in Figure 7.
As theboxplot shows, it performs ignificantly better thanthe methods that only use estimates of lexical a,~so-clarion.
Compared with the "'Split Hindle Sz Rooth'"method, the samples are a little less spread out, andthere is no overlap at all between the central 50% ofthe samples from the two methods.4.5 DiscussionThe simpler "V NP PP" pattern with two syntacti-cally different attachment sites yielded a null result:The loglinear method did not perform significantlybetter than the lexical association method.
Thiscould mean that the results of the lexical associa-tion method can not be improved by adding otherfeatures, but it is also possible that the features thatcould result in improved accuracy were not identi-fied.The lexical association strategy does not performwell on the more difficult pattern with three possibleattachment sites.
The loglinear model, on the otherhand, predicts attachment with significantly higheraccuracy, achieving a clear separation of the central50% of the evaluation samples.5 Conc lus ionsWe have contrasted two types of statistical languagemodels: A model that derives a probability distribu-tion over the response variable that is properly con-ditioned on the combination of the explanatory vari-able, and a simpler model that treats the explana-tory variables as independent, and therefore modelsthe response variable simply a~s the addition of theindividual main effects of the explanatory variables.2These features use tile s~unc Mutual Information-ba.~ed measure of lcxic',d a.sso(:iation a.s tim prc.vious log-linear model for two possibh~" attachment sites, whichwcrc estimated from all nomin'M azt(l vcrhal PP att~t(:h-ments in the corpus.
The features FIRST-NOUN-LEVELaaM SECOND-NOUN-LEVEL use the same estimates: inother words, in contrm~t to the "split Lexi(:al Associa-tion" method, they were not estimated sepaxatcly forthe two different nominaJ, attachment sites.188The experimental results how that, with the samefeature set, inodeling feature interactions yields bet-ter performance: such nmdels achieves higher accu-racy, and its accura~,y can be raised with additionalfeatures.
It is interesting to note that modeling vari-able interactions yields a higher perforlnanee gainthan including additional explanatory variables.While these results do not prove that modelingfeature interactions i necessary, we believe that theyprovide a strong indication.
This suggests a mlmberof avenues for filrther research.First, we could attempt to improve the specificmodels that were presented by incorporating addi-tional features, and perhal)S by taking into accounthigher-order features.
This might help to addressthe performance gap between our models and hu-man subjects that ha,s been documented in the lit-erature, z A more ambitious idea would be to use astatistical model to rank overall parse quality for en-tire sentences.
This would be an improvement overschemes that a,ssnlne independence between a num-ber of individual scoring fimctions, such ms (Alshawiand Carter, 1994).
If such a model were to includeonly a few general variables to account for such fea-tures a.~ lexical a.ssociation and recency preferencefor syntactic attachment, it might even be worth-while to investigate it a.s an approximation to thehuman parsing mechanism.ReferencesAgresti, Alan.
1990.
Categorical Data Analysis..John Wiley & Sons, New York.Alshawi, Hiyan and David Carter.
1994.
Trainingand scaling preference functions for disambigua-tion.
Computational Linguistics, 20(4):635-648.Bishop.
Y. M., S. E. Fienberg, and P. W. Holland.1975.
Discrete Multivariate Analysis: Th, eory andPractice.
MIT Press, Cambridge, MA.Charniak, Eugene, Curtis Hendrickson, Neil ,Jacob-son, and Mike Perkowitz.
1993.
Equations forpart-of-speech tagging.
In AAAI-93, pages 784~789.Church, Kenneth W. and Patrick Hanks.
1990.Word a,~soeiation orms, mutual information,and lexicography.
Computational Linguistics,16(1):22-29.Crain, Stephen and Mark 3.
Steedman.
1985.
Onnot being led up the garden path: The use of3For cXaml)l(', If random s(;ntcnc(;s with "V('rb NPPP" (:~(:s from th(: Penn tr(',(;l)ank aa'(: tak(:n ms the gohlstandard, then (Hindlc and Rooth, 1993) and (Ratna-l)arkhi, Ryn~r, aal(t Roukos.
1994) rcl)ort that human,(:xi)(;rts using only hca(t words obtain 85%-88% a('cu-ra~:y.
If the huma~l CXl)erts arc allow(:d to consult thewhoh," scntcn(:(:, their accuracy judged against randomTrc(}l)ank s(',ntclm(:s ri es to al)l)roximatcly 93%.context by the psychological syntax processor.In David R. Dowty, Lauri Karttunen, and An-rnold M. Zwicky, editors, Natural Language Pars-ing, pages 320-358, Cambridge, UK.
CambridgeUniversity Press.de Marcken, Carl G. 1990.
Parsing the LOB corpus.In Proceedings of A CL-90, pages 243-251.Deming, W. E. and F. F. Stephan.
1940.
On a lea.stsquares adjustment of a sampled frequency ta-ble when the expected marginal totals are known.Ann.
Math.
Statis, (11):427--444.Duda, Richard O. and Peter E. Hart.
1973.
PatternClassification and Scene Analysis.
John Wiley &Sons, New York.Fienberg, Stephen E. 1980.
Th.e Analysis of Cross-Classified Categorical Data.
The MIT Press,Cambridge, MA, second edition edition.Franz, Alexander.
1996.
Automatic Ambiguity Res-olution in Natural Language Processing.
volume1171 of Lecture Notes in Artificial Intelligence.Springer Verlag, Berlin.Gibson, Ted and Neal Pearhnutter.
1994.
A corpus-ba,sed analysis of psycholinguistic constraints onPP attachment.
In Charles Clifton Jr., LynFrazier, and Keith Rayner, editors, Perspectiveson Sentence Processing.
Lawrence Erlbaum Asso-ciates.Hindle, Donald and Mats Rooth.
1993.
Structuralambiguity and lexical relations.
ComputationalLinguistics, 19( 1 ): 103-120.Hirst, Graeme.
1986.
Semantic Interpretation andthe Resolution of Ambiguity.
Cambridge Univer-sity Press, Cambridge.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building a largeannotated corpus of English: The Penn Treebank.Computational Linguistics, 19(2):313-330.Ratnaparkhi, Adwait, Jeff B ynar, and SalimRoukos.
1994.
A maximum entropy modelfor Prepositional Phra,se attachment.
In ARPAWorkshop on Human Language Technology.Plainsboro, N.\], March 8-11.Weischedel, Ralph, Marie Meteer, Richard Schwartz,Lance Ramshaw, and Jeff Palmucci.
1993.
Cop-ing with ambiguity and unknown words throughprobabilistic models.
Computational Linguistics,19(2):359-382.189
