Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 669?680,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsThe Topology of Semantic KnowledgeJimmy Dubuisson Jean-Pierre EckmannDe?partement de Physique The?orique and Section de Mathe?matiquesUniversite?
de Gene`veJimmy.Dubuisson@unige.chChristian ScheibleInstitut fu?r Maschinelle SprachverarbeitungUniversity of Stuttgartscheibcn@ims.uni-stuttgart.deHinrich Schu?tzeCenter for Informationand Language ProcessingUniversity of MunichAbstractStudies of the graph of dictionary definitions(DD) (Picard et al 2009; Levary et al 2012)have revealed strong semantic coherence oflocal topological structures.
The techniquesused in these papers are simple and the mainresults are found by understanding the struc-ture of cycles in the directed graph (wherewords point to definitions).
Based on our ear-lier work (Levary et al 2012), we study a dif-ferent class of word definitions, namely thoseof the Free Association (FA) dataset (Nelsonet al 2004).
These are responses by subjectsto a cue word, which are then summarized bya directed, free association graph.We find that the structure of this network isquite different from both the Wordnet and thedictionary networks.
This difference can beexplained by the very nature of free associa-tion as compared to the more ?logical?
con-struction of dictionaries.
It thus sheds some(quantitative) light on the psychology of freeassociation.In NLP, semantic groups or clusters are inter-esting for various applications such as wordsense disambiguation.
The FA graph is tighterthan the DD graph, because of the large num-ber of triangles.
This also makes drift ofmeaning quite measurable so that FA graphsprovide a quantitative measure of the seman-tic coherence of small groups of words.1 IntroductionThe computer study of semantic networks has beenaround since the advent of computers (Brunet, 1974)and has been used to study semantic relations be-tween concepts and for analyzing semantic data.Traditionally, a popular lexical database of Englishis Wordnet (Miller, 1995; Miller and Fellbaum,1998), which organizes the semantic network interms of graph theory.
In contrast to manual ap-proaches, the automatic analysis of semantically in-teresting graph structures of language has receivedincreasing attention.
For example, it has becomeclear more recently that cycles and triangles playan important role in semantic networks, see e.g.,(Dorow et al 2005).
These results suggest that theunderlying semantic structure of language may bediscovered through graph-theoretical methods.
Thisis in line with similar findings in much wider realmsthan NLP (Eckmann and Moses, 2002).In this paper, we compare two different typesof association networks.
The first network is con-structed from an English dictionary (DD), the sec-ond from a free association (FA) database (Nelsonet al 2004).
We represent both datasets throughdirected graphs.
For DD, the nodes are words andthe directed edges point from a word to its defini-tion(s).
For FA, the nodes are again words, and eachcue word has a directed edge to each association itelicits.Although the links in these graphs were not con-structed by following a rational centralized process,their graph exhibits very specific features and weconcentrate on the study of its topological proper-ties.
We will show that these graphs are quite dif-ferent in global and local structure, and we inter-pret this as a reflection of the different nature ofDD vs. FA.
The first is an objective set of rela-669tions between words and their meaning, as explainedby other words, while the second reveals the natureof subjective reactions to cue words by individuals.This matter of fact is reflected by several quantita-tive differences in the structure of the correspondinggraphs.The main contribution of this paper is an empiri-cal analysis of the way semantic knowledge is struc-tured, comparing two different types of associationnetworks (DD and FA).
We conduct a mathemati-cal analysis of the structure of the graphs to showthat the way humans express their thoughts exhibitsstructural properties in which one can find seman-tic patterns.
We show that a simple graph-basedapproach can leverage the information encoded infree association to narrow down the ambiguity ofmeaning, resulting in precise semantic groups.
Inparticular, we find that the main strongly connectedcomponent of the FA graph (the so-called core) isvery cyclic in nature and contains a large predom-inance of short cycles (i.e., co-links and triangles).In contrast to the DD graph, bunches of trianglesform well-delimited lexical fields of collective se-mantic knowledge.
This property may be promisingfor downstream tasks.
Further, the methods devel-oped in this paper may be applicable to graph rep-resentations that occur in other problems such asword sense disambiguation (e.g., (Heylighen, 2001;Agirre and Soroa, 2009)) or sentiment polarity in-duction (Hassan and Radev, 2010; Scheible, 2010).To show the semantic coherence of these lexi-cal fields of the FA graph, we perform an exper-iment with human raters and find that cycles arestrongly semantically connected even when com-pared to close neighbors in the graph.The reader might wonder why sets of pairwiseassociations can lead to any interesting structure.One of the deep results in graph theory, (Bolloba?s,2001), is that in sparse graphs, i.e., in graphs withfew links per node, the number of triangles is ex-tremely rare.
Therefore, if one does find many tri-angles in a graph, they must be not only a signalof non-randomness, but carry relevant informationabout the domain of research as shown earlier (Eck-mann and Moses, 2002).2 The USF FA datasetThis dataset is one of the largest existing databasesof free associations (FA) and has been collected atthe University of South Florida since 1973 by re-searchers in psychology (Nelson et al 2004).
Overthe years, more than 6?000 participants producedabout 750?000 responses to 5?019 stimulus words.The procedure for collecting the data is called dis-crete association task and consists in asking partici-pants to give the first word that comes to mind (tar-get) when presented a stimulus word (cue).For creating the initial set of stimulus words,the Jenkins and Palermo word association norms(Palermo and Jenkins, 1964) proved useful but toolimited as they consist of only 200 words.
For thisreason, additional words have been regularly addedto the pool of normed words, unfortunately withoutwell established rules being followed.
For instance,some were selected as potentially interesting cues,some were added as responses to the first sets of cuesand, some others were collected for supporting newstudies on verbs.
We still work with this database,because of its breadth.The final pool of stimuli comprises 5?019 wordsof which 76% are nouns, 13% adjectives, and 7%verbs.
A word association is said to be normedwhen the target is also part of the set of norms, i.e.,a cue.
The USF dataset of free associations con-tains 72?176 cue-target pairs, 63?619 of which arenormed.
As an example, the association puberty-sexis normed whereas the association puberty-thirteenis not, because thirteen is not a cue.3 Mathematical definitionsWe collect here those notions we need for the analy-sis of the data.A directed graph is a pair G = (V,E) of a setV of vertices and, a set E of ordered pairs of ver-tices also called directed edges.
For a directed edge(u, v) ?
E, u is called the tail and v the head ofthe edge.
The number of edges incident to a vertexv ?
V is called the degree of v. The in-degree(resp.
out-degree) of a vertex v is the number ofedge heads (resp.
edge tails) adjacent to it.
A vertexwith null in-degree is called a source and a vertexwith null out-degree is called a sink.A directed path is a sequence of vertices such670that a directed edge exists between each consecutivepair of vertices of the graph.
A directed graph issaid to be strongly connected, (resp.
weakly con-nected) if for every pair of vertices in the graph,there exists a directed path (resp.
undirected path)between them.
A strongly connected component,SCC, (resp.
weakly connected component, WCC)of a directed graph G is a maximal strongly con-nected (resp.
weakly connected) subgraph of G.A directed cycle is a directed path such that itsstart vertex is the same as its end vertex.
A co-link is a directed cycle of length 2 and a triangle adirected cycle of length 3.The distance between two vertices in a graph isthe number of edges in the shortest path connectingthem.
The diameter of a graph G is the greatestdistance between any pair of vertices.
The charac-teristic path length is the average distance betweenany two vertices of G.The density of a directed graph G(V,E) is theproportion of existing edges over the total numberof possible edges and is defined as:d = |E|/(|V |(|V | ?
1))The neighborhoodNi of a vertex vi isNi = {vj :eij ?
E or eji ?
E}.The local clustering coefficient Ci for a vertex vicorresponds to the density of its neighborhood sub-graph.
For a directed graph, it is thus given by:Ci =|{ejk : vj , vk ?
Ni, ejk ?
E}||Ni|(|Ni| ?
1)The clustering coefficient of a graph G is the aver-age of the local clustering coefficients of all its ver-tices.The efficiency Eff of a directed graph G is an in-dicator of the traffic capacity of a network.
It is theharmonic mean of the distance between any two ver-tices of G. It is defined as:Eff =1|V |(|V | ?
1)?i 6=j?V1dijThe linear correlation coefficient between tworandom variables X and Y is defined as:?
(X,Y ) = (E[XY ]?
?X?Y )/(?X?Y )where ?X and ?X are respectively the mean andstandard deviation of the random variable X .The linear degree correlation coefficient of agraph is called assortativity and is expressed as:?D =?xyxy(exy ?
axby)/(?a?b)where exy is the fraction of all links that connectnodes of degree x and y and where ax and by are re-spectively the fraction of links whose tail is adjacentto nodes with degree x and whose head is adjacent tonodes with degree y, satisfying the following threeconditions:?xyexy = 1, ax =?yexy, by =?xexyWhen ?D is positive, the graph possesses assor-tative mixing and high-degree nodes tend to con-nect to other high-degree nodes.
On the other hand,when ?D is negative, the graph features disassorta-tive mixing and high-degree nodes tend to connectto low degree nodes.The intersection graph of sets Ai, i = 1, .
.
.
,m,is constructed by representing each setAi as a vertexvi ?
V and adding an edge for each pair of sets witha non-empty intersection:E = {(vi, vj) : Ai ?Aj 6= ?
}4 Graph topology analysis4.1 Graph generationOur goal being to study the FA network topology,we first concentrate on the generation of an un-weighted directed graph.
We generate the corre-sponding graph by adding a directed edge for eachcue-target pair of the dataset.
We only consider pairswhose target was normed in order to avoid overload-ing the graph with noisy data (e.g., a response mean-ingful only to a specific participant).
The graph has5?019 vertices and 63?619 edges.
It is composed ofa single WCC and 166 SCCs.For comparison with dictionary definitions (DD),we construct a graph from the Wordnet2 dictionary(nouns only), following (Levary et al 2012).
Thisgraph contains 54?453 vertices and 179?848 edges.6714.2 Core extractionThe so-called core was defined previously in (Picardet al 2009; Levary et al 2012) as that subset ofnodes in which a random walker gets trapped afteronly a few steps.The shave algorithm was used in (Levary et al2012) to isolate this subset.
It consists in recursivelyremoving the source and sink nodes from a weaklyconnected directed graph and permits to get the sub-graph induced by the union of its strongly connectedcomponents.
Note that the dictionary graph (DD)has no sinks (i.e., words that never get defined) andthat it contains a giant SCC whose size is compara-ble to the one of the initial graph.It turns out that the FA graph also contains a giantSCC, therefore getting the core consists more simplyin extracting the main SCC of the initial graph.
Weuse Tarjan?s algorithm (Tarjan, 1972) for isolatingthe FA core.4.3 Vertex degree analysisThe FA core has a maximum in-degree of 313, amaximum out-degree of 33 and an average degreeof 25.42.
The in-degree distribution follows a powerlaw (?
= 1.93) and the out-degrees are Poisson-likedistributed with a peak at 14 (Steyvers and Tenen-baum, 2005; Gravino et al 2012).Words having a high in-degree are targets thattend to be cited more frequently.
On the other hand,words having a high out-degree are cues that evokemany different targets.The most evocative cues are, in decreasing orderof out-degree: field (33), body (31), condemn (29),farmer (29), crisis (28), plan (28), attention (27),animal (27), and hang (27).
Interestingly, the mostcited targets (i.e., targets with highest in-degree) arein decreasing order: food (313), money (295), water(271), car (251), good (246), bad (221), work (187),house (183), school (182), love (179).4.4 Cycle decomposition of the coreWe define the vertex k-cycle multiplicity(resp.
edge k-cycle multiplicity) as the num-ber of k-cycles a given vertex (resp.
edge) belongsto.
We call core-ER the set of Erdo?s-Re?nyi (ER)random graphs G(n,M) having the same numberof nodes and the same number of edges as the FA2 4 6 8 10 12 14 16101102103104Cycles length#ofshortestcyclescoreERFigure 1: Distribution of shortest cycles lengths in thecore compared to equivalent ER modelsOne should bear in mind that we only consider the set of short-est cycles.
Thus, a k-cycle is not counted if each of its nodesbelongs to a cycle whose length is < k. Although the num-ber of 4-shortest cycles is comparable in the core and core-ERgraphs for example, there are in reality far more 4-cycles in thecore (i.e., 42?738 versus 6?517).
We see that when consideringshortest cycles, short cycles tend to hide long ones, and, as alarge proportion of nodes in the core belong to 2- and 3-cycles,many longer cycles do not get counted at all.core.
We start by extracting the 2- and 3-cycles byusing a customized version of Johnson?s algorithm(Johnson, 1975).
The first thing we observe is thatthe core has a very high density of short cycles: thesubset of nodes belonging to 2-cycles (i.e., nodeswith 2-cycle multiplicities > 0) cover 95% of thecore vertices and the 3-cycles cover 88% of thecore vertices.
The corresponding core-ER graphshave on average about 100 times fewer 2-cycles andalmost 20 times fewer 3-cycles.This shows that the core is very cyclic in natureand that it remains very well connected for short-length cycles: most vertices of the core indeed be-long to at least one co-link or triangle.In order to limit computation times, we only con-sidered shortest cycles for lengths ?
3 and analyzedthe distribution of the number of shortest cyclesin the core compared to equivalent random graphs.Whereas there are many more short cycles in thecore, we observe a predominance of 4, 5 and 6-cycles in core-ER graphs.
However, we find again aslight predominance of long cycles (length between7 and 15) in the core (see Fig.
1).
See (Levary et al2012), Fig.
3, where the cycle distribution is verydifferent, with a minimum at length 5.6724.5 Interpretation of cycles2-cycles are composed of concretely related words(e.g., drug-coke, destiny-fate, einstein-genius, .
.
.
).The vertex with highest 2-cycle multiplicity is music(22).Words in 3- and 4-cycles often belong to thesame lexical field.
Examples of 3-cycles: protect-guard-defend or space-universe-star.
The vertex(resp.
edge) with highest 3-cycle multiplicity iscar (86) (resp.
bad-crime (11)).
Examples of 4-cycles: monster-dracula-vampire-ghost or flu-virus-infection-sick.Longer cycles are more difficult to describe: Re-lations linking words of a given cycle exhibit se-mantic drift with increasing length (cf.
(Levary etal., 2012)).
Examples of 5-cycles: yellow-coward-chicken-soup-noodles and sleep-relax-music-art-beauty.The cumulated set of free associations reflects theway in which a group of people retrieved its seman-tic knowledge.
As the associated graph is highlycircular, this suggests that this knowledge is notstored in a hierarchical way (Steyvers and Tenen-baum, 2005).
The large predominance of short cy-cles in the core may indeed be a natural conse-quence of the semantic information being acquiredby means of associative learning (Ashcraft and Rad-vansky, 2009; Shanks, 1995).4.6 FA core clustering4.6.1 The walktrap community algorithmComplex networks are globally sparse but con-tain locally dense subgraphs.
These groups of highlyinterconnected vertices are called communities andconvey important properties of the network.Although the notion of community is difficult todefine formally, the current consensus establishesthat a partition P = {C1, C2, .
.
.
, Ck} of the ver-tex set of a graph G represents a good communitystructure if the proportion of edges inside the Ci ishigher than the proportion of edges between them(Fortunato, 2010).Computing such communities in a large graph isgenerally computationally expensive (Lancichinettiand Fortunato, 2009).
We use the so-called ?Walk-trap?
community detection algorithm (Pons and Lat-apy, 2006) for extracting communities from the FAnetworks.
The idea lying behind this algorithm isthat random walks on a graph will tend to get trappedin the densely connected subgraphs.Let P tij be the probability of going from vertex ito vertex j through a random walk of length t. Thedistance between two vertices i and j of the graph isdefined as:rij(t) =???
?n?k=1(P tik ?
Ptjk)2d(k)where d(k) is the degree of vertex k.One defines the probability P tC,j to go fromcommunity C to vertex j in t steps: P tC,j =?i?C Ptij/|C|, and then the distance is easily gen-eralized for two communities C1, C2.The algorithm starts with a partition P1 = {{v} ?V } of the initial graph into n communities each ofwhich is a single vertex.
At each step, two communi-ties are chosen and merged according to the criteriondescribed below and the distances between commu-nities are updated.
The process goes on until we ob-tain the partition Pn = {V }.In order to reduce complexity, only adjacent com-munities are considered for merging.
The decisionis then made according to Ward?s method (Everittet al 2001): at each step k, the two communitiesthat minimize the mean ?k of the squared distancesbetween each vertex and its community are merged:?k =1n?C?Pk?i?Cr2iC4.6.2 Clustering of the coreWe first identify the communities of the FA coreusing the Walktrap algorithm.
We immediatelyobserve that when the path length parameter in-creases, the number of identified communities de-creases (i.e., for a length of 2, we find 35 communi-ties whereas for a length of 9, we only find 8 com-munities).For a path length of 2, the algorithm extracts 35communities, 7 of which contain more than 100 ver-tices, 3 of which contain between 100 and 50 ver-tices and 25 of which contain less than 50 vertices.We observe that for most small communities (i.e.,the ones containing less than 50 vertices), there ex-ists a clear relation between the labels of their ver-673tices.
Typically, the labels are part of the same lexi-cal field (e.g., all the planets (except earth) or relatedby a common grammatical function (such as why,where, what, .
.
.
).4.6.3 Clustering of the core co-linksWe define the k-cycle induced subgraph of agraph G as the subgraph of G induced by the setof its vertices with k-cycle multiplicity > 0.The co-link graph of a graphG(V,E) is the undi-rected graph obtained by replacing each co-link (i.e.,2-cycle) of the 2-cycle induced subgraph of G by asingle undirected edge and removing all other edges.The co-link graph of the FA core has 4?508 ver-tices and 8?309 edges for a density of 8?10?4.
Itis composed of a single weakly connected compo-nent that can be seen as a projection of the strongestsemantic links from the original graph.
Extractingthe co-link graph is thus an efficient way of select-ing the set of most important semantic links (i.e., theset of 2-cycles that appear in large predominance inthe core compared to what is found in an equivalentrandom graph) while filtering out the noisy or negli-gible ones.The sets of communities extracted by the Walk-trap algorithm exhibit different degrees of granular-ity depending on the length parameter.
For shortpaths, a large number of very small communities arereturned (e.g., 923 communities when length equals2) whereas for longer paths the average size of thecommunities increases more and more.The community detection exhibits thus a far finerdegree of granularity for the core co-links graph thanfor the core itself.
The size of the communities beingmuch smaller in average, it is striking to notice towhich extent the words of a given community aresemantically related.Examples of communities found in the core co-links graph include (standards, values, morals,ethics), (hopeless, romantic, worthless, useless),(thesaurus, dictionary, vocabulary, encyclopedia)or (molecule, atom, electron, nucleus, proton, neu-tron).4.6.4 DD core clustering vs FA core clusteringThe clustering of both cores has very differentcharacteristics: We illustrate the neighborhoods ofconflict for both cases in Fig.
2 and 3.quarrelpersonalityconfusionangerdisagreementdilemmafightwarbattletroubleconflictargumentstruggledisagreeschedulefrustrationproblemFigure 2: Neighborhood of conflict in the FA coreThe set of words belonging to the neighborhood of conflict areclearly part of the same lexical field.
The high density of co-links leads to cyclicity and we see that many directed trianglesare present in the local subgraph (e.g., conflict-trouble-fight,conflict-argument-disagree).
We can even find triangles of co-links that link together words semantically strongly related (e.g.,fight-war-battle, fight-quarrel-argument).
Nodes that are part ofthe neighborhood of conflict in both FA and DD are in emptycircles.On one hand, the words in communities of the DDcore are in most cases either synonyms, e.g., (decla-ration, assertion, claim) or an instance-of kind ofrelation, e.g., (signal, gesture, motion) or (zero, inte-ger).On the other hand, communities of the FA coreare generally composed of words belonging to thesame lexical field and sharing the same level of ab-straction.Moreover, we notice that it is often difficult to es-tablish the semantic relation existing between wordsof many small communities (i.e., containing lessthan 10 words) of the DD core.
Two such examplesare: (choice, probate, executor, chosen, certificate,testator, will) and (numeral, monarchy, monarch,crown, significance, autocracy, symbol, interpreta-tion).The comparison of DD and FA reveals, in a quan-titative way, fundamental differences between thetwo realms.
The interesting data are shown in ta-ble 1.674frictionwarconflicttwodisagreementgroupFigure 3: Neighborhood conflict in the DD coreFirst, we note that the neighborhood has a lower density thanin the FA core.
We also see that there is no cycle and thereseems to be a flow going from source nodes to sink nodes.
Asit generally happens in the neighborhood subgraphs of the DDcore, source nodes are rather specific words whereas sink nodesare generic words.FA core DD core# vertices 4?843 1?496# edges 61?544 4?766density 2.5?10?3 2.1?10?3avg degree 25.4 6.37max in-degree 313 59directed diameter 10 29characteristic path length 4.26 10.42efficiency 2.5?10?1 1.2?10?1clustering coefficient 8.5?10?2 5.1?10?2assortativity 5.5?10?2 6.1?10?2Table 1: Comparison FA vs DDNote that while the FA core is in fact larger thanthe DD core, its diameter is smaller.
This illustratesin a beautiful way the nature of free association ascompared to the more neutral dictionary.
In par-ticular, the characteristic path length is smaller inthe FA graph, because humans use generalized eventknowledge (McRae and Matsuki, 2009) in free asso-ciation, producing semantic shortcuts.
For example,FA contains a direct link mirage?water, whereasin DD, the shortest path between the two words ismirage?refraction?wave?water.5 The Bricks of Meaning5.1 Extraction of the seedWe already saw that most vertices of the core be-long to directed 2- and 3-cycles.
Whereas 2-cyclesestablish strong semantic links (i.e., synonymy orantonymy relations) and provide cyclicity to the un-derlying directed graph, we claim that 3-cycles (i.e.,triangles) form the set of elementary concepts of thecore.These structues are common to DD and to FA, butwe will see that the links in FA are somehow moredirect than in DD.We call seed the subgraph of the core induced bythe set V3 of vertices belonging to directed trianglesand shell the subgraph of the core induced by theset V \V3 (i.e., the set of vertices with a null 3-cyclemultiplicity), see Fig.
4.Initial graphcoreshellseedFigure 4: Composition of the FA graphThe graph of FA contains a giant SCC (the core).
The subgraphof the core induced by the set of nodes belonging to at least onetriangle also forms a giant component we call the ?seed?.
Thesubgraph of the core induced by the set of nodes not belongingto any triangle is called the ?shell?
and is composed of manysmall SCCs, including single vertices.
Although the shell has alow density, its nodes are very well connected to the seed.The shell contains 530 nodes and 309 edges.There are 7?035 edges connecting the shell to theseed.
The shell consists of of many small SCCs andalthough its average degree is low (1.17), its ver-tices have on average many (13.27) connections tothe seed.The seed contains 4?313 vertices (89% of thecore) and 54?197 edges.
The first thing to noticeis that it has 100 times more co-links (7?895) and20 times more triangles (13?119) than an equivalentrandom graph.
We call shortcuts the 32?773 edgesof the seed that do not belong to 3-cycles, see Fig.
5.The seed obviously also contains cycles whoselength is greater than 3.
One can check that there ex-ist only 5 basic motifs involving 2 attached trianglesand 1 shortcut for creating 4- and 5-cycles, and thatlinking 2 isolated triangles with 2 shortcuts also per-675SFigure 5: Shortcut edges between two triangles sharing asingle vertex STwo triangles can share 0, 1 or 2 vertices.
For each of these threebasic motifs, we count the maximum number of shortcut edges(i.e., edges not belonging to 3-cycles) that can be added.
Bylinking two triangles, these shortcuts permit to move two basicsemantic units closer together and create longer cycles (i.e., 4,5, and 6-cycles).
Long cycles can be thus considered as group-ings of basic semantic units.
In the case of two triangles sharingone vertex for example, it is possible to add at most 6 short-cuts, whereas, for two triangles sharing two vertices, at most 2shortcuts can be added.mit to form 4-, 5- and 6-cycles.
All longer cycles aresimply made of a juxtaposition of these basic motifs.Furthermore, there is a limit on the number ofshortcuts that can possibly be added in the seed be-fore it gets saturated, as all its vertices belong to atleast one triangle.
We show that at most 16 shortcutscan be added between two isolated triangles, at most6 between 2 triangles sharing 2 vertex and at most 2between 2 triangles sharing 2 vertices (see Fig.
5).5.2 The elementary lexical fieldsOnce the seed is isolated, we go on digging into itsstructure.
We focus on the arrangements of trianglesas they constitute the set of elementary concepts.We start by removing all shortcuts from the seedand convert it then to an undirected graph, in orderto get a homogeneous simplicial 2-complex.Let t be the graph operator which transforms agraph G into the intersection graph tG of its 2-simplices (i.e., triangles sharing an edge).
We applyt to the homogeneous simplicial 2-complex foundpreviously.
The result represents the links betweenthe basic semantic units of the seed.
We call seed-crux the giant WCC in the intersection graph.We enumerate the 8?380 maximal cliques of FAseed-crux and get the list of words composing eachDistance Acc ?
KSoriginal ?
0.404 301 74 0.522 422 97 0.899 89?
99 0.899 89Table 2: Accuracy, ?, and count(p < 0.05) for KSclique.
By removing the ones that are subsets of big-ger lists, we finally obtain 3?577 lists of words .These lists of words have a rather small and ho-mogeneous size (between 4 and 17) and 95% havea size comprised between 4 and 10.
More in-terestingly, they clearly define well-delimited lexi-cal fields.
We will show this through two experi-ments in the following sections.
A few examplesinclude (honest, trustworthy, reliable, responsible),(stress, problem, worry, frustration) and (data, pro-cess, computer, information).From a topological perspective, we deduce thatbunches of triangles (i.e., cliques of elementary con-cepts) span the seed in a homogeneous way.
Thesebunches form a set of cohesive lexical fields andconstitute essential bricks of semantic knowledge.5.3 Semantic similarity of the lexical fieldsIn order to quantify the relative meaning of wordsin the lexical fields of the seed-crux, we define thefollowing semantic similarity metric based on theWordnet WUP metric (Wu and Palmer, 1994) for agiven set of words L:S`(L) = 2?wi,wj?L,wi 6=wjSw(wi, wj)/(|L|(|L| ?
1))where Sw(wi, wj) = maxSk3wiandS`3wj{wup(Sk, S`)}and wup is the WUP semantic metric and Sk and S`are Wordnet synsets.The average value of S` for the set of cliques ofseed-crux is 0.6 whereas it is only 0.43 for randomlysampled set of words.
This suggests the correspond-ing lists of words are indeed semantically related.We will show the strength of this relation in the fol-lowing experiment with human raters.5.4 Human evaluation of the lexical fieldsTo validate our findings, we conducted an empiricalevaluation through human annotators.
Starting from676the 1?204 4-groups, we designed the following ex-periment: We corrupt the groups by exchanging oneof the 4 elements with a randomly chosen word at adistance from the group of 1, 2, and ?infinity?
(i.e.,any word of the whole core).
We presented 100 ran-dom samples for each of the 3 distances as well as100 unperturbed groups (original) to annotators atAmazon Mechanical Turk1, asking which word fitsthe group the least.
Intuitively, the closer the ran-domly chosen words get to the group, the closer thedistribution of the votes for each sample should beto the uniform distribution.
We collected 10 votesfor each of the 4 problems of 100 random samples.We calculated accuracy (i.e., the relative frequencyof correctly identified random words) for the 3 ran-dom confounder experiments and Fleiss?
?.
Fur-ther, we used the Kolmogorov-Smirnov (KS) test forhow uniform the label distribution is, reporting therelative frequency of samples that are significantly(p < 0.05) different from the uniform distribution.The results of this experiment are summarized in Ta-ble 2 and show clearly that the certainty about the?odd man out?
increases together with the distance.5.5 Error analysisIf we view our results as a resource for a downstreamtask, it is important to know about possible down-sides.
First, we note that there are words which arenot in a triangle and will thus be missing in the in-tersection graph.
This is an indication that the corre-sponding word is less well embedded contextually,so conversely, any prediction made about it from thedata may be less reliable.
Additionally, semanticleaps caused by generalized event knowledge maylead to lesser-connected groups such as (steel, pipe,lead, copper).
Jumps like these may or may not bedesired in a subsequent application.6 The Case of the EAT FA datasetThe Edinburgh Associative Thesaurus (EAT) (Kisset al 1973) is a large dataset of free associations.We extract the EAT FA seed-crux with the previ-ously described methods.We start by generating the initial graph (23?219vertices and 325?589 edges), then extract its core(7?754 vertices and 247?172 edges) and its seed1http://www.mturk.com(7?500 vertices and 238?677 edges).
It is interest-ing to notice at this stage that the EAT seed contains74% of the words belonging to the USF seed.
Af-ter generating the seed-crux which contains 63?363vertices, 6?825?731 edges, and 342?490 maximalcliques, we finally obtain 40?998 lists of words.These lists comprise between 4 and 233 words but80% of them have a relatively small size between 4and 20.
Although we find exceptions for this graph,most of the extracted lists again form well-delimitedlexical fields (e.g., (health, resort, spa, bath, salts) or(god, devil, angel, satan).Comparing the two association experiments, wesee that the local topologies are quite similar.
BothFA cores have a high density of connected trian-gles, whereas cycles in the DD graph tend to belonger and most triangles are isolated.
This can beattributed to the different ways in which DD and FAare obtained, the former being built rationally by fol-lowing a humanly-driven process and the latter re-flecting an implicit collective semantic knowledge.7 Related WorkA number of metrics like Latent Semantic Analy-sis (Deerwester et al 1990) and Word AssociationSpaces (Steyvers et al 2004) have been recentlydeveloped for quantifying the relative meaning ofwords.
As the topological properties of free associ-ation graphs reflect key aspects of semantic knowl-edge, we believe some graph theory metrics couldbe used efficiently to derive new ways of measuringsemantic similarity between words.Topological analysis of the Florida Word Associa-tions (FA) was started by (Steyvers and Tenenbaum,2005; Gravino et al 2012), who extracted globalstatistics.
We follow the basic methodology of thesestudies, but extend their approach.
First, we conductdeeper analyses by examining the neighborhood ofnodes and extracting the statistics of cycles.
Second,we compare the properties of FA and DD graphs.Word clustering based on graphs has been the sub-ject of various earlier studies.
Close to our workis (Widdows and Dorow, 2002).
These authors rec-ognize that nearest-neighbor-based clustering of co-occurrence give rise to semantic groups.
This type ofapproach has since been applied in various modifiedforms, e.g., by (Biemann, 2006) who performs label-677propagation based on randomized nearest neighbors,or Matsuo et al(2006) who perform greedy cluster-ing.
Hierarchical clustering algorithms (e.g., (Jonyeret al 2002; Manning et al 2008)) are related aswell, however, the key difference is that in hierarchi-cal clustering, the granularity of a cluster is difficultto determine.Dorow et al(2005) recognize that triangles formsemantically strongly cohesive groups and applyclustering coefficients for word sense disambigua-tion.
Their work focuses on undirected graphs ofcorpus co-occurrences whereas our work builds ondirected associations.
Building on this work, wetake finer topological graph structures into account,which is one of the main contributions in this paper.8 ConclusionThe cognitive process of discrete free association be-ing an epiphenomenon of our semantic memory atwork, the cumulative set of free associations of theUSF dataset can be viewed as the projection of a col-lective semantic memory.To analyze the semantic memory, we use the toolsof graph theory, and compare it also to dictionarygraphs.
In both cases, triangles play a crucial rolein the local topology and they form the set of ele-mentary concepts of the underlying graph.
We alsoshow that cohesive lexical fields (taking the formof cliques of concepts) constitute essential bricks ofmeaning, and span the core homogeneously at theglobal level; 89% of all words in the core belongto at least one triangle, and 77% belong to cliquesof triangles containing 4 words (i.e., pairs of trian-gles sharing an edge or forming tetrahedras).
As thewords of a graph of free associations acquire theirmeaning from the set of associations they are in-volved in (Deese, 1962), we go a step further byexamining the neighborhood of nodes and extractingthe statistics of cycles.
We further check through hu-man evaluation that the clustering is strongly relatedto meaning, and furthermore, the meaning becomesmeasurably more confused as one walks away froma cluster.-?
-?I call the pairs of triangles sharing an edgethe 2-clovers ;-)Comparing dictionaries to free association, wefind the free association graph being more conceptdriven, with words in small clusters being on thesame level of abstraction.
Moreover, we think thatgraphs of free associations could find interestingapplications for Word Sense Disambiguation (e.g.,(Heylighen, 2001; Agirre and Soroa, 2009)), andcould be used for detecting psychological disorders(e.g., depression, psychopathy) or whether someoneis lying (Hancock et al 2013; Kent and Rosanoff,1910).Finally, we believe that studying the dynamics ofgraphs of free associations may be of particular in-terest for observing the change in meaning of certainwords (Deese, 1967), or more generally to follow thecultural evolution arising among a social group.ReferencesEneko Agirre and Aitor Soroa.
2009.
Personalizingpagerank for word sense disambiguation.
In Proceed-ings of the 12th Conference of the European Chap-ter of the Association for Computational Linguistics,EACL ?09, pages 33?41, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Mark H Ashcraft and Gabriel A Radvansky.
2009.
Cog-nition.
Pearson Prentice Hall.Chris Biemann.
2006.
Chinese whispers: an efficientgraph clustering algorithm and its application to natu-ral language processing problems.
In Proceedings ofthe First Workshop on Graph Based Methods for Natu-ral Language Processing, TextGraphs-1, pages 73?80,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Be?la Bolloba?s.
2001.
Random graphs, volume 73 ofCambridge Studies in Advanced Mathematics.
Cam-bridge University Press, Cambridge, second edition.Etienne Brunet.
1974.
Le traitement des faitslinguistiques et stylistiques sur ordinateur.
Texted?application: Giraudoux, Statistique et Linguistique.David, J. y Martin, R.(eds.).
Paris: Klincksieck, pages105?137.Scott Deerwester, Susan T. Dumais, George W Furnas,Thomas K Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican society for information science, 41(6):391?407.James Deese.
1962.
On the structure of associativemeaning.
Psychological review, 69:161.James Deese.
1967.
Meaning and change of meaning.The American psychologist, 22(8):641.Beate Dorow, Dominic Widdows, Katerina Ling, Jean-Pierre Eckmann, Danilo Sergi, and Elisha Moses.6782005.
Using curvature and Markov clustering ingraphs for lexical acquisition and word sense discrim-ination.
In MEANING-2005, 2nd Workshop organizedby the MEANING Project, February 3rd-4th 2005,Trento, Italy.Jean-Pierre Eckmann and Elisha Moses.
2002.
Curva-ture of co-links uncovers hidden thematic layers inthe World Wide Web.
Proc.
Natl.
Acad.
Sci.
USA,99(9):5825?5829 (electronic).Brian Everitt, Sabine Landau, and Morven Leese.
2001.Cluster analysis.
4th Edition.
Arnold, London.Santo Fortunato.
2010.
Community Detection in Graphs.Physics Reports, 486(3):75?174.Pietro Gravino, Vito DP Servedio, Alain Barrat, and Vit-torio Loreto.
2012.
Complex structures and semanticsin free word association.
Advances in Complex Sys-tems, 15(03n04).Jeffrey T Hancock, Michael T Woodworth, and StephenPorter.
2013.
Hungry like the wolf: A word-patternanalysis of the language of psychopaths.
Legal andCriminological Psychology, 18(1):102?114.Ahmed Hassan and Dragomir Radev.
2010.
Identifyingtext polarity using random walks.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics, pages 395?403.
Association forComputational Linguistics.Francis Heylighen.
2001.
Mining associative meaningsfrom the web: from word disambiguation to the globalbrain.
In Proceedings of Trends in Special Language& Language Technology, pages 15?44.Donald B Johnson.
1975.
Finding all the elementarycircuits of a directed graph.
SIAM Journal on Com-puting, 4(1):77?84.Istvan Jonyer, Diane J Cook, and Lawrence B Holder.2002.
Graph-based hierarchical conceptual clustering.The Journal of Machine Learning Research, 2:19?43.Grace H Kent and Aaron J Rosanoff.
1910.
A study ofassociation in insanity.
American Journal of Insanity.George R Kiss, Christine Armstrong, Robert Milroy, andJames Piper.
1973.
An associative thesaurus of en-glish and its computer analysis.
The computer and lit-erary studies, pages 153?165.Andrea Lancichinetti and Santo Fortunato.
2009.
Com-munity detection algorithms: A comparative analysis.Physical review E, 80(5):056117.David Levary, Jean-Pierre Eckmann, Elisha Moses, andTsvi Tlusty.
2012.
Loops and self-reference in theconstruction of dictionaries.
Phys.
Rev.
X, 2:031018.Christopher D Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to information re-trieval, volume 1.
Cambridge University Press Cam-bridge.Yutaka Matsuo, Takeshi Sakaki, Ko?ki Uchiyama, andMitsuru Ishizuka.
2006.
Graph-based word cluster-ing using a web search engine.
In Proceedings ofthe 2006 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?06, pages 542?550,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Ken McRae and Kazunaga Matsuki.
2009.
People usetheir knowledge of common events to understand lan-guage, and do so as quickly as possible.
Language andlinguistics compass, 3(6):1417?1429.George Miller and Christiane Fellbaum.
1998.
WordNet:An Electronic Lexical database.
MIT Press, Cam-bridge, MA.George A Miller.
1995.
WordNet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.Douglas L Nelson, Cathy L McEvoy, and Thomas ASchreiber.
2004.
The University of South Florida freeassociation, rhyme, and word fragment norms.
Be-havior Research Methods, Instruments, & Computers,36(3):402?407.David S Palermo and James J Jenkins.
1964.
Word asso-ciation norms: Grade school through college.
Univer-sity of Minnesota Press.Olivier Picard, Alexandre Blondin-Masse?, Stevan Har-nad, Odile Marcotte, Guillaume Chicoisne, and Yas-sine Gargouri.
2009.
Hierarchies in dictionary defini-tion space.
In Annual Conference on Neural Informa-tion Processing Systems.Pascal Pons and Matthieu Latapy.
2006.
Computingcommunities in large networks using random walks.In Journal of Graph Algorithms and Applications,pages 284?293.
Springer.Christian Scheible.
2010.
Sentiment translation throughlexicon induction.
In Proceedings of the ACL 2010Student Research Workshop, pages 25?30, Uppsala,Sweden, July.
Association for Computational Linguis-tics.David R Shanks.
1995.
The psychology of associativelearning, volume 13.
Cambridge University Press.Mark Steyvers and Joshua B Tenenbaum.
2005.
Thelarge-scale structure of semantic networks: Statisticalanalyses and a model of semantic growth.
CognitiveScience, 29(1):41?78.Mark Steyvers, Richard M Shiffrin, and Douglas L Nel-son.
2004.
Word association spaces for predictingsemantic similarity effects in episodic memory.
Ex-perimental cognitive psychology and its applications:Festschrift in honor of Lyle Bourne, Walter Kintsch,and Thomas Landauer, pages 237?249.Robert Tarjan.
1972.
Depth-first search and linear graphalgorithms.
SIAM journal on computing, 1(2):146?160.679Dominic Widdows and Beate Dorow.
2002.
A graphmodel for unsupervised lexical acquisition.
In Pro-ceedings of the 19th international conference on Com-putational linguistics - Volume 1, COLING ?02, pages1?7, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Zhibiao Wu and Martha Palmer.
1994.
Verbs semanticsand lexical selection.
In Proceedings of the 32nd an-nual meeting on Association for Computational Lin-guistics, pages 133?138.
Association for Computa-tional Linguistics.680
