Combining Prosodic and Text Features for Segmentation of MandarinBroadcast NewsGina-Anne LevowUniversity of Chicagolevow@cs.uchicago.eduAbstractAutomatic topic segmentation, separation of a dis-course stream into its constituent stories or topics,is a necessary preprocessing step for applicationssuch as information retrieval, anaphora resolution,and summarization.
While significant progress hasbeen made in this area for text sources and for En-glish audio sources, little work has been done inautomatic, acoustic feature-based segmentation ofother languages.
In this paper, we consider exploit-ing both prosodic and text-based features for topicsegmentation of Mandarin Chinese.
As a tone lan-guage, Mandarin presents special challenges for ap-plicability of intonation-based techniques, since thepitch contour is also used to establish lexical iden-tity.
We demonstrate that intonational cues such asreduction in pitch and intensity at topic boundariesand increase in duration and pause still provide sig-nificant contrasts in Mandarin Chinese.
We builda decision tree classifier that, based only on wordand local context prosodic information without ref-erence to term similarity, cue phrase, or sentence-level information, achieves boundary classificationaccuracy of 84.6-95.6% on a balanced test set.
Wecontrast these results with classification using text-based features, exploiting both text similarity andn-gram cues, to achieve accuracies between 77-95.6%, if silence features are used.
Finally we in-tegrate prosody, text, and silence features using avoting strategy to combine decision tree classifiersfor each feature subset individually and all subsetsjointly.
This voted decision tree classifier yieldsan overall classification accuracy of 96.85%, with2.8% miss and 3.15% false alarm rates on a repre-sentative corpus sample, demonstrating synergisticcombination of prosodic and text features for topicsegmentation.1 IntroductionNatural spoken discourse is composed of a sequenceof utterances, not independently generated or ran-domly strung together, but rather organized accord-ing to basic structural principles.
This structure inturn guides the interpretation of individual utter-ances and the discourse as a whole.
Formal writ-ten discourse signals a hierarchical, tree-based dis-course structure explicitly by the division of the textinto chapters, sections, paragraphs, and sentences.This structure, in turn, identifies domains for in-terpretation; many systems for anaphora resolutionrely on some notion of locality (Grosz and Sidner,1986).
Similarly, this structure represents topicalorganization, and thus would be useful in informa-tion retrieval to select documents where the primarysections are on-topic, and, for summarization, to se-lect information covering the different aspects of thetopic.Unfortunately, spoken discourse does not includethe orthographic conventions that signal structuralorganization in written discourse.
Instead, one mustinfer the hierarchical structure of spoken discoursefrom other cues.
Prior research (Nakatani et al,1995; Swerts, 1997) has shown that human label-ers can more sharply, consistently, and confidentlyidentify discourse structure in a word-level tran-scription when an original audio recording is avail-able than they can on the basis of the transcribedtext alone.
This finding indicates that substantialadditional information about the structure of thediscourse is encoded in the acoustic-prosodic fea-tures of the utterance.
Given the often errorful tran-scriptions available for large speech corpora, wechoose to focus here on fully exploiting the prosodiccues to discourse structure present in the originalspeech.
We then compare the effectiveness of a pureprosodic classification to text-based and mixed textand prosodic based classification.In the current set of experiments, we concentrateon sequential segmentation of news broadcasts intoindividual stories.
While a richer hierarchical seg-mentation is ultimately desirable, sequential storysegmentation provides a natural starting point.
Thislevel of segmentation can also be most reliably per-formed by human labelers and thus can be consid-ered most robust, and segmented data sets are pub-licly available.Furthermore, we apply prosodic-based segmenta-tion to Mandarin Chinese, in addition to textual fea-tures.
Not only is the use of prosodic cues to topicsegmentation much less well-studied in general thanis the use of text cues, but the use of prosodic cueshas been largely limited to English and other Euro-pean languages.2 Related WorkMost prior research on automatic topic segmenta-tion has been applied to clean text only and thusused textual features.
Text-based segmentation ap-proaches have utilized term-based similarity mea-sures computed across candidate segments (Hearst,1994) and also discourse markers to identify dis-course structure (Marcu, 2000).The Topic Detection and Tracking (TDT) eval-uations focused on segmentation of both text andspeech sources.
This framework introduced newchallenges in dealing with errorful automatic tran-scriptions as well as new opportunities to exploitcues in the original speech.
The most successfulapproach (Beeferman et al, 1999) produced auto-matic segmentations that yielded retrieval resultsapproaching those with manual segmentations, us-ing text and silence features.
(Tur et al, 2001) ap-plied both a prosody-only and a mixed text-prosodymodel to segmentation of TDT English broadcastnews, with the best results combining text andprosodic features.
(Hirschberg and Nakatani, 1998)also examined automatic topic segmentation basedon prosodic cues, in the domain of English broad-cast news, while (Hirschberg et al, 2001) appliedsimilar cues to segmentation of voicemail.Work in discourse analysis (Nakatani et al, 1995;Swerts, 1997) in both English and Dutch has iden-tified features such as changes in pitch range, in-tensity, and speaking rate associated with seg-ment boundaries and with boundaries of differentstrengths.
They also demonstrated that access toacoustic cues improves the ease and quality of hu-man labeling.3 Prosody and MandarinIn this paper we focus on topic segmentation inMandarin Chinese broadcast news.
Mandarin Chi-nese is a tone language in which lexical identity isdetermined by a pitch contour - or tone - associ-ated with each syllable.
This additional use of pitchraises the question of the cross-linguistic applicabil-ity of the prosodic cues, especially pitch cues, iden-tified for non-tone languages.
Specifically, do wefind intonational cues in tone languages?
The factthat emphasis is marked intonationally by expansionof pitch range even in the presence of Mandarin lex-ical tone (Shen, 1989) suggests encouragingly thatprosodic, intonational cues to other aspects of in-formation structure might also prove robust in tonelanguages.4 Data SetWe utilize the Topic Detection and Tracking (TDT)3 (Wayne, 2000) collection Mandarin Chinesebroadcast news audio corpus as our data set.
Storysegmentation in Mandarin and English broadcastnews and newswire text was one of the TDT tasksand also an enabling technology for other retrievaltasks.
We use the segment boundaries provided withthe corpus as our gold standard labeling.
Our col-lection comprises 3014 news stories drawn from ap-proximately 113 hours over three months (October-December 1998) of news broadcasts from the Voiceof America (VOA) in Mandarin Chinese, with 800regions of other program material including musi-cal interludes and teasers.
The transcriptions spanapproximately 750,000 words.
Stories average ap-proximately 250 words in length to span a full story.No subtopic segmentation is performed.
The audiois stored in NIST Sphere format sampled at 16KHzwith 16-bit linear encoding.5 Prosodic FeaturesWe consider four main classes of prosodic featuresfor our analysis and classification: pitch, intensity,silence and duration.
Pitch, as represented by f0 inHertz was computed by the ?To pitch?
function ofthe Praat system (Boersma, 2001).
We selected thehighest ranked pitch candidate value in each voicedregion.
We then applied a 5-point median filter tosmooth out local instabilities in the signal such asvocal fry or small regions of spurious doubling orhalving.
Analogously, we computed the intensityin decibels for each 10ms frame with the Praat ?Tointensity?
function, followed by similar smoothing.For consistency and to allow comparability, wecompute all figures for word-based units, usingthe automatic speech recognition transcriptions pro-vided with the TDT Mandarin data.
The words areused to establish time spans for computing pitchor intensity mean or maximum values, to enabledurational normalization and the pairwise compar-isons reported below, and to identify silence or non-speech duration.It is well-established (Ross and Ostendorf, 1996)that for robust analysis pitch and intensity shouldbe normalized by speaker, since, for example, aver-age pitch is largely incomparable for male and fe-male speakers.
In the absence of speaker identifica-tion software, we approximate speaker normaliza-tion with story-based normalization, computed as , assuming one speaker per topic1.
For du-ration, we consider both absolute and normalizedword duration, where average word duration is usedas the mean in the calculation above.6 Prosodic AnalysisTo evaluate the potential applicability of prosodicfeatures to story segmentation in Mandarin Chinese,we performed some initial data analysis to deter-mine if words in story-final position differed fromthe same words used throughout the story in newsstories.
This lexical match allows direct pairwisecomparison.
We anticipated that since words inMandarin varied not only in phoneme sequence butalso in tone sequence, a direct comparison might beparticularly important to eliminate sources of vari-ability.
Features that differed significantly wouldform the basis of our classifier feature set.We found significant differences for each of thefeatures we considered.
Specifically, word dura-tion, normalized mean pitch, and normalized meanintensity all differed significantly for words in topic-final position relative to occurrences throughout thestory (paired t-test, two-tailed,    fffi , respectively) .
Word dura-tion increased, while both pitch and intensity de-creased.
A small side experiment using 15 hoursof English broadcast news from the TDT collectionshows similar trends, though the magnitude of thechange in intensity is smaller than that observed forthe Chinese.
Furthermore, comparison of averagepitch and average intensity for 1, 5, and 10 wordwindows at the beginning and end of news storiesfinds that pitch and intensity are both significantlyhigher (flffi  ) at the start of stories than at theend.These contrasts are consistent with, though insome cases stronger than, those identified for En-glish (Nakatani et al, 1995) and Dutch (Swerts,1997).
The relatively large size of the corpus en-hances the salience of these effects.
We find, im-portantly, that reduction in pitch as a signal oftopic finality is robust across the typological con-trast of tone and non-tone languages.
These findingsdemonstrate highly significant intonational effectseven in tone languages and suggest that prosodiccues may be robust across wide ranges of languages.1This is an imperfect approximation as some stories includeoff-site interviews, but seems a reasonable choice in the ab-sence of automatic speaker identification.7 Classification7.1 Prosodic Feature SetThe results above indicate that duration, pitch, andintensity should be useful for automatic prosody-based identification of topic boundaries.
To facili-tate cross-speaker comparisons, we use normalizedrepresentations of average pitch, average intensity,and word duration.
We also include absolute wordduration.
These features form a word-level context-independent feature set.Since segment boundaries and their cues existto contrastively signal the separation between top-ics, we augment these features with local context-dependent measures.
Specifically, we add featuresthat measure the change between the current wordand the next word.2 This contextualization addsfour contextual features: change in normalized av-erage pitch, change in normalized average intensity,change in normalized word duration, and durationof following silence or non-speech region.7.2 Text Feature SetIn addition to the prosodic features which are ourprimary interest, we also consider a set of featuresthat exploit textual similarity to identify segmentboundaries.
Motivated by text and topic similaritymeasures in the vector space model of informationretrieval (Salton, 1989), we compute a vector rep-resentation of the words in 50 word windows pre-ceding and following the current potential bound-ary position.
We compute the cosine similarity ofthese two vectors.
We employ a !#"%$'&)(*" weight-ing; each term is weighted by the product of itsfrequency in the window ( !#" ) and its inverse doc-ument frequency ( &)(*" ) as a measure of topicality.We also consider the same similarity measure com-puted across 30 word windows.
The final text sim-ilarity measure we consider is simple word overlap,counting the number of words that appear in both 50word windows defined above.
We did not removestopwords, and used the word-based units from theASR transcription directly as our term units.
We ex-pect that these measures will be minimized at topicboundaries where changes in topic are accompaniedby changes in topical terminology.Finally we identified a small set of word unigramfeatures occuring within a ten-word window imme-diately preceding or following a story boundary that2We have posed the task of boundary detection as the taskof finding segment-final words, so the technique incorporates asingle-word lookahead.
We could also repose the task as iden-tification of topic-initial words and avoid the lookahead to havea more on-line process.
This is an area for future research.Figure 1: Prosody-based decision tree classifier labeling words as segment-final or non-segment-finalwere indicative of such a boundary.3 These fea-tures include the Mandarin Chinese words for ?au-dience?, ?reporting?, and ?Voice of America.?
Weused a boolean feature for each such word corre-sponding to its presence or absence in the currentword?s environment in the classifier formulation.7.3 Classifier Training and TestingConfigurationWe employed Quinlan?s C4.5 (Quinlan, 1992) deci-sion tree classifier to provide a readily interpretableclassifier.
Now, the vast majority of word positionsin our collection are non-segment-final.
So, in or-der to focus training and test on segment boundaryidentification and to assess the discriminative capa-bility of the classifier, we downsample our corpusto produce a 50/50 split of segment-final and non-final words.
We train on 3500 segment-final words4and 3500 non-final words, not matched in any way,drawn randomly from the full corpus.
We test on asimilarly balanced test set of 500 instances.7.4 Classifier Evaluation7.4.1 Prosody-only classificationThe resulting classifier achieved 95.6% accuracy,with 2% missed boundaries and 7% false alarms.This effectiveness is a substantial improvement overthe sample baseline of 50%.
A portion of the deci-sion tree is reproduced in Figure 1.
Inspection of thetree indicates the key role of silence as well as theuse of both contextual and purely local features ofpitch, intensity, and duration.
The classifier relies3We used the ngram functionality to Boostexter (Schapireand Singer, 2000) to identify these units.4We excluded a small proportion of words for which thepitch tracker returned no results.on the theoretically and empirically grounded fea-tures of pitch, intensity, duration, and silence, whereit has been suggested that higher pitch and widerrange are associated with topic initiation and lowerpitch or narrower range is associated with topic fi-nality.We performed a set of contrastive experiments toexplore the impact of different lexical tones on clas-sification accuracy.
We grouped words based on thelexical tone of the initial syllable into high, rising,low, and falling.
We found no tone-based differ-ences in classification with all groups achieving 94-96% accuracy.
Since the magnitude of the differ-ence in pitch based on discourse position is compa-rable to that based on lexical tone identity, and theoverlap between pitch values in non-final and finalpositions is relatively small, we obtain consistent re-sults.7.4.2 Text and Silence-based ClassificationIn a comparable experiment, we employed only thetext similarity, text unigram, and silence durationfeatures to train and test the classifier.
These fea-tures similarly achieved a 95.6% overall classifi-cation accuracy, with 3.6% miss and 5.2% falsealarms.
Here the best classification accuracy wasachieved by the ! "
$ &*(*" weighted 50 word windowbased text similarity measure.
The text unigram fea-tures also contributed to overall classifier effective-ness.
A portion of the decision tree classifier usingtext-based features is reproduced in Figure 2.7.4.3 Combined Prosody and TextClassificationFinally we built a combined classifier integrating allprosodic, textual, and silence features.
This classi-fier yielded an accuracy of 96.4%, somewhat bet-ter effectiveness, still with more than twice as manyfalse alarms as missed detections.
The decision treeutilized all prosodic features.
!#" $ &)(" weighted co-sine similarity alone performed as well as any of theother text similarity or overlap measures.
The textunigram features also contributed to overall classi-fier effectiveness.
A portion of the decision treeclassifier using prosodic, textual, and silence fea-tures is reproduced in Figure 3.7.5 Feature ComparisonWe also performed a set of contrastive experimentswith different subsets of available features to as-sess the dependence on these features.5 We groupedfeatures into 5 sets: pitch, intensity, duration, si-lence, and text-similarity.
For each of the prosody-only, text-only, and combined prosody and text-based classifiers, we successively removed the fea-ture class at the root of the decision tree and re-trained with the remaining features (Table 1).We observe that although silence duration playsa very significant role in story boundary identifi-cation for all feature sets, the richer prosodic andmixed text-prosodic classifiers are much more ro-bust to the absence of silence information.
Furtherwe observe that intensity and then pitch play thenext most important roles in classification.
This be-havior can be explained by the observation that, likesilence or non-speech regions, pitch and intensitychanges provide sharp, local cues to topic finality orinitiation.
Thus the prosodic features provide somemeasure of redundancy for the silence feature.
Incontrast, the text similarity measures apply to rel-atively wide regions, comparing pairs of 50 wordwindows.7.6 Further Feature IntegrationFinally we considered effectiveness on a representa-tive test sampling of the full data set, rather than thedownsampled, balanced set, adding a proportionalnumber of unseen non-final words to the test set.We observed that although the overall classificationaccuracy was 95.6% and we were missing only 2%of the story boundaries, we produced a high levelof false alarms (4.4%), accounting for most of theobserved classification errors.
Given the large pre-dominance of non-boundary positions in the real-word distribution, we sought to better understandand reduce this false alarm rate, and hopefully re-duce the overall error rates.
At the same time, wehoped to avoid a dramatic increase in the miss rate.5For example, VOA Mandarin has been observed stylis-tically to make idiosyncratically large use of silence at storyboundaries.
(personal communication, James Allan).To explore this question, we considered the con-tribution of each of the three main feature types -prosody, text, and silence - and their combined ef-fects on false alarms.
We constructed independentfeature-set-specific decision tree classifiers for eachof the feature types and compared their independentclassifications to those of the integrated classifier.We found that while there was substantial agree-ment across the different feature-based classifiers inthe cases of correct classification, erroneous clas-sifications often occured when the assignment wasa minority decision.
Specifically, one-third of thefalse alarms were based on a minority assignemnt,where only the fully integrated classifier deemed theposition a boundary or where it agreed with onlyone of the feature-set-specific classifiers.Based on these observations, we completed ourmulti-feature integration by augmenting the deci-sion tree based classification with a voting mech-anism.
In this configuration, a boundary was onlyassigned in cases where the integrated classifieragreed with at least two of the feature-set-specificclassifiers.
This approach reduced the false alarmrate by one-third, to 3.15%, while the miss rate roseonly to 2.8%.
The overall accuracy on a representa-tive sample distribution reached 96.85%.8 Conclusion and Future WorkWe have demonstrated the utility of prosody-only,text-only, and mixed text-prosody features for auto-matic topic segmentation of Mandarin Chinese.
Wehave demonstrated the applicability of intonationalprosodic features, specifically pitch, intensity, pauseand duration, to the identification of topic bound-aries in a tone language.
We find highly signifi-cant decreases in pitch and intensity at topic finalpositions, and significant increases in word dura-tion.
Furthermore, these features in both local formand contextualized form provide the basis for an ef-fective decision tree classifier of boundary positionsthat does not use term similarity or cue phrase infor-mation, but only prosodic features.We observe similar effectiveness for all featuresets when all features are available, with slightlybetter classification accuracy for the text and hybridtext-prosody approach.
We further observe that theprosody-only and hybrid feature sets are much lesssensitive to the absence of individual features, and,in particular, to silence features, as pitch and inten-sity provide comparable sharp cues to the positionof topic boundaries.
These findings indicate thatprosodic features are robust cues to topic bound-aries, both with and without textual cues.Finally, we demonstrate the joint utility of the dif-Prosody-only Text   Silence Text   ProsodyAccuracy Pct.
Change Accuracy Pct.
Change Accuracy Pct.
ChangeAll 95.6% 0 95.6% 0 96.4% 0Silence 84.6% -11.5% 77.4% -19% 89.6% -6.9%Intensity 80.4% -15.9% 85.4% -11.4%Pitch 63.6% -33.4% 78.6% -18.5%Table 1: Reduction in classification accuracy with removal of features.
Each row is labeled with the featurethat is newly removed from the set of available features.ferent feature sets - prosodic, textual, and silence.The use of a simple voting mechanism exploits thedifferent contributions of each of the feature-set-specific classifiers in conjunction with the integratedclassifier.
This final combination allows a substan-tial reduction of the false alarm rate, reduction inthe overall error rate, and only a small increase inthe miss rate.
Further tuning of relative miss andfalse alarm rates is certainly possible, but should betied to a specific task application.There is still substantial work to be done.
Wewould like to integrate speaker identification fornormalization and speaker change detection.
Wealso plan to explore the integration of text andprosodic features for the identification of more fine-grained sub-topic structure, to provide more focusedunits for information retrieval, summarization, andanaphora resolution.
We also plan to explore the in-teraction of prosodic and textual features with cuesfrom other modalities, such as gaze and gesture, forrobust segmentation of varied multi-modal data.ReferencesD.
Beeferman, A. Berger, and J. Lafferty.
1999.Statistical models for text segmentation.
Ma-chine Learning, 34((1-3)):177?210.P.
Boersma.
2001.
Praat, a system for doing pho-netics by computer.
Glot International, 5(9?10):341?345.B.
Grosz and C. Sidner.
1986.
Attention, inten-tion, and the structure of discourse.
Computa-tional Linguistics, 12(3):175?204.M.
Hearst.
1994.
Multi-paragraph segmentation ofexpository text.
In Proceedings of the 32nd An-nual Meeting of the Association for Computa-tional Linguistics.Julia Hirschberg and Christine Nakatani.
1998.Acoustic indicators of topic segmentation.
InProceedings on ICSLP-98.J.
Hirschberg, M. Bacchiani, D. Hindel, P. Isenhour,A.
Rosenberg, L. Stark, L. Stead, S. Whittaker,and G. Zamchick.
2001.
Scanmail: Browsingand searching speech data by content.
In Pro-ceedings of EUROSPEECH 2001.D.
Marcu.
2000.
The Theory and Practice of Dis-course Parsing and Summarization.
MIT Press.C.
H. Nakatani, J. Hirschberg, and B. J. Grosz.1995.
Discourse structure in spoken language:Studies on speech corpora.
In Working Notes ofthe AAAI Spring Symposium on Empirical Meth-ods in Discourse Interpretation and Generation,pages 106?112.J.R.
Quinlan.
1992.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.K.
Ross and M. Ostendorf.
1996.
Prediction ofabstract labels for speech synthesis.
ComputerSpeech and Language, 10:155?185.Gerard Salton.
1989.
Automatic Text Processing:The Transformation, Analysis and Retrieval of In-formation by Computer.
Addison-Wesley, Read-ing, MA.Robert E. Schapire and Yoram Singer.
2000.
Boos-Texter: A boosting-based system for text catego-rization.
Machine Learning, 39((2/3)):135?168.X.-N. Shen.
1989.
The Prosody of Mandarin Chi-nese, volume 118 of University of CaliforniaPublications in Linguistics.
University of Cali-fornia Press.Marc Swerts.
1997.
Prosodic features at discourseboundaries of different strength.
Journal of theAcoustical Society of America, 101(1):514?521.G.
Tur, D. Hakkani-Tur, A. Stolcke, andE.
Shriberg.
2001.
Integrating prosodic andlexical cues for automatic topic segmentation.Computational Linguistics, 27(1):31?57.C.
Wayne.
2000.
Multilingual topic detection andtracking: Successful research enabled by cor-pora and evaluation.
In Language Resourcesand Evaluation Conference (LREC) 2000, pages1487?1494.Figure 2: Text-feature-based decision tree classifier labeling words as segment-final or non-segment-finalFigure 3: Prosody, text, and silence based decision tree classifier labeling words as segment-final or non-segment-final
