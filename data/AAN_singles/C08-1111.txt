Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 881?888Manchester, August 2008Emotion Classification Using Massive Examples Extracted from the WebRyoko TOKUHISA??
?Toyota Central R&D Labs., INC.Nagakute Aichi JAPANtokuhisa@mosk.tytlabs.co.jpKentaro INUI?
?Nara Institute of Science and TechnologyIkoma Nara JAPAN{ryoko-t,inui,matsu}@is.naist.jpYuji MATSUMOTO?AbstractIn this paper, we propose a data-orientedmethod for inferring the emotion of aspeaker conversing with a dialog systemfrom the semantic content of an utterance.We first fully automatically obtain a hugecollection of emotion-provoking event in-stances from the Web.
With Japanese cho-sen as a target language, about 1.3 millionemotion provoking event instances are ex-tracted using an emotion lexicon and lexi-cal patterns.
We then decompose the emo-tion classification task into two sub-steps:sentiment polarity classification (coarse-grained emotion classification), and emo-tion classification (fine-grained emotionclassification).
For each subtask, thecollection of emotion-proviking event in-stances is used as labelled examples totrain a classifier.
The results of our ex-periments indicate that our method signif-icantly outperforms the baseline method.We also find that compared with the single-step model, which applies the emotionclassifier directly to inputs, our two-stepmodel significantly reduces sentiment po-larity errors, which are considered fatal er-rors in real dialog applications.1 IntroductionPrevious research into human-computer interac-tion has mostly focused on task-oriented dialogs,where the goal is considered to be to achieve ac?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.given task as precisely and efficiently as possi-ble by exchanging information required for thetask through dialog (Allen et al, 1994, etc.
).More recent research (Foster, 2007; Tokuhisa andTerashima, 2006, etc.
), on the other hand, has beenproviding evidence for the importance of the af-fective or emotional aspect in a wider range of di-alogic contexts, which has been largely neglectedin the context of task-oriented dialogs.A dialog system may be expected to serve, forexample, as an active listening 1 partner of an el-derly user living alone who sometimes wishes tohave a chat.
In such a context, the dialog systemis expected to understand the user?s emotions andsympathize with the user.
For example, given anutterence I traveled far to get to the shop, but itwas closed from the user, if the system could inferthe user?s emotion behind it, it would know thatit would be appropriate to say That?s too bad orThat?s really disappointing.
It can be easily imag-ined that such affective behaviors of a dialog sys-tem would be beneficial not only for active listen-ing but also for a wide variety of dialog purposesincluding even task-oriented dialogs.To be capable of generating sympathetic re-sponses, a dialog system needs a computationalmodel that can infer the user?s emotion behindhis/her utterence.
There have been a range of stud-ies for building a model for classifying a user?semotions based on acoustic-prosodic features andfacial expressions (Pantic and Rothkrantz, 2004,etc.).
Such methods are, however, severely lim-ited in that they tend to work well only when theuser expresses his/her emotions by ?exaggerated?1Active listening is a specific communication skill, basedon the work of psychologist Carl Rogers, which involves giv-ing free and undivided attention to the speaker (Robertson,2005).881								 				 !"#$	%	&	'&		((	)&)$	(	(( 		'	)	)(	 '(		%	&	'&	$&(	&	*	%	&	'&	+,,-..-.&&Figure 1: Overview of our approach to emotion classificationprosodic or facial expressions.
Furthermore, whatis required in generating sympathetic responses isthe identification of the user?s emotion in a finergrain-size.
For example, in contrast to the aboveexample of disappointing, one may expect the re-sponse to My pet parrot died yesterday should beThat?s really sad, wheras the response to I mayhave forgotten to lock my house should be You?reworried about that.In this paper, we address the above issue ofemotion classification in the context of human-computer dialog, and demonstrate that massive ex-amples of emotion-provoking events can be ex-tracted from the Web with a reasonable accuracyand those examples can be used to build a seman-tic content-based model for fine-grained emotionclassification.2 Related WorkRecently, several studies have reported about di-alog systems that are capable of classifying emo-tions in a human-computer dialog (Batliner et al,2004; Ang et al, 2002; Litman and Forbes-Riley,2004; Rotaru et al, 2005).
ITSPOKE is a tutoringdialog system, that can recognize the user?s emo-tion using acoustic-prosodic features and lexicalfeatures.
However, the emotion classes are limitedto Uncertain and Non-Uncertain because the pur-pose of ITSPOKE is to recognize the user?s prob-lem or discomfort in a tutoring dialog.
Our goal,on the other hand, is to classify the user?s emotionsinto more fine-grained emotion classes.In a more general research context, while quitea few studies have been presented about opinionmining and sentiment analysis (Liu, 2006), re-search into fine-grained emotion classification hasemerged only recently.
There are two approachescommonly used in emotion classification: a rule-based approach and a statistical approach.
Ma-sum et al (2007) and Chaumartin (2007) pro-pose a rule-based approach to emotion classifica-tion.
Chaumartin has developed a linguistic rule-based system, which classifies the emotions engen-dered by news headlines using the WordNet, Sen-tiWordNet, and WordNet-Affect lexical resources.The system detects the sentiment polarity for eachword in a news headline based on linguistic re-sources, and then attempts emotion classificationby using rules based on its knowledge of sen-tence structures.
The recall of this system is low,however, because of the limited coverage of thelexical resources.
Regarding the statistical ap-proach, Kozareva et al (2007) apply the theory of(Hatzivassiloglou and McKeown, 1997) and (Tur-ney, 2002) to emotion classification and proposea method based on the co-occurrence distributionover content words and six emotion words (e.g.joy, fear).
For example, birthday appears more of-ten with joy, while war appears more often withfear.
However, the accuracy achieved by theirmethod is not practical in applications assumedin this paper.
As we demonstrate in Section 4,our method significantly outperforms Kozareva?smethod.3 Emotion Classification3.1 The basic ideaWe consider the task of emotion classification asa classification problem where a given input sen-tence (a user?s utterance) is to be classified eitherinto such 10 emotion classes as given later in Ta-ble 1 or as ?neutral?
if no emotion is involved in theinput.
Since it is a classification problem, the taskshould be approached straightforwardly in a vari-882Table 1: Distribution of the emotion expressions and examplesSentiment 10 Emotion Emotion lexicon (349 Japanese emotion words)Polarity Classes Total Exampleshappiness 90 ???
(happy)???
(joyful)???
(glad)???
(glad)Positive pleasantness 7 ???
(pleasant)????
(enjoy)?????
(can enjoy)relief 5 ??
(relief)????
(relief)fear 22 ??
(fear)???
(fear)?????
(frightening)sadness 21 ???
(sad)????
(sad)????
(feel sad)disappointment 15 ????
(lose heart)?????
(drop one?s head)Negative unpleasantness 109 ?
(disgust)????
(dislike)???
(dislike)loneliness 15 ???
(lonely)????
(lonely)?????
(lonely)anxiety 17 ??
(anxiety)???
(anxiety)?????
(worry)anger 48 ?????
(angry)????
(get angry)???
(angry)ety of machine learning-based methods if a suffi-cient number of labelled examples were available.Our basic idea is to learn what emotion is typicallyprovoked in what situation, from massive exam-ples that can be collected from the Web.
The devel-opment of this approach and its subsequent imple-mentation has forced us to consider the followingtwo issues.First, we have to consider the quantity and ac-curacy of emotion-provoking examples to be col-lected.
The process we use to collect emotion-provoking examples is illustrated in the upper halfof Figure 1.
For example, from the sentence I wasdisappointed because the shop was closed and I?dI traveled a long way to get there, pulled from theWeb, we learn that the clause the shop was closedand I?d traveled a long way to get there is an ex-ample of an event that provokes ?disappointment?.In this paper, we refer to such an example as anemotion-provoking event and a collection of event-provoking events as an emotion-provoking eventcorpus (an EP corpus).
Details are described inSection 3.2.Second, assuming that an EP corpus can be ob-tained, the next issue is how to use it for ouremotion classification task.
We propose a methodwhereby an input utterance (sentence) is classi-fied in two steps, sentiment polarity classificationfollowed by fine-grained emotion classification asshown in the lower half of Figure 1.
Details aregiven in Sections 3.3 and 3.4.3.2 Building an EP corpusWe used ten emotions happiness, pleasantness,relief, fear, sadness, disappointment, unpleasant-ness, loneliness, anxiety, anger in our emotionclassification experiment.
First, we built a hand-crafted lexicon of emotion words classified intothe ten emotions.
From the Japanese EvaluationExpression Dictionary created by Kobayashi etal.
(2005), we identified 349 emotion words based 			&	'&&	"Figure 2: An example of a lexico-syntactic patternTable 2: Number of emotion-provoking events10 Emotions EP event 10 Emotions EP eventhappiness 387,275 disappoint- 106,284mentpleasantness 209,682 unpleasantness 396,002relief 46,228 loneliness 26,493fear 49,516 anxiety 45,018sadness 31,369 anger 8,478on the definition of emotion words proposed byTeramura (1982).
The distribution is shown in Ta-ble 1 with major examples.We then went on to find sentences in the Webcorpus that possibly contain emotion-provokingevents.
A subordinate clause was extracted as anemotion-provoking event instance if (a) it was sub-ordinated to a matrix clause headed by an emo-tion word and (b) the relation between the sub-ordinate and matrix clauses is marked by one ofthe following eight connectives: ?
?, ?
?, ?
?,?, ?
?, ?
?, ??
?, ???.
An example is givenin Figure 2.
In the sentence ?????????????????
(I was disappointed that it suddenlystarted raining)?, the subordinate clause ??????????
(it suddenly started raining)?
mod-ifies ??????
(I was disappointed)?
with theconnective ???
(that)?.
In this case, therefore,the event mention ??????????
(it suddenlystarted raining)?
is learned as an event instancethat provokes?disappointment?.Applying the emotion lexicon and the lexicalpatterns to the Japanese Web corpus (Kawaharaand Kurohashi, 2006), which contains 500 millionsentences, we were able to collect about 1.3 mil-lion events as causes of emotion.
The distributionis shown in Table 2.Tables 3 and 4 show the results of our evalua-883Table 4: Examples from in the EP corpusEP-Corpus Result of evaluationEmotion-provoking Event Emotion word 10 Emotions (P/N) Polarity Emotion??????
(A flower died quickly) ???
(diappointed) ?disappointment(N)?
Correct Correct????
(There are a lot of enemies) ???
(lose interest) ?unpleasantness(N)?
Correct Context-dep.????????
(There is a lot ofChinese cabbage)???
(happy) ?happiness(P)?
Context-dep.
Context-dep.?????????
(I would like todrink orange juice)???
(terrible) ?unpleasantness(N)?
Error ErrorTable 3: Correctness of samples from the EP cor-pusPolarity EmotionCorrect 1140 (57.0%) 988 (49.4%)Context-dep.
678 (33.9%) 489 (24.5%)Error 182 (9.1%) 523 (26.2%)tion for the resultant EP corpus.
One annotator,who was not the developer of the EP corpus, eval-uated 2000 randomly chosen events.
The ?Polar-ity?
column in Table 3 shows the results of evaluat-ing whether the sentiment polarity of each event iscorrectly labelled, whereas the?Emotion?
columnshows the correctness at the level of the 10 emo-tion classes.
The correctness of each example wasevaluated as exemplified in Table 4.
Correct indi-cates a correct example, Contex-dep.
indicates acontext-dependent example, and Error is an errorexample.
For example, in the case of There are alot of enemies in Table 4, the ?Polarity?
is Correctbecause it represents a negative emotion.
How-ever, its emotion class?unpleasantness?
is judgedContext-dep.As Table 3 shows, the Sentiment Polarity is cor-rect in 57.0% of cases and partially correct (Cor-rect + Context-dep.)
in 90.9% of cases.
On theother hand, the Emotion is correct in only 49.4%of cases and partially correct in 73.9% of cases.These figures may not seem very impressive.
Asfar as its impact on the emotion classification accu-racy is concerned, however, the use of our EP cor-pus, which requires no supervision, makes remark-able improvements upon Kozareva et al (2007)?sunsupervised method as we show later.3.3 Sentiment Polarity ClassificationGiven the large collection of emotion-labelled ex-amples, it may seem straightforward to develop atrainable model for emotion classification.
Beforemoving on to emotion classification, however, itshould be noted that a user?s input utterance maynot involve any emotion.
For example, if the usergives an utterance I have a lunch at the school cafe-teria every day, it is not appropriate for the systemto make any sympathetic response.
In such a case,the user?s input should be classified as ?neutral?.The classification between emotion-involvedand neutral is not necessarily a simple problem,however, because we have not found yet any prac-tical method for collecting training examples of theclass?neutral?.
We cannot rely on the analogy tothe pattern-based method we have adopted to col-lect emotion-provoking events ?
there seems noreliable lexico-syntactic pattern for extracting neu-tral examples.
Alternatively, if the majority of thesentences on the Web were neutral, one would sim-ply use a set of randomly sampled sentences as la-belled data for ?neutral?.
This strategy, however,does not work because neutral sentences are notthe majority in real Web texts.
As an attempt, wecollected 1000 sentences randomly from the Weband investigated their distribution of sentiment po-larity.
The results, shown in Table 5, revealed thatthe ratio of neutral events was unexpectedly low.These results indicate the difficulty of collectingneutral events from Web documents.Taking this problem into account, we adopt atwo-step approach, where we first classify a giveninput into three sentiment polarity classes, eitherpositive, negative or neutral, and then classify onlythose judged positive or negative into our 10 fine-grained emotion classes.
In the first step, i.e.
sen-timent polarity classification, we use only the pos-itive and negative examples stored in the EP cor-pus and assume sentence to be neutral if the out-put of the classification model is near the deci-sion boundary.
There are additional advantagesin this approach.
First, it is generally known thatperforming fine-grained classification after coarseclassification often provides good results particu-larly when the number of the classes is large.
Sec-ond, in the context of dialog, a misunderstandingthe user?s emotion at the sentiment polarity levelwould be a disaster.
Imagine that the system saysYou must be happy when the user in fact feels sad.As we show in Section 4.2, such fatal errors can bereduced by taking the two-step approach.884Table 5: Distribution of the Sentiment polarity ofsentences randomly sampled from the WebSentiment Polarity Number Ratiopositive 650 65.0%negative 153 15.3%neutral 117 11.7%Context-dep.
80 8.0%Positivechild educationPositivecostNegative SUBJECTincreaseFigure 3: An example of a word-polarity latticeVarious methods have already been proposed forsentiment polarity classification, ranging from theuse of co-occurrence with typical positive and neg-ative words (Turney, 2002) to bag of words (Panget al, 2002) and dependency structure (Kudo andMatsumoto, 2004).
Our sentiment polarity clas-sification model is trained with SVMs (Vapnik,1995), and the features are {1-gram, 2-gram, 3-gram} of words and the sentiment polarity of thewords themselves.
Figure 3 illustrates how the sen-tence ?????????????
(The cost of educat-ing my child increases)?
is encoded to a featurevector.
Here we assume the sentiment polarity ofthe ???
(child)?
and ???
(education)?
are pos-itive, while the ???
(cost)?
is negative.
Thesepolarity values are represented in parallel with thecorresponding words, as shown in Figure 3.
Byexpanding {1-gram, 2-gram, 3-gram} in this lat-tice representation, the following list of featuresare extracted: ??
(child), Positive, ??
(child)-?
(of), Positive-?
(of),??
(child)-?
(of)-??
(ed-ucation), etc..
The polarity value of each word isdefined in our sentiment polarity dictionary, whichincludes 1880 positive words and 2490 negativewords.
To create this dictionary, one annotatoridentified positive and negative words from the 50thousand most frequent words sampled from theWeb.
Table 6 shows some examples.3.4 Emotion ClassificationFor fine-grained emotion classification, we pro-pose a k-nearest-neighbor approach (kNN) usingthe EP corpus.Given an input utterance, the kNN model re-trieves k-most similar labelled examples from theEP corpus.
Given the input The restaurant wasvery far but it was closed as Figure 1, for exam-ple, the kNN model finds similar labelled exam-ples, say, labelled example {the shop was closedand I?d traveled far to get there} in the EP corpus.Table 6: Examples of positive and negative wordsP ??
(child)????
(summer vacation)????
(useful)?????
(succeed)N ??
(cost)????
(difficult)????
(difficult)?????
(failure)Ranking of similar eventsrank event emotion similarity1.2.2.4.5.
{event1} <disappointment> 1.2.3.
{event2} <unpleasantness>{event3} <loneliness>          0.70{event4} <loneliness>          0.670.750.70{event5} <loneliness>          0.63Ranking of emotionrank emotion score<loneliness><unpleasantness><disappointment>2.00.750.70votingFigure 4: Emotion Classification by kNN (k=5)For the similarity measure, we use cosine similar-ity between bag-of-words vectors; sim(I,EP ) =I?EP|I||EP |for input sentence I and an emotion-provoking event EP in the EP corpus.
The scoreof each class is given by the sum of its similarityscores.An example is presented in Figure 4.
The emo-tion of the most similar event is ?disappointment?,that of the second-most similar event is?unpleasantness?
tied with ?loneliness?.
Af-ter calculating the sum for each emotion, thesystem outputs ?loneliness?
as the emotion for theinput I because the score for ?loneliness?
is thehighest.4 Experiments4.1 Sentiment polarity classificationWe conducted experiments on sentiment polarityclassification using the following two test sets:TestSet1: The first test set was a set of utteranceswhich 6 subject speakers produced interact-ing with our prototype dialog system.
Thisdata include 31 positive utterances, 34 nega-tive utterances, and 25 neutral utterances.TestSet2: For the second test set, we used the1140 samples that were judged Correct withrespect to sentiment polarity in Table 3.491 samples (43.1%) were positive and 649(56.9%) were negative.
We then added 501neutral sentences newly sampled from theWeb.
These samples are disjoint from the EPcorpus used for training classifiers.For each test set, we tested our sentimentpolarity classifier in both the two-class (posi-tive/negative) setting, where only positive or neg-ative test samples were used, and the three-class(positive/negative/neutral) setting.
The perfor-mance was evaluated in F-measure.885Table 7: F-values of sentiment polarity classifica-tion (positive/negative)TestSet1 TestSet2Pos Neg Pos NegWord 0.839 0.853 0.794 0.842Word + Polarity 0.833 0.857 0.793 0.841Table 8: F-values of sentiment polarity classifica-tion (positive/negative/neutral)TestSet1 TestSet2Pos Neg Pos NegWord 0.743 0.758 0.610 0.742Word + Polarity 0.758 0.769 0.610 0.742Table 7 shows the results for the two-classsetting, whereas Table 8 shows the results forthe three-class.
?Word?
denotes the modeltrained with only word n-gram features, whereas?Word+Polarity?
denotes the model trained withn-gram features extracted from a word-polarity lat-tice (see Figure 3).The results shown in Table 7 indicate that boththe ?Word?
and ?Word+Polarity?
models are ca-pable of separating positive samples from negativeones at a high level of accuracy.
This is an im-portant finding, given the degree of the correctnessof our EP corpus.
As we have shown in Table 3,only 57% of samples in our EP corpus are ?ex-actly?
correct in terms of sentiment polarity.
Thefigures in Table 7 indicate that context-dependentsamples are also useful for training a classifier.
Ta-ble 7 also indicates that no significant difference isfound between the ?Word?
and ?Word+Polarity?models.
In fact, we also examined another modelwhich used dependency-structure information aswell; however, no significant gain was achieved.From these results, we speculate that, as far as thetwo-class sentiment polarity problem is concerned,word n-gram features might be sufficient if a verylarge set of labelled data are available.On the other hand, Table 8 indicates that thethree-class problem is much harder than the two-class problem.
Specifically, positive sentencestend to be classified as neutral.
This method hasto be improved in future models.4.2 Emotion classificationFor fine-grained emotion classification, we usedthe following three test sets:TestSet1 (2p, best): Two annotators were askedto annotate each positive or negative sen-tence in TestSet1 with one of the 10 emotionclassess.
The annotators chose only one emo-tion class even if they thought several emo-tions would fit a sentence.
Some examples areshown in Table 9.
The inter-annotator agree-ment is ?=0.76 in the kappa statistic (Cohen,1960).
For sentences annotated with two dif-ferent labels (i.e.
in the cases where the twoannotators disagreed with), both labels wereconsidered correct in the experiments ?
amodel?s answer was considered correct if itwas identical with either of the two labels.TestSet1 (1p, acceptable): One of the above twoannotators was asked to annotate each posi-tive or negative sentence in TestSet1 with allthe emotions involved in it.
The number ofemotions for a positive sentence was 1.48 onaverage, and 2.47 for negative sentences.
Ta-ble 10 lists some examples.
In the experi-ments, a model?s answer was considered cor-rect if it was identical with one of the labelledclasses.TestSet2: For TestSet2, we used the results of ourjudgments on the correctness for estimatingthe quality of the EP corpus described in Sec-tion 3.2.In the experiments, the following two modelswere compared:Baseline: The baseline model simulates themethod proposed by (Kozareva et al,2007).
Given an input sentence, theirmodel first estimates the pointwise mu-tual information (PMI) between eachcontent word cwjincluded in the sen-tence and emotion expression e ?
{anger, disgust, fear, joy, sudness, surprise}by PMI(e, cw) = log hits(e,cw)hits(e)hits(cw), wherehits(x) is a hit count of word(s) x on aWeb search engine.
The model then cal-culates the score of each emotion class Eiby summing the PMI scores between eachcontent word cwjin the input and emotionexpression eicorresponding to that emotionclass: score(Ei) =?jPMI(ei, cwj).Finally, the model chooses the best scoredemotion class as an output.
For our experi-ments, we selected the following 10 emotionexpressions:???
(happy),???
(pleased),??
(re-lieved), ??
(affraid), ???
(sad), ??(disappointed),?
(hate),???
(lonely),??
(anxious),?????
(angry)For hit counts, we used the Google search en-gine.886Table 9: Examples of TestSet1 (2p, best)Annotator A Annotator B????????????????
(I got a Christmas present) ?happiness?
?happiness???????????
(I?m going to go to my friend?s house ) ?pleasantness?
?pleasantness?????????????????
(It rained suddenly when I went tosee the cherry blossoms)?sadness?
?sadness????????????
(My car can?t move because of the traffic jam) ?unpleasantness?
?anger?Table 10: Examples of TestSet1 (1p, acceptable)Annotator A????????????????
(I got a Christmas present) ?happiness???????????
(I?m going to go to my friend?s house ) ?pleasantness?, ?happiness?????????????????
(It rained suddenly when I went to see thecherry blossoms)?anger?, ?sad?, ?unpleasantness?,?disappointment????????????
(My car can?t move because of the traffic jam) ?unpleasantness?, ?anger?????????????????????
??????????
?????????????????????????
??????????
????????????????????????
??????????
??????????????????????????????????????????????
????
????
?????
????
????
???????????????????????????
???????????????????????????????????????????????????????????????????
???????????????????????
???????
?Figure 5: Results of emotion classificationk-NN: We tested the 1-NN, 3-NN and 10-NNmodels.
In each model, we examined asingle-step emotion classification and two-step emotion classification.
In the formermethod, the kNN model retrieves k-most sim-ilar examples from the all of the EP corpus.
Inthe latter method, when the sentiment polar-ity of the input utterance has obtained by thesentiment polarity classifier, the kNN modelretrieves similar examples from only the ex-amples whose sentiment polarity are the sameas the input utterance in the EP corpus.The results are shown in Figure 5.
?Emo-tion Classification?
denotes the single-step mod-els, whereas ?Sentiment Polarity + Emotion Clas-sification?
denotes the two-step models.An important observation from Figure 5 is thatour models remarkably outperformed the base-line.
Apparently, an important motivation behindKozareva et al (2007)?s method is that it doesnot require any manual supervion.
However, ourmodels, which rely on emotion-provoking eventinstances, are also totally unsupervised ?
no su-pervision is required to collect emotion-provokingevent instances.
Given this commonality betweenthe two methods, the superiority of our method inaccuracy can be considered as a crucial advantage.Regarding the issue of single-step vs. two-step,Figure 5 indicates that the two-step models tendedto outperform the single-step models for all the testset.
A paired t-test for TestSet2, however, did notreach significance 2.
So we next examined this is-sue in further detail.As argued in Section 3.3, in the context ofhuman-computer dialog, a misunderstanding ofthe user?s emotion at the level of sentiment polar-ity would lead to a serious problem, which we calla fatal error.
On the other hand, misclassifying acase of?happiness?
as, for example,?pleasantness?may well be tolerable.
Table 11 shows the ratioof fatal errors for each model.
For TestSet2, thesingle-step 10-NN model made fatal errors in 30%of cases, while the two-step 10-NN model in only17%.
This improvement is statistically significant(p<0.01).5 ConclusionIn this paper, we have addressed the issue of emo-tion classification assuming its potential applica-tions to be human-computer dialog system includ-ing active-listening dialog.
We first automaticallycollected a huge collection, as many as 1.3M, ofemotion-provoking event instances from the Web.We then decomposed the emotion classificationtask into two sub-steps: sentiment polarity clas-sification and emotion classification.
In sentimentpolarity classification, we used the EP-corpus astraining data.
The results of the polarity classifi-cation experiment showed that word n-gram fea-tures alone are more or less sufficient to classifypositive and negative sentences when a very largeamount of training data is available.
In the emo-tion classification experiments, on the other hand,2The data size of TestSet1 was not sufficient for statisticalsignificance test887Table 11: Fatal error rate in emotion classification experimentsBaseline Emotion Classification Sentiment Polarity1-NN 3-NN 10-NN + Emotion ClassificationTestSet1 49.2% 29.2% 26.2% 24.6% 15.4%TestSet2 41.5% 37.6% 32.8% 30.0% 17.0%we adopted the k-nearest-neighbor (kNN) method.The results of the experiments showed that ourmethod significantly outperformed the baselinemethod.
The results also showed that our two-step emotion classification was effective for fine-grained emotion classification.
Specifically, fatalerrors were significantly reduced with sentimentpolarity classification before fine-grained emotionclassification.For future work, we first need to examine othermachine learning methods to see their advantagesand disadvantages in our task.
We also need anextensive improvement in identifying neutral sen-tences.
Finally, we are planning to apply our modelto the active-listening dialog system that our grouphas been developing and investigate its effects onthe user?s behavior.ReferencesAllen, James F., Lenhart K. Schubert, George Fergu-son, Peter Heeman, Chung Hee Hwang, TsuneakiKato, Marc Light, Nathaniel G. Martin, Bradford W.Miller, Massimo Poesio, and David R. Traum.
1994.The TRAINS Project: A case study in building aconversational planning agent.
Journal of Experi-mental and Theoretical AI (JETAI).Ang, Jeremy, Rajdip Dhillon, Ashley Krupski, Eliza-beth Shriberg, and Andreas Stolcke.
2002.
Prosody-Based Automatic Detection Of Annoyance AndFrustration In Human-Computer Dialog.
SpokenLanguage Processing, pages 2037?2040.Batliner, A., K. Fischer, R. Huber, J. Spilker, andE.
Noth.
2004.
How to find trouble in communi-cation.
Speech Communication, 40(1-2):117?143.Chaumartin, Francois-Regis.
2007.
A knowledge-based system for headline sentiment tagging.
In Pro-ceedings of the 4th International Workshop on Se-mantic Evaluations.Cohen, Jacob.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement 20, pages 37?46.Foster, Mary Ellen.
2007.
Enhancing Human-Computer Interaction with Embodied Conversa-tional Agents.
Lecture Notes in Computer Science,4555:828?837.Hatzivassiloglou, Vasileios and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
Proceedings of the 35th Annual Meeting of theAssociation for Computational Linguistics.Kawahara, Daisuke and Sadao Kurohashi.
2006.Case Frame Compilation from the Web using High-Performance Computing.
In Proceedings of the 5thInternational Conference on Language Resourcesand Evaluation.Kobayashi, Nozomi, Ryu Iida, Kentaro Inui, and YujiMatsumoto.
2005.
Collecting Evaluative Expres-sions for Opinion Extraction.
Lecture Notes in Arti-ficial Intelligence, 3248.Kozareva, Zornitsa, Borja Navarro, Sonia Vazquez, andAndres Nibtoyo.
2007.
UA-ZBSA: A HeadlineEmotion Classification through Web Information.
InProceedings of the 4th International Workshop onSemantic Evaluations.Kudo, Taku and Yuji Matsumoto.
2004.
A Boost-ing Algorithm for Classification of Semi-StructuredText.
In Proceedings of the EMNLP.Litman, Diane J. and Kate Forbes-Riley.
2004.
Predict-ing Student Emotions in Computer-Human TutoringDialogues.
Proceedings of the 42nd Annual Meetingof the Association for Computational Linguistics.Liu, Bing.
2006.
Web Data Mining.
Springer, pages411?440.Masum, Shaikh Mostafa AI, Helmut Prendinger, andMitsuru Ishizuka.
2007.
Emotion Sensitive NewsAgent: An Approach Towards User Centric Emo-tion Sensing from the News.
In Proceedings of theIEEE/WIC/ACM International Conference on WebIntelligence.Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification usingmachine learning techniques.
Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 76?86.Pantic, Maja and Leon J. M. Rothkrantz.
2004.
Fa-cial Action Recognition for Facial Expression Anal-ysis From Static Face Images.
IEEE Transactions onSMC-B, 34(3):1449?1461.Robertson, Kathryn.
2005.
Active listening: morethan just paying attention.
Aust Fam Physician,34(12):1053?1055.Rotaru, Mihai, Diane J. Litman, and Katherine Forbes-Riley.
2005.
Interactions between Speech Recog-nition Problems and User Emotions.
Proceedings9th European Conference on Speech Communicationand Technology.Teramura, Hideo.
1982.
Japanese Syntax and Mean-ing.
Kurosio Publishers (in Japanese).Tokuhisa, Ryoko and Ryuta Terashima.
2006.
Re-lationship between Utterance and ?Enthusiasm?
inNon-Task-Orieinted Conversational Dialogue.
InProceedings of the 7th SIGdial Workshop on Dis-course and Dialogue.Turney, P.D.
2002.
Thumbs up?
thumbs down?
seman-tic orientation applied to unsupervised classificationof reviews.
Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics,pages 417?424.Vapnik, Vladimir N. 1995.
The Nature of StatisticalLearning Theory.
Springer.888
