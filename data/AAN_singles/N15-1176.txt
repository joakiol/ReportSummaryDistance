Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1527?1536,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsLearning Translation Models fromMonolingual Continuous RepresentationsKai Zhao?Graduate Center, CUNYNew York, NY 10016, USAkzhao.hf@gmail.comHany HassanMicrosoft ResearchRedmond, WA 98502, USAhanyh@microsoft.comMichael Auli?Facebook AI ResearchMenlo Park, CA 94025, USAmichaelauli@fb.comAbstractTranslation models often fail to generate goodtranslations for infrequent words or phrases.Previous work attacked this problem by in-ducing new translation rules from monolin-gual data with a semi-supervised algorithm.However, this approach does not scale verywell since it is very computationally expen-sive to generate new translation rules for onlya few thousand sentences.
We propose a muchfaster and simpler method that directly hallu-cinates translation rules for infrequent phrasesbased on phrases with similar continuous rep-resentations for which a translation is known.To speed up the retrieval of similar phrases,we investigate approximated nearest neighborsearch with redundant bit vectors which wefind to be three times faster and significantlymore accurate than locality sensitive hashing.Our approach of learning new translation rulesimproves a phrase-based baseline by up to1.6 BLEU on Arabic-English translation, it isthree-orders of magnitudes faster than existingsemi-supervised methods and 0.5 BLEU moreaccurate.1 IntroductionStatistical translation models (Koehn et al 2003,Chiang et al 2005) are trained with bilingual dataand a simple solution to improve accuracy is to trainon more data.
However, for many language pairswe only have a very limited amount of bilingualdata and even when dealing with resource-rich lan-guages, we still often perform poorly when dealingwith rare words or phrases.On the other hand, there is plenty of monolingualdata and previous work has investigated its use inlearning translation models (Rapp, 1995; Callison-Burch et al, 2006; Haghighi et al, 2008; Saluja et*The entirety of this work was conducted while at Mi-crosoft Research.al., 2014).
However, most methods rely on statisticsthat are computationally expensive.
As a concreteexample, the graph propagation algorithm of Salujaet al (2014) relies on pair-wise mutual informationstatistics between any pair of phrases in the monolin-gual corpus that is very expensive to compute, evenfor moderately sized corpora.In this paper, we study the use of standard con-tinuous representations for words to generate trans-lation rules for infrequent phrases (?2).
We ex-plore linear projections that map continuous repre-sentations of rare foreign phrases to English phrases.In particular, we propose to learn many local pro-jections that are specific to a given foreign phrase.We find this to be much more accurate than a sin-gle globally learned mapping such as proposed by(Mikolov et al 2013; ?3).Our method relies on the fast retrieval of simi-lar phrases in continuous space.
We explore bothLocality Sensitive Hashing (LSH; Indyk and Mot-wani, 2008) as well as the lesser known RedundantBit Vector method (RBV; Goldstein et al 2005) forfast k-nearest neighbor (k-NN) search.
RBV outper-forms the popular LSH algorithm by a large margin,both in speed as well as accuracy (?4).Our results show that the local linear projectionmethod is not only three orders of magnitudes fasterthan the algorithm of Saluja et al (2014) but alsoby 0.5 BLEU more accurate.
We achieve a 1.6BLEU improvement in Arabic-English translationcompared to a standard phrase-based baseline (?5).2 Continuous Phrase RepresentationsContinuous representations of words have beenfound to capture syntactic and semantic regularitiesin language (Turian et al, 2014; Collobert et al,2011; Mikolov et al, 2013c).
The induced represen-tations often tend to cluster similar words togetheras illustrated in Figure 1.1527source targetgato (cat)catcaballo (horse)vaca (cow)cerdo (pig)perro (dog)horsecowpigdogFigure 1: Illustration of word representations in Spanishand English (Figure from Mikolov et al (2013a)).
Theplots are based on a two-dimensional projection of theoriginal vectors with principal component analysis.A logical next step is to learn representations forlarger linguistic units, a topic which has received alot of interest (Mitchell and Lapata, 2010; Socheret al, 2011; Le and Mikolov, 2014).
For machinetranslation there have been efforts to learn represen-tations for entire bilingual phrases (Zou et al, 2013;Zhang et al, 2014; Gao et al, 2014).In this work, we only require representations formonolingual phrases that are relatively short.1Wetherefore decided to use off-the-shelf word repre-sentations to build phrase vectors.
In particular, wechose the continuous bag-of-words model (Mikolovet al, 2013b) which is very fast to train and scalesvery well to large monolingual corpora.The resulting word vectors are then used to buildphrase vectors via simple element-wise additionwhich has been found to perform very competitivelyin comparison to alternative approaches (Mitchelland Lapata, 2010).
Note that all the algorithms de-scribed in this paper are agnostic to the choice ofphrase-representation and other schemes may per-form better.We use these monolingual phrase representationsto generate translation rules for infrequent, or unla-beled, phrases.
Unlabeled phrases do not appear inthe bilingual data and thus do not have translationrules.
The general idea behind the following algo-rithms is to identify labeled phrases for which weknow translation rules that are similar to an unla-beled phrase, and to use them to induce translationrules for the unlabeled phrase.1For simplicity, we only consider unigrams and bigrams onthe source side, see ?53 Translation Rule GenerationWe first describe how we can learn a single mappingbetween the foreign and English continuous spacesto find translations for an infrequent foreign phrase(?3.1).
Next, we make this approach more robust bylearning many mappings that are specific to a givenforeign phrase (?3.2).
Finally, we review the semi-supervised label propagation algorithm of Saluja etal.
(2014) which we make much faster using con-tinuous word representations and k-NN algorithms(?3.3).3.1 Global Linear ProjectionMikolov et al (2013a) find that the relative po-sitions between words are preserved between lan-guages (Figure 1), and, thus, it is possible to learn alinear projection that maps the continuous represen-tation of source phrases to points on the target side.The hope is to learn a mapping that captures the re-lationship between the source and target spaces.
Wecall this linear transform global linear projection,since we use a single mapping that we apply to everysource phrase.More formally, we denote f and e as sourceside and target side phrases respectively, and f ?R1?dand e ?
R1?das the corresponding phrasalvectors with dimension d. Following Mikolov etal.
(2013a), we learn a global linear projectionmatrix W ?
Rd?dbased on the translations ofthe n most frequent labeled source side phrases:(f1, e1), (f2, e2), .
.
.
, (fn, en), n ?
d.2Let F =[fT1, fT2, .
.
.
, fTn]T, and E = [eT1, eT2, .
.
.
, eTn]T. Wecalculate W by solving the following linear system:FW = Ewhose solution is:W ?
(FTF )?1FTEUsing the linear transform W , we can compute?e = fW for each unlabeled source phrase f , where?e will be close to target phrases that are potentialtranslation candidates for f .
We denote the set ofall nearby English phrase vectors as N(?e) and use2We need more than d phrases to be fetched to make thelinear system solvable.
Similar is for the local linear projectionin ?3.2.1528source targetun gatoel gatolos gatosthe catthe catsa catcatssource targetun gatoel gatolos gatosthe catthe catsa catcats(a) Global Linear Projection (b) Local Linear ProjectionFigure 2: (a) Illustration of the global linear projection mapping the unlabeled Spanish phrase ?un gato?
to the targetspace.
The neighbors of the projected point serve as translation candidates and are fetched via a k-NN query.
(b) Alocal linear projection is learned individually for ?un gato?
based on the translations ?the cats?, ?the cat?
of the labeledneighbors ?los gatos?, ?el gato?.fast k-NN query algorithms to retrieve this set (?4).Figure 2 (a) illustrates the method.The translation probability for each translationcandidate e ?
N(?e) is based on the similarity tothe projected point?e:P (e|f) =exp{sim(e,?e)}?e??N(?e)exp{sim(e?,?e)}.
(1)Note that we normalize over the neighbor set N(?e)of the projected point?e of foreign phrase f .
Thisuses the similarity sim(?e, e) between?e and e whichis defined symmetrically assim(?e, e) =11 + ??e?
e?, (2)where ?
?e ?
e?
is the Euclidean distance betweenvectors?e and e.Before adding the generated candidate transla-tions to the MT system, we also calculate the back-ward maximum likelihood translation probabilityusing Bayes?
Theorem:P(f |e) =P(e|f)P(f)P(e),where the marginal probabilities are based on thecounts of phrases seen in the monolingual corpora.Similar to Saluja et al (2014), we use word-basedtranslation probabilities from the baseline system toobtain forward and backward lexicalized translationprobabilities.3.2 Local Linear ProjectionThe global linear projection uses a single projectionmatrix for all unlabeled source phrases.
This is sim-ple and fast but assumes that we can capture all re-lations between the source and target representationspace with a single Rd?dmapping.
We show laterthat this is clearly not the case (?5.4) and that a sin-gle projection struggles particularly with infrequentphrases - the precise situation in which we wouldlike our projection to be robust.We therefore propose to learn many local lin-ear projections which are individually trained foreach unlabeled source phrase.
Specifically, for eachunlabeled source phrase f , we learn a mappingWf?
Rd?dbased on the translations of m of f ?slabeled neighbors: (f1, e1), (f2, e2), .
.
.
, (fm, em),fi?
N(f), 1 ?
i ?
m, m ?
d (see Figure 2 (b)).Compared to the global projection, we require anadditional k-NN query to find the labeled neighborsfor each unlabeled source phrase.
However, this ex-tra computation takes only a negligible amount oftime, since the number of labeled phrases on thesource side is significantly smaller than the numberof phrases on the target side.Our approach of learning many different map-pings is similar to the locality preserving projectionsmethod of He and Niyogi (2004), which also con-struct a locally precise projection in order to map toanother space.15293.3 Structured Label Propagation withContinuous RepresentationSaluja et al (2014) use Structured Label Propaga-tion (SLP; Liu et al 2012) to propagate candidatetranslations from frequent source phrases that are la-beled to unlabeled neighbors that are infrequent.The algorithm works as follows: for a knowntranslation rule (f?, e?
), SLP propagates the targetside phrases e ?
N(e?
), that are similar to e?, to theunlabeled source phrases f ?
N(f?
), that are similarto f?, as new translation rules.
This propagation runsfor several iterations.
At each iteration, the transla-tion probability between known translations is fixed.More formally, for iteration t+ 1 we havePt+1(e|f) =?f?
?N(f)T (f?|f)?e??H(f?
)T (e|e?)Pt(e?|f?
),where T (f?|f) is the probability that phrase f ispropagated through phrase f?, similarly for T (e|e?);H(f?)
is the set of translation candidates for sourcephrase f?, which is learned from the bilingual cor-pus.In Saluja et al (2014), both T (f?|f) and T (e|e?
)are based on the pairwise mutual information (PMI)between two phrases.
Computing PMI statistics be-tween any two phrases over a large corpus is infea-sible and therefore the authors resort to a simple ap-proximation that only considers co-occurrences withother phrases within a fixed-sized context window.Even after this simplification the running time ofthe SLP is vastly dominated by gathering similaritystatistics and by constructing the resulting graph.However, once the PMI statistics are collected andthe graph is constructed, actual label propagation isvery fast.
To speed up the algorithm, we replacethe costly PMI statistics by continuous phrase rep-resentations and adopt the same similarity measurethat we used for the global and local projections (seeEquation 1).
Moreover, we replace the static graphconstruction with on-demand graph expansion us-ing the fast phrase query mechanisms described inthe next section.
These modifications allow us todramatically speed up the original SLP algorithm asdemonstrated in our experiments (?5).4 Fast Phrase Query with ContinuousRepresentationThe algorithms presented in the previous section re-quire rapid retrieval of neighboring phrases in con-tinuous space.
Linear search over all n candidatephrases is impractical, particularly for the SLP al-gorithm (?3.3).
SLP requires the construction of agraph encoding the nearest neighbors for each tar-get phrase, be it online or offline.
To construct thisgraph na?
?vely requires O(n2) comparisons which isclearly impractical for our setup where we have overone million target phrases (?5).
For the linear projec-tions, we still need to run at least one k-NN query inthe target space for each infrequent foreign phrase.Various methods, e.g., k-d trees, were proposedfor fast k-NN queries but most of them are not ef-ficient enough in high dimensional space, such asour setting.
We therefore investigate approximatedk-NN query methods which sacrifice some accu-racy for a large gain in speed.
Specifically, we lookinto locality sensitive hashing (LSH; ?4.1), a popu-lar method, as well as redundant bit vectors (RBV;?4.2), which to our knowledge has not been previ-ously used for natural language processing tasks.4.1 Locality Sensitive HashingOne popular approximated method is Locality Sen-sitive Hashing (LSH; Indyk and Motwani, 1998),which has been used in many NLP tasks such asnoun clustering (Ravichandran et al, 2005), topicdetection (Petrovi?c et al, 2010), and fast k-NNquery for similar words (Goyal et al, 2012).For our particular task, assume each phrase is rep-resented by a d-dimensional vector p of real values.The core of LSH is a set of hash functions.
Wechoose p-stable distribution based functions (Dataret al, 2004) of the following form:hi(p) = bxi?
p + biwc, 1 ?
i ?
s.This function can be viewed as a quantized randomprojection, where each element in xiis selected ran-domly from a Gaussian distribution N (0, 1), w isthe width of the bin, biis a linear bias selected froma uniform distribution U(0, w) (see Figure 3 (a)).By concatenating the results from hi, 1 ?
i ?
s,phrase p is projected from d-dimensional space to1530p1p4p3p2bin1bin2bin3p1p3p2p4p5p1p3p2p4p511100p1p2p3p4p50111000111bin 1 bin 2 bin 3(a) (b) (c)Figure 3: (a) A quantized random projection in LSH.
The arrows show the direction of the projection.
Points p1, p2, p3are correctly projected to the same bin, while p4falls into another bin, despite being very close to p1.
(b) A simplifiedexample illustrating RBV in two dimensions.
The circle with radius r is centered at p1and contains all neighbors ofp1.
RBV approximates the circle by a square of width d = 2?
0.95r, which contains most of the neighbors of p1butalso p4, a false positive, while missing p5, a closer point.
(c) On each dimension, RBV uses bit vectors to maintainthe set of points whose hypercubes (represented as the segments on the points in 1-dimensional view) intersect with abin.an s-dimensional space.
Phrases whose projectionscollide in the s-dimensional space are consideredcandidates to be neighbors.
A fast retrieval of thosecolliding phrases can be done via a hash table.
How-ever, since the projection is random, it is very likelythat true neighbors in the d-dimensional space fallinto different bins after projection (false negatives;e.g., p1and p4in Figure 3 (a)).
To ease this problem,LSH employs a set of such projections and runs alinear search over the union of all possible neighborcandidates resulting from these projections to findthe approximated k-nearest neighbors.4.2 Redundant Bit VectorsThe performance of LSH decreases as the numberof dimensions grows.
Redundant bit vectors (RBV;Goldstein et al, 2005) address this problem and canquickly search in high dimensional space, whichsuits our task better.RBV is a combination of: a) an approximatedneighborhood test designed for high dimensionalspace, and b) an efficient data structure for fastneighborhood query.First, for a given point p in high dimensionalspace, the volume of a hypersphere of radius r cen-tered at p can be approximately covered by a hyper-cube of width d = 2r,   1.3Figure 3 (b) shows3Here we use in an imprecise way.
 1 does not mean is smaller than 1 by orders of magnitudes; usually  > 0.1.an illustration in two dimensional space where asquare of width d = 2 ?
0.95r covers most of acircle with radius r. In higher dimensional space,e.g., d = 256 as in Goldstein et al (2005), we cancover 99% of the volume of a hypersphere of r = 1with a hypercube whose width is only ?2?
0.2r.4This surprising result allows us to use a very smallhypercube to approximate the hypersphere.
Fur-thermore, if two points q and p are within a cer-tain radius r, i.e., ?q ?
p?
?
r, then frequently|q(i)?
p(i)| ?
r, where x(i)denotes the i-th ele-ment of vector x.
Thus, the neighbor query can beapproximated as a check whether the distance be-tween p and q on each dimension is less than r, 1.Second, each dimension is quantized into bins.Each bin redundantly maintains a set of pointswhose hypercubes intersect with the bin on that di-mension.
This set is an approximation of the neigh-bors of a query point p that falls into the same binon this dimension.
RBV uses bit vectors to store thisset of points for each bin.
(See Figure 3 (c).
)For a given query vector p, we fetch the binswhere p falls into for each dimension.
We then per-What we mean is that in high dimensional space, the volume ofa hypercube of width 2r is more than hundreds of magnitudessmaller than a hypercube of width 2r.4Note that this does not mean the volume of the hypercubeis smaller than the hypersphere.
It just means that most of thevolume of the hypersphere is covered in the hypercube.1531English Arabic UrduToken count 5b 5b 75mWord vector count 2.9m 2.7m 0.2mWord vector train time 100hrs 100hrs 3hrsTable 1: Monolingual corpora statistics, number of wordvectors, and time to learn the word vectors (on singleCPU core) for each source language.form a bitwise and over the corresponding bit vec-tors to find the set of points that actually fall into p?shypercube, i.e., the approximated candidate neigh-bor set of p. Finally, a linear search over this muchsmaller set finds the approximate k-nearest neigh-bors, similar to LSH.5 ExperimentsWe first evaluate the speed and accuracy of the pre-sented approximate k-NN query algorithms (?5.2).Next we experiment with the translation rule gen-eration approaches (?5.3), and then we analyze theglobal and local projection methods (?5.4).
Fol-lowing Saluja et al (2014), we present most resultson Arabic-English translation and then validate ourfindings on Urdu-English (?5.5), a low-resource set-ting.
Lastly, we discuss some qualitative results(?5.6).5.1 Datasets & PreprocessingWe test our approach on both Arabic-English andUrdu-English translation.
For Arabic-English ourbilingual training data comprises of 685k sentencepairs.
The NIST MT08 and MT09 data sets serve astuning and testing sets, respectively.
Both are com-binations of newswire and weblog articles, and eachArabic sentence has four reference translations.
ForUrdu-English our bilingual training corpus contains165k sentence pairs, and the tuning and testing setsare NIST MT08 and NIST MT09, respectively.Table 1 shows some statistics for the monolingualdata we use.
The majority of the data for Arabic andEnglish is drawn from the AFP Gigaword corpus.For Urdu most of the data is mined by a web crawler,mainly because there are not many official resourcesfor this language.We run standard tokenization and segmentationon the monolingual corpora.
After that we use theWord2Vec tool (Mikolov et al, 2013b) to generateFalse Negative (%) Time (s)Linear Search 0 342LSH 14.29 69RBV 9.08 19Table 2: Performance of linear search, locality sensitivehashing, and redundant bit vectors, for k = 200.word embeddings for each language with the bag-of-words model, where the number of dimensions isset to d = 300.
See Table 1 for the number of wordvectors learned for each language.To obtain phrases in each language, we use a sim-ilar strategy as in Saluja et al (2014).
For Arabicand Urdu, we collect all unigrams and bigrams fromthe tuning and testing sets.
This gives 0.66m phrasesfor Arabic and 0.2m phrases for Urdu.
For English,we collect unigrams and bigrams from the monolin-gual data instead.
However, the English monolin-gual corpus is much larger than the tuning and test-ing sets for Arabic and Urdu.
We therefore train alanguage model over the monolingual data, and col-lect the unigrams and bigrams from the ARPA file,filtering out all candidates that have a probabilitysmaller than 10?7.
Similar to Saluja et al (2014),we use a baseline MT system to translate the Ara-bic or Urdu phrases and add their translations to theEnglish phrase set.
After this procedure we end upwith 1.5m English phrases.We use simple component-wise addition to gen-erate phrase vectors from word vectors.
Some rarewords do not receive a vector representation afterrunning Word2Vec, and we simply remove phrasescontaining those words, resulting in a total of 0.65mphrases for Arabic, 0.18m phrases for Urdu, and1.2m phrases for English.5.2 Evaluation of Approximated k-NN QueryWe first evaluate the performances of different k-NNquery approaches on English word vectors.There are 2.9m word vectors in d = 300 di-mensional space.
We randomly select 1,000 words,and query for each word the 200 nearest neighbors,k = 200, with either linear search, LSH, and RBV.We measure the false negative ratio, i.e., the percent-age of true neighbors missed by each query method,as well as time.
For LSH and RBV, we tune the pa-rameters for best performance (LSH: number of pro-jected dimensions, number of layers, and width of1532Tune Test Time (hr)Baseline 39.33 38.09 -SLP w/ PMI 40.93 39.16 ?10,000SLP w/ Cont.
Repr.
41.31 39.34 120+200GLP 40.46 38.68 20+200LLP 41.17 39.57 30+200LLP w/ backoff 41.48 39.70 30+200Table 3: Arabic-English translation accuracy of struc-tured label propagation with PMI (SLP) and with con-tinuous representations (SLP w/ PMI), the global linearprojection (GLP), our local linear projection (LLP) andwith an added backoff scheme (LLP w/ backoff).
For ap-plicable methods, we list the running time to compute dis-tributional representations as a separate term in the timecolumn.
This is usually only required once per languagewhich is why we report it separately.the bin; RBV: hypercube width and number of binsfor each dimension).Table 2 shows that RBV gives significantly betterperformance than LSH, both in terms of accuracyand speed.
RBV reduces the false negative ratio by1/3 compared to LSH and is 3.6 times faster.
Thisis in line with Goldstein et al (2005) who observedthat the performance of LSH degrades in high di-mensional space.
We therefore use RBV in the fol-lowing experiments.5.3 Evaluation of Rule GenerationNext, we evaluate the quality of the generated trans-lation rules for Arabic-English translation (Table 3)using either SLP, the global linear projection (GLP),or the local linear projection (LLP).Our baseline system is an in-house phrase-basedsystem similar to Moses with a 4-gram languagemodel.
The underlying log-linear model comprisesof 13 features: two maximum likelihood transla-tion probabilities, two lexicalized translation prob-abilities, five hierarchical reordering model fea-tures (Galley and Manning, 2008), one languagemodel, word penalty, phrase length, and distortionpenalty), and is tuned with minimum error rate train-ing (MERT; Och 2003).
Translation quality is mea-sured with BLEU (Papineni et al, 2002).For comparison, we reimplemented the graph-based method in Saluja et al (2014).
This methodcalculates the pairwise mutual information (PMI)between phrases, and employs all the techniquesmentioned in Saluja et al (2014) to speedup thecomputations.
Our reimplementation achieves simi-lar performance to Saluja et al (2014) (with a neg-ligible ?
0.06 drop in BLEU).
We parallelized thealgorithm on a cluster since a single core implemen-tation would run for ?10k hours.5Our continuous phrase based version of SLP isorders of magnitudes faster than the SLP variant ofSaluja et al (2014) because it replaces the compu-tationally expensive PMI calculation by an approx-imated k-NN query in distributional space.
More-over, our variant of SLP even improves translationquality by 0.2-0.3 BLEU.
Overall, our version ofSLP improves the baseline by 2.0 BLEU on the tun-ing set and by 1.3 BLEU on the test set.The linear projection based methods, GLP andLLP, are in turn again several times faster than SLPwith continuous representations.
This is becausethey require significantly fewer k-NN queries.
Forboth GLP and LLP, we retrieve the 200 nearestneighbors of the projected point.
For LLP, the lo-cal projection is calculated based on the 500 nearestlabeled neighbors of the infrequent source phrase.LLP achieves slightly better accuracy on the test setthan PMI-based SLP but at four times the speed.GLP is the fastest method but also the least accurate,improving the baseline only by about 0.6 BLEU.
Weexplore this result in more detail in the next section.Overall, our local projection outperforms the globalprojection by 0.9 BLEU on the test set.For some infrequent source phrases, approxi-mated k-NN query does not retrieve enough (?
d)neighbors to learn a local linear projection.
For thesephrases, we employ a backoff strategy that uses thetranslations of their neighbors as additional transla-tion candidates.
This strategy provides helpful addi-tional rules for LLP (Table 3).65.4 Evaluation of Global Linear ProjectionTo learn why GLP does not generate high qualitytranslation rules, we run an extra experiment to mea-sure the projection quality of GLP.We train a global linear projection on an increas-5Confirmed with the authors of Saluja et al (2014) from per-sonal communication.6The backoff scheme in the Arabic-English setting generatesaround 15% of the translations rules, which adds 0.13 BLEU onthe test set.
This is not a big improvement and so we did notemploy this scheme for our Urdu-English experiments.1533Training Set Hit Rate: Freq Hit Rate: Infreq.500 0.87 01,000 0.6 0.015,000 0.42 0.0725,000 0.4 0.05Table 4: Quality of global linear projection measured bythe ratio that GLP can fetch the most possible translationin the 200-nearest neighbors.Tune Test Time (hr)Baseline 26.32 27.41 -SLP w/ PMI 27.26 27.89 ?7,000SLP w/ Cont.
Repr.
27.34 27.73 100+103LLP 27.06 27.98 30+103Table 5: Urdu-English translation accuracy (cf.
Table 3).ing amount of training data and measure its accuracyon two test sets (Table 4).
The first test set containsthe 100 most frequent source phrases and their trans-lations.
The second test set contains less frequentexamples; we choose the 50,000 to 50,100 most fre-quent source phrases.
The training data uses the lmost frequent source phrases and their translationswhich are not already contained in the first test.
Theprojection quality is measured by the ratio of howmany times the correct translation is one of the 200-nearest neighbors of the projected point computedby GLP.The results in Table 4 clearly show that GLP canfind the best translation for very frequent sourcephrases which is in line with previous work Mikolovet al (2013a).
However, the accuracy for infrequentphrases is poor.
This explains why GLP helps rel-atively little in our translation experiments becauseour setup requires a method that can find good trans-lations for infrequent source phrases.5.5 Evaluation on Urdu-EnglishResources for Urdu are limited compared to Arabic(?5.1) which results in fewer word vectors and fewersource phrases.
This will also affect the quality ofthe word vectors in Urdu, since more training datausually results in better representations.Table 5 shows that the improvements of both SLPand LLP in Urdu-English are not as significant asfor Arabic-English.
Our reimplementation of SLPis ?
1 BLEU better on the tuning set than the base-line, and ?
0.5 BLEU better on the test set.
As ex-Source  Generated target??????????
?????????
the humanitarian obligations??????????
?????????
humanitarian commitments?????
??????????
both these two groups?????
??????????
these two communities????
????????
building their institutions??????
????
certainly efforts??????
????
efforts must???
????????
healthier youth?????
?????
services special???????
????????
community developmentFigure 4: Examples of the generated rules from LLP.pected, the translation quality improvement on smallcorpora is not as significant as on large corpora likeArabic, since the monolingual data in Urdu is muchsmaller than for Arabic (75m tokens vs. 5b tokens)which makes it more difficult to learn good represen-tations.
In general, with continuous representations,SLP and LLP achieve similar performance to PMI-based SLP but the projection based methods are or-ders of magnitudes faster.5.6 Analysis of OutputFigure 4 shows some examples of the translationrules produced by our system.
The first five ex-amples are for the Arabic-English system, while thelast five are for the Urdu-English system.
All sourcephrases are unknown to the baseline system whichusually results in sub-optimal translations.
Our sys-tem on the other hand, managed to generate trans-lation rules for them.
The Arabic-English exam-ples show mostly morphological variants of phraseswhich did not appear in the parallel data; this canbe helpful for highly inflected languages since mostof the inflectional variations are underrepresented inthe parallel data.
The Urdu-English examples showmostly unknown phrases since there is much lessparallel data than for Arabic.6 Conclusion and Future WorkIn this work, we showed how simple continuous rep-resentations of phrases can be successfully used toinduce translation rules for infrequent phrases anddemonstrated substantial gains in translation accu-racy.
Continuous representations not only increasethe speed of the semi-supervised approach of Salujaet al (2014) by two orders of magnitude but alsoimprove its accuracy at the same time.
Simpler1534linear projections are up to three orders of magni-tudes faster once phrasal representations have beenlearned and can be as accurate.
Our novel local lin-ear projection is much more accurate than the globalprojection of Mikolov et al (2013a) at only a smallincrease in running time.
This brings us closer togenerating new translation rules on-the-fly for un-seen sentences.
Finally, we showed that redundantbit vectors are three times faster but also signifi-cantly more accurate than locality sensitive hashingin our setting.
To our knowledge this is the first ap-plication of redundant bit vectors on a natural lan-guage processing task.In future work, we would like to investigate moreelaborate projection schemes that use contextual in-formation from the source side or non-linear projec-tions.
Furthermore, we would like to apply redun-dant bit vectors to other NLP tasks.AcknowledgmentWe thank the three anonymous reviewers for help-ful suggestions.
We are also grateful to ChrisQuirk, Kristina Toutanova, Jonathan Clark, QinGao, Austin Matthews, Liang Huang, Mingbo Ma,and Mo Yu for discussion.
Kai Zhao was partiallysupported by DARPA FA8750-13-2-0041 (DEFT).ReferencesChris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine translationusing paraphrases.
In Proceedings of the main con-ference on Human Language Technology Conferenceof the North American Chapter of the Association ofComputational Linguistics, pages 17?24.
Associationfor Computational Linguistics.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics, pages 263?270.
Association forComputational Linguistics.Ronan Collobert, Jason Weston, Leon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab SMirrokni.
2004.
Locality-sensitive hashing schemebased on p-stable distributions.
In Proceedings of thetwentieth annual symposium on Computational geom-etry, pages 253?262.
ACM.Michel Galley and Christopher D Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, pages848?856.
Association for Computational Linguistics.Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng.2014.
Learning continuous phrase representations fortranslation modeling.
In Proc.
ACL.Jonathan Goldstein, John C Plat, and Christopher JCBurges.
2005.
Redundant bit vectors for quicklysearching high-dimensional regions.
In Deterministicand Statistical Methods in Machine Learning, pages137?158.
Springer.Amit Goyal, Hal Daum?e III, and Raul Guerra.
2012.Fast large-scale approximate graph construction fornlp.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 1069?1080.
Association for Computational Lin-guistics.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In ACL, volume 2008,pages 771?779.
Citeseer.Xiaofei He and Partha Niyogi.
2004.
Locality preserv-ing projections.
In Neural information processing sys-tems, volume 16, page 153.
MIT.Piotr Indyk and Rajeev Motwani.
1998.
Approximatenearest neighbors: towards removing the curse of di-mensionality.
In Proceedings of the thirtieth annualACM symposium on Theory of computing, pages 604?613.
ACM.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology-Volume 1, pages48?54.
Association for Computational Linguistics.Quoc Le and Tomas Mikolov.
2014.
Distributed repre-sentations of sentences and documents.
In Proceed-ings of the 31st International Conference on MachineLearning.Shujie Liu, Chi-Ho Li, Mu Li, and Ming Zhou.
2012.Learning translation consensus with structured labelpropagation.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguistics:Long Papers-Volume 1, pages 302?310.
Associationfor Computational Linguistics.Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a.Exploiting similarities among languages for machinetranslation.
arXiv preprint arXiv:1309.4168.1535Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed represen-tations of words and phrases and their composition-ality.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Tomas Mikolov, Yih Wen-tau, and Zweig Geoffrey.2013c.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technology, pages 746?751.
Association forComputational Linguistics.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive science,34(8):1388?1429.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 160?167.
Associa-tion for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th annual meeting on association for computationallinguistics, pages 311?318.
Association for Computa-tional Linguistics.Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applica-tion to twitter.
In Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 181?189.
Association for Computational Lin-guistics.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the 33rd annualmeeting on Association for Computational Linguistics,pages 320?322.
Association for Computational Lin-guistics.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized algorithms and nlp: using localitysensitive hash function for high speed noun clustering.In Proceedings of the 43rd Annual Meeting on Associ-ation for Computational Linguistics, pages 622?629.Association for Computational Linguistics.Avneesh Saluja, Hany Hassan, Kristina Toutanova, andChris Quirk.
2014.
Graph-based semi-supervisedlearning of translation models from monolingual data.In Proceedings of the 52th Annual Meeting of the As-sociation for Computational Linguistics.
Associationfor Computational Linguistics.Richard Socher, Eric Huang, Jeffrey Pennington, An-drew Y Ng, and Christopher D. Manning.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural Infor-mation Processing Systems 24.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2014.Word representations: a simple and general method forsemi-supervised learning.
In Proc.
ACL.Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, andChengqing Zong.
2014.
Bilingually-constrainedphrase embeddings for machine translation.
In Pro-ceedings of the 52th Annual Meeting on Associationfor Computational Linguistics.
Association for Com-putational Linguistics.Will Y Zou, Richard Socher, Daniel M Cer, and Christo-pher D Manning.
2013.
Bilingual word embeddingsfor phrase-based machine translation.
In EMNLP,pages 1393?1398.1536
