Proceedings of the 25th International Conference on Computational Linguistics, pages 38?45,Dublin, Ireland, August 23-29 2014.A Poodle or a Dog?
Evaluating Automatic Image Annotation UsingHuman Descriptions at Different Levels of GranularityJosiah K. Wang1Fei Yan2Ahmet Aker1Robert Gaizauskas11Department of Computer Science, University of Sheffield, UK2Centre for Vision, Speech and Signal Processing, University of Surrey, UK{j.k.wang, ahmet.aker, r.gaizauskas}@sheffield.ac.uk f.yan@surrey.ac.ukAbstractDifferent people may describe the same object in different ways, and at varied levels of granular-ity (?poodle?, ?dog?, ?pet?
or ?animal??)
In this paper, we propose the idea of ?granularity-aware?
groupings where semantically related concepts are grouped across different levels ofgranularity to capture the variation in how different people describe the same image content.The idea is demonstrated in the task of automatic image annotation, where these semantic group-ings are used to alter the results of image annotation in a manner that affords different insightsfrom its initial, category-independent rankings.
The semantic groupings are also incorporatedduring evaluation against image descriptions written by humans.
Our experiments show that se-mantic groupings result in image annotations that are more informative and flexible than withoutgroupings, although being too flexible may result in image annotations that are less informative.1 IntroductionDescribing the content of an image is essential for various tasks such as image indexing and retrieval, andthe organization and browsing of large image collections.
Recent years have seen substantial progressin the field of visual object recognition, allowing systems to automatically annotate an image with a listof terms representing concepts depicted in the image.
Fueled by advances in recognition algorithms andthe availability of large scale datasets such as ImageNet (Deng et al., 2009), current systems are able torecognize thousands of object categories with reasonable accuracy, for example achieving an error rateof 0.11 in classifying 1, 000 categories in the ImageNet Large Scale Visual Recognition Challenge 2013(ILSVRC13) (Russakovsky et al., 2013).However, the ILSVRC13 classification challenge assumes each image is annotated with only onecorrect label, although systems are allowed up to five guesses per image to make the correct prediction(or rather, to match the ground truth label).
The problem with this is that it becomes difficult to guesswhat the ?correct?
label is, especially when many other categories can equally be considered correct.For instance, should a system label an image containing an instance of a dog (and possibly some otherobjects like a ball and a couch) as ?dog?, ?poodle?, ?puppy?, ?pet?, ?domestic dog?, ?canine?
or even?animal?
(in addition to ?ball?, ?tennis ball?, ?toy?, ?couch?, ?sofa?, etc.)?
The problem becomes evenharder when the number of possible ways to refer to the same object instance increases, but the numberof prediction slots to fill remains limited.
With so many options from which to choose, how do we knowwhat the ?correct?
annotation is supposed to be?In this paper, we take a human-centric view of the problem, motivated by the observation that humansare likely to be the end-users or consumers of such linguistic image annotations.
In particular, we investi-gate the effects of grouping semantically related concepts that may refer to the same object instance in animage.
Our work is related to the idea of basic-level categories (Biederman, 1995) in Linguistics, wheremost people have a natural preference to classify certain object categories at a particular level of granu-larity, e.g.
?bird?
instead of ?sparrow?
or ?animal?.
However, we argue that what one person considersThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footer areadded by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/38?basic-level?
may not necessarily be ?basic-level?
to another, depending on the person?s knowledge, ex-pertise, interest, or the context of the task at hand.
For example, Rorissa (2008) shows that users labelgroups of images and describe individual images differently with regards to the level of abstraction.
Thekey idea behind our proposed ?granularity-aware?
approach is to group semantically related categoriesacross different levels of granularity to account for how different people would describe content in animage differently.We demonstrate the benefits of the ?granularity-aware?
approach by producing a re-ranking of visualclassifier outputs for groups of concept nodes, e.g.
WordNet synsets.
The concept nodes are groupedacross different levels of specificity within a semantic hierarchy (Section 3.1).
This models better therichness of the vocabulary and lexical semantic relations in natural language.
In this sense these group-ings are used to alter the results of image annotation in a manner that affords different insights fromits initial, category-independent rankings.
For example, if the annotation mentions only ?dog?
but not?poodle?, a system ranking ?poodle?
at 1 and ?dog?
at 20 will have a lower overall score than a systemranking ?dog?
at 1, although both are equally correct.
Grouping (?poodle?
or ?dog?)
however will allow afairer evaluation and comparison where both systems are now considered equally good.
The ?granularity-aware?
groupings will also be used in evaluating these re-rankings using textual descriptions written byhumans, rather than a keyword-based gold-standard annotation.
The hypothesis is that by modeling thevariation in granularity levels for different concepts, we can gain a more informative insight as to howthe output of image annotation systems can relate to how a person describes what he or she perceives inan image, and consequently produce image annotation systems that are more human-centric.Overview.
The remainder of the paper is organized as follows: Section 2 discusses related work.
Sec-tion 3 describes our proposed ?granularity-aware?
approach to group related concepts across differentlevels of granularity.
It also discusses how to apply the idea both in automatic image annotation, byre-ranking noisy visual classifier outputs in a ?granularity-aware?
manner, and in evaluation of classi-fier outputs against human descriptions of images.
The results of the proposed method are reported inSection 4.
Finally, Section 5 offers conclusions and proposes possible future work.2 Related workWork on automatic image annotation traditionally relies heavily on image datasets annotated with a fixedset of labels as training data.
For example, Duygulu et al.
(2002) investigated learning from imagesannotated with a set of keywords, posing the problem as a machine translation task between imageregions and textual labels.
Gupta and Davis (2008) includes some semantic information by incorporatingprepositions and comparative adjectives, which also requires manual annotation as no such data is readilyavailable.
Recent work has moved beyond learning image annotation from constrained text labels tolearning from real world texts, for example from news captions (Feng and Lapata, 2008) and sportsarticles (Socher and Fei-Fei, 2010).There is also recent interest in treating texts as richer sources of information than just simple bagsof keywords, for example with the use of semantic hierarchies for object recognition (Marsza?ek andSchmid, 2008; Deng et al., 2012b) and the inclusion of attributes for a richer representation (Lampertet al., 2009; Farhadi et al., 2009).
Another line of recent work uses textual descriptions of images forvarious vision tasks, for example for recognizing butterfly species from butterfly descriptions (Wanget al., 2009) and discovering attributes from item descriptions on fashion shopping websites (Berg etal., 2010).
There has also been interest in recent years in producing systems that annotate images withfull sentences rather than just a list of terms (Kulkarni et al., 2011; Yang et al., 2011).
We considerour work to complement the work of generating full sentences, as it is important to filter and select themost suitable object instances from noisy visual output.
The shift from treating texts as mere labels toutilizing them as human-centric, richer forms of annotations is important to gain a better understandingof the processes underlying image and text understanding or interpretation.Deng et al.
(2012b) address the issue of granularity in a large number of object categories by allowingclassifiers to output decisions at the optimum level in terms of being accurate and being informative, forexample outputting ?mammal?
rather than ?animal?
while still being correct.
Their work differs from39ours in that the semantic hierarchy is used from within the visual classifier to make a decision aboutits output, rather than for evaluating existing outputs.
More directly related to our work is recent workby Ordonez et al.
(2013), which incorporates the notion of basic-level categories by modeling word?naturalness?
from text corpora on the web.
While their focus is on obtaining the most ?natural?
basic-level categories for different encyclopedic concepts as well as for image annotation, our emphasis is onaccommodating different levels of naturalness, not just a single basic level.
We adapt their model directlyto our work, details of which will be discussed in Section 3.1.3 Granularity-aware approach to image annotationThe proposed ?granularity-aware?
approach to image annotation consists of several components.
We firstdefine semantic groupings of concepts by considering hypernym/hyponym relations in WordNet (Fell-baum, 1998) and also how people describe image content (Section 3.1).
The groupings are then used tore-rank the output of a set of category-specific visual classifiers (Section 3.2), and also used to producea grouped ?gold standard?
from image captions (Section 3.3).
The re-ranked output is then evaluatedagainst the ?gold standard?, and the initial rankings and ?granularity-aware?
re-rankings are comparedto gain a different insight into the visual classifiers?
performance as human-centric image annotationsystems.3.1 Semantic grouping across different granularity levelsThe goal of semantic grouping is to aggregate related concepts such that all members of the group referto the same instance of an object, even across different specificity levels.
In particular, we exploit thehypernym/hyponym hierarchy of WordNet (Fellbaum, 1998) for this task.
WordNet is also the naturalchoice as it pairs well with our visual classifiers which are trained on ImageNet (Deng et al., 2009)categories, or synsets.The WordNet hypernym hierarchy alone is insufficient for semantic grouping as we still need a way todetermine what constitutes a reasonable group, e.g.
putting all categories into a single ?entity?
group istechnically correct but uninformative.
For this, we draw inspiration from previous work by Ordonez et al.
(2013), where a ?word naturalness?
measure is proposed to reflect how people typically describe imagecontent.
More specifically, we adapt for our purposes their proposed approach of mapping encyclopedicconcepts to basic-level concepts (mapping ?Grampus griseus?
to the more ?natural?
?dolphin?).
In thisapproach, the task is defined as learning a translation function ?
(v, ?)
: V 7?W that best maps a node vto a hypernym node w which optimizes a trade-off between the ?naturalness?
of w (how likely a personis to use w to describe something) and the distance between v and w (to constrain the translation frombeing too general, e.g.
?entity?
), with the parameter ?
controlling this trade-off between naturalness andspecificity.
Formally, ?
(v, ?)
is defined as:?
(v, ?)
= arg maxw ?
?(v)[??(w)?
(1?
?)?
(w, v) ] (1)where ?
(v) is the set of hypernyms for v (including v), ?
(w) is naturalness measure for node w, and?
(w, v) is the number of edges separating nodes w and v in the hypernym structure of WordNet.For our work, all synsets that map to a common hypernym w are clustered as a single semantic groupG?w:G?w= {v : ?v?
(v, ?)
= w} (2)In this sense, the parameter ?
?
[0, 1] essentially also controls the average size of the groups: ?
= 0results in no groupings, while ?
= 1 results in synsets being grouped with their most ?natural?
hypernym,giving the largest possible difference in the levels of granularity within each group.Estimating the naturalness function using Flickr.
Ordonez et al.
(2013) use n-gram counts of theGoogle IT corpus (Brants and Franz, 2006) as an estimate for term naturalness ?(w).
Although large,the corpus might not be optimal as it is a general corpus and may not necessarily mirror how people40describe image content.
Thus, we explore a different corpus that (i) better reflects how humans describeimage content; (ii) is sufficiently large for a reasonable estimate of ?(w).
The Yahoo!
Webscope YahooFlickr Creative Commons 100M (YFCC-100M) dataset (Yahoo!
Webscope, 2014) fits these criteria with100 million images containing image captions written by users.
Hence, we compute term occurrencestatistics from the title, description, and user tags of images from this dataset.
Following Ordonez et al.,we measure ?
(w) as the maximum log count of term occurrences for all terms appearing in synset w.Internal nodes.
Unlike Ordonez et al.
(2013), we do not constrain v to be a leaf node, but insteadalso allow for internal nodes to be translated to one of their hypernyms.
We could choose to limitvisual recognition to leaf nodes and estimate the visual content of internal nodes by aggregating theoutputs from all its leaf nodes, as done by Ordonez et al.
(2013).
However, since the example images inImageNet are obtained for internal nodes pretty much in the same way as leaf nodes (by querying ?dog?rather than by combining images from ?poodle?, ?terrier?
and ?border collie?)
(Deng et al., 2009), thevisual models learnt from images at internal nodes may capture different kinds of patterns than fromtheir hyponyms.
For example, a model trained with ImageNet examples of ?dog?
might capture somehigher-level information that may otherwise not be captured by merely accumulating the outputs of theleaf nodes under it, and vice versa.3.2 Re-ranking of visual classifier outputThe visual classifier used in our experiments (Section 4.2) outputs a Platt-scaled (Platt, 2000) confidencevalue for each synset estimating the probability of the synset being depicted in a given image.
Theclassifier outputs are then ranked in descending order of these probability values, and are treated asimage annotation labels.As mentioned, these rankings do not take into consideration that some of these synsets are semanticallyrelated.
Thus, we aggregate classifier outputs within our semantic groupings (Section 3.1), and then re-rank the scores of each grouped classifier.
Formally, the new score of a classifier c, ?c(G?w), for asemantic group G?wis defined as:?c(G?w) = maxv?G?wpc(v) (3)where v is a synset from the semantic groupG?w, and pc(v) is the original probability estimate of classifierc for synset v.
I.e., the probability of the most probable synset in the group is taken as the probability ofthe group.To enable comparison of the rankings against a gold standard keyword annotation, a word label isalso generated for each semantic group.
We assign as the semantic group?s label `(G?w) the first term ofsynset w, the common hypernym node to which members of the group best translates.
Note that the termmerely acts a label for evaluation purposes and should not be treated as a word in a traditional sense.
Wealso merge semantic groups with the same label to account for polysemes/homonyms, again taking themaximum of ?camong the semantic groups as the new score.The semantic grouping of synsets is performed independently of visual classifier output.
As such, weonly need to train each visual classifier once for each synset, without requiring re-training for differentgroupings since we only aggregate the output of the visual classifiers.
This allows for more flexibilitysince the output for each semantic group is only aggregated at evaluation time.3.3 Evaluation using human descriptionsThe image dataset used in our experiments (Section 4.1) is annotated with five full-sentence captionsper image but not keyword labels.
Although an option would be to obtain keyword annotations viacrowdsourcing, it is time consuming and expensive and also requires validating the annotation quality.Instead, we exploit the existing full-sentence captions from the dataset to automatically generate a goldstandard keyword annotation for evaluating our ranked classifier outputs.
The use of such captions isalso in line with our goal of making the evaluation of image annotation systems more human-centric.For each caption, we extract nouns using the open source tool FreeLing (Padr?o and Stanilovsky, 2012).41?0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Semantic Grouping 0.3450 0.3450 0.3548 0.3735 0.4025 0.4417 0.4562 0.4702 0.4834 0.5059 0.5395Random Grouping 0.3450 0.3450 0.3493 0.3529 0.3585 0.3689 0.3823 0.4067 0.4241 0.4359 0.4467Number of groups 1294 1294 1237 1105 949 817 693 570 474 419 368Table 1: Results of re-ranking with semantic groupings.
The first two rows show the average NDCGscores for the proposed groupings and the random baseline groupings, for different groupings formed byvarying ?.
The bottom row shows the number of semantic groups formed for different values of ?.For each image, each noun is assigned an individual relevance score, which is the number of captionsthat mentions the noun.
This upweights important objects while downweighting less important objects(or errors from the annotator or the parser).
The result is a list of nouns that humans use to describeobjects present in the image, each weighted by its relevance score.
We assume nouns that appear in thesame WordNet synset (?bicycle?
and ?bike?)
are synonyms and that they refer to the same object instancein the image.
Hence, we group them as a single label-group, with the relevance score taken to be themaximum relevance score among the nouns in the group.Since there are only five captions per image, the proposed approach will result in a sparse set ofkeywords.
This mirrors the problem described in Section 1 where systems have to ?guess?
the so-called?correct?
labels, thus allowing us to demonstrate the effectiveness of our ?granularity-aware?
re-rankings.In order to compare the annotations against the re-rankings, we will need to map the keywords to thesemantic groupings.
This is done by matching the nouns to any of the terms in a semantic group, with acorresponding label `(G?w) for each group (Section 3.2).
Nouns assigned the same label are merged, withthe new relevance score being the maximum relevance score among the nouns.
If a noun matches morethan one semantic group (polyseme/homonym), we treat all groups as relevant and divide the relevancescore uniformly among the groups.
Evaluation is then performed by matching the semantic group labelsagainst the image annotation output.4 Experimental evaluationOur proposed method is evaluated on the dataset and categories as will be described in Section 4.1, by re-ranking the output of the visual classifiers in Section 4.2.
The effects of semantic groupings are exploredusing different settings of ?
(see Section 3.1).Baseline.
To ensure any improvements in scores are not purely as a result of having a shorter list ofconcepts to rank, we compare the results to a set of baseline groupings where synsets are grouped in arandom manner.
For a fair comparison the baselines contain the same number of groups and cluster sizedistributions as our semantic groupings.4.1 Dataset and Object CategoriesThe Flickr8k dataset (Hodosh et al., 2013) is used in our image annotation experiments.
The datasetcontains 8,091 images, each annotated with five textual descriptions.
To demonstrate the notion of gran-ularity in large-scale object hierarchies, we use as object categories synset nodes from WordNet (Fell-baum, 1998).
Ideally, we would like to be able to train visual classifiers for all synset categories inImageNet (Deng et al., 2009).
However, we limit the categories to only synsets with terms occurring inthe textual descriptions of the Flickr8k dataset to reduce computational complexity, and regard the useof more categories as future work.
This results in a total of 1,372 synsets to be used in our experiments.The synsets include both leaf nodes as well as internal nodes in the WordNet hierarchy.4.2 Visual classifierDeep learning (LeCun et al., 1989; Hinton and Salakhutdinov, 2006) based approaches have be-come popular in visual recognition following the success of deep convolutional neural networks42(CNN) (Krizhevsky et al., 2012) in the ImageNet Large Scale Visual Recognition Challenge 2012(ILSVRC12) (Deng et al., 2012a).
Donahue et al.
(2013) report that features extracted from the acti-vation of a deep CNN trained in a fully supervised fashion can also be re-purposed to novel generic tasksthat differ significantly from the original task.
Inspired by Donahue et al.
(2013), we extract such acti-vation as feature for ImageNet images that correspond to the 1,372 synsets, and train binary classifiersto detect the presence of the synsets in the images of Flickr8k.
More specifically, we use as our trainingset the 1,571,576 ImageNet images in the 1,372 synsets, where a random sample of 5,000 images servesas negative examples, and as our test set the 8,091 images in Flickr8k.
For each image in both sets, weextracted activation of a pre-trained CNN model as its feature.
The model is a reference implementationof the structure proposed in Krizhevsky et al.
(2012) with minor modifications, and is made publiclyavailable through the Caffe project (Jia, 2013).
It is shown in Donahue et al.
(2013) that the activationof layer 6 of the CNN performs the best for novel tasks.
Our study on a toy example with 10 ImageNetsynsets however suggests that the activation of layer 7 has a small edge.
Once the 4,096 dimensionalactivation of layer 7 is extracted for both training and test sets, 1,372 binary classifiers are trained andapplied using LIBSVM (Chang and Lin, 2011), which give probability estimates for the test images.
Foreach image, the 1,372 classifiers are then ranked in order of their probability estimates.4.3 Evaluation measureThe systems are evaluated using the Normalized Discounted Cumulative Gain (NDCG) (Wang et al.,2013) measure.
This measure is commonly used in Information Retrieval (IR) to evaluate ranked retrievalresults where each document is assigned a relevance score.
This measure favours rankings where themost relevant items are ranked ahead of less relevant items, and does not penalize irrelevant items.The NDCG at position k, NDCGk, for a set of test images I is defined as:NDCGk(I) =1|I||I|?i=11IDCGk(i)k?p=12Rp?
1log2(1 + p)(4)where Rpis the relevance score of the concept at position p, and IDCGk(i) is the ideal discountedcumulative gain for a perfect ranking algorithm at position k, which normalizes the overall measure tobe between 0.0 to 1.0.
This makes the scores comparable across rankings regardless of the number ofsynset groups involved.
For each grouping, we report the results of NDCGkfor the largest possible k(i.e.
the number of synset groups), which gives the overall performance of the rankings.4.4 ResultsTable 1 shows the results of re-ranking the output of the visual classifiers (Section 4.2), with differentsemantic groupings formed by varying ?.
The effects of the proposed groupings is apparent when com-pared to the random baseline groupings.
As we increase the value of ?
(allowing groups to have a largerrange of granularity), the NDCG scores also consistently increase.
However, higher NDCG scores donot necessarily equate to better groupings, as semantic groups with too much flexibility in granularitylevels may end up being less informative, for example by annotating a ?being?
in an image.
The in-formativeness of the groupings is a subjective issue depending on the context, and makes an interestingopen question.
To provide insight into the effects of our groupings, Figure 1 shows an example where atlow levels of ?
(rigid flexibility), the various dog species are highly ranked but none of them is consid-ered relevant by the evaluation system.
However, at ?
= 0.5 most dog species are grouped as a ?dog?semantic group resulting in a highly relevant prediction, while at the same time allowing the ?sidewalk?group to rise higher in the rankings.
At higher levels of ?, however, the semantic groupings become lessinformative when superordinate groups like ?being?, ?artifact?
and ?equipment?
are formed, suggestingthat higher flexibility with granularity levels may not always be more informative.5 Conclusions and future workWe presented a ?granularity-aware?
approach to grouping semantically related concepts across differentlevels of granularity, taking into consideration that different people describe the same thing in different43dog (5)road (2)pavement (1)street (1)?
= 0.0score: 0.241beagleboston terriercorgibassethoundspanielborder collieterrierdachshundpupst bernardbulldogspringer spanielleashkittenpetdogsheepdogpenguinsidewalk?
= 0.3score: 0.480beagleboston terrierdogbassethoundspanielborder collieterrierdachshundpupbulldogspringer spanielleashkittenpetpenguinsidewalkdobermancolliecat?
= 0.5score: 0.941doganimalleashkittenpetpenguinsidewalkcatartifactpersonstudentgoatlivestockrabbitduckbaseballchairchildfrisbeespectator?
= 0.8score: 0.943doganimalleashbeingbirdsidewalkcatartifactstudentbaseballchairchildfrisbeeballslopeequipmentfabricrugseatsupport?
= 1.0score: 0.944doganimalleashbeingbirdsidewalkcatartifactsportchairchilddeviceballslopeequipmentfabricrugseatsupportstickFigure 1: Example re-ranking of our visual classifier by semantic groupings, for selected values of?.
Words directly below the image indicate the ?gold standard?
nouns extracted automatically from itscorresponding five captions.
The number next to each noun indicate its relevance score.
For each re-ranking, we show the labels representing the semantic groupings.
Italicized labels indicate a match withthe (grouped) ?gold standard?
nouns (see Section 3.3).ways, and at varied levels of specificity.
To gain insight into the effects of our semantic groupings onhuman-centric applications, the proposed idea was investigated in the context of re-ranking the outputof visual classifiers, and was also incorporated during evaluation against human descriptions.
We foundthat although the groupings help provide a more human-centric and flexible image annotation system,too much flexibility may result in an uninformative image annotation system.
Future work could include(i) exploring different ways of grouping concepts; (ii) incorporating the output of visual classifiers toimprove both groupings and rankings; (iii) using information from more textual sources to improveimage annotation; (iv) taking the approach further to generate full sentence annotations.
We believe thatthese steps are important to bridge the semantic gap between computer vision and natural language.AcknowledgementsThe authors would like to acknowledge funding from the EU CHIST-ERA D2K programme, EPSRCgrant reference: EP/K01904X/1.ReferencesTamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010.
Automatic attribute discovery and characterizationfrom noisy web data.
In Proceedings of ECCV, volume 1, pages 663?676.Irving Biederman.
1995.
Visual object recognition.
In S. F. Kosslyn and D. N. Osherson, editors, An Invitation toCognitive Science, 2nd edition, Volume 2, Visual Cognition, pages 121?165.
MIT Press.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram Version 1.
In Linguistic Data Consortium.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM: A library for support vector machines.
ACM Transactionson Intelligent Systems and Technology, 2(3):1?27.
http://www.csie.ntu.edu.tw/?cjlin/libsvm.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
2009.
ImageNet: A large-scale hierarchicalimage database.
In Proceedings of CVPR.Jia Deng, Alexander C. Berg, Sanjeev Satheesh, Hao Su, Aditya Khosla, and Li Fei-Fei.
2012a.
ImageNet largescale visual recognition challenge (ILSVRC) 2012. http://image-net.org/challenges/LSVRC/2012/.Jia Deng, Jonathan Krause, Alexander C. Berg, and Li Fei-Fei.
2012b.
Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition.
In Proceedings of CVPR.Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2013.DeCAF: A deep convolutional activation feature for generic visual recognition.
arXiv:1310.1531 [cs.CV].44Pinar Duygulu, Kobus Barnard, Nando de Freitas, and David A. Forsyth.
2002.
Object recognition as machinetranslation: Learning a lexicon for a fixed image vocabulary.
In Proceedings of ECCV, pages 97?112.Ali Farhadi, Ian Endres, Derek Hoiem, and David A. Forsyth.
2009.
Describing objects by their attributes.
InProceedings of CVPR.Christiane Fellbaum, editor.
1998.
WordNet: An Electronic Lexical Database.
MIT Press, Cambridge, MA.Yansong Feng and Mirella Lapata.
2008.
Automatic image annotation using auxiliary text information.
In Pro-ceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-nologies, pages 272?280.
Association for Computational Linguistics.Abhinav Gupta and Larry S. Davis.
2008.
Beyond nouns: Exploiting prepositions and comparative adjectives forlearning visual classifiers.
In Proceedings of ECCV, pages 16?29.Geoffrey E. Hinton and Ruslan R. Salakhutdinov.
2006.
Reducing the dimensionality of data with neural networks.Science, 313:504?507.Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013.
Framing image description as a ranking task: Data,models and evaluation metrics.
Journal of Artificial Intelligence Research, 47:853?899.Yangqing Jia.
2013.
Caffe: An open source convolutional architecture for fast feature embedding.
http://caffe.berkeleyvision.org.Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.
2012.
ImageNet classification with deep convolutionalneural networks.
In Advances in Neural Information Processing Systems.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg.2011.
Baby talk: Understanding and generating simple image descriptions.
In Proceedings of CVPR.Chris H. Lampert, Hannes Nickisch, and Stefan Harmeling.
2009.
Learning to detect unseen object classes bybetween-class attribute transfer.
In Proceedings of CVPR.Y.
LeCun, B. Boser, J. Denker, D. Henerson, R. Howard, W. Hubbard, and L. Jackel.
1989.
Backpropagationapplied to handwritten zip code recognition.
Neural Computation, 1(4):541?551.Marcin Marsza?ek and Cordelia Schmid.
2008.
Constructing category hierarchies for visual recognition.
In DavidForsyth, Philip Torr, and Andrew Zisserman, editors, Proceedings of ECCV, volume 5305 of Lecture Notes inComputer Science, pages 479?491.
Springer Berlin Heidelberg.Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C. Berg, and Tamara L. Berg.
2013.
From large scale imagecategorization to entry-level categories.
In Proceedings of ICCV.Llu?
?s Padr?o and Evgeny Stanilovsky.
2012.
Freeling 3.0: Towards wider multilinguality.
In Proceedings of theLanguage Resources and Evaluation Conference, LREC ?12, Istanbul, Turkey, May.
ELRA.John C. Platt.
2000.
Probabilities for SV machines.
Advances in Large-Margin Classifiers, pages 61?74.Abebe Rorissa.
2008.
User-generated descriptions of individual images versus labels of groups of images: Acomparison using basic level theory.
Information Processing and Management, 44(5):1741?1753.Olga Russakovsky, Jia Deng, Jonathan Krause, Alexander C. Berg, and Li Fei-Fei.
2013.
ImageNet largescale visual recognition challenge (ILSVRC) 2013. http://image-net.org/challenges/LSVRC/2013/results.php.Richard Socher and Li Fei-Fei.
2010.
Connecting modalities: Semi-supervised segmentation and annotation ofimages using unaligned text corpora.
In Proceedings of CVPR, pages 966?973.Josiah Wang, Katja Markert, and Mark Everingham.
2009.
Learning models for object recognition from naturallanguage descriptions.
In Proceedings of BMVC.Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, and Tie-Yan Liu.
2013.
A theoretical analysis of NDCGranking measures.
In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013).Yahoo!
Webscope.
2014.
Yahoo!
Webscope dataset YFCC-100M.
http://labs.yahoo.com/Academic_Relations.Yezhou Yang, Ching Lik Teo, Hal Daum?e, III, and Yiannis Aloimonos.
2011.
Corpus-guided sentence generationof natural images.
In Proceedings of EMNLP, pages 444?454.45
