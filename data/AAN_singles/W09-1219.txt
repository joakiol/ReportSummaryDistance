Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 120?125,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Simple Generative Pipeline Approach to Dependency Parsing and Se-mantic Role LabelingDaniel Zeman?stav form?ln?
a aplikovan?
lingvistikyUniverzita Karlova v PrazeMalostransk?
n?mst?
25, Praha, CZ-11800, Czechiazeman@ufal.mff.cuni.czAbstractWe describe our CoNLL 2009 Shared Tasksystem in the present paper.
The system in-cludes three cascaded components: a genera-tive dependency parser, a classifier forsyntactic dependency labels and a semanticclassifier.
The experimental results show thatthe labeled macro F1 scores of our system onthe joint task range from 43.50% (Chinese) to57.95% (Czech), with an average of 51.07%.1 IntroductionThe CoNLL 2009 shared task is an extension ofthe tasks addressed in previous years: unlike theEnglish-only 2008 task, the present year deals withseven languages; and unlike 2006 and 2007, se-mantic role labeling is performed atop the surfacedependency parsing.We took part in the closed challenge of the jointtask.1 The input of our system contained gold stan-dard lemma, part of speech and morphological fea-tures for each token.
Tokens which wereconsidered predicates were marked in the inputdata.
The system was required to find the follow-ing information:?
parent (syntactic dependency) for each to-ken1For more details on the two tasks and challenges, see Haji etal.
(2009).?
label for each syntactic dependency (to-ken)?
label for every predicate?
for every token (predicate or non-predicate) A and every predicate P in thesentence, say whether there is a semanticrelation between P and A (A is an argu-ment of P) and if so, provide a label for therelation (role of the argument)The organizers of the shared task provided train-ing and evaluation data (Haji et al, 2006; Sur-deanu et al, 2008; Burchardt et al, 2006; Taul?
etal., 2008; Kawahara et al, 2002; Xue and Palmer,2009) converted to a uniform CoNLL Shared Taskformat.2 System DescriptionThe system is a sequence of three components: asurface syntactic parser, a syntactic tagger that as-signs labels to the syntactic dependencies and asemantic classifier (labels both the predicates andthe roles of their arguments).
We did not attempt togain advantage from training a joint classifier forall the subtasks.
We did not have time to do muchbeyond putting together the basic infrastructure.The components 2 and 3 are thus fairly primitive.2.1 Surface Dependency ParserWe use the parser described by Zeman (2004).
Theparser takes a generative approach.
It has a modelof dependency statistics in which a dependency is120specified by the lemma and tag of the parent andthe child nodes, by direction (left or right) and ad-jacency.
The core of the algorithm can be de-scribed as repeated greedy selecting of best-weighted allowed dependencies and adding themto the dependency tree.There are other components which affect the de-pendency selection, too.
They range from support-ing statistical models to a few hard-coded rules.However, some features of the parser are designedto work with Czech, or even with the Prague De-pendency Treebank.
For instance, there is a spe-cialized model for coordinative constructions.
Themodel itself is statistical but it depends on the PDTannotation guidelines in various ways.
Most nota-bly, the training component recognizes coordina-tion by the Coord dependency label, which is notpresent in other treebanks.
Other rules (e.g.
theconstraints on the set of allowed dependencies)rely on correct interpretation of the part-of-speechtags.In order to make the parser less language-dependent in the multilingual environment of theshared task, we disabled most of the abovemen-tioned treebank-bound features.
Of course, it led todecreased performance on the Czech data.22.2 Assignment of Dependency LabelsThe system learns surface dependency labels as afunction of the part-of-speech tags and features ofthe parent and the child node.
Almost no back-offis applied.
The most frequent label for the givenpair of tags (and feature structures) is always se-lected.
If the pair of tags is unknown, the label isbased on the features of the child node, and if it isunknown, too, the most frequent label of the train-ing data is selected.Obviously, both the training and the labelingprocedures have to know the dependencies.
Goldstandard dependencies are examined during train-ing while parser-generated dependencies are usedfor real labeling.2.3 Semantic ClassifierThe semantic component solves several tasks.First, all predicates have to be labeled.
Tokens that2However, the parser ?
without adaptation ?
would not dowell on Czech anyway because the PDT tags are presented ina different format in the shared task data.are considered predicates in the particular treebankare marked on input, so this is a simple classifica-tion problem.
Again, we took the path of least re-sistance and trained the PRED labels as a functionof gold-standard lemmas.Second, we have to find semantic dependencies.Any token (predicate or not) can be the argumentof one or more predicates.
These relations may ormay not be parallel to a syntactic dependency.
Foreach token, we need to find out 1. which predicatesit depends on, and 2. what is the label of its seman-tic role in this relation?The task is complex and there are apparently nosimple solutions to it.
We learn the semantic rolelabels as a function of the gold-standard part ofspeech of the argument, the gold-standard lemmaof the predicate and the flag whether there is a syn-tactic dependency between the two nodes or not.This approach makes it theoretically possible tomake one token semantically dependent on morethan one predicate.
However, we have no means tocontrol the number of the dependencies.3 ResultsThe official results of our system are given inTable 1.
The system made the least syntactic errors(attachment and labels) for Japanese.
The Japanesetreebank seems to be relatively easy to parse, asmany other systems achieved very high scores onthis data.
At the other end of the rating scale, Chi-nese seems to be the syntactically hardest lan-guage.
Our second-worst syntactic score was forCzech, most likely owing to the turning off all lan-guage-dependent (and Czech-biased) features ofthe parser.An obvious feature of the table is the extremelypoor semantic scores (in contrast to the accuracy ofsurface dependency attachment and labels).
Whilethe simplicity of the additional models does notseem to hurt too much the dependency labeling, itapparently is too primitive for semantic role label-ing.
We analyze the errors in more detail in Sec-tion 4.The system is platform-independent;3 we havebeen running all the experiments under Linux onan AMD Opteron 848 processor, 2 GHz, with32 GB RAM.
The running times and memory re-quirements are shown in Table 2.3It is written entirely in Perl.121To assess the need for data, Table 3 presents se-lected points on the learning curve of our system.The system has been retrained on 25, 50 and 75%of the training data for each language (the selectionprocess was simple: the first N% of sentences ofthe training data set were used).Generally, our method does not seem very data-hungry.
Even for Japanese, with the smallest train-ing data set, reducing training data to 25% of theoriginal size makes the scores drop less than 1%point.
The drop for other languages lies mostlybetween 1 and 2 points.
The exceptions are (unla-beled) syntactic attachment accuracies of Czechand Spanish, and labeled semantic F1 of Spanishand Chinese.
The Chinese learning curve also con-tains a nonmonotonic anomaly of syntactic de-pendency labeling between data sizes of 50 and75% (shown in boldface).
This can be probablyexplained by uneven distribution of the labels intraining data.As to the comparison of the various languagesand corpora, Japanese seems to be the most spe-cific (relatively high scores even with such smalldata).
Spanish and Catalan are related languages,their treebanks are of similar size, conform to simi-lar guidelines and were prepared by the same team.Their scores are very similar.4 DiscussionIn order to estimate sources of errors, we are nowgoing to provide some analysis of the data and theerrors our system does.4.1 DEPREL CoverageThe syntactic tagger (assigns DEPREL syntacticlabels) and the semantic tagger (assigns PRED andAPRED labels) are based on simple statisticalmodels without sophisticated back-off techniques.Language Cs En De Es Ca Ja ZhTraining sentences 43955 40613 38020 15984 14924 4643 24039Training tokens 740532 991535 680710 477810 443317 119144 658680Average sentence length 17 24 18 30 30 26 27Training minutes 9:21 10:41 8:28 6:17 5:42 1:24 7:01Training sentences per secnd 78 63 75 42 44 55 57Training tokens per second 1320 1547 1340 1267 1296 1418 1565Training rsize memory 3.9 GB 2.2 GB 2.7 GB 2.7 GB 2.4 GB 416 MB 1.5 GBTest sentences 4213 2399 2000 1725 1862 500 2556Test tokens 70348 57676 31622 50630 53355 13615 73153Parsing minutes 6:36 3:11 2:24 5:47 6:05 0:46 5:45Parsing sentences per second 10.6 12.6 13.9 5.0 5.1 10.9 7.4Parsing tokens per second 178 302 220 146 146 296 212Parsing rsize memory 980 MB 566 MB 779 MB 585 MB 487 MB 121 MB 444 MBTable 2.
Time and space requirements of the syntactic parser.Language Average Cs En De Es Ca Ja ZhLabeled macro F1 51.07 57.95 50.27 49.57 48.90 49.61 57.69 43.50OOD lab mac F1 43.67 54.49 48.56 27.97Labeled syn accur 64.92 57.06 61.82 69.79 65.98 67.68 82.66 49.48Unlab syn accur 70.84 66.04 70.68 72.91 71.22 73.81 83.36 57.87Syn labeling accur 79.20 69.10 74.24 84.63 81.83 82.46 95.98 66.13OOD lab syn acc 50.20 51.45 62.83 36.31OOD unl syn acc 58.08 60.56 71.78 41.90OOD syn labeling 69.65 65.64 75.22 68.08Semantic lab F1 32.14 58.13 36.05 16.44 25.36 24.19 30.13 34.71OOD sem lab F1 32.86 56.83 31.77 9.98Table 1.
The official results of the system.
ISO 639-1 language codes are used (cs = Czech, en = English, de = Ger-man, es = Spanish, ca = Catalan, ja = Japanese, zh = Chinese).
?OOD?
means ?out-of-domain test data?.122Sparse data could pose a serious problem.
So howsparse are the data?
Some cue could be drawn fromTable 3.
However, we should also know how oftenthe labels had to be assigned to an unknown set ofinput features.DEPREL (syntactic dependency label) is esti-mated based on morphological tag (i.e.
POS +FEAT) of both the child and parent.
If the pair oftags is unknown, then it is based on the tag of thechild, and if it is unknown, too, the most frequentlabel is chosen.
Coverage is high: 93 (Czech) to97 % (Chinese) of the pairs of tags in test datawere known from training data.
Moreover, the er-ror rate on the unknown pairs is actually muchlower than on the whole data!44.2 PRED CoveragePRED (predicate sense label) is estimated based onlemma.
For most languages, this seems to be agood selection.
Japanese predicate labels are al-ways identical to lemmas; elsewhere, there are byaverage 1.05 (Chinese) to 1.48 (Spanish) labels perlemma; the exception is German with a label-lemma ratio of 2.33.Our accuracy of PRED label assignment rangesfrom 71% (German) to 100% (Japanese).
We al-ways assign the most probable label for the given4This might suggest that the input features are chosen inap-propriately and that the DEPREL label should be based just onthe morphology of the child.lemma; if the lemma is unknown, we copy thelemma to the PRED column.
Coverage is not anissue here.
It goes from 94% (Czech) to almost100% (German).5 The accuracy on unknown lem-mas could probably be improved using the sub-categorization dictionaries accompanying thetraining data.Language Lemma PREDs1.
m?t 77 Cs 2. pijmout 81. take 20 En 2. go 181. kommen 28 De 2. nehmen 251. pasar 101. dar 103. llevar 9 Es3.
hacer 91. fer 11 Ca 2. pasar 9Ja Always 1 PRED per lemma1.
(y?o) 81.
(yu) 8 Zh1.
(d) 8Table 4.
Most homonymous predicates.5The coverage of Japanese is 88% but since Japanese PREDlabels are exact copies of lemmas, even unknown lemmasyield 100%-correct labels.Score TrSize Average Cs En De Es Ca Ja Zh25% 69.38 63.72 69.70 71.36 68.99 72.41 82.58 56.9050% 70.14 64.96 70.13 72.11 70.37 72.83 82.99 57.5875% 70.51 65.50 70.37 72.50 70.83 73.47 83.17 57.73UnLabSynAttach 100% 70.84 66.04 70.68 72.91 71.22 73.81 83.36 57.8725% 78.47 68.28 73.79 84.21 80.67 81.92 95.70 64.7150% 78.94 68.68 74.08 84.44 81.59 81.99 95.86 65.9475% 79.03 68.87 74.14 84.51 81.67 82.19 95.97 65.83SynLabel100% 79.20 69.10 74.24 84.63 81.83 82.46 95.98 66.1325% 30.10 56.29 34.47 15.51 22.78 22.14 28.91 30.5850% 33.85 57.24 35.34 16.03 24.46 23.13 29.60 33.3175% 31.76 57.76 35.85 16.29 24.96 23.77 29.96 33.71LabeledSem F1100% 32.14 58.13 36.05 16.44 25.36 24.19 30.13 34.7125% 49.19 55.87 49.06 48.10 46.22 47.76 56.66 40.6450% 50.28 56.99 49.66 48.90 47.97 48.53 57.23 42.6675% 50.68 57.53 50.01 49.26 48.47 49.21 57.52 42.73LabeledMacroF1 100% 51.07 57.95 50.27 49.57 48.90 49.61 57.69 43.50Table 3.
The learning curve of the principal scores.1234.3 APRED Assignment AnalysisThe most complicated part of the task is the as-signment of the APRED labels.
In a sense, APREDlabeling is dependency parsing on a deeper level.
Itconsists of several sub-problems:?
Is the node an argument of any predicate atall??
If so, how many predicates is the node ar-gument of?
Should the predicate be, say,coordination, then the node would seman-tically depend on all members of the coor-dination.?
In what way is the semantic dependencyrelated to the syntactic dependency be-tween the node and its syntactic parent?
Inmajority of cases, syntactic and semanticdependencies go parallel; however, thereare still a significant number of semanticrelations for which this assumption doesnot hold.6?
Once we know that there is a semantic re-lation (an APRED field should not beempty), we still have to figure out the cor-rect APRED label.
This is the semanticrole labeling (or tagging) proper.6Nearly all Spanish and Catalan semantic dependencies areparallel to syntactic ones (but not all syntactic dependenciesare also semantic); in most other languages, about two thirdsof semantic relations match syntax.
Japanese is the only lan-guage in which this behavior does not prevail.Our system always makes semantic roles paral-lel to surface syntax.
It even does not allow forempty APRED if there is a syntactic dependency?this turned out to be one of the major sources oferrors.7The role labels are estimated based on thelemma of the predicate and the part of speech ofthe argument.
Low coverage of this pair of featuresin the training data turns to be another majorsource of errors.
If the pair is not known fromtraining data, the system selects the most frequentAPRED in the given treebank.
Table 5 gives anoverview of the principal statistics relevant to theanalysis of APRED errors.5 Post-evaluation ExperimentsFinally, we performed some preliminary experi-ments focused on the syntactic parser.
As men-tioned in Section 2.1, many features of the parserhave to be turned off unless the parser understandsthe part-of-speech and morphological features.
Weused DZ Interset (Zeman, 2008) to convert Czechand English CoNLL POS+FEAT strings to PDT-like positional tags.
Then we switched back on theparser options that use up the tags and re-ran pars-ing.
The results (Table 6) confirm that the tag ma-nipulation significantly improves Czech parsingwhile it does not help with English.7This is a design flaw that we overlooked.
Most likely, mak-ing empty APRED one of the predictable values would im-prove accuracy.Language Cs En De Es Ca Ja ZhPotential APRED slots 1287545 195029 12066 192103 197976 57394 329757Filled in APREDs 87934 32968 10480 49904 52786 6547 49047Feature pair coverage (%) 46.05 40.04 14.99 29.34 29.89 18.31 38.08Non-empty APRED accuracy 73.19 64.65 67.37 56.90 57.89 59.20 68.77Unlabeled precision 34.94 26.86 10.88 21.71 20.25 9.13 25.66Unlabeled recall 62.61 63.86 97.52 93.40 92.72 22.10 67.82Unlabeled F 44.86 37.81 19.57 35.23 33.24 12.93 37.23Labeled precision 25.58 17.36 7.33 12.35 11.72 5.41 17.64Labeled recall 45.83 41.28 65.70 53.15 53.67 13.08 46.64Labeled F 32.83 24.44 13.19 20.05 19.24 7.65 25.60Table 5.
APRED detailed analysis.
Non-empty APRED accuracy includes only APRED cells that were non-emptyboth in gold standard and system output.
Feature-pair coverage includes all cells filled by the system.
Unlabeled preci-sion and recall count non-empty vs. empty APREDs without respect to their actual labels.
Counted on developmentdata with gold-standard surface syntax.124Cs EnBefore 65.81 69.48After 71.76 68.92Table 6.
Unlabeled attachment accuracy on de-velopment data before and after tagset conversion.6 ConclusionWe described one of the systems that participatedin the CoNLL 2009 Shared Task.
We analyzed theweaknesses of the system and identified possibleroom for improvement.
The most important pointto focus on in future work is specifying whereAPRED should be filled in.
The links between syn-tactic and semantic structures have to be studiedfurther.
Subcategorization frames could probablyhelp improve these decisions, too?our presentsystem ignores the subcategorization dictionariesthat accompany the participating treebanks.AcknowledgmentsThis research has been supported by the Ministryof Education of the Czech Republic, project No.MSM0021620838.ReferencesAljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pad?
and Manfred Pinkal.
2006.The SALSA Corpus: a German Corpus Resource forLexical Semantics.
Proceedings of the 5th Interna-tional Conference on Language Resources andEvaluation (LREC-2006).
Genova, Italy.Jan Haji, Massimiliano Ciaramita, Richard Johansson,Daisuke Kawahara, Maria Antonia Mart?, Llu?sM?rquez, Adam Meyers, Joakim Nivre, SebastianPad?, Jan ?tp?nek, Pavel Stra?k, Mihai Surdeanu,Nianwen Xue and Yi Zhang.
2009.
The CoNLL-2009Shared Task: Syntactic and Semantic Dependenciesin Multiple Languages.
Proceedings of the 13th Con-ference on Computational Natural Language Lear-ning (CoNLL-2009).
June 4-5. pp.
3-22.
Boulder,Colorado, USA.Jan Haji, Jarmila Panevov?, Eva Hajiov?, Petr Sgall,Petr Pajas, Jan ?tp?nek, Ji?
Havelka, Marie Miku-lov?
and Zdenk ?abokrtsk?.
2006.
The Prague De-pendency Treebank 2.0.
CD-ROM.
Linguistic DataConsortium, Philadelphia, Pennsylvania, USA.
ISBN1-58563-370-4.
LDC Cat.
No.
LDC2006T01.
URL:http://ldc.upenn.edu/.Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida.2002.
Construction of a Japanese Relevance-taggedCorpus.
Proceedings of the 3rd International Confer-ence on Language Resources and Evaluation (LREC-2002).
pp.
2008-2013.
Las Palmas, Spain.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?s M?rquez and Joakim Nivre.
2008.
The CoNLL-2008 Shared Task on Joint Parsing of Syntactic andSemantic Dependencies.
In Proceedings of the 12thConference on Computational Natural LanguageLearning (CoNLL-2008).
August 16 ?
17.
Manches-ter, UK.Mariona Taul?, Maria Ant?nia Mart?
and Marta Reca-sens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
Proceedings of the 6th In-ternational Conference on Language Resources andEvaluation (LREC-2008).
Marrakech, Morocco.Nianwen Xue and Martha Palmer.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143-172.Daniel Zeman.
2004.
Parsing with a Statistical Depend-ency Model (PhD thesis).
Univerzita Karlova, Praha,Czechia.
URL: http://ufal.mff.cuni.cz/~zeman/pro-jekty/parser/index.htmlDaniel Zeman.
2008.
Reusable Tagset Conversion Us-ing Tagset Drivers.
In Proceedings of the 6th Interna-tional Conference on Language Resources andEvaluation (LREC-2008).
ISBN 2-9517408-4-0.Marrakech, Morocco.125
