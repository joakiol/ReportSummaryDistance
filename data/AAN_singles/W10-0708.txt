Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 53?56,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUsing Amazon Mechanical Turk for Transcription of Non-Native SpeechKeelan Evanini, Derrick Higgins, and Klaus ZechnerEducational Testing Service{KEvanini, DHiggins, KZechner}@ets.orgAbstractThis study investigates the use of AmazonMechanical Turk for the transcription of non-native speech.
Multiple transcriptions wereobtained from several distinct MTurk workersand were combined to produce merged tran-scriptions that had higher levels of agreementwith a gold standard transcription than the in-dividual transcriptions.
Three different meth-ods for merging transcriptions were comparedacross two types of responses (spontaneousand read-aloud).
The results show that themerged MTurk transcriptions are as accurateas an individual expert transcriber for the read-aloud responses, and are only slightly less ac-curate for the spontaneous responses.1 IntroductionOrthographic transcription of large amounts ofspeech is necessary for improving speech recogni-tion results.
Transcription, however, is a time con-suming and costly procedure.
Typical transcriptionspeeds for spontaneous, conversational speech arearound 7 to 10 times real-time (Glenn and Strassel,2008).
The transcription of non-native speech is aneven more difficult task?one study reports an aver-age transcription time of 12 times real-time for spon-taneous non-native speech (Zechner, 2009).In addition to being more costly and time consum-ing, transcription of non-native speech results in ahigher level of disagreement among transcribers incomparison to native speech.
This is especially truewhen the speaker?s proficiency is low and the speechcontains large numbers of grammatical errors, in-correct collocations, and disfluencies.
For exam-ple, one study involving highly predictable speechshows a decline in transcriber agreement (measuredusing Word Error Rate, WER) from 3.6% for na-tive speech to 6.4% for non-native speech (Marge etal., to appear).
Another study involving spontaneousnon-native speech showed a range of WER between15% and 20% (Zechner, 2009).This study uses the Amazon Mechanical Turk(MTurk) resource to obtain multiple transcriptionsfor non-native speech.
We then investigate severalmethods for combining these multiple sources of in-formation from individual MTurk workers (turkers)in an attempt to obtain a final merged transcriptionthat is more accurate than the individual transcrip-tions.
This methodology results in transcriptionsthat approach the level of expert transcribers on thisdifficult task.
Furthermore, a substantial savings incost can be achieved.2 Previous WorkDue to its ability to provide multiple sources ofinformation for a given task in a cost-effectiveway, several recent studies have combined multi-ple MTurk outputs for NLP annotation tasks.
Forexample, one study involving annotation of emo-tions in text used average scores from up to 10 turk-ers to show the minimum number of MTurk anno-tations required to achieve performance compara-ble to experts (Snow et al, 2008).
Another studyused preference voting to combine up to 5 MTurkrankings of machine translation quality and showedthat the resulting judgments approached expert inter-annotator agreement (Callison-Burch, 2009).
These53tasks, however, are much simpler than transcription.MTurk has been used extensively as a transcrip-tion provider, as is apparent from the success of amiddleman site that act as an interface to MTurkfor transcription tasks.1 However, to our knowledge,only one previous study has systematically evaluatedthe quality of MTurk transcriptions (Marge et al,to appear).
This recent study also combined multi-ple MTurk transcriptions using the ROVER method(Fiscus, 1997) to produce merged transcriptions thatapproached the accuracy of expert transcribers.
Ourstudy is similar to that study, except that the speechdata used in our study is much more difficult totranscribe?the utterances used in that study were rel-atively predictable (providing route instructions forrobots), and contained speech from native speak-ers and high-proficiency non-native speakers.
Fur-thermore, we investigate two additional merging al-gorithms in an attempt to improve over the perfor-mance of ROVER.3 Experimental Design3.1 AudioThe audio files used in this experiment consist ofresponses to an assessment of English proficiencyfor non-native speakers.
Two different types of re-sponses are examined: spontaneous and read-aloud.In the spontaneous task, the speakers were asked torespond with their opinion about a topic describedin the prompt.
The speech in these responses is thushighly unpredictable.
In the read-aloud task, on theother hand, the speakers were asked to read a para-graph out loud.
For these responses, the speech ishighly predictable; any deviations from the targetscript are due to reading errors or disfluencies.For this experiment, one set of 10 spontaneous(SP) responses (30 seconds in duration) and two setsof 10 read-aloud (RA) responses (60 seconds in du-ration) were used.
Table 1 displays the characteris-tics of the responses in the three batches.3.2 Transcription ProcedureThe tasks were submitted to the MTurk interface inbatches of 10, and a turker was required to completethe entire batch in order to receive payment.
Turkers1http://castingwords.com/Batch Duration # of Words(Mean)# of Words(Std.
Dev.
)SP 30 sec.
33 14RA1 60 sec.
97 4RA2 60 sec.
93 10Table 1: Characteristics of the responses used in the studyreceived $3 for a complete batch of transcriptions($0.30 per transcription).Different interfaces were used for transcribing thetwo types of responses.
For the spontaneous re-sponses, the task was a standard transcription task:the turkers were instructed to enter the words thatthey heard in the audio file into a text box.
For theread-aloud responses, on the other hand, they wereprovided with the target text of the prompt, one wordper line.
They were instructed to make annotationsnext to words in cases where the speaker deviatedfrom the target text (indicating substitutions, dele-tions, and insertions).
For both types of transcriptiontask, the turkers were required to successfully com-plete a short training task before proceeding onto thebatch of 10 responses.4 Methods for Merging Transcriptions4.1 ROVERThe ROVER method was originally developed forcombining the results from multiple ASR systems toproduce a more accurate hypothesis (Fiscus, 1997).This method iteratively aligns pairs of transcriptionsto produce a word transition network.
A voting pro-cedure is then used to produce the merged transcrip-tion by selecting the most frequent word (includingNULL) in each correspondence set; ties are brokenby a random choice.4.2 Longest Common SubsequenceIn this method, the Longest Common Subsequence(LCS) among the set of transcriptions is found byfirst finding the LCS between two transcriptions,comparing this output with the next transcription tofind their LCS, and iterating over all transcriptions inthis manner.
Then, each transcription is compared tothe LCS, and any portions of the transcription thatare missing between words of the LCS are tallied.Finally, words are interpolated into the LCS by se-54lecting the most frequent missing sequence from theset of transcriptions (including the empty sequence);as with the ROVER method, ties are broken by a ran-dom choice among the most frequent candidates.4.3 LatticeIn this method, a word lattice is formed from theindividual transcriptions by iteratively adding tran-scriptions into the lattice to optimize the match be-tween the transcription and the lattice.
New nodesare only added to the graph when necessary.
Then,to produce the merged transcription, the optimalpath through the lattice is determined.
Three dif-ferent configurations for computing the optimal paththrough the lattice method were compared.
In thefirst configuration, ?Lattice (TW),?
the weight ofa path through the lattice is determined simply byadding up the total of the weights of each edgein the path.
Note that this method tends to fa-vor longer paths over shorter ones, assuming equaledge weights.
In the next configuration, ?Lattice(AEW),?
a cost for each node based on the aver-age edge weight is subtracted as each edge of thelattice is traversed, in order to ameliorate the prefer-ence for longer paths.
Finally, in the third configura-tion, ?Lattice (TWPN),?
the weight of a path throughthe lattice is defined as the total path weight in the?Lattice (TW)?
method, normalized by the numberof nodes in the path (again, to offset the preferencefor longer paths).4.4 WER calculationAll three of the methods for merging transcriptionsare sensitive to the order in which the individualtranscriptions are considered.
Thus, in order to accu-rately evaluate the methods, for each number of tran-scriptions used to create the merged transcription,N ?
{3, 4, 5}, all possible permutations of all pos-sible combinations were considered.
This resultedin a total of 5!(5?N)!
merged transcriptions to be eval-uated.
For each N, the overall WER was computedfrom this set of merged transcriptions.5 ResultsTables 2 - 4 present the WER results for differ-ent merging algorithms for the two batches of read-aloud responses and the batch of spontaneous re-sponses.
In each table, the merging methods are or-Method N=3 N=4 N=5Individual Turkers 7.0%Lattice (TWPN) 6.4% 6.4% 6.4%Lattice (TW) 6.4% 6.4% 6.4%LCS 6.0% 5.6% 5.6%Lattice (AEW) 6.1% 6.0% 5.5%ROVER 5.5% 5.2% 5.1%Expert 4.7%Table 2: WER results 10 read-aloud responses (RA1)Method N=3 N=4 N=5Individual Turkers 9.7%Lattice (TW) 9.5% 9.5% 9.4%Lattice (TWPN) 8.3% 8.0% 8.0%Lattice (AEW) 8.2% 7.4% 7.8%ROVER 7.9% 7.9% 7.6%LCS 8.3% 8.0% 7.5%Expert 8.1%Table 3: WER results for 10 read-aloud responses (RA2)dered according to their performance when all tran-scriptions were used (N=5).
In addition, the overallWER results for the individual turkers and an experttranscriber are provided for each set of responses.In each case, the WER is computed by comparisonwith a gold standard transcription that was createdby having an expert transcriber edit the transcriptionof a different expert transcriber.In all cases, the merged transcriptions have alower WER than the overall WER for the individualturkers.
Furthermore, for all methods, the mergedoutput using all 5 transcriptions has a lower (orequal) WER to the output using 3 transcriptions.
Forthe first batch of read-aloud responses, the ROVERmethod performed best, and reduced the WER inthe set of individual transcriptions by 27.1% (rela-tive) to 5.1%.
For the second batch of read-aloudresponses, the LCS method performed best, and re-duced the WER by 22.6% to 7.5%.
Finally, for thebatch of spontaneous responses, the Lattice (TW)method performed best, and reduced the WER by25.6% to 22.1%.55Method N=3 N=4 N=5Individual Turkers 29.7%Lattice (TWPN) 29.1% 28.9% 28.3%LCS 29.2% 28.4% 27.0%Lattice (AEW) 28.1% 25.8% 25.1%ROVER 25.4% 24.5% 24.9%Lattice (TW) 25.5% 23.5% 22.1%Expert 18.3%Table 4: WER results for 10 spontaneous responses6 ConclusionsAs is clear from the levels of disagreement be-tween the expert transcriber and the gold standardtranscription for all three tasks, these responses aremuch more difficult to transcribe accurately thannative spontaneous speech.
For native speech, ex-pert transcribers can usually reach agreement lev-els over 95% (Deshmukh et al, 1996).
For theseresponses, however, the WER for the expert tran-scriber was worse than this even for the read-aloudspeech.
These low levels of agreement can be at-tributed to the fact that the speech is drawn from awide range of English proficiency levels among test-takers.
Most of the responses contain disfluencies,grammatical errors, and mispronunciations, leadingto increased transcriber uncertainty.The results of merging multiple MTurk transcrip-tions of this non-native speech showed an improve-ment over the performance of the individual tran-scribers for all methods considered.
For the read-aloud speech, the agreement level of the mergedtranscriptions approached that of the expert tran-scription when only three MTurk transcriptions wereused.
For the spontaneous responses, the perfor-mance of the best methods still lagged behind the ex-pert transcription, even when five MTurk transcrip-tions were used.
Due to the consistent increase inperformance, and the low cost of adding additionaltranscribers (in this study the cost was $0.30 per au-dio minute for read-aloud speech and $0.60 per au-dio minute for spontaneous speech), the approach ofcombining multiple transcriptions should always beconsidered when MTurk is used for transcription.
Itis also possible that lower payments per task couldbe provided without a decrease in transcription qual-ity, as demonstrated by Marge et al (to appear).
Ad-ditional experiments will address the practicality ofproducing more accurate merged transcriptions foran ASR system?simply collecting larger amountsof non-expert transcriptions may be a better invest-ment than producing higher quality data (Novotneyand Callison-Burch, 2010).It is interesting that the Lattice (TW) methodof merging transcriptions clearly outperformed allother methods for the spontaneous responses, butwas less beneficial than the LCS and ROVER meth-ods for read-aloud speech.
It is likely that this iscaused by the preference of the Lattice (TW) methodfor longer paths through the word lattice, since indi-vidual transcribers of spontaneous speech may markdifferent words as unitelligible, even though thesewords exist in the gold standard transcription.
Fur-ther studies with a larger number of responses willbe needed to test this hypothesis.ReferencesChris Callison-Burch.
2009.
Fast, cheap and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In Proc.
EMNLP.Neeraj Deshmukh, Richard Jennings Duncan, AravindGanapathiraju, and Joseph Picone.
1996.
Benchmark-ing human performance for continuous speech recog-nition.
In Proc.
ICSLP.Jonathan G. Fiscus.
1997.
A post-processing system toyield word error rates: Recognizer Ooutput Voting Er-ror Reduction (ROVER).
In Proc.
ASRU.Meghan Lammie Glenn and Stephanie Strassel.
2008.Shared linguistic resources for the meeting domain.In Lecture Notes in Computer Science, volume 4625,pages 401?413.
Springer.Matthew Marge, Satanjeev Banerjee, and Alexander I.Rudnicky.
to appear.
Using the Amazon MechanicalTurk for transcription of spoken language.
In Proc.ICASSP.Scott Novotney and Chris Callison-Burch.
2010.
Cheap,fast, and good enough: Automatic speech recognitionwith non-expert transcription.
In Proc.
NAACL.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast ?
But is itgood?
Evaluating non-expert annotations for naturallanguage tasks.
In Proc.
EMNLP.Klaus Zechner.
2009.
What did they actually say?Agreement and disagreement among transcribers ofnon-native spontaneous speech responses in an En-glish proficiency test.
In Proc.
ISCA-SLaTE.56
