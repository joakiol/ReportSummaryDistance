Last WordsImproving Our Reviewing ProcessesInderjeet Mani?The MITRE Corporation1.
IntroductionOur reviewing practices today are failing.
With the number of ACL submissions steadilygrowing over the last several years (for example, ACL 2009 had a 24% increase in sub-missions over ACL 2008), the need for more reviewers has become more pronounced.However, qualified reviewers are becoming hard to find, and when they are found,they are often hard-pressed for time.
As a result, slipshod reviews are becoming com-monplace.
Allowing this situation to continue as before will result in the deteriorationof our ability to recognize excellence in our research.
An ?intervention?
is thereforeneeded.As I see it, there are two distinct problems to tackle: first, a lack of qualifiedreviewers, and second, a lack of quality control in reviews.
After discussing these, Iwill suggest some solutions that I believe are worth implementing.2.
Problems with Reviewing2.1 The Lack of Qualified ReviewersIn an earlier Last Words piece, Ken Church (Church 2006) pointed out how the ACLconference reviewing process can be derailed by the lack of positive endorsement byreviewers who are not well qualified to review a given paper.
He went on to suggestthat papers rejected by NAACL are ?often strong contenders for the best-paper awardat ACL.?
An instance of this phenomenon was observed in 2009, when a paper rejectedfrom NAACL 2009 with an average acceptance score of 2.3 out of 5 was given a bestpaper award at ACL 2009 (Branavan et al 2009).1It is especially hard to find qualified reviewers these days partly because compu-tational linguistics has become increasingly specialized.
Papers in fields like parsingand machine translation involve very technical modifications to a few current models.Reviewers for such areas need to be ?insiders?, well-versed in the latest developments inthe sub-area.
This need is likely to become more pronounced as the specialization trendcontinues.Reviewers are currently selected based on informal social networks.
Unfortunately,most researchers do not have an extensive set of names of reviewers at hand, and relying?
The MITRE Corporation, 202 Burlington Road, Bedford, MA 01730, USA.
E-mail: imani@mitre.org.1 Such discrepancies in judgments across conferences are not confined to computational linguistics; forexample, the classic Page Rank paper from WWW 1998 (Brin and Page 1998) had previously beenrejected by SIGIR 1998.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 1on personal connections (not to mention memories of reviewers?
prior performance)is limiting and could bias the selection of reviewers to those who share a particularpoint-of-view.
This lack of information as to whom to contact can result in woefullyinappropriate selections of reviewers.2.2 The Lack of Quality ControlEven when qualified reviewers can be found, reviews are often hurried.
At the 2009 ACLBusiness Meeting, Ido Dagan pointed to the growing dissatisfaction with the quality ofconference reviewing, adding that the problems seemed to be exacerbated by increasingthe number of reviewers.
The lack of quality is in part due to the large number ofconferences that compete for the reviewer?s time.
As Fortnow (2009) observes, in thecase of computer science conferences the intensive time commitment required for re-viewing makes it less likely that more experienced researchers will sign on as reviewers.Such hurried reviewing and decision-making can result in a preference for safe, moreincremental papers rather than those that develop new models and research directions(Fortnow 2009).3.
Finding Qualified Reviewers: An ACL Reviewer DatabaseTo encourage better selection of reviewers, improved information management isneeded in terms of keeping track of reviewer background and areas of expertise.
Thisgoal can be achieved by maintaining an ACL database of reviewer profiles that is accessibleto the public, as a resource for use in selecting reviewers.
It would work as follows:Whenever reviews are produced in an ACL-related forum, the Program Chair or Editorwould be responsible for updating the database (assisted in part by the conference orjournal management software).
S/he would record the reviewer?s name and affiliation,the name and type of forum (conference, workshop, journal, etc.
), its time and place, thenumber of papers reviewed, and the reviewer?s areas of interest.
To protect a reviewer?sprivacy, information as to which papers were assigned to the reviewer should not berevealed.
Note that most conferences already publish a list of reviewers involved, sothis is not a huge step beyond what is there today.Once created, the ACL Reviewer Database would provide for easier selection ofreviewers, and would allow a forum organizer or editor to determine, at least semi-automatically, categories of reviewers such as specialist/generalist, prolific/occasional,and skill level (e.g., journal paper vs. workshop reviewing, senior vs. extended programcommittee experience).Many reviewers work extremely hard at their reviews.
Being recognized as awell-respected reviewer is well worth striving for, and ideally, this reputation wouldbe reflected in part by one?s reviewing profile.
Just as the impacts of journals andauthors are measured based on citations, there is no reason why reviewers shouldnot be assessed in terms of their impacts in guiding and encouraging computationallinguistics.
Specifically, the database can be used to track how many journal andconference articles a person has reviewed, and how many articles were submittedto those venues.
A reviewer?s impact factor can be computed by dividing the for-mer by the latter, and including some normalization parameters (such as taking intoaccount the level of publishing activity in the particular computational linguisticssubfield).262Mani Improving our Reviewing Processes4.
Encouraging High-Quality Reviews4.1 Open Peer Review: Signing and Publication HistoryReviewers often get away with slipshod, low-quality reviews because they can hideunder the cloak of anonymity.
Other research communities have reduced the level ofanonymity by using open peer review.
In an open peer-review system, reviewers maysometimes become known to the paper authors, and in some instances, reviewers andthe content of their reviews may become known to all the readers as well.
Journals thathave successfully deployed open peer-review include Atmospheric Chemistry and Physics(ACP),2 the British Medical Journal (BMJ),3 and all forty-one of the Biomed Central (BMC)medical journals.4Open peer-review systems can differ in terms of reviewer transparency: Somevenues (e.g., the BMJ, and BMC medical journals such as BMC Cancer) always requirethat reviews be signed, that is, visible to the author, whereas others do not (e.g., theACP journal), or else they leave it up to the reviewer.
Signed reviews can considerablyraise the stakes on review quality: It is one thing to palm off a hurried review on ahapless author, quite another to be accountable in front of all one?s colleagues for thepoor quality of one?s review.
Although it is theoretically possible that signed reviewsmight be less frank, in order to avoid alienating particular authors, such a problemtends not to occur in practice.
In the case of a randomized trial with medical articlesin the BMJ, signing did not lower the review quality or recommendations (van Rooyenet al 1999).
Nor has the presence of signed reviews in journals that have used them ledto a rise in litigation against those journals.
Finally, many journals using signed reviews,such as BMC Cancer, continue to thrive and flourish.Whereas the BMJ publishes only the final version of the paper, both the BMC med-ical journals and the ACP journal provide public access to the publication history, namely,all previous versions of the paper along with their reviews and author responses.5 Aview of such a publication history can be extremely instructive to both prospectiveauthors and reviewers.
A further advantage of publication history is that prior submis-sions within the community can be tracked, providing a collective memory of reviewers?comments.
That is far better than the situation today, where regular reviewers can oftenrecall reviewing some version of a paper earlier for some other forum, but may not beable to recall the individual recommendations.4.2 Reviewer TrainingReviewing is one of the most important activities a researcher carries out, and yet noformal training is provided to reviewers.
If we require high-quality reviews, we needto train reviewers as to the best practices in reviewing as carried out by a particularACL-related forum.
This could be organized as an on-line course specific to the journalor conference, which every reviewer for that forum should take.
The course, even if2 http://www.atmos-chem-phys.net/volumes and issues.html.3 http://www.bmj.com/.4 http://www.biomedcentral.com/.5 For an example of an accepted paper with signed reviews and author comments, seewww.biomedcentral.com/1471-2407/9/348/prepub.
For an interesting discussion around a rejectedpaper, see the following ACP paper: http://preview.tinyurl.com/ycxq65e.263Computational Linguistics Volume 37, Number 1involving only self-study, can be viewed as a more stringent requirement than simplybeing encouraged to read, as is customary today, the reviewing guidelines.
Here, too,the conference or journal management software can check that the course has beencompleted within the last couple of years before allowing the reviewer to proceed.
Thecourse can be updated from time to time.As an example, journals like the BMJ offer materials for training reviewers.6 Thesematerials include PowerPoint presentations on reviewing best practices (?What weknow about peer review,?
?What editors want?)
and written exercises (reading andassessing three referee reports for a paper, and comparing the assessments with theeditor?s critique of those reports, as well as doing a practice review of a paper andcomparing it with the published reviews).
As shown in a BMJ study of the effectivenessof these materials (Schroter et al 2004), trained reviewers detected significantly moremajor errors in papers than those who weren?t trained.For computational linguistics journals and conferences, it would be straightfor-ward, under either an open peer review system and/or with permission of authors andreviewers, to collect published reviews of several articles and have potential reviewersgo through a similar exercise to that provided by the BMJ.
In particular, one could focuson practice reviews for a paper.
In addition to such on-line courses, improved reviewquality can be engendered by discussion of reviewing methods in classroom settings,particularly in seminar courses where papers have to be read and jointly discussed.5.
ConclusionsTo discover qualified reviewers, I have suggested creating an ACL Reviewer Database.To improve review quality, I have advocated more use of open peer review, with reviewsigning and/or maintaining a history of the publication of each article, as well as specificmeasures for improved reviewer training.
These methods for improving review qualityhave become common practice in some other fields, and it is high time computationallinguists started to explore them.These improvements will require a higher degree of transparency than has beencustomary in computational linguistics reviewing, but this transparency will reap con-siderable benefits in further recognizing and promoting excellence in our research.
Itwould therefore be worthwhile to initiate a discussion group or a workshop to plan apilot that will try out some of these methods.
Over time, it will also be useful to conductstudies of how effective these methods are.AcknowledgmentsThis research has been funded by the MITREInnovation Program (Public Release CaseNumber 07-0862).
I am grateful to RobertDale for his in-depth comments.ReferencesBranavan, S. R. K., Harr Chen, LukeZettlemoyer, and Regina Barzilay.
2009.Reinforcement learning for mappinginstructions to actions.
Proceedings of theJoint Conference of the 47th Annual Meetingof the ACL and the 4th International JointConference on Natural Language Processingof the AFNLP, pages 82?90, Singapore,August 2?7.Brin, Sergey and Lawrence Page.
1998.
Theanatomy of a large-scale hypertextualWeb search engine.
Proceedings of theSeventh International Conference on the6 http://resources.bmj.com/bmj/reviewers/training-materials.264Mani Improving our Reviewing ProcessesWorld Wide Web, pages 107?117, Brisbane,Australia, April 14?18.Church, Kenneth.
2006.
Reviewing thereviewers.
Computational Linguistics,31(4):575?578.Fortnow, Lance.
2009.
Viewpoint: Timefor computer science to grow up.Communications of the ACM, 52(8):33?35.Schroter, Sara, Nick Black, Stephen Evans,James Carpenter, Fiona Godlee, andRichard Smith.
2004.
Effects of trainingon quality of peer review: Randomisedcontrolled trial.
British Medical Journal,328:673.van Rooyen, Susan, Fiona Godlee, StephenEvans, Nick Black, and Richard Smith.1999.
Effect of open peer review onquality of reviews and on reviewers?recommendations: A randomised trial.British Medical Journal, 318:23?27.265
