Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 35?42,Beijing, August 2010Co-occurrence Graph Based Iterative Bilingual Lexicon Extraction FromComparable CorporaDiptesh Chatterjee and Sudeshna Sarkar and Arpit MishraDepartment of Computer Science and EngineeringIndian Institute of Technology Kharagpur{diptesh,sudeshna,arpit}@cse.iitkgp.ernet.inAbstractThis paper presents an iterative algorithmfor bilingual lexicon extraction from com-parable corpora.
It is based on a bag-of-words model generated at the level ofsentences.
We present our results of ex-perimentation on corpora of multiple de-grees of comparability derived from theFIRE 2010 dataset.
Evaluation results on100 nouns shows that this method outper-forms the standard context-vector basedapproaches.1 IntroductionBilingual dictionaries play a pivotal role in a num-ber of Natural Language Processing tasks likeMachine Translation and Cross Lingual Informa-tion Retrieval(CLIR).
Machine Translation sys-tems often use bilingual dictionaries in order toaugment word and phrase alignment (Och andNey, 2003).
CLIR systems use bilingual dictio-naries in the query translation step (Grefenstette,1998).
However, high coverage electronic bilin-gual dictionaries are not available for all languagepairs.
So a major research area in Machine Trans-lation and CLIR is bilingual dictionary extraction.The most common approach for extracting bilin-gual dictionary is applying some statistical align-ment algorithm on a parallel corpus.
However,parallel corpora are not readily available for mostlanguage pairs.
Also, it takes a lot of effort to ac-tually get the accurate translations of sentences.Hence, constructing parallel corpora involves a lotof effort and time.
So in recent years, extract-ing bilingual dictionaries from comparable cor-pora has become an important area of research.Comparable corpora consist of documents on sim-ilar topics in different languages.
Unlike parallelcorpora, they are not sentence aligned.
In fact,the sentences in one language do not have to bethe exact translations of the sentence in the otherlanguage.
However, the two corpora must be onthe same domain or topic.
Comparable corporacan be obtained more easily than parallel corpora.For example, a collection of news articles fromthe same time period but in different languagescan form a comparable corpora.
But after care-ful study of news articles in English and Hindipublished on same days at the same city, we haveobserved that along with articles on similar top-ics, the corpora also contain a lot of articles whichhave no topical similarity.
Thus, the corpora arequite noisy, which makes it unsuitable for lexiconextraction.
Thus another important factor in com-parable corpora construction is the degree of sim-ilarity of the corpora.Approaches for lexicon extraction from compara-ble corpora have been proposed that use the bag-of-words model to find words that occur in similarlexical contexts (Rapp, 1995).
There have beenapproaches proposed which improve upon thismodel by using some linguistic information (Yuuand Tsujii, 2009).
However, these require somelinguistic tool like dependency parsers which arenot commonly obtainable for resource-poor lan-guages.
For example, in case of Indian languageslike Hindi and Bengali, we still do not have goodenough dependency parsers.
In this paper, wepropose a word co-occurrence based approach forlexicon extraction from comparable corpora usingEnglish and Hindi as the source and target lan-guages respectively.
We do not use any language-35specific resource in our approach.We did experiments with 100 words in En-glish,and show that our approach performs signif-icantly better than the the Context Heterogeneityapproach (Fung, 1995).
We show the results overcorpora with varying degrees of comparability.The outline of the paper is as follows.
In section2, we analyze the different approaches for lexiconextraction from comparable corpora.
In section 3,we present our algorithm and the experimental re-sults.
In section 4, we present an analysis of theresults followed by the conclusion and future re-search directions in section 5.2 Previous WorkOne of the first works in the area of comparablecorpora mining was based on word co-occurrencebased approach (Rapp, 1995).
The basic assump-tion behind this approach was two words are likelyto occur together in the same context if their jointprobability of occurrence in a corpus exceeds theprobability that the words occur randomly.
In hispaper, Rapp made use of a similarity matrix andusing a joint probability estimate determined theword maps.
However this approach did not yieldsignificantly good results.The ?Context Heterogeneity?
approach was oneof the pioneering works in this area.
It uses a 2-dimensional context vector for each word basedon the right and left context.
The context vectordepended on how many distinct words occur in theparticular context and also the unigram frequencyof the word to be translated.
Euclidean distancebetween context vectors was used as a similaritymeasure.Another approach used Distributed Clustering ofTranslational Equivalents for word sense acqui-sition from bilingual comparable corpora (Kaji,2003).
However, the major drawback of this paperis the assumption that translation equivalents usu-ally represent only one sense of the target word.This may not be the case for languages havingsimilar origin, for example, Hindi and Bengali.Approaches using context information for extract-ing lexical translations from comparable corporahave also been proposed (Fung and Yee, 1998;Rapp, 1999).
But they resulted in very poor cov-erage.
These approaches were improved uponby extracting phrasal alignments from comparablecorpora using joint probability SMT model (Ku-mano et al, 2007) .Another proposed method uses dependency pars-ing and Dependency Heterogeneity for extractingbilingual lexicon (Yuu and Tsujii, 2009) .
Thisapproach was similar to that of Fung, except theyused a dependency parser to get the tags for eachword and depending on the frequency of each tagthey defined a vector to represent each word inquestion.
Here too, Euclidean similarity was usedto compute the similarity between two words us-ing their context vectors.
However, this method isdependent on availability of a dependency parserfor the languages and is not feasible for languagesfor which resources are scarce.3 Bilingual Dictionary Extraction UsingCo-occurrence Information3.1 MotivationThe Context Heterogeneity and Dependency Het-erogeneity approaches suffer from one majordrawback.
They do not use any kind of infor-mation about how individual words combine in aparticular context to form a meaningful sentence.They only use some statistics about the number ofwords that co-occur in a particular context or thenumber of times a word receives a particular tagin dependency parsing.
So, we wished to study ifthe quality of dictionary extracted would improveif we consider how individual words co-occur intext and store that information in the form of avector, with one dimension representing one wordin the corpus.
One important point to note hereis that the function words in a language are usu-ally very small in number.
If we need to constructa dictionary of function words in two languages,that can be done without much effort manually.Also, the function words do not play an impor-tant role in CLIR applications, as they are usuallystripped off.Our algorithm is based on the intuition that wordshaving similar semantic connotations occur to-gether.
For example, the words ?bread?
is morelikely to occur with ?eat?
than with ?play?.
Ouralgorithm uses this distribution of co-occurrencefrequency along with a small initial seed dictio-36nary to extract words that are translations of oneanother.
We define a co-occurrence vector ofwords in both the languages, and also record thenumber of times two words co-occur.
To findthe translation for word Wx, we check for thewords co-occurring with Wx such that this wordalready has a map in the other language, and com-pute a scoring function using all such words co-occurring with Wx.
In short, we use the alreadyexisting information to find new translations andadd them to the existing lexicon to grow it.
Be-low is a snapshot of a part of the data from oneof our experiments using the FIRE 20101 cor-pus.
For each word in English and Hindi, the co-occurrence data is expressed as a list of tuples.Each tuple has the form (word, co-occurrencefrequency).
For the Hindi words, the Englishmeaning has been provided in parenthesis.
Forthe seed lexicon and final lexicon, the format is(source word, target word, strength).English:1. teacher:{(training,49),(colleges,138),(man,22)}2. car:{(drive,238),(place,21)}3. drive:{(car,238),(steer,125),(city,12),(road,123)}Hindi:1. ghar(home):{(khidki(window),133),(makAn(house),172), (rAstA(road),6)}2. gAdi(car):{(rAsta,92),(chAlak(driver),121),(signal,17)}3. shikshaka(teacher):{(vidyalaya(school),312),(makAn(house),6)}Seed lexicon:1.
(colleges,vidyalaya,0.4)2.
(colleges,mahavidyalaya(college),0.6)3.
(car,gAdi,1.0)The following is a snapshot from the final resultsgiven by the algorithm:1Forum For Information Retrievalhttp://www.isical.ac.in/?clia/index.html1.
(car,gAdi,1.0)2.
(teacher,shikshak,0.62)3.
(teacher, vidyalaya,0.19)4.
(road, rAsta, 0.55)3.2 The AlgorithmFor extracting bilingual lexicon, we have not con-sidered the function words of the two languages.In order to filter out the function words, we havemade use of the assumption that content wordsusually have low frequency in the corpus, whereasfunction words have very high frequency.
First,we define some quantities:Let the languages be E and H.We = Set of words in E = {e1, e2, ...., eN}Wh = Set of words in H = {h1, h2, ...., hM}|We| = N|Wh| = MMAP = Initial map given= {(ei, hj , wij)|wij = wt(ei, hj), ei ?
We, hj ?
Wh}EM = Set of words in E which are included inentries of MAPHM = Set of words in H which are included inentries of MAPCo occ(x) = Set of words which co-occur with word xCo occ?
(x) =(Co occ(x) ?
EM if x ?
WeCo occ(x) ?HM if x ?
WhWte(x) = {Wey|y ?
We and y ?
Co occ(x)}Wth(x) = {Why|y ?
Wh and y ?
Co occ(x)}Given a comparable corpus, we follow the fol-lowing steps of processing:1.
A sentence segmentation code is run to seg-ment the corpus into sentences.2.
The sentence-segmented corpus is cleaned ofall punctuation marks and special symbols byreplacing them with spaces.37Algorithm 1 Algorithm to Extract Bilingual Dictionary by using word Co-occurrence Informationrepeatfor ei ?
We dofor hj ?
Wh doif (ei, hj , 0) ?
MAP thenwt(ei, hj) =Pe?Co occ?
(ei)Ph?Co occ?
(hj)(WijWeeiWhhj )Pe?Co occ?
(ei)Ph?Co occ?
(hj)(WeeiWhhj )end ifend forend forSelect the pair with highest value of wt(ei, bj) and add it to the existing map and normalizeuntil termination3.
The collection frequency of all the terms arecomputed and based on a threshold, the func-tion words are filtered out.4.
The co-occurrence information is computedat sentence-level for the remaining terms.
Ina sentence, if words wi and wj both occur,then wi ?
Co occ(wj) and vice versa.5.
Since we can visualize the co-occurrence in-formation in the form of a graph, we nextcluster the graph into C clusters.6.
From each cluster Ci, we choose some fixednumber number of words and manually findout their translation in the target language.This constitutes the initial map.7.
Next we apply Algorithm 1 to compute theword maps.The time complexity of the algorithm isO(IM2N2), where I is the number of itera-tions of the algorithm.3.3 Corpus ConstructionThe corpora used for evaluating our algorithmwere derived from the FIRE 2010 English andHindi corpora for the ad-hoc retrieval task.
Thesecorpora contained news articles spanning over atime period of three years from two Indian news-papers, ?The Dainik Jagaran?
in Hindi and ?TheTelegraph?
in English.
However, due to the ex-treme level of variation of the topics in these cor-pora, we applied a filtering algorithm to select asubset of the corpora.Our approach to make the text similar involvedreducing the corora based on matching NamedEntities.
Named Entities of English and Hindicorpus were listed using LingPipe2 and a HindiNER system built at IIT Kharagpur(Saha et al,1999).
The listed Named Entities of the two cor-pora were compared to find the matching NamedEntities.
Named Entities in Hindi Unicode wereconverted to iTRANS3 format and matched withEnglish Named Entities using edit distance.
Unitcost was defined for each insert and delete opera-tion.
Similar sounding characters like ?s?, ?c?,?a?,?e?
etc were assigned a replacement cost of 1 andother characters were assigned a replacement costof 2.
Two Named Entities were adjudged match-ing if:(2 ?
Cost)/(WLh +WLe) < 0.5where,WLh = Length of Hindi wordWLe = Length of English wordUsing this matching scheme, accuracy of match-ing of Hindi and English Named Entities wasfound to be > 95%.
It was observed that thereare large number of Named Entities with smallfrequency and few Named Entities with large fre-quency.
So a matching list was prepared whichcontained only those Named Entities which hadfrequency larger than a ?MaxFreq .
This en-sured that matching list had words with high fre-quency in both corpus.So English words with fre-quency larger than 368 and Hindi words withfrequency larger than 223 were considered formatching.
Based on this matching list, the two2http://alias-i.com/lingpipe/3http://www.aczoom.com/itrans/38Language Total NE UniqueNENE with freqlarger than?MaxFreqNEMatchedTotal Noof docs% of NE coveredAccordingto Zipf?sLawIn theactualcorpusHindi 1195474 37606 686 360 54271 63.0% 74.3%English 5723292 137252 2258 360 87387 65.2% 71.0%Table 1: Statistics of the main corpora used for extractionCorpus Max FreqWordMaxFreq?MaxFreqHindi bharat 50072 223English calcutta 135780 368Table 2: Criteria used for thresholding in the twocorporaMatching% ofNE perdocumentTotal documents incorporaHindi English> 10% 34694 16950> 20% 14872 4927> 30% 2938 1650Table 3: Statistics of extracted corporacorpora were reduced by including only those fileseach of which contained more than a certain fixedpercentage of total matching Named Entities.
Thecorpus statistics are provided in tables 1, 2 and 3.We assume that distribution of Named Entitiesfollows Zipf?s law (Zipf, 1949).
And analysisshows that Named Entities with frequency greaterthan the chosen threshold lead to high cover-age both theoretically and in practice (Table 1).Hence, the threshold was chosen as ?MaxFreq.The differences in the theoretical and actual val-ues can be attributed to the poor performance ofthe NER systems, especially the Hindi NER sys-tem, whose output contained a number of falsepositives.3.4 Experimental SetupThe languages we used for our experiments wereEnglish and Hindi.
English was the source lan-guage and Hindi was chosen as the target.
Forour experiments, we used a collection frequencythreshold of 400 to filter out the function words.The words having a collection frequency morethan 400 were discarded.
This threshold was ob-tained manually by ?Trial and Error?
method inorder to perform an effective function word fil-tering.
For each corpora, we extracted the co-occurrence information and then clustered the co-occurrence graph into 20 clusters.
From eachcluster we chose 15 words, thus giving us an over-all initial seed dictionary size of 300.
We ran thealgorithm for 3000 iterations.For graph clustering, we used the Graclus system(Dhillon et al, 2007) which uses a weighted ker-nel k-means clustering algorithm at various levelsof coarseness of the input graph.3.5 Evaluation Method and ResultsFor evaluation, we have used the Accuracy andMMR measure (Voorhees, 1999).
The measuresare defined as follows:Accuracy = 1NPNi=1 tiwhere, ti =(1 if correct translation in top n0 otherwiseMMR = 1NPNi=11rankiwhere, ranki =(ri if ri ?
n0 otherwisen means top n evaluationri means rank of correct translation in top n rankingN means total number of words used for evaluationFor our experiments, we have used:39Corpus Context Het-erogeneityCo-occurrenceAcc MMR Acc MMR> 10% 0.14 0.112 0.16 0.135> 20% 0.21 0.205 0.27 0.265> 30% 0.31 0.285 0.35 0.333Table 4: Comparison of performance betweenContext Heterogeneity and Co-occurrence Ap-proach for manual evaluationn = 5N = 100The 100 words used for evaluation were chosenrandomly from the source language.Two evaluation methods were followed - manualand automated.
In the manual evaluation, aperson who knows both English and Hindi wasasked to find the candidate translation in the targetlanguage for the words in the source language.Using this gold standard map, the Accuracy andMMR values were computed.In the second phase (automated), lexicon ex-tracted is evaluated against English to Hindiwordnet4.
The evaluation process proceeds asfollows:1.
Hashmap is created with English words askeys and Hindi meanings as values.2.
English words in the extracted lexicon arecrudely stemmed so that inflected wordsmatch the root words in the dictionary.
Stem-ming is done by removing the last 4 charac-ters, one at a time and checking if word foundin dictionary.3.
Accuracy and MMR are computed.As a reference measure, we have used Fung?smethod of Context Heterogeneity with a contextwindow size of 4.
The results are tabulated inTables 4 and 6.
We can see that our proposedalgorithm shows a significant improvement overthe Context Heterogeneity method.
The degreeof improvement over the Context Heterogeneity4Downloadable fromhttp://sanskritdocuments.org/hindi/dict/eng-hin-itrans.htmlCorpus Accuracy MMR> 10% ?
14.28% ?
20.53%> 20% ?
28.57% ?
29.27%> 30% ?
12.9% ?
16.84%Table 5: Degree of improvement shown by Co-occurrence approach over Context Heterogeneityfor manual evaluationCorpus Context Het-erogeneityCo-occurrenceAcc MMR Acc MMR> 10% 0.05 0.08 0.05 0.08> 20% 0.06 0.06 0.11 0.10> 30% 0.13 0.11 0.15 0.13Table 6: Comparison of performance betweenContext Heterogeneity and Co-occurrence Ap-proach for auto-evaluationis summarized in Tables 5 and 7.
For autoevaluation, We see that the proposed approachshows the maximum improvement (83.33% inAccuracy and 66.67% in MMR) in performancewhen the corpus size is medium.
For very large(too general) corpora, both the approaches giveidentical result while for very small (too specific)corpora, the proposed approach gives slightlybetter results than the reference.The trends are similar for manual evaluation.Once again, the maximum improvement isobserved for the medium sized corpus (> 20%).However, in this evaluation system, the proposedapproach performs much better than the referenceeven for the large (more general) corpora.Corpus Accuracy MMR> 10% 0.0% 0.0%> 20% ?
83.33% ?
66.67%> 30% ?
15.38% ?
18.18%Table 7: Degree of improvement shown by Co-occurrence approach over Context Heterogeneityfor auto-evaluation404 DiscussionThe co-occurrence based approach used in thispaper is quite a simple approach in the sense thatit does not make use of any kind of linguisticinformation.
From the aforementioned resultswe can see that a model based on simple wordco-occurrence highly outperforms the ?ContextHeterogeneity?
model in almost all the cases.One possible reason behind this is the amount ofinformation captured by our model is more thanthat captured by the ?Context Heterogeneity?model.
?Context Heterogeneity?
does not modelactual word-word interactions.
Each word isrepresented by a function of the number ofdifferent contexts it can occur in.
However, werepresent the word by a co-occurrence vector.This captures all possible contexts of the word.Also, we can actually determine which are thewords which co-occur with any other word.
Soour model captures more semantics of the word inquestion than the ?Context Heterogeneity?
model,thereby leading to better results.
Another possiblefactor is the nature in which we compute thetranslation scores.
Due to the iterative nature ofthe algorithm and since we normalize after eachiteration, some of the word pairs that receivedunduly high score in an earlier iteration end uphaving a substantially low score.
However, sincethe ?Context Heterogeneity?
does only a singlepass over the set of words, it fails to tackle thisproblem.The seed dictionary plays an important role inour algorithm.
A good seed dictionary gives ussome initial information to work with.
However,since ?Context Heterogeneity?
does not use aseed dictionary, it loses out on the amount ofinformation initially available to it.
Since the seeddictionary size for our approach is quite small,it can be easily constructed manually.
However,how the seed dictionary size varies with corpussize is an issue that remains to be seen.Another important factor in our algorithm is theway in which we have defined the co-occurrencevectors.
This is not the same as the context vectorthat we define in case of Context Heterogeneity.In a windowed context vector, we fail to capture alot of dependencies that might be captured usinga sentence-level co-occurrence.
This problem isespecially more visible in case of free-word-orderlanguages like the Indo-European group of lan-guages.
For these languages, a windowed contextvector is also likely to introduce many spuriousdependencies.
Since Hindi is a language of thisfamily, our algorithm captures many more correctsemantic dependencies than Context Heterogene-ity algorithm, resulting in better preformance.Another strong point of our proposed approachis the closeness of the values of Accuracy andMMR.
This shows that the translation candidatesextracted by our algorithm are not only correct,but also the best translation candidate gets thehighest score with high probability.
This is a veryimportant factor in Machine Translation systems,where a more accurate dictionary would give usan improved performance.A noticeable point about the evaluation scores isthe difference in scores given by the automatedsystem and the manual system.
This can beattributed to synonymy and spelling errors.
Inthe target language Hindi, synonymy plays avery important part.
It is not expected that allsynonyms of a particular word may be presentin an online dictionary.
In such cases, themanual evaluator marks a translation pair asTrue, whereas the automated system marks it asFalse.
Instances of spelling errors have also beenfound.
For example, for the word ?neighbors?,the top translation provided by the system was?paDosana?
(female neighbor).
If we considerroot form of words, this is correct.
But the actualtranslation should be ?paDosiyAn?
(neighbors,may refer to both male and female).
Thus theauto evaluation system tags it as False, whereasthe manual evaluator tags it as True.
There aremany more such occurrences throughout.Apart from that, the manual evaluation processhas been quite relaxed.
Even if the properties liketense, number of words does not match, as longas the root forms match the manual evaluator hasmarked it as True.
But this is not the case forthe automated evaluator.
Although stemming hasbeen done, but problems still persist which can beonly solved by lemmatization, because Hindi is ahighly inflected language.415 Conclusion and Future WorkIn this paper we present a completely new ap-proach for extracting bilingual lexicon from com-parable corpora.
We show the results of experi-mentation on corpora of different levels of com-parability.
The basic feature of this approach isthat it is language independent and needs no ad-ditional resource.
We could not compare its per-formance with the Dependency Heterogeneity al-gorithm due to the lack of resources for Hindi.So this can be taken up as a future work.
Also,the algorithm is quite inefficient.
Another direc-tion of research can be in trying to explore waysto reduce the complexity of this algorithm.
Wecan also try to incorporate more linguistic infor-mation into this model instead of just word co-occurrence.
It remains to be seen how these fac-tors affect the performance of the algorithm.
An-other important question is what should be the sizeof the seed dictionary for optimum performanceof the algorithm.
This too can be taken up as afuture research direction.ReferencesDhillon, I., Y. Guan, and B. Kulis.
2007.
Weightedgraph cuts without eigenvectors: A multilevel ap-proach.
IEEE Transactions on Pattern Analy-sis and Machine Intelligence (PAMI), 29:11:1944?1957, November.Fung, Pascale and Lo Yuen Yee.
1998.
An ir ap-proach for translating new words from nonparallel,comparable texts.
In Proceedings of the 36th An-nual Meeting of the Association for ComputationalLinguistics / the 17th International Conference onComputational Linguistics, pages 414?420.Fung, Pascale.
1995.
Compiling bilingual lexiconentries from a non-parallel english-chinese corpus.In Third Annual Workshop on Very Large Corpora,Boston, Massachusetts, June.Grefenstette, G. 1998.
The problem of cross-languageinformation retrieval.
Cross-language InformationRetrieval.Kaji, H. 2003.
Word sense acquisition from bilingualcomparable corpora.
In Proc.
of HLT-NAACL 2003Main papers, pages 32?39.Kumano, T., H. Takana, and T. Tokunaga.
2007.
Ex-tracting phrasal alignments from comparable cor-pora by using joint probability smt model.
In Proc.of TMI.Och, F. and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51, March.Rapp, Reinhard.
1995.
Identifying word translationsin non-parallel texts.
In Proc.
of TMI.Rapp, Reinhard.
1999.
Automatic identification ofword translations from unrelated english and ger-man corpora.
In Proceedings of the 37th AnnualMeeting of the Association for Computational Lin-guistics, pages 519?526.Saha, Sujan Kumar, Sudeshna Sarkar, and Pabitra Mi-tra.
1999.
A hybrid feature set based maximumentropy hindi named entity recognition.
In Proceed-ings of the Third International Joint Conference onNatural Language Processing, pages 343?349, Hy-derabad, India, January.Voorhees, E.M. 1999.
The trec-8 question answer-ing track report.
In Proceedings of the 8th Text Re-trieval Conference.Yuu, K. and J. Tsujii.
2009.
Extracting bilingual dic-tionary from comparable corpora with dependencyheterogeneity.
In Proc.
of NAACL-HLT, short pa-pers, pages 121?124.Zipf, George Kingsley.
1949.
Human Behaviour andthe Principle of Least Effort: an Introduction to Hu-man Ecology.
Addison-Wesley.42
