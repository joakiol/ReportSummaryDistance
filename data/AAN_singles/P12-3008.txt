Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 43?48,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsLetsMT!
: A Cloud-Based Platform for Do-It-YourselfMachine TranslationAndrejs Vasi?jevs Raivis Skadi??
J?rg TiedemannTILDE TILDE Uppsala UniversityVienbas gatve 75a, Riga Vienbas gatve 75a, Riga Box 635, UppsalaLV-1004, LATVIA LV-1004, LATVIA SE-75126, SWEDENandrejs@tilde.com raivis.skadins@tilde.lvjorg.tiedemann@lingfil.uu.seAbstractTo facilitate the creation and usage of customSMT systems we have created a cloud-basedplatform for do-it-yourself MT.
The platform isdeveloped in the EU collaboration projectLetsMT!.
This system demonstration paperpresents the motivation in developing theLetsMT!
platform, its main features,architecture, and an evaluation in a practical usecase.1 IntroductionCurrent mass-market and online MT systems are ofa general nature and perform poorly for smallerlanguages and domain specific texts.
TheEuropean Union ICT-PSP Programme projectLetsMT!
develops a user-driven MT ?factory inthe cloud?
enabling web users to get customisedMT that better fits their needs.
Harnessing the hugepotential of the web together with open statisticalmachine translation (SMT) technologies, LetsMT!has created an online collaborative platform fordata sharing and MT building.The goal of the LetsMT!
project is to facilitatethe use of open source SMT tools and to involveusers in the collection of training data.
TheLetsMT!
project extends the use of existing state-of-the-art SMT methods by providing them ascloud-based services.
An easy-to-use web interfaceempowers users to participate in data collectionand MT customisation to increase the quality,domain coverage, and usage of MT.The LetsMT!
project partners are companiesTILDE (coordinator), Moravia, and SemLab, andthe Universities of Edinburgh, Zagreb,Copenhagen, and Uppsala.2 LetsMT!
Key FeaturesThe LetsMT!
platform 1  (Vasi?jevs et al, 2011)gathers public and user-provided MT training dataand enables generation of multiple MT systems bycombining and prioritising this data.
Users canupload their parallel corpora to an onlinerepository and generate user-tailored SMT systemsbased on data selected by the user.Authenticated users with appropriatepermissions can also store private corpora that canbe seen and used only by this user (or a designateduser group).
All data uploaded into the LetsMT!repository is kept in internal format, and only itsmetadata is provided to the user.
Data cannot bedownloaded or accessed for reading by any means.The uploaded data can only be used for SMTtraining.
In such a way, we encourage institutionsand individuals to contribute their data to bepublicly used for SMT training, even if they arenot willing to share the content of the data.A user creates SMT system definition byspecifying a few basic parameters like systemname, source/target languages, domain, andchoosing corpora (parallel for translation models ormonolingual for language models) to use for theparticular system.
Tuning and evaluation data canbe automatically extracted from the trainingcorpora or specified by the user.
The access levelof the system can also be specified - whether it willbe public or accessible only to the particular useror user group.1 http://letsmt.com43When the system is specified, the user can begintraining it.
Progress of the training can bemonitored on the dynamic training chart (Figure 1).It provides a detailed visualisation of the trainingprocess showing (i) steps queued for execution of aparticular training task, (ii) current execution statusof active training steps, and (iii) steps where anyerrors have occurred.
The training chart remainsavailable after the training to facilitate analysis ofthe performed trainings.
The last step of thetraining task is automatic evaluation using BLEU,NIST, TER, and METEOR scores.A successfully trained SMT system can bestarted and used for translation in several ways:?
on the translation webpage of LetsMT!
fortesting and short translations;?
using LetsMT!
plug-ins in computer-assisted translation (CAT) tools forprofessional translation;?
integrating the LetsMT!
widget for web-site translation;?
using LetsMT!
plug-ins for IE and FireFoxto integrate translation into the browsers;?
using LetsMT!
API for MT integration intodifferent applications.LetsMT!
allows for several system instances torun simultaneously to speed up translation andbalance the workload from numerous translationrequests.LetsMT!
user authentication and authorisationmechanisms control access rights to privatetraining data, trained modelsand SMT systems, per-missions to initiate andmanage training tasks, runtrained systems, and accessLetsMT!
services throughexternal APIs.The LetsMT!
platform ispopulated with initial SMTtraining data collected andprepared by the projectpartners.
It currently containsmore than 730 millionparallel sentences in almost50 languages.
In the first 4months since launching theinvitation only beta versionof the platform, 82 SMTsystems have beensuccessfully trained.3 SMT Training and Decoding FacilitiesThe SMT training and decoding facilities ofLetsMT!
are based on the open source toolkitMoses.
One of the important achievements of theproject is the adaptation of the Moses toolkit to fitinto the rapid training, updating, and interactiveaccess environment of the LetsMT!
platform.The Moses SMT toolkit (Koehn et al, 2007)provides a complete statistical translation systemdistributed under the LGPL license.
Mosesincludes all of the components needed to pre-process data and to train language and translationmodels.
Moses is widely used in the researchcommunity and has also reached the commercialsector.
While the use of the software is not closelymonitored, Moses is known to be in commercialuse by companies such as Systran (Dugast et al,2009), Asia Online, Autodesk (Plitt and Masselot,2010), Matrixware2, Adobe, Pangeanic, Logrus3,and Applied Language Solutions (Way et al,2011).The SMT training pipeline implemented inMoses involves a number of steps that each requirea separate program to run.
In the framework of2 Machine Translation at Matrixware: http://ir-facility.net/downloads/mxw_factsheet_smt_200910.pdf3 TDA Members doing business with Moses:http://www.tausdata.org/blog/2010/10/doing-business-with-moses-open-source-translation/Figure 1.
Training chart providing dynamic representation of training steps.44LetsMT!, this process is streamlined and madeautomatically configurable given a set of user-specified variables (training corpora, languagemodel data, tuning sets).
SMT training isautomated using the Moses experiment mana-gement system (Koehn, 2010).
Other impro-vements of Moses, implemented by the Universityof Edinburgh as part of LetsMT!
project, are:?
the incremental training of SMT models(Levenberg et al, 2010);?
randomised language models (Levenberget al, 2009);?
a server mode version of the Mosesdecoder and multithreaded decoding;?
multiple translation models;?
distributed language models (Brants et al,2007).Many improvements in the Moses experimentmanagement system were implemented to speed upSMT system training and to use the full potentialof the HPC cluster.
We revised and improvedMoses training routines (i) by finding tasks that areexecuted sequentially but can be executed inparallel and (ii) by splitting big training tasks intosmaller ones and executing them in parallel.4 Multitier ArchitectureThe LetsMT!
system has a multitier architecture(Figure 2).
It has (i) an interface layer implemen-ting the user interface and APIs with externalsystems, (ii) an application logic layer for thesystem logic, (iii) a data storage layer consisting offile and database storage, and (iv) a highperformance computing (HPC) cluster.
TheLetsMT!
system performs various time andresource consuming tasks; these tasks are definedby the application logic and data storage and aresent to the HPC cluster for execution.The Interface layer provides interfaces betweenthe LetsMT!
system and external users.
The systemhas both human and machine users.
Human userscan access the system through web browsers byusing the LetsMT!
web page interface.
Externalsystems such as Computer Aided Translation(CAT) tools and web browser plug-ins can accessthe LetsMT!
system through a public API.
Thepublic API is available through both REST/JSONand SOAP protocol web services.
An HTTPSprotocol is used to ensure secure userauthentication and secure data transfer.The application logic layer contains a set ofmodules responsible for the main functionality andlogic of the system.
It receives queries andcommands from the interface layer and preparesanswers or performs tasks using data storage andthe HPC cluster.
This layer contains severalmodules such as the Resource Repository Adapter,the User Manager, the SMT Training Manager, etc.The interface layer accesses the application logiclayer through the REST/JSON and SOAP protocolweb services.
The same protocols are used forcommunication between modules in theapplication logic layer.Figure 2.
The LetsMT!
system architectureThe data is stored in one central ResourceRepository (RR).
As training data may change (forexample, grow), the RR is based on a version-controlled file system (currently we use SVN asthe backend system).
A key-value store is used tokeep metadata and statistics about training data andtrained SMT systems.
Modules from theapplication logic layer and HPC cluster access RRthrough a REST-based web service interface.A High Performance Computing Cluster is usedto execute many different computationally heavydata processing tasks ?
SMT training and running,corpora processing and converting, etc.
Modulesfrom the application logic and data storage layers45create jobs and send them to the HPC cluster forexecution.
The HPC cluster is responsible foraccepting, scheduling, dispatching, and managingremote and distributed execution of large numbersof standalone, parallel, or interactive jobs.
It alsomanages and schedules the allocation of distributedresources such as processors, memory, and diskspace.
The LetsMT!
HPC cluster is based on theOracle Grid Engine (SGE).The hardware infrastructure of the LetsMT!platform is heterogeneous.
The majority ofservices run on Linux platforms (Moses, RR, dataprocessing tools, etc.).
The Web server andapplication logic services run on a MicrosoftWindows platform.The system hardware architecture is designed tobe highly scalable.
The LetsMT!
platform containsseveral machines with both continuous and on-demand availability:?
Continuous availability machines are usedto run the core frontend and backendservices and the HPC grid master toguarantee stable system functioning;?
On-demand availability machines are used(i) to scale up the system by adding morecomputing power to training, translation,and data import services (HPC clusternodes) and (ii) to increase  performance offrontend and backend server instances.To ensure scalability of the system, the wholeLetsMT!
system including the HPC cluster ishosted by Amazon Web Services infrastructure,which provides easy access to on-demandcomputing and storage resources.5 Data Storage and Processing FacilitiesAs a data sharing and MT platform, the LetsMT!system has to store and process large amounts ofSMT training data (parallel and monolingualcorpora) as well as trained models of SMTsystems.
The Resource Repository (RR) softwareis fully integrated into the LetsMT!
Platform andprovides the following major components:?
Scalable data storage based on version-controlled file systems;?
A flexible key-value store for metadata;?
Access-control mechanisms defining threelevels of permission (private data, publicdata, shared data);?
Data import modules that include tools fordata validation, conversion and automaticsentence alignment for a variety of populardocument formats.The general architecture of the ResourceRepository is illustrated in Figure 3.
It isimplemented in terms of a modular package thatcan easily be installed in a distributed environment.RR services are provided via Web API?s andsecure HTTP requests.
Data storage can bedistributed over several servers as is illustrated inFigure 3.
Storage servers communicate with thecentral database server that manages all metadatarecords attached to resources in the RR.
Dataresources are organised in slots that correspond tofile systems with user-specific branches.
Currently,the RR package implements two storage backends:a plain file system and a version-controlled filesystem based on subversion (SVN).
The latter isthe default mode, which has several advantagesover non-revisioned data storage.
Revision controlsystems are designed to handle dynamicallygrowing collections of mainly textual data in amulti-user environment.
Furthermore, they keeptrack of modifications and file histories to make itpossible to backtrack to prior revisions.
This canbe a strong advantage, especially in cases of shareddata access.
Another interesting feature is thepossibility to create cheap copies of entirebranches that can be used to enable datamodifications by other users withoutcompromising data integrity for others.
Finally,SVN also naturally stores data in a compressedformat, which is useful for large-scale documentcollections.
In general, the RR implementation ismodular, other storage backends may be addedlater, and each individual slot can use its ownbackend type.Another important feature of the RR is thesupport of a flexible database for metadata.
Wedecided to integrate a modern key-value store intothe platform in order to allow a maximum offlexibility.
In contrast to traditional relationaldatabases, key-value stores allow the storage ofarbitrary data sets based on pairs of keys andvalues without being restricted to a pre-definedschema or a fixed data model.
Our implementationrelies on TokyoCabinet4, a modern implementationof schema-less databases that supports all of our4 https://fallabs/tokyocabinet46requirements in terms of flexibility and efficiency.In particular, we use the table mode ofTokyoCabinet that supports storage of arbitrarydata records connected to a single key in thedatabase.
We use resource URL?s in our repositoryto define unique keys in the database, and datarecords attached to these keys may include anynumber of key-value pairs.
In this way, we can addany kind of information to each addressableresource in the RR.
The software also supportskeys with unordered lists of values, which is usefulfor metadata such as languages (in a datacollection) and for many other purposes.Moreover, TokyoCabinet provides powerful querylanguage and software bindings for the mostcommon programming languages.
It can be run inclient-server mode, which ensures robustness in amulti-user environment and natively supports datareplication.
Using TokyoCabinet as our backend,we implemented a key-value store for metadata inthe RR that can easily be extended and queriedfrom the frontend of the LetsMT!
Platform viadedicated web-service calls.Yet another important feature of the RR is thecollection of import modules that take care ofvalidation and conversion of user-provided SMTtraining material.
Our main goal was to make thecreation of appropriate data resources as painlessas possible.
Therefore, we included support for themost common data formats to be imported intoLetsMT!.
Pre-aligned parallel data can be uploadedin TMX, XLIFF, and Moses formats.
Monolingualdata can be provided in plain text, PDF, and MSWord formats.
We also support the upload ofcompressed archives in zip and tar format.
In thefuture, other formatscan easily beintegrated in ourmodular implemen-tation.Validation of sucha variety of formats isimportant.
Thereforeamong others, weincluded XML/DTDvalidation, text en-coding detection soft-ware, and languageidentification toolswith pre-trained mo-dels for over 60 lan-guages.Furthermore, our system also includes tools forautomatic sentence alignment.
Import processesautomatically align translated documents with eachother using standard length-based sentencealignment methods (Gale and Church, 1993; Vargaet al, 2005).Finally, we also integrated a general batch-queuing system (SGE) to run off-line processessuch as import jobs.
In this way, we furtherincrease the scalability of the system by taking theload off repository servers.
Data uploadsautomatically trigger appropriate import jobs thatwill be queued on the grid engine using a dedicatedjob web-service API.6 Evaluation for Usage in LocalisationOne of the usage scenarios particularly targeted bythe project is application in the localisation andtranslation industry.
Localisation companiesusually have collected significant amounts ofparallel data in the form of translation memories.They are interested in using this data to createcustomised MT engines that can increaseproductivity of translators.
Productivity is usuallymeasured as an average number of wordstranslated per hour.
For this use case, LetsMT!
hasdeveloped plug-ins for integration into CAT tools.In addition to translation candidates fromtranslation memories, translators receivetranslation suggestions provided by the selectedMT engine running on LetsMT!.As part of the system evaluation, project partnerMoravia used the LetsMT!
platform to train andFigure 3.
Resource repository overview47evaluate SMT systems for Polish and Czech.
AnEnglish-Czech engine was trained on 0.9M parallelsentences coming from Moravia translationmemories in the IT and tech domain part of theCzech National Corpus.
The resulting systemincreased translator productivity by 25.1%.
AnEnglish-Polish system was trained on 1.5Mparallel sentences from Moravia production data inthe IT domain.
Using this system, translatorproductivity increased by 28.5%.For evaluation of English-Latvian translation,TILDE created a MT system using a significantlylarger corpus of 5.37M parallel sentence pairs,including 1.29M pairs in the IT domain.Additional tweaking was made by manuallyadding a factored model over disambiguatedmorphological tags.
The resulting systemincreased translator productivity by 32.9%(Skadi??
et al, 2011).7 ConclusionsThe results described in this paper show that theLetsMT!
project is on track to fulfill its goal todemocratise the creation and usage of custom SMTsystems.
LetsMT!
demonstrates that the opensource SMT toolkit Moses is reaching maturity toserve as a base for large scale and heavy useproduction purposes.
The architecture of theplatform and Resource Repository enablesscalability of the system and very large amounts ofdata to be handled in a variety of formats.Evaluation shows a strong increase in translationproductivity by using LetsMT!
systems in ITlocalisation.AcknowledgmentsThe research within the LetsMT!
project hasreceived funding from the ICT Policy SupportProgramme (ICT PSP), Theme 5 ?
Multilingualweb, grant agreement 250456.ReferencesL.
Dugast, J. Senellart, P. Koehn.
2009.
Selectiveaddition of corpus-extracted phrasal lexical rules to arule-based machine translation system.
Proceedingsof MT Summit XII.T.
Brants, A.C.  Popat, P.  Xu, F.J  Och, J.
Dean.
2007.Large Language Models in Machine Translation.Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Processingand Computational Natural Language Learning(EMNLP-CoNLL), 858-867.
Prague, Czech RepublicW.
A. Gale, K. W. Church.
1993.
A Program forAligning Sentences in Bilingual Corpora.Computational Linguistics 19 (1): 75?102P.
Koehn, M. Federico, B. Cowan, R. Zens, C. Duer, O.Bojar, A. Constantin, E. Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.Proceedings of the ACL 2007 Demo and PosterSessions, 177-180.
Prague.P.
Koehn.
2010.
An experimental management system.The Prague Bulletin of Mathematical Linguistics, 94.A.
Levenberg, M. Osborne.
2009.
Stream-based Ran-domised Language Models for SMT.
Proceedings ofthe 2009 Conference on Empirical Methods inNatural Language Processing.A.
Levenberg, C. Callison-Burch, M. Osborne.
2010.Stream-based Translation Models for StatisticalMachine Translation.
Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics (HLT '10)M. Plitt, F. Masselot.
2010.
A Productivity Test ofStatistical Machine Translation Post-Editing in aTypical Localisation Context.
The Prague Bulletin ofMathematical Linguistics, 93(January 2010): ?16R.
Skadi?
?, M.
Puri?
?, I. Skadi?a, A. Vasi?jevs.
2011.Evaluation of SMT in localization to under-resourcedinflected language.
Proceedings of the 15thInternational Conference of the EuropeanAssociation for Machine Translation EAMT 2011,35-40.
Leuven, BelgiumA.
Vasi?jevs, R.
Skadi?
?, I. Skadi?a.
2011.
TowardsApplication of User-Tailored Machine Translation inLocalization.
Proceedings of the Third JointEM+/CNGL Workshop ?Bringing MT to the User:Research Meets Translators?
JEC 2011, 23-31.LuxembourgD.
Varga, L. N?meth, P. Hal?csy, A. Kornai, V. Tr?n,V.
Nagy.
2005.
Parallel corpora for medium densitylanguages.
Recent Advances in Natural LanguageProcessing IV Selected papers from RANLP05, 590-596A.
Way, K. Holden, L. Ball, G. Wheeldon.
2011.SmartMATE: online self-serve access to state-of-the-art SMT.
Proceedings of the Third Joint EM+/CNGLWorkshop ?Bringing MT to the User: ResearchMeets Translators?
(JEC ?11), 43-52.
Luxembourg48
