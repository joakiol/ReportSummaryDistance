Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 180?185,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsNon-Linear Text Regression with a Deep Convolutional Neural NetworkZsolt BitvaiUniversity of Sheffield, UKz.bitvai@shef.ac.ukTrevor CohnUniversity of Melbourne, Australiat.cohn@unimelb.edu.auAbstractText regression has traditionally beentackled using linear models.
Here wepresent a non-linear method based on adeep convolutional neural network.
Weshow that despite having millions of pa-rameters, this model can be trained ononly a thousand documents, resulting in a40% relative improvement over sparse lin-ear models, the previous state of the art.Further, this method is flexible allowingfor easy incorporation of side informationsuch as document meta-data.
Finally wepresent a novel technique for interpretingthe effect of different text inputs on thiscomplex non-linear model.1 IntroductionText regression involves predicting a real worldphenomenon from textual inputs, and has beenshown to be effective in domains including elec-tion results (Lampos et al, 2013), financial risk(Kogan et al, 2009) and public health (Lamposand Cristianini, 2010).
Almost universally, thetext regression problem has been framed as lin-ear regression, with the modelling innovation fo-cussed on effective regression, e.g., using Lassopenalties to promote feature sparsity (Tibshirani,1996).1Despite their successes, linear models arelimiting: text regression problems will often in-volve complex interactions between textual inputs,thus requiring a non-linear approach to properlycapture such phenomena.
For instance, in mod-elling movie revenue conjunctions of features arelikely to be important, e.g., a movie described as?scary?
is likely to have different effects for chil-dren?s versus adult movies.
While these kinds of1Some preliminary work has shown strong results for non-linear text regression using Gaussian Process models (Lam-pos et al, 2014), however this approach has not been shownto scale to high dimensional inputs.features can be captured using explicit feature en-gineering, this process is tedious, limited in scope(e.g., to conjunctions) and ?
as we show here ?can be dramatically improved by representationallearning as part of a non-linear model.In this paper, we propose an artificial neu-ral network (ANN) for modelling text regression.In language processing, ANNs were first pro-posed for probabilistic language modelling (Ben-gio et al, 2003), followed by models of sentences(Kalchbrenner et al, 2014) and parsing (Socheret al, 2013) inter alia.
These approaches haveshown strong results through automatic learningdense low-dimensional distributed representationsfor words and other linguistic units, which havebeen shown to encode important aspects of lan-guage syntax and semantics.
In this paper wedevelop a convolutional neural network, inspiredby their breakthrough results in image process-ing (Krizhevsky et al, 2012) and recent applica-tions to language processing (Kalchbrenner et al,2014; Kim, 2014).
These works have mainly fo-cused on ?big data?
problems with plentiful train-ing examples.
Given their large numbers of pa-rameters, often in the millions, one would expectthat such models can only be effectively learnedon very large datasets.
However we show herethat a complex deep convolution network can betrained on about a thousand training examples, al-though careful model design and regularisation isparamount.We consider the problem of predicting the fu-ture box-office takings of movies based on reviewsby movie critics and movie attributes.
Our ap-proach is based on the method and dataset of Joshiet al (2010), who presented a linear regressionmodel over uni-, bi-, and tri-gram term frequencycounts extracted from reviews, as well as movieand reviewer metadata.
This problem is especiallyinteresting, as comparatively few instances areavailable for training (see Table 1) while each in-180train dev test total# movies 1147 317 254 1718# reviews per movie 4.2 4.0 4.1 4.1# sentences per movie 95 84 82 91# words per movie 2879 2640 2605 2794Table 1: Movie review dataset (Joshi et al, 2010).stance (movie) includes a rich array of data includ-ing the text of several critic reviews from variousreview sites, as well as structured data (genre, rat-ing, actors, etc.)
Joshi et al found that regressionpurely from movie meta-data gave strong predic-tive accuracy, while text had a weaker but comple-mentary signal.
Their best results were achievedby domain adaptation whereby text features wereconjoined with a review site identifier.
Inspired byJoshi et al (2010) our model also operates over n-grams, 1 ?
n ?
3, and movie metadata, albeitusing an ANN in place of their linear model.
Weuse word embeddings to represent words in a lowdimensional space, a convolutional network withmax-pooling to represent documents in terms ofn-grams, and several fully connected hidden lay-ers to allow for learning of complex non-linear in-teractions.
We show that including non-linearitiesin the model is crucial for accurate modelling, pro-viding a relative error reduction of 40% (MAE)over their best linear model.
Our final contribu-tion is a novel means of model interpretation.
Al-though it is notoriously difficult to interpret the pa-rameters of an ANN, we show a simple method ofquantifying the effect of text n-grams on the pre-diction output.
This allows for identification of themost important textual inputs, and investigation ofnon-linear interactions between these words andphrases in different data instances.2 ModelThe outline of the convolutional network is shownin Figure 1.
We have n training examples of theform {bi, ri, yi}ni=1, where biis the meta data as-sociated with movie i, yiis the target gross week-end revenue, and riis a collection of uinumber ofreviews, ri= {xj, tj}uij=1where each review hasreview text xjand a site id tj.
We concatenate allthe review texts di= (x1,x2, ...,xu) to form ourtext input (see part I of Figure 1).To acquire a distributed representation of thetext, we look up the input tokens in a pretrainedword embedding matrix E with size |V |?e, whereEmbeddings, E?
?
?
?This movie is the bestone of the century, theactors are exceptional...Boston Globe, r1= {x1,t1}New York Timesrumu|V|Thismovieisthebestone...?
?
?
?...econcatenate?
?
?
??
?
?
??
?
?
??
?
?
?
(I)lookupSite ids, tDocument matrix, De0 1 1 0 0Thismovieisthebestone...mReLugReLu ReLuW(1)W(2)W(3)hhhgg(II)mm-1m-2maxpool(III)p(1)p(2)p(3)Meta datagenre,rating,famousactors...bReLughReLughReLughgprediction$Revenueconvolutionfully connectederrorJfyW1W2W3w4a1o1=a2o2=a3o3=a4(IV)(V)uni-gramsC(1)bi-gramstri-gramsC(2)C(3)?
?
?
??
?
?
?...?
?
?
??
?
?
??
?
?
??
?
?
?fully connectedfully connectedfully connectedFigure 1: Outline of the network architecture.|V | is the size of our vocabulary and e is the em-bedding dimensionality.
This gives us a densedocument matrix Di,j= Edi,jwith dimensionsm ?
e where m is the number of tokens in thedocument.Since the length of text documents vary, in partII we apply convolutions with width one, two andthree over the document matrix to obtain a fixedlength representation of the text.
The n-gram con-volutions help identify local context and map thatto a new higher level feature space.
For each fea-ture map, the convolution takes adjacent word em-beddings and performs a feed forward computa-tion with shared weights over the convolution win-dow.
For a convolution with width 1 ?
q ?
m thisisS(q)i,?= (Di,?,Di+1,?, ...,Di+q?1,?
)C(q)i,?= ?S(q)i,?,W(q)?where S(q)i,?is q adjacent word embeddings con-catenated, and C(q)is the convolution output ma-trix with (m?q+1) rows after a linear transforma-tion with weights W(q).
To allow for a non-linear181transformation, we make use of rectified linear ac-tivation units, H(q)= max(C(q), 0), which areuniversal function approximators.
Finally, to com-press the representation of text to a fixed dimen-sional vector while ensuring that important infor-mation is preserved and propagated throughout thenetwork, we apply max pooling over time, i.e.
thesequence of words, for each dimension, as shownin part III,p(q)j= maxH(q)?,jwhere p(q)jis dimension j of the pooling layer forconvolution q, and p is the concatenation of allpooling layers, p = (p(1),p(2), ...,p(q)).Next, we perform a series of non-linear trans-formations to the document vector in order to pro-gressively acquire higher level representations ofthe text and approximate a linear relationship inthe final output prediction layer.
Applying multi-ple hidden layers in succession can require expo-nentially less data than mapping through a singlehidden layer (Bengio, 2009).
Therefore, in partIV, we apply densely connected neural net layersof the form ok= h(g(ak,Wk)) where akis theinput and ok= ak+1is the output vector for layerk, g is a linear transformation function ?ak,Wk?,and h is the activation function, i.e.
rectified linearseen above.
l = 3 hidden layers are applied be-fore the final regression layer to produce the out-put f = g(ol,wl+1) in part V.The mean absolute error is measured betweenthe predictions f and the targets y, which is morepermissible to outliers than the squared error.
Thecost J is defined asJ =1nn?v=1|fv?
yv|.The network is trained with stochastic gradient de-scent and the Ada Delta (Zeiler, 2012) update ruleusing random restarts.
Stochastic gradient descentis noisier than batch training due to a local esti-mation of the gradient, but it can start convergingmuch faster.
Ada Delta keeps an exponentially de-caying history of gradients and updates in order toadapt the learning rate for each parameter, whichpartially smooths out the training noise.
Regulari-sation and hyperparmeter selection are performedby early stopping on the development set.
Thesize of the vocabulary is 90K words.
Note that10% of our lexicon is not found in the embeddingsModel Description MAE($M)Baseline mean 11.7Linear Text 8.0Linear Text+Domain+POS 7.4Linear Meta 6.0Linear Text+Meta 5.9Linear Text+Meta+Domain+Deps 5.7ANN Text 6.3ANN Text+Domain 6.0ANN Meta 3.9ANN Text+Meta 3.4ANN Text+Meta+Domain 3.4Table 2: Experiment results on test set.
Linearmodels by (Joshi et al, 2010).pretrained on Google News.
Those terms are ini-tialised with random small weights.
The modelhas around 4 million weights plus 27 million tun-able word embedding parameters.Structured data Besides text, injecting metadata and domain information into the model likelyprovides additional predictive power.
Combin-ing text with structured data early in the networkfosters joint non-linear interaction in subsequenthidden layers.
Hence, if meta data b is present,we concatenate that with the max pooling layera1= (p,b) in part III.
Domain specific infor-mation t is appended to each n-gram convolutioninput (S(q)i,?, t) in part II, where tz= 1zindicateswhether domain z has reviewed the movie.2Thishelps the network bias the convolutions, and thuschange which features get propagated in the pool-ing layer.3 ResultsThe results in Table 2 show that the neural net-work performs very well, with around 40% im-provement over the previous best results (Joshi etal., 2010).
Our dataset splits are identical, and wehave accurately reproduced the results of their lin-ear model.
Non-linearities are clearly helpful asevidenced by the ANN Text model beating the bagof words Linear Text model with a mean absolutetest error of 6.0 vs 8.0.
Moreover, simply usingstructured data in the ANN Meta beats all the Lin-ear models by a sizeable margin.
Further improve-ments are realised through the inclusion of text,giving the lowest error of 3.4.
Note that Joshi et al(2010) preprocessed the text by stemming, down-2Alternatively, site information can be encoded with one-hot categorical variables.182Model Description MAE($M)fixed word2vec embeddings 3.4*tuned word2vec embeddings 3.6fixed random embeddings 3.6tuned random embeddings 3.8uni-grams 3.6uni+bi-grams 3.5uni+bi+tri-grams 3.4*uni+bi+tri+four-grams 3.60 hidden layer 6.31 hidden layer 3.92 hidden layers 3.53 hidden layers 3.4*4 hidden layers 3.6Table 3: Various alternative configurations, basedon the ANN Text+Meta model.
The asterisk (?
)denotes the settings in the ANN Text+Meta model.casing, and discarding feature instances that oc-curred in fewer than five reviews.
In contrast, wedid not perform any processing of the text or fea-ture engineering, apart from tokenization, insteadlearning this automatically.3We find that both text and meta data con-tain complementary signals with some informa-tion overlap between them.
This confirms the find-ing of Bitvai and Cohn (2015) on another text re-gression problem.
The meta features alone almostachieve the best results whereas text alone per-forms worse but still well above the baseline.
Forthe combined model, the performance improvesslightly.
In Table 3 we can see that contrary to ex-pectations, fine tuning the word embeddings doesnot help significantly compared to keeping themfixed.
Moreover, randomly initialising the embed-dings and fixing them performs quite well.
Finetuning may be a challenging optimisation due tothe high dimensional embedding space and theloss of monolingual information.
This is furtherexacerbated due to the limited supervision signal.One of the main sources of improvement ap-pears to come from the non-linearity applied to theneural network activations.
To test this, we try us-ing linear activation units in parts II and IV of thenetwork.
Composition of linear functions yields alinear function, and therefore we recover the lin-ear model results.
This is much worse than themodel with non-linear activations.
Changing thenetwork depth, we find that the model performsmuch better with a single hidden layer than with-3Although we do make use of pretrained word embed-dings in our text features.out any, while three hidden layers are optimal.
Forthe weight dimensions we find square 1058 dimen-sional weights to perform the best.
The ideal num-ber of convolutions are three with uni, bi and tri-grams, but unigrams alone perform only slightlyworse, while taking a larger n-gram window n > 3does not help.
Average and sum pooling performcomparatively well, while max pooling achievesthe best result.
Note that sum pooling recovers anon-linear bag-of-words model.
With respect toactivation functions, both ReLU and sigmoid workwell.Model extensions Multi task learning with taskidentifiers, ANN Text+Domain, does improve theANN Text model.
This suggests that the tendencyby certain sites to review specific movies is in it-self indicative of the revenue.
However this im-provement is more difficult to discern with theANN Text+Meta+Domain model, possibly due toredundancy with the meta data.
An alternative ap-proach for multi-task learning is to have a sepa-rate convolutional weight matrix for each reviewsite, which can learn site specific characteristicsof the text.
This can also be achieved with sitespecific word embedding dimensions.
Howeverneither of these methods resulted in performanceimprovements.
In addition, we experimented withapplying a hierarchical convolution over reviewsin two steps with k-max pooling (Kalchbrenner etal., 2014), as well as parsing sentences recursively(Socher et al, 2013), but did not observe any im-provements.For optimisation, both Ada Grad and SteepestGradient Descent had occasional problems withlocal minima, which Ada Delta was able to es-cape more often.
In contrast to earlier work (Kim,2014), applying dropout on the final layer did notimprove the validation error.
The optimiser mostlyfound good parameters after around 40 epochswhich took around 30 minutes on a NVidia KeplerTesla K40m GPU.Model interpretation Next we perform anal-ysis to determine which words and phrases in-fluenced the output the most in the ANN Textmodel.
To do so, we set each phrase input tozeros in turn and measure the prediction differ-ence for each movie across the test set.
We re-port the min/max/average/count values in Table4.
We isolate the effect of each n-gram by mak-ing sure the uni, bi and trigrams are independent,183transformers2pinkpanther2objectivemylifeinruinsinglouriousbasterdsinformersgreatbuckhowardgracefastandfurious4bobfunkbudget > $15Mbudget < $15M $0$3.6K$140K$5.2M$200MRevenueFigure 2: Projection of the last hidden layer oftest movies using t-SNE.
Red means high and bluemeans low revenue.
The cross vs dot symbols in-dicate a production budget above or below $15M.i.e.
we process ?Hong Kong?
without zeroing?Hong?
or ?Kong?.
About 95% of phrases re-sult in no output change, including common sen-timent words, which shows that text regression isa different problem to sentiment analysis.
We seethat words related to series ?# 2?, effects, awards?praise?, positive sentiment ?intense?, locations,references ?Batman?, body parts ?chest?, and oth-ers such as ?plot twist?, ?evil?, and ?cameo?
re-sult in increased revenue by up to $5 million.On the other hand, words related to independentfilms ?v?erit?e?, documentaries ?the period?, for-eign film ?English subtitles?
and negative senti-ment decrease revenue.
Note that the model hasidentified structured data in the unstructured text,such as related to revenue of prequels ?39 mil-lion?, crew members, duration ?15 minutes in?,genre ?
[sci] fi?, ratings, sexuality, profanity, re-lease periods ?late 2008 release?, availability ?Inselected theaters?
and themes.
Phrases can becomposed, such as ?action unfolds?
amplifies ?ac-tion?, and ?cautioned?
is amplified by ?stronglycautioned?.
?functional?
is neutral, but ?func-tional at best?
is strongly negative.
Some wordsexhibit both positive and negative impacts depend-ing on the context.
This highlights the limitationof a linear model which is unable to discover thesenon-linear relationships.
?13 - year [old]?
is posi-tive in New in Town, a romantic comedy and nega-tive in Zombieland, a horror.
The character strings?k /?
(mannerism of reviewer), ?they?re?
(uniqueapostrophe), ?&#39?
(encoding error) are high im-pact and unique to specific review sites, showingthat the model indirectly uncovers domain infor-mation.
This can explain the limited gain that canbe achieved via multi task learning.
Last, we haveTop 5 positive phrases min max avg #sequel 20 4400 2300 28flick 0 3700 1600 22k / 1500 3600 2200 3product 10 3400 1800 27predecessor 22 3400 1400 13Top 5 negative phrases min max avg #Mildly raunchy lang.
-3100 -3100 -3100 1( Under 17 -2500 1 -570 75Lars von -2400 -900 -1500 3talk the language -2200 -2200 -2200 1. their English -2200 -2200 -2200 1Selected phrases min max avg #CGI 145 3000 1700 28action -7 1500 700 105summer 3 1200 560 42they?re 3 1300 530 681950s 10 1600 500 17hit 8 950 440 72fi -15 340 160 26Cage 7 95 45 28Hong Kong -440 40 -85 11requires acc.
parent -780 1 -180 77English -850 6 -180 41Sundance Film Festival -790 3 -180 10written and directed -750 -3 -220 19independent -990 -2 -320 12some strong language -1600 6 -520 13Table 4: Selected phrase impacts on the predic-tions in $ USD(K) in the test set, showing min,max and avg change in prediction value and num-ber of occurrences (denoted #).
Periods denote ab-breviations (language, accompanying).plotted the last hidden layer of each test set moviewith t-SNE (Van der Maaten and Hinton, 2008).This gives a high level representation of a movie.In Figure 2 it is visible that the test set movies canbe discriminated into high and low revenue groupsand this also correlates closely with their produc-tion budget.4 ConclusionsIn this paper, we have shown that convolutionalneural networks with deep architectures greatlyoutperform linear models even with very little su-pervision, and they can identify key textual andnumerical characteristics of data with respect topredicting a real world phenomenon.
In addition,we have demonstrated a way to intuitively inter-pret the model.
In the future, we will investi-gate ways for automatically optimising the hyper-parameters of the network (Snoek et al, 2012) andvarious extensions to recursive or hierarchical con-volutions.184ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
The Journal of Machine Learning Re-search, 3:1137?1155.Yoshua Bengio.
2009.
Learning deep architectures forAI.
Foundations and Trends in Machine Learning,2(1):1?127.Zsolt Bitvai and Trevor Cohn.
2015.
Predicting peer-to-peer loan rates using Bayesian non-linear regres-sion.
In Proceedings of the 29th AAAI conferenceon Artificial Intelligence, pages 2203?2210.Mahesh Joshi, Dipanjan Das, Kevin Gimpel, andNoah A Smith.
2010.
Movie reviews and revenues:An experiment in text regression.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 293?296.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
Proceedings of the 52nd An-nual Meeting of the Association for ComputationalLinguistics.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing.Shimon Kogan, Dimitry Levin, Bryan R Routledge,Jacob S Sagi, and Noah A Smith.
2009.
Pre-dicting risk from financial reports with regression.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 272?280.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-ton.
2012.
ImageNet classification with deep con-volutional neural networks.
In Advances in NeuralInformation Processing Systems, pages 1097?1105.Vasileios Lampos and Nello Cristianini.
2010.
Track-ing the flu pandemic by monitoring the social web.In Cognitive Information Processing, pages 411?416.Vasileios Lampos, Daniel Preotiuc-Pietro, and TrevorCohn.
2013.
A user-centric model of voting inten-tion from social media.
In Proc 51st Annual Meet-ing of the Association for Computational Linguis-tics, pages 993?1003.Vasileios Lampos, Nikolaos Aletras, D Preot?iuc-Pietro,and Trevor Cohn.
2014.
Predicting and characteris-ing user impact on twitter.
In Proceedings of the14th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 405?
?413.Jasper Snoek, Hugo Larochelle, and Ryan P Adams.2012.
Practical Bayesian optimization of machinelearning algorithms.
In Advances in Neural Infor-mation Processing Systems, pages 2951?2959.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 1631?1642.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety.
Series B (Methodological), pages 267?288.Laurens Van der Maaten and Geoffrey Hinton.
2008.Visualizing data using t-SNE.
Journal of MachineLearning Research, 9(2579-2605):85.Matthew D Zeiler.
2012.
Adadelta: an adaptive learn-ing rate method.
arXiv preprint arXiv:1212.5701.185
