Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 908?918,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsText Understanding with the Attention Sum Reader NetworkRudolf Kadlec, Martin Schmid, Ondrej Bajgar & Jan KleindienstIBM WatsonV Parku 4, Prague, Czech Republic{rudolf kadlec,martin.schmid,obajgar,jankle}@cz.ibm.comAbstractSeveral large cloze-style context-question-answer datasets have been introduced re-cently: the CNN and Daily Mail newsdata and the Children?s Book Test.
Thanksto the size of these datasets, the asso-ciated text comprehension task is wellsuited for deep-learning techniques thatcurrently seem to outperform all alterna-tive approaches.
We present a new, simplemodel that uses attention to directly pickthe answer from the context as opposed tocomputing the answer using a blended rep-resentation of words in the document as isusual in similar models.
This makes themodel particularly suitable for question-answering problems where the answer isa single word from the document.
Ensem-ble of our models sets new state of the arton all evaluated datasets.1 IntroductionMost of the information humanity has gathered upto this point is stored in the form of plain text.Hence the task of teaching machines how to un-derstand this data is of utmost importance in thefield of Artificial Intelligence.
One way of testingthe level of text understanding is simply to ask thesystem questions for which the answer can be in-ferred from the text.
A well-known example of asystem that could make use of a huge collection ofunstructured documents to answer questions is forinstance IBM?s Watson system used for the Jeop-ardy challenge (Ferrucci et al, 2010).Cloze style questions (Taylor, 1953), i.e.
ques-tions formed by removing a phrase from a sen-tence, are an appealing form of such questions (forexample see Figure 1).
While the task is easy toevaluate, one can vary the context, the questionDocument: What was supposed to be a fantasy sportscar ride at Walt Disney World Speedway turned deadlywhen a Lamborghini crashed into a guardrail.
Thecrash took place Sunday at the Exotic Driving Experi-ence, which bills itself as a chance to drive your dreamcar on a racetrack.
The Lamborghini?s passenger, 36-year-old Gary Terry of Davenport, Florida, died at thescene, Florida Highway Patrol said.
The driver of theLamborghini, 24-year-old Tavon Watson of Kissimmee,Florida, lost control of the vehicle, the Highway Patrolsaid.
(...)Question: Officials say the driver, 24-year-old TavonWatson, lost control of aAnswer candidates: Tavon Watson, Walt Disney WorldSpeedway, Highway Patrol, Lamborghini, Florida, (...)Answer: LamborghiniFigure 1: Each example consists of a contextdocument, question, answer cadidates and, in thetraining data, the correct answer.
This examplewas taken from the CNN dataset (Hermann et al,2015).
Anonymization of this example that makesthe task harder is shown in Table 3.sentence or the specific phrase missing in the ques-tion to dramatically change the task structure anddifficulty.One way of altering the task difficulty is to varythe word type being replaced, as in (Hill et al,2015).
The complexity of such variation comesfrom the fact that the level of context understand-ing needed in order to correctly predict differenttypes of words varies greatly.
While predictingprepositions can easily be done using relativelysimple models with very little context knowledge,predicting named entities requires a deeper under-standing of the context.Also, as opposed to selecting a random sentencefrom a text (as done in (Hill et al, 2015)), thequestions can be formed from a specific part ofa document, such as a short summary or a list of908CNN Daily Mail CBT CN CBT NEtrain valid test train valid test train valid test train valid test# queries 380,298 3,924 3,198 879,450 64,835 53,182 120,769 2,000 2,500 108,719 2,000 2,500Max # options 527 187 396 371 232 245 10 10 10 10 10 10Avg # options 26.4 26.5 24.5 26.5 25.5 26.0 10 10 10 10 10 10Avg # tokens 762 763 716 813 774 780 470 448 461 433 412 424Vocab.
size 118,497 208,045 53,185 53,063Table 1: Statistics on the 4 data sets used to evaluate the model.
CBT CN stands for CBT CommonNouns and CBT NE stands for CBT Named Entites.
Statistics were taken from (Hermann et al, 2015)and the statistics provided with the CBT data set.tags.
Since such sentences often paraphrase in acondensed form what was said in the text, theyare particularly suitable for testing text compre-hension (Hermann et al, 2015).An important property of cloze style questionsis that a large amount of such questions can be au-tomatically generated from real world documents.This opens the task to data-hungry techniques suchas deep learning.
This is an advantage com-pared to smaller machine understanding datasetslike MCTest (Richardson et al, 2013) that haveonly hundreds of training examples and thereforethe best performing systems usually rely on hand-crafted features (Sachan et al, 2015; Narasimhanand Barzilay, 2015).In the first part of this article we introduce thetask at hand and the main aspects of the relevantdatasets.
Then we present our own model to tacklethe problem.
Subsequently we compare the modelto previously proposed architectures and finallydescribe the experimental results on the perfor-mance of our model.2 Task and datasetsIn this section we introduce the task that we areseeking to solve and relevant large-scale datasetsthat have recently been introduced for this task.2.1 Formal Task DescriptionThe task consists of answering a cloze style ques-tion, the answer to which is dependent on the un-derstanding of a context document provided withthe question.
The model is also provided with a setof possible answers from which the correct one isto be selected.
This can be formalized as follows:The training data consist of tuples (q,d, a, A),where q is a question, d is a document that con-tains the answer to question q, A is a set of possi-ble answers and a ?
A is the ground truth answer.Both q and d are sequences of words from vocab-ulary V .
We also assume that all possible answersare words from the vocabulary, that is A ?
V , andthat the ground truth answer a appears in the doc-ument, that is a ?
d.2.2 DatasetsWe will now briefly summarize important featuresof the datasets.2.2.1 News Articles ?
CNN and Daily MailThe first two datasets1(Hermann et al, 2015) wereconstructed from a large number of news articlesfrom the CNN and Daily Mail websites.
The mainbody of each article forms a context, while thecloze style question is formed from one of shorthighlight sentences, appearing at the top of eacharticle page.
Specifically, the question is createdby replacing a named entity from the summarysentence (e.g.
?Producer X will not press chargesagainst Jeremy Clarkson, his lawyer says.?
).Furthermore the named entities in the wholedataset were replaced by anonymous tokens whichwere further shuffled for each example so that themodel cannot build up any world knowledge aboutthe entities and hence has to genuinely rely on thecontext document to search for an answer to thequestion.Qualitative analysis of reasoning patternsneeded to answer questions in the CNN dataset to-gether with human performance on this task areprovided in (Chen et al, 2016).1The CNN and Daily Mail datasets are available athttps://github.com/deepmind/rc-data9092.2.2 Children?s Book TestThe third dataset2, the Children?s Book Test(CBT) (Hill et al, 2015), is built from books thatare freely available thanks to Project Gutenberg3.Each context document is formed by 20 consecu-tive sentences taken from a children?s book story.Due to the lack of summary, the cloze style ques-tion is then constructed from the subsequent (21st)sentence.One can also see how the task complexity varieswith the type of the omitted word (named entity,common noun, verb, preposition).
(Hill et al,2015) have shown that while standard LSTM lan-guage models have human level performance onpredicting verbs and prepositions, they lack be-hind on named entities and common nouns.
Inthis article we therefore focus only on predictingthe first two word types.Basic statistics about the CNN, Daily Mail andCBT datasets are summarized in Table 1.3 Our Model ?
Attention Sum ReaderOur model called the Attention Sum Reader (ASReader)4is tailor-made to leverage the fact that theanswer is a word from the context document.
Thisis a double-edged sword.
While it achieves state-of-the-art results on all of the mentioned datasets(where this assumption holds true), it cannot pro-duce an answer which is not contained in the doc-ument.
Intuitively, our model is structured as fol-lows:1.
We compute a vector embedding of the query.2.
We compute a vector embedding of each indi-vidual word in the context of the whole doc-ument (contextual embedding).3.
Using a dot product between the questionembedding and the contextual embedding ofeach occurrence of a candidate answer in thedocument, we select the most likely answer.3.1 Formal DescriptionOur model uses one word embedding functionand two encoder functions.
The word embedding2The CBT dataset is available at http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz3https://www.gutenberg.org/4An implementation of AS Reader is available at https://github.com/rkadlec/asreaderfunction e translates words into vector represen-tations.
The first encoder function is a documentencoder f that encodes every word from the doc-ument d in the context of the whole document.We call this the contextual embedding.
For con-venience we will denote the contextual embeddingof the i-th word in d as fi(d).
The second encoderg is used to translate the query q into a fixed lengthrepresentation of the same dimensionality as eachfi(d).
Both encoders use word embeddings com-puted by e as their input.
Then we compute aweight for every word in the document as the dotproduct of its contextual embedding and the queryembedding.
This weight might be viewed as anattention over the document d.To form a proper probability distribution overthe words in the document, we normalize theweights using the softmax function.
This way wemodel probability sithat the answer to query qappears at position i in the document d. In a func-tional form this is:si?
exp (fi(d) ?
g(q)) (1)Finally we compute the probability that word wis a correct answer as:P (w|q,d) ?
?i?I(w,d)si(2)where I(w,d) is a set of positions where w ap-pears in the document d. We call this mechanismpointer sum attention since we use attention as apointer over discrete tokens in the context docu-ment and then we directly sum the word?s atten-tion across all the occurrences.
This differs fromthe usual use of attention in sequence-to-sequencemodels (Bahdanau et al, 2015) where attention isused to blend representations of words into a newembedding vector.
Our use of attention was in-spired by Pointer Networks (Ptr-Nets) (Vinyals etal., 2015).A high level structure of our model is shown inFigure 2.3.2 Model instance detailsIn our model the document encoder f is imple-mented as a bidirectional Gated Recurrent Unit(GRU) network (Cho et al, 2014; Chung et al,2014) whose hidden states form the contextualword embeddings, that is fi(d) =?
?fi(d) ||?
?fi(d),where || denotes vector concatenation and?
?fiand910Document QuestionSoftmax ?
?over words in the documentP Obama q, d =???(?????,d)??
= ??
+ ??+5?..
Obama    and     Putin     ??
said   Obama    in      Prague XXXXX  visited  PragueProbability of the answer???????1?|?|?|?|?1?..?...
.
.
.
.
.
.Recurrentneural networksDot products????
?|?|?|?|???
?probtEmbeddingsInput text?..
?
(Obama) ?
and ?
(Putin) ?..
?
(said) ?
(Obama) ?
(in) ?
(Prague) ?
(XXXXX) ?
visited ?
(Prague)Figure 2: Structure of the model....what was supposed to be a fantasy sports car ride at@entity3 turned deadly when a @entity4 crashedinto a guardrail .
the crash took place sunday at the @en-tity8 , which bills itself as a chance to drive your dreamcar on a racetrack .
the @entity4 ?s passenger , 36 -year - old @entity14 of @entity15 , @entity16 , died atthe scene , @entity13 said .
the driver of the @entity4, 24 - year - old @entity18 of @entity19 , @entity16 ,lost control of the vehicle , the @entity13 said ....officials say the driver , 24 - year - old @entity18 , lostcontrol of aFigure 3: Attention in an example withanonymized entities where our system selected thecorrect answer.
Note that the attention is focusedonly on named entities.?
?fidenote forward and backward contextual em-beddings from the respective recurrent networks.The query encoder g is implemented by anotherbidirectional GRU network.
This time the lasthidden state of the forward network is concate-nated with the last hidden state of the backwardnetwork to form the query embedding, that isg(q) =?
?g|q|(q) ||??g1(q).
The word embeddingfunction e is implemented in a usual way as alook-up table V. V is a matrix whose rows canbe indexed by words from the vocabulary, that ise(w) = Vw, w ?
V .
Therefore, each row of Vcontains embedding of one word from the vocab-ulary.
During training we jointly optimize param-eters of f , g and e....@entity11 film critic @entity29 writes in his reviewthat ?anyone nostalgic for childhood dreams of trans-formation will find something to enjoy in an upliftingmovie that invests warm sentiment in universal themesof loss and resilience , experience and maturity .
?
more: the best and worst adaptations of ?@entity?
@entity43,@entity44 and @entity46 star in director @entity48?scrime film about a hit man trying to save his estrangedson from a revenge plot.
@entity11 chief film critic@entity52 writes in his review that the film...stars in crime film about hit man trying to save hisestranged sonFigure 4: Attention over an example where oursystem failed to select the correct answer (en-tity43).
The system was probably mislead by theco-occurring word ?film?.
Namely, entity11 occurs7 times in the whole document and 6 times it is to-gether with the word ?film?.
On the other hand,the correct answer occurs only 3 times in total andonly once together with ?film?.4 Related WorkSeveral recent deep neural network architec-tures (Hermann et al, 2015; Hill et al, 2015; Chenet al, 2016; Kobayashi et al, 2016) were appliedto the task of text comprehension.
The last twoarchitectures were developed independently at thesame time as our work.
All of these architec-tures use an attention mechanism that allows themto highlight places in the document that might berelevant to answering the question.
We will nowbriefly describe these architectures and compare911them to our approach.4.1 Attentive and Impatient ReadersAttentive and Impatient Readers were proposedin (Hermann et al, 2015).
The simpler AttentiveReader is very similar to our architecture.
It alsouses bidirectional document and query encoders tocompute an attention in a similar way we do.
Themore complex Impatient Reader computes atten-tion over the document after reading every wordof the query.
However, empirical evaluation hasshown that both models perform almost identicallyon the CNN and Daily Mail datasets.The key difference between the AttentiveReader and our model is that the Attentive Readeruses attention to compute a fixed length repre-sentation r of the document d that is equal to aweighted sum of contextual embeddings of wordsin d, that is r =?isifi(d).
A joint query anddocument embedding m is then a non-linear func-tion of r and the query embedding g(q).
This jointembedding m is in the end compared against allcandidate answers a??
A using the dot producte(a?)
?m, in the end the scores are normalized bysoftmax.
That is: P (a?|q,d) ?
exp (e(a?)
?m).In contrast to the Attentive Reader, we select theanswer from the context directly using the com-puted attention rather than using such attention fora weighted sum of the individual representations(see Eq.
2).
The motivation for such simplifica-tion is the following.Consider a context ?AUFOwas observed aboveour city in January and again in March.?
andquestion ?An observer has spotted a UFO in.
?Since both January and March are equally goodcandidates, the attention mechanism might put thesame attention on both these candidates in the con-text.
The blending mechanism described abovewould compute a vector between the representa-tions of these two words and propose the clos-est word as the answer - this may well happen tobe February (it is indeed the case for Word2Vectrained on Google News).
By contrast, our modelwould correctly propose January or March.4.2 Chen et al 2016A model presented in (Chen et al, 2016) is in-spired by the Attentive Reader.
One differenceis that the attention weights are computed with abilinear term instead of simple dot-product, thatis: si?
exp (fi(d)?W g(q)).
The document em-bedding r is computed using a weighted sum as inthe Attentive Reader: r =?isifi(d).
In the endP (a?|q,d) ?
exp (e?(a?)
?
r), where e?is a newembedding function.Even though it is a simplification of the Atten-tive Reader this model performs significantly bet-ter than the original.4.3 Memory NetworksMemNNs (Weston et al, 2014) were applied to thetask of text comprehension in (Hill et al, 2015).The best performing memory networks modelsetup - window memory - uses windows of fixedlength (8) centered around the candidate words asmemory cells.
Due to this limited context window,the model is unable to capture dependencies outof scope of this window.
Furthermore, the repre-sentation within such window is computed simplyas the sum of embeddings of words in that win-dow.
By contrast, in our model the representationof each individual word is computed using a recur-rent network, which not only allows it to capturecontext from the entire document but also the em-bedding computation is much more flexible than asimple sum.To improve on the initial accuracy, a heuristicapproach called self supervision is used in (Hillet al, 2015) to help the network to select theright supporting ?memories?
using an attentionmechanism showing similarities to the ours.
PlainMemNNs without this heuristic are not competi-tive on these machine reading tasks.
Our modeldoes not need any similar heuristics.4.4 Dynamic Entity RepresentationThe Dynamic Entity Representationmodel (Kobayashi et al, 2016) has a com-plex architecture also based on the weightedattention mechanism and max pooling overcontextual embeddings of vectors for each namedentity.4.5 Pointer NetworksOur model architecture was inspired by Ptr-Nets (Vinyals et al, 2015) in using an attentionmechanism to select the answer in the contextrather than to blend words from the context into ananswer representation.
While a Ptr-Net consists ofan encoder as well as a decoder, which uses the at-tention to select the output at each step, our modeloutputs the answer in a single step.
Furthermore,912the pointer networks assume that no input in thesequence appears more than once, which is not thecase in our settings.4.6 SummaryOur model combines the best features of the ar-chitectures mentioned above.
We use recurrentnetworks to ?read?
the document and the queryas done in (Hermann et al, 2015; Chen et al,2016; Kobayashi et al, 2016) and we use atten-tion in a way similar to Ptr-Nets.
We also usesummation of attention weights in a way similarto MemNNs (Hill et al, 2015).From a high level perspective we simplify allthe discussed text comprehension models by re-moving all transformations past the attention step.Instead we use the attention directly to computethe answer probability.5 EvaluationIn this section we evaluate our model on theCNN, Daily Mail and CBT datasets.
We showthat despite the model?s simplicity its ensemblesachieve state-of-the-art performance on each ofthese datasets.5.1 Training DetailsTo train the model we used stochastic gradient de-scent with the ADAM update rule (Kingma andBa, 2015) and learning rate of 0.001 or 0.0005.During training we minimized the following neg-ative log-likelihood with respect to ?:?logP?
(a|q,d) (3)where a is the correct answer for query q and doc-ument d, and ?
represents parameters of the en-coder functions f and g and of the word embed-ding function e. The optimized probability distri-bution P (a|q,d) is defined in Eq.
2.The initial weights in the word embedding ma-trix were drawn randomly uniformly from theinterval [?0.1, 0.1].
Weights in the GRU net-works were initialized by random orthogonal ma-trices (Saxe et al, 2014) and biases were ini-tialized to zero.
We also used a gradient clip-ping (Pascanu et al, 2012) threshold of 10 andbatches of size 32.During training we randomly shuffled all exam-ples in each epoch.
To speedup training, we al-ways pre-fetched 10 batches worth of examplesand sorted them according to document length.Hence each batch contained documents of roughlythe same length.For each batch of the CNN and Daily Maildatasets we randomly reshuffled the assignmentof named entities to the corresponding word em-bedding vectors to match the procedure proposedin (Hermann et al, 2015).
This guaranteed thatword embeddings of named entities were usedonly as semantically meaningless labels not en-coding any intrinsic features of the representedentities.
This forced the model to truly deducethe answer from the single context document as-sociated with the question.
We also do not usepre-trained word embeddings to make our trainingprocedure comparable to (Hermann et al, 2015).We did not perform any text pre-processingsince the original datasets were already tokenized.We do not use any regularization since in ourexperience it leads to longer training times of sin-gle models, however, performance of a model en-semble is usually the same.
This way we can trainthe whole ensemble faster when using multipleGPUs for parallel training.For Additional details about the training proce-dure see Appendix A.5.2 Evaluation MethodWe evaluated the proposed model both as a singlemodel and using ensemble averaging.
Althoughthe model computes attention for every word inthe document we restrict the model to select ananswer from a list of candidate answers associatedwith each question-document pair.For single models we are reporting results forthe best model as well as the average of accura-cies for the best 20% of models with best perfor-mance on validation data since single models dis-play considerable variation of results due to ran-dom weight initialization5, even for identical hy-perparameter values.
Single model performancemay consequently prove difficult to reproduce.What concerns ensembles, we used simple aver-aging of the answer probabilities predicted by en-semble members.
For ensembling we used 14, 16,84 and 53 models for CNN, Daily Mail and CBTCN and NE respectively.
The ensemble modelswere chosen either as the top 70% of all trainedmodels, we call this avg ensemble.
Alternativelywe use the following algorithm: We started with5The standard deviation for models with the same hyper-parameters was between 0.6-2.5% in absolute test accuracy.913CNN Daily Mailvalid test valid testAttentive Reader?61.6 63.0 70.5 69.0Impatient Reader?61.8 63.8 69.0 68.0MemNNs (single model)?63.4 66.8 NA NAMemNNs (ensemble)?66.2 69.4 NA NADynamic Entity Repres.
(max-pool)]71.2 70.7 NA NADynamic Entity Repres.
(max-pool + byway)]70.8 72.0 NA NADynamic Entity Repres.
+ w2v]71.3 72.9 NA NAChen et al (2016) (single model) 72.4 72.4 76.9 75.8AS Reader (single model) 68.6 69.5 75.0 73.9AS Reader (avg for top 20%) 68.4 69.9 74.5 73.5AS Reader (avg ensemble) 73.9 75.4 78.1 77.1AS Reader (greedy ensemble) 74.5 74.8 78.7 77.7Table 2: Results of our AS Reader on the CNN and Daily Mail datasets.
Results for models markedwith?are taken from (Hermann et al, 2015), results of models marked with?are taken from (Hill et al,2015) and results marked with]are taken from (Kobayashi et al, 2016).
Performance of?and]modelswas evaluated only on CNN dataset.Named entity Common nounvalid test valid testHumans (query)(?
)NA 52.0 NA 64.4Humans (context+query)(?
)NA 81.6 NA 81.6LSTMs (context+query)?51.2 41.8 62.6 56.0MemNNs (window memory + self-sup.
)?70.4 66.6 64.2 63.0AS Reader (single model) 73.8 68.6 68.8 63.4AS Reader (avg for top 20%) 73.3 68.4 67.7 63.2AS Reader (avg ensemble) 74.5 70.6 71.1 68.9AS Reader (greedy ensemble) 76.2 71.0 72.4 67.5Table 3: Results of our AS Reader on the CBT datasets.
Results marked with?are taken from (Hill etal., 2015).(?
)Human results were collected on 10% of the test set.the best performing model according to validationperformance.
Then in each step we tried addingthe best performing model that had not been pre-viously tried.
We kept it in the ensemble if it didimprove its validation performance and discardedit otherwise.
This way we gradually tried eachmodel once.
We call the resulting model a greedyensemble.5.3 ResultsPerformance of our models on the CNN and DailyMail datasets is summarized in Table 2, Table 3shows results on the CBT dataset.
The tables alsolist performance of other published models thatwere evaluated on these datasets.
Ensembles ofour models set the new state-of-the-art results onall evaluated datasets.Table 4 then measures accuracy as the pro-portion of test cases where the ground truth wasamong the top k answers proposed by the greedyensemble model for k = 1, 2, 5.CNN and Daily Mail.
The CNN dataset is themost widely used dataset for evaluation of textcomprehension systems published so far.
Perfor-914l l l ll l l lll0.690.720.750.780.81400 800 1200 1600Number of tokens in context documentTestaccuracyDatasetl Daily MailCNNCNN & Daily Mail(a)l l l l lll l l l0.660.690.720.75300 400 500 600 700Number of tokens in context documentTestaccuracyWord typel Common NounsNamed EntitiesChildren's Book Test(b)0.000.040.080.120 500 1000 1500 2000Number of tokens in context documentFrequency intestdata DatasetDaily MailCNNCNN & Daily Mail(c)0.000.050.100.150 300 600 900Number of tokens in context documentFrequency intestdata Word typeCommon NounsNamed EntitiesChildren's Book Test(d)Figure 5: Sub-figures (a) and (b) plot the test accuracy against the length of the context document.
Theexamples were split into ten buckets of equal size by their context length.
Averages for each bucket areplotted on each axis.
Sub-figures (c) and (d) show distributions of context lengths in the four datasets.l l l l l l l l ll0.50.60.70.80.920 40 60Number of candidate answersTestaccuracyDatasetl Daily MailCNNCNN & Daily Mail(a)0.000.050.100.150.200 25 50 75 100Number of candidate answer entitiesFrequency intestdata DatasetDaily MailCNNCNN & Daily Mail(b)Figure 6: Subfigure (a) illustrates how the modelaccuracy decreases with an increasing number ofcandidate named entities.
Subfigure (b) shows theoverall distribution of the number of candidate an-swers in the news datasets.ll l l l l l l l0.50.60.70.80.91 2 3 4 5 6 7 8 9nTestaccuracyDatasetl Daily MailCNNCNN & Daily Mail(a)0.000.050.100.151 2 3 4 5 6 7 8 9nFrequency intestdata DatasetDaily MailCNNCNN & Daily Mail(b)Figure 7: Subfigure (a) shows the model accu-racy when the correct answer is the nthmost fre-quent named entity for n ?
[1, 10].
Subfigure (b)shows the number of test examples for which thecorrect answer was the n?th most frequent entity.The plots for CBT look almost identical (see Ap-pendix B).915mance of our single model is a little bit worse thanperformance of simultaneously published models(Chen et al, 2016; Kobayashi et al, 2016).
Com-pared to our work these models were trained withDropout regularization (Srivastava et al, 2014)which might improve single model performance.However, ensemble of our models outperformsthese models even though they use pre-trainedword embeddings.On the CNN dataset our single model withbest validation accuracy achieves a test accuracyof 69.5%.
The average performance of the top20% models according to validation accuracy is69.9% which is even 0.5% better than the singlebest-validation model.
This shows that there weremany models that performed better on test set thanthe best-validation model.
Fusing multiple modelsthen gives a significant further increase in accu-racy on both CNN and Daily Mail datasets..CBT.
In named entity prediction our best singlemodel with accuracy of 68.6% performs 2% abso-lute better than the MemNN with self supervision,the averaging ensemble performs 4% absolute bet-ter than the best previous result.
In common nounprediction our single models is 0.4% absolute bet-ter than MemNN however the ensemble improvesthe performance to 69% which is 6% absolute bet-ter than MemNN.Dataset k = 1 k = 2 k = 5CNN 74.8 85.5 94.8Daily Mail 77.7 87.6 94.8CBT NE 71.0 86.9 96.8CBT CN 67.5 82.5 95.4Table 4: Proportion of test examples for which thetop k answers proposed by the greedy ensembleincluded the correct answer.6 AnalysisTo further analyze the properties of our model,we examined the dependence of accuracy on thelength of the context document (Figure 5), thenumber of candidate answers (Figure 6) and thefrequency of the correct answer in the context(Figure 7).On the CNN and Daily Mail datasets, the ac-curacy decreases with increasing document length(Figure 5a).
We hypothesize this may be dueto multiple factors.
Firstly long documents maymake the task more complex.
Secondly such casesare quite rare in the training data (Figure 5b) whichmotivates the model to specialize on shorter con-texts.
Finally the context length is correlated withthe number of named entities, i.e.
the number ofpossible answers which is itself negatively corre-lated with accuracy (see Figure 6).On the CBT dataset this negative trend seems todisappear (Fig.
5c).
This supports the later twoexplanations since the distribution of documentlengths is somewhat more uniform (Figure 5d) andthe number of candidate answers is constant (10)for all examples in this dataset.The effect of increasing number of candidateanswers on the model?s accuracy can be seen inFigure 6a.
We can clearly see that as the num-ber of candidate answers increases, the accuracydrops.
On the other hand, the amount of exampleswith large number of candidate answers is quitesmall (Figure 6b).Finally, since the summation of attention in ourmodel inherently favours frequently occurring to-kens, we also visualize how the accuracy dependson the frequency of the correct answer in the doc-ument.
Figure 7a shows that the accuracy signif-icantly drops as the correct answer gets less andless frequent in the document compared to othercandidate answers.
On the other hand, the correctanswer is likely to occur frequently (Fig.
7a).7 ConclusionIn this article we presented a new neural networkarchitecture for natural language text comprehen-sion.
While our model is simpler than previouslypublished models, it gives a new state-of-the-artaccuracy on all the evaluated datasets.An analysis by (Chen et al, 2016) suggests thaton CNN and Daily Mail datasets a significant pro-portion of questions is ambiguous or too difficultto answer even for humans (partly due to entityanonymization) so the ensemble of our modelsmay be very near to the maximal accuracy achiev-able on these datasets.AcknowledgmentsWe would also like to thank Tim Klinger for pro-viding us with masked softmax code that we usedin our implementation.916ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural Machine Translation by JointlyLearning to Align and Translate.
International Con-ference on Learning Representations.Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian J. Goodfellow, Arnaud Berg-eron, Nicolas Bouchard, and Yoshua Bengio.
2012.Theano: new features and speed improvements.Deep Learning and Unsupervised Feature LearningNIPS 2012 Workshop.Danqi Chen, Jason Bolton, and Christopher D. Man-ning.
2016.
A Thorough Examination of the CNN /Daily Mail Reading Comprehension Task.
In Asso-ciation for Computational Linguistics (ACL).Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-ger Schwenk, and Yoshua Bengio.
2014.
Learn-ing Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.
Em-pirical Methods in Natural Language Processing(EMNLP).Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho,and Yoshua Bengio.
2014.
Empirical Evaluationof Gated Recurrent Neural Networks on SequenceModeling.
arXiv, pages 1?9.David Ferrucci, Eric Brown, Jennifer Chu-Carroll,James Fan, David Gondek, Aditya a. Kalyanpur,Adam Lally, J. William Murdock, Eric Nyberg, JohnPrager, Nico Schlaefer, and Chris Welty.
2010.Building Watson: An Overview of the DeepQAProject.
AI Magazine, 31(3):59?79.Karl Moritz Hermann, Tomas Kocisky, EdwardGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom.
2015.
Teaching ma-chines to read and comprehend.
In Advances in Neu-ral Information Processing Systems, pages 1684?1692.Felix Hill, Antoine Bordes, Sumit Chopra, and JasonWeston.
2015.
The goldilocks principle: Readingchildren?s books with explicit memory representa-tions.
arXiv preprint arXiv:1511.02301.Diederik P. Kingma and Jimmy Lei Ba.
2015.
Adam:a Method for Stochastic Optimization.
InternationalConference on Learning Representations, pages 1?13.Sosuke Kobayashi, Ran Tian, Naoaki Okazaki, andKentaro Inui.
2016.
Dynamic Entity Representa-tion with Max-pooling Improves Machine Reading.Proceedings of the North American Chapter of theAssociation for Computational Linguistics and Hu-man Language Technologies (NAACL-HLT).Karthik Narasimhan and Regina Barzilay.
2015.Machine Comprehension with Discourse Relations.Proceedings of the 53rd Annual Meeting of theAssociation for Computational Linguistics and the7th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages1253?1262.Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-gio.
2012.
On the difficulty of training recurrentneural networks.
Proceedings of The 30th Inter-national Conference on Machine Learning, pages1310?1318.Matthew Richardson, Christopher J C Burges, and ErinRenshaw.
2013.
MCTest: A Challenge Dataset forthe Open-Domain Machine Comprehension of Text.Empirical Methods in Natural Language Processing(EMNLP), pages 193?203.Mrinmaya Sachan, Avinava Dubey, Eric P Xing, andMatthew Richardson.
2015.
Learning Answer-Entailing Structures for Machine Comprehension.Association for Computational Linguistics (ACL),pages 239?249.Andrew M Saxe, James L Mcclelland, and Surya Gan-guli.
2014.
Exact solutions to the nonlinear dynam-ics of learning in deep linear neural networks.
Inter-national Conference on Learning Representations.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: prevent NN from overfitting.
Journal ofMachine Learning Research, 15:1929?1958.Wilson L Taylor.
1953.
Cloze procedure: a new toolfor measuring readability.
Journalism and MassCommunication Quarterly, 30(4):415.Bart van Merrienboer, Dzmitry Bahdanau, Vincent Du-moulin, Dmitriy Serdyuk, David Warde-farley, JanChorowski, and Yoshua Bengio.
2015.
Blocks andFuel : Frameworks for deep learning.
pages 1?5.Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.2015.
Pointer networks.
In Advances in Neural In-formation Processing Systems, pages 2674?2682.Jason Weston, Sumit Chopra, and Antoine Bor-des.
2014.
Memory networks.
arXiv preprintarXiv:1410.3916.917Appendix A Training DetailsDuring training we evaluated the model perfor-mance after each epoch and stopped the trainingwhen the error on the validation set started increas-ing.The models usually converged after two epochsof training.
Time needed to complete a singleepoch of training on each dataset on an NvidiaK40 GPU is shown in Table 5.Dataset Time per epochCNN 10h 22minDaily Mail 25h 42minCBT Named Entity 1h 5minCBT Common Noun 0h 56minTable 5: Average duration of one epoch of trainingon the four datasets.The hyperparameters, namely the recurrent hid-den layer dimension and the source embedding di-mension, were chosen by grid search.
We startedwith a range of 128 to 384 for both parametersand subsequently kept increasing the upper boundby 128 until we started observing a consistent de-crease in validation accuracy.
The region of theparameter space that we explored together with theparameters of the model with best validation accu-racy are summarized in Table 6.Rec.
Hid.
Layer EmbeddingDataset min max best min max bestCNN 128 512 384 128 512 128Daily Mail 128 1024 512 128 512 384CBT NE 128 512 384 128 512 384CBT CN 128 1536 256 128 512 384Table 6: Dimension of the recurrent hidden layerand of the source embedding for the best modeland the range of values that we tested.
We reportnumber of hidden units of the unidirectional GRU;the bidirectional GRU has twice as many hiddenunits.Our model was implemented usingTheano (Bastien et al, 2012) and Blocks (vanMerrienboer et al, 2015).Appendix B Dependence of accuracy onthe frequency of the correctanswerIn Section 6 we analysed how the test accuracydepends on how frequent the correct answer iscompared to other answer candidates for the newsdatasets.
The plots for the Children?s Book Testlooks very similar, however we are adding it herefor completeness.ll l l ll0.40.50.60.70.81 2 3 4 5 6nTestaccuracyWord typel Common NounsNamed EntitiesChildren's Book Test(a)0.000.050.100.150.201 2 3 4 5 6nFrequency intestdata Word typeCommon NounsNamed EntitiesChildren's Book Test(b)Figure 8: Subfigure (a) shows the model accu-racy when the correct answer is among nmost fre-quent named entities for n ?
[1, 10].
Subfigure (b)shows the number of test examples for which thecorrect answer was the n?th most frequent entity.918
