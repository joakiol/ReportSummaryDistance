Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 30?41,Vancouver, October 2005. c?2005 Association for Computational LinguisticsParsing with Soft and Hard Constraints on Dependency Length?Jason Eisner and Noah A. SmithDepartment of Computer Science / Center for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD 21218 USA{jason,nasmith}@cs.jhu.eduAbstractIn lexicalized phrase-structure or dependency parses, a word?smodifiers tend to fall near it in the string.
We show that a crudeway to use dependency length as a parsing feature can sub-stantially improve parsing speed and accuracy in English andChinese, with more mixed results on German.
We then showsimilar improvements by imposing hard bounds on dependencylength and (additionally) modeling the resulting sequence ofparse fragments.
This simple ?vine grammar?
formalism hasonly finite-state power, but a context-free parameterization withsome extra parameters for stringing fragments together.
We ex-hibit a linear-time chart parsing algorithm with a low grammarconstant.1 IntroductionMany modern parsers identify the head word ofeach constituent they find.
This makes it possibleto identify the word-to-word dependencies implicitin a parse.1 (Some parsers, known as dependencyparsers, even return these dependencies as their pri-mary output.
)Why bother to identify these dependencies?
Thetypical reason is to model the fact that some wordpairs are more likely than others to engage in a de-pendency relationship.2 In this paper, we propose adifferent reason to identify dependencies in candi-date parses: to evaluate not the dependency?s wordpair but its length (i.e., the string distance betweenthe two words).
Dependency lengths differ from?
This work was supported by NSF ITR grant IIS-0313193to the first author and a fellowship from the Fannie and JohnHertz Foundation to the second author.
The views expressedare not necessarily endorsed by the sponsors.
The authors thankMark Johnson, Eugene Charniak, Charles Schafer, Keith Hall,and John Hale for helpful discussion and Elliott Dra?bek andMarkus Dreyer for insights on (respectively) Chinese and Ger-man parsing.
They also thank an anonymous reviewer for sug-gesting the German experiments.1In a phrase-structure parse, if phrase X headed by wordtoken x is a subconstituent of phrase Y headed by word tokeny 6= x, then x is said to depend on y.
In a more powerfulcompositional formalism like LTAG or CCG, dependencies canbe extracted from the derivation tree.2It has recently been questioned whether these ?bilexical?features actually contribute much to parsing performance (Kleinand Manning, 2003; Bikel, 2004), at least when one has only amillion words of training.typical parsing features in that they cannot be deter-mined from tree-local information.
Though lengthsare not usually considered, we will see that bilexicaldynamic-programming parsing algorithms can eas-ily consider them as they build the parse.Soft constraints.
Like any other feature of trees,dependency lengths can be explicitly used as fea-tures in a probability model that chooses amongtrees.
Such a model will tend to disfavor long de-pendencies (at least of some kinds), as these are em-pirically rare.
In the first part of the paper, we showthat such features improve a simple baseline depen-dency parser.Hard constraints.
If the bias against long de-pendencies is strengthened into a hard constraintthat absolutely prohibits long dependencies, then theparser turns into a partial parser with only finite-statepower.
In the second part of the paper, we show howto perform chart parsing in asymptotic linear timewith a low grammar constant.
Such a partial parserdoes less work than a full parser in practice, and inmany cases recovers a more precise set of dependen-cies (with little loss in recall).2 Short Dependencies in LangugageWe assume that correct parses exhibit a ?short-dependency preference?
: a word?s dependents tendto be close to it in the string.3 If the j th word of a sen-tence depends on the ith word, then |i?j| tends to be3 In this paper, we consider only a crude notion of ?close-ness?
: the number of intervening words.
Other distance mea-sures could be substituted or added (following the literature onheavy-shift and sentence comprehension), including the phono-logical, morphological, syntactic, or referential (given/new)complexity of the intervening material (Gibson, 1998).
In pars-ing, the most relevant previous work is due to Collins (1997),who considered three binary features of the intervening mate-rial: did it contain (a) any word tokens at all, (b) any verbs,(c) any commas or colons?
Note that (b) is effective becauseit measures the length of a dependency in terms of the numberof alternative attachment sites that the dependent skipped over,a notion that could be generalized.
Similarly, McDonald et al(2005) separately considered each of the intervening POS tags.30small.
This implies that neither i nor j is modified bycomplex phrases that fall between i and j.
In termsof phrase structure, it implies that the phrases mod-ifying word i from a given side tend to be (1) fewin number, (2) ordered so that the longer phrases fallfarther from i, and (3) internally structured so thatthe bulk of each phrase falls on the side of j awayfrom i.These principles can be blamed for several lin-guistic phenomena.
(1) helps explain the ?late clo-sure?
or ?attach low?
heuristic (e.g., Frazier, 1979;Hobbs and Bear, 1990): a modifier such as a PP ismore likely to attach to the closest appropriate head.
(2) helps account for heavy-shift: when an NP islong and complex, take NP out, put NP on the ta-ble, and give NP to Mary are likely to be rephrasedas take out NP, put on the table NP, and give MaryNP.
(3) explains certain non-canonical word orders:in English, a noun?s left modifier must become aright modifier if and only if it is right-heavy (a tallerpolitician vs. a politician taller than all her rivals4),and a verb?s left modifier may extrapose its right-heavy portion (An aardvark walked in who had cir-cumnavigated the globe5).Why should sentences prefer short dependencies?Such sentences may be easier for humans to produceand comprehend.
Each word can quickly ?dischargeits responsibilities,?
emitting or finding all its depen-dents soon after it is uttered or heard; then it canbe dropped from working memory (Church, 1980;Gibson, 1998).
Such sentences also succumb nicelyto disambiguation heuristics that assume short de-pendencies, such as low attachment.
Thus, to im-prove comprehensibility, a speaker can make stylis-tic choices that shorten dependencies (e.g., heavy-shift), and a language can categorically prohibitsome structures that lead to long dependencies (*ataller-than-all-her-rivals politician; *the sentence4Whereas *a politician taller and *a taller-than-all-her-rivals politician are not allowed.
The phenomenon is pervasive.5This actually splits the heavy left dependent [an aardvarkwho ...] into two non-adjacent pieces, moving the heavy secondpiece.
By slightly stretching the aardvark-who dependency inthis way, it greatly shortens aardvark-walked.
The same is pos-sible for heavy, non-final right dependents: I met an aardvarkyesterday who had circumnavigated the globe again stretchesaardvark-who, which greatly shortens met-yesterday.
These ex-amples illustrate (3) and (2) respectively.
However, the resultingnon-contiguous constituents lead to non-projective parses thatare beyond the scope of this paper.that another sentence that had center-embeddingwas inside was incomprehensible).Such functionalist pressures are not all-powerful.For example, many languages use SOV basic wordorder where SVO (or OVS) would give shorter de-pendencies.
However, where the data exhibit someshort-dependency preference, computer parsers aswell as human parsers can obtain speed and accu-racy benefits by exploiting that fact.3 Soft Constraints on Dependency LengthWe now enhance simple baseline probabilisticparsers for English, Chinese, and German so thatthey consider dependency lengths.
We confine our-selves (throughout the paper) to parsing part-of-speech (POS) tag sequences.
This allows us to ig-nore data sparseness, out-of-vocabulary, smoothing,and pruning issues, but it means that our accuracymeasures are not state-of-the-art.
Our techniquescould be straightforwardly adapted to (bi)lexicalizedparsers on actual word sequences, though not neces-sarily with the same success.3.1 Grammar FormalismThroughout this paper we will use split bilexicalgrammars, or SBGs (Eisner, 2000), a notationallysimpler variant of split head-automaton grammars,or SHAGs (Eisner and Satta, 1999).
The formalismis context-free.
We define here a probabilistic ver-sion,6 which we use for the baseline models in ourexperiments.
They are only baselines because theSBG generative process does not take note of de-pendency length.An SBG is an tuple G = (?, $, L,R).
?
is analphabet of words.
(In our experiments, we parseonly POS tag sequences, so ?
is actually an alpha-bet of tags.)
$ 6?
?
is a distinguished root symbol;let ??
= ?
?
{$}.
L and R are functions from ?
?to probabilistic ?-free finite-state automata over ?.Thus, for each w ?
?
?, the SBG specifies ?left?
and?right?
probabilistic FSAs, Lw and Rw.We use Lw(G) : ???
?
[0, 1] to denote the prob-abilistic context-free language of phrases headed byw.
Lw(G) is defined by the following simple top-down stochastic process for sampling from it:6There is a straightforward generalization to weightedSBGs, which need not have a stochastic generative model.311.
Sample from the finite-state language L(Lw) asequence ?
= w?1w?2 .
.
.
w?` ?
??
of leftchildren, and from L(Rw) a sequence ?
=w1w2 .
.
.
wr ?
??
of right children.
Each se-quence is found by a random walk on its proba-bilistic FSA.
We say the children depend on w.2.
For each i from ?` to r with i 6= 0, recursivelysample ?i ?
??
from the context-free languageLwi(G).
It is this step that indirectly determinesdependency lengths.3.
Return ?
?` .
.
.
??2?
?1w?1?2 .
.
.
?r ?
??
?, aconcatenation of strings.Notice that w?s left children ?
were generated inreverse order, so w?1 and w1 are its closest childrenwhile w?` and wr are the farthest.Given an input sentence ?
= w1w2 .
.
.
wn ?
?
?,a parser attempts to recover the highest-probabilityderivation by which $?
could have been generatedfrom L$(G).
Thus, $ plays the role of w0.
A samplederivation is shown in Fig.
1a.
Typically, L$ andR$ are defined so that $ must have no left children(` = 0) and at most one right child (r ?
1), thelatter serving as the conventional root of the parse.3.2 Baseline ModelsIn the experiments reported here, we defined onlyvery simple automata for Lw and Rw (w ?
?
).However, we tried three automaton types, of vary-ing quality, so as to evaluate the benefit of addinglength-sensitivity at three different levels of baselineperformance.In model A (the worst), each automaton has topol-ogy }  ff, with a single state q1, so token w?s leftdependents are conditionally independent of one an-other given w. In model C (the best), each au-tomaton }??}
 ff has an extra state q0 that al-lows the first (closest) dependent to be chosen dif-ferently from the rest.
Model B is a compromise:7it is like model A, but each type w ?
?
mayhave an elevated or reduced probability of havingno dependents at all.
This is accomplished by us-ing automata }??}
 ff as in model C, which al-lows the stopping probabilities p(STOP | q0) andp(STOP | q1) to differ, but tying the conditional dis-7It is equivalent to the ?dependency model with valence?
ofKlein and Manning (2004).tributions p(q0 w?
?q1 | q0,?STOP) and p(q1 w?
?q1 |q1,?STOP).Finally, in ?3, L$ and R$ are restricted as above,so R$ gives a probability distribution over ?
only.3.3 Length-Sensitive ModelsNone of the baseline models A?C explicitly modelthe distance between a head and child.
We enhancedthem by multiplying in some extra length-sensitivefactors when computing a tree?s probability.
Foreach dependency, an extra factor p(?
| .
.
.)
is mul-tiplied in for the probability of the dependency?slength ?
= |i ?
j|, where i and j are the positionsof the head and child in the surface string.8Again we tried three variants.
In one version, thisnew probability p(?| .
.
.)
is conditioned only on thedirection d = sign(i ?
j) of the dependency.
Inanother version, it is conditioned only on the POStag h of the head.
In a third version, it is conditionedon d, h, and the POS tag c of the child.3.4 Parsing AlgorithmFig.
2a gives a variant of Eisner and Satta?s (1999)SHAG parsing algorithm, adapted to SBGs, whichare easier to understand.9 (We will modify this al-gorithm later in ?4.)
The algorithm obtains O(n3)runtime, despite the need to track the position ofhead words, by exploiting the conditional indepen-dence between a head?s left children and right chil-dren.
It builds ?half-constituents?
denoted by @(a head word together with some modifying phraseson the right, i.e., w?1 .
.
.
?r) and   (a head wordtogether with some modifying phrases on the left,i.e., ?
?` .
.
.
??1w).
A new dependency is intro-duced when @ +   are combined to get Hor (a pair of linked head words with all theintervening phrases, i.e., w?1 .
.
.
?r???`?
.
.
.
??
?1w?,where w is respectively the parent or child of w?
).One can then combine H + @ = @ , or8Since the ?
values are fully determined by the tree but ev-ery p(?
| .
.
.)
?
1, this crude procedure simply reduces theprobability mass of every legal tree.
The resulting model is de-ficient (does not sum to 1); the remaining probability mass goesto impossible trees whose putative dependency lengths ?
areinconsistent with the tree structure.
We intend in future workto explore non-deficient models (log-linear or generative), buteven the present crude approach helps.9The SHAG notation was designed to highlight the connec-tion to non-split HAGs.32  +  =   .
Only O(n3) combinationsare possible in total when parsing a length-n sen-tence.3.5 A Note on Word Senses[This section may be skipped by the casual reader.
]A remark is necessary about :w and :w?
in Fig.
2a,which represent senses of the words at positionsh and h?.
Like past algorithms for SBGs (Eisner,2000), Fig.
2a is designed to be a bit more generaland integrate sense disambiguation into parsing.
Itformally runs on an input ?
= W1 .
.
.Wn ?
?
?,where each Wi ?
?
is a ?confusion set?
over pos-sible values of the ith word wi.
The algorithm re-covers the highest-probability derivation that gener-ates $?
for some ?
?
?
(i.e., ?
= w1 .
.
.
wn with(?i)wi ?Wi).This extra level of generality is not needed for anyof our experiments, but it is needed for SBG parsersto be as flexible as SHAG parsers.
We include it inthis paper to broaden the applicability of both Fig.
2aand our extension of it in ?4.The ?senses?
can be used in an SBG to pass afinite amount of information between the left andright children of a word, just as SHAGs allow.10 Forexample, to model the fronting of a direct object, anSBG might use a special sense of a verb, whose au-tomata tend to generate both one more noun in ?
andone fewer noun in ?.Senses can also be used to pass information be-tween parents and children.
Important uses areto encode lexical senses, or to enrich the de-pendency parse with constituent labels or depen-10Fig.
2a enhances the Eisner-Satta version with explicitsenses while matching its asymptotic performance.
On thispoint, see (Eisner and Satta, 1999, ?8 and footnote 6).
How-ever, it does have a practical slowdown, in that START-LEFTnondeterministically guesses every possible sense of Wi, andthese senses are pursued separately.
To match the Eisner-Sattaalgorithm, we should not need to commit to a word?s sense un-til we have seen all its left children.
That is, left triangles andleft trapezoids should not carry a sense :w at all, except for thecompleted left triangle (marked F) that is produced by FINISH-LEFT.
FINISH-LEFT should choose a sense w of Wh accord-ing to the final state q, which reflects knowledge of Wh?s leftchildren.
For this strategy to work, the transitions in Lw (usedby ATTACH-LEFT) must not depend on the particular sense wbut only on W .
In other words, all Lw : w ?
Wh are reallycopies of a shared LWh , except that they may have different fi-nal states.
This requirement involves no loss of generality, sincethe nondeterministic shared LWh is free to branch as soon as itlikes onto paths that commit to the various senses w.dency labels (Eisner, 2000).
For example, the in-put token Wi = {bank1/N/NP , bank2/N/NP ,bank3/V/VP , bank3/V/S} ?
?
allows four?senses?
of bank, namely two nominal meanings,and two syntactically different versions of the verbalmeaning, whose automata require them to expandinto VP and S phrases respectively.The cubic runtime is proportional to the num-ber of ways of instantiating the inference rules inFig.
2a: O(n2(n + t?
)tg2), where n = |?| is theinput length, g = maxni=1 |Wi| bounds the size ofa confusion set, t bounds the number of states perautomaton, and t?
?
t bounds the number of au-tomaton transitions from a state that emit the sameword.
For deterministic automata, t?
= 1.113.6 Probabilistic ParsingIt is easy to make the algorithm of Fig.
2a length-sensitive.
When a new dependency is added by anATTACH rule that combines @ +   , the an-notations on @ and   suffice to determinethe dependency?s length ?
= |h ?
h?|, directiond = sign(h ?
h?
), head word w, and child wordw?.12 So the additional cost of such a dependency,e.g.
p(?
| d,w,w?
), can be included as the weightof an extra antecedent to the rule, and so included inthe weight of the resulting  or H .To execute the inference rules in Fig.
2a, weuse a prioritized agenda.
Derived items such as@ ,   ,, and H are prioritized bytheir Viterbi-inside probabilities.
This is knownas uniform-cost search or shortest-hyperpath search(Nederhof, 2003).
We halt as soon as a full parse(the accept item) pops from the agenda, sinceuniform-cost search (as a special case of the A?algorithm) guarantees this to be the maximum-probability parse.
No other pruning is done.11Confusion-set parsing may be regarded as parsing a par-ticular lattice with n states and ng arcs.
The algorithm canbe generalized to lattice parsing, in which case it has runtimeO(m2(n + t?
)t) for a lattice of n states and m arcs.
Roughly,h : w is replaced by an arc, while i is replaced by a state andi?
1 is replaced by the same state.12For general lattice parsing, it is not possible to determine ?while applying this rule.
There h and h?
are arcs in the lattice,not integers, and different paths from h to h?
might cover dif-ferent numbers of words.
Thus, if one still wanted to measuredependency length in words (rather than in, say, millisecondsof speech), each item would have to record its width explicitly,leading in general to more items and increased runtime.33With a prioritized agenda, a probability modelthat more sharply discriminates among parses willtypically lead to a faster parser.
(Low-probabilityconstituents languish at the back of the agenda andare never pursued.)
We will see that the length-sensitive models do run faster for this reason.3.7 Experiments with Soft ConstraintsWe trained models A?C, using unsmoothed maxi-mum likelihood estimation, on three treebanks: thePenn (English) Treebank (split in the standard way,?2?21 train/?23 test, or 950K/57K words), the PennChinese Treebank (80% train/10% test or 508K/55Kwords), and the German TIGER corpus (80%/10%or 539K/68K words).13 Estimation was a simplematter of counting automaton events and normaliz-ing counts into probabilities.
For each model, wealso trained the three length-sensitive versions de-scribed in ?3.3.The German corpus contains non-projective trees.None of our parsers can recover non-projective de-pendencies (nor can our models produce them).
Thisfact was ignored when counting events for maxi-mum likelihood estimation: in particular, we alwaystrained Lw and Rw on the sequence of w?s immedi-ate children, even in non-projective trees.Our results (Tab.
1) show that sharpening theprobabilities with the most sophisticated distancefactors p(?
| d, h, c), consistently improved thespeed of all parsers.14 The change to the code istrivial.
The only overhead is the cost of looking upand multiplying in the extra distance factors.Accuracy also improved over the baseline mod-els of English and Chinese, as well as the simplerbaseline models of German.
Again, the most so-phisticated distance factors helped most, but eventhe simplest distance factor usually obtained mostof the accuracy benefit.German model C fell slightly in accuracy.
Thespeedup here suggests that the probabilities weresharpened, but often in favor of the wrong parses.We did not analyze the errors on German; it may13Heads were extracted for English using Michael Collins?rules and Chinese using Fei Xia?s rules (defaulting in both casesto right-most heads where the rules fail).
German heads wereextracted using the TIGER Java API; we discarded all resultingdependency structures that were cyclic or unconnected (6%).14We measure speed abstractly by the number of items builtand pushed on the agenda.be relevant that 25% of the German sentences con-tained a non-projective dependency between non-punctuation tokens.Studying the parser output for English, we foundthat the length-sensitive models preferred closer at-tachments, with 19.7% of tags having a nearer parentin the best parse under model C with p(?
| d, h, c)than in the original model C, 77.7% having a par-ent at the same distance, and only 2.5% having afarther parent.
The surviving long dependencies (atany length > 1) tended to be much more accurate,while the (now more numerous) length-1 dependen-cies were slightly less accurate than before.We caution that length sensitivity?s most dramaticimprovements to accuracy were on the worse base-line models, which had more room to improve.
Thebetter baseline models (B and C) were already ableto indirectly capture some preference for short de-pendencies, by learning that some parts of speechwere unlikely to have multiple left or multiple rightdependents.
Enhancing B and C therefore con-tributed less, and indeed may have had some harmfuleffect by over-penalizing some structures that werealready appropriately penalized.15 It remains tobe seen, therefore, whether distance features wouldhelp state-of-the art parsers that are already muchbetter than model C. Such parsers may already in-corporate features that indirectly impose a goodmodel of distance, though perhaps not as cheaply.4 Hard Dependency-Length ConstraintsWe have seen how an explicit model of distance canimprove the speed and accuracy of a simple proba-bilistic dependency parser.
Another way to capital-ize on the fact that most dependencies are local isto impose a hard constraint that simply forbids longdependencies.The dependency trees that satisfy this constraintyield a regular string language.16 The constraint pre-vents arbitrarily deep center-embedding, as well asarbitrarily many direct dependents on a given head,15Owing to our deficient model.
A log-linear or discrimina-tive model would be trained to correct for overlapping penaltiesand would avoid this risk.
Non-deficient generative models arealso possible to design, along lines similar to footnote 16.16One proof is to construct a strongly equivalent CFG withoutcenter-embedding (Nederhof, 2000).
Each nonterminal has theform ?w, q, i, j?, where w ?
?, q is a state of Lw or Rw, andi, j ?
{0, 1, .
.
.
k?1,?
k}.
We leave the details as an exercise.34English (Penn Treebank) Chinese (Chinese Treebank) German (TIGER Corpus)recall (%) runtime model recall (%) runtime model recall (%) runtime modelmodel train test test size train test test size train test test sizeA (1 state) 62.0 62.2 93.6 1,878 50.7 49.3 146.7 782 70.9 72.0 53.4 1,598+ p(?
| d) 70.1 70.6 97.0 2,032 59.0 58.0 161.9 1,037 72.3 73.0 53.2 1,763+ p(?
| h) 70.5 71.0 94.7 3,091 60.5 59.1 148.3 1,759 73.1 74.0 48.3 2,575+ p(?
| d, h, c) 72.8 73.1 70.4 16,305 62.2 60.6 106.7 7,828 75.0 75.1 31.6 12,325B (2 states, tied arcs) 69.7 70.4 93.5 2,106 56.7 56.2 151.4 928 73.7 75.1 52.9 1,845+ p(?
| d) 72.6 73.2 95.3 2,260 60.2 59.5 156.9 1,183 72.9 73.9 52.6 2,010+ p(?
| h) 73.1 73.7 92.1 3,319 61.6 60.7 144.2 1,905 74.1 75.3 47.6 2,822+ p(?
| d, h, c) 75.3 75.6 67.7 16,533 62.9 61.6 104.0 7,974 75.2 75.5 31.5 12,572C (2 states) 72.7 73.1 90.3 3,233 61.8 61.0 148.3 1,314 75.6 76.9 48.5 2,638+ p(?
| d) 73.9 74.5 91.7 3,387 61.5 60.6 154.7 1,569 74.3 75.0 48.9 2,803+ p(?
| h) 74.3 75.0 88.6 4,446 63.1 61.9 141.9 2,291 75.2 76.3 44.3 3,615+ p(?
| d, h, c) 75.3 75.5 66.6 17,660 63.4 61.8 103.4 8,360 75.1 75.2 31.0 13,365Table 1: Dependency parsing of POS tag sequences with simple probabilistic split bilexical grammars.
The models differ onlyin how they weight the same candidate parse trees.
Length-sensitive models are larger but can improve dependency accuracyand speed.
(Recall is measured as the fraction of non-punctuation tags whose correct parent (if not the $ symbol) was correctlyrecovered by the parser; it equals precision, unless the parser left some sentences unparsed (or incompletely parsed, as in ?4), inwhich case precision is higher.
Runtime is measured abstractly as the average number of items (i.e., @ ,   ,  , H )built per word.
Model size is measured as the number of nonzero parameters.
)either of which would allow the non-regular lan-guage {anbcn : 0 < n < ?}.
It does allow ar-bitrarily deep right- or left-branching structures.4.1 Vine GrammarsThe tighter the bound on dependency length, thefewer parse trees we allow and the faster we can findthem using the algorithm of Fig.
2a.
If the boundis too tight to allow the correct parse of some sen-tence, we would still like to allow an accurate partialparse: a sequence of accurate parse fragments (Hin-dle, 1990; Abney, 1991; Appelt et al, 1993; Chen,1995; Grefenstette, 1996).
Furthermore, we wouldlike to use the fact that some fragment sequences arepresumably more likely than others.Our partial parses will look like the one in Fig.
1b.where 4 subtrees rather than 1 are dependent on $.This is easy to arrange in the SBG formalism.
Wemerely need to construct our SBG so that the au-tomaton R$ is now permitted to generate multiplechildren?the roots of parse fragments.This R$ is a probabilistic finite-state automatonthat describes legal or likely root sequences in ?
?.In our experiments in this section, we will train itto be a first-order (bigram) Markov model.
(Thuswe construct R$ in the usual way to have |?| + 1states, and train it on data like the other left and rightautomata.
During generation, its state remembersthe previously generated root, if any.
Recall that weare working with POS tag sequences, so the roots,like all other words, are tags in ?.
)The 4 subtrees in Fig.
1b appear as so manybunches of grapes hanging off a vine.
We refer tothe dotted dependencies upon $ as vine dependen-cies, and the remaining, bilexical dependencies astree dependencies.One might informally use the term ?vine gram-mar?
(VG) for any generative formalism, intendedfor partial parsing, in which a parse is a constrainedsequence of trees that cover the sentence.
In gen-eral, a VG might use a two-part generative process:first generate a finite-state sequence of roots, thenexpand the roots according to some more powerfulformalism.
Conveniently, however, SBGs and otherdependency grammars can integrate these two stepsinto a single formalism.4.2 Feasible ParsingNow, for both speed and accuracy, we will restrictthe trees that may hang from the vine.
We define afeasible parse under our SBG to be one in which alltree dependencies are short, i.e., their length neverexceeds some hard bound k. The vine dependenciesmay have unbounded length, of course, as in Fig.
1b.Sentences with feasible parses form a regular lan-guage.
This would also be true under other defini-tions of feasibility, e.g., we could have limited thedepth or width of each tree on the vine.
However,that would have ruled out deeply right-branchingtrees, which are very common in language, and35(a) $ would```````````````````````````````````````aaaaaaaaaaaaaaaaaaaaeeeee YYYYY[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[AccordingYYYYY, changesbbbbbbbbbbeeeeecut\\\\\\\\\\]]]]]]]]]]]]]]]to\\\\\\\\\\the rule filingseeeeebyYYYYY.estimateseeeeeinsider moreYYYYYsome than\\\\\\\\\\thirdeeeeea(b) $ AccordingWWWW, wouldgggg WWWWto[[[[[[[[changesccccccccggggcut[[[[[[[[\\\\\\\\\\\estimatesggggthe rule filingsggggbyWWWW.some insider moreWWWWthan[[[[[[[[thirdggggaFigure 1: (a) A dependency tree on words.
(Our experiments use only POS tags.)
(b) A partialparse for the same sentence retaining only tree dependencies of length ?
k = 3.
The roots of the4 resulting parse fragments are now connected only by their dotted-line ?vine dependencies?
on $.Transforming (a) into (b) involves grafting subtrees rooted at ?According?, ?,?, and ?.?
onto the vine.are also the traditional way to describe finite-statesublanguages within a context-free grammar.
Bycontrast, our limitation on dependency length en-sures regularity while still allowing (for any boundk ?
1) arbitrarily wide and deep trees, such asa?
b?
.
.
.?
root?
.
.
.?
y ?
z.Our goal is to find the best feasible parse (ifany).
Rather than transform the grammar as in foot-note 16, our strategy is to modify the parser so that itonly considers feasible parses.
The interesting prob-lem is to achieve linear-time parsing with a grammarconstant that is as small as for ordinary parsing.We also correspondingly modify the training dataso that we only train on feasible parses.
That is, webreak any long dependencies and thereby fragmenteach training parse (a single tree) into a vine of oneor more restricted trees.
When we break a child-to-parent dependency, we reattach the child to $.17This process, grafting, is illustrated in Fig.
1.
Al-though this new parse may score less than 100% re-call of the original dependencies, it is the best feasi-ble parse, so we would like to train the parser to findit.18 By training on the modified data, we learn more17Any dependency covering the child must also be broken topreserve projectivity.
This case arises later; see footnote 25.18Although the parser will still not be able to find it if it isnon-projective (possible in German).
Arguably we should havedefined ?feasible?
to also require projectivity, but we did not.appropriate statistics for both R$ and the other au-tomata.
If we trained on the original trees, we wouldinaptly learn that R$ always generates a single rootrather than a certain kind of sequence of roots.For evaluation, we score tree dependencies in ourfeasible parses against the tree dependencies in theunmodified gold standard parses, which are not nec-essarily feasible.
We also show oracle performance.4.3 Approach #1: FSA ParsingSince we are now dealing with a regular language,it is possible in principle to use a weighted finite-state automaton (FSA) to search for the best feasibleparse.
The idea is to find the highest-weighted paththat accepts the input string ?
= w1w2 .
.
.
wn.
Us-ing the Viterbi algorithm, this takes time O(n).The trouble is that this linear runtime hides a con-stant factor, which depends on the size of the rele-vant part of the FSA and may be enormous for anycorrect FSA.19Consider an example from Fig 1b.
Af-ter nondeterministically reading w1 .
.
.
w11 =According.
.
.
insider along the correct path, the FSAstate must record (at least) that insider has no parentyet and that R$ and Rcut are in particular states that19The full runtime is O(nE), where E is the number of FSAedges, or for a tighter estimate, the number of FSA edges thatcan be traversed by reading ?.36may still accept more children.
Else the FSA cannotknow whether to accept a continuation w12 .
.
.
wn.In general, after parsing a prefix w1 .
.
.
wj , theFSA state must somehow record information aboutall incompletely linked words in the past.
It mustrecord the sequence of past words wi (i ?
j) thatstill need a parent or child in the future; if wi stillneeds a child, it must also record the state of Rwi .Our restriction to dependency length ?
k is whatallows us to build a finite-state machine (as opposedto some kind of pushdown automaton with an un-bounded number of configurations).
We need onlybuild the finitely many states where the incompletelylinked words are limited to at most w0 = $ and the kmost recent words, wj?k+1 .
.
.
wj .
Other states can-not extend into a feasible parse, and can be pruned.However, this still allows the FSA to be inO(2ktk+1) different states after reading w1 .
.
.
wj .Then the runtime of the Viterbi algorithm, thoughlinear in n, is exponential in k.4.4 Approach #2: Ordinary Chart ParsingA much better idea for most purposes is to use achart parser.
This allows the usual dynamic pro-gramming techniques for reusing computation.
(TheFSA in the previous section failed to exploit manysuch opportunities: exponentially many states wouldhave proceeded redundantly by building the samewj+1wj+2wj+3 constituent.
)It is simple to restrict our algorithm of Fig.
2a tofind only feasible parses.
It is the ATTACH rules@ +   that add dependencies: simply use aside condition to block them from applying unless|h?
h?| ?
k (short tree dependency) or h = 0 (vinedependency).
This ensures that all H and will have width ?
k or have their left edge at 0.One might now incorrectly expect runtime linearin n: the number of possible ATTACH combinationsis reduced from O(n3) to O(nk2), because i and h?are now restricted to a narrow range given h.Unfortunately, the half-constituents @ and  may still be arbitrarily wide, thanks to arbi-trary right- and left-branching: a feasible vine parsemay be a sequence of wide trees   @ .
Thus thereare O(n2k) possible COMPLETE combinations, notto mention O(n2) ATTACH-RIGHT combinationsfor which h = 0.
So the runtime remains quadratic.4.5 Approach #3: Specialized Chart ParsingHow, then, do we get linear runtime and a rea-sonable grammar constant?
We give two ways toachieve runtime of O(nk2).First, we observe without details that we can eas-ily achieve this by starting instead with the algo-rithm of Eisner (2000),20 rather than Eisner andSatta (1999), and again refusing to add long tree de-pendencies.
That algorithm effectively concatenatesonly trapezoids, not triangles.
Each is spanned by asingle dependency and so has width ?
k. The vinedependencies do lead to wide trapezoids, but theseare constrained to start at 0, where $ is.
So the algo-rithm tries at most O(nk2) combinations of the formh i+ i j (like the ATTACH combinations above)and O(nk) combinations of the form 0 i + i j,where i?
h ?
k, j ?
i ?
k. The precise runtime isO(nk(k + t?
)tg3).We now propose a hybrid linear-time algorithmthat further improves runtime to O(nk(k + t?
)tg2),saving a factor of g in the grammar constant.21 Weobserve that since within-tree dependencies musthave length ?
k, they can all be captured withinEisner-Satta trapezoids of width ?
k. So our VGparse   @ ?
can be assembled by simply concate-nating a sequence (    ?
H ?
@ )?
of thesenarrow trapezoids interspersed with width-0 trian-gles.
As this is a regular sequence, we can assem-ble it in linear time from left to right (rather than inthe order of Eisner and Satta (1999)), multiplyingthe items?
probabilities together.
Whenever we startadding the right half H ?
@ of a tree along thevine, we have discovered that tree?s root, so we mul-tiply in the probability of a $?
root dependency.Formally, our hybrid parsing algorithm restrictsthe original rules of Fig.
2a to build only trapezoidsof width ?
k and triangles of width < k.22 Theadditional inference rules in Fig.
2b then assemblethe final VG parse as just described.20With a small change that when two items are combined, theright item (rather than the left) must be simple.21This savings comes from building the internal structure ofa trapezoid from both ends inward rather than from left to right.The corresponding unrestricted algorithms (Eisner, 2000; Eis-ner and Satta, 1999, respectively) have exactly the same run-times with k replaced by n.22For the experiments of ?4.7, where k varied by type, werestricted these rules as tightly as possible given h and h?.37(a)START-LEFT:w?Whq?init(Lw)  qhh:w1?h?nSTART-RIGHT:q?init(Rw)@@qh:wh  Fih:wSTART-VINE:q?init(R$)@@q0:$0FINISH-LEFT:  qih:wq?final(Lw)  Fih:wFINISH-RIGHT:@@qh:wiq?final(Rw)@@Fh:wiEND-VINE:@@F0:$nacceptATTACH-LEFT:?
?@@Fh?
:w?i?1  qih:w?
?qw???r?Lwrh?
:w?h:wATTACH-RIGHT:?
?@@qh:wi?1  Fih?
:w??
?qw???r?RwHHrh?
:w?h:wCOMPLETE-LEFT:  Fih?
:w?qh?
:w?h:w  qih:wCOMPLETE-RIGHT:HHqh?
:w?h:w@@Fh?
:w?i@@qh:wiFigure2:(a)AnalgorithmthatparsesW1...Wnincu-bictimeO(n2(n+t?)tg2).Adaptedwithimprove-mentsfrom(EisnerandSatta,1999,Fig.3).TheparenthesesintheATTACHrulesindicatethede-ductionofanintermediateitemthat?forgets?i.(b)IftheATTACHrulesarerestrictedtoapplyonlywhencase|h?h?
|?k,andtheCOMPLETErulesonlywhen|h?i|<k,thentheadditionalrulesin(b)willassembletheresultingfragmentsintoavineparse.Inthiscase,ATTACH-RIGHTshouldalsoberestrictedtoh>0,topreventduplicatederivations.TheruntimeisO(nk(k+t?)tg2),dominatedbytheATTACHrules;therulesin(b)requireonlyO(nktg2+ngtt?)time.Eachalgorithmisspecifiedasacollectionofdeductiveinferencerules.Onceonehasderivedallantecedentitemsabovethehorizontallineandanysideconditionstotherightoftheline,onemayde-rivetheconsequentitembelowtheline.Weightedagenda-baseddeductionishandledintheusualway(Nederhof,2003;Eisneretal.,2005).TheprobabilitiesgoverningtheautomatonLw,namelyp(startatq),p(qw???r|q),andp(stop|q),arerespectivelyassociatedwiththeaxiomaticitemsq?init(Lw),qw???r?Lw,andq?final(Lw).Anacousticscorep(observationath|w)couldbeassociatedwiththeitemw?Wh.
(b)TREE-START:@@q0:$i?1  Fii:w @@qi:w0:$TREE-LEFT: @@qi:w0:$Fi:wj:x @@qj:x0:$GRAFT-VINE: @@qi:w0:$qw ??r?R$XXyXXyri:w0:$TREE-RIGHT:XXyXXyqi:w0:$HHFj:xi:wXXyXXyqj:x0:$TREE-END:XXyXXyqi:w0:$@@Fi:wi@@q0:$iSEAL-LEFT:qh?
:w?h:wq?final(Lw)Fh?
:w?h:wSEAL-RIGHT:HHqh?
:w?h:wq?final(Rw)HHFh?
:w?h:w380.40.50.60.70.80.910.3  0.4  0.5  0.6  0.7  0.8  0.9recallprecision ECGk = 1Model C, no boundsingle bound (English)(Chinese)(German)Figure 3: Trading precision and recall: Imposing bounds canimprove precision at the expense of recall, for English and Chi-nese.
German performance suffers more.
Bounds shown arek = {1, 2, ..., 10, 15, 20}.
The dotted lines show constant F -measure of the unbounded model.4.6 Experiments with Hard ConstraintsOur experiments used the asymptotically fast hybridparsing algorithm above.
We used the same left andright automata as in model C, the best-performingmodel from ?3.2.
However, we now define R$ tobe a first-order (bigram) Markov model (?4.1).
Wetrained and tested on the same headed treebanks asbefore (?3.7), except that we modified the trainingtrees to make them feasible (?4.2).Results are shown in Figures 3 (precision/recalltradeoff) and 4 (accuracy/speed tradeoff), for k ?
{1, 2, ..., 10, 15, 20}.
Dots correspond to differentvalues of k. On English and Chinese, some values ofk actually achieve better F -measure accuracy thanthe unbounded parser, by eliminating errors.23We observed that changing R$ from a bigramto a unigram model significantly hurt performance,showing that it is in fact useful to empirically modellikely sequences of parse fragments.4.7 Finer-Grained Hard ConstraintsThe dependency length bound k need not be a sin-gle value.
Substantially better accuracy can be re-tained if each dependency type?each (h, c, d) =(head tag, child tag, direction) tuple?has its own23Because our prototype implementation of each kind ofparser (baseline, soft constraints, single-bound, and type-specific bounds) is known to suffer from different inefficiencies,runtimes in milliseconds are not comparable across parsers.
Togive a general idea, 60-word English sentences parsed in around300ms with no bounds, but at around 200ms with either a dis-tance model p(?|d, h, c) or a generous hard bound of k = 10.bound k(h, c, d).
We call these type-specific bounds:they create a many-dimensional space of possibleparsers.
We measured speed and accuracy along asensible path through this space, gradually tighten-ing the bounds using the following process:1.
Initialize each bound k(h, c, d) to the maximumdistance observed in training (or 1 for unseentriples).242.
Greedily choose a bound k(h, c, d) such that, ifits value is decremented and trees that violate thenew bound are accordingly broken, the fewest de-pendencies will be broken.253.
Decrement the bound k(h, c, d) and modify thetraining data to respect the bound by breaking de-pendencies that violate the bound and ?grafting?the loose portion onto the vine.
Retrain the parseron the training data.4.
If all bounds are not equal to 1, go to step 2.The performance of every 200th model along thetrajectory of this search is plotted in Fig.
4.26 Thegraph shows that type-specific bounds can speed upthe parser to a given level with less loss in accuracy.5 Related WorkAs discussed in footnote 3, Collins (1997) and Mc-Donald et al (2005) considered the POS tags inter-vening between a head and child.
These soft con-straints were very helpful, perhaps in part becausethey helped capture the short dependency preference(?2).
Collins used them as conditioning variablesand McDonald et al as log-linear features, whereasour ?3 predicted them directly in a deficient model.As for hard constraints (?4), our limitation on de-pendency length can be regarded as approximatinga context-free language by a subset that is a regular24In the case of the German TIGER corpus, which containsnon-projective dependencies, we first make the training treesinto projective vines by raising all non-projective child nodes tobecome heads on the vine.25Not counting dependencies that must be broken indirectlyin order to maintain projectivity.
(If word 4 depends on word7 which depends on word 2, and the 4 ?
7 dependency isbroken, making 4 a root, then we must also break the 2 ?
7dependency.
)26Note that k(h, c, right) = 7 bounds the width of @ +  =.
For a finer-grained approach, we could in-stead separately bound the widths of @ and   , say bykr(h, c, right) = 4 and kl(h, c, right) = 2.39language.
Our ?vines?
then let us concatenate sev-eral strings in this subset, which typically yields asuperset of the original context-free language.
Sub-set and superset approximations of (weighted) CFLsby (weighted) regular languages, usually by pre-venting center-embedding, have been widely ex-plored; Nederhof (2000) gives a thorough review.We limit all dependency lengths (not just center-embedding).27 Further, we derive weights from amodified treebank rather than by approximating thetrue weights.
And though regular grammar approxi-mations are useful for other purposes, we argue thatfor parsing it is more efficient to perform the approx-imation in the parser, not in the grammar.Brants (1999) described a parser that encoded thegrammar as a set of cascaded Markov models.
Thedecoder was applied iteratively, with each iterationtransforming the best (or n-best) output from theprevious one until only the root symbol remained.This is a greedy variant of CFG parsing where thegrammar is in Backus-Naur form.Bertsch and Nederhof (1999) gave a linear-timerecognition algorithm for the recognition of the reg-ular closure of deterministic context-free languages.Our result is related; instead of a closure of deter-ministic CFLs, we deal in a closure of CFLs that areassumed (by the parser) to obey some constraint ontrees (like a maximum dependency length).6 Future WorkThe simple POS-sequence models we used as an ex-perimental baseline are certainly not among the bestparsers available today.
They were chosen to illus-trate how modeling and exploiting distance in syntaxcan affect various performance measures.
Our ap-proach may be helpful for other kinds of parsers aswell.
First, we hope that our results will generalizeto more expressive grammar formalisms such as lex-icalized CFG, CCG, and TAG, and to more expres-sively weighted grammars, such as log-linear mod-els that can include head-child distance among otherrich features.
The parsing algorithms we presentedalso admit inside-outside variants, allowing iterativeestimation methods for log-linear models (see, e.g.,Miyao and Tsujii, 2002).27Of course, this still allows right-branching or left-branching to unbounded depth.0.50.550.60.650.70.750.80.850.90.9510  20  40  60  80  100runtime (items/word)EnglishFk = 123 15 20Model C, baselinesoft constraintsingle boundtype-specific bounds0.40.50.60.70.80.910  20  40  60  80  100  120  140  160runtime (items/word)ChineseFk = 12 315 20Model C, baselinesoft constraintsingle boundtype-specific bounds0.50.550.60.650.70.750.80.850.90.9510  10  20  30  40  50  60runtime (items/word)GermanFk = 12315 20Model C, baselinesoft constraintsingle boundtype-specific boundsFigure 4: Trading off speed and accuracy by varying the setof feasible parses: The baseline (no length bound) is shownas +.
Tighter bounds always improve speed, except for themost lax bounds, for which vine construction overhead incursa slowdown.
Type-specific bounds tend to maintain good F -measure at higher speeds than the single-bound approach.
Thevertical error bars show the ?oracle?
accuracy for each experi-ment (i.e., the F -measure if we had recovered the best feasibleparse, as constructed from the gold-standard parse by grafting:see ?4.2).
Runtime is measured as the number of items per word(i.e., @ ,   ,  , H , @ , XyXy) builtby the agenda parser.
The ?soft constraint?
point marked with?
represents the p(?
| d, h, c)-augmented model from ?3.40Second, fast approximate parsing may play a rolein more accurate parsing.
It might be used to rapidlycompute approximate outside-probability estimatesto prioritize best-first search (e.g., Caraballo andCharniak, 1998).
It might also be used to speed upthe early iterations of training a weighted parsingmodel, which for modern training methods tends torequire repeated parsing (either for the best parse, asby Taskar et al, 2004, or all parses, as by Miyao andTsujii, 2002).Third, it would be useful to investigate algorith-mic techniques and empirical benefits for limitingdependency length in more powerful grammar for-malisms.
Our runtime reduction from O(n3) ?O(nk2) for a length-k bound applies only to a?split?
bilexical grammar.28 Various kinds of syn-chronous grammars, in particular, are becoming im-portant in statistical machine translation.
Their highruntime complexity might be reduced by limitingmonolingual dependency length (for a related ideasee Schafer and Yarowsky, 2003).Finally, consider the possibility of limiting depen-dency length during grammar induction.
We reasonthat a learner might start with simple structures thatfocus on local relationships, and gradually relax thisrestriction to allow more complex models.7 ConclusionWe have described a novel reason for identifyingheadword-to-headword dependencies while parsing:to consider their length.
We have demonstratedthat simple bilexical parsers of English, Chinese,and German can exploit a ?short-dependency pref-erence.?
Notably, soft constraints on dependencylength can improve both speed and accuracy, andhard constraints allow improved precision and speedwith some loss in recall (on English and Chinese,remarkably little loss).
Further, for the hard con-straint ?length ?
k,?
we have given an O(nk2) par-tial parsing algorithm for split bilexical grammars;the grammar constant is no worse than for state-of-the-art O(n3) algorithms.
This algorithm strings to-gether the partial trees?
roots along a ?vine.
?28The obvious reduction for unsplit head automaton gram-mars, say, is only O(n4) ?
O(n3k), following (Eisner andSatta, 1999).
Alternatively, one can convert the unsplit HAG toa split one that preserves the set of feasible (length ?
k) parses,but then g becomes prohibitively large in the worst case.Our approach might be adapted to richer parsingformalisms, including synchronous ones, and shouldbe helpful as an approximation to full parsing whenfast, high-precision recovery of syntactic informa-tion is needed.ReferencesS.
P. Abney.
Parsing by chunks.
In Principle-Based Parsing:Computation and Psycholinguistics.
Kluwer, 1991.D.
E. Appelt, J. R. Hobbs, J.
Bear, D. Israel, and M. Tyson.FASTUS: A finite-state processor for information extractionfrom real-world text.
In Proc.
of IJCAI, 1993.E.
Bertsch and M.-J.
Nederhof.
Regular closure of deterministiclanguages.
SIAM J. on Computing, 29(1):81?102, 1999.D.
Bikel.
A distributional analysis of a lexicalized statisticalparsing model.
In Proc.
of EMNLP, 2004.T.
Brants.
Cascaded Markov models.
In Proc.
of EACL, 1999.S.
A. Caraballo and E. Charniak.
New figures of merit for best-first probabilistic chart parsing.
Computational Linguistics,24(2):275?98, 1998.S.
Chen.
Bayesian grammar induction for language modeling.In Proc.
of ACL, 1995.K.
W. Church.
On memory limitations in natural language pro-cessing.
Master?s thesis, MIT, 1980.M.
Collins.
Three generative, lexicalised models for statisticalparsing.
In Proc.
of ACL, 1997.J.
Eisner.
Bilexical grammars and their cubic-time parsing al-gorithms.
In Advances in Probabilistic and Other ParsingTechnologies.
Kluwer, 2000.J.
Eisner, E. Goldlust, and N. A. Smith.
Compiling Comp Ling:Practical weighted dynamic programming and the Dyna lan-guage.
In Proc.
of HLT-EMNLP, 2005.J.
Eisner and G. Satta.
Efficient parsing for bilexical cfgs andhead automaton grammars.
In Proc.
of ACL, 1999.L.
Frazier.
On Comprehending Sentences: Syntactic ParsingStrategies.
PhD thesis, University of Massachusetts, 1979.E.
Gibson.
Linguistic complexity: Locality of syntactic depen-dencies.
Cognition, 68:1?76, 1998.G.
Grefenstette.
Light parsing as finite-state filtering.
In Proc.of Workshop on Extended FS Models of Language, 1996.D.
Hindle.
Noun classification from predicate-argument struc-ture.
In Proc.
of ACL, 1990.J.
R. Hobbs and J.
Bear.
Two principles of parse preference.
InProc.
of COLING, 1990.D.
Klein and C. D. Manning.
Accurate unlexicalized parsing.In Proc.
of ACL, 2003.D.
Klein and C. D. Manning.
Corpus-based induction of syn-tactic structure: Models of dependency and constituency.
InProc.
of ACL, 2004.R.
McDonald, K. Crammer, and F. Pereira.
Online large-margintraining of dependency parsers.
In Proc.
of ACL, 2005.Y.
Miyao and J. Tsujii.
Maximum entropy estimation for featureforests.
In Proc.
of HLT, 2002.M.-J.
Nederhof.
Practical experiments with regular approxima-tion of context-free languages.
CL, 26(1):17?44, 2000.M.-J.
Nederhof.
Weighted deductive parsing and Knuth?s algo-rithm.
Computational Linguistics, 29(1):135?143, 2003.C.
Schafer and D. Yarowsky.
A two-level syntax-based ap-proach to Arabic-English statistical machine translation.
InProc.
of Workshop on MT for Semitic Languages, 2003.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.Max-margin parsing.
In Proc.
of EMNLP, 2004.41
