Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 1?6,Baltimore, Maryland USA, June 26 2014. c?2014 Association for Computational LinguisticsLearning a Lexicon for Broad-Coverage Semantic ParsingJames F. AllenDept.
of Computer Science, University of Rochesterjames@cs.rochester.eduAbstractWhile there has been significant recent work onlearning semantic parsers for specific task/domains, the results don?t transfer from onedomain to another domains.
We describe aproject to learn a broad-coverage semanticlexicon for domain independent semanticparsing.
The technique involves severalbootstrapping steps starting from a semanticparser based on a modest-sized hand-builtsemantic lexicon.
We demonstrate that theapproach shows promise in building a semanticlexicon on the scale of WordNet, with morecoverage and detail that currently available inwidely-used resources such as VerbNet.
We viewhaving such a lexicon as a necessary prerequisitefor any attempt at  attaining broad-coveragesemantic parsing in any domain.
The approachwe described applies to all word classes,  but inthis paper we focus here on verbs,  which are themost critical phenomena facing semanticparsing.1.
Introduction and MotivationRecently we have seen an explosion of workon learning semantic parsers (e.g., Matuszek, etal, 2012; Tellex et al, 2013; Branavan et  al, 2010,Chen et al, 2011).
While such work showspromise, the results are highly domain dependentand useful only for that domain.
One cannot, forinstance, reuse a lexical entry learned in onerobotic domain in another robotic domain, letalone in a database query domain.
Furthermore,the techniques being developed require domainsthat are simple enough so that  the semanticmodels can be produced, either by hand orinduced from the application.
Language ingeneral, however, involves  much more complexconcepts and connections, including discussionof involves abstract concepts, such as plans,theories, political views, and so on.
It is not  clearhow the techniques currently being developedcould be generalized to such language.The challenge we are addressing is learning abroad-coverage, domain-independent  semanticparser, i.e., a semantic parser that  can be used inany domain.
At present, there is a tradeoffbetween the depth of semantic representationproduced and the coverage of the techniques.One of the critical gaps in enabling more general,deeper semantic systems is the lack of any broad-coverage deep semantic lexicon.
Such a lexiconmust contain at least the following information:i. an enumeration of the set of distinct  senses forthe word (e.g., as  in WordNet, PropBank),linked into an ontology that supports reasoningii.
For each sense, we would have?Deep argument structure, i.e., semanticroles with selectional preferences?Constructions that  map syntax to the deepargument structure (a.k.a.
linking rules)?Lexical entailments that characterize thetemporal consequences of the eventdescribed by the verbThe closest example to such lexical entries canbe found in VerbNet  (Kipper et  al, 2008), a hand-built resource widely used for a range of generalapplications.
An example entry from VerbNet  isseen in Figure 1, which describes a class of verbscalled murder-42.1.
VerbNet  clusters verbs bythe constructions they take, not  by sense ormeaning, although many times, the set  ofconstructions a verb takes is a good feature forclustering by semantic meaning.
We see that theverbs in this class can take an AGENT,PATIENT  and INSTRUMENT role, and we seethe possible constructions that  map syntacticstructure to the deep argument  structure.
Forinstance, the first  entry indicates that  the simpletransitive construction has the AGENT as thesubject and the PATIENT as the object.
Inaddition, it  specifies lexical entailments in aninformal notation, roughly stating that murderverbs involve causing a event that  is a transitionf rom being a l ive to not be ing a l ive .Unfortunately, VerbNet  only covers a fewthousand verbs.
This paper reports on work toautomatically build entries with much greatercoverage and more detail than found in VerbNet,for all the senses in WordNet.
This includes thedeep argument  structure and constructions  foreach sense, as well as axioms describing lexicalentailments, expressed in a formally defined1temporal logic (Allen, 1984; Allen & Teng,2013).2.
Overview of the ApproachTo attain broader coverage of the verbs (and theirsenses) for English, we look to WordNet.Though WordNet  has excellent  coverage, it doesnot contain information about argumentstructure, and has varying quality of ontologicalinformation (good for nouns, some informationfor verbs, and little for adjective and adverbs).But  it  does contain rich sources of information inunstructured form, i.e., each sense has a glossthat defines the word?s meaning, and oftenprovides examples of the word?s usage.
Thetechnique we describe here uses an existinghand-built, but  relatively limited coverage,semant ic l ex icon to boots t rap in to acomprehensive lexicon by processing thesedefinitions and examples.
In other words, we arelearning the lexicon by reading the dictionary.Specifically, we use the TRIPS parser (Allen etal, 2008) as the starting point, which has asemantic lexicon of verbs about the same size asVerbNet.
To build the comprehensive semanticlexicon, we use two bootstrapping steps.
Thefirst  uses ontology mapping techniques togenerate underspecified lexical entries forunknown words.
This technique enables theparser to construct  interpretations of sentencesinvolving words not encoded in the core lexicon.We then use information extracted from thedefinitions and examples to build much moredetailed and deeper lexical entries.
We have runthis process over the entire set  of WordNetentries and provide preliminary results belowevaluating the results along a number of keydimensions.2.1.
The TRIPS Parsing SystemThe TRIPS system is a packed-forest  chart parserwhich builds constituents bottom-up using abest-first search strategy (Allen et  al, 2008).
Thecore grammar is a hand-built, lexicalizedcontext-free grammar, augmented with featurestructures and feature unification, and driven bya semantic lexicon and ontology.
The coresemantic lexicon1was constructed by hand andcontains more than 7000 lemmas, For each word,it  specifies its possible senses (i.e., its ontologytype), and for each sense, its semantic roles andsemantic preferences, and constructions formapping from syntax to semantics.The system uses variety of statistical andpreprocessors to improve accuracy.
Theseinclude the Stanford tools for POS tagging,named entity recognition and syntactic parsing.The parser produces and detailed logical formcapturing the semantics of the sentence in agraphical notation equivalent  to an unscoped,modal logic (Manshadi et al, 2012).2.2.
Level One Bootstrapping: GeneratingLexical Entries Foe Unknown WordsThe key idea in generating abstract  lexicalentries for unknown verbs builds from the sameintuition the motivations underlying VerbNet -1 you can browse the lexicon and ontology at www.cs.rochester.edu/research/trips/lexicon/browse-ont-lex.htmlFigure 2: WordNet Entry for murderFigure 1: VerbNet Entry for murder2that the set of constructions a verb supportsreflects its semantic meaning.
While in VerbNet,the constructions are used to cluster verbs intosemantic classes, we work in the oppositedirection and use the semantic classes to predictthe likely syntactic constructions.To generate the lexical entries for an unknownverb we use the synset hierarchy in WordNet,plus a hand-built  mapping between certain keysynsets and the classes in the TRIPS ontology.The whole process operates as follows, given anunknown word w:i.
Look up word w in WordNet and obtain itspossible synsetsii.
For each synset, find a mapping to the TRIPSontologyi.
If there is a direct mapping, we are doneii.
If not, traverse up the WordNet Hypernymhierarchy and recursively check for amappingiii.
For each TRIPS ontology type found, gatherall the words in the TRIPS lexicon that areassociated with the typeiv.
Take the union of all the constructions definedon the words associated with the TRIPS typev.
Generate a lexical entry for each possiblecombination of constructions and typesThe result of this process is an over-generated setof underspecified lexical entries.
Figure 3illustrates this with a very simple example ofderiving the lexical entries for the verb?collaborate?
: it is first looked up in WordNet,then we traverse the hypernym hierarchy untilwe find a mapping to the TRIPS ontology, fromwork%2:41:02 to ONT::WORKING.
From therewe find all the lexical entries associated withONT::WORKING, and then take the union of thelexical information to produce new entries.
Thevalid entries will be the ones that  contribute tosuccessful parses of the sentences involving theunknown words.
In addition to what  is shown,other lexical information is also derived in thesame way, including weak select ionalpreferences for the argument roles.While the result of this stage of bootstrappingproduces lexical entries that  identify the TRIPStype, the semantic roles and constructions, manyof the lexical entries are not  valid and not verydeep.
In particular, even considering just thecorrect entries, the semantic models are limitedto the relatively small TRIPS ontology, and donot  capture lexical entailments.
Also, theselectional preferences for the semantic roles arevery weak.
These problems are all addressed inthe second bootstrapping step.2.3.
Level Two Bootstrapping: ReadingDefinitions and ExamplesThe key idea in this stage of processing is to usethe lexicon bootstrapped in level one to parse allthe definitions and examples for each WordNetsynset.
We then use this information to build aricher ontology, better identify the semantic rolesand their selectional preferences, and identify theappropr ia te cons t ruc t ions and lex ica lentailments.
The hope is that the result  is thisprocess will be lexical entries suitable forsemantic parsing, and tightly coupled with anontology and commonsense knowledge basesuitable for reasoning.Consider an example processing a sense of theverb keep up, defined as prevent from  going tobed at night.
We use sense tagged glossesobtained from the Princeton Gloss Corpus toprovide guidance to the parser.
The TRIPS parserproduces the logical form for the definition asshown in Figure 4.
Each node in the graphspecifies the most specific TRIPS ontology classthat covers the word plus the WordNet sense.
Forexample, the verb prevent is captured by a nodeindicating its WordNet sense prevent%2:41:00and the TRIPS class ont::HINDERING.
Note theverb go to bed, tagged as a multi-word verb inthe Gloss corpus, has no information in theTRIPS ontology other than being an event ofsome sort.
The semantic roles are indicated bythe labelled arcs between the nodes.
The nodeslabelled IMPRO are the elided arguments in thedefinition (i.e., the missing subject and object).work%2:41:02collaborate%2:41:01WordNet HypernymHierarchyONT::WORKINGONT::INTENTIONALLY-ACTTRIPS OntologyOntologyMappingunknown verb:"collaborate""labor": subj/AGENT PP(over)/AFFECTED"labor": subj/AGENT"work": subj/AGENT PP(on)/AFFECTED"work": subj/AGENT?
?TRIPS Lexiconlookup inWordNetTRIPS Lexiconlookup by type"collaborate":ONT::WORKING subj/AGENT"collaborate":ONT::WORKING  subj/AGENT PP(on)/AFFECTED"collaborate":ONT::WORKING subj/AGENT PP(over)/AFFECTEDAutomatically Generated Lexical EntriesLexical EntryGenerationFigure 3: Example of Ontology-based Automatic Lexicon Generation3From this definition alone we can extractseveral key pieces of semantic information aboutthe verb keep up, namely2i.
Ontological: keep_up  is a subclass of prevent%2:41:00 and ont::HINDERING eventsii.
Argument Structure: keep_up has twosemantic roles: AGENT and AFFECTED3iii.
Lexical Entailment: When a keep_up eventoccurs, the AGENT  prevents the AFFECTEDfrom going to bedDefinitions can be notoriously terse and complexto parse, and thus in many cases the parser canonly extract key fragments of the definition.
Weuse the TRIPS robust parsing mechanism toextract  the most meaningful parse fragmentswhen a complete parse cannot be found.To identify the selectional preferences for theroles and the valid constructions, we parse theexamples given in WordNet, plus syntheticallyproduced sentences derived from the WordNetsentence frame information, plus additionalexamples extracted from SEMCOR, in which thewords are tagged with their WordNet senses.From parsing the examples, we obtain a set ofexamples of the semantic roles used, plus theconstructions used to produce them.
We apply aheuristic process to combine the proposed rolesets from the definitions and the glosses to arriveat  a final role set for the verb.
We then gather thesemantic types of all the arguments from theexamples, and abstract  them using the derivedontology to produce the most  compact  set oftypes that  cover all the examples seen.
Here wepresent a few more details of the approach.Determining Semantic RolesOne of the interesting observations that  wediscovered in this project is that  the missing partsof definitions are highly predictive of the roles ofthe verb being defined.
For instance, looking atFigure 4, we see that the verb prevent, used inthe definition, has three roles: AGENT,AFFECTED, and EFFECT.
Two of these arefilled by implicit  pro (IMPRO) forms (i.e., theywere elided in the definition), and one is fullyinstantiated.
Almost  all the time it is the IMPROroles that  are promoted be the roles of keep up.We have found this technique to be highlyreliable when we have fully accurate parsing.Because of the inevitable errors in parsing suchterse language, however, we find the combiningthe information from the definition withadditional evidence produced by parsingconcrete examples gives better accuracy.Computing Lexical EntailmentsTo compute lexical entailments, we use thedefinitions, often expanding them by recursivelyexpanding the senses in the definition with theirdefinitions.
At  some stage, the definitions ofcertain verb verbs become quite abstract  and/orcircular.
To deal with this, we hand codedaxiomatic definitions for a small set of aspectualverbs such as start, end, and continue, and causalverbs such as cause, prevent, stop, in a temporallogic.
When a definition is expanded to the pointof including one of these verbs, we can create a?temporal map?
of entailments from the event.Thus, from the definition of keep up, we caninfer that  the event of going to bed does notoccur over the time over which the keep up eventoccurs.
A description of our first  attempt  togenerate entailments can be found in Allen et al(2011), and the temporal logic we havedeveloped to support  compositional derivation ofentailments is described in Allen & Teng (2013).Computing Selectional PreferencesWe compute selectional preferences by gatheringthe ontological types of elements that fill eachargument position, using examples drawn fromWordNet and SEMCOR.
We then generalize thisset by trying to find non-trivial subsuming typesthat cover the examples.
For example, for theverb kill, we might find examples of theAFFECTED role of being a person, a pig, and aplant.
We try to find a subsuming type thatcovers all of these classes that  is more specificthan the extremely abstract classes such as2 The ontology is represented in OWL-DL (www.w3.org/TR/owl-guide), and the entailments in a logic based onAllen?s (1984) Logic of Action and Time.
There is no space to present these details in this paper.3 The AFFECTED role in TRIPS includes most cases using the PATIENT role in VerbNetFigure 4: The parse of prevent from going to bed at night(F  ont::HINDERING prevent%2:41:00)(IMPRO agent)(F  ont::SITUATION-ROOT go_to_bed%2:29:00)(IMPRO affected):effect:affected:agent(BARE ont::TIME-INTERVAL night%1:28:00):time-clock-rel:agent4REFERENTIAL-SEM (the class of all things thatcan be referred to).
We compute this over acombined ontology using the TRIPS ontologyplus the ontology that we derive from parsing allthe WordNet definitions.
Using both allows us toavoid the pitfalls of lack of coverage in onesource or the other.
As an example, in this casewe would find the class LIVING-THING coversthe three examples above, so this would be thederived selectional preference for this role of kill.Selectional preferences derived by the methodhave been shown to be useful in automaticallyidentifying metaphors (Wilks et al, 2013).3.
EvaluationsThis is a work in progress, so we do not  yet havea comprehensive evaluation.
We do havepreliminary evaluations of specific aspects of thelexical entries we are producing, however.
Forthe most part, our evaluations have beenperformed using set  of human judges (somefellow colleagues and some recruited usingAmazon Turk).
Because of the complexity ofsuch judging tasks, we generally use at  leastseven judges, and sometimes up to eleven.
Wethen eliminate cases where there is notsubstantial human agreement, typically at  least75%.
We have found that this eliminates less that20% of the potential test cases.
The remainingcases provide a gold standard.The Event OntologyTo evaluate the derived event  ontology, werandomly created a evaluation set consisting of1) subclass pairs derived by our system, 2)hypernym pairs extracted from WordNet, and 3)random pairs of classes.
We used eleven humanjudges to judge whether one class is a subclass ofthe other, and evaluated the system on the caseswhere at  least eight judges in agreement  (83% ofcases).
The system had 83% precision and 42%recall in this test, indicating good accuracy.
Thelow recall score, however, indicates ourtechniques do not extract many of the hypernymrelations present in WordNet.
It suggests that  weshould also incorporate the hypernym relationsas a ontology source when constructing the finaldeep semantic lexicon.
More details can be foundin Allen et al (2013).Causal Relations Between EventsWe used a similar technique to evaluate ourability to extract causal relationships betweenevents classes (e.g., kill causes die).
We tested ona similar blend of derived casual relations,explicitly annotated causal relations in WordNetand random other pairs.
The system achieved100% precision and 55% recall on this test.Interestingly, there was almost no overlapbetween the system-derived causal relations andthose in WordNet, indicating that combining thetwo sources will produce a much richer resource.More details can be found in Allen et al (2013).Selectional Preferences for RolesWe performed a preliminary evaluation on thecorrectness of the selectional preferences bycomparing our derived classes with therestrictions in VerbNet.
This is not an idealevaluation as the VerbNet restrictions are quiteabstract.
For instance, VerbNet  has one class forabstract  objects, whereas the our derivedontology has a much richer classification,including plans, words, properties, beliefs, and soon.
Thus, we expected that often our derivedpreferences would be more specific than theVerbNet  restrictions.
On a test  set  of 50randomly selected verbs, 51% of the restrictionswere exactly correct, 26% were too specific,19% too general, and 2% were inconsistent.These results suggest promise for the approach.We are designing a more refined experimentusing human judges to attempt to drill deeper.4.
ConclusionThe preliminary evaluations are promising andsuggest  it  could be feasible to automaticallybuild a deep semantic lexicon on the scale ofWordNet, tightly integrated with an ontologyalso derived from the same sources.
We arecontinuing this work in a number of directions,and designing better evaluation metrics.
Inaddition, as many researchers find the WordNetinventory of word senses too fine grained, we aredeveloping techniques that used the derivedinformation to automatically cluster sets ofsenses in more abstract senses that cover them.When the project is completed, we will bereleasing the full semantic lexicon for use byother researchers.As a final note, while the TRIPS system is anessential part of the bootstrapping process, it istrivial to remove all traces of TRIPS in the finalresource, removing the hand-built  lexical entriesand the TRIPS ontology, leaving a resourceentirely grounded in WordNet.5.
AcknowledgementsThis paper summarizes work performed over aseveral years involving a large group of peopleincluding William de Beaumont, HyuckchulJung, Lucian Galescu, Janson Orfan, Mary Swift,and Choh Man Teng.
This work was supported inpart by NSF grant 0958193, ONR grantN000141110417, and the Nuance Foundation.56.
ReferencesAllen, J. F. (1984).
"Towards a General Theory of Actionand Time."
Artifical Intelligence 23: 123-154.Allen, J., M. Swift and W. de Beaumont (2008).
DeepSemantic Analysis for Text Processing.
Symposium onSemantics in Systems for Text Processing (STEP 2008)Allen, J., W. de Beaumont, et al.
(2011).
AcquiringCommonsense Knowledge for a Cognitive Agent.
AAAISymposium on Advances in Cognitive Systems,Washington, DC.Allen, J., W. de Beaumont, et al.
(2013).
AutomaticallyDeriving Event Ontologies for a CommonSense KnowledgeBase.
Proc.
10th  International Conference on ComputationalSemantics (IWCS 2013), Potsdam, Germany.Allen, J. and C. M. Teng (2013).
Becoming Different:  ALanguage-driven formalism for commonsense knowledge.CommonSense 2013: 11th Intl Symposium on LogicalFormalization on Commonsense Reasoning, Cypress.Branavan, S., L. Zettlemoyer, and R. Barzilay.
(2010)Reading between the lines: Learning to map high-levelinstructions to commands.
In ACL, pages 1268?1277.Chen D.L., and R.
Mooney.
(2011)  Learning to interpretnatural language navigation instructions from observations.Proc.
AAAI (AAAI-2011), pages 859?865Dzikovska, M., J. F. Allen, et al.
(2008).
"Linking Semanticand Knowledge Representation  in a Multi-Domain DialogueSystem."
Logic and Computation 18(3): 405-430.Fellbaum, S.  (1998, ed.)
WordNet: An Electronic LexicalDatabase.
Cambridge, MA: MIT PrKipper, K, A Korhonen, N Ryant, M Palmer.
(2008) "ALarge-scale Classification of English Verbs."
LanguageResources and Evaluation Journal,42(1).Manshadi, M. and J. Allen (2012a).
A UniversalRepresentation for Shallow and Deep Semantics.
LRECWorkshop on Semantic Annotation.
Instanbul, Turkey.Matuszek, C., E Herbst, L Zettlemoyer, D. Fox (2012),Learning to Parse Natural Language Commands to a RobotControl System, Proc.
of the 13th International Symposiumon Experimental Robotics (ISER)Tellex, S., P  Thaker, J Joseph, N Roy.
(2013).
LearningPerceptually Grounded Word Meanings From UnalignedParallel Data.
Machine Learning JournalWilks, Y., L. Galescu, et al.
(2013).
Automatic MetaphorDetection using Large-Scale Lexical  Resources andConventional Metaphor Extraction.
Proceedings of the FirstWorkshop on Metaphor in NLP, Atlanta, GA.6
