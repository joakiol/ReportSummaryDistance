Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 129?136, Vancouver, October 2005. c?2005 Association for Computational LinguisticsInvestigating Unsupervised Learningfor Text Categorization BootstrappingAlfio Gliozzo and Carlo StrapparavaITC-irstIstituto per la Ricerca Scientifica e TecnologicaI-38050 Trento, Italy{gliozzo,strappa}@itc.itIdo DaganComputer Science DepartmentBar Ilan UniversityRamat Gan, Israeldagan@cs.biu.ac.ilAbstractWe propose a generalized bootstrappingalgorithm in which categories are de-scribed by relevant seed features.
Ourmethod introduces two unsupervised stepsthat improve the initial categorization stepof the bootstrapping scheme: (i) using La-tent Semantic space to obtain a general-ized similarity measure between instancesand features, and (ii) the Gaussian Mixturealgorithm, to obtain uniform classificationprobabilities for unlabeled examples.
Thealgorithm was evaluated on two Text Cate-gorization tasks and obtained state-of-the-art performance using only the categorynames as initial seeds.1 IntroductionSupervised classification is the task of assigning cat-egory labels, taken from a predefined set of cate-gories (classes), to instances in a data set.
Within theclassical supervised learning paradigm, the task isapproached by providing a learning algorithm witha training data set of manually labeled examples.
Inpractice it is not always easy to apply this schemato NLP tasks.
For example supervised systems forText Categorization (TC) require a large amount ofhand labeled texts, while in many applicative casesit is quite difficult to collect the required amounts ofhand labeled data.
Unlabeled text collections, on theother hand, are in general easily available.An alternative approach is to provide the neces-sary supervision by means of sets of ?seeds?
of in-tuitively relevant features.
Adopting terminologyfrom computability theory, we refer to the stan-dard example-based supervision mode as Exten-sional Learning (EL), as classes are being specifiedby means of examples of their elements (their ex-tension).
Feature-based supervision is referred to asIntensional Learning (IL), as features may often beperceived as describing the intension of a category,such as providing the name or prominent key termsfor a category in text categorization.The IL approach reflects on classical rule-basedclassification methods, where the user is expectedto specify exact classification rules that operate inthe feature space.
Within the machine learningparadigm, IL has been incorporated as a techniquefor bootstrapping an extensional learning algorithm,as in (Yarowsky, 1995; Collins and Singer, 1999;Liu et al, 2004).
This way the user does notneed to specify exact classification rules (and fea-ture weights), but rather perform a somewhat sim-pler task of specifying few typical seed features forthe category.
Given the list of seed features, thebootstrapping scheme consists of (i) preliminary un-supervised categorization of the unlabeled data setbased on the seed features, and (ii) training an (ex-tensional) supervised classifier using the automaticclassification labels of step (i) as the training data(the second step is possibly reiterated, such as byan Expectation-Maximization schema).
The corepart of IL bootstrapping is step (i), i.e.
the initialunsupervised classification of the unlabeled dataset.This step was often approached by relatively sim-ple methods, which are doomed to obtain mediocrequality.
Even so, it is hoped that the second step ofsupervised training would be robust enough to thenoise in the initial training set.129The goal of this paper is to investigate additionalprincipled unsupervised mechanisms within the ini-tial classification step, applied to the text catego-rization.
In particular, (a) utilizing a Latent Se-mantic Space to obtain better similarity assessmentsbetween seeds and examples, and (b) applying aGaussian Mixture (GM) algorithm, which provides aprincipled unsupervised estimation of classificationprobability.
As shown in our experiments, incor-porating these steps consistently improved the ac-curacy of the initial categorization step, which inturn yielded a better final classifier thanks to themore accurate training set.
Most importantly, we ob-tained comparable or better performance than previ-ous IL methods using only the category names as theseed features; other IL methods required collectinga larger number of seed terms, which turns out to bea somewhat tricky task.Interesting results were revealed when compar-ing our IL method to a state-of-the-art extensionalclassifier, trained on manually labeled documents.The EL classifier required 70 (Reuters dataset) or160 (Newsgroup dataset) documents per category toachieve the same performance that IL obtained usingonly the category names.
These results suggest thatIL may provide an appealing cost-effective alterna-tive when sub-optimal accuracy suffices, or when itis too costly or impractical to obtain sufficient la-beled training.
Optimal combination of extensionaland intensional supervision is raised as a challeng-ing topic for future research.2 Bootstrapping for Text CategorizationThe TC task is to assign category labels to docu-ments.
In the IL setting, a category Ci is describedby providing a set of relevant features, termed anintensional description (ID), idci ?
V , where Vis the vocabulary.
In addition a training corpusT = {t1, t2, .
.
.
tn} of unlabeled texts is provided.Evaluation is performed on a separate test corpusof labeled documents, to which standard evaluationmetrics can be applied.The approach of categorizing texts based on listsof keywords has been attempted rather rarely in theliterature (McCallum and Nigam, 1999; Ko and Seo,2000; Liu et al, 2004; Ko and Seo, 2004).
Severalnames have been proposed for it ?
such as TC bybootstrapping with keywords, unsupervised TC, TCby labelling words ?
where the proposed methodsfall (mostly) within the IL settings described here1.It is possible to recognize a common structure ofthese works, based on a typical bootstrap schema(Yarowsky, 1995; Collins and Singer, 1999):Step 1: Initial unsupervised categorization.
Thisstep was approached by applying some similar-ity criterion between the initial category seedand each unlabeled document.
Similarity maybe determined as a binary criterion, consider-ing each seed keyword as a classification rule(McCallum and Nigam, 1999), or by applyingan IR style vector similarity measure.
The re-sult of this step is an initial categorization of (asubset of) the unlabeled documents.
In (Ko andSeo, 2004) term similarity techniques were ex-ploited to expand the set of seed keywords, inorder to improve the quality of the initial cate-gorization.Step 2: Train a supervised classifier on the ini-tially categorized set.
The output of Step1 is exploited to train an (extensional) su-pervised classifier.
Different learning algo-rithms have been tested, including SVM, NaiveBayes, Nearest Neighbors, and Rocchio.
Someworks (McCallum and Nigam, 1999; Liu etal., 2004) performed an additional ExpectationMaximization algorithm over the training data,but reported rather small incremental improve-ments that do not seem to justify the additionaleffort.
(McCallum and Nigam, 1999) reported catego-rization results close to human agreement on thesame task.
(Liu et al, 2004) and (Ko and Seo,2004) contrasted their word-based TC algorithmwith the performance of an extensional supervisedalgorithm, achieving comparable results, while ingeneral somewhat lower.
It should be noted that ithas been more difficult to define a common evalua-tion framework for comparing IL algorithms for TC,due to the subjective selection of seed IDs and to thelack of common IL test sets (see Section 4).1The major exception is the work in (Ko and Seo, 2004),which largely follows the IL scheme but then makes use of la-beled data to perform a chi-square based feature selection be-fore starting the bootstrap process.
This clearly falls outside theIL setting, making their results incomparable to other IL meth-ods.1303 Incorporating Unsupervised Learninginto Bootstrap SchemaIn this section we show how the core Step 1 of the ILscheme ?
the initial categorization ?
can be boostedby two unsupervised techniques.
These techniquesfit the IL setting and address major constraints of it.The first is exploiting a generalized similarity metricbetween category seeds (IDs) and instances, whichis defined in a Latent Semantic space.
Applyingsuch unsupervised similarity enables to enhance theamount of information that is exploited from eachseed feature, aiming to reduce the number of neededseeds.
The second technique applies the unsuper-vised Gaussian Mixture algorithm, which maps sim-ilarity scores to a principled classification probabil-ity value.
This step enables to obtain a uniform scaleof classification scores across all categories, whichis typically obtained only through calibration overlabeled examples in extensional learning.3.1 Similarity in Latent Semantic SpaceAs explained above, Step 1 of the IL scheme as-sesses a degree of ?match?
between the seed termsand a classified document.
It is possible first tofollow the intuitively appealing and principled ap-proach of (Liu et al, 2004), in which IDs (categoryseeds) and instances are represented by vectors in ausual IR-style Vector Space Model (VSM), and sim-ilarity is measured by the cosine function:simvsm(idci , tj) = cos (~idci , ~tj) (1)where ~idci ?
R|V | and ~tj ?
R|V | are the vectorialrepresentations in the space R|V | respectively of thecategory ID idci and the instance tj , and V is the setof all the features (the vocabulary).However, representing seeds and instances in astandard feature space is severely affected in the ILsetting by feature sparseness.
In general IDs arecomposed by short lists of features, possibly justa single feature.
Due to data sparseness, most in-stances do not contain any feature in common withany category?s ID, which makes the seeds irrelevantfor most instances (documents in the text categoriza-tion case).
Furthermore, applying direct matchingonly for a few seed terms is often too crude, as it ig-nores the identity of the other terms in the document.The above problems may be reduced by consid-ering some form of similarity in the feature space,as it enables to compare additional document termswith the original seeds.
As mentioned in Section2, (Ko and Seo, 2004) expanded explicitly the orig-inal category IDs with more terms, using a con-crete query expansion scheme.
We preferred using ageneralized similarity measure based on represent-ing features and instances a Latent Semantic (LSI)space (Deerwester et al, 1990).
The dimensions ofthe Latent Semantic space are the most explicativeprincipal components of the feature-by-instance ma-trix that describes the unlabeled data set.
In LSIboth coherent features (i.e.
features that often co-occur in the same instances) and coherent instances(i.e.
instances that share coherent features) are rep-resented by similar vectors in the reduced dimen-sionality space.
As a result, a document would beconsidered similar to a category ID if the seed termsand the document terms tend to co-occur overall inthe given corpus.The Latent Semantic Vectors for IDs and docu-ments were calculated by an empirically effectivevariation (self-reference omitted for anonymity) ofthe pseudo-document methodology to fold-in docu-ments, originally suggested in (Berry, 1992).
Thesimilarity function simlsi is computed by the cosinemetric, following formula 1, where ~idci and ~tj arereplaced by their Latent Semantic vectors.
As willbe shown in section 4.2, using such non sparse rep-resentation allows to drastically reduce the numberof seeds while improving significantly the recall ofthe initial categorization step.3.2 The Gaussian Mixture Algorithm and theinitial classification stepOnce having a similarity function between categoryIDs and instances, a simple strategy is to base theclassification decision (of Step 1) directly on theobtained similarity values (as in (Liu et al, 2004),for example).
Typically, IL works adopt in Step 1a single-label classification approach, and classifyeach instance (document) to only one category.
Thechosen category is the one whose ID is most simi-lar to the classified instance amongst all categories,which does not require any threshold tuning over la-beled examples.
The subsequent training in Step 2yields a standard EL classifier, which can then beused to assign multiple categories to a document.Using directly the output of the similarity func-tion for classification is problematic, because the ob-tained scales of similarity values vary substantiallyacross different categories.
The variability in sim-131ilarity value ranges is caused by variations in thenumber of seed terms per category and the levels oftheir generality and ambiguity.
As a consequence,choosing the class with the highest absolute similar-ity value to the instance often leads to selecting acategory whose similarity values tend to be gener-ally higher, while another category could have beenmore similar to the classified instance if normalizedsimilarity values were used.As a solution we propose using an algorithmbased on unsupervised estimation of Gaussian Mix-tures (GM), which differentiates relevant and non-relevant category information using statistics fromunlabeled instances.
We recall that mixture mod-els have been widely used in pattern recognition andstatistics to approximate probability distributions.
Inparticular, a well-known nonparametric method fordensity estimation is the so-called Kernel Method(Silverman, 1986), which approximates an unknowdensity with a mixture of kernel functions, such asgaussians functions.
Under mild regularity condi-tions of the unknown density function, it can beshown that mixtures of gaussians converge, in a sta-tistical sense, to any distribution.More formally, let ti ?
T be an instance describedby a vector of features ~ti ?
R|V | and let idci ?
Vbe the ID of category Ci; let sim(idci , tj) ?
R bea similarity function among instances and IDs, withthe only expectation that it monotonically increasesaccording to the ?closeness?
of idci and tj (see Sec-tion 3.1).For each category Ci, GM induces a mappingfrom the similarity scores between its ID and anyinstance tj , sim(idci , tj), into the probability of Cigiven the text tj , P (Ci|tj).
To achieve this goal GMperforms the following operations: (i) it computesthe set Si = {sim(idci , tj)|tj ?
T} of the sim-ilarity scores between the ID idci of the categoryCi and all the instances tj in the unlabeled train-ing set T ; (ii) it induces from the empirical distri-bution of values in Si a Gaussian Mixture distribu-tion which is composed of two ?hypothetic?
distri-butions Ci and Ci, which are assumed to describe re-spectively the distributions of similarity scores forpositive and negative examples; and (iii) it estimatesthe conditional probability P (Ci|sim(idci , tj)) byapplying the Bayes theorem on the distributions Ciand Ci.
These steps are explained in more detail be-low.The core idea of the algorithm is in step (ii).
Sincewe do not have labeled training examples we canonly obtain the set Si which includes the similar-ity scores for all examples together, both positiveand negative.
We assume, however, that similar-ity scores that correspond to positive examples aredrawn from one distribution, P (sim(idci , tj)|Ci),while the similarity scores that correspond to neg-ative examples are drawn from another distribution,P (sim(idci , tj)|Ci).
The observed distribution ofsimilarity values in Si is thus assumed to be a mix-ture of the above two distributions, which are recov-ered by the GM estimation.Figure 1 illustrates the mapping induced by GMfrom the empirical mixture distribution: dotted linesdescribe the Probability Density Functions (PDFs)estimated by GM for Ci, Ci, and their mixture fromthe empirical distribution (Si) (in step (ii)).
Thecontinuous line is the mapping induced in step (iii)of the algorithm from similarity scores between in-stances and IDs (x axis) to the probability of the in-stance to belong to the category (y axis).0123456789-0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7GM-scoreCi gaussian (relevant)Ci gaussian (not relevant)MixtureSimilarity ScoreProbability/ PDFFigure 1: Mapping induced by GM for the categoryrec.motorcycles in the 20newsgroups data set.The probabilistic mapping estimated in step (iii)for a category Ci given an instance tj is computedby applying Bayes rule:P (Ci|tj) = P (Ci|sim(idci , tj)) = (2)=P (sim(idci ,tj)|Ci)P (Ci)P (sim(idci ,tj)|Ci)P (Ci)+P (sim(Ci,tj)|Ci)P (Ci)where P (sim(idci , tj)|Ci) is the value ofthe PDF of Ci at the point sim(idci , tj),P (sim(idci , tj)|Ci) is the value of the PDF of Ci atthe same point, P (Ci) is the area of the distribution132Ci and P (Ci) is the area of the distribution Ci.
Themean and variance parameters of the two distribu-tions Ci and Ci, used to evaluate equation 2, are esti-mated by the rather simple application of the Expec-tation Maximization (EM) algorithm for GaussianMixtures, as summarized in (Gliozzo et al, 2004).Finally, following the single-labeled categoriza-tion setting of Step 1 in the IL scheme, the mostlikely category is assigned to each instance, that is,argmaxCiP (Ci|tj).3.3 Summary of the Bootstrapping Algorithmstep 1.a: Latent Semantic Space.
Instances andIntensional Descriptions of categories (the seeds) arerepresented by vectors in Latent Semantic space.
Asan option, the algorithm can work with the classi-cal Vector Space Model using the original featurespace.
Similarity scores between IDs and instancesare computed by the Cosine measure.step 1.b: GM.
The mapping functions P (Ci|tj)for each category, conditioned on instances tj , areinduced by the GM algorithm.
To that end, an Ex-pectation Maximization algorithm estimates the pa-rameters of the two component distributions of theobserved mixture, which correspond to the distribu-tions of similarity values for positive and negativeexamples.
As an option, the GM mapping can beavoided.step 1.c: Categorization.
Each instanceis classified to the most probable category -argmaxCiP (Ci|tj).step 2: Bootstrapping an extensional classifier.An EL classifier (SVM) is trained on the set of la-beled instances resulting from step 1.c.4 Evaluation4.1 Intensional Text Categorization DatasetsEven though some typical data sets have been usedin the TC literature (Sebastiani, 2002), the datasetsused for IL learning were not standard.
Often thereis not sufficient clarity regarding details such as theexact version of the corpus used and the training/testsplitting.
Furthermore, the choice of categories wasoften not standard: (Ko and Seo, 2004) omitted 4categories from the 20-Newsgroup dataset, while(Liu et al, 2004) evaluated their method on 4 sepa-rate subsets of the 20-Newsgroups, each containingonly 4-5 categories.
Such issues make it rather diffi-cult to compare thoroughly different techniques, yetwe have conducted several comparisons in Subsec-tion 4.5 below.
In the remainder of this Subsectionwe clearly state the corpora used in our experimentsand the pre-processing steps performed on them.20newsgroups.
The 20 Newsgroups data set isa collection of newsgroup documents, partitioned(nearly) evenly across 20 different newsgroups.
Assuggested in the dataset Web site2, we used the?bydate?
version: the corpus (18941 documents)is sorted by date and divided in advance into atraining (60%) set and a chronologically follow-ing test set (40%) (so there is no randomness intrain/test set selection), it does not include cross-posts (duplicates), and (more importantly) does notinclude non-textual newsgroup-identifying headerswhich often help classification (Xref, Newsgroups,Path, Followup-To, Date).We will first report results using initial seedsfor the category ID?s, which were selected usingonly the words in the category names, with sometrivial transformations (i.e.
cryptography#nfor the category sci.crypt, x-windows#nfor the category comp.windows.x).
Wealso tried to avoid ?overlapping?
seeds, i.e.for the categories rec.sport.baseballand rec.sport.hockey the seeds are only{baseball#n} and {hockey#n} respec-tively and not {sport#n, baseball#n} and{sport#n, hockey#n}3.Reuters-10.
We used the top 10 categories(Reuters-10) in the Reuters-21578 collectionApte` split4.
The complete Reuters collectionincludes 12,902 documents for 90 categories,with a fixed splitting between training and testdata (70/30%).
Both the Apte` and Apte`-10splits are often used in TC tasks, as surveyedin (Sebastiani, 2002).
To obtain the Reuters-102The collection is available atwww.ai.mit.edu/people/jrennie/20Newsgroups.3One could propose as a guideline for seed selectionthose seeds that maximize their distances in the LSI vec-tor space model.
On this perspective the LSI vectorsbuilt from {sport#n, baseball#n} and {sport#n,hockey#n} are closer than the vectors that represent{baseball#n} and {hockey#n}.
It may be noticed thatthis is a reason for the slight initial performance decrease in thelearning curve in Figure 2 below.4available at http://kdd.ics.uci.edu/databases/-reuters21578/reuters21578.html).133Apte` split we selected the 10 most frequent cate-gories: Earn, Acquisition, Money-fx,Grain, Crude, Trade, Interest,Ship, Wheat and Corn.
The final data setincludes 9296 documents.
The initial seeds are onlythe words appearing in the category names.Pre-processing.
In both data sets we tagged thetexts for part-of-speech and represented the docu-ments by the frequency of each pos-tagged lemma,considering only nouns, verbs, adjectives, and ad-verbs.
We induced the Latent Semantic Space fromthe training part5 and consider the first 400 dimen-sions.4.2 The impact of LSI similarity and GM on ILperformanceIn this section we evaluate the incremental impactof LSI similarity and the GM algorithm on IL per-formance.
When avoiding both techniques the algo-rithm uses the simple cosine-based method over theoriginal feature space, which can be considered as abaseline (similar to the method of (Liu et al, 2004)).We report first results using only the names of thecategories as initial seeds.Table 1 displays the F1 measure for the 20news-groups and Reuters data sets, with and without LSIand with and without GM.
The performance figuresshow the incremental benefit of both LSI and GM.
Inparticular, when starting with just initial seeds anddo not exploit the LSI similarity mechanism, thenthe performance is heavily penalized.As mentioned above, the bootstrapping step of thealgorithm (Step 2) exploits the initially classified in-stances to train a supervised text categorization clas-sifier based on Support Vector Machines.
It is worth-while noting that the increment of performance afterbootstrapping is generally higher when GM and LSIare incorporated, thanks to the higher quality of theinitial categorization which was used for training.4.3 Learning curves for the number of seedsThis experiment evaluates accuracy change as afunction of the number of initial seeds.
The ex-5From a machine learning point of view, we could run theLSA on the full corpus (i.e.
training and test), the LSA being acompletely unsupervised technique (i.e.
it does not take into ac-count the data annotation).
However, from an applicative pointof view it is much more sensible to have the LSA built on thetraining part only.
If we run the LSA on the full corpus, theperformance figures increase in about 4 points.Reuters 20 NewsgroupsLSI GM F1 F1no no 0.38 0.25+ bootstrap 0.42 0.28no yes 0.41 0.30+ bootstrap 0.46 0.34yes no 0.46 0.50+ bootstrap 0.47 0.53yes yes 0.58 0.60+ bootstrap 0.74 0.65Table 1: Impact of LSI vector space and GM0.20.250.30.350.40.450.50.550.60.651 5 10 15 20F1number of seeds (1 means only the category names)LSI VSMClassical VSMFigure 2: Learning curves on initial seeds for 20newsgroups, LSI and Classical VSM (no LSI)periment was performed for the 20 newsgroups cor-pus using both the LSI and the Classical vectorspace model.
Additional seeds, beyond the cate-gory names, were identified by two lexicographers.For each category, the lexicographers were providedwith a list of 100 seeds produced by the LSI similar-ity function applied to the category name (one list of100 candidate terms for each category).
From theselists the lexicographers selected the words that werejudged as significantly related to the respective cat-egory, picking a mean of 40 seeds per category.As seen in Figure 2, the learning curve usingLSI vector space model dramatically outperformsthe one using classical vector space.
As can beexpected, when using the original vector space (nogeneralization) the curve improves quickly with afew more terms.
More surprisingly, with LSI sim-ilarity the best performance is obtained using theminimal initial seeds of the category names, whileadding more seeds degrades performance.
Thismight suggest that category names tend to be highly134indicative for the intensional meaning of the cate-gory, and therefore adding more terms introducesadditional noise.
Further research is needed to findout whether other methods for selecting additionalseed terms might yield incremental improvements.The current results, though, emphasize the bene-fit of utilizing LSI and GM.
These techniques ob-tain state-of-the-art performance (see comparisonsin Section 4.5) using only the category names asseeds, allowing us to skip the quite tricky phase ofcollecting manually a larger number of seeds.4.4 Extensional vs. Intensional LearningA major point of comparison between IL and EL isthe amount of supervision effort required to obtain acertain level of performance.
To this end we traineda supervised classifier based on Support Vector Ma-chines, and draw its learning curves as a functionof percentage of the training set size (Figure 3).
Inthe case of 20newsgroups, to achieve the 65% F1performance of IL the supervised settings requiresabout 3200 documents (about 160 texts per cate-gory), while our IL method requires only the cate-gory name.
Reuters-10 is an easier corpus, there-fore EL achieves rather rapidly a high performance.But even here using just the category name is equalon average to labeling 70 documents per-category(700 in total).
These results suggest that IL may pro-vide an appealing cost-effective alternative in prac-tical settings when sub-optimal accuracy suffices, orwhen it is too costly or impractical to obtain suffi-cient amounts of labeled training sets.It should also be stressed that when using thecomplete labeled training corpus state-of-the-art ELoutperforms our best IL performance.
This resultdeviates from the flavor of previous IL literature,which reported almost comparable performance rel-ative to EL.
As mentioned earlier, the method of (Koand Seo, 2004) (as we understand it) utilizes labeledexamples for feature selection, and therefore cannotbe compared with our strict IL setting.
As for theresults in (Liu et al, 2004), we conjecture that theircomparable performance for IL and EL may not besufficiently general, for several reasons: the easierclassification task (4 subsets of 20-Newsgroups of4-5 categories each); the use of the usually weakerNaive-Bayes as the EL device; the use of cluster-ing as an aid for selecting the seed terms from the20-Newsgroup subsets, which might not scale upwell when applied to a large number of categoriesof varying size.0.10.20.30.40.50.60.70.80.910 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1F1Percentage of training20 NewsgroupsReuters3200 docs700 docsFigure 3: Extensional learning curves on as percent-age of the training set.4.5 Comparisons with other algorithmsAs mentioned earlier it is not easy to conduct a thor-ough comparison with other algorithms in the litera-ture.
Most IL data sets used for training and evalua-tion are either not available (McCallum and Nigam,1999) or are composed by somewhat arbitrary sub-sets of a standard data set.
Another crucial aspectis the particular choice of the seed terms selected tocompose an ID, which affects significantly the over-all performance of the algorithm.As a baseline system, we implemented a rulebased approach in the spirit of (McCallum andNigam, 1999).
It is based on two steps.
First, allthe documents in the unlabeled training corpus con-taining at least one word in common with one andonly one category ID are assigned to the respectiveclass.
Second, a supervised classifier based on SVMis trained on the labeled examples.
Finally, the su-pervised classifier is used to perform the final cate-gorization step on the test corpus.
Table 2 reportsthe F1 measure of our replication of this method, us-ing the category name as seed, which is substantiallylower than the performance of the method we pre-sented in this paper.Reuters 20 Newsgroups0.34 0.30+ bootstrap 0.42 0.47Table 2: Rule-based baseline performance135We also tried to replicate two of the non-standarddata sets used in (Liu et al, 2004)6.
Table 3 displaysthe performance of our approach in comparison tothe results reported in (Liu et al, 2004).
Follow-ing the evaluation metric adopted in that paper wereport here accuracy instead of F1.
For each dataset (Liu et al, 2004) reported several results vary-ing the number of seed words (from 5 to 30), as wellas varying some heuristic thresholds, so in the ta-ble we report their best results.
Notably, our methodobtained comparable accuracy by using just the cat-egory name as ID for each class instead of multipleseed terms.
This result suggests that our method en-ables to avoid the somewhat fuzzy process of col-lecting manually a substantial number of additionalseed words.Our IDs per cat.
Liu et al IDs per cat.REC 0.94 1 0.95 5TALK 0.80 1 0.80 20Table 3: Accuracy on 4 ?REC?
and 4 ?TALK?
news-groups categories5 ConclusionsWe presented a general bootstrapping algorithm forIntensional Learning.
The algorithm can be appliedto any categorization problem in which categoriesare described by initial sets of discriminative fea-tures and an unlabeled training data set is provided.Our algorithm utilizes a generalized similarity mea-sure based on Latent Semantic Spaces and a Gaus-sian Mixture algorithm as a principled method toscale similarity scores into probabilities.
Both tech-niques address inherent limitations of the IL setting,and leverage unsupervised information from an un-labeled corpus.We applied and evaluated our algorithm on sometext categorization tasks and showed the contribu-tion of the two techniques.
In particular, we obtain,for the first time, competitive performance usingonly the category names as initial seeds.
This mini-mal information per category, when exploited by theIL algorithm, is shown to be equivalent to labelingabout 70-160 training documents per-category forstate of the art extensional learning.
Future work is6We used sequential splitting (70/30) rather than randomsplitting and did not apply any feature selection.
This settingmight be somewhat more difficult than the original one.needed to investigate optimal procedures for collect-ing seed features and to find out whether additionalseeds might still contribute to better performance.Furthermore, it may be very interesting to exploreoptimal combinations of intensional and extensionalsupervision, provided by the user in the forms ofseed features and labeled examples.AcknowledgmentsThis work was developed under the collaborationITC-irst/University of Haifa.ReferencesM.
Berry.
1992.
Large-scale sparse singular value com-putations.
International Journal of SupercomputerApplications, 6(1):13?49.M.
Collins and Y.
Singer.
1999.
Unsupervised modelsfor named entity classification.
In Proc.
of EMNLP99,College Park, MD, USA.S.
Deerwester, S. Dumais, G. Furnas, T. Landauer, andR.
Harshman.
1990.
Indexing by latent semantic anal-ysis.
Journal of the American Society of InformationScience.A.
Gliozzo, C. Strapparava, and I. Dagan.
2004.
Unsu-pervised and supervised exploitation of semantic do-mains in lexical disambiguation.
Computer Speechand Language, 18:275?299.Y.
Ko and J. Seo.
2000.
Automatic text categorization byunsupervised learning.
In Proc.
of COLING?2000.Y.
Ko and J. Seo.
2004.
Learning with unlabeled datafor text categorization using bootstrapping abd fea-ture projection techniques.
In Proc.
of the ACL-04,Barcelona, Spain, July.B.
Liu, X. Li, W. S. Lee, and P. S. Yu.
2004.
Text clas-sification by labeling words.
In Proc.
of AAAI-04, SanJose, July.A.
McCallum and K. Nigam.
1999.
Text classificationby bootstrapping with keywords, em and shrinkage.
InACL99 - Workshop for Unsupervised Learning in Nat-ural Language Processing.F.
Sebastiani.
2002.
Machine learning in automated textcategorization.
ACM Computing Surveys, 34(1):1?47.B.
W. Silverman.
1986.
Density Estimation for Statisticsand Data Analysis.
Chapman and Hall.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proc.
ofACL-95, pages 189?196, Cambridge, MA.136
