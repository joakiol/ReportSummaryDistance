MEASURES AND MODELSFOR PHRASE RECOGNIT IONSteven AbneyBell Communications Research445 South StreetMorristown, NJ 07960ABSTRACTI present an entropy measure for evaluating parserperformance.
The measure is fine-grained, and permits us toevaluate performance atthe level of individual phrases.
Theparsing problem is characterized as statisticallyapproximating the Penn Treebank annotations.
I consider aseries of models to "calibrate" the measure by determiningwhat scores can be achieved using the most obvious kinds ofinformation.
I also relate the entropy measure to measures ofrecall/precision and grammar coverage.1.
INTRODUCTIONEntropy measures of parser performance have focussed onthe parser's contribution to word prediction.
This isappropriate for evaluating a parser as a language model forspeech recognition, but it is less appropriate for evaluatinghow well a parser does at parsing.
I would like to presentan entropy measure for phrase recognition, along withclosely-related measures of precision and recall.
I considera seres of models, in order to establish a baseline forperformance, and to give some sense of what parts of theproblem are hardest, and what kinds of informationcontribute most to a solution.Specifically, I consider the problem of recognizing chunks(Abney 1991)--non-recursive pi ces of major-categoryphrases, omitting post-head complements and modifiers.Chunks correspond to prosodic phrases (Abney 1992) andcan be assembled into complete parse trees by adding head-head ependencies.2.
THE PARSING PROBLEMParsing is usually characterized as the problem ofrecovering parse trees for sentences, given a grammar thatdefines the mapping of sentences toparse-trees.
However,I wish to characterize the problem without assuming agrammar, for two reasons.
First, we cannot assume agrammar for unrestricted English.
For unrestrictedEnglish, failure of coverage will be a significant problemfor any grammar, and we would like a measure ofperformance that treats failure of coverage and failureswithin the grammar uniformly.Second, I am particularly interested in parsers like Fidditch(Hindle 1983) and Cass (Abney 1990) that avoid search byrelying on highly reliable patterns for recognizingindividual phrases.
Such parsers may need to considercompeting patterns when scoring a given pattern--forexample, Cass relies heavily on a preference for the patternthat matches the longest prefix of the input.
Such cross-pattern dependencies cannot be expressed within, forexample, a stochastic ontext-free grammar (SCFG).Hence I am interested in a more general evaluationframework, one that subsumes both Fidditch/Cass-styleparsers and SCFG parsing.Instead of assuming a grammar, I take the Penn Treebank(Marcus & Santorini 1991) to provide a representativesample of English, viewed as a function from sentences toparse trees.
A parser's task is to statistically approximatethat function.
We can measure the (in)accuracy of theparser by the amount of additional information we mustprovide in order to specify the correct (Treebank) parse for asentence, given the output of the parser.
This is theentropy of the corpus given the parser, and approaches zeroas the parser approaches perfect emulation of Treebankannotation.We can characterize the parser's task at two levels ofgranularity.
At the level of the sentence, the task is toassign a probability distribution over the set of possibleparse-trees for the sentence.
At the phrase level, theproblem is to give, for each candidate phrase c, theprobability that c belongs to the correct parse.
I will focuson the latter characterization, for several reasons:(1) as mentioned, I am interested in developing reliablepatterns for recognizing individual phrases, in order toreduce the necessity for search and to increase parsingspeed, (2) evaluating at the phrase level allows us to assignblame for error at a finer grain, (3) there are applicationssuch as data extraction where we may have good models forcertain phrase types, but not for entire sentences, and (4) aphrase model can easily be embedded in a sentence model,so evaluating at the finer grain does not exclude valuationat the coarser grain.2333.
MEASURESGiven a sentence, the chunk candidates are all tuples c =(x,id), for x a syntactic ategory, and i andj the start andend positions of the chunk.
For each candidate, there aretwo possible vents in the Treebank: the candidate is indeeda phrase in the Treebank parse (T), or it is not a true phrase(~T).
For each candidate, the parsing model providesP(TIc), the probability of the candidate being a true phrase,and P(~TIc) = 1 - P(TIc).Given the probabilities provided by the parsing model, theinformation that must be provided to specify that T occurs(that the candidate is a true phrase) is - lg P(TIc); and tospecify that ~T occurs, -lg P(~TIc).
The entropy of thecorpus given the model is the average -lg P(Eclc), for Ecbeing T or ~T according as candidate c does or does notappear in the Treebank parse.
That is,H = -( l /N) Z lg  P(Eclc ) for N the number ofc candidatesA perfect model would have P(Eclc) = 1 for all c, hence H =0.
At the other extreme, a 'random-guess' model wouldhave P(Eclc) = 1/2 for all c, hence H = 1 bit/candidate (b/c).This provides an upper bound on H, in the sense that anymodel that has H > 1 b/c can be changed into a model withH < 1 by systematically interchanging P(TIc) and P(~TIc).Hence, for all models, 0 _< H _< 1 b/c.There are some related measures of interest.
We cantranslate ntropy into an equivalent number of equally-likely parses (perplexity) by the relation:PP = 2allfor H in bits/candidate and a the number of candidates persentence.
In the test corpus I used, a = 8880, so PPranges from 1 to 28880 = 102670.We can also measure xpected precision and recall, byconsidering P(TIc) as a probabilistic 'Yes' to candidate c.For example, if the model says P(TIc) = 3/4, that counts as3/4 of a 'Yes'.
Then the expected number of Yes's is thesum of P(TIc) over all candidates, and the expected numberof correct Yes's is the sum of P(TIc) over candidates thatare true chunks.
From that and the number of true chunks,which can simply be counted, we can compute precisionand recall:E(#Y) =  P(TIc)CE(#TY)= X P(TIc)t rue  cEP = E(#TY) /E(#Y)ER = E(#TY) / #T4.
MODELSTo establish abaseline for performance, and to determinehow much can be accomplished with 'obvious', easily-acquired information, I consider a series of models.
Model0 is a zero-parameter, random-guess model; it establishes alower bound on performance.
Model 1 estimates oneparameter, the proportion of true chunks among candidates.Model XK takes the category and length of candidates intoaccount.
Model G induces a simple grammar from thetraining corpus.
Model C considers a small amount ofcontext.
And model S is a sentence-level model based onG.4.1.
Models 0 and 1Models 0 and 1 take P(TIc) to be constant.
Model 0 (therandom-guess model) takes P(T) = 1/2, and provides alower bound on performance.
Model 1 (the one-parametermodel) estimates P(T) as the proportion of true chunksamong candidates in a training corpus.
The training corpusI used consists of 1706 sentences, containing 19,025 truechunks (11.2 per sentence), and 14,442,484 candidates(8470 per sentence).
The test corpus consisted of 1549sentences, 17,676 true chunks (11.4 per sentence), and13,753,628 candidates (8880 per sentence).
Theperformance ofthe random-guess and one-parameter modelsis as follows:b/c prs/sent EP ER0 1 102670 .129% (50%)1 .014 2 1038 .129% (.132%)For these two models (in fact, for any model with P(TIc)constan0, precision is at a minimum, and equals theproportion of true chunks in the test corpus.
Recall i suninformative, being equal to P(TIc).4.2.
Model XKModel XK is motivated by the observation that very longchunks are highly unlikely.
It takes P(TIc) = P(TIx,k), forx the category of c and k its length.
It estimates P(TIx, k)as the proportion of true chunks among candidates ofcategory x and length k in the training corpus.
Asexpected, this model does better than the previous ones:b/c prs/sent EP ERXK .007 95 1021 5.5% 5.6%4.3.
Models G and CFor model G, I induced a simple grammar f om the trainingcorpus.
I used Ken Church's tagger (Church 1988) to234assign part-of-speech probabilities to words.
The grammarcontains a rule x ---> T for every Treebank chunk \[x "t\] inthe training corpus.
(x is the syntactic ategory of thechunk, and y is the part-of-speech sequence assigned to thewords of the chunk.)
Ix V\] is counted as being observedP(y) times, for P('t) the probability of assigning the part-of-speech sequence y to the words of the chunk.
I used asecond corpus to estimate P(TIx,?)
for each rule in thegrammar, by counting the proportion of true phrasesamong candidates of form Ix Y\].
For candidates thatmatched no rule, I estimated the probabilities P(TIx, k) as inthe XK model.Model C is a variant of model G, in which a small amountof context, namely, the following part of speech, is alsotaken into account.The results on the test corpus are as follows:b/c prs/sent EP ERG .003 81 10 lo 47.3% 48.2%C .003 36 109 54.5% 58.7%The improvement in expected precision and recall isdramatic.4.4 Assigning BlameWe can make some observations about the sources ofFor example, we can break out entropy by entropy.category:%H -E(%H)NP 39.0 +18.7PP 21.1 +7.2VP 19.0 +4.4Null 7.5 -8.4AdjP 3.9 +1.4other (23) 9.5 -23.4The first column represents he percentage of total entropyaccounted for by candidates of the given category.
In thesecond column, I have subtracted the amount we wouldhave expected if entropy were divided among candidateswithout regard to category.
The results clearly confirm ourintuitions that, for example, noun phrases are moredifficult to recognize than verb clusters, and that the Nullcategory, consisting mostly of punctuation andconnectives, i  easy to recognize.We can also break out entropy among candidates covered bythe grammar, and those not covered by the grammar.
Theusual measure of grammar coverage is simply theproportion of true chunks covered, but we can moreaccurately determine how much of a problem coverage isby measuring how much we stand to gain by improvingcoverage, versus how much we stand to gain by improvingour model of covered candidates.
On our test corpus, only4% of the candidates are uncovered by the grammar, but19% of the information cost (entropy) is due to uncoveredcandidates.4.5.
Model SNone of the models discussed so far take into account theconstraint that the set of true chunks must partition thesentence.
Now, if a perfect sentence model exists--if analgorithm exists that assigns to each sentence its Treebankparse---then a perfect phrase model also exists.
And to theextent that a model uses highly reliably local patterns (as Iwould like), little information is lost by not evaluating atthe sentence l vel.
But for other phrase-level models, suchas those considered here, embedding them in a sentence-level model can significantly improve performance.Model S is designed to gauge how much information islost in model G by not evaluating parses as a whole.
Ituses model O's assignments of probabilities P(TIc) forindividual candidates as the basis for assigning probabilitiesP(s) to entire parses, that is, to chunk-sequences s thatcover the entire sentence.To choose a sequence of chunks stochastically, we beginwith s = the null sequence at position i = 0.
We choosefrom among the candidates at position i, taking theprobability P(c) of choosing candidate c to be proportionalto P(TIc).
The chosen chunk c is appended to s, and thecurrent position i is advanced to the end position of c. Weiterate to the end of the sentence.
In brief:P(c)=P(TIc)/ Z P(TIc')c'at ifor i the startposition of cP(s) = IF\[ P(c)c insThe entropy of a sentence given the model is -lg P(s), for sthe true sequence of chunks.
We can also compute actual(not expected) precision and recall by counting the truechunks in the most-likely parse according to the model.The results on the test corpus are:M b/s present P~dsion RecallS 14.1 104 74.1% 75.6%(By way of comparison, the bits/sentence numbers for theother models are as follows:)0 1 XK G C S8880 126 70.6 33.8 29.8 14.1For model S, the number of  parses per sentence is stillrather high, but the precision and recall are surprisingly235good, given the rudimentary information that the modeltakes into account.
I think there is cause for optimism thatthe chunk recognition problem can be solved in the nearterm, using models that take better account of context andword-level information.4.
CONCLUSIONTo summarize, I have approached the problem of parsingEnglish as a problem of statistically approximating thePenn Treebank.
For the purposes of parsing, English is afunction from sentences to parse-trees, and the Treebankprovides a (sufficiently representative) sample from theextension of that function.
A parsing model approximatesTreebank annotation.
Our basic measure of the goodnessof the approximation is the amount of additionalinformation we must provide in order to specify theTreebank parse, given the probabilities assigned by theparser.
I have presented a series of models to "calibrate"the measure, showing what kind of performance isachievable using obvious kinds of information.An impetus for this work is the success of parsers likeFidditch and Cass, which are able to greatly reduce search,and increase parsing speed, by using highly reliablepatterns for recognizing phrases.
The limitation of suchwork is the impracticality of constructing reliable patternsby hand, past a certain point.
One hindrance to automaticacquisition of reliable patterns has been the lack of aframework for evaluating such parsers at a fine grain, andexploring which kinds of information contribute most toparsing accuracy.In the current work, I have presented a framework for fine-grained evaluation of parsing models.
It does not assumestochastic context-free grammars, and it quantifies parsers'performance at parsing, rather than at a more indirectlyrelated task like word prediction.REFERENCES1.2.3.4.Steven Abney (1990).
Rapid Incremental Parsing withRepair.
Proceedings of the 6th New OED Conference.University of Waterloo, Waterloo, Ontario.Steven Abney (1991).
Parsing by Chunks.
InBerwick, Abney & Tenny, eds.
Principle-BasedParsing, pp.257-278.
Kluwer Academic Publishers,Dordrecht.Steven Abney (1992).
Prosodic Structure, PerformanceStructure and Phrase Structure.
Proc.
5th DARPAWorkshop on Speech and Natural Language (Harriman,NY).
Morgan Kaufmann.E.
Black, S. Abney, D. Flickenger, R. Grishman, P.Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans,M.
Liberman, M. Marcus, S. Roukos, B. Santorini, andT.
Strzalkowski (1991).
A procedure for quantitativelycomparing the syntactic coverage of English5.6.7,8.9.10.11.12.grammars.
DARPA Speech and Natural LanguageWorkshop, pp.306-311.
Morgan Kaufmann.Peter L. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, Jennifer C. Lai, and Robert L Mercer(1992).
An Estimate of an Upper Bound for theEntropy of English.
Computational Linguistics 18.1,pp.31-40.Kenneth Church (1988).
A Stochastic Part of SpeechTagger and Noun Phrase Parser for English.Proceedings of the 2rid Conference on Applied NaturalLanguage Processing.
Austin, Texas.T.
Fujisaki, F. Jelinek, J. Cooke, E. Black, T. Nishino(1989).
A Prnbabilistic Parsing Method for SentenceDisambiguation.
International Workshop on ParsingTechnologies 1989, pp.85-94.Donald Hindle (1983).
User manual for Fidditeh.Naval Reserach Laboratory Technical Memorandum#7590-142.F.
Jelinek.
Self-Organized Language Modeling forSpeech Recognition.
IBM report.F.
Jelinek, J.D.
Lafferty, and R.L.
Mercer.
BasicMethods of Probabilistic Context-Free Grammars.IBM report.Mitchell Marcus and Beatrice Santorini (1991).Building very large natural language corpora: the PennTreebank.
Ms., University of Pennsylvania.Fernando Pereira and Yves Schabes (1992).
Inside-Outside Reestimation from Partially BracketedCorpora.
ACL 92, pp.128-135.236
