Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 24?34,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsReducing the Size of the Representation for the uDOP-EstimateChristoph TeichmannAbteilung Automatische SprachverarbeitungInstitute of ComputerscienceUniversity of LeipzigMax Planck Institute for Human Cognitive and Brain SciencesLeipzigteichmann@informatik.uni-leipzig.deAbstractThe unsupervised Data Oriented Parsing(uDOP) approach has been repeatedly re-ported to achieve state of the art perfor-mance in experiments on parsing of dif-ferent corpora.
At the same time the ap-proach is demanding both in computationtime and memory.
This paper describes anapproach which decreases these demands.First the problem is translated into thegeneration of probabilistic bottom up treeautomata (pBTA).
Then it is explainedhow solving two standard problems forthese automata results in a reduction inthe size of the grammar.
The reduction ofthe grammar size by using efficient algo-rithms for pBTAs is the main contributionof this paper.
Experiments suggest thatthis leads to a reduction in grammar sizeby a factor of 2.
This paper also suggestssome extensions of the original uDOP al-gorithm that are made possible or aided bythe use of tree automata.1 IntroductionThe approaches to unsupervised parsing givenby Bod (2006a,2006b,2007) are all based on us-ing all possible subtrees over a training corpus.This means that a great number of subtrees hasto be represented.
For every sentence the num-ber of binary trees that can be proposed for thatsentence1 is given by the Catalan number of thelength of the sentence.
The number of subtreesAcknowledgments: The author would like tothank Amit Kirschbaum, Robert Remus, Anna Janskaand the anonymus reviewers for their remarks.1Only a single nonterminal X is usedfor a tree in this set is exponential with respectto the length of the sentence.In Bod (2007) a packed representation forall subtrees was proposed that is based on atechnique for supervised Data Oriented Parsing(DOP) given in Goodman (2003).
This paperaims to relate the problem of representing anestimate for all subtrees over a corpus to thefield of tree automata (Fu?lo?p and Vogler, 2009).With this step it will be possible to reduce thesize of the packed representation of the subtreeseven further.
This newly formulated approachwill also consider working with partially brack-eted corpora.The next step in this paper will be a shortdiscussion of uDOP.
Then the necessary termi-nology is introduced.
The reduction approachof this paper is given in section 4.
In the finalsection it will be discussed how the step to treeautomata creates additional possibilities to in-fluence the final estimate produced by the uDOPestimator.
In section 5 some evaluation resultswill be given for the decrease in grammar sizethat can be achieved by the techniques presentedhere.2 A Short Discussion of uDOPThe unsupervised Data Oriented Parsing(uDOP) approach (Bod 2006a,2006b,2007) isdefined by two steps.
The first step is proposingevery binary parse tree for each sentence in thecorpus.
This is followed by adding any subtreethat occurs in the trees to the grammar as apossible derivation step.
Since binary trees havemore subtrees than nonbinary trees, the binary24XjohnXXX Xsees jim(a) A possible tree pro-posed over a corpussentenceXX Xsees(b) another pos-sible subtreeFigure 1: an example for the uDOP approachones would always be the parses the approachprefers.
Therefore the uDOP approach only usesthe binary trees.
The only nonterminal used isa new symbol usually refered to as ?X?.The second step is estimation.
For each sub-tree the number of occurrences in the proposedtrees is counted.
This number is divided by thesum of all occurrences of subtrees starting withthe same nonterminal, which allows to derive aprobability distribution over all trees.If one takes the sentence ?john sees jim?, onetree that can be proposed is shown in Figure1(a).
Then one possible subtree is shown in Fig-ure 1(b).
The subtree in Figure 1(b) would oc-cur twice among the parses for the sentence ?jimsees the french guest?, since there are two pos-sible binary parses with the nonterminal ?X?
forthe substring ?the french guest?.
One is given byX(X(X(the)X(french))X(guest)) (1)the other is given by:X(X(the)X(X(french)X(guest))) (2)In this paper a small extension of the originaluDOP algorithm is considered.
The idea is wellknown from Pereira and Schabes (1992).
Theextension is assuming that the corpus may con-sist of partial parses.
The algorithm is changedso that for every partial tree all binary trees thatare completions of the partial tree are proposed.Labels for the constituents in the partial tree arekept.
Only a single nonterminal is used for thecompletions.Take for example the sentence ?john sees thereporter?.
If one is confident that ?the reporter?is a constituent of the type NP then the corpusentry would be:X(X(john)X(sees)NP (X(the)X(reporter)))(3)This entry has two completions, the first oneis given by:X(X(X(john)X(sees))NP (X(the)X(reporter)))(4)The second one is given by:X(X(john)X(X(sees)NP (X(the)X(reporter))))(5)So making a parse complete means introduc-ing additional brackets until the tree is binary.One may also consider not introducing bracketsinside of existing brackets in order to allow fornonbinary branching.These two parses contain subtrees startingand terminating with the nonterminalNP .
Thisshows that such partial brackets and their classlabels can create recursion on the introduced la-bels.
These partial parses could come from otheralgorithms and reduce the final grammar size.Approaches like the ones in Ha?nig (2010),Santamaria and Araujo (2010) and Ponvert etal.
(2011) could be combined with the uDOPapproach using this simple extension.
All threeapproaches do not necessarily produce binaryparse trees.
This could be used to extend uDOPto nonbinary trees.
Using the low level bracket-ings from the algorithms would reduce the size ofan uDOP grammar estimated from them.
Par-tial bracketing could also be approximated byusing HTML annotation, punctuation and se-mantical annotation (Spitkovsky et al, 2010;Spitkovsky et al, 2011; Naseem and Barzilay,2011).253 TerminologyThis section introduces stochastic tree substitu-tion grammars.
It will also introduce a versionof probabilistic bottom up tree automata suitedfor representation of large stochastic tree substi-tution grammars.
Furthermore it gives a moreformal definition of the uDOP-estimate.
Somedefinitions are not standard.2The first definition necessary is the concept oftrees.Definition 1 (Trees).
The set of trees overleaf nodes L and internal nodes I is denoted byT (L, I) and is defined as the smallest set con-forming to:??
?
(T (L, I) ?
L)?
: ?y ?
I : y(?)
?
T (L, I)(6)Where X?
denotes all tuples over the set X.3If a tree has the form y(?)
then y ?
I is calledthe root node.
The leftmost node of an elementt ?
(L ?
T (L, I)) is denoted by lm(t) and givenby:lm(t) ={t if t ?
Llm(x1) if t = y(x1, .
.
.
, xn)(7)This definition basically states that trees arebracketed structures.
Annotation gives the typeof the bracket.
Note that the definition of treesexcludes trees that consist of only a single leafnode.
This is a restriction that is common forSTSGs.The next element that needs to be defined isthe concept of extending a tree.
If a node ina tree has more than two daughters, then thetree can be extended.
This is done by replacingtwo of the daughter nodes by a new node Nlabeled with any nonterminal and making thetwo removed daughter nodes the daughter nodesof the new node N .
A complete tree is a tree thathas no extensions.
In other words, a completetree has only nodes with less than two daughters.A tree t is a completion of the tree t?
if t iscomplete and can be generated from t?
by any2No rank is assumed for the labels of trees, to give anexample.3The empty tuple is included.number of completions.
Next it is necessary todefine subtrees.Definition 2 (Subtrees).
Lett =L(.
.
.M(N1(.
.
.
), .
.
.
, Ni(?),.
.
.
, Nk(.
.
. ))
.
.
.
)be a tree thent?
=M(N1(.
.
.
), .
.
.
, Ni(?),.
.
.
, Nk(.
.
.
))is a direct subtree of t and if the root of ?
isin I thent??
=L(.
.
.M(N1(.
.
.
), .
.
.
, Ni(),.
.
.
, Nk(.
.
. ))
.
.
.
)is also a direct subtree of t. The set of subtreesfor a tree t is denoted by ST (t) and contains tand all direct subtrees of trees in ST (t).The first important fact about subtrees is thateach node has either all or none of its daughtersincluded in a subtree.
The second importantfact is that subtrees of less than two nodes arenot allowed.Definition 3 (Stochastic Tree SubstitutionGrammar).
A stochastic tree substitution gram-mar (STSG) is a tuple ?
?, N, ?,N0, ??
where ?is a finite alphabet of terminals, N is a finite setof nonterminals, N0 ?
N is the start nontermi-nal, ?
?
T ((?
?
N), N) is the set of trees4 and?
: ?
?
R+ is the weight function, where R+ isthe set of positive real numbers.For space reasons it will not be discussed howa STSG defines a distribution over strings andtrees.
Note that since a CFG can be found thatdefines the same distribution over strings for ev-ery STSG (Goodman, 2003) similar constraintshold for STSGs and CFGs when it comes todefining proper distributions.
In order to ensurethat all string weights sum up to 1 the trees in4This set may be finite or infinite.
The uDOP Esti-mate results in a finite set if the corpus is finite.26T for each possible root nonterminal must sumto one.5Definition 4 (Probabilistic Bottom Up TreeAutomaton).
A probabilistic bottom up tree au-tomaton (pBTA) is a tuple ?Q,?, ?, q0, ?, ?
?where Q is a finite set of states, ?
is the finitealphabet, ?
?
Q+??
?Q is the finite set of tran-sitions where Q+ denotes all nonempty tuplesover the states, q0 is the start state, ?
: ?
?
R+is the transition weight function and ?
: ?
?
R+is the final weight function.Definition 5 (Weight of a Tree in a pBTA).The weight of an element t ?
T (?, Q???{q0}??)
given an automaton A = ?Q,?, ?, q0, ?, ??
isdenoted by ?
(t, A) and is defined by:?
(q0, A) =1 (8)?
(q, l(?
), A) =???Qn?(???
?, l, q?)??lm(tm)???
(qm, lm(tm), A) (9)Where ?
= l1(t1), .
.
.
, ln(tn) and ?
=q1, .
.
.
, qn.
Where these formulas do not definea weight, it is assumed to be 0.The final weight of the tree t =l(l1(t1), .
.
.
, ln(tn)) for the automaton A isdenoted by ?
(l(l1(t1), .
.
.
, ln(tn)), A) and isdefined as:?(l(?
), A) =?q?Q???Qn?(???
?, l, q?)??tm???
(qm, lm(t1), A) (10)Where again ?
= l1(t1), .
.
.
, ln(tn) and ?
=q1, .
.
.
, qn.The definitions for pBTAs basically specify abottom up parsing proceedure in which finishedtrees are combined.
The intermediate trees arelabeled with states that guide the derivationprocess.5Ensuring that the weight of the finite strings sumsto one is more difficult.
See Nederhof and Satta (2006).Definition 6 (Language).
The Language of apBTA A denoted L(A) is the set:L(A) = {t|?
(t, A) 6= 0} (11)The penultimate set of definitions is con-cerned with the language weight of a pBTA, in-side and outside weights.Definition 7 (Language Weight).
The languageweight for a pBTA A = ?Q,?, ?, q0, ?, ??
is de-noted by wl(A) and defined by:wl(A) =?t?T (?,?)?
(t, A) (12)The inside weight for a state q ?
Q for anautomaton A = ?Q,?, ?, q0, ?, ??
is denoted byinside(q, A).
It is the language weight of A?.Here A?
is A changed so that it only has one finaltransition from ?q?
to some state with weight 1.The outside weight for a state q ?
Q needsa recursive definition.
The weight is made upof two summands.
The first summand is theoutside weight of the right hand side of all tran-sitions q occurs in.6 This is multiplied with theinside weight of all states other than q in the lefthand side of the transition.
Finally this value istaken times the number of occurrences of q inthe left hand side.
The second summand is thesame as the first only with the outside weightreplaced by the final weight of the transitions.Now only the uDOP estimate and the connec-tion between STSGs and pBTAs are still miss-ing.Definition 8 (uDOP Estimate).
For a STSGG = ?
?, {X}, T,N0, ??
and a corpus c =?c1, .
.
.
, cm?
such that each cl is a tree of theform L(L1(x1), .
.
.
, Ln(xn)) or an extension ofsuch a tree.
Let c?
be derived from c by replac-ing each cl by all the complete trees in Ext(cl).Then the uDOP estimate uDOP(t, c) is given by:?
(t) =?c1?c?
num(t, c1)?t?
?T (N,N??)?c1?c?
num(t?, c1)(13)where num(t, x) is the number of times sub-tree t occurs in the tree x.6In a transition ?
?, l, q?
?
is the left hand side and qthe right hand side.27Here c?
is a corpus that contains each com-pletion t?
once for every tree t in the originalcorpus, such that t?
is a completion of t. Thiscorpus is of course never generated explictly andonly used in the definition.Definition 9 (STSG Given by a pBTA).
LetG = ?
?, N, ?,N0, ??
be a STSG.
The grammaris given by a pBTA A if t ?
T ?
L(A) and?
(x) = ?
(x,A).This definition states that the set of trees isthe language of the automaton and the weightof each tree is the weight the automaton assignsto it.The goal of this paper can now be describedin the following way: given a corpus of trees?c1, .
.
.
, cn?
find a pBTA A that gives the uDOPestimate and is as small as possible.
The rele-vant measure here is the number of transitions.The number of states that are useful, the num-ber of labels that are used and the number ofentries for the weight function are all dependenton the number of transitions.
This measure isalso independent of any specific implementationdetails.
From the connection between STSGsand pBTAs some extensions to the uDOP algo-rithm are possible that will be discussed at theend of the paper in section 6.Only completion with the nonterminal X isused.
All algorithms given in this paper can beadapted to more brackets by creating a transi-tion for the additional labels whenever one forX is created.4 Reducing the Size of the EstimateThe generation process for the uDOP estimatethat this paper proposes is as follows.
Firstgenerate a pBTA representing all the completeparse trees for the corpus.
Every tree t in thecorpus will have as its weight in the automatonthe number of times it occurs in the completedcorpus.
The completed corpus is again the cor-pus with each tree replaced by all the trees com-pletions.As a second step manipulate the automatonfor the set of completions in such a way thatthe set of subtrees is given and they are asso-ciated with the intended relative weights.
Thenapply normalization similar to the one employedby Maletti and Satta (2009).
The normalizationalgorithm has to be slightly changed to accountfor the fact that the trees are not supposed tostand on their own, but rather be used in anSTSG.
A sketch will be given.
For all final tran-sition with label l sum up the final weight of thetransition times the inside weight of all states onthe left hand side of the transition.
Then mul-tiply the weight of final transitions with label lwith the multiplication of the inside weights oftheir left hand side states and divide the weightby the sum for the label l. All other weightsare normalized as described in Maletti and Satta(2009).The reduction that will be proposed here isbased on reducing the size of the representationof all trees.
Once this is achieved, a simple algo-rithm can be applied that gives the uDOP esti-mate and only increases the size of the represen-tation by a maximum factor of 2 ?
|I| + |I|2 + 1plus one transition for every nonterminal label.7To understand the mapping to subtrees con-sider the following: If an automaton gives the setof all trees, then the outside weight of any statewill be the number of trees this state is embed-ded in.
The inside weight will be the number oftrees embedded at this position.
This is the casebecause inside and outside weights sum over thepossible derivations.For each nonterminal label l a state ql is cre-ated only to represent the introduction of l. Atransition of the form ?
?q0?, l, ql?
is added to therepresentation of all trees.8 This transition isweighted by 1.Denote the automaton representing all treesby AT .
Let r = ?
?q1, q2?, X, q?
be a transition inAT .
For each label l:inlab(qx, l) =???,l,qx????
(r) ??qy?
?inside(qx, AT )(14)7I is the set of labels that occur on internal nodesor - in other words - the nonterminal labels.
The factoris explained further into the section.
Note that for thestandard uDOP approach |I| = 1.8q0 is assumed to be the start state.28For each nonterminal label l create the rules9:r1 =?
?ql, q2?, X, q?
(15)r2 =?
?q1, ql?, X, q?
(16)For each pair of nonterminal labels l1, l2 createa rule10:r3 = ?
?ql1 , ql2?, X, q?
(17)Let w be the weight of the original transition.Set ?
(r1) = in(q2, l) ?w,?
(r2) = in(q1, l) ?w and?
(r3) = in(q1, l1)?in(q2, l2)?w respectively.
Addfinal weight out(q) to each transition.11 This as-signs to each subtree the number of counts.
Af-ter the transformation each transition can be apoint at which a derivation ends.
Outside weightis assigned according to the number of trees thesubtree is embedded in.
The derivation can alsostart with any node.
Therefore inside weightis added according to the number of embeddedtrees.Normalizing the automaton afterwards givesthe weights according to the uDOP estimator.Bansal and Klein (2010) give a transformationfrom parse trees to subtrees that reduces the sizeof the representation even further.
Since a ver-sion of the transformation from their paper canbe applied to any representation of the full parsetrees, it is complementary to the approach usedhere.
For this reason it will not be discussedhere and it should suffice to say that using thistransformation would improve the results in thispaper even further.Before it is discussed how the size of the rep-resentation of all trees can be reduced further,the first step will be to present the approach byGoodman (2003).4.1 The Goodman ReductionThe approach from Goodman (2003) was in-tended for use with the supervised version of9This accounts for the factor 2 ?
|I|.10This accounts for the factor |I|2.11An actual implementation would not create a rule ifall weights are 0.Data Oriented Parsing.
We will discuss a ver-sion of the algorithm that is based on tree au-tomata and the considerations made so far.The original approach works by creating astate for every node in the corpus.
Each group ofdaughter nodes is then connected to its mothernode by a weighted transition with weight 1.The transition from the daughters of the rootnode of a sentence is assigned a final weight of1.
Finally, the projection to the subtrees is ap-plied.The version for unsupervised parsing is sim-ilar to and based on parse forests and parseitems.
The states correspond to parsing itemsas in the CYK algorithm.12The reduction can be described as follows:create a state/parse item ?i, j, k?
for every sen-tence k and every range from i to j in the sen-tence.
Also create one state for every type ofterminal node.
This is illustrated in Figures 2(a)and 2(b).
Rules are introduced from the startstate for each possible terminal to the terminaltype nodes with weight 1.
If terminal x occurs insentence k from i to i+1, create a transition fromthe terminal state for x to the state ?i, i + 1, k?with weight 1.
Label those transitions with Xor with the appropriate preterminal nontermi-nal if there is one in the partial corpus tree.
Allstates with a difference greater than 1 betweenthe start and the end point are connected to allstate pairs ?i,m, k?, ?m, j, k?.
Here m is a pointbetween i and j.
These transitions are again la-beled by X unless there is a bracket labeled byL from i to j in this case the transition is labeledby L. The weight for each such transition is 1.If i is 0 and j is the length of the sentencenumber k then the final weight for transitionsto the state ?i, j, k?
is 1.In order to comply with the requirement thatwe only use completions of the given trees, oneadjustment is necessary.
When a bracket a, bis present, no state i, j, k is proposed such thata < i < b < j ?
i < a < j < b.
Thereby allcrossing brackets are ruled out.12See for example (Younger, 1967).294.2 Making the Representation of AllTrees DeterministicA possible step in size reduction is making therepresentation deterministic.
Generally deter-minization does not lead to a reduction in sizeof a finite state automaton.
Here however, de-terminization means simply that states repre-senting equivalent subtrees are merged.
This issimilar to the graph packing idea used in Bansaland Klein (2010).Assume a partial tree is given in the stringform that was used in section 3, i.e.
, as astring of brackets and symbols.
Then two iden-tical strings represent identical trees which haveidentical completions.
Let the bracketing for se-quence from i to j in sentence k be identical tothe bracketing for the sequence from l to m insentence n. Assume also that the sequences rep-resent the same string.
Then the state ?i, j, k?may be replaced in every transition by the state?l,m, n?.
The only thing that has to be kepttrack of is how many times a certain string cor-responded to a full corpus entry.
In the Good-man approach a final weight of 1 is used, sincenew states are created for every sentence.
In thedeterministic case the final weight for all tran-sitions reaching a state that represents a brack-eted sequence x must be increased by 1 for eachtime x occurs in the corpus.
An illustration ofthe idea13 is given in Figures 2(c) and 2(d).4.3 Using MinimizationFinally one can try finding the minimal deter-ministic weighted tree automaton for the distri-bution.
This is a well defined notion.Definition 10 (Minimal determinis-tic pBTA).
The minimal deterministicpBTA A?
= ?Q?,?
?, ?
?, q?0, ?
?, ???
for agiven pBTA A = ?Q,?, ?, q0, ?, ??
fulfills?
(x, p) = ?(x,A?)
and there is no automatonA??
= ?Q??,??
?, ??
?, q?
?0 , ??
?, ????
fulfilling thiscriterion with |Q?
?| < |Q?|.A minimal deterministic pBTA is unique forthe distribution it represents up to renaming thestates.13Here shown without any bracketing dataX|0,1,1johnX|0,3,1X|1,3,1X|2,3,1X|0,2,1X|1,2,1sees jim(a) Goodman reduction statesX|0,1,2johnX|0,3,2X|1,3,2X|2,3,2X|0,2,2X|1,2,2sees frank(b) Goodman reduction statesX|johnjohnX|john sees jimX|sees jimX|jimX|john seesX|seessees jim(c) Deterministic reduction statesNote that this means that after the minimiza-tion the automaton is as small as possible for adeterministic pBTA.
The only way to improveon this while staying in the pBTA frameworkwould be to find a minimal nondeterministicautomaton.
That this is possible is shown inBozapalidis (1991).
It is however not clear thatthis problem could be solved in reasonable timefor an automaton with hundreds of thousands ofstates.In order to generate a minimal automaton,an efficiently verifiable criterion for two states30X|johnjohnX|john sees frankX|sees frankX|frankX|john seesX|seessees frank(d) Deterministic reduction statesX|johnjohnX|john sees 1X|sees 1X| 1X|john seesX|seessees jim(e) Minimization reduction statesX|johnjohnX|john sees 1X|sees 1X| 1X|john seesX|seessees frank(f) Minimization reduction statesFigure 2: the different reduction approaches illus-trated, different edge colors correspond to differentparses.
The Figures 2(b) and 2(a) are for the Good-man approach.
Every span of words has its own stateproposed.
Figures 2(c) and 2(d) show how equalsspans of words will lead to the repetition of the samestate in the deterministic approach.
Figures 2(e) and2(f) show how two states that have equal contexts aremerged into one state called ?1?.
This is an illustra-tion of the minimization approach.to be equivalent is necessary.
For deterministicpBTA this is given by Maletti (2009).
Since theautomaton for all trees is nonrecursive after thedeterministic construction, normalization allowsminimizing the automaton in linear time, de-pending on the number of transitions.14 For thealgorithms to work, the fact that a deterministicpBTA is constructed is a necessary precondition.Figures 2(e) and 2(f) illustrate this approach.The tree X(jim) is distributed equally to thetree X(frank).
Since this is the case, a mergedstate for both trees is introduced.
This state islabeled as 1.5 Experimental ResultsThe algorithm was tested in two domains.
Thefirst one was the Negra Corpus (Skut et al,1997).
The second one was the Brown Corpus(Francis, 1964).
The standard approach in unsu-pervised parsing is to use sequences of tags withcertain punctuation removed (Klein and Man-ning, 2002).
This is supposed to simulate spo-ken language.
Once the punctuation is removedall sequences of length 10 or less are used formost approaches in unsupervised parsing.
Thisensures that the hypothesis space is relativelysmall for the sentences left in the corpus.
Thesame approach is chosen for this paper, as this isthe context in which uDOP grammars are mostlikely to be evaluated.
A slightly different def-inition of punctuation is used.
Note that nobracketing structure is used.
This means thatfor every string in the corpus, a number of tran-sitions has to be created that is limited by n3 inthe worst case, were n is the length of the string.Note that the removal of more punctuationmarks will lead to a sample that is harder to re-duce in size by determinization and minimiza-tion.
Punctuation occurs frequently and cantherefore easily net a great number of reductionsby merging states.For the Negra Corpus all tags matching/\$ \S?/are removed.15 This leads to a corpus of 724814The normalization can also be implemented in lineartime for nonrecursive pBTA.15Here/\$ \S?/is an regular expression according to the ruby regu-lar expression specifications (Flanagan and Matsumoto,31negra brown5000Goodman Based 1528256 1238717Deterministic 857150 785427Minimized 633907 602491brown10000 brown15000Goodman Based 2389442 3603050Deterministic 1402536 2030252Minimized 1029786 1457499Table 1: The results from the experimental evalua-tion.
The numbers given reflect the number of tran-sitions after the transformation to subtrees.tag sequences of length 10 or less.For the Brown Corpus the tags that are re-moved are specified by the regular expression/\W+/Not the whole sample from the Brown Cor-pus is used.
Instead samples of 5000,10000 and15000 sequences of tags are used.The results of the algorithms can be seen inTable 5.
In order to make the comparison im-plementation independent, the number of tran-sitions after the transformation to subtrees, asexplained in section 4, is given.The results show that the minimization algo-rithm tends to cut the number of transitionsin half for all corpora.
This means these re-ductions in the number of transitions could beused to double the size of the corpus used inuDOP.16 Note that if one was to extend the cor-pus with more strings of limited size the bene-fit of the new approaches should become morepronounced.
This is the case since the deter-minstic construction only introduces one stateper observed substring.
The set of possible tagsequences of length 10 or less is limited.
Thisholds especially true if one considers linguisticconstraints.
This tendency can be seen from thestatistics for 15000 sentences from the Browncorpus.2008).16This is the case, since the number of states growslinearly with corpus size for fixed sentence length.6 Possible ExtensionsNote that tree automata are closed under inter-section (Fu?lo?p and Vogler, 2009).
Bansal andKlein (2010) propose improving a DOP estimateby changing the weights of the subtrees.
Thisis done by using a weighting scheme that dis-tributes along the packed representation.
Thiscan be extended with the techniques in this pa-per in the following way: Assume one wants togive the weight of the subtree as the joint prob-ability of a tree automaton model that has pre-viously been given and the uDOP estimate.
Allthat is necessary to achieve this would be to rep-resent the uDOP estimate as a tree automaton,intersect it with the previously given automatonand apply a normalization as discussed above.17.The algorithm allows another generalizationin addition to the one proposed.
This is thecase since the mapping to subtrees can be im-plemented by application of a tree transducer(Knight and Graehl, 2005).
Therefore, the finalestimation can be made more complex.
Simplyreplace the mapping step by the application ofa transducer.7 ConclusionIn this paper it was discussed how the size ofthe unsupervised Data Oriented Parsing esti-mate for STSGs can be reduced.
By translatingthe problem into the domain of finite tree au-tomata, the problem of reducing the grammarsize could be handled by solving standard prob-lems in that domain.The code used for the experiments in this pa-per can be found at http://code.google.com/p/gragra/.ReferencesMohit Bansal and Dan Klein.
2010.
Simple, accu-rate parsing with an all-fragments grammar.
InACL 2010, Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, Uppsala, Sweden, pages 1098?1107.
The As-sociation for Computer Linguistics.17the last step is necessary for the subtree probabilitiesto sum to 132Rens Bod.
2006a.
An all-subtrees approach to unsu-pervised parsing.
In ACL-44: Proceedings of the21st International Conference on ComputationalLinguistics and the 44th annual meeting of theAssociation for Computational Linguistics, pages865?872.
The Association for Computational Lin-guistics.Rens Bod.
2006b.
Unsupervised parsing with u-dop.
In CoNLL-X ?06: Proceedings of the TenthConference on Computational Natural LanguageLearning, pages 85?92.
Association for Computa-tional Linguistics.Rens Bod.
2007.
Is the end of supervised parsing insight?
In Proceedings of the 45th Annual Meetingof the Association for Computational Linguistics,pages 400?407.
The Association for Computer Lin-guistics.Symeon Bozapalidis.
1991.
Effective construction ofthe syntactic algebra of a recognizable series ontrees.
Acta Informatica, 28(4):351?363.David Flanagan and Yukihiro Matsumoto.
2008.The ruby programming language.
O?Reilly, firstedition.W.
Nelson Francis.
1964.
A standard sample ofpresent-day english for use with digital comput-ers.
Technical report, Brown University.Zoltan Fu?lo?p and Heiko Vogler, 2009.
Weighted TreeAutomata and Tree Transducers, chapter 9, pages313?394.
Springer Publishing Company, Incorpo-rated, 1st edition.Joshua Goodman.
1998.
Parsing Inside-Out.
Ph.D.thesis, Harvard University.Joshua Goodman.
2003.
Efficient algorithms for thedop model.
In Data Oriented Parsing.
Center forthe Study of Language and Information, Stanford,California.Christian Ha?nig.
2010.
Improvements in unsuper-vised co-occurrence based parsing.
In Proceedingsof the Fourteenth Conference on ComputationalNatural Language Learning, pages 1?8.
Associa-tion for Computational Linguistics.Dan Klein and Christopher D. Manning.
2002.
Agenerative constituent-context model for improvedgrammar induction.
In Proceedings of the Associ-ation for Computational Linguistics, pages 128?135.
Association for Computational Linguistics.Kevin Knight and Jonathan Graehl.
2005.
Anoverview of probabilistic tree transducers for nat-ural language processing.
In CICLing, volume vol-ume 3406 of Lecture Notes in Computer Science,pages 1?24.Andreas Maletti and Giorgio Satta.
2009.
Parsingalgorithms based on tree automata.
In IWPT ?09:Proceedings of the 11th International Conferenceon Parsing Technologies, pages 1?12.
Associationfor Computational Linguistics.Andreas Maletti.
2009.
Minimizing deterministicweighted tree automata.
Information and Com-putation, 207(11):1284?1299.Tahira Naseem and Regina Barzilay.
2011.
Us-ing semantic cues to learn syntax.
In AAAI2011: Twenty-Fifth Conference on Artificial In-telligence.Mark-Jan Nederhof and Giorgio Satta.
2006.
Es-timation of consistent probabilistic context-freegrammars.
In Proceedings of the main confer-ence on Human Language Technology Conferenceof the North American Chapter of the Associa-tion of Computational Linguistics, pages 343?350,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed cor-pora.
In Proceedings of the 30th annual meeting onAssociation for Computational Linguistics, ACL?92, pages 128?135, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Elias Ponvert, Jason Baldridge, and Katrin Erk.2011.
Simple unsupervised grammar inductionfrom raw text with cascaded finite state models.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: Hu-man Language Technologies.Jesus Santamaria and Lourdes Araujo.
2010.
Identi-fying patterns for unsupervised grammar induc-tion.
In Proceedings of the Fourteenth Confer-ence on Computational Natural Language Learn-ing (CoNLL).
Association for Computational Lin-guistics.Wojciech Skut, Brigitte Krenn, Thorsten Brants,and Hans Uszkoreit.
1997.
An annotation schemefor free word order languages.
In Proceedings ofthe fifth conference on Applied natural languageprocessing, ANLC ?97, pages 88?95.
Associationfor Computational Linguistics.Valentin I. Spitkovsky, Daniel Jurafsky, and HiyanAlshawi.
2010.
Profiting from mark-up: hyper-text annotations for guided parsing.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, ACL ?10, pages1278?1287, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Valentin I. Spitkovsky, Hiyan Alshawi, and DanielJurafsky.
2011.
Punctuation: Making a point inunsupervised dependency parsing.
In In Proceed-ings of the Fifteenth Conference on ComputationalNatural Language Learning (CoNLL-2011).33Daniel H. Younger.
1967.
Recognition and parsingof context-free languages in time n3.
Informationand Control, 10:189?208.34
