Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 193?198,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsTwo Knives Cut Better Than One:Chinese Word Segmentation with Dual DecompositionMengqiu WangComputer Science DepartmentStanford UniversityStanford, CA 94305Rob VoigtLinguistics DepartmentStanford UniversityStanford, CA 94305{mengqiu,manning}@cs.stanford.edu robvoigt@stanford.eduChristopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305AbstractThere are two dominant approaches toChinese word segmentation: word-basedand character-based models, each with re-spective strengths.
Prior work has shownthat gains in segmentation performancecan be achieved from combining thesetwo types of models; however, past effortshave not provided a practical techniqueto allow mainstream adoption.
We pro-pose a method that effectively combinesthe strength of both segmentation schemesusing an efficient dual-decomposition al-gorithm for joint inference.
Our methodis simple and easy to implement.
Ex-periments on SIGHAN 2003 and 2005evaluation datasets show that our methodachieves the best reported results to dateon 6 out of 7 datasets.1 IntroductionChinese text is written without delimiters betweenwords; as a result, Chinese word segmentation(CWS) is an essential foundational step for manytasks in Chinese natural language processing.
Asdemonstrated by (Shi and Wang, 2007; Bai etal., 2008; Chang et al, 2008; Kummerfeld et al,2013), the quality and consistency of segmentationhas important downstream impacts on system per-formance in machine translation, POS tagging andparsing.State-of-the-art performance in CWS is high,with F-scores in the upper 90s.
Still, challengesremain.
Unknown words, also known as out-of-vocabulary (OOV) words, lead to difficulties forword- or dictionary-based approaches.
Ambiguitycan cause errors when the appropriate segmenta-tion is determined contextually, such as??
(?tal-ent?)
and?
/?
(?just able?)
(Gao et al, 2003).There are two primary classes of models:character-based, where the foundational units forprocessing are individual Chinese characters (Xue,2003; Tseng et al, 2005; Zhang et al, 2006;Wang et al, 2010), and word-based, where theunits are full words based on some dictionary ortraining lexicon (Andrew, 2006; Zhang and Clark,2007).
Sun (2010) details their respective theo-retical strengths: character-based approaches bet-ter model the internal compositional structure ofwords and are therefore more effective at inducingnew OOV words; word-based approaches are bet-ter at reproducing the words of the training lexi-con and can capture information from significantlylarger contextual spans.
Prior work has shown per-formance gains from combining these two typesof models to exploit their respective strengths, butsuch approaches are often complex to implementand computationally expensive.In this work, we propose a simple and prin-cipled joint decoding method for combiningcharacter-based and word-based segmenters basedon dual decomposition.
This method has strongoptimality guarantees and works very well empir-ically.
It is easy to implement and does not re-quire retraining of existing character- and word-based segmenters.
Perhaps most importantly, thiswork presents a much more practical and usableform of classifier combination in the CWS contextthan existing methods offer.Experimental results on standard SIGHAN2003 and 2005 bake-off evaluations show that ourmodel outperforms the character and word base-lines by a significant margin.
In particular, outapproach improves OOV recall rates and segmen-tation consistency, and gives the best reported re-sults to date on 6 out of 7 datasets.2 Models for CWSHere we describe the character-based and word-based models we use as baselines, review existingapproaches to combination, and describe our algo-rithm for joint decoding with dual decomposition.1932.1 Character-based ModelsIn the most commonly used contemporary ap-proach to character-based segmentation, first pro-posed by (Xue, 2003), CWS is seen as a charac-ter sequence tagging task, where each characteris tagged on whether it is at the beginning, mid-dle, or end of a word.
Conditional random fields(CRF) (Lafferty et al, 2001) have been widelyadopted for this task, and give state-of-the-art re-sults (Tseng et al, 2005).
In a first-order linear-chain CRF model, the conditional probability of alabel sequence y given a word sequence x is de-fined as:P (y|x) =1Z|y|?t=1exp (?
?
f(x, yt, yt+1))f(x, yt, yt?1) are feature functions that typicallyinclude surrounding character n-gram and mor-phological suffix/prefix features.
These types offeatures capture the compositional properties ofcharacters and are likely to generalize well to un-known words.
However, the Markov assumptionin CRF limits the context of such features; it isdifficult to capture long-range word features in thismodel.2.2 Word-based ModelsWord-based models search through lists of wordcandidates using scoring functions that directlyassign scores to each.
Early word-based seg-mentation work employed simple heuristics likedictionary-lookup maximum matching (Chen andLiu, 1992).
More recently, Zhang and Clark(2007) reported success using a linear modeltrained with the average perceptron algorithm(Collins, 2002).
Formally, given input x, theirmodel seeks a segmentation y such that:F (y|x) = maxy?GEN(x)(?
?
?
(y))F (y|x) is the score of segmentation result y.Searching through the entire GEN(x) space isintractable even with a local model, so a beam-search algorithm is used.
The search algorithmconsumes one character input token at a time, anditerates through the existing beams to score twonew alternative hypotheses by either appendingthe new character to the last word in the beam, orstarting a new word at the current position.Algorithm 1 Dual decomposition inference algo-rithm, and modified Viterbi and beam-search algo-rithms.
?i ?
{1 to |x|} : ?k ?
{0, 1} : ui(k) = 0for t?
1 to T doyc?= argmaxyP (yc|x) +?i?|x|ui(yci)yw?= argmaxy?GEN(x)F (yw|x)?
?j?|x|uj(ywj)if yc?= yw?thenreturn (yc?,yw?
)end iffor all i ?
{1 to |x|} do?k ?
{0, 1} : ui(k) = ui(k) + ?t(2k ?
1)(yw?i?yc?i)end forend forreturn (yc?,yw?
)Viterbi:V1(1) = 1, V1(0) = 0for i = 2 to |x| do?k ?
{0, 1} : Vi(k) = argmaxk?Pi(k|k?
)Vi?1k?+ui(k)end forBeam-Search:for i = 1 to |x| dofor item v = {w0, ?
?
?
, wj} in beam(i) doappend xito wj, score(v)+= ui(0)v = {w0, ?
?
?
, wj, xi}, score(v)+= ui(1)end forend for2.3 Combining Models with DualDecompositionVarious mixing approaches have been proposed tocombine the above two approaches (Wang et al,2006; Lin, 2009; Sun et al, 2009; Sun, 2010;Wang et al, 2010).
These mixing models performwell on standard datasets, but are not in wide usebecause of their high computational costs and dif-ficulty of implementation.Dual decomposition (DD) (Rush et al, 2010)offers an attractive framework for combining thesetwo types of models without incurring high costsin model complexity (in contrast to (Sun et al,2009)) or decoding efficiency (in contrast to bag-ging in (Wang et al, 2006; Sun, 2010)).
DD hasbeen successfully applied to similar situations forcombining local with global models; for example,in dependency parsing (Koo et al, 2010), bilingualsequence tagging (Wang et al, 2013) and wordalignment (DeNero and Macherey, 2011).The idea is that jointly modelling bothcharacter-sequence and word information can becomputationally challenging, so instead we can tryto find outputs that the two models are most likely194Academia Sinica Peking Univ.R P F1RoovC R P F1RoovCChar-based CRF 95.2 93.6 94.4 58.9 0.064 94.6 95.3 94.9 77.8 0.089Word-based Perceptron 95.8 95.0 95.4 69.5 0.060 94.1 95.5 94.8 76.7 0.099Dual-decomp 95.9 94.9 95.4 67.7 0.055 94.8 95.7 95.3 78.7 0.086City Univ.
of Hong Kong Microsoft ResearchR P F1RoovC R P F1RoovCChar-based CRF 94.7 94.0 94.3 76.1 0.065 96.4 96.6 96.5 71.3 0.074Word-based Perceptron 94.3 94.0 94.2 71.7 0.073 97.0 97.2 97.1 74.6 0.063Dual-decomp 95.0 94.4 94.7 75.3 0.062 97.3 97.4 97.4 76.0 0.055Table 1: Results on SIGHAN 2005 datasets.
Roovdenotes OOV recall, and C denotes segmentationconsistency.
Best number in each column is highlighted in bold.to agree on.
Formally, the objective of DD is:maxyc,ywP (yc|x) + F (yw|x) s.t.
yc= yw(1)where ycis the output of character-based CRF, ywis the output of word-based perceptron, and theagreements are expressed as constraints.
s.t.
isa shorthand for ?such that?.Solving this constrained optimization problemdirectly is difficult.
Instead, we take the La-grangian relaxation of this term as:L (yc,yw,U) = (2)P (yc|x) + F (yw|x) +?i?|x|ui(yci?
ywi)where U is the set of Lagrangian multipliers thatconsists of a multiplier uiat each word position i.We can rewrite the original objective with theLagrangian relaxation as:maxyc,ywminUL (yc,yw,U) (3)We can then form the dual of this problem bytaking the min outside of the max, which is an up-per bound on the original problem.
The dual formcan then be decomposed into two sub-components(the two max problems in Eq.
4), each of which islocal with respect to the set of Lagrangian multi-pliers:minU(maxyc?
?P (yc|x) +?i?|x|ui(yci)??(4)+maxyw?
?F (yw|x)??j?|x|uj(ywj)??
)This method is called dual decomposition (DD)(Rush et al, 2010).
Similar to previous work(Rush and Collins, 2012), we solve this DD prob-lem by iteratively updating the sub-gradient as de-picted in Algorithm 1.1In each iteration, if thebest segmentations provided by the two models donot agree, then the two models will receive penal-ties for the decisions they made that differ from theother.
This penalty exchange is similar to messagepassing, and as the penalty accumulates over itera-tions, the two models are pushed towards agreeingwith each other.
We also give an updated Viterbidecoding algorithm for CRF and a modified beam-search algorithm for perceptron in Algorithm 1.
Tis the maximum number of iterations before earlystopping, and ?tis the learning rate at time t. Weadopt a learning rate update rule from Koo et al(2010) where ?tis defined as1N, where N is thenumber of times we observed a consecutive dualvalue increase from iteration 1 to t.3 ExperimentsWe conduct experiments on the SIGHAN 2003(Sproat and Emerson, 2003) and 2005 (Emer-son, 2005) bake-off datasets to evaluate the ef-fectiveness of the proposed dual decompositionalgorithm.
We use the publicly available Stan-ford CRF segmenter (Tseng et al, 2005)2as ourcharacter-based baseline model, and reproducethe perceptron-based segmenter from Zhang andClark (2007) as our word-based baseline model.We adopted the development setting from(Zhang and Clark, 2007), and used CTB sections1-270 for training and sections 400-931 for devel-opment in hyper-parameter setting; for all resultsgiven in tables, the models are trained and eval-uated on the standard train/test split for the givendataset.
The optimized hyper-parameters used are:1See Rush and Collins (2012) for a full introduction toDD.2http://nlp.stanford.edu/software/segmenter.shtml195`2regularization parameter ?
in CRF is set to3; the perceptron is trained for 10 iterations withbeam size 200; dual decomposition is run to maxiteration of 100 (T in Algo.
1) with step size 0.1(?tin Algo.
1).Beyond standard precision (P), recall (R) andF1scores, we also evaluate segmentation consis-tency as proposed by (Chang et al, 2008), whohave shown that increased segmentation consis-tency is correlated with better machine transla-tion performance.
The consistency measure cal-culates the entropy of segmentation variations ?the lower the score the better.
We also reportout-of-vocabulary recall (Roov) as an estimation ofthe model?s generalizability to previously unseenwords.4 ResultsTable 1 shows our empirical results on SIGHAN2005 dataset.
Our dual decomposition methodoutperforms both the word-based and character-based baselines consistently across all four sub-sets in both F1and OOV recall (Roov).
Ourmethod demonstrates a robustness across domainsand segmentation standards regardless of whichbaseline model was stronger.
Of particular noteis DD?s is much more robust in Roov, where thetwo baselines swing a lot.
This is an importantproperty for downstream applications such as en-tity recognition.
The DD algorithm is also moreconsistent, which would likely lead to improve-ments in applications such as machine translation(Chang et al, 2008).The improvement over our word- and character-based baselines is also seen in our results on theearlier SIGHAN 2003 dataset.
Table 2 puts ourmethod in the context of earlier systems for CWS.Our method achieves the best reported score on 6out of 7 datasets.5 Discussion and Error AnalysisOn the whole, dual decomposition produces state-of-the-art segmentations that are more accurate,more consistent, and more successful at induc-ing OOV words than the baseline systems that itcombines.
On the SIGHAN 2005 test set, inover 99.1% of cases the DD algorithm convergedwithin 100 iterations, which gives an optimalityguarantee.
In 77.4% of the cases, DD convergedin the first iteration.
The number of iterations toconvergence histogram is plotted in Figure 1.SIGHAN 2005AS PU CU MSRBest 05 95.2 95.0 94.3 96.4Zhang et al 06 94.7 94.5 94.6 96.4Z&C 07 94.6 94.5 95.1 97.2Sun et al 09 - 95.2 94.6 97.3Sun 10 95.2 95.2 95.6 96.9Dual-decomp 95.4 95.3 94.7 97.4SIGHAN 2003Best 03 96.1 95.1 94.0Peng et al 04 95.6 94.1 92.8Z&C 07 96.5 94.0 94.6Dual-decomp 97.1 95.4 94.9Table 2: Performance of dual decomposition incomparison to past published results on SIGHAN2003 and 2005 datasets.
Best reported F1scorefor each dataset is highlighted in bold.
Z&C 07refers to Zhang and Clark (2007).
Best 03, 05 areresults of the winning systems for each dataset inthe respective shared tasks.Error analysis In many cases the relative con-fidence of each model means that dual decom-position is capable of using information fromboth sources to generate a series of correctsegmentations better than either baseline modelalone.
The example below shows a difficult-to-segment proper name comprised of common char-acters, which results in undersegmentation by thecharacter-based CRF and oversegmentation by theword-based perceptron, but our method achievesthe correct middle ground.Gloss Tian Yage / ?s / creationsGold ???
/?
/?
?CRF ????
/?
?PCPT ??
/?
/?
/?
?DD ???
/?
/?
?A powerful feature of the dual decompositionapproach is that it can generate correct segmenta-tion decisions in cases where a voting or product-of-experts model could not, since joint decod-ing allows the sharing of information at decod-ing time.
In the following example, both baselinemodels miss the contextually clear use of the word??
(?sweets / snack food?)
and instead attach?to the prior word to produce the otherwise com-mon compound ???
(?a little bit?
); dual de-composition allows the model to generate the cor-rect segmentation.Gloss Enjoy / a bit of / snack food / , ...Gold ??
/??
/??
/?CRF ??
/???
/?
/?PCPT ??
/???
/?
/?DD ??
/??
/??
/?196Figure 1: No.
of iterations till DD convergence.We found more than 400 such surprisingly ac-curate instances in our dual decomposition output.Finally, since dual decomposition is a method ofjoint decoding, it is still liable to reproduce errorsmade by the constituent systems.6 ConclusionIn this paper we presented an approach to Chineseword segmentation using dual decomposition forsystem combination.
We demonstrated that thismethod allows for joint decoding of existing CWSsystems that is more accurate and consistent thaneither system alone, and further achieves the bestperformance reported to date on standard datasetsfor the task.
Perhaps most importantly, our ap-proach is straightforward to implement and doesnot require retraining of the underlying segmenta-tion models used.
This suggests its potential forbroader applicability in real-world settings thanexisting approaches to combining character-basedand word-based models for Chinese word segmen-tation.AcknowledgementsWe gratefully acknowledge the support of the U.S.Defense Advanced Research Projects Agency(DARPA) Broad Operational Language Transla-tion (BOLT) program through IBM.
Any opinions,findings, and conclusion or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the view of DARPA, orthe US government.ReferencesGalen Andrew.
2006.
A hybrid Markov/semi-Markovconditional random field for sequence segmentation.In Proceedings of EMNLP.Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.2008.
Improving word alignment by adjusting chi-nese word segmentation.
In Proceedings of the thirdInternational Joint Conference on Natural Lan-guage Processing (IJCNLP).Pichuan Chang, Michel Galley, and Chris Manning.2008.
Optimizing chinese word segmentation formachine translation performance.
In Proceedings ofthe ACL Workshop on Statistical Machine Transla-tion.Keh-Jiann Chen and Shing-Huan Liu.
1992.
Wordidentification for mandarin chinese sentences.
InProceedings of COLING.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof EMNLP.John DeNero and Klaus Macherey.
2011.
Model-based aligner combination using dual decomposi-tion.
In Proceedings of ACL.Thomas Emerson.
2005.
The second internationalChinese word segmentation bakeoff.
In Proceed-ings of the fourth SIGHAN workshop on Chineselanguage Processing.Jianfeng Gao, Mu Li, and Chang-Ning Huang.
2003.Improved source-channel models for Chinese wordsegmentation.
In Proceedings of ACL.Terry Koo, Alexander M. Rush, Michael Collins,Tommi Jaakkola, and David Sontag.
2010.
Dualdecomposition for parsing with non-projective headautomata.
In Proceedings of EMNLP.Jonathan K. Kummerfeld, Daniel Tse, James R. Cur-ran, and Dan Klein.
2013.
An empirical examina-tion of challenges in chinese parsing.
In Proceed-ings of ACL-Short.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of 18th InternationalConference on Machine Learning (ICML).Dekang Lin.
2009.
Combining language modeling anddiscriminative classification for word segmentation.In Proceedings of the 10th International Conferenceon Intelligent Text Processing and ComputationalLinguistics (CICLing).Alexander M. Rush and Michael Collins.
2012.
A tu-torial on dual decomposition and Lagrangian relax-ation for inference in natural language processing.JAIR, 45:305?362.197Alexander M. Rush, David Sontag, Michael Collins,and Tommi Jaakkola.
2010.
On dual decomposi-tion and linear programming relaxations for naturallanguage processing.
In Proceedings of EMNLP.Yanxin Shi and Mengqiu Wang.
2007.
A dual-layercrfs based joint decoding method for cascaded seg-mentation and labeling tasks.
In Proceedings ofJoint Conferences on Artificial Intelligence (IJCAI).Richard Sproat and Thomas Emerson.
2003.
Thefirst international Chinese word segmentation bake-off.
In Proceedings of the second SIGHAN work-shop on Chinese language Processing.Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-masa Tsuruoka, and Jun?ichi Tsujii.
2009.
A dis-criminative latent variable chinese segmenter withhybrid word/character information.
In Proceedingsof HLT-NAACL.Weiwei Sun.
2010.
Word-based and character-basedword segmentation models: Comparison andcombination.
In Proceedings of COLING.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurasfky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proceedings of the fourth SIGHANworkshop on Chinese language Processing.Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, andXihong Wu.
2006.
Chinese word segmentation withmaximum entropy and n-gram language model.
InProceedings of the fifth SIGHAN workshop on Chi-nese language Processing.Kun Wang, Chengqing Zong, and Keh-Yih Su.
2010.A character-based joint model for chinese word seg-mentation.
In Proceedings of COLING.Mengqiu Wang, Wanxiang Che, and Christopher D.Manning.
2013.
Joint word alignment and bilingualnamed entity recognition using dual decomposition.In Proceedings of ACL.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
International Journal of Compu-tational Linguistics and Chinese Language Process-ing, pages 29?48.Yue Zhang and Stephen Clark.
2007.
Chinese seg-mentation with a word-based perceptron algorithm.In Proceedings of ACL.Ruiqiang Zhang, Genichiro Kikui, and EiichiroSumita.
2006.
Subword-based tagging by condi-tional random fields for Chinese word segmentation.In Proceedings of HLT-NAACL.198
