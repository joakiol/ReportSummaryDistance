Des ign ing  the Human Machine Interfacein the ATIS DomainB.
Bly P. J. Pr ice S. Park S. TepperSRI International333 Ravenswood Ave.Menlo Park, CA 94025E.
Jackson V. AbrashAbstractSpoken language systems for the near future will nothandle all of English, but, rather, will be limited to adomain-specific sub-language.
Accurate modeling of thesub-language will depend on analysis of domain-specificdata.
Since no spoken language systems currently havea wide range of users, and since variability across usersis expected to be large, we are simulating applicationsin which a large population of potential users can besampled.
The data resulting from the simulations can beused for system development and for system evaluation.The application discussed here is the air travel domainusing the Official Airline Guide (OAG) reformatted in arelational structure.This study assesses the effects of changes in the simu-lations on the speech and language of the experimentalsubjects.
These results are relevant o both the exper-imental conditions for data collection and the design ofthe human interface for spoken language systems.
We re-port here on five experiments: (1) the effect of longer in-structions with examples vs. shorter instructions, usingour earlier data collection system, (2) a baseline xperi-ment using a functional equivalent of the data collectioneffort at Texas Instruments (TI), (3) the use of a morespecific version of the scenario used in the baseline ex-periment, (4) the use of a short, simple familiarizationscenario before the main scenario, and (5) in additionto the short familiarization scenario, the use of a finitevocabulary with rejection of sentences with extra-lexicalitems.In t roduct ionThe data reported here are part of an endeavor whosegoal is to design an appropriate human-machine inter-face by examining various parameters in a simulated in-teraction involving air travel planning.
The design ofthe system is such that either a spoken language system(SLS) or a simulation of one can be inserted betweenthe user and the relational database version of the Offi-cial Airline Guide data for North American flights andfares.
In this way we can gather data for developmentand evaluation of both the SLS and the user interface.Perhaps the greatest source of variability in the systemis that across subjects.
Individuals differ greatly in theirlanguage skills, in their problem solving skills, and intheir attention spans.
It is therefore important o sam-ple a variety of subjects from the relevant population.Individuals are also very adaptable.
In many cases, itmay be easier to rely on subject adaptability than to tryto find technological solutions.
However, the dimensionsalong which humans might adapt are largely unknownfor spoken language interfaces.
Thus, the simulationsprovide us with a mechanism to test experimentally var-ious interface strategies that may be appropriate for SLStechnology as it develops.We describe here five experiments aimed at answeringvarious questions about the interface.
Our first exper-iment, the only one reported here that was not basedon a functional equivalent of the TI data collection sys-tem, investigated the effect of a long set of instructionswith examples compared to a shorter set with no ex-amples.
The goal of this study was to investigate howmuch one "poisons the data" by using such examples.The next four experiments were based on either a func-tional equivalent of the TI system, or a minor variation:?
To serve as a baseline experiment to compare ourresults to those of TI, and to serve as a control forthe other experiments, we collected ata in a fashionthat imitated the TI system as much as possible.?
To investigate the effects on yield that might resultwhen subjects interpret what a vague scenario mightmean, we modified the scenario to fill in details thatwere unspecified in the original.?
To investigate the first session effect, which waslarge in our earlier work, we used a simple, short(about 5-minute) familiarization scenario.?
To investigate how well subjects might adapt to afixed vocabulary, we used a short familiarization sce-nario, gave subjects a list of about 1000 words, andgave error messages for utterances with words noton that list.Data Col lect ion Condi t ionsExcept for the first experiment, which was carried outbefore the functional equivalent of the TI data collection136system had been completed, our aim was to imitate aswell as we could the system used by T I  for data collec-tion.
In particular, we have used the same data fromOAG formatted in the same relational structure; thesame tool for the "wizard" (NLParse) and accompanyingNLParse grammar; the same relational database (Ora-cle) and interface to NLParse; the same set of tools forcommunication among subject, wizard, and transcriber;the same subject and experimenter instructions; and thesame formatting of tables and other objects displayed onthe screens (controlled by Oracle).
We used only one ofTI 's  scenarios, planning a family reunion involving fam-ily members of various types.Our data collection differed from that of T I  in a fewways that we felt were either unavoidable or unimpor-tant for the resulting data.
We are aware of the follow-ing differences: our A /D system uses a NEXT machine;our push-to-talk mechanism writes out a time stamp forpush and for release (this allows us to calculate the timespent speaking, waiting for an answer and thinking be-fore making the next query, which the TI  system doesnot allow); instead of the color coding used by TI, weuse a "ready" prompt when the system is ready to ac-cept speech, a "listening" prompt when the subject ispushing the mouse button, and a "processing" promptafter the subject releases the button and before the an-swer is sent.
We offered a free "DECIPHER" T-shirt toparticipants in an experimental session.Data AnalysisEach session was timed from beginning to end, the train-ing scenarios were timed, and the delay until the sub-ject initiated the first utterance was timed.
The num-bers of words and utterances produced per session werecounted, as were the numbers of words and utterancesproduced during the training scenario.
A time stampwas automatically recorded each time the subject usedthe push-to-talk button, each time a transcription wassent, and each time a response was sent to the subject'sscreen.
This allowed us to determine the average timethe subject took after receiving an answer and beforeformulating a query (thinking time), the average timethe subject held down the push-to-talk button (speakingtime), and the average time it took the wizard and thewizard's assistant to send the transcription and databaseresponse to the subject's creen (subject waiting time).The average number of words per utterance, the averagevocabulary size per subject, and the number of sentencesoutside the restricted vocabulary used in the Fixed Vo-cabulary Condition were counted.
We also counted thenumber of cancellations subjects used per session, andthe number of error messages sent.
After the session, allsubjects filled out an eleven-item questionnaire designedto assess their subjective impressions of the system andtheir satisfaction with their interaction with the system.Analyses of these measures were completed for the tensubjects in each of the four conditions that were basedon the TI  data collection system.For the word counts, we used the .nli files (see \[2\]),and used functions to reformat he data so that, for ex-ample "845" would count as three words rather thanone.
Other, similar changes were made to regularize thespellings.Condit ion 0: Long InstructionsThis condition is the only one that is not based on theT I  data collection system; it is based on the system de-scribed in \[1\].
We describe it briefly here since the resultswere part of the motivation for the two training condi-tions described below.This experiment tested the effect of subject instruc-tions on the language produced by the subjects.
Twosets of instructions were used: one that included tengrammatical and parsable utterances as examples, andone that included no examples.
In all other respects theywere identical.
Based on previous work, we expected alarge effect of experience with the system, so subjectswere asked to perform two tasks, and performance wascompared across the two tasks as well as between the twosets of instructions.
208z We found a strong interactionbetween the type of instructions given and the amount ofexperience the subject had with the system; that is, on asubject's first task, those who received long instructionsbehaved like the more experienced, second-task subjectson the measures used in the previous study.
They alsoused more complete sentences and did not show the pat-tern of short, choppy, telegraphic speech demonstratedby the subjects who received a short set of instructions.It is possible, then, to affect the speech the subject ad-dresses to an SLS by providing examples.
It is impor-tant to note that the effects of longer instructions andadditional experience with the system were not additive:new users appear to need either detailed instructions oradditional practice time but not both.The data collected in this experiment was different inimportant ways from data collected and reported by TI.The sentences, especially those produced by subjects notgiven examples, were shorter (an average number of 7.4words per utterance compared to about 12 for the TIdata).
However, due to the many differences betweenthis interface and that used by TI, it was impossible toreliably attribute these differences to any specific causes.We therefore designed a series of minor modifications ofthe T I  version, as described below.Condit ion 1: TI EquivalentThe goal of the "TI" Condition was to establish thatour data collection system was a functional equivalentof the T I  system, and then to serve as a baseline for thesubsequent conditions.
We tried to conform as closely aspossible to TI 's  methods, physical setup and materials.In this condition, subjects were read a set of instructionsidentical to the instructions used by TI, the task theywere asked to perform was one of the TI  scenarios, and137TI  SI~I-TINo.
utterances 26.2 23.5No.
words 305 298Words/utterance 11.6 12.7No.
unique words/subj.
83 81No.
unique words/cond.
286 296Time between utterances 90 sec.
89 sec.Table h SRI-TI  Condition Compared with T I  Datathe wizard was familiar with NLParse and had practiced,using the transcription and query data released by TI.The data from our T I  Condition seems to match TI 'sreleased ata very well.
As shown in Table 1, the variousmeasures made are all very similar.Perhaps the most striking difference between TI 's  dataand SRI's in the TI  Condition appeared in an analysisof word frequency.
We were astonished that the frequen-cies were so different for "show" (75 occurrences in T I 'sdata vs. 8 in ours).
Similar discrepancies showed up forthe words "me", "nonstop" and "flights".
We then real-ized that the sentence used by T I  as an example demon-strating the use of the mouse and the formatting of thetables, "Show me all the nonstop flights from Atlantato Philadelphia", had a profound effect on the result-ing data (though, of course, these utterances from eachspeaker were not used in the analysis).
In our data col-lection, we asked the subject to read the first sentenceof the scenario while we verified the recording procedureand demonstrated the push-to-talk button.Condi t ion 2: Task Specif icityWe found, in examining both data released by T I  andour own data in the T I  Condition, that it was often hardto tell how a subject had interpreted a given task, andeven which task was being performed.
The data couldbe more valuable if we could ascertain whether and howwell the subject completed the task.
We also thoughtthat subjects would be more cooperative and the taskwould be more realistic if they were concentrating onsolving the task rather than on exploring the limits ofthe system.
In addition, we suspected that some timemight be wasted while the subject tries to figure outwhat the task is.To eliminate the effect of individual interpretation ofthe task and to standardize the task across all subjects,we ran a "Specific Task" Condition.
In this condition,subjects were given the same instructions as in our TICondition.
The task they were asked to perform, how-ever, while structurally the same as the tasks performedby TI 's  subjects and by our own subjects in the T I  Con-dition, was more specific.
Instead of leaving the inter-pretation of certain aspects of the task to the subjects(for instance, find a flight for a person with an "adven-turous" lifestyle), we set explicit constraints (find an air-plane that holds the fewest number of passengers).
Inaddition, instead of choosing any cities from the databaseto complete the task, subjects were assigned the originand destination cities.
Each of the ten subjects in thiscondition used a different set of four cities, determinedrandomly from the set of cities in the database.
In allother aspects, this condition was identical to the previ-ous condition.We found no significant differences on any of our mea-sures between the subjects in our TI  Condition and ourTask Specificity Condition.
It may be that any bene-fits gained by subjects not being required to fill in thedetails themselves were offset by the fact that assigningrandom cities does not work as well as when subjectspick the cities themselves.
For example, several of oursubjects had difficulties because they did not realize thatDallas and Fort Worth shared an airport.
Subjectively,however, it did appear that subjects completed the as-signed task, whereas in the TI  Condition, many subjectsgave up or quit before fulfilling the various parts of thetask required by the scenario.
We are working to developobjective measures of this subjective impression of the"dialogue" quality of the collected utterances.Condi t ion 3: Famil iar izat ionOur past data collection efforts showed a large effect ofuser experience in human-human i teractions and in ex-perimental human-machine interactions \[1\].
In both con-ditions, the more domain-experienced speakers producedfewer words, fewer false starts and fewer filler words thandid the less-experienced speakers.
In addition, subjectselicited fewer error messages in their second scenarioscompared to their first.
Further, the dramatic effect ofone sentence read by all subjects at T I  shows just howadaptable subjects can be, at least in an initial session.In the "Familiarization Condition", after reading thesame instructions as in the other conditions, the exper-imenter stayed in the room with the subject and an-swered any questions the subject had in finding a singleone-way flight between San Francisco and Dallas.
Theexperimenter responded to questions including those re-garding the kind of requests the system could handle,the kind of information in the database, and the push-to-talk button.
The experimenter also provided possibleexplanations for any error messages the subject receivedduring the training scenario.
The familiarization sce-nario remained constant across all subjects, although thescenarios that constituted the main task varied amongsubjects as described in the Task Specificity Conditionabove.
The average length of a training scenario was6.57 minutes.Among the various conditions we ran, the largest ef-fect by far was that of the familiarization scenario.
Asshown in Table 2, subjects who used familiarization sce-138Task timeUtterances/TaskWords/TaskWords/UtteranceFormat queriesErrorsCancellationsThinking timeSpeaking timeWaiting timeNo WithFamiliarization Familiarization40 min.2427612.225%3.93.846 sec.8.2 sec.42 sec.23 min.171468.713%1.21.634 sec.6.9 sec.39 sec.Table 2: Comparison of Conditions with and withoutFamiliarization ScenarioUniquewords/subjectUniquewords/conditionExtra-lexical items,No.
wordsExtra-lexical items,No.
sentences(percent sentences)Vocabulary errorsOther errorsTask Time(min)Table 3: Comparison of Condition 1 (SRI-TI), 2 (TaskSpecificity), 3 (Familiarization Scenario), and 4 (FiniteVocabulary).narios took significantly less time to complete the maintask (23.2 vs. 39.9 minutes, p < .01) and used signif-icantly fewer words to complete the task (276 vs. 146,p < .01) than subjects in the other two conditions.
Thedifference between the number of utterances producedby the two groups was not significant, however (24.4 vs.17.2, p > .05), while the number of words per ut-terance used by subjects in the training conditions wasfewer (8.7 vs. 12.2, p < .01).
Subjects in the familiar-ization conditions also received fewer error messages perutterance produced (.07 vs .
.13)  and asked fewer ques-tions concerning the meanings of table headings (13% ofall queries, compared to 25% for subjects with no famil-iarization scenario).Condit ion 4: Finite VocabularyEarlier work concerning the vocabulary used by sub-jects and the percent of new words introduced ineach session suggested that expert human-machine userscould potentially adapt to a restricted vocabulary andstill maintain efficiency \[1\].
In order to test whether sub-jects would adapt to a restricted vocabulary, we slightlymodified our system to accept only a limited vocabu-lary from the subjects.
The wizard's assistant, insteadof being provided with a normal spell-checker, used aspell-checker that contained only a subset of approxi-mately 1000 most frequently used words, based on thedata released by TI in distributions 1-4 (prepilot dataplus NIST Release 1).
Subjects were made aware of thisrestriction in the instructions and were provided with alist of acceptable words.
If they used a word outside thevocabulary, they were sent the message: "You have useda word outside the system's vocabulary.
Try rephrasingyour request."
In all other respects, this "Fixed Vocabu-lary" Condition was identical to the Familiarization Con-dition (i.e., subjects in this condition were given a famil-iarization scenario and performed a constrained task).If we compare the subjects who received a familiar-ization scenario but were unlimited in vocabulary andthose who received a familiarization scenario but werelimited to a 1000-word vocabulary, we find that the er-ror messages received by the latter group for using out-of-vocabulary items is higher.
During the familiariza-tion session, they received an average of 2.0 error mes-sages of this kind, and an average of 3.8 messages of thiskind for the main task.
When added to the other errormessages they received, this gave them a slightly highernumber of total error messages received than subjectsin the comparable but unlimited-vocabulary condition(4.4 vs. 1.8).
The mean number of error messages re-ceived by the group was not, however, different from themean number of error messages received by subjects ineither of the non-familiarization scenario conditions.
Inaddition, there is evidence for the adaptation of sub-jects to a fixed vocabulary as indicated in Table 3.
Thistable indicates that with a short familiarization sessionand consistent feedback one can dramatically affect thenumber of unique words used by the subject, the num-ber outside a fixed set, and the number of sentences withsuch "extra-lexical" items, without increasing the totaltime to complete the task.
The discrepancies betweenthe-number of "extra-lexical" items and the number of139sentences in which they occur arise because some sub-jects will use a given lexical item in many subsequentsentences once it has "worked".DiscussionIn addition to replicating the results released by TI, us-ing a setup similar to TI's, we have shown the effect ofaltering various aspects of the experimental setup, in-cluding scenario specificity, subject familiarization andrestricting the vocabulary.We believe that our results indicate that we have suc-ceeded in implementing a functional equivalent of theTI data collection system.
The one major exception tothis claim is the observed discrepancy in the word fre-quency distributions.
This discrepancy can be remediedby avoiding any sample sentences from the domain whileinstructing subjects.In assessing scenario specificity, we found no differ-ences on either yield measures (time to complete task,utterances per task, words per task, etc.)
or on qualitymeasures (error message rates, cancellation rates) be-tween subjects in the unconstrained task condition andthose in the constrained (specific) task condition.
Inlight of this, one might argue for adopting specific sce-narios on the basis of the benefits gained by knowingsubjects are interpreting the task the same way (in ef-fect, are performing the same task) and by obtainingdata useful for both analysis of isolated queries and ofdialogue.Our most significant results pertain to subject famil-iarization.
In two separate xperiments using two verydifferent interfaces and procedures, we demonstrated theimpact of subject familiarization with the system: sub-jects less familiar with the system produced longer ut-terances, needed more time to complete the task, andproduced fewer utterances per subject hour.
The timeto familiarize subjects with the system (5 to 6 minutes)was short relative to the gains in subject efficiency (17minutes saved on average in subject time to completetask).Our Fixed Vocabulary Condition showed that sub-jects can adapt quickly to a restricted vocabulary with-out increasing task time: subjects in the Fixed Vocabu-lary Condition did not take longer to complete the taskor to plan each utterance than those in the unlimited-vocabulary conditions, so the constraint doesn't appearto slow them down unnaturally or lower the yield ofthe experimental session.
It is worth noting that thesesubjects howed significant improvement in the numberof out-of-vocabulary error messages received uring themain task (3.8 in 24.29 minutes) as compared to thetraining scenario (2.0 errors in 7.27 minutes).
This sup-ports the position that subjects can adapt to-using alimited vocabulary.
This result may be very importantin the development of scalable technologies that will fiton a variety of platforms.We found no systematic differences in the answers ub-jects provided to the questionnaire we presented to themafter the session.
The subjective xperience of the sub-jects in the various conditions, then, seems to have beenabout the same.The goals of designing an appropriate spoken languagesystem can sometimes conflict with the goM of collect-ing data for evaluation of spoken database queries.
Thatis, some major causes of errors (e.g., out-of-vocabularyitems, out-of-domain queries) may disappear with asmall amount of either detailed instruction or subjectfamiliarization.
However, we are convinced that it ispossible to find ways of coordinating the two endeavors.For example, the needs of both dialogue analysis andof query-answer pairs for evaluation can be met using amore specific scenario; the needs of restricted vocabularycan be met by providing consistent feedback; and thelarge effect of subject familiarization can be addressedby spending a short time in the room with the subjectto answer questions as the subject works on a task.We plan to continue these experiments to help us de-sign an appropriate human-machine interface.
In ournext set of experiments we will include a revised gram-mar for NLParse that reduces the number of words thewizard needs to produce by about 35% (on "cheapest"constructions it can reduce the number of words to abouta quarter of the number that would be needed withoutthe modification).
Other experiments we are planninginclude the reformatting of tables sent by Oracle (thehigh percentage of queries concerning the meanings ofvarious column headings indicate that much could bedone to improve the user interface in this area), andsome variations on the use of push-to-talk mechanism.We will also be running repeat subjects to test the effectof longer use of the system on the resulting data.AcknowledgementsThe authors gratefully acknowledge TI and CharlesHemphill, in particular, for helping us to get a licenseto NLParse, for providing much of the related softwareand data, and for helpful responses to our endless pleasfor assistance.
We also thank CMU for providing record-ing and playback software for the NEXT machine.
Thisresearch was funded by DARPA under Office of NavalResearch contract N00014-90-C-0085.Re ferences\[1\] J. Kowtko, P. Price and S. Tepper, "Data Collectionand Analysis in the Air Travel Planning Domain,"DARPA Speech and Natural Language Workshop,October 1989.\[2\] C. Hemphill, J. Godfrey,, and G. Doddington, "TheATIS Spoken Language Systems Pilot Corpus," thisvolume.140
