Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 791?801,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAdditive Neural Networks for Statistical Machine TranslationLemao Liu1, Taro Watanabe2, Eiichiro Sumita2, Tiejun Zhao11School of Computer Science and TechnologyHarbin Institute of Technology (HIT), Harbin, China2National Institute of Information and Communication Technology (NICT)3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan{lmliu | tjzhao}@mtlab.hit.edu.cn{taro.watanabe | eiichiro.sumita}@nict.go.jpAbstractMost statistical machine translation(SMT) systems are modeled using a log-linear framework.
Although the log-linearmodel achieves success in SMT, it stillsuffers from some limitations: (1) thefeatures are required to be linear withrespect to the model itself; (2) featurescannot be further interpreted to reachtheir potential.
A neural network isa reasonable method to address thesepitfalls.
However, modeling SMT with aneural network is not trivial, especiallywhen taking the decoding efficiencyinto consideration.
In this paper, wepropose a variant of a neural network, i.e.additive neural networks, for SMT to gobeyond the log-linear translation model.In addition, word embedding is employedas the input to the neural network, whichencodes each word as a feature vector.Our model outperforms the log-lineartranslation models with/without embed-ding features on Chinese-to-English andJapanese-to-English translation tasks.1 IntroductionRecently, great progress has been achieved inSMT, especially since Och and Ney (2002) pro-posed the log-linear model: almost all the state-of-the-art SMT systems are based on the log-linearmodel.
Its most important advantage is that arbi-trary features can be added to the model.
Thus,it casts complex translation between a pair of lan-guages as feature engineering, which facilitates re-search and development for SMT.Regardless of how successful the log-linearmodel is in SMT, it still has some shortcomings.This joint work was done while the first author visitedNICT.On the one hand, features are required to be lin-ear with respect to the objective of the translationmodel (Nguyen et al, 2007), but it is not guaran-teed that the potential features be linear with themodel.
This induces modeling inadequacy (Duhand Kirchhoff, 2008), in which the translation per-formance may not improve, or may even decrease,after one integrates additional features into themodel.
On the other hand, it cannot deeply in-terpret its surface features, and thus can not ef-ficiently develop the potential of these features.What may happen is that a feature p does initiallynot improve the translation performance, but aftera nonlinear operation, e.g.
log(p), it does.
Thereason is not because this feature is useless but themodel does not efficiently interpret and representit.
Situations such as this confuse explanations forfeature designing, since it is unclear whether sucha feature contributes to a translation or not.A neural network (Bishop, 1995) is a reason-able method to overcome the above shortcomings.However, it should take constraints, e.g.
the de-coding efficiency, into account in SMT.
Decod-ing in SMT is considered as the expansion oftranslation states and it is handled by a heuris-tic search (Koehn, 2004a).
In the search pro-cedure, frequent computation of the model scoreis needed for the search heuristic function, whichwill be challenged by the decoding efficiency forthe neural network based translation model.
Fur-ther, decoding with non-local (or state-dependent)features, such as a language model, is also a prob-lem.
Actually, even for the (log-) linear model,efficient decoding with the language model is nottrivial (Chiang, 2007).In this paper, we propose a variant of neural net-works, i.e.
additive neural networks (see Section3 for details), for SMT.
It consists of two com-ponents: a linear component which captures non-local (or state dependent) features and a non-linearcomponent (i.e., neural nework) which encodes lo-791Xte ?
X?}
\XXfriendly cooperationover the last yearsFigure 1: A bilingual tree with two synchronous rules, r1 : X ?
??
}\; friendly cooperation?and r2 : X ?
?te?
X;X over the last years?.
The inside rectangle denotes the partial derivationd1 = {r1} with the partial translation e1 =?friendly cooperation?, and the outside rectangle denotes thederivation d2 = {r1, r2} with the translation e2=?friendly cooperation over the last years?.cal (or state independent) features.
Compared withthe log-linear model, it has more powerful expres-sive abilities and can deeply interpret and repre-sent features with hidden units in neural networks.Moreover, our method is simple to implement andits decoding efficiency is comparable to that of thelog-linear model.
We also integrate word embed-ding into the model by representing each word asa feature vector (Collobert and Weston, 2008).Because of the thousands of parameters and thenon-convex objective in our model, efficient train-ing is not simple.
We propose an efficient train-ing methodology: we apply the mini-batch conju-gate sub-gradient algorithm (Le et al, 2011) to ac-celerate the training; we also propose pre-trainingand post-training methods to avoid poor local min-ima.
The biggest contribution of this paper is thatit goes beyond the log-linear model and proposes anon-linear translation model instead of re-rankingmodel (Duh and Kirchhoff, 2008; Sokolov et al,2012).On both Chinese-to-English and Japanese-to-English translation tasks, experiment results showthat our model can leverage the shortcomings suf-fered by the log-linear model, and thus achievessignificant improvements over the log-linear basedtranslation.2 Log-linear Model, Revisited2.1 Log-linear Translation ModelOch and Ney (2002) proposed the log-linear trans-lation model, which can be formalized as follows:P(e, d|f ;W ) = exp{W> ?
h(f, e, d)}?e?,d?
exp{W> ?
h(f, e?, d?)}
, (1)where f denotes the source sentence, ande(e?)
denotes its translation candidate; d(d?
)is a derivation over the pair ?f, e?, i.e.,a collection of synchronous rules for Hierogrammar (Chiang, 2005), or phrase pairs inMoses (Koehn et al, 2007); h(f, e, d) =(h1(f, e, d), h2(f, e, d), ?
?
?
, hK(f, e, d))> is aK-dimensional feature vector defined on the tu-ple ?f, e, d?
; W = (w1, w2, ?
?
?
, wK)> is a K-dimensional weight vector of h, i.e., the parame-ters of the model, and it can be tuned by the toolkitMERT (Och, 2003).
Different from Brown?sgenerative model (Brown et al, 1993), the log-linear model does not assume strong indepen-dency holds, and allows arbitrary features to beintegrated into the model easily.
In other words,it can transform complex language translation intofeature engineering: it can achieve high translationperformance if reasonable features are chosen andappropriate parameters are assigned for the weightvector.2.2 Decoding By SearchGiven a source sentence f and a weight W , de-coding finds the best translation candidate e?
viathe programming problem:?e?, d??
= arg maxe,dP(e, d|f ;W )= arg maxe,d{W> ?
h(f, e, d)}.
(2)Since the range of ?e, d?
is exponential with re-spect to the size of f , the exact decoding is in-tractable and an inexact strategy such as beamsearch is used instead in practice.The idea of search for decoding can be shownin Figure 1: it encodes each search state as apartial translation together with its derivation, e.g.
?e1, d1?
; it consequently expands the states fromthe initial (empty) state to the end state ?e2, d2?according to the translation rules r1 and r2.
Dur-ing the state expansion process, the score wi ?792hi(f, e, d) for a partial translation is calculated re-peatedly.
In the log-linear model, if hi(f, e, d) isa local feature, the calculation of its score wi ?hi(f, e, d) has a substructure, and thus it can becalculated with dynamic programming which ac-celerates its decoding.
For the non-local featuressuch as the language model, Chiang (2007) pro-posed a cube-pruning method for efficient decod-ing.
The main reason why cube-pruning works isthat the translation model is linear and the modelscore for the language model is approximatelymonotonic (Chiang, 2007).3 Additive Neural Networks3.1 MotivationAlthough the log-linear model has achieved greatprogress for SMT, it still suffers from some pit-falls: it requires features be linear with the modeland it can not interpret and represent featuresdeeply.
The neural network model is a reason-able method to overcome these pitfalls.
However,the neural network based machine translation is farfrom easy.As mentioned in Section 2, the decoding proce-dure performs an expansion of translation states.Firstly, let us consider a simple case in neural net-work based translation where all the features in thetranslation model are independent of the transla-tion state, i.e.
all the components of the vectorh(f, e, d) are local features.
In this way, we caneasily define the following translation model witha single-layer neural network:S(f, e, d;W,M,B) =W> ?
?
(M ?
h(f, e, d) +B), (3)where M ?
Ru?K is a matrix, and B ?
Ru is avector, i.e.
bias; ?
is a single-layer neural networkwith u hidden units, i.e.
an element wise sigmoidfunction sigmoid(x) = 1/(1 + exp(?x)).
Forconsistent description in the rest, we also representEq.
(3) as a function of a feature vector h, i.e.S(h;W,M,B) = W> ?
?
(M ?
h+B).Now let us consider the search procedure withthe model in Eq.
(3) using Figure 1 as our ex-ample.
Suppose the current translation state is en-coded as ?e1, d1?, which is expanded into ?e2, d2?using the rule r2 (d2 = d1 ?
{r2}).
Since h isstate-independent, h(f, e2, d2) = h(f, e1, d1) +h(r2).
However, since S(f, e, d;W,M,B) is non-decomposable as a linear model, there is no sub-structure for calculating S(f, e2, d2;W,M,B),and one has to re-calculate it via Eq.
(3) evenif the score of S(f, e1, d1;M,B) for its previousstate ?e1, d1?
is available.
When the size of the pa-rameter (W,M,B) is relatively large, it will be achallenge for the decoding efficiency.In order to keep the substructure property,S(f, e2, d2;W,M,B) should be represented asF(S(f, e1, d1;W,M,B);S(h(r2);M,B)) by afunction F .
For simplicity, we suppose that theadditive property holds in F , and then we can ob-tain a new translation model via the following re-cursive equation:S(f, e2, d2;W,M,B) = S(f, e1, d1;W,M,B)+ S(h(r2);W,M,B).
(4)Since the above model is defined only on lo-cal features, it ignores the contributions from non-local features.
Actually, existing works empir-ically show that some non-local features, espe-cially language model, contribute greatly to ma-chine translation.Scoring for non-local features such as a n-gram language model is not easily done.
In log-linear translation model, Chiang (2007) proposeda cube-pruning method for scoring the languagemodel.
The premise of cube-pruning is that thelanguage model score is approximately monotonic(Chiang, 2007).
However, if scoring the languagemodel with a neural network, this premise is diffi-cult to hold.
Therefore, one of the solutions is topreserve a linear model for scoring the languagemodel directly.3.2 DefinitionAccording to the above analysis, we proposea variant of a neural network model for ma-chine translation, and we call it Additive NeuralNetworks or AdNN for short.The AdNN model is a combination of a lin-ear model and a neural network: non-local fea-tures, e.g.
LM, are linearly modeled for the cube-pruning strategy, and local features are modeledby the neural network for deep interpretation andrepresentation.
Formally, the AdNN based transla-tion model is discriminative but non-probabilistic,and it can be defined as follows:S(f, e, d; ?)
= W> ?
h(f, e, d)+?r?dW ?> ?
?
(M ?
h?
(r) +B), (5)793where h and h?
are feature vectors with dimensionK and K ?
respectively, and each component of h?is a local feature which can be defined on a ruler : X ?
?
?, ??
; ?
= (W,W ?,M,B) is the modelparameters with M ?
Ru?K?
.
In this paper, wefocus on a single-layer neural network for its sim-plicity, and one can similarly define ?
as a multi-layer neural network.Again for the example shown in Figure 1, themodel score defined in Eq.
(5) for the pair ?e2, d2?can be represented as follows:S(f, e2, d2; ?)
= W> ?
h(f, e2, d2)+W ?>??
(M ?h?
(r1)+B)+W ?>??
(M ?h?(r2)+B).Eq.
(5) is similar to both additive models (Bujaet al, 1989) and generalized additive neural net-works (Potts, 1999): it consists of many additiveterms, and each term is either a linear or a non-linear (a neural network) model.
That is the rea-son why our model is called ?additive neural net-works?.
Of course, our model still has some dif-ferences from both of them.
Firstly, our model isdecomposable with respect to rules instead of thecomponent variables.
Secondly, some of its addi-tive terms share the same parameters (M,B).There are also strong relationships betweenAdNN and the log-linear model.
If we considerthe parameters (M,B) as constant and ?
(M ?h?
(r) +B) as a new feature vector, then AdNN isreduced to a log-linear model.
Since both (M,B)and (W,W ?)
are parameters in AdNN, our modelcan jointly learn the feature ?
(M ?h?
(r) +B) andtune the weight (W,W ?)
of the log-linear modeltogether.
That is different from most works un-der the log-linear translation framework, whichfirstly learn features or sub-models and then tunethe log-linear model including the learned featuresin two separate steps.
By joint training, AdNN canlearn the features towards the translation evalua-tion metric, which is the main advantage of ourmodel over the log-linear model.In this paper, we apply our AdNN model to hi-erarchical phrase based translation, and it can besimilarly applied to phrase-based or syntax-basedtranslation.
Similar to Hiero (Chiang, 2005), thefeature vector h in Eq.
(5) includes 8 default fea-tures, which consist of translation probabilities,lexical translation probabilities, word penalty, gluerule penalty, synchronous rule penalty and lan-guage model.
These default features are includedbecause they empirically perform well in the log-linear model.
For the local feature vector h?
in Eq(5), we employ word embedding features as de-scribed in the following subsection.3.3 Word Embedding features for AdNNWord embedding can relax the sparsity introducedby the lexicalization in NLP, and it improves thesystems for many tasks such as language model,named entity recognition, and parsing (Collobertand Weston, 2008; Turian et al, 2010; Collobert,2011).
Here, we propose embedding features forrules in SMT by combining word embeddings.Firstly, we will define the embedding for thesource side ?
of a rule r : X ?
?
?, ??.
LetVS be the vocabulary in the source language withsize |VS |; Rn?|VS | be the word embedding matrix,each column of which is the word embedding (n-dimensional vector) for the corresponding word inVS ; and maxSource be the maximal length of ?for all rules.
We further assume that the ?
for allrules share the same length as maxSource; other-wise, we add maxSource ?
|?| words ?NULL?to the end of ?
to obtain a new ?.
We define theembedding of ?
as the concatenation of the wordembedding of each word in ?.
In particular, forthe non-terminal in ?, we define its word embed-ding as the vector whose components are 0.1; andwe define the word embedding of ?NULL?
as 0.Then, we similarly define the embedding for thetarget side of a rule, given an embedding matrixfor the target vocabulary.
Finally, we define theembedding of a rule as the concatenation of theembedding of its source and target sides.In this paper, we apply the word embedding ma-trices from the RNNLM toolkit (Mikolov et al,2010) with the default settings: we train two RNNlanguage models on the source and target sides oftraining corpus, respectively, and then we obtaintwo matrices as their by-products1.
It would bepotentially better to train the word embedding ma-trix from a much larger corpus as (Collobert andWeston, 2008), and we will leave this as a futuretask.3.4 DecodingSubstituting the P(e, d|f ;W ) in Eq.
(2) withS(f, e, d; ?)
in Eq.
(5), we can obtain its corre-1In the RNNLM toolkit, the default dimension for wordembedding is n = 30.
In our experiments, the maximallength of ?
and ?
are 5 and 12 respectively.
Thus the di-mension for h?
is K?
= 30?
(5 + 12) = 510.794sponding decoding formula:?e?, d??
= arg maxe,dS(f, e, d; ?
).Given the model parameter ?
= (W,W ?,M,B), ifwe consider (M,B) as constant and ?
(M ?h?
(r)+B) as an additional feature vector besides h, thenEq.
(5) goes back to being a log-linear model withparameter (W,W ?).
In this way, the decoding forAdNN can share the same search strategy and cubepruning method as the log-linear model.4 Training Method4.1 Training ObjectiveFor the log-linear model, there are various tun-ing methods, e.g.
MERT (Och, 2003), MIRA(Watanabe et al, 2007; Chiang et al, 2008), PRO(Hopkins and May, 2011) and so on, which itera-tively optimize a weight such that, after re-rankinga k-best list of a given development set with thisweight, the loss of the resulting 1-best list is mini-mal.
In the extreme, if the k-best list consists onlyof a pair of translations ?
?e?, d?
?, ?e?, d??
?, the de-sirable weight should satisfy the assertion: if theBLEU score of e?
is greater than that of e?, thenthe model score of ?e?, d??
with this weight willbe also greater than that of ?e?, d??.
In this paper,a pair ?e?, e??
for a source sentence f is called asa preference pair for f .
Following PRO, we definethe following objective function under the max-margin framework to optimize the AdNN model:12 ??
?2 + ?N?f?e?,d?,e?,d??
(f, e?, d?, e?, d?
; ?
), (6)with?(?)
= max{S(f, e?, d?
; ?)?
S(f, e?, d?
; ?)
+ 1, 0}where f is a source sentence in a given devel-opment set, and ?
?e?, d?
?, ?e?, d???
is a preferencepair for f ; N is the number of all preference pairs;?
> 0 is a regularizer.4.2 Optimization AlgorithmSince there are thousands of parameters in Eq.
(6)and the tuning in SMT will minimize Eq.
(6) re-peatedly, efficient and scalable optimization meth-ods are required.
Following Le et al (2011),we apply the mini-batch Conjugate Sub-Gradient(mini-batch CSG) method to minimize Eq.
(6).Compared with the sub-gradient descent, mini-batch CSG has some advantages: (1) it can ac-celerate the calculation of the sub-gradient sinceit calculates the sub-gradient on a subset of pref-erence pairs (i.e.
mini-batch) instead of all of thepreference pairs; (2) it reduces the number of iter-ations since it employs the conjugate informationbesides the sub-gradient.
Algorithm 1 shows theprocedure to minimize Eq.
(6).Algorithm 1 Mini-batch conjugate subgradientInput: ?1, T , CGIter, batch-size, k-best-list1: for all t such that 1 ?
t ?
T do2: Sample mini-batch preference pairs withsize batch-size from k-best-list3: Calculate some quantities for CG, e.g.training objective Obj, subgradient ?, ac-cording to Eq.
(6) defined over the sampledpreference pairs4: ?t+1 = CG(?t, Obj,?, CGIter)5: end forOutput: ?T+1In detail, line 2 in Algorithm 1 firstly fol-lows PRO to sample a set of preference pairsfrom k-best-list, and then uniformly samplesbatch-size pairs from the preference pair set.
Line3 calculates some quantities for CG, and Line 4calls a CG optimizer 2 and obtains ?t+1.
At theend of the algorithm, it returns the result ?T+1.
Inthis work, we set the maximum number of CG iter-ations, CGIter, to a small number, which means?t+1 will be returned withinCGIter iterations be-fore the CG converges, for faster learning.4.3 Pre-Training and Post-TrainingSince Eq.
(6) is non-linear, there are many localminimal solutions.
Actually, this problem is inher-ent and is one many works based on the neural net-work for other NLP tasks such as language modeland parsing, also suffer from.
And these worksempirically show that some pre-training methods,which provide a reasonable initial solution, canimprove the performance.
Observing the structureof Eq.
(5) and the relationships between our modeland a log-linear model, we propose the followingsimple pre-training method.2In implementation, we call the CG toolkit (Hager andZhang, 2006), which requires overloading objective and sub-gradient functions.
For easier description, we substitute over-loading functions and transform the value of functions in thepseudo-code.795If we set W ?
= 0, the model defined in Eq.
(5)can be regarded as a log-linear model with featuresh.
Therefore, we pre-trainW using MERT or PROby holding W ?
= 0, and use (W,W ?
= 0,M,B)as an initializer3 for Algorithm 1.Although the above pre-training would providea reasonable solution, Algorithm 1 may still fallinto local minima.
We also propose a post-trainingmethod: after obtaining a solution with Algorithm1, we modify this solution slightly to get a newsolution.
The idea of the post-training method issimilar to that of the pre-training method.
Suppose?
= (W,W ?,M,B) be the solution obtained fromAlgorithm 1.
If we consider both M and B to beconstant, the Eq.
(5) goes back to the log-linearmodel whose features are (h, ?
(M ?
h?
+B)) andparameters are (W,W ?).
Again, we train the pa-rameters (W,W ?)
with MERT or PRO and get thenew parameters (W?
, W?
?).
Therefore, we can set?
= (W?
, W?
?,M,B) as the final solution for Eq.(6).
The advantage of post-training is that it op-timizes a convex programming derived from theoriginal nonlinear (non-convex) programming inEq.
(6), and thus it may decrease the risk of poorlocal optima.4.4 Training AlgorithmAlgorithm 2 Training AlgorithmInput: MaxIter, a dev set, parameters (e.g.
?
)for Algorithm 11: Pre-train to obtain ?1 = (W,W ?
= 0,M,B)as the initial parameter2: for all i such that 1 ?
i ?MaxIter do3: Decode with ?i on the dev set and merge allk-best-lists4: Run Algorithm 1 based on the merged k-best-list to obtain ?i+15: end for6: Post-train based on ?MaxIter+1 to obtain ?Output: ?The whole training for the AdNN model is sum-marized in Algorithm 2.
Given a development set,we first run pre-training to obtain an initial param-eter ?1 for Algorithm 1 in line 1.
Secondly, it it-eratively performs decoding and optimization forMaxIter times in the loop from line 2 to line 5: itdecodes with the parameter ?i and merges all the3To avoid the symmetry in the solution, we sample a verysmall (M,B) from the gaussian distribution in practice in-stead of setting (M,B) = 0.k-best-lists in line 3; and it then runs Algorithm 1to optimize ?i+1.
Thirdly, it runs the post-trainingto get the result ?
based on ?MaxIter+1.Of course, we can run post-training after run-ning Algorithm 1 at each iteration i. However,since each pass of post-training (e.g.
PRO) takesseveral hours because of multiple decoding times,we run it only once, at the end of the iterationsinstead.5 Experiments and Results5.1 Experimental SettingWe conduct our experiments on the Chinese-to-English and Japanese-to-English translation tasks.For the Chinese-to-English task, the training datais the FBIS corpus (news domain) with about240k sentence pairs; the development set is theNIST02 evaluation data; the development test setis NIST05; and the test datasets are NIST06, andNIST08.
For the Japanese-to-English task, thetraining data with 300k sentence pairs is from theNTCIR-patent task (Fujii et al, 2010); the devel-opment set, development test set, and two test setsare averagely extracted from a given developmentset with 4000 sentences, and these four datasetsare called test1, test2, test3 and test4, respectively.We run GIZA++ (Och and Ney, 2000) on thetraining corpus in both directions (Koehn et al,2003) to obtain the word alignment for each sen-tence pair.
Using the SRILM Toolkits (Stolcke,2002) with modified Kneser-Ney smoothing, wetrain a 4-gram language model for the Chinese-to-English task on the Xinhua portion of the EnglishGigaword corpus and a 4-gram language modelfor the Japanese-to-English task on the target sideof its training data.
In our experiments, the transla-tion performances are measured by case-sensitiveBLEU4 metric4 (Papineni et al, 2002).
The sig-nificance testing is performed by paired bootstrapre-sampling (Koehn, 2004b).We use an in-house developed hierarchicalphrase-based translation (Chiang, 2005) for ourbaseline system, which shares the similar settingas Hiero (Chiang, 2005), e.g.
beam-size=100, k-best-size=100, and is denoted as L-Hiero to em-phasize its log-linear model.
We tune L-Hierowith two methods MERT and PRO implementedin the Moses toolkit.
On the same experiment set-tings, the performance of L-Hiero is comparable4We use mteval-v13a.pl as the evaluation tool(Ref.http://www.itl.nist.gov/iad/mig/tests/mt/2008/scoring.html).796Seconds/SentL-Hiero 1.77AdNN-Hiero-E 1.88Table 1: The decoding time comparison onNIST05 between L-Hiero and AdNN-Hiero-E.to that of Moses: on the NIST05 test set, L-Hieroachieves 25.1 BLEU scores and Moses achieves24.8.
Further, we integrate the embedding fea-tures (See Section 3.3) into the log-linear modelalong with the default features as L-Hiero, whichis called L-Hiero-E.
Since L-Hiero-E has hun-dreds of features, we use PRO as its tuning toolkit.AdNN-Hiero-E is our implementation of theAddNN model with embedding features, as dis-cussed in Section 3, and it shares the samecodebase and settings as L-Hiero.
We adoptthe following setting for training AdNN-Hiero-E: u=10; batch-size=1000 and CGiter=3, as re-ferred in (Le et al, 2011), and T=200 in Algo-rithm 1; the pre-training and post-training meth-ods as PRO; the regularizer ?
in Eq.
(6) as 10and 30, and MaxIter as 16 and 20 in Algorithm2, for Chinese-to-English and Japanese-to-Englishtasks, respectively.
Although there are several pa-rameters in AdNN which may limit its practica-bility, according to many of our internal studies,most parameters are insensitive to AdNN except?
and MaxIter, which are common in other tun-ing toolkits such as MIRA and can be tuned5 on adevelopment test dataset.Since both MERT and PRO tuning toolkits in-volve randomness in their implementations, allBLEU scores reported in the experiments are theaverage of five tuning runs, as suggested by Clarket al (2011) for fairer comparisons.
For AdNN,we report the averaged scores of five post-trainingruns, but both pre-training and training are per-formed only once.5.2 Results and AnalysisAs discussed in Section 3, our AdNN-Hiero-Eshares the same decoding strategy and pruningmethod as L-Hiero.
When compared with L-Hiero, decoding for AdNN-Hiero-E only needsadditional computational times for the features inthe hidden units, i.e.
?
(M ?
h?
(r) + B).
Since5For easier tuning, we tuned these two parameters on agiven development test set without post-training in Algorithm2.Chinese-to-EnglishNIST05 NIST06 NIST08L-Hiero MERT 25.10+ 24.46+ 17.42+PRO 25.57+ 25.27+ 18.33+L-Hiero-E PRO 24.80+ 24.46+ 18.20+AdNN-Hiero-E 26.37 25.93 19.42Japanese-to-Englishtest2 test3 test4L-Hiero MERT 24.35+ 25.62+ 23.68+PRO 24.38+ 25.55+ 23.66+L-Hiero-E PRO 24.47+ 25.86+ 24.03+AdNN-Hiero-E 25.14 26.32 24.45Table 2: The BLEU comparisons between AdNN-Hiero-E and Log-linear translation models onthe Chinese-to-English and Japanese-to-Englishtasks.
+ means the comparison is significant overAdNN-Hiero-E with p < 0.05.these features are not dependent on the transla-tion states, they are computed and saved to mem-ory when loading the translation model.
Duringdecoding, we just look up these scores insteadof re-calculating them on the fly.
Therefore, thedecoding efficiency of AdNN-Hiero-E is almostthe same as that of L-Hiero.
As shown in Table1 the average decoding time for L-Hiero is 1.77seconds/sentence while that for AdNN-Hiero-E is1.88 seconds/sentence on the NIST05 test set.Word embedding features can improve the per-formance on other NLP tasks (Turian et al, 2010),but its effect on log-linear based SMT is not as ex-pected.
As shown in Table 2, L-Hiero-E gains lit-tle over L-Hiero for the Japanese-to-English task,and even decreases the performance over L-Hierofor the Chinese-to-English task.
These results fur-ther prove our claim in Section 1, i.e.
the log-linear model requires the features to be linear withthe model and thus limits its expressive abilities.However, after the single-layer non-linear opera-tor (sigmoid functions) on the embedding featuresfor deep interpretation and representation, AdNN-Hiero-E gains improvements over both L-Hieroand L-Hiero-E, as depicted in Table 2.
In detail,for the Chinese-to-English task, AdNN-Hiero-Eimproves more than 0.6 BLEU scores over L-Hiero on both test sets: the gains over L-Hierotuned with PRO are 0.66 and 1.09 on NIST06 andNIST08, respectively, and the gains over L-Hierotuned with MERT are even more.
Similar re-sults are achieved on the Japanese-to-English task.AdNN-Hiero-E gains about 0.7 BLEU scores on797Chinese-to-EnglishNIST05 NIST06 NIST08L-Hiero 25.57+ 25.27+ 18.33+AdNN-Hiero-E 26.37 25.93 19.42AdNN-Hiero-D 26.21 26.07 19.54Japanese-to-Englishtest2 test3 test4L-Hiero 24.38 25.55 23.66AdNN-Hiero-E 25.14+ 26.32+ 24.45+AdNN-Hiero-D 24.42 25.46 23.73Table 3: The effect of different feature setting onAdNN model.
+ means the comparison is signifi-cant over AdNN-Hiero-D with p < 0.05.both test sets.In addition, to investigate the effect of differ-ent feature settings on AdNN, we alternatively de-sign another setting for h?
in Eq.
(5): we usethe default features for both h?
and h. In partic-ular, the language model of a rule for h?
is lo-cally calculated without the contexts out of therule as described in (Chiang, 2007).
We call theAdNN model with this setting AdNN-Hiero-D6.Although there are serious overlaps between h andh?
for AdNN-Hiero-D which may limit its gener-alization abilities, as shown in Table 3, it is stillcomparable to L-Hiero on the Japanese-to-Englishtask, and significantly outperforms L-Hiero on theChinese-to-English translation task.
To investigatethe reason why the gains for AdNN-Hiero-D onthe two different translation tasks differ, we cal-culate the perplexities between the target side oftraining data and test datasets on both translationtasks.
We find that the perplexity of the 4-gramlanguage model for the Chinese-to-English task is321.73, but that for the Japanese-to-English taskis only 81.48.
Based on these similarity statistics,we conjecture that the log-linear model does notfit well for difficult translation tasks (e.g.
transla-tion task on the news domain).
The problem seemsto be resolved by simply alternating feature repre-sentations through non-linear models, i.e.
AddN-Hiero-D, even with single-layer networks.6 Related WorkNeural networks have achieved widespread at-tentions in many NLP tasks, e.g.
the language6All its parameters are shared with AdNN-Hiero-E except?
and MaxIter, which are tuned on the development testdatasets.model (Bengio et al, 2003); POS, Chunking,NER, and SRL (Collobert and Weston, 2008);Parsing (Collobert and Weston, 2008; Socher etal., 2011); and Machine transliteration (Deselaerset al, 2009).
Our work is, of course, highly mo-tivated by these works.
Unlike these works, wepropose a variant neural network, i.e.
additive neu-ral networks, starting from SMT itself and takingboth of the model definition and its inference (de-coding) together into account.Our variant of neural network, AdNN, is highlyrelated to both additive models (Buja et al, 1989)and generalized additive neural networks (Potts,1999; Waal and Toit, 2007), in which an additiveterm is either a linear model or a neural network.Unlike additive models and generalized additiveneural networks, our model is decomposable withrespect to translation rules rather than its compo-nent variables considering the decoding efficiencyof machine translation; and it allows its additiveterms of neural networks to share the same param-eters for a compact structure to avoid sparsity.The idea of the neural network in machinetranslation has already been pioneered in previ-ous works.
Castan?o et al (1997) introduced a neu-ral network for example-based machine transla-tion.
In particular, Son et al (2012) and Schwenk(2012) employed a neural network to model thephrase translation probability on the rule level?
?, ??
instead of the bilingual sentence level ?f, e?as in Eq.
(5), and thus they did not go beyond thelog-linear model for SMT.There are also works which exploit non-linearmodels in SMT.
Duh and Kirchhoff (2008) pro-posed a boosting re-ranking algorithm usingMERT as a week learner to improve the model?sexpressive abilities; Sokolov et al (2012) simi-larly proposed a boosting re-ranking method fromthe ranking perspective rather than the classifica-tion perspective.
Instead of considering the re-ranking task in SMT, Xiao et al (2010) employeda boosting method for the system combination inSMT.
Unlike their post-processing models (eithera re-ranking or a system combination model) inSMT, we propose a non-linear translation modelwhich can be easily incorporated into the existingSMT framework.7 Conclusion and Future WorkIn this paper, we go beyond the log-linear modelfor SMT and propose a novel AdNN based trans-798lation model.
Our model overcomes some of theshortcomings suffered by the log-linear model:linearity and the lack of deep interpretation andrepresentation in features.
One advantage of ourmodel is that it jointly learns features and tunesthe translation model and thus learns features to-wards the translation evaluation metric.
Addi-tionally, the decoding of our model is as efficientas that of the log-linear model.
For Chinese-to-English and Japanese-to-English translation tasks,our model significantly outperforms the log-linearmodel, with the help of word embedding.We plan to explore more work on the additiveneural networks in the future.
For example, wewill train word embedding matrices for source andtarget languages from a larger corpus, and takeinto consideration the bilingual information, forinstance, word alignment; the multi-layer neuralnetwork within the additive neural networks willbe also investigated in addition to the single-layerneural network; and we will test our method onother translation tasks with larger training data aswell.AcknowledgmentsWe would like to thank our colleagues in bothHIT and NICT for insightful discussions, TomasMikolov for the helpful discussion about the wordembedding in RNNLM, and three anonymous re-viewers for many invaluable comments and sug-gestions to improve our paper.
This work issupported by National Natural Science Founda-tion of China (61173073, 61100093, 61073130,61272384), the Key Project of the NationalHigh Technology Research and Development Pro-gram of China (2011AA01A207), and the Fun-damental Research Funds for Central Universities(HIT.NSRIF.2013065).ReferencesYoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
J. Mach.
Learn.
Res., 3:1137?1155,March.Christopher M. Bishop.
1995.
Neural Networks forPattern Recognition.
Oxford University Press, Inc.,New York, NY, USA.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: pa-rameter estimation.
Comput.
Linguist., 19:263?311,June.Andreas Buja, Trevor Hastie, and Robert Tibshirani.1989.
Linear smoothers and additive models.
TheAnnals of Statistics, 17:453?510.M.
Asuncin Castan?o, Francisco Casacuberta, and En-rique Vidal.
1997.
Machine translation using neu-ral networks and finite-state models.
In TMI, pages160?167.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proc.
of EMNLP.
ACL.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL ?05, pages 263?270, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Comput.
Linguist., 33(2):201?228, June.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: controlling for opti-mizer instability.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies: short pa-pers - Volume 2, HLT ?11, pages 176?181, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.R.
Collobert and J. Weston.
2008.
A unified architec-ture for natural language processing: Deep neuralnetworks with multitask learning.
In InternationalConference on Machine Learning, ICML.R.
Collobert.
2011.
Deep learning for efficient dis-criminative parsing.
In AISTATS.Thomas Deselaers, Sas?a Hasan, Oliver Bender, andHermann Ney.
2009.
A deep learning approachto machine transliteration.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, StatMT ?09, pages 233?241, Stroudsburg, PA,USA.
Association for Computational Linguistics.Kevin Duh and Katrin Kirchhoff.
2008.
Beyond log-linear models: Boosted minimum error rate trainingfor n-best re-ranking.
In Proceedings of ACL-08:HLT, Short Papers, pages 37?40, Columbus, Ohio,June.
Association for Computational Linguistics.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2010.
Overview of the patenttranslation task at the ntcir-8 workshop.
In InProceedings of the 8th NTCIR Workshop Meet-ing on Evaluation of Information Access Technolo-gies: Information Retrieval, Question Answeringand Cross-lingual Information Access, pages 293?302.799William W. Hager and Hongchao Zhang.
2006.
Algo-rithm 851: Cg descent, a conjugate gradient methodwith guaranteed descent.
ACM Trans.
Math.
Softw.,32(1):113?137, March.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1352?1362, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of HLT-NAACL.
ACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Philipp Koehn.
2004a.
Pharaoh: A beam search de-coder for phrase-based statistical machine transla-tion models.
In AMTA.Philipp Koehn.
2004b.
Statistical significance tests formachine translation evaluation.
In Proc.
of EMNLP.ACL.Quoc V. Le, Jiquan Ngiam, Adam Coates, AhbikLahiri, Bobby Prochnow, and Andrew Y. Ng.
2011.On optimization methods for deep learning.
InICML, pages 265?272.Tomas Mikolov, Martin Karafia?t, Lukas Burget, JanCernocky?, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Patrick Nguyen, Milind Mahajan, and Xiaodong He.2007.
Training non-parametric features for statisti-cal machine translation.
In Proceedings of the Sec-ond Workshop on Statistical Machine Translation,pages 72?79, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings ofthe 38th Annual Meeting on Association for Com-putational Linguistics, ACL ?00, pages 440?447,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Franz Josef Och and Hermann Ney.
2002.
Discrim-inative training and maximum entropy models forstatistical machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 295?302,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan, July.
Association for ComputationalLinguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.William J. E. Potts.
1999.
Generalized additive neuralnetworks.
In Proceedings of the fifth ACM SIGKDDinternational conference on Knowledge discoveryand data mining, KDD ?99, pages 194?200, NewYork, NY, USA.
ACM.Holger Schwenk.
2012.
Continuous space translationmodels for phrase-based statistical machine trans-lation.
In Proceedings of the 24th InternationalConference on Computational Linguistics, COLING?12, Mumbai, India.
Association for ComputationalLinguistics.Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,and Christopher D. Manning.
2011.
Parsing NaturalScenes and Natural Language with Recursive NeuralNetworks.
In Proceedings of the 26th InternationalConference on Machine Learning (ICML).A.
Sokolov, G. Wisniewski, and F. Yvon.
2012.
Non-linear n-best list reranking with few features.
InAMTA, San Diego, USA.Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models withneural networks.
In Proceedings of the 2012 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, NAACL HLT ?12, pages 39?48, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proc.
of ICSLP.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, ACL ?10, pages 384?394,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.D.
A. de Waal and J. V. du Toit.
2007.
Generalized ad-ditive models from a neural network perspective.
InProceedings of the Seventh IEEE International Con-ference on Data Mining Workshops, ICDMW ?07,pages 265?270, Washington, DC, USA.
IEEE Com-puter Society.800Taro Watanabe, Jun Suzuki, Hajime Tsukada, andHideki Isozaki.
2007.
Online large-margin train-ing for statistical machine translation.
In Proceed-ings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech Republic,June.
Association for Computational Linguistics.Tong Xiao, Jingbo Zhu, Muhua Zhu, and HuizhenWang.
2010.
Boosting-based system combina-tion for machine translation.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, ACL ?10, pages 739?748,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.801
