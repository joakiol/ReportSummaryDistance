In: Proceedings of CoNLL-2000 and LLL-2000, pages 25-30, Lisbon, Portugal, 2000.Increasing our Ignorance of Language: Identifying LanguageStructure in an Unknown 'Signal'J ohn  E l l i o t t  and Er i c  A twe l l  and Bi l l  WhyteCentre for Computer  Analysis of Language and Speech, School of Computer  StudiesUniversity of Leeds, Leeds, Yorkshire, LS2 9JT England{ j re ,  e r i c ,  b i l lw}?scs .
leeds .ac .ukAbst rac tThis paper describes algorithms and softwaredeveloped to characterise and detect genericintelligent language-like features in an inputsignal, using natural language learning tech-niques: looking for characteristic statistical"language-signatures" in test corpora.
Asa first step towards such species-independentlanguage-detection, we present a suite of pro-grams to analyse digital representations of arange of data, and use the results to extrap-olate whether or not there are language-likestructures which distinguish this data fromother sources, such as music, images, and whitenoise.
Outside our own immediate NLP sphere,generic communication techniques are of par-ticular interest in the astronautical community,where two sessions are dedicated to SETI attheir annual International conference with top-ics ranging from detecting ET technology to theethics and logistics of message construction (E1-liott and Atwell, 1999; Ollongren, 2000; Vakoch,2000).1 In t roduct ionA useful thought experiment is to imagineeavesdropping on a signal from outer space.How can you decide that it is a message be-tween intelligent life forms?
We need a 'lan-guage detector': or, to put it more accu-rately, something that separates language fromnon-language.
But what is special about thelanguage signal that separates it from non-language?
Is it, indeed, separable?The problem goal is to separate languagefrom non-language without dialogue, and learnsomething about the structure of language inthe passing.
The language may not be human(animals, aliens, computers...), the perceptualspace can be unknown, and we cannot assumehuman language structure but must begin some-where.
We need to approach the language signalfrom a naive viewpoint, in effect, increasing ourignorance and assuming as little as possible.Given this standpoint, an informal descrip-tion of 'language' might include that it:?
has structure at several interrelated levels?
is not random?
has grammar?
has letters/characters, words, phrases andsentences?
has parts of speech?
is recursive?
has a theme with variations?
is aperiodic but evolving?
is generative?
has transformation rules?
is designed for communication?
has Zipfian type-token distributions at sev-eral levelsLanguage as a 'signal'?
has some signalling elements (a 'script')?
has a hierarchy of signalling elements?
('Words', 'phrases' etc.)?
is serial??
is correlated across a distance of several sig-nalling elements applying at various levelsin the hierarchy?
is usually not truly periodic?
is quasi-stationary??
is non-ergodic?We assume that a language-like signal will beencoded symbolically, i.e.
with some kind ofcharacter-stream.
Our language-detection al-gorithm for symbolic input uses a number of25statistical clues such as entropy, "chunking" tofind character bit-length and boundaries, andmatching against a Zipfian type-token distribu-tion for "letters" and "words".2 Ident i fy ing  S t ructure  and  the'Character  Set 'The initial task, given an incoming bit-stream,is to identify if a language-like structure ex-ists and if detected what are the unique pat-terns/symbols, which constitute its 'characterset'.
A visualisation of the alternative possiblebyte-lengths i gleaned by plotting the entropycalculated for a range of possible byte-lengths(fig 1).In 'real' decoding of unknown scripts it is ac-cepted that identifying the correct set of dis-crete symbols is no mean feat (Chadwick, 1967).To make life simple for ourselves we assumea digital signal with a fixed number of bitsper character.
Very different echniques are re-quired to deal with audio or analogue quivalentwaveforms (Elliott and Atwell, 2000; Elliott andAtwell, 1999).
We have reason to believe thatthe following method can be modified to relaxthis constraint, but this needs to be tested fur-ther.
The task then reduces to trying to iden-tify the number of bits per character.
Given theprobability of a bit is Pi; the message ntropyof a string of length N will be given by the firstorder measure:E = SUM\[P i lnP i \ ] ; i  = 1, NIf the signal contains merely a set of random dig-its, the expected value of this function will risemonotonically as N increases.
However, if thestring contains a set of symbols of fixed lengthrepresenting a character set used for commu-nication, it is likely to show some decrease inentropy when analysed in blocks of this length,because the signal is 'less random' when thusblocked.
Of course, we need to analyse blocksthat begin and end at character boundaries.
Wesimply carry out the measurements in slidingwindows along the data.
In figure 1, we seewhat happens when we applied this to samplesof 8-bit ASCII text.
We notice a clear drop,as predicted, for a bit length of 8.
Modestprogress though it may be, it is not unreason-able to assume that the first piece of ev-idence for the presence of language- l ikes t ruc ture ,  wou ld  be the  ident i f icat ion of  alow-entropy,  character  set w i th in  the  sig-nal.The next task, still below the stages normallytackled by NLL researchers, is to chunk the in-coming character-stream into words.
Lookingat a range of (admittedly human language) text,if the text includes a space-like word-separatorcharacter, this will be the most frequent charac-ter.
So, a plausible hypothesis would be that themost frequent character is a word-separator1;then plot type-token frequency distributions forwords, and for word-lengths.
If the distribu-tions are Zipfian, and there are no significant'outliers' (very large gaps between 'spaces' sig-nifying very long words) then we have evidencecorroborating our space hypothesis; this alsocorroborates our byte-length ypothesis, sincethe two are interdependent.3 Ident i fy ing  'Words 'Again, work by crytopaleologists suggests that,once the character set has been found, the sep-aration into word-like units, is not trivial andagain we cheat, slightly: we assume that thelanguage possesses something akin to a 'space'character.
Taking our entropy measurement de-scribed above as a way of separating characters,we now try to identify which character epre-sents 'space'.
It is not unreasonable to believethat, in a word-based language, it is likely to beone of the most frequently used characters.Using a number of texts in a variety of lan-guages, we first identified the top three mostused characters.
For each of these we hy-pothesised in turn that it represented 'space'.This then allowed us to segment the signal intowords-like units ('words' for simplicity).
Wecould then compute the frequency distributionof words as a function of word length, for eachof the three candidate 'space' characters (fig 2).It can be seen that one 'separator' candidate(unsurprisingly, in fact, the most frequent char-acter of all) results in a very varied distribu-tion of word lengths.
This is an interestingdistribution, which, on the right hand side ofthe peak, approximately follows the well-known'law' according to Zipf (1949), which predictsthis behaviour on the grounds of minimum ef-1Work is currently progressing on techniques for un-supervised word separation without spaces.26fort in a communication act.
Conversely, re-sults obtained similar to the 'flatter' distribu-tions above, when using the most frequent char-acter, is likely to indicate the absence of wordseparators in the signal.To ascertain whether the word-length fre-quency distribution holds for language in gen-eral, multiple samples from 20 different lan-guages from Indo-European, Bantu, Semitic,Finno-Ugrian and Malayo-Polynesian groupswere analysed (fig 3).
Using statistical measuresof significance, it was found that most groupsfell well within 5- only two individual languageswere near exceeding these limits - of the pro-posed Human language word-length profile (E1-liott et al, 2000).Zipf 's  law is a s t rong  ind icat ion  oflanguage- l ike behav iour .
I t  can be usedto segment  the  signal p rov ided  a 'space'character  exists.
However, we should notassume Zipf to be an infallible language detec-tor.
Natural phenomena such as molecular dis-tribution in yeast DNA possess characteristicsof power laws (Jenson, 1998).
Nevertheless, itis worth noting, that such non-language posses-sors of power law characteristics generally dis-play distribution ranges far greater than lan-guage with long repeats far from each other(Baldi and Brunak, 1998); characteristics de-tectable at this level or at least higher orderentropic evaluation.4 Ident i fy ing  'Phrase- l i ke '  chunksHaving detected a signal which satisfies cri-teria indicating language-like structures at aphysical evel (Elliott and Atwell, 2000; Elliottand Atwell, 1999), second stage analysis is re-quired to begin the process of identifying inter-nal grammatical components, which constitutethe basic building blocks of the symbol system.With the use of embedded clauses and phrases,humans are able to represent an expression ordescription, however complex, as a single com-ponent of another description.
This allows us tobuild up complex structures far beyond our oth-erwise restrictive cognitive capabilities (Minsky,1984).
Without committing ourselves to a for-mal phrase structure approach, (in the Chom-skian sense) or even to a less formal 'chunk-ing' of language (Sparkle Project, 2000), it isthis universal hierarchical structure, evident inall human languages and believed necessary forany advanced communicator ,  that constitutesthe next phase in our signal analysis (Elliott andAtwell, 2000).
It is f rom these 'discovered' ba-sic syntactic units that analysis of behaviouraltrends and inter-relationships amongst  termi-nals and non-terminals alike can begin to unlockthe encoded internal grammatical  structure andindicate candidate parts of speech.
To  do this,we  make use of a particular feature common tomany known languages, the 'function' words,which occur in corpora with approximately thesame statistics.
These tend to act as bound-aries to fairly self-contained semantic/syntactic'chunks.'
They  can be identified in corpora bytheir usually high frequency of occurrence andcross-corpora invariance, as opposed to 'con-tent' words which are usually less frequent andmuch more  context dependent.Now suppose the function words arrived in atext independent of the other words, then theywould have a Poisson distribution, with somelong tails (distance between successive functionwords.)
But  this is NOT what  happens.
In-stead, there is empirical evidence that functionword  separation is constrained to within shortlimits, with very few more  than nine wordsapart (see fig 4).
We conjecture that this ishighly suggestive of chunking.5 C lus ter ing  in tosyntact i co -semant ic  lassesUnlike traditional natural language process-ing, a solution cannot be assisted using vastamounts of training data with well-documented'legal' syntax and semantic interpretation orknown statistical behaviour of speech cate-gories.
Therefore, at this stage we are endeav-ouring to extract the syntactic elements with-out a 'Rossetta' stone and by making as few as-sumptions as possible.
Given this, a generic sys-tem is required to facilitate the analysis of be-havioural trends amongst selected pairs of ter-minals and non-terminals alike, regardless of thetarget language.Therefore, an intermediate r search goal is toapply Natural Language Learning techniques tothe identification of "higher-level" lexical andgrammatical patterns and structure in a lin-guistic signal.
We have begun the developmentof tools to visualise the correlation profiles be-27tween pairs of words or parts of speech, as a pre-cursor to deducing eneral principles for 'typing'and clustering into syntactico-semantic lexicalclasses.
Linguists have long known that collo-cation and combinational patterns are charac-teristic features of natural anguages, which setthem apart (Sinclair, 1991).
Speech and lan-guage technology researchers have used word-bigram and n-gram models in speech recogni-tion, and variants of PoS-bigram models forPart-of-Speech tagging.
In general, these mod-els focus on immediate neighbouring words, butpairs of words may have bonds despite sepa-ration by intervening words; this is more rele-vant in semantic analysis, eg Wilson and Rayson(1993), Demetriou (1997).
We sought to in-vestigate possible bonding between type tokens(i.e., pairs of words or between parts of speechtags) at a range of separations, by mapping thecorrelation profile between a pair of words ortags.
This can be computed for given word-pairtype (wl,w2) by recording each word-pair token(wl,w2,d) in a corpus, where d is the distance ornumber of intervening words.
The distributionof these word-pair tokens can be visualised byplotting d (distance between wl and w2) againstfrequency (how many (wl,w2,d) tokens found atthis distance).
Distance can be negative, mean-ing that w2 occurred be/ore wl and for any sizewindow (i.e., 2 to n).
In other words, we postu-late that it might be possible to deduce part-of-speech membership and, indeed, identify a setof part-of-speech classes, using the joint proba-bility of words themselves.
But is this possible?One test would be to take an already taggedcorpus and see if the parts-of-speech did indeedfall into separable clusters.Using a five thousand-word extract from theLOB corpus (Johansson et al, 1986) to test thistool, a number of parts-of-speech pairings wereanalysed for their cohesive profiles.
The arbi-trary figure of five thousand was chosen, as itboth represents a sample large enough to re-flect trends seen in samples much larger (with-out loosing any valuable data) and a samplesize, which we see as at least plausible whenanalysing ancient or extra-terrestrial l nguageswhere data is at a premium.Figure 5 shows the results for the relationshipbetween a pair of content and function words, soidentified by looking at their cross-corpus statis-tics.
It can be seen that the function word has ahigh probability of preceding the content wordbut has no instance of directly following it.
Atleast metaphorically, the graph can be consid-ered to show the 'binding force' between the twowords varying with their separation.
We arelooking at how this metaphor might be used inorder to describe language as a molecular struc-ture, whose 'inter-molecular forces' can be re-lated to part-of-speech interaction and the de-velopment of potential semantic ategories forthe unknown language.Examining language in such a manner alsolends itself to summarising ('compressing') thebehaviour to its more notable features whenforming profiles.
Figure 6 depicts a 3D repre-sentation of results obtained from profiling VB-tags with six other major syntactic ategories;figure 7 shows the main syntactic behaviouralfeatures found for the co-occurrence of some ofthe major syntactic lasses ranging over the cho-sen window of ten words.Such a tool may also be useful in other areas,such a lexico-grammatical analysis or taggingof corpora.
Data-oriented approaches to cor-pus annotation use statistical n-grams and/orconstraint-based models; n-grams or constraintswith wider windows can improve error-rates,by examining the topology of the annotation-combination space.
Such information could beused to guide development of Constraint Gram-mars.
The English Constraint Grammar de-scribed in (1995) includes constraint rules upto 4 words either side of the current word (seeTable 16, p352); the peaks and troughs in thevisualisation tool might be used to find candi-date patterns for such long-distance constraints.Our research topic NLL4SETI (Natural Lan-guage Learning for the Search for Extra-Terrestrial Intelligence) is distinctive in that -it is potentially a VERY useful application ofunsupervised NLL; - it starts from more ba-sic assumptions than most NLL research: wedo not assume tokenisation i to characters andwords, and have no tagged/parsed training cor-pus; - it focuses on utilising statistical distri-butional universals of language which are com-putable and diagnostic; - this focus has led usto develop distributional visualisation tools toexplore type/token combination distributions; -the goal is NOT learning algorithms which anal-28yse/annotate human language in a way whichhuman experts would approve of (eg phrase-chunking corresponding to a human linguist'sparsing of English text); but algorithms whichrecognise language-like structuring in a poten-tially much wider range of digital data sets.6 Summary  and  fu turedeve lopmentsTo summarise, our achievements to date include- a method for splitting a binary digit-streaminto characters, by using entropy to diagnosebyte-length; - a method for tokenising unknowncharacter-streams into words of language; -an approach to chunking words into phrase-like sub-sequences, by assuming high-frequencyfunction words act as phrase-delimiters; - a vi-sualisation tool for exploring word-combinationpatterns, where word-pairs need not be imme-diate neighbours but characteristically combinedespite several intervening words.So far, our approaches have involved workingwith languages with which we are most familiarand, to a certain extent, making use of linguistic'knowns' such as pre-tagged corpora.
It is earlydays yet and we make no apology for this initialapproach.
However, we feel that by deliberatelyreducing our dependence on prior knowledge('increasing our ignorance of language') and bytreating language as a 'signal', we might be con-tributing a novel approach to natural anguageprocessing which might ultimately lead to a bet-ter, more fundamental understanding of whatdistinguishes language from the rest of the sig-nal universe.Re ferencesP.
Baldi and S. Brunak.
1998.
Bioinformatics - TheMachine Learning Approach.
MIT press, Cam-bridge Massachusetts.J.
Chadwick.
1967.
The Decipherment ofLinear B.Cambridge University Press.George Demetriou.
1997.
PhD thesis.
School ofComputer Studies, University of Leeds.John Elliott and Eric Atwell.
1999.
Language insig-nals: the detection of generic species-independentintelligent language f atures in symbolic and oralcommunications.
In Proceedings of the 50th In-ternational Astronautical Congress.
paper IAA-99-IAA.9.1.08, International Astronautical Feder-ation, Paris.John Elliott and Eric Atwell.
2000.
Is there any-body out there?
: The detection of intelligentand generic language-like f atures.
Journal of theBritish Interplanetary Society, 53:1/2:13-22.John Elliott, Eric Atwell, and Bill Whyte.
2000.Language identification in unknown signals.In Proceedings of COLING'2000 InternationalConference on Computational Linguistics.
Saar-bruecken.H.
Jenson.
1998.
Self Organised Criticality.
Cam-bridge University Press.Stig Johansson, Eric Atwell, Roger Garside,and Geoffrey Leech.
1986.
The Tagged LOBcorpus: users' manual.
Bergen University,Norway: ICAME, The Norwegian Comput-ing Centre for the Humanities.
Availablefrom http://www.hit.uib.no/icame/lobman/lob-cont.html.Fred Karlsson, Atro Voutilainen, Juha Heikkila,and Arto Anttila.
1995.
Constraint Grammar:a language-independent system for parsing unre-stricted text.
Berlin: Mouton de Gruyter.Geoffrey Leech, Roger Garside, and Eric Atwell.1983.
The automatic grammatical tagging of thelob corpus.
ICAME Journal, 7:13-33.Christopher Manning and Hinrich Schutze.
1999.Foundations of Statistical Natural Language Pro-cessing.
Cambridge: MIT Press.M.
Minsky.
1984.
Why Intelligent Aliens will be In-telligible.
Cambridge University Press.Alexander Ollongren.
2000.
Large-size message con-struction for eti.
In Proceedings of the 50th In-ternational Astronautical Congress.
paper IAA-99-IAA.9.1.09, International Astronautical Feder-ation, Paris.Sparkle Project.
2000. http://www.ilc.pi.cnr.it/sparkle/wp 1-prefinal/node25.html.John Sinclair.
1991.
Corpus, concordance, colloca-tion describing English language.
Oxford Univer-sity Press.Doug Vakoch.
2000.
Communicating scientifi-cally formulated spiritual principles in interstel-lar messages.
In Proceedings of the 50th Inter-national Astronautical Congress.
paper IAA-99-IAA.9.1.10, International Astronautical Federa-tion, Paris.Andrew Wilson and Paul Rayson.
1993.
The au-tomatic ontent analysis of spoken discourse.
InC. Souter and E. Atwell, editors, Corpus basedcomputational linguistics.
Rodopi, Amsterdam.G.K.
Zipf.
1949.
Human Behaviour and The Prin-ciple of Least Effort.
Addison Wesley Press, NewYork.
(1965 reprint).29IEntropy87654321Figure 1LanguageI I I I I I4 5 6 7 8 9400350"~" 300~ 25020O150IO05OOFigure 2: Candidate word-length distributionsusin~ the 3 most freouent charaelers.Entropy profile as an indicator of character bit-lengthMulUple samples from Indo-European, Bantu, Semitic, Fin no-Ug rian.
and M~ayo-Polyne$ian l guage groups25.0020.00=o 15.oo -?
~", , .
5.00 ~,t:;~ ',~%~,~Figure 3 Word lengthF~quencyFig are 4FunctionWordseparation iEnglish.I[ll,,.3 4 5 6 7 8 9Number of words between candidate functional wordsi l aa  - .
- - t  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.foa l  .....q .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.9t~ .
-4  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.J too  ,.
, .
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.6a l  .
.
.
.
.4  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.l~ to  :- .
.
.
.
.
.
.
.
.
.
.
.
.
4. .
.
.
.
.
\[Correlation profile tbr word pairi Figure 5I....... ............................. 1.................................... .
It........................ i ................. i .... .
.
.
.
.
.
.
.
.
.
t .
.
.
.
.
i-ill .............. i J~, .........P(w I !
'unctional~w2 coalcn:) Itmquen~Figure 6: VB-tag profileFigure 7 CnounCnoun \[3, ~,3Jj 13.Rb Z, XsPrep 6", )~2Cc 6", ~3:4Vb X2Art 13"Jj Rb Prep Cc Vb Art6" ~'2 13" 13', ~6 6, ~'2 6, ~'2\[3 6, ~,5,9 ~'2 ~2,4 6 6, ~,3~7 \[3 13" 6, ~9 13 ~2~2 6*, ~7 6, ~'3 ~'3 Z*,~9 \[3\[3 13, 26 ~,4 Z ~,5 13"k2 13 13" 6, z9 z 13.13" 6, ~,3,8 Z, ~,2 Z* Z Z, ~4Key: Z = Zero bigram - or at offset specified - occurrences.
6 = Very weak bonding - near zero - atbigram occurrences.
\[3 = Strong bonding at bigram co-occurrences.
* = Indicates opposing cohesivetrend when P.O.S.
reversed.
Xn = High peak beyond bigram at offset distance of  'n'.
@ = Flatdistribution across offsets - bigram bonding evident.
