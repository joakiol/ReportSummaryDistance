Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 66?73,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsRUBISC - a Robust Unification-Based Incremental Semantic ChunkerMichaela AttererDepartment for LinguisticsUniversity of Potsdamatterer@ling?uni-potsdam.deDavid SchlangenDepartment for LinguisticsUniversity of Potsdamdas@ling?uni-potsdam.deAbstractWe present RUBISC, a new incremen-tal chunker that can perform incrementalslot filling and revising as it receives astream of words.
Slot values can influ-ence each other via a unification mecha-nism.
Chunks correspond to sense units,and end-of-sentence detection is done in-crementally based on a notion of seman-tic/pragmatic completeness.
One of RU-BISC?s main fields of application is indialogue systems where it can contributeto responsiveness and hence naturalness,because it can provide a partial or com-plete semantics of an utterance while thespeaker is still speaking.
The chunker isevaluated on a German transcribed speechcorpus and achieves a concept error rate of43.3% and an F-Score of 81.5.1 IntroductionReal-time NLP applications such as dialogue sys-tems can profit considerably from incrementalprocessing of language.
When syntactic and se-mantic structure is built on-line while the speechrecognition (ASR) is still working on the speechstream, unnatural silences can be avoided andthe system can react in a faster and more user-friendly way.
As (Aist et al, 2007) and (Skantzeand Schlangen, 2009) show, such incremental sys-tems are typically preferred by users over non-incremental systems.To achieve incrementality, most dialogue sys-tems employ an incremental chart parser (cf.
(Stoness et al, 2004; Seginer, 2007) etc.).
How-ever, most existing dialogue systems operate invery limited domains, e.g.
moving objects, peo-ple, trains etc.
from one place to another (cf.
(Aist et al, 2007), (Skantze, 2007), (Traum et al,1996)).
The complexity of the semantic repre-sentations needed is thus limited.
Moreover, userbehaviour (ungrammatical sentences, hesitations,false starts) and error-prone ASR require the pars-ing process to be robust.1 We argue that obtainingrelatively flat semantics in a limited domain whileneeding exigent robustness calls for investigatingshallower incremental chunking approaches as al-ternatives to CFG or dependency parsing.
Previ-ous work that uses a combination of shallow anddeep parsing in dialogue systems also indicatesthat shallow methods can be superior to deep pars-ing (Lewin et al, 1999).The question addressed in this paper is how toconstruct a chunker that works incrementally androbustly and builds the semantics required in adialogue system.
In our framework chunks arebuilt according to the semantic information theycontain while syntactic structure itself is less im-portant.
This approach is inspired by Selkirk?ssense units (Selkirk, 1984).
She claims suchunits to be relevant for prosodic structure and dif-ferent to syntactic structure.
Similarly, (Abney,1991) describes some characteristics of chunks asfollows?properties which also make them seemto be useful units to be considered in spoken dia-logue systems:?when I read a sentence, I read it a chunk ata time.
[...] These chunks correspond in someway to prosodic patterns.
Chunks also represent agrammatical watershed of sorts.
The typical chunkconsists of a single content word surrounded by aconstellation of function words, matching a fixedtemplate.
By contrast, the relationships betweenchunks are mediated more by lexical selection1cf.
The incremental parser in (Skantze, 2007) can jumpover a configurable number of words in the input.66than by rigid templates.
[...] and the order inwhich chunks occur is much more flexible than theorder of words within chunks.
?In our approach chunks are built incrementally(one at a time) and are defined semantically (asense unit is complete when a slot in our templateor frame semantics can be filled).
Ideally, in a fullsystem, the definition of their boundaries will alsobe aided by prosodic information.
The current im-plementation builds the chunks or sense units byidentifying a more or less fixed sequence of con-tent and function words, similar to what Abneydescribes as a fixed template.
The relationshipsbetween the units are mediated by a unificationmechanism which prevents selectional restrictionsfrom being violated.
This allows the order of thesense units to be flexible, even as flexible as theyappear in ungrammatical utterances.
This unifi-cation mechanism and the incremental method ofoperation are also the main difference to Abney?swork and other chunkers.In this paper, we first present our approach ofchunking, show our grammar formalism, the mainfeatures of the chunker (unification mechanism,incrementality, robustness), and explain how thechunker can cope with certain tasks that are an is-sue in dialogue systems, such as online utteranceendpointing and revising hypotheses.
In Section 3,we evaluate the chunker on a German corpus (oftranscribed spontaneous speech) in terms of con-cept error rate and slot filling accuracy.
Then wediscuss related work, followed by a general dis-cussion and the conclusion.2 Incremental ChunkingFigure 1 shows a simple example where the chun-ker segments the input stream incrementally intosemantically relevant chunks.
The figure also dis-plays how the frame is being filled incrementally.The chunk grammar developed for this work andthe dialogue corpus used were German, but wegive some examples in English for better readabil-ity.As time passes the chunker receives more andmore words from the ASR.
It puts the words in aqueue and waits until the semantic content of theaccumulated words is enough for filling a slot inthe frame semantics.
When this is the case thechunk is completed and a new chunk is started.At the same time the frame semantics is updated ifslot unification (see below) is possible and a checktimeturnermthepieceermthesecondintheupperrowtoermclockwisechunk:[turn]ermerm theerm the pieceerm the piece ermerm the piece erm theinin thein the upper[in the upper row]toto erm[to erm clockwise]action:turningend:?grammar:action:turning ?>turnend:right?>to the right|clockwise...action:turning[erm the piece erm the second]RUBISCsemantics:input:object:xpos:2?>the secondobject:ypos:?1?>the upper rowobject: name:?end:?object: name:?action:turningend:?object: name:?action:turningend:rightobject: name:?xpos:2ypos:?xpos:2ypos:?1xpos:2ypos:?1ypos:?xpos:?Figure 1: Incremental robust sense unit construc-tion by RUBISC.Figure 2: Puzzle-task of the corpus used for gram-mar building and testing.whether the utterance is complete is made, so thatthe chunker can be restarted for the next utteranceif necessary.2.1 A Regular Grammar for SemanticsThe grammar we are using for the experiments inthis paper was developed using a small corpus ofGerman dialogue (Siebert and Schlangen, 2008),(Siebert, 2007).
Figure 2 shows a picture of thetask that the subjects completed for this corpus.2 Anumber of pentomino pieces were presented.
Thepieces had to be moved into an animal-shaped fig-ure.
The subjects were shown partly completedpuzzles and had to give concise and detailed ver-bal instructions of the next move that had to bedone.
The locations inside this figure were usuallyreferred to in terms of body parts (move the x into2For the corpus used here the difference was that the but-ton labels were German and that the pentomino pieces werenot ordered in two rows.
For better readability, we show thepicture with the English labels.67the head of the elephant).For such restricted tasks, a simple frame se-mantics seems sufficient, representing the action(grasping, movement, flipping or turning of an ob-ject), the object that is involved, and where orin which position the object will end up.
In ourcurrent grammar implementation the object canbe described with three attributes: name is thename of the object.
In our domain, the objects arepentomino-pieces (i.e., geometrical forms that canbe built out of five squares) which have traditionalletter names such as x or w; the grammar mapsother descriptions such as cross or plus to suchcanonical names.
A piece can also be describedby its current position, as in the lower piece inthe third column.
This is covered by the attributesxpos and ypos demarking the x-position and y-position of a piece.
The x- or y-position canbe a positive or negative number, depending onwhether the description counts from left or right,respectively.The possible slots must be defined in the gram-mar file in the following format:@:action@:entity:name@:entity:xpos@:entity:ypos@:end(That is: definition marker @:level1: (optional) level 2.
)The position where or in which the piece endsup could also be coded as a complex entry, but forsimplicity?s sake (in the data used for evaluation,we have a very limited set of end positions thatwould each be described by just one attribute re-spectively), we restrict ourselves to a simple entrycalled end which takes the value of a body part(head, back, leg1 etc.)
in the case of movement,and the value of a direction or end position hor-izontal, vertical, right, left in the case of a turn-ing or flipping action.
It will be (according toour current grammar) set to empty in the case of agrasping action, because grasping does not specifyan end position.
This will also become importantlater, when unification comes into play.
Figure 3shows a part of the German grammar used withapproximate translations (in curly brackets) of theright-hand side into English.
The English parts incurly brackets is meta-notation and not part of thegrammar file.
Note that one surface string can de-termine the value of more than one semantic slot.The grammar used in the experiments in this paperaction:grasping,end:empty -> nimm|nehme{take}action:turning -> drehe?
{turn}action:flipping -> spieg(le|el) {flip}action:movement -> bewegt {moved}action:turning -> gedreht {turned}entity:name:x -> kreuz|plus|((das|ein) x){cross|pluss|((the|an) x)}entity:name:w -> treppe|((das|ein) w$){staircase|(the|a) w}entity:name:w -> (das|ein) m${(the|an) m}entity:name:z -> (das|ein) z${(the|a) z}end:head -> (in|an) den kopf{(on|in) the head}end:leg2 -> ins?
das (hinterbein|hinterebein|rechte bein|zweites bein) {in the hindleg|back leg|right leg| second leg}entity:ypos:lower -> der (unteren|zweiten)reihe {(lower|second) row}entity:xpos:1 -> das erste {the first}entity:ypos:-1 -> das letzte {the last}end:horizontal,action:flipping -> horizontal{horizontally}Figure 3: Fragment of the grammar file used inthe experiments (with English translations of thepatterns for illustration only).had 97 rules.2.2 UnificationUnification is an important feature of RUBISCfor handling aspects of long-distance dependen-cies and preventing wrong semantic representa-tions.
Unification enables a form of ?semanticspecification?
of verb arguments, avoiding that thewrong arguments are combined with a given verb.It also makes possible that rules can check for thevalue of other slots and hence possibly becomeinapplicable.
The verb move, for instance, en-sures that action is set to movement.
For the ut-terance schieb das a?h das horizontal a?h liegt insVorderbein (move that uh which is horizontal intothe front leg).
The action-slot will be filledwith movement but the end-slot remains emptybecause horizontal as an end fits only with a flip-ping action, and so is ignored here.
Figure 4 illus-trates how the slot unification mechanism works.2.3 RobustnessThe chunker meets various robustness require-ments to a high degree.
First, pronunciation vari-ants can be taken account of in the grammar ina very flexible way, because the surface string orterminal symbols can be expressed through regu-68action:?end:?...unify frame withInput: unification component:time: Frame:[schieb][action:movement]?>unification success: action:movementend:?...dasdas mh[das mh horizontal] unify frame withaction:flippingend:horizontal?>unification failed: action:movementend:?liegtliegt ins[liegt ins Vorderbein]unify frame with[end:leg1] action:movementend:leg1......Figure 4: Example of slot unification and failureof unification.lar expression patterns.
move in German for in-stance can be pronounced with or without a final-e as bewege or beweg.
flip (spiegle can be pro-nounced with or without -el-inversion at the end.Note, that this is due to the performance of speak-ers in our corpus and does not necessarily reflectGerman grammar rules.
A system, however, needsto be able to cope with performance-based varia-tions.Disfluencies are handled through how the chun-ker constructs chunks as sense units.
First, thechunker only searches for relevant information ina chunk.
Irrelevant information such as an initialuh in uh second row is put in the queue, but ig-nored as the chunker picks only second row as thesemantically relevant part.
Furthermore the chun-ker provides a mechanism that allows it to jumpover words, so that second row will be found inthe second uh row and the cross will be found inthe strange cross, where strange is an unknownword.2.4 IncrementalityOne of the main features of RUBISC is its incre-mentality.
It can receive one word at a time andextract semantic structure from it.
Incrementalityis not strict here in the sense of (Nivre, 2004), be-cause sometimes more than one word is neededbefore parts of the frame are constructed and out-put: into the right, for instance, needs to wait for aword like leg that completes the chunk.
We don?tnecessarily consider this a disadvantage, though,as our chunks closely correlate to the minimal bitsof information that can usefully be reacted to.
Inour corpus the first slot gets on average filled after3.5 words (disregarding examples where no slotsare filled).
The average utterance is 12.4 wordslong.2.5 End-of-Sentence DetectionAn incremental parser in a dialogue system needsto know when to stop processing a sentence andwhen to start the next one.
This can be done byusing prosodic and syntactic information (Attereret al, 2008) or by checking whether a syntacticS-node is complete.
Since RUBISC builds senseunits, the completeness of an utterance can be de-fined as semantic-pragmatic completeness, i.e.
bya certain number of slots that must be filled.
In ourdomain, for instance, it makes sense to restart thechunker when the action and end slot and eitherthe name slot or the two position slots are filled.2.6 HistoryThe chunker keeps a history of the states of theframes.
It is able to go back to a previous statewhen the incremental speech recognition revokesa word hypothesis.
As an example consider thecurrent word hypothesis to be the L. The slot en-tity name will be filled with l. Then the speechrecognition decides to change the hypothesis intothe elephant.
This results in clearing the slot forentity name again.3 EvaluationThe sense unit chunker was evaluated in terms ofhow well it performed in slot filling on an unseenpart of our corpus.
This corpus comes annotatedwith utterance boundaries.
500 of these utteranceswere manually labelled in terms of the semanticslots defined in the grammar.
The annotators werenot involved in the construction of the chunker orgrammar.
The annotation guidelines detailed thepossible values for each slot.
The entity nameshad to be filled in with the letter names of thepieces, the end slot with body parts or right, left,horizontal etc., and the position slots with posi-tive and negative numbers.3 The chunker was thenrun on 400 of these utterances and the slot valueswere compared with the annotated frames.
100of the labelled utterances and 50 additional utter-3In a small fraction (21) of the 500 cases an utteranceactually contained 2 statements that were combined withund/and.
In these cases the second statement was neglected.69ances were used by the author for developing thegrammar.We examined the following evaluation mea-sures:?
the concept error (concept err) rate (percentageof wrong frames)?
the percentage of complete frames that werecorrect (frames corr)?
the percentage of slots that were correct?
the percentage of action slots correct?
the percentage of end slots correct?
the percentage of object:name slots correct?
the percentage of object:xpos slots correct?
the percentage of object:ypos slots correctThe results are shown in Table 1.
We useda very simple baseline: a system that does notfill any slots.
This strategy still gets 17% ofthe frames right, because some utterances donot contain any real content.
For the sentenceAlso das ist recht schwer (Trans: That?s quitedifficult.
), for instance, the gold standard seman-tic representation would be: {action:None,end:None, object:{xpos:None, name:None,ypos:None}}.
As the baseline ?system?
alwaysreturns the empty frame, it scores perfectly forthis example sentence.
We are aware that thisappears to be a very easy baseline.
However, forsome slots, such as the xpos and ypos slots it stillturned out to be quite hard to beat this baseline,as wrong entries were common for those slots.The chunker achieves a frame accuracy of 54.5%and an overall slot filling accuracy of 86.80%(compared to 17% and 64.3% baseline).
Of theindividual slots the action slot was the one thatimproved the most.
The position slots were theonly ones to deteriorate.
As 17% of our utterancesdid not contain any relevant content, i.e.
the framewas completely empty, we repeated the evaluationwithout these irrelevant data.
The results areshown in brackets in the table.To check the impact of the unification mecha-nism, we performed another evaluation with thismechanism turned off, i.e.
slots are always filledwhen they are empty without regarding other slots.In the second step in Figure 4, the end slot wouldhence be filled.
This resulted in a decline in per-formance as can also be seen in Table 1.
We alsoturned off robustness features to test for their im-pact.
Surprisingly, turning off the skipping of oneword within a string specified by a grammar rule(as in to erm clockwise), did not have an effect onthe results on our corpus.
When we also turn offallowing initial material (erm the piece), however,performance drops considerably.We also tested a variant of the system RUBISC-o (for RUBISC-overlap) which considers overlap-ping chunks: Take the third piece will result inxpos:3 for the original chunker, even if the utter-ance is continued with from the right.
RUBISC-oalso considers the previous chunk the third piecefor the search of a surface representation.
In thiscase, it overwrites 3 with -3.
In general, this be-haviour improves the results.4To allow a comparison with other work that re-ports recall and precision as measures, we alsocomputed those values for RUBISC: for our testcorpus recall was 83.47% and precision was79.69% (F-score 81.54).
A direct comparison withother systems is of course not possible, becausethe tasks and data are different.
Nevertheless, thenumbers allow an approximate feel of how wellthe system performs.To get an even better idea of the performance,we let a second annotator label the data we testedon; inter-annotator agreement is given in Table 1.The accuracy for most slots is around 90% agree-ment beween annotators.
The concept error rateis 32.25%.
We also examined 50 utterances of thetest corpus for an error analysis.
The largest part ofthe errors was due to vocabulary restrictions or re-strictions in the regular expressions: subjects usednames for pieces or body parts or even verbs whichhad not been seen or considered during grammardevelopment.
As our rules for end positions con-tained pronouns like (into the back), they weretoo restricted for some description variants (suchthat it touches the back).
Another problem thatappears is that descriptions of starting positionscan be confounded with descriptions of end po-sitions.
Sometimes subjects refer to end positionsnot with body parts but with at the right side etc.In some cases this leads to wrong entries in theobject-position slots.
In some cases a full parsermight be helpful, but not always, because someexpressions are syntactically ambiguous: fu?ge dasTeil ganz rechts in das Rechteck ein.
(put the pieceon the right into the square/put the piece into thesquare on the right.)
A minority of errors was also4Testing significance, there is a significant difference be-tween RUBISC and the baseline, and RUBISC and RIBISCw/o rob (for all measures except xpos and ypos).
The othervariants show no significance compared with RUBISC butclear tendencies in the directions described above.70baseline RUBISC w/o unif w/o rob RUBISC-o i-annotatorconcept err 83.0 (100) 45.5 (44.6) 49.5 (49.7) 73.3 (85.5) 43.3 (42.8) 32.3 (35.5)frames corr 17.0 (0) 54.5 (55.4) 50.3 (50.3) 26.8 (14.5) 56.8 (57.2) 67.8 (64.5)slots corr 64.3 (57.0) 86.8 (87.2) 84.6 (84.5) 78.8 (74.9) 87.6 (87.6) 92.1 (91.5)action corr 27.8 (13.0) 90.3 (92.2) 85.8 (86.7) 64.3 (57.5) 89.8 (90.7) 89.0 (88.6)end corr 68.0 (61.4) 85.8 (87.3) 81.0 (81.6) 73.8 (69.0) 85.5 (87.0) 95.8 (95.1)name corr 48.8 (38.3) 86.3 (88.3) 84.5 (86.1) 79.0 (76.2) 86.5 (88.0) 86.8 (85.8)xpos corr 87.5 (84.9) 83.0 (80.7) 83.0 (80.7) 86.5 (83.7) 85.5 (83.4) 94.5 (94.0)ypos corr 89.5 (87.3) 88.8 (87.3) 88.8 (87.3) 90.3 (88.3) 90.5 (88.9) 94.5 (94.0)Table 1: Evaluation results (in %) for RUBISC in comparison with the baseline, RUBISC without uni-fication mechanism (w/o unif), without robustness (w/o rob), RUBISC with overlap (RUBISC-o), andinter-annotator aggreement (i-annotator).
See the text for more information.due to complex descriptions (the damaged t wherethe right part has dropped downwards ?
referringto the f), transcription errors (recht statt rechts) etc.4 Related WorkSlot filling is used in dialogue systems such asthe Ravenclaw-Olympus system5, but the slots arefilled by using output from a chart parser (Ward,2008).
The idea is similar in that word strings aremapped onto semantic frames.
A filled slot, how-ever, does not influence other slots via unificationas in our framework, nor can the system deal withincrementality.
This is also the main differenceto systems such as Regulus (Rayner et al, 2006).Our unification is carried out on filled slots and inan incremental fashion.
It is not directly specifiedin our grammar formalism.
The chunker ratherchecks whether slot entries suggested by variousindependent grammar rules are unifiable.Even though not incremental either, the ap-proach by (Milward, 2000) is similar in that it canpick information from various parts of an utter-ance; for example, it can extract the arrival timefrom sentences like I?d like to arrive at York nowlet?s see yes at 3pm.
It builds a semantic chart us-ing a Categorial grammar.
The entries of this chartare then mapped into slots.
A number of settingsare compared and evaluated using recall and preci-sion measures.
The setting with the highest recall(52%) achieves a precision of 79%.
The settingwith the highest precision (96%) a recall of 22%.These are F-scores of 62.7 and 35.8 respectively.
(Aist, 2006) incrementally identifies what theycall ?pragmatic fragments?, which resemble thesense units produced in this paper.
However, their5http://www.ravenclaw-olympus.org/system is provided with syntactic labels and theidea is to pass those on to a parser (this part ap-pears to not be implemented yet).
No evaluation isgiven.
(Zechner, 1998) also builds frame representa-tions.
Contrary to our approach, semantic infor-mation is extracted in a second step after syntac-tic chunks have been defined.
The approach doesnot address the issue of end of sentence-detection,and also differs in that it was designed for use withunrestricted domains and hence requires resourcessuch as WordNet (Miller et al, 1993).
Depend-ing on the WordNet output, usually more than oneframe representation is built.
In an evaluation, in21.4% of the cases one of the frames found is cor-rect.
Other approaches like (Rose, 2000) also needlexicons or similar resources.
(Helbig and Hartrumpf, 1997) developed an in-cremental word-oriented parser for German thatuses the notion of semantic kernels.
This ideais similar in that increments correspond to con-stituents that have already been understood se-mantically.
The parser was later on mainly usedfor question answering systems and, even thoughstrongly semantically oriented, places more em-phasis on syntactic and morphological analysisand less on robustness than our approach.
Ituses quite complex representations in the form ofmulti-layered extended semantic networks.Finally, speech grammars such as JSFG6 aresimilar in that recognition patterns for slots like?action?
are defined via regular patterns.
The maindifferences are non-incrementality and that the re-sult of employing the grammar is a legal sequentialstring for each individual slot, while our grammar6java.sun.com/products/java-media/speech/forDevelopers/JSGF/71also encodes, what is a legal (distributed) combi-nation of slot entries.5 Discussion and Future WorkThe RUBISC chunker presented here isnot the first NLU component that is robustagainst unknown words or structures, or non-grammaticalities and disfluencies in the input, northe first that works incrementally, or chunk-based,or focusses predominantly on semantic contentinstead of syntactic structure.
But we believe thatit is the first that is all of this combined, and thatthe combination of these features provides anadvantage?at least for the domains that we areworking on.
The novel combination of unificationand incrementality has the potential to handlemore phenomena than simple key word spotting.Consider the sentence: Do not take the piece thatlooks like an s, rather the one that looks like a w.The idea is to introduce a negation slot or flag,that will be set when a negation occurs.
nicht dass (not the s) will trigger the flag to be set while atthe same time the name slot is filled with s. Thisnegation slot could then trigger a switch of themode of integration of new semantic informationfrom unification to overwriting.
We will test thisin future work.One of the main restrictions of our approach isthat the grammar is strongly word-oriented anddoes not abstract over syntactic categories.
Itsexpressive power is thus limited and some extracoding work might be necessary due to the lackof generalization.
However, we feel that this ismediated by the simplicity of the grammar for-malism.
A grammar for a restricted domain (andthe approach is mainly aiming at such domains)like ours can be developed within a short timeand its limited size also restricts the extra cod-ing work.
Another possible objection to our ap-proach is that handcrafting grammars like ours iscostly and to some extent arbitrary.
However, fora small specialized vocabulary as is typical formany dialogue systems, we believe that our ap-proach can lead to a good fast-running system in ashort developing time due to the simplicity of thegrammar formalism and algorithm, which makesit easier to handle than systems that use large lexi-cal resources for complexer domains (e.g.
tutoringsystems).
Other future directions are to expandthe unification mechanism and grammar formal-ism such that alternatives for slots are possible.This feature would allow the grammar writer tospecify that end:right requires a turning action ora flipping action.6 ConclusionWe presented a novel framework for chunking.The main new ideas are that of incremental chunk-ing and chunking by sense units, where the rela-tionship between chunks is established via a uni-fication mechanism instead of syntactic bounds,as in a full parsing approach.
This mechanismis shown to have advantages over simple keywordspotting.
The approach is suitable for online end-of-sentence detection and can handle revised wordhypotheses.
It is thus suitable for use in a spokendialogue system which aims at incrementality andresponsiveness.
Nevertheless it can also be usedfor other NLP applications.
It can be used in anincremental setting, but also for non-incrementaltasks.
The grammar format is easy to grasp, andthe user can specify the slots he wants to be filled.In an evaluation it achieved a concept error rate of43.25% compared to a simple baseline of 83%.7 AcknowledgementThis work was funded by the DFG Emmy-Noethergrant SCHL845/3-1.
Many thanks to Ewan Kleinfor valuable comments.
All errors are of courseours.ReferencesSteven Abney.
1991.
Parsing by chunks.
In Principle-based Parsing: Computation and Psycholinguistics,volume 44 of Studies in Linguistics and Philosophy.Kluwer.Gregory Aist, James Allen, Ellen Campana, Car-los Gomez Gallo, Scott Stoness, Mary Swift, andMichael K. Tanenhaus.
2007.
Incremental under-standing in human-computer dialogue and experi-mental evidence for advantages over nonincrementalmethods.
In Decalog 2007, Trento, Italy.Gregory S. Aist.
2006.
Incrementally segment-ing incoming speech into pragmatic fragments.
InThe Third Midwest Computational Linguistics Col-loquium (MCLC-2006), Urbana, USA.Michaela Atterer, Timo Baumann, and DavidSchlangen.
2008.
Towards incremental end-of-utterance detection in dialogue systems.
InProceedings of Coling 2008, Manchester, UK.Hermann Helbig and Sven Hartrumpf.
1997.
Wordclass functions for syntactic-semantic analysis.
In72Proceedings of the 2nd International Conference onRecent Advances in Natural Language Processing(RANLP?97).I.
Lewin, R. Becket, J. Boye, D. Carter, M. Rayner, andM.
Wiren.
1999.
Language processing for spokendialogue systems: is shallow parsing enough?
InAccessing Information in Spoken Audio: Proceed-ings of ESCA ETRW Workshop, Cambridge, USA.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1993.Five papers on wordnet.
Technical report, PrincetonUniversity.David Milward.
2000.
Distributing representation forrobust interpretation of dialogue utterances.
In Pro-ceedings of ACL 2000, pages 133?141.Joakim Nivre.
2004.
Incrementality in determinis-tic dependency parsing.
In Frank Keller, StephenClark, Matthew Crocker, and Mark Steedman, edi-tors, Proceedings of the ACL Workshop IncrementalParsing: Bringing Engineering and Cognition To-gether, pages 50?57, Barcelona, Spain, July.
Asso-ciation for Computational Linguistics.M.
Rayner, B.A.
Hockey, and P. Bouillon.
2006.Putting Linguistics into Speech Recognition: TheRegulus Grammar Compiler.
CSLI Press, Chicago.Carolyn P. Rose.
2000.
A framework for robust se-mantic interpretation.
In Procs of NACL.Yoav Seginer.
2007.
Fast unsupervised incrementalparsing.
In Proceedings of ACL, Prague, Czech Re-public.E.
Selkirk.
1984.
Phonology and Syntax.
The rela-tion between sound and structure.
MIT Press, Cam-bridge, USA.Alexander Siebert and David Schlangen.
2008.
A sim-ple method for resolution of definite reference in ashared visual context.
In Procs of SIGdial, Colum-bus, Ohio.Alexander Siebert.
2007.
Maschinelles Lernender Bedeutung referenzierender und relationalerAusdru?cke in einem Brettspieldialog.
Diploma The-sis, University of Potsdam.Gabriel Skantze and David Schlangen.
2009.
Incre-mental dialogue processing in a micro-domain.
InProceedings of EACL 2009, Athens, Greece, April.Gabriel Skantze.
2007.
Error Handling in Spoken Di-alogue Systems.
Ph.D. thesis, KTH, Stockholm.Scott C. Stoness, Joel Tetreault, and James Allen.2004.
Incremental parsing with reference inter-action.
In Frank Keller, Stephen Clark, MatthewCrocker, and Mark Steedman, editors, Proceedingsof the ACL Workshop Incremental Parsing: Bring-ing Engineering and Cognition Together, Barcelona,Spain, July.David R. Traum, Lenhart K. Schubert, Massimo Poe-sio, Nathaniel G. Martin, Marc Light, Chung HeeHwang, P. Heeman, George Ferguson, and JamesAllen.
1996.
Knowledge representation in thetrains-93 conversation system.
International Jour-nal of Expert Systems, 9(1):173?223.Wayne H. Ward.
2008.
The phoenix parser user man-ual.
http://cslr.colorado.edu/ whw/phoenix/phoenix-manual.htm.Klaus Zechner.
1998.
Automatic construction offrame representations for spontaneous speech in un-restricted domains.
In Proceedings of COLING-ACL 1998, Montreal, Canada.73
