Proceedings of the Fifth Law Workshop (LAW V), pages 101?109,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsAssessing the Practical Usabilityof an Automatically Annotated CorpusMd.
Faisal Mahbub Chowdhury ?
?
and Alberto Lavelli ??
Human Language Technology Research Unit, Fondazione Bruno Kessler, Trento, Italy?
Department of Information Engineering and Computer Science, University of Trento, Italy{chowdhury,lavelli}@fbk.euAbstractThe creation of a gold standard corpus (GSC)is a very laborious and costly process.
Silverstandard corpus (SSC) annotation is a very re-cent direction of corpus development whichrelies on multiple systems instead of humanannotators.
In this paper, we investigate thepractical usability of an SSC when a machinelearning system is trained on it and tested onan unseen benchmark GSC.
The main focus ofthis paper is how an SSC can be maximally ex-ploited.
In this process, we inspect several hy-potheses which might have influenced the ideaof SSC creation.
Empirical results suggest thatsome of the hypotheses (e.g.
a positive impactof a large SSC despite of having wrong andmissing annotations) are not fully correct.
Weshow that it is possible to automatically im-prove the quality and the quantity of the SSCannotations.
We also observe that consideringonly those sentences of SSC which contain an-notations rather than the full SSC results in aperformance boost.1 IntroductionThe creation of a gold standard corpus (GSC) isnot only a very laborious task due to the manual ef-fort involved but also a costly and time consumingprocess.
However, the importance of the GSC to ef-fectively train machine learning (ML) systems can-not be underestimated.
Researchers have been tryingfor years to find alternatives or at least some com-promise.
As a result, self-training, co-training andunsupervised approaches targeted for specific tasks(such as word sense disambiguation, syntactic pars-ing, etc) have emerged.
In the process of these re-searches, it became clear that the size of the (manu-ally annotated) training corpus has an impact on thefinal outcome.Recently an initiative is ongoing in the context ofthe European project CALBC1 which aims to createa large, so called silver standard corpus (SSC) us-ing harmonized annotations automatically producedby multiple systems (Rebholz-Schuhmann et al,2010; Rebholz-Schuhmann et al, 2010a; Rebholz-Schuhmann et al, 2010b).
The basic idea is thatindependent biomedical named entity recognition(BNER) systems annotate a large corpus of biomed-ical articles without any restriction on the methodol-ogy or external resources to be exploited.
The differ-ent annotations are automatically harmonized usingsome criteria (e.g.
minimum number of systems toagree on a certain annotation) to yield a consensusbased corpus.
This consensus based corpus is calledsilver standard corpus because, differently from aGSC, it is not created exclusively by human anno-tators.
Several factors can influence the quantity andquality of the annotations during SSC development.These include varying performance, methodology,annotation guidelines and resources of the SSC an-notation systems (henceforth annotation systems).The annotation of SSC in the framework of theCALBC project is focused on (bio) entity mentions(a specific application of the named entity recogni-tion (NER)2 task).
However, the idea of SSC cre-ation might also be applied to other types of anno-tations, e.g.
annotation of relations among entities,annotation of treebanks and so on.
Hence, if it can be1http://www.ebi.ac.uk/Rebholz-srv/CALBC/project.html2Named entity recognition is the task of locating boundariesof the entity mentions in a text and tagging them with their cor-responding semantic types (e.g.
person, location, disease andso on).101shown that an SSC is a useful resource for the NERtask, similar resources can be developed for anno-tation of information other than entities and utilizedfor the relevant natural language processing (NLP)tasks.The primary objective of SSC annotation is tocompensate the cost, time and manual effort re-quired for a GSC.
The procedure of SSC develop-ment is inexpensive, fast and yet capable of yieldinghuge amount of annotated data.
These advantagestrigger several hypotheses.
For example:?
The size of annotated training corpus alwaysplays a crucial role in the performance of MLsystems.
If the annotation systems have veryhigh precision and somewhat moderate recall,they would be also able to annotate automat-ically a huge SSC which would have a goodquality of annotations.
So, one might assumethat, even if such an SSC may contain wrongand missing annotations, a relatively 15 or 20times bigger SSC than a smaller GSC shouldallow an ML based system to ameliorate the ad-verse effects of the erroneous annotations.?
Rebholz-Schuhmann et al (2010) hypothesizedthat an SSC might serve as an approximation ofa GSC.?
In the absence of a GSC, it is expected thatML systems would be able to exploit the har-monised annotations of an SSC to annotate un-seen text with reasonable accuracy.?
An SSC could be used to semi-automate the an-notations of a GSC.
However, in that case, itis expected that the annotation systems wouldhave very high recall.
One can assume thatconverting an SSC into a GSC would be lesstime consuming and less costly than develop-ing a GSC from scratch.All these hypotheses are yet to be verified.
Nev-ertheless, once we have an SSC annotated with cer-tain type of information, the main question would behow this corpus can be maximally exploited giventhe fact that it might be created by annotation sys-tems that used different resources and possibly notthe same annotation guidelines.
This question is di-rectly related to the practical usability of an SSC,which is the focus of this paper.Taking the aforementioned hypotheses into ac-count, our goal is to investigate the following re-search questions which are fundamental to the max-imum exploitation of an SSC:1.
How can the annotation quality of an SSC beimproved automatically?2.
How would a system trained on an SSC per-form if tested on an unseen benchmark GSC?3.
Can an SSC combined with a GSC produce abetter trained system?4.
What would be the impact on system perfor-mance if unannotated sentences3 are removedfrom an SSC?5.
What would be the effects of the variation inthe size of an SSC on precision and recall?Our goal is not to judge the procedure of SSC cre-ation, rather our objective is to examine how an SSCcan be exploited automatically and maximally for aspecific task.
Perhaps this would provide useful in-sights to re-evaluate the approach of SSC creation.For our experiments, we use a benchmark GSCcalled the BioCreAtIvE II GM corpus (Smith etal., 2008) and the CALBC SSC-I corpus (Rebholz-Schuhmann et al, 2010a).
Both of these corporaare annotated with genes.
Our motivation behind thechoice of a gene annotated GSC for the SSC evalu-ation is that ML based BNER for genes has alreadyachieved a sufficient level of maturity.
This is notthe case for other important bio-entity types, primar-ily due to the absence of training GSC of adequatesize.
In fact, for many bio-entity types there exist noGSC.
If we can achieve a reasonably good baselinefor gene mention identification by maximizing theexploitation of SSC, we might be able to apply al-most similar strategies to exploit SSC for other bio-entity types, too.The remaining of this paper is organised as fol-lows.
Section 2 includes brief discussion of the re-lated work.
Apart from mentioning the related liter-ature, this section also underlines the difference of3For the specific SSC that we use in this work, unannotatedsentences correspond to those sentences that contain no geneannotation.102SSC development with respect to approaches suchas self-training and co-training.
Then in Section 3,we describe the data used in our experiments and theexperimental settings.
Following that, in Section 4,empirical results are presented and discussed.
Fi-nally, we conclude with a description of what welearned from this work in Section 5.2 Related WorkAs mentioned, the concept of SSC has been initi-ated by the CALBC project (Rebholz-Schuhmann etal., 2010a; Rebholz-Schuhmann et al, 2010).
So far,two versions of SSC have been released as part of theproject.
The CALBC SSC-I has been harmonisedfrom the annotations of the systems provided bythe four project partners.
Three of them are dictio-nary based systems while the other is an ML basedsystem.
The systems utilized different types of re-sources such as GENIA corpus (Kim et al, 2003),Entrez Genes4, Uniprot5, etc.
The CALBC SSC-II corpus has been harmonised from the annotationsdone by the 11 participants of the first CALBC chal-lenge and the project partners.6 Some of the par-ticipants have used the CALBC SSC-I versions fortraining while others used various gene databases orbenchmark GSCs such as the BioCreAtIvE II GMcorpus.One of the key questions regarding an SSC wouldbe how close its annotation quality is to a corre-sponding GSC.
On the one hand, every GSC con-tains its special view of the correct annotation of agiven corpus.
On the other hand, an SSC is createdby systems that might be trained with resources hav-ing different annotation standards.
So, it is possiblethat the annotations of an SSC significantly differwith respect to a manually annotated (i.e., gold stan-dard) version of the same corpus.
This is becausehuman experts are asked to follow specific annota-tion guidelines.Rebholz-Schuhmann and Hahn (2010c) did an in-trinsic evaluation of the SSC where they created an4http://jura.wi.mit.edu/entrez gene/5http://www.uniprot.org/6See proceedings of the 1st CALBC Work-shop, 2010, Editors: Dietrich Rebholz-Schuhmannand Udo Hahn (http://www.ebi.ac.uk/Rebholz-srv/CALBC/docs/FirstProceedings.pdf) for details.SSC and a GSC on a dataset of 3,236 Medline7 ab-stracts.
They were not able to make any specific con-clusion whether the SSC is approaching to the GSC.They were of the opinion that SSC annotations aremore similar to terminological resources.Hahn et al (2010) proposed a policy where sil-ver standards can be dynamically optimized and cus-tomized on demand (given a specific goal function)using a gold standard as an oracle.
The gold stan-dard is used for optimization only, not for trainingfor the purpose of SSC annotation.
They argued thatthe nature of diverging tasks to be solved, the lev-els of specificity to be reached, the sort of guide-lines being preferred, etc should allow prospectiveusers of an SSC to customize one on their own andnot stick to something that is already prefabricatedwithout concrete application in mind.Self-training and co-training are two of the exist-ing approaches that have been used for compensat-ing the lack of a training GSC with adequate sizein several different tasks such as word sense disam-biguation, semantic role labelling, parsing, etc (Ngand Cardie, 2003; Pierce and Cardie, 2004; Mc-Closky et al, 2006; He and Gildea, 2006).
Accord-ing to Ng and Cardie (2003), self-training is the pro-cedure where a committee of classifiers are trainedon the (gold) annotated examples to tag unannotatedexamples independently.
Only those new annota-tions to which all the classifiers agree are added tothe training set and classifiers are retrained.
Thisprocedure repeats until a stop condition is met.
Ac-cording to Clark et al (2003), self-training is a pro-cedure in which ?a tagger is retrained on its own la-beled cache at each round?.
In other words, a sin-gle classifier is trained on the initially (gold) anno-tated data and then applied on a set of unannotateddata.
Those examples meeting a selection criterionare added to the annotated dataset and the classifieris retrained on this new data set.
This procedure cancontinue for several rounds as required.Co-training is another weakly supervised ap-proach (Blum and Mitchell, 1998).
It applies forthose tasks where each of the two (or more) sets offeatures from the initially (gold) annotated trainingdata is sufficient to classify/annotate the unannotateddata (Pierce and Cardie, 2001; Pierce and Cardie,7http://www.nlm.nih.gov/databases/databases medline.html1032004; He and Gildea, 2006).
As with SSC annota-tion and self-training, it also attempts to increase theamount of annotated data by making use of unanno-tated data.
The main idea of co-training is to repre-sent the initially annotated data using two (or more)separate feature sets, each called a ?view?.
Then,two (or more) classifiers are trained on those viewsof the data which are then used to tag new unanno-tated data.
From this newly annotated data, the mostconfident predictions are added to the previously an-notated data.
This whole process may continue forseveral iterations.
It should be noted that, by limit-ing the number of views to one, co-training becomesself-training.Like the SSC, the multiple classifier approachof self-training and co-training, as described above,adopts the same vision of utilizing automatic sys-tems for producing the annotation.
Apart from that,SSC annotation is completely different from bothself-training and co-training.
For example, classi-fiers in self-training and co-training utilizes the same(manually annotated) resource for their initial train-ing.
But SSC annotation systems do not necessar-ily use the same resource.
Both self-training andco-training are weakly supervised approaches wherethe classifiers are based on supervised ML tech-niques.
In the case of SSC annotation, the annota-tion systems can be dictionary based or rule based.This attractive flexibility allows SSC annotation tobe a completely unsupervised approach since theannotation systems do not necessarily need to betrained.3 Experimental settingsWe use the BioCreAtIvE II GM corpus (henceforth,only the GSC) for evaluation of an SSC.
The trainingcorpus in the GSC has in total 18,265 gene annota-tions in 15,000 sentences.
The GSC test data has6,331 annotations in 5,000 sentences.Some of the CALBC challenge participants haveused the BioCreAtIvE II GM corpus for training toannotate gene/protein in the CALBC SSC-II corpus.We wanted our benchmark corpus and benchmarkcorpus annotation to be totally unseen by the sys-tems that annotated the SSC to be used in our experi-ments so that there is no bias in our empirical results.SSC-I satisfies this criteria.
So, we use the SSC-I(henceforth, we would refer the CALBC SSC-I assimply the SSC) in our experiments despite the factthat it is almost 3 times smaller than the SSC-II.The SSC has in total 137,610 gene annotations in316,869 sentences of 50,000 abstracts.Generally, using a customized dictionary of en-tity names along with annotated corpus boosts NERperformance.
However, since our objective is to ob-serve to what extent a ML system can learn fromSSC, we avoid the use of any dictionary.
We usean open source ML based BNER system namedBioEnEx8 (Chowdhury and Lavelli, 2010).
Thesystem uses conditional random fields (CRFs), andachieves comparable results (F1 score of 86.22% onthe BioCreAtIvE II GM test corpus) to that of theother state-of-the-art systems without using any dic-tionary or lexicon.One of the complex issues in NER is to come to anagreement regarding the boundaries of entity men-tions.
Different annotation guidelines have differentpreferences.
There may be tasks where a longer en-tity mention such as ?human IL-7 protein?
may beappropriate, while for another task a short one suchas ?IL-7?
is adequate (Hahn et al, 2010).However, usually evaluation on BNER corpora(e.g., the BioCreAtIvE II GM corpus) is performedadopting exact boundary match.
Given that we haveused the official evaluation script of the BioCre-AtIvE II GM corpus, we have been forced toadopt exact boundary match.
Considering a relaxedboundary matching (i.e.
the annotations might dif-fer in uninformative terms such as the, a, acute, etc.
)rather than exact boundary matching might providea slightly different picture of the effectiveness of theSSC usage.4 Results and analyses4.1 Automatically improving SSC qualityThe CALBC SSC-I corpus has a negligible num-ber of overlapping gene annotations (in fact, only 6).For those overlapping annotations, we kept only thelongest ones.
Our hypothesis is that a certain tokenin the same context can refer to (or be part of) onlyone concept name (i.e.
annotation) of a certain se-mantic group (i.e.
entity type).
After removing thesefew overlaps, the SSC has 137,604 annotations.
We8Freely available at http://hlt.fbk.eu/en/people/chowdhury/research104will refer to this version of the SSC as the initialSSC (ISSC).We construct a list9 using the lemmatized formof 132 frequently used words that appear in genenames.
These words cannot constitute a gene namethemselves.
If (the lemmatized form of) all thewords in a gene name belong to this list then thatgene annotation should be discarded.
We use this listto remove erroneous annotations in the ISSC.
Afterthis purification step, the total number of annotationsis reduced to 133,707.
We would refer to this versionas the filtered SSC (FSSC).Then, we use the post-processing module ofBioEnEx, first to further filter out possible wronggene annotations in the FSSC and then to automati-cally include potential gene mentions which are notannotated.
It has been observed that some of theannotated mentions in the SSC-I span only part ofthe corresponding token10.
For example, in the to-ken ?IL-2R?, only ?IL-?
is annotated.
We extendthe post-processing module of BioEnEx to automat-ically identify all such types of annotations and ex-pand their boundaries when their neighbouring char-acters are alphanumeric.Following that, the extended post-processingmodule of BioEnEx is used to check in every sen-tence whether there exist any potential unannotatedmentions11 which differ from any of the annotatedmentions (in the same sentence) by a single charac-ter (e.g.
?IL-2L?
and ?IL-2R?
), number (e.g.
?IL-2R?
and ?IL-345R?)
or Greek letter (e.g.
?IFN-alpha?
and ?IFN-beta?).
After this step, the totalnumber of gene annotations is 144,375.
This meansthat we were able to remove/correct some specifictypes of errors and then further expand the totalnumber of annotations (by including entities not an-notated in the original SSC) up to 4.92% with re-spect to the ISSC.
We will refer to this expandedversion of the SSC as the processed SSC (PSSC).When BioEnEx is trained on the above versions9The words are collected fromhttp://pir.georgetown.edu/pirwww/iprolink/general nameand the annotation guideline of GENETAG (Tanabe et al,2005).10By token we mean a sequence of consecutive non-whitespace characters.11Any token or sequence of tokens is considered to verifywhether it should be annotated or not, if its length is more than2 characters excluding digits and Greek letters.TP FP FN P R F1ISSC 2,396 594 3,935 80.13 37.85 51.41FSSC 2,518 557 3,813 81.89 39.77 53.54PSSC 2,606 631 3,725 80.51 41.16 54.47Table 1: The results of experiments when trained withdifferent versions of the SSC and tested on the GSC testdata.of the SSC and tested on the GSC test data, we ob-served an increase of more than 3% of F1 score be-cause of the filtering and expansion (see Table 1).One noticeable characteristic in the results is that thenumber of annotations obtained (i.e.
TP+FP12) bytraining on any of the versions of the SSC is almosthalf of the actual number annotations of the GSC testdata.
This has resulted in a low recall.
There couldbe mainly two reasons behind this outcome:?
First of all, it might be the case that a consid-erable number of gene names are not annotatedinside the SSC versions.
As a result, the fea-tures shared by the annotated gene names (i.e.TP) and unnannotated gene names (i.e.
FN)might not have enough influence.?
There might be a considerable number ofwrong annotations which are actually not genes(i.e.
FP).
Consequently, a number of bad fea-tures might be collected from those wrong an-notations which are misleading the trainingprocess.To verify the above conditions, it would be re-quired to annotate the huge CALBC SSC manually.This would be not feasible because of the cost ofhuman labour and time.
Nevertheless, we can try tomeasure the state of the above conditions roughly byusing only annotated sentences (i.e.
sentences con-taining at least one annotation) and varying the sizeof the corpus, which are the subjects of our next ex-periments.12TP (true positive) = corresponding annotation done by thesystem is correct, FP (false positive) = corresponding anno-tation done by the system is incorrect, FN (false negative) =corresponding annotation is correct but it is not annotated bythe system.105Figure 1: Graphical representation of the experimentalresults with varying size of the CSSC.4.2 Impact of annotated sentences anddifferent sizes of the SSCWe observe that only 77,117 out of the 316,869sentences in the PSSC contain gene annotations.We will refer to the sentences having at least onegene annotation collectively as the condensed SSC(CSSC).
Table 2 and Figure 1 show the results whenwe used different portions of the CSSC for training.There are four immediate observations on theabove results:?
Using the full PSSC, we obtain total (i.e.TP+FP) 3,237 annotations on the GSC testdata.
But when we use only annotated sen-tences of the PSSC (i.e.
the CSSC), the totalnumber of annotations is 4,562, i.e.
there is anincrement of 40.93%.?
Although we have a boost in F1 score due to theincrease in recall using the CSSC in place of thePSSC, there is a considerable drop in precision.?
The number of FP is almost the same for theusage of 10-75% of the CSSC.?
The number of FN kept decreasing (and TPkept increasing) for 10-75% of the CSSC.These observations can be interpreted as follows:?
Unannotated sentences inside the SSC in real-ity contain many gene annotations; so the in-clusion of such sentences misleads the trainingprocess of the ML system.?
Some of the unannotated sentences actuallydo not contain any gene names, while otherswould contain such names but the automaticannotations missed them.
As a consequence,the former sentences contain true negative ex-amples which could provide useful features thatcan be exploited during training so that less FPsare produced (with a precision drop using theCSSC).
So, instead of simply discarding all theunannotated sentences, we could adopt a filter-ing strategy that tries to distinguish between thetwo classes of sentences above.?
The experimental results with the increasingsize of the CSSC show a decrease in both pre-cision (74.55 vs 76.17) and recall (53.72 vs54.04).
We plan to run again these experimentswith different randomized splits to better assessthe performance.?
Even using only 10% of the whole CSSC doesnot produce a drastic difference with the resultswhen the full CSSC is used.
This indicates thatperhaps the more CSSC data is fed, the morethe system tends to overfit.?
It is evident that the more the size of the CSSCincreases, the lower the improvement of F1score, if the total number of annotations inthe newly added sentences and the accuracy ofthe annotations are not considerably higher.
Itmight be not surprising if, after the addition ofmore sentences in the CSSC, the F1 score dropsfurther rather than increasing.
The assumptionthat having a huge SSC would be beneficiarymight not be completely correct.
There mightbe some optimal limit of the SSC (dependingon the task) that can provide maximum bene-fits.4.3 Training with the GSC and the SSCtogetherOur final experiments were focused on whether it ispossible to improve performance by simply merg-ing the GSC training data with the PSSC and theCSSC.
The PSSC has almost 24 times the num-ber of sentences and almost 8 times the number ofgene annotations than the GSC.
There is a possibilitythat, when we do a simple merge, the weight of the106Total tokens in the corpus No of annotated genes TP FP FN P R F1PSSC 6,955,662 144,375 2,606 631 3,725 80.51 41.16 54.47100% of CSSC 1,983,113 144,375 3,401 1,161 2,930 74.55 53.72 62.4475% of CSSC 1,487,823 108,213 3,421 1,070 2,910 76.17 54.04 63.2250% of CSSC 992,392 72,316 3,265 1,095 3,066 74.89 51.57 61.0825% of CSSC 494,249 35,984 3,179 1,048 3,152 75.21 50.21 60.2210% of CSSC 196,522 14,189 2,988 1,097 3,343 73.15 47.20 57.37Table 2: The results of SSC experiments with varying size of the CSSC = condensed SSC (i.e.
sentences containingat least one annotation).
SSC size = 316,869 sentences.
CSSC size = 77,117.TP FP FN P R F1GSC 5,373 759 958 87.62 84.87 86.22PSSC +GSC 3,745 634 2,586 85.52 59.15 69.93PSSC +GSC * 8 4,163 606 2,168 87.29 65.76 75.01CSSC +GSC * 8 4,507 814 1,824 84.70 71.19 77.36Table 3: The results of experiments by training on theGSC training data merged with the PSSC and the CSSC.gold annotations would be underestimated.
So, apartfrom doing a simple merge, we also try to balancethe annotations of the two corpora.
There are twooptions to do this ?
(i) by duplicating the GSC train-ing corpus 8 times to make its total number of anno-tations equal to that of the PSSC, or (ii) by choos-ing randomly a portion of the PSSC that would havealmost similar amount of annotations as that of theGSC.
We choose the 1st option.Unfortunately, when an SSC (i.e.
the PSSC or theCSSC) is combined with the GSC, the result is farbelow than that of using the GSC only (see Table 3).Again, low recall is the main issue partly due to thelower number of annotations (i.e.
TP+FP) done bythe system trained on an SSC and the GSC instead ofthe GSC only.
As we know, a GSC is manually an-notated following precise guidelines, while an SSCis annotated with automatic systems that do not nec-essarily follow the same guidelines as a GSC.
So,it would not have been surprising if the number ofannotations were high (since we have much biggertraining corpus due to SSC) but precision were low.But in practice, precision obtained by combining anSSC and the GSC is almost as high as the precisionachieved using the GSC.One reason for the lower number of annotationsmight be the errors that have been propagated in-side the SSC.
Some of the systems that have beenused for the annotation of the SSC might have lowrecall.
As a result, during harmonization of their an-notations several valid gene mentions might not havebeen included13.One other possible reason could be the differencein the entity name boundaries in the GSC and anSSC.
We have checked some of the SSC annotationsrandomly.
It appears that in those annotated entitynames some relevant (neighbouring) words (in thecorresponding sentences) are not included.
It is mostlikely that the SSC annotation systems had disagree-ments on those words.When the annotations of the GSC were givenhigher preference (by duplicating), there is a sub-stantial improvement in the F1 score, although stilllower than the result with the GSC only.5 ConclusionsThe idea of SSC development is simple and yet at-tractive.
Obtaining better results on a test datasetby combining output of multiple (accurate and di-verse14) systems is not new (Torii et al, 2009; Smithet al, 2008).
But adopting this strategy for cor-13There can be two reasons for this ?
(i) when a certain validgene name is not annotated by any of the annotation systems,and (ii) when only a few of those systems have annotated thevalid name but the total number of such systems is below thanthe minimum required number of agreements, and hence thegene name is not considered as an SSC annotation.14A system is said to be accurate if its classification perfor-mance is better than a random classification.
Two systems areconsidered diverse if they do not make the same classificationmistakes.
(Torii et al, 2009)107pus development is a novel and unconventional ap-proach.
Some natural language processing tasks (es-pecially the new ones) lack adequate GSCs to beused for the training of ML based systems.
For suchtasks, domain experts can provide patterns or rulesto build systems that can be used to annotate an ini-tial version of SSC.
Such systems might lack highrecall but are expected to have high precision.
Al-ready available task specific lexicons or dictionariescan also be utilized for SSC annotation.
Such aninitial version of SSC can be later enriched usingautomatic process which would utilize existing an-notations in the SSC.With this vision in mind, we pose ourselves sev-eral questions (see Section 1) regarding the practi-cal usability and exploitation of an SSC.
Our experi-ments are conducted on a publicly available biomed-ical SSC developed for the training of biomedicalNER systems.
For the evaluation of a state-of-the-art ML system trained on such an SSC, we use awidely used benchmark biomedical GSC.In the search of answers for our questions, we ac-cumulate several important empirical observations.We have been able to automatically reduce the num-ber of erroneous annotations from the SSC and in-clude unannotated potential entity mentions simplyusing the annotations that the SSC already provides.Our techniques have been effective for improvingthe annotation quality as there is a considerable in-crement of F1 score (almost 11% higher when weuse CSSC instead of using ISSC; see Table 1 and 2).We also observe that it is possible to obtain morethan 80% of precision using the SSC.
But recall re-mains quite low, partly due to the low number ofannotations provided by the system trained with theSSC.
Perhaps, the entity names in the SSC that aremissed by the annotation systems is one of the rea-sons for that.Perhaps, the most interesting outcome of thisstudy is that, if only annotated sentences (whichwe call condensed corpus) are considered, then thenumber of annotations as well as the performanceincreases significantly.
This indicates that manyunannotated sentences contain annotations missedby the automatic annotation systems.
However, itappears that correctly unannotated sentences influ-ence the achievement of high precision.
Maybe amore sophisticated approach should be adopted in-stead of completely discarding the unannotated sen-tences, e.g.
devising a filter able to distinguishbetween relevant unannotated sentences (i.e., thosethat should contain annotations) from non-relevantones (i.e., those that correctly do not contain any an-notation).
Measuring lexical similarity between an-notated and unannotated sentences might help in thiscase.We notice the size of an SSC affects performance,but increasing it above a certain limit does notalways guarantee an improvement of performance(see Figure 1).
This rejects the hypothesis that hav-ing a much larger SSC should allow an ML basedsystem to ameliorate the effect of having erroneousannotations inside the SSC.Our empirical results show that combining GSCand SSC do not improve results for the particulartask of NER, even if GSC annotations are givenhigher weights (through duplication).
We assumethat this is partly due to the variations in the guide-lines of entity name boundaries15.
These impact thelearning of the ML algorithm.
For other NLP taskswhere the possible outcome is boolean (e.g.
relationextraction, i.e.
whether a particular relation holdsbetween two entities or not), we speculate the resultsof such combination might be better.We use a CRF based ML system for our exper-iments.
It would be interesting to see whether theobservations are similar if a system with a differentML algorithm is used.To conclude, this study suggests that an automat-ically pre-processed SSC might already contain an-notations with reasonable quality and quantity, sinceusing it we are able to reach more than 62% of F1score.
This is encouraging since in the absence ofa GSC, an ML system would be able to exploit anSSC to annotate unseen text with a moderate (if nothigh) accuracy.
Hence, SSC development might bea good option to semi-automate the annotation of aGSC.AcknowledgmentsThis work was carried out in the context of the project?eOnco - Pervasive knowledge and data management incancer care?.
The authors would like to thank PierreZweigenbaum for useful discussion, and the anonymousreviewers for valuable feedback.15For example, ?human IL-7 protein?
vs ?IL-7?.108ReferencesAvrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of the 11th Annual Conference on Computationallearning theory (COLT?98), pages 92?100.Md.
Faisal Mahbub Chowdhury and Alberto Lavelli.2010.
Disease mention recognition with specific fea-tures.
In Proceedings of the Workshop on BiomedicalNatural Language Processing (BioNLP 2010), 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 83?90, Uppsala, Sweden, July.Stephen Clark, James R. Curran, and Miles Osborne.2003.
Bootstrapping POS taggers using unlabelleddata.
In Proceedings of the 7th Conference on NaturalLanguage Learning (CoNLL-2003), pages 49?55.Udo Hahn, Katrin Tomanek, Elena Beisswanger, and ErikFaessler.
2010.
A proposal for a configurable silverstandard.
In Proceedings of the 4th Linguistic Anno-tation Workshop, 48th Annual Meeting of the Associ-ation for Computational Linguistics, pages 235?242,Uppsala, Sweden, July.Shan He and Daniel Gildea.
2006.
Self-training andco-training for semantic role labeling: Primary report.Technical report, University of Rochester.Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichiTsujii.
2003.
Genia corpus - semantically annotatedcorpus for bio-textmining.
Bioinformatics, 19(Suppl1):i180?182.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adapta-tion.
In Proceedings of the 21st International Con-ference on Computational Linguistics, pages 337?344,Sydney, Australia.Vincent Ng and Claire Cardie.
2003.
Weakly supervisednatural language learning without redundant views.In Proceedings of the 2003 Human Language Tech-nology Conference of the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL-2003), pages 173?180.David Pierce and Claire Cardie.
2001.
Limitations ofco-training for natural language learning from largedatasets.
In Proceedings of the 2001 Conference onEmpirical Methods in Natural Language Processing(EMNLP-2001), pages 1?9.David Pierce and Claire Cardie.
2004.
Co-training andself-training for word sense disambiguation.
In Pro-ceedings of the 8th Conference on Computational Nat-ural Language Learning (CoNLL-2004), pages 33?40.Dietrich Rebholz-Schuhmann and Udo Hahn.
2010c.Silver standard corpus vs. gold standard corpus.
InProceedings of the 1st CALBC Workshop, Cambridge,U.K., June.Dietrich Rebholz-Schuhmann, Antonio Jimeno, Chen Li,Senay Kafkas, Ian Lewin, Ning Kang, Peter Corbett,David Milward, Ekaterina Buyko, Elena Beisswanger,Kerstin Hornbostel, Alexandre Kouznetsov, ReneWitte, Jonas B Laurila, Christopher JO Baker, Chen-JuKuo, Simon Clematide, Fabio Rinaldi, Richrd Farkas,Gyrgy Mra, Kazuo Hara, Laura Furlong, MichaelRautschka, Mariana Lara Neves, Alberto Pascual-Montano, Qi Wei, Nigel Collier, Md.
Faisal Mah-bub Chowdhury, Alberto Lavelli, Rafael Berlanga,Roser Morante, Vincent Van Asch, Walter Daele-mans, Jose?
Lu?
?s Marina, Erik van Mulligen, Jan Kors,and Udo Hahn.
2010.
Assessment of NER solu-tions against the first and second CALBC silver stan-dard corpus.
In Proceedings of the fourth Interna-tional Symposium on Semantic Mining in Biomedicine(SMBM?2010), October.Dietrich Rebholz-Schuhmann, Antonio Jose?
Jimeno-Yepes, Erik van Mulligen, Ning Kang, Jan Kors, DavidMilward, Peter Corbett, Ekaterina Buyko, Elena Beis-swanger, and Udo Hahn.
2010a.
CALBC silver stan-dard corpus.
Journal of Bioinformatics and Computa-tional Biology, 8:163?179.Dietrich Rebholz-Schuhmann, Antonio Jose?
Jimeno-Yepes, Erik van Mulligen, Ning Kang, Jan Kors, DavidMilward, Peter Corbett, Ekaterina Buyko, KatrinTomanek, Elena Beisswanger, and Udo Hahn.
2010b.The CALBC silver standard corpus for biomedicalnamed entities ?
a study in harmonizing the contri-butions from four independent named entity taggers.In Proceedings of the 7th International conference onLanguage Resources and Evaluation (LREC?10), Val-letta, Malta, May.Larry Smith, Lorraine Tanabe, Rie Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-ShiLin, Roman Klinger, Christoph Friedrich, KuzmanGanchev, Manabu Torii, Hongfang Liu, Barry Had-dow, Craig Struble, Richard Povinelli, Andreas Vla-chos, William Baumgartner, Lawrence Hunter, BobCarpenter, Richard Tsai, Hong-Jie Dai, Feng Liu,Yifei Chen, Chengjie Sun, Sophia Katrenko, PieterAdriaans, Christian Blaschke, Rafael Torres, MarianaNeves, Preslav Nakov, Anna Divoli, Manuel Mana-Lopez, Jacinto Mata, and W John Wilbur.
2008.Overview of BioCreAtIvE II gene mention recogni-tion.
Genome Biology, 9(Suppl 2):S2.Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-ten, and W John Wilbur.
2005.
GENETAG: atagged corpus for gene/protein named entity recogni-tion.
BMC Bioinformatics, 6(Suppl 1):S3.Manabu Torii, Zhangzhi Hu, Cathy H Wu, and Hong-fang Liu.
2009.
Biotagger-GM: a gene/protein namerecognition system.
Journal of the American MedicalInformatics Association : JAMIA, 16:247?255.109
