Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 30?37,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsLatent Semantic Transliteration using Dirichlet MixtureMasato Hagiwara Satoshi SekineRakuten Institute of Technology, New York215 Park Avenue South, New York, NY{masato.hagiwara, satoshi.b.sekine}@mail.rakuten.comAbstractTransliteration has been usually recog-nized by spelling-based supervised models.However, a single model cannot deal withmixture of words with different origins,such as ?get?
in ?piaget?
and ?target?.Li et al (2007) propose a class translit-eration method, which explicitly modelsthe source language origins and switchesthem to address this issue.
In contrastto their model which requires an explic-itly tagged training corpus with languageorigins, Hagiwara and Sekine (2011) haveproposed the latent class transliterationmodel, which models language origins aslatent classes and train the transliterationtable via the EM algorithm.
However, thismodel, which can be formulated as uni-gram mixture, is prone to overfitting sinceit is based on maximum likelihood estima-tion.
We propose a novel latent seman-tic transliteration model based on Dirichletmixture, where a Dirichlet mixture prioris introduced to mitigate the overfittingproblem.
We have shown that the pro-posed method considerably outperform theconventional transliteration models.1 IntroductionTransliteration (e.g., ??????
barakuobama ?Barak Obama?)
is phonetic transla-tion between languages with different writingsystems, which is a major way of importingforeign words into different languages.
Su-pervised, spelling-based grapheme-to-graphememodels such as (Brill and Moore, 2000; Li etal., 2004), which directly align characters in thetraining corpus without depending on phoneticinformation, and statistically computing theircorrespondence, have been a popular method todetect and/or generate transliterations, in con-trast to phonetic-based methods such as (Knightand Jonathan, 1998).
However, single, mono-lithic models fail to deal with sets of foreignwords with multiple language origins mixed to-gether.
For example, the ?get?
part of ?pi-aget / ????piaje?
and ?target / ?????
t?getto?
differ in pronunciation and spellingcorrespondence depending on their source lan-guages, which are French and English in thiscase.To address this issue, Li et al (2007) haveproposed class transliteration model, which ex-plicitly models and classifies classes of languages(such as Chinese Hanzi, Japanese Katakana,and so on) and genders, and switches corre-sponding transliteration models based on theinput.
This model requires training sets oftransliterated word pairs tagged with languageorigin, which is difficult to obtain.
Hagiwaraand Sekine proposed the latent class translitera-tion (LCT) model (Hagiwara and Sekine, 2011),which models source language origins as directlyunobservable latent classes and applies appro-priate transliteration models to given transliter-ation pairs.
The model parameters are learnedfrom corpora without language origins in an un-supervised manner.
This enables us to correctlyassign latent classes for English and French to?piaget / ????piaje?
and ?target / ????30?
t?getto?
and to identify their transliterationcorrespondence correctly.
However, this modelis based on maximum likelihood estimation onmultinomials and thus sensitive to noise in thetraining data such as transliteration pairs withirregular pronunciation, and tends to overfit thedata.Considering the atomic re-writing unit(transliteration unit, or TU, e.g., ?get / ???
getto?)
as a word, and a transliteration pairas a document consisting of a word sequence,class-based transliteration can be modeled bythe perfect analogy to document topic modelsproposed in tha past.
In fact, the LCT model,where the transliteration probability is definedby a mixture of multinomials, can be regardedas a variant of a topic model, namely UnigramMixuture (UM) (Nigam et al, 2000).
Therehas been an extension of unigram mixture pro-posed (Sj?lander et al, 1996; Yamamoto andSadamitsu, 2005) which introduces a Dirichletmixture distribution as a prior and alleviates theoverfitting problem.
We can expect to improvethe transliteration accuracy by formulating thetransliteration problem using a similar frame-work to these topic models.In this paper, we formalize class-basedtransliteration based on language origins in theframework of topic models.
We then propose thelatent semantic transliteration model based onDirichlet mixture (DM-LST).
We show throughexperiments that it can significantly improve thetransliteration performance by alleviating theoverfitting issue.Note that we tackle the task of transliterationgeneration in this paper, in contrast to translit-eration recognition.
A transliteration generationtask is, given an input word s (such as ?piaget?
),the system is asked to generate from scratchthe most probable transliterated word t (e.g.,?????piaje?).
The transliteration recogni-tion task, on the other hand, is to induce themost probable transliteration t?
?
T such thatt?
= arg maxt?T P (?s, t?)
given the input words and a pool of transliteration candidates T .
Wecall P (?s, t?)
transliteration model in this paper.This model can be regarded as the hy-brid of an unsupervised alignment techniquefor transliteration and class-based translitera-tion.
Related researches for the former in-clude (Ahmad and Kondrak, 2005), who esti-mate character-based error probabilities fromquery logs via the EM algorithm.
For the lat-ter, Llitjos and Black (2001) showed that sourcelanguage origins may improve the pronunciationof proper nouns in text-to-speech systems.The structure of this paper is as follows:we introduce the alpha-beta model(Brill andMoore, 2000) in Section 2, which is the most ba-sic spelling-based transliteration model on whichother models are based.
In the following Section3, we introduce and relate the joint source chan-nel (JSC) model (Li et al, 2004) to the alpha-beta model.
We describe the LCT model as anextension to the JSC model in Section 4.
InSection 5, we propose the DM-LST model, andshow the experimental results on transliterationgeneration in Section 6.2 Alpha-Beta ModelIn this section, we describe the alpha-betamodel, which is one of the simplest spelling-based transliteration models.
Though simple,the model has been shown to achieve betterperformance in tasks such as spelling correction(Brill and Moore, 2000), transliteration (Brill etal., 2001), and query alteration (Hagiwara andSuzuki, 2009).The method directly models spelling-basedre-writing probabilities of transliteration pairs.It is an extension to the normal edit distance,where the cost of operations (substitution, in-sertion, and deletion) is fixed to 1, and assigns aprobability to a string edit operation of the formsi ?
ti (si and ti are any substrings of length 0to w).
We call the unit operation of string re-writing ui = ?si, ti?
as transliteration unit (TU)as in (Li et al, 2004).
The total transliterationprobability of re-writing a word s to t is givenbyPAB(?s, t?)
= maxu1...uff?i=1P (ui), (1)where f is the number of TUs and u1...uf is anysequence of TUs (e.g., ?pi ??
a ??
get ?31???)
created by splitting up the input/outputtransliteration pair ?s, t?.
The above equa-tion can be interpreted as a problem of find-ing a TU sequence u1...uf which maximizes theprobability defined by the product of individ-ual probabilities of independent TUs.
After tak-ing the logarithm of the both sides, and regard-ing ?
logP (ui) as the cost of string substitutionsi ?
ti, the problem is equivalent to minimizingthe sum of re-writing costs, and therefore canbe efficiently solved by dynamic programmingas done in the normal edit distance.TU probabilities P (ui) are calculated from atraining set of transliteration pairs.
However,training sets usually lack alignment informationspecifying which characters in s correspondingwhich characters in t. Brill and Moore (2000)resorted to heuristics to align same charactersand to induce the alignment of string chunks.Hagiwara and Sekine (2011) converted JapaneseKatakana sequences into Roman alphabets be-cause their model also assumed that the stringssi and ti are expressed in the same alphabet sys-tem.
Our method on the contrary, does not posesuch assumption so that strings in different writ-ing systems (such as Japanese Katakana andEnglish alphabets, and Chinese characters andEnglish alphabets, etc.)
can be aligned withoutbeing converted to phonetic representation.
Forthis reason, we cannot adopt algorithms (suchas the one described in (Brill and Moore, 2000))which heuristically infer alignment based on thecorrespondence of the same characters.When applying this alpha-beta model, wecomputed TU probabilities by counting relativefrequencies of all the alignment possibilities for atransliteration pair.
For example, all the align-ment possibilities for a pair of strings ?abc?
and?xy?
are (a-x b-y c-?
), (a-x b-?
c-y), and (a-?
b-xc-y).
By considering merging up to two adjacentaligned characters in the first alignment, one ob-tains the following five aligned string pairs: a-x,b-y, c-?, ab-xy bc-y.
Note that all the translit-eration models described in this paper implic-itly depend on the parameter w indicating themaximum length of character n-grams.
We fixedw = 3 throughout this paper.3 Joint Source Channel ModelThe alpha-beta model described above hasshortcomings that the character alignment isfixed based on heuristics, and it cannot cap-ture the dependencies between TUs.
One ex-ample of such dependencies is the phenomenonthat the suffix ?-ed?
in English verbs followinga voiced consonant is pronounced /d/, whereasthe one followed by an unvoiced consonant is/t/.
This section describes the JSC model(Liet al, 2004), which was independently proposedfrom the alpha-beta model.
The JSC model isessentially equivalent to the alpha-beta modelexcept: 1) it can also incorporate higher orderof n-grams of TUs and 2) the TU statistics istaken not by fixing the heuristic initial align-ment but by iteratively updating via an EM-likealgorithm.In the JSC model, the transliteration proba-bility is defined by the n-gram probabilities ofTUs ui = ?si, ti?
as follows:PJSC(?s, t?)
=f?i=1P (ui|ui?n+1, ..., ui?1).
(2)Again, f is the number of TUs.
The TU n-gramprobabilities P (ui|ui?n+1, ..., ui?1) can be calcu-lated by the following iterative updates similarto the EM algorithm:1.
Set the initial alignment randomly.2.
E-step: Take the TU n-gram statistics fix-ing the current alignment, and update thetransliteration model.3.
M-step: Compute the alignment based onthe current transliteration model.
Thealignment is inferred by dynamic program-ming similar to the alpha-beta model.4.
Iterate the E- and M- step until conver-gence.Notice the alpha-beta model and the JSCmodel are both transliteration recognition mod-els.
In order to output a transliterated word tfor a given input s, we generated transliterationcandidates with high probability using a stack32s?s?append reduce(s,?)(s,?)shifts?m?(s,?)(s,?)smmmap.ap.ap.(s,?)
(m,?)(s,?)
(m,?)r.
?reduce (sm,??)smshifts:?
5.22 s:?
6.69m: ?
6.14m: ?
8.47th: ?
6.72?Figure 1: Overview of the stack decoder (generationof ????
sumisu?
from the input ?smith?
)decoder, whose overview is shown in Figure 1.One character in the input string s (which is?smith?
in the figure) is given at a time, whichis appended at the end of the last TUs for eachcandidate.
(the append operation in the fig-ure).
Next, the last TU of each candidate iseither reduced or shifted.
When it is reduced,top R TUs with highest probabilities are gener-ated and fixed referring to the TU table (shownin the bottom-left of the figure).
In Figure 1,two candidates, namely (?s?, ??
su?)
and (?s?,??
zu?)
are generated after the character ?s?
isgiven.
When the last TU is shifted, it remainsunchanged and unfixed for further updates.
Ev-ery time a single character is given, the translit-eration probability is computed using Eq.
2 foreach candidate, and all but the top-B candidateswith highest probabilities are discarded.
The re-duce width R and the beam width B were deter-mined using the determined using developmentsets, as mentioned in Section 6.4 Latent Class Transliteration ModelAs mentioned in Section 1, the alpha-betamodel and the JSC model build a single translit-eration model which is simply the monolithicaverage of training set statistics, failing to cap-ture the difference in the source language ori-gins.
Li et al (2004) address this issue by defin-ing classes c, i.e., the factors such as source lan-guage origins, gender, and first/last names, etc.which affect the transliteration probability.
Theauthors then propose the class transliterationmodel which gives the probability of s ?
t asfollows:PLI(t|s) =?cP (t, c|s) =?cP (c|s)P (t|c, s) (3)However, this model requires a training setexplicitly tagged with the classes.
Instead ofassigning an explicit class c to each transliter-ated pair, Hagiwara and Sekine (2011) introducea random variable z which indicates implicitclasses and conditional TU probability P (ui|z).The latent class transliteration (LCT) model isthen defined as1:PLCT(?s, t?)
=K?z=1P (z)f?i=1P (ui|z) (4)where K is the number of the latent classes.The latent classes z correspond to classes suchas the language origins and genders mentionedabove, shared by sets of transliterated pairs withsimilar re-writing characteristics.
The classes zare not directly observable from the training set,but can be induced by maximizing the trainingset likelihood via the EM algorithm as follows.Parameters: P (z = k) = pik, P (ui|z) (5)E-Step: ?nk =pikP (?sn, tn?|z = k)?Kk?=1 pik?P (?sn, tn?|z = k?
), (6)P (?sn, tn?|z) = maxu1..uffn?i=1P (ui|z) (7)M-Step: pinewk ?N?n=1?nk, (8)P (ui|z = k)new =1NkN?n=1?nkfn(ui)fn(9)where Nk =?n ?nk.
Here, ?sn, tn?
is the n-th transliterated pair in the training set, and fnand fn(ui) indicate how many TUs there are intotal in the n-th transliterated pair, and howmany times the TU ui appeared in it, respec-tively.
As done in the JSC model, we update thealignment in the training set before the E-Stepfor each iteration.
Thus fn takes different values1Note that this LCT model is formalized by intro-ducing a latent variable to the transliteration generativeprobability P (?s, t?)
as in the JSC model, not to P (t|s).33from iteration to iteration in general.
Further-more, since the alignment is updated based onP (ui|z) for each z = k, M different alignmentcandidates are retained for each transliteratedpairs, which makes the value of fn dependenton k, i.e., fkn .
We initialize P (z = k) = 1/Mto and P (ui|z) = PAB(u) + ?, that is, the TUprobability induced by the alpha-beta algorithmplus some random noise ?.Considering a TU as a word, and a translit-eration pair as a document consisting of a wordsequence, this LCT model defines the transliter-ation probability as the mixture of multinomi-als defined over TUs.
This can be formulatedby unigram mixture (Nigam et al, 2000), whichis a topic model over documents.
This follows ageneration story where documents (i.e., translit-erated pairs) are generated firstly by choosing aclass z by P (z) and then by generating a word(i.e., TU) by P (ui|z).
Nevertheless, as men-tioned in Section 1, since this model trains theparameters based on the maximum likelihoodestimation over multinomials, it is vulnerable tonoise in the training set, thus prone to overfitthe data.5 Latent Semantic TransliterationModel based on Dirichlet MixtureWe propose the latent semantic translitera-tion model based on Dirichlet mixture (DM-LST), which is an extension to the LCT modelbased on unigram mixture.
This model enablesto prevent multinomials from being exceedinglybiased towards the given data, still being able tomodel the transliteration generation by a mix-ture of multiple latent classes, by introducingDirichlet mixture as a prior to TU multinomi-als.
The compound distribution of multinomi-als when their parameters are given by Dirichletmixtures is given by the Polya mixture distribu-tion(Yamamoto and Sadamitsu, 2005):PDM (?s, t?)
(10)=?PMul(?s, t?
;p)PDM (p;?,?K1 )dp?K?k=1?kPPolya(?s, t?
;?K1 ) (11)=K?k=1?k?(?k)?
(?k + f)f?i=1?
(f(ui) + ?kui)?
(?kui)where PMul(?
;p) is multinomial with the pa-rameter p. PDM is Dirichlet mixture, whichis a mixture (with co-efficients ?1, ..., ?K) of KDirichlet distributions with parameters ?K1 =(?1,?2, ...,?K).The model parameters can be induced by thefollowing EM algorithm.
Notice that we adopteda fast induction algorithm which extends an in-duction method using leaving-one-out to mix-ture distributions(Yamamoto et al, 2003).Parameters: ?
= (?1, ..., ?K),(12)?K1 = (?1,?2, ...,?K) (13)E-Step: ?nk =?kPPolya(?sn, tn?;?k)?k?
?k?PPolya(?sn, tn?;?k?
)(14)M-Step: ?newk ?N?n=1?nk (15)?newku = ?ku?n ?nk{fn(u)/(fn(u) ?
1 + ?ku)}?n ?nk{fn/(fn ?
1 + ?k)}(16)The prediction distribution when a sin-gle TU u is the input is given PDM (u) =?Kk=1 ?k?ku/?k.
We therefore updated thealignment in the training corpus, as done in theJSC model updates, based on the probabilityproportional to ?ku/?k for each k before ev-ery M-Step.
The parameters are initially set to?k = 1/K, ?ku = PAB(u) + ?, as explained inthe previous section.Since neither LCT nor DM-LST is a translit-eration generation model, we firstly generatedtransliteration candidates T by using the JSCmodel and the stack decoder (Section 3) as a34baseline, then re-ranked the candidates usingthe probabilities given by LCT (Eq.
4 or DM-LST (Eq.
11), generating the re-ranked listof transliterated outputs.
Because the parame-ters trained by the EM algorithm differ depend-ing on the initial values, we trained 10 modelsP 1DM , ..., P 10DM using the same training data andrandom initial values and computed the aver-age 110?10j=1 PjDM (?s, t?)
to be used as the finaltransliteration model.It is worth mentioning that another topicmodel, namely latent Dirichlet alocation (LDA)(Blei et al, 2003), assumes that words in a doc-ument can be generated from different topicsfrom each other.
This assumption correspondsto the notion that TUs in a single transliter-ated pairs can be generated from different sourcelanguages, which is presumably a wrong as-sumption for transliteration tasks, probably ex-cept for compound-like words with mixed ori-gins such as ?na?veness?.
In fact, we con-firmed through a preliminary experiment thatLDA does not improve the transliteration per-formance over the baseline.6 Experiments6.1 EvaluationIn this section, we compare the followingmodels: alpha-beta (AB), joint source channel(JSC), latent class transliteration (LCT), andlatent semantic transliteration based on Dirich-let mixture (DM-LST).For the performance evaluation, we used threelanguage pairs, namely, English-Japanese (En-Ja), English-Chinese (En-Ch), and English-Korean (En-Ko), from the transliteration sharedtask at NEWS 2009 (Li et al, 2009a; Li et al,2009b).
The size of each training/test set isshown in the first column of Table 1.
In general,rn, a set of one or more reference transliteratedwords, is associated with the n-th input sn in thetraining/test corpus.
Let cn,i, cn,2, ... be the out-put of the transliteration system, i.e., the candi-dates with highest probabilities assigned by thetransliteration model being evaluated.
We usedthe following three performance measures:?
ACC (averaged Top-1 accuracy): For ev-ery ?sn, rn?, let an be an = 1 if the can-didate with the highest probability cn,1 iscontained in the reference set rn and an =0 otherwise.
ACC is then calculated asACC 1N?Ni=1 sn.?
MFS (mean F score): Let thereference transliterated word clos-est to the top-1 candidate cn, 1 ber?n = arg minrn,j?rn ED(cn,1, rn,j), whereED is the edit distance.
The F-score of thetop candidate cn,1 for the n-th input sn isthen given by:Pn = LSC(cn,1, r?n)/|cn,1| (17)Rn = LCS(cn,1, r?n)/|r?n| (18)Fn = 2RiPi/(Ri + Pi), (19)where |x| is the length of string x, andLCS(x, y) is the length of the longest com-mon subsequence of x and y. Edit distance,lengths of strings, and LCS are measuredin Unicode characters.
Finally, MFS is de-fined as MFS = 1N?Ni=1 Fn.?
MRR (mean reciprocal rank): Of theranked candidates cn,1, cn,2, ..., let the high-est ranked one which is also included inthe reference set rn be cn,j .
We thendefine reciprocal rank RRn = 1/j.
Ifnone of the candidates are in the refer-ence, RRn = 0.
MRR is then defined byMRR = 1N?Nn=1RRn.We used Kneser-Nay smoothing to smooth theTU probabilities for LCT.
The number of EMiterations is fixed to 15 for all the models, basedon the result of preliminary experiments.The reduce width R and the beam width Bfor the stack decoder are fixed to R = 8 andB = 32, because the transliteration generationperformance increased very little beyond thesewidths based on the experiment using the de-velopment set.
We also optimized M , i.e., thenumber of latent classes for LCT and DM-LST,for each language pair and model in the sameway based on the development set.35Table 1: Performance comparison of transliterationmodelsLanguage pair Model ACC MFS MRREn-Ja AB 0.293 0.755 0.378Train: 23,225 JSC 0.326 0.770 0.428Test: 1,489 LCT 0.345 0.768 0.437DM-LST 0.349 0.776 0.444En-Ch AB 0.358 0.741 0.471Train: 31,961 JSC 0.417 0.761 0.527Test: 2,896 LCT 0.430 0.764 0.532DM-LST 0.445 0.770 0.546En-Ko AB 0.145 0.537 0.211Train: 4,785 JSC 0.151 0.543 0.221Test: 989 LCT 0.079 0.483 0.167DM-LST 0.174 0.556 0.2376.2 ResultsWe compared the performance of eachtransliteration model in Table 1.
For the lan-guage pairs En-Ja and En-Ch, all the perfor-mance increase in the order of AB < JSC <LCT < DM-LST, showing the superiority ourproposed method.
For the language pair En-Ko, the performance for LCT re-ranking con-siderably decreases compared to JSC.
We sus-pect this is due to the relatively small numberof training set, which caused the excessive fittingto the data.
We also found out that the optimalvalue of M which maximizes the performance ofDM-LST is equal to or smaller than that of LCT.This goes along with the findings (Yamamotoand Sadamitsu, 2005) that Dirichlet mixture of-ten achieves better language model perplexitywith smaller dimensionality compared to othermodels.Specific examples in the En-Ja test set whosetransliteration is improved by the proposedmethods include ?dijon ??????
dijon?
and?goldenberg ?????????
g?rudenb?gu?.Conventional methods, including LCT, sug-gested ?????
diyon?
and ????????
?g?rudenberugu?, meaning that the translitera-tion model is affected and biased towards non-English pronunciation.
The proposed methodcan retain the major class of transliteration char-acteristics (which is English in this case) and candeal with multiple language origins dependingon transliteration pairs at the same time.This trend can be also confirmed in otherlanguage pairs, En-Ch and En-Ko.
In En-Ch,the transliterated words of ?covell?
and ?nether-wood?
are improved ?
????
kefuer ????
keweier?
and ??????
neitehewude ?????
neisewude?, respectively.
in En-Ko, thetransliterated word of ?darling?
is improved ????
dareuling?
?
???
dalling?.We also observed that ?gutheim ????
?gutehaimu in En-Ch and martina ?????
mareutina in En-Ko are correctly translatedby the proposed method, even though they donot have the English origin.
Generally speak-ing, however, how these non-English words arepronounced depend on the context, as ?charles?has different pronunciation in English and inFrench, with the soft ?sh?
sound at the begin-ning.
We need external clues to disambiguatesuch transliteration, such as context informationand/or Web statistics.7 ConclusionIn this paper, we proposed the latent seman-tic transliteration model based on Dirichlet mix-ture (DM-LST) as the extension to the latentclass transliteration model.
The experimentalresults showed the superior transliteration per-formance over the conventional methods, sinceDM-LST can alleviate the overfitting problemand can capture multiple language origins.
Onedrawback is that it cannot deal with dependen-cies of higher order of TU n-grams than bigrams.How to incorporate these dependencies into thelatent transliteration models is the future work.ReferencesFarooq Ahmad and Grzegorz Kondrak.
2005.
Learn-ing a spelling error model from search query logs.In Proc.
of EMNLP-2005, pages 955?962.David M. Blei, Andrew Y. Ng, and Michael I. Jor-dan.
2003.
Latent dirichlet alocation.
Journal ofMachine Learning Research, 3:993?1022.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling.
In Proc.ACL-2000, pages 286?293.36Eric Brill, Gary Kacmarcik, and Chris Brockett.2001.
Automatically harvesting katakana-englishterm pairs from search engine query logs.
In Proc.NLPRS-2001, pages 393?399.Masato Hagiwara and Satoshi Sekine.
2011.
Latentclass transliteration based on source language ori-gin.
In Proc.
of ACL-HLT 2011, pages 53?57.Masato Hagiwara and Hisami Suzuki.
2009.Japanese query alteration based on semantic sim-ilarity.
In Proc.
of NAACL-2009, page 191.Kevin Knight and Graehl Jonathan.
1998.
Ma-chine transliteration.
Computational Linguistics,24:599?612.Haizhou Li, Zhang Min, and Su Jian.
2004.
A jointsource-channel model for machine transliteration.In Proc.
of ACL 2004, pages 159?166.Haizhou Li, Khe Chai Sum, Jin-Shea Kuo, andMinghui Dong.
2007.
Semantic transliterationof personal names.
In Proc.
of ACL 2007, pages120?127.Haizhou Li, A Kumaran, Vladimir Pervouchine, andMin Zhang.
2009a.
Report of news 2009 machinetransliteration shared task.
In Proc.
of the 2009Named Entities Workshop, pages 1?18.Haizhou Li, A Kumaran, Min Zhang, and VladimirPervouchine.
2009b.
Whitepaper of news 2009machine transliteration shared task.
In Proc.
ofthe 2009 Named Entities Workshop, pages 19?26.Ariadna Font Llitjos and Alan W. Black.
2001.Knowledge of language origin improves pronun-ciation accuracy.
In Proc.
of Eurospeech, pages1919?1922.Kamal Nigam, Andrew Kachites McCallum, Sebas-tian Thrun, and Tom Mitchell.
2000.
Text clas-sification from labeled and unlabeled documentsusing em.
Machine Learning, 39(2):103?134.K.
Sj?lander, K. Karplus, M. Brown, R. Hunghey,A.
Krogh, I.S.
Mian, and D. Haussler.
1996.Dirichlet mixtures:a method for improved detec-tion of weak but significant protein sequencehomology.
Computer Applications in the Bio-sciences, 12(4):327?345.Mikio Yamamoto and Kugatsu Sadamitsu.
2005.Dirichlet mixtures in text modeling.
CS TechnicalReport, CS-TR-05-1.Mikio Yamamoto, Kugatsu Sadamitsu, and TakuyaMishina.
2003.
Context modeling using dirichletmixtures and its applications to language models(in japnaese).
IPSJ, 2003-SLP-48:29?34.37
