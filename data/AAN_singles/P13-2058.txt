Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 323?327,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTask Alternation in Parallel Sentence Retrieval for Twitter TranslationFelix Hieber and Laura Jehl and Stefan RiezlerDepartment of Computational LinguisticsHeidelberg University69120 Heidelberg, Germany{jehl,hieber,riezler}@cl.uni-heidelberg.deAbstractWe present an approach to mine com-parable data for parallel sentences us-ing translation-based cross-lingual infor-mation retrieval (CLIR).
By iteratively al-ternating between the tasks of retrievaland translation, an initial general-domainmodel is allowed to adapt to in-domaindata.
Adaptation is done by training thetranslation system on a few thousand sen-tences retrieved in the step before.
Oursetup is time- and memory-efficient and ofsimilar quality as CLIR-based adaptationon millions of parallel sentences.1 IntroductionStatistical Machine Translation (SMT) cruciallyrelies on large amounts of bilingual data (Brown etal., 1993).
Unfortunately sentence-parallel bilin-gual data are not always available.
Various ap-proaches have been presented to remedy this prob-lem by mining parallel sentences from comparabledata, for example by using cross-lingual informa-tion retrieval (CLIR) techniques to retrieve a targetlanguage sentence for a source language sentencetreated as a query.
Most such approaches try toovercome the noise inherent in automatically ex-tracted parallel data by sheer size.
However, find-ing good quality parallel data from noisy resourceslike Twitter requires sophisticated retrieval meth-ods.
Running these methods on millions of queriesand documents can take weeks.Our method aims to achieve improvements sim-ilar to large-scale parallel sentence extraction ap-proaches, while requiring only a fraction of the ex-tracted data and considerably less computing re-sources.
Our key idea is to extend a straightfor-ward application of translation-based CLIR to aniterative method: Instead of attempting to retrievein one step as many parallel sentences as possible,we allow the retrieval model to gradually adapt tonew data by using an SMT model trained on thefreshly retrieved sentence pairs in the translation-based retrieval step.
We alternate between thetasks of translation-based retrieval of target sen-tences, and the task of SMT, by re-training theSMT model on the data that were retrieved in theprevious step.
This task alternation is done itera-tively until the number of newly added pairs stabi-lizes at a relatively small value.In our experiments on Arabic-English Twittertranslation, we achieved improvements of over 1BLEU point over a strong baseline that uses in-domain data for language modeling and parametertuning.
Compared to a CLIR-approach which ex-tracts more than 3 million parallel sentences froma noisy comparable corpus, our system producessimilar results in terms of BLEU using only about40 thousand sentences for training in each of afew iterations, thus being much more time- andresource-efficient.2 Related WorkIn the terminology of semi-supervised learning(Abney, 2008), our method resembles self-trainingand co-training by training a learning method onits own predictions.
It is different in the aspect oftask alternation: The SMT model trained on re-trieved sentence pairs is not used for generatingtraining data, but for scoring noisy parallel datain a translation-based retrieval setup.
Our methodalso incorporates aspects of transductive learningin that candidate sentences used as queries are fil-tered for out-of-vocabulary (OOV) words and sim-ilarity to sentences in the development set in or-der to maximize the impact of translation-basedretrieval.Our work most closely resembles approachesthat make use of variants of SMT to mine com-parable corpora for parallel sentences.
Recentwork uses word-based translation (Munteanu and323Marcu, 2005; Munteanu and Marcu, 2006), full-sentence translation (Abdul-Rauf and Schwenk,2009; Uszkoreit et al, 2010), or a sophisticatedinterpolation of word-based and contextual trans-lation of full sentences (Snover et al, 2008; Jehlet al, 2012; Ture and Lin, 2012) to project sourcelanguage sentences into the target language for re-trieval.
The novel aspect of task alternation in-troduced in this paper can be applied to all ap-proaches incorporating SMT for sentence retrievalfrom comparable data.For our baseline system we use in-domain lan-guage models (Bertoldi and Federico, 2009) andmeta-parameter tuning on in-domain developmentsets (Koehn and Schroeder, 2007).3 CLIR for Parallel Sentence Retrieval3.1 Context-Sensitive Translation for CLIROur CLIR model extends the translation-based re-trieval model of Xu et al (2001).
While transla-tion options in this approach are given by a lexicaltranslation table, we also select translation optionsestimated from the decoder?s n-best list for trans-lating a particular query.
The central idea is to letthe language model choose fluent, context-awaretranslations for each query term during decoding.For mapping source language query terms totarget language query terms, we follow Ture etal.
(2012a; 2012).
Given a source language queryQ with query terms qj , we project it into the tar-get language by representing each source token qjby its probabilistically weighted translations.
Thescore of target documentD, given source languagequery Q, is computed by calculating the OkapiBM25 rank (Robertson et al, 1998) over projectedterm frequency and document frequency weightsas follows:score(D|Q) =|Q|?j=1bm25(tf(qj , D), df(qj))tf(q,D) =|Tq|?i=1tf(ti, D)P (ti|q)df(q) =|Tq|?i=1df(ti)P (ti|q)where Tq = {t|P (t|q) > L} is the set of trans-lation options for query term q with probabilitygreater than L. Following Ture et al (2012a;2012) we impose a cumulative thresholdC, so thatonly the most probable options are added until Cis reached.Like Ture et al (2012a; 2012) we achieved bestretrieval performance when translation probabil-ities are calculated as an interpolation between(context-free) lexical translation probabilities Plexestimated on symmetrized word alignments, and(context-aware) translation probabilities Pnbest es-timated on the n-best list of an SMT decoder:P (t|q) = ?Pnbest(t|q) + (1?
?
)Plex(t|q) (1)Pnbest(t|q) is the decoder?s confidence to trans-late q into t within the context of query Q. Letak(t, q) be a function indicating an alignment oftarget term t to source term q in the k-th deriva-tion of query Q.
Then we can estimate Pnbest(t|q)as follows:Pnbest(t|q) =?nk=1 ak(t, q)D(k,Q)?nk=1 ak(?, q)D(k,Q)(2)D(k,Q) is the model score of the k-th derivationin the n-best list for query Q.In our work, we use hierarchical phrase-basedtranslation (Chiang, 2007), as implemented in thecdec framework (Dyer et al, 2010).
This allowsus to extract word alignments between source andtarget text for Q from the SCFG rules used in thederivation.
The concept of self-translation is cov-ered by the decoder?s ability to use pass-throughrules if words or phrases cannot be translated.3.2 Task Alternation in CLIRThe key idea of our approach is to iteratively al-ternate between the tasks of retrieval and trans-lation for efficient mining of parallel sentences.We allow the initial general-domain CLIR modelto adapt to in-domain data over multiple itera-tions.
Since our set of in-domain queries wassmall (see 4.2), we trained an adapted SMT modelon the concatenation of general-domain sentencesand in-domain sentences retrieved in the step be-fore, rather than working with separate models.Algorithm 1 shows the iterative task alternationprocedure.
In terms of semi-supervised learning,we can view algorithm 1 as non-persistent as wedo not keep labels/pairs from previous iterations.We have tried different variations of label persis-tency but did not find any improvements.
A sim-ilar effect of preventing the SMT model to ?for-get?
general-domain knowledge across iterationsis achieved by mixing models from current andprevious iterations.
This is accomplished in twoways: First, by linearly interpolating the transla-tion option weights P (t|q) from the current and324Algorithm 1 Task AlternationRequire: source language TweetsQsrc, target language TweetsDtrg , general-domain parallel sentences Sgen, general-domainSMT model Mgen, interpolation parameter ?procedure TASK-ALTERNATION(Qsrc, Dtrg, Sgen,Mgen, ?)t?
1while true doSin ?
?
.
Start with empty parallel in-domain sentencesif t == 1 thenM (t)clir ?Mgen .
Start with general-domain SMT model for CLIRelseM (t)clir ?
?M(t?1)smt + (1?
?
)M (t)smt .
Use mixture of previous and current SMT model for CLIRend ifSin ?
CLIR(Qsrc, Dtrg,M (t)clir) .
Retrieve top 1 target language Tweets for each source language queryM (t+1)smt ?
TRAIN(Sgen + Sin) .
Train SMT model on general-domain and retrieved in-domain datat?
t+ 1end whileend procedureBLEU (test) # of in-domain sentsStandard DA 14.05 -Full-scale CLIR 14.97 3,198,913Task alternation 15.31 ?40kTable 1: Standard Domain Adaptation with in-domain LMand tuning; Full-scale CLIR yielding over 3M in-domain par-allel sentences; Task alternation (?
= 0.1, iteration 7) using?40k parallel sentences per iteration.previous model with interpolation parameter ?.Second, by always using Plex(t|q) weights esti-mated from word alignments on Sgen.We experimented with different ways of usingthe ranked retrieval results for each query andfound that taking just the highest ranked docu-ment yielded the best results.
This returns one pairof parallel Twitter messages per query, which arethen used as additional training data for the SMTmodel in each iteration.4 Experiments4.1 DataWe trained the general domain model Mgen ondata from the NIST evaluation campaign, includ-ing UN reports, newswire, broadcast news andblogs.
Since we were interested in relative im-provements rather than absolute performance, wesampled 1 million parallel sentences Sgen from theoriginally over 5.8 million parallel sentences.We used a large corpus of Twitter messages,originally created by Jehl et al (2012), as com-parable in-domain data.
Language identificationwas carried out with an off-the-shelf tool (Lui andBaldwin, 2012).
We kept only Tweets classifiedas Arabic or English with over 95% confidence.After removing duplicates, we obtained 5.5 mil-lion Arabic Tweets and 3.7 million English Tweets(Dtrg).
Jehl et al (2012) also supply a set of 1,022Arabic Tweets with 3 English translations each forevaluation purposes, which was created by crowd-sourcing translation on Amazon Mechanical Turk.We randomly split the parallel sentences into 511sentences for development and 511 sentences fortesting.
All URLs and user names in Tweets werereplaced by common placeholders.
Hashtags werekept, since they might be helpful in the retrievalstep.
Since the evaluation data do not contain anyhashtags, URLs or user names, we apply a post-processing step after decoding in which we re-move those tokens.4.2 Transductive SetupOur method can be considered transductive in twoways.
First, all Twitter data were collected bykeyword-based crawling.
Therefore, we can ex-pect a topical similarity between development, testand training data.
Second, since our setup aimsfor speed, we created a small set of queries Qsrc,consisting of the source side of the evaluation dataand similar Tweets.
Similarity was defined bytwo criteria: First, we ranked all Arabic Tweetswith respect to their term overlap with the devel-opment and test Tweets.
Smoothed per-sentenceBLEU (Lin and Och, 2004) was used as a similar-ity metric.
OOV-coverage served as a second cri-terion to remedy the problem of unknown wordsin Twitter translation.
We first created a generallist of all OOVs in the evaluation data under Mgen(3,069 out of 7,641 types).
For each of the top 100BLEU-ranked Tweets, we counted OOV-coveragewith respect to the corresponding source Tweetand the general OOV list.
We only kept Tweets3250 1 2 3 4 5 6 7 8iteration14.0514.9715.3116.00BLEU (test)(a)?=0.0?=0.1?=0.5?=0.91 2 3 4 5 6 7 8iteration 010000200003000040000500006000070000# new pairs(b)?=0.0?=0.1?=0.5?=0.9Figure 1: Learning curves for varying ?
parameters.
(a) BLEU scores and (b) number of new pairs added per iteration.containing at least one OOV term from the corre-sponding source Tweet and two OOV terms fromthe general list, resulting in 65,643 Arabic queriescovering 86% of all OOVs.
Our query set Qsrcperformed better (14.76 BLEU) after one iterationthan a similar-sized set of random queries (13.39).4.3 Experimental ResultsWe simulated the full-scale retrieval approach byJehl et al (2012) with the CLIR model describedin section 3.
It took 14 days to run 5.5M Arabicqueries on 3.7M English documents.
In contrast,our iterative approach completed a single iterationin less than 24 hours.1In the absence of a Twitter data set for re-trieval, we selected the parameters ?
= 0.6 (eq.1),L = 0.005 and C = 0.95 in a mate-findingtask on Wikipedia data.
The n-best list size forPnbest(t|q) was 1000.
All SMT models includeda 5-gram language model built from the Englishside of the NIST data plus the English side of theTwitter corpus Dtrg.
Word alignments were cre-ated using GIZA++ (Och and Ney, 2003).
Ruleextraction and parameter tuning (MERT) was car-ried out with cdec, using standard features.
Weran MERT 5 times per iteration, carrying over theweights which achieved median performance onthe development set to the next iteration.Table 1 reports median BLEU scores on test ofour standard adaptation baseline, the full-scale re-trieval approach and the best result from our taskalternation systems.
Approximate randomizationtests (Noreen, 1989; Riezler and Maxwell, 2005)showed that improvements of full-scale retrievaland task alternation over the baseline were statis-1Retrieval was done in 4 batches on a Hadoop cluster us-ing 190 mappers at once.tically significant.
Differences between full-scaleretrieval and task alternation were not significant.2Figure 1 illustrates the impact of ?, which con-trols the importance of the previous model com-pared to the current one, on median BLEU (a) andchange of Sin (b) over iterations.
For all ?, fewiterations suffice to reach or surpass full-scale re-trieval performance.
Yet, no run achieved goodperformance after one iteration, showing that thetransductive setup must be combined with task al-ternation to be effective.
While we see fluctuationsin BLEU for all ?-values, ?
= 0.1 achieves highscores faster and more consistently, pointing to-wards selecting a bolder updating strategy.
Thisis also supported by plot (b), which indicates thatchoosing ?
= 0.1 leads to faster stabilization inthe pairs added per iteration (Sin).
We used thisstabilization as a stopping criterion.5 ConclusionWe presented a method that makes translation-based CLIR feasible for mining parallel sentencesfrom large amounts of comparable data.
The keyof our approach is a translation-based high-qualityretrieval model which gradually adapts to the tar-get domain by iteratively re-training the underly-ing SMT model on a few thousand parallel sen-tences retrieved in the step before.
The numberof new pairs added per iteration stabilizes to afew thousand after 7 iterations, yielding an SMTmodel that improves 0.35 BLEU points over amodel trained on millions of retrieved pairs.2Note that our full-scale results are not directly compara-ble to those of Jehl et al (2012) since our setup uses less thanone fifth of the NIST data, a different decoder, a new CLIRapproach, and a different development and test split.326ReferencesSadaf Abdul-Rauf and Holger Schwenk.
2009.
On theuse of comparable corpora to improve SMT perfor-mance.
In Proceedings of the 12th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics (EACL?09), Athens, Greece.Steven Abney.
2008.
Semisupervised Learning forComputational Linguistics.
Chapman and Hall.Nicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translationwith monolingual resources.
In Proceedings of the4th EACL Workshop on Statistical Machine Transla-tion (WMT?09), Athens, Greece.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19(2).David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2).Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of the ACL 2010 System Demonstra-tions (ACL?10), Uppsala, Sweden.Laura Jehl, Felix Hieber, and Stefan Riezler.
2012.Twitter translation using translation-based cross-lingual retrieval.
In Proceedings of the Sev-enth Workshop on Statistical Machine Translation(WMT?12), Montreal, Quebec, Canada.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin domain adaptation for statistical machine trans-lation.
In Proceedings of the Second Workshop onStatistical Machine Translation, Prague, Czech Re-public.Chin-Yew Lin and Franz Josef Och.
2004.
Orange: amethod for evaluating automatic evaluation metricsfor machine translation.
In Proceedings the 20th In-ternational Conference on Computational Linguis-tics (COLING?04).Marco Lui and Timothy Baldwin.
2012. langid.py: Anoff-the-shelf language identification tool.
In Pro-ceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics, Demo Session(ACL?12), Jeju, Republic of Korea.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguis-tics, 31(4).Dragos Stefan Munteanu and Daniel Marcu.
2006.
Ex-tracting parallel sub-sentential fragments from non-parallel corpora.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th annual meeting of the Association for Com-putational Linguistics (COLING-ACL?06), Sydney,Australia.Eric W. Noreen.
1989.
Computer Intensive Meth-ods for Testing Hypotheses.
An Introduction.
Wiley,New York.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1).Stefan Riezler and John Maxwell.
2005.
On some pit-falls in automatic evaluation and significance testingfor MT.
In Proceedings of the ACL-05 Workshop onIntrinsic and Extrinsic Evaluation Measures for MTand/or Summarization, Ann Arbor, MI.Stephen E. Robertson, Steve Walker, and MichelineHancock-Beaulieu.
1998.
Okapi at TREC-7.
InProceedings of the Seventh Text REtrieval Confer-ence (TREC-7), Gaithersburg, MD.Matthew Snover, Bonnie Dorr, and Richard Schwartz.2008.
Language and translation model adaptationusing comparable corpora.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP?08), Honolulu, Hawaii.Ferhan Ture and Jimmy Lin.
2012.
Why not grab afree lunch?
mining large corpora for parallel sen-tences to improve translation modeling.
In Proceed-ings of the Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL-HLT?12),Montreal, Canada.Ferhan Ture, Jimmy Lin, and Douglas W. Oard.2012.
Combining statistical translation techniquesfor cross-language information retrieval.
In Pro-ceedings of the International Conference on Compu-tational Linguistics (COLING?12), Mumbai, India.Ferhan Ture, Jimmy Lin, and Douglas W. Oard.
2012a.Looking inside the box: Context-sensitive transla-tion for cross-language information retrieval.
InProceedings of the ACM SIGIR Conference on Re-search and Development in Information Retrieval(SIGIR?12), Portland, OR.Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, andMoshe Dubiner.
2010.
Large scale parallel doc-ument mining for machine translation.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics (COLING?10), Beijing,China.Jinxi Xu, Ralph Weischedel, and Chanh Nguyen.
2001.Evaluating a probabilistic model for cross-lingualinformation retrieval.
In Proceedings of the 24th An-nual International ACM SIGIR Conference on Re-search and Development in Information Retrieval(SIGIR?01), New York, NY.327
