Proceedings of the 12th European Workshop on Natural Language Generation, pages 50?57,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsClass-Based Ordering of Prenominal ModifiersMargaret MitchellCenter for Spoken Language UnderstandingPortland, Oregon, U.S.Aitallow@cslu.ogi.eduAbstractThis paper introduces a class-based ap-proach to ordering prenominal modifiers.Modifiers are grouped into broad classesbased on where they tend to occur prenom-inally, and a framework is developed to or-der sets of modifiers based on their classes.This system is developed to generate sev-eral orderings for modifiers with moreflexible positional constraints, and lendsitself to bootstrapping for the classificationof previously unseen modifiers.1 IntroductionOrdering prenominal modifiers is a necessary taskin the generation of natural language.
For a systemto effectively generate fluent utterances, the sys-tem must determine the proper order for any givenset of modifiers.
The order of modifiers before anoun affects the meaning and fluency of generatedutterances.
Determining ways to order modifiersprenominally has been an area of considerable re-search (cf.
Shaw and Hatzivassiloglou, 1999; Mal-ouf, 2000).In this paper, we establish and evaluate a classi-fication system that can be used to order prenom-inal modifiers automatically.
This may be im-plemented in a surface realization component ofa natural language generation system, or may beused to help specify the ordering of properties thatfeed into a referring expression generation algo-rithm.
Predictions of prenominal modifier order-ing based on these classes are shown to be robustand accurate.The work here diverges from the approachescommonly employed in modifier classification byassuming no underlying relationship between se-mantics and prenominal order or morphology andprenominal order.
The approach instead relieson generalizing empirical evidence from a corpus.This allows the system to be robust and portable toa variety of applications, without precluding anyunderlying linguistic constraints.In the next section, we discuss prior work onthis topic, and address the differences in our ap-proach.
Section 3 discusses the relationship be-tween modifier ordering and referring expressiongeneration, a principal component of natural lan-guage generation.
Section 4 describes the ideasbehind the modifier classification system.
Sec-tions 5 and 6 present the materials and method-ology of the current study, with a discussion of thecorpus involved and the basic modules used in theprocess.
In Section 7 we discuss the results of ourstudy.
Finally, in Section 8, we outline areas forimprovement and possible future work.2 Related WorkDiscerning the rules governing the ordering of ad-jectives has been an area of research for quite sometime (see, for example, Panini?s work on San-skrit grammar ca.
350 BCE).
Most approaches as-sume an underlying relationship between seman-tics and prenominal position (cf.
Whorf, 1945;Ziff, 1960; Bever, 1970; Danks and Glucksberg,1971).
These approaches can be characterized aspredicting modifier order based on degrees of se-mantic closeness to the noun.
This follows whatis known as Behaghel?s First Law (Behaghel,1930):Word groups: What belongs togethermentally is placed close together syntac-tically.
(Clark and Clark, 1977: 545)However, there is disagreement on the exactqualities that affect position.
These theories arealso difficult to implement in a generation system,as they require determining the semantic proper-ties of each modifier used, relative to the contextin which it occurs.
If a modifier classification50scheme is to be implemented, it should be able tocreate a variety of natural, unmarked orders; be ro-bust enough to handle a wide variety of modifiers;and be flexible enough to allow different naturalorderings.Shaw and Hatzivassiloglou (1999) examine thisproblem, and develop ways to order all prenominalmodifier types.
This includes adjectives as wellas nouns, such as ?baseball?
in ?baseball field?
;gerunds, such as ?running?
in ?running man?
; andpast participles, such as ?heated?
in ?heated de-bate?.
The authors devise three different meth-ods that may be implemented in a generation sys-tem to order these kinds of prenominal modifiers.These are the direct evidence method, the transi-tivity method, and the clustering method.Briefly, given prenominal modifiers a and b ina training corpus, the direct evidence method uti-lizes probabilistic reasoning to determine whetherthe frequency count of the ordered sequence<a,b> or <b,a> is stronger.
The transitiv-ity method makes inferences about unseen order-ings among prenominal modifiers; given a thirdprenominal modifier c, where a precedes b and bprecedes c, the authors can conclude that a pre-cedes c. In the clustering method, an order sim-ilarity metric is used to group modifiers togetherthat share a similar relative order to other modi-fiers.Shaw and Hatzivassiloglou achieve their high-est prediction accuracy of 90.67% using their tran-sitivity technique on prenominal modifiers froma medical corpus.
However, with their systemtrained on the medical corpus and then testedon the Wall Street Journal corpus (Marcus et al,1993), they achieve an overall prediction accuracyof only 54%.
The authors conclude that prenomi-nal modifier ordering is domain-specific.Malouf (2000) continues this work, determin-ing the order for sequences of prenominal adjec-tives by examining several different statistical andmachine learning techniques.
These achieve goodresults, ranging from 78.28% to 89.73% accuracy.Malouf achieves the best results by combiningmemory-based learning and positional probabil-ity, which reaches 91.85% accuracy at predictingthe prenominal adjective orderings in the first 100million tokens of the BNC.
However, this analysisdoes not extend to other kinds of prenominal mod-ifiers.
The model is also not tested on a differentdomain.The approach to modifier classification takenhere is similar to the clustering method discussedby Shaw and Hatzivassiloglou.
Modifiers aregrouped into classes based on where they occurprenominally.
This approach differs, however, inhow classes are assigned.
In our approach, modi-fiers are grouped into classes based on the frequen-cies with which they occur in different prenominalpositions.
Classes are built based not on wheremodifiers are positioned in respect to other mod-ifiers, but on where modifiers are positioned ingeneral.
Grouping modifiers into classes based onprenominal positions mitigates the problems notedby Shaw and Hatzivassiloglou that ordering pre-dictions cannot be made (1) when both a and b be-long to the same class, (2) when either a or b arenot associated to a class that can be ordered withrespect to the other, and (3) when the evidence forone class preceding the other is equally strong forboth classes.This approach allows modifiers with strongpositional preferences to be in a class separatefrom modifiers with weaker positional prefer-ences.
This also ensures that any prenominal mod-ifiers a and b seen in the training corpus can beordered, regardless of which particular modifiersthey appear with and whether they occur togetherin the training data at all.
This approach also hasthe added benefit of developing modifier classesthat are usable across many different domains.Further, this method is conceptually simple andeasy to implement.
Although this approach is lesscontext-sensitive than earlier work, we find that itis highly accurate, with comparable token preci-sion.
We discuss this in greater detail in Sections6 and 7.3 The Problem of Ordering PrenominalModifiersGenerating referring expressions in part requiresgenerating the adjectives, verbs, and nouns thatmodify head nouns.
In order for these expressionsto clearly convey the intended referent, the mod-ifiers must appear in an order that sounds naturaland mimics human language use.For example, consider the alternation given inFigure 1.
Some combinations of modifiers be-fore a noun are more marked than others, althoughall are strictly speaking grammatical.
This speaksto the need for a broad modifier classes to orderprenominal modifiers, where individual modifiers51(1) big beautiful white wooden house(2) ?white wooden beautiful big house(3) comfortable red chair(4) ?red comfortable chair(5) big rectangular green Chinese silk carpet(6) ?Chinese big silk green rectangular carpetFigure 1: Grammatical Modifier Alternations(Vendler, 1968: 122)may be ordered separately as required by particu-lar contexts.Along these lines, almost all referring expres-sion generation algorithms rely on the availabilityof a predefined ordering or weighting of properties(Dale and Reiter, 1995; van Deemter, 2002; Krah-mer et al, 2003).
This requires that for every refer-ent, an ordered or weighted listing of all the prop-erties that can apply to it must be created beforereferring expression generation begins.
In thesemodels, the order or weights of the input proper-ties map to the order of the output modifiers.However, the method used to determine the or-dering or weighting of properties is an open is-sue.
The difficulty with capturing the ordering ofproperties and their corresponding modifiers stemsfrom the problem of data sparsity.
In the examplein Figure 1, the modifier silkmay be rare enough inany corpus that finding it in combination with an-other modifier, in order to create a generalizationabout its ordering constraints, is nearly impossi-ble.
Malouf (2000) examined the first million sen-tences of the British National Corpus and foundonly one sequence of adjectives for every twentysentences.
With sequences of adjectives occurringso rarely, the chances of finding information onany particular sequence is small.
The data is justtoo sparse.4 Towards a SolutionTo create a flexible system capable of predicting awide variety of orderings, we used several corporato build broad modifier classes.
Modifiers are clas-sified by where they tend to appear prenominally,and ordering constraints between the classes de-termine the order for any set of modifiers.
Thissystem incorporates three main ideas:1.
Not all modifiers have equally stringent or-dering preferences.2.
Modifier ordering preferences can be learnedempirically.3.
Modifiers can be grouped into classes indica-tive of their ordering preferences.The classification scheme therefore allows rigidas well as more loose orders (compare big redball and ?red big ball with white floppy hat andfloppy white hat).
It is not based on any mappingbetween position and semantics, morphology, orphonology, but does not exclude any such rela-tionship in the classification: This classificationscheme builds on what there is direct evidence for,independent of why each modifier appears whereit does.To create our model, all simplex noun phrases(NPs) are extracted from parsed corpora.
A sim-plex NP is defined as a maximal noun phrasethat includes premodifiers such as determiners andpossessives but no post-nominal constituents suchas prepositional phrases or relative clauses (Shawand Hatzivassiloglou, 1999: 137).
From thesesimplex NPs, we extract all those headed by anoun and preceded by only prenominal modifiers.This includes modifiers tagged as adjectives (JJ),nouns (NN), gerunds (VBG), and past participles(VBN).
The counts and relative positions of eachmodifier are stored, and these are converted intoposition probabilities in vector file format.
Modi-fiers are classified based on the positions in whichthey have the highest probabilities of occurring.Examples of the intermediary files in this pro-cess are given in Tables 1 and 2.
Table 1 listsmodifiers followed by their frequency counts ineach prenominal position.
Table 2 lists these mod-ifiers associated to their classes, with the propor-tions that determine the class.wealthy four 2 three 2 two 3 one 1red four 13 three 35 two 35 one 21golden four 1 three 5 two 5 one 3strongest four 5 three 5 two 5 one 5Table 1: Example Modifier Classification Interme-diate File: Step 35 MaterialsTo create the training and test data, we utilize thePenn Treebank-3 (Marcus et al, 1999) releases of52wealthy two two 0.38red two three three 0.34 two 0.34golden one two three three 0.33 two 0.33 one 0.29strongest two three four four 0.33 three 0.33 two 0.33Table 2: Example Modifier Classification Interme-diate File: Step 4the parsed Wall Street Journal corpus, the parsedBrown corpus, and the parsed Switchboard cor-pus.
The Wall Street Journal corpus is a selec-tion of over one million words collected from theWall Street Journal over a three-year period.
TheBrown corpus is over one million words of prosewritten in various genres, including mystery, hu-mor, and popular lore, collected from newspapersand periodicals in 1961.
The Switchboard corpusis over one million words of spontaneous speechcollected from thousands of five-minute telephoneconversations.
Several programs were constructedto analyze the information provided by these data.The details of each module are outlined below.5.1 Code ModulesThe following five components were developed (inPython) for this project.Modifier Extractor ?
This program takes as in-put a parsed corpus, and outputs a list of alloccurrences of all noun phrases in that cor-pus.input: Parsed Corpusoutput: List of simplex NPsModifier Organizer ?
This program takes as in-put a list of simplex NPs and filters out wordsthat appear prenominally and are occasion-ally mistagged as modifiers.
A list of thesefiltered words is available in Table 3.
Thisreturns a vector with frequency counts forall positions in which each observed modifieroccurs.input: Modifier-rich noun phrases and theirfrequenciesoutput: Vector file with distributional infor-mation for each modifier positionModifier Classifier ?
This program takes as in-put a vector file with distributional informa-tion for each modifier?s position, and fromthis builds our model by determining the clas-sification for each modifier.about behind onabove in underafter inside outoutside up overdown like pastnear through offthe aTable 3: Filtered Mistagged Wordsinput: Vector file with distributional infor-mation for each modifier positionoutput: Ordering model: File with eachmodifier associated to a classPrenominal Modifier Ordering Predictor ?This program takes as input two files: an or-dering model and a list of simplex NPs (fortesting).
The program then uses the modelto assign a class to each modifier seen in thetesting data, and predicts the ordering for allthe modifiers that appear prenominally.
Adiscussion of the ordering decisions is givenbelow.
This program then compares its pre-dicted ordering of modifiers prenominally tothe observed ordering of modifiers prenom-inally.
It returns precision and recall valuesfor its predictions.input: Vector file with each modifier associ-ated to a class, list of simplex NPsoutput: Precision and recall for modifier or-dering predictions6 Method6.1 Classification SchemeTo develop modifier classes and create our model,we assume four primary modifier positions.
Thisassumption is based on the idea that people rarelyproduce more than four modifiers before a noun.This assumption covers 99.70% of our data (seeTable 5).
The longest noun phrases for this ex-periment are therefore those with five words: Fourmodifiers followed by a noun.small smiling white fuzzy bunnyfour three two oneFigure 2: Example Simplex NP with PrenominalPositionsEach modifier?s class is determined by countingthe frequency of each modifier in each position.53Class 1: one Class 6: two-threeClass 2: two Class 7: three-fourClass 3: three Class 8: one-two-threeClass 4: four Class 9: two-three-fourClass 5: one-twoTable 4: Modifier ClassesThis is turned into a probability over all four posi-tions.
All position probabilities ?
0.25 (baseline)are discarded.
Those positions that remain deter-mine the modifier class.To calculate modifier position for each phrase,counts were incremented for all feasible positions.This is a way of sharing evidence among sev-eral positions.
For example, in the phrase cleanwooden spoon, the adjective clean was counted asoccurring in positions two, three, and four, whilethe adjective wooden was counted as occurring inpositions one, two, and three.The classification that emerges after applyingthis technique to a large body of data gives riseto the broad positional preferences of each modi-fier.
In this way, a modifier with a strict positionalpreference can emerge as occurring in just one po-sition; a modifier with a less strict preference canemerge as occurring in three.The final class for each modifier is dependenton the positions the modifier appears in more than25% of the time.
Since there are four possiblepositions, 25% is the baseline: A single modifierpreceding a noun has equal probability of being ineach of the four positions.
There are nine derivablemodifier classes in this approach, listed in Table 4.A diagram of how a modifier is associated to aclass is shown in Figure 3.
In this example, redappears in several simplex NPs.
In each sequence,we associate red to its possible positions withinthe four prenominal slots.
We see that red occursin positions one, two and three; two, three, andfour; and three and four.
With only this data, redhas a 12.5% probability of being in position one; a25% probability of being in position two; a 37.5%probability of being in position three; and a 25%probability of being in position four.
It can there-fore be classified as belonging to Class 3, the classfor modifiers that tend to occur in position three.This kind of classification allows the system tobe flexible to the idea that some modifiers exhibitstringent ordering constraints, while others havemore loose constraints.
Some modifiers may al-ways appear immediately before the noun, whileFigure 3: Constructing the Class of the Modifierredothers may generally appear close to or far fromthe noun.
By counting the occurrences of eachmodifier in each position, classes for all observedmodifiers may be derived.The frequencies of all extracted groupings ofprenominal modifiers used to build our model areshown in Table 5.
The frequencies of the extractedclasses are shown in Table 6.Mods Count Percentage2 15856 88.90%3 1770 9.92%4 155 0.87%5 21 0.12%6 1 .03%Table 5: Frequency of Prenominal ModifierAmountsClass Position Count Percentage1 one 18 0.23%2 two 46 0.68%3 three 62 0.92%4 four 21 0.31%5 one-two 329 4.88%6 two-three 1136 16.86%7 three-four 261 3.87%8 one-two-three 2671 39.65%9 two-three-four 2193 32.55%Table 6: Modifier Class DistributionModifiers of Class 8, the class for modifiers thatshow a general preference to be closer to the headnoun but do not have a strict positional preference,make up the largest portion of the data.
An exam-ple of a modifier from Class 8 is golden.
The next54Class Position Generated Before Class1 one 2 3 4 5 6 7 8 92 two 3 4 6 7 93 three 4 74 four5 one-two 2 3 4 6 7 8 96 two-three 3 4 7 97 three-four 48 one-two-three 4 6 7 99 two-three-four 4 7Table 7: Proposed Modifier Orderinglargest portion of the data are modifiers of Class 9,the class for modifiers that show a general prefer-ence to be farther from the head noun.
An exam-ple of a modifier from Class 9 is strongest.
Withthese defined, strongest golden arch is predictedto sound grammatical and unmarked, but ?goldenstrongest arch is not.Some expected patterns also emerge in thesegroupings.
For example, green, yellow, red andother colors are determined to be Class 6.
Ex-plained and unexplained are both determined to beClass 5, and big and small are both determined tobe Class 9.Once classified, modifiers may be ordered ac-cording to their classes.
The proposed orderingconstraints for these classes are listed in Table 7.Note that using this classification scheme, the or-dering of modifiers that belong to the same classis not predicted.
This seems to be reflective of nat-ural language use.
For example, both wealthy andperforming are predicted to be Class 2.
This seemsreasonable; whether wealthy performing man orperforming wealthy man is a more natural order-ing of prenominal modifiers is at least debatable.The freedom of intra-class positioning allows forsome randomization in the generation of prenom-inal modifiers, where other factors may be used todetermine the final ordering.6.2 EvaluationIn order to test how well the proposed systemworks, 10-fold cross-validation was used on theextracted corpora.
The held-out data was selectedas random lines from the corpus, with a list stor-ing the index of each selected line to ensure noline was selected more than once.
In each trial,modifier classification was learned from 90% ofthe data and the resulting model was used to pre-dict the prenominal ordering of modifiers in theremaining 10%.The modifiers preceding each noun were storedin unordered groups, and the ordering for each un-ordered prenominal modifier pair {a,b} was pre-dicted based on the classes of the modifiers inour model.
The ordering predictions followed theconstraints listed in Table 7.
When the class wasknown for one modifier but not for the other, thetwo modifiers were ordered based on the class ofthe known modifier: Modifiers in Classes 1, 2, 5,and 8 were placed closer to the head noun than theunknown modifier, while modifiers in Classes 3,4, 7, and 9 were placed farther from the head nounthan the unknown modifier.
If the known modifierwas of Class 6 (occurring in position two-three), arandom guess decided the ordering.
This reflectsthe idea that Classes 1, 2, 5, and 8 are all classesfor modifiers that broadly prefer to be closer tothe head noun, while Classes 3, 4, 7, and 9 areall classes for modifiers that broadly prefer to befarther from the head noun.In the context of classification tasks, precisionand recall measurements provide useful informa-tion of system accuracy.
Precision, as defined in(7), is the number of true positives divided by thenumber of true positives plus false positives.
Thisis calculated here as tp/(tp + fp), where tp is thenumber of orderings that were correctly predicted,and fp is the number of orderings not correctly pre-dicted.
This measure provides information abouthow accurate the modifier classification is.
Recall,as defined in (8), is the number of true positivesdivided by the number of true positives plus falsenegatives.
This is calculated here as tp/(tp + fn),where tp is the number of orderings that were cor-rectly predicted, and fn is the total number of or-derings that could not be predicted by our system.This measure provides information about the pro-portion of modifiers in the training data that can becorrectly ordered.
(7) Precision = tp/(tp + fp)tp = number of orderings correctly predictedfp = number of orderings not correctlypredicted(8) Recall = tp/(tp + fn)tp = number of orderings correctly predictedfn = number of orderings that could not bepredicted55Precision RecallToken 89.63% (0.02) 74.14% (0.03)Type 90.26% (0.02) 67.17% (0.03)Table 8: Precision and Recall for PrenominalModifier Ordering7 ResultsResults are shown in Table 8.
Our model pre-dicts the correct order for 89.63% of unorderedmodifiers {a,b} for which an ordering decisioncan be made, making correct predictions for74.14% of all unordered modifiers in the test data.The system also correctly predicts 90.26% of theunordered modifier {a,b} types in the test data forwhich an ordering decision can be made.
Thiscovers 67.17% of the modifier pair types in thetest data.
This lower value appears to be due tothe large amount of modifier pairs that are in thedata only once.The values given are averages over each trial.The standard deviation for each average is givenin parentheses.
On average, 191 modifier pairswere ordered in each trial, based on the assignedorders of 273 individual modifiers, with an aver-age of 23.01% of the modifiers outside of the vo-cabulary in each trial.The system precision and recall here are compa-rable to previously reported results (see Section 2).Extending our analysis over entire simplex NPs,where we generate all possible orderings based onour system constraints, we are able to predict anaverage of 94.44% of the sequences for which adetermination can be made.
This is a correct pre-diction for 78.59% of all the simplex NPs in thedata.Previous attempts have achieved very poor re-sults when testing their models on a new domain.We conclude our analysis by testing the accuracyof our models on different domains.
To do this, wecombine two corpora to build our model and thentest this model on the third.Combining the WSJ corpus and the Brown cor-pus to build our modifier classes and then testingon the Switchboard (Swbd) corpus, we achievequite promising results.
Our token precision is89.57% and our type precision is 94.17%.
How-ever, our recall values are much lower than thosereported above (63.47% and 58.18%).
Other train-ing and testing combinations follow this pattern:A model built from the Switchboard corpus andTraining Testing Token TokenCorpus Corpus Precision RecallBrown+WSJ Swbd 89.57% 63.47%Swbd+WSJ Brown 82.75% 57.14%Swbd+Brown WSJ 79.82% 39.55%Training Testing Type TypeCorpus Corpus Precision RecallBrown+WSJ Swbd 94.17% 58.18%Swbd+WSJ Brown 87.00% 51.18%Swbd+Brown WSJ 82.43% 27.16%Table 9: Precision and Recall for PrenominalModifier Ordering of a New Domainthe WSJ corpus achieves 82.75% token precisionand 87% type precision when tested on the Browncorpus (57.14% token recall, 51.18% type recall),while a model built from the Switchboard corpusand the Brown corpus achieves 79.82% token pre-cision and 82.43% type precision when tested onthe WSJ corpus (39.55% token recall and 27.16%type recall).8 DiscussionThe system precision is comparable to previouslyreported results.
The results show that order-ing modifiers based on this classification systemcan aid in generating simplex noun phrases withprenominal modifiers ordered in a way that soundsnatural.
We now turn to a discussion of areas forfuture work.It seems reasonable that the classes for previ-ously unseen modifiers could be developed basedon the known classes of surrounding modifiers.This system lends itself to bootstrapping, wherea lexical acquisition task that constructed classprobabilities based on the surrounding contextcould classify previously unseen modifiers:grey shining metallic chainthree-four unknown one-two head-nounGiven its position and the classes of the surround-ing modifiers, unknown could be two-three.Grouping modifiers into classes that determinetheir order also lends itself to incorporation intogenerative grammars.
For example, Head-drivenPhrase Structure Grammar (Sag et al, 2003),a constraint-based grammatical framework thatgroups lexical items into broader classes, couldutilize the classes proposed here to determinemodifier positions prenominally.
Advancing re-56search in this area could help grow the generativecapabilities of class-based grammars.It bears mentioning that this same system wasattempted on the Google Web 1T 5-Gram corpus(Brants and Franz, 2006), where we used WordNet(Miller et al, 2006) to extract sequences of nounspreceded by modifiers.
The precision and recallwere similar to the values reported here, however,the proportions of prenominal modifiers belied aproblem in using such a corpus for this approach:82.56% of our data had two prenominal modifiers,16.79% had four, but only 0.65% had three.
Thispattern was due to the many extracted sequencesof modifiers preceding a noun that were not actu-ally simplex NPs.
That is, the 5-Grams includemany sequences of words in which the final onehas a use as a noun and the earlier ones have usesas adjectives, but the 5-Gram itself may not be anoun phrase.
We found that many of our extracted5-Grams were actually lists of words (for example,Chinese Polish Portuguese Romanian Russian wasobserved 115 times).
In the future, we would liketo examine ways to use the 5-Gram corpus to sup-plement our system.The results reported here are encouraging, andwe hope to continue this work on a parsed versionof the Gutenberg corpus (Hart, 2009).
This cor-pus is a collection of text versions of novels andother written works, and is available online.
Usinga corpus of modifier-rich text such as this wouldaid the system in classifying a greater number ofmodifiers.
Further work should also test how ro-bust the acquisition of unseen modifiers is usingthese classes, and examine implementing this or-dering system into a language generation system.ReferencesOtto Behaghel.
1930.
Von Deutscher Wortstellung,volume 44.
Zeitschrift Fu?r Deutschen, Unterricht.Thomas G. Bever.
1970.
The cognitive basis for lin-guistic structures.
In J. R. Hayes, editor, Cogni-tion and the Development of Language.
Wiley, NewYork.Gemma Boleda and Laura Alonso.
2003.
Cluster-ing adjectives for class acquisition.
In Proceedingsof the EACL?03 Student Session, pages 9?16, Bu-dapest.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram version 1. http://www.ldc.upenn.edu.
Lin-guistic Data Consortium.H.
H. Clark and E. V. Clark.
1976.
Psychologyand language: An introduction to psycholinguistics.Harcourt Brace Jovanovich, New York.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the gricean maxims in the gener-ation of referring expressions.
Cognitive Science,18:233?263.M.A.K.
Halliday and Christian Matthiessen.
1999.Construing experience as meaning: a language-based approach to cognition.
Cassell, London.Michael Hart.
2009.
Project Gutenberg collection.http://www.gutenberg.org.
Project Gutenberg.Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg.2003.
Graph-based generation of referring expres-sions.
Computational Linguistics, 29(1):53?72.Robert Malouf.
2000.
The order of prenominal ad-jectives in natural language generation.
In Proceed-ings of the 38th Annual Meeting of the Associationfor Computational Linguistics, pages 85?92, HongKong.Christopher D. Manning.
1993.
Automatic acquisi-tion of a large subcategorization dictionary from cor-pora.
In Meeting of the Association for Computa-tional Linguistics, pages 235?242.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19:313?330.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3.
Linguistic Data Consortium.George A. Miller, Christiane Fellbaum, Randee Tengi,PamelaWakefield, Helen Langone, and Benjamin R.Haskell.
2006.
WordNet: A lexical database for theenglish language.Ivan Sag, Tom Wasow, and Emily Bender.
2003.
Syn-tactic Theory: A Formal Introduction.
CSLI Publi-cations, Stanford University.James Shaw and Vasileios Hatzivassiloglou.
1999.
Or-dering among premodifiers.
In Proceedings of the37th Annual Meeting of the Association for Compu-tational Linguistics on Computational Linguistics,pages 135?143, Morristown, NJ, USA.
Associationfor Computational Linguistics.Kees van Deemter.
2002.
Generating referring expres-sions: Boolean extensions of the incremental algo-rithm.
Computational Linguistics, 28(1):37?52.Zeno Vendler.
1968.
Adjectives and Nominalizations.Mouton.Benjamin Lee Whorf.
1945.
Grammatical categories.Language, 21(1):1?11.Paul Ziff.
1960.
Semantic Analysis.
Cornell Univer-sity Press, Ithaca, New York.57
