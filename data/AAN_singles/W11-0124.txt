Abductive Reasoning with a Large Knowledge Basefor Discourse ProcessingEkaterina OvchinnikovaUniversity of Osnabru?ckeovchinn@uos.deNiloofar MontazeriUSC ISIniloofar@isi.eduTheodore AlexandrovUniversity of Brementheodore@uni-bremen.deJerry R. HobbsUSC ISIhobbs@isi.eduMichael C. McCordIBM Researchmcmccord@us.ibm.comRutu Mulkar-MehtaUSC ISIme@rutumulkar.comAbstractThis paper presents a discourse processing framework based on weighted abduction.
We elabo-rate on ideas described in Hobbs et al (1993) and implement the abductive inference procedure in asystem called Mini-TACITUS.
Particular attention is paid to constructing a large and reliable knowl-edge base for supporting inferences.
For this purpose we exploit such lexical-semantic resources asWordNet and FrameNet.
We test the proposed procedure and the obtained knowledge base on theRecognizing Textual Entailment task using the data sets from the RTE-2 challenge for evaluation.
Inaddition, we provide an evaluation of the semantic role labeling produced by the system taking theFrame-Annotated Corpus for Textual Entailment as a gold standard.1 IntroductionIn this paper, we elaborate on a semantic processing framework based on a mode of inference calledabduction, or inference to the best explanation.
In logics, abduction is a kind of inference which arrivesat an explanatory hypothesis given an observation.
Hobbs et al (1993) describe how abductive reasoningcan be applied to the discourse processing problem viewing the process of interpreting sentences indiscourse as the process of providing the best explanation of why the sentence would be true.
In thisframework, interpreting a sentence means 1) proving its logical form, 2) merging redundancies wherepossible, and 3) making assumptions where necessary.
As the reader will see later in this paper, abductivereasoning as a discourse processing technique helps to solve many pragmatic problems such as referenceresolution, the interpretation of noun compounds, the resolution of some kinds of syntactic, and semanticambiguity as a by-product.
We adopt this approach.
Specifically, we use a system we have built calledMini-TACITUS1 (Mulkar et al, 2007) that provides the expressivity of logical inference but also allowsprobabilistic, fuzzy, or defeasible inference and includes measures of the ?goodness?
of abductive proofsand hence of interpretations of texts and other situations.The success of a discourse processing system based on inferences heavily depends on a knowledgebase.
The main contribution of this paper is in showing how a large and reliable knowledge base can beobtained by exploiting existing lexical semantic resources and can be successfully applied to reasoningtasks on a large scale.
In particular, we experiment with axioms extracted from WordNet, see Fellbaum(1998), and FrameNet, see Ruppenhofer et al (2006).
In axiomatizing FrameNet we rely on the studydescribed in Ovchinnikova et al (2010).We evaluate our inference system and the obtained knowledge base in recognizing textual entailment(RTE).
As the reader will see in the following sections, inferences carried out by Mini-TACITUS arefairly general and not tuned for a particular application.
We decided to test our approach on RTE becausethis is a well-defined task that captures major semantic inference needs across many natural language1http://www.rutumulkar.com/download/TACITUS/tacitus.php225processing applications, such as question answering, information retrieval, information extraction, anddocument summarization.
For evaluation, we have chosen the RTE-2 data set (Bar-Haim et al, 2006),because besides providing text-hypothesis pairs and a gold standard this data set has been annotated withFrameNet frame and role labels (Burchardt and Pennacchiotti, 2008) which gives us the possibility ofevaluating our frame and role labeling based on the axioms extracted from FrameNet.2 NL Pipeline and Abductive ReasoningOur natural language pipeline produces interpretations of texts given the appropriate knowledge base.
Atext is first input to the English Slot Grammar (ESG) parser (McCord, 1990, 2010).
For each segment,the parse produced by ESG is a dependency tree that shows both surface and deep structure.
The deepstructure is exhibited via a word sense predication for each node, with logical arguments.
These logicalpredications form a good start on a logical form (LF) for the whole segment.
An add-on to ESG convertsthe parse tree into a LF in the style of Hobbs (1985).
The LF is a conjunction of predications, which havegeneralized entity arguments that can be used for showing relationships among the predications.
TheseLFs are used by the downstream components.The interpretation of the text is carried out by an inference system called Mini-TACITUS usingweighted abduction as described in detail in Hobbs et al (1993).
Mini-TACITUS tries to prove the logicalform of the text, allowing assumptions where necessary.
Where the system is able to prove parts of theLF, it is anchoring it in what is already known from the overall discourse or from a knowledge base.Where assumptions are necessary, it is gaining new information.
Obviously, there are many possibleproofs in this procedure.
A cost function on proofs enables the system to chose the ?best?
(the cheapest)interpretation.
The key factors involved in assigning a cost are the following: 1) proofs with fewerassumptions are favored, 2) short proofs are favored over long ones, 3) plausible axioms are favored overless plausible axioms, and 4) proofs are favored that exploit the inherent implicit redundancy in text.Let us illustrate the procedure with a simple example.
Suppose that we want to construct the bestinterpretation of the sentence John composed a sonata.
As a by-product, the procedure will disambiguatebetween two readings of compose, namely between the ?form?
reading instantiated for example in thesentence Three representatives composed a committee, and the ?create art?
meaning instantiated in thegiven sentence.
After being processed by the parser, the sentence will be assigned the following logicalform where the numbers (20) after every proposition correspond to the default costs of these proposi-tions.2 The total cost of this logical form is equal to 60.John(x1):20 & compose(e1,x1,x2):20 & sonata(x2):20Suppose our knowledge base contains the following axioms:1) form(e0,x1,x2):90 ?
compose(e0,x1,x2)2) create art(e0,x1,x2):50 & art piece(x2):40 ?
compose(e0,x1,x2)3) art piece(x1):90 ?
sonata(x1)Unlike deductive axioms, abductive axioms should be read ?right to left?.
Thus, the propositions onthe right hand side (compose, sonata) correspond to an input, whereas the left hand side propositionswill be assumed given the input.
The number assigned to each proposition on the left hand side showswhat percentage of the total input cost the assumption of this proposition will cost.3 For example, if theproposition compose costs 20 then the assumption of form will cost 18.Two interpretations can be constructed for the given logical form.
The first one is the result of theapplication of axioms 1 and 3.
Note that the costs of the backchained propositions (compose, sonata) are2The actual value of the default costs of the input propositions does not matter, because, as the reader will see in this section,the axiom weights which affect the costs of the resulting interpretations are given as percentages of the input proposition costs.The only heuristic we use here concerns setting all costs of the input propositions to be equal (all propositions cost 20 in thediscussed example).
This heuristic needs a further investigation to be approved or modified.3The axiom weights in the given example are arbitrary.226set to 0, because their costs are now carried by the newly introduces assumptions (form, art piece).
Thetotal cost of the first interpretation I1 is equal to 56.I1: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 & form(e1,x1,x2):18 & art piece(x2):18The second interpretation is constructed in two steps.
First, axioms 2 and 3 are applied as follows.I21: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 &create art(e1,x1,x2):10 & art piece(x2):8 & art piece(x2):18The total cost of I21 is equal to 56.
This interpretation is redundant, because it contains the propo-sition art piece twice.
The procedure will merge propositions with the same predicate, setting the cor-responding arguments of these propositions to be equal and assigning the minimum of the costs to theresult of merging.
The idea behind such mergings is that if an assumption has already been made thenthere is no need to make it again.
The final form of the second interpretation I22 with the cost of 38is as follows.
The ?create art?
meaning of compose has been brought forward because of the implicitredundancy in the sentence which facilitated the disambiguation.I22: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 & create art(e1,x1,x2):10 &art piece(x2):8Thus, on each reasoning step the procedure 1) applies axioms to propositions with non-zero costsand 2) merges propositions with the same predicate, assigning the lowest cost to the result of merging.Reasoning terminates when no more axioms can be applied.4 The procedure favors the cheapest inter-pretations.
Among them, the shortest proofs are favored, i.e.
if two interpretations have the same costthen the one which has been constructed with fewer axiom application steps is considered to be ?better?.It is easy to see that changing weights of axioms can crucially influence the reasoning process.
Axiomweights can help to propagate more frequent and reliable inferences and to distinguish between ?real?abduction and deduction.
For example, an axiom backchaining from dog to animal should in the generalcase have a weight below 100, because it is cheap to assume that there is an animal if there is a dog; it isa reliable deduction.
On the contrary, assuming dog given animal should have a weight above 100.In order to avoid undesirable mergings, we introduce non-merge constraints.
For example, in thesentence John reads a book and Bill reads a book the two read propositions should not be mergedbecause they refer to different actions.
This is ensured by the following non-merge constraint: if not allarguments of two propositions (which are not nouns) with the same predicate can be merged, then thesepropositions cannot be merged.
The constraint implies that in the sentence above two read propositionscannot be merged, because John being the first argument of the first read cannot be merged with Bill.5This constraint is a heuristic; it corresponds to the intuition that it is unlikely that the same noun refers todifferent objects in a short discourse, while for other parts of speech it is possible.
An additional corpusstudy is needed in order to prove or disprove it.The described procedure provides solutions to a whole range of natural language pragmatics prob-lems, such as resolving ambiguity, discovering implicit relations in nouns compounds, prepositionalphrases, or discourse structure.
Moreover, this account of interpretation solves the problem of where tostop drawing inferences, which could easily be unlimited in number; an inference is appropriate if it ispart of the lowest-cost proof of the logical form.Adapting Mini-TACITUS to a Large-Scale Knowledge BaseMini-TACITUS (Mulkar et al, 2007) began as a simple backchaining theorem-prover intended to be amore transparent version of the original TACITUS system, which was based on Stickel?s PTTP system(Stickel, 1988).
Originally, Mini-TACITUS was not designed for treating large amounts of data.
A clearand clean reasoning procedure rather than efficiency was in the focus of its developers.
In order to makethe system work with the large-scale knowledge base, we had to perform several optimization steps andadd a couple of new features.4In practice, we use the depth parameter d and do not allow an inference chain with more that d steps.5Recall that only propositions with the same predicate can be merged, therefore John and Bill cannot be merged.227For avoiding the reasoning complexity problem, we have introduced two parameters.
The time pa-rameter t is used to restrict the processing time.
After the processing time exceeds t the reasoningterminates and the best interpretation so far is output.
The time parameter ensures that an interpretationwill be always returned by the procedure even if reasoning could not be completed in a reasonable time.The depth parameter d restricts the depth of the inference chain.
Suppose that a proposition p occurringin the input has been backchained and a proposition p?
has been introduced as a result.
Then, p?
will bebackchained and so on.
The number of such iterations cannot exceed d. The depth parameter reducesthe number of reasoning steps.Since Mini-TACITUS processing time increases exponentially with the input size (sentence lengthand number of axioms), making such a large set of axioms work was an additional issue.
For speedingup reasoning it was necessary to reduce both the number of the input propositions and the number ofaxioms.
In order to reduce the number of axioms, a two-step reduction of the axiom set is performed.First, only the axioms which could be evoked by the input propositions or as a result of backchainingfrom the input are selected for each reasoning task.
Second, the axioms which could never lead to anymerging are filtered out.
Concerning the input propositions, those which could never be merged with theothers (even after backchaining) are excluded from the reasoning process.3 Knowledge BaseAs described in the previous section, the Mini-TACITUS inferences are based on a knowledge base (KB)consisting of a set of axioms.
In order to obtain a reliable KBwith a sufficient coverage we have exploitedexisting lexical-semantic resources.First, we have extracted axioms from WordNet (Fellbaum, 1998), version 3.0, which has alreadyproved itself to be useful in knowledge-intensive NLP applications.
The central entity in WordNet iscalled a synset.
Synsets correspond to word senses, so that every lexeme can participate in severalsynsets.
For every word sense, WordNet indicates the frequency of this particular word sense in theWordNet annotated corpora.
We have used the lexeme-synset mapping for generating axioms, with thecorresponding frequencies of word senses converted into the axiom weights.
For example, in the axiomsbelow, the verb compose is mapped to its sense 2 in WordNet which participates in synset-X.compose-2(e1,x1,x2):80 ?
compose(e1,x1,x2)synset-X(e0,e1):100 ?
compose-2(e1,x1,x2)Moreover, we have converted the following WordNet relations defined on synsets into axioms: hy-pernymy, instantiation, entailment, similarity, meronymy.
Hypernymy and instantiation relations pre-suppose that the related synsets refer to the same entity (the first axiom below), whereas other types ofrelations relate synsets referring to different entities (the second axiom below).
All axioms based onWordNet relations have the weights equal to 100.synset-1(e0,e1):100 ?
synset-2(e0,e1)synset-1(e0,e1):100 ?
synset-2(e2,e3)WordNet alo provides morphosemantic relations which relate verbs and nouns, e.g., buy-buyer.WordNet distinguishes between 14 types of such relations.We use relation types in order to define thedirection of the entailment and map the arguments.
For example, the ?agent?
relation (buy-buyer) standsfor a bi-directional entailment such that the noun is the first (agentive) argument of the verb:buy-1(e0,x1,x2):100 ?
buyer-1(x1)buyer-1(x1):100 ?
buy-1(e0,x1,x2)Additionally, we have exploited the WordNet synset definitions.
In WordNet the definitions are givenin natural language form.
We have used the extended WordNet resource6 which provides logical formsfor the definition in WordNet version 2.0.
We have adapted logical forms from extended WordNet to our6http://xwn.hlt.utdallas.edu/228representation format and converted them into axioms; for example the following axiom represents themeaning of the synset containing such lexemes as horseback.
These axioms have the total weight of 100.on(e2,e1,x2):25 & back(e3,x2):25 & of (e4,x2,x1):25 & horse(e5,x1):25 ?
synset-X(e0,x0)The second resource which we have used as a source of axioms is FrameNet, release 1.5, see Rup-penhofer et al (2006).
FrameNet has a shorter history in NLP applications thanWordNet, but lately moreand more researchers have been demonstrating its potential to improve the quality of question answering(Shen and Lapata, 2007) and recognizing textual entailment (Burchardt et al, 2009).
The lexical mean-ing of predicates in FrameNet is represented in terms of frames which describe prototypical situationsspoken about in natural language.
Every frame contains a set of roles corresponding to the participants ofthe described situation.
Predicates with similar semantics are assigned to the same frame; e.g.
both giveand hand over refer to the GIVING frame.
For most of the lexical elements FrameNet provides syntacticpatterns showing the surface realization of these lexical elements and their arguments.
Syntactic patternsalso contain information about their frequency in the FrameNet annotated corpora.
We have used thepatterns and the frequencies for deriving axioms such as for example the following.GIVING(e1,x1,x2,x3):70 & DONOR(e1,x1):0 & RECIPIENT(e1,x2):0 & THEME(e1,x3):0 ?give(e1,x1,x3) & to(e2,e1,x2)HIRING(e1,x1,x3):90 & EMPLOYER(e1,x1) & EMPLOYEE(e1,x3) ?give(e1,x1,x2,x3):10 & job(x2)The first pattern above corresponds to the phrases like John gave a book to Mary and the second ?less frequent ?
to phrases like John gave Mary a job.
It is interesting to note that application of suchaxioms provides a solution to the problem of semantic role labeling as a by-product.
As in the statis-tical approaches, more frequent patterns will be favored.
Moreover, patterns helping to detect implicitredundancy will be brought forward.FrameNet alo introduces semantic relations defined on frames such as inheritance, causation orprecedence; for example the GIVING and GETTING frames are connected with the causation relation.Roles of the connected frames are also linked, e.g.
DONOR in GIVING is linked with SOURCE in GETTING.Frame relations have no formal semantics in FrameNet.
In order to generate corresponding axioms, wehave used the previous work on axiomatizing frame relations and extracting new relations from corpora(Ovchinnikova et al, 2010).
Weights of the axioms derived from frame relations depend on corpus-basedsimilarity of the lexical items assigned to the corresponding frames.
An example of an axiomatizedrelation is given below.7GIVING(e0,x1,x2,x3):120 & DONOR(e0,x1):0 & RECIPIENT(e0,x2):0 & THEME(e0,x3):0 &causes(e0,e1):0 ?
GETTING(e1,x2,x3,x1) & SOURCE(e1,x1) & RECIPIENT(e1,x2) & THEME(e1,x3)Both WordNet and FrameNet are manually created resources which ensures a relatively high qualityof the resulting axioms as well as the possibility of exploiting the linguistic information provided forstructuring the axioms.
Although manual creation of resources is a very time-consuming task, WordNetand FrameNet, being long-term projects, have an extensive coverage of English vocabulary.
The cover-age of WordNet is currently larger than that of FrameNet (155 000 vs. 12 000 lexemes).
However, thefact that FrameNet introduces complex argument structures (roles) for frames and provides mappings ofthese structures makes FrameNet especially valuable for reasoning.The complete list of axioms we have extracted from these resources is given in table 1.4 Recognizing Textual EntailmentAs the reader can see from the previous sections, the discourse processing procedure we have presentedis fairly general and not tuned for any particular type of inferences.
We have evaluated the procedure and7The ?causes?
predicate is supposed to be linked to an underlying causation theory, see for examplehttp://www.isi.edu/?hobbs/bgt-cause.text.
However, in the described experimental settings we have left the abstract theoriesout and evaluated only the axioms extracted from the lexical-semantic resources.229Table 1: Statistics for extracted axiomsAxiom type Source Numb.
of axiomsLexeme-synset mappings WN 3.0 422,000Lexeme-synset mappings WN 2.0 406,000Synset relations WN 3.0 141,000Derivational relations WN 3.0 (annotated) 35,000Synset definitions WN 2.0 (parsed, annotated) 120,500Lexeme-frame mappings FN 1.5 50,000Frame relations FN 1.5 + corpora 6,000the KB derived from WordNet and FrameNet on the Recognizing Textual Entailment (RTE) task, whichis a generic task that seems to capture major semantic inference needs across many natural languageprocessing applications.
In this task, the system is given a text and a hypothesis and must decide whetherthe hypothesis is entailed by the text plus commonsense knowledge.Our approach is to interpret both the text and the hypothesis using Mini-TACITUS, and then seewhether adding information derived from the text to the knowledge base will reduce the cost of the bestabductive proof of the hypothesis as compared to using the original knowledge base only.
If the costreduction exceeds a threshold determined from a training set, then we predict entailment.A simple example would be the text John gave a book to Mary and the hypothesis Mary got a book.Our pipeline constructs the following logical forms for these two sentences.T: John(x1):20 & give(e1,x1,x2):20 & book(x3):20 & to(e2,e1,x3):20 & Mary(x3):20H: Mary(x1):20 & get(e1,x1,x2):20 & book(x2):20These logical forms constitute the Mini-TACITUS input.
Mini-TACITUS applies the axioms fromthe knowledge base to the input logical forms in order to reduce the overall cost of the interpretations.Suppose that we have three FrameNet axioms in our knowledge base.
The first one maps give to to theGIVING frame, the second one maps get to GETTING and the third one relates GIVING and GETTING withthe causation relation.
The first two axioms have the weights of 90 and the third 120.
As a result of theapplication of the axioms the following best interpretations will be constructed for T and H.I(T): John(x1):20 & give(e1,x1,x2):0 & book(x3):20 & to(e2,e1,x3):0 & Mary(x3):20 &GIVING(e0,x1,x2,x3):18I(H): Mary(x1):20 & get(e1,x1,x2):0 & book(x2):20 & GETTING(e0,x1,x2):18The total cost of the best interpretation for H is equal to 58.
Now the best interpretation of T willbe added to H with the zero costs (as if T has been totally proven) and we will try to prove H onceagain.
First of all, merging of the propositions with the same names will result in reducing costs of thepropositions Mary and book to 0, because they occur in T:I(T+H): John(x1):0 & give(e1,x1,x2):0 & book(x3):0 & to(e2,e1,x3):0 & Mary(x3):0 &GIVING(e0,x1,x2,x3):0 & get(e1,x1,x2):0 & GETTING(e0,x1,x2):18The only proposition left to be proved is GETTING.
Using the GETTING-GIVING relation as describedin the previous section, this proposition can be backchained on to GIVING which will merge with GIVINGcoming from the T sentence.
H appears to be proven completely with respect to T; the total cost of itsbest interpretation given T is equal to 0.
Thus, using knowledge from T helped to reduce the cost of thebest interpretation of H from 58 to 0.The approach presented does not have any special account for logical connectors such as if, not, oretc.
Given a text If A then B and a hypothesis A and B our procedure will most likely predict entailment.At the moment our RTE procedure mainly accounts for the informational content of texts, being able todetect the ?aboutness?
overlap of T and H. In our framework, a fuller treatment of the logical structure230of the natural language would presuppose a more complicated strategy of merging redundancies.5 Evaluation ResultsWe have evaluated our procedure on the RTE-2 dataset 8, see Bar-Haim et al (2006) .
The RTE-2dataset contains the development and the test set, both including 800 text-hypothesis pairs.
Each datasetconsists of four subsets, which correspond to typical success and failure settings in different applications:information extraction (IE), information retrieval (IR), question answering (QA), and summarization(SUM).
In total, 200 pairs were collected for each application in each dataset.As a baseline we have processed the datasets with an empty knowledge base.
Then we have done 2runs, first, using axioms extracted fromWordNet 3.0 plus FrameNet, and, second, using axioms extractedfrom the WordNet 2.0 definitions.
In both runs the depth parameter was set to 3.
The developmentset was used to train the threshold as described in the previous section.9 Table 2 contains results ofour experiments.10 Accuracy was calculated as the percentage of pairs correctly judged.
The resultssuggest that the proposed method seems to be promising as compared to the other systems evaluatedon the same task.
Our best run gives 63% accuracy.
Two systems participating the RTE-2 Challengehad 73% and 75% accuracy, two systems achieved 62% and 63%, while most of the systems achieved55%-61%, cf.
Bar-Haim et al (2006).
For our best run (WN 3.0 + FN), we present the accuracy datafor each application separately (table 2).
The distribution of the performance of Mini-TACITUS on thefour datasets corresponds to the average performance of systems participating in RTE-2 as reported byGaroufi (2007).
The most challenging task in RTE-2 appeared to be IE.
QA and IR follow, and finally,SUM was titled the ?easiest?
task, with a performance significantly higher than that of any other task.11It is worth noting that the performance of Mini-TACITUS increases with the increasing time of pro-cessing.
This is not surprising.
We use the time parameter t for restricting the processing time.
Thesmaller t is, the fewer chances Mini-TACITUS has for applying all relevant axioms.
The experimentscarried out suggest that optimizing the system computationally could lead to producing significantly bet-ter results.
Tracing the reasoning process, we found out that given a long sentence and a short processingtime Mini-TACITUS had time to construct only a few interpretations, and the real best interpretation wasnot always among them.The lower performance of the system using the KB based on axioms extracted from extended Word-Net can be easily explained.
At the moment we define non-merge constraints (see section 2) for the inputpropositions only.
The axioms extracted from the synset definitions introduce a lot of new lexemes intothe logical form, since these axioms define words with the help of other words rather than abstract con-cepts.
These new lexemes, especially those which are frequent in English, result in undesired mergings(e.g., mergings of frequent prepositions), since no non-merge constraints are defined for them.
In orderto fix this problem, we will need to implement dynamic non-merge constraints which will be added onthe fly if a new lexeme is introduced during reasoning.
The WN 3.0 + FN axiom set does not fall intothis problem, because these axioms operate on frames and synsets rather than on lexemes.In addition, for the run using axioms derived from FrameNet, we have evaluated how well we doin assigning frames and frame roles.
For Mini-TACITUS, semantic role labeling is a by-product ofconstructing the best interpretation.
But since this task is considered to be important as such in the NLPcommunity, we provide an additional evaluation for it.
As a gold standard we have used the Frame-Annotated Corpus for Textual Entailment, FATE, see Burchardt and Pennacchiotti (2008).
This corpusprovides frame and semantic role label annotations for the RTE-2 challenge test set.12 It is important to8http://pascallin.ecs.soton.ac.uk/Challenges/RTE2/9Interpretation costs were normalized to the number of propositions in the input.10?Time?
stands for the value of the time parameter ?
processing time per sentence, in minutes; ?Numb.
of ax.?
stands forthe average number of axioms per sentence.11In order to get a better understanding of which parts of our KB are useful for computing entailment and for which types ofentailment, in future, we are planning to use the detailed annotation of the RTE-2 dataset describing the source of the entailmentwhich was produced by Garoufi (2007).
We would like to thank one of our reviewers for giving us this idea.12FATE was annotated with the FrameNet 1.3 labels, while we have been using 1.5 version for extracting axioms.
However,231Table 2: Evaluation results for the RTE-2 test setKB Accuracy TimeNumb.
of ax.T HNo KB 57% 1 0 0WN 3.0 + FN 62% 20 533 237WN 3.0 + FN 63% 30 533 237Ext.
WN 2.0 60% 20 3700 1720Ext.
WN 2.0 61% 30 3700 1720Task AccuracySUM 75%IR 64%QA 62%IE 50%Table 3: Evaluation of frames/roles labeling towards FATESystemFrame matchRecallRole matchPrecision RecallShalmaneser 0.55 0.54 0.37Shalmaneser + Detour 0.85 0.52 0.36Mini-TACITUS 0.65 0.55 0.30note that FATE annotates only those frames which are relevant for computing entailment.
Since Mini-TACITUS makes all possible frame assignments for a sentence, we provide only the recall measure forthe frame match and leave the precision out.The FATE corpus was also used as a gold standard for evaluating the Shalmaneser system (Erk andPado, 2006) which is a state-of-the-art system for assigning FrameNet frames and roles.
In table 2 wereplicate results for Shalmaneser alone and Shalmaneser boosted with the WordNet Detour to FrameNet(Burchardt et al, 2005).
The WN-FN Detour extended the frame labels assigned by Shalmaneser withthe labels related via the FrameNet hierarchy or by the WordNet inheritance relation, cf.
Burchardt et al(2009).
In frame matching, the number of frame labels in the gold standard annotation that can also befound in the system annotation (recall) was counted.
Role matching was evaluated only on the framesthat are correctly annotated by the system.
The number of role labels in the gold standard annotationthat can also be found in the system annotation (recall) as well as the number of role labels found bythe system which also occur in the gold standard (precision) were counted.13 Table 3 shows that givenFrameNet axioms, the performance of Mini-TACITUS on semantic role labeling is compatible with thoseof the system specially designed to solve this task.6 Conclusion and Future WorkThis paper presents a discourse processing framework underlying the abductive reasoner called Mini-TACITUS.
We have shown that interpreting texts using weighted abduction helps solve pragmatic prob-lems in discourse processing as a by-product.
In this paper, particular attention was paid to the construc-tion of a large and reliable knowledge base populated with axioms extracted from such lexical-semanticresources as WordNet and FrameNet.
The reasoning procedure as well as the knowledge base were eval-uated in the Recognizing Textual Entailment task.
The data for evaluation were taken from the RTE-2Challenge.
First, we have evaluated the accuracy of the entailment prediction.
Second, we have eval-in the new FN version the number of frames and roles increases and there is no message about removed frames in the GeneralRelease Notes R1.5, see http://framenet.icsi.berkeley.edu.
Therefore we suppose that most of the frames and roles used for theFATE annotation are still present in FN 1.5.13We do not compare filler matching, because the FATE syntactic annotation follows different standards as the one producedby the ESG parser, which makes aligning fillers non-trivial.232uated frame and role labeling using the Frame-Annotated Corpora for Textual Entailment as the goldstandard.
In both tasks our system showed performance compatible with those of the state-of-the artsystems.
Since the inference procedure and the axiom set are general and not tuned for a particular task,we consider the results of our experiments to be promising concerning possible manifold applications ofMini-TACITUS.The experiments we have carried out have shown that there is still a lot of space for improving theprocedure.
First, for successful application of Mini-TACITUS on a large scale the system needs to becomputationally optimized.
In its current state, Mini-TACITUS requires too much time for producingsatisfactory results.
As our experiments suggest (cf.
table 2), speeding up reasoning may lead to signif-icant improvements in the system performance.
Since Mini-TACITUS was not originally designed forlarge-scale processing, its implementation is in many aspects not effective enough.
We hope to improveit by changing the data structure and re-implementing some of the main algorithms.Second, in the future we plan to elaborate our treatment of natural language expressions standing forlogical connectors such as implication if, negation not, disjunction or and others.
Quantifiers such asall, each, some also require a special treatment.
This advance is needed in order to achieve more preciseentailment inferences, which are at the moment based in our approach on the core information content(?aboutness?)
of texts.
Concerning the heuristic non-merge constraints preventing undesired mergingsas well as the heuristic for assigning default costs (see section 2), in the future we would like to performa corpus study for evaluating and possibly changing these heuristics.Another future direction concerns the enlargement of the knowledge base.
Hand-crafted lexical-semantic resources such as WordNet and FrameNet provide both an extensive lexical coverage and ahigh-value semantic labeling.
However, such resources still lack certain features essential for captur-ing some of the knowledge required for linguistic inferences.
First of all, manually created resourcesare static; updating them with new information is a slow and time-consuming process.
By contrast,commonsense knowledge and the lexicon undergo daily updates.
In order to accommodate dynamicknowledge, we plan to make use of the distributional similarities of words in a large Web-corpus suchas for example Wikipedia.
Many researchers working on RTE have already been using word similarityfor computing similarity between texts and hypotheses, e.g., Mehdad et al (2010).
In our approach, weplan to incorporate word similarities into the reasoning procedure making them affect proposition costsso that propositions implied by the context (similar to other words in the context) will become cheaperto prove.
This extension might give us a performance improvement in RTE, because it will help to relatethose propositions from H for which there are no appropriate axioms in the KB to propositions in T.Lexical-semantic resources as knowledge sources for reasoning have another shortcoming: Theyimply too little structure.
WordNet and FrameNet enable some argument mappings of related synsets orframes, but they cannot provide a more detailed concept axiomatization.
We are engaged in two types ofefforts to obtain more structured knowledge.
The first effort is the manual encoding of abstract theoriesexplicating concepts that pervade natural language discourse, such as causality, change of state, andscales, and the manual encoding of axioms linking lexical items to these theories.
A selection of the coretheories can be found at http://www.isi.edu/ hobbs/csk.html.
The second effort concerns making use ofthe existing ontologies.
The recent progress of the Semantic Web technologies has stimulated extensivedevelopment of the domain-specific ontologies as well as development of inference machines speciallydesigned to reason with these ontologies.14 In practice, domain-specific ontologies usually representdetailed and structured knowledge about particular domains (e.g.
geography, medicine etc.).
We intendto make Mini-TACITUS able to use this knowledge through querying an externally stored ontology withthe help of an existing reasoner.
This extension will give us a possibility to access elaborated domain-specific knowledge which might be crucial for interpretation of domain-specific texts.We believe that implementation of the mentioned improvements and extensions will make Mini-TACITUS a powerful reasoning system equipped with enough knowledge to solve manifold NLP tasks ona large scale.
In our view, the experiments with the axioms extracted from the lexical-semantic resourcespresented in this paper show the potential of weighted abduction for natural language reasoning and open14www.w3.org/2001/sw/,http://www.cs.man.ac.uk/ sattler/reasoners.html233new ways for its application.ReferencesBar-Haim, R., I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, and I. Szpektor (2006).
Thesecond PASCAL recognising textual entailment challenge.
In Proc.
of the Second PASCAL ChallengesWorkshop on Recognising Textual Entailment.Burchardt, A., K. Erk, and A. Frank (2005).
A WordNet Detour to FrameNet.
In Sprachtechnologie,mobile Kommunikation und linguistische Resourcen, Volume 8.Burchardt, A. and M. Pennacchiotti (2008).
FATE: a FrameNet-Annotated Corpus for Textual Entail-ment.
In Proc.
of LREC?08.Burchardt, A., M. Pennacchiotti, S. Thater, and M. Pinkal (2009).
Assessing the impact of frame seman-tics on textual entailment.
Natural Language Engineering 15(4), 527?550.Erk, K. and S. Pado (2006).
Shalmaneser - a flexible toolbox for semantic role assignment.
In Proc.
ofLREC?06, Genoa, Italy.Fellbaum, C.
(Ed.)
(1998).
WordNet: An Electronic Lexical Database (First ed.).
MIT Press.Garoufi, K. (2007).
Towards a better understanding of applied textual entailment: Annotation and eval-uation of the rte-2 dataset.
Master?s thesis, Saarland University.Hobbs, J. R. (1985).
Ontological promiscuity.
In Proceedings, 23rd Annual Meeting of the Associationfor Computational Linguistics, Chicago, Illinois, pp.
61?69.Hobbs, J. R., M. Stickel, and P. Martin (1993).
Interpretation as abduction.
Artificial Intelligence 63,69?142.McCord, M. C. (1990).
Slot grammar: A system for simpler construction of practical natural languagegrammars.
In Natural Language and Logic: International Scientific Symposium, Lecture Notes inComputer Science, pp.
118?145.
Springer Verlag.McCord, M. C. (2010).
Using Slot Grammar.
Technical report, IBM T. J. Watson Research Center.
RC23978Revised.Mehdad, Y., A. Moschitti, and F. M. Zanzotto (2010).
Syntactic/semantic structures for textual entailmentrecognition.
In Proc.
of HLT ?10: The 2010 Annual Conference of the North American Chapter of theAssociation for Computational Linguistics, pp.
1020?1028.Mulkar, R., J. R. Hobbs, and E. Hovy (2007).
Learning from Reading Syntactically Complex Biol-ogy Texts.
In Proc.of the 8th International Symposium on Logical Formalizations of CommonsenseReasoning.
Palo Alto.Ovchinnikova, E., L. Vieu, A. Oltramari, S. Borgo, and T. Alexandrov (2010).
Data-Driven and Onto-logical Analysis of FrameNet for Natural Language Reasoning.
In Proc.
of LREC?10, Valletta, Malta.Ruppenhofer, J., M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk (2006).
FrameNet II: ExtendedTheory and Practice.
International Computer Science Institute.Shen, D. and M. Lapata (2007).
Using Semantic Roles to Improve Question Answering.
In Proc.
ofEMNLP-CoNLL, pp.
12?21.Stickel, M. E. (1988).
A prolog technology theorem prover: Implementation by an extended prologcompiler.
Journal of Automated Reasoning 4(4), 353?380.234
