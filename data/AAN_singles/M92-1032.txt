NEW YORK UNIVERSITYDESCRIPTION OF THE PROTEUS SYSTEM AS USED FOR MUC- 4Ralph Grishman, Catherine Macleod, and John Sterling,The PROTEUS ProjectComputer Science Departmen tNew York University715 Broadway, 7th FloorNew York, NY 1000 3{ grishman,macleod,sterling)@cs .nyu.eduHISTORYThe PROTEUS system which we have used for MUC-4 is largely unchanged from that used for MUC-3 .
Ithas three main components : a syntactic analyzer, a semantic analyzer, and a template generator .The PROTEUS Syntactic Analyzer was developed starting in the fall of 1984 as a common base for all theapplications of the PROTEUS Project.
Many aspects of its design reflect its heritage in the Linguistic Strin gParser, previously developed and still in use at New York University .
The current system, including the RestrictionLanguage compiler, the lexical analyzer, and the parser proper, comprise approximately 4500 lines of CommonLisp.The Semantic Analyzer was initially developed in 1987 for the MUCK-I (RAINFORMs) application ,extended for the MUCK-II (OPREPs) application, and has been incrementally revised since .
It currently consist sof about 3000 lines of Common Lisp (excluding the domain-specific information) .The Template Generator was written from scratch for the MUC-3 task and then revised for the MUC-4 tem-plates; it is about 1200 lines of Common Lisp..STAGES OF PROCESSINGThe text goes through the five major stages of processing : lexical analysis, syntactic analysis, semanticanalysis, reference resolution, and template generation (see Figure 1) .
In addition, some restructuring of the logicalform is performed both after semantic analysis and after reference resolution (only the restructuring after referenc eresolution is shown in Figure 1) .
Processing is basically sequential: each sentence goes through lexical, syntactic ,and semantic analysis and reference resolution ; the logical form for the entire message is then fed to template gen-eration .
However, semantic (selectional) checking is performed during syntactic analysis, employing essentiall ythe same code later used for semantic analysis .Each of these stages is described in a section which follows .LEXICAL ANALYSISDictionary FormatOur dictionaries contain only syntactic information : the parts of speech for each word, information about thecomplement structure of verbs, distributional information (e .g ., for adjectives and adverbs), etc .
We follow closelythe set of syntactic features established for the NYU Linguistic String Parser .
This information is entered in LISPform using noun, verb, adjective, and adverb macros for the open-class words, and a word macrofor other parts of speech:(ADVERB "ABRUPTLY" :ATTRIBUTES (DSA) )(ADJECTIVE "ABRUPT" )(NOUN :ROOT "ABSCESS" :ATTRIBUTES (NCOUNT) )233Knowledge SourcesTextDictionaryGrammarSemantic ModelsLexical Analysi sSyntactic Analysi sSemantic AnalysisConcept Hier.Mapping RulesReference Resolutio nLF Transformatio nTemplate Generatio nTemplate sFigure 1 .
Structure of the Proteus System as used for MUC-4(VERB :ROOT "ABSCOND" :OBJLIST (NULLOBJ PN (PVAL (FROM WITH))) )The noun and verb macros automatically generate the regular inflectional forms .Dictionary FilesThe primary source of our dictionary information about open-class words (nouns, verbs, adjectives, an dadverbs) is the machine-readable version of the Oxford Advanced Learner's Dictionary ("OALD") .
We have writ-ten programs which take the SGML (Standard Generalized Markup Language) version of the dictionary, extrac tinformation on inflections, parts of speech, and verb subcategorization (including information on adverbial parti-cles and prepositions gleaned from the examples), and generate the LISP-ified form shown above.
This is supple-mented by a manually-coded dictionary (about 1500 lines, 900 entries) for closed-class words, words no t234adequately defined in the OALD, and a few very common words .For MUC-4 we used several additional dictionaries.
There was a dictionary (about 900 lines) for domain -specific English words not defined in the OALD, or too richly defined there .
In addition, we extracted from the tex tand templates lists of organizations, locations, and proper names, and prepared small dictionaries for each (abou t2500 lines total) .LookupThe text reader splits the input text into tokens and then attempts to assign to each token (or sequence o ftokens, in the case of an idiom) a definition (part of speech and syntactic attributes) .
The matching processproceeds in four steps: dictionary lookup, lexical pattern matching, spelling correction, and prefix stripping .
Dic-tionary lookup immediately retrieves definitions assigned by any of the dictionaries (including inflected forms) ,while lexical pattern matching is used to identify a variety of specialized patterns, such as numbers, dates, times ,and possessive forms .If neither dictionary lookup nor lexical pattern matching is successful, spelling correction and prefix strippin gare attempted.
For words of any length, we identify an input token as a misspelled form of a dictionary entry if on eof the two has a single instance of a letter while the other has a doubled instance of the letter (e .g ., "mispelled" and"misspelled") .
For words of 8 or more letters, we use a more general spelling corrector which allows for any singl einsertion, deletion, or substitution.
[The prefix stripper attempts to identify the token as a combination of a prefix and a word defined in the dic-tionary.
We currently use a list of 17 prefixes, including standard English ones like "un" and MUC-3/MUC-4 spe-cials like "narco-" .If all of these procedures fail, the word is tagged as a proper noun (name), since we found that most of ou rremaining undefined words were names .For MUC-4, we have incorporated the stochastic part-of-speech tagger from BBN in order to assign proba-bilities to each part-of-speech assigned by the lexical analyzer.
The log probabilities are used as scores, and com-bined with other scores to determine the overall score of each parsing hypothesis .FilteringIn order to avoid full processing of sentences which would make no contribution to the templates, we per -form a keyword-based filtering at the sentence level : if a sentence contains no key terms, it is skipped .
This filter-ing is done after lexical analysis because the lexical analysis has identified the root form of all inflected words ;these root forms provide links into the semantic hierarchy .
The filtering can therefore be specified in terms of asmall number of word classes, one of which must be present for the sentence to be worth processing .SYNTACTIC ANALYSISSyntactic analysis involves two stages of processing : parsing and syntactic regularization .
At the core of thesystem is an active chart parser .
The grammar is an augmented context-free grammar, consisting of BNF rules plusprocedural restrictions which check grammatical constraints not easily captured in the BNF rules .
Most restrictionsare stated in PROTEUS Restriction Language (a variant of the language developed for the Linguistic String Parser)and translated into LISP ; a few are coded directly in LISP [1] .
For example, the count noun restriction (that singu-lar countable nouns have a determiner) is stated asWCOUNT = IN LNR AFTER NVAR :IF BOTH CORE Xcore IS NCOUNT AND Xcore IS SINGULA RTHEN IN LN, TPOS IS NOT EMPTY .Associated with each BNF rule is a regularization rule, which computes the regularized form of each node i nthe parse tree from the regularized forms of its immediate constituents .
These regularization rules are based onlambda-reduction, as in GPSG .
The primary function of syntactic regularization is to reduce all clauses to a stan-dard form consisting of aspect and tense markers, the operator (verb or adjective), and syntactically marked cases .'
The minimum word length requirement is needed to avoid false hits where proper names are incorrectly identified as misspellings ofwords defined in the dictionary.235For example, the definition of assertion, the basic S structure in our grammar, i s<assertion>.
._ <sa> <subject> <sa> <verb> <sa> <object> <sa >:(s !
(<object> <subject> <verb> <sa*>)) .Here the portion after the single colon defines the regularized structure .Coordinate conjunction is introduced by a metarule (as in GPSG), which is applied to the context-free com-ponents of the grammar prior to parsing .
The regularization procedure expands any conjunction into a conjuntio nof clauses or of noun phrases .The output of the parser for the first sentence of TST2-0048, "SALVADORAN PRESIDENT-ELEC TALFREDO CRISTIANI CONDEMNED THE TERRORIST KILLING OF ATTORNEY GENERAL ROBERTOGARCIA ALVARADO AND ACCUSED THE FARABUNDO MARTI NATIONAL LIBERATION FRON T(FMLN) OF THE CRIME.
", i s(SENTENCE(CENTERS(CENTER(ASSERTION(ASSERTION(SUBJECT(NSTG(LNR(LN(NPOS (NPOSVAR (LCDN (ADJ "SALVADORAN")) (N "PRESIDENT" "-" "ELECT"))) )(NVAR (NAMESTG (LNAMER (N "ALFREDO") (MORENAME (N "CRISTIANI"))))))) )(VERB (LTVR (TV "CONDEMNED")) )(OBJECT(NSTGO(NSTG(LNR (LN (TPOS (LTR (T "THE"))) (NPOS (NPOSVAR (N "TERRORIST"))) )(NVAR (N "KILLING") )(RN(RN-VAL(PN (P "OF" )(NSTGO(NSTG(LNR (LN (NPOS (NPOSVAR (N "ATTORNEY" "GENERAL"))) )(NVAR(NAMESTG(LNAMER (N "ROBERTO" )(MORENAME (N "GARCIA") (MORENAME (N "ALVARADO")))))))))))))))) )(CONJ-WORD ("AND" "AND") )(ASSERTION(SUBJECT(NSTG(LNR(LN(NPOS (NPOSVAR (LCDN (ADJ "SALVADORAN")) (N "PRESIDENT" "-" "ELECT"))) )(NVAR (NAMESTG (LNAMER (N "ALFRED O" ) (MORENAME (N "CRISTIANI"))))))) )(VERB (LTVR (TV "ACCUSED")) )(OBJECT(NPN(NSTGO(NSTG(LNR (LN (TPOS (LTR (T "THE"))) )(NVAR(NAMESTG(LNAMER (N "FARABUNDO" "MARTI" "NATIONAL" "LIBERATION" "FRONT") )236(NAME-APPOS ("(" "(" )(NSTG (LNR (NVAR (NAMESTG (LNAMER (N "FMLN")))))) (' .')"
")")))))) )(PN (P "OF" )(NSTGO (NSTG (LNR (LN (TPOS (LTR (T "THE")))) (NVAR (N "CRIME"))))))))))) )(ENDMARK (" ."
" ."))
)and the corresponding regularized structure is(AND(S CONDEMN (VTENSE PAST )(SUBJECT(NP A-NAME SINGULAR (NAMES (ALFREDO CRISTIANI)) (SN NP1499 )(N-POS (NP PRESIDENT-ELECT SINGULAR (SN NP1489) (A-POS SALVADORAN)))) )(OBJECT(NP KILLING SINGULAR (SN NP1532) (T-POS THE )(N-POS (NP TERRORIST SINGULAR (SN NP1504)) )(OF(NP A-NAME SINGULAR (NAMES (ROBERTO GARCIA ALVARADO)) (SN NP1531 )(N-POS (NP 'ATTORNEY GENERAL' SINGULAR (SN NP1506))))))) )(S ACCUSE (VTENSE PAST )(SUBJECT(NP A-NAME SINGULAR (NAMES (ALFREDO CRISTIANI)) (SN NP1499 )(N-POS (NP PRESIDENT-ELECT SINGULAR (SN NP1489) (A-POS SALVADORAN)))) )(OBJECT(NP FMLN SINGULAR (RN-APPOS (NP FMLN SINGULAR (SN NP1539))) (SN NP1544 )(T-POS THE)) )(OF (NP CRIME SINGULAR (SN NP1543) (T-POS THE)))) )The system uses a chart parser operating top-down, left-to-right .
As edges are completed (i.e., as nodes ofthe parse tree are built), restrictions associated with those productions are invoked to assign and test features of th eparse tree nodes .
If a restriction fails, that edge is not added to the chart .
When certain levels of the tree are com-plete (those producing noun phrase and clause structures), the regularization rules are invoked to compute a regu-larized structure for the partial parse, and selection is invoked to verify the semantic well-formedness of the struc-ture (as noted earlier, selection uses the same "semantic analysis" code subsequently employed to translate the treeinto logical form).One unusual feature of the parser is its weighting capability .
Restrictions may assign scores to nodes ; theparser will perform a best-first search for the parse tree with the highest score .
This scoring is used to implementvarious preference mechanisms:?
closest attachment of modifiers (we penalize each modifier by the number of words separating it from it shead)?
preferred narrow conjoining for clauses (we penalize a conjoined clause structure by the number of words i tsubsumes)?
preference semantics (selection does not reject a structure, but imposes a heavy penalty if the structure doe snot match any lexico-semantic model, and a lesser penalty if the structure matches a model but with som eoperands or modifiers left over) [2,3 ]?
relaxation of certain syntactic constraints, such as the count noun constraint, adverb position constraints, andcomma constraint s?
disfavoring (penalizing) headless noun phrases and headless relatives (this is important for parsingefficiency)The grammar is based on Harris's Linguistic String Theory and adapted from the larger Linguistic Strin gProject (LSP) grammar developed by Naomi Sager at NYU [4] .
The grammar is gradually being enlarged to cove rmore of the LSP grammar.
The current grammar is 1600 lines of BNF and Restriction Language plus 300 lines o fLisp; it includes 186 non-terminals, 464 productions, and 132 restrictions .Over the course of the MUCs we have added several mechanisms for recovering from sentences the gram -mar cannot fully parse :237?
allowing the grammar to skip a single word, or a series of words enclosed in parentheses or dashes, with alarge score penalty?
if no parse is obtained for the entire sentence, taking the analysis which, starting at the first word, subsumesthe most words?
optionally, taking the remainder of the sentence and "covering" it with noun phrases and clauses, preferringthe longest noun phrases or clauses which can be identifie dSEMANTIC ANALYSIS AND REFERENCE RESOLUTIONThe output of syntactic analysis goes through semantic analysis and reference resolution and is then added t othe accumulating logical form for the message.
Following both semantic analysis and reference resolution certai ntransformations are performed to simplify the logical form .
All of this processing makes use of a concept hierarch ywhich captures the class/subclass/instance relations in the domain.Semantic analysis uses a set of lexico-semantic models to map the regularized syntactic analysis into asemantic representation.
Each model specifies a class of verbs, adjectives, or nouns and a set of operands ; for eac hoperand it indicates the possible syntactic case markers, the semantic class of the operand, whether or not th eoperand is required, and the semantic case to be assigned to the operand in the output representation .
For example,the model for "<explosive-object> damages <target>" i s(add-clause-model :id 'clause-damage- 3:parent 'clause-an y:constraint 'damage:operands (list (make-specifie r:marker 'subject:class 'explosive-objec t:case :instrument )(make-specifier:marker 'objec t:class 'target-entit y:case :patient:essential-required 'required)) )The models are arranged in a shallow hierarchy with inheritance, so that arguments and modifiers which are sharedby a class of verbs need only be stated once.
The model above inherits only from the most general clause model ,clause?any, which includes general clausal modifiers such as negation, time, tense, modality, etc .
Theevaluated MUC-4 system had 124 clause models, 21 nominalization models, and 39 other noun phrase models, atotal of about 2500 lines .
The class explosive?object in the clause model refers to the concept in the con-cept hierarchy, whose entries have the form :(defconcept explosive-object:typeof instrument-type )(defconcept explosive:typeof explosive-objec t:muctype explosive )(defconcept grenade:typeof explosive )(defconcept explosive-charge:typeof explosiv e:alias (dynamite-charge) )(defconcept bomb:typeof explosive-objec t:muctype bomb )(defconcept (VEHICLE BOMB(:typeof explosive-objec t:muctype (VEHICLE BOMB( )(defconcept car-bomb:typeof ?
VEHICLE BOMB( )(defconcept bus-bomb:typeof IVEHICLE BOMB( )(defconcept dynamite:typeof explosive-objec t:alias tnt:muctype DYNAMITE )There are currently a total of 2474 concepts in the hierarchy, of which 1734 are place names .The output of semantic analysis is a nested set of entity and event structures, with arguments labeled by key -words primarily designating semantic roles .
For the first sentence of TST2-0048, the output i s238(EVENT:IDENTIFIER E000000149 5:PREDICATE AND:CONJUNCT1 (EVEN T:IDENTIFIER E000000148 3:TOP-LEVEL-FLAG T:PREDICATE CONDEMN:AGENT (ENTITY:SN NP1499:NAMES (ALFREDO CRISTIANI ):IDENTIFIER N000000148 4:CLASS A-NAME ):EVENT (EVENT:IDENTIFIER E000000148 5:TOP-LEVEL-FLAG NI L:PREDICATE KILLING:AGENT (ENTITY:SN NP150 4:IDENTIFIER N000000148 6:CLASS TERRORIST ):PATIENT (ENTITY:JOB (ENTITY :SN NP150 6:IDENTIFIER N000000148 8:CLASS !ATTORNEY GENERAL!
):SN NP153 1:NAMES (ROBERTO GARCIA ALVARADO ):IDENTIFIER N000000148 7:CLASS A-NAME ):SN NP1532 ):TENSE PAST ):CONJUNCT2 (EVENT :IDENTIFIER E000000148 9:TOP-LEVEL-FLAG T:PREDICATE IMPLICAT E:AGENT (ENTITY:SN NP149 9:NAMES (ALFREDO CRISTIANI ):IDENTIFIER N0000001490:CLASS A-NAME ):EVENT (EVENT :IDENTIFIER E000000149 4:TOP-LEVEL-FLAG NI L:PREDICATE CRIME:SN NP1543 ):PATIENT (ENTITY:SN NP1544:IDENTIFIER N0000001493:CLASS FMLN ):TENSE PAST) )Reference ResolutionReference resolution is applied to the output of semantic analysis in order to replace anaphoric noun phrase s(representing either events or entities) by appropriate antecedents .
Each potential anaphor is compared to priorentities or events, looking for a suitable antecedent such that the class of the anaphor (in the concept hierarchy) i sequal to or more general than that of the antecedent, the anaphor and antecedent match in number, the restrictiv emodifiers in the anaphor have corresponding arguments in the antecedent, and the non-restrictive modifiers (e .g .
,apposition) of the anaphor are not inconsistent with those of the antecedent .
Special tests are provided for names(people may be referred to a subset of the ir names) and for referring to groups by typical members ("terrorist force"239.
.. "terrorists").Logical Form Transformation sThe transformations which are applied after semantic analysis and after reference resolution simplify an dregularize the logical form in various ways .
For example, if a verb governs an argument of a nominalization, th eargument is inserted into the event created from the nominalization : "x conducts the attack", "x claims responsibil-ity for the attack", "x was accused of the attack" etc .
are all mapped to "x attacks" (with appropriate settings of th econfidence slot) .
For example, the rule to take "X was accused of Y" and make X the agent of Y i s(((event :predicate accusation-even t:agent ?agent- 1:event (event :identifier ?id-1 .
?R2 ).
?R1 )(event :identifier ?id-1 .
?R4) )->((modify 2 '( :agent ?agent-1 :confidence 'SUSPECTED OR ACCUSEDI) )(delete 1)) )Transformations are also used to expand conjoined structures .
For example, there is a rule to expand "the towns o fx and y " into "the town of x and the town of y", and there is a rule to expand "event at location-1 and location-2 "into "event at location-1 and event at location-2" .There are currently 32 such rules.
These transformations are written as productions and applied using a sim-ple data-driven production system interpreter which is part of the PROTEUS system .TEMPLATE GENERATO ROnce all the sentences in an article have been processed through syntactic and semantic analysis, the result-ing logical forms are sent to the template generator .
The template generator operates in four stages .
First, a framestructure resembling a simplified template (with incident-type, perpetrator, physical-target, human-target, date ,location, instrument, physical-effect, and human-effect slots) is generated for each event .
Date and locationexpressions are reduced to a normalized form at this point.
In particular, date expressions such as "tonight", "lastmonth", " last April", "a year ago", etc.
are replaced by explicit dates or date ranges, based on the dateline of th earticle.
Second, a series of heuristics attempt to merge these frames, mergin g?
frames referring to a common target?
frames arising from the same sentenc e?
an effect frame following an attack frame (e.g ., "The FMLN attacked the town .
Seven civilians died ."
)This merging is blocked if the dates or locations are different, the incident types are incompatible, or the perpetra-tors are incompatible .
Third, a series of filters removes frames involving only military targets and those involvin gevents more than two months old .
Finally, MUC templates are generated from these frames .REFERENCES[1]Grishman, R .
PROTEUS Parser Reference Manual .
PROTEUS Project Memorandum #4-C, Computer ScienceDepartment, New York University, May 1990.
[2] Grishman, R., and Sterling, J.
Preference Semantics for Message Understanding.
Proc .
DARPA Speech andNatural Language Workshop, Morgan Kaufman, 1990 (proceedings of the conference at Harwich Port, MA, Oct .15-18, 1989) .
[3]Grishman, R ., and Sterling, J ., Information Extraction and Semantic Constraints .
Proc.
13th Int' I Conf Compu-tational Linguistics (COLING 90), Helsinki, August 20-25, 1990 .
[4]Sager, N .
Natural Language Information Processing, Addison-Wesley, 1981 .240SPONSORSHIPThe development of the entire PROTEUS system has been sponsored primarily by the Defense AdvancedResearch Projects Agency as part of the Strategic Computing Program, under Contract N00014-85-K-0163 an dGrant N00014-90-J-1851 from the Office of Naval Research .
Additional support has been received from th eNational Science Foundation under grant DCR-85-01843 for work on enhancing system robustness .241
