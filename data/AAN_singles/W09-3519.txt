Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 88?91,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPA Noisy Channel Model for Grapheme-based Machine TransliterationYuxiang Jia, Danqing Zhu, Shiwen YuInstitute of Computational Linguistics, Peking University, Beijing, ChinaKey Laboratory of Computational Linguistics, Ministry of Education, China{yxjia,zhudanqing,yusw}@pku.edu.cnAbstractMachine transliteration is an important Natu-ral Language Processing task.
This paperproposes a Noisy Channel Model for Graph-eme-based machine transliteration.
Moses, aphrase-based Statistical Machine Translationtool, is employed for the implementation ofthe system.
Experiments are carried out onthe NEWS 2009 Machine TransliterationShared Task English-Chinese track.
English-Chinese back transliteration is studied as well.1 IntroductionTransliteration is defined as phonetic translationof names across languages.
Transliteration ofNamed Entities is necessary in many applications,such as machine translation, corpus alignment,cross-language information retrieval, informationextraction and automatic lexicon acquisition.The transliteration modeling approaches canbe classified into phoneme-based, grapheme-based and hybrid approach of phoneme andgrapheme.Many previous studies are devoted to the pho-neme-based approach (Knight and Graehl, 1998;Virga and Khudanpur, 2003).
Suppose that E isan English name and C is its Chinese translitera-tion.
The phoneme-based approach first convertsE into an intermediate phonemic representation p,and then converts p into its Chinese counterpartC.
The idea is to transform both source and targetnames into comparable phonemes so that thephonetic similarity between two names can bemeasured easily.The grapheme-based approach has also at-tracted much attention (Li et al, 2004).
It treatsthe transliteration as a statistical machine transla-tion problem under monotonic constraint.
Theidea is to obtain the bilingual orthographical cor-respondence directly to reduce the possible errorsintroduced in multiple conversions.The hybrid approach attempts to utilize bothphoneme and grapheme information for translit-eration.
(Oh and Choi, 2006) proposed a way tofuse both phoneme and grapheme features into asingle learning process.The rest of this paper is organized as follows.Section 2 briefly describes the noisy channelmodel for machine transliteration.
Section 3 in-troduces the model?s implementation details.
Ex-periments and analysis are given in section 4.Conclusions and future work are discussed insection 5.2 Noisy Channel ModelMachine transliteration can be regarded as anoisy channel problem.
Take the English-Chinese transliteration as an example.
An Eng-lish name E is considered as the output of thenoisy channel with its Chinese transliteration Cas the input.
The transliteration process is as fol-lows.
The language model generates a Chinesename C, and the transliteration model converts Cinto its back-transliteration E. The channel de-coder is used to find ?
that is the most likely tothe word C that gives rise to E. ?
is the resulttransliteration of E.The process can be formulated with equation 1.
)()|()(maxarg)|(maxarg EPCEPCPECPCCC==)(1)Since P(E) is constant for the given E, we canrewrite equation 1 as follows:)|()(maxarg CEPCPCC=)(2)The language model P(C) is simplified as n-gram model of Chinese characters and is trainedwith a Chinese name corpus.
The transliterationmodel P(E|C) is estimated from a parallel corpusof English names and their Chinese translitera-tions.
The channel decoder combines the lan-88guage model and transliteration model to gener-ate Chinese transliterations for given Englishnames.3 ImplementationMoses (Koehn et al, 2007), a phrase-based sta-tistical machine translation tool, is leveraged toimplement the noisy channel model for graph-eme-based machine transliteration without reor-dering process (Matthews, 2007).
Figure 1 is anillustration of the phrase alignment result in ma-chine transliteration of the name pairs ?Clinton?and ????
?, where characters are as words andcombinations of characters are as phrases.Figure 1.
Example phrase alignmentA collection of tools are used by Moses.SRILM is used to build statistical language mod-els.
GIZA++ is used to perform word alignmentsover parallel corpora.
Mert is used for weightoptimization.
It includes several improvements tothe basic training method including randomizedinitial conditions and permuted model order anddynamic parameter range expansion or restric-tion.
Bleu, an automatic machine translationevaluation metric, is used during Mert optimiza-tion.
Moses?
beam-search decoding algorithm isan efficient search algorithm that quickly findsthe highest probability translation among the ex-ponential number of choices.Moses automatically trains translation modelsfor any language pairs with only a collection ofparallel corpora.
The parallel transliteration cor-pora need to be preprocessed at first.
Englishnames need to be lowercased.
Both Englishnames and Chinese transliterations are space de-limited.
Samples of preprocessed input areshown in figure 2.a a b y e  ?
?a a g a a r d  ?
?
?a a l l i b o n e  ?
?
?a a l t o  ?
?
?a a m o d t  ?
?
?Figure 2.
Sample preprocessed name pairs4 ExperimentsThis section describes the data sets, experimentalsetup, experiment results and analysis.4.1 Data SetsThe training set contains 31961 paired namesbetween English and Chinese.
The developmentset has 2896 pairs.
2896 English names are givento test the English-Chinese transliteration per-formance.Some statistics on the training data are shownin table 1.
All the English-Chinese transliterationpairs are distinct.
English names are uniquewhile some English names may share the sameChinese transliteration.
So the total number ofunique Chinese names is less than that of Englishnames.
The Chinese characters composing theChinese transliterations are limited, where thereare only 370 unique characters in the 25033 Chi-nese names.
Supposing that the name length iscomputed as the number of characters it contains,the average length of English names is abouttwice that of Chinese names.
Name length is use-ful when considering the order of the character n-gram language model.#unique transliteration pairs  31961#unique English names 31961#unique Chinese names 25033#unique Chinese characters 370Average number of English charactersper name6.8231Average number of Chinese charactersper name3.1665Maximum number of English charac-ters per name15Maximum number of Chinese charac-ters per name7Table 1.
Training data statistics4.2 Experimental setupBoth English-Chinese forward transliteration andback transliteration are studied.
The process canbe divided into four steps: language model build-ing, transliteration model training, weight tuning,and decoding.
When building language model,data smoothing techniques Kneser-Ney and in-terpolate are employed.
In transliteration modeltraining step, the alignment heuristic is grow-diag-final, while other parameters are defaultsettings.
Tuning parameters are all defaults.When decoding, the parameter distortion-limit isset to 0, meaning that no reordering operation isc lin ton?
?
?89needed.
The system outputs the 10-best distincttransliterations.The whole training set is used for languagemodel building and transliteration model training.The development set is used for weight tuningand system testing.4.3 Evaluation MetricsThe following 6 metrics are used to measure thequality of the transliteration results (Li et al,2009a): Word Accuracy in Top-1 (ACC), Fuzzi-ness in Top-1 (Mean F-score), Mean ReciprocalRank (MRR), MAPref, MAP10, and MAPsys.In the data of English-Chinese transliterationtrack, each source name only has one referencetransliteration.
Systems are required to output the10-best unique transliterations for every sourcename.
Thus, MAPref equals ACC, and MAPsys isthe same or very close to MAP10.
So we onlychoose ACC, Mean F-score, MRR, and MAP10to show the system performance.4.4 ResultsThe language model n-gram order is an impor-tant factor impacting transliteration performance,so we experiment on both forward and backtransliteration tasks with increasing n-gram order,trying to find the order giving the best perform-ance.
Here the development set is used for test-ing.Figure 3 and 4 show the results of forward andback transliteration respectively, where the per-formances become steady when the order reaches6 and 11.
The orders with the best performancein all metrics for forward and back transliterationare 2 and 5, which may relate to the averagelength of Chinese and English names.Language Model N-Gram (Order)00.20.40.60.811 2 3 4 5 6ACC Mean F-scoreMRR MAP10Figure 3.
E2C language model n-gram (forward)Language Model N-Gram (Order)00.20.40.60.811 2 3 4 5 6 7 8 9 10 11ACC Mean F-scoreMRR MAP10Figure 4.
E2C language model n-gram (back)Weights generated in the training step can beoptimized through the tuning process.
The de-velopment set, 2896 name pairs, is divided into 4equal parts, 1 for testing and other 3 for tuning.We take the best settings as the baseline, and in-crease tuning size by 1 part at one time.
Table 2and 3 show the tuning results of forward andback transliteration, where the best results areboldfaced.
Tuning set size of 0 refers to the bestsettings before tuning.
Performances get im-proved after tuning, among which the ACC offorward transliteration gets improved by over11%.
The forward transliteration performancegets improved steadily with the increase of tun-ing set size, while the back transliteration per-formance peaks at tuning set size of 2.TuningsizeACC Mean F-score MRR MAP100 0.543 0.797 0.669 0.2091 0.645 0.851 0.752 0.2312 0.645 0.850 0.749 0.2303 0.655 0.854 0.758 0.233Table 2.
E2C tuning performance (forward)TuningsizeACC Mean F-score MRR MAP100 0.166 0.790 0.278 0.0921 0.181 0.801 0.306 0.1022 0.190 0.806 0.314 0.1043 0.187 0.801 0.312 0.104Table 3.
E2C tuning performance (back)Table 2 shows that forward transliteration per-formance gets improved with the increase of tun-ing set size, so we use the whole development setas the tuning set to tune the final system and thefinal official results from the shared task report(Li et al, 2009b) are shown in table 4.90ACC MeanF-scoreMRR MAPref MAP10 MAPsys0.652 0.858 0.755 0.652 0.232 0.232Table 4.
The final official results of E2C forwardExperiments show that forward transliterationhas better performance than back transliteration.One reason may be that on average English nameis longer than Chinese name, thus need moredata to train a good character level languagemodel.
Another reason is that some informationis lost during transliteration which can not berecovered in back transliteration.
One more veryimportant reason is as follows.
Typically in backtransliteration, you have only one correct refer-ence transliteration, and therefore, a wide cover-age word level language model is very useful.Without it, back transliteration may have a poorperformance.5 Conclusions and future workThis paper proposes a Noisy Channel Model forgrapheme-based machine transliteration.
Thephrase-based statistical machine translation tool,Moses, is leveraged for system implementation.We participate in the NEWS 2009 MachineTransliteration Shared Task English-Chinesetrack.
English-Chinese back transliteration is alsostudied.
This model is language independent andcan be applied to transliteration of any languagepairs.To improve system performance, extensive er-ror analyses will be made in the future and meth-ods will be proposed according to different errortypes.
We will pay much attention to back trans-literation for its seemingly greater difficulty andexplore relations between forward and backtransliteration to seek a strategy solving the twosimultaneously.AcknowledgementsThe authors are grateful to the organizers of theNEWS 2009 Machine Transliteration SharedTask for their hard work to provide such a goodresearch platform.
The work in this paper is sup-ported by a grant from the National Basic Re-search Program of China (No.2004CB318102)and a grant from the National Natural ScienceFoundation of China (No.60773173).ReferencesK.
Knight and J. Graehl.
1998.
Machine Translitera-tion.
Computational Linguistics, Vol.
24, No.
4, pp.599-612.P.
Virga and S. Khudanpur.
2003.
Transliteration ofProper Names in Cross-lingual Information Re-trieval.
In Proceedings of the ACL Workshop onMulti-lingual Named Entity Recognition 2003.H.Z.
Li, M. Zhang and J. Su.
2004.
A Joint SourceChannel Model for Machine Transliteration.
InProceedings of the 42nd ACL, pp.
159-166.J.H.
Oh and K.S.
Choi.
2006.
An Ensemble of Trans-literation Models for Information Retrieval.
In In-formation Processing and Management, Vol.
42,pp.
980-1002.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.Federico, N. Bertoldi, B. Cowan, W. Shen, C.Moran, R. Zens, C. Dyer, O. Bojar, A. Constantinand E. Herbst.
2007.
Moses: Open Source Toolkitfor Statistical Machine Translation.
In Proceedingsof the 45th ACL Companion Volume of the Demoand Poster Sessions, pp.
177-180.D.
Matthews.
2007.
Machine Transliteration of ProperNames.
Master thesis.
University of Edinburgh.H.Z.
Li, A. Kumaran, M. Zhang and V. Pervouchine.2009a.
Whitepaper of NEWS 2009 Machine Trans-literation Shared Task.
In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS2009), Singapore.H.Z.
Li, A. Kumaran, V. Pervouchine and M. Zhang.2009b.
Report on NEWS 2009 Machine Translit-eration Shared Task.
In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS2009), Singapore.91
