Multilingual Speech Recognition for Information Retrievalin Indian contextUdhyakumar.N, Swaminathan.R and Ramakrishnan.S.KDept.
of Electronics and Communication EngineeringAmrita Institute of Technology and ScienceCoimbatore, Tamilnadu ?
641105, INDIA.udhay_ece@rediffmail.com,{rswami_ece,skram_ece}@yahoo.com.AbstractThis paper analyzes various issues in buildinga HMM based multilingual speech recognizerfor Indian languages.
The system is originallydesigned for Hindi and Tamil languages andadapted to incorporate Indian accented Eng-lish.
Language-specific characteristics inspeech recognition framework are highlighted.The recognizer is embedded in information re-trieval applications and hence several issueslike handling spontaneous telephony speech inreal-time, integrated language identificationfor interactive response and automatic graph-eme to phoneme conversion to handle Out OfVocabulary words are addressed.
Experimentsto study relative effectiveness of different al-gorithms have been performed and the resultsare investigated.1 IntroductionHuman preference for speech communication has led tothe growth of spoken language systems for informationexchange.
Such systems need a robust and versatilespeech recognizer at its front-end, capable of decodingthe speech utterances.
A recognizer developed for spo-ken language information retrieval in Indian languagesshould have the following features: It must be insensitive to spontaneous speech effectsand telephone channel noise. Language Switching is common in India wherethere is a general familiarity of more than one lan-guage.
This demands for a multilingual speech rec-ognition system to decode sentences with wordsfrom several languages. Integrated language identification should be possi-ble which helps later stages like speech synthesis tointeract in user?s native language.This paper reports our work in building a multilingualspeech recognizer for Tamil, Hindi and accented Eng-lish.
To handle sparseness in speech data and linguisticknowledge for Indian languages, we have addressedtechniques like cross-lingual bootstrapping, automaticgrapheme to phoneme conversion and adaptation ofphonetic decision trees.In the area of Indian language speech recogni-tion, various issues in building Hindi LVCSR systemshave been dealt in (Nitendra Rajput, et al 2002).
ForTamil language (Yegnanarayana.B et al 2001) attemptsto develop a speaker independent recognition system forrestricted vocabulary tasks.
Speech recognizer for rail-way enquiry task in Hindi is developed by(Samudravijaya.K 2001).The paper is organized as follows.
In sections 2,3 and 4 we discuss the steps involved in building a mul-tilingual recognizer for Hindi and Tamil.
In section 5automatic generation of phonetic baseforms from or-thography is explained.
Section 6 presents the results ofadapting the system for accented English.
Techniques toincorporate Language Identification are described inSection 7.
Finally we conclude with a note on futurework in section 8.2 Monolingual Baseline systemsMonolingual baseline systems are designed for Tamiland Hindi using HTK as the first step towards multilin-gual recognition.
We have used OGI Multilanguagetelephone speech corpus for our experiments (YeshwantK.
Muthusamy et al1992).
The database is initiallycleaned up and transcribed both at word and phonelevel.
The phoneme sets for Hindi and Tamil are ob-tained from (Rajaram.S 1990) and (Nitendra Rajput, etal.
2002).
The spontaneous speech effects like filledpauses (ah, uh, hm), laughter, breathing, sighing etc.
aremodeled with explicit words.
The background noisesfrom radio, fan and crosstalk are pooled together andrepresented by a single model to ensure sufficient train-ing.
Front-end features consist of 39 dimensionalMelscale cepstral coefficients.
Vocal Tract LengthNormalization (VTLN) is used to reduce inter and intra-speaker variability.2.1 Train and Test setsThe OGI Multilanguage corpus consists up to nine sepa-rate responses from each caller, ranging from singlewords to short-topic specific descriptions to 60 secondsof unconstrained spontaneous speech.
Tamil data totaledaround 3 hours and Hindi data around 2.5 hours of con-tinuous speech.
The details of training and test data usedfor our experiments are shown in Table.1.Lang Data Sent Words SpkrsTrain 900 7320 250 Tamil Test 300 1700 15Train 125 10500 125 Hindi Test 200 1250 12Table.1: Details of Training and Test Corpus.2.2 Context Independent TrainingThe context independent monophones are modeled byindividual HMMs.
They are three state strict left-to-right models with a single Gaussian output probabilitydensity function for each state.
Baum-Welch training iscarried out to estimate the HMM parameters.
The re-sults of the monolingual baseline systems are shown inTable.2.Accuracy Language Word Level Sentence LevelHindi 49.7% 46.2%Tamil 50.3% 48.7%Table.2: Recognition Accuracy of Monophone Models.The difference in accuracy cannot be attributed to thelanguage difficulties because there are significant varia-tions in database quality, vocabulary and quantity be-tween both the languages.TAMIL: The recognition result for monophones showsthat prominent errors are due to substitution betweenphones, which are acoustic variants of the same alpha-bet (eg.ch and s, th and dh, etc.).
Hence the lexicon isupdated with alternate pronunciations for these words.As a result the accuracy improved to 56%.HINDI: Consonant clusters are the main sources of er-rors in Hindi.
They are replaced with a single consonantfollowed by a short spelled ?h?
phone in the lexicon.This increased the accuracy to 52.9%.2.3 Context Dependent TrainingEach monophone encountered in the training data iscloned into a triphone with left and right contexts.
Alltriphones that have the same central phone end up withdifferent HMMs that have the same initial parametervalues.
HMMs that share the same central phone areclustered using decision trees and incrementally trained.The phonetic questions (nasals, sibilants, etc.)
for treebased state tying require linguistic knowledge about theacoustic realization of the phones.
Hence the decisiontree built for American English is modified to modelcontext-dependency in Hindi and Tamil.
Further unsu-pervised adaptation using Maximum Likelihood LinearRegression (MLLR) is used to handle calls from non-native speakers.
Environment adaptation is analyzed forhandling background noise.3 Multilingual Recognition SystemMultilingual phoneme set is obtained from monolingualmodels by combining acoustically similar phones.
Themodel combination is based on the assumption that thearticulatory representations of phones are so similaracross languages that they can be considered as unitsthat are independent from the underlying language.Such combination has the following benefits (Schultz.Tet al1998): Model sharing across languages makes the systemcompact by reducing the complexity of the system. Data sharing results in reliable estimation of modelparameters especially for less frequent phonemes. Multilingual models, bootstrapped as seed modelsfor an unseen target language improve the recogni-tion accuracy considerably. Global phoneme pool allows accurate modeling ofOOV (Out Of Vocabulary) words.International Phonetic Association has classified soundsbased on the phonetic knowledge, which is independentof languages.
Hence IPA mapping is used to form theglobal phoneme pool for multilingual recognizer.
In thisscheme, phones of Tamil and Hindi having the sameIPA representation are combined and trained with datafrom both the languages (IPA 1999).The phonetic inventory of the multilingual rec-ognizer ML?
can be expressed as a group of languageindependent phones LI?
unified with a set of languagedependent phones LD?
that are unique to Hindi orTamil.LDHLDTLIML ????
?=?where LDT?
is the set of Tamil dependent models.LDH?
is the set of Hindi dependent modelsThe share factor SF is calculated as5.1704359?+=?
?+?=MLHTSFwhich implies a sharing rate of 75% between boththe languages.
The share factor is a measure of relationbetween the sum of language specific phones and thesize of the global phoneme set.
The high overlap ofHindi and Tamil phonetic space is evident from thevalue of SF.
This property has been a motivating factorto develop a multilingual system for these languages.After merging the monophone models, context depend-ent triphones are created as stated earlier.
Alternatedata-driven techniques can also be used for acousticmodel combination, but they are shown to be outper-formed by IPA mapping (Schultz.T et al1998).4 Cross Language AdaptationOne major time and cost limitation in developingLVCSR systems in Indian languages is the need forlarge training data.
Cross-lingual bootstrapping is ad-dressed to overcome these drawbacks.
The key idea inthis approach is to initialize a recognizer in the targetlanguage by using already developed acoustic modelsfrom other language as seed models.
After the initializa-tion the resulting system is rebuilt using training data ofthe target language.
The cross-language seed modelsperform better than flat starts or random models.
Hencethe phonetic space of Hindi and Tamil ML?
is popu-lated with English models E?
in the following steps. The English phones are trained with Networkspeech database (NTIMIT), which is the telephonebandwidth version of widely used TIMIT database. To suit Indian telephony conditions, 16KHzNTIMIT speech data is down-sampled to 8KHz. A heuristic IPA mapping combined with data-driven approach is used to map English modelswith multilingual models.
The mappings are shownin Table.3. If any phone in E?
maps to two or more phones inML?
the vectors are randomly divided between thephones since duplication reduces classification rate. After bootstrapping, the models are trained withdata from both the languages.Hindi Tamil English Hindi Tamil English  AA  - KD - AA+N  - KH  AY 	  L - AY+N  M  AW   N - AW+N  NG 	 AX  OW - AX+N  - OW+N  B   P - BD  - PD - BD+HH  - F  CH  R - CH+HH   S  D  - K+SH - DD   T - DH  - TD - DH+HH  - TH+HH - DX+HH   TX  EY  - TH - EY+N    UH - F   - UH+N! - G "  UW# - GD+HH " - UW+N$  HH %  V&  IH '  Y&(  IY ) - Z&( - IY+N * - DX+N)  JH -  N+  JH+HH -  N+Y  K -  N,  L -  R,  L -  AETable.3: Mapping between Multilingual phoneme setand English phones for crosslingual bootstrapping.The improvement in accuracy due to crosslingual boot-strapping is evident from the results shown in Table.4.This is due to the implicit initial alignment caused bybootstrapped seed models.
The results are calculated forcontext dependent triphones in each case.
The degrada-tion in accuracy of the multilingual system compared tomonolingual counterparts is attributed to generalizationand parameter reduction.System AccuracyMonolingual_Hindi 95%Monolingual_Tamil 97.6%Multilingual 90.3%Bootstrapped 94.5%Table.4: Comparison of accuracy for Monolingual,Multilingual and Bootstrapped systems.5 Grapheme to Phoneme conversionDirect dictionary lookup to generate phonetic baseforms is limited by time, effort and knowledge broughtto bear on the construction process.
The dictionary cannever be exhaustive due to proper names and pronuncia-tion variants.
A detailed lexicon also occupies a largedisk space.
The solution is to derive the pronunciationof a word from its orthography.
Automatic grapheme tophoneme conversion is essential in both speech synthe-sis and automatic speech recognition.
It helps to solvethe out-of-vocabulary word problem, unlike the caseusing a soft lookup dictionary.
We have examined bothrule-based and data-driven self-learning approaches forautomatic letter-to-sound (LTS) conversion.5.1 Rule-Based LTSInspired by the phonetic property of Indian languagesgrapheme to phoneme conversion is usually carried outby a set of handcrafted phonological rules.
For examplethe set of rules that maps the alphabet to its correspond-ing phones is given below.
The letter  (/p/) in Tamilcan be pronounced as ?p?
as in  !
or ?b?
as in!"
or ?P?
as in  !.P_Rules:1.
{Anything, "pp", Anything, "p h" },2.
{Nothing, "p", Anything, "p*b" },3.
{Anything, "p", CONSONANT, "p" },4.
{NASAL, "p", Anything, "b" },5.
{Anything, "p", Anything, "P" }Here * indicates a pronunciation variant.
It maybe noted that for any word, these context sensitive rulesgive a phonemic transcription.
These rules are orderedas most specific first with special conventions aboutlookup, context and target (Xuedong Huang et al 2001).For example, the rule 4 means that the alphabet /p/when preceeded by a nasal and followed by anything ispronounced as ?b?.
These rules could not comprehendall possibilities.
The exceptions are stored in an excep-tion list.
The system first searches this lookup dictionaryfor any given word.
If a match is found, it reads out thetranscription from the list.
Otherwise, it generates pro-nunciation using the rules.
This approach helps to ac-commodate pronunciation variations specific to somewords and thus avoiding the need to redraft the com-plete rules.5.2 CART Based LTSExtensive linguistic knowledge is necessary to developLTS rules.
As with any expert system, it is difficult toanticipate all possible relevant cases and sometimeshard to check for rule interface and redundancy.
In viewof how tedious it is to develop phonological rulesmanually, machine-learning algorithms are used to auto-mate the acquisition of LTS conversion rules.
We haveused statistical modeling based on CART (ClassificationAnd Regression Trees) to predict phones based on let-ters and their context.Indian languages usually have a one-to-one map-ping between the alphabets and corresponding phones.This avoids the need for complex alignment methods.The basic CART component includes a set of Yes-Noquestions about the set membership of phones and let-ters that provide the orthographic context.
The questionthat has the best entropy reduction is chosen at eachnode to grow the tree from the root.
The performancecan be improved by including composite questions,which are conjunctive and disjunctive combinations ofprimitive questions and their negations.
The use ofcomposite questions can achieve longer-range optimum,improves entropy reduction and avoids data fragmenta-tion caused by greedy nature of the CART (Breiman.Let al1984).
The target class or leafs consist of individ-ual phones.
In case of alternate pronunciations thephone variants are combined into a single class.5.3 ExperimentsBoth rule-based and CART based LTS systems are de-veloped for Tamil and evaluated on a 2k word hand-crafted lexicon.
Transliterated version of Tamil text isgiven as input to the rule-based system.
The perform-ance of decision trees is comparable to the phonologicalrules (Table.5).
We observed some interesting resultswhen the constructed tree is visualized after pruning.The composite questions generated sensible clusters ofalphabets.
Nasals, Rounded vowels, Consonants aregrouped together.
The other phenomenon is that CARThas derived some intricate rules among the words,which were considered as exceptions by the linguistswho prepared the phonological rules.
Statistical meth-ods to use phonemic trigrams to rescore n-best list gen-erated by decision tree and use of Weighted Finite StateAutomata for LTS rules are under study.LTS System Word Accuracy Phone AccuracyRule-Based 95.2% 97.5%CART 96.3% 98.7%Table.5: LTS results using Rule-based and CART sys-tems on Tamil Lexicon.The results show that automatic rule generation withCART performs better than manually coded rules.6 Adaptation for Accented EnglishEnglish words are more common in Indian conversa-tion.
In OGI multi language database, 32% of Hindi and24% of Tamil sentences have English words.
Thereforeit is necessary to include English in a multilingual rec-ognizer designed for Indian languages.
English being anon-native language, Indian accents suffer from disflu-ency, low speaking rate and repetitions.
Hence accuracyof a system trained with American English degradessignificantly when used to recognize Indian accentedEnglish.
Various techniques like lexical modeling,automatic pronunciation modeling using FST, speakeradaptation, retraining with pooled data and model inter-polation are being explored to reduce the Word ErrorRates for non-native speakers.6.1 Speech CorpusWe have used Foreign Accented English corpus fromCentre for Spoken Language Understanding (CSLU)and Native American accented Network TIMIT corpusfor the experiments.
Table.6 gives the details of speechdatabases used for our study:Database Sent Words SpkrsAmerican Accent (N_ENG) 4500 43k 460Tamil Accent (NN_TAE) 300 3.2k 300Native Tamil (N_TAM) 900 7.3k 250Table.6.
Details of databases used for Tamil accentedEnglish recognition.6.2 Previous WorkLexical adaptation techniques introduce pronunciationvariants in the lexicon for decoding accented speech(Laura.M.Tomokiyo 2000).
The problem with thesemethods is that the context dependent phones in theadapted lexicon seldom appear in the training data andhence they are not trained properly.
Statistical methodsfor automatic acquisition of pronunciation variants hadproduced successful results (Chao Huang et al2001).These algorithms are costlier in terms of memory spaceand execution time, which makes them difficult to han-dle real-time speech.
In acoustic modeling techniquesthe native models are modified with available accentedand speakers?
native language data.6.3 ExperimentsThe base line system is trained on N_ENG corpus and isused to recognize NN_TAE utterances.
It is a well-known fact that accented speech is influenced by nativelanguage of the speaker.
Hence we tried decodingNN_TAE data using Tamil recognizer.
The lexicon isgenerated using grapheme to phoneme rules.
The accu-racy dropped below the baseline, which means thatthere is no direct relationship between N_TAM andNN_TAE speech.
The result is already confirmed by(Laura.M.Tomokiyo 2000).Careful analysis of the accented speech showsthat perception of the target phone is close to acousti-cally related phone in speaker?s native language.
Asspeaker gains proficiency, his pronunciation is tunedtowards the target phone and hence the influence ofinterfering phone is less pronounced.
This clearly sug-gests that any acoustic modeling technique should startwith native language models and suitably modify themto handle accented English.
Hence attempts to retrain oradapt N_ENG models using N_TAM data have de-graded the accuracy.
First set of experiments is carriedout using N_ENG models.
MLLR adaptation and re-training with NN_TAE data increased the accuracy.
Inthe second set of experiments English models are boot-strapped using N_TAM models by heuristic IPA map-ping.
They are then trained by pooling N_ENG andNN_TAE data.
This method showed better performancethan other approaches.
The comparative results areshown in figure.1.Figure.1: Comparison of Accuracies of acoustic model-ing techniques on Tamil accented English.7 Language IdentificationAutomatic language identification (LID) has receivedincreased interest with the development of multilingualspoken language systems.
LID can be used in Tele-phone companies handling foreign calls to automaticallyroute the call to an operator who is fluent in that lan-guage.
In information retrieval systems, it can be usedby speech synthesis module to respond in user?s nativelanguage.
It can also serve as a front-end in speech-to-speech translation.
LVCSR based LID can be incorpo-rated in both acoustic and language modeling.In language independent approach a multilingualrecognizer using language independent models is used.Tamil and Hindi bigram models are used to rescore therecognized phone string.
The language providing high-est log probability is hypothesized.
The bigrams forboth the languages are evaluated on the transcribedtraining data.
In language dependent approach eachphone is given a language tag along with its label.
Themodels are trained solely with data from its own lan-guage.
Language is identified implicitly from the recog-nized phone labels.
This approach has the advantage ofcontext and text independent language identification.
(Lamel.L et al1994).
The results for both the ap-proaches are given in Table.7.LID System 2s Chunks 5s ChunksLanguage Independent 87.1% 98.5%Language Dependent 91.3% 97.8%Table.7: Comparison of LID accuracy of language in-dependent and Language dependent systems.8 Conclusion and Future WorkThis work presents the recent results in building a full-fledged multilingual speech recognizer for our ongoingproject ?Telephone-based spoken language informationretrieval system for Indian languages?.
Techniques likeCART based LTS, language identification using bi-grams and accented English recognition by native lan-guage bootstrapping have been experimented.Significant amount of research remains in han-dling spontaneous speech effects and nonverbal sounds,which are common in real world data.
We have plannedto explore language modeling and adaptive signal proc-essing techniques to address these problems (Acero.A1993).
Use of model interpolation and weighted finitestate transducers (Karen Livescu 1999) are presentlyanalyzed to improve the system performance on ac-cented English.
From our experience we understood theimportance of the acronym ?while there is no data likemore data, there is also no data like real data?.
Hencewe have started data collection to carry out next phaseof experiments.9 AcknowledgementsThe authors wish to thank Mr. C.Santosh Kumar, for hisactive support, great encouragement and guidance.
Wegratefully thank Mr.Nitendra Rajput, IBM ResearchLab, India and IIT Madras speech group for their valu-able help.
We also thank Mr.Shunmugom, Linguisticdepartment, Bharathiar university, Coimbatore andMr.Omkar.N.Koul, Head of faculty of languages, Mus-soori for useful linguistic discussions.
We would like toacknowledge all the members of Amrita Institute fortheir enthusiasm in transcribing the data.
We also thankMata Amritananda Mayi for her love and blessings.10 References[Acero.A 1993] Acero.A 1993.
Acoustic and Environ-mental robustness in speech recognition, KluwerAcademic Publishers.
[Breiman.L et al1984] Breiman.L et al1984.
Classifi-cation and regression trees.
Monterey, Calif.,U.S.A Wadsworth, Inc.[Chao Huang et al2001] Chao Huang, Eric Chang, TaoChen 2001.
Accent Issues in Large VocabularyContinuous Speech Recognition (LVCSR) Techni-cal Report MSR-TR-2001-69.
[IPA 1999] IPA 1999.
Handbook of the InternationalPhonetic Association, Cambridge University Press.
[Karen Livescu 1999] Karen Livescu 1999.
Analysisand Modeling of Non-Native Speech for AutomaticSpeech Recognition.
[Lamel.L et al1994] Lamel.L.F, Gauvain.S 1994.
Lan-guage Identification using phone-based acousticlikelihoods.
In Proc.ICASSP,Adelaide, Australia.
[Laura.M.Tomokiyo 2000] Laura Mayfield Tomokiyo2000.
Handling Non-native speech in LVCSR, InProc.InSTIL..[Nitendra Rajput, et al 2002] Nitendra Rajput, et al2000.
A large vocabulary continuous speech recog-nition system for hindi In Proc.
NCC, India.
[Rajaram.S 1990] Rajaram.S 1990.
Tamil PhoneticReader, Central Institute of Indian Languages.
[Samudravijaya.K 2001] Samudravijaya.K 2001.
HindiSpeech Recognition, J. Acoustic Society of India,vol 29, pp 385-393.
[Schultz.T et al1998] T. Schultz et al1998.
Multilin-gual and Crosslingual Speech Recognition In Proc.DARPA Workshop on Broadcast News Transcrip-tion and Understanding, Lansdowne, VA.[Xuedong Huang et al 2001] Xuedong Huang, AlexAcero, Hsiao-Wuen Hon 2001.
Spoken LanguageProcessing, A guide to theory, Algorithm, and Sys-tem development,Prentice Hall.
[Yegnanarayana.B et al 2001] Yegnanarayana.B andNayeemullah Khan.A 2001.
Development of  aspeech recognition system for Tamil for restrictedsmall tasks  In Proc.
NCC , India.
[Yeshwant K. Muthusamy et al1992] Yeshwant K.Muthusamy et al1992.
The OGI Multi-LanguageTelephone Speech Corpus In Proc.
ICSLP.
