Proceedings of the First Workshop on Argumentation Mining, pages 88?97,Baltimore, Maryland USA, June 26, 2014. c?2014 Association for Computational LinguisticsTowards segment-based recognition of argumentation structurein short textsAndreas PeldszusApplied Computational LinguisticsUniversity of Potsdampeldszus@uni-potsdam.deAbstractDespite recent advances in discourse pars-ing and causality detection, the automaticrecognition of argumentation structure ofauthentic texts is still a very challeng-ing task.
To approach this problem, wecollected a small corpus of German mi-crotexts in a text generation experiment,resulting in texts that are authentic butof controlled linguistic and rhetoric com-plexity.
We show that trained annotatorscan determine the argumentation struc-ture on these microtexts reliably.
We ex-periment with different machine learningapproaches for automatic argumentationstructure recognition on various levels ofgranularity of the scheme.
Given the com-plex nature of such a discourse under-standing tasks, the first results presentedhere are promising, but invite for furtherinvestigation.1 IntroductionAutomatic argumentation recognition has manypossible applications, including improving docu-ment summarization (Teufel and Moens, 2002),retrieval capabilities of legal databases (Palau andMoens, 2011), opinion mining for commercialpurposes, or also as a tool for assessing publicopinion on political questions.However, identifying and classifying argumentsin naturally-occurring text is a very challengingtask for various reasons: argumentative strategiesand styles vary across texts genres; classifying ar-guments might require domain knowledge; fur-thermore, argumentation is often not particularlyexplicit ?
the argument proper is being infiltratedwith the full range of problems of linguistic ex-pression that humans have at their disposal.Although the amount of available texts featur-ing argumentative behaviour is growing rapidly inthe web, we suggest there is yet one resource miss-ing that could facilitate the development of auto-matic argumentation recognition systems: Shorttexts with explicit argumentation, little argumenta-tively irrelevant material, less rhetorical gimmicks(or even deception), in clean written language.For this reason, we conducted a text generationexperiment, designed to control the linguistic andrhetoric complexity of written ?microtexts?.
Thesetexts have then been annotated with argumentationstructures.
We present first results of automaticclassification of these arguments on various levelsof granularity of the scheme.The paper is structured as follows: In the nextsection we describe related work.
Section 3presents the annotation scheme and an agreementstudy to prove the reliability.
Section 4 describesthe text generation experiment and the resultingcorpus.
Section 5 and 6 present the results of ourfirst attempts in automatically recognizing the ar-gumentative structure of those texts.
Finally, Sec-tion 7 concludes with a summary and an outlookon future work.2 Related WorkThere exist a few ressources for the study of ar-gumentation, most importantly perhaps the AIFdatabase, the successor of the Araucaria corpus(Reed et al., 2008), that has been used in dif-ferent studies.
It contains several annotated En-glish datasets, most interestingly for us one cov-ering online newspaper articles.
Unfortunately,the full source text is not part of the download-able database, which is why the linguistic ma-terial surrounding the extracted segments is noteasy to retrieve for analysis.
Instead of manu-ally annotating, Cabrio and Villata (2012) cre-ated an argumentation resource by extracting ar-gumentations from collaborative debate portals,such as debatepedia.org, where arguments are al-ready classified into pro and con classes by the88users.
Unfortunately, those arguments are them-selves small texts and their internal argumenta-tive structure is not marked up.
Finally, to thebest of our knowledge, the only existing corpusof German newspaper articles, essays or editori-als annotated with argumentation structure is thatused by Stede and Sauermann (2008), featuringten commentaries from the Potsdam CommentaryCorpus (Stede, 2004).
Although short, these textsare rhetorically already quite complex and oftenhave segments not relevant to the argument.1In terms of automatic recognition, scientificdocuments of different fields have been studied in-tensively in the Argumentative Zoning approachor in similar text zoning approaches (Teufel andMoens, 2002; Teufel et al., 2009; Teufel, 2010;Liakata et al., 2012; Guo et al., 2013).
Here, sen-tences are classified into different functional orconceptual roles, grouped together with adjacentsentences of the same class to document zones,which induces a flat partitioning of the text.
A va-riety of machine learning schemes have been ap-plied here.Another line of research approaches argumen-tation from the perspective of Rhetorical Struc-ture Theory (RST) (Mann and Thompson, 1988)and works with argumentation-enriched RST trees(Azar, 1999; Green, 2010).
However, we do notconsider RST to be the best level for representingargumentation, due to its linearization constraints(Peldszus and Stede, 2013a, sec.
3).
Nevertheless,noteworthy advances have been made recently inrhetorical parsing (Hernault et al., 2010; Feng andHirst, 2012).
Whether hybrid RST argumenta-tion structures will profit similarly remains to beshown.
A more linguistically oriented approachis given with the TextCoop platform (Saint-Dizier,2012) for analyzing text on the discourse levelwith emphasis on argumentation.One step further, Feng and Hirst (2011) concen-trate on types of arguments and use a statisticalapproach to classify already identified premisesand conclusions into five common argumentationschemes (Walton et al., 2008).3 Annotation SchemeOur representation of the argumentation structureof a text is based on Freeman?s theory of ar-gumentation structure (Freeman, 1991; Freeman,1We intend to use this resource, when we move on to ex-periment with more complex texts.2011).2 Its central idea is to model argumen-tation as a hypothetical dialectical exchange be-tween the proponent, who presents and defendshis claims, and the opponent, who critically ques-tions them in a regimented fashion.
Every move insuch a dialectical exchange corresponds to a struc-tural element in the argument graph.
The nodes ofthis graph represent the propositions expressed intext segments (round nodes are proponent?s nodes,square ones are opponent?s nodes), the arcs be-tween those nodes represent different supporting(arrow-head links) and attacking moves (circle-head links).
The theory distinguishes only a fewgeneral supporting and attacking moves.
Thosecould be specified further with a more fine grainedset, as provided for example by the theory of ar-gumentation schemes (Walton et al., 2008).
Still,we focus on the coarse grained set, since this re-duces the complexity of the already sufficientlychallenging task of automatic argument identifica-tion and classifcation.
Our adaption of Freeman?stheory and the resulting annotation scheme is de-scribed in detail and with examples in (Peldszusand Stede, 2013a).3.1 Reliability of annotationThe reliability of the annotation scheme has beenevaluated in two experiments.
We will first reca-pitulate the results of a previous study with naiveannotators and then present the new results withexpert annotators.Naive annotators: In (Peldszus and Stede,2013b), we presented an agreement study with26 naive and untrained annotators: undergradu-ate students in a ?class-room annotation?
szenario,where task introduction, guideline reading and theactual annotation is all done in one obligatory90 min.
session and the subjects are likely tohave different experience with annotation in gen-eral, background knowledge and motivation.
Weconstructed a set of 23 microtexts (each 5 seg-ments long) covering different linearisations ofseveral combinations of basic argumentation con-structs.
An example text and the correspondingargumentation structure graph is shown in Fig-ure 1.
On these texts, the annotators achievedmoderate agreement3 for certain aspects of the ar-2The theory aims to integrate the ideas of Toulmin (1958)into the argument diagraming techniques of the informallogic tradition (Beardsley, 1950; Thomas, 1974) in a system-atic and compositional way.3Agreement is measured in Fleiss ?
(Fleiss, 1971).89gument graph (e.g.
?=.52 in distinguishing pro-ponent and opponent segments, or ?=.58 in des-tinguishing supporting and attacking segments),yet only a marginal agreement of ?=.38 on thefull labelset describing all aspects of the argumentgraph.
However, we could systematically identifysubgroups performing much better than averageusing clustering techniques: e.g.
a subgroup of6 annotators reached a relatively high IAA agree-ment of ?=.69 for the full labelset and also highagreement with gold data.Expert annotators: Here, we present the re-sults of an agreement study with three expert an-notators: two of them are the guideline authors,one is a postdoc in computational linguistics.
Allthree are familiar with discourse annotation tasksin general and specifically with this annotationscheme.
They annotated the same set of 23 mi-crotexts and achieved a high agreement of ?=.83on the full labelset describing all aspects of the ar-gument graph.
The distinction between supportingand attacking was drawn with very high agreementof ?=.95, the one between proponent and oppo-nent segments even with perfect agreement.Since argumentation structures can be reliablyannotated using this scheme, we decided to createa small corpus of annotated microtexts.4 DatasetThe corpus used in this study consists of two parts:on the one hand, the 23 microtexts used in the an-notation experiments just described; on the otherhand, 92 microtexts that have been collected in acontrolled text generation experiment.
We will de-scribe this experiment in the following subsection.4.1 Microtext generation experimentWe asked 23 probands to discuss a controversialissue in a short text of 5 segments.
A list of 17of these issues was given, concerning recent po-litical, moral, or everyday?s life questions.
Eachproband was allowed to discuss at maximum fiveof the given questions.
Probands were instructedto first think about the pros & cons of the con-troversial question, about possible refutation andcounter-refutations of one side to the other.
Onthis basis, probands should decide for one sideand write a short persuasive text (correspondingto the standards of the written language), arguingin favour of their chosen position.The written texts were required to have a lengthof five segments.
We decided not to bother ourprobands with an exact definition of a segment,as this would require the writers to reliably iden-tify different complex syntactic constructions.
In-stead, we simply characterized it as a clause ora sentence, expressing an argumentative point onits own.
We also required all segments to be ar-gumentatively relevant, in the sense that they ei-ther formulate the main claim of the text, sup-port the main claim or another segment, or attackthe main claim or another segment.
This require-ment was put forward in order to prevent digres-sion and argumentatively irrelevant but commonsegment types, such as theme or mood setters, aswell as background information.
Furthermore, wedemanded that at least one possible objection tothe main claim be considered in the text, leavingopen the choice of whether to counter that objec-tion or not.
Finally, the text should be written insuch a way that it would be understandable with-out having the question as a headline.In total, 100 microtexts have been collected.The five most frequently chosen issues are:?
Should the fine for leaving dog excrementson sideways be increased??
Should shopping malls generally be allowedto open on Sundays??
Should Germany introduce the deathpenalty??
Should public health insurance cover treat-ments in complementary and alternativemedicine??
Should only those viewers pay a TV licencefee who actually want to watch programs of-fered by public broadcasters?4.2 Cleanup and annotationSince we aim for a corpus of clean, yet authen-tic argumentation, all texts have been checked forspelling and grammar errors.
As a next step, thetexts were segmented into elementary units of ar-gumentation.
Due to the (re-)segmentation, not alltexts conform to the length restriction of five seg-ments, they can be one segment longer or shorter.Unfortunately, some probands wrote more thanfive main clauses, yielding texts with up to ten seg-ments.
We decided to shorten these texts downto six segments by removing segments that ap-pear redundant or negligible.
This removal alsorequired modifications in the remaining segmentsto maintain text coherence, which we made as90[Energy-saving light bulbs contain a considerable amountof toxic substances.
]1[A customary lamp can for instancecontain up to five milligrams of quicksilver.
]2[For this rea-son, they should be taken off the market,]3[unless theyare virtually unbreakable.
]4[This, however, is simply notcase.
]5(a) (b)node id rel.
id full label target1 1 PSNS (n+2)2 2 PSES (n-1)3 3 PT (0)4 4 OAUS (r-3)5 5 PARS (n-1)(c)Figure 1: An example microtext: the (translated) segmented text in (a), the argumentation structure graphin (b), the segment-based labeling representation in (c).minimal as possible.
Another source of problemswere segments that do not meet our requirementof argumentative relevance.
Some writers did notconcentrate on discussing the thesis, but movedon to a different issue.
Others started the textwith an introductory presentation of backgroundinformation, without using it in their argument.We removed those segments, again with minimalchanges in the remaining segments.
Some textscontaining several of such segments remained tooshort after the removal and have been discardedfrom the dataset.After cleanup, 92 of the 100 written texts re-mained for annotation of argumentation structure.We found that a few texts did not meet the require-ment of considering at least one objection to theown position.
In a few other texts, the objection isnot present as a full segment, but rather implicitlymentioned (e.g.
in a nominal phrase or participle)and immediatly rejected in the very same segment.Those segments are to be annotated as a support-ing segment according to the guidelines, since theattacking moves cannot be expressed as a relationbetween segments in this case.We will present some statistics of the resultingdataset at the end of the following subsection.5 ModellingIn this section we first present, how the argu-mentation structure graphs can be interpreted asa segment-wise labelling that is suitable for au-tomatic classification.
We then describe the setof extracted features and the classifiers set up forrecognition.5.1 PreparationsIn the annotation process, every segment is as-signed one and only one function, i.e.
every nodein the argumentative graph has maximally one out-going arc.
The graph can thus be reinterpreted asa list of segment labels.Every segment is labeled on different levels:The ?role?-level specifies the dialectical role (pro-ponent or opponent).
The ?typegen?-level specifiesthe general type, i.e.
whether the segment presentsthe central claim (thesis) of the text, supports orattacks another segment.
The ?type?-level addi-tionally specifies the kind of support (normal orexample) and the kind of attack (rebutter or under-cutter).
Whether a segment?s function holds onlyin combination with that of another segment (com-bined) or not (simple) is represented on the ?com-bined?-level.
The target is finally specified by aposition relative identifier: The offset -x. .
.
0. .
.
+xidentifies the targeted segment, relative from theposition of the current segment.
The prefix ?n?states that the proposition of the node itself is thetarget, while the prefix ?r?
states that the relationcoming from the node is the target.4The labels of each separate level can be mergedto form a complex tagset.
We interpret the re-sult as a hierarchical tagset as it is presented inFigure 2.
The label ?PSNS(n+2)?
for examplestands for a proponent?s segment, giving normal,non-combined support to the next but one seg-ment, while ?OAUS(r-1)?
represents an opponent?ssegment, undercutting the relation established bythe immediately previous segment, not combined.Figure 1c illustrates the segment-wise labelling forthe example microtext.The dataset with its 115 microtexts has 8183word tokens, 2603 word types and 579 segmentsin total.
The distribution of the basic labels andthe complex ?role+type?
level is presented in Ta-ble 1.
The label distribution on the ?role+type?level shows that most of the opponent?s attacks arerebutting attacks, directed against the central claim4Segments with combined function (as e.g.
linked sup-porting arguments) are represented by equal relation ids,which is why segments can have differing node and relationids.
However, for the sake of simplicity, we will only con-sider example of non-combined nature in this paper.91Figure 2: The hierarchy of segment labels.or its premises directly (OAR>OAU).
In contrast,the proponent?s counters of these attack are typi-cally untercutting attacks, directed against the at-tack relation (PAU>PAR).
This is due to the au-thor?s typical strategy of first conceding some as-pect in conflict with the main claim and then ren-dering it irrelevant or not applicable without di-rectly challenging it.
Note however, that about40% of the opponents objections have not beencountered by the proponent (OA*>PA*).5.2 FeaturesAll (unsegmented) texts have been automaticallysplit into sentences and been tokenized by theOpenNLP-tools.
The mate-pipeline then pro-cessed the tokenized input, yielding lemmati-zation, POS-tags, word-morphology and depen-dency parses (Bohnet, 2010).
The annotated gold-standard segmentation in the dataset was then au-tomatically mapped to the automatic sentence-splitting/tokenization, in order to be able to ex-tract exactly those linguistic features present in thegold-segments.
Using this linguistic output andseveral other resources, we extracted the follow-ing features:Lemma Unigrams: We add a set of binary fea-tures for every lemma found in the present seg-ment, in the preceding and the subsequent seg-ment in order to represent the segment?s contextin a small window.Lemma Bigrams: We extracted lemma bi-gramms of the present segment.POS Tags: We add a set of binary features forevery POS tag found in the present, preceding andsubsequent segment.Main verb morphology: We added binary fea-tures for tempus and mood of the segment?s mainverb, as subjunctive mood might indicate antici-pated objections and tempus might help to identifythe main claim.Dependency triples: The dependency parseswere used to extract features representing depen-dency triples (relation, head, dependent) for eachtoken of the present segment.
Two features setswere built, one with lemma representations, theother with POS tag representations of head and de-pendent.Sentiment: We calculate the sentiment value ofthe current segment by summing the values of alllemmata marked as positive or negative in Sen-tiWS (Remus et al., 2010).5Discourse markers: For every lemma in thesegment that is listed as potentially signalling adiscourse relation (cause, concession, contrast,asymmetriccontrast) in a lexicon of German dis-course markers (Stede, 2002) we add a binaryfeature representing the occurance of the marker,and one representing the occurance of the relation.Again, discourse marker / relations in the preced-ing and subsequent segment are registered in sep-arate features.First three lemmata: In order to capturesentence-initial expressions that might indicate ar-gumentative moves, but are not strictly defined asdiscourse markers, we add binary features repre-senting the occurance of the first three lemmata.Negation marker presence: We use a list of 76German negation markers derived in (Warzecha,2013) containing both closed class negation opera-tors (negation particles, quantifiers and adverbialsetc.)
and open class negation operators (nouns like?denial?
or verbs like ?refuse?)
to detect negationin the segment.Segment position: The (relative) position ofthe segment in the text might be helpful to identifytypical linearisation strategies of argumentation.In total a number of ca.
19.000 features hasbeen extracted.
The largest chunks are bigramsand lemma-based dependencies with ca.
6.000features each.
Each set of lemma unigrams (for5We are aware that this summation is a rather trivial andpotentially error-prone way of deriving an overall sentimentvalue from the individual values of the tokens, but postponethe use of more sophisticated methods to future work.92level role typegen type comb target role+typelabels P (454) T (115) T (115) / (115) n-4 (26) PT (115)O (125) S (286) SN (277) S (426) n-3 (52) PSN (265)A (178) SE (9) C (38) n-2 (58) PSE (9)AR (112) n-1 (137) PAR (12)AU (66) 0 (115) PAU (53)n+1 (53) OSN (12)n+2 (35) OSE (0)r-1 (54) OAR (100)r-2 (7) OAU (13).
.
.# of lbls 2 3 5 3 16 9Table 1: Label distribution on the basic levels and for illustration on the complex ?role+type?
level.Labels on remaining complex level combine accoringly: ?role+type+comb?
with in total 12 differentlabels and ?role+type+comb+target?
with 48 different labels found in the dataset.the present, preceding, and subsequent segment)has around 2.000 features.5.3 ClassifiersFor automatic recognition we compare classifiersthat have frequently been used in related work:Na?ve Bayes (NB) approaches as in (Teufel andMoens, 2002), Support Vector Machines (SVM)and Conditional Random Fields (CRF) as in (Li-akata et al., 2012) and maximum entropy (Max-Ent) approaches as in (Guo et al., 2013) or (Teufeland Kan, 2011).
We used the Weka data miningsoftware, v.3.7.10, (Hall et al., 2009) for all ap-proaches, except MaxEnt and CRF.Majority: This classifier assignes the most fre-quent class to each item.
We use it as a lowerbound of performance.
The used implementationis Weka?s ZeroR.One Rule: A simple but effective baseline isthe one rule classification approach.
It selects anduses the one feature whose values can describe theclass majority with the smallest error rate.
Theused implementation is Weka?s OneR with stan-dard parameters.Na?ve Bayes: We chose to apply a feature se-lected Na?ve Bayes classifier to better cope withthe large and partially redundant feature set.6 Be-fore training, all features are ranked accoring totheir information gain observed on the training set.Features with information gain ?
0 are excluded.SVM: For SVMs, we used Weka?s wrapper toLibLinear (Fan et al., 2008) with the Crammer andSinger SVM type and standard wrapper parame-ters.6With feature selection, we experienced better scores withthe Na?ve Bayes classifier, the only exception being the mostcomplex level ?role+type+comb+target?, where only very fewfeatures reached the information gain threshold.MaxEnt: The maximum entropy classifiers aretrained and tested with the MaxEnt toolkit (Zhang,2004).
We used at maximum 50 iterations of L-BFGS parameter estimation without a Gaussianprior.CRF: For the implementation of CRFs wechose Mallet (McCallum, 2002).
We used theSimpleTagger interface with standard parameters.Nonbinary features have been binarized for theMaxEnt and CRF classifiers.6 ResultsAll results presented in this section have beenproduced in 10 repetitions (with different randomseeds) of 10-fold cross validation, i.e.
for eachscore we have 100 fold-specific values of whichwe can calculate the average and the standard devi-ation.
We report A(ccuracy), micro-averaged F(1-score) as a class-frequency weighted measure andCohen?s ?
(Cohen, 1960) as a measure focussingon less frequent classes.
All scores are given inpercentages.6.1 Comparing classifiersA comparison of the different classifiers is shownin Table 2.
Due to the skewed label distribution,the majority classifier places the lower boundsalready at a quite high level for the ?role?
and?comb?-level.
Also note that the agreement be-tween predicted and gold for the majority classi-fier is equivalent to chance agreement and thus ?is 0 on every level, even though there are F-scoresnear the .70.Bold values in Table 2 indicate highest aver-age.
However note, that differences of one or twopercent points between the non-baseline classifiersare not significant, due to the variance over the93level Majority OneR CRFA F ?
A F ?
A F ?role 78?1 69?1 0?0 83?3 79?4 33?13 86?5 84?6 49?16typegen 49?1 33?1 0?0 58?3 47?3 23?7 68?7 67?8 46?12type 48?1 31?1 0?0 56?3 45?3 22?6 62?7 58?8 38?10comb 74?1 62?1 0?0 81?4 77?4 44?12 84?5 81?7 55?13target 24?1 9?1 0?0 37?5 29?4 24?6 47?11 45?11 38?12role+typegen 47?1 30?1 0?0 56?3 45?3 22?6 67?7 65?8 49?11role+type 46?1 29?1 0?0 54?3 43?3 21?6 61?7 56?8 38?11role+type+comb 41?1 24?1 0?0 50?4 38?3 19?6 56?7 51?8 36?9role+type+comb+target 20?1 7?1 0?0 28?4 19?3 18?5 36?10 30?9 28?10level Na?ve Bayes MaxEnt LibLinearA F ?
A F ?
A F ?role 84?5 84?5 52?14 86?4 85?5 52?15 86?4 84?4 50?14typegen 74?5 74?5 57?8 70?6 70?6 51?10 71?5 71?5 53?9type 68?5 67?5 52?8 63?6 62?6 43?9 65?6 62?6 44?9comb 74?6 75?5 42?11 84?5 81?7 56?12 84?3 81?4 54?10target 38?6 38?6 29?6 47?8 44?8 37?9 48?5 44?5 38?6role+typegen 69?6 69?6 55?9 68?7 67?7 51?10 69?5 67?6 52?9role+type 61?5 61?5 45?7 63?6 61?6 45?9 64?5 60?5 45?8role+type+comb 53?6 51?6 36?8 58?6 54?7 41?8 61?5 56?5 44?8role+type+comb+target 22?4 19?4 16?4 36?6 33?6 29?6 39?5 32?4 31?5Table 2: Classifier performance comparison: Percent average and standard deviation in 10 repetitions of10-fold cross-validation of A(ccuracy), micro averages of F1-scores, and Cohen?s ?.folds on this rather small dataset.The Na?ve Bayes classifier profits from the fea-ture selection on levels with a small number oflabels and gives best results for the ?type(gen)?and ?role+typegen?
levels.
On the most complexlevel with 48 possible labels, however, perfor-mance drops even below the OneR baseline, be-cause features do not reach the information gainthreshold.
The MaxEnt classifier performs well onthe ?role?
and ?comb?, as well as on the ?role+type?levels.
It reaches the highest F-score on the mostcomplex level, although the highest accuracy andagreement on this levels is achieved by the SVM,indicating that the SVM accounted better for theless frequent labels.
The SVM generally per-forms well in terms of accuracy and specifically onthe most interesting levels for future applications,namely in target identification and the complex?role+type?
and ?role+type+comb+target?
levels.For the CRF classifier, we had hoped that ap-proaching the dataset as a sequence labelling prob-lem would be of advantage.
However, applied outof the box as done here, it did not perform as wellas the segment-based MaxEnt or SVM classifier.6.2 Feature ablation on ?role+type?
levelWe performed feature ablation tests with multi-ple classifiers on multiple levels.
For the sake ofbrevity, we only present the results of the SVMand MaxEnt classifiers here on the ?role+type?level.
The results are shown in Table 3.
Bold val-ues indicate greatest impact, i.e.
strongest loss inthe upper leave-one-feature-out half of the tableand highest gain in the lower only-one-feature halfof the table.The greatest loss is produced by leaving out thediscourse marker features.
We assume that thisimpact can be attributed to the useful abstractionof introducing the signalled discourse relation as afeatures, since the markers are also present in otherfeatures (as lemma unigrams, perhaps first threelemma or even lemma dependencies) that produceminor losses.For the single feature runs, lemma unigramsproduce the best results, followed by discoursemarkers and other lemma features as bigrams,first three lemma and lemma dependencies.
Notethat negation markers, segment position and senti-ment perform below or equal to the majority base-line.
Whether at least the sentiment feature canprove more useful when we apply a more sophisti-cated calculation of a segment?s sentiment value issomething we want to investigate in future work.POS-tag based features are around the OneR base-line in terms of F-score and ?, but less accurate.Interestingly, when using the LibLinear SVM,lemma bigrams have a larger impact on the overallperformance than lemma based dependency triplesin both tests, even for a language with a relativelyfree word order as German.
This indicates thatthe costly parsing of the sentences might not berequired after all.
However, this difference is not94Features LibLinear MaxEntA F ?
A F ?all 64?5 60?5 45?8 63?6 61?6 45?9all w/o dependencies lemma 64?5 60?5 46?8 62?6 60?6 44?9all w/o dependencies pos 65?5 61?5 46?8 63?6 61?7 45?9all w/o discourse markers 62?5 59?5 43?8 61?7 58?7 42?9all w/o first three lemma 64?5 60?5 44?8 63?6 60?7 44?9all w/o lemma unigrams 63?5 60?5 45?8 62?6 60?7 44?9all w/o lemma bigrams 63?5 60?5 44?8 62?6 60?6 44?9all w/o main verb morph 64?5 60?5 45?8 62?6 60?6 43?9all w/o negation marker 64?5 60?6 45?8 63?6 61?7 45?9all w/o pos 64?5 61?5 45?8 63?6 60?7 44?8all w/o segment position 64?5 60?5 45?8 63?6 61?6 45?9all w/o sentiment 64?5 60?5 45?8 62?6 60?6 44?9only dependencies lemma 56?4 47?4 27?6 56?6 49?7 30?8only dependencies pos 42?6 41?6 18?8 41?7 40?7 16?9only discourse markers 56?6 53?6 34?9 53?6 52?7 30?10only first three lemma 54?6 52?6 33?9 50?6 48?6 26?8only lemma unigrams 59?5 55?5 37?8 59?6 56?7 38?8only lemma bigrams 59?4 53?5 34?8 55?7 51?7 30?9only main verb morph 49?6 39?4 16?7 52?5 41?6 20?6only negation marker 25?14 19?8 0?4 46?5 29?5 0?0only pos 45?6 45?6 24?9 46?8 45?7 23?10only segment position 31?12 25?10 4?7 46?5 29?6 0?0only sentiment 22?14 15?11 -1?3 46?5 29?6 0?0Table 3: Feature ablation tests on the ?role+type?
level: Percent average and standard deviation in 10repetitions of 10-fold cross-validation of A(ccuracy), micro averages of F1-scores, and Cohen?s ?.as clear for the MaxEnt classifier.6.3 Class specific resultsFinally, we present class-specific results of theMaxEnt classifier for the ?role+type?
level in Ta-ble 4.
Frequent categories give good results, butfor low-frequency classes there are just not enoughinstances in the dataset.
We hope improve this byextending the corpus by corresponding examples.label Precision Recall F1-scorePT 75?12 74?13 74?11PSN 65?8 79?7 71?6PSE 1?6 1?6 1?6PAR 12?29 12?27 11?24PAU 57?26 49?24 50?22OSN 1?12 1?12 1?12OAR 54?18 42?16 46?13OAU 8?27 7?23 7?23Table 4: MaxEnt class-wise results on the?role+type?
level: Percent average and stan-dard deviation in 10 repetitions of 10-fold cross-validation of Precision, Recall and F1-score.7 Summary and outlookWe have presented a small corpus of Germanmicrotexts that features authentic argumentations,yet has been collected in a controlled fashion toreduce the amount of distracting or complicatedrhetorical phenomena, focussing instead on the ar-gumentative moves.
The corpus has been anno-tated with a scheme that ?as we have shown?
canbe reliably used by trained and experienced anno-tators.
To get a first impression of the performanceof frequently used modelling approaches on ourdataset, we experimented with different classifierswith rather out-of-the-box parameter settings onvarious levels of granularity of the scheme.
Giventhe complex nature of such a discourse under-standing tasks, the first results presented here arepromising, but invite for further investigation.We aim to generate a significantly larger corpusof argumentative microtexts by a crowd-sourcedexperiment.
For the improvement of models, weconsider various strategies: Integrating top downconstraints on the argumentation structure, as donein (Guo et al., 2013) for the zoning of scientificdocuments, is one option.
Hierarchical modelsthat apply classifiers along the levels of our la-bel hierarchy are another option.
Furthermore, wewant to explore sequence labelling models in moredetail.
Ultimately, the goal will be to apply thesemethods to authentic news-paper commentaries.AcknowledgmentsThanks to Manfred Stede and to the anonymousreviewers for their helpful comments.
The authorwas supported by a grant from Cusanuswerk.95ReferencesMoshe Azar.
1999.
Argumentative text as rhetoricalstructure: An application of rhetorical structure the-ory.
Argumentation, 13:97?114.Monroe C. Beardsley.
1950.
Practical Logic.Prentice-Hall, New York.Bernd Bohnet.
2010.
Very high accuracy and fast de-pendency parsing is not a contradiction.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, COLING ?10, pages 89?97,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Elena Cabrio and Serena Villata.
2012.
Natural lan-guage arguments: A combined approach.
In Luc DeRaedt, Christian Bessiere, Didier Dubois, PatrickDoherty, Paolo Frasconi, Fredrik Heintz, and PeterJ.
F. Lucas, editors, ECAI, volume 242 of Frontiersin Artificial Intelligence and Applications, pages205?210.
IOS Press.Jacob Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Educational and PsychologicalMeasurement, 20(1):37?46.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: A li-brary for large linear classification.
J. Mach.
Learn.Res., 9:1871?1874, June.Vanessa Wei Feng and Graeme Hirst.
2011.
Classi-fying arguments by scheme.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies - Volume 1, HLT ?11, pages 987?996, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Vanessa Wei Feng and Graeme Hirst.
2012.
Text-leveldiscourse parsing with rich linguistic features.
InProceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics: Long Papers- Volume 1, ACL ?12, pages 60?68, Stroudsburg, PA,USA.
Association for Computational Linguistics.Joseph L. Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 76(5):378?382.James B. Freeman.
1991.
Dialectics and theMacrostructure of Argument.
Foris, Berlin.James B. Freeman.
2011.
Argument Structure: Repre-sentation and Theory.
Argumentation Library (18).Springer.Nancy L. Green.
2010.
Representation of argumenta-tion in text with rhetorical structure theory.
Argu-mentation, 24:181?196.Yufan Guo, Roi Reichart, and Anna Korhonen.
2013.Improved information structure analysis of scien-tific documents through discourse and lexical con-straints.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 928?937, Atlanta, Georgia, June.Association for Computational Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.SIGKDD Explor.
Newsl., 11(1):10?18, November.Hugo Hernault, Hemut Prendinger, David duVerle, andMitsuru Ishizuka.
2010.
HILDA: A discourseparser using support vector machine classification.Dialogue and Discourse, 1(3):1?33.Maria Liakata, Shyamasree Saha, Simon Dob-nik, Colin R. Batchelor, and Dietrich Rebholz-Schuhmann.
2012.
Automatic recognition of con-ceptualization zones in scientific articles and two lifescience applications.
Bioinformatics, 28(7):991?1000.William Mann and Sandra Thompson.
1988.
Rhetori-cal structure theory: Towards a functional theory oftext organization.
TEXT, 8:243?281.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Raquel Mochales Palau and Marie-Francine Moens.2011.
Argumentation mining.
Artificial Intelligenceand Law, 19(1):15?22.Andreas Peldszus and Manfred Stede.
2013a.
Fromargument diagrams to automatic argument mining:A survey.
International Journal of Cognitive Infor-matics and Natural Intelligence (IJCINI), 7(1):1?31.Andreas Peldszus and Manfred Stede.
2013b.
Rankingthe annotators: An agreement study on argumenta-tion structure.
In Proceedings of the 7th LinguisticAnnotation Workshop and Interoperability with Dis-course, pages 196?204, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.Chris Reed, Raquel Mochales Palau, Glenn Rowe, andMarie-Francine Moens.
2008.
Language resourcesfor studying argument.
In Nicoletta Calzolari (Con-ference Chair), Khalid Choukri, Bente Maegaard,Joseph Mariani, Jan Odijk, Stelios Piperidis, andDaniel Tapias, editors, Proceedings of the SixthInternational Conference on Language Resourcesand Evaluation (LREC?08), Marrakech, Morocco,may.
European Language Resources Association(ELRA).Robert Remus, Uwe Quasthoff, and Gerhard Heyer.2010.
SentiWS - A Publicly Available German-language Resource for Sentiment Analysis.
In Nico-letta Calzolari (Conference Chair), Khalid Choukri,Bente Maegaard, Joseph Mariani, Jan Odijk, SteliosPiperidis, Mike Rosner, and Daniel Tapias, editors,Proceedings of the 7th International Conference onLanguage Resources and Evaluation (LREC?10),pages 1168?1171, Valletta, Malta, May.
EuropeanLanguage Resources Association (ELRA).96Patrick Saint-Dizier.
2012.
Processing natural lan-guage arguments with the TextCoop platform.
Jour-nal of Argumentation and Computation, 3(1):49?82.Manfred Stede and Antje Sauermann.
2008.
Lin-earization of arguments in commentary text.
In Pro-ceedings of the Workshop on Multidisciplinary Ap-proaches to Discourse.
Oslo.Manfred Stede.
2002.
DiMLex: A Lexical Ap-proach to Discourse Markers.
In Vittorio Di TomasoAlessandro Lenci, editor, Exploring the Lexicon- Theory and Computation.
Edizioni dell?Orso,Alessandria, Italy.Manfred Stede.
2004.
The Potsdam Commentary Cor-pus.
In Proceedings of the ACL Workshop on Dis-course Annotation, pages 96?102, Barcelona.Simone Teufel and Min-Yen Kan. 2011.
Robust ar-gumentative zoning for sensemaking in scholarlydocuments.
In Raffaella Bernadi, Sally Cham-bers, Bj?rn Gottfried, Fr?d?rique Segond, and IlyaZaihrayeu, editors, Advanced Language Technolo-gies for Digital Libraries, volume 6699 of Lec-ture Notes in Computer Science, pages 154?170.Springer Berlin Heidelberg.Simone Teufel and Marc Moens.
2002.
Summarizingscientific articles: Experiments with relevance andrhetorical status.
Comput.
Linguist., 28(4):409?445,December.Simone Teufel, Advaith Siddharthan, and Colin Batch-elor.
2009.
Towards discipline-independent ar-gumentative zoning: evidence from chemistry andcomputational linguistics.
In Proceedings of the2009 Conference on Empirical Methods in Natu-ral Language Processing: Volume 3, EMNLP ?09,pages 1493?1502, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Simone Teufel.
2010.
The Structure of Scientific Arti-cles: Applications to Citation Indexing and Summa-rization.
CSLI Studies in Computational Linguis-tics.
CSLI Publications.Stephen N. Thomas.
1974.
Practical Reasoning inNatural Language.
Prentice-Hall, New York.Stephen Toulmin.
1958.
The Uses of Argument.
Cam-bridge University Press, Cambridge.Douglas Walton, Chris Reed, and Fabrizio Macagno.2008.
Argumentation Schemes.
Cambridge Univer-sity Press.Saskia Warzecha.
2013.
Klassifizierung und Skopus-bestimmung deutscher Negationsoperatoren.
Bach-elor thesis, Potsdam University.Le Zhang, 2004.
Maximum Entropy Modeling Toolkitfor Python and C++, December.97
