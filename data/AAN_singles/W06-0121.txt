Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 138?141,Sydney, July 2006. c?2006 Association for Computational LinguisticsChinese Word Segmentation with Maximum Entropyand N-gram Language ModelWang Xinhao, Lin Xiaojun, Yu Dianhai, Tian Hao, Wu XihongNational Laboratory on Machine Perception,School of Electronics Engineering and Computer Science,Peking University, China, 100871{wangxh,linxj,yudh,tianhao,wxh}@cis.pku.edu.cnAbstractThis paper presents the Chinese word seg-mentation systems developed by Speechand Hearing Research Group of Na-tional Laboratory on Machine Perception(NLMP) at Peking University, which wereevaluated in the third International Chi-nese Word Segmentation Bakeoff held bySIGHAN.
The Chinese character-basedmaximum entropy model, which switchesthe word segmentation task to a classi-fication task, is adopted in system de-veloping.
To integrate more linguisticsinformation, an n-gram language modelas well as several post processing strate-gies are also employed.
Both the closedand open tracks regarding to all four cor-pora MSRA, UPUC, CITYU, CKIP areinvolved in our systems?
evaluation, andgood performance are achieved.
Espe-cially, in the closed track on MSRA, oursystem ranks 1st.1 IntroductionChinese word segmentation is one of the core tech-niques in Chinese language processing and attractslots of research interests in recent years.
Sev-eral promising methods are proposed by previousresearchers, in which Maximum Entropy (ME)model has turned out to be a successful way forthis task (Hwee Tou Ng et al, 2004; Jin KiatLow et al, 2005).
By employing Maximum En-tropy (ME) model, the Chinese word segmentationtask is regarded as a classification problem, whereeach character will be classified to one of the fourclasses, i.e., the beginning, middle, end of a multi-character word and a single-character word.However, in a high degree, ME model pays itsemphasis on Chinese characters while debases theconsideration on the relationship of the contextwords.
Motivated by this view, several strategiesused for reflecting the context words?
relationshipand integrating more linguistics information, areemployed in our systems.As known, an n-gram language model could ex-press the relationship of the context words well, ittherefore as a desirable choice is imported in oursystem to modify the scoring of the ME model.An analysis on our preliminary experiments showsthe combination ambiguity is another issue thatshould be specially tackled, and a division andcombination strategy is then adopted in our sys-tem.
To handle the numeral words, we also intro-duce a number conjunction strategy.
In addition,to deal with the long organization names problemin MSRA corpus, a post processing strategy fororganization name is presented.The remainder of this paper is organized as fol-lows.
Section 2 describes our system in detail.Section 3 presents the experiments and results.And in last section, we draw our conclusions.2 System DescriptionWith the ME model, n-gram language model, andseveral post processing strategies, our systems areestablished.
And detailed description on thesecomponents are given in following subsections.2.1 Maximum Entropy ModelThe ME model used in our system is based on theprevious works (Jin Kiat Low et al, 2005; HweeTou Ng et al, 2004).
As mentioned above, theME model based word segmentation is a 4-classeslearning process.
Here, we remarked four classes,i.e.
the beginning, middle, end of a multi-character138word and a single-character word, as b, m, e and srespectively.In ME model, the following features (Jin KiatLow et al, 2005) are selected:a) cn (n = ?2,?1, 0, 1, 2)b) cncn+1 (n = ?2,?1, 0, 1)c) c?1c+1where cn indicates the character in the left or rightposition n relative to the current character c0.For the open track especially, three extendedfeatures are extracted with the help of an externaldictionary as follows:d) Pu (c0)e) L and t0f) cnt0 (n = ?1, 0, 1)where Pu(c0) denotes whether the current charac-ter is a punctuation, L is the length of word W thatconjoined from the character and its context whichmatching a word in the external dictionary as longas possible.
t0 is the boundary tag of the characterin W.With the features, a ME model is trained whichcould output four scores for each character withregard to four classes.
Based on scores of all char-acters, a completely segmented semiangle matrixcan be constructed.
Each element wji in this ma-trix represents a word that starts at the ith charac-ter and ends at jth character, and its value ME(j, i),the score for these (j ?
i+1) characters to form aword, is calculated as follow:ME[j, i] = ?
log p(w = ci...cj)= ?
log[p(bci)p(mci+1)...p(mcj?1)p(ecj )](1)As a consequence, the optimal segmentation re-sults corresponding to the best path with the low-est overall score could be reached via a dynamicprogramming algorithm.
For example:@?c????
(I was 19 years old that year)Table 1 shows its corresponding matrix.
In thisexample, the ultimate segmented result is:@ ?c ?
??
?2.2 Language ModelN-gram language model, a widely used methodin natural language processing, can represent thecontext relation of words.
In our systems, a bi-gram model is integrated with ME model in thephase of calculating the path score.
In detail, thescore of a path will be modified by adding the bi-gram of words with a weight ?
at the word bound-aries.
The approach used for modifying path scoreis based on the following formula.V [j, i] = ME[j, i]+mini?1k=1{[(V [i ?
1, k]+?Bigram(wk,i?1, wi,j)}(2)where V[j,i] is the score of local best path whichends at the jth character and the last word on thepath is wi,j = ci...cj , the parameter ?
is optimizedby the test set used in the 2nd International Chi-nese Word Segmentation Bakeoff.
When scoringthe path, if one of the words wk,i?1 and wi,j is outof the vocabulary, their bigram will backoff to theunigram.
And the unigram of the OOV word willbe calculated as:Unigram(OOV Word) = pl (3)where p is the minimal unigram value of words invocabulary; l is the length of the word acting asa punishment factor to avoid overemphasizing thelong OOV words.2.3 Post Processing StrategiesThe analysis on preliminary experiments, wherethe ME model and n-gram language model are in-volved, lead to several post processing strategiesin developing our final systems.2.3.1 Division and Combination StrategyTo handle the combination ambiguity issue,we introduce a division and combination strategywhich take in use of unigram and bigram.
Foreach two words A and B, if their bigrams doesnot exist while there exists the unigram of wordAB, then they can be conjoined as one word.
Forexample, ??ff(August)?
and ???
(revolution)?are two segmented words, and in training set thebigram of ??ff?
and ????
is absent, whilethe word ??ff??
(the August Revolution)?
ap-peares, then the character string ??ff???
isconjoined as one word.
On the other hand, for aword C which can be divided as AB, if its uni-gram does not exit in training set, while the bigramof its subwords A and B exits, then it will be re-segmented.
For example, Taking the word ??LN?U?
(economic system reform)?
for instance,if its corresponding unigram is absent in trainingset, while the bigram of two subwords ?
?LN139@ ?
c ?
?
?
?1 2 3 4 5 6 7@ 1 6.3180e-07?
2 33.159 7.5801c 3 26.401 0.0056708 5.2704?
4 71.617 45.221 49.934 3.1001e-07?
5 83.129 56.734 61.446 33.869 7.0559?
6 90.021 63.625 68.337 40.760 12.525 12.534?
7 77.497 51.101 55.813 28.236 0.0012012 10.077 10.055Table 1: A completely segmented matrix?
(economic system)?
and ?U?(reform)?
exists,as a consequence, it will be segmented into twowords ??LN??
and ?U?
?.2.3.2 Numeral Word Processing StrategyThe ME model always segment a numeralword into several words.
For instance, the word?4.34(RMB Yuan 4.34)?, may be segmentedinto two words ?4.?
and ?34?.
To tackle thisproblem, a numeral word processing strategy isused.
Under this strategy, those words that containArabic numerals are manually marked in the train-ing set firstly, then a list of high frequency charac-ters which always appear alone between the num-bers in the training set can be extracted, based onwhich numeral word issue can be tackled as fol-lows.
When segmenting one sentence, if two con-joint words are numeral words, and the last char-acter of the former word is in the list, then they arecombined as one word.2.3.3 Long Organization Name ProcessingStrategySince an organization name is usually an OOV,it always will be segmented as several words, es-pecially for a long one, while in MSRA corpus, itis required to be recognized as one word.
In oursystems, a corresponding strategy is presented todeal with this problem.
Firstly a list of organiza-tion names is manually selected from the trainingset and stored in the prefix-tree based on charac-ters.
Then a list of prefixes is extracted by scan-ning the prefix-tree, that is, for each node, if thefrequencies of its child nodes are all lower than thepredefined threshold k and half of the frequency ofthe current node, the string of the current node willbe extracted as a prefix; otherwise, if there existsa child node whose frequency is higher than thethreshold k, scan the corresponding subtree.
In thesame way, the suffixes can also be extracted.
Theonly difference is that the order of characters is in-verse in the lexical tree.During recognizing phase, to a successivewords string that may include 2-5 words, will becombined as one word, if all of the following con-ditions are satisfied.a) Does not include numbers, full stop or comma.b) Includes some OOV words.c) Has a tail substring matching some suffix.d) Appears more than twice in the test data.e) Has a higher frequency than any of its substring whichis an OOV word or combined by multiple words.f) Satisfy the condition that for any two successive wordsw1 w2 in the strings, freq(w1w2)/freq(w1)?0.1, unless w1contains some prefix in its right.3 Experiments and ResultsWe have participated in both the closed and opentracks of all the four corpora.
For MSRA corpusand other three corpora, we build System I andSystem II respectively.
Both systems are based onthe ME model and the Maximum Entropy Toolkit1, provided by Zhang Le, is adopted.Four systems are derived from System I with re-gard to whether or not the n-gram language modeland three post processing strategies are used on theclosed track of MSRA corpus.
Table 2 shows theresults of four derived systems.System R P F ROOV RIVIA 95.0 95.7 95.3 66.0 96.0IB 96.0 95.6 95.8 60.3 97.3IC 96.4 96.0 96.2 60.3 97.7ID 96.4 96.1 96.3 61.2 97.6Table 2: The effect of MEmodel, n-gram languagemodel and three post processing strategies on theclosed track of MSRA corpus.System IA only adopts the ME model.
SystemIB integrates the ME model and the bigram lan-guage model.
System IC integrates the divisionand combination strategy and the numeral words1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html140processing strategy.
System ID adds the long or-ganization name processing strategy.For the open track of MSRA, an external dictio-nary is utilized to extract the e and f features.
Theexternal dictionary is built from six sources, in-cluding the Chinese Concept Dictionary from In-stitute of Computational Linguistics, Peking Uni-versity(72,716 words), the LDC dictionary(43,120words), the Noun Cyclopedia(111,633), the wordsegmentation dictionary from Institute of Com-puting Technology, Chinese Academy of Sci-ences(84,763 words), the dictionary from Insti-tute of Acoustics, and the dictionary from Insti-tute of Computational Linguistics, Peking Univer-sity(68,200 words) and a dictionary collected byourselves(63,470 words).The union of the six dictionaries forms a bigdictionary, and those words appearing in five orsix dictionaries are extracted to form a core dic-tionary.
If a word belongs to one of the followingdictionaries or word sets, it is added into the exter-nal dictionary.a) The core dictionary.b) The intersection of the big dictionary and the trainingdata.c) The words appearing in the training data twice or moretimes.Those words in the external dictionaries will beeliminated, if in most cases they are divided inthe training data.
Table 3 shows the effect of MEmodel, n-gram language model, three post pro-cessing strategies on the open track of MSRA.Here System IO only adopts the basic features,while the external dictionary based features areused in four derived systems related to open track:IA, IB, IC, ID.System R P F ROOV RIVIO 96.0 96.5 96.3 71.1 96.9IA 97.5 96.9 97.2 65.9 98.6IB 97.6 96.8 97.2 64.8 98.7IC 97.7 97.0 97.4 66.8 98.8ID 97.7 97.1 97.4 67.5 98.8Table 3: The effect of MEmodel, n-gram languagemodel, three post processing strategies on the opentrack of MSRA.System II only adopts ME model, the divisionand combination strategy and the numeral wordprocessing strategy.
In the open track of the cor-pora CKIP and CITYU, the training set and test setfrom the 2nd Chinese Word Segmentation Backoffare used for training.
For the corpora UPUC andCITYU, the external dictionaries are used, whichis constructed in the same way as that in the opentrack of MSRA Corpus.
Table 4 shows the officialresults of system II on UPUC, CKIP and CITYU.Corpus R P F ROOV RIVUPUC-C 93.6 92.3 93.0 68.3 96.1UPUC-O 94.0 90.7 92.3 56.1 97.6CKIP-C 95.8 94.8 95.3 64.6 97.2CKIP-O 95.8 94.8 95.3 64.7 97.2CITYU-C 96.9 97.0 97.0 77.3 97.8CITYU-O 97.9 97.6 97.7 81.3 98.5Table 4: Official results of our systems on UPUCCKIP and CITYUOn the UPUC corpus, an interesting observationis that the performance of the open track is worsethan the closed track.
The investigation and analy-sis lead to a possible explanation.
That is, the seg-mentation standard of the dictionaries, which areused to construct the external dictionary, is differ-ent from that of the UPUC corpus.4 ConclusionIn this paper, a detailed description on several Chi-nese word segmentation systems are presented,where ME model, n-gram language model as wellas three post processing strategies are involved.
Inthe closed track of MSRA, the integration of bi-gram language model greatly improves the recallratio of the words in vocabulary, although it willimpairs the performance of system in recognizingthe words out of vocabulary.
In addition, threestrategies are introduced to deal with combinationambiguity, numeral word, long organization nameissues.
And the evaluation results reveal the valid-ity and effectivity of our approaches.ReferencesJin Kiat Low, Hwee Tou Ng and Wenyuan Guo.A maximum Entropy Approach to Chinese WordSegmentation.
2005.
Preceedings of the FourthSIGHAN Workshop on Chinese Language Process-ing, pp.
161-164.Hwee Tou Ng and Jin Kiat Low.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based?
2004.
Preceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing(EMNLP), pp.
277-284.Zhang Huaping and Liu Qun.
Model of ChineseWords Rough Segmentation Based on N-Shortest-Paths Method.
2002.
Journal of Chinese Informa-tion Processing, 28(1):pp.
1-7.141
