Maximum Entropy Tagging with Binary and Real-Valued FeaturesVanessa Sandrini Marcello Federico Mauro CettoloITC-irst - Centro per la Ricerca Scientifica e Tecnologica38050 Povo (Trento) - ITALY{surname}@itc.itAbstractRecent literature on text-tagging reportedsuccessful results by applying MaximumEntropy (ME) models.
In general, MEtaggers rely on carefully selected binaryfeatures, which try to capture discrimi-nant information from the training data.This paper introduces a standard settingof binary features, inspired by the litera-ture on named-entity recognition and textchunking, and derives corresponding real-valued features based on smoothed log-probabilities.
The resulting ME modelshave orders of magnitude fewer parame-ters.
Effective use of training data to esti-mate features and parameters is achievedby integrating a leaving-one-out methodinto the standard ME training algorithm.Experimental results on two tagging tasksshow statistically significant performancegains after augmenting standard binary-feature models with real-valued features.1 IntroductionThe Maximum Entropy (ME) statistical frame-work (Darroch and Ratcliff, 1972; Berger et al,1996) has been successfully deployed in severalNLP tasks.
In recent evaluation campaigns, e.g.DARPA IE and CoNLL 2000-2003, ME modelsreached state-of-the-art performance on a range oftext-tagging tasks.With few exceptions, best ME taggers rely oncarefully designed sets of features.
Features cor-respond to binary functions, which model events,observed in the (annotated) training data and sup-posed to be meaningful or discriminative for thetask at hand.
Hence, ME models result in a log-linear combination of a large set of features, whoseweights can be estimated by the well known Gen-eralized Iterative Scaling (GIS) algorithm by Dar-roch and Ratcliff (1972).Despite ME theory and its related training algo-rithm (Darroch and Ratcliff, 1972) do not set re-strictions on the range of feature functions1 , pop-ular NLP text books (Manning and Schutze, 1999)and research papers (Berger et al, 1996) seemto limit them to binary features.
In fact, onlyrecently, log-probability features have been de-ployed in ME models for statistical machine trans-lation (Och and Ney, 2002).This paper focuses on ME models for two text-tagging tasks: Named Entity Recognition (NER)and Text Chuncking (TC).
By taking inspirationfrom the literature (Bender et al, 2003; Borth-wick, 1999; Koeling, 2000), a set of standard bi-nary features is introduced.
Hence, for each fea-ture type, a corresponding real-valued feature isdeveloped in terms of smoothed probability distri-butions estimated on the training data.
A directcomparison of ME models based on binary, real-valued, and mixed features is presented.
Besides,performance on the tagging tasks, complexity andtraining time by each model are reported.
ME es-timation with real-valued features is accomplishedby combining GIS with the leave-one-out method(Manning and Schutze, 1999).Experiments were conducted on two publiclyavailable benchmarks for which performance lev-els of many systems are published on theWeb.
Re-sults show that better ME models for NER and TCcan be developed by integrating binary and real-valued features.1Darroch and Ratcliff (1972) show how any set of real-valued feature functions can be properly handled.12 ME Models for Text TaggingGiven a sequence of words wT1 = w1, .
.
.
, wT anda set of tags C, the goal of text-tagging is to finda sequence of tags cT1 = c1, .
.
.
, cT which maxi-mizes the posterior probability, i.e.
:c?T1 = argmaxcT1p(cT1 | wT1 ).
(1)By assuming a discriminative model, Eq.
(1) canbe rewritten as follows:c?T1 = argmaxcT1T?t=1p(ct | ct?11 , wT1 ), (2)where p(ct|ct?11 , wT1 ) is the target conditionalprobability of tag ct given the context (ct?11 , wT1 ),i.e.
the entire sequence of words and the full se-quence of previous tags.
Typically, independenceassumptions are introduced in order to reduce thecontext size.
While this introduces some approxi-mations in the probability distribution, it consid-erably reduces data sparseness in the samplingspace.
For this reason, the context is limited hereto the two previous tags (ct?1t?2) and to four wordsaround the current word (wt+2t?2).
Moreover, limit-ing the context to the two previous tags permits toapply dynamic programming (Bender et al, 2003)to efficiently solve the maximization (2).Let y = ct denote the class to be guessed (y ?
Y)at time t and x = ct?1t?2, wt+2t?2 its context (x ?
X ).The generic ME model results:p?
(y | x) =exp(?ni=1 ?ifi(x, y))?y?
exp(?ni=1 ?ifi(x, y?)).
(3)The n feature functions fi(x, y) represent any kindof information about the event (x, y) which can beuseful for the classification task.
Typically, binaryfeatures are employed which model the verifica-tion of simple events within the target class andthe context.InMikheev (1998), binary features for text taggingare classified into two broad classes: atomic andcomplex.
Atomic features tell information aboutthe current tag and one single item (word or tag) ofthe context.
Complex features result as a combina-tion of two or more atomic features.
In this way, ifthe grouped events are not independent, complexfeatures should capture higher correlations or de-pendencies, possibly useful to discriminate.In the following, a standard set of binary fea-tures is presented, which is generally employedfor text-tagging tasks.
The reader familiar with thetopic can directly check this set in Table 1.3 Standard Binary FeaturesBinary features are indicator functions of specifiedevents of the sample space X ?
Y .
Hence, theytake value 1 if the event occurs or 0 otherwise.
Forthe sake of notation, the feature name denotes thetype of event, while the index specifies its param-eters.
For example:Orthperson,Cap,?1(x, y)corresponds to an Orthographic feature which isactive if and only if the class at time t is personand the word at time t?1 in the context starts withcapitalized letter.3.1 Atomic FeaturesLexical features These features model co-occurrences of classes and single words of the con-text.
Lexical features are defined on a windowof ?2 positions around the current word.
Lexicalfeatures are denoted by the name Lex and indexedwith the triple c, w, d which fixes the current class,i.e.
ct = c, the identity and offset of the word inthe context, i.e.
wt+d = w. Formally, the featureis computed by:Lex c,w,d(x, y) =?
?
(ct = c) ?
?
(wt+d = w).For example, the lexical feature for wordVerona, at position t with tag loc (location) is:Lexloc,Verona,0(x, y) = ?
(ct = loc) ???
(wt = Verona).Lexical features might introduce data sparsenessin the model, given that in real texts an impor-tant fraction of words occur only once.
In otherwords, many words in the test set will have nocorresponding features-parameter pairs estimatedon the training data.
To cope with this problem,all words observed only once in the training datawere mapped into the special symbol oov.Syntactic features They model co-occurrencesof the current class with part-of-speech or chunktags of a specific position in the context.
Syntacticfeatures are denoted by the name Syn and indexedwith a 4-tuple (c, Pos, p, d) or (c, Chnk, p, d),2Name Index DefinitionLex c, w, d ?
(ct = c) ?
?
(wt+d = w), d ?
ZSyn c, T, p, d ?
(ct = c) ?
?
(T(wt+d) = p) , T ?
{Pos, Chnk}, d ?
ZOrth c, F, d ?
(ct = c) ?
F(wt+d) , F ?
{IsCap, IsCAP}, d ?
ZDict c, L, d ?
(ct = c) ?
InList(L,wt+d), d ?
ZTran c, c?, d ?
(ct = c) ?
?
(ct?d = c?)
d ?
N+Lex+ c, s, k, ws+k?1s?s+k?1d=s Lexc,wd,d(x, y), k ?
N+, s ?
ZSyn+ c, T, s, k, ps+k?1s?s+k?1d=s Sync,T,pd,d(x, y), k ?
N+, s ?
ZOrth+ c, F, k, b+k?k ?
(ct = c) ?
?kd=?k ?
(Orthc,F,d(x, y) = bd) , bd ?
{0, 1}, k ?
N+Dict+ c, L, k, b+k?k ?
(ct = c) ?
?kd=?k ?
(Dictc,L,d(x, y) = bd) , bd ?
{0, 1}, k ?
N+Tran+ c, k, ck1?kd=1?
Tranc,cd,d(x, y) k ?
N+Table 1: Standard set of binary features for text tagging.which fixes the class ct, the considered syntacticinformation, and the tag and offset within the con-text.
Formally, these features are computed by:Sync,Pos,p,d(x, y)=??
(ct = c) ?
?
(Pos(wt+d) = p)Sync,Chnk,p,d(x, y)=??
(ct = c)??
(Chnk(wt+d) = p).Orthographic features These features modelco-occurrences of the current class with surfacecharacteristics of words of the context, e.g.
checkif a specific word in the context starts with cap-italized letter (IsCap) or is fully capitalized(IsCAP).
In this framework, only capitalizationinformation is considered.
Analogously to syntac-tic features, orthographic features are defined asfollows:Orthc,IsCap,d(x, y)=??
(ct = c) ?
IsCap(wt+d)Orthc,IsCAP,d(x, y)=??
(ct = c) ?
IsCAP(wt+d).Dictionary features These features check ifspecific positions in the context contain words oc-curring in some prepared list.
This type of featureresults relevant for tasks such as NER, in whichgazetteers of proper names can be used to improvecoverage of the training data.
Atomic dictionaryfeatures are defined as follows:Dictc,L,d(x, y)=??
(ct = c) ?
InList(L,wt+d)where L is a specific pre-compiled list, andInList is a function which returns 1 if the spec-ified word matches one of the multi-word entriesof list L, and 0 otherwise.Transition features Transition features modelMarkov dependencies between the current tag anda previous tag.
They are defined as follows:Tranc,c?,d(x, y)=??
(ct = c) ?
?
(ct?d = c?
).3.2 Complex FeaturesMore complex events are defined by combiningtwo or more atomic features in one of two ways.Product features take the intersection of the cor-responding atomic events.
V ector features con-sider all possible outcomes of the component fea-tures.For instance, the product of 3 atomic Lexicalfeatures, with class c, offsets ?2,?1, 0, and wordsv?2, v?1, v0, is:Lex+c,?2,3,v?2,v?1,v0(x, y)=?0?d=?2Lexc,vd,d(x, y).Vector features obtained from three Dictionaryfeatures with the same class c, list L, and offsets,respectively, -1,0,+1, are indexed over all possiblebinary outcomes b?1, b0, b1 of the single atomicfeatures, i.e.
:Dict+c,L,1,b?1,b0,b+1(x, y)=??
(ct = c)?1?d=?1?
(Dictc,L,d(x, y) = bd).Complex features used in the experiments are de-scribed in Table 1.The use of complex features significantly in-creases the model complexity.
Assuming thatthere are 10, 000 words occurring more than oncein the training corpus, the above lexical feature po-tentially adds O(|C|1012) parameters!As complex binary features might result pro-hibitive from a computational point of view, real-valued features should be considered as an alter-native.3Feature Index Probability DistributionLex d p(ct | wt+d)Syn T, d p(ct | T(wt+d))Orth F, d p(ct | F(wt+d))Dict List, d p(ct | IsIn(List, wt+d))Tran d p(ct | ct?d)Lex+ s, k p(ct | wt+s, .., wt+s+k?1Syn+ T, s, k p(ct | T(wt+s, .
.
.
, wt+s+k?1))Orth+ k, F p(ct | F(wt?k), .
.
.
, F(wt+k))Dict+ k,L p(ct | InList(L, wt?k), .
.
.
, InList(L, wt+k))Tran+ k p(ct | ct?k, .
.
.
, ct+k))Table 2: Corresponding standard set of real-values features.4 Real-valued FeaturesA binary feature can be seen as a probability mea-sure with support set made of a single event.
Ac-cording to this point of view, we might easily ex-tend binary features to probability measures de-fined over larger event spaces.
In fact, it resultsconvenient to introduce features which are log-arithms of conditional probabilities.
It can beshown that in this way linear constraints of theMEmodel can be interpreted in terms of Kullback-Leibler distances between the target model and theconditional distributions (Klakow, 1998).Let p1(y|x), p2(y|x), .
.
.
, pn(y|x) be n differentconditional probability distributions estimated onthe training corpus.
In our framework, each con-ditional probability pi is associated to a feature fiwhich is defined over a subspace [X ]i ?
Y of thesample space X ?
Y .
Hence, pi(y|x) should beread as a shorthand of p(y | [x]i).The corresponding real-valued feature is:fi(x, y) = log pi(y | x).
(4)In this way, the ME in Eq.
(3) can be rewritten as:p?
(y|x) =?ni pi(y|x)?i?y?
?i pi(y?
|x)?i .
(5)According to the formalism adopted in Eq.
(4),real-valued features assume the following form:fi(ct, ct?1t?2, wt+2t?2) = log pi(ct | ct?1t?2, wt+2t?2).
(6)For each so far presented type of binary feature,a corresponding real-valued type can be easily de-fined.
The complete list is shown in Table 2.
Ingeneral, the context subspace was defined on thebasis of the offset parameters of each binary fea-ture.
For instance, all lexical features selectingtwo words at distances -1 and 0 from the currentposition t are modeled by the conditional distri-bution p(ct | wt?1, wt).
While distributions oflexical, syntactic and transition features are con-ditioned on words or tags, dictionary and ortho-graphic features are conditioned on binary vari-ables.An additional real-valued feature that was em-ployed is the so called prior feature, i.e.
the prob-ability of a tag to occur:Prior(x, y) = log p(ct)A major effect of using real-valued features isthe drastic reduction of model parameters.
Forexample, each complex lexical features discussedbefore introduce just one parameter.
Hence, thesmall number of parameters eliminates the needof smoothing the ME estimates.Real-valued features present some drawbacks.Their level of granularity, or discrimination, mightresult much lower than their binary variants.
Formany features, it might result difficult to computereliable probability values due to data sparseness.For the last issue, smoothing techniques devel-oped for statistical language models can be applied(Manning and Schutze, 1999).5 Mixed Feature ModelsThis work, beyond investigating the use of real-valued features, addresses the behavior of modelscombining binary and real-valued features.
Thereason is twofold: on one hand, real-valued fea-tures allow to capture complex information withfewer parameters; on the other hand, binary fea-tures permit to keep a good level of granularityover salient characteristics.
Hence, finding a com-promise between binary and real-valued features4might help to develop ME models which bettertrade-off complexity vs. granularity of informa-tion.6 Parameter EstimationFrom the duality of ME and maximum likeli-hood (Berger et al, 1996), optimal parameters??
for model (3) can be found by maximizingthe log-likelihood function over a training sample{(xt, yt) : t = 1, .
.
.
,N}, i.e.:??
= argmax?N?t=1log p?(yt|xt).
(7)Now, whereas binary features take only two valuesand do not need any estimation phase, conditionalprobability features have to be estimated on somedata sample.
The question arises about how to ef-ficiently use the available training data in order toestimate the parameters and the feature distribu-tions of the model, by avoiding over-fitting.Two alternative techniques, borrowed from sta-tistical language modeling, have been consid-ered: the Held-out and the Leave-one-out methods(Manning and Schutze, 1999).Held-out method.
The training sample S is splitinto two parts used, respectively, to estimate thefeature distributions and the ME parameters.Leave-one-out.
ME parameters and feature dis-tributions are estimated over the same sample S.The idea is that for each addend in eq.
(7), the cor-responding sample point (xt, yt) is removed fromthe training data used to estimate the feature distri-butions of the model.
In this way, it can be shownthat occurrences of novel observations are simu-lated during the estimation of the ME parameters(Federico and Bertoldi, 2004).In our experiments, language modeling smooth-ing techniques (Manning and Schutze, 1999) wereapplied to estimate feature distributions pi(y|x).In particular, smoothing was based on the dis-counting method in Ney et al (1994) combined tointerpolation with distributions using less context.Given the small number of smoothing parametersinvolved, leave-one-out probabilities were approx-imated by just modifying count statistics on thefly (Federico and Bertoldi, 2004).
The rationale isthat smoothing parameters do not change signifi-cantly after removing just one sample point.For parameter estimation, the GIS algorithmby Darroch and Ratcliff (1972) was applied.
Itis known that the GIS algorithm requires featurefunctions fi(x, y) to be non-negative.
Hence, fea-tures were re-scaled as follows:fi(x, y) = log pi(y|x) + log1 + min pi, (8)where  is a small positive constant and the de-nominator is a constant term defined by:min pi = min(x,y)?Spi(y|x).
(9)The factor (1 + ) was introduced to ensure thatreal-valued features are always positive.
This con-dition is important to let features reflect the samebehavior of the conditional distributions, whichassign a positive probability to each event.It is easy to verify that this scaling operationdoes not affect the original model but only impactson the GIS calculations.
Finally, a slack featurewas introduced by the algorithm to satisfy the con-straint that all features sum up to a constant value(Darroch and Ratcliff, 1972).7 ExperimentsThis section presents results of MEmodels appliedto two text-tagging tasks, Named Entity Recogni-tion (NER) and Text Chunking (TC).After a short introduction to the experimen-tal framework, the detailed feature setting is pre-sented.
Then, experimental results are presentedfor the following contrastive conditions: binaryversus real-valued features, training via held-outversus leave-one-out, atomic versus complex fea-tures.7.1 Experimental Set-upNamed Entity Recognition English NER ex-periments were carried out on the CoNLL-2003shared task2.
This benchmark is based on textsfrom the Reuters Corpus which were manuallyannotated with parts-of-speech, chunk tags, andnamed entity categories.
Four types of categoriesare defined: person, organization, location andmiscellaneous, to include e.g.
nations, artifacts,etc.
A filler class is used for the remaining words.After including tags denoting the start of multi-word entities, a total of 9 tags results.
Data arepartitioned into training (200K words), develop-ment (50K words), and test (46K words) samples.2Data and results in http://cnts.uia.ac.be/conll2003/ner.5Text Chunking English TC experiments wereconducted on the CoNLL-2000 shared task3.Texts originate from the Wall Street Journal andare annotated with part-of-speech tags and chunks.The chunk set consists of 11 syntactic classes.
Theset of tags which also includes start-markers con-sists of 23 classes.
Data is split into training (210Kwords) and test (47K words) samples.Evaluation Tagging performance of both tasksis expressed in terms of F-score, namely the har-monic mean of precision and recall.
Differences inperformance have been statistically assessed withrespect to precision and recall, separately, by ap-plying a standard test on proportions, with signif-icance levels ?
= 0.05 and ?
= 0.1.
Henceforth,claimed differences in precision or recall will havetheir corresponding significance level shown inparenthesis.7.2 Settings and Baseline ModelsFeature selection and setting for ME models is anart.
In these experiments we tried to use the sameset of features with minor modifications acrossboth tasks.
In particular, used features and theirsettings are shown in Table 3.Training of models with GIS and estimationof feature distributions used in-house developedtoolkits.
Performance of binary feature modelswas improved by smoothing features with Gaus-sian priors (Chen and Rosenfeld, 1999) with meanzero and standard deviation ?
= 4.
In general,tuning of models was carried out on a developmentset.Most of the comparative experiments were per-formed on the NER task.
Three baseline modelsusing atomic features Lex, Syn, and Tran wereinvestigated first: model BaseBin, with all binaryfeatures; model BaseReal, with all real-valued fea-tures plus the prior feature; model BaseMix, withreal-valued Lex and binary Tran and Syn.
Mod-els BaseReal and BaseMix were trained with theheld-out method.
In particular, feature distribu-tions were estimated on the training data while MEparameters on the development set.7.3 Binary vs. Real-valued FeaturesThe first experiment compares performance of thebaseline models on the NER task.
Experimentalresults are summarized in Table 4.
Models Base-Bin, BaseReal, and BaseMix achieved F-scores of3Data and results in http://cnts.uia.ac.be/conll2000/chunking.Model ID Num P% R% F-scoreBaseBin 580K 78.82 75.62 77.22BaseReal 10 79.74 74.15 76.84BaseMix 753 78.90 75.85 77.34Table 4: Performance of baseline models on theNER task.
Number of parameters, precision, re-call, and F-score are reported for each model.Model Methods P% R% F-scoreBaseMix Held-Out 78.90 75.85 77.34BaseMix L-O-O 80.64 76.40 78.46Table 5: Performance of mixed feature modelswith two different training methods.77.22, 76.84, and 77.34.
Statistically meaning-ful differences were in terms of recall, betweenBaseBin and BaseReal (?
= 0.1), and betweenBaseMix and BaseReal (?
= 0.05).Despite models BaseMix and BaseBin performcomparably, the former has many fewer parame-ters, i.e.
753 against 580,000.
In fact, BaseMix re-quires storing and estimating feature distributions,which is however performed at a marginal compu-tational cost and off-line with respect to GIS train-ing.7.4 Training with Mixed FeaturesAn experiment was conducted with the BaseMixmodel to compare the held-out and leave-one-outtraining methods.
Results in terms of F-score arereported in Table 5.
By applying the leave-one-out method F-score grows from 77.34 to 78.46,with a meaningful improvement in recall (?
=0.05).
With respect to models BaseBin and Base-Real, leave-one-out estimation significantly im-proved precision (?
= 0.05).In terms of training time, ME models with real-valued features took significantly more GIS iter-ations to converge.
Figures of cost per iterationand number of iterations are reported in Table 6.
(Computation times are measured on a single CPUPentium-4 2.8GHz.)
Memory size of the trainingprocess is instead proportional to the number n ofparameters.7.5 Complex FeaturesA final set of experiments aims at comparing thebaseline MEmodels augmented with complex fea-tures, again either binary only (model FinBin),6Feature Index NE Task Chunking TaskLex c, w, d N(w) > 1,?2 ?
d ?
+2 ?2 ?
d ?
+2Syn c, T, p, d T ?
{Pos, Chnk}, d = 0 T = Pos,?2 ?
d ?
+2Tran c, c?, d d = ?2,?1 d = ?2,?1Lex+ c, s, k, ws+k?1s s = ?1, 0, k = 1 s = ?1, 0 k = 1Syn+ c, T, s, k, ps+k?1s not used s = ?1, 0 k = 1Orth+ c, k, F, b+k?k F = {Cap, CAP}, k = 2 F = Cap, k = 1Dict+ c, k, L, b+k?k k = 3L = {LOC, PER, ORG, MISC} not usedTran+ c, k, ck1 k = 2 k = 2Table 3: Setting used for binary and real-valued features in the reported experiments.Model Single Iteration Iterations TotalBaseBin 54 sec 750 ?
11 hBaseReal 9.6 sec 35,000 ?
93 hBaseMix 42 sec 4,000 ?
46 hTable 6: Computational cost of parameter estima-tion by different baseline models.real-valued only (FinReal), or mixed (FinMix).Results are provided both for NER and TC.This time, compared models use different fea-ture settings.
In fact, while previous experimentsaimed at comparing the same features, in eitherreal or binary form, these experiments explore al-ternatives to a full-fledged binary model.
In par-ticular, real-valued features are employed whosebinary versions would introduce a prohibitivelylarge number of parameters.
Parameter estima-tion of models including real-valued features al-ways applies the leave-one-out method.For the NER task, model FinBin adds Orth+and Dict+; FinReal adds Lex+, Orth+ andDict+; and, FinMix adds real-valued Lex+ andbinary-valued Orth+ and Dict+.In the TC task, feature configurations are as fol-lows: FinBin uses Lex, Syn, Tran, and Orth+;FinReal uses Lex, Syn, Tran, Prior, Orth+,Lex+, Syn+, Tran+; and, finally, FinMix usesbinary Syn, Tran, Orth+ and real-valued Lex,Lex+, Syn+.Performance of the models on the two tasks arereported in Table 7 and Table 8, respectively.In the NER task, all final models outperform thebaseline model.
Improvements in precision andrecall are all significant (?
= 0.05).
Model Fin-Mix improves precision with respect to model Fin-Bin (?
= 0.05) and requires two order of magni-tude fewer parameters.Model Num P% R% F-scoreFinBin 673K 81.92 80.36 81.13FinReal 19 83.58 74.03 78.07FinMix 3K 84.34 80.38 82.31Table 7: Results with complex features on theNER task.Model Num P% R% F-scoreFinBin 2M 91.04 91.48 91.26FinReal 19 88.73 90.58 89.65FinMix 6K 91.93 92.24 92.08Table 8: Results with complex features on the TCtask.In the TC task, the same trend is observed.Again, best performance is achieved by the modelcombining binary and real-valued features.
In par-ticular, all observable differences in terms of pre-cision and recall are significant (?
= 0.05).8 DiscussionIn summary, this paper addressed improvements toME models for text tagging applications.
In par-ticular, we showed how standard binary featuresfrom the literature can be mapped into correspond-ing log-probability distributions.
ME training withthe so-obtained real-valued features can be accom-plished by combining the GIS algorithm with theleave-one-out or held-out methods.With respect to the best performing systems atthe CoNLL shared tasks, our models exploit a rel-atively smaller set of features and perform signifi-cantly worse.
Nevertheless, performance achievedby our system are comparable with those reportedby other ME-based systems taking part in the eval-uations.Extensive experiments on named-entity recog-7nition and text chunking have provided support tothe following claims:?
The introduction of real-valued features dras-tically reduces the number of parameters ofthe ME model with a small loss in perfor-mance.?
The leave-one-out method is significantlymore effective than the held-out method fortraining ME models including real-valuedfeatures.?
The combination of binary and real-valuedfeatures can lead to better MEmodels.
In par-ticular, state-of-the-art ME models with bi-nary features are significantly improved byadding complex real-valued features whichmodel long-span lexical dependencies.Finally, the GIS training algorithm does notseem to be the optimal choice for ME models in-cluding real-valued features.
Future work will in-vestigate variants of and alternatives to the GISalgorithm.
Preliminary experiments on the Base-Real model showed that training with the Simplexalgorithm (Press et al, 1988) converges to simi-lar parameter settings 50 times faster than the GISalgorithm.9 AcknowledgmentsThis work was partially financed by the Euro-pean Commission under the project FAME (IST-2000-29323), and by the Autonomous Province ofTrento under the the FU-PAT project WebFaq.ReferencesO.
Bender, F. J. Och, and H. Ney.
2003.
Maximumentropy models for named entity recognition.
InWalter Daelemans and Miles Osborne, editors, Pro-ceedings of CoNLL-2003, pages 148?151.
Edmon-ton, Canada.A.
L. Berger, S. A. Della Pietra, and V. J. Della Pietra.1996.
A Maximum Entropy Approach to NaturalLanguage Processing.
Computational Linguistics,22(1):39?72.A.
Borthwick.
1999.
A Maximum Entropy approachto Named Entity Recognition.
Ph.D. thesis, Com-puter Science Department - New York University,New York, USA.S.
Chen and R. Rosenfeld.
1999.
A Gaussian priorfor smoothing maximum entropy models.
Techni-cal Report CMUCS-99-108, Carnegie Mellon Uni-versity.J.N.
Darroch and D. Ratcliff.
1972.
Generalized Itera-tive Scaling for Log-Liner models.
Annals of Math-ematical Statistics, 43:1470?1480.M.
Federico and N. Bertoldi.
2004.
Broadcast newslm adaptation over time.
Computer Speech and Lan-guage, 18(4):417?435, October.D.
Klakow.
1998.
Log-linear interpolation of languagemodels.
In Proceedings of the International Confer-ence of Spoken Language P rocessing (ICSLP), Sid-ney, Australia.R.
Koeling.
2000.
Chunking with maximum entropymodels.
In Proceedings of CoNLL-2000, pages139?141, Lisbon, Portugal.C.
D. Manning and H. Schutze.
1999.
Foundationsof Statistical Natural Language Processing.
MITPress.A.
Mikheev.
1998.
Feature lattices for maximum en-tropy modelling.
In COLING-ACL, pages 848?854.H.
Ney, U. Essen, and R. Kneser.
1994.
On structur-ing probabilistic dependences in stochastic languagemodeling.
Computer Speech and Language, 8(1):1?38.F.J.
Och and H. Ney.
2002.
Discriminative training andmaximum entropy models for statistical machin etranslation.
In ACL02: Proceedings of the 40th An-nual Meeting of the Association for ComputationalLinguistics, pages 295?302, PA, Philadelphia.W.
H. Press, B. P. Flannery, S. A. Teukolsky, and W. T.Vetterling.
1988.
Numerical Recipes in C. Cam-bridge University Press, New York, NY.8
