Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 30?38,Portland, Oregon, June 2011. c?2011 Association for Computational LinguisticsA Statistical Test for GrammarCharles YangDepartment of Linguistics & Computer ScienceInstitute for Research in Cognitive ScienceUniversity of Pennsylvaniacharles.yang@ling.upenn.eduAbstractWe propose a statistical test for measuringgrammatical productivity.
We show thatvery young children?s knowledge is consistentwith a systematic grammar that independentlycombines linguistic units.
To a testable extent,the usage-based approach to language and lan-guage learning, which emphasizes the role oflexically specific memorization, is inconsis-tent with the child language data.
We also dis-cuss the connection of this research with de-velopments in computational and theoreticallinguistics.1 IntroductionEistein was a famously late talker.
The great physi-cist?s first words, at the ripe age of three, were to pro-claim ?The soup is too hot.?
Apparently he hadn?thad anything interesting to say.The moral of the story is that one?s linguistic be-havior may not be sufficiently revealing of one?s lin-guistic knowledge.
The problem is especially acutein the study of child language since children?s lin-guistic production is often the only, and certainlythe most accessible, data on hand.
Much of the tra-ditional research in language acquisition recognizesthis challenge (Shipley et al 1969, Slobin 1971,Bowerman 1973, Brown 1973) and has in generaladvocated the position that child language be inter-preted in terms of adult-like grammatical devices.This tradition has been challenged by the usage-based approach to language (Tomasello 1992,2000a) which, while reviving some earlier theoriesof child grammar (Braine 1964), also reflects a cur-rent trend in linguistic theorizing that emphasizesthe storage of specific linguistic forms and con-structions at the expense of general combinatoriallinguistic principles and overarching points of lan-guage variation (Goldberg 2003, Sag 2010, etc.
).Child language, especially in the early stages, isclaimed to consist of specific usage-based schemas,rather than productive linguistic system as pre-viously conceived.
The main evidence for thisapproach comes from the lack of combinatorialdiversity?the hallmark of a productive grammar?in child language data (Tomasello 2000a).
Forinstance, verbs in young children?s language tendto appear in very few frames rather than acrossmany; this ?uneveness?
has been attributed to theverb-specific predicate structures rather than gen-eral/categorical rules.
Similar observations havebeen made in the acquisition of inflectional mor-phology, where many stems are used only in rel-atively few morphosyntactic contexts (e.g., person,number).
Another concrete example comes fromthe syntactic use of the determiners ?a?
and ?the?,which can be interchangeably used with singularnouns.1 An overlap metric has been defined as theratio of nouns appearing with both ?a?
and ?the?out of those appearing with either.
Pine & Lieven(1997) find that overlap values are generally low inchild language, in fact considerably below chancelevel.
This finding is taken to support the view thatthe child?s determiner use is bound with specificnouns rather than reflecting a productive grammardefined over the abstract categories of determinersand nouns (Valian 1986).1Although ?a?
is typically described as combining withcountable nouns, instances such as ?a water?, ?a sun?
and ?afloor?
are frequently attested in both child and adult speech fromCHILDES.30The computational linguistics literature has seenthe influence of usage-based approach: computa-tional models have been proposed to proceed froman initial stage of lexicalized constructions towarda more general grammatical system (Felman 2004,Steels 2004, cf.
Wintner 2009).
However, as far aswe can tell, the evidence for an unproductive stageof grammar as discussed above was established onthe basis of intuition rather than rigorous assess-ments.
We are not aware of a statistical test againstwhich the predictions of usage-based learning canbe verified.
Nor are we of any demonstration thatthe child language data described above is inconsis-tent with the expectation of a fully productive gram-mar, the position rejected in usage-based learning.It is also worth noting that while the proponents ofthe grammar based approach have often producedtests for the quality of the grammar?e.g., the errorsin child language are statistically significantly low?they have likewise failed to provide tests for the exis-tence of the grammar.
As has been pointed out in theusage-based learning literature, low error rates couldbe the result of rote memorization of adult linguisticforms.In this paper, we provide statistical analysis ofgrammar to fill these gaps.
The test is designedto show whether a corpus of linguistic expressionscan be accounted for as the output of a produc-tive grammar that freely combines linguistic units.We demonstrate through case studies based onCHILDES (MacWhinney 2000) that children?s lan-guage shows the opposite of the usage-based view,and it is the productivity hypothesis that is con-firmed.
We also aim to show that the child datais inconsistent with the memory-and-retrieval ap-proach in usage-based learning (Tomasello 2000b).Furthermore, through empirical data and numericalsimulations, we show that our statistical test (cor-rectly) over-predicts productivity for linguistic com-binations that are subject to lexical exceptions (e.g.,irregular tense inflection).
We conclude by drawingconnections between this work and developments incomputational and theoretical linguistics.0246810120 1 2 3 4 5 6 7 8 9log(freq)log(rank)Figure 1: The power law frequency distribution of Tree-bank rules.2 Quantifying Productivity2.1 Zipfian CombinatoricsZipf?s law has long been known to be an om-nipresent feature of natural language (Zipf 1949,Mendelbrot 1954).
Specifically, the probability prof the word nr with the rank r among N word typesin a corpus can be expressed as follows:pr =(Cr)/( N?i=1Ci)=1rHN, HN =N?i=11i(1)Empirical tests show that Zipf?s law provides an ex-cellent fit of word frequency distributions across lan-guages and genres (Baroni 2008).It has been noted that the linguistic combinationssuch as n-grams show Zipf-like power law distribu-tions as well (Teahna 1997, Ha et al 2002), whichcontributes to the familiar sparse data problem incomputational linguistics.
These observations gen-eralize the combination of morphemes (Chan 2008)and grammatical rules.
Figure 1 plots the ranks andfrequencies syntactic rules (on log-log scale) fromthe Penn Treebank (Marcus et al 1993); certainrules headed by specific functional words have beenmerged.Claims of usage-based learning build on the31premise that linguistic productivity entails diver-sity of usage: the ?unevenness?
in usage distribu-tion such as the low overlap in D(eterminer)-N(oun)combinations is taken to be evidence against a sys-tematic grammar.
Paradoxically, however, Valian etal.
(2008) find that the D-N overlap values in moth-ers?
speech to children do not differ significantlyfrom those in children?s speech.
In fact, when ap-plied to the Brown corpus, we find that ?a/the?
over-lap for singular nouns is only 25.2%: almost threequarters that could have appeared with both deter-miners only appeared with one exclusively.
Theoverlap value of 25.2% is actually lower than thoseof some children reported in Pine & Lieven (1997):the language of the Brown corpus, which drawsfrom various genres of professional print materials,must be regarded as less productive and more usage-based than that of a toddler?which seems absurd.Consider the alternative to the usage based view,a fully productive rule that combines a determinerand a singular noun, or ?DP?
D N?, where ?D?a|the?
and ?N?
cat|book|desk|...?.
Other rulescan be similarly formulated: e.g., ?VP?
V DP?,?Vinflection ?
Vstem + Person + Number + Tense?.Suppose a linguistic sample contains S determiner-noun pairs, which consist of D and N unique deter-miners and nouns.
(In the present case D = 2 for?a?
and ?the?.)
The full productivity of the DP rule,by definition, means that the two categories combineindependently.
Two observations, one obvious andthe other novel, can be made in the study of D-Nusage diversity.
First, nouns will follow zipf?s law.For instance, the singular nouns that appear in theform of ?DP?
D N?
in the Brown corpus show alog-log slope of -0.97.
In the CHILDES speech tran-scripts of six children (see section 3.1 for details fordata analysis), the average value of log-log slope is-0.98.
Thus, relatively few nouns occur often butmany will occur only once?which of course cannotoverlap with more than one determiners.Second, while the combination of D and N in theDP rule is syntactically interchangeable, N ?s mayfavor one of the two determiners, a consequence ofpragmatics and indeed non-linguistic factors.
For in-stance, we say ?the bathroom?
more often than ?abathroom?
but ?a bath?
more often than ?the bath?,even though all four DPs are perfectly grammatical.As noted earlier, about 75% of distinct nouns in theBrown corpus occur with exclusively ?the?
or ?a?but not both.
Even the remaining 25% which do oc-cur with both tend to have favorites: only a further25% (i.e.
12.5% of all nouns) are used with ?a?
and?the?
equally frequently, and the remaining 75% areunbalanced.
Overall, for nouns that appear with bothdeterminers as least once (i.e.
25% of all nouns), thefrequency ratio between the more over the less fa-vored determiner is 2.86:1.
These general patternshold for child and adult speech data as well.
In thesix children?s transcripts (section 3), the average per-centage of balanced nouns among those that appearwith both ?the?
and ?a?
is 22.8%, and the more fa-vored vs. less favored determiner has an averagefrequency ratio of 2.54:1.
As a result, even whena noun appears multiple times in a sample, there isstill a significant chance that it has been paired witha single determiner in all instances.We now formalize the overlap measure under theassumption of a rule and Zipfian frequencies ofgrammatical combinations.2.2 Theoretical analysisConsider a sample (N,D, S), which consists ofN unique nouns, D unique determiners, and Sdeterminer-noun pairs.
The nouns that have ap-peared with more than one (i.e.
two, in the caseof ?a?
and ?the?)
determiners will have an overlapvalue of 1; otherwise, they have the overlap value of0.
The overlap value for the entire sample will bethe number of 1?s divided by N .Our analysis calculates the expected value of theoverlap value for the sample (N,D, S) under theproductive rule ?DP?D N?
; let it be O(N,D, S).This requires the calculation of the expected over-lap value for each of the N nouns over all possiblecompositions of the sample.
Consider the noun nrwith the rank r out of N .
Following equation (1), ithas the probability pr = 1/(rHN ) of being drawn atany single trial in S. Let the expected overlap valueof nr be O(r,N,D, S).
The overlap for the samplecan be stated as:O(D,N, S) =1NN?r=1O(r,N,D, S) (2)Consider now the calculation O(r,N,D, S).Since nr has the overlap value of 1 if and only if32it has been used with more than one determiner inthe sample, we have:O(r,N,D, S) = 1?
Pr{nr not sampled during S trials}?D?i=1Pr{nr sampled ith exclusively}= 1?
(1?
pr)S?D?i=1[(dipr + 1?
pr)S ?
(1?
pr)S](3)The last term above requires a brief comment.Under the hypothesis that the language learner hasa productive rule ?DP?D N?, the combination ofdeterminer and noun is independent.
Therefore, theprobability of noun nr combining with the ith deter-miner is the product of their probabilities, or dipr.The multinomial expression(p1 + p2 + ...+ pr?1 + dipr + pr+1 + ...+ pN )S (4)gives the probabilities of all the compositions ofthe sample, with nr combining with the ith deter-miner 0, 1, 2, ... S times, which is simply (dipr +1?
pr)S since (p1 + p2 + pr?1 + pr + pr+1 + ...+pN ) = 1.
However, this value includes the proba-bility of nr combining with the ith determiner zerotimes?again (1?
pr)S?which must be subtracted.Thus, the probability with which nr combines withthe ith determiner exclusively in the sample S is[(dipr + 1 ?
pr)S ?
(1 ?
pr)S ].
Summing thesevalues over all determiners and collecting terms, wehave:O(r,N,D, S) = 1+(D?1)(1?pr)S?D?i=1[(dipr+1?pr)S](5)The formulations in (2)?
(5) allow us to calculatethe expected value of overlap using only the samplesize S, the number of unique noun N and the num-ber of unique determiners D.2 We now turn to the2For the present case involving only two determiners ?the?and ?a?, d1 = 2/3 and d2 = 1/3.
As noted in section 2.1, theempirical probabilities of the more vs. less frequent determinersdeviate somewhat from the strict Zipfian ratio of 2:1, numericalresults show that the 2:1 ratio is a very accurate surrogate fora wide range of actual rations in the calculation of (2)?
(5).This is because most of average overlap value comes from therelatively few and high frequent nouns.empirical evaluations of the overlap test (2).3 Testing Grammar Productivity3.1 Testing grammar in child languageTo study the determiner system in child language,we consider the data from six children Adam, Eve,Sarah, Naomi, Nina, and Peter.
These are the all andonly children in the CHILDES database with sub-stantial longitudinal data that starts at the very begin-ning of syntactic development (i.e, one or two wordstage) so that the usage-based stage, if exists, couldbe observed.
For comparison, we also consider theoverlap measure of the Brown corpus (Kucera &Francis 1967), for which the writers?
productivity isnot in doubt.We applied a variant of the Brill tagger (1995)(http://gposttl.sourceforge.net/) to prepare the childdata before extracting adjacent pairs of determinersfollowed by singular nouns.
While no tagger worksperfectly, the determiners ?a?
and ?the?
are not am-biguous which reliably contribute the tagging of thefollowing word.
The Brown Corpus is already man-ually tagged and the D-N pairs are extracted directly.In an additional test, we pooled together the first100, 300, and 500 D-N pairs from the six childrenand created three hypothetical children in the veryearliest, and presumably least productive, stage oflearning.For each child, the theoretical expectation of over-lap is calculated based on equations in (2)?
(5),that is, only with the sample size S and the num-ber of unique nouns N in determiner-noun pairswhile D = 2.
These expectations are then com-pared against the empirical overlap values computedfrom the determiner-noun samples extracted withthe methods above; i.e., the percentage of nouns ap-pearing with both ?a?
and ?the?.
The results aresummarized in Table 1.The theoretical expectations and the empiricalmeasures of overlap agree extremely well (column5 and 6 in Table 1).
Neither paired t- nor pairedWilcoxon test reveal significant difference betweenthe two sets of values.
A linear regression producesempirical = 1.08 ?
theoretical, R2 = 0.9716: aperfect fit between theory and data would have theslope of 1.0.
Thus we may conclude that the deter-miner usage data from child language is consistent33SubjectSampleSize (S)a or the Nountypes (N )Overlap%(expected)Overlap%(empirical)SNNaomi (1;1-5;1) 884 349 21.8 19.8 2.53Eve (1;6-2;3) 831 283 25.4 21.6 2.94Sarah (2;3-5;1) 2453 640 28.8 29.2 3.83Adam (2;3-4;10) 3729 780 33.7 32.3 4.78Peter (1;4-2;10) 2873 480 42.2 40.4 5.99Nina (1;11-3;11) 4542 660 45.1 46.7 6.88First 100 600 243 22.4 21.8 2.47First 300 1800 483 29.1 29.1 3.73First 500 3000 640 33.9 34.2 4.68Brown corpus 20650 4664 26.5 25.2 4.43Table 1: Empirical and expected determiner-noun overlaps in child speech and the Brown corpus (last row).with the productive rule ?DP?
D N?.The results in Table 1 also reveal considerable in-dividual variation in the overlap values, and it is in-structive to understand why.
As the Brown corpusresult shows (Table 1 last row), sample size S, thenumber of nouns N , or the language user?s age aloneis not predictive of the overlap value.
The variationcan be roughly analyzed as follows.
Given N uniquenouns in a sample of S, greater overlap value can beobtained if more nouns occur more than once.
Zipf?slaw (1) allows us to express this cutoff line in termswith ranks, as the probability of the noun nr withrank r has the probability of 1/(rHN ).
The deriva-tion below uses the fact that the HN =?Ni=1 1/ican be approximated by lnN .S1rHN= 1r =SHN?SlnN(6)That is, only nouns whose ranks are lower thanS/(lnN) can be expected to be non-zero overlaps.The total overlap is thus a monotonically increas-ing function of S/(N lnN) which, given the slowgrowth of lnN , is approximately S/N , a term thatmust be positively correlated with overlap measures.This result is strongly confirmed: S/N is a nearperfect predictor for the empirical values of over-lap (last two columns of Table 1): r = 0.986,p < 0.00001.3.2 Testing usage-based learningWe turn to the question whether children?s deter-miner usage data can be accounted for equally wellby the usage based approach.
In the limiting case,the usage-based child learner could store the inputdata in its entirety and simply retrieve these memo-rized determiner-noun pairs in production.Our effort is hampered by the lack of concrete pre-dictions about child language from the usage-basedliterature.
Explicit models in usage-based learningand similar approaches (e.g., Chang et al 2005,Freudenthal et al 2007, etc.)
generally involveprogramming efforts for which no analytical resultssuch as (2)?
(5) are possible.
Nevertheless, a plau-sible approach can be construed based on a centraltenet of usage-based learning, that the child does notform grammatical generalizations but rather mem-orizes and retrieves specific and item-based combi-nations.
For instance, Tomasello (2000b) suggests?
(w)hen young children have something they wantto say, they sometimes have a set expression read-ily available and so they simply retrieve that expres-sion from their stored linguistic experience.?
Fol-lowing this line of reasoning, we consider a learningmodel that memorizes jointly formed, as opposed toproductively composed, determiner-noun pairs fromthe input.
These pairs will then be sampled; foreach sample, the overlap values can be calculatedand compared against the empirical values in Table1.We consider two variants of the memory model.The first can be called a global memory learner inwhich the learner memorizes all past linguistic ex-34Child sample % (global) % (local) % (emp.
)Eve 831 16.0 17.8 21.6Naomi 884 16.6 18.9 19.8Sarah 2453 24.5 27.0 29.2Peter 2873 25.6 28.8 40.4Adam 3729 27.5 28.5 32.3Nina 4542 28.6 41.1 46.7First 100 600 13.7 17.2 21.8First 300 1800 22.1 25.6 29.1First 500 3000 25.9 30.2 34.2Table 2: The comparison of determiner-noun overlap be-tween two variants of usage-based learning and empiricalresults.perience.
To implement this, we extracted all D-Npairs from about 1.1 million child directed Englishutterances in CHILDES.
The second model is a localmemory learner, which is construed to capture thelinguistic experience of a particular child.
The lo-cal memory learner only memorizes the determiner-noun pairs from the adult utterances in that partic-ular child?s CHILDES transcripts.
In both models,the memory consists of a list of jointly memorizedD-N pairs, which are augmented with their frequen-cies in the input.For each child with a sample size of S (see Table1, column 2), and for each variant of the memorymodel, we use Monte Carlo simulation to randomlydraw S pairs from the memorized lists.
The proba-bility with which a pair is drawn is proportional to itsfrequency.
We then calculate the D-N overlap value,i.e, the the percentage of nouns that appear with both?a?
and ?the?, for each sample.
The results are aver-aged over 1000 draws and presented in Table 2.Both sets of overlap values from the two variantsof usage-based learning (column 3 and 4) differ sig-nificantly from the empirical measures (column 5):p < 0.005 for both paired t-test and paired Wilcoxontest.
This suggests that children?s use of determinersdoes not follow the predictions of the usage-basedlearning approach.
This conclusion is tentative, ofcourse, as we reiterate the need for the usage-basedapproach to provide testable quantitative predictionsabout child language.
At the minimum, child lan-guage does not appear to stem from frequency sensi-tive retrieval of jointly stored determiner-noun con-structions (Tomasello 2000b).Similar considerations apply to other linguisticexamples.
For instance, it is often noted (Lieven,Pine & Baldwin 1997) that child language is dom-inated by a small number of high frequency frozenframes (e.g, ?give me (a) X?
).3 True, but that appearsno more than the reflection of the power law dis-tribution of linguistic units.
In the Harvard corpusof child English (Brown 1973), the frequencies of?give me?, ?give him?
and ?give her?
are 93:15:12,or 7.75:1.23:1, and the frequencies of ?me?, ?him?and ?her?
are 2870:466:364, or the virtually identi-cal 7.88:1.28:1.3.3 Testing for UnproductivityAny statistical test worth its salt should be able todistinguish occurrences from non-occurrences of thepattern which it is designed to detect.
If the produc-tivity test predicts higher overlap values than em-pirically attested?assuming that these classes andtheir combinations follow Zipfian distribution?thenthere would be reason to suspect that the linguistictypes in question do not combine completely inde-pendently, and that some kind of lexically specificprocesses are at work.We test the utility of the productivity test on in-flectional morphology.
In English, the -ing suffixcan attach to all verb stems, only some of whichcan take the -ed suffix?the rest are irregulars.
Chan(2008) shows that in morphological systems acrosslanguages, stems, affixes, and their combinationstend to show Zipf-like distributions.
Therefore, ifwe apply the productivity test to -ing and -ed in-flected forms (i.e, assuming that -ing and -ed werefully interchangeable), then the predicted overlapvalue should be higher than the empirical value.
Ta-ble 3 gives the results based on the verbal morphol-ogy data from the Brown corpus and the six chil-dren studied in section 3.1.
Clearly there are verysignificant discrepancies between the empirical andpredicted overlap values.It can be reasonably objected that English irreg-ular paste tense forms are highly frequent, whichmay contribute to the large discrepancies observedin Table 3.
To address this concern, we created anartificial morphological system in which 100 stems3Thanks to an anonymous reviewer for bringing up this ex-ample.35Subject sample # stems % emp.
% pred.Adam 6774 263 31.3 75.6Eve 1028 120 20.0 61.7Sarah 3442 230 28.7 76.8Naomi 1797 192 32.3 61.9Peter 2112 139 25.9 78.8Nina 2830 191 34.0 77.2Brown 62807 3044 45.5 75.6Table 3: Empirical vs. predicted overlap values for -ingand -ed inflections.Histogram of (Theory-Emprical)Frequency-0.05 0.00 0.05 0.1002468Figure 2: Overlap test applied to linguistic combinationswith lexical exceptions.may take two affixes A and B: A can attach to allstems but B can only attach to 90 while the other10, randomly chosen from the 100, are exceptions.Again, we assume that frequencies of the stems andtheir combinations with affixes follow Zipfian distri-bution.
We random combine stems with affixes 1000times obtaining a sample size of 1000, and count thepercentage of stems that are combined with both Aand B.
We then compare this value against the calcu-lation from (2) which assumes A and B are fully in-terchangeable (where in this case they are not).
Thehistogram of the difference between the theoreticaland empirical values from 100 such simulations aregiven in Figure 3.
The overlap test correctly over-predicts (p < 10?15).4 DiscussionFor the study of child language acquisition, our re-sults show that the usage-based approach to lan-guage learning is not supported by the child dataonce the statistical properties of linguistic units andtheir combinations are taken into account.
A gram-mar based approach is supported (section 3.1) Theseresults do not resolve the innateness debate in lan-guage acquisition: they only point to the very earlyavailability of an abstract and productive grammar.The simulation results on the inadequacy of thememory-and-retrieval approach to child language(section 3.2) show the limitations of lexically spe-cific approach to language learning.
These resultsare congruent with the work in statistical parsing thatalso demonstrates the diminishing returns of lexical-ization (Gildea 2001, Klein & Manning 2003, Bikel2004).
They are also consistent with previous statis-tical studies (Buttery & Korhonen 2005) that childdirected language data appear to be even more lim-ited in syntactic usage diversity.
The ?uneveness?
inverb islands (Tomasello 1992) is to be expected es-pecially when the language sample is small as in thecase of most child language acquisition studies.
Itthus seems necessary for the child learner to derivesyntactic rules with overarching generality in a rel-atively short period of time (and with a few millionutterances).Finally, we envision the overlap test to be oneof many tests for the statistical properties of gram-mar.
Similar tests may be constructed to include awider linguistic context (e.g., three or more wordsinstead of two, but the sparse data problem becomesfar more severe).
The ability to detect lexicalizedprocesses (section 3.3) may prove useful in the au-tomatic induction of grammars.
Such tests would bea welcome addition to the quantitative analysis toolsin the behavioral study of language, which tend toestablish mismatches between observations and nullhypotheses; the favored hypotheses are those thatcannot be rejected (though cannot be confirmed ei-ther).
The present work shows that it is possible totest for statistical matches between observations andwell formulated hypotheses.References36Baroni, M. (2008).
Distributions in text.
InLu?delign, A.
& Kyto?, M.
(Eds.)
Corpus linguis-tics: An international hanbook.
Berlin: Moutonde Gruyter.Bikel, D. (2004) Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30, 479?511.Bowerman, M. (1973).
Early syntactic develop-ment: A cross-linguistic study with special ref-erence to Finnish.
Cambridge: Cambridge Uni-versity Press.Braine, M. (1963).
The ontogeny of English phrasestructure: The first phase.
Language, 39, 3-13.Brill, E. (1995).
Transformation-based error-drivenlearning and natural language processing: Acase study in part-of-speech tagging.
Compu-tational Linguistics, 21 (4), 543?565.Brown, R. (1973).
A first language.
Cambridge,MA: Harvard University Press.Buttery, P. & Korhonen, A.
(2005).
Large-scaleanalysis of verb subcategorization differencesbetween child directed speech and adult speech.Interdisciplinary Workshop on the Identifica-tion and Representation of Verb Features andVerb Classes, Saarland University.Chan, E. (2008).
Structures and distributions inmorphology learning.
Ph.D. Dissertation.
De-partment of Computer and Information Science.University of Pennsylvania.
Philadelphia, PA.Chang, F., Lieven, E., & Tomasello, M. (2006).
Us-ing child utterances to evaluate syntax acquisi-tion algorithms.
In Proceedings of the 28th An-nual Conference of the Cognitive Science Soci-ety.
Vancouver, CanadaFeldman, J.
(2004).
Computational cognitive lin-guistics.
In COLING 2004.Freudenthal, D., Pine, J. M., Aguado-Orea, J.
&Gobet, F. (2007).
Modelling the developmen-tal patterning of finiteness marking in English,Dutch, German and Spanish using MOSAIC.Cognitive Science, 31, 311-341.Gildea, D. (2001) Corpus variation and parser per-formance.
In 2001 Conference on Empiri-cal Methods in Natural Lan- guage Processing(EMNLP).Goldberg, A.
(2003).
Constructions.
Trends in Cog-nitive Science, 219?224.Ha, Le Quan, Sicilia-Garcia, E. I., Ming, Ji.
&Smith, F. J.
(2002).
Extension of Zipf?s law towords and phrases.
Proceedings of the 19th In-ternational Conference on Computational Lin-guistics.
315-320.Klein, D. & Manning, C. (2003).
Accurate unlexi-calized parsing.
In ACL 2003.
423-430.Kuc?era, H & Francis, N. (1967).
Computationalanalysis of present-day English.
Providence,RI: Brown University Press.Lieven, E., Pine, J.
& Baldwin, G. (1997).Lexically-based learning and early grammaticaldevelopment.
Journal of Child Language, 24,187-219.MacWhinney, B.
(2000).
The CHILDES Project.Lawrence Erlbaum.Mandelbrot, B.
(1954).
Structure formelle des texteset communication: Deux e?tudes.
Words, 10, 1?27.Marcus, M., Marcinkiewicz, M. & Santorini, B.(1993).
Building a large annotated corpus ofEnglish: the Penn Treebank.
ComputationalLinguistics, 19, 313-330.Pine, J.
& Lieven, E. (1997).
Slot and frame patternsin the development of the determiner category.Applied Psycholinguistics, 18, 123-138.Sag, I.
(2010).
English filler-gap constructions.
Lan-guage, 486?545.Shipley, E., Smith, C. & Gleitman, L. (1969).
Astudy in the acquisition of language: Free re-ponses to commands.
Language, 45, 2: 322-342.Slobin, Dan.
(1971).
Data for the symposium.
InSlobin, Dan (Ed.)
The Ontogenesis of gram-mar.
New York: Academic Press.
3-14.Steels, L. (2004).
Constructivist development ofgrounded construction grammars.
In ACL2004.Teahan, W. J.
(1997).
Modeling English text.
DPhilthesis.
University of Waikato, New Zealand.37Tomasello, M. (1992).
First verbs: A case studyof early grammatical development.
Cambridge,MA: Harvard University Press.Tomasello, M. (2000a).
Do young children haveadult syntactic competence.
Cognition, 74,209-253.Tomasello, M. (2000b).
First steps toward a usage-based theory of language acquisition.
CognitiveLinguistics, 11, 61-82.Valian, V. (1986).
Syntactic categories in the speechof young children.
Developmental Psychology,22, 562-579.Valian, V., Solt, S. & Stewart, J.
(2008).
Abstractcategories or limited-scope formulae?
The caseof children?s determiners.
Journal of ChildLanguage, 35, 1-36.Wintner, S. (2009).
What science underlies naturallanguage engineering.
Computational Linguis-tics, 641?644.Zipf, G. K. (1949).
Human behavior and the prin-ciple of least effort: An introduction to humanecology.
Addison-Wesley.38
