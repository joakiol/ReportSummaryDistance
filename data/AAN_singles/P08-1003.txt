Proceedings of ACL-08: HLT, pages 19?27,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsWeakly-Supervised Acquisition of Open-Domain Classes and ClassAttributes from Web Documents and Query LogsMarius Pas?caGoogle Inc.Mountain View, California 94043mars@google.comBenjamin Van Durme?University of RochesterRochester, New York 14627vandurme@cs.rochester.eduAbstractA new approach to large-scale informationextraction exploits both Web documents andquery logs to acquire thousands of open-domain classes of instances, along with rel-evant sets of open-domain class attributes atprecision levels previously obtained only onsmall-scale, manually-assembled classes.1 IntroductionCurrent methods for large-scale information ex-traction take advantage of unstructured text avail-able from either Web documents (Banko et al,2007; Snow et al, 2006) or, more recently, logs ofWeb search queries (Pas?ca, 2007) to acquire use-ful knowledge with minimal supervision.
Given amanually-specified target attribute (e.g., birth yearsfor people) and starting from as few as 10 seed factssuch as (e.g., John Lennon, 1941), as many as amillion facts of the same type can be derived fromunstructured text within Web documents (Pas?ca etal., 2006).
Similarly, given a manually-specified tar-get class (e.g., Drug) with its instances (e.g., Vi-codin and Xanax) and starting from as few as 5 seedattributes (e.g., side effects and maximum dose forDrug), other relevant attributes can be extracted forthe same class from query logs (Pas?ca, 2007).
Theseand other previous methods require the manual spec-ification of the input classes of instances before anyknowledge (e.g., facts or attributes) can be acquiredfor those classes.
?Contributions made during an internship at Google.The extraction method introduced in this papermines a collection of Web search queries and a col-lection of Web documents to acquire open-domainclasses in the form of instance sets (e.g., {whales,seals, dolphins, sea lions,...}) associated with classlabels (e.g., marine animals), as well as large setsof open-domain attributes for each class (e.g., circu-latory system, life cycle, evolution, food chain andscientific name for the class marine animals).
Inthis light, the contributions of this paper are four-fold.
First, instead of separately addressing thetasks of collecting unlabeled sets of instances (Lin,1998), assigning appropriate class labels to a givenset of instances (Pantel and Ravichandran, 2004),and identifying relevant attributes for a given set ofclasses (Pas?ca, 2007), our integrated method fromSection 2 enables the simultaneous extraction ofclass instances, associated labels and attributes.
Sec-ond, by exploiting the contents of query logs duringthe extraction of labeled classes of instances fromWeb documents, we acquire thousands (4,583, tobe exact) of open-domain classes covering a widerange of topics and domains.
The accuracy reportedin Section 3.2 exceeds 80% for both instance setsand class labels, although the extraction of classesrequires a remarkably small amount of supervision,in the form of only a few commonly-used Is-A ex-traction patterns.
Third, we conduct the first study inextracting attributes for thousands of open-domain,automatically-acquired classes, at precision levelsover 70% at rank 10, and 67% at rank 20 as de-scribed in Section 3.3.
The amount of supervision islimited to five seed attributes provided for only onereference class.
In comparison, the largest previous19Knowledge extracted from documents and queriesamino acids={phenylalanine, l?cysteine, tryptophan, glutamic acid, lysine, thr,marine animals={whales, seals, dolphins, turtles, sea lions, fishes, penguins, squids,movies={jay and silent bob strike back, romeo must die, we were soldiers, matrix,zoonotic diseases={rabies, west nile virus, leptospirosis, brucellosis, lyme disease,movies: [opening song, cast, characters, actors, film review, movie script,zoonotic diseases: [scientific name, causative agent, mode of transmission,Open?domain labeled classes of instancesmarine animals: [circulatory system, life cycle, evolution, food chain, eyesight,Open?domain class attributes(2)ornithine, valine, serine, isoleucine, aspartic acid, aspartate, taurine, histidine,...}pacific walrus, aquatic birds, comb jellies, starfish, florida manatees, walruses,...}kill bill, thelma and louise, mad max, field of dreams, ice age, star wars,...}cat scratch fever, foot and mouth disease, venezuelan equine encephalitis,...}amino acids: [titration curve, molecular formula, isoelectric point, density,extinction coefficient, pi, food sources, molecular weight, pka values,...]scientific name, skeleton, digestion, gestation period, reproduction, taxonomy,...]symbolism, special effects, soundboards, history, screenplay, director,...]life cycle, pathology, meaning, prognosis, incubation period, symptoms,...]QuerylogsWebdocuments(1)(2)Figure 1: Overview of weakly-supervised extraction ofclass instances, class labels and class attributes from Webdocuments and query logsstudy in attribute extraction reports results on a setof 40 manually-assembled classes, and requires fiveseed attributes to be provided as input for each class.Fourth, we introduce the first approach to infor-mation extraction from a combination of both Webdocuments and search query logs, to extract open-domain knowledge that is expected to be suitablefor later use.
In contrast, the textual data sourcesused in previous studies in large-scale informationextraction are either Web documents (Mooney andBunescu, 2005; Banko et al, 2007) or, recently,query logs (Pas?ca, 2007), but not both.2 Extraction from Documents and Queries2.1 Open-Domain Labeled Classes of InstancesFigure 1 provides an overview of how Web docu-ments and queries are used together to acquire open-domain, labeled classes of instances (phase (1) in thefigure); and to acquire attributes that capture quan-tifiable properties of those classes, by mining querylogs based on the class instances acquired from thedocuments, while guiding the extraction based on afew attributes provided as seed examples (phase (2)).As described in Figure 2, the algorithm for de-riving labeled sets of class instances starts with theacquisition of candidate pairs {ME} of a class la-bel and an instance, by applying a few extractionpatterns to unstructured text within Web documents{D}, while guiding the extraction by the contentsof query logs {Q} (Step 1 in Figure 2).
This is fol-Input: set of Is-A extraction patterns {E}.
large repository of search queries {Q}.
large repository of Web docs {D}.
weighting parameters J?
[0,1] and K?1..?Output: set of pairs of a class label and an instance {<C,I>}Variables: {S} = clusters of distributionally similar phrases.
{V} = vectors of contextual matches of queries in text.
{ME} = set of pairs of a class label and an instance.
{CS} = set of class labels.
{X}, {Y} = sets of queriesSteps:01.
{ME} = Match patterns {E} in docs {D} around {Q}02.
{V} = Match phrases {Q} in docs {D}03.
{S} = Generate clusters of queries based on vectors {V}04.
For each cluster of phrases S in {S}05.
{CS} = ?06.
For each query Q of S07.
Insert labels of Q from {ME} into {CS}08.
For each label CS of {CS}09.
{X} = Find queries of S with the label CS in {ME}10.
{Y} = Find clusters of {S} containing some query10.
with the label CS in {ME}11.
If |{X}| > J?|{S}|12.
If |{Y}| < K13.
For each query X of {X}14.
Insert pair <CS ,X> into output pairs {<C,I>}15.
Return pairs {<C,I>}Figure 2: Acquisition of labeled sets of class instanceslowed by the generation of unlabeled clusters {S} ofdistributionally similar queries, by clustering vectorsof contextual features collected around the occur-rences of queries {Q} within documents {D} (Steps2 and 3).
Finally, the intermediate data {ME} and{S} is merged and filtered into smaller, more accu-rate labeled sets of instances (Steps 4 through 15).Step 1 in Figure 2 applies lexico-syntactic pat-terns {E} that aim at extracting Is-A pairs of an in-stance (e.g., Google) and an associated class label(e.g., Internet search engines) from text.
The twopatterns, which are inspired by (Hearst, 1992) andhave been the de-facto extraction technique in previ-ous work on extracting conceptual hierarchies fromtext (cf.
(Ponzetto and Strube, 2007; Snow et al,2006)), can be summarized as:?[..]
C [such as|including] I [and|,|.
]?,where I is a potential instance (e.g., Venezuelanequine encephalitis) and C is a potential class labelfor the instance (e.g., zoonotic diseases), for exam-ple in the sentence: ?The expansion of the farmsincreased the spread of zoonotic diseases such asVenezuelan equine encephalitis [..]?.During matching, all string comparisons are case-insensitive.
In order for a pattern to match a sen-tence, two conditions must be met.
First, the class20label C from the sentence must be a non-recursivenoun phrase whose last component is a plural-formnoun (e.g., zoonotic diseases in the above sentence).Second, the instance I from the sentence must alsooccur as a complete query somewhere in the querylogs {Q}, that is, a query containing the instance andnothing else.
This heuristic acknowledges the dif-ficulty of pinpointing complex entities within doc-uments (Downey et al, 2007), and embodies thehypothesis that, if an instance is prominent, Websearch users will eventually ask about it.In Steps 4 through 14 from Figure 2, each clus-ter is inspected by scanning all labels attached toone or more queries from the cluster.
For each la-bel CS , if a) {ME} indicates that a large numberof all queries from the cluster are attached to the la-bel (as controlled by the parameter J in Step 12);and b) those queries are a significant portion of allqueries from all clusters attached to the same labelin {ME} (as controlled by the parameter K in Step13), then the label CS and each query with that la-bel are stored in the output pairs {<C,I>} (Steps13 and 14).
The parameters J and K can be usedto emphasize precision (higher J and lower K) orrecall (lower J and higher K).
The resulting pairsof an instance and a class label are arranged intosets of class instances (e.g., {rabies, west nile virus,leptospirosis,...}), each associated with a class label(e.g., zoonotic diseases), and returned in Step 15.2.2 Open-Domain Class AttributesThe labeled classes of instances collected automat-ically from Web documents are passed as inputto phase (2) from Figure 1, which acquires classattributes by mining a collection of Web searchqueries.
The attributes capture properties that arerelevant to the class.
The extraction of attributes ex-ploits the set of class instances rather than the asso-ciated class label, and consists of four stages:1) identification of a noisy pool of candidate at-tributes, as remainders of queries that also containone of the class instances.
In the case of the classmovies, whose instances include jay and silent bobstrike back and kill bill, the query ?cast jay andsilent bob strike back?
produces the candidate at-tribute cast;2) construction of internal search-signature vectorrepresentations for each candidate attribute, basedon queries (e.g., ?cast selection for kill bill?)
thatcontain a candidate attribute (cast) and a class in-stance (kill bill).
These vectors consist of countstied to the frequency with which an attribute occurswith a given ?templatized?
query.
The latter replacesspecific attributes and instances from the query withcommon placeholders, e.g., ?X for Y?
;3) construction of a reference internal search-signature vector representation for a small set ofseed attributes provided as input.
A reference vec-tor is the normalized sum of the individual vectorscorresponding to the seed attributes;4) ranking of candidate attributes with respect toeach class (e.g., movies), by computing similarityscores between their individual vector representa-tions and the reference vector of the seed attributes.The result of the four stages is a ranked list ofattributes (e.g., [opening song, cast, characters,...])for each class (e.g., movies).In a departure from previous work, the instancesof each input class are automatically generated asdescribed earlier, rather than manually assembled.Furthermore, the amount of supervision is limitedto seed attributes being provided for only one ofthe classes, whereas (Pas?ca, 2007) requires seed at-tributes for each class.
To this effect, the extrac-tion includes modifications such that only one ref-erence vector is constructed internally from the seedattributes during the third stage, rather one such vec-tor for each class in (Pas?ca, 2007); and similarityscores are computed cross-class by comparing vec-tor representations of individual candidate attributesagainst the only reference vector available during thefourth stage, rather than with respect to the referencevector of each class in (Pas?ca, 2007).3 Evaluation3.1 Textual Data SourcesThe acquisition of open-domain knowledge, in theform of class instances, labels and attributes, re-lies on unstructured text available within Web doc-uments maintained by, and search queries submittedto, the Google search engine.The collection of queries is a random sample offully-anonymized queries in English submitted byWeb users in 2006.
The sample contains approx-imately 50 million unique queries.
Each query is21Found in Count Pct.
ExamplesWordNet?Yes 1931 42.2% baseball players,(original) endangered speciesYes 2614 57.0% caribbean countries,(removal) fundamental rightsNo 38 0.8% agrochemicals, celebs,handhelds, mangasTable 1: Class labels found in WordNet in original form,or found in WordNet after removal of leading words, ornot found in WordNet at allaccompanied by its frequency of occurrence in thelogs.
The document collection consists of approx-imately 100 million Web documents in English, asavailable in a Web repository snapshot from 2006.The textual portion of the documents is cleaned ofHTML, tokenized, split into sentences and part-of-speech tagged using the TnT tagger (Brants, 2000).3.2 Evaluation of Labeled Classes of InstancesExtraction Parameters: The set of instances thatcan be potentially acquired by the extraction algo-rithm described in Section 2.1 is heuristically lim-ited to the top five million queries with the highestfrequency within the input query logs.
In the ex-tracted data, a class label (e.g., search engines) isassociated with one or more instances (e.g., google).Similarly, an instance (e.g., google) is associatedwith one or more class labels (e.g., search enginesand internet search engines).
The values chosenfor the weighting parameters J and K from Sec-tion 2.1 are 0.01 and 30 respectively.
After dis-carding classes with fewer than 25 instances, the ex-tracted set of classes consists of 4,583 class labels,each of them associated with 25 to 7,967 instances,with an average of 189 instances per class.Accuracy of Class Labels: Built over many years ofmanual construction efforts, lexical gold standardssuch as WordNet (Fellbaum, 1998) provide wide-coverage upper ontologies of the English language.Built-in morphological normalization routines makeit straightforward to verify whether a class label(e.g., faculty members) exists as a concept in Word-Net (e.g., faculty member).
When an extracted label(e.g., central nervous system disorders) is not foundin WordNet, it is looked up again after iteratively re-moving its leading words (e.g., nervous system dis-Class Label={Set of Instances} Parent in C?WordNetamerican composers={aaron copland, composers Yeric ewazen, george gershwin,...}modern appliances={built-in oven, appliances Sceramic hob, tumble dryer,...}area hospitals={carolinas medical hospitals Scenter, nyack hospital,...}multiple languages={chuukese, languages Nladino, mandarin, us english,...}Table 2: Correctness judgments for extracted classeswhose class labels are found in WordNet only after re-moval of their leading words (C=Correctness, Y=correct,S=subjectively correct, N=incorrect)orders, system disorders and disorders).As shown in Table 1, less than half of the 4,583extracted class labels (e.g., baseball players) arefound in their original forms in WordNet.
The ma-jority of the class labels (2,614 out of 4,583) can befound in WordNet only after removal of one or moreleading words (e.g., caribbean countries), whichsuggests that many of the class labels correspond tofiner-grained, automatically-extracted concepts thatare not available in the manually-built WordNet.
Totest whether that is the case, a random sample of200 class labels, out of the 2,614 labels found tobe potentially-useful specific concepts, are manuallyannotated as correct, subjectively correct or incor-rect, as shown in Table 2.
A class label is: correct,if it captures a relevant concept although it could notbe found in WordNet; subjectively correct, if it isrelevant not in general but only in a particular con-text, either from a subjective viewpoint (e.g., mod-ern appliances), or relative to a particular tempo-ral anchor (e.g., current players), or in connectionto a particular geographical area (e.g., area hospi-tals); or incorrect, if it does not capture any use-ful concept (e.g., multiple languages).
The manualanalysis of the sample of 200 class labels indicatesthat 154 (77%) are relevant concepts and 27 (13.5%)are subjectively relevant concepts, for a total of 181(90.5%) relevant concepts, whereas 19 (9.5%) of thelabels are incorrect.
It is worth emphasizing the im-portance of automatically-collected classes judgedas relevant and not present in WordNet: caribbeancountries, computer manufacturers, entertainmentcompanies, market research firms are arguably veryuseful and should probably be considered as part of22Class Label Size of Instance Sets Class Label Size of Instance SetsM (Manual) E (Extracted) M E M?EM M (Manual) E (Extracted) M E M?EMActor actors 1500 696 23.73 Movie movies 626 2201 30.83AircraftModel - 217 - - NationalPark parks 59 296 0Award awards 200 283 13 NbaTeam nba teams 30 66 86.66BasicFood foods 155 3484 61.93 Newspaper newspapers 599 879 16.02CarModel car models 368 48 5.16 Painter painters 1011 823 22.45CartoonChar cartoon 50 144 36 ProgLanguage programming 101 153 26.73characters languagesCellPhoneModel cell phones 204 49 0 Religion religions 128 72 11.71ChemicalElem chemicals 118 487 1.69 River river systems 167 118 15.56City cities 589 3642 50.08 SearchEngine search engines 25 133 64Company companies 738 7036 26.01 SkyBody constellations 97 37 1.03Country countries 197 677 91.37 Skyscraper - 172 - -Currency currencies 55 128 25.45 SoccerClub football clubs 116 101 22.41DigitalCamera digital cameras 534 58 0.18 SportEvent sports events 143 73 12.58Disease diseases 209 3566 65.55 Stadium stadiums 190 92 6.31Drug drugs 345 1209 44.05 TerroristGroup terrorist groups 74 134 33.78Empire empires 78 54 6.41 Treaty treaties 202 200 7.42Flower flowers 59 642 25.42 University universities 501 1127 21.55Holiday holidays 82 300 48.78 VideoGame video games 450 282 17.33Hurricane - 74 - - Wine wines 60 270 56.66Mountain mountains 245 49 7.75 WorldWarBattle battles 127 135 9.44Total mapped: 37 out of 40 classes - - 26.89Table 3: Comparison between manually-assembled instance sets of gold-standard classes (M ) and instance sets ofautomatically-extracted classes (E).
Each gold-standard class (M ) was manually mapped into an extracted class (E),unless no relevant mapping was found.
Ratios ( M?EM ) are shown as percentagesany refinements to hand-built hierarchies, includingany future extensions of WordNet.Accuracy of Class Instances: The computation ofthe precision of the extracted instances (e.g., fifth el-ement and kill bill for the class label movies) relieson manual inspection of all instances associated toa sample of the extracted class labels.
Rather thaninspecting a random sample of classes, the evalua-tion validates the results against a reference set of 40gold-standard classes that were manually assembledas part of previous work (Pas?ca, 2007).
A class fromthe gold standard consists of a manually-createdclass label (e.g., AircraftModel) associated with amanually-assembled, and therefore high-precision,set of representative instances of the class.To evaluate the precision of the extracted in-stances, the manual label of each gold-standard class(e.g., SearchEngine) is mapped into a class label ex-tracted from text (e.g., search engines).
As shownin the first two columns of Table 3, the mapping intoextracted class labels succeeds for 37 of the 40 gold-standard classes.
28 of the 37 mappings involvelinking an abstract class label (e.g., SearchEngine)with the corresponding plural forms among the ex-tracted class labels (e.g., search engines).
The re-maining 9 mappings link a manual class label witheither an equivalent extracted class label (e.g., Soc-cerClub with football clubs), or a strongly-relatedclass label (e.g., NationalPark with parks).
No map-ping is found for 3 out of the 40 classes, namely Air-craftModel, Hurricane and Skyscraper, which aretherefore removed from consideration.The sizes of the instance sets available for eachclass in the gold standard are compared in the thirdthrough fifth columns of Table 3.
In the table, Mstands for manually-assembled instance sets, and Efor automatically-extracted instance sets.
For ex-ample, the gold-standard class SearchEngine con-tains 25 manually-collected instances, while theparallel class label search engines contains 133automatically-extracted instances.
The fifth col-umn shows the percentage of manually-collected in-stances (M ) that are also extracted automatically(E).
In the case of the class SearchEngine, 16 of the25 manually-collected instances are among the 133automatically-extracted instances of the same class,23Label Value Examples of Attributesvital 1.0 investors: investment strategiesokay 0.5 religious leaders: coat of armswrong 0.0 designers: stephanieTable 4: Labels for assessing attribute correctnesswhich corresponds to a relative coverage of 64%of the manually-collected instance set.
Some in-stances may occur within the manually-collected setbut not the automatically-extracted set (e.g., zoom-info and brainbost for the class SearchEngine) or,more frequently, vice-versa (e.g., surfwax, blinkx,entireweb, web wombat, exalead etc.).
Overall,the relative coverage of automatically-extracted in-stance sets with respect to manually-collected in-stance sets is 26.89%, as an average over the 37gold-standard classes.
More significantly, the sizeadvantage of automatically-extracted instance setsis not the undesirable result of those sets contain-ing many spurious instances.
Indeed, the manualinspection of the automatically-extracted instancessets indicates an average accuracy of 79.3% over the37 gold-standard classes retained in the experiments.To summarize, the method proposed in this paper ac-quires open-domain classes from unstructured textof arbitrary quality, without a-priori restrictions tospecific domains of interest and with virtually no su-pervision (except for the ubiquitous Is-A extractionpatterns), at accuracy levels of around 90% for classlabels and 80% for class instances.3.3 Evaluation of Class AttributesExtraction Parameters: Given a target class spec-ified as a set of instances and a set of five seed at-tributes for a class (e.g., {quality, speed, number ofusers, market share, reliability} for SearchEngine),the method described in Section 2.2 extracts rankedlists of class attributes from the input query logs.Internally, the ranking uses Jensen-Shannon (Lee,1999) to compute similarity scores between internalrepresentations of seed attributes, on one hand, andeach of the candidate attributes, on the other hand.Evaluation Procedure: To remove any possiblebias towards higher-ranked attributes during the as-sessment of class attributes, the ranked lists of at-tributes to be evaluated are sorted alphabetically intoa merged list.
Each attribute of the merged list is00.20.40.60.810  10  20  30  40  50PrecisionRankClass: Holidaymanually assembled instancesautomatically extracted instances00.20.40.60.810  10  20  30  40  50PrecisionRankClass: Average-Classmanually assembled instancesautomatically extracted instances00.20.40.60.810  10  20  30  40  50PrecisionRankClass: Mountainmanually assembled instancesautomatically extracted instances00.20.40.60.810  10  20  30  40  50PrecisionRankClass: Average-Classmanually assembled instancesautomatically extracted instancesFigure 3: Accuracy of attributes extracted based on man-ually assembled, gold standard (M ) vs. automatically ex-tracted (E) instance sets, for a few target classes (left-most graphs) and as an average over all (37) target classes(rightmost graphs).
Seed attributes are provided as inputfor each target class (top graphs), or for only one targetclass (bottom graphs)manually assigned a correctness label within its re-spective class.
An attribute is vital if it must bepresent in an ideal list of attributes of the class; okayif it provides useful but non-essential information;and wrong if it is incorrect.To compute the overall precision score over aranked list of extracted attributes, the correctness la-bels are converted to numeric values as shown in Ta-ble 4.
Precision at some rank N in the list is thusmeasured as the sum of the assigned values of thefirst N candidate attributes, divided by N .Accuracy of Class Attributes: Figure 3 plots pre-cision values for ranks 1 through 50 of the lists ofattributes extracted through several runs over the 37gold-standard classes described in the previous sec-tion.
The runs correspond to different amounts ofsupervision, specified through a particular choice inthe number of seed attributes, and in the source ofinstances passed as input to the system:?
number of input seed attributes: seed attributesare provided either for each of the 37 classes, for atotal of 5?37=185 attributes (the graphs at the top ofFigure 3); or only for one class (namely, Country),24Class Precision Top Ten Extracted Attributes# Class Label={Set of Instances} @5 @10 @15 @201 accounting systems={flexcube, 0.70 0.70 0.77 0.70 overview, architecture, interview questions, freemyob, oracle financials, downloads, canadian version, passwords, modules,peachtree accounting, sybiz,...} crystal reports, property management, free trial2 antimicrobials={azithromycin, 1.00 1.00 0.93 0.95 chemical formula, chemical structure, history,chloramphenicol, fusidic acid, invention, inventor, definition, mechanism ofquinolones, sulfa drugs,...} action, side-effects, uses, shelf life5 civilizations={ancient greece, 1.00 1.00 0.93 0.90 social pyramid, climate, geography, flag,chaldeans, etruscans, inca population, social structure, natural resources,indians, roman republic,...} family life, god, goddesses9 farm animals={angora goats, 1.00 0.80 0.83 0.80 digestive system, evolution, domestication,burros, cattle, cows, donkeys, gestation period, scientific name, adaptations,draft horses, mule, oxen,...} coloring pages, p**, body parts, selective breeding10 forages={alsike clover, rye grass, 0.90 0.95 0.73 0.57 types, picture, weed control, planting, uses,tall fescue, sericea lespedeza,...} information, herbicide, germination, care, fertilizerAverage-Class (25 classes) 0.75 0.70 0.68 0.67Table 5: Precision of attributes extracted for a sample of 25 classes.
Seed attributes are provided for only one class.for a total of 5 attributes over all classes (the graphsat the bottom of Figure 3);?
source of input instance sets: the instance setsfor each class are either manually collected (M fromTable 3), or automatically extracted (E from Ta-ble 3).
The choices correspond to the two curvesplotted in each graph in Figure 3.The graphs in Figure 3 show the precision overindividual target classes (leftmost graphs), and as anaverage over all 37 classes (rightmost graphs).
Asexpected, the precision of the extracted attributes asan average over all classes is best when the input in-stance sets are hand-picked (M ), as opposed to au-tomatically extracted (E).
However, the loss of pre-cision from M to E is small at all measured ranks.Table 5 offers an alternative view on the qualityof the attributes extracted for a random sample of25 classes out of the larger set of 4,583 classes ac-quired from text.
The 25 classes are passed as in-put for attribute extraction without modifications.
Inparticular, the instance sets are not manually post-filtered or otherwise changed in any way.
To keepthe time required to judge the correctness of all ex-tracted attributes within reasonable limits, the eval-uation considers only the top 20 (rather than 50) at-tributes extracted per class.
As shown in Table 5, themethod proposed in this paper acquires attributes forautomatically-extracted, open-domain classes, with-out a-priori restrictions to specific domains of inter-est and relying on only five seed attributes specifiedfor only one class, at accuracy levels reaching 70%at rank 10, and 67% at rank 20.4 Related Work4.1 Acquisition of Classes of InstancesAlthough some researchers focus on re-organizingor extending classes of instances already availableexplicitly within manually-built resources such asWikipedia (Ponzetto and Strube, 2007) or Word-Net (Snow et al, 2006) or both (Suchanek et al,2007), a large body of previous work focuses oncompiling sets of instances, not necessarily labeled,from unstructured text.
The extraction proceedseither iteratively by starting from a few seed ex-traction rules (Collins and Singer, 1999), or bymining named entities from comparable news arti-cles (Shinyama and Sekine, 2004) or from multilin-gual corpora (Klementiev and Roth, 2006).A bootstrapping method (Riloff and Jones, 1999)cautiously grows very small seed sets of five in-stances of the same class, to fewer than 300 itemsafter 50 consecutive iterations, with a final preci-sion varying between 46% and 76% depending onthe type of semantic lexicon.
Experimental resultsfrom (Feldman and Rosenfeld, 2006) indicate thatnamed entity recognizers can boost the performanceof weakly supervised extraction of class instances,but only for a few coarse-grained types such as Per-son and only if they are simpler to recognize intext (Feldman and Rosenfeld, 2006).25In (Cafarella et al, 2005), handcrafted extractionpatterns are applied to a collection of 60 million Webdocuments to extract instances of the classes Com-pany and Country.
Based on the manual evaluationof samples of extracted instances, an estimated num-ber of 1,116 instances of Company are extracted ata precision score of 90%.
In comparison, the ap-proach of this paper pursues a more aggressive goal,by extracting a larger and more diverse number oflabeled classes, whose instances are often more dif-ficult to extract than country names and most com-pany names, at precision scores of almost 80%.The task of extracting relevant labels to describesets of documents, rather than sets of instances, isexplored in (Treeratpituk and Callan, 2006).
Givenpre-existing sets of instances, (Pantel and Ravichan-dran, 2004) investigates the task of acquiring appro-priate class labels to the sets from unstructured text.Various class labels are assigned to a total of 1,432sets of instances.
The accuracy of the class labelsis computed over a sample of instances, by manu-ally assessing the correctness of the top five labelsreturned by the system for each instance.
The result-ing mean reciprocal rank of 77% gives partial creditto labels of an evaluated instance, even if only thefourth or fifth assigned labels are correct.
Our eval-uation of the accuracy of class labels is stricter, as itconsiders only one class label of a given instance at atime, rather than a pool of the best candidate labels.As a pre-requisite to extracting relations amongpairs of classes, the method described in (Davidov etal., 2007) extracts class instances from unstructuredWeb documents, by submitting pairs of instances asqueries and analyzing the contents of the top 1,000documents returned by a Web search engine.
Foreach target class, a small set of instances must beprovided manually as seeds.
As such, the methodcan be applied to the task of extracting a large set ofopen-domain classes only after manually enumerat-ing through the entire set of target classes, and pro-viding seed instances for each.
Furthermore, no at-tempt is made to extract relevant class labels for thesets of instances.
Comparatively, the open-domainclasses extracted in our paper have an explicit la-bel in addition to the sets of instances, and do notrequire identifying the range of the target classesin advance, or providing any seed instances as in-put.
The evaluation methodology is also quite dif-ferent, as the instance sets acquired based on the in-put seed instances in (Davidov et al, 2007) are onlyevaluated for three hand-picked classes, with preci-sion scores of 90% for names of countries, 87% forfish species and 68% for instances of constellations.Our evaluation of the accuracy of class instances isagain stricter, since the evaluation sample is larger,and includes more varied classes, whose instancesare sometimes more difficult to identify in text.4.2 Acquisition of Class AttributesPrevious work on the automatic acquisition of at-tributes for open-domain classes from text is lessgeneral than the extraction method and experimentspresented in our paper.
Indeed, previous evalua-tions were restricted to small sets of classes (fortyclasses in (Pas?ca, 2007)), whereas our evaluationsalso consider a random, more diverse sample ofopen-domain classes.
More importantly, by drop-ping the requirement of manually providing a smallset of seed attributes for each target class, and rely-ing on only a few seed attributes specified for onereference class, we harvest class attributes withoutthe need of first determining what the classes shouldbe, what instances they should contain, and fromwhich resources the instances should be collected.5 ConclusionIn a departure from previous approaches to large-scale information extraction from unstructured texton the Web, this paper introduces a weakly-supervised extraction framework for mining usefulknowledge from a combination of both documentsand search query logs.
In evaluations over labeledclasses of instances extracted without a-priori re-strictions to specific domains of interest and withvery little supervision, the accuracy exceeds 90%for class labels, approaches 80% for class instances,and exceeds 70% (at rank 10) and 67% (at rank 20)for class attributes.
Current work aims at expandingthe number of instances within each class while re-taining similar precision levels; extracting attributeswith more consistent precision scores across classesfrom different domains; and introducing confidencescores in attribute extraction, allowing for the detec-tion of classes for which it is unlikely to extract largenumbers of useful attributes from text.26ReferencesM.
Banko, Michael J Cafarella, S. Soderland, M. Broad-head, and O. Etzioni.
2007.
Open information ex-traction from the Web.
In Proceedings of the 20th In-ternational Joint Conference on Artificial Intelligence(IJCAI-07), pages 2670?2676, Hyderabad, India.T.
Brants.
2000.
TnT - a statistical part of speech tagger.In Proceedings of the 6th Conference on Applied Natu-ral Language Processing (ANLP-00), pages 224?231,Seattle, Washington.M.
Cafarella, D. Downey, S. Soderland, and O. Etzioni.2005.
KnowItNow: Fast, scalable information extrac-tion from the Web.
In Proceedings of the HumanLanguage Technology Conference (HLT-EMNLP-05),pages 563?570, Vancouver, Canada.M.
Collins and Y.
Singer.
1999.
Unsupervised mod-els for named entity classification.
In Proceed-ings of the 1999 Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora (EMNLP/VLC-99), pages 189?196, CollegePark, Maryland.D.
Davidov, A. Rappoport, and M. Koppel.
2007.
Fullyunsupervised discovery of concept-specific relation-ships by Web mining.
In Proceedings of the 45thAnnual Meeting of the Association for ComputationalLinguistics (ACL-07), pages 232?239, Prague, CzechRepublic.D.
Downey, M. Broadhead, and O. Etzioni.
2007.
Locat-ing complex named entities in Web text.
In Proceed-ings of the 20th International Joint Conference on Ar-tificial Intelligence (IJCAI-07), pages 2733?2739, Hy-derabad, India.R.
Feldman and B. Rosenfeld.
2006.
Boosting unsu-pervised relation extraction by using NER.
In Pro-ceedings of the 2006 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-ACL-06), pages 473?481, Sydney, Australia.C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lexi-cal Database and Some of its Applications.
MIT Press.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14th In-ternational Conference on Computational Linguistics(COLING-92), pages 539?545, Nantes, France.A.
Klementiev and D. Roth.
2006.
Weakly super-vised named entity transliteration and discovery frommultilingual comparable corpora.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associ-ation for Computational Linguistics (COLING-ACL-06), pages 817?824, Sydney, Australia.L.
Lee.
1999.
Measures of distributional similarity.
InProceedings of the 37th Annual Meeting of the Asso-ciation of Computational Linguistics (ACL-99), pages25?32, College Park, Maryland.D.
Lin.
1998.
Automatic retrieval and clustering of sim-ilar words.
In Proceedings of the 17th InternationalConference on Computational Linguistics and the 36thAnnual Meeting of the Association for ComputationalLinguistics (COLING-ACL-98), pages 768?774, Mon-treal, Quebec.R.
Mooney and R. Bunescu.
2005.
Mining knowledgefrom text using information extraction.
SIGKDD Ex-plorations, 7(1):3?10.M.
Pas?
ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.2006.
Organizing and searching the World Wide Webof facts - step one: the one-million fact extraction chal-lenge.
In Proceedings of the 21st National Confer-ence on Artificial Intelligence (AAAI-06), pages 1400?1405, Boston, Massachusetts.M.
Pas?
ca.
2007.
Organizing and searching the WorldWide Web of facts - step two: Harnessing the wisdomof the crowds.
In Proceedings of the 16th World WideWeb Conference (WWW-07), pages 101?110, Banff,Canada.P.
Pantel and D. Ravichandran.
2004.
Automaticallylabeling semantic classes.
In Proceedings of the2004 Human Language Technology Conference (HLT-NAACL-04), pages 321?328, Boston, Massachusetts.S.
Ponzetto and M. Strube.
2007.
Deriving a large scaletaxonomy from Wikipedia.
In Proceedings of the 22ndNational Conference on Artificial Intelligence (AAAI-07), pages 1440?1447, Vancouver, British Columbia.E.
Riloff and R. Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.In Proceedings of the 16th National Conference onArtificial Intelligence (AAAI-99), pages 474?479, Or-lando, Florida.Y.
Shinyama and S. Sekine.
2004.
Named entity dis-covery using comparable news articles.
In Proceed-ings of the 20th International Conference on Com-putational Linguistics (COLING-04), pages 848?853,Geneva, Switzerland.R.
Snow, D. Jurafsky, and A. Ng.
2006.
Semantic tax-onomy induction from heterogenous evidence.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics (COLING-ACL-06), pages 801?808, Sydney, Australia.F.
Suchanek, G. Kasneci, and G. Weikum.
2007.
Yago:a core of semantic knowledge unifying WordNet andWikipedia.
In Proceedings of the 16th World WideWeb Conference (WWW-07), pages 697?706, Banff,Canada.P.
Treeratpituk and J. Callan.
2006.
Automatically la-beling hierarchical clusters.
In Proceedings of the 7thAnnual Conference on Digital Government Research(DGO-06), pages 167?176, San Diego, California.27
