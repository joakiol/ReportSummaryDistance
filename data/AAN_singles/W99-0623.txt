Exploiting Diversity in Natural Language Processing:Combining ParsersJ ohn  C.  Henderson  and  Er i c  Br i l lDepar tment  of Computer  ScienceJohns Hopkins UniversityBalt imore, MD 21218{jhndrsn,bri l l}@cs.jhu.eduAbst rac tThree state-of-the-art statistical parsers are com-bined to produce more accurate parses, as well asnew bounds on achievable Treebank parsing accu-racy.
Two general approaches are presented and twocombination techniques are described for each ap-proach.
Both parametric and non-parametric mod-els are explored, i The resulting parsers urpass thebest previously published performance results forthe Penn Treebank.1 In t roduct ionThe natural language processing community is in thestrong position of having many available approachesto solving some of its most fundamental problems.The machine learning community has been in a simi-lar situation and has studied the combination ofmul-tiple classifiers (Wolpert, 1992; Heath et al, 1996).Their theoretical I finding is simply stated: classifica-tion error rate decreases toward the noise rate ex-ponentially in the number of independent, accurateclassifiers.
The theory has also been validated em-pirically.Recently, combination techniques have been in-vestigated for part of speech tagging with positiveresults (van Halteren et al, 1998; Brill and Wu,1998).
In both cases the investigators were able toachieve significant improvements over the previousbest tagging results.
Similar advances have beenmade in machine translation (Frederking and Niren-burg, 1994), speech recognition (Fiscus, 1997) andnamed entity recognition (Borthwick et al, 1998).The corpus-based statistical parsing communityhas many fast and accurate automated parsing sys-tems, including systems produced by Collins (1997),Charniak (1997) and Ratnaparkhi (1997).
Thesethree parsers have given the best reported parsingresults on the Penn Treebank Wall Street Journalcorpus (Marcus et al, 1993).
We used these threeparsers to explore parser combination techniques.2 Techn iques  for  Combin ing  Parsers2.1 Parse HybridizationWe are interested in combining the substructures ofthe input parses to produce a better parse.
Wecall this approach parse hybridization.
The sub-structures that are unanimously hypothesized bytheparsers hould be preserved after combination, andthe combination technique should not foolishly cre-ate substructures for which there is no supportingevidence.
These two principles guide experimenta-tion in this framework, and together with the evalu-ation measures help us decide which specific type ofsubstructure to combine.The precision and recall measures (described inmore detail in Section 3) used in evaluating Tree-bank parsing treat each constituent as a separateentity, a minimal unit of correctness.
Since our goalis to perform well under these measures we will simi-larly treat constituents a the minimal substructuresfor combination.2.1.1 Constituent VotingOne hybridization strategy is to let the parsers voteon constituents' membership in the hypothesizedset.
If enough parsers uggest that a particular con-stituent belongs in the parse, we include it.
We callthis technique constituent voting.
We include a con-stituent in our hypothesized parse if it appears in theoutput of a majority of the parsers.
In our particularcase the majority requires the agreement ofonly twoparsers because we have only three.
This techniquehas the advantage of requiring no training, but ithas the disadvantage of treating all parsers equallyeven though they may have differing accuracies ormay specialize in modeling different phenomena.2.1.2 Naive BayesAnother technique for parse hybridization is to usea naive Bayes classifier to determine which con-stituents to include in the parse.
The development ofa naive Bayes classifier involves learning how mucheach parser should be trusted for the decisions itmakes.
Our original hope in combining these parsersis that their errors are independently distributed.This is equivalent to the assumption used in proba-187bility estimation for naive Bayes classifiers, namelythat the attribute values are conditionally indepen-dent when the target value is given.
For this reason,naive Bayes classifiers are well-matched to this prob-lem.In Equations 1 through 3 we develop the modelfor constructing our parse using naive Bayes classi-fication.
C is the union of the sets of constituentssuggested by the parsers.
It(c) is a binary functionreturning t (for true) precisely when the constituentc 6 C should be included in the hypothesis.
Mi(c)is a binary function returning t when parser i (fromamong the k parsers) suggests constituent c shouldbe in the parse.
The hypothesized parse is then theset of constituents hat are likely (P  > 0.5) to be inthe parse according to this model.argmax P(Tr(c)\[M1 (c ) .
.
.
Mk (c) )= argmax P(Ml(C)'"Mk(e)lTr(c))P(Tr(c)).
(1),(c) P(M1 (c)... Mk (c))k= argmaxP(rr(c))II P(Mi(c)lTr(c)) (2)~(c) i=1 P(Mi(c))k= argmaxP(?r(c)) I I P(Mi(c)l~r(c)) (3)~r(c) i :1The estimation of the probabilities in the model iscarried out as shown in Equation 4.
Here N(.
)counts the number of hypothesized constituents inthe development set that match the binary predi-cate specified as an argument.kP(~-(c) = t) I I  P(M'(c) l~r(c) = t)i=1Y(Tr(c) = t) ~ N(Mi(c),r(c) = t)- ICl ~ N ~  ----~ (4)i=12.1.3 Lemma: No Crossing BracketsUnder certain conditions the constituent voting andnaive Bayes constituent combination techniques areguaranteed to produce sets of constituents with nocrossing brackets.
There are simply not enoughvotes remaining to allow any of the crossing struc-tures to enter the hypothesized constituent set.Lemma: If the number of votes required by con-stituent voting is greater than half of the parsersunder consideration the resulting structure has nocrossing constituents.P roo f :  Assume a pair of crossing constituents ap-pears in the output of the constituent voting tech-nique using k parsers.
Call the crossing constituentsA and B.
A receives a votes, and B receives b votes.Each of the constituents must have received at least\[~--~-~q votes from the k parsers, so a > \[k2~ \] and 2 /b > F~.2_!l Let s = a + b.
None of the parsers pro- _ / 2 / "duce parses with crossing brackets, so none of themvotes for both of the assumed constituents.
Hence,s _< k. But by addition of the votes on the twoparses, s > 9.F~.t.11 > k, a contradiction.
?
- - -~  2 !Similarly, when the naive Bayes classifier is con-figured such that the constituents require estimatedprobabilities trictly larger than 0.5 to be accepted,there is not enough probability mass remaining oncrossing brackets for them to be included in the hy-pothesis.2.2 Parser SwitchingIn general, the lemma of the previous section doesnot ensure that all the productions in the combinedparse are found in the grammars of the memberparsers.
There is a guarantee of no crossing bracketsbut there is no guarantee that a constituent in thetree has the same children as it had in any of thethree original parses.
One can trivially create sit-uations in which strictly binary-branching trees arecombined to create a tree with only the root nodeand the terminal nodes, a completely flat structure.This drastic tree manipulation is not appropriatefor situations in which we want to assign particu-lar structures to sentences.
For example, we mayhave semantic information (e.g.
database query op-erations) associated with the productions in a gram-mar.
If the parse contains productions from outsideour grammar the machine has no direct method forhandling them (e.g.
the resulting database querymay be syntactically malformed).We have developed a general approach for combin-ing parsers when preserving the entire structure ofa parse tree is important.
The combining algorithmis presented with the candidate parses and asked tochoose which one is best.
The combining techniquemust act as a multi-position switch indicating whichparser should be trusted for the particular sentence.We call this approach parser switching.
Once againwe present both a non-parametric and a parametrictechnique for this task.2.2.1 Similarity SwitchingFirst we present the non-parametric version of parserswitching, similarity switching:1.
From each candidate parse, 7ri, for a sentence,create the constituent set Si by converting eachconstituent into its tuple representation.2.
The score for rri is ~ IS# N Sil, where j ranges#?iover the candidate parses for the sentence.3.
Switch to (use) the parser with the highest scorefor the sentence.
Ties are broken arbitrarily.The intuition for this technique is that we canmeasure a similarity between parses by counting the188constituents they have in common.
We pick theparse that is most similar to the other parses bychoosing the one with the highest sum of pairwisesimilarities.
This is the parse that is closest o thecentroid of the Observed parses under the similaritymetric.2.2.2 NaYve BayesThe probabilistic version of this procedure isstraightforward: We once again assume indepen-dence among our various member parsers.
Further-more, we know one of the original parses will be thehypothesized parse, so the direct method of deter-mining which one is best is to compute the proba-bility of each of the candidate parses using the prob-abilistic model iwe developed in Section 2.1.
Wemodel each parse as the decisions made to createit, and model those decisions as independent events.Each decision determines the inclusion or exclusionof a candidate constituent.
The set of candidateconstituents comes from the union of all the con-stituents suggested by the member parsers.
Thisis summarized in Equation 5.
The computation ofP(Tri(c)lM1...Mk(c)) has been sketched before inEquations 1 th~:ough 4.
In this case we are inter-ested in findingl the maximum probability parse, ~'~,and Mi is the s:et of relevant (binary) parsing deci-sions made by parser i.
7r~ is a parse selected fromamong the outputs of the individual parsers.
It ischosen such that the decisions it made in includingor excluding constituents are most probable underthe models for all of the parsers.argmax P(~'i\[M1 ?
?
?
Mk)7rl= argmaxHPbr, Cc)iM1Cc)...Mk(c)) (5)7fi C ?i3 ExperimentsThe three parsers were trained and tuned by theircreators on various sections of the WSJ portion ofthe Penn Treebank, leaving only sections 22 and 23completely untouched uring the development ofanyof the parsers, i We used section 23 as the develop-ment set for our combining techniques, and section22 only for final testing.
The development set con-talned 44088 constituents in 2416 sentences and thetest set contained 30691 constituents in 1699 sen-tences.
A sentence was withheld from section 22because its exireme length was troublesome for acouple of the parsers)The standard measures for evaluating Penn Tree-bank parsing performance are precision and recall ofthe predicted Constituents.
Each parse is convertedinto a set of constituents represented as a tuples:1The sentence: in question was more than 100 words inlength and included nested quotes and parenthetical expres-sions.
(label, start, end).
The set is then compared withthe set generated from the Penn Treebank parse todetermine the precision and recall.
P rec is ion  is theportion of hypothesized constituents that are cor-rect and recal l  is the portion of the Treebank con-stituents that are hypothesized.For our experiments we also report the mean ofprecision and recall, which we denote by (P + R)/2and F-measure.
F-measure is the harmonic mean ofprecision and recall, 2PR/(P + R).
It is closer tothe smaller value of precision and recall when thereis a large skew in their values.We performed three experiments to evaluate ourtechniques.
The first shows how constituent featuresand context do not help in deciding which parserto trust.
We then show that the combining tech-niques presented above give better parsing accuracythan any of the individual parsers.
Finally we showthe combining techniques degrade very little when apoor parser is added to the set.3.1 ContextIt is possible one could produce better models by in-troducing features describing constituents and theircontexts because one parser could be much betterthan the majority of the others in particular situa-tions.
For example, one parser could be more ac-curate at predicting noun phrases than the otherparsers.
None of the models we have presented uti-lize features associated with a particular constituent(i.e.
the label, span, parent label, etc.)
to influenceparser preference.
This is not an oversight.
Fea-tures and context were initially introduced into themodels, but they refused to offer any gains in per-formance.
While we cannot prove there are no suchuseful features on which one should condition trust,we can give some insight into why the features weexplored offered no gain.Because we are working with only three parsers,the only situation in which context will help us iswhen it can indicate we should choose to believe asingle parser that disagrees with the majority hy-pothesis instead of the majority hypothesis itself.This is the only important case, because otherwisethe simple majority combining technique would pickthe correct constituent.
One side of the decisionmaking process is when we choose to believe a con-stituent should be in the parse, even though onlyone parser suggests it.
We call such a constituent anisolated constituent.
If we were working with morethan three parsers we could investigate minority con-stituents, those constituents that are suggested byat least one parser, but which the majority of theparsers do not suggest.Adding the isolated constituents to our hypothe-sis parse could increase our expected recall, but inthe cases we investigated it would invariably hurtour precision more than we would gain on recall.189Consider for a set of constituents the isolated con-stituent precision parser metric, the portion of iso-lated constituents that are correctly hypothesized.When this metric is less than 0.5, we expect o in-cur more errors 2than we will remove by adding thoseconstituents o the parse.We show the results of three of the experiments weconducted to measure isolated constituent precisionunder various partitioning schemes.
In Table 1 wesee with very few exceptions that the isolated con-stituent precision is less than 0.5 when we use theconstituent label as a feature.
The counts representportions of the approximately 44000 constituents hy-pothesized by the parsers in the development set.In the cases where isolated constituent precision islarger than 0.5 the affected portion of the hypothesesis negligible.Similarly Figures 1 and 2 show how the iso-lated constituent precision varies by sentence lengthand the size of the span of the hypothesized con-stituent.
In each figure the upper graph shows theisolated constituent precision and the bottom graphshows the corresponding number of hypothesizedconstituents.
Again we notice that the isolated con-stituent precision is larger than 0.5 only in thosepartitions that contain very few samples.
From thiswe see that a finer-grained model for parser combi-nation, at least for the features we have examined,will not give us any additional power.3.2 Performance TestingThe results in Table 2 were achieved on the develop-ment set.
The first two rows of the table are base-lines.
The first row represents the average accuracyof the three parsers we combine.
The second rowis the accuracy of the best of the three parsers.
3The next two rows are results of oracle experiments.The parser switching oracle is the upper bound onthe accuracy that can be achieved on this set in theparser switching framework.
It is the performancewe could achieve if an omniscient observer told uswhich parser to pick for each of the sentences.
Themaximum precision row is the upper bound on accu-racy if we could pick exactly the correct constituentsfrom among the constituents suggested by the threeparsers.
Another way to interpret his is that lessthan 5% of the correct constituents are missing fromthe hypotheses generated by the union of the threeparsers.
The maximum precision oracle is an upperbound on the possible gain we can achieve by parsehybridization.2This is in absolute terms, total errors being the sum ofprecision errors and recall errors.3The identity of this parser is not given, nor is the iden-tity disclosed for the results of any of the individual parsers.We do not aim to compare the performance of the individualparsers, nor do we want to bias further esearch by giving theindividual parser results for the test set.Parser SentencesParser 1 279Parser 2 216Parser 3 1204%161371Table 4: Bayes Switching Parser UsageWe do not show the numbers for the Bayes modelsin Table 2 because the parameters involved were es-tablished using this set.
The precision and recall ofsimilarity switching and constituent voting are bothsignificantly better than the best individual parser,and constituent voting is significantly better thanparser switching in precision.
4 Constituent votinggives the highest accuracy for parsing the Penn Tree-bank reported to date.Table 3 contains the results for evaluating our sys-tems on the test set (section 22).
All of these systemswere run on data that was not seen during their de-velopment.
The difference in precision between sim-ilarity and Bayes switching techniques i significant,but the difference in recall is not.
This is the firstset that gives us a fair evaluation of the Bayes mod-els, and the Bayes switching model performs ignif-icantly better than its non-parametric counterpart.The constituent voting and naive Bayes techniquesare equivalent because the parameters learned in thetraining set did not sufficiently discriminate betweenthe three parsers.Table 4 shows how much the Bayes switching tech-nique uses each of the parsers on the test set.
Parser3, the most accurate parser, was chosen 71% of thetime, and Parser 1, the least accurate parser was cho-sen 16% of the time.
Ties are rare in Bayes switch-ing because the models are fine-grained - many es-timated probabilities are involved in each decision.3.3 Robustness TestingIn the interest of testing the robustness of these com-bining techniques, we added a fourth, simple non-lexicalized PCFG parser.
The PCFG was trainedfrom the same sections of the Penn Treebank as theother three parsers.
It was then tested on section22 of the Treebank in conjunction with the otherparsers.The results of this experiment can be seen in Ta-ble 5.
The entries in this table can be compared withthose of Table 3 to see how the performance of thecombining techniques degrades in the presence of aninferior parser.
As seen by the drop in average indi-vidual parser performance baseline, the introducedparser does not perform very well.
The average in-dividual parser accuracy was reduced by more than5% when we added this new parser, but the preci-4All significance claims are made based on a binomial hy-pothesis test of equality with an a < 0.01 confidence l vel.190Constituent Parser1 Parser2 Parser3Label count Precision count Precision count PrecisionADJPADVPCONJP' FRAG:INTJLSTNACNP'NXPPPRNPRTQPRRCSSBARSBARQSINVSQUCPVPWHADJPWHADVPWHNPWHPPX132 28.78150 25.332 50.0051 3.923 66.660 NA0 NA1489 21.087 85.71732 23.6320 55.0O12 16.6621 38.091 0.00757 13.73331 ii.780 NA3 66.662 06 16.66868 13.360 NA2 100.0033 33.330 NA0 NA215 21.86129 21.708 37.5029 27.581 100.000 NA13 53.841550 18.389 22.22643 20.0633 54.5420 40.0034 44.111 0.00482 23.65196 23.976 16.6611 81.8111 18.1812 8.33630 24.120 NA5 40.008 25.000 NA2 100.00173 34.10102 31.373 0.0011 9.092 50.000 NA7 14.281178 27.333 0.00503 27.8338 15.7816 37.5076 14.472 0.00434 38.94178 34.833 0.0013 30.763 33.338 12.50477 35.421 0.001 100.0017 58.822 100.001 0.00Table 1: Isolated Constituent Precision By Constituent LabelReference / System P R (P+R)/2 FAverage Individual Parser 87.14 86.91 87.02 87.02Best Individual Parser 88.73 88.54 88.63 88.63Parser Switching Oracle 93.12 92.84 92.98 92.98Maximum Precision Oracle 100.00 95.41 97.70 97.65Similarity Switching 89.50 89.88 89.69 89.69Constituent Voting 92.09 89.18 90.64 90.61Table 2: Summary of Development Set PerformanceReference / System P R (P+R)/2 FAverage Individual Parser 87.61 87.83 87.72 87.72Best Individual Parser 89.61 89.73 89.67 89.67Parser Switching Oracle 93.78 93.87 93.82 93.82Maximum Precision Oracle 100.00 95.91 97.95 97.91Similarity Switching 90.04 90.81 90.43 90.43Bayes Switching 90.78 90.70 90.74 90.74Constituent Voting 92.42 90.10 91.26 91.25Naive Bayes 92.42 90.10 91.26 91.25Table 3: Test Set Results19166zlO0806040200080070060050040030020010000!Parser I JParser 2 - - -x - - -Parser 3 ---~---?
., ,.
: ",,.?"???
.
.
.~ .
.
.
.
.
.~  .
.
.
.
.
.
.
~., '" "?.
,,,"/I I I I I I ~-10 20 30 40 50 60lO/ \: \/i?
,,/'/.,.
;.. "', '\""',.~.
; \'\%I I I I I20 30 40 50 60Sentence LengthPa~ser 1 *Parser 2 - - -x- - -Parser 3 ---~---7070Figure 1: Isolated Constituent Precision and Sentence Lengthsion of the constituent voting technique was the onlyresult that decreased significantly.
The Bayes mod-els were able to achieve significantly higher preci-sion than their non-parametric counterparts.
We seefrom these results that the behavior of the paramet-ric techniques are robust in the presence of a poorparser?
Surprisingly, the non-parametric switchingtechnique also exhibited robust behaviour in this sit-uation.4 Conc lus ionWe have presented two general approaches tostudy-ing parser combination: parser switching and parsehybridization.
For each experiment we gave an non-192ogaggz6O40200016001200g00600400200s t !/t,,?
?/ I// I/: l / t" A ;  // : !?
.
o~-- . "'"
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
: i /.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
i i l. .
.
.
-X... .~ .
-X10 20 30 40I i t /II ////!1 /!/Parser  1 iParser 2 - - -x - - -Parser 3 ---~---\] I50iParser I iParser 2 - - -x-- -Parser 3 ---~---""",~ \\ \\',\\~1~ .
?60l0 20 30 40 50 60Constituent Span Length (Tokens)Figure 2: Isolated Constituent Precision and Span Lengthparametric and a parametric technique for combin-ing parsers: All four of the techniques tudied resultin parsing systems that perform better than any pre-viously reported.
Both of the switching techniques,as well as the parametric hybridization techniquewere also shown to be robust when a poor parser wasintroduced into the experiments.
Through parsercombination we have reduced the precision error rateby 30% and the recall error rate by 6% compared tothe best previously published result.Combining multiple highly-accurate independentparsers yields promising results.
We plan to exploremore powerful techniques for exploiting the diversityof parsing methods.193Reference / System P R (P+R)/2 FAverage Individual Parser 84.55 80.91 82.73 82.69Best Individual Parser 89.61 89.73 89.67 89.67Parser Switching Oracle 93.92 93.88 93.90 93.90Maximum Precision Oracle 100.00 96.66 98.33 98.30Similarity Switching 89.90 90.89 90.40 90.39Bayes Switching 90.94 90.70 90.82 90.82Constituent Voting 89.78 91.80 90.79 90.78Naive Bayes 92.42 90.10 91.26 91.25Table 5: Robustness Test Results5 AcknowledgementsWe would like to thank Eugene Charniak, MichaelCollins, and Adwait Ratnaparkhi for enabling all ofthis research by providing us with their parsers andhelpful comments.This work was funded by NSF grant IRI-9502312.Both authors are members of the Center for Lan-guage and Speech Processing at Johns Hopkins Uni-versity.ReferencesAndrew Borthwick, John Sterling, EugeneAgichtein, and Ralph Grishman.
1998.
Ex-ploiting diverse knowledge sources via maximumentropy in named entity recognition.
In EugeneCharniak, editor, Proceedings of the Sixth Work-shop on Very Large Corpora, pages 152-160,Montreal.Eric Brill and Jun Wu.
1998.
Classifier combinationfor improved lexical combination.
In Proceedingsof the 17th International Conference on Compu-tational Linguistics.Eugene Charniak.
1997.
Statistical parsing witha context-free grammar and word statistics.
InProceedings of the Fourteenth National Confer-ence on Artificial Intelligence, Menlo Park.
AAAIPress/MIT Press.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings ofthe Annual Meeting of the Association of Compu-tational Linguistics, volume 35, Madrid.Jonathan G. Fiscus.
1997.
A post-processing sys-tem to yield reduced word error rates: Recognizeroutput voting error reduction (ROVER).
In Eu-roSpeech 1997 Proceedings, volume 4, pages 1895-1898.Robert Frederking and Sergei Nirenburg.
1994.Three heads are better than one.
In Proceedings ofthe 4th Conference on Applied Natural LanguageProcessing, pages 95-100, Stuttgart, Germany.David Heath, Simon Kasif, and Steven Salzberg.1996.
Committees of decision trees.
InB.
Gorayska and J. Mey, editors, CognitiveTechnology: In Search of a Humane Inter-face, pages 305-317.
Elsevier Science B.V.,Amsterdam.Mitchell  P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building a largeannotated corpus of english: The Penn Treebank.Computational Linguistics, 19(2):313-330.Adwait Ratnaparkhi.
1997.
A linear observed timestatistical parser based on maximum entropymodels.
In Second Conference on Empirical Meth-ods in Natural Language Processing, Providence,R.I.Hans van Halteren, Jakub Zavrel, and Walter Daele-mans.
1998.
Improving data driven wordclass tag-ging by system combination.
In Proceedings of the17th International Conference on ComputationalLinguistics, Montreal.David H. Wolpert.
1992.
Stacked generalization.Neural Networks, 5:241-259.194
