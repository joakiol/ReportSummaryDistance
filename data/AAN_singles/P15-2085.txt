Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 518?523,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsPoint Process Modelling of Rumour Dynamics in Social MediaMichal Lukasik1, Trevor Cohn2and Kalina Bontcheva11Department of Computer Science,The University of Sheffield2Department of Computing and Information Systems,The University of Melbourne{m.lukasik, k.bontcheva}@shef.ac.ukt.cohn@unimelb.edu.auAbstractRumours on social media exhibit complextemporal patterns.
This paper develops amodel of rumour prevalence using a pointprocess, namely a log-Gaussian Cox pro-cess, to infer an underlying continuoustemporal probabilistic model of post fre-quencies.
To generalize over different ru-mours, we present a multi-task learningmethod parametrized by the text in postswhich allows data statistics to be sharedbetween groups of similar rumours.
Ourexperiments demonstrate that our modeloutperforms several strong baseline meth-ods for rumour frequency prediction eval-uated on tweets from the 2014 Fergusonriots.1 IntroductionThe ability to model rumour dynamics helps withidentifying those, which, if not debunked early,will likely spread very fast.
One such example isthe false rumour of rioters breaking into McDon-ald?s during the 2011 England riots.
An effectiveearly warning system of this kind is of interest togovernment bodies and news outlets, who strugglewith monitoring and verifying social media postsduring emergencies and social unrests.
Anotherapplication of modelling rumour dynamics couldbe to predict the prevalence of a rumour through-out its lifespan, based on occasional spot checksby journalists.The challenge comes from the observation thatdifferent rumours exhibit different trajectories.Figure 1 shows two example rumours from ourdataset (see Section 3): online discussion of ru-mour #10 quickly drops away, whereas rumour#37 takes a lot longer to die out.
Two charac-teristics can help determine if a rumour will con-tinue to be discussed.
One is the dynamics of postoccurrences, e.g.
if the frequency profile decaysquickly, chances are it would not attract furtherattention.
A second factor is text from the poststhemselves, where phrases such as not true, un-confirmed, or debunk help users judge veracity andthus limit rumour spread (Zhao et al., 2015).This paper considers the problem of modellingtemporal frequency profiles of rumours by takinginto account both the temporal and textual infor-mation.
Since posts occur at continuous times-tamps, and their density is typically a smooth func-tion of time, we base our model on point pro-cesses, which have been shown to model well suchdata in epidemiology and conflict mapping (Brixand Diggle, 2001; Zammit-Mangion et al., 2012).This framework models count data in a continuoustime through the underlying intensity of a Poissondistribution.
The posterior distribution can thenbe used for several inference problems, e.g.
toquery the expected count of posts, or to find theprobability of a count of posts occurring duringan arbitrary time interval.
We model frequencyprofiles using a log-Gaussian Cox process (M?llerand Syversveen, 1998), a point process where thelog-intensity of the Poisson distribution is mod-elled via a Gaussian Process (GP).
GP is a non-parametric model which allows for powerful mod-elling of the underlying intensity function.Modelling the frequency profile of a rumourbased on posts is extremely challenging, sincemany rumours consist of only a small number ofposts and exhibit complex patterns.
To overcomethis difficulty we propose a multi-task learning ap-proach, where patterns are correlated across mul-tiple rumours.
In this way statistics over a largertraining set are shared, enabling more reliable pre-dictions for distant time periods, in which no postsfrom the target rumour have been observed.
Wedemonstrate how text from observed posts can beused to weight influence across rumours.
Using aset of Twitter rumours from the 2014 Ferguson un-rest, we demonstrate that our models provide good5180.0 0.2 0.4 0.6 0.8 1.013579111315 LGCPLGCPICMLGCPTXT(a) rumour #370.0 0.2 0.4 0.6 0.8 1.01357911131517 LGCPLGCPICMLGCPTXT(b) rumour #10Figure 1: Predicted frequency profiles for example rumours.
Black bars denote training intervals, white bars denote testintervals.
Dark-coloured lines correspond to mean predictions by the models, light shaded areas denote the 95% confidenceinterval, ??
2?.
This figure is best viewed in colour.prediction of rumour popularity.This paper makes the following contributions:1.
Introduces the problem of modelling rumourfrequency profiles, and presents a method basedon a log-Gaussian Cox process; 2.
Incorporatesmulti-task learning to generalize across disparaterumours; and 3.
Demonstrates how incorporatingtext into multi-task learning improves results.2 Related WorkThere have been several descriptive studies of ru-mours in social media, e.g.
Procter et al.
(2013)analyzed rumours in tweets about the 2011 Lon-don riots and showed that they follow similar life-cycles.
Friggeri et al.
(2014) showed how Face-book constitutes a rich source of rumours and con-versation threads on the topic.
However, none ofthese studies tried to model rumour dynamics.The problem of modelling the temporal natureof social media explicitly has received little at-tention.
The work most closely related modelledhash tag frequency time-series in Twitter usingGP (Preotiuc-Pietro and Cohn, 2013).
It madeseveral simplifications, including discretising timeand treating the problem of modelling counts asregression, which are both inappropriate.
In con-trast we take a more principled approach, usinga point process.
We use the proposed GP-basedmethod as a baseline to demonstrate the benefit ofusing our approaches.The log-Gaussian Cox process has been appliedfor disease and conflict mapping, e.g.
Zammit-Mangion et al.
(2012) developed a spatio-temporalmodel of conflict events in Afghanistan.
Incontrast here we deal with temporal text data,and model several correlated outputs rather thantheir single output.
Related also is the extensivework done in spatio-temporal modelling of memespread.
One example is application of Hawkesprocesses (Yang and Zha, 2013), a probabilisticframework for modelling self-excitatory phenom-ena.
However, these models were mainly used fornetwork modelling rather than revealing complextemporal patterns, which may emerge only implic-itly, and are more limited in the kinds of temporalpatterns that may be represented.3 Data & ProblemIn this section we describe the data and we formal-ize the problem of modelling rumour popularity.Data We use the Ferguson rumour data set (Zu-biaga et al., 2015), consisting of tweets collectedin August and September 2014 during the Fergu-son unrest.
It contains both source tweets and theconversational threads around these (where avail-able).
All source tweets are categorized as ru-mour vs non-rumour, other tweets from the samethread are assigned automatically as belonging tothe same event as the source tweet.
Since somerumours have few posts, we consider only thosewith at least 15 posts in the first hour as rumoursof particular interest.
This results in 114 rumoursconsisting of a total of 4098 tweets.Problem Definition Let us consider a time in-terval [0, l] of length l=2 hours, a set of n rumoursR = {Ei}ni=1, where rumour Eiconsists of aset of miposts Ei= {pij}mij=1.
Posts are tuplespij= (xij, tij), where xijis text (in our case a bagof words text representation) and tijis a timestampdescribing post pij, measured in time elapsed sincethe first post on rumour Ei.Posts occur at different timestamps, yieldingvarying density of posts over time, which we areinterested in estimating.
To evaluate the predicteddensity for a given rumour Eiwe leave out postsfrom a set of intervals Tte= {[sik, eik]}Kik=1(wheresikand eikare respectively start and end points of519interval k for rumour i) and estimate performanceat predicting counts in them by the trained model.The problem is considered in supervisedsettings, where posts on this rumour out-side of these intervals form the training setEOi={pij: tij6?
?Kik=1[sik, eik]}.
Let the number ofelements in EOibe mOi.
We also consider a do-main adaptation setting, where additionally postsfrom other rumours are observed ROi=R\Ei.Two instantiations of this problem formulationare considered.
The first is interpolation, wherethe test intervals are not ordered in any particularway.
This corresponds to a situation, e.g., whena journalist analyses a rumour during short spotchecks, but wants to know the prevalence of therumour at other times, thus limiting the need forconstant attention.
The second formulation is thatof extrapolation, where all observed posts occurbefore the test intervals.
This corresponds to ascenario where the user seeks to predict the futureprofile of the rumour, e.g., to identify rumours thatwill attract further attention or wither away.Although our focus here is on rumours, ourmodel is more widely applicable.
For example,one could use it to predict whether an advertise-ment campaign would be successful or how a po-litical campaign would proceed.4 ModelWe consider a log-Gaussian Cox process (LGCP)(M?ller and Syversveen, 1998), a generalizationof inhomogeneous Poisson process.
In LGCPthe intensity function is assumed to be a stochas-tic process which varies over time.
In fact, theintensity function ?
(t) is modelled using a la-tent function f(t) sampled from a Gaussian pro-cess (Rasmussen and Williams, 2005), such that?
(t) = exp (f(t)) (exponent ensures positivity).This provides a non-parametric approach to modelthe intensity function.
The intensity function canbe automatically learned from the data set and itscomplexity depends on the data points.We model the occurrence of posts in a rumourEito follow log-Gaussian Cox process (LGCP)with intensity ?i(t), where ?i(t) = exp(fi(t)).We associate a distinct intensity function witheach rumour as they have varying temporal pro-files.
LGCP models the likelihood that a singletweet occurs at time t in the interval [s, t] for a ru-mour Eigiven the latent function fi(t) asp(y = 1|fi) = exp(fi(t)) exp(?
?tsexp(fi(u))du).Then, the likelihood of posts EOiin time intervalT given a latent function fican be obtained asp(EOi|fi)=exp???
?T?Tteexp (fi(u)) du+mOi?j=1fi(tij)??
(1)The likelihood of posts in the rumour data isobtained by taking the product of the likelihoodsover individual rumours.
The likelihood (1) iscommonly approximated by considering sub-regions of T and assuming constant intensitiesin sub-regions of T (M?ller and Syversveen,1998; Vanhatalo et al., 2013) to overcome com-putational difficulties arising due to integration.Following this, we approximate the likelihood asp(EOi|fi) =?Ss=1Poisson(ys| lsexp(fi(?ts))).Here, time is divided into S intervals indexedby s,?tsis the centre of the sthinterval, lsis thelength of the sthinterval and ysis number oftweets posted during this interval.The latent function f is modelled via a Gaussianprocess (GP) (Rasmussen and Williams, 2005):f(t) ?
GP(m(t), k(t, t?
)), where m is the meanfunction (equal 0) and k is the kernel specifyinghow outputs covary as a function of the inputs.We use a Radial Basis Function (RBF) kernel,k(t, t?)
= a exp(?(t?
t?
)2/l), where lengthscalel controls the extent to which nearby points influ-ence one another and a controls the scale of thefunction.The distribution of the posterior p(fi(t)|EOi) atan arbitrary timestamp t is calculated based on thespecified prior and the Poisson likelihood.
It isintractable and approximation techniques are re-quired.
There exist various methods to deal withcalculating the posterior; here we use the Laplaceapproximation, where the posterior is approxi-mated by a Gaussian distribution based on the first2 moments.
For more details about the model andinference we refer the reader to (Rasmussen andWilliams, 2005).
The predictive distribution overtime t?is obtained using the approximated poste-rior.
This predictive distribution is then used toobtain the intensity function value at the point t?
:?i(t?|EOi) =?exp (fi(t)) p(fi(t)|EOi)dfi.The predictive distribution over counts at a par-ticular time interval of length w with a mid-pointt?for rumour Eiis Poisson distributed with ratew?i(t?|EOi).520Multi-task learning and incorporating text Inorder to exploit similarities across rumours wepropose a multi-task approach where each rumourrepresents a task.
We consider two approaches.First, we employ a multiple output GP basedon the Intrinsic Coregionalization Model (ICM)(?Alvarez et al., 2012).
It is a method which hasbeen successfully applied to a range of NLP tasks(Beck et al., 2014; Cohn and Specia, 2013).
ICMparametrizes the kernel by a matrix representingsimilarities between pairs of tasks.
We expect it tofind correlations between rumours exhibiting sim-ilar temporal patterns.
The kernel takes the formkICM((t, i), (t?, i?
))=ktime(t, t?
)Bi,i?,whereB is a square coregionalization matrix (rank1,B = ?I + vvT), i and i?denote the tasks of thetwo inputs, ktimeis a kernel for comparing inputst and t?
(here RBF) and ?
is a vector of valuesmodulating the extent of each task independence.In a second approach, we parametrize the inter-task similarity measures by incorporating text ofthe posts.
The full multi-task kernel takes formkTXT((t, i), (t?, i?))
= ktime(t, t?)
?ktext(?pij?EOixij,?pi?j?EOi?xi?j).We compare text vectors using cosine similar-ity, ktext(x,y) = b+ cxTy?x?
?y?, where the hyper-parameters b > 0 and c > 0 modulate betweentext similarity and a global constant similarity.
Wealso consider combining both multi-task kernels,yielding kICM+TXT= kICM+ kTXT.Optimization All hyperparameters are opti-mized by maximizing the marginal likelihood ofthe data L(EOi|?
), where ?
= (a, l,?,v, b, c) or asubset thereof, depending on the choice of kernel.5 Experimental SetupEvaluation metric We use mean squared error(MSE) to measure the difference between truecounts and predicted counts in the test intervals.Since probabilistic models (GP, LGCP) return dis-tributions over possible outputs, we also evalu-ate them via the log-likelihood (LL) of the truecounts under the returned distributions (respec-tively Gaussian and Poisson distribution).Baselines We use the following baselines.
Thefirst is the Homogenous Poisson Process (HPP)trained on the training set of the rumour.
We se-lect its intensity ?
using maximum likelihood esti-mate, which equals to the mean frequency of postsin the training intervals.
The second baseline isGaussian Process (GP) used for predicting hash-tag frequencies in Twitter by Preotiuc-Pietro andCohn (2013).
Authors considered various kernelsin their experiments, most notably periodic ker-nels.
In our case it is not apparent that rumoursexhibit periodic characteristics, as can be seen inFigure 1.
We restrict our focus to RBF kernel andleave inspection of other kernels such as periodicones for both GP and LGCP models for future.The third baseline is to always predict 0 posts inall intervals.
The fourth baseline is tailored for theinterpolation setting, and uses simple interpolationby averaging over the frequencies of the closestleft and right intervals, or the frequency of theclosest interval for test intervals on a boundary.Data preprocessing In our experiments, weconsider the first two hours of each rumour lifes-pan, which we split into 20 evenly spaced inter-vals.
This way, our dataset consists in total of 2280intervals.
We iterate over rumours using a formof folded cross-validation, where in each iterationwe exclude some (but not all) time intervals for asingle target rumour.
The excluded time intervalsform the test set: either by selecting half at random(interpolation); or by taking only the second halffor testing (extrapolation).
To ameliorate the prob-lems of data sparsity, we replace words with theirBrown cluster ids, using 1000 clusters acquired ona large scale Twitter corpus (Owoputi et al., 2013).The mean function for the underlying GP inLGCP methods is assumed to be 0, which resultsin intensity function to be around 1 in the absenceof nearby observations.
This prevents our methodfrom predicting 0 counts in these regions.
Weadd 1 to the counts in the intervals to deal withthis problem as a preprocessing step.
The originalcounts can be obtained by decrementing 1 fromthe predicted counts.
Instead, one could use a GPwith a non-zero mean function and learn the meanfunction, a more elegant way of approaching thisproblem, which we leave for future work.6 ExperimentsThe left columns of Table 1 report the resultsfor the extrapolation experiments, showing themean and variance of results across the 114 ru-mours.
According to log likelihood evaluationmetric, GP is the worst from the probabilistic ap-521Extrapolation InterpolationMSE LL MSE LLHPP 7.14?10.1?
-23.5?10.1?
7.66?7.55?
-25.8?11.0?GP 4.58?11.0?
-34.6?8.78?
6.13?6.57?
-90.1?198 ?Interpolate 4.90?13.1?
- 5.29?6.06?
-0 2.76?7.81?
- 7.65?11.0?
-LGCP 3.44?9.99?
-15.8?11.6??
6.01?6.29?
-21.0?8.77?
?LGCP ICM 2.46?7.82??
-14.8?11.2??
8.59?19.9?
-20.7?9.87?
?LGCP TXT 2.32?7.06?
-14.7?9.12?
3.66?5.67?
-16.9?5.91?LGCP ICM+TXT 2.31?7.80?
-14.6?10.8?
3.92?5.20?
-16.8?5.34?Table 1: MSE between the true counts and the predicted counts (lower is better) and predictive log likelihood of the truecounts from probabilistic models (higher is better) for test intervals over the 114 Ferguson rumours for extrapolation (left) andinterpolation (right) settings, showing mean ?
std.
dev.
Baselines are shown above the line, with LGCP models below.
Key:?
denotes significantly better than the best baseline; ?
denotes significantly worse than LGCP TXT, according to one-sidedWilcoxon signed rank test p < 0.05.proaches.
This is due to GP modelling a dis-tribution with continuous support, which is inap-propriate for modelling discrete counts.
Chang-ing the model from a GP to a better fitting to themodelling temporal count data LGCP gives a bigimprovement, even when a point estimate of theprediction is considered (MSE).
The 0 baseline isvery strong, since many rumours have compara-tively little discussion in the second hour of theirlifespan relative to the first hour.
Incorporating in-formation about other rumours helps outperformthis method.
ICM, TXT and ICM+TXT multi-task learning approaches achieve the best scoresand significantly outperform all baselines.
TXTturns out to be a good approach to multi-task learn-ing and outperforms ICM.
In Figure 1a we showan example rumour frequency profile for the ex-trapolation setting.
TXT makes a lower error thanLGCP and LGCPICM, both of which underesti-mate the counts in the second hour.Next, we move to the interpolation setting.
Un-surprisingly, Interpolate is the strongest baseline,and outperforms the raw LGCP method.
Again,HPP and GP are outperformed by LGCP in termsof both MSE and LL.
Considering the output dis-tributions (LL) the difference in performance be-tween the Poisson Process based approaches andGP is especially big, demonstrating how well theprincipled models handle uncertainty in the pre-dictive distributions.
As for the multi-task meth-ods, we notice that text is particularly useful, withTXT achieving the highest MSE score out of allconsidered models.
ICM turns out to be not veryhelpful in this setting.
For example, ICM (just asLGCP) does not learn there should be a peak at thebeginning of a rumour frequency profile depictedin Figure 1b.
TXT manages to make a signifi-cantly smaller error by predicting a large postingfrequency there.
We also found, that for a few ru-mours ICM made a big error by predicting a highfrequency at the start of a rumour lifespan whenthere was no such peak.
We hypothesize ICM per-forms poorly because it is hard to learn correct cor-relations between frequency profiles when trainingintervals do not form continuous segments of sig-nificant sizes.
ICM manages to learn correlationsmore properly in extrapolation setting, where thefirst hour is fully observed.7 ConclusionsThis paper introduced the problem of modellingfrequency profiles of rumours in social media.We demonstrated that joint modelling of collec-tive data over multiple rumours using multi-tasklearning resulted in more accurate models that areable to recognise and predict commonly occurringtemporal patterns.
We showed how text data fromsocial media posts added important informationabout similarities between different rumours.
Ourmethod is generalizable to problems other thanmodelling rumour popularity, such as predictingsuccess of advertisement campaigns.8 AcknowledgmentsWe would like to thank Srijith P. K. for helpfulcomments.
This work was funded by the PHEMEFP7 project (grant No.
611233) and partially sup-ported by the Australian Research Council.522ReferencesMauricio A.?Alvarez, Lorenzo Rosasco, and Neil D.Lawrence.
2012.
Kernels for vector-valued func-tions: A review.
Found.
Trends Mach.
Learn.,4(3):195?266.The GPy authors.
2012?2015.
GPy: A Gaussianprocess framework in Python.
http://github.com/SheffieldML/GPy.Daniel Beck, Trevor Cohn, and Lucia Specia.
2014.Joint emotion analysis via multi-task Gaussian pro-cesses.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?14, pages 1798?1803.Anders Brix and Peter J. Diggle.
2001.
Spatiotemporalprediction for log-gaussian cox processes.
Journalof the Royal Statistical Society Series B, 63(4):823?841.Trevor Cohn and Lucia Specia.
2013.
Modelling an-notator bias with multi-task Gaussian processes: Anapplication to machine translation quality estima-tion.
In 51st Annual Meeting of the Association forComputational Linguistics, ACL ?13, pages 32?42.Adrien Friggeri, Lada Adamic, Dean Eckles, and JustinCheng.
2014.
Rumor cascades.
In InternationalAAAI Conference on Weblogs and Social Media.Jesper M?ller and Anne Randi Syversveen.
1998.
LogGaussian Cox processes.
Scandinavian Journal ofStatistics, pages 451?482.Olutobi Owoputi, Chris Dyer, Kevin Gimpel, NathanSchneider, and Noah A. Smith.
2013.
Improvedpart-of-speech tagging for online conversational textwith word clusters.
In Proceedings of NAACL, pages380?390.Daniel Preotiuc-Pietro and Trevor Cohn.
2013.
A tem-poral model of text periodicities using Gaussian pro-cesses.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?13, pages 977?988.Rob Procter, Jeremy Crump, Susanne Karstedt, AlexVoss, and Marta Cantijoch.
2013.
Reading the riots:What were the police doing on twitter?
Policing andsociety, 23(4):413?436.Carl Edward Rasmussen and Christopher K. I.Williams.
2005.
Gaussian Processes for Ma-chine Learning (Adaptive Computation and Ma-chine Learning).
The MIT Press.Jarno Vanhatalo, Jaakko Riihim?aki, Jouni Hartikainen,Pasi Jyl?anki, Ville Tolvanen, and Aki Vehtari.
2013.Gpstuff: Bayesian modeling with Gaussian pro-cesses.
J. Mach.
Learn.
Res., 14(1):1175?1179.Shuang-Hong Yang and Hongyuan Zha.
2013.
Mix-ture of mutually exciting processes for viral diffu-sion.
In ICML (2), volume 28 of JMLR Proceedings,pages 1?9.Andrew Zammit-Mangion, Michael Dewar, VisakanKadirkamanathan, and Guido Sanguinetti.
2012.Point process modelling of the Afghan War Diary.Proceedings of the National Academy of Sciencesof the United States of America, 109(31):12414?12419.Zhe Zhao, Paul Resnick, and Qiaozhu Mei.
2015.Early detection of rumors in social media from en-quiry posts.
In International World Wide Web Con-ference Committee (IW3C2).Arkaitz Zubiaga, Maria Liakata, Rob Procter, KalinaBontcheva, and Peter Tolmie.
2015.
Towards de-tecting rumours in social media.
In AAAI Workshopon AI for Cities.523
