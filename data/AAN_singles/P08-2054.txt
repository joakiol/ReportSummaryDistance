Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 213?216,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsUnlexicalised Hidden Variable Models of Split Dependency Grammars?Gabriele Antonio MusilloDepartment of Computer Scienceand Department of LinguisticsUniversity of Geneva1211 Geneva 4, Switzerlandmusillo4@etu.unige.chPaola MerloDepartment of LinguisticsUniversity of Geneva1211 Geneva 4, Switzerlandmerlo@lettres.unige.chAbstractThis paper investigates transforms of splitdependency grammars into unlexicalisedcontext-free grammars annotated with hiddensymbols.
Our best unlexicalised grammarachieves an accuracy of 88% on the PennTreebank data set, that represents a 50%reduction in error over previously publishedresults on unlexicalised dependency parsing.1 IntroductionRecent research in natural language parsing hasextensively investigated probabilistic models ofphrase-structure parse trees.
As well as being themost commonly used probabilistic models of parsetrees, probabilistic context-free grammars (PCFGs)are the best understood.
As shown in (Klein andManning, 2003), the ability of PCFG models to dis-ambiguate phrases crucially depends on the expres-siveness of the symbolic backbone they use.Treebank-specific heuristics have commonly beenused both to alleviate inadequate independenceassumptions stipulated by naive PCFGs (Collins,1999; Charniak, 2000).
Such methods stand in sharpcontrast to partially supervised techniques that haverecently been proposed to induce hidden grammati-cal representations that are finer-grained than thosethat can be read off the parsed sentences in tree-banks (Henderson, 2003; Matsuzaki et al, 2005;Prescher, 2005; Petrov et al, 2006).
?Part of this work was done when Gabriele Musillo wasvisiting the MIT Computer Science and Artificial IntelligenceLaboratory, funded by a grant from the Swiss NSF (PBGE2-117146).
Many thanks to Michael Collins and Xavier Carrerasfor their insightful comments on the work presented here.This paper presents extensions of such gram-mar induction techniques to dependency grammars.Our extensions rely on transformations of depen-dency grammars into efficiently parsable context-free grammars (CFG) annotated with hidden sym-bols.
Because dependency grammars are reduced toCFGs, any learning algorithm developed for PCFGscan be applied to them.
Specifically, we use theInside-Outside algorithm defined in (Pereira andSchabes, 1992) to learn transformed dependencygrammars annotated with hidden symbols.
Whatdistinguishes our work from most previous work ondependency parsing is that our models are not lexi-calised.
Our models are instead decorated with hid-den symbols that are designed to capture both lex-ical and structural information relevant to accuratedependency parsing without having to rely on anyexplicit supervision.2 Transforms of Dependency GrammarsContrary to phrase-structure grammars that stipulatethe existence of phrasal nodes, dependency gram-mars assume that syntactic structures are connectedacyclic graphs consisting of vertices representingterminal tokens related by directed edges represent-ing dependency relations.
Such terminal symbolsare most commonly assumed to be words.
In our un-lexicalised models reported below, they are insteadassumed to be part-of-speech (PoS) tags.
A typicaldependency graph is illustrated in Figure 1 below.Various projective dependency grammars exem-plify the concept of split bilexical dependency gram-mar (SBG) defined in (Eisner, 2000).
1 SBGs are1An SBG is a tuple ?V,W,L,R?
such that:213R1rootkk]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]R1root/R1V BDBSSR1V BDBYYYYYYYYYYYYYk1rV BDBRRRlllR1V BDB/R1INDRRRR1INDlllL1V BDBRR0V BDB RpV BDB/R1NNPCRR1rIND R1IND/R1NNFRRRL1V BDB\L1NNPAll1rNNPC 0IND 1rNNFRRRRllll1lNNPA 0NNPC L1NNFRRR0NNF0NNPA L1NNF \L1DTElll1lDTE0DTENica hituu33 66Miles with 88the trumpetvvFigure 1: A projective dependency graph for the sentence Nica hit Miles with the trumpet paired with its second-orderunlexicalised derivation tree annotated with hidden variables.closely related to CFGs as they both define struc-tures that are rooted ordered projective trees.
Such aclose relationship is clarified in this section.It follows from the equivalence of finite au-tomata and regular grammars that any SBG canbe transformed into an equivalent CFG.
Let D =?V,W,L,R?
be a SBG and G = ?N,W,P, S?
aCFG.
To transform D into G we to define the setP of productions, the set N of non-terminals, andthe start symbol S as follows:?
For each v in W , transform the automaton Lvinto a right-linear grammar GLv whose startsymbol is L1v; by construction, GLv consists ofrules such as Lpv ?
u Lqv or Lpv ?
, where ter-minal symbols such as u belong to W and non-terminals such as Lpv correspond to the states ofthe Lv automaton; include all -productions inP , and, if a rule such as Lpv ?
u Lqv is in GLv ,include the rule Lpv ?
2lu Lqv in P .?
For each v in V , transform the automaton Rvinto a left-linear grammar GRv whose startsymbol is R1v; by construction, GRv consists?
V is a set of terminal symbols which include a distin-guished element root;?
L is a function that, for any v ?
W (= V ?
{ root}),returns a finite automaton that recognises the well-formedsequences in W ?
of left dependents of v;?
R is a function that, for each v ?
V , returns a finiteautomaton that recognises the well-formed sequences ofright dependents in W ?
for v.of rules such as Rpv ?
Rqv u or Rpv ?
,where terminal symbols such as u belongs toW and non-terminals such as Rpv correspondto the states of the Rv automaton; include all -productions in P , and, if a rule such as Rpv ?Rqv u is in GRv , include the rule Rpv ?
Rqv 2ruin P .?
For each symbol 2lu occurring in P , include theproductions 2lu ?
L1u 1lu, 1lu ?
0u R1u, and0u ?
u in P ; for each symbol 2ru in P , includethe productions 2ru ?
1ru R1u, 1ru ?
L1u 0u,and 0u ?
u in P .?
Set the start symbol S to R1root.2Parsing CFGs resulting from such transformsruns in O(n4).
The head index v decorating non-terminals such as 1lv, 1rv, 0v, Lpv and Rqv can be com-puted in O(1) given the left and right indices of thesub-string wi,j they cover.
3 Observe, however, thatif 2lv or 2rv derives wi,j , then v does not functionallydepend on either i or j.
Because it is possible for thehead index v of 2lv or 2rv to vary from i to j, v hasto be tracked by the parser, resulting in an overallO(n4) time complexity.In the following, we show how to transformour O(n4) CFGs into O(n3) grammars by ap-2CFGs resulting from such transformations can further benormalised by removing the -productions from P .3Indeed, if 1lv or 0v derives wi,j , then v = i; if 1rv deriveswi,j , then v = j; if wi,j is derived from Lpv , then v = j + 1;and if wi,j is derived from Rqv , then v = i?
1.214plying transformations, closely related to those in(McAllester, 1999) and (Johnson, 2007), that elimi-nate the 2lv and 2rv symbols.We only detail the elimination of the symbols 2rv.The elimination of the 2lv symbols can be derivedsymmetrically.
By construction, a 2rv symbol is theright successor of a non-terminalRpu.
Consequently,2rv can only occur in a derivation such as?
Rpu ?
` ?
Rqu 2rv ?
` ?
Rqu 1rv R1v ?.To substitute for the problematic 2rv non-terminal inthe above derivation, we derive the form Rqu 1rv R1vfrom Rpu/R1v R1v where Rpu/R1v is a new non-terminal whose right-hand side is Rqu 1rv.
We thustransform the above derivation into the derivation?
Rpu ?
` ?
Rpu/R1v R1v?
` ?
Rqu 1rv R1v ?.4Because u = i ?
1 and v = j if Rpu/R1v deriveswi,j , and u = j + 1 and v = i if Lpu\L1v deriveswi,j , the parsing algorithm does not have to trackany head indices and can consequently parse stringsin O(n3) time.The grammars described above can be furthertransformed to capture linear second-order depen-dencies involving three distinct head indices.
Asecond-order dependency structure is illustrated inFigure 1 that involves two adjacent dependents,Miles and with, of a single head, hit.To see how linear second-order dependencies canbe captured, consider the following derivation of asequence of right dependents of a head u:?
Rpu/R1v ?
` ?
Rqu 1rv ?
` ?
Rqu/R1w R1w 1rv ?.The form Rqu/R1w R1w 1v mentions three heads: uis the the head that governs both v and w, and wprecedes v. To encode the linear relationship be-tween w and v, we redefine the right-hand side ofRpu/R1v as Rqu/R1w ?R1w, 1rv?
and include the pro-duction ?R1w, 1rv?
?
R1w 1rv in the productions.The relationship between the dependents w and v ofthe head u is captured, because Rpu/R1v jointly gen-erates R1w and 1rv.5Any second-order grammar resulting from trans-forming the derivations of right and left dependents4Symmetrically, the derivation ?
Lpu ?
` ?
2lv Lqu ?
`?
L1v 1lv Lqu ?
involving the 2lv symbol is transformed into?
Lpu ?
` ?
L1v Lpu\L1v ?
` ?
L1v 1lv Lqu ?.5Symmetrically, to transform the derivation of a sequence ofleft dependents of u, we redefine the right-hand side of Lpu\L1vas ?1lv,L1w?
Lqu\L1w and include the production ?1lv,L1w?
?1lv L1w in the set of rules.in the way described above can be parsed in O(n3),because the head indices decorating its symbols canbe computed in O(1).In the following section, we show how to enrichboth our first-order and second-order grammars withhidden variables.3 Hidden Variable ModelsBecause they do not stipulate the existence ofphrasal nodes, commonly used unlabelled depen-dency models are not sufficiently expressive to dis-criminate between distinct projections of a givenhead.
Both our first-order and second-order gram-mars conflate distributionally distinct projections ifthey are projected from the same head.
6To capture various distinct projections of a head,we annotate each of the symbols that refers to it witha unique hidden variable.
We thus constrain the dis-tribution of the possible values of the hidden vari-ables in a linguistically meaningful way.
Figure 1 il-lustrates such constraints: the same hidden variableB decorates each occurrence of the PoS tag VBD ofthe head hit.Enforcing such agreement constraints betweenhidden variables provides a principled way to cap-ture not only phrasal information but also lexical in-formation.
Lexical pieces of information conveyedby a minimal projection such as 0V BDB in Figure 1will consistently be propagated through the deriva-tion tree and will condition the generation of theright and left dependents of hit.In addition, states such as p and q that decoratenon-terminal symbols such as Rpu or Lqu can alsocapture structural information, because they can en-code the most recent steps in the derivation history.In the models reported in the next section, thesestates are assumed to be hidden and a distributionover their possible values is automatically induced.4 Empirical Work and DiscussionThe models reported below were trained, validated,and tested on the commonly used sections from thePenn Treebank.
Projective dependency trees, ob-6As observed in (Collins, 1999), an unambiguous verbalhead such as prove bearing the VB tag may project a clause withan overt subject as well as a clause without an overt subject, butonly the latter is a possible dependent of subject control verbssuch as try.215Development Data ?
section 24 per word per sentenceFOM: q = 1, h = 1 75.7 9.9SOM: q = 1, h = 1 80.5 16.2FOM: q = 2, h = 2 81.9 17.4FOM: q = 2, h = 4 84.7 22.0SOM: q = 2, h = 2 84.3 21.5SOM: q = 1, h = 4 87.0 25.8Test Data ?
section 23 per word per sentence(Eisner and Smith, 2005) 75.6 NASOM: q = 1, h = 4 88.0 30.6(McDonald, 2006) 91.5 36.7Table 1: Accuracy results on the development and testdata set, where q denotes the number of hidden states andh the number of hidden values annotating a PoS tag in-volved in our first-order (FOM) and second-order (SOM)models.tained using the rules stated in (Yamada and Mat-sumoto, 2003), were transformed into first-order andsecond-order structures.
CFGs extracted from suchstructures were then annotated with hidden variablesencoding the constraints described in the previoussection and trained until convergence by means ofthe Inside-Outside algorithm defined in (Pereira andSchabes, 1992) and applied in (Matsuzaki et al,2005).
To efficiently decode our hidden variablemodels, we pruned the search space as in (Petrov etal., 2006).
To evaluate the performance of our mod-els, we report two of the standard measures: the perword and per sentence accuracy (McDonald, 2006).Figures reported in the upper section of Table 1measure the effect on accuracy of the transformswe designed.
Our baseline first-order model (q =1, h = 1) reaches a poor per word accuracy that sug-gests that information conveyed by bare PoS tags isnot fine-grained enough to accurately predict depen-dencies.
Results reported in the second line showsthat modelling adjacency relations between depen-dents as second-order models do is relevant to accu-racy.
The third line indicates that annotating boththe states and the PoS tags of a first-order modelwith two hidden values is sufficient to reach a per-formance comparable to the one achieved by a naivesecond-order model.
However, comparing the re-sults obtained by our best first-order models to theaccuracy achieved by our best second-order modelconclusively shows that first-order models exploitsuch dependencies to a much lesser extent.
Overall,such results provide a first solution to the problemleft open in (Johnson, 2007) as to whether second-order transforms are relevant to parsing accuracy ornot.The lower section of Table 1 reports the resultsachieved by our best model on the test data set andcompare them both to those obtained by the only un-lexicalised dependency model we know of (Eisnerand Smith, 2005) and to those achieved by the state-of-the-art dependency parser in (McDonald, 2006).While clearly not state-of-the-art, the performanceachieved by our best model suggests that massivelexicalisation of dependency models might not benecessary to achieve competitive performance.
Fu-ture work will lie in investigating the issue of lex-icalisation in the context of dependency parsing byweakly lexicalising our hidden variable models.ReferencesEugene Charniak.
2000.
A maximum-entropy-inspired parser.In NAACL?00.Michael John Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Universityof Pennsylvania.Jason Eisner and Noah A. Smith.
2005.
Parsing with soft andhard constraints on dependency length.
In IWPT?05.Jason Eisner.
2000.
Bilexical grammars and their cubic-timeparsing algorithms.
In H.Bunt and A. Nijholt, eds., Ad-vances in Probabilistic and Other Parsing Technologies,pages 29?62.
Kluwer Academic Publishers.Jamie Henderson.
2003.
Inducing history representations forbroad-coverage statistical parsing.
In NAACL-HLT?03.Mark Johnson.
2007.
Transforming projective bilexical de-pendency grammars into efficiently-parsable cfgs withunfold-fold.
In ACL?06.Dan Klein and Christopher D. Manning.
2003.
Accurate unlex-icalized parsing.
In ACL?03.Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii.
2005.Probabilistic CFG with latent annotations.
In ACL?05.David McAllester.
1999.
A reformulation of eisner andsatta?s cubit time parser for split head automata gram-mars.
http://ttic.uchicago.edu/d?mcallester.Ryan McDonald.
2006.
Discriminative Training and SpanningTree Algorithms for Dependency Parsing.
Ph.D. thesis,University of Pennsylvania.Fernando Pereira and Yves Schabes.
1992.
Inside-outside rees-timation form partially bracketed corpora.
In ACL?92.Slav Petrov, Leon Barrett Romain Thibaux, and Dan Klein.2006.
Learning accurate, compact, and interpretable treeannotation.
In ACL?06.Detlef Prescher.
2005.
Head-driven PCFGs with latent-headstatistics.
In IWPT?05.H.
Yamada and Y. Matsumoto.
2003.
Statistical dependencyanalysis with support vectore machines.
In IWPT?03.216
