Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 313?320,Sydney, July 2006. c?2006 Association for Computational LinguisticsExpressing Implicit Semantic Relations without SupervisionPeter D. TurneyInstitute for Information TechnologyNational Research Council CanadaM-50 Montreal RoadOttawa, Ontario, Canada, K1A 0R6peter.turney@nrc-cnrc.gc.caAbstractWe present an unsupervised learning al-gorithm that mines large text corpora forpatterns that express implicit semantic re-lations.
For a given input word pairYX :  with some unspecified semanticrelations, the corresponding output list ofpatterns mPP ,,1   is ranked accordingto how well each pattern iP  expresses therelations between X  and Y .
For exam-ple, given ostrich=X  and bird=Y , thetwo highest ranking output patterns are?X is the largest Y?
and ?Y such as theX?.
The output patterns are intended tobe useful for finding further pairs withthe same relations, to support the con-struction of lexicons, ontologies, and se-mantic networks.
The patterns are sortedby pertinence, where the pertinence of apattern iP  for a word pair YX :  is theexpected relational similarity between thegiven pair and typical pairs for iP .
Thealgorithm is empirically evaluated on twotasks, solving multiple-choice SAT wordanalogy questions and classifying seman-tic relations in noun-modifier pairs.
Onboth tasks, the algorithm achieves state-of-the-art results, performing signifi-cantly better than several alternative pat-tern ranking algorithms, based on tf-idf.1 IntroductionIn a widely cited paper, Hearst (1992) showedthat the lexico-syntactic pattern ?Y such as theX?
can be used to mine large text corpora forword pairs YX :  in which X is a hyponym (type)of Y.
For example, if we search in a large corpususing the pattern ?Y such as the X?
and we findthe string ?bird such as the ostrich?, then we caninfer that ?ostrich?
is a hyponym of ?bird?.
Ber-land and Charniak (1999) demonstrated that thepatterns ?Y?s X?
and ?X of the Y?
can be used tomine corpora for pairs YX :  in which X is ameronym (part) of Y (e.g., ?wheel of the car?
).Here we consider the inverse of this problem:Given a word pair YX :  with some unspecifiedsemantic relations, can we mine a large text cor-pus for lexico-syntactic patterns that express theimplicit relations between X  and Y ?
For exam-ple, if we are given the pair ostrich:bird, can wediscover the pattern ?Y such as the X??
We areparticularly interested in discovering high qualitypatterns that are reliable for mining further wordpairs with the same semantic relations.In our experiments, we use a corpus of webpages containing about 10105 ?
English words(Terra and Clarke, 2003).
From co-occurrencesof the pair ostrich:bird in this corpus, we cangenerate 516 patterns of the form ?X ... Y?
and452 patterns of the form ?Y ... X?.
Most of thesepatterns are not very useful for text mining.
Themain challenge is to find a way of ranking thepatterns, so that patterns like ?Y such as the X?are highly ranked.
Another challenge is to find away to empirically evaluate the performance ofany such pattern ranking algorithm.For a given input word pair YX :  with someunspecified semantic relations, we rank the cor-responding output list of patterns mPP ,,1   inorder of decreasing pertinence.
The pertinence ofa pattern iP  for a word pair YX :  is the expectedrelational similarity between the given pair andtypical pairs that fit iP .
We define pertinencemore precisely in Section 2.Hearst (1992) suggests that her work may beuseful for building a thesaurus.
Berland andCharniak (1999) suggest their work may be use-ful for building a lexicon or ontology, likeWordNet.
Our algorithm is also applicable tothese tasks.
Other potential applications and re-lated problems are discussed in Section 3.To calculate pertinence, we must be able tomeasure relational similarity.
Our measure isbased on Latent Relational Analysis (Turney,2005).
The details are given in Section 4.Given a word pair YX : , we want our algo-rithm to rank the corresponding list of patterns313mPP ,,1   according to their value for miningtext, in support of semantic network constructionand similar tasks.
Unfortunately, it is difficult tomeasure performance on such tasks.
Thereforeour experiments are based on two tasks that pro-vide objective performance measures.In Section 5, ranking algorithms are comparedby their performance on solving multiple-choiceSAT word analogy questions.
In Section 6, theyare compared by their performance on classify-ing semantic relations in noun-modifier pairs.The experiments demonstrate that ranking bypertinence is significantly better than several al-ternative pattern ranking algorithms, based ontf-idf.
The performance of pertinence on thesetwo tasks is slightly below the best performancethat has been reported so far (Turney, 2005), butthe difference is not statistically significant.We discuss the results in Section 7 and con-clude in Section 8.2 PertinenceThe relational similarity between two pairs ofwords, 11 :YX  and 22 :YX , is the degree towhich their semantic relations are analogous.
Forexample, mason:stone and carpenter:wood havea high degree of relational similarity.
Measuringrelational similarity will be discussed in Sec-tion 4.
For now, assume that we have a measureof the relational similarity between pairs ofwords, ??
):,:(sim 2211r YXYX .Let }:,,:{ 11 nn YXYXW =  be a set of wordpairs and let },,{ 1 mPPP =  be a set of patterns.The pertinence of pattern iP  to a word pairjj YX :  is the expected relational similarity be-tween a word pair kk YX : , randomly selectedfrom W  according to the probability distribution):(p ikk PYX , and the word pair jj YX : :),:(pertinence ijj PYX=?=nkkkjjikk YXYXPYX1r ):,:(sim):(pThe conditional probability ):(p ikk PYX  can beinterpreted as the degree to which the pairkk YX :  is representative (i.e., typical) of pairsthat fit the pattern iP .
That is, iP  is pertinent tojj YX :  if highly typical word pairs kk YX :  forthe pattern iP  tend to be relationally similar tojj YX : .Pertinence tends to be highest with patternsthat are unambiguous.
The maximum value of),:(pertinence ijj PYX  is attained when the pairjj YX :  belongs to a cluster of highly similarpairs and the conditional probability distribution):(p ikk PYX  is concentrated on the cluster.
Anambiguous pattern, with its probability spreadover multiple clusters, will have less pertinence.If a pattern with high pertinence is used fortext mining, it will tend to produce word pairsthat are very similar to the given word pair; thisfollows from the definition of pertinence.
Webelieve this definition is the first formal measureof quality for text mining patterns.Let ikf ,  be the number of occurrences in acorpus of the word pair kk YX :  with the patterniP .
We could estimate ):(p ikk PYX  as follows:==njijikikk ffPYX1,,):(pInstead, we first estimate ):(p kki YXP :==mjjkikkki ffYXP1,,):(pThen we apply Bayes?
Theorem:=?
?=njjjijjkkikkikkYXPYXYXPYXPYX1):p():p():p():p():p(We assume nYX jj 1):p( =  for all pairs in W :==njjjikkiikk YXPYXPPYX1):p():p():p(The use of Bayes?
Theorem and the assumptionthat nYX jj 1):p( =  for all word pairs is a wayof smoothing the probability ):(p ikk PYX , simi-lar to Laplace smoothing.3 Related WorkHearst (1992) describes a method for findingpatterns like ?Y such as the X?, but her methodrequires human judgement.
Berland andCharniak (1999) use Hearst?s manual procedure.Riloff and Jones (1999) use a mutual boot-strapping technique that can find patterns auto-matically, but the bootstrapping requires an ini-tial seed of manually chosen examples for eachclass of words.
Miller et al (2000) propose anapproach to relation extraction that was evalu-ated in the Seventh Message Understanding Con-ference (MUC7).
Their algorithm requires la-beled examples of each relation.
Similarly, Ze-lenko et al (2003) use a supervised kernelmethod that requires labeled training examples.Agichtein and Gravano (2000) also require train-ing examples for each relation.
Brin (1998) usesbootstrapping from seed examples of author:titlepairs to discover patterns for mining further pairs.Yangarber et al (2000) and Yangarber (2003)present an algorithm that can find patterns auto-matically, but it requires an initial seed of manu-ally designed patterns for each semantic relation.Stevenson (2004) uses WordNet to extract rela-tions from text, but also requires initial seed pat-terns for each relation.314Lapata (2002) examines the task of expressingthe implicit relations in nominalizations, whichare noun compounds whose head noun is derivedfrom a verb and whose modifier can be inter-preted as an argument of the verb.
In contrastwith this work, our algorithm is not restricted tonominalizations.
Section 6 shows that our algo-rithm works with arbitrary noun compounds andthe SAT questions in Section 5 include all ninepossible pairings of nouns, verbs, and adjectives.As far as we know, our algorithm is the firstunsupervised learning algorithm that can findpatterns for semantic relations, given only a largecorpus (e.g., in our experiments, about 10105 ?words) and a moderately sized set of word pairs(e.g., 600 or more pairs in the experiments), suchthat the members of each pair appear togetherfrequently in short phrases in the corpus.
Theseword pairs are not seeds, since the algorithmdoes not require the pairs to be labeled orgrouped; we do not assume they are homogenous.The word pairs that we need could be gener-ated automatically, by searching for word pairsthat co-occur frequently in the corpus.
However,our evaluation methods (Sections 5 and 6) bothinvolve a predetermined list of word pairs.
If ouralgorithm were allowed to generate its own wordpairs, the overlap with the predetermined listswould likely be small.
This is a limitation of ourevaluation methods rather than the algorithm.Since any two word pairs may have some rela-tions in common and some that are not shared,our algorithm generates a unique list of patternsfor each input word pair.
For example, ma-son:stone and carpenter:wood share the pattern?X carves Y?, but the patterns ?X nails Y?
and?X bends Y?
are unique to carpenter:wood.
Theranked list of patterns for a word pair YX :gives the relations between X and Y in the corpus,sorted with the most pertinent (i.e., characteristic,distinctive, unambiguous) relations first.Turney (2005) gives an algorithm for measur-ing the relational similarity between two pairs ofwords, called Latent Relational Analysis (LRA).This algorithm can be used to solve multiple-choice word analogy questions and to classifynoun-modifier pairs (Turney, 2005), but it doesnot attempt to express the implicit semantic rela-tions.
Turney (2005) maps each pair YX :  to ahigh-dimensional vector v .
The value of eachelement iv  in vis based on the frequency, forthe pair YX : , of a corresponding pattern iP .The relational similarity between two pairs,11 :YX  and 22 :YX , is derived from the cosine ofthe angle between their two vectors.
A limitationof this approach is that the semantic content ofthe vectors is difficult to interpret; the magnitudeof an element iv  is not a good indicator of howwell the corresponding pattern iP  expresses arelation of YX : .
This claim is supported by theexperiments in Sections 5 and 6.Pertinence (as defined in Section 2) builds onthe measure of relational similarity in Turney(2005), but it has the advantage that the semanticcontent can be interpreted; we can point to spe-cific patterns and say that they express the im-plicit relations.
Furthermore, we can use the pat-terns to find other pairs with the same relations.Hearst (1992) processed her text with a part-of-speech tagger and a unification-based con-stituent analyzer.
This makes it possible to usemore general patterns.
For example, instead ofthe literal string pattern ?Y such as the X?, whereX and Y are words, Hearst (1992) used the moreabstract pattern ?
0NP  such as 1NP ?, where iNPrepresents a noun phrase.
For the sake of sim-plicity, we have avoided part-of-speech tagging,which limits us to literal patterns.
We plan toexperiment with tagging in future work.4 The AlgorithmThe algorithm takes as input a set of word pairs}:,,:{ 11 nn YXYXW =  and produces as outputranked lists of patterns mPP ,,1   for each inputpair.
The following steps are similar to the algo-rithm of Turney (2005), with several changes tosupport the calculation of pertinence.1.
Find phrases: For each pair ii YX : , make alist of phrases in the corpus that contain the pair.We use the Waterloo MultiText System (Clarkeet al, 1998) to search in a corpus of about10105 ?
English words (Terra and Clarke, 2003).Make one list of phrases that begin with iX  andend with iY  and a second list for the oppositeorder.
Each phrase must have one to three inter-vening words between iX  and iY .
The first andlast words in the phrase do not need to exactlymatch iX  and iY .
The MultiText query languageallows different suffixes.
Veale (2004) has ob-served that it is easier to identify semantic rela-tions between nouns than between other parts ofspeech.
Therefore we use WordNet 2.0 (Miller,1995) to guess whether iX  and iY  are likely tobe nouns.
When they are nouns, we are relativelystrict about suffixes; we only allow variation inpluralization.
For all other parts of speech, weare liberal about suffixes.
For example, we allowan adjective such as ?inflated?
to match a nounsuch as ?inflation?.
With MultiText, the query?inflat*?
matches both ?inflated?
and ?inflation?.2.
Generate patterns: For each list of phrases,generate a list of patterns, based on the phrases.Replace the first word in each phrase with thegeneric marker ?X?
and replace the last wordwith ?Y?.
The intervening words in each phrase315may be either left as they are or replaced with thewildcard ?*?.
For example, the phrase ?carpenternails the wood?
yields the patterns ?X nails theY?, ?X nails * Y?, ?X * the Y?, and ?X * * Y?.Do not allow duplicate patterns in a list, but notethe number of times a pattern is generated foreach word pair ii YX :  in each order ( iX  first andiY  last or vice versa).
We call this the patternfrequency.
It is a local frequency count, analo-gous to term frequency in information retrieval.3.
Count pair frequency: The pair frequencyfor a pattern is the number of lists from the pre-ceding step that contain the given pattern.
It is aglobal frequency count, analogous to documentfrequency in information retrieval.
Note that apair ii YX :  yields two lists of phrases and hencetwo lists of patterns.
A given pattern might ap-pear in zero, one, or two of the lists for ii YX : .4.
Map pairs to rows: In preparation for build-ing a matrix X , create a mapping of word pairsto row numbers.
For each pair ii YX : , create arow for ii YX :  and another row for ii XY : .
If Wdoes not already contain }:,,:{ 11 nn XYXY  ,then we have effectively doubled the number ofword pairs, which increases the sample size forcalculating pertinence.5.
Map patterns to columns: Create a mappingof patterns to column numbers.
For each uniquepattern of the form ?X ... Y?
from Step 2, createa column for the original pattern ?X ... Y?
andanother column for the same pattern with X andY swapped, ?Y ... X?.
Step 2 can generate mil-lions of distinct patterns.
The experiment in Sec-tion 5 results in 1,706,845 distinct patterns,yielding 3,413,690 columns.
This is too manycolumns for matrix operations with today?s stan-dard desktop computer.
Most of the patterns havea very low pair frequency.
For the experiment inSection 5, 1,371,702 of the patterns have a pairfrequency of one.
To keep the matrix X  man-ageable, we drop all patterns with a pair fre-quency less than ten.
For Section 5, this leaves42,032 patterns, yielding 84,064 columns.
Tur-ney (2005) limited the matrix to 8,000 columns,but a larger pool of patterns is better for our pur-poses, since it increases the likelihood of findinggood patterns for expressing the semantic rela-tions of a given word pair.6.
Build a sparse matrix: Build a matrix X  insparse matrix format.
The value for the cell inrow i and column j is the pattern frequency of thej-th pattern for the the i-th word pair.7.
Calculate entropy: Apply log and entropytransformations to the sparse matrix X  (Lan-dauer and Dumais, 1997).
Each cell is replacedwith its logarithm, multiplied by a weight basedon the negative entropy of the correspondingcolumn vector in the matrix.
This gives moreweight to patterns that vary substantially in fre-quency for each pair.8.
Apply SVD: After log and entropy transforms,apply the Singular Value Decomposition (SVD)to X  (Golub and Van Loan, 1996).
SVD de-composes X  into a product of three matricesTVU?
, where U  and V  are in column or-thonormal form (i.e., the columns are orthogonaland have unit length) and ?
is a diagonal matrixof singular values (hence SVD).
If X  is of rankr , then ?
is also of rank r .
Let k?
, whererk < , be the diagonal matrix formed from thetop k  singular values, and let kU  and kV  be thematrices produced by selecting the correspond-ing columns from U  and V .
The matrixTkkk VU ?
is the matrix of rank k  that best ap-proximates the original matrix X , in the sensethat it minimizes the approximation errors(Golub and Van Loan, 1996).
Following Lan-dauer and Dumais (1997), we use 300=k .
Wemay think of this matrix Tkkk VU ?
as a smoothedversion of the original matrix.
SVD is used toreduce noise and compensate for sparseness(Landauer and Dumais, 1997).9.
Calculate cosines: The relational similaritybetween two pairs, ):,:(sim 2211r YXYX , isgiven by the cosine of the angle between theircorresponding row vectors in the matrixTkkk VU ?
(Turney, 2005).
To calculate perti-nence, we will need the relational similarity be-tween all possible pairs of pairs.
All of the co-sines can be efficiently derived from the matrixTkkkk )( ??
UU  (Landauer and Dumais, 1997).10.
Calculate conditional probabilities: UsingBayes?
Theorem (see Section 2) and the raw fre-quency data in the matrix X  from Step 6, beforelog and entropy transforms, calculate the condi-tional probability ):(p jii PYX  for every row(word pair) and every column (pattern).11.
Calculate pertinence: With the cosines fromStep 9 and the conditional probabilities fromStep 10, calculate ),:(pertinence jii PYX  forevery row ii YX :  and every column jP  forwhich 0):(p >jii PYX .
When 0):(p =jii PYX ,it is possible that 0),:(pertinence >jii PYX , butwe avoid calculating pertinence in these cases fortwo reasons.
First, it speeds computation, be-cause X  is sparse, so 0):(p =jii PYX  for mostrows and columns.
Second, 0):(p =jii PYX  im-plies that the pattern jP  does not actually appearwith the word pair ii YX :  in the corpus; we areonly guessing that the pattern is appropriate forthe word pair, and we could be wrong.
Thereforewe prefer to limit ourselves to patterns and wordpairs that have actually been observed in the cor-pus.
For each pair ii YX :  in W, output two sepa-rate ranked lists, one for patterns of the form?X ?
Y?
and another for patterns of the form316?Y ?
X?, where the patterns in both lists aresorted in order of decreasing pertinence to ii YX : .Ranking serves as a kind of normalization.
Wehave found that the relative rank of a pattern ismore reliable as an indicator of its importancethan the absolute pertinence.
This is analogous toinformation retrieval, where documents areranked in order of their relevance to a query.
Therelative rank of a document is more importantthan its actual numerical score (which is usuallyhidden from the user of a search engine).
Havingtwo separate ranked lists helps to avoid bias.
Forexample, ostrich:bird generates 516 patterns ofthe form ?X ... Y?
and 452 patterns of the form?Y ... X?.
Since there are more patterns of theform ?X ...
Y?, there is a slight bias towardsthese patterns.
If the two lists were merged, the?Y ... X?
patterns would be at a disadvantage.5 Experiments with Word AnalogiesIn these experiments, we evaluate pertinence us-ing 374 college-level multiple-choice wordanalogies, taken from the SAT test.
For eachquestion, there is a target word pair, called thestem pair, and five choice pairs.
The task is tofind the choice that is most analogous (i.e., hasthe highest relational similarity) to the stem.
Thischoice pair is called the solution and the otherchoices are distractors.
Since there are six wordpairs per question (the stem and the five choices),there are 22446374 =?
pairs in the input set W.In Step 4 of the algorithm, we double the pairs,but we also drop some pairs because they do notco-occur in the corpus.
This leaves us with 4194rows in the matrix.
As mentioned in Step 5, thematrix has 84,064 columns (patterns).
The sparsematrix density is 0.91%.To answer a SAT question, we generateranked lists of patterns for each of the six wordpairs.
Each choice is evaluated by taking the in-tersection of its patterns with the stem?s patterns.The shared patterns are scored by the average oftheir rank in the stem?s lists and the choice?s lists.Since the lists are sorted in order of decreasingpertinence, a low score means a high pertinence.Our guess is the choice with the lowest scoringshared pattern.Table 1 shows three examples, two questionsthat are answered correctly followed by one thatis answered incorrectly.
The correct answers arein bold font.
For the first question, the stem isostrich:bird and the best choice is (a) lion:cat.The highest ranking pattern that is shared by bothof these pairs is ?Y such as the X?.
The thirdquestion illustrates that, even when the answer isincorrect, the best shared pattern (?Y powered ** X?)
may be plausible.Word pair Best shared pattern Score1.
ostrich:bird(a) lion:cat ?Y such as the X?
1.0(b) goose:flock ?X * * breeding Y?
43.5(c) ewe:sheep ?X are the only Y?
13.5(d) cub:bear ?Y are called X?
29.0(e) primate:monkey ?Y is the * X?
80.02. traffic:street(a) ship:gangplank ?X * down the Y?
53.0(b) crop:harvest ?X * adjacent * Y?
248.0(c) car:garage ?X * a residential Y?
63.0(d) pedestrians:feet ?Y * accommodate X?
23.0(e) water:riverbed ?Y that carry X?
17.03. locomotive:train(a) horse:saddle ?X carrying * Y?
82.0(b) tractor:plow ?X pulled * Y?
7.0(c) rudder:rowboat ?Y * X?
319.0(d) camel:desert ?Y with two X?
43.0(e) gasoline:automobile ?Y powered * * X?
5.0Table 1.
Three examples of SAT questions.Table 2 shows the four highest ranking pat-terns for the stem and solution for the first exam-ple.
The pattern ?X lion Y?
is anomalous, but theother patterns seem reasonable.
The shared pat-tern ?Y such as the X?
is ranked 1 for both pairs,hence the average score for this pattern is 1.0, asshown in Table 1.
Note that the ?ostrich is thelargest bird?
and ?lions are large cats?, but thelargest cat is the Siberian tiger.Word pair ?X ... Y?
?Y ... X?ostrich:bird ?X is the largest Y?
?Y such as the X?
?X is * largest Y?
?Y such * the X?lion:cat ?X lion Y?
?Y such as the X?
?X are large Y?
?Y and mountain X?Table 2.
The highest ranking patterns.Table 3 lists the top five pairs in W that matchthe pattern ?Y such as the X?.
The pairs aresorted by ):(p PYX .
The pattern ?Y such as theX?
is one of 146 patterns that are shared by os-trich:bird and lion:cat.
Most of these shared pat-terns are not very informative.Word pair Conditional probabilityheart:organ 0.49342dodo:bird 0.08888elbow:joint 0.06385ostrich:bird 0.05774semaphore:signal 0.03741Table 3.
The top five pairs for ?Y such as the X?.In Table 4, we compare ranking patterns bypertinence to ranking by various other measures,mostly based on varieties of tf-idf (term fre-quency times inverse document frequency, acommon way to rank documents in informationretrieval).
The tf-idf measures are taken fromSalton and Buckley (1988).
For comparison, wealso include three algorithms that do not rank317patterns (the bottom three rows in the table).These three algorithms can answer the SATquestions, but they do not provide any kind ofexplanation for their answers.Algorithm Prec.
Rec.
F1 pertinence (Step 11) 55.7 53.5 54.62 log and entropy matrix(Step 7)43.5 41.7 42.63 TF = f, IDF = log((N-n)/n) 43.2 41.4 42.34 TF = log(f+1), IDF = log(N/n) 42.9 41.2 42.05 TF = f, IDF = log(N/n) 42.9 41.2 42.06 TF = log(f+1),IDF = log((N-n)/n)42.3 40.6 41.47 TF = 1.0, IDF = 1/n 41.5 39.8 40.68 TF = f, IDF = 1/n 41.5 39.8 40.69 TF = 0.5 + 0.5 * (f/F),IDF = log(N/n)41.5 39.8 40.610 TF = log(f+1), IDF = 1/n 41.2 39.6 40.411 p(X:Y|P) (Step 10) 39.8 38.2 39.012 SVD matrix (Step 8) 35.9 34.5 35.213 random 27.0 25.9 26.414 TF = 1/f, IDF = 1.0 26.7 25.7 26.215 TF = f, IDF = 1.0 (Step 6) 18.1 17.4 17.716 Turney (2005) 56.8 56.1 56.417 Turney and Littman (2005) 47.7 47.1 47.418 Veale (2004) 42.8 42.8 42.8Table 4.
Performance of various algorithms on SAT.All of the pattern ranking algorithms are givenexactly the same sets of patterns to rank.
Anydifferences in performance are due to the rankingmethod alone.
The algorithms may skip ques-tions when the word pairs do not co-occur in thecorpus.
All of the ranking algorithms skip thesame set of 15 of the 374 SAT questions.
Preci-sion is defined as the percentage of correct an-swers out of the questions that were answered(not skipped).
Recall is the percentage of correctanswers out of the maximum possible numbercorrect (374).
The F measure is the harmonicmean of precision and recall.For the tf-idf methods in Table 4, f is the pat-tern frequency, n is the pair frequency, F is themaximum f for all patterns for the given wordpair, and N is the total number of word pairs.
By?TF = f, IDF = n/1 ?, for example (row 8), wemean that f plays a role that is analogous to termfrequency and n/1  plays a role that is analogousto inverse document frequency.
That is, in row 8,the patterns are ranked in decreasing order ofpattern frequency divided by pair frequency.Table 4 also shows some ranking methodsbased on intermediate calculations in the algo-rithm in Section 4.
For example, row 2 in Table 4gives the results when patterns are ranked in or-der of decreasing values in the correspondingcells of the matrix X  from Step 7.Row 12 in Table 4 shows the results we wouldget using Latent Relational Analysis (Turney,2005) to rank patterns.
The results in row 12support the claim made in Section 3, that LRA isnot suitable for ranking patterns, although itworks well for answering the SAT questions (aswe see in row 16).
The vectors in LRA yield agood measure of relational similarity, but themagnitude of the value of a specific element in avector is not a good indicator of the quality of thecorresponding pattern.The best method for ranking patterns is perti-nence (row 1 in Table 4).
As a point of compari-son, the performance of the average seniorhighschool student on the SAT analogies is about57% (Turney and Littman, 2005).
The secondbest method is to use the values in the matrix Xafter the log and entropy transformations inStep 7 (row 2).
The difference between these twomethods is statistically significant with 95% con-fidence.
Pertinence (row 1) performs slightlybelow Latent Relational Analysis (row 16; Tur-ney, 2005), but the difference is not significant.Randomly guessing answers should yield an Fof 20% (1 out of 5 choices), but ranking patternsrandomly (row 13) results in an F of 26.4%.
Thisis because the stem pair tends to share more pat-terns with the solution pair than with the distrac-tors.
The minimum of a large set of randomnumbers is likely to be lower than the minimumof a small set of random numbers.6 Experiments with Noun-ModifiersIn these experiments, we evaluate pertinence onthe task of classifying noun-modifier pairs.
Theproblem is to classify a noun-modifier pair, suchas ?flu virus?, according to the semantic relationbetween the head noun (virus) and the modifier(flu).
For example, ?flu virus?
is classified as acausality relation (the flu is caused by a virus).For these experiments, we use a set of 600manually labeled noun-modifier pairs (Nastaseand Szpakowicz, 2003).
There are five generalclasses of labels with thirty subclasses.
We pre-sent here the results with five classes; the resultswith thirty subclasses follow the same trends(that is, pertinence performs significantly betterthan the other ranking methods).
The five classesare causality (storm cloud), temporality (dailyexercise), spatial (desert storm), participant(student protest), and quality (expensive book).The input set W consists of the 600 noun-modifier pairs.
This set is doubled in Step 4, butwe drop some pairs because they do not co-occurin the corpus, leaving us with 1184 rows in thematrix.
There are 16,849 distinct patterns with apair frequency of ten or more, resulting in 33,698columns.
The matrix density is 2.57%.318To classify a noun-modifier pair, we use a sin-gle nearest neighbour algorithm with leave-one-out cross-validation.
We split the set 600 times.Each pair gets a turn as the single testing exam-ple, while the other 599 pairs serve as trainingexamples.
The testing example is classified ac-cording to the label of its nearest neighbour inthe training set.
The distance between two noun-modifier pairs is measured by the average rank oftheir best shared pattern.
Table 5 shows the re-sulting precision, recall, and F, when rankingpatterns by pertinence.Class name Prec.
Rec.
F Class sizecausality 37.3 36.0 36.7 86participant 61.1 64.4 62.7 260quality 49.3 50.7 50.0 146spatial 43.9 32.7 37.5 56temporality 64.7 63.5 64.1 52all 51.3 49.5 50.2 600Table 5.
Performance on noun-modifiers.To gain some insight into the algorithm, weexamined the 600 best shared patterns for eachpair and its single nearest neighbour.
For each ofthe five classes, Table 6 lists the most frequentpattern among the best shared patterns for thegiven class.
All of these patterns seem appropri-ate for their respective classes.Class Most frequent pattern Example paircausality ?Y * causes X?
?cold virus?participant ?Y of his X?
?dream analysis?quality ?Y made of X?
?copper coin?spatial ?X * * terrestrial Y?
?aquatic mammal?temporality ?Y in * early X?
?morning frost?Table 6.
Most frequent of the best shared patterns.Table 7 gives the performance of pertinenceon the noun-modifier problem, compared tovarious other pattern ranking methods.
The bot-tom two rows are included for comparison; theyare not pattern ranking algorithms.
The bestmethod for ranking patterns is pertinence (row 1in Table 7).
The difference between pertinenceand the second best ranking method (row 2) isstatistically significant with 95% confidence.Latent Relational Analysis (row 16) performsslightly better than pertinence (row 1), but thedifference is not statistically significant.Row 6 in Table 7 shows the results we wouldget using Latent Relational Analysis (Turney,2005) to rank patterns.
Again, the results supportthe claim in Section 3, that LRA is not suitablefor ranking patterns.
LRA can classify the noun-modifiers (as we see in row 16), but it cannotexpress the implicit semantic relations that makean unlabeled noun-modifier in the testing setsimilar to its nearest neighbour in the training set.Algorithm Prec.
Rec.
F1 pertinence (Step 11) 51.3 49.5 50.22 TF = log(f+1), IDF = 1/n 37.4 36.5 36.93 TF = log(f+1), IDF = log(N/n) 36.5 36.0 36.24 TF = log(f+1),IDF = log((N-n)/n)36.0 35.4 35.75 TF = f, IDF = log((N-n)/n) 36.0 35.3 35.66 SVD matrix (Step 8) 43.9 33.4 34.87 TF = f, IDF = 1/n 35.4 33.6 34.38 log and entropy matrix(Step 7)35.6 33.3 34.19 TF = f, IDF = log(N/n) 34.1 31.4 32.210 TF = 0.5 + 0.5 * (f/F),IDF = log(N/n)31.9 31.7 31.611 p(X:Y|P) (Step 10) 31.8 30.8 31.212 TF = 1.0, IDF = 1/n 29.2 28.8 28.713 random 19.4 19.3 19.214 TF = 1/f, IDF = 1.0 20.3 20.7 19.215 TF = f, IDF = 1.0 (Step 6) 12.8 19.7 8.016 Turney (2005) 55.9 53.6 54.617 Turney and Littman (2005) 43.4 43.1 43.2Table 7.
Performance on noun-modifiers.7 DiscussionComputing pertinence took about 18 hours forthe experiments in Section 5 and 9 hours for Sec-tion 6.
In both cases, the majority of the time wasspent in Step 1, using MultiText (Clarke et al,1998) to search through the corpus of 10105 ?words.
MultiText was running on a Beowulfcluster with sixteen 2.4 GHz Intel Xeon CPUs.The corpus and the search index require aboutone terabyte of disk space.
This may seem com-putationally demanding by today?s standards, butprogress in hardware will soon allow an averagedesktop computer to handle corpora of this size.Although the performance on the SAT anal-ogy questions (54.6%) is near the level of theaverage senior highschool student (57%), there isroom for improvement.
For applications such asbuilding a thesaurus, lexicon, or ontology, thislevel of performance suggests that our algorithmcould assist, but not replace, a human expert.One possible improvement would be to addpart-of-speech tagging or parsing.
We have donesome preliminary experiments with parsing andplan to explore tagging as well.
A difficulty isthat much of the text in our corpus does not con-sist of properly formed sentences, since the textcomes from web pages.
This poses problems formost part-of-speech taggers and parsers.8 ConclusionLatent Relational Analysis (Turney, 2005) pro-vides a way to measure the relational similaritybetween two word pairs, but it gives us little in-sight into how the two pairs are similar.
In effect,319LRA is a black box.
The main contribution ofthis paper is the idea of pertinence, which allowsus to take an opaque measure of relational simi-larity and use it to find patterns that express theimplicit semantic relations between two words.The experiments in Sections 5 and 6 show thatranking patterns by pertinence is superior toranking them by a variety of tf-idf methods.
Onthe word analogy and noun-modifier tasks, perti-nence performs as well as the state-of-the-art,LRA, but pertinence goes beyond LRA by mak-ing relations explicit.AcknowledgementsThanks to Joel Martin, David Nadeau, and DenizYuret for helpful comments and suggestions.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collec-tions.
In Proceedings of the Fifth ACM Conferenceon Digital Libraries (ACM DL 2000), pages 85-94.Matthew Berland and Eugene Charniak.
1999.
Find-ing parts in very large corpora.
In Proceedings ofthe 37th Annual Meeting of the Association forComputational Linguistics (ACL-99), pages 57-64.Sergey Brin.
1998.
Extracting patterns and relationsfrom the World Wide Web.
In WebDB Workshopat the 6th International Conference on ExtendingDatabase Technology (EDBT-98), pages 172-183.Charles L.A. Clarke, Gordon V. Cormack, and Chris-topher R. Palmer.
1998.
An overview of MultiText.ACM SIGIR Forum, 32(2):14-15.Gene H. Golub and Charles F. Van Loan.
1996.
Ma-trix Computations.
Third edition.
Johns HopkinsUniversity Press, Baltimore, MD.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th International Conference on Computa-tional Linguistics (COLING-92), pages 539-545.Thomas K. Landauer and Susan T. Dumais.
1997.
Asolution to Plato?s problem: The latent semanticanalysis theory of the acquisition, induction, andrepresentation of knowledge.
Psychological Review,104(2):211-240.Maria Lapata.
2002.
The disambiguation of nominali-sations.
Computational Linguistics, 28(3):357-388.George A. Miller.
1995.
WordNet: A lexical databasefor English.
Communications of the ACM,38(11):39-41.Scott Miller, Heidi Fox, Lance Ramshaw, and RalphWeischedel.
2000.
A novel use of statistical pars-ing to extract information from text.
In Proceed-ings of the Sixth Applied Natural Language Proc-essing Conference (ANLP 2000), pages 226-233.Vivi Nastase and Stan Szpakowicz.
2003.
Exploringnoun-modifier semantic relations.
In Fifth Interna-tional Workshop on Computational Semantics(IWCS-5), pages 285-301.Ellen Riloff and Rosie Jones.
1999.
Learning diction-aries for information extraction by multi-levelbootstrapping.
In Proceedings of the 16th NationalConference on Artificial Intelligence (AAAI-99),pages 474-479.Gerard Salton and Chris Buckley.
1988.
Term weight-ing approaches in automatic text retrieval.
Informa-tion Processing and Management, 24(5):513-523.Mark Stevenson.
2004.
An unsupervised WordNet-based algorithm for relation extraction.
In Proceed-ings of the Fourth International Conference onLanguage Resources and Evaluation (LREC)Workshop, Beyond Named Entity Recognition: Se-mantic Labelling for NLP Tasks, Lisbon, Portugal.Egidio Terra and Charles L.A. Clarke.
2003.
Fre-quency estimates for statistical word similaritymeasures.
In Proceedings of the Human LanguageTechnology and North American Chapter of Asso-ciation of Computational Linguistics Conference(HLT/NAACL-03), pages 244-251.Peter D. Turney.
2005.
Measuring semantic similarityby latent relational analysis.
In Proceedings of theNineteenth International Joint Conference on Arti-ficial Intelligence (IJCAI-05), pages 1136-1141.Peter D. Turney and Michael L. Littman.
2005.
Cor-pus-based learning of analogies and semantic rela-tions.
Machine Learning, 60(1-3):251-278.Tony Veale.
2004.
WordNet sits the SAT: A knowl-edge-based approach to lexical analogy.
In Pro-ceedings of the 16th European Conference on Arti-ficial Intelligence (ECAI 2004), pages 606-612.Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen.
2000.
Unsupervised discoveryof scenario-level patterns for information extrac-tion.
In Proceedings of the Sixth Applied NaturalLanguage Processing Conference (ANLP 2000),pages 282-289.Roman Yangarber.
2003.
Counter-training in discov-ery of semantic patterns.
In Proceedings of the 41stAnnual Meeting of the Association for Computa-tional Linguistics (ACL-03), pages 343-350.Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-ardella.
2003.
Kernel methods for relation extrac-tion.
Journal of Machine Learning Research,3:1083-1106.320
