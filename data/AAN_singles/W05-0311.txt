Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 76?83,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsThe Reliability of Anaphoric Annotation, Reconsidered: Taking Ambiguityinto AccountMassimo Poesio and Ron ArtsteinUniversity of Essex,Language and Computation Group / Department of Computer ScienceUnited KingdomAbstractWe report the results of a study of thereliability of anaphoric annotation which(i) involved a substantial number of naivesubjects, (ii) used Krippendorff?s ?
in-stead of K to measure agreement, as re-cently proposed by Passonneau, and (iii)allowed annotators to mark anaphoric ex-pressions as ambiguous.1 INTRODUCTIONWe tackle three limitations with the current state ofthe art in the annotation of anaphoric relations.
Thefirst problem is the lack of a truly systematic study ofagreement on anaphoric annotation in the literature:none of the studies we are aware of (Hirschman,1998; Poesio and Vieira, 1998; Byron, 2003; Poe-sio, 2004) is completely satisfactory, either becauseonly a small number of coders was involved, orbecause agreement beyond chance couldn?t be as-sessed for lack of an appropriate statistic, a situationrecently corrected by Passonneau (2004).
The sec-ond limitation, which is particularly serious whenworking on dialogue, is our still limited understand-ing of the degree of agreement on references to ab-stract objects, as in discourse deixis (Webber, 1991;Eckert and Strube, 2001).The third shortcoming is a problem that affects alltypes of semantic annotation.
In all annotation stud-ies we are aware of,1 the fact that an expression maynot have a unique interpretation in the context of its1The one exception is Rosenberg and Binkowski (2004).occurrence is viewed as a problem with the anno-tation scheme, to be fixed by, e.g., developing suit-ably underspecified representations, as done partic-ularly in work on wordsense annotation (Buitelaar,1998; Palmer et al, 2005), but also on dialogue acttagging.
Unfortunately, the underspecification solu-tion only genuinely applies to cases of polysemy, nothomonymy (Poesio, 1996), and anaphoric ambigu-ity is not a case of polysemy.
Consider the dialogueexcerpt in (1):2 it?s not clear to us (nor was to ourannotators, as we?ll see below) whether the demon-strative that in utterance unit 18.1 refers to the ?badwheel?
or ?the boxcar?
; as a result, annotators?
judg-ments may disagree ?
but this doesn?t mean that theannotation scheme is faulty; only that what is beingsaid is genuinely ambiguous.
(1) 18.1 S: ....18.6 it turns out that the boxcarat Elmira18.7 has a bad wheel18.8 and they?re .. gonna startfixing that at midnight18.9 but it won?t be ready until 819.1 M: oh what a pain in the buttThis problem is encountered with all types of anno-tation; the view that all types of disagreement indi-cate a problem with the annotation scheme?i.e., thatsomehow the problem would disappear if only wecould find the right annotation scheme, or concen-trate on the ?right?
types of linguistic judgments?is, in our opinion, misguided.
A better approach2This example, like most of those in the rest of the paper, istaken from the first edition of the TRAINS corpus collected atthe University of Rochester (Gross et al, 1993).
The dialoguesare available at ftp://ftp.cs.rochester.edu/pub/papers/ai/92.tn1.trains_91_dialogues.txt.76is to find when annotators disagree because of in-trinsic problems with the text, or, even better, todevelop methods to identify genuinely ambiguousexpressions?the ultimate goal of this work.The paper is organized as follows.
We first brieflyreview previous work on anaphoric annotation andon reliability indices.
We then discuss our experi-ment with anaphoric annotation, and its results.
Fi-nally, we discuss the implications of this work.2 ANNOTATING ANAPHORAIt is not our goal at this stage to propose a newscheme for annotating anaphora.
For this study wesimply developed a coding manual for the purposesof our experiment, broadly based on the approachadopted in MATE (Poesio et al, 1999) and GNOME(Poesio, 2004), but introducing new types of annota-tion (ambiguous anaphora, and a simple form of dis-course deixis) while simplifying other aspects (e.g.,by not annotating bridging references).The task of ?anaphoric annotation?
discussed hereis related, although different from, the task of an-notating ?coreference?
in the sense of the so-calledMUCSS scheme for the MUC-7 initiative (Hirschman,1998).
This scheme, while often criticized, is stillwidely used, and has been the basis of coreferenceannotation for the ACE initiative in the past twoyears.
It suffers however from a number of prob-lems (van Deemter and Kibble, 2000), chief amongwhich is the fact that the one semantic relation ex-pressed by the scheme, ident, conflates a numberof relations that semanticists view as distinct: be-sides COREFERENCE proper, there are IDENTITYANAPHORA, BOUND ANAPHORA, and even PRED-ICATION.
(Space prevents a fuller discussion andexemplification of these relations here.
)The goal of the MATE and GNOME schemes (aswell of other schemes developed by Passonneau(1997), and Byron (2003)) was to devise instructionsappropriate for the creation of resources suitable forthe theoretical study of anaphora from a linguis-tic / psychological perspective, and, from a compu-tational perspective, for the evaluation of anaphoraresolution and referring expressions generation.
Thegoal is to annotate the discourse model resultingfrom the interpretation of a text, in the sense both of(Webber, 1979) and of dynamic theories of anaphora(Kamp and Reyle, 1993).
In order to do this, annota-tors must first of all identify the noun phrases whicheither introduce new discourse entities (discourse-new (Prince, 1992)) or are mentions of previouslyintroduced ones (discourse-old), ignoring those thatare used predicatively.
Secondly, annotators haveto specify which discourse entities have the sameinterpretation.
Given that the characterization ofsuch discourse models is usually considered partof the area of the semantics of anaphora, and thatthe relations to be annotated include relations otherthan Sidner?s (1979) COSPECIFICATION, we will usethe term ANNOTATION OF ANAPHORA for this task(Poesio, 2004), but the reader should keep in mindthat we are not concerned only with nominal expres-sions which are lexically anaphoric.3 MEASURING AGREEMENT ONANAPHORIC ANNOTATIONThe agreement coefficient which is most widelyused in NLP is the one called K by Siegel and Castel-lan (1988).
Howewer, most authors who attemptedanaphora annotation pointed out that K is not appro-priate for anaphoric annotation.
The only sensiblechoice of ?label?
in the case of (identity) anaphoraare anaphoric chains (Passonneau, 2004); but ex-cept when a text is very short, few annotators willcatch all mentions of the same discourse entity?mostforget to mark a few, which means that agreementas measured with K is always very low.
Follow-ing Passonneau (2004), we used the coefficient ?
ofKrippendorff (1980) for this purpose, which allowsfor partial agreement among anaphoric chains.33.1 Krippendorf?s alphaThe ?
coefficient measures agreement among a setof coders C who assign each of a set of items I toone of a set of distinct and mutually exclusive cat-egories K; for anaphora annotation the coders arethe annotators, the items are the markables in thetext, and the categories are the emerging anaphoricchains.
The coefficient measures the observed dis-agreement between the coders Do, and corrects for3We also tried a few variants of ?, but these differed from ?only in the third to fifth significant digit, well below any of theother variables that affected agreement.
In the interest of spacewe only report here the results obtained with ?.77chance by removing the amount of disagreement ex-pected by chance De.
The result is subtracted from 1to yield a final value of agreement.?
= 1?DoDeAs in the case of K, the higher the value of ?,the more agreement there is between the annotators.?
= 1 means that agreement is complete, and ?
= 0means that agreement is at chance level.What makes ?
particularly appropriate foranaphora annotation is that the categories are notrequired to be disjoint; instead, they must be or-dered according to a DISTANCE METRIC?a func-tion d from category pairs to real numbers that spec-ifies the amount of dissimilarity between the cate-gories.
The distance between a category and itself isalways zero, and the less similar two categories are,the larger the distance between them.
Table 1 givesthe formulas for calculating the observed and ex-pected disagreement for ?.
The amount of disagree-ment for each item i ?
I is the arithmetic mean of thedistances between the pairs of judgments pertainingto it, and the observed agreement is the mean of allthe item disagreements.
The expected disagreementis the mean of the distances between all the judg-ment pairs in the data, without regard to items.Do =1ic(c?1) ?i?I ?k?K ?k?
?K niknik?dkk?De =1ic(ic?1) ?k?K ?k?
?K nknk?dkk?c number of codersi number of itemsnik number of times item i is classified in category knk number of times any item is classified in category kdkk?
distance between categories k and k?Table 1: Observed and expected disagreement for ?3.2 Distance measuresThe distance metric is not part of the general defini-tion of ?, because different metrics are appropriatefor different types of categories.
For anaphora anno-tation, the categories are the ANAPHORIC CHAINS:the sets of markables which are mentions of thesame discourse entity.
Passonneau (2004) proposesa distance metric between anaphoric chains based onthe following rationale: two sets are minimally dis-tant when they are identical and maximally distantwhen they are disjoint; between these extremes, setsthat stand in a subset relation are closer (less distant)than ones that merely intersect.
This leads to the fol-lowing distance metric between two sets A and B.dAB =??????
?0 if A = B1/3 if A ?
B or B ?
A2/3 if A?B 6= /0, but A 6?
B and B 6?
A1 if A?B = /0We also tested distance metrics commonly usedin Information Retrieval that take the size of theanaphoric chain into account, such as Jaccard andDice (Manning and Schuetze, 1999), the ratio-nale being that the larger the overlap between twoanaphoric chains, the better the agreement.
Jac-card and Dice?s set comparison metrics were sub-tracted from 1 in order to get measures of distancethat range between zero (minimal distance, identity)and one (maximal distance, disjointness).dAB = 1?|A?B||A?B|(Jaccard)dAB = 1?2 |A?B||A|+ |B|(Dice)The Dice measure always gives a smaller distancethan the Jaccard measure, hence Dice always yieldsa higher agreement coefficient than Jaccard whenthe other conditions remain constant.
The differencebetween Dice and Jaccard grows with the size of thecompared sets.
Obviously, the Passonneau measureis not sensitive to the size of these sets.3.3 Computing the anaphoric chainsAnother factor that affects the value of the agree-ment coefficient?in fact, arguably the most impor-tant factor?is the method used for constructing fromthe raw annotation data the ?labels?
used for agree-ment computation, i.e., the anaphoric chains.
Weexperimented with a number of methods.
How-ever, since the raw data are highly dependent onthe annotation scheme, we will postpone discussingour chain construction methods until after we havedescribed our experimental setup and annotationscheme.
We will also discuss there how compar-isons are made when an ambiguity is marked.784 THE ANNOTATION STUDY4.1 The Experimental SetupMaterials.
The text annotated in the experimentwas dialogue 3.2 from the TRAINS 91 corpus.
Sub-jects were trained on dialogue 3.1.Tools.
The subjects performed their annotationson Viglen Genie workstations with LG Flatron mon-itors running Windows XP, using the MMAX 2 anno-tation tool (Mu?ller and Strube, 2003).4Subjects.
Eighteen paid subjects participated inthe experiment, all students at the University of Es-sex, mostly undergraduates from the Departments ofPsychology and Language and Linguistics.Procedure.
The subjects performed the experi-ment together in one lab, each working on a separatecomputer.
The experiment was run in two sessions,each consisting of two hour-long parts separated bya 30 minute break.
The first part of the first sessionwas devoted to training: subjects were given the an-notation manual and taught how to use the software,and then annotated the training text together.
Afterthe break, the subjects annotated the first half of thedialogue (up to utterance 19.6).
The second sessiontook place five days later.
In the first part we quicklypointed out some problems in the first session (forinstance reminding the subjects to be careful duringthe annotation), and then immediately the subjectsannotated the second half of the dialogue, and wroteup a summary.
The second part of the second sessionwas used for a separate experiment with a differentdialogue and a slightly different annotation scheme.4.2 The Annotation SchemeMMAX 2 allows for multiple types of markables;markables at the phrase, utterance, and turn lev-els were defined before the experiment.
All nounphrases except temporal ones were treated as phrasemarkables (Poesio, 2004).
Subjects were instructedto go through the phrase markables in order (us-ing MMAX 2?s markable browser) and mark eachof them with one of four attributes: ?phrase?
if itreferred to an object which was mentioned earlierin the dialogue; ?segment?
if it referred to a plan,4Available from http://mmax.eml-research.de/event, action, or fact discussed earlier in the dia-logue; ?place?
if it was one of the five railway sta-tions Avon, Bath, Corning, Dansville, and Elmira,explicitly mentioned by name; or ?none?
if it didnot fit any of the above criteria, for instance if it re-ferred to a novel object or was not a referential nounphrase.
(We included the attribute ?place?
in orderto avoid having our subjects mark pointers from ex-plicit place names.
These occur frequently in thedialogue?49 of the 151 markables?but are rather un-interesting as far as anaphora goes.)
For markablesdesignated as ?phrase?
or ?segment?
subjects wereinstructed to set a pointer to the antecedent, a mark-able at the phrase or turn level.
Subjects were in-structed to set more than one pointer in case of am-biguous reference.
Markables which were not givenan attribute or which were marked as ?phrase?
or?segment?
but did not have an antecedent specifiedwere considered to be data errors; data errors oc-curred in 3 out of the 151 markables in the dialogue,and these items were excluded from the analysis.We chose to mark antecedents using MMAX 2?spointers, rather than its sets, because pointers allowus to annotate ambiguity: an ambiguous phrase canpoint to two antecedents without creating an asso-ciation between them.
In addition, MMAX 2 makesit possible to restrict pointers to a particular level.In our scheme, markables marked as ?phrase?
couldonly point to phrase-level antecedents while mark-ables marked as ?segment?
could only point to turn-level antecedents, thus simplifying the annotation.As in previous studies (Eckert and Strube, 2001;Byron, 2003), we only allowed a constrained formof reference to discourse segments: our subjectscould only indicate turn-level markables as an-tecedents.
This resulted in rather coarse-grainedmarkings, especially when a single turn was longand included discussion of a number of topics.
Ina separate experiment we tested a more compli-cated annotation scheme which allowed a more fine-grained marking of reference to discourse segments.4.3 Computing anaphoric chainsThe raw annotation data were processed usingcustom-written Perl scripts to generate coreferencechains and calculate reliability statistics.The core of Passonneau?s proposal (Passonneau,2004) is her method for generating the set of dis-79tinct and mutually exclusive categories required by?
out of the raw data of anaphoric annotation.
Con-sidering as categories the immediate antecedentswould mean a disagreement every time two anno-tators mark different members of an anaphoric chainas antecedents, while agreeing that these differentantecedents are part of the same chain.
Passonneauproposes the better solution to view the emerginganaphoric chains themselves as the categories.
Andin a scheme where anaphoric reference is unambigu-ous, these chains are equivalence classes of mark-ables.
But we have a problem: since our annotationscheme allows for multiple pointers, these chainstake on various shapes and forms.Our solution is to associate each markable m withthe set of markables obtained by following the chainof pointers from m, and then following the pointersbackwards from the resulting set.
The rationale forthis method is as follows.
Two pointers to a singlemarkable never signify ambiguity: if B points to Aand C points to A then B and C are cospecificational;we thus have to follow the links up and then backdown.
However, two pointers from a single mark-able may signify ambiguity, so we should not followan up-link from a markable that we arrived at via adown-link.
The net result is that an unambiguousmarkable is associated with the set of all markablesthat are cospecificational with it on one of their read-ings; an ambiguous markable is associated with theset of all markables that are cospecificational with atleast one of its readings.
(See figure 1.
)UnambiguousAB C  @@IA 7?
{A,B,C}B 7?
{A,B,C}C 7?
{A,B,C}AmbiguousD EF@@I  D 7?
{D,F}E 7?
{E,F}F 7?
{D,E,F}Figure 1: Anaphoric chainsThis method of chain construction also allows toresolve apparent discrepancies between reference tophrase-level and turn-level markables.
Take for ex-ample the snippet below: many annotators markeda pointer from the demonstrative that in utteranceunit 4.2 to turn 3; as for that in utterance unit 4.3,some marked a pointer to the previous that, whileothers marked a pointer directly to turn 3.
(2) 3.1 M: and while it?s there itshould pick up the tanker4.1 S: okay4.2 and that can get4.3 we can get that done bythreeIn this case, not only do the annotators mark differ-ent direct antecedents for the second that; they evenuse different attributes??phrase?
when pointing to aphrase antecedent and ?segment?
when pointing toa turn.
Our method of chain construction associatesboth of these markings with the same set of threemarkables ?
the two that phrases and turn 3 ?
captur-ing the fact that the two markings are in agreement.54.4 Taking ambiguity into accountThe cleanest way to deal with ambiguity would beto consider each item for which more than one an-tecedent is marked as denoting a set of interpreta-tions, i.e., a set of anaphoric chains (Poesio, 1996),and to develop methods for comparing such setsof sets of markables.
However, while our instruc-tions to the annotators were to use multiple point-ers for ambiguity, they only followed these instruc-tions for phrase references; when indicating the ref-erents of discourse deixis, they often used multi-ple pointers to indicate that more than one turn hadcontributed to the development of a plan.
So, forthis experiment, we simply used as the interpreta-tion of markables marked as ambiguous the unionof the constituent interpretations.
E.g., a markable Emarked as pointing both to antecedent A, belongingto anaphoric chain {A,B}, and to antecedent C, be-longing to anaphoric chain {C,D}, would be treatedby our scripts as being interpreted as referring toanaphoric chain {A,B,C,D}.5 RESULTS5.1 Agreement on category labelsThe following table reports for each of the four cate-gories the number of cases (in the first half) in which5It would be preferable, of course, to get the annotators tomark such configurations in a uniform way; this however wouldrequire much more extensive training of the subjects, as well assupport which is currently unavailable from the annotation toolfor tracking chains of pointers.80a good number (18, 17, 16) annotators agreed on aparticular label?phrase, segment, place, or none?orno annotators assigned a particular label to a mark-able.
(The figures for the second half are similar.
)Number of judgments 18 17 16 0phrase 10 3 1 30segment 1 52place 16 1 1 54none 10 5 1 29Table 2: Cases of good agreement on categoriesIn other words, in 49 cases out of 72 at least 16annotators agreed on a label.5.2 Explicitly annotated ambiguity, and itsimpact on agreementNext, we attempted to get an idea of the amountof explicit ambiguity?i.e., the cases in which codersmarked multiple antecedents?and the impact on re-liability resulting by allowing them to do this.
Inthe first half, 15 markables out of 72 (20.8%) weremarked as explicitly ambiguous by at least one an-notator, for a total of 55 explicit ambiguity mark-ings (45 phrase references, 10 segment references);in the second, 8/76, 10.5% (21 judgments of ambi-guity in total).
The impact of these cases on agree-ment can be estimated by comparing the values ofK and ?
on the antecedents only, before the con-struction of cospecification chains.
Recall that thedifference between the coefficients is that K doesnot allow for partial disagreement while ?
gives itsome credit.
Thus if one subject marks markable Aas antecedent of an expression, while a second sub-ject marks markables A and B, K will register a dis-agreement while ?
will register partial agreement.Table 3 compares the values of K and ?, computedseparately for each half of the dialogue, first withall the markables, then by excluding ?place?
mark-ables (agreement on marking place names was al-most perfect, contributing substantially to overallagreement).
The value of ?
is somewhat higher thanthat of K, across all conditions.5.3 Agreement on anaphoraFinally, we come to the agreement values obtainedby using ?
to compare anaphoric chains computedWith place Without placeFirst Half K 0.62773 0.50066?
0.65615 0.53875Second Half K 0.66201 0.44997?
0.67736 0.47490The coefficient reported here as K is the one called K by Siegeland Castellan (1988).The value of ?
is calculated using Passonneau?s distance metric;for other distance metrics, see table 4.Table 3: Comparing K and ?as discussed above.
Table 4 gives the value of ?
forthe first half (the figures for the second half are sim-ilar).
The calculation of ?
was manipulated underthe following three conditions.Place markables.
We calculated the value of ?
onthe entire set of markables (with the exception ofthree which had data errors), and also on a subset ofmarkables ?
those that were not place names.
Agree-ment on marking place names was almost perfect:45 of the 48 place name markables were marked cor-rectly as ?place?
by all 18 subjects, two were markedcorrectly by all but one subject, and one was markedcorrectly by all but two subjects.
Place names thuscontributed substantially to the agreement amongthe subjects.
Dropping these markables from theanalysis resulted in a substantial drop in the valueof ?
across all conditions.Distance measure.
We used the three measuresdiscussed earlier to calculate distance between sets:Passonneau, Jaccard, and Dice.6Chain construction.
Substantial variation in theagreement values can be obtained by makingchanges to the way we construct anaphoric chains.We tested the following methods.NO CHAIN: only the immediate antecedents of ananaphoric expression were considered, insteadof building an anaphoric chain.PARTIAL CHAIN: a markable?s chain included onlyphrase markables which occurred in the dia-6For the nominal categories ?place?
and ?none?
we assigna distance of zero between the category and itself, and of onebetween a nominal category and any other category.81With place markables Without place markablesPass Jacc Dice Pass Jacc DiceNo chain 0.65615 0.64854 0.65558 0.53875 0.52866 0.53808Partial 0.67164 0.65052 0.67667 0.55747 0.53017 0.56477Inclusive [?top] 0.65380 0.64194 0.69115 0.53134 0.51693 0.58237Exclusive [?top] 0.62987 0.60374 0.64450 0.49839 0.46479 0.51830Inclusive [+top] 0.60193 0.58483 0.64294 0.49907 0.47894 0.55336Exclusive [+top] 0.57440 0.53838 0.58662 0.46225 0.41766 0.47839Table 4: Values of ?
for the first half of dialogue 3.2logue before the markable in question (as wellas all discourse markables).FULL CHAIN: chains were constructed by lookingupward and then back down, including allphrase markables which occurred in the dia-logue either before or after the markable inquestion (as well as the markable itself, and alldiscourse markables).We used two separate versions of the full chain con-dition: in the [+top] version we associate the top ofa chain with the chain itself, whereas in the [?top]version we associate the top of a chain with its orig-inal category label, ?place?
or ?none?.Passonneau (2004) observed that in the calcula-tion of observed agreement, two full chains alwaysintersect because they include the current item.
Pas-sonneau suggests to prevent this by excluding thecurrent item from the chain for the purpose of cal-culating the observed agreement.
We performed thecalculation both ways ?
the inclusive condition in-cludes the current item, while the exclusive condi-tion excludes it.The four ways of calculating ?
for full chains,plus the no chain and partial chain condition, yieldthe six chain conditions in Table 4.
Other things be-ing equal, Dice yields a higher agreement than Jac-card; considering both halves of the dialogue, thePassonneau measure always yielded a higher agree-ment that Jaccard, while being higher than Dice in10 of the 24 conditions, and lower in the remaining14 conditions.The exclusive chain conditions always give loweragreement values than the corresponding inclusivechain conditions, because excluding the current itemreduces observed agreement without affecting ex-pected agreement (there is no ?current item?
in thecalculation of expected agreement).The [?top] conditions tended to result in a higheragreement value than the corresponding [+top] con-ditions because the tops of the chains retained their?place?
and ?none?
labels; not surprisingly, the ef-fect was less pronounced when place markableswere excluded from the analysis.
Inclusive [?top]was the only full chain condition which gave ?
val-ues comparable to the partial chain and no chainconditions.
For each of the four selections of mark-ables, the highest ?
value was given by the Inclusive[?top] chain with Dice measure.5.4 Qualitative AnalysisThe difference between annotation of (identity!
)anaphoric relations and other semantic annotationtasks such as dialogue act or wordsense annotationis that apart from the occasional example of care-lessness, such as marking Elmira as antecedent forthe boxcar at Elmira,7 all other cases of disagree-ment reflect a genuine ambiguity, as opposed to dif-ferences in the application of subjective categories.8Lack of space prevents a full discussion of thedata, but some of the main points can already bemade with reference to the part of the dialogue in(2), repeated with additional context in (3).7According to our (subjective) calculations, at least one an-notator made one obvious mistake of this type for 20 items outof 72 in the first half of the dialogue?for a total of 35 carelessor mistaken judgment out of 1296 total judgments, or 2.7%.8Things are different for associative anaphora, see (Poesioand Vieira, 1998).82(3) 1.4 M: first thing I?d like you to do1.5 is send engine E2 off with a boxcarto Corning to pick up oranges1.6 uh as soon as possible2.1 S: okay [6 sec]3.1 M: and while it?s there itshould pick up the tankerThe two it pronouns in utterance unit 3.1 are exam-ples of the type of ambiguity already seen in (1).All of our subjects considered the first pronoun a?phrase?
reference.
9 coders marked the pronounas ambiguous between engine E2 and the boxcar, 6marked it as unambiguous and referring to engineE2, and 3 as unambiguous and referring to the box-car.
This example shows that when trying to de-velop methods to identify ambiguous cases it is im-portant to consider not only the cases of explicit am-biguity, but also so-called implicit ambiguity?casesin which subjects do not provide evidence of beingconsciously aware of the ambiguity, but the presenceof ambiguity is revealed by the existence of two ormore annotators in disagreement (Poesio, 1996).6 DISCUSSIONIn summary, the main contributions of this work sofar has been (i) to further develop the methodologyfor annotating anaphoric relations and measuring thereliability of this type of annotation, adopting ideasfrom Passonneau and taking ambiguity into account;and (ii) to run the most extensive study of reliabil-ity on anaphoric annotation todate, showing the im-pact of such choices.
Our future work includes fur-ther developments of the methodology for measur-ing agreement with ambiguous annotations and forannotating discourse deictic references.ACKNOWLEDGMENTSThis work was in part supported by EPSRC projectGR/S76434/01, ARRAU.
We wish to thank TonySanford, Patrick Sturt, Ruth Filik, Harald Clahsen,Sonja Eisenbeiss, and Claudia Felser.ReferencesP.
Buitelaar.
1998.
CoreLex : Systematic Polysemy andUnderspecification.
Ph.D. thesis, Brandeis University.D.
Byron.
2003.
Annotation of pronouns and their an-tecedents: A comparison of two domains.
TechnicalReport 703, University of Rochester.M.
Eckert and M. Strube.
2001.
Dialogue acts, synchro-nising units and anaphora resolution.
Journal of Se-mantics.D.
Gross, J. Allen, and D. Traum.
1993.
The TRAINS 91dialogues.
TRAINS Technical Note 92-1, ComputerScience Dept.
University of Rochester, June.L.
Hirschman.
1998.
MUC-7 coreference task definition,version 3.0.
In N. Chinchor, editor, In Proc.
of the 7thMessage Understanding Conference.H.
Kamp and U. Reyle.
1993.
From Discourse to Logic.D.
Reidel, Dordrecht.K.
Krippendorff.
1980.
Content Analysis: An introduc-tion to its Methodology.
Sage Publications.C.
D. Manning and H. Schuetze.
1999.
Foundations ofStatistical Natural Language Processing.
MIT Press.C.
Mu?ller and M. Strube.
2003.
Multi-level annotationin MMAX.
In Proc.
of the 4th SIGDIAL.M.
Palmer, H. Dang, and C. Fellbaum.
2005.
Mak-ing fine-grained and coarse-grained sense distinctions,both manually and automatically.
Journal of NaturalLanguage Engineering.
To appear.R.
J. Passonneau.
1997.
Instructions for applying dis-course reference annotation for multiple applications(DRAMA).
Unpublished manuscript., December.R.
J. Passonneau.
2004.
Computing reliability for coref-erence annotation.
In Proc.
of LREC, Lisbon.M.
Poesio and R. Vieira.
1998.
A corpus-based investi-gation of definite description use.
Computational Lin-guistics, 24(2):183?216, June.M.
Poesio, F. Bruneseaux, and L. Romary.
1999.
TheMATE meta-scheme for coreference in dialogues inmultiple languages.
In M. Walker, editor, Proc.
of theACL Workshop on Standards and Tools for DiscourseTagging, pages 65?74.M.
Poesio.
1996.
Semantic ambiguity and perceived am-biguity.
In K. van Deemter and S. Peters, editors, Se-mantic Ambiguity and Underspecification, chapter 8,pages 159?201.
CSLI, Stanford, CA.M.
Poesio.
2004.
The MATE/GNOME scheme foranaphoric annotation, revisited.
In Proc.
of SIGDIAL,Boston, May.E.
F. Prince.
1992.
The ZPG letter: subjects, def-initeness, and information status.
In S. Thompsonand W. Mann, editors, Discourse description: diverseanalyses of a fund-raising text, pages 295?325.
JohnBenjamins.A.
Rosenberg and E. Binkowski.
2004.
Augmenting thekappa statistic to determine interannotator reliabilityfor multiply labeled data points.
In Proc.
of NAACL.C.
L. Sidner.
1979.
Towards a computational theoryof definite anaphora comprehension in English dis-course.
Ph.D. thesis, MIT.S.
Siegel and N. J. Castellan.
1988.
Nonparametricstatistics for the Behavioral Sciences.
McGraw-Hill.K.
van Deemter and R. Kibble.
2000.
On coreferring:Coreference in MUC and related annotation schemes.Computational Linguistics, 26(4):629?637.
Squib.B.
L. Webber.
1979.
A Formal Approach to DiscourseAnaphora.
Garland, New York.B.
L. Webber.
1991.
Structure and ostension in the inter-pretation of discourse deixis.
Language and CognitiveProcesses, 6(2):107?135.83
