Coling 2010: Poster Volume, pages 1158?1166,Beijing, August 2010Shallow Information Extraction from Medical Forum DataParikshit Sondhi and Manish Gupta and ChengXiang Zhai and Julia HockenmaierDepartment of Computer ScienceUniversity of Illinois at Urbana Champaign{sondhi1, gupta58, czhai, juliahmr}@illinois.eduAbstractWe study a novel shallow information ex-traction problem that involves extractingsentences of a given set of topic cate-gories from medical forum data.
Givena corpus of medical forum documents,our goal is to extract two related typesof sentences that describe a biomedicalcase (i.e., medical problem descriptionsand medical treatment descriptions).
Suchan extraction task directly generates med-ical case descriptions that can be usefulin many applications.
We solve the prob-lem using two popular machine learningmethods Support Vector Machines (SVM)and Conditional Random Fields (CRF).We propose novel features to improve theaccuracy of extraction.
Experiment resultsshow that we can obtain an accuracy of upto 75%.1 IntroductionConventional information extraction tasks gener-ally aim at extracting finer granularity semanticinformation units such as entities and relations.While such detailed information is no doubt veryuseful, extraction of such information also tendsto be difficult especially when the mentions of theentities to be extracted do not conform to regularsyntactic patterns.In this paper, we relax this conventional goalof extraction and study an easier extraction taskwhere we aim at extracting sentences that belongto a set of predefined semantic categories.
That is,we take a sentence as a unit for extraction.
Specif-ically, we study this problem in the context of ex-tracting medical case description from medical fo-rums.A variety of medical health forums exist online.People use them to post their problems, get ad-vices from experienced patients, get second opin-ions from other doctors, or merely to vent out theirfrustration.Compared with well-structured sources such asWikipedia, forums are more valuable in the sensethat they contain first hand patient experienceswith richer information in terms of what treat-ments are better than others and why.
Besidesthis, on forums, patients explain their symptomsmuch more freely than those mentioned on rela-tively formal sources like Wikipedia.
And hence,forums are much more easier to understand for ana?
?ve user.However, even on targeted forums (which fo-cus on a single disease), data is quite unstruc-tured.
There is therefore a need to structure outthis information and present it in a form that candirectly be used for a variety of other informationextraction applications like the collecting of med-ical case studies pertaining to a particular disease,mining frequently discussed symptoms, identify-ing correlation between symptoms and treatments,etc.A typical medical case description tends to con-sist of two aspects:?
Physical Examination/Symptoms (PE):This covers current conditions and includesany condition that is the focus of currentdiscussion.
Note that if a drug causes anallergy, then we consider it as a PE andnot a medication.
Any condition that is thefocus of conversation, i.e.
around which1158treatments are being proposed or questionsare being asked is considered PE even if theuser is recounting their past experience.?
Medications (MED): Includes medicationsthe person is currently taking, or is intend-ing to take, or any medication on which thequestion is targeted.
Medications do not nec-essarily mean drugs.
Any measures (includ-ing avoiding of substances) taken to treat oravoid the symptoms are considered as medi-cation.
Sometimes, users also mention otherthings like constituents of the drug, howmuch of the drug to consume at a time, howto get access to a medication, how much itcosts, side effects of medications, other qual-ities of medications etc.Figure 1 shows an example of PE and MED la-belings.Figure 1: Example of PE and MED labelingsWe thus frame the problem of extracting med-ical case descriptions as extracting sentences thatdescribe any of these two aspects.
Specifically,the task is to identify sentences in each of the tworelated categories (i.e., PE and MED) from forumposts.
As an extraction task, this task is ?shal-lower?
than conventional information extractiontasks such as entity extraction in the sense thatwe extract a sentence as a unit, which makes theextraction task more tractable.
Indeed, the taskis more similar to sentence categorization.
How-ever, it also differs from a regular sentence cat-egorization task (e.g., sentiment analysis) in thatthe multiple categories are usually closely relatedand categorization of multiple sentences may bedependent in the sense that knowing the categoryof one sentence may influence our decision aboutthe category of another sentence nearby.
For ex-ample, knowing that a sentence is in the categoryPE should increase our belief that the next sen-tence is of category of PE or MED.We solve the problem using two popular ma-chine learning methods, Support Vector Machines(SVM) and Conditional Random Fields (CRF).We define and study a large set of features, includ-ing two kinds of novel features: (1) novel featuresbased on semantic generalization of terms, and (2)novel features specific to forums.Since this is a novel task, there is no existingdata set that we can use for evaluation.
We thuscreate a new data set for evaluation.
Experimentresults show that both groups of novel featuresare effective and can improve extraction accuracy.With the best configurations, we can obtain an ac-curacy of up to 75%, demonstrating feasibility ofautomatic extraction of medical case descriptionsfrom forums.2 Related workMedical data mining has been looked atleast sincethe early 2000s.
Cios and Moore (2002) em-phasize the uniqueness of medical data mining.They stress that data mining in medicine is dis-tinct from that in other fields, because the dataare heterogeneous, and special ethical, legal, andsocial constraints apply to private medical infor-mation.
Treatment recommendation systems havebeen built that use the structured data to diag-nose based on symptoms (Lazarus et al, 2001)and recommend treatments.
Holt et al(2005) pro-vide references to medical systems that use casebased reasoning methodologies for medical diag-nosis.
Huge amounts of medical data stored inclinical data warehouses can be used to detect pat-terns and relationships, which could provide newmedical knowledge (Lazarus et al, 2001).
In con-trast, we look at the problem of converting someof the unstructured medical text data present in fo-rum threads into structured symptoms and treat-ments.
This data can then be used by all of theabove mentioned applications.Structuring of unstructured text has been stud-ied by many works in the literature.
Auto-matic information extraction (Aone and Ramos-Santacruz, 2000; Buttler et al, 2001) and wrap-per induction techniques have been used for struc-turing web data.
Sarawagi (2008) and Laen-1159der et al (2002) offer comprehensive overviewsof information extraction and wrapper inductiontechniques respectively.
The main difference be-tween our work and main stream work on extrac-tion is that we extract sentences as units, whichis shallower but presumably more robust.
Heinzeet al (2002) state that the current state-of-the-art in NLP is suitable for mining information ofmoderate content depth across a diverse collec-tion of medical settings and specialties.
Zhouet al (2006), the authors perform information ex-traction from clinical medical records using a de-cision tree based classifier using resources such asWordNet 1, UMLS 2 etc.
They extract past medi-cal history and social behaviour from the records.In other related works, sentiment classifica-tion (Pang et al, 2002; Prabowo and Thelwall,2009; Cui et al, 2006; Dave et al, 2003) attemptsto categorize text based on polarity of sentimentsand is often applied at the sentence level (Kim andZhai, 2009).
Some work has also been done onextracting content from forum data.
This includesfinding question answer pairs (Cong et al, 2008)from online forums, auto-answering queries on atechnical forum (Feng et al, 2006), ranking an-swers (Harabagiu and Hickl, 2006) etc.
To thebest of our knowledge, this is the first work onshallow extraction from medical forum data.3 Problem formulationLet P = (s1, ...sn) be a sequence of sentencesin a forum post.
Given a set of interesting cate-gories C = {c1, ..., ck} that describe a medicalcase, our task is to extract sentences in each cat-egory from the post P .
That is, we would like toclassify each sentence si into one of the categoriesci or Background, which we treat as a special cat-egory meaning that the sentence is irrelevant toour extraction task.
Depending on specific appli-cations, a sentence may belong to more than onecategory.In this paper, we focus on extracting sen-tences of two related categories describing a med-ical case: (1) Physical Examination (PE), whichincludes sentences describing the condition of1http://wordnet.princeton.edu/2http://www.nlm.nih.gov/research/umlsa patient (i.e., roughly symptoms) (2) Medica-tions (MED), which includes sentences mention-ing medications (i.e., roughly treatment).
Thesesentences provide a basic description of a medi-cal case and can already be very useful if we canextract them.We chose to analyze at the sentence level be-cause a sentence provides enough context to de-tect the category accurately.
For example, de-tecting the categories at word level will not helpus to mark a sentence like ?I get very uncom-fortable after eating cheese?
as PE or mark asentence like ?It?s best to avoid cheese in thatcase?
as MED.
Here the problem is loosely repre-sented by a combination of ?uncomfortable eatingcheese?
and the solution is represented loosely by?avoid cheese?.
Indeed, in preliminary analysis,we found that most of the times, the postings con-sist of PE and MED type sentences.4 MethodsWe use SVMs and CRFs to learn classifiersto solve our problem.
SVMs represent ap-proaches that solve the problem as a classifi-cation/categorization task while CRFs solve theproblem as a sequence labeling task.
In this sec-tion, we provide the basics of SVMs and CRFs.4.1 Support Vector MachinesSVM first introduced in (Boser et al, 1992), area binary classifier that constructs a hyperplanewhich separates the training instances belongingto the two classes.
SVMs maximize the separa-tion margin between this hyperplane and the near-est training datapoints of any class.
The larger themargin, the lower the generalization error of theclassifier.
SVMs have been used to classify bothlinearly and non-linearly seperable data, and havebeen shown to outperform other popular classi-fiers like decision trees, Na?
?ve Bayes classifiers,k-nearest neighbor classifiers, etc.
We use SVMsas a representative classifier that does not considerdependencies between the predictions on multiplesentences.4.2 Conditional Random FieldsEach of the sentences in the postings can itselfcontain features which help us to categorize it.1160Besides this, statistical dependencies exist be-tween sentences.
Intuitively, a MED sentence willfollow a PE sentence with high probability, but theprobability of a PE sentence following an MEDsentence would be low.
Conditional random fieldsare graphical models that can capture such depen-dencies among input sentences.
A CRF model de-fines a conditional distribution p(y|x) where y isthe predicted category (label) and x is the set ofsentences (observations).
CRF is an undirectedgraphical model in which each vertex representsa random variable whose distribution is to be in-ferred, and each edge represents a dependency be-tween two random variables.
The observation xcan be dependent on the current hidden label y,previous n hidden labels and on any of the otherobservations in a n order CRF.
CRFs have beenshown to outperform other probabilistic graphicalmodels like Hidden Markov Models (HMMs) andMaximum Entropy Markov Models (MeMMs).Sutton and McCallum (2006) provide an excellenttutorial on CRFs.5 FeaturesTo perform our categorization task, we use the fol-lowing features.?
Word based features: This includes uni-grams, bigrams and trigrams in the currentsentence.
Each of the n-grams is mapped to aseparate boolean feature per sentence wherevalue is 1 if it appears in sentence and 0 oth-erwise.?
Semantic features: This includes UnifiedMedical Language System (UMLS3) seman-tic groups of words in the current sentence.UMLS is a prominent bio-medical domainontology.
It contains approximately a mil-lion bio-medical concepts grouped under 135semantic groups.
MMTX4 is a tool that al-lows mapping of free text into UMLS con-cepts and groups.
We use these 135 semanticgroups as our semantic features.
In order togenerate these features, we first process thissentence through MMTX API which pro-vides all the semantic groups that were found3http://www.nlm.nih.gov/research/umls/4http://mmtx.nlm.nih.gov/in the sentence.
Each of the semantic groupsbecomes a boolean feature.?
Position based features: We define twotypes of position based features: position ofthe current sentence in the post and positionof the current post in the thread.
These fea-tures are specific to the forum data.
We in-clude these features based on the observa-tions that first post usually contains conditionrelated sentences while subsequent posts of-ten contain treatment measures for the cor-responding condition.
Each of the positionnumber of a sentence in a post and a postin a thread is mapped to a boolean featurewhich gets fired for a sentence at a partic-ular position.
E.g.
For a sentence at po-sition i in a post, POSITION IN POST iwould be set to 1 while other features PO-SITION IN POST j where j 6= i would beset to 0.?
User based features: We include a booleanfeature which gets fired when the sentenceis a part of a post by the thread creator.This feature is important because most of theposts by a thread creator have a high proba-bility of being a PE.?
Tag based features(Edge features): We de-fine features on tags (PE/MED/Backgnd) ofprevious two sentences to capture local de-pendencies between sentences.
E.g., a setof medication related tags often follow a de-scription of a condition.
We use these fea-tures only for CRF based experiments.?
Morphological features: These include oneboolean feature each for presence of?
a capitalized word in the sentence?
an abbreviation in the sentence?
a number in the sentence?
a question mark in the sentence?
an exclamation mark in the sentence?
Length based features: We also consider thenumber of words in a sentence as a separatetype of feature.
Feature LENGTH i becomestrue for a sentence containing i words.1161Category Labeler 1 Labeler 2PE 513 517MED 286 280Background 695 697Table 1: Labeling results6 Experiments6.1 DatasetEvaluation of this new extraction task is chal-lenging as no test set is available.
To solvethis problem, we opted to created our own testset.
HealthBoards5 is a medical forum web por-tal that allows patients to discuss their ailments.We scraped 175 posts contained in 50 threads onallergy i.e., an average of 3.5 posts per threadand around 2 posts per user with a maximumof 9 posts by a particular user.
Two humanswere asked to tag this corpus as conditions (i.e.,PE category) or treatments (i.e., MED category)or none on a per sentence basis.
The corpusconsists of 1494 sentences.
Table 1 shows thelabeling results.
The data set is available at(http://timan.cs.uiuc.edu/downloads.html).
Alsothe labeling results match quite well (82.86%)with a Kappa statistic value of 0.73.
Occasion-ally (around 3%) PE and MED both occur in thesame sentence and the labelers chose to mark suchsentences as PE.
In the case when the two label-ers disagree, we manually analyzed the results andfurther chose one of them for our experiments.6.2 Evaluation methodologyFor evaluation, we use 5-fold cross validation.For CRFs, we used the Mallet6 toolkit and forSVM, we used SVM-Light7.
We experimentedby varying the size of the training set, with differ-ent feature sets, using two machine learning mod-els: SVMs and CRFs.
Our aim is to accuratelyclassify any sentence in a post as PE or MEDor background.
First we explore and identify thefeature sets that help us in attaining higher accu-racy.
Next, we identify the setting (sequence la-beling by CRFs or independent classification bySVMs) that works better to model our problem.5http://www.healthboards.com6http://mallet.cs.umass.edu/7http://svmlight.joachims.org/We present most of our results using four metrics:precision, recall, F1 measure and average accu-racy which is the ratio of correctly labeled sen-tences to the total sentences.We considered the following features: all the2647 words in the vocabulary (no stop-word re-moval or any other type of selection), 10858 bi-grams, 135 semantic groups from UMLS, two po-sition based features, one user based feature, twotag based features, four morphological featuresand one length based feature as described in theprevious section.
Thus our feature set is quiterich.
Note that other than the usual features, se-mantic, position-based and user-based features arespecific to the medical domain or to forum data.6.3 Basic ResultsFirst we considered word features, and learned alinear chain CRF model.
We added other sets offeatures one by one, and observed variations in ac-curacy.
Table 2 shows the accuracy in terms ofprecision, recall and F1.
Note that these results arefor an Order 1 linear-chain CRF.
Accuracy is mea-sured as ratio of the number of correct labelings ofPE, MED and background to the total number ofsentences in our dataset.
Notice that the MED ac-curacy values are in general quite low comparedto those of PE.
As we will discuss later, accuracyis low for MED because our word-based featuresare not discriminative enough for the MED cate-gory.From Table 2, we see that the accuracy keepsincreasing as we add semantic UMLS based fea-tures, position based features and morphologicalfeatures.
However, length based features (wordcount), user-based faetures, and bigrams do not re-sult in any improvements.
We also tried trigrams,but did not observe any accuracy gains.
Thus wefind that semantic features and position-based fea-tures which are specific to the medical domainand the forum data respectively are helpful whenadded on top of word features, while generic fea-tures such as length-based features tend to not addvalue.We also trained an order 2 CRF using the sameset of features.
Results obtained were similar toorder 1 CRFs and so we do not report them here.This shows that local dependencies are more im-1162Feature set PE Prec MED Prec PE Recall MED Recall PE F1 MED F1 Accuracy %Word 0.60 0.49 0.65 0.36 0.62 0.42 63.43+Semantic 0.61 0.52 0.68 0.37 0.64 0.43 65.05?+Position 0.63 0.54 0.7 0.34 0.66 0.42 65.45+Morphological 0.64 0.52 0.69 0.36 0.66 0.42 65.70+WordCount 0.62 0.51 0.70 0.33 0.66 0.40 65.23+Thread Creator 0.62 0.51 0.71 0.34 0.66 0.41 65.49+Bigrams 0.62 0.51 0.69 0.34 0.66 0.41 64.82Table 2: Order 1 Linear Chain CRF.
?Improvement over only word features significant at 0.05-level,using Wilcoxon?s signed-rank testportant in medical forum data and global depen-dencies do not add further signal.Further, we perform experiments using SVMsusing the same set of features.
Table 3 showsaccuracy results on SVM.
Again PE is detectedwith higher accuracy compared to MED.
UnlikeCRFs, SVMs do not incorporate the notion of lo-cal dependencies between sentences.
However,we observe that SVMs outperform CRFs, as is ev-ident from the results in Table 3.
This is interest-ing, since it suggests that the SVM accuracy canpotentially be further enhanced by incorporatingsuch dependency information (e.g.
in the formof new features).
We leave this as part of futurework.Figure 2 shows an example of a forum post(which talks about allergy to dogs) being taggedusing our CRF model.Figure 2: Tagging example of a forum post6.4 Feature selectionIncremental addition of different feature types didnot lead to substantial improvement in perfor-mance.
This suggests that none of the featureclasses contains all ?good?
features.
We there-fore perform feature selection based on informa-tion gain and choose the top 4253 features fromamong all the features discussed earlier, based ona threshold for the gain.
This results in improve-ment in the accuracy values over the previous bestresults (Table 4).Among the word feature set, we found thatimportant features were allergy, alergies, food,hives, allergic, sinus, bread.
Among bigrams, al-lergic to, ear infections, my throat, are allergic,to gluten, food allergies have high informationgain values.
Among the UMLS based se-mantic groups, we found that patf (PathologicFunction), dsyn (Disease or Syndrome), orch(Organic Chemical), phsu (Pharmacologic Sub-stance), sosy (Sign or Symptom) have high in-formation gain values.
Also looking at the wordcount feature, we notice that background sen-tences are generally short sentences.
All these fea-tures are clearly highly discriminative.6.5 Variation in training data sizeWe varied the amount of training data used forlearning the models to observe the variation inperformance with size of training data.
Table 5shows the variation in accuracy (PE F1, MEDF1 and average accuracy) for different sizes oftraining data using CRFs.
In general, we observethat accuracy improves as we increase the trainingdata, but the degree varies with the feature setsused.
We see similar trends in SVM also.
Theseresults show that it is possible to further improveprediction accuracy by obtaining additional train-ing data.6.6 Probing into the low MED accuracyAs observed in Tables 2 and 3, MED accuracyis quite low compared to PE accuracy.
We wishto gain a deeper insight into why the MED ac-curacy suffers.
Therefore, we plot the frequencyof words in sentences marked as PE or MED ver-sus the rank of the word as shown in the figure 3.We removed the stop words.
Observe that for PEthe curve is quite steep.
This indicates that there1163Feature set PE Prec MED Prec PE Recall MED Recall PE F1 MED F1 Accuracy %Word 0.65 0.52 0.71 0.28 0.68 0.36 66.13+Semantic 0.73 0.54 0.73 0.38 0.73 0.45 71.02?+Position 0.71 0.52 0.71 0.35 0.71 0.42 69.61+Morphological 0.72 0.53 0.72 0.38 0.72 0.44 70.28+WordCount 0.74 0.54 0.72 0.37 0.73 0.44 71.55+Thread Creator 0.74 0.56 0.72 0.39 0.73 0.46 72.02+Bigrams 0.75 0.54 0.72 0.40 0.74 0.46 71.69Table 3: SVM results.
?Improvement over only word features significant at 0.05-level, usingWilcoxon?s signed-rank testClassifier PE Prec PE Recall PE F1 MED Prec MED Recall MED F1 Accuracy %SVM (all* features) 0.72 0.53 0.72 0.38 0.72 0.44 70.28SVM (selected features) 0.75 0.75 0.75 0.61 0.33 0.44 75.08?CRF (all* features) 0.64 0.52 0.69 0.36 0.66 0.42 65.70CRF (selected features) 0.60 0.77 0.67 0.58 0.37 0.45 65.93?Table 4: Accuracy using the best feature set.
(*Word +Semantic +Position +Morphological features).
?Improvement over all* features significant at 0.05-level, using Wilcoxon?s signed-rank testare some discriminative words which have veryhigh frequency and so the word features observedin the training set alo get fired for sentences inthe test set with high probability.
While for MED,we observe that most of the words have very lowfrequencies.
This basically means that discrimi-native words for MED may not occur with goodenough frequency.
So, many of the word featuresthat show up in the training set may not appear inthe test data.
Hence, MED accuracy suffers.50607080f thetermPEMED010203040Frequencyof1 31 61 91 121151181211241271301331361391421451Rank of the termFigure 3: Freq of words vs rank for PE and MED6.7 Multi-class vs Single class categorizationNote that our task is quite different from plain sen-tence categorization task.
We observe that there isa dependence between the categories (PE/MED)that we are trying to predict per sentence.
For ex-ample, considering 100% training data, Table 6compares the precision, recall and F1 values whenPE MED Backgnd EOPPE 0.54 0.13 0.28 0.05MED 0.15 0.51 0.30 0.04Backgnd 0.18 0.08 0.54 0.20BOP 0.40 0.07 0.53 0.0Table 7: Transition probability valuesSVM and CRF are trained as single class classi-fiers using word+semantic features with the multi-class results obtained previously.
Results are gen-erally better when we do multi-class categoriza-tion versus single-class categorization.
This trendwas reflected for other featuresets also.6.8 Analysis of transition probabilitiesTable 7 shows the transition probabilities fromone category to another as calculated based on ourlabelled dataset.
BOP is beginning of posting andEOP is end of posting.
Note that posts often startwith a PE or a background sentence and often endwith a background sentence.
Also, consecutivesentences within a posting tend to belong to thesame category.6.9 Error analysisWe also perform some error analysis on results us-ing the best feature set.
Table 8 shows the confu-sion matrix for CRF/SVM.
We observe many ofthe MED errors are because an MED sentence of-ten gets marked as PE.
This basically happens be-cause some sentences contain both PE and MED.1164Feature set 25% 50% 75% 100%Word 0.59/0.21/0.57 0.6/0.36/0.60 0.61/0.39/0.62 0.62/0.42/0.63+Semantic 0.61/0.17/0.59 0.63/0.32/0.61 0.64/0.38/0.63 0.64/0.43/0.65+Position 0.59/0.18/0.56 0.64/0.29/0.60 0.65/0.33/0.62 0.66/0.42/0.65+Morphological 0.6/0.19/0.57 0.64/0.32/0.61 0.65/0.37/0.63 0.66/0.42/0.65Best 0.61/0.18/0.65 0.66/0.28/0.64 0.66/0.38/0.66 0.69/0.43/0.68Table 5: Precision, recall, and F value for various sizes of training data set.Classifier Type PE Prec PE Recall PE F1 MED Prec MED Recall MED F1SVM PE vs BKG 0.79 0.64 0.71 - - -SVM MED vs BKG - - - 0.6 0.28 0.39SVM Multi-class 0.73 0.73 0.73 0.54 0.38 0.45CRF PE vs BKG 0.68 0.64 0.66 - - -CRF MED vs BKG - - - 0.53 0.3 0.39CRF Multi-class 0.61 0.68 0.64 0.52 0.37 0.43Table 6: Multi-class vs Single-class categorization with word+semantic featuresPE MED BackgndPE 424/404 37/37 81/101MED 102/70 107/95 81/125Backgnd 164/62 55/21 618/754Table 8: Confusion matrix showing counts ofactual vs predicted labels for (Best CRF Classi-fier/Best SVM Classifier)Other than that some of the PE keywords are alsopresent in MED sentences, and since the few dis-criminative MED keywords are quite low in fre-quency, MED accuracy suffers.
E.g.
The sen-tence ?i?m still on antibiotics for the infection butthey don?t seem to be doing any good anymore.
?was labeled as MED but marked as PE by theCRF.
The sentence clearly talks about a medica-tion.
However, the keyword ?infection?
is oftenobserved in PE sentences and so the CRF marksthe sentence as PE.7 ConclusionIn this paper, we studied a novel shallow infor-mation extraction task where the goal is to extractrelevant sentences to a predefined set of categoriesthat describe a medical case.
We proposed tosolve the problem using supervised learning andexplored two representative approaches (i.e., CRFand SVM).
We proposed and studied two differenttypes of novel features for this task, including gen-eralized terms and forum structure features.
Wealso created the first test set for evaluating thisproblem.
Our experiment results show that (1) theproposed new features are effective for improvingthe extraction accuracy, and (2) it is feasible to au-tomatically extract medical cases in this way, withthe best prediction accuracy above 75%.Our work can be further extended in severalways.
First, since constructing a test set is labor-intensive, we could only afford experimentingwith a relatively small data set.
It would be in-teresting to further test the proposed features onlarger data set.
Second, while in CRF, we haveshown adding dependency features improves per-formance, it is unclear how to evaluate this po-tential benefit with SVM.
Since SVM generallyoutperforms CRF for this task, it would be veryinteresting to further explore how we can extendSVM to incorporate dependency.8 AcknowledgementWe thank the anonymous reviewers for their use-ful comments.
This paper is based upon work sup-ported in part by an IBM Faculty Award, an AlfredP.
Sloan Research Fellowship, an AFOSR MURIGrant FA9550-08-1-0265, and by the NationalScience Foundation under grants IIS-0347933,IIS-0713581, IIS-0713571, and CNS-0834709.ReferencesAone, Chinatsu and Mila Ramos-Santacruz.
2000.Rees: a large-scale relation and event extraction sys-tem.
In ANLP.Boser, Bernhard E., Isabelle Guyon, and Vladimir Vap-nik.
1992.
A training algorithm for optimal mar-1165gin classifiers.
In Computational Learing Theory,pages 144?152.Buttler, David, Ling Liu, and Calton Pu.
2001.
A fullyautomated object extraction system for the worldwide web.
In ICDCS.Cios, Krzysztof J. and William Moore.
2002.
Unique-ness of medical data mining.
Artificial Intelligencein Medicine, 26:1?24.Cong, Gao, Long Wang, Chin-Yew Lin, Young-InSong, and Yueheng Sun.
2008.
Finding question-answer pairs from online forums.
In SIGIR ?08:Proceedings of the 31st annual international ACMSIGIR conference on Research and development ininformation retrieval, pages 467?474, New York,NY, USA.
ACM.Cui, Hang, Vibhu Mittal, and Mayur Datar.
2006.Comparative Experiments on Sentiment Classifica-tion for Online Product Reviews.
In Proc.
of the Na-tional Conf.
on Artificial Intelligence, pages 1265?1270.Dave, Kushal, Steve Lawrence, and David M. Pen-nock.
2003.
Mining the Peanut Gallery: OpinionExtraction and Semantic Classification of ProductReviews.
In Proc.
of WWW, pages 519?528.Feng, Donghui, Erin Shaw, Jihie Kim, and EduardHovy.
2006.
An intelligent discussion-bot for an-swering student queries in threaded discussions.
InIUI ?06: Proceedings of the 11th international con-ference on Intelligent user interfaces, pages 171?177, New York, NY, USA.
ACM.Harabagiu, Sanda and Andrew Hickl.
2006.
Methodsfor using textual entailment in open-domain ques-tion answering.
In ACL-44: Proceedings of the 21stInternational Conference on Computational Lin-guistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 905?912, Morristown, NJ, USA.
Association for Com-putational Linguistics.Heinze, Daniel T., Mark L. Morsch, and John Hol-brook.
2002.
Mining free-text medical records.
InProceedings of the AMIA Annual Symposium.Holt, Alec, Isabelle Bichindaritz, Rainer Schmidt, andPetra Perner.
2005.
Medical applications in case-based reasoning.
Knowl.
Eng.
Rev., 20(3):289?292.Kim, Hyun Duk and ChengXiang Zhai.
2009.
Gener-ating comparative summaries of contradictory opin-ions in text.
In CIKM, pages 385?394.Laender, Alberto H. F., Berthier A. Ribeiro-neto, Alti-gran S. da Silva, and Juliana S. Teixeira.
2002.
Abrief survey of web data extraction tools.
SIGMODRecord.Lazarus, R, K P Kleinman, I Dashevsky, A DeMaria,and R Platt.
2001.
Using automated medicalrecords for rapid identification of illness syndromes(syndromic surveillance): the example of lower res-piratory infection.
BMC Public Health, 1:9.Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment Classification usingMachine Learning techniques.
In Proc.
of EMNLP,pages 79?86.Prabowo, Rudy and Mike Thelwall.
2009.
Sentimentanalysis: A combined approach.
Journal of Infor-metrics, 3(2):143?157, April.Sarawagi, Sunita.
2008.
Information extraction.Foundations and Trends in Databases, 1.Sutton, Charles and Andrew Mccallum, 2006.
In-troduction to Conditional Random Fields for Rela-tional Learning.
MIT Press.Zhou, Xiaohua, Hyoil Han, Isaac Chankai, Ann Pre-strud, and Ari Brooks.
2006.
Approaches to textmining for clinical medical records.
In SAC ?06:Proceedings of the 2006 ACM symposium on Ap-plied computing, pages 235?239, New York, NY,USA.
ACM.1166
