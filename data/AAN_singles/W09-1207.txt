Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49?54,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMultilingual Dependency-based Syntactic and Semantic ParsingWanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, Ting LiuInformation Retrieval LabSchool of Computer Science and TechnologyHarbin Institute of Technology, China, 150001{car, lzh, yqli, yhguo, qinb, tliu}@ir.hit.edu.cnAbstractOur CoNLL 2009 Shared Task system in-cludes three cascaded components: syntacticparsing, predicate classification, and semanticrole labeling.
A pseudo-projective high-ordergraph-based model is used in our syntactic de-pendency parser.
A support vector machine(SVM) model is used to classify predicatesenses.
Semantic role labeling is achieved us-ing maximum entropy (MaxEnt) model basedsemantic role classification and integer linearprogramming (ILP) based post inference.
Fi-nally, we win the first place in the joint task,including both the closed and open challenges.1 System ArchitectureOur CoNLL 2009 Shared Task (Hajic?
et al, 2009):multilingual syntactic and semantic dependenciessystem includes three cascaded components: syn-tactic parsing, predicate classification, and semanticrole labeling.2 Syntactic Dependency ParsingWe extend our CoNLL 2008 graph-basedmodel (Che et al, 2008) in four ways:1.
We use bigram features to choose multiple pos-sible syntactic labels for one arc, and decide the op-timal label during decoding.2.
We extend the model with sibling features (Mc-Donald, 2006).3.
We extend the model with grandchildren fea-tures.
Rather than only using the left-most and right-most grandchildren as Carreras (2007) and Johans-son and Nugues (2008) did, we use all left and rightgrandchildren in our model.4.
We adopt the pseudo-projective approach in-troduced in (Nivre and Nilsson, 2005) to handle thenon-projective languages including Czech, Germanand English.2.1 Syntactic Label DeterminingThe model of (Che et al, 2008) decided one la-bel for each arc before decoding according to uni-gram features, which caused lower labeled attach-ment score (LAS).
On the other hand, keeping allpossible labels for each arc made the decoding in-efficient.
Therefore, in the system of this year, weadopt approximate techniques to compromise, asshown in the following formulas.f lbluni(h, c, l) = f lbl1 (h, 1, d, l) ?
f lbl1 (c, 0, d, l)L1(h, c) = arg maxK1l?L(w ?
f lbluni(h, c, l))f lblbi (h, c, l) = f lbl2 (h, c, l)L2(h, c) = arg maxK2l?L1(h,c)(w ?
{f lbluni ?
f lblbi })For each arc, we firstly use unigram features tochoose the K1-best labels.
The second parameter off lbl1 (?)
indicates whether the node is the head of thearc, and the third parameter indicates the direction.L denotes the whole label set.
Then we re-rank thelabels by combining the bigram features, and chooseK2-best labels.
During decoding, we only use theK2 labels chosen for each arc (K2 ?
K1 < |L|).2.2 High-order Model and AlgorithmFollowing the Eisner (2000) algorithm, we use spansas the basic unit.
A span is defined as a substringof the input sentence whose sub-tree is already pro-duced.
Only the start or end words of a span can linkwith other spans.
In this way, the algorithm parsesthe left and the right dependence of a word indepen-dently, and combines them in the later stage.We follow McDonald (2006)?s implementation offirst-order Eisner parsing algorithm by modifying itsscoring method to incorporate high-order features.Our extended algorithm is shown in Algorithm 1.There are four different span-combining opera-tions.
Here we explain two of them that correspondto right-arc (s < t), as shown in Figure 1 and 2.
We49Algorithm 1 High-order Eisner Parsing Algorithm1: C[s][s][c] = 0, 0 ?
s ?
N , c ?
cp, icp # cp: complete; icp: incomplete2: for j = 1 to N do3: for s = 0 to N do4: t = s+ jL5: if t > N then6: break7: end if# Create incomplete spans8: C[s][t][icp] = maxs?r<t;l?L2(s,t)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(s, r, t, l))9: C[t][s][icp] = maxs?r<t;l?L2(t,s)(C[s][r][cp] + C[t][r + 1][cp] + Sicp(t, r, s, l))# Create complete spans10: C[s][t][cp] = maxs<r?t;l=C[s][r][icp].label(C[s][r][icp] + C[r][t][cp] + Scp(s, r, t, l))11: C[t][s][cp] = maxs?r<t;l=C[t][r][icp].label(C[r][s][cp] + C[t][r][icp] + Scp(t, r, s, l))12: end for13: end forfollow the way of (McDonald, 2006) and (Carreras,2007) to represent spans.
The other two operationscorresponding to left-arc are similar.Figure 1: Combining two spans into an incomplete spanFigure 1 illustrates line 8 of the algorithm in Al-gorithm 1, which combines two complete spans intoan incomplete span.
A complete span means thatonly the head word can link with other words fur-ther, noted as ???
or ???.
An incomplete spanindicates that both the start and end words of thespan will link with other spans in the future, noted as?99K?
or ?L99?.
In this operation, we combine twosmaller spans, sps?r and spr+1?t, into sps99Kt withadding arcs?t.
As shown in the following formu-las, the score of sps99Kt is composed of three parts:the score of sps?r, the score of spr+1?t, and thescore of adding arcs?t.
The score of arcs?t isdetermined by four different feature sets: unigramfeatures, bigram features, sibling features and leftgrandchildren features (or inside grandchildren fea-tures, meaning that the grandchildren lie between sand t).
Note that the sibling features are only relatedto the nearest sibling node of t, which is denoted assck here.
And the inside grandchildren features arerelated to all the children of t. This is different fromthe models used by Carreras (2007) and Johanssonand Nugues (2008).
They only used the left-mostchild of t, which is tck?
here.ficp(s, r, t, l) = funi(s, t, l) ?
fbi(s, t, l)?
fsib(s, sck, t) ?
{?k?i=1 fgrand(s, t, tci, l)}Sicp(s, r, t, l) = w ?
ficp(s, r, t, l)S(sps99Kt) = S(sps?r) + S(spr+1?t)+ Sicp(s, r, t, l)In Figure 2 we combine sps99Kr and spr?t intosps?t, which explains line 10 in Algorithm 1.
Thescore of sps?t also includes three parts, as shownin the following formulas.
Although there is no newarc added in this operation, the third part is neces-sary because it reflects the right (or called outside)grandchildren information of arcs?r.r trc1 rcks r s tr rc1 rckl lFigure 2: Combining two spans into a complete spanfcp(s, r, t, l) = ?ki=1 fgrand(s, r, rci, l)Scp(s, r, t, l) = w ?
fcp(s, r, t, l)S(sps?t) = S(sps99Kr)+ S(spr?t) + Scp(s, r, t, l)502.3 FeaturesAs shown above, features used in our model can bedecomposed into four parts: unigram features, bi-gram features, sibling features, and grandchildrenfeatures.
Each part can be seen as two different sets:arc-related and label-related features, except siblingfeatures, because we do not consider labels when us-ing sibling features.
Arc-related features can be un-derstood as back-off of label-related features.
Actu-ally, label-related features are gained by simply at-taching the label to the arc-features.The unigram and bigram features used in ourmodel are similar to those of (Che et al, 2008), ex-cept that we use bigram label-related features.
Thesibling features we use are similar to those of (Mc-Donald, 2006), and the grandchildren features aresimilar to those of (Carreras, 2007).3 Predicate ClassificationThe predicate classification is regarded as a super-vised word sense disambiguation (WSD) task here.The task is divided into four steps:1.
Target words selection: predicates with multi-ple senses appearing in the training data are selectedas target words.2.
Feature extraction: features in the contextaround these target words are extracted as shown inTable 4.
The detailed explanation about these fea-tures can be found from (Che et al, 2008).3.
Classification: for each target word, a SupportVector Machine (SVM) classifier is used to classifyits sense.
As reported by Lee and Ng (2002) andGuo et al (2007), SVM shows good performance onthe WSD task.
Here libsvm (Chang and Lin, 2001)is used.
The linear kernel function is used and thetrade off parameter C is 1.4.
Post processing: for each predicate in the testdata which does not appear in the training data, itsfirst sense in the frame files is used.4 Semantic Role LabelingThe semantic role labeling (SRL) can be dividedinto two separate stages: semantic role classification(SRC) and post inference (PI).During the SRC stage, a Maximum en-tropy (Berger et al, 1996) classifier is used topredict the probabilities of a word in the sentenceLanguage No-duplicated-rolesCatalan arg0-agt, arg0-cau, arg1-pat, arg2-atr, arg2-locChinese A0, A1, A2, A3, A4, A5,Czech ACT, ADDR, CRIT, LOC, PAT, DIR3, CONDEnglish A0, A1, A2, A3, A4, A5,German A0, A1, A2, A3, A4, A5,Japanese DE, GA, TMP, WOSpanish arg0-agt, arg0-cau, arg1-pat, arg1-tem, arg2-atr,arg2-loc, arg2-null, arg4-des, argL-null, argM-cau, argM-ext, argM-finTable 1: No-duplicated-roles for different languagesto be each semantic role.
We add a virtual role?NULL?
(presenting none of roles is assigned)to the roles set, so we do not need semantic roleidentification stage anymore.
For a predicateof each language, two classifiers (one for nounpredicates, and the other for verb predicates) predictprobabilities of each word in a sentence to be eachsemantic role (including virtual role ?NULL?).
Thefeatures used in this stage are listed in Table 4.The probability of each word to be a semantic rolefor a predicate is given by the SRC stage.
The re-sults generated by selecting the roles with the largestprobabilities, however, do not satisfy some con-strains.
As we did in the last year?s system (Che etal., 2008), we use the ILP (Integer Linear Program-ming) (Punyakanok et al, 2004) to get the global op-timization, which is satisfied with three constrains:C1: Each word should be labeled with one andonly one label (including the virtual label ?NULL?
).C2: Roles with a small probability should neverbe labeled (except for the virtual role ?NULL?).
Thethreshold we use in our system is 0.3.C3: Statistics show that some roles (except forthe virtual role ?NULL?)
usually appear once fora predicate.
We impose a no-duplicate-roles con-straint with a no-duplicate-roles list, which is con-structed according to the times of semantic roles?duplication for each single predicate.
Table 1 showsthe no-duplicate-roles for different languages.Our maximum entropy classifier is implementedwith Maximum Entropy Modeling Toolkit1.
Theclassifier parameters are tuned with the developmentdata for different languages respectively.
lp solve5.52 is chosen as our ILP problem solver.1http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html2http://sourceforge.net/projects/lpsolve515 Experiments5.1 Experimental SetupWe participate in the CoNLL 2009 shared taskwith all 7 languages: Catalan (Taule?
et al, 2008),Chinese (Palmer and Xue, 2009), Czech (Hajic?
etal., 2006), English (Surdeanu et al, 2008), Ger-man (Burchardt et al, 2006), Japanese (Kawaharaet al, 2002), and Spanish (Taule?
et al, 2008).
Be-sides the closed challenge, we also submitted theopen challenge results.
Our open challenge strategyis very simple.
We add the SRL development dataof each language into their training data.
The pur-pose is to examine the effect of the additional data,especially for out-of-domain (ood) data.Three machines (with 2.5GHz Xeon CPU and16G memory) were used to train our models.
Dur-ing the peak time, Amazon?s EC2 (Elastic Com-pute Cloud)3 was used, too.
Our system requires15G memory at most and the longest training timeis about 36 hours.During training the predicate classification (PC)and the semantic role labeling (SRL) models, goldensyntactic dependency parsing results are used.
Pre-vious experiments show that the PC and SRL test re-sults based on golden parse trees are slightly worsethan that based on cross trained parse trees.
It is,however, a pity that we have no enough time and ma-chines to do cross training for so many languages.5.2 Results and DiscussionIn order to examine the performance of the ILPbased post inference (PI) for different languages, weadopt a simple PI strategy as baseline, which se-lects the most likely label (including the virtual la-bel ?NULL?)
except for those duplicate non-virtuallabels with lower probabilities (lower than 0.5).
Ta-ble 2 shows their performance on development data.We can see that the ILP based post inference canimprove the precision but decrease the recall.
Ex-cept for Czech, almost all languages are improved.Among them, English benefits most.The final system results are shown in Table 3.Comparing with our CoNLL 2008 (Che et al, 2008)syntactic parsing results on English4, we can see thatour new high-order model improves about 1%.3http://aws.amazon.com/ec2/4devel: 85.94%, test: 87.51% and ood: 80.73%Precision Recall F1Catalan simple 78.68 77.14 77.90Catalan ILP 79.42 76.49 77.93Chinese simple 80.74 74.36 77.42Chinese ILP 81.97 73.92 77.74Czech simple 88.54 84.68 86.57Czech ILP 89.23 84.05 86.56English simple 83.03 83.55 83.29English ILP 85.63 83.03 84.31German simple 78.88 75.87 77.34German ILP 82.04 74.10 77.87Japanese simple 88.04 70.68 78.41Japanese ILP 89.23 70.16 78.56Spanish simple 76.73 75.92 76.33Spanish ILP 77.71 75.34 76.51Table 2: Comparison between different PI strategiesFor the open challenge, because we did not mod-ify the syntactic training data, its results are the sameas the closed ones.
We can, therefore, examine theeffect of the additional training data on SRL.
We cansee that along with the development data are addedinto the training data, the performance on the in-domain test data is increased.
However, it is inter-esting that the additional data is harmful to the oodtest.6 Conclusion and Future WorkOur CoNLL 2009 Shared Task system is com-posed of three cascaded components.
The pseudo-projective high-order syntactic dependency modeloutperforms our CoNLL 2008 model (in English).The additional in-domain (devel) SRL data can helpthe in-domain test.
However, it is harmful to the oodtest.
Our final system achieves promising results.
Inthe future, we will study how to solve the domainadaptive problem and how to do joint learning be-tween syntactic and semantic parsing.AcknowledgmentsThis work was supported by National NaturalScience Foundation of China (NSFC) via grant60803093, 60675034, and the ?863?
National High-Tech Research and Development of China via grant2008AA01Z144.52Syntactic Accuracy (LAS) Semantic Labeled F1 Macro F1 Scoredevel test ood devel test ood devel test oodCatalan closed 86.65 86.56 ??
77.93 77.10 ??
82.30 81.84 ?
?open ??
??
77.36 ??
81.97Chinese closed 75.73 75.49 ??
77.74 77.15 ??
76.79 76.38 ?
?open ??
??
77.23 ??
76.42Czech closed 80.07 80.01 76.03 86.56 86.51 85.26 83.33 83.27 80.66open ??
??
86.57 85.21 ??
83.31 80.63English closed 87.09 88.48 81.57 84.30 85.51 73.82 85.70 87.00 77.71open ??
??
85.61 73.66 ??
87.05 77.63German closed 85.69 86.19 76.11 77.87 78.61 70.07 81.83 82.44 73.19open ??
??
78.61 70.09 ??
82.44 73.20Japanese closed 92.55 92.57 ??
78.56 78.26 ??
85.86 85.65 ?
?open ??
??
78.35 ??
85.70Spanish closed 87.22 87.33 ??
76.51 76.47 ??
81.87 81.90 ?
?open ??
??
76.66 ??
82.00Average closed ??
85.23 77.90 ??
79.94 76.38 ??
82.64 77.19open 80.06 76.32 82.70 77.15Table 3: Final system resultsReferencesAdam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22.Aljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado?, and Manfred Pinkal.
2006.The SALSA corpus: a German corpus resource forlexical semantics.
In LREC-2006.Xavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In EMNLP/CoNLL-2007.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: alibrary for support vector machines.Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,Bing Qin, Ting Liu, and Sheng Li.
2008.
A cascadedsyntactic and semantic dependency parsing system.
InCoNLL-2008.Jason Eisner.
2000.
Bilexical grammars and their cubic-time parsing algorithms.
In Advances in Probabilisticand Other Parsing Technologies.Yuhang Guo, Wanxiang Che, Yuxuan Hu, Wei Zhang,and Ting Liu.
2007.
HIT-IR-WSD: A wsd system forenglish lexical sample task.
In SemEval-2007.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka, MarieMikulova?, and Zdene?k Z?abokrtsky?.
2006.
Prague De-pendency Treebank 2.0.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In CoNLL-2009.Richard Johansson and Pierre Nugues.
2008.Dependency-based semantic role labeling of Prop-Bank.
In EMNLP-2008.Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.2002.
Construction of a Japanese relevance-taggedcorpus.
In LREC-2002.Yoong Keok Lee and Hwee Tou Ng.
2002.
An empir-ical evaluation of knowledge sources and learning al-gorithms for word sense disambiguation.
In EMNLP-2002.Ryan McDonald.
2006.
Discriminative Learning andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In ACL-2005.Martha Palmer and Nianwen Xue.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1).Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-mak.
2004.
Semantic role labeling via integer linearprogramming inference.
In Coling-2004.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In CoNLL-2008.Mariona Taule?, Maria Anto`nia Mart?
?, and Marta Re-casens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
In LREC-2008.53Catalan Chinese Czech English German Japanese SpanishChildrenPOS ?
?
?
?ChildrenPOSNoDup ?
?
?
?ConstituentPOSPattern ?
?
?
?
?
?
?
?
?
?
?
?
?
?ConstituentPOSPattern+DepRelation ?
?
?
?
?
?ConstituentPOSPattern+DepwordLemma ?
?
?
?
?
?ConstituentPOSPattern+HeadwordLemma ?
?
?
?
?
?
?
?
?
?DepRelation N M ?
?
N M ?
?
N M ?
?
N M ?
?
N M ?
?
N M ?
?DepRelation+DepwordLemma ?
?
?
?DepRelation+Headword N M N M N N M N M NDepRelation+HeadwordLemma ?
?
?
?
?
?
?
?DepRelation+HeadwordLemma+DepwordLemma ?
?
?
?
?
?
?
?
?
?
?
?DepRelation+HeadwordPOS N M N M N M N M N M NDepword ?
?
?
?DepwordLemma ?
?
?
?
?
?
?
?
?
?
?
?DepwordLemma+HeadwordLemma ?
?
?
?
?
?DepwordLemma+RelationPath ?
?
?
?
?
?
?
?
?
?DepwordPOS N M N M N M ?
?
N M N M ?
?
N MDepwordPOS+HeadwordPOS ?
?
?
?DownPathLength ?
?
?
?FirstLemma ?
?
?
?
?
?
?
?
?
?
?
?FirstPOS ?
?
?
?FirstPOS+DepwordPOS ?
?
?
?
?
?FirstWord ?
?
?
?Headword N M N M N M N M N M ?
?
NHeadwordLemma N M ?
?
N M ?
?
N M ?
?
N M ?
?
N M ?
?
?
?
NHeadwordLemma+RelationPath ?
?
?
?
?
?
?
?
?
?
?
?HeadwordPOS N M N M N M ?
?
N M ?
?
N M ?
?
N MLastLemma ?
?
?
?
?
?
?
?
?
?LastPOS ?
?
?
?LastWord ?
?Path ?
?
?
?
?
?
?
?
?
?
?
?Path+RelationPath ?
?
?
?
?
?
?
?
?
?PathLength ?
?
?
?
?
?
?
?
?
?
?
?PFEAT N M N M N MPFEATSplit N M ?
?
N M ?
?
N M ?
?
N M ?
?PFEATSplitRemoveNULL N M N M N MPositionWithPredicate ?
?
?
?
?
?
?
?
?
?Predicate N M ?
?
N M N M ?
?
N M N M N M ?
?Predicate+PredicateFamilyship ?
?
?
?
?
?
?
?
?
?PredicateBagOfPOSNumbered M N M N M N MPredicateBagOfPOSNumberedWindow5 N M N M N M N M N MPredicateBagOfPOSOrdered N M N M N M N M NPredicateBagOfPOSOrderedWindow5 N M N M N M N M N M N MPredicateBagOfPOSWindow5 N N M N M N M N M NPredicateBagOfWords M N M N N M N MPredicateBagOfWordsAndIsDesOfPRED N M N M M N M N MPredicateBagOfWordsOrdered M N M N M M N M N MPredicateChildrenPOS N M ?
?
N M N M N M N M N M ?
?PredicateChildrenPOSNoDup N M N M N M N M N M N MPredicateChildrenREL N M ?
?
N M N M N M N M ?
?
N MPredicateChildrenRELNoDup N M ?
?
N M N M N M N M ?
?
N MPredicateFamilyship ?
?PredicateLemma N M ?
?
N M ?
?
N M ?
?
N M ?
?
N M ?
?
?
?
N M ?
?PredicateLemma+PredicateFamilyship ?
?
?
?
?
?PredicateSense ?
?
?
?
?
?
?
?
?
?
?
?
?
?PredicateSense+DepRelation ?
?
?
?PredicateSense+DepwordLemma ?
?
?
?PredicateSense+DepwordPOS ?
?
?
?PredicateSiblingsPOS N M N M N N M N M N MPredicateSiblingsPOSNoDup N M ?
?
N M N M N M N M N M ?
?PredicateSiblingsREL N M ?
?
N M N M N M N M N MPredicateSiblingsRELNoDup N M N M ?
?
M N M N M ?
?
N M ?
?PredicateVoiceEn N MPredicateWindow5Bigram N M N M N M N MPredicateWindow5BigramPOS N M N M N M N M N M N MRelationPath ?
?
?
?
?
?
?
?
?
?
?
?
?
?SiblingsPOS ?
?
?
?SiblingsREL ?SiblingsRELNoDup ?
?
?
?UpPath ?
?
?
?
?
?
?UpPathLength ?
?UpRelationPath ?
?
?
?
?
?UpRelationPath+HeadwordLemma ?
?
?
?
?
?
?
?Table 4: Features that are used in predicate classification (PC) and semantic role labeling (SRL).
N: noun predicatePC, M: verb predicate PC, ?
: noun predicate SRL, ?
: verb predicate SRL.54
