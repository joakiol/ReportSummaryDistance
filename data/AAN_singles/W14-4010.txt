Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 86?93,October 25, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsTernary Segmentation for Improving Searchin Top-down Induction of Segmental ITGsMarkus Saers Dekai WuHKUSTHuman Language Technology CenterDepartment of Computer Science and EngineeringHong Kong University of Science and Technology{masaers|dekai}@cs.ust.hkAbstractWe show that there are situations whereiteratively segmenting sentence pairs top-down will fail to reach valid segments andpropose a method for alleviating the prob-lem.
Due to the enormity of the searchspace, error analysis has indicated that it isoften impossible to get to a desired embed-ded segment purely through binary seg-mentation that divides existing segmentalrules in half ?
the strategy typically em-ployed by existing search strategies ?
asit requires two steps.
We propose a newmethod to hypothesize ternary segmenta-tions in a single step, making the embed-ded segments immediately discoverable.1 IntroductionOne of the most important improvements to sta-tistical machine translation to date was the movefrom token-basedmodel to segmental models (alsocalled phrasal).
This move accomplishes twothings: it allows a flat surface-based model tomemorize some relationships between word real-izations, but more importantly, it allows the modelto capture multi-word concepts or chunks.
Thesechunks are necessary in order to translate fixed ex-pressions, or other multi-word units that do nothave a compositional meaning.
If a sequence inone language can be broken down into smallerpieces which are then translated individually andreassembled in another language, the meaning ofthe sequence is compositional; if not, the only wayto translate it accurately is to treat it as a single unit?
a chunk.
Existing surface-based models (Och etal., 1999) have high recall in capturing the chunks,but tend to over-generate, which leads to big mod-els and low precision.
Surface-based models haveno concept of hierarchical composition, insteadthey make the assumption that a sentence consistsof a sequence of segments that can be individuallytranslated and reordered to form the translation.This is counter-intuitive, as the who-did-what-to-whoms of a sentence tends to be translated and re-ordered as units, rather than have their componentsmixed together.
Transduction grammars (Aho andUllman, 1972; Wu, 1997), also called hierarchicaltranslation models (Chiang, 2007) or synchronousgrammars, address this through a mechanism sim-ilar to context-free grammars.
Inducing a segmen-tal transduction grammar is hard, so the standardpractice is to use a similar method as the surface-based models use to learn the chunks, which isproblematic, since that method mostly relies onmemorizing the relationships that the mechanicsof a compositional model is designed to general-ize.
A compositional translation model would beable to translate lexical chunks, as well as gener-alize different kinds of compositions; a segmen-tal transduction grammar captures this by havingsegmental lexical rules and different nonterminalsymbols for different categories of compositions.In this paper, we focus on inducing the former:segmental lexical rules in inversion transductiongrammars (ITGs).One natural way would be to start with a token-based grammar and chunk adjacent tokens to formsegments.
The main problemwith chunking is thatthe data becomes more and more likely as the seg-ments get larger, with the degenerate end point ofall sentence pairs being memorized lexical items.Zhang et al.
(2008) combat this tendency by intro-ducing a sparsity prior over the rule probabilities,and variational Bayes to maximize the posteriorprobability of the data subject to this symmetricDirichlet prior.
To hypothesize possible chunks,they examine the Viterbi biparse of the existingmodel.
Saers et al.
(2012) use the entire parse for-est to generate the hypotheses.
They also boot-strap the ITG from linear and finite-state transduc-tion grammars (LTGs, Saers (2011), and FSTGs),86rather than initialize the lexical probabilities fromIBM models.Another way to arrive at a segmental ITG is tostart with the degenerate chunking case: each sen-tence pair as a lexical item, and segment the exist-ing lexical rules into shorter rules.
Since the startpoint is the degenerate case when optimizing fordata likelihood, this approach requires a differentobjective function to optimize against.
Saers et al.
(2013c) proposes to use description length of themodel and the data given the model, which is sub-sequently expressed in a Bayesian form with theaddition of a prior over the rule probabilities (SaersandWu, 2013).
The way they generate hypothesesis restricted to segmenting an existing lexical iteminto two parts, which is problematic, because em-bedded lexical items are potentially overlooked.There is also the option of implicitly definingall possible grammars, and sample from that dis-tribution.
Blunsom et al.
(2009) do exactly that;they induce with collapsed Gibbs sampling whichkeeps one derivation for each training sentencethat is altered and then resampled.
The operationsto change the derivations are split, join, deleteand insert.
The split-operator corresponds to bi-nary segmentation, the join-operator correspondsto chunking; the delete-operator removes an inter-nal node, resulting in its parent having three chil-dren, and the insert-operator allows a parent withthree children to be normalized to have only two.The existence of ternary nodes in the derivationmeans that the learned grammar contains ternaryrules.
Note that it still takes three operations: twosplit-operations and one delete-operation for theirmodel to do what we propose to do in a singleternary segmentation.
Also, although we allow forsingle-step ternary segmentations, our grammardoes not contain ternary rules; instead the results ofa ternary segmentation is immediately normalizedto the 2-normal form.
Although their model cantheoretically sample from the entire model space,the split-operation alone is enough to do so; theother operations were added to get the model to doso in practice.
Similarly, we propose ternary seg-mentation to be able to reach areas of the modelspace that we failed to reach with binary segmen-tation.To illustrate the problem with embedded lexi-cal items, we will introduce a small example cor-pus.
Although Swedish and English are relativelysimilar, with the structure of basic sentences beingidentical, they already illustrate the common prob-lem of rare embedded correspondences.
Imaginea really simple corpus of three sentence pairs withidentical structure:he has a red book / han har en r?d bokshe has a biology book / hon har en biologibokit has begun / det har b?rjatThe main difference is that Swedish concate-nates rather than juxtaposes compounds such asbiologibok instead of biology book.
A bilingualperson looking at this corpus would produce bilin-gual parse trees like those in Figure 1.
Inducingthis relatively simple segmental ITG from the datais, however, quite a challenge.The example above illustrates a problem withthe chunking approach, as one of the most com-mon chunks is has a/har en, whereas the linguis-tically motivated chunk biology book/biologibokoccurs only once.
There is very little in this datathat would lead the chunking approach towards thedesired ITG.
It also illustrates a problem with thebinary segmentation approach, as all the bilingualprefixes and suffixes, the biaffixes, are unique;there is no way of discovering that all the abovesentences have the exact same verb.In this paper, we propose a method to al-low bilingual infixes to be hypothesized and usedto drive the minimization of description length,which would be able to induce the desired ITGfrom the above corpus.The paper is structured so that we start by giv-ing a definition of the grammar formalism we use:ITGs (Section 2).
We then describe the notionof description length that we use (Section 3), andhow ternary segmentation differs from and com-plements binary segmentation (Section 4).
Wethen present our induction algorithm (Section 5)and give an example of a run through (Section 6).Finally we offer some concluding remarks (Sec-tion 7).2 Inversion transduction grammarsInversion transduction grammars, or ITGs (Wu,1997), are an expressive yet efficient way tomodel translation.
Much like context-free gram-mars (CFGs), they allow for sentences to be ex-plained through composition of smaller units intolarger units, but where CFGs are restricted to gen-erate monolingual sentences, ITGs generate setsof sentence pairs ?
transductions ?
rather thanlanguages.
Naturally, the components of differ-87hassheabiology bookharenbiologibokhonhasitbegunharb?rjatdethashearedharenr?dhanbookbokFigure 1: Possible inversion transduction trees over the example sentence pairs.ent languages may have to be ordered differently,which means that transduction grammars need tohandle these differences in order.
Rather than al-lowing arbitrary reordering and pay the price of ex-ponential time complexity, ITGs allow only mono-tonically straight or inverted order of the produc-tions, which cuts the time complexity down to amanageable polynomial.Formally, an ITG is a tuple ?N,?,?, R, S?,where N is a finite nonempty set of nonterminalsymbols, ?
is a finite set of terminal symbols inL0, ?
is a finite set of terminal symbols in L1, Ris a finite nonempty set of inversion transductionrules and S ?
N is a designated start symbol.
Aninversion transduction rule is restricted to take oneof the following forms:S ?
[A] , A?
[?+], A?
?
?+?where S ?
N is the start symbol, A ?
N is a non-terminal symbol, and ?+ is a nonempty sequenceof nonterminals and biterminals.
A biterminal isa pair of symbol strings: ??
??
?, where at leastone of the strings have to be nonempty.
The squareand angled brackets signal straight and inverted or-der respectively.
With straight order, both the L0and the L1productions are generated left-to-right,but with inverted order, theL1production is gener-ated right-to-left.
The brackets are frequently leftout when there is only one element on the right-hand side, which means that S ?
[A] is shortenedto S ?
A.Like CFGs, ITGs also have a 2-normal form,analogous to the Chomsky normal form for CFGs,where the rules are further restricted to only thefollowing four forms:S ?
A, A?
[BC] , A?
?BC?, A?
e/fwhere S ?
N is the start symbol, A,B,C ?
Nare nonterminal symbols and e/f is a biterminalstring.A bracketing ITG, or BITG, has only one non-terminal symbol (other than the dedicated startsymbol), which means that the nonterminals carryno information at all other than the fact that theiryields are discrete unit.
Rather than make a properanalysis of the sentence pair they only produce abracketing, hence the name.A transduction grammar such as ITG can beused in three modes: generation, transductionand biparsing.
Generation derives a bisentence, asentence pair, from the start symbol.
Transductionderives a sentence in one language from a sentencein the other language and the start symbol.
Bipars-ing verifies that a given bisentence can be derivedfrom the start symbol.
Biparsing is an integral partof any learning that requires expected counts suchas expectation maximization, and transduction isthe actual translation process.3 Description lengthWe follow the definition of description length fromSaers et al.
(2013b,c,d,a); Saers and Wu (2013),that is: the size of the model is determined bycounting the number of symbols needed to encodethe rules, and the size of the data given the modelis determined by biparsing the data with the model.Formally, given a grammar?
its description lengthDL (?)
is the sum of the length of the symbolsneeded to serialize the rule set.
For conveniencelater on, the symbols are assumed to be uniformlydistributed with a length of?lg 1Nbits each (whereN is the number of different symbols).
The de-scription length of the data D given the model isdefined as DL (D|?)
= ?lgP (D|?
).88Figure 2: The four different kinds of binary seg-mentation hypotheses.Figure 3: The two different hypotheses that canbe made from an infix-to-infix link.4 Segmenting lexical itemsWith a background in computer science it is tempt-ing to draw the conclusion that any segmentationcan be made as a sequence of binary segmenta-tions.
This is true, but only relevant if the entiresearch space can be exhaustively explored.
Wheninducing transduction grammars, the search spaceis prohibitively large; in fact, we are typically af-forded only an estimate of a single step forwardin the search process.
In such circumstances, thekinds of steps you can take start to matter greatly,and adding ternary segmentation to the typicallyused binary segmentation adds expressive power.Figure 2 contains a schematic illustration of bi-nary segmentation: To the left is a lexical itemwhere a good biaffix (anL0prefix or suffix associ-ated with anL1prefix or suffix) has been found, asillustrated with the solid connectors.
To the right isthe segmentation that can be inferred.
For binarysegmentation, there is no uncertainty in this step.When adding ternary segmentation, there arefive more situations: one situation where an in-Figure 4: The eight different hypotheses that canbemade from the four different infix-to-affix links.fix is linked to an infix, and four situations wherean infix is linked to an affix.
Figure 3 shows theinfix-to-infix situation, where there is one addi-tional piece of information to be decided: are thesurroundings linked straight or inverted?
Figure 4shows the situations where one infix is linked to anaffix.
In these situations, there are twomore piecesof information that needs to be inferred: (a) wherethe sibling of the affix needs to be segmented, and(b) how the two pieces of the sibling of the affixlink to the siblings of the infix.
The infix-to-affixsituations require a second monolingual segmen-tations decision to be made.
As this is beyond thescope of this paper, we will limit ourselves to theinfix-to-infix situation.5 Finding segmentation hypothesesPrevious work on binary hypothesis generationmakes assumptions that do not hold with ternarysegmentation; this section explains why that is andhow we get around it.
The basic problem with bi-nary segmentation is that any bisegment hypothe-sized to be good on its own has to be anchored toeither the beginning or the end of an existing biseg-ment.
An infix, by definition, does not.While recording all affixes is possible, even fornon-toy corpora (Saers and Wu, 2013; Saers et al.,2013b,c), recording all bilingual infixes is not, socollecting them all is not an option (while there are89Algorithm 1 Pseudo code for segmenting an ITG.?
?
The ITG being induced.?
?
The token-based ITG used to evaluate lexical rules.hmax?
The maximum number of hypotheses to keep from a single lexical rule.repeat?
?
0H??
Initial hypothesesfor all lexical rules A?
e/f dop?
parse(?, e/f)c ?
Fractional counts of bispansfor all bispans s, t, u, v ?
e/f doc(s, t, u, v)?
0H???
[]for all items Bs,t,u,v?
p doc(s, t, u, v)?
c(s, t, u, v) + ?(Bs,t,u,v)?(Bs,t,u,v)/?(S0,T,0,V)H???
[H?
?, ?s, t, u, v, c(s, t, u, v)?
]sort H ??
on c(s, t, u, v)for all ?s, t, u, v, c(s, t, u, v)?
?
H ??
[0..hmax] doH?(es..t/fu..v)?
[H?
(es..t/fu..v), ?s, t, u, v, A?
e/f?
]H ?
Evaluated hypothesesfor all bisegments es..t/fu..v?
keys(H?)
do???
?R?
[]for all bispan-rule pairs ?s, t, u, v, A?
e/f?
?
H ?
(es..t/fu..v) do???
make_grammar_change(?
?, e/f, s, t, u, v)R?
[R,A?
e/f ]???
DL(??)?DL(?)
+DL(D|??)?DL(D|?
)if ??
< 0 thenH ?
[H, ?es..t/fu..v, R, ???
]sort H on ?
?for all ?es..t/fu..v, R, ???
?
H do???
?for all rules A?
e/f ?
R ?R??
do???
make_grammar_change(?
?, e/f, s, t, u, v)???
DL(??)?DL(?)
+DL(D|??)?DL(D|?
)if ??
< 0 then??
???
?
?
+ ?
?until ?
?
0return ?only O(n2)possible biaffixes for a parallel sen-tence of average length n, there are O(n4)possi-ble bilingual infixes).
A way to prioritize, withinthe scope of a single bisegment, which infixes andaffixes to consider as hypotheses is crucial.
In thispaper we use an approach similar to Saers et al.
(2013d), in which we use a token-based ITG toevaluate the lexical rules in the ITG that is be-ing induced.
Using a transduction grammar hasthe advantage of calculating fractional counts forhypotheses, which allows both long and short hy-potheses to compete on a level playing field.In Algorithm 1, we start by parsing all the lex-ical rules in the grammar ?
being learned using atoken-based ITG ?.
For each rule, we only keepthe best hmax bispans.
In the second part, all col-lected bispans are evaluated as if they were theonly hypothesis being considered for changing ?.Any hypothesis with a positive effect is kept forfurther processing.
These hypotheses are sorted90and applied.
Since the grammarmay have changedsince the effect of the hypothesis was estimated,we have to check that the hypothesis would havea positive effect on the updated grammar beforecommitting to it.
All this is repeated as long asthere are improvements that can be made.Themake_grammar_changemethod deletes theold rule, and distributes its probability mass tothe rules replacing it.
For ternary segmentation,this will be three lexical rules, and two structuralrules (which happens to be identical in a bracket-ing grammar, giving that one rule two shares ofthe probability mass being distributed).
For binarysegmentation it is two lexical rules and one struc-tural rule.Rather than calculating DL (D|?)?
DL (D|?
)explicitly by biparsing the entire corpus, we es-timate the change.
For binary rules, we use thesame estimate as Saers and Wu (2013): multiply-ing in the new rule probabilities and dividing outthe old.
For ternary rules, we make the assump-tion that the three new lexical rules are combinedusing structural rules the way they would duringparsing, which means two binary structural rulesbeing applied.
The infix-to-infix situation must begenerated either by two straight combinations orby two inverted combinations, so for a bracketinggrammar it is always two applications of a singlestructural rule.
We thus multiply in the three newlexical rules and the structural rule twice, and di-vide out the old rule.
In essence, both these meth-ods are recreating the situations in which the parserwould have used the old rule, but now uses the newrules.Having exhausted all the hypotheses, we alsorun expectation maximization to stabilize the pa-rameters.
This step is not shown in the pseudocode.Examining the pseudocode closer reveals thatthe outer loop will continue as long as the grammarchanges; since the only way the grammar changesis by making lexical rules shorter, this loop is guar-anteed to terminate.
Inside the outer loop there arethree inner loops: one over the rule set, one overthe set of initial hypothesesH ?
and one over the setof evaluated hypothesesH .
The sets of hypothesesare related such that |H| ?
|H ?|, which means thatthe size of the initial set of hypotheses will dom-inate the time complexity.
The size of this initialset of hypotheses is itself limited so that it cannotcontain more than hmax hypotheses from any onerule.
The dominating factor is thus the size of therule set, which we will further analyze.The first thing we do is to parse the right-handside of the rule, which requires O(n3)with theSaers et al.
(2009) algorithm, where n is the av-erage length of the lexical items.
We then initial-ize the counts, which does not actually require aspecific step in implementation.
We then iterateover all bispans in the parse, which has the sameupper bound as the parsing process, since the ap-proximate parsing algorithm avoids exploring theentire search space.
We then sort the set of hy-potheses derived from the current rule only, whichis asymptotically bound byO(n3lgn), since thereis exactly one hypothesis per parse item.
Finally,there is a selection being made from the set of hy-potheses derived from the current rule.
In prac-tice, the parsing is more complicated than the sort-ing, making the time complexity of the whole innerloop be dominated by the time it takes to parse therules.6 ExampleIn this section we will trace through how the ex-ample from the introduction fails to go throughbinary segmentation, but succeeds when infix-to-infix segmentations are an option.The initial grammar consists of all the sentencepairs as segmental lexical rules:S ?
A 1A?he has a red bookhan har en r?d bok 0.3A?she has a biology bookhon har en biologibok 0.3A?it has begundet har b?rjat 0.3As noted before, there are no shared biaffixesamong the three lexical rules, so binary segmen-tation cannot break this grammar down further.There are, however, three shared bisegments rep-resenting three different segmentation hypotheses:has a/har en, has/har and a/en.
In this example itdoes not matter which hypothesis you choose, sowe will go with the first one, since that is the oneour implementation chose.
Breaking out all occur-rences of has a/har en gives the following gram-91mar:S ?
A 1A?
[AA] 0.36A?
it has begun/det har b?rjat 0.09A?
has a/har en 0.18A?
he/han 0.09A?
red book/r?d bok 0.09A?
she/hon 0.09A?
biology book/biologibok 0.09At this point there are two bisegments that occurin more than one rule: has/har and a/en.
Again,it does not matter for the final outcome which ofthe hypotheses we choose, so we will chose thefirst one, again because that is the one our imple-mentation chose.
Breaking out all occurrences ofhas/har gives the following grammar:S ?
A 1A?
[AA] 0.421A?
he/han 0.053A?
red book/r?d bok 0.053A?
she/hon 0.053A?
biology book/biologibok 0.053A?
has/har 0.158A?
it/det 0.053A?
begun/b?rjat 0.053A?
a/en 0.105There are no shared bisegments left in the gram-mar now, so no more segmentations can be done.Obviously, the probability of the data given thisnew grammar is much smaller, but the grammaritself has generalized far beyond the training data,to the point where it largely agrees with the pro-posed trees in Figure 1 (except that this grammarbinarizes the constituents, and treats red book/r?dbok as a segment).7 ConclusionsWe have shown that there are situations in whicha top-down segmenting approach that relies solelyon binary segmentation will fail to generalize, de-spite there being ample evidence to a human thata generalization is warranted.
We have proposedternary segmentation as a solution to provide hy-potheses that are considered good under a mini-mum description length objective.
And we haveshown that the proposed method could indeed per-form generalizations that are clear to the humaneye, but not discoverable through binary segmen-tation.
The algorithm is comparable to previ-ous segmentation approaches in terms of time andspace complexity, so scaling up to non-toy trainingcorpora is likely to work when the time comes.AcknowledgementsThis material is based upon work supportedin part by the Defense Advanced ResearchProjects Agency (DARPA) under BOLT contractnos.
HR0011-12-C-0014 and HR0011-12-C-0016,and GALE contract nos.
HR0011-06-C-0022 andHR0011-06-C-0023; by the European Union un-der the FP7 grant agreement no.
287658; and bythe Hong Kong Research Grants Council (RGC)research grants GRF620811, GRF621008, andGRF612806.
Any opinions, findings and conclu-sions or recommendations expressed in this mate-rial are those of the authors and do not necessarilyreflect the views of DARPA, the EU, or RGC.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
The The-ory of Parsing, Translation, and Compiling.Prentice-Halll, Englewood Cliffs, New Jersey,1972.Phil Blunsom, Trevor Cohn, Chris Dyer, andMilesOsborne.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Joint Confer-ence of the 47th Annual Meeting of the Asso-ciation for Computational Linguistics and 4thInternational Joint Conference on Natural Lan-guage Processing of the AFNLP (ACL-IJCNLP2009), pp.
782?790, Suntec, Singapore, August2009.David Chiang.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228, 2007.Frans Josef Och, Christoph Tillmann, and Her-mann Ney.
Improved alignment models for sta-tistical machine translation.
In 1999 Joint SIG-DAT Conference on Empirical Methods in Nat-ural Language Processing and Very Large Cor-pora, pp.
20?28, University of Maryland, Col-lege Park, Maryland, June 1999.Markus Saers and Dekai Wu.
Bayesian induc-tion of bracketing inversion transduction gram-mars.
In Sixth International Joint Conference onNatural Language Processing (IJCNLP2013),pp.
1158?1166, Nagoya, Japan, October 2013.Asian Federation of Natural Language Process-ing.92Markus Saers, Joakim Nivre, and Dekai Wu.Learning stochastic bracketing inversion trans-duction grammars with a cubic time biparsingalgorithm.
In 11th International Conferenceon Parsing Technologies (IWPT?09), pp.
29?32,Paris, France, October 2009.Markus Saers, Karteek Addanki, and Dekai Wu.From finite-state to inversion transductions: To-ward unsupervised bilingual grammar induc-tion.
In 24th International Conference onComputational Linguistics (COLING 2012), pp.2325?2340, Mumbai, India, December 2012.Markus Saers, Karteek Addanki, and Dekai Wu.Augmenting a bottom-up ITG with top-downrules by minimizing conditional descriptionlength.
In Recent Advances in Natural Lan-guage Processing (RANLP 2013), Hissar, Bul-garia, September 2013.Markus Saers, Karteek Addanki, and Dekai Wu.Combining top-down and bottom-up search forunsupervised induction of transduction gram-mars.
In Seventh Workshop on Syntax, Se-mantics and Structure in Statistical Translation(SSST-7), pp.
48?57, Atlanta, Georgia, June2013.Markus Saers, Karteek Addanki, and Dekai Wu.Iterative rule segmentation under minimumdescription length for unsupervised transduc-tion grammar induction.
In Adrian-HoriaDediu, Carlos Mart?n-Vide, Ruslan Mitkov, andBianca Truthe, editors, Statistical Language andSpeech Processing, First International Confer-ence, SLSP 2013, Lecture Notes in Artificial In-telligence (LNAI).
Springer, Tarragona, Spain,July 2013.Markus Saers, Karteek Addanki, and Dekai Wu.Unsupervised transduction grammar inductionvia minimum description length.
In SecondWorkshop on Hybrid Approaches to Transla-tion (HyTra), pp.
67?73, Sofia, Bulgaria, Au-gust 2013.Markus Saers.
Translation as Linear Trans-duction: Models and Algorithms for EfficientLearning in Statistical Machine Translation.PhD thesis, Uppsala University, Department ofLinguistics and Philology, 2011.Dekai Wu.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3):377?403, 1997.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
Bayesian learning of non-compositional phrases with synchronous pars-ing.
In 46th Annual Meeting of the Associationfor Computational Linguistics: Human Lan-guage Technologies (ACL-08: HLT), pp.
97?105, Columbus, Ohio, June 2008.93
