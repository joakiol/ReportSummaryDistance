Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2124?2133,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsNeural Relation Extraction with Selective Attention over InstancesYankai Lin1, Shiqi Shen1, Zhiyuan Liu1,2?, Huanbo Luan1, Maosong Sun1,21Department of Computer Science and Technology,State Key Lab on Intelligent Technology and Systems,National Lab for Information Science and Technology, Tsinghua University, Beijing, China2Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, ChinaAbstractDistant supervised relation extraction hasbeen widely used to find novel relationalfacts from text.
However, distant su-pervision inevitably accompanies with thewrong labelling problem, and these noisydata will substantially hurt the perfor-mance of relation extraction.
To allevi-ate this issue, we propose a sentence-levelattention-based model for relation extrac-tion.
In this model, we employ convolu-tional neural networks to embed the se-mantics of sentences.
Afterwards, webuild sentence-level attention over multi-ple instances, which is expected to dy-namically reduce the weights of thosenoisy instances.
Experimental results onreal-world datasets show that, our modelcan make full use of all informative sen-tences and effectively reduce the influenceof wrong labelled instances.
Our modelachieves significant and consistent im-provements on relation extraction as com-pared with baselines.
The source code ofthis paper can be obtained from https://github.com/thunlp/NRE.1 IntroductionIn recent years, various large-scale knowledgebases (KBs) such as Freebase (Bollacker et al,2008), DBpedia (Auer et al, 2007) and YAGO(Suchanek et al, 2007) have been built and widelyused in many natural language processing (NLP)tasks, including web search and question answer-ing.
These KBs mostly compose of relational factswith triple format, e.g., (Microsoft, founder,Bill Gates).
Although existing KBs contain a?Corresponding author: Zhiyuan Liu (li-uzy@tsinghua.edu.cn).massive amount of facts, they are still far fromcomplete compared to the infinite real-world facts.To enrich KBs, many efforts have been investedin automatically finding unknown relational facts.Therefore, relation extraction (RE), the process ofgenerating relational data from plain text, is a cru-cial task in NLP.Most existing supervised RE systems require alarge amount of labelled relation-specific trainingdata, which is very time consuming and labor in-tensive.
(Mintz et al, 2009) proposes distant su-pervision to automatically generate training datavia aligning KBs and texts.
They assume that iftwo entities have a relation in KBs, then all sen-tences that contain these two entities will expressthis relation.
For example, (Microsoft, founder,Bill Gates) is a relational fact in KB.
Distant su-pervision will regard all sentences that containthese two entities as active instances for relationfounder.
Although distant supervision is aneffective strategy to automatically label trainingdata, it always suffers from wrong labelling prob-lem.
For example, the sentence ?Bill Gates ?s turnto philanthropy was linked to the antitrust prob-lems Microsoft had in the U.S. and the Europeanunion.?
does not express the relation founderbut will still be regarded as an active instance.Hence, (Riedel et al, 2010; Hoffmann et al, 2011;Surdeanu et al, 2012) adopt multi-instance learn-ing to alleviate the wrong labelling problem.
Themain weakness of these conventional methods isthat most features are explicitly derived from NLPtools such as POS tagging and the errors generatedby NLP tools will propagate in these methods.Some recent works (Socher et al, 2012; Zenget al, 2014; dos Santos et al, 2015) attempt touse deep neural networks in relation classifica-tion without handcrafted features.
These meth-ods build classifier based on sentence-level anno-tated data, which cannot be applied in large-scale2124r1 r2 r3 rcCNN CNN CNN CNNm1 m2 m3 mcr?1 ?2 ?3 ?cFigure 1: The architecture of sentence-levelattention-based CNN, wheremiindicates the orig-inal sentence for an entity pair, ?iis the weightgiven by sentence-level attention.KBs due to the lack of human-annotated train-ing data.
Therefore, (Zeng et al, 2015) incor-porates multi-instance learning with neural net-work model, which can build relation extractorbased on distant supervision data.
Although themethod achieves significant improvement in re-lation extraction, it is still far from satisfactory.The method assumes that at least one sentence thatmentions these two entities will express their rela-tion, and only selects the most likely sentence foreach entity pair in training and prediction.
It?s ap-parent that the method will lose a large amountof rich information containing in neglected sen-tences.In this paper, we propose a sentence-levelattention-based convolutional neural network(CNN) for distant supervised relation extraction.As illustrated in Fig.
1, we employ a CNN toembed the semantics of sentences.
Afterwards, toutilize all informative sentences, we represent therelation as semantic composition of sentence em-beddings.
To address the wrong labelling prob-lem, we build sentence-level attention over mul-tiple instances, which is expected to dynamicallyreduce the weights of those noisy instances.
Fi-nally, we extract relation with the relation vectorweighted by sentence-level attention.
We evaluateour model on a real-world dataset in the task ofrelation extraction.
The experimental results showthat our model achieves significant and consistentimprovements in relation extraction as comparedwith the state-of-the-art methods.The contributions of this paper can be summa-rized as follows:?
As compared to existing neural relation ex-traction model, our model can make full useof all informative sentences of each entitypair.?
To address the wrong labelling problem indistant supervision, we propose selectiveattention to de-emphasize those noisy in-stances.?
In the experiments, we show that selectiveattention is beneficial to two kinds of CNNmodels in the task of relation extraction.2 Related WorkRelation extraction is one of the most impor-tant tasks in NLP.
Many efforts have been investedin relation extraction, especially in supervised re-lation extraction.
Most of these methods need agreat deal of annotated data, which is time con-suming and labor intensive.
To address this issue,(Mintz et al, 2009) aligns plain text with Free-base by distant supervision.
However, distant su-pervision inevitably accompanies with the wronglabelling problem.
To alleviate the wrong la-belling problem, (Riedel et al, 2010) models dis-tant supervision for relation extraction as a multi-instance single-label problem, and (Hoffmann etal., 2011; Surdeanu et al, 2012) adopt multi-instance multi-label learning in relation extraction.Multi-instance learning was originally proposed toaddress the issue of ambiguously-labelled trainingdata when predicting the activity of drugs (Diet-terich et al, 1997).
Multi-instance learning con-siders the reliability of the labels for each instance.
(Bunescu and Mooney, 2007) connects weak su-pervision with multi-instance learning and extendsit to relation extraction.
But all the feature-basedmethods depend strongly on the quality of the fea-tures generated by NLP tools, which will sufferfrom error propagation problem.Recently, deep learning (Bengio, 2009) hasbeen widely used for various areas, including com-puter vision, speech recognition and so on.
It hasalso been successfully applied to different NLPtasks such as part-of-speech tagging (Collobertet al, 2011), sentiment analysis (dos Santos andGatti, 2014), parsing (Socher et al, 2013), andmachine translation (Sutskever et al, 2014).
Dueto the recent success in deep learning, many re-searchers have investigated the possibility of us-ing neural networks to automatically learn features2125for relation extraction.
(Socher et al, 2012) usesa recursive neural network in relation extraction.They parse the sentences first and then representeach node in the parsing tree as a vector.
More-over, (Zeng et al, 2014; dos Santos et al, 2015)adopt an end-to-end convolutional neural networkfor relation extraction.
Besides, (Xie et al, 2016)attempts to incorporate the text information of en-tities for relation extraction.Although these methods achieve great success,they still extract relations on sentence-level andsuffer from a lack of sufficient training data.
Inaddition, the multi-instance learning strategy ofconventional methods cannot be easily applied inneural network models.
Therefore, (Zeng et al,2015) combines at-least-one multi-instance learn-ing with neural network model to extract relationson distant supervision data.
However, they assumethat only one sentence is active for each entity pair.Hence, it will lose a large amount of rich informa-tion containing in those neglected sentences.
Dif-ferent from their methods, we propose sentence-level attention over multiple instances, which canutilize all informative sentences.The attention-based models have attracted a lotof interests of researchers recently.
The selectiv-ity of attention-based models allows them to learnalignments between different modalities.
It hasbeen applied to various areas such as image clas-sification (Mnih et al, 2014), speech recognition(Chorowski et al, 2014), image caption generation(Xu et al, 2015) and machine translation (Bah-danau et al, 2014).
To the best of our knowl-edge, this is the first effort to adopt attention-basedmodel in distant supervised relation extraction.3 MethodologyGiven a set of sentences {x1, x2, ?
?
?
, xn} andtwo corresponding entities, our model measuresthe probability of each relation r. In this section,we will introduce our model in two main parts:?
Sentence Encoder.
Given a sentence x andtwo target entities, a convolutional neutralnetwork (CNN) is used to construct a dis-tributed representation x of the sentence.?
Selective Attention over Instances.
Whenthe distributed vector representations of allsentences are learnt, we use sentence-level at-tention to select the sentences which reallyexpress the corresponding relation.3.1 Sentence EncoderBill_Gates   is     the   founder of  Microsoft.SentenceVectorRepresentaionwordpositionConvolutionLayerMaxPooling= rxW * + bNon-linearLayerFigure 2: The architecture of CNN/PCNN used forsentence encoder.As shown in Fig.
2, we transform the sentencex into its distributed representation x by a CNN.First, words in the sentence are transformed intodense real-valued feature vectors.
Next, convo-lutional layer, max-pooling layer and non-lineartransformation layer are used to construct a dis-tributed representation of the sentence, i.e., x.3.1.1 Input RepresentationThe inputs of the CNN are raw words of thesentence x.
We first transform words into low-dimensional vectors.
Here, each input word istransformed into a vector via word embedding ma-trix.
In addition, to specify the position of each en-tity pair, we also use position embeddings for allwords in the sentence.Word Embeddings.
Word embeddings aim totransform words into distributed representationswhich capture syntactic and semantic meaningsof the words.
Given a sentence x consisting ofm words x = {w1, w2, ?
?
?
, wm}, every wordwiis represented by a real-valued vector.
Wordrepresentations are encoded by column vectors inan embedding matrix V ?
Rda?|V |where V is afixed-sized vocabulary.Position Embeddings.
In the task of relationextraction, the words close to the target entities areusually informative to determine the relation be-tween entities.
Similar to (Zeng et al, 2014), weuse position embeddings specified by entity pairs.It can help the CNN to keep track of how close2126each word is to head or tail entities.
It is definedas the combination of the relative distances fromthe current word to head or tail entities.
For ex-ample, in the sentence ?Bill Gates is the founderof Microsoft.
?, the relative distance from the word?founder?
to head entity Bill Gates is 3 and tailentity Microsoft is 2.In the example shown in Fig.
2, it is assumedthat the dimension daof the word embedding is 3and the dimension dbof the position embedding is1.
Finally, we concatenate the word embeddingsand position embeddings of all words and denoteit as a vector sequence w = {w1,w2, ?
?
?
,wm},where wi?
Rd(d = da+ db?
2).3.1.2 Convolution, Max-pooling andNon-linear LayersIn relation extraction, the main challenges arethat the length of the sentences is variable and theimportant information can appear in any area ofthe sentences.
Hence, we should utilize all lo-cal features and perform relation prediction glob-ally.
Here, we use a convolutional layer to mergeall these features.
The convolutional layer firstextracts local features with a sliding window oflength l over the sentence.
In the example shownin Fig.
2, we assume that the length of the slidingwindow l is 3.
Then, it combines all local featuresvia a max-pooling operation to obtain a fixed-sizedvector for the input sentence.Here, convolution is defined as an operation be-tween a vector sequence w and a convolution ma-trix W ?
Rdc?
(l?d), where dcis the sentence em-bedding size.
Let us define the vector qi?
Rl?das the concatenation of a sequence of w word em-beddings within the i-th window:qi= wi?l+1:i(1 ?
i ?
m+ l ?
1).
(1)Since the window may be outside of the sen-tence boundaries when it slides near the boundary,we set special padding tokens for the sentence.
Itmeans that we regard all out-of-range input vec-tors wi(i < 1 or i > m) as zero vector.Hence, the i-th filter of convolutional layer iscomputed as:pi= [Wq+ b]i(2)where b is bias vector.
And the i-th element of thevector x ?
Rdcas follows:[x]i= max(pi), (3)Further, PCNN (Zeng et al, 2015), which is avariation of CNN, adopts piecewise max poolingin relation extraction.
Each convolutional filter piis divided into three segments (pi1,pi2,pi3) byhead and tail entities.
And the max pooling pro-cedure is performed in three segments separately,which is defined as:[x]ij= max(pij), (4)And [x]iis set as the concatenation of [x]ij.Finally, we apply a non-linear function at theoutput, such as the hyperbolic tangent.3.2 Selective Attention over InstancesSuppose there is a set S contains n sen-tences for entity pair (head, tail), i.e., S ={x1, x2, ?
?
?
, xn}.To exploit the information of all sentences, ourmodel represents the set S with a real-valued vec-tor s when predicting relation r. It is straightfor-ward that the representation of the set S dependson all sentences?
representations x1,x2, ?
?
?
,xn.Each sentence representation xicontains informa-tion about whether entity pair (head, tail) con-tains relation r for input sentence xi.The set vector s is, then, computed as aweighted sum of these sentence vector xi:s =?i?ixi, (5)where ?iis the weight of each sentence vector xi.In this paper, we define ?iin two ways:Average: We assume that all sentences in theset X have the same contribution to the represen-tation of the set.
It means the embedding of the setS is the average of all the sentence vectors:s =?i1nxi, (6)It?s a naive baseline of our selective attention.Selective Attention: However, the wrong la-belling problem inevitably occurs.
Thus, if weregard each sentence equally, the wrong labellingsentences will bring in massive of noise duringtraining and testing.
Hence, we use a selec-tive attention to de-emphasize the noisy sentence.Hence, ?iis further defined as:?i=exp(ei)?kexp(ek), (7)2127where eiis referred as a query-based functionwhich scores how well the input sentence xiandthe predict relation r matches.
We select the bilin-ear form which achieves best performance in dif-ferent alternatives:ei= xiAr, (8)where A is a weighted diagonal matrix, and r isthe query vector associated with relation r whichindicates the representation of relation r.Finally, we define the conditional probabilityp(r|S, ?)
through a softmax layer as follows:p(r|S, ?)
=exp(or)?nrk=1exp(ok), (9)where nris the total number of relations and o isthe final output of the neural network which cor-responds to the scores associated to all relationtypes, which is defined as follows:o = Ms+ d, (10)where d ?
Rnris a bias vector and M is the rep-resentation matrix of relations.
(Zeng et al, 2015) follows the assumption thatat least one mention of the entity pair will reflecttheir relation, and only uses the sentence with thehighest probability in each set for training.
Hence,the method which they adopted for multi-instancelearning can be regarded as a special case as ourselective attention when the weight of the sentencewith the highest probability is set to 1 and othersto 0.3.3 Optimization and Implementation DetailsHere we introduce the learning and optimiza-tion details of our model.
We define the objectivefunction using cross-entropy at the set level as fol-lows:J(?)
=s?i=1log p(ri|Si, ?
), (11)where s indicates the number of sentence sets and?
indicates all parameters of our model.
To solvethe optimization problem, we adopt stochastic gra-dient descent (SGD) to minimize the objectivefunction.
For learning, we iterate by randomlyselecting a mini-batch from the training set untilconverge.In the implementation, we employ dropout (Sri-vastava et al, 2014) on the output layer to pre-vent overfitting.
The dropout layer is defined asan element-wise multiplication with a a vector hof Bernoulli random variables with probability p.Then equation (10) is rewritten as:o = M(s ?
h) + d. (12)In the test phase, the learnt set representationsare scaled by p, i.e.,?si= psi.
And the scaled setvector?riis finally used to predict relations.4 ExperimentsOur experiments are intended to demonstratethat our neural models with sentence-level selec-tive attention can alleviate the wrong labellingproblem and take full advantage of informativesentences for distant supervised relation extrac-tion.
To this end, we first introduce the dataset andevaluation metrics used in the experiments.
Next,we use cross-validation to determine the parame-ters of our model.
And then we evaluate the ef-fects of our selective attention and show its per-formance on the data with different set size.
Fi-nally, we compare the performance of our methodto several state-of-the-art feature-based methods.4.1 Dataset and Evaluation MetricsWe evaluate our model on a widely useddataset1which is developed by (Riedel et al,2010) and has also been used by (Hoffmann etal., 2011; Surdeanu et al, 2012).
This dataset wasgenerated by aligning Freebase relations with theNew York Times corpus (NYT).
Entity mentionsare found using the Stanford named entity tagger(Finkel et al, 2005), and are further matched to thenames of Freebase entities.
The Freebase relationsare divided into two parts, one for training and onefor testing.
It aligns the the sentences from thecorpus of the years 2005-2006 and regards themas training instances.
And the testing instancesare the aligned sentences from 2007.
There are53 possible relationships including a special rela-tion NA which indicates there is no relation be-tween head and tail entities.
The training data con-tains 522,611 sentences, 281,270 entity pairs and18,252 relational facts.
The testing set contains172,448 sentences, 96,678 entity pairs and 1,950relational facts.Similar to previous work (Mintz et al, 2009),we evaluate our model in the held-out evaluation.It evaluates our model by comparing the relation1http://iesl.cs.umass.edu/riedel/ecml/2128facts discovered from the test articles with thosein Freebase.
It assumes that the testing systemshave similar performances in relation facts insideand outside Freebase.
Hence, the held-out evalua-tion provides an approximate measure of precisionwithout time consumed human evaluation.
Wereport both the aggregate curves precision/recallcurves and Precision@N (P@N) in our experi-ments.4.2 Experimental Settings4.2.1 Word EmbeddingsIn this paper, we use the word2vec tool2to trainthe word embeddings on NYT corpus.
We keepthe words which appear more than 100 times inthe corpus as vocabulary.
Besides, we concatenatethe words of an entity when it has multiple words.4.2.2 Parameter SettingsFollowing previous work, we tune our mod-els using three-fold validation on the training set.We use a grid search to determine the optimalparameters and select learning rate ?
for SGDamong {0.1, 0.01, 0.001, 0.0001}, the sliding win-dow size l ?
{1, 2, 3, ?
?
?
, 8}, the sentence embed-ding size n ?
{50, 60, ?
?
?
, 300}, and the batchsize B among {40, 160, 640, 1280}.
For other pa-rameters, since they have little effect on the results,we follow the settings used in (Zeng et al, 2014).For training, we set the iteration number over allthe training data as 25.
In Table 1 we show allparameters used in the experiments.Table 1: Parameter settingsWindow size l 3Sentence embedding size dc230Word dimension da50Position dimension db5Batch size B 160Learning rate ?
0.01Dropout probability p 0.54.3 Effect of Sentence-level SelectiveAttentionTo demonstrate the effects of the sentence-levelselective attention, we empirically compare dif-ferent methods through held-out evaluation.
Weselect the CNN model proposed in (Zeng et al,2014) and the PCNN model proposed in (Zeng2https://code.google.com/p/word2vec/et al, 2015) as our sentence encoders and imple-ment them by ourselves which achieve compara-ble results as the authors reported.
And we com-pare the performance of the two different kindsof CNN with sentence-level attention (ATT) , itsnaive version (AVE) which represents each sen-tence set as the average vector of sentences insidethe set and the at-least-one multi-instance learning(ONE) used in (Zeng et al, 2015).0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.40.30.40.50.60.70.80.91RecallPrecisionCNNCNN+ONECNN+AVECNN+ATT0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.40.30.40.50.60.70.80.91RecallPrecisionPCNNPCNN+ONEPCNN+AVEPCNN+ATTFigure 3: Top: Aggregate precion/recall curves ofCNN, CNN+ONE, CNN+AVE, CNN+ATT.
Bot-tom: Aggregate precion/recall curves of PCNN,PCNN+ONE, PCNN+AVE, PCNN+ATTFrom Fig.
3, we have the following observa-tion: (1) For both CNN and PCNN, the ONEmethod brings better performance as compared toCNN/PCNN.
The reason is that the original distantsupervision training data contains a lot of noiseand the noisy data will damage the performance ofrelation extraction.
(2) For both CNN and PCNN,the AVE method is useful for relation extractionas compared to CNN/PCNN.
It indicates that con-sidering more sentences is beneficial to relationextraction since the noise can be reduced by mu-tual complementation of information.
(3) For both2129CNN and PCNN, the AVE method has a similarperformance compared to the ONE method.
It in-dicates that, although the AVE method brings ininformation of more sentences, since it regardseach sentence equally, it also brings in the noisefrom the wrong labelling sentences which mayhurt the performance of relation extraction.
(4) Forboth CNN and PCNN, the ATT method achievesthe highest precision over the entire range of re-call compared to other methods including the AVEmethod.
It indicates that the proposed selective at-tention is beneficial.
It can effectively filter outmeaningless sentences and alleviate the wrong la-belling problem in distant supervised relation ex-traction.4.4 Effect of Sentence NumberIn the original testing data set, there are 74,857entity pairs that correspond to only one sen-tence, nearly 3/4 over all entity pairs.
Sincethe superiority of our selective attention lies inthe entity pairs containing multiple sentences, wecompare the performance of CNN/PCNN+ONE,CNN/PCNN+AVE and CNN/PCNN+ATT on theentity pairs which have more than one sentence.And then we examine these three methods in threetest settings:?
One: For each testing entity pair, we ran-domly select one sentence and use this sen-tence to predict relation.?
Two: For each testing entity pair, we ran-domly select two sentences and proceed re-lation extraction.?
All: We use all sentences of each entity pairfor relation extraction.Note that, we use all the sentences in training.
Wewill report the P@100, P@200, P@300 and themean of them for each model in held-out evalua-tion.Table 2 shows the P@N for compared models inthree test settings.
From the table, we can see that:(1) For both CNN and PCNN, the ATT methodachieves the best performance in all test settings.It demonstrates the effectiveness of sentence-levelselective attention for multi-instance learning.
(2)For both CNN and PCNN, the AVE method iscomparable to the ATT method in the One test set-ting.
However, when the number of testing sen-tences per entity pair grows, the performance ofthe AVE methods has almost no improvement.
Iteven drops gradually in P@100, P@200 as thesentence number increases.
The reason is that,since we regard each sentence equally, the noisecontained in the sentences that do not express anyrelation will have negative influence in the perfor-mance of relation extraction.
(3) CNN+AVE andCNN+ATT have 5% to 8% improvements com-pared to CNN+ONE in the ONE test setting.
Sinceeach entity pair has only one sentence in this testsetting, the only difference of these methods isfrom training.
Hence, it shows that utilizing allsentences will bring in more information althoughit may also bring in some extra noises.
(4) Forboth CNN and PCNN, the ATT method outper-forms other two baselines over 5% and 9% in theTwo and All test settings.
It indicates that by tak-ing more useful information into account, the re-lational facts which CNN+ATT ranks higher aremore reliable and beneficial to relation extraction.4.5 Comparison with Feature-basedApproaches0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.40.30.40.50.60.70.80.91RecallPrecisionMintzMultiRMIMLCNN+ATTPCNN+ATTFigure 4: Performance comparison of proposedmodel and traditional methodsTo evaluate the proposed method, we select thefollowing three feature-based methods for com-parison through held-out evaluation:Mintz (Mintz et al, 2009) is a traditional distantsupervised model.MultiR (Hoffmann et al, 2011) proposes aprobabilistic, graphical model of multi-instancelearning which handles overlapping relations.MIML (Surdeanu et al, 2012) jointly modelsboth multiple instances and multiple relations.We implement them with the source codes re-leased by the authors.2130Table 2: P@N for relation extraction in the entity pairs with different number of sentencesTest Settings One Two AllP@N(%) 100 200 300 Mean 100 200 300 Mean 100 200 300 MeanCNN+ONE 68.3 60.7 53.8 60.9 70.3 62.7 55.8 62.9 67.3 64.7 58.1 63.4+AVE 75.2 67.2 58.8 67.1 68.3 63.2 60.5 64.0 64.4 60.2 60.1 60.4+ATT 76.2 65.2 60.8 67.4 76.2 65.7 62.1 68.0 76.2 68.6 59.8 68.2PCNN+ONE 73.3 64.8 56.8 65.0 70.3 67.2 63.1 66.9 72.3 69.7 64.1 68.7+AVE 71.3 63.7 57.8 64.3 73.3 65.2 62.1 66.9 73.3 66.7 62.8 67.6+ATT 73.3 69.2 60.8 67.8 77.2 71.6 66.1 71.6 76.2 73.1 67.4 72.2Fig.
4 shows the precision/recall curvesfor each method.
We can observe that: (1)CNN/PCNN+ATT significantly outperforms allfeature-based methods over the entire range of re-call.
When the recall is greater than 0.1, the perfor-mance of feature-based method drop out quickly.In contrast, our model has a reasonable preci-sion until the recall approximately reaches 0.3.It demonstrates that the human-designed featurecannot concisely express the semantic meaning ofthe sentences, and the inevitable error brought byNLP tools will hurt the performance of relationextraction.
In contrast, CNN/PCNN+ATT whichlearns the representation of each sentences auto-matically can express each sentence well.
(2)PCNN+ATT performs much better as comparedto CNN+ATT over the entire range of recall.
Itmeans that the selective attention considers theglobal information of all sentences except the in-formation inside each sentence.
Hence, the perfor-mance of our model can be further improved if wehave a better sentence encoder.4.6 Case StudyTable 3 shows two examples of selective at-tention from the testing data.
For each relation,we show the corresponding sentences with high-est and lowest attention weight respectively.
Andwe highlight the entity pairs with bold formatting.From the table we find that: The former exam-ple is related to the relation employer of.
Thesentence with low attention weight does not ex-press the relation between two entities, while thehigh one shows that Mel Karmazin is the chief ex-ecutive of Sirius Satellite Radio.
The later exam-ple is related to the relation place of birth.The sentence with low attention weight expresseswhere Ernst Haefliger is died in, while the highone expresses where he is born in.Table 3: Some examples of selective attention inNYT corpusRelation employer ofLow When Howard Stern was prepar-ing to take his talk show to SiriusSatellite Radio, following his for-mer boss, Mel Karmazin, Mr. Hol-lander argued that ...High Mel Karmazin, the chief executiveof Sirius Satellite Radio, made alot of phone calls ...Relation place of birthLow Ernst Haefliger, a Swiss tenorwho ... roles , died on Saturdayin Davos, Switzerland, where hemaintained a second home.High Ernst Haefliger was born in Davoson July 6, 1919, and studied at theWettinger Seminary ...5 Conclusion and Future WorksIn this paper, we develop CNN with sentence-level selective attention.
Our model can make fulluse of all informative sentences and alleviate thewrong labelling problem for distant supervised re-lation extraction.
In experiments, we evaluate ourmodel on relation extraction task.
The experimen-tal results show that our model significantly andconsistently outperforms state-of-the-art feature-based methods and neural network methods.In the future, we will explore the following di-rections:?
Our model incorporates multi-instance learn-ing with neural network via instance-level se-lective attention.
It can be used in not onlydistant supervised relation extraction but alsoother multi-instance learning tasks.
We willexplore our model in other area such as text2131categorization.?
CNN is one of the effective neural net-works for neural relation extraction.
Re-searchers also propose many other neural net-work models for relation extraction.
In thefuture, we will incorporate our instance-levelselective attention technique with those mod-els for relation extraction.AcknowledgmentsThis work is supported by the 973 Program(No.
2014CB340501), the National Natural Sci-ence Foundation of China (NSFC No.
61572273,61303075) and the Tsinghua University InitiativeScientific Research Program (20151080406).ReferencesS?oren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary Ives.2007.
Dbpedia: A nucleus for a web of open data.Springer.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprintarXiv:1409.0473.Yoshua Bengio.
2009.
Learning deep architectures forai.
Foundations and trendsR?
in Machine Learning,2(1):1?127.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of KDD, pages1247?1250.Razvan Bunescu and Raymond Mooney.
2007.
Learn-ing to extract relations from the web using minimalsupervision.
In Proceedings of ACL, volume 45,page 576.Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho,and Yoshua Bengio.
2014.
End-to-end continuousspeech recognition using attention-based recurrentnn: first results.
arXiv preprint arXiv:1412.1602.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
JMLR, 12:2493?2537.Thomas G Dietterich, Richard H Lathrop, and Tom?asLozano-P?erez.
1997.
Solving the multiple instanceproblem with axis-parallel rectangles.
Artificial in-telligence, 89(1):31?71.C?cero Nogueira dos Santos and Ma?ra Gatti.
2014.Deep convolutional neural networks for sentimentanalysis of short texts.
In Proceedings of COLING.C?cero Nogueira dos Santos, Bing Xiang, and BowenZhou.
2015.
Classifying relations by ranking withconvolutional neural networks.
In Proceedings ofACL, volume 1, pages 626?634.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of ACL, pages 363?370.Association for Computational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S Weld.
2011.
Knowledge-based weak supervision for information extractionof overlapping relations.
In Proceedings of ACL-HLT, pages 541?550.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of ACL-IJCNLP, pages 1003?1011.Volodymyr Mnih, Nicolas Heess, Alex Graves, et al2014.
Recurrent models of visual attention.
In Pro-ceedings of NIPS, pages 2204?2212.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions with-out labeled text.
In Proceedings of ECML-PKDD,pages 148?163.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of EMNLP-CoNLL, pages 1201?1211.Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013.
Parsing with compo-sitional vector grammars.
In Proceedings of ACL.Citeseer.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
JMLR, 15(1):1929?1958.Fabian M Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge.
In Proceedings of WWW, pages 697?706.ACM.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,and Christopher D Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In Pro-ceedings of EMNLP, pages 455?465.Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.Sequence to sequence learning with neural net-works.
In Proceedings of NIPS, pages 3104?3112.Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, andMaosong Sun.
2016.
Representation learning ofknowledge graphs with entity descriptions.2132Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville,Ruslan Salakhutdinov, Richard Zemel, and YoshuaBengio.
2015.
Show, attend and tell: Neural imagecaption generation with visual attention.
Proceed-ings of ICML.Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,and Jun Zhao.
2014.
Relation classification via con-volutional deep neural network.
In Proceedings ofCOLING, pages 2335?2344.Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.2015.
Distant supervision for relation extraction viapiecewise convolutional neural networks.
In Pro-ceedings of EMNLP.2133
