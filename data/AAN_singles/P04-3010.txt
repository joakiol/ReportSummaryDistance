Part-of-Speech Tagging Considering Surface Formfor an Agglutinative LanguageDo-Gil Lee and Hae-Chang RimDept.
of Computer Science & EngineeringKorea University1, 5-ka, Anam-dong, Seongbuk-kuSeoul 136-701, Koreadglee, rim@nlp.korea.ac.krAbstractThe previous probabilistic part-of-speech taggingmodels for agglutinative languages have consid-ered only lexical forms of morphemes, not surfaceforms of words.
This causes an inaccurate cal-culation of the probability.
The proposed modelis based on the observation that when there existwords (surface forms) that share the same lexicalforms, the probabilities to appear are different fromeach other.
Also, it is designed to consider lexi-cal form of word.
By experiments, we show thatthe proposed model outperforms the bigram HiddenMarkov model (HMM)-based tagging model.1 IntroductionPart-of-speech (POS) tagging is a job to assign aproper POS tag to each linguistic unit such as wordfor a given sentence.
In English POS tagging, wordis used as a linguistic unit.
However, the num-ber of possible words in agglutinative languagessuch as Korean is almost infinite because words canbe freely formed by gluing morphemes together.Therefore, morpheme-unit tagging is preferred andmore suitable in such languages than word-unit tag-ging.
Figure 1 shows an example of morphemestructure of a sentence, where the bold lines indi-cate the most likely morpheme-POS sequence.
Asolid line represents a transition between two mor-phemes across a word boundary and a dotted linerepresents a transition between two morphemes in aword.The previous probabilistic POS models for ag-glutinative languages have considered only lexicalforms of morphemes, not surface forms of words.This causes an inaccurate calculation of the proba-bility.
The proposed model is based on the obser-vation that when there exist words (surface forms)that share the same lexical forms, the probabilitiesto appear are different from each other.
Also, it isdesigned to consider lexical form of word.
By ex-periments, we show that the proposed model outper-forms the bigram Hidden Markov model (HMM)-based tagging model.2 Korean POS tagging modelIn this section, we first describe the standardmorpheme-unit tagging model and point out a mis-take of this model.
Then, we describe the proposedmodel.2.1 Standard morpheme-unit modelThis section describes the HMM-based morpheme-unit model.
The morpheme-unit POS tagging modelis to find the most likely sequence of morphemes and corresponding POS tags  for a given sentence , as follows (Kim et al, 1998; Lee et al, 2000):       (1)    (2)In the equation, 	 denotes the number ofmorphemes in the sentence.
A sequence of   is a sentence of  words, and asequence of  	   and a se-quence of  	    denote a sequenceof 	 lexical forms of morphemes and a sequence ofmorpheme categories (POS tags), respectively.To simplify Equation 2, a Markov assumption isusually used as follows:        (3)where, is a pseudo tag which denotes the begin-ning of word and is also written as .
de-notes a type of transition from the previous tag tothe current tag.
It has a binary value according tothe type of the transition (either intra-word or inter-word transition).As can be seen, the word1 sequence is dis-carded in Equation 2.
This leads to an inaccurate1A word is a surface form.na/NNPna/VVna/VXnal/VVneun/PXneun/EFDhag-gyo/NNC e/PAga/VVga/VXgal/VVn-da/EFFn-da/EFCBOSEOSFigure 1: Morpheme structure of the sentence ?na-neun hag-gyo-e gan-da?
(I go to school)calculation of the probability.
A lexical form of aword can be mapped to more than one surface word.In this case, although the different surface forms aregiven, if they have the same lexical form, then theprobabilities will be the same.
For example, a lexi-cal form mong-go/nc+leul/jc2 , can be mapped fromtwo surface forms mong-gol and mong-go-leul.
Byapplying Equation 1 and Equation 2 to both words,the following equations can be derived: mong-go  leul  mong-gol  mong-go  leul  (4) mong-go  leul  mong-go-leul  mong-go  leul  (5)As a result, we can acquire the following equationfrom Equation 4 and Equation 5:  mong-go  leul  mong-gol mong-go  leul  mong-go-leul (6)That is, they assume that probabilities ofthe results that have the same lexical formare the same.
However, we can easilyshow that Equation 6 is mistaken: Actually, mong-go  leul   mong-go-leuland  mong-gol   mong-gol 	 .Hence,  mong-go  leul   mong-gol  mong-go  leul  mong-go-leul.To overcome the disadvantage, we propose a newtagging model that can consider the surface form.2.2 The proposed modelThis section describes the proposed model.
To sim-plify the notation, we introduce a variable R, whichmeans a tagging result of a given sentence and con-sists of  and  .     (7)    (8)2mong-go means Mongolia, nc is a common noun, and jc isa objective case postposition.The probability     is given as follows:    	   (9)   (10)   (11)where, denotes the tagging result of th word(), and denotes a pseudo variable to indicatethe beginning of word.
Equation 9 becomes Equa-tion 10 by the chain rule.
To be a more tractableform, Equation 10 is simplified by a Markov as-sumption as Equation 11.The probability    cannot be calcu-lated directly, so it is derived as follows:        (12)     (13)   (14)      (15)Equation 12 is derived by Bayes rule, Equation13 by a chain rule and an independence assumption,and Equation 15 by Bayes rule.
In Equation 15, wecall the left term ?morphological analysis model?and right one ?transition model?.The morphological analysis model   canbe implemented in a morphological analyzer.
If amorphological analyzer can provide the probability,then the tagger can use the values as they are.
Ac-tually, we use the probability that a morphologicalanalyzer, ProKOMA (Lee and Rim, 2004) produces.Although it is not necessary to discuss the morpho-logical analysis model in detail, we should note thatsurface forms are considered here.The transition model is a form of point-wise mu-tual information.           (16)       (17)where, a superscript  in and denotes theposition of the word in a sentence.The denominator means a joint probability thatthe morphemes and the tags in a word appear to-gether, and the numerator means a joint probabilitythat all the morphemes and the tags between twowords appear together.
Due to the sparse data prob-lem, they cannot also be calculated directly from thetest data.
By aMarkov assumption, the denominatorand the numerator can be broken down into Equa-tion 18 and Equation 19, respectively.       (18)            (19)where,   means a transition probabil-ity between the last morpheme of the th wordand the first morpheme of the th word.By applying Equation 18 and Equation 19 toEquation 17, we obtain the following equation:       (20)For a given sentence, Figure 2 shows the bigramHMM-based tagging model, and Figure 3 the pro-posed model.
The main difference between thetwo models is the proposed model considers surfaceforms but the HMM does not.3 ExperimentsFor evaluation, two data sets are used: ETRI POStagged corpus and KAIST POS tagged corpus.
Wedivided the test data into ten parts.
The perfor-mances of the model are measured by averagingover the ten test sets in the 10-fold cross-validationexperiment.
Table 1 shows the summary of the cor-pora.Table 1: Summary of the dataCorpus ETRI KAISTTotal # of words 288,291 175,468Total # of sentences 27,855 16,193# of tags 27 54Generally, POS tagging goes through the fol-lowing steps: First, run a morphological analyzer,where it generates all the possible interpretationsfor a given input text.
Then, a POS tagger takesthe results as input and chooses the most likely oneamong them.
Therefore, the performance of the tag-ger depends on that of the preceding morphologicalanalyzer.If the morphological analyzer does not generatethe exact result, the tagger has no chance to se-lect the correct one, thus an answer inclusion rateof the morphological analyzer becomes the upperbound of the tagger.
The previous works prepro-cessed the dictionary to include all the exact an-swers in the morphological analyzer?s results.
How-ever, this evaluation method is inappropriate to thereal application in the strict sense.
In this experi-ment, we present the accuracy of the morphologi-cal analyzer instead of preprocessing the dictionary.ProKOMA?s results with the test data are listed inTable 2.Table 2: Morphological analyzer?s results with thetest dataCorpus ETRI KAISTAnswer inclusion rate (%) 95.82 95.95Average # of results per word 2.16 1.811-best accuracy (%) 88.31 90.12In the table, 1-best accuracy is defined as thenumber of words whose result with the highestprobability is matched to the gold standard over theentire words in the test data.
This can also be a tag-ging model that does not consider any outer context.To compare the proposed model with the standardmodel, the results of the two models are given inTable 3.
As can be seen, our model outperforms theHMM model.
Moreover, the HMM model is evenworse than the ProKOMA?s 1-best accuracy.
Thistells that the standard HMM by itself is not a goodmodel for agglutinative languages.4 ConclusionWe have presented a new POS tagging model thatcan consider the surface form for Korean, whichBOS EOSNNPnaPXneunNNChag-gyoPAeVVgaEFFn-daFigure 2: Lattice of the bigram HMM-based modelna/NNP+neun/PX hag-gyo/NNC+e/PA ga/VV+n-da/EFFBOS EOSna-neun hag-gyo-e gan-daFigure 3: Lattice of the proposed modelTable 3: Tagging accuracies (%) of the standardHMM and the proposed modelCorpus ETRI KAISTThe standard HMM 87.47 89.83The proposed model 90.66 92.01is an agglutinative language.
Although the modelleaves much room for improvement, it outperformsthe HMM based model according to the experimen-tal results.AcknowledgementThis work was supported by Korea Research Foun-dation Grant (KRF-2003-041-D20485)ReferencesJ.-D. Kim, S.-Z.
Lee, and H.-C. Rim.
1998.
Amorpheme-unit POS tagging model consideringword-spacing.
In Proceedings of the 1998 Con-ference on Hangul and Korean Information Pro-cessing, pages 3?8.D.-G. Lee and H.-C. Rim.
2004.
ProKOMA:A probabilistic Korean morphological analyzer.Technical Report KU-NLP-04-01, Department ofComputer Science and Engineering, Korea Uni-versity.S.-Z.
Lee, Jun?ichi Tsujii, and H.-C. Rim.
2000.Hidden markov model-based Korean part-of-speech tagging considering high agglutinativity,word-spacing, and lexical correlativity.
In Pro-ceedings of the 38th Annual Meeting of the Asso-ciation for Computational Linguistics.
