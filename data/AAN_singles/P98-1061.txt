A structure-sharing parser for lexicalized grammarsRoger EvansInformation Technology Research InstituteUniversity of BrightonBrighton, BN2 4G J, UKRoger.
Evans @it ri.
brighton, ac.
ukDavid WeirCognitive and Computing SciencesUniversity of SussexBrighton, BN1 9QH, UKDavid.Weir@cogs.susx.ac.ukAbst rac tIn wide-coverage l xicalized grammars many ofthe elementary structures have substructures incommon.
This means that in conventional pars-ing algorithms ome of the computation associ-ated with different structures is duplicated.
Inthis paper we describe a precompilation tech-nique for such grammars which allows some ofthis computation to be shared.
In our approachthe elementary structures of the grammar aretransformed into finite state automata whichcan be merged and minimised using standard al-gorithms, and then parsed using an automaton-based parser.
We present algorithms for con-structing automata from elementary structures,merging and minimising them, and string recog-nition and parse recovery with the resultinggrammar.1 I n t roduct ionIt is well-known that fully lexicalised grammarformalisms uch as LTAG (Joshi and Schabes,1991) are difficult to parse with efficiently.
Eachword in the parser's input string introduces anelementary tree into the parse table for eachof its possible readings, and there is often asubstantial overlap in structure between thesetrees.
A conventional parsing algorithm (Vijay-Shanker and Joshi, 1985) views the trees as in-dependent, and so is likely to duplicate the pro-cessing of this common structure.
Parsing couldbe made more efficient (empirically if not for-mally), if the shared structure could be identi-fied and processed only once.Recent work by Evans and Weir (1997) andChen and Vijay-Shanker (1997) addresses thisproblem from two different perspectives.
Evansand Weir (1997) outline a technique for com-piling LTAG grammars into automata which arethen merged to introduce some sharing of struc-ture.
Chen and Vijay-Shanker (1997) use un-derspecified tree descriptions to represent setsof trees during parsing.
The present paper takesthe former approach, but extends our previouswork by:?
showing how merged automata can be min-imised, so that they share as much struc-ture as possible;?
showing that by precompiling additionalinformation, parsing can be broken downinto recognition followed by parse recovery;?
providing a formal treatment of the algo-rithms for transforming and minimising thegrammar, recognition and parse recovery.In the following sections we outline the basicapproach, and describe informally our improve-ments to the previous account.
We then give aformal account of the optimisation process anda possible parsing algorithm that makes use ofit 1 .2 Automaton-based  pars ingConventional LTAG parsers (Vijay-Shanker andJoshi, 1985; Schabes and Joshi, 1988; Vijay-Shanker and Weir, 1993) maintain a parse ta-ble, a set of i tems corresponding to completeand partial constituents.
Parsing proceeds byfirst seeding the table with items anchored onthe input string, and then repeatedly scanningthe table for parser  actions.
Parser actionsintroduce new items into the table licensed byone or more items already in the table.
Themain types of parser actions are:1. extending a constituent by incorporatinga complete subconstituent (on the left or1However, due to lack of space, no proofs and onlyminimal informal descriptions are given in this paper.372right);2. extending a constituent by adjoining a sur-rounding complete auxiliary constituent;3. predicting the span of the foot node of anauxiliary constituent ( o the left or right).Parsing is complete when all possible parser ac-tions have been executed.In a completed parse table it is possible totrace the sequence of items corresponding to therecognition of an elementary tree from its lexi-cal anchor upwards.
Each item in the sequencecorresponds to a node in the tree (with the se-quence as a whole corresponding to a completetraversal of the tree), and each step correspondsto the parser action that licensed the next item,given the current one.
From this perspective,parser actions can be restated relative to theitems in such a sequence as:1. substitute a complete subconstituent (onthe left or right);2. adjoin a surrounding complete auxiliaryconstituent;3. predict he span of the tree's foot node (tothe left or right).The recognition of the tree can thus be viewedas the computation of a finite state automaton,whose states correspond to a traversal of thetree and whose input symbols are these relaot ivised parser actions.This perspective suggests a re-casting of theconventional LTAG parser in terms of such au-tomata 2.
For this automaton-based parser, thegrammar structures are not trees, but automatacorresponding to tree traversals whose inputsare strings of relativised parser actions.
Itemsin the parse table reference automaton statesinstead of tree addresses, and if the automa-ton state is final, the item represents a completeconstituent.
Parser actions arise as before, butare executed by relativising them with respectto the incomplete item participating in the ac-tion, and passing this relativised parser actionas the next input symbol for the automaton ref-erenced by that item.
The resulting state ofthat automaton is then used as the referent ofthe newly licensed item.On a first pass, this re-casting isexactly that: itdoes nothing new or different from the original2Evans and Weir (1997) provides a longer informalintroduction to this approach.parser on the original grammar.
However thereare a number of subtle differences3:?
the automata re more abstract han thetrees: the only grammatical informationthey contain are the input symbols and theroot node labels, indicating the category ofthe constituent the automaton recognises;?
automata for several trees can be mergedtogether and optimised using standardwell-studied techniques, resulting in a sin-gle automaton that recognises many treesat once, sharing as many of the commonparser actions as possible.It is this final point which is the focus of thispaper.
By representing trees as automata, wecan merge trees together and apply standardoptimisation techniques to share their commonstructure.
The parser will remain unchanged,but will operate more efficiently where struc-ture has been shared.
Additionally, becausethe automata re more abstract han the trees,capturing precisely the parser's view of thetrees, sharing may occur between trees whichare structurally quite different, but which hap-pen to have common parser actions associatedwith them.3 Merg ing  and  min imis ing  automataCombining the automata for several trees canbe achieved using a variety of standard algo-rithms (Huffman, 1954; Moore, 1956).
How-ever any transformations must respect one im-portant feature: once the parser reaches a fi-nal state it needs to know what tree it has justrecognised 4.
When automata for trees with dif-ferent root categories are merged, the resultingautomaton eeds to somehow indicate to theparser what trees are associated with its finalstates.In Evans and Weir (1997), we combined au-tomata by introducing a new initial state withe-transitions toeach of the original initial states,3A further difference is that the traversal encodedin the automaton captures part of the parser's controlstrategy.
However for simplicity we assume here a fixedparser control strategy (bottom-up, anchor-out) and donot pursue this point further - Evans and Weir (1997)offers some discussion.4For recognition alone it only needs to know the rootcategory of the tree, but to recover the parse it needs toidentify the tree itself.373and then determinising the resulting automa-ton to induce some sharing of structure.
Torecover trees, final automaton states were an-notated with the number of the tree the finalstate is associated with, which the parser canthen readily access.However, the drawback of this approach is thatdifferently annotated final states can never bemerged, which restricts the scope for structuresharing (minimisation, for example, is not pos-sible since all the final states are distinct).
Toovercome this, we propose an alternative ap-proach as follows:?
each automaton transition is annotatedwith the set of trees which pass throughit: when transitions are merged in au-tomaton optimisation, their annotationsare unioned;?
the parser maintains for each item in thetable the set of trees that are valid for theitem: initially this is all the valid trees forthe automaton, but gets intersected withthe annotation of any transition followed;also if two paths through the automatonmeet (i.e., an item is about to be addedfor a second time), their annotations getunioned.This approach supports arbitrary merging ofstates, including merging all the final states intoone.
The parser maintains a dynamic record ofwhich trees are valid for states (in particular fi-nal states) in the parse table.
This means thatwe can minimise our automata s well as deter-minising them, and so share more structure (forexample, common processing at the end of therecognition process as well as the beginning).4 Recogn i t ion  and  parse  recoveryWe noted above that a parsing algorithmneeds to be able to access the tree thatan automaton has recognised.
The algo-rithm we describe below actually needs rathermore information than this, because it uses atwo-phase recognition/parse-recovery approach.The recognition phase only needs to know, foreach complete item, what the root label of thetree recognised is.
This can be recovered fromthe 'valid tree' annotation of the complete itemitself (there may be more than one valid tree,corresponding to a phrase which has more thanone parse which happen to have been merged to-gether).
Parse recovery, however, involves run-ning the recogniser 'backwards' over the com-pleted parse table, identifying for each item, theitems and actions which licensed it.A complication arises because the automata, es-pecially the merged automata, do not directlycorrespond to tree structure.
The recogniser re-turns the tree recognised, and a search of theparse table reveals the parser action which com-pleted its recognition, but that information initself may not be enough to locate exactly wherein the tree the action took place.
However, theadditional information required is static, andso can be pre-compiled as the automata them-selves are built up.
For each action transition(the action, plus the start and finish states)we record the tree address that the transitionreaches (we call this the action-site, or justa-site for short).
During parse recovery, whenthe parse table indicates an action that licensedan item, we look up the relevant ransition todiscover where in the tree (or trees, if we aretraversing several simultaneously) the presentitem must be, so that we can correctly constructa derivation tree.5 Techn ica l  deta i l s5.1 Const ruct ing  the  automataWe identify each node in an elementary tree 7with an e lementary  address 7/i.
The rootof 7 has the address 7/e where e is the emptystring.
Given a node 7/i, its n children are ad-dressed from left to right with the addresses7/il,..."//in, respectively.
For convenience,let anchor (7) and foot (7) denote the elemen-tary address of the node that is the anchor andfootnode (if it has one) of 7, respectively; andlabel (7/i) and parent (7/i) denote the label of7/i and the address of the parent of 7/i, respec-tively.In this paper we make the following assumup-tions about elementary trees.
Each tree has asingle anchor node and therefore a single spine 5.In the algorithms below we assume that nodesnot on the spine have no children.
In practice,not all elementary LTAG trees meet these con-ditions, and we discuss how the approach de-scribed here might be extended to the more gen-5The path from the root to the anchor node.374eral case in Section 6.Let "y/i be an elementary address of anode on the spine of 7 with n children"y/ i l , .
.
.
,7 / i k , .
.
.
,7~in for n > 1, where k issuch that 7/ ik dominates anchor (7).7/ ik+l  i f j= l&n>k"l/ij -1  i f2_<j<_knext(-y/ i j )= " l / i j+ l  i f k< j<n7/i otherwisenext defines a function that traverses a spine,starting at the anchor.
Traversal of an elemen-tary tree during recognition yields a sequence ofparser  act ions,  which we annotate as follows:the two actions A and ~ indicate a substitu-tion of a tree rooted with A to the left or right,respectively; A and +A indicate the presenceof the foot node, a node labelled A, to the leftor right, respectively; Finally A indicates anadjunct?on of a tree with root and foot labelledA.
These actions constitute the input languageof the automaton that traverses the tree.
Thisautomaton is defined as follows (note that weuse e-transitions between odes to ease the con-struction - we assume these are removed usinga standard algorithm).Let 9' be an elementary tree with terminal andnonterminal alphabets VT and VN, respectively.Each state of the following automaton specifiesthe elementary address 7/i  being visited.
Whenthe node is first visited we use the state _L\[-y/i\];when ready to move on we use the state T\[7/i\].Define as follows the finite state automatonM = (Q, E, \]_\[anchor (7)\],6, F).
Q is the setof states, E is the input alphabet, q0 is the ini-tial state, (~ is the transition relation, and F isthe set of final states.Q = { T\['l/i\], ?\['l/i\] I'l/i is an address in "l };= { A, IA };F = { T\[')'/e\] }; and6 includes the following transitions:(?\[foot ('l)\], _A., T\[foot ('l)\]) if foot (7) is to the rightof anchor ('l)(?\[foot ('/)\], +A_, T\[foot ('l)\]), if foot ('l) is to the leftof anchor ('l){ (T\['l/i\], e,?\[next ('l/i)\]) I "l/i is an address in 'li ce}{ (m\['y/i\], A T\['l/i\]) I "y/i substitution ode,label ('l/i) = A,"l/i to right of anchor (7) }{ (?\[7/i\], ~ ,  T\[7/i\]) I 7/i substitution ode,label ('l/i) = A,"l/i to left of anchor (7) }{ (?\['l/i\], 4 ,  T\['l/i\]) I "l/i adjunct?on nodelabel ('I/i) = A }{ (?\['l/i\], e, T\['l/i\]) \[ 7/i adjunct?on node }{ (T\[7/i\], ~__+, T\['l/i\]) \[ 7/i adjunct?on node,label ('l/i) = A }In order to recover derivation trees, we alsodefine the partial function a-site(q,a,q') for(q, a, q') E ~ which provides information aboutthe site within the e lementary tree of actionsoccurring in the automaton.a-site(q, a, q') = { "y/i if a ?
e & q' -- T\['l/i\]undefined otherwise5.2 Combin ing  AutomataSuppose we have a set of trees F --{71, .
.
.
,% }.
Let M~I , .
.
.
,M~, be the e-freeautomata that are built from members of theset F using the above construction, where for1 < k < n, Mk = (Qk, P,k, qk,~k, Fk).Construction of a single automaton for F is atwo step process.
First we build an automa-ton that accepts all elementary computationsfor trees in F; then we apply the standard au-tomaton determinization and minimization al-gorithms to produce an equivalent, compact au-tomaton.
The first step is achieved simply byintroducing a new initial state with e-transitionsto each of the qk:Let M = (Q, ~, qo, 6, F) whereQ = { qo } u Ul<k<.
Qi;~2 = U,<k<, P~kF = Ul<k<_,, Fk(~ = Ul<k<n(q0, e qk) U Ul<k<n 6k.We determinize and then minimize M usingthe standard set-of-states constructions to pro-duce Mr  -- (Q', P,, Q0, (V, F') .
Whenever twostates are merged in either the determinizingor minimizing algorithms the resulting state isnamed by the union of the states from which itis formed.For each transition (Q1, a, Q2) E (V we definethe function a-sites(Q1, a, Q2) to be a set of el-ementary nodes as follows:a-sites(Q1, a, Q2) = Uq, eq,,q=eq= a-site(ql, a, q2)Given a transition in Mr,  this function returnsall the nodes in all merged trees which that tran-375sition reaches.Finally, we define:cross(Q1, a, Q2) = { 7 \['y/i E a-sites(Q1, a, Q2) }This gives that subset of those trees whose el-ementary computations take the Mr throughstate Q1 to Q2.
These are the transition an-notations referred to above, used to constrainthe parser's et of valid trees.5.3 The Recognit ion PhaseThis section illustrates a simple bottom-upparsing algorithm that makes use of minimizedautomata produced from sets of trees that an-chor the same input symbol.The input to the parser takes the form of a se-quence of minimized automata, one for each ofthe symbols in the input.
Let the input stringbe w = at .
.
.ar~ and the associated automatabe M1,.
.
.Mn where Mk = (Qk, Ek, qk,(~k, Fk)for 1 _< k < n. Let treesof(Mk) = Fk where Fkis a set of the names of those elementary treesthat were used to construct he automata Mk.During the recognition phase of the algorithm,a set I of i tems are created.
An item hasthe form (T, q, \[l, r,l', r'\]) where T is a set ofelementary tree names, q is a automata stateand l, r, l', r '  ?
{ 0, .
.
.
, n, - } such that eitherl<_l'<_r ~<_ror l<rand l  ~=r '=- .
Thein-dices l, l', #, r are positions between input sym-bols (position 0 is before the first input symbolsand position n is after the final input symbol)and we use wp,p, to denote that substring of theinput w between positions p and p~.
I can beviewed as a four dimensional array, each entryof which contains a set of pairs comprising of aset of nonterminals and an automata state.Roughly speaking, an item (T, q, \[l, r, l', r\]) is in-cluded in I when for every 't ?
T, anchoredby some ak (where I < k < r and i f l  I ~ -then k < l ~ or r t < k); q is a state in Qk, suchthat some elementary subcomputation reachingq from the initial state, qk, of Mk is an ini-tial substring of the elementary computation for't that reaches the elementary address "t/i, thesubtree rooted at "t/i spans Wl,r, and if't/i dom-inates a foot node then that foot node spansWl, r, , otherwise l ~ = r ~ = - .The input is accepted if an item(T, qs,\[O,n,- , - \])  is added to I where Tcontains some initial tree rooted in the startsymbol S and qf ?
Fk for some k.When adding items to I we use the procedureadd(T, q, \[/, r, l', r'\]) which is defined such thatif there is already an entry (T ~, q, \[/, r, l ~, rq/ ?I for some T ~ then replace this with the entry(T U T', q, \[/, r, l', #\])6; otherwise add the newentry {T, q, \[l, r, l', r'\]) to I.I is initialized as follows.
For each k ?
{ 1,. .
.
,n } call add(T, qk,\[k- 1, k , - , - \ ] )  whereT = treesof(Mk) and qk is the initial state ofthe automata Mk.We now present he rules with which the com-plete set I is built.
These rules correspondclosely to the familiar steps in existing bottom-up LTAG parser, in particular, the way thatwe use the four indices is exactly the same asin other approaches (Vijay-Shanker and Joshi,1985).
As a result a standard control strategycan be used to control the order in which theserules are applied to existing entries of I.1.
If (T,q,\[ l ,r , l ' ,r ' \]),(T',qI,\[r,r",- ,- \]) e I,ql E Fk for some k, (q, A ,  q,) E ~k' forsome k r, label ( ' / /e)  = A from some 't' ET' & T" = T n cross(q,A, qt) then calladd(T", q', If, r", l', r'\]).2.
If (T, q, \[l, r, l r, rq), (T', ql, \[l", l, - ,  -\]) ?
I,ql ?
Fk for some k, (q,A,q~) ?
~k' forsome k t, label ('t~/e) = A from some 't~ ?T ~ & T" = T N cross(q,A,q~) then calladd(T", q', \[l", r, l', r'\]).3.
If (T,q, \ [ l , r , - , - \ ] )  ?
I, (q,_A.,q,) ?
~k forsome k & T' = T n cross(q,_A.,q') thenfor each r' such that r < r' < n callmadd(T', q', \[l, r', r, r'\]}.4.
If (T, q, \[l, r, - ,  - \ ] )  ?
I , (q,?A,q') ?
~kfor some k & T ~ = Tncross (q , .A ,q~)then for each I r such that 0 < l ~ < l calladd(T', q', \[l', r, l', l\]).5.
If (T,q,\[l,r,l',r'\]),(T',q/,\[l",r",l,r\]) ?
I,ql ?
Fk for some k, (q,A,q' )  ?
(fk, forsome k ~, label ('t~/e) = A from some 't~ ?T' & T" = T r'l cross(q, A,q,)  then calladd(T", q', \[l", r", l', r'\]).6This replacement is treated as a new entry in thetable.
If the old entry has already licenced other entries,this may result in some duplicate processing.
This couldbe eliminated by a more sophisticated treatment oftreesets.376The running time of this algorithm is O(n 6)since the last rule must be embedded within sixloops each of which varies with n. Note thatalthough the third and fourth rules both takeO(n) steps, they need only be embedded withinthe l and r loops.5.4 Recover ing  Parse  TreesOnce the set of items I has been completed, thefinal task of the parser is to a recover derivationtree 7.
This involves retracing the steps of therecognition process in reverse.
At each point,we look for a rule that would have caused theinclusion of item in I.
Each of these rules in-volves some transition (q, a, ql) ?
5k for some kwhere a is one of the parser actions, and fromthis transition we consult the set of elementaryaddresses in a-sites(q, a, q~) to establish how tobuild the derivation tree.
We eventually reachitems added during the initialization phase andthe process ends.
Given the way our parser hasbeen designed, some search will be needed tofind the items we need.
As usual, the need forsuch search can be reduced through the inclu-sion of pointers in items, though this is at thecost of increasing parsing time.
There are var-ious points in the following description wherenondeterminism exists.
By exploring all possi-ble paths, it would be straightforward to pro-duce an AND/OR derivation tree that encodesall derivation trees for the input string.We use the procedure der((T, q, If, r, l', r'\]), r)which completes the partial derivation tree r bybacking up through the moves of the automatain which q is a state.A derivation tree for the input is returnedby the call der((T, ql, \[0, n, - ,  - \]) ,  ~-) where(T, qs , \ [O,n , - , - \ ] )  ?
I such that T containssome initial tree 7 rooted with the start non-terminal S and ql is the final state of some au-tomata Mk, 1 <_ k <_ n. r is a derivation treecontaining just one node labelled with name %In general, on a call to der((T, q, \[l, r, l ~, rq), T)we examine I to find a rule that has caused thisitem to be included in I.
There are six rulesto consider, corresponding to the five recogniserrules, plus lexical introduction, as follows:1.
If (T', q', \[l, r", l', r'\]), (T ' ,  ql, \[r", r, - ,  -\]) ?7Derivation trees axe labelled with tree names andedges axe labelled with tree addresses.I, qs E Fk for some k, (q', A ,  q) E ~k' forsome k ~, "), is the label of the root of r,")' E T', label (7'/e) = A from some "y' E T"& "y/i e a-sites(q', A ,  q), then let r '  be thederivation tree containing a single nodelabelled "/', and let r '~ be the result of at-taching der((T", ql, Jr", r, - ,  - \]) ,  r ' )  underthe root of r with an edge labelled the treeaddress i.
We then complete the derivationtree by calling der((T', q', \[l, r I', l', r'\]), T').2.
I f (T ' ,q ' , \ [ r " , r , l ' , r ' \ ] ) , (T" ,q l , \ [ l , r " , - , - \ ] )  ?I, qs ?
Fk for some k, (q~, A ,  q) ?
5k, forsome k' ~, is the label of the root of T,~/ ?
T ~, label ("/~/e) = A from some "/~ ?
T"& ~/i ?
a-sites(q I, A ,  q), then let T' be thederivation tree containing a single nodelabelled -y~, and let T ~ be the result of at-taching der((T", ql, \[l, r ' ,  - ,  - \]) ,  r I) underthe root of T with an edge labelled the treeaddress i.
We then complete the derivationtree by calling der((T', q', \[r '~, r, l ~, rq), r'~).3.
If r = r ~, (T~,q~,\[l,l~,-,-\]) ?
I and(q~,_A,,q) ?
5k for some k, "y is thelabel of the root of 7-, ~/ ?
T'  andfoot ('),) ?
a-sites(q t,A?, q) then make thecall der((T',  q', \[l, l ' , - , - \ ] ) ,  r).4.
If / = l', (T', q', \[r', r, - ,  -\]) E I and(q,,+A,ql) ?
5k for some k, "), is thelabel of the root of ~-, -), E T ~ andfoot (~/) ?
a-sites(q', +A, q) then make thecall der((T',  ql, Jr', r, - ,  - \ ]) ,  r).5.
If (T~,q ', \[l',r'~,l~,r'\]), (T~I, qs, \[l,r,l ' ,r"\]) ?I, ql ?
Fk for some k, (q~, A ,  q) ?
5k, forsome k ~, ~, is the label of the root of r,"), ?
T ~, label ('y~/e) = A from some ~/' ?
T"and "I/i ?
a-sites(q', A ,q) ,  then let T' bethe derivation tree containing a single nodelabelled "/~, and let T" be the result of at-taching der((T", q/, \[l, r, l", r"\]), ~-') underthe root of r with an edge labelled the treeaddress i.
We then complete the derivationtree by calling der((T', ql, \[In, r 'l, l', r'\]), Tll).6.
If l + 1 = r, r ~ = l ~ ---- -- q is the initial stateof Mr, ")' is the label of the root ofT, ",/?
T,then return the final derivation tree T.6 D iscuss ionThe approach described here offers empiricalrather than formal improvements in perfor-mance.
In the worst case, none of the trees377wordcomebreakgiveno.
of trees automaton no.
of states no.
of transitions trees per state133 merged 898 1130 1minimised 50 130 11.86177 merged 1240 1587 1minimised 68 182 12.13337 merged 2494 3177 1minimised 83 233 20.25Table 1: DTG compaction results (from Carroll et al (1998)).in the grammar share any structure so no op-timisation is possible.
However, in the typi-cal case, there is scope for substantial structuresharing among closely related trees.
Carroll etal.
(1998) report preliminary results using thistechnique on a wide-coverage DTG (a variantof LTAG) grammar.
Table 1 gives statistics forthree common verbs in the grammar: the totalnumber of trees, the size of the merged automa-ton (before any optimisation has occurred) andthe size of the minimised automaton.
The fi-nal column gives the average of the number oftrees that share each state in the automaton.These figures show substantial optimisation ispossible, both in the space requirements of thegrammar and in the sharing of processing statebetween trees during parsing.As mentioned earlier, the algorithms we havepresented assume that elementary trees haveone anchor and one spine.
Some trees, how-ever, have secondary anchors (for example, asubcategorised preposition).
One possible wayof including such cases would be to constructautomata from secondary anchors up the sec-ondary spine to the main spine.
The automatafor both the primary and secondary anchorsassociated with a lexical item could then bemerged, minimized and used for parsing asabove.Using automata for parsing has a long his-tory dating back to transition etworks (Woods,1970).
More recent uses include Alshawi (1996)and Eisner (1997).
These approaches differ fromthe present paper in their use of automata spart of the grammar formalism itself.
Here,automata re used purely as a stepping-stoneto parser optimisation: we make no linguisticclaims about them.
Indeed one view of thiswork is that it frees the linguistic descriptionsfrom overt computational considerations.
Thiswork has perhaps more in common with thetechnology of LR parsing as a parser optimi-sation technique, and it would be interesting tocompare our approach with a direct applicationof LR ideas to LTAGs.ReferencesH.
Alshawi.
1996.
Head automata nd bilingualtilings: Translation with minimal representations.In ACL96, pages 167-176.J.
Carroll, N. Nicolov, O. Shaumyan, M. Smets, andD.
Weir.
1998.
Grammar compaction and computa-tion sharing in automaton-based parsing.
In Pro-ceedings of the First Workshop on Tabulation inParsing and Deduction, pages 16-25.J.
Chen and K. Vijay-Shanker.
1997.
Towards areduced-commitment D-theory style TAG parser.
InIWPT97, pages 18-29.J.
Eisner.
1997.
Bilexical grammars and a cubic-time probabilistic parser.
In IWPT97, pages 54-65.R.
Evans and D. Weir.
1997.
Automaton-basedparsing for lexicalized grammars.
In IWPT97, pages66-76.D.
A. Huffman.
1954.
The synthesis of sequentialswitching circuits.
J. Franklin Institute.A.
K. Joshi and Y. Schabes.
1991.
Tree-adjoininggrammars and lexicalized grammars.
In Maurice Ni-vat and Andreas Podelski, editors, Definability andRecognizability of Sets of Trees.
Elsevier.E.
F. Moore, 1956.
Automata Studies, chap-ter Gedanken experiments on sequential machines,pages 129-153.
Princeton University Press, N.J.Y.
Schabes and A. K. Joshi.
1988.
An Earley-typeparsing algorithm for tree adjoining rammars.
InACL88.K.
Vijay-Shanker and A. K. Joshi.
1985.
Some com-putational properties of tree adjoining rammars.
InACL85, pages 82-93.K.
Vijay-Shanker and D. Weir.
1993.
Parsing someconstrained grammar formalisms.
ComputationalLinguistics, 19(4):591-636.W.
A.
Woods.
1970.
Transition network gram-mars for natural language analysis.
Commun.
ACM,13:591-606.378
