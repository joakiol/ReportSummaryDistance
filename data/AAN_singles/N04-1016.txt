The Web as a Baseline: Evaluating the Performance ofUnsupervised Web-based Models for a Range of NLP TasksMirella LapataDepartment of Computer ScienceUniversity of Sheffield211 Portobello St., Sheffield S1 4DPmlap@dcs.shef.ac.ukFrank KellerSchool of InformaticsUniversity of Edinburgh2 Buccleuch Pl., Edinburgh EH8 9LWkeller@inf.ed.ac.ukAbstractPrevious work demonstrated that web countscan be used to approximate bigram frequen-cies, and thus should be useful for a wide va-riety of NLP tasks.
So far, only two gener-ation tasks (candidate selection for machinetranslation and confusion-set disambiguation)have been tested using web-scale data sets.
Thepresent paper investigates if these results gener-alize to tasks covering both syntax and seman-tics, both generation and analysis, and a largerrange of n-grams.
For the majority of tasks, wefind that simple, unsupervised models performbetter when n-gram frequencies are obtainedfrom the web rather than from a large corpus.However, in most cases, web-based models failto outperform more sophisticated state-of-the-art models trained on small corpora.
We ar-gue that web-based models should therefore beused as a baseline for, rather than an alternativeto, standard models.1 IntroductionKeller and Lapata (2003) investigated the validity of webcounts for a range of predicate-argument bigrams (verb-object, adjective-noun, and noun-noun bigrams).
Theypresented a simple method for retrieving bigram countsfrom the web by querying a search engine and demon-strated that web counts (a) correlate with frequencies ob-tained from a carefully edited, balanced corpus such asthe 100M words British National Corpus (BNC), (b) cor-relate with frequencies recreated using smoothing meth-ods in the case of unseen bigrams, (c) reliably predict hu-man plausibility judgments, and (d) yield state-of-the-artperformance on pseudo-disambiguation tasks.Keller and Lapata?s (2003) results suggest that web-based frequencies can be a viable alternative to bigramfrequencies obtained from smaller corpora or recreatedusing smoothing.
However, they do not demonstrate thatrealistic NLP tasks can benefit from web counts.
In or-der to show this, web counts would have to be applied toa diverse range of NLP tasks, both syntactic and seman-Task n POS Ling TypeMT candidate select.
1,2 V, N Sem GenerationSpelling correction 1,2,3 Any Syn/Sem GenerationAdjective ordering 1,2 Adj Sem GenerationCompound bracketing 1,2 N Syn AnalysisCompound interpret.
1,2,3 N, P Sem AnalysisCountability detection 1,2 N, Det Sem AnalysisTable 1: Overview of the tasks investigated in this paper(n: size of n-gram; POS: parts of speech; Ling: linguisticknowledge; Type: type of task)tic, involving analysis (e.g., disambiguation) and gener-ation (e.g., selection among competing outputs).
Also, itremains to be shown that the web-based approach scalesup to larger n-grams (e.g., trigrams), and to combinationsof different parts of speech (Keller and Lapata 2003 onlytested bigrams involving nouns, verbs, and adjectives).Another important question is whether web-based meth-ods, which are by definition unsupervised, can be com-petitive alternatives to supervised approaches used formost tasks in the literature.This paper aims to address these questions.
We start byusing web counts for two generation tasks for which theuse of large data sets has shown promising results: (a) tar-get language candidate selection for machine translation(Grefenstette, 1998) and (b) context sensitive spellingcorrection (Banko and Brill, 2001a,b).
Then we investi-gate the generality of the web-based approach by apply-ing it to a range of analysis and generations tasks, involv-ing both syntactic and semantic knowledge: (c) orderingof prenominal adjectives, (d) compound noun bracketing,(e) compound noun interpretation, and (f) noun count-ability detection.
Table 1 gives an overview of these tasksand their properties.In all cases, we propose a simple, unsupervised n-grambased model whose parameters are estimated using webcounts.
We compare this model both against a baseline(same model, but parameters estimated on the BNC) andagainst state-of-the-art models from the literature, whichare either supervised (i.e., use annotated training data) orunsupervised but rely on taxonomies to recreate missingcounts.2 MethodFollowing Keller and Lapata (2003), web counts for n-grams were obtained using a simple heuristic based onqueries to the search engine Altavista.1 In this approach,the web count for a given n-gram is simply the number ofhits (pages) returned by the search engine for the queriesgenerated for this n-gram.
Three different types of querieswere used for the NLP tasks in the present paper:Literal queries use the quoted n-gram directly as asearch term for Altavista (e.g., the bigram history changesexpands to the query "history changes").Near queries use Altavista?s NEAR operator to ex-pand the n-gram; a NEAR b means that a has to oc-cur in the same ten word window as b; the window istreated as a bag of words (e.g., history changes expandsto "history" NEAR "changes").Inflected queries are performed by expanding ann-gram into all its morphological forms.
These formsare then submitted as literal queries, and the result-ing hits are summed up (e.g., history changes ex-pands to "history change", "histories change","history changed", etc.).
John Carroll?s suite of mor-phological tools (morpha, morphg, and ana) was usedto generate inflected forms of verbs and nouns.2 In cer-tain cases (detailed below), determiners were inserted be-fore nouns in order to make it possible to recognize sim-ple NPs.
This insertion was limited to a/an, the, and theempty determiner (for bare plurals).All queries (other than the ones using the NEAR oper-ator) were performed as exact matches (using quotationmarks in Altavista).
All search terms were submitted tothe search engine in lower case.
If a query consists of asingle, highly frequent word (such as the), Altavista willreturn an error message.
In these cases, we set the webcount to a large constant (108).
This problem is limitedto unigrams, which were used in some of the models de-tailed below.
Sometimes the search engine fails to returna hit for a given n-gram (for any of its morphological vari-ants).
We smooth zero counts by setting them to .5.For all tasks, the web-based models are comparedagainst identical models whose parameters were esti-mated from the BNC (Burnard, 1995).
The BNC is astatic 100M word corpus of British English, which isabout 1000 times smaller than the web (Keller and La-pata, 2003).
Comparing the performance of the samemodel on the web and on the BNC allows us to assesshow much improvement can be expected simply by usinga larger data set.
The BNC counts were retrieved usingthe Gsearch corpus query tool (Corley et al, 2001); themorphological query expansion was the same as for webqueries; the NEAR operator was simulated by assuminga window of five words to the left and five to the right.1We did not use Google counts, as Google limits the numberof queries to 1000 per day, which makes the process of retriev-ing a large number of web counts very time consuming.2The tools can be downloaded from http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html.# best model on development set?
6 ?
(not) sign.
different from best BNC model on test set?
6 ?
(not) sign.
different from baseline?
6 ?
(not) sign.
different from best model in the literatureTable 2: Meaning of diacritics indicating statistical sig-nificance (?2 tests)Gsearch was used to search solely for adjacent words; noPOS information was incorporated in the queries, and noparsing was performed.For all of our tasks, we have to select either the best ofseveral possible models or the best parameter setting fora single model.
We therefore require a separate develop-ment set.
This was achieved by using the gold standarddata set from the literature for a given task and randomlydividing it into a development set and a test set (of equalsize).
We report the test set performance for all modelsfor a given task, and indicate which model shows optimalperformance on the development set (marked by a ?#?
inall subsequent tables).
We then compare the test set per-formance of this optimal model to the performance of themodels reported in the literature.
It is important to notethat the figures taken from the literature were typicallyobtained on the whole gold standard data set, and hencemay differ from the performance on our test set.
We workon the assumption that such differences are negligible.We use ?2 tests to determine whether the performanceof the best web model on the test set is significantly differ-ent from that of the best BNC model.
We also determinewhether both models differ significantly from the base-line and from the best model in the literature.
A set ofdiacritics is used to indicate significance throughout thispaper, see Table 2.3 Candidate Selection for MachineTranslationTarget word selection is a generation task that occurs inmachine translation (MT).
A word in a source languagecan often be translated into different words in the targetlanguage and the choice of the appropriate translation de-pends on a variety of semantic and pragmatic factors.
Thetask is illustrated in (1) where there are five translation al-ternatives for the German noun Geschichte listed in curlybrackets, the first being the correct one.
(1) a.
Die Geschichte a?ndert sich, nicht jedoch dieGeographie.b.
{History, story, tale, saga, strip} changes butgeography does not.Statistical approaches to target word selection rely onbilingual lexica to provide all possible translations ofwords in the source language.
Once the set of translationcandidates is generated, statistical information gatheredfrom target language corpora is used to select the mostappropriate alternative (Dagan and Itai, 1994).
The task issomewhat simplified by Grefenstette (1998) and Prescheret al (2000) who do not produce a translation of the en-tire sentence.
Instead, they focus on specific syntactic re-lations.
Grefenstette translates compounds from Germanand Spanish into English, and uses BNC frequencies asa filter for candidate translations.
He observes that thisapproach suffers from an acute data sparseness problemand goes on to obtain counts for candidate compoundsthrough web searches, thus achieving a translation accu-racy of 86?87%.Prescher et al (2000) concentrate on verbs and theirobjects.
Assuming that the target language translation ofthe verb is known, they select from the candidate transla-tions the noun that is semantically most compatible withthe verb.
The semantic fit between a verb and its argumentis modeled using a class-based lexicon that is derivedfrom unlabeled data using the expectation maximizationalgorithm (verb-argument model).
Prescher et al alsopropose a refined version of this approach that only mod-els the fit between a verb and its object (verb-objectmodel), disregarding other arguments of the verb.
Thetwo models are trained on the BNC and evaluated againsttwo corpora of 1,340 and 814 bilingual sentence pairs,with an average of 8.63 and 2.83 translations for the ob-ject noun, respectively.
Table 4 lists Prescher et al?s re-sults for the two corpora and for both models togetherwith a random baseline (select a target noun at random)and a frequency baseline (select the most frequent targetnoun).Grefenstette?s (1998) evaluation was restricted to com-pounds that are listed in a dictionary.
These com-pounds are presumably well-established and fairly fre-quent, which makes it easy to obtain reliable web fre-quencies.
We wanted to test if the web-based approachextends from lexicalized compounds to productive syn-tactic units for which dictionary entries do not exist.
Wetherefore performed our evaluation using Prescher et al?s(2000) test set of verb-object pairs.
Web counts were re-trieved for all possible verb-object translations; the mostlikely one was selected using either co-occurrence fre-quency ( f (v,n)) or conditional probability ( f (v,n)/ f (n)).The web counts were gathered using inflected queries in-volving the verb, a determiner, and the object (see Sec-tion 2).
Table 3 compares the web-based models againstthe BNC models.
For both the high ambiguity and thelow ambiguity data set, we find that the performanceof the best Altavista model is not significantly differentfrom that of the best BNC model.
Table 4 compares oursimple, unsupervised methods with the two sophisticatedclass-based models discussed above.
The results showthat there is no significant difference in performance be-tween the best model reported in the literature and thebest Altavista or the best BNC model.
However, bothmodels significantly outperform the baseline.
This holdsfor both the high and low ambiguity data sets.Altavista BNChigh low high lowModel ambig ambig ambig ambigf (v,n) 45.74 68.73#6 ?
45.89# 70.06#f (v,n)/ f (n) 45.16#6 ?
64.96 46.18 66.07Table 3: Performance of Altavista counts and BNC countsfor candidate selection for MT (data from Prescher et al2000)high lowModel ambig ambigRandom baseline 14.20 45.90Frequency baseline 31.90 45.50Prescher et al (2000): verb-argument 43.30 61.50Best Altavista 45.16?6 ?
68.73?6 ?Best BNC 45.89?6 ?
70.06?6 ?Prescher et al (2000): verb-object 49.40 68.20Table 4: Performance comparison with the literature forcandidate selection for MT4 Context-sensitive Spelling CorrectionContext-sensitive spelling correction is the task of cor-recting spelling errors that result in valid words.
Sucha spelling error is illustrated in (4) where principal wastyped when principle was intended.
(2) Introduction of the dialogue principal proved strik-ingly effective.The task can be viewed as generation task, as it consistsof choosing between alternative surface realizations of aword.
This choice is typically modeled by confusion setssuch as {principal, principle} or {then, than} under theassumption that each word in the set could be mistakenlytyped when another word in the set was intended.
Thetask is to infer which word in a confusion set is the cor-rect one in a given context.
This choice can be either syn-tactic (as for {then, than}) or semantic (as for {principal,principle}).A number of machine learning methods have been pro-posed for context-sensitive spelling correction.
These in-clude a variety of Bayesian classifiers (Golding, 1995;Golding and Schabes, 1996), decision lists (Golding,1995) transformation-based learning (Mangu and Brill,1997), Latent Semantic Analysis (LSA) (Jones andMartin, 1997), multiplicative weight update algorithms(Golding and Roth, 1999), and augmented mixture mod-els (Cucerzan and Yarowsky, 2002).
Despite their differ-ences, most approaches use two types of features: contextwords and collocations.
Context word features record thepresence of a word within a fixed window around the tar-get word (bag of words); collocational features capturethe syntactic environment of the target word and are usu-ally represented by a small number of words and/or part-of-speech tags to the left or right of the target word.The results obtained by a variety of classification meth-ods are given in Table 6.
All methods use either the fullset or a subset of 18 confusion sets originally gathered byGolding (1995).
Most methods are trained and tested onModel Alta BNC Model Alta BNCf (t) 72.98 70.00 f (w1, t,w2)/ f (t) 87.77 76.33f (w1, t) 84.40 83.02 f (w1,w2, t)/ f (t) 86.27 74.47f (t,w1) 84.89 82.74 f (t,w2,w2)/ f (t) 84.94 74.23f (w1, t,w2) 89.24#*77.13 f (w1, t,w2)/ f (w1, t) 80.70 73.69f (w1,w2, t) 87.13 74.89 f (w1, t,w2)/ f (t,w2) 82.24 75.10f (t,w1,w2) 84.68 75.08 f (w1,w2, t)/ f (w2, t) 72.11 69.28f (w1, t)/ f (t) 82.81 77.84 f (t,w1,w2)/ f (t,w1) 75.65 72.57f (t,w1)/ f (t) 77.49 80.71#Table 5: Performance of Altavista counts and BNCcounts for context sensitive spelling correction (data fromCucerzan and Yarowsky 2002)Model AccuracyBaseline BNC 70.00Baseline Altavista 72.98Best BNC 80.71?
?Golding (1995) 81.40Jones and Martin (1997) 84.26Best Altavista 89.24?
?Golding and Schabes (1996) 89.82Mangu and Brill (1997) 92.79Cucerzan and Yarowsky (2002) 92.20Golding and Roth (1999) 94.23Table 6: Performance comparison with the literature forcontext sensitive spelling correctionthe Brown corpus, using 80% for training and 20% fortesting.3We devised a simple, unsupervised method forperforming spelling correction using web counts.The method takes into account collocational features,i.e., words that are adjacent to the target word.
For eachword in the confusion set, we used the web to estimatehow frequently it co-occurs with a word or a pair of wordsimmediately to its left or right.
Disambiguation is thenperformed by selecting the word in the confusion set withthe highest co-occurrence frequency or probability.
Theweb counts were retrieved using literal queries (see Sec-tion 2).
Ties are resolved by comparing the unigram fre-quencies of the words in the confusion set and defaultingto the word with the highest one.
Table 5 shows the typesof collocations we considered and their corresponding ac-curacy.
The baseline ( f (t)) in Table 5 was obtained byalways choosing the most frequent unigram in the confu-sion set.
We used the same test set (2056 tokens from theBrown corpus) and confusion sets as Golding and Sch-abes (1996), Mangu and Brill (1997), and Cucerzan andYarowsky (2002).Table 5 shows that the best result (89.24%) for the web-based approach is obtained with a context of one wordto the left and one word to the right of the target word( f (w1, t,w2)).
The BNC-based models perform consis-tently worse than the web-based models with the excep-tion of f (t,w1)/t; the best Altavista model performs sig-nificantly better than the best BNC model.
Table 6 shows3An exception is Golding (1995), who uses the entire Browncorpus for training (1M words) and 3/4 of the Wall Street Jour-nal corpus (Marcus et al, 1993) for testing.that both the best Altavista model and the best BNCmodel outperform their respective baselines.
A compari-son with the literature shows that the best Altavista modeloutperforms Golding (1995), Jones and Martin (1997)and performs similar to Golding and Schabes (1996).
Thehighest accuracy on the task is achieved by the class ofmultiplicative weight-update algorithms such as Winnow(Golding and Roth, 1999).
Both the best BNC model andthe best Altavista model perform significantly worse thanthis model.
Note that Golding and Roth (1999) use al-gorithms that can handle large numbers of features andare robust to noise.
Our method uses a very small featureset, it relies only on co-occurrence frequencies and doesnot have access to POS information (the latter has beenshown to have an improvement on confusion sets whosewords belong to different parts of speech).
An advantageof our method is that it can be used for a large numberof confusion sets without relying on the availability oftraining data.5 Ordering of Prenominal AdjectivesThe ordering of prenominal modifiers is important fornatural language generation systems where the text mustbe both fluent and grammatical.
For example, the se-quence big fat Greek wedding is perfectly acceptable,whereas fat Greek big wedding sounds odd.
The orderingof prenominal adjectives has sparked a great deal of the-oretical debate (see Shaw and Hatzivassiloglou 1999 foran overview) and efforts have concentrated on definingrules based on semantic criteria that account for differentorders (e.g., age ?
color, value ?
dimension).Data intensive approaches to the ordering problem relyon corpora for gathering evidence for the likelihood ofdifferent orders.
They rest on the hypothesis that the rel-ative order of premodifiers is fixed, and independent ofcontext and the noun being modified.
The simplest strat-egy is what Shaw and Hatzivassiloglou (1999) call di-rect evidence.
Given an adjective pair {a,b}, they counthow many times ?a,b?
and ?b,a?
appear in the corpus andchoose the pair with the highest frequency.Unfortunately the direct evidence method performspoorly when a given order is unseen in the trainingdata.
To compensate for this, Shaw and Hatzivassiloglou(1999) propose to compute the transitive closure of theordering relation: if a ?
c and c ?
b, then a ?
b. Mal-ouf (2000) further proposes a back-off bigram modelof adjective pairs for choosing among alternative orders(P(?a,b?|{a,b}) vs. P(?b,a?|{a,b})).
He also proposespositional probabilities as a means of estimating howlikely it is for a given adjective a to appear first in a se-quence by looking at each pair in the training data thatcontains the adjective a and recording its position.
Fi-nally, he uses memory-based learning as a means to en-code morphological and semantic similarities among dif-ferent adjective orders.
Each adjective pair ab is encodedas a vector of 16 features (the last eight characters of aand the last eight characters of b) and a class (?a,b?
orModel Altavista BNCf (a1,a2) : f (a2,a1) 89.6#?6 ?
80.4#?f (a1,a2)/ f (a2) : f (a2,a1)/ f (a1) 83.2 77.0f (a1,a2)/ f (a1) : f (a2,a1)/ f (a2) 80.2 80.6Malouf (2000): memory-based ?
91.0Table 7: Performance of Altavista counts and BNC countsfor adjective ordering (data from Malouf 2000)?b,a?
).Malouf (2000) extracted 263,838 individual pairs ofadjectives from the BNC which he randomly partitionedinto test (10%) and training data (90%) and evaluatedall the above methods for ordering prenominal adjec-tives.
His results showed that a memory-based classi-fier that uses morphological information as well as po-sitional probabilities as features outperforms all othermethods (see Table 7).
For the ordering task we restrictedourselves to the direct evidence strategy which simplychooses the adjective order with the highest frequencyor probability (see Table 7).
Web counts were obtainedby submitting literal queries to Altavista (see Section 2).We used the same 263,838 adjective pairs that Malouf ex-tracted from the BNC.
These were randomly partitionedinto a training (90%) and test corpus (10%).
The testcorpus contained 26,271 adjective pairs.
Given that sub-mitting 26,271 queries to Altavista would be fairly time-consuming, a random sample of 1000 sequences was ob-tained from the test corpus and the web frequencies ofthese pairs were retrieved.
The best Altavista model sig-nificantly outperformed the best BNC model, as indicatedin Table 7.
We also found that there was no significantdifference between the best Altavista model and the bestmodel reported by Malouf, a supervised method usingpositional probability estimates from the BNC and mor-phological variants.6 Bracketing of Compound NounsThe first analysis task we consider is the syntactic disam-biguation of compound nouns, which has received a fairamount of attention in the NLP literature (Pustejovskyet al, 1993; Resnik, 1993; Lauer, 1995).
The task canbe summarized as follows: given a three word compoundn1 n3 n3, determine the correct binary bracketing of theword sequence (see (3) for an example).
(3) a.
[[backup compiler] disk]b.
[backup [compiler disk]]Previous approaches typically compare different brack-etings and choose the most likely one.
The adjacencymodel compares [n1 n2] against [n2 n3] and adopts a rightbranching analysis if [n2 n3] is more likely than [n1 n2].The dependency model compares [n1 n2] against [n1 n3]and adopts a right branching analysis if [n1 n3] is morelikely than [n1 n2].The simplest model of compound noun disambiguationcompares the frequencies of the two competing analysesand opts for the most frequent one (Pustejovsky et al,Model Alta BNCBaseline 63.93 63.93f (n1,n2) : f (n2,n3) 77.86 66.39f (n1,n2) : f (n1,n3) 78.68#?
65.57f (n1,n2)/ f (n1) : f (n2,n3)/ f (n2) 68.85 65.57f (n1,n2)/ f (n2) : f (n2,n3)/ f (n3) 70.49 63.11f (n1,n2)/ f (n2) : f (n1,n3)/ f (n3) 80.32 66.39f (n1,n2) : f (n2,n3) (NEAR) 68.03 63.11f (n1,n2) : f (n1,n3) (NEAR) 71.31 67.21f (n1,n2)/ f (n1) : f (n2,n3)/ f (n2) (NEAR) 61.47 62.29f (n1,n2)/ f (n2) : f (n2,n3)/ f (n3) (NEAR) 65.57 57.37f (n1,n2)/ f (n2) : f (n1,n3)/ f (n3) (NEAR) 75.40 68.03#Table 8: Performance of Altavista counts and BNC countsfor compound bracketing (data from Lauer 1995)Model AccuracyBaseline 63.93Best BNC 68.036 ?
?Lauer (1995): adjacency 68.90Lauer (1995): dependency 77.50Best Altavista 78.68?6 ?Lauer (1995): tuned 80.70Upper bound 81.50Table 9: Performance comparison with the literature forcompound bracketing1993).
Lauer (1995) proposes an unsupervised methodfor estimating the frequencies of the competing brack-etings based on a taxonomy or a thesaurus.
He uses aprobability ratio to compare the probability of the left-branching analysis to that of the right-branching (see (4)for the dependency model and (5) for the adjacencymodel).Rdep =?ti?cats(wi)P(t1 ?
t2)P(t2 ?
t3)?ti?cats(wi)P(t1 ?
t3)P(t2 ?
t3)(4)Radj =?ti?cats(wi)P(t1 ?
t2)?ti?cats(wi)P(t2 ?
t3)(5)Here t1, t2 and t3 are conceptual categories in the taxon-omy or thesaurus, and the nouns w1 .
.
.wi are members ofthese categories.
The estimation of probabilities over con-cepts (rather than words) reduces the number of modelparameters and effectively decreases the amount of train-ing data required.
The probability P(t1 ?
t2) denotes themodification of a category t2 by a category t1.Lauer (1995) tested both the adjacency and de-pendency models on 244 compounds extracted fromGrolier?s encyclopedia, a corpus of 8 million words.
Fre-quencies for the two models were obtained from the samecorpus and from Roget?s thesaurus (version 1911) bycounting pairs of nouns that are either strictly adjacent orco-occur within a window of a fixed size (e.g., two, three,fifty, or hundred words).
The majority of the bracketingsin our test set were left-branching, yielding a baselineof 63.93% (see Table 9).
Lauer?s best results (77.50%)were obtained with the dependency model and a trainingscheme which takes strictly adjacent nouns into account.Performance increased further by 3.2% when POS tagswere taken into account.
The results for this tuned modelare also given in Table 9.
Finally, Lauer conducted an ex-periment with human judges to assess the upper boundfor the bracketing task.
An average accuracy of 81.50%was obtained.We replicated Lauer?s (1995) results for compoundnoun bracketing using the same test set.
We comparedthe performance of the adjacency and dependency mod-els (see (4) and (5)), but instead of relying on a corpus anda thesaurus, we estimated the relevant probabilities us-ing web counts.
The latter were obtained using inflectedqueries (see Section 2) and Altavista?s NEAR operator.Ties were resolved by defaulting to the most frequentanalysis (i.e., left-branching).
To gauge the performanceof the web-based models we compared them against theirBNC-based alternatives; the performance of the best Al-tavista model was significantly higher than that of thebest BNC model (see Table 8).
A comparison with theliterature (see Table 9) shows that the best BNC modelfails to significantly outperform the baseline, and it per-forms significantly worse than the best model in the liter-ature (Lauer?s tuned model).
The best Altavista model, onthe other hand, is not significantly different from Lauer?stuned model and significantly outperforms the baseline.Hence we achieve the same performance as Lauer with-out recourse to a predefined taxonomy or a thesaurus.7 Interpretation of Compound NounsThe second analysis task we consider is the semanticinterpretation of compound nouns.
Most previous ap-proaches to this problem have focused on the interpre-tation of two word compounds whose nouns are relatedvia a basic set of semantic relations (e.g., CAUSE relatesonion tears, FOR relates pet spray).
The majority of pro-posals are symbolic and therefore limited to a specificdomain due to the large effort involved in hand-codingsemantic information (see Lauer 1995 for an extensiveoverview).Lauer (1995) is the first to propose and evaluate an un-supervised probabilistic model of compound noun inter-pretation for domain independent text.
By recasting theinterpretation problem in terms of paraphrasing, Lauerassumes that the semantic relations of compound headsand modifiers can be expressed via prepositions that (incontrast to abstract semantic relations) can be found in acorpus.
For example, in order to interpret war story, oneneeds to find in a corpus related paraphrases: story aboutthe war, story of the war, story in the war, etc.
Lauer useseight prepositions for the paraphrasing task (of, for, in,at, on, from, with, about).
A simple model of compoundnoun paraphrasing is shown in (6):p?
= argmaxpP(p|n1,n2)(6)Lauer (1995) points out that the above model containsone parameter for every triple ?p,n1,n2?, and as a resultModel Altavista BNCf (n1, p) f (p,n2) 50.71 27.85#f (n1, p,n2) 55.71#* 11.42f (n1, p) f (p,n2)/ f (p) 47.14 26.42f (n1, p,n2)/ f (p) 55.00 10.71Table 10: Performance of Altavista counts and BNCcounts for compound interpretation (data from Lauer1995)Model AccuracyBest BNC 27.856 ?
?Lauer (1995): concept-based 28.00Baseline 33.00Lauer (1995): word-based 40.00Best Altavista 55.71?
?Table 11: Performance comparison with the literature forcompound interpretationhundreds of millions of training instances would be nec-essary.
As an alternative to (6), he proposes the modelin (7) which combines the probability of the modifiergiven a certain preposition with the probability of thehead given the same preposition, and assumes that thesetwo probabilities are independent.p?
= argmaxp ?t1 ?
cats(n1)t2 ?
cats(n2)P(t1|p)P(t2|p)(7)Here, t1 and t2 represent concepts in Roget?s thesaurus.Lauer (1995) also experimented with a lexicalized ver-sion of (7) where probabilities are calculated on the basisof word (rather than concept) frequencies which Lauerobtained from Grolier?s encyclopedia heuristically viapattern matching.Lauer (1995) tested the model in (7) on 282 com-pounds that he selected randomly from Grolier?s encyclo-pedia and annotated with their paraphrasing prepositions.The preposition of accounted for 33% of the paraphrasesin this data set (see Baseline in Table 11).
The concept-based model (see (7)) achieved an accuracy of 28% onthis test set, whereas its lexicalized version reached anaccuracy of 40% (see Table 11).We attempted the interpretation task with the lexi-calized version of the bigram model (see (7)), but alsotried the more data intensive trigram model (see (6)),again in its lexicalized form.
Furthermore, we experi-mented with several conditional and unconditional vari-ants of (7) and (6).
Co-occurrence frequencies were es-timated from the web using inflected queries (see Sec-tion 2).
Determiners were inserted before nouns result-ing in queries of the type story/stories about andabout the/a/0 war/wars for the compound war story.As shown in Table 10, the best performance was ob-tained using the web-based trigram model ( f (n1, p,n2));it significantly outperformed the best BNC model.
Thecomparison with the literature in Table 11 showed thatthe best Altavista model significantly outperformed boththe baseline and the best model in the literature (Lauer?sword-based model).
The BNC model, on the other hand,Altavista BNCModel Count Uncount Count Uncountf (n) 87.01 90.13 87.32# 90.39#f (det,n) 88.38#6 ?
91.22#6 ?
51.01 50.23f (det,n)/ f (n) 83.19 85.38 50.95 50.23Backoff 87.01 89.80 ?
?Table 12: Performance of Altavista counts and BNCcounts for noun countability detection (data from Bald-win and Bond 2003)achieved a performance that is not significantly differentfrom the baseline, and significantly worse than Lauer?sbest model.8 Noun Countability DetectionThe next analysis task that we consider is the problemof determining the countability of nouns.
Countability isthe semantic property that determines whether a noun canoccur in singular and plural forms, and affects the rangeof permissible modifiers.
In English, nouns are typicallyeither countable (e.g., one dog, two dogs) or uncountable(e.g., some peace, *one peace, *two peaces).Baldwin and Bond (2003) propose a method for auto-matically learning the countability of English nouns fromthe BNC.
They obtain information about noun countabil-ity by merging lexical entries from COMLEX (Grishmanet al, 1994) and the ALTJ/E Japanese-to-English seman-tic transfer dictionary (Ikehara et al, 1991).
Words areclassified into four classes: countable, uncountable, bi-partite (e.g., trousers), and plural only (e.g., goods).
Amemory-based classifier is used to learn the four-way dis-tinction on the basis of several linguistically motivatedfeatures such as: number of the head noun, number of themodifier, subject-verb agreement, plural determiners.We devised unsupervised models for the countabilitylearning task and evaluated their performance on Bald-win and Bond?s (2003) test data.
We concentrated solelyon countable and uncountable nouns, as they accountfor the vast majority of the data.
Four models weretested: (a) compare the frequency of the singular andplural forms of the noun; (b) compare the frequency ofdeterminer-noun pairs that are characteristic of countableor uncountable nouns; the determiners used were manyfor countable and much for uncountable ones; (c) sameas model (b), but the det-noun frequencies are normalizedby the frequency of the noun; (d) backoff: try to makea decision using det-noun frequencies; if these are toosparse, back off to singular/plural frequencies.Unigram and bigram frequencies were estimated fromthe web using literal queries; for models (a)?
(c) a thresh-old parameter was optimized on the development set (thisparameter determines the ratio of singular/plural frequen-cies or det-noun frequencies above which a noun wasconsidered as countable).
For model (b), an additionalbackoff parameter was used, specifying the minimum fre-quency that triggers backoff.The models and their performance on the test set areModel Count UncountBaseline 74.60 78.30Best BNC 87.32??
90.39?
?Best Altavista 88.38??
91.22?
?Baldwin and Bond (2003) 93.90 95.20Table 13: Performance comparison with the literature fornoun countability detectionlisted in Table 12.
The best Altavista model is the condi-tional det-noun model ( f (det,n)/ f (n)), which achieves88.38% on countable and 91.22% on uncountable nouns.On the BNC, the simple unigram model performs best.
Itsperformance is not statistically different from that of thebest Altavista model.
Note that for the BNC models, datasparseness means the det-noun models perform poorly,which is why the backoff model was not attempted here.Table 13 shows that both the Altavista model and BNCmodel significantly outperform the baseline (relative fre-quency of the majority class on the gold-standard data).The comparison with the literature shows that both theAltavista and the BNC model perform significantly worsethan the best model proposed by Baldwin and Bond(2003); this is a supervised model that uses many morefeatures than just singular/plural frequency and det-nounfrequency.9 ConclusionsWe showed that simple, unsupervised models using webcounts can be devised for a variety of NLP tasks.
Thetasks were selected so that they cover both syntax and se-mantics, both generation and analysis, and a wider rangeof n-grams than have been previously used.For all but two tasks (candidate selection for MT andnoun countability detection) we found that simple, un-supervised models perform significantly better when n-gram frequencies are obtained from the web rather thanfrom a standard large corpus.
This result is consistentwith Keller and Lapata?s (2003) findings that the webyields better counts than the BNC.
The reason for thisseems to be that the web is much larger than the BNC(about 1000 times); the size seems to compensate forthe fact that simple heuristics were used to obtain webcounts, and for the noise inherent in web data.Our results were less encouraging when it comes tocomparisons with state-of-the-art models.
We found thatin all but one case, web-based models fail to significantlyoutperform the state of the art.
The exception was com-pound noun interpretation, for which the Altavista modelwas significantly better than the Lauer?s (1995) model.For three tasks (candidate selection for MT, adjective or-dering, and compound noun bracketing), we found thatthe performance of the web-based models was not signif-icantly different from the performance of the best modelsreported in the literature.Note that for all the tasks we investigated, the bestperformance in the literature was obtained by supervisedmodels that have access not only to simple bigram or tri-gram frequencies, but also to linguistic information suchas part-of-speech tags, semantic restrictions, or context(or a thesaurus, in the case of Lauer?s models).
When un-supervised web-based models are compared against su-pervised methods that employ a wide variety of features,we observe that having access to linguistic informationmakes up for the lack of vast amounts of data.Our results therefore indicate that large data sets suchas those obtained from the web are not the panacea thatthey are claimed to be (at least implicitly) by authorssuch as Grefenstette (1998) and Keller and Lapata (2003).Rather, in our opinion, web-based models should be usedas a new baseline for NLP tasks.
The web baseline indi-cates how much can be achieved with a simple, unsuper-vised model based on n-grams with access to a huge dataset.
This baseline is more realistic than baselines obtainedfrom standard corpora; it is generally harder to beat, asour comparisons with the BNC baseline throughout thispaper have shown.Note that for certain tasks, the performance of a webbaseline model might actually be sufficient, so that the ef-fort of constructing a sophisticated supervised model andannotating the necessary training data can be avoided.Another possibility that needs further investigation is thecombination of web-based models with supervised meth-ods.
This can be done with ensemble learning methodsor simply by using web-based frequencies (or probabil-ities) as features (in addition to linguistically motivatedfeatures) to train supervised classifiers.AcknowledgmentsWe are grateful to Tim Baldwin, Silviu Cucerzan, MarkLauer, Rob Malouf, Detelef Prescher, and Adwait Ratna-parkhi for making their data sets available.ReferencesBaldwin, Timothy and Francis Bond.
2003.
Learning the count-ability of English nouns from corpus data.
In Proceedingsof the 41st Annual Meeting of the Association for Computa-tional Linguistics.
Sapporo, Japan, pages 463?470.Banko, Michele and Eric Brill.
2001a.
Mitigating the paucity-of-data problem: Exploring the effect of training corpus sizeon classifier performance for natural language processing.In James Allan, editor, Proceedings of the 1st InternationalConference on Human Language Technology Research.
Mor-gan Kaufmann, San Francisco.Banko, Michele and Eric Brill.
2001b.
Scaling to very verylarge corpora for natural language disambiguation.
In Pro-ceedings of the 39th Annual Meeting of the Association forComputational Linguistics.
Toulouse, France.Burnard, Lou.
1995.
The Users Reference Guide for the BritishNational Corpus.
British National Corpus Consortium, Ox-ford University Computing Service.Corley, Steffan, Martin Corley, Frank Keller, Matthew W.Crocker, and Shari Trewin.
2001.
Finding syntactic struc-ture in unparsed corpora: The Gsearch corpus query system.Computers and the Humanities 35(2):81?94.Cucerzan, Silviu and David Yarowsky.
2002.
Augmented mix-ture models for lexical disambiguation.
In Jan Hajic?
and YujiMatsumoto, editors, Proceedings of the Conference on Em-pirical Methods in Natural Language Processing.
Philadel-phia, PA, pages 33?40.Dagan, Ido and Alon Itai.
1994.
Machine translation diver-gences: A formal description and proposed solution.
Com-putational Linguistics 20(4):563?597.Golding, Andrew R. 1995.
A Bayesian hybrid method forcontext-sensitive spelling correction.
In David Yarowsky andKenneth W. Church, editors, Proceedings of the 3rd Work-shop on Very Large Corpora.
Cambridge, MA, pages 39?53.Golding, Andrew R. and Dan Roth.
1999.
A winnow-basedapproach to context sensitive spelling correction.
MachineLearning 34(1?3):1?25.Golding, Andrew R. and Yves Schabes.
1996.
Combin-ing trigram-based and feature-based methods for context-sensitive spelling correction.
In Proceedings of the 34th An-nual Meeting of the Association for Computational Linguis-tics.
Santa Cruz, CA, pages 71?78.Grefenstette, Gregory.
1998.
The World Wide Web as a resourcefor example-based machine translation tasks.
In Proceedingsof the ASLIB Conference on Translating and the Computer.London.Grishman, Ralph, Catherine Macleod, and Adam Meyers.
1994.COMLEX syntax: Building a computational lexicon.
In Pro-ceedings of the 15th International Conference on Computa-tional Linguistics.
Kyoto, Japan, pages 268?272.Ikehara, Satoru, Satoshi Shirai, Akio Yokoo, and HiromiNakaiwa.
1991.
Toward an MT system without pre-editingeffects of new methods in ALT-J/E.
In Proceedings of theThird Machine Translation Summit.
Washington, DC, pages101?106.Jones, Michael P. and James H. Martin.
1997.
Contextualspelling correction using latent semantic analysis.
In Pro-ceedings of the 5th Conference on Applied Natural LanguageProcessing.
Washington, DC, pages 166?173.Keller, Frank and Mirella Lapata.
2003.
Using the web to obtainfrequencies for unseen bigrams.
Computational Linguistics29(3):459?484.Lauer, Mark.
1995.
Corpus statistics meet the noun compound:Some empirical results.
In Proceedings of the 33rd AnnualMeeting of the Association for Computational Linguistics.Cambridge, MA, pages 47?54.Malouf, Robert.
2000.
The order of prenominal adjectives innatural language generation.
In Proceedings of the 38th An-nual Meeting of the Association for Computational Linguis-tics.
Hong Kong, pages 85?92.Mangu, Lidia and Eric Brill.
1997.
Automatic rule acquisi-tion of spelling correction.
In Proceedings of the 14th Inter-national Conference on Machine Learning.
Nashville, Ten-nessee, pages 187?194.Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: The Penn treebank.
Computational Linguistics19(2):313?330.Prescher, Detlef, Stefan Riezler, and Mats Rooth.
2000.
Usinga probabilistic class-based lexicon for lexical ambiguity res-olution.
In Proceedings of the 18th International Conferenceon Computational Linguistics.
Saarbru?cken, Germany, pages649?655.Pustejovsky, James, Sabine Bergler, and Peter Anick.
1993.Lexical semantic techniques for corpus analysis.
Computa-tional Linguistics 19(3):331?358.Resnik, Philip Stuart.
1993.
Selection and Information: AClass-Based Approach to Lexical Relationships.
Ph.D. the-sis, University of Pennsylvania.Shaw, James and Vassilis Hatzivassiloglou.
1999.
Orderingamong premodifiers.
In Proceedings of the 37th AnnualMeeting of the Association for Computational Linguistics.College Park, MD, pages 135?143.
