AbstractWe demonstrate a new research approach to theproblem of predicting the reading difficulty of atext passage, by recasting readability in terms ofstatistical language modeling.
We derive a measurebased on an extension of multinomial na?ve Bayesclassification that combines multiple languagemodels to estimate the most likely grade level for agiven passage.
The resulting classifier is not spe-cific to any particular subject and can be trainedwith relatively little labeled data.
We perform pre-dictions for individual Web pages in English andcompare our performance to widely-used semanticvariables from traditional readability measures.
Weshow that with minimal changes, the classifier maybe retrained for use with French Web documents.For both English and French, the classifier main-tains consistently good correlation with labeledgrade level (0.63 to 0.79) across all test sets.
Sometraditional semantic variables such as type-tokenratio gave the best performance on commercial cal-ibrated test passages, while our language modelingapproach gave better accuracy for Web documentsand very short passages (less than 10 words).1 IntroductionIn the course of constructing a search engine for stu-dents, we wanted a method for retrieving Web pagesthat were not only relevant to a student's query, but alsowell-matched to their reading ability.
Widely-used tra-ditional readability formulas such as Flesch-Kincaidusually perform poorly in this scenario.
Such formulasmake certain assumptions about the text: for example,that the sample has at least 100 words and uses well-defined sentences.
Neither of these assumptions needbe true for Web pages or other non-traditional docu-ments.
We seek a more robust technique for predictingreading difficulty that works well on a wide variety ofdocument types.To do this, we turn to simple techniques from statis-tical language modeling.
Advances in this field in thepast 20 years, along with greater access to training data,make the application of such techniques to readabilityquite timely.
While traditional formulas are based onlinear regression with two or three variables, statisticallanguage models can capture more detailed patterns ofindividual word usage.
As we show in our evaluation,this generally results in better accuracy for Web docu-ments and very short passages (less than 10 words).Another benefit of a language modeling approach is thatwe obtain a probability distribution across all grademodels, not just a single grade prediction.Statistical models of text rely on training data, so inSection 2 we describe our Web training corpus and notesome trends that are evident in word usage.
Section 3summarizes related work on readability, focusing onexisting vocabulary-based measures that can be thoughtof as simplified language model techniques.
Section 4defines the modified multinomial na?ve Bayes model.Section 5 describes our smoothing and feature selectiontechniques.
Section 6 evaluates our model's generaliza-tion performance, accuracy on short passages, and sen-sitivity to the amount of training data.
Sections 7 and 8discuss the evaluation results and give our observationsand conclusions.2 Description of Web CorpusFirst, we define the following standard terms whenreferring to word frequencies in a corpus.
A token is de-fined as any word occurrence in the collection.
A typerefers to a specific word-string, and is counted only onceno matter how many times the word token of that typeoccurs in the collection.For training our model, we were aware of no signifi-cant collection of Web pages labeled by reading diffi-culty level, so we assembled our own corpus.
There arenumerous commercial reading comprehension testsavailable that have graded passages, but this would havereduced the emphasis we wanted on Web documents.Also, some commercial packages themselves use read-A Language Modeling Approach to Predicting Reading DifficultyKevyn Collins-Thompson       Jamie CallanLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon University4502 Newell Simon HallPittsburgh, PA 15213-8213{kct, callan}@cs.cmu.eduability measures when authoring the graded passages,making the data somewhat artificial and biased towardtraditional semantic variables.We gathered 550 English documents across 12American grade levels, containing a total of 448,715tokens and 17,928 types.
The pages were drawn from awide variety of subject areas: fiction, non-fiction, his-tory, science, etc.
We were interested in the accuracyavailable at individual grade levels, so we selectedpages which had been assigned a specific grade level bythe Web site author.
For example, in some cases theassigned grade level was that of the classroom pagewhere the document was acquired.Before defining a classification model, we examinedthe corpus for trends in word frequency.
One obviouspattern was that more difficult words were introduced atlater grade levels.
Earlier researchers (e.g.
Chall, 1983,p.
63) have also observed that concrete words like ?red?become less likely in higher grades.
Similarly, highergrade levels use more abstract words with increased fre-quency.
We observed both types of behavior in our Webcorpus.
Figure 1 shows four words drawn from our cor-pus.
Data from each of the 12 grades in the corpus areshown, ordered by ascending grade level.
The solid lineis a smoothed version of the word frequency data.
Theword ?red?
does indeed show a steady decline in usagewith grade level, while the probability of the word?determine?
increases.
Other words like ?perimeter?attain maximum probability in a specific grade range,perhaps corresponding to the period in which these con-cepts are emphasized in the curriculum.
The word ?the?is very common and varies less in frequency acrossgrade levels.Our main hypothesis in this work is that there areenough distinctive changes in word usage patterns be-tween grade levels to give accurate predictions withsimple language models, even when the subject domainof the documents is unrestricted.3 Related WorkThere is a significant body of work on readability thatspans the last 70 years.
A comprehensive summary ofearly readability work may be found in Chall (1958) andProbability of the word "perimeter"00.000050.00010.000150.00020.000250.00030.000350.00040.000450.00050 1 2 3 4 5 6 7 8 9 10 11 12Grade ClassP(word|grade)Probability of the word "red"00.00020.00040.00060.00080.0010.00120.00140.00160 1 2 3 4 5 6 7 8 9 10 11 12Grade ClassP(word|grade)Probability of the word "determine"00.00020.00040.00060.00080.0010.00120.00140.00160 1 2 3 4 5 6 7 8 9 10 11 12Grade ClassP(word|grade)Probability of the word "the"00.010.020.030.040.050.060.070.080.090 1 2 3 4 5 6 7 8 9 10 11 12Grade ClassP(word|grade)Figure 1.
Examples of four different word usage trends across grades 1-12, as sampled from our 400K-tokencorpus of Web documents.
Curves showing word frequency data smoothed across grades using kernel regressionfor the words (clockwise from top left): ?red?, ?determine?, ?the?, and ?perimeter?.Klare (1963).
In 1985 a study by Mitchell (1985)reviewed 97 different reading comprehension tests,although few of these have gained wide use.?Traditional?
readability measures are those that relyon two main factors: the familiarity of semantic units(words or phrases) and the complexity of syntax.
Mea-sures that estimate semantic difficulty using a word list(as opposed to, say, number of syllables in a word) aretermed ?vocabulary-based measures?.Most similar to our work are the vocabulary-basedmeasures, such as the Lexile measure (Stenner et al,1988), the Revised Dale-Chall formula (Chall and Dale,1995) and the Fry Short Passage measure (Fry, 1990).All of these use some type of word list to estimatesemantic difficulty: Lexile (version 1.0) uses the Car-roll-Davies-Richman corpus of 86,741 types (Carroll etal., 1971); Dale-Chall uses the Dale 3000 word list; andFry's Short Passage Measure uses Dale & O'Rourke's?The Living Word Vocabulary?
of 43,000 types (Daleand O'Rourke, 1981).
Each of these word lists may bethought of as a simplified language model.
The modelwe present below may be thought of as a generalizationof the vocabulary-based approach, in which we buildmultiple language models - in this study, one for eachgrade - that capture more fine-grained information aboutvocabulary  usage.To our knowledge, the only previous work whichhas considered a language modeling approach to read-ability is a preliminary study by Si and Callan (2001).Their work was limited to a single subject domain - sci-ence - and three broad ranges of difficulty.
In contrast,our model is not specific to any subject and uses 12 indi-vidual grade models trained on a greatly expanded train-ing set.
While our model is also initially based on na?veBayes, we do not treat each class as independent.Instead, we use a mixture of grade models, whichgreatly improves accuracy.
We also do not include sen-tence length as a syntactic component.
Si and Callandid not perform any analysis of feature selection meth-ods so it is unclear whether their classifier was conflat-ing topic prediction with difficulty prediction.
In thispaper we examine feature selection as well as ourmodel's ability to generalize.4 The Smoothed Unigram ModelOur statistical model is based on a variation of the mult-inomial na?ve Bayes classifier, which we call the?Smoothed Unigram?
model.
In text classificationterms, each class is described by a language model cor-responding to a predefined level of difficulty.
ForEnglish Web pages, we trained 12 language models cor-responding to the 12 American grade levels.The language models we use are simple: they arebased on unigrams and assume that the probability of atoken is independent of the surrounding tokens, giventhe grade language model.
A unigram language modelis defined by a list of types (words) and their individualprobabilities.
Although this is a weak model, it can betrained from less data than more complex models, andturns out to give good accuracy for our problem.4.1 Prediction with Multinomial Na?ve BayesWe define a generative model for a text passage T inwhich we assume T was created by a hypotheticalauthor using the following algorithm:1.
Choose a grade language model Gi  from somecomplete set of unigram models G according to a priordistribution P(Gi).
Each Gi has a multinomial distribu-tion over a vocabulary V.2.
Choose a passage length L in tokens according tothe distribution  P(L | Gi).3.
Assuming a ?bag of words?
model for the passage,sample L tokens from Gi ?s multinomial distributionbased on the ?na?ve?
assumption that each token is inde-pendent of all other tokens in the passage, given the lan-guage model Gi.The probability of T given model Gi is therefore:where C(w) is the count of the type w in T.Our goal is to find the most likely grade languagemodel given the text T, or equivalently, the model Gi thatmaximizes .
We derive L(Gi | T)from (1) via Bayes?
Rule, which is:However, we first make two further assumptions:1.
All grades are equally likely a priori, and there-fore  where NG  is the number of grades.2.
The passage length probability P(L|Gi) is indepen-dent of grade level.Substituting (1) into (2), simplifying, and taking log-arithms, we obtain:where log Z represents combined factors involving pas-sage length and the uniform prior P(Gi) which, accord-ing to our assumptions, do not influence the predictionoutcome and may be ignored.
The sum in (3) is easilycomputed: for each token in T, we simply look up its logprobability in the language model of Gi and sum over allP T Gi( ) P L Gi( ) L !P w Gi( )C w( )C w( )!-------------------------------w T??
?=L Gi T( ) P Gi T( )log=P Gi T( )P Gi( )P T Gi( )P T( )----------------------------------=P Gi( ) 1 NG?=L Gi T( ) C w( ) P w Gi( )logw T?Zlog+=(1)(2)(3)tokens to obtain the total likelihood of the passage giventhe grade.
We do this for all language models, andselect the one with maximum likelihood.
An exampleof the set of log-likelihoods calculated across all 12grade models, with a maximum point clearly evident, isshown in Figure 2.5 ImplementationGiven the above theoretical model, we describe two fur-ther aspects of our classification method: smoothing andfeature selection.5.1 SmoothingWe will likely see some types in test documents that aremissing or rarely seen in our training documents.
Thisis a well-known issue in language model applications,and it is standard to compensate for this sparseness bysmoothing the frequencies in the trained models.
To dothis, we adjust our type probability estimates by shiftingpart of the model?s probability mass from observedtypes to unseen and rare types.We first apply smoothing to each grade?s languagemodel individually.
We use a technique called SimpleGood-Turing smoothing, which is a popular method fornatural language applications.
We omit the details here,which are available in Gale and Sampson (1995).Next, we apply smoothing across grade languagemodels.
This is a departure from standard text classifi-cation methods, which treat the classes as independent.For reading difficulty, however, we hypothesize thatnearby grade models are in fact highly related, so thateven if a type is unobserved in one grade?s training data,we can estimate its probability in the model by interpo-lating estimates from nearby grade models.For example, suppose we wish to estimate P(w|G)for a type w in a grade model G.  If the type w occurs inat least one grade language model, we can performregression with a Gaussian kernel (Hastie et al, 2001, p.165) across all grade models to obtain a smoothed valuefor P(w|G).
With training, we found the optimal kernelwidth to be 2.5 grade levels.
If w does not occur in anygrade model (an ?out-of-vocabulary?
type) we can backoff to a traditional semantic variable.
In this study, weused an estimate which is a function of type length:where w is a type, i is a grade index between 1 and 12,|w| is w?s length in characters, and C = -13, D = 10 basedon statistics from the Web corpus.5.2 Feature SelectionFeature selection is an important step in text classifica-tion: it can lessen the computational burden by reducingthe number of features and increase accuracy by remov-ing ?noise?
words having low predictive power.The first feature selection step for many text classifi-ers is to remove the most frequent types (?stopwords?
).This must be considered carefully for our problem: atlower grade levels, stopwords make up the majority oftoken occurrences and removing them may introducebias.
We therefore do not remove stopwords.Another common step is to remove low-frequencytypes ?
typically those that occur less than 2 to 5 timesin a model?s training data.
Because we smooth acrossgrade models, we perform a modified version of thisstep, removing from all models any types occurring lessthan 3 times in the entire corpus.Unlike the usual text classification scenario, we alsowish to avoid some types that are highly grade-specific.For example, a type that is very frequent in the grade 3model but that never occurs in any other model seemsmore likely to be site-specific noise than a genuinevocabulary item.
We therefore remove any types occur-ring in less than 3 grade models, no matter how hightheir frequency.
Further study is needed to explore waysto avoid over-fitting the classifier while reducing theexpense of removing possibly useful features.We investigated scoring each remaining type basedon its estimated ability to predict (positively or nega-tively) a particular grade.
We used a form of Log-OddsRatio, which has been shown to give superior perfor-mance for multinomial na?ve Bayes classifiers (Mlad-enic and Grobelnik, 1998).
Our modified Log-Oddsmeasure computes the largest absolute change in log-likelihood between a given grade and all other grades.P w Gi( )log CwD------+ i w?
( )?
?Figure 2.
The log-likelihood of a typical 100-word Grade5 passage relative to the language models for grades 1 to12.
The maximum log-likelihood in this example isachieved for the Grade 6 language model.
Note the nega-tive scale.L o g-L ike lih o o d  o f S a m p le G rad e 5 P as sa geR elative  to  L an gu a ge  M o d els  fo r Gra d es 1  - 1 2-810-800-790-780-770-760-7501 2 3 4 5 6 7 8 9 10 11 12G ra d e  Le ve lLog-likelihoodWe tried various thresholds for our Log-Odds measureand found that the highest accuracy was achieved byusing all remaining features.5.3 Implementation SpecificsWe found that we could reduce prediction variance withtwo changes to the model.
First, rather than choosingthe single most likely grade language model, we calcu-late the average grade level of the top N results,weighted by the relative differences in likelihood(essentially the expected class).
The tradeoff is a smallbias toward the middle grades.
All results reported hereuse this averaging method, with N=2.Second, to account for vocabulary variation withinlonger documents, we partition the document text intopassages of 100 tokens each.
We then obtain a gradelevel prediction for each passage.
This creates a distri-bution of grade levels across the document.
Previouswork (Stenner, 1996, also citing Squires et al, 1983 andCrawford et al, 1975) suggests that a comprehensionrate of 75% for a text is a desirable target.
We thereforechoose the grade level that lies at the 75th-percentile ofthe distribution, interpolating if necessary, to obtain ourfinal prediction.6 EvaluationState-of-the-art performance for this classification taskis hard to estimate.
The results from the most closelyrelated previous work (Si and Callan, 2001) are notdirectly comparable to ours; among other factors, theirtask used a dataset trained on science curriculumdescriptions, not text written at different levels of diffi-culty.
There also appear to be few reliable studies ofhuman-human interlabeler agreement.
A very limitedstudy by Gartin et al (1994) gave a mean interlabelerstandard deviation of 1.67 grade levels, but this studywas limited to just 3 samples across 10 judges.
Never-theless, we believe that an objective element to readabil-ity assessment exists, and we state our main results interms of correlation with difficulty level, so that at leasta broad comparison with existing measures is possible.Our evaluation looked at four aspects of the model.First, we measured how well the model trained on ourWeb corpus generalized to other, previously unseen, testdata.
Second, we looked at the effect of passage lengthon accuracy.
Third, we estimated the effect of addi-tional training data on the accuracy of the model.Finally, we looked at how well the model could beextended to a language other than English ?
in thisstudy, we give results for French.6.1 Overall Accuracy and Generalization AbilityWe used two methods for assessing how well our classi-fier generalizes beyond the Web training data.
First, weapplied 10-fold cross-validation on the Web corpus(Kohavi 1995).
This chooses ten random partitions foreach grade?s training data such that 90% is used fortraining and 10% held back as a test set.
Second, weused two previously unseen test sets: a set of 228 lev-eled documents from Reading A-Z.com, spanning grade1 through grade 6; and 17 stories from Diagnostic Read-ing Scales (DRS) spanning grades 1.4 through 5.5.
TheReading A-Z files were converted from PDF files usingoptical character recognition; spelling errors were cor-rected but sentence boundary errors were left intact tosimulate the kinds of problems encountered with Webdocuments.
The DRS files were noise-free.Because the Smoothed Unigram classifier only mod-els semantic and not syntactic difficulty, we comparedits accuracy to predictions based on three widely-usedsemantic difficulty variables as shown below.
All pre-diction methods used a 100-token window size.1.
UNK:  The fraction of ?unknown?
tokens in thetext, relative to the Dale 3000 word list.
This is thesemantic variable of the Revised Dale-Chall measure.2.
TYPES:  The number of types (unique words) ina 100-token passage.3.
MLF: The mean log frequency of the passage rel-ative to a large English corpus.
This is approximatelythe semantic variable of the unnormalized Lexile (ver-sion 1.0) score.
Because the Carroll-Davies-Richmancorpus was not available to us, we used the written sub-set of the British National Corpus (Burnard, 1995)which has 921,074 types.
(We converted these to theAmerican equivalents.
)We also included a fourth predictor: the Flesch-Kincaid score (Kincaid et al 1975), which is a linearcombination of the text?s average sentence length (intokens), and the average number of syllables per token.This was included for illustration purposes only, to ver-ify the effect of syntactic noise.
The results of the evalu-ation are summarized in Table 1.On the DRS test collection, the TYPES and Flesch-Kincaid predictors had the best correlation with labeledgrade level (0.93).
TYPES also obtained the best corre-lation (0.86) for the Reading A-Z documents.
However,Reading A-Z documents were written to pre-establishedcriteria which includes objective factors such as type/token ratio (Reading A-Z.com, 2003), so it is not sur-prising that the correlation is high.
The Smoothed Uni-gram measure achieved consistently good correlation(0.63 ?
0.67) on both DRS and Reading A-Z test sets.Flesch-Kincaid performs much more poorly for theReading A-Z data, probably because of the noisy sen-tence structure.
In general, mean log frequency (MLF)performed worse than expected ?
the reasons for thisrequire further study but may be due to the fact the BNCcorpus may not be representative enough of vocabularyfound at earlier grades.For Web data, we examined two subsets of the cor-pus: grades 1?
6 and grades 1?
12.
The correlation of allvariables with difficulty dropped substantially for Webgrades 1?6, except for Smoothed Unigram, whichstayed at roughly the same level (0.64) and was the bestperformer.
The next best variable was UNK (0.38).
Forthe entire Web grades 1?
12 data set, the Smoothed Uni-gram measure again achieved the best correlation(0.79).
The next best predictor was again UNK (0.63).On the Web corpus, the largest portions of SmoothedUnigram?s accuracy gains were achieved in grades 4?
8.Without cross-grade smoothing, correlation for Webdocument predictions fell significantly, to 0.46 and 0.68for the grade 1-6 and 1-12 subsets respectively.We measured the type coverage of the languagemodels created from our Web training corpus, using theWeb (via cross-validation) and Reading A-Z test sets.Type coverage tells us how often on average a type froma test passage is found in our statistical model.
On theReading A-Z test set (Grades 1 ?
6), we observed amean type coverage of 89.1%, with a standard deviationof 6.65%.
The mean type coverage for the Web corpuswas 91.69%, with a standard deviation of 5.86%.
Thesefigures suggest that the 17,928 types in the training setare sufficient to give enough coverage of the test datathat we only need to back off outside the languagemodel-based estimates for an average of 8-10 tokens inany 100-token passage.6.2 Effect of Passage Length on AccuracyMost readability formulas become unreliable for pas-sages of less than 100 tokens (Fry 1990).
With Webapplications, it is not uncommon for samples to containas few as 10 tokens or less.
For example, educationalWeb sites often segment a story or lesson into a series ofimage pages, with the only relevant page content being acaption.
Short passages also arise for tasks such as esti-mating the reading difficulty of page titles, user queries,or questionnaire items.
Our hypothesis was that theSmoothed Unigram model, having more fine-grainedmodels of word usage, would be less sensitive to pas-sage length and give superior accuracy for very shortpassages, compared to traditional semantic statistics.In the extreme case, consider two single-word ?pas-sages?
: ?bunny?
and ?bulkheads?.
Both words have twosyllables and both occur 5 times in the Carroll-Davies-Richman corpus.
A variable such as mean log fre-quency would assign identical difficulty to both of thesepassages, while our model would clearly distinguishthem according to each word?s grade usage.To test this hypothesis, we formed passages oflength L by sampling L consecutive tokens from nearthe center of each Reading A-Z test document.
Wecompared the RMS error of the Smoothed Unigram pre-diction on these passages to that obtained from the UNKsemantic variable.
We computed different predictionsfor both methods by varying the passage length L from 3tokens to 100 tokens.The results are shown in Figure 3.
Accuracy for thetwo methods was comparable for passages longer thanabout 50 tokens, but Smoothed Unigram obtained statis-tically significant improvements at the 0.05 level for 4,5, 6, 7, and 8-word passages.
In those cases, the predic-tion is accurate enough that very short passages may bereliably classified into low, medium, and high levels ofdifficulty.6.3 Effect of Training Set Size on AccuracyWe derived the learning curve of our classifier as a func-tion of the mean model training set size in tokens.
Thelowest mean RMS error of 1.92 was achieved at themaximum training set size threshold of 32,000 tokensper grade model.
We fit a monotonically decreasingpower-law function to the data points (Duda et al 2001,p.
492).
This gave extrapolated estimates for meanRMS error of about 1.79 at 64,000 tokens per model,1.71 at 128,000 tokens per model, and 1.50 at 1,000,000tokens per model.While doubling the current mean training set size to64,000 tokens per model would give a useful reductionin RMS error (about 6.7%), each further reduction ofFiles Grade RangeSmoothedUnigram UNK TYPES MLF FKDRS 17 1.4 - 5.5 0.67 0.72 0.93 0.50 0.93Reading A-Z 228 1.0 - 6.0 0.63 0.78 0.86 0.49 0.30Web (Gr.
1-6) 250 1.0 - 6.0 0.64 0.38 0.26 0.36 0.25Web (Gr.
1-12) 550 1.0 - 12 0.79 0.63 0.38 0.47 0.47Table 1.
Correlations between predictors and grade level, for the English collections used in our study.All predictors were trained on the Web corpus, with the Web tests using 10-fold cross-validation.that magnitude would require a corresponding doublingof the training set size.
This is the trade-off that must beconsidered between overall RMS accuracy and the costof gathering labeled data.6.4 Application to French Web PagesTo test the flexibility of our language model approach,we did a preliminary study for French reading difficultyprediction.
We created a corpus of 189 French Webpages labeled at 5 levels of difficulty, containing a totalof  394,410 tokens and 27,632 types (unstemmed).The classification algorithm was identical to thatused for English except for a minor change in the fea-ture selection step.
We found that, because of theinflected nature of French and the relatively small train-ing set, we obtained better accuracy by normalizingtypes into ?type families?
by using a simplified stem-ming algorithm that removed plurals, masculine/femi-nine endings, and basic verb endings.A chart of the actual versus predicted difficulty labelis shown in Figure 4.
The classifier consistently under-predicts difficulty for the highest level, while somewhatover-predicting for the lowest level.
This may be partlydue to the bias toward central grades caused by averag-ing the top 2 predictions.
More work on language-spe-cific smoothing may also be needed.
With 10-foldcross-validation, the French model obtained a mean cor-relation of 0.64 with labeled difficulty.
For comparison,using the type/token ratio gave a mean correlation of0.48.
While further work and better training data areneeded, the results seem promising given that only a fewhours of effort were required to gather the French dataand adjust the classifier?s feature selection.7 DiscussionWhile word difficulty is well-known to be an excellentpredictor of reading difficulty (Chall & Edgar, 1995), itwas not at all clear how effective our language modelapproach would be for predicting Web page reading dif-ficulty.
It was also unknown how much training datawould be required to get good vocabulary coverage onWeb data.
Although retraining for other applications ordomains may be desirable, two factors appear responsi-ble for the fact that our classifier, trained on Web data,generalizes reasonably well to unseen test data fromother sources.First, smoothing across classes greatly reduces thetraining data required for individual grade models.
By?borrowing?
word frequency data from nearby grades,the effective number of types for each grade model ismultiplied by a factor of five or more.
This helpsexplain the type coverage of about 90% on our test data.Second, because we are interested in the relativelikelihoods of grade levels, accurate relative type proba-bilities are more important than absolute probabilities.Indeed, trying to learn absolute type probabilities wouldbe undesirable since it would fit the model too closely towhatever specific topics were in the training set.
Theimportant functions of relative likelihood appear to begeneral indicators such as the grade when a word is firstintroduced into usage, whether it generally increases ordecreases with grade level, and whether it is most fre-quent in a particular grade range.Further study is required to explore just how muchthis model of vocabulary usage can be generalized toother languages.
Our results with French suggest thatonce we have normalized incoming types to accommo-Passage Length vs.
Prediction Accuracy(Grade 4: ReadingA-Z)012345671 10 100Passage Length (Words)MeanRMSError%-UNKSmoothedUnigramFigure 3.
The effect of passage size on RMS predic-tion error for Grade 4 documents, comparingSmoothed Unigram to the UNK semantic variable.Error bars show 95% confidence interval.
The greyvertical lines mark logarithmic length.Actual v s. P re dicte d D ifficulty Le v e lFre nch Le v e ls 1 - 500 .511 .522 .533 .544 .550 1 2 3 4 5L ab e le d  L e ve lPredictedLevelFigure 4.
Actual vs. predicted difficulty label for docu-ments from the French Web corpus.
The data havebeen ?jittered?
to show clusters more clearly.
The diag-onal line represents perfect prediction.date the morphology of a language, the same core classi-fier approach may still be applicable, at least for somefamily of languages.8 ConclusionsWe have shown that reading difficulty can be estimatedwith a simple language modeling approach using a mod-ified na?ve Bayes classifier.
The classifier's effective-ness is improved by explicitly modeling classrelationships and smoothing frequency data acrossclasses as well as within each class.Our evaluation suggests that reasonably effectivemodels can be trained with small amounts of easily-acquired data.
While this data is less-rigorously graded,such material also greatly reduces the cost of creating areadability measure, making it easy to modify for spe-cific tasks or populations.As an example of retraining, we showed that theclassifier obtained good correlation with difficulty for atleast two languages, English and French, with the onlyalgorithm difference being a change in the morphologyhandling during feature processing.We also showed that the Smoothed Unigram methodis robust for short passages and Web documents.
Sometraditional variables like type/token ratio gave excellentcorrelation with difficulty on commercial leveled pas-sages, but the same statistics performed inconsistentlyon Web-based test sets.
In contrast, the Smoothed Uni-gram method had good accuracy across all test sets.The problem of reading difficulty prediction lies inan interesting region between classification and regres-sion, with close connections to ordinal regression (Mac-Cullagh, 1980) and discriminative ranking models(Crammer and Singer, 2001).
While simple methodslike modified na?ve Bayes give reasonably good results,more sophisticated techniques may give more accuratepredictions, especially at lower grades, where vocabu-lary progress is measured in months, not years.AcknowledgementsThis work was supported by NSF grant IIS-0096139and Dept.
of Education grant R305G03123.
Any opin-ions, findings, conclusions, or recommendationsexpressed in this material are the authors?, and do notnecessarily reflect those of the sponsors.
We thank theanonymous reviewers for their comments and Luo Si forhelpful discussions.ReferencesBurnard, L.
(ed.)
1995.
The Users Reference Guide for theBritish National Corpus.
Oxford: Oxford UniversityComputing Services.Carroll, J.
B., Davies, P., Richman, B.
1971.
Word FrequencyBook.
Boston: Houghton Mifflin.Chall, J.S.
1958.
Readability: An appraisal of research andapplication.
Bureau of Educational Research Mono-graphs, No.
34.
Columbus, OH: Ohio State Univ.
Press.Chall, J.S.
1983.
Stages of Reading Development.
McGraw-Hill.Chall, J.S.
and Dale, E.  1995.
Readability Revisited: The NewDale-Chall Readability Formula.
Cambridge, MA:Brookline Books.Crammer, K. and Singer, Y.
2001.
Pranking with ranking.Proceedings of NIPS 2001.
641-647.Dale, E. and O'Rourke, J.
1981.
The Living Word Vocabulary.Chicago, IL: World Book/Childcraft International.Duda, R. O., Hart, P. E., and Stork, D. G.  2001.
Pattern Clas-sification (Second Edition), Wiley, New York.Fry, E.  1990.
A readability formula for short passages.
J. ofReading, May 1990, 594-597.Gale, W., Sampson, G.  1995.
Good-Turing frequency estima-tion without tears, J. of Quant.
Linguistics, v. 2, 217-237.Gartin, S., et al 1994.
W. Virginia Agriculture Teachers?
Esti-mates of Magazine Article Readability.
J. Agr.
Ed.
35(1).Hastie, T., Tibshirani, R., Friedman, J.
2001.
The Elements ofStatistical Learning.
Springer-Verlag, New York.Kincaid, J., Fishburne, R., Rodgers, R. and Chissom, B.
1975.Derivation of new readability formulas for navy enlistedpersonnel.
Branch Report 8-75.
Millington, TN: Chiefof Naval Training.Klare, G. R.  1963.
The Measurement of Readability.
Ames,IA.
Iowa State University Press.Kohavi, R.  1995.
A study of cross-validation and bootstrapfor accuracy estimation and model selection.
Proc.
of the14th Int.
Joint Conf.
on Artificial Intelligence (IJCAI1995).
Montreal, Canada.
1137 - 1145.MacCullagh, P.  1980.
Regression models for ordinal data.
J.of the Royal Statistical Society B, vol.42, 109-142.Mitchell, J.V.
1985.
The Ninth Mental Measurements Year-book.
Lincoln, Nebraska: Univ.
of Nebraska Press.Mladenic D., and Grobelnik, M.  1998.
Feature selection forclassification based on text hierarchy.
Working Notes ofLearning from Text and the Web, CONALD-98.
CarnegieMellon Univ., Pittsburgh, PA.Reading A-Z.com  2003.
Reading A-Z Leveling and Correla-tion Chart (HTML page).
http://www.readinga-z.com/newfiles/correlate.htmlSi, L. and Callan, J.
2001.
A statistical model for scientificreadability.
Proc.
of CIKM 2001.
Atlanta, GA, 574-576.Stenner, A. J., Horabin, I., Smith, D.R., and Smith, M. 1988.The Lexile Framework.
Durham, NC: Metametrics.
