Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 659?666,Sydney, July 2006. c?2006 Association for Computational LinguisticsUsing Machine Learning to Explore Human Multimodal ClarificationStrategiesVerena RieserDepartment of Computational LinguisticsSaarland UniversitySaarbru?cken, D-66041vrieser@coli.uni-sb.deOliver LemonSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, GBolemon@inf.ed.ac.ukAbstractWe investigate the use of machine learn-ing in combination with feature engineer-ing techniques to explore human multi-modal clarification strategies and the useof those strategies for dialogue systems.We learn from data collected in a Wizard-of-Oz study where different wizards coulddecide whether to ask a clarification re-quest in a multimodal manner or else usespeech alone.
We show that there is auniform strategy across wizards which isbased on multiple features in the context.These are generic runtime features whichcan be implemented in dialogue systems.Our prediction models achieve a weightedf-score of 85.3% (which is a 25.5% im-provement over a one-rule baseline).
Toassess the effects of models, feature dis-cretisation, and selection, we also conducta regression analysis.
We then interpretand discuss the use of the learnt strategyfor dialogue systems.
Throughout the in-vestigation we discuss the issues arisingfrom using small initial Wizard-of-Oz datasets, and we show that feature engineer-ing is an essential step when learning fromsuch limited data.1 IntroductionGood clarification strategies in dialogue systemshelp to ensure and maintain mutual understand-ing and thus play a crucial role in robust conversa-tional interaction.
In dialogue application domainswith high interpretation uncertainty, for examplecaused by acoustic uncertainties from a speechrecogniser, multimodal generation and input leadsto more robust interaction (Oviatt, 2002) and re-duced cognitive load (Oviatt et al, 2004).
In thispaper we investigate the use of machine learning(ML) to explore human multimodal clarificationstrategies and the use of those strategies to decide,based on the current dialogue context, when a di-alogue system?s clarification request (CR) shouldbe generated in a multimodal manner.In previous work (Rieser and Moore, 2005)we showed that for spoken CRs in human-human communication people follow a context-dependent clarification strategy which systemati-cally varies across domains (and even across Ger-manic languages).
In this paper we investigatewhether there exists a context-dependent ?intu-itive?
human strategy for multimodal CRs as well.To test this hypothesis we gathered data in aWizard-of-Oz (WOZ) study, where different wiz-ards could decide when to show a screen output.From this data we build prediction models, usingsupervised learning techniques together with fea-ture engineering methods, that may explain the un-derlying process which generated the data.
If wecan build a model which predicts the data quite re-liably, we can show that there is a uniform strategythat the majority of our wizards followed in certaincontexts.Figure 1: Methodology and structureThe overall method and corresponding structureof the paper is as shown in figure 1.
We proceed659as follows.
In section 2 we present the WOZ cor-pus from which we extract a potential context us-ing ?Information State Update?
(ISU)-based fea-tures (Lemon et al, 2005), listed in section 3.
Wealso address the question how to define a suit-able ?local?
context definition for the wizard ac-tions.
We apply the feature engineering methodsdescribed in section 4 to address the questions ofunique thresholds and feature subsets across wiz-ards.
These techniques also help to reduce thecontext representation and thus the feature spaceused for learning.
In section 5 we test differentclassifiers upon this reduced context and separateout the independent contribution of learning al-gorithms and feature engineering techniques.
Insection 6 we discuss and interpret the learnt strat-egy.
Finally we argue for the use of reinforcementlearning to optimise the multimodal clarificationstrategy.2 The WOZ CorpusThe corpus we are using for learning was col-lected in a multimodal WOZ study of Germantask-oriented dialogues for an in-car music playerapplication, (Kruijff-Korbayova?
et al, 2005) .
Us-ing data from a WOZ study, rather than from realsystem interactions, allows us to investigate howhumans clarify.
In this study six people played therole of an intelligent interface to an MP3 playerand were given access to a database of informa-tion.
24 subjects were given a set of predefinedtasks to perform using an MP3 player with a mul-timodal interface.
In one part of the session theusers also performed a primary driving task, us-ing a driving simulator.
The wizards were ableto speak freely and display the search results orthe playlist on the screen by clicking on vari-ous pre-computed templates.
The users were alsoable to speak, as well as make selections on thescreen.
The user?s utterances were immediatelytranscribed by a typist.
The transcribed user?sspeech was then corrupted by deleting a varyingnumber of words, simulating understanding prob-lems at the acoustic level.
This (sometimes) cor-rupted transcription was then presented to the hu-man wizard.
Note that this environment introducesuncertainty on several levels, for example multiplematches in the database, lexical ambiguities, anderrors on the acoustic level, as described in (Rieseret al, 2005).
Whenever the wizard produced aCR, the experiment leader invoked a questionnairewindow on a GUI, where the wizard classifiedtheir CR according to the primary source of theunderstanding problem, mapping to the categoriesdefined by (Traum and Dillenbourg, 1996).2.1 The DataThe corpus gathered with this setup comprises70 dialogues, 1772 turns and 17076 words.
Ex-ample 1 shows a typical multimodal clarificationsub-dialogue, 1 concerning an uncertain reference(note that ?Venus?
is an album name, song title,and an artist name), where the wizard selects ascreen output while asking a CR.
(1) User: Please play ?Venus?.Wizard: Does this list contain the song?
[shows list with 20 DB matches]User: Yes.
It?s number 4.
[clicks on item 4]For each session we gathered logging informa-tion which consists of e.g., the transcriptions ofthe spoken utterances, the wizard?s database queryand the number of results, the screen option cho-sen by the wizard, classification of CRs, etc.
Wetransformed the log-files into an XML structure,consisting of sessions per user, dialogues per task,and turns.22.2 Data analysis:Of the 774 wizard turns 19.6% were annotatedas CRs, resulting in 152 instances for learning,where our six wizards contributed about equalproportions.
A ?2 test on multimodal strategy(i.e.
showing a screen output or not with a CR)showed significant differences between wizards(?2(1) = 34.21, p < .000).
On the other hand, aKruskal-Wallis test comparing user preference forthe multimodal output showed no significant dif-ference across wizards (H(5)=10.94, p > .05).
3Mean performance ratings for the wizards?
multi-modal behaviour ranged from 1.67 to 3.5 on a five-point Likert scale.
Observing significantly differ-ent strategies which are not significantly differentin terms of user satisfaction, we conjecture that thewizards converged on strategies which were ap-propriate in certain contexts.
To strengthen this1Translated from German.2Where a new ?turn?
begins at the start of each new userutterance after a wizard utterance, taking the user utterance asa most basic unit of dialogue progression as defined in (Paekand Chickering, 2005).3The Kruskal-Wallis test is the non-parametric equivalentto a one-way ANOVA.
Since the users indicated their satis-faction on a 5-point likert scale, an ANOVA which assumesnormality would be invalid.660hypothesis we split the data by wizard and and per-formed a Kruskal-Wallis test on multimodal be-haviour per session.
Only the two wizards with thelowest performance score showed no significantvariation across session, whereas the wizards withthe highest scores showed the most varying be-haviour.
These results again indicate a context de-pendent strategy.
In the following we test this hy-pothesis (that good multimodal clarification strate-gies are context-dependent) by building a predic-tion model of the strategy an average wizard tookdependent on certain context features.3 Context/Information-State FeaturesA state or context in our system is a dialogue in-formation state as defined in (Lemon et al, 2005).We divide the types of information representedin the dialogue information state into local fea-tures (comprising low level and dialogue features),dialogue history features, and user model fea-tures.
We also defined features reflecting the ap-plication environment (e.g.
driving).
All fea-tures are automatically extracted from the XMLlog-files (and are available at runtime in ISU-based dialogue systems).
From these features wewant to learn whether to generate a screen out-put (graphic-yes), or whether to clarify usingspeech only (graphic-no).
The case that thewizard only used screen output for clarification didnot occur.3.1 Local FeaturesFirst, we extracted features present in the ?lo-cal?
context of a CR, such as the numberof matches returned from the data base query(DBmatches), how many words were deletedby the corruption algorithm4 (deletion), whatproblem source the wizard indicated in the pop-up questionnaire (source), the previous userspeech act (userSpeechAct), and the delay be-tween the last wizard utterance and the user?s reply(delay).
5One decision to take for extracting these localfeatures was how to define the ?local?
context ofa CR.
As shown in table 1, we experimented witha number of different context definitions.
Context1 defined the local context to be the current turnonly, i.e.
the turn containing the CR.
Context 24Note that this feature is only an approximation of theASR confidence score that we would expect in an automateddialogue system.
See (Rieser et al, 2005) for full details.5We introduced the delay feature to handle clarificationsconcerning contact.id Context (turns) acc/ wf-score ma-jority(%)acc/ wf-scoreNa?
?ve Bayes(%)1 only current turn 83.0/54.9 81.0/68.32 current and next 71.3/50.4 72.01/68.23 current and previous 60.50/59.8 76.0*/75.34 previous, current, next 67.8/48.9 76.9*/ 74.8Table 1: Comparison of context definitions for lo-cal features (* denotes p < .05)also considered the current turn and the turn fol-lowing (and is thus not a ?runtime?
context).
Con-text 3 considered the current turn and the previousturn.
Context 4 is the maximal definition of a lo-cal context, namely the previous, current, and nextturn (also not available at runtime).
6To find the context type which provides the rich-est information to a classifier, we compared the ac-curacy achieved in a 10-fold cross validation bya Na?
?ve Bayes classifier (as a standard) on thesedata sets against the majority class baseline, us-ing a paired t-test, we found that that for context3 and context 4, Na?
?ve Bayes shows a significantimprovement (with p < .05 using Bonferroni cor-rection).
In table 1 we also show the weightedf-scores since they show that the high accuracyachieved using the first two contexts is due to over-prediction.
We chose to use context 3, since thesefeatures will be available during system runtimeand the learnt strategy could be implemented in anactual system.3.2 Dialogue History FeaturesThe history features account for events in thewhole dialogue so far, i.e.
all information gath-ered before asking the CR, such as the number ofCRs asked (CRhist), how often the screen outputwas already used (screenHist), the corruptionrate so far (delHist), the dialogue duration sofar (duration), and whether the user reacted tothe screen output, either by verbally referencing(refHist) , e.g.
using expressions such as ?It?sitem number 4?, or by clicking (clickHist) asin example 1.3.3 User Model FeaturesUnder ?user model features?
we consider featuresreflecting the wizards?
responsiveness to the be-6Note that dependent on the context definition a CRmight get annotated differently, since placing the questionand showing the graphic might be asynchronous events.661haviour and situation of the user.
Each sessioncomprised four dialogues with one wizard.
Theuser model features average the user?s behaviourin these dialogues so far, such as how responsivethe user is towards the screen output, i.e.
how of-ten this user clicks (clickUser) and how fre-quently s/he uses verbal references (refUser);how often the wizard had already shown a screenoutput (screenUser) and how many CRs werealready asked (CRuser); how much the user?sspeech was corrupted on average (delUser), i.e.an approximation of how well this user is recog-nised; and whether this user is currently driving ornot (driving).
This information was availableto the wizard.LOCAL FEATURESDBmatches: 20deletion: 0source: reference resolutionuserSpeechAct: commanddelay: 0HISTORY FEATURES[CRhist, screenHist, delHist,refHist, clickHist]=0duration= 10sUSER MODEL FEATURES[clickUser,refUser,screenUser,CRuser]=0driving= trueFigure 2: Features in the context after the first turnin example 1.3.4 DiscussionNote that all these features are generic overinformation-seeking dialogues where database re-sults can be displayed on a screen; except fordriving which only applies to hands-and-eyes-busy situations.
Figure 2 shows a context for ex-ample 1, assuming that it was the first utterance bythis user.This potential feature space comprises 18 fea-tures, many of them taking numeric attributes asvalues.
Considering our limited data set of 152training instances we run the risk of severe datasparsity.
Furthermore we want to explore whichfeatures of this potential feature space influencedthe wizards?
multimodal strategy.
In the nexttwo sections we describe feature engineering tech-niques, namely discretising methods for dimen-sionality reduction and feature selection methods,which help to reduce the feature space to a sub-set which is most predictive of multimodal clarifi-cation.
For our experiments we use implementa-tions of discretisation and feature selection meth-ods provided by the WEKA toolkit (Witten andFrank, 2005).4 Feature Engineering4.1 Discretising Numeric FeaturesGlobal discretisation methods divide all contin-uous features into a smaller number of distinctranges before learning starts.
This has two advan-tages concerning the quality of our data for ML.First, discretisation methods take feature distribu-tions into account and help to avoid sparse data.Second, most of our features are highly positivelyskewed.
Some ML methods (such as the standardextension of the Na?
?ve Bayes classifier to handlenumeric features) assume that numeric attributeshave a normal distribution.
We use Proportionalk-Interval (PKI) discretisation as a unsupervisedmethod, and an entropy-based algorithm (Fayyadand Irani, 1993) based on the Minimal DescriptionLength (MDL) principle as a supervised discreti-sation method.4.2 Feature SelectionFeature selection refers to the problem of select-ing an optimum subset of features that are mostpredictive of a given outcome.
The objective of se-lection is two-fold: improving the prediction per-formance of ML models and providing a better un-derstanding of the underlying concepts that gener-ated the data.
We chose to apply forward selec-tion for all our experiments given our large fea-ture set, which might include redundant features.We use the following feature filtering methods:correlation-based subset evaluation (CFS) (Hall,2000) and a decision tree algorithm (rule-basedML) for selecting features before doing the actuallearning.
We also used a wrapper method calledSelective Na?
?ve Bayes, which has been shown toperform reliably well in practice (Langley andSage, 1994).
We also apply a correlation-basedranking technique since subset selection modelsinner-feature relations at the expense of sayingless about individual feature performance itself.4.3 Results for PKI and MDL DiscretisationFeature selection and discretisation influence one-another, i.e.
feature selection performs differentlyon PKI or MDL discretised data.
MDL discreti-sation reduces our range of feature values dra-matically.
It fails to discretise 10 of 14 nu-meric features and bars those features from play-ing a role in the final decision structure becausethe same discretised value will be given to allinstances.
However, MDL discretisation cannotreplace proper feature selection methods since662Table 2: Feature selection on PKI-discretised data (left) and on MDL-discretised data (right)it doesn?t explicitly account for redundancy be-tween features, nor for non-numerical features.For the other 4 features which were discretisedthere is a binary split around one (fairly low)threshold: screenHist (.5), refUser (.375),screenUser (1.0), CRUser (1.25).Table 2 shows two figures illustrating the dif-ferent subsets of features chosen by the featureselection algorithms on discretised data.
Fromthese four subsets we extracted a fifth, using allthe features which were chosen by at least twoof the feature selection methods, i.e.
the featuresin the overlapping circle regions shown in figure2.
For both data sets the highest ranking fea-tures are also the ones contained in the overlappingregions, which are screenUser, refUserand screenHist.
For implementation dialoguemanagement needs to keep track of whether theuser already saw a screen output in a previous in-teraction (screenUser), or in the same dialogue(screenHist), and whether this user (verbally)reacted to the screen output (refUser).5 Performance of Different Learners andFeature EngineeringIn this section we evaluate the performance of fea-ture engineering methods in combination with dif-ferent ML algorithms (where we treat feature op-timisation as an integral part of the training pro-cess).
All experiments are carried out using 10-fold cross-validation.
We take an approach similarto (Daelemans et al, 2003) where parameters ofthe classifier are optimised with respect to featureselection.
We use a wide range of different multi-variate classifiers which reflect our hypothesis thata decision is based on various features in the con-text, and compare them against two simple base-line strategies, reflecting deterministic contextualbehaviour.5.1 BaselinesThe simplest baseline we can consider is to alwayspredict the majority class in the data, in our casegraphic-no.
This yields a 45.6% wf-score.This baseline reflects a deterministic wizard strat-egy never showing a screen output.A more interesting baseline is obtained by us-ing a 1-rule classifier.
It chooses the featurewhich produces the minimum error (which isrefUser for the PKI discretised data set, andscreenHist for the MDL set).
We use the im-plementation of a one-rule classifier provided inthe WEKA toolkit.
This yields a 59.8% wf-score.This baseline reflects a deterministic wizard strat-egy which is based on a single feature only.5.2 Machine LearnersFor learning we experiment with five differenttypes of supervised classifiers.We chose Na?
?veBayes as a joint (generative) probabilistic model,using the WEKA implementation of (John and Lan-gley, 1995)?s classifier; Bayesian Networks as agraphical generative model, again using the WEKAimplementation; and we chose maxEnt as a dis-criminative (conditional) model, using the Max-imum Entropy toolkit (Le, 2003).
As a rule in-duction algorithm we used JRIP, the WEKA imple-mentation of (Cohen, 1995)?s Repeated Incremen-tal Pruning to Produce Error Reduction (RIPPER).And for decision trees we used the J4.8 classi-fier (WEKA?s implementation of the C4.5 system(Quinlan, 1993)).5.3 Comparison of ResultsWe experimented using these different classifierson raw data, on MDL and PKI discretised data,and on discretised data using the different fea-ture selection algorithms.
To compare the clas-sification outcomes we report on two measures:accuracy and wf-score, which is the weighted663Feature transformation/(acc./ wf-score (%))1-rulebaselineRuleInductionDecisionTreemaxEnt Na?
?veBayesBayesianNetworkAverageraw data 60.5/59.8 76.3/78.3 79.4/78.6 70.0/75.3 76.0/75.3 79.5/72.0 73.62/73.21PKI + all features 60.5/ 64.6 67.1/66.4 77.4/76.3 70.7/76.7 77.5/81.6 77.3/82.3 71.75/74.65PKI+ CFS subset 60.5/64.4 68.7/70.7 79.2/76.9 76.7/79.4 78.2/80.6 77.4/80.7 73.45/75.45PKI+ rule-based ML 60.5/66.5 72.8/76.1 76.0/73.9 75.3/80.2 80.1/78.3 80.8/79.8 74.25/75.80PKI+ selective Bayes 60.5/64.4 68.2/65.2 78.4/77.9 79.3/78.1 84.6/85.3 84.5/84.6 75.92/75.92PKI+ subset overlap 60.5/64.4 70.9/70.7 75.9/76.9 76.7/78.2 84.0/80.6 83.7/80.7 75.28/75.25MDL + all features 60.5/69.9 79.0/78.8 78.0/78.1 71.3/76.8 74.9/73.3 74.7/73.9 73.07/75.13MDL + CFS subset 60.5/69.9 80.1/78.2 80.6/78.2 76.0/80.2 75.7/75.8 75.7/75.8 74.77/76.35MDL + rule-based ML 60.5/75.5 80.4/81.6 78.7/80.2 79.3/78.8 82.7/82.9 82.7/82.9 77.38/80.32MDL + select.
Bayes 60.5/75.5 80.4/81.6 78.7/80.8 79.3/80.1 82.7/82.9 82.7/82.9 77.38/80.63MDL + overlap 60.5/75.5 80.4/81.6 78.7/80.8 79.3/80.1 82.7/82.9 82.7/82.9 77.38/80.63average 60.5/68.24 74.9/75.38 78.26/78.06 75.27/78.54 79.91/79.96 80.16/79.86Table 3: Average accuracy and wf-scores for models in feature engineering experiments .sum (by class frequency in the data; 39.5%graphic-yes, 60.5% graphic-no) of the f-scores of the individual classes.
In table 3 wesee fairly stable high performance for Bayesianmodels with MDL feature selection.
However, thebest performing model is Na?
?ve Bayes using wrap-per methods (selective Bayes) for feature selectionand PKI discretisation.
This model achieves a wf-score of 85.3%, which is a 25.5% improvementover the 1-rule baseline.We separately explore the models and featureengineering techniques and their impact on theprediction accuracy for each trial/cross-validation.In the following we separate out the independentcontribution of models and features.
To assessthe effects of models, feature discretisation andselection on performance accuracy, we conducta hierarchical regression analysis.
The modelsalone explain 18.1% of the variation in accuracy(R2 = .181) whereas discretisation methods onlycontribute 0.4% and feature selection 1% (R2 =.195).
All parameters, except for discretisationmethods have a significant impact on modellingaccuracy (P < .001), indicating that feature selec-tion is an essential step for predicting wizard be-haviour.
The coefficients of the regression modellead us to the following hypotheses which we ex-plore by comparing the group means for models,discretisation, and features selection methods.
Ap-plying a Kruskal-Wallis test with Mann-Whitneytests as a post-hoc procedure (using Bonferronicorrection for multiple comparisons), we obtainedthe following results: 7?
All ML algorithms are significantly betterthan the majority and one-rule baselines.
All7We cannot report full details here.
Supplementarymaterial is available at www.coli.uni-saarland.de/?vrieser/acl06-supplementary.htmlexcept maxEnt are significantly better thanthe Rule Induction algorithm.
There is nosignificant difference in the performance ofDecision Tree, maxEnt, Na?
?ve Bayes, andBayesian Network classifiers.
Multivariatemodels being significantly better than thetwo baseline models indicates that we havea strategy that is based on context features.?
For discretisation methods we found that theclassifiers were performing significantly bet-ter on MDL discretised data than on PKI orcontinuous data.
MDL being significantlybetter than continuous data indicates that allwizards behaved as though using thresholdsto make their decisions, and MDL being bet-ter than PKI supports the hypothesis that de-cisions were context dependent.?
All feature selection methods (except forCFS) lead to better performance than usingall of the features.
Selective Bayes and rule-based ML selection performed significantlybetter than CFS.
Selective Bayes, rule-basedML, and subset-overlap showed no signifi-cant differences.
These results show that wiz-ards behaved as though specific features wereimportant (but they suggest that inner-featurerelations used by CFS are less important).Discussion of results: These experimental re-sults show two things.
First, the results indi-cate that we can learn a good prediction modelfrom our data.
We conclude that our six wiz-ards did not behave arbitrarily, but selected theirstrategy according to certain contextual features.By separating out the individual contributions ofmodels and feature engineering techniques, wehave shown that wizard behaviour is based onmultiple features.
In sum, Decision Tree, max-664Ent, Na?
?ve Bayes, and Bayesian Network clas-sifiers on MDL discretised data using SelectiveBayes and Rule-based ML selection achievedthe best results.
The best performing featuresubset was screenUser,screenHist, anduserSpeechAct.
The best performing modeluses the richest feature space including the featuredriving.Second, the regression analysis shows that us-ing these feature engineering techniques in combi-nation with improved ML algorithms is an essen-tial step for learning good prediction models fromthe small data sets which are typically availablefrom multimodal WOZ studies.6 Interpretation of the learnt StrategyFor interpreting the learnt strategies we discussRule Induction and Decision Trees since they arethe easiest to interpret (and to implement in stan-dard rule-based dialogue systems).
For both weexplain the results obtained by MDL and selectiveBayes, since this combination leads to the best per-formance.Rule induction: Figure 3 shows a reformula-tion of the rules from which the learned classifieris constructed.
The feature screenUser playsa central role.
These rules (in combination withthe low thresholds) say that if you have alreadyshown a screen output to this particular user inany previous turn (i.e.
screenUser > 1), thendo so again if the previous user speech act wasa command (i.e.
userSpeechAct=command)or if you have already shown a screen out-put in a previous turn in this dialogue (i.e.screenHist>0.5).
Otherwise don?t showscreen output when asking a clarification.Decision tree: Figure 4 shows the decision treelearnt by the classifier J4.8.
The five rulescontained in this tree also heavily rely on theuser model as well as the previous screen his-tory.
The rules constructed by the first two nodes(screenUser, screenHist) may lead to arepetitive strategy since the right branch will resultin the same action (graphic-yes) in all futureactions.
The only variation is introduced by thespeech act, collapsing the tree to the same rule setas in figure 3.
Note that this rule-set is based ondomain independent features.Discussion: Examining the classifications madeby our best performing Bayesian models we foundthat the learnt conditional probability distribu-tions produce similar feature-value mappings tothe rules described above.
The strategy learntby the classifiers heavily depends on features ob-tained in previous interactions, i.e.
user model fea-tures.
Furthermore these strategies can lead torepetitive action, i.e.
if a screen output was onceshown to this user, and the user has previouslyused or referred to the screen, the screen will beused over and over again.For learning a strategy which varies in contextbut adapts in more subtle ways (e.g.
to the usermodel), we would need to explore many morestrategies through interactions with users to findan optimal one.
One way to reduce costs for build-ing such an optimised strategy is to apply Rein-forcement Learning (RL) with simulated users.
Infuture work we will begin with the strategy learntby supervised learning (which reflects sub-optimalaverage wizard behaviour) and optimise it for dif-ferent user models and reward structures.Figure 4: Five-rule tree from J4.8 (?inf?
= ?
)7 Summary and Future WorkWe showed that humans use a context-dependentstrategy for asking multimodal clarification re-quests by learning such a strategy from WOZ data.Only the two wizards with the lowest performancescores showed no significant variation across ses-sions, leading us to hypothesise that the better wiz-ards converged on a context-dependent strategy.We were able to discover a runtime context basedon which all wizards behaved uniformly, usingfeature discretisation methods and feature selec-tion methods on dialogue context features.
Basedon these features we were able to predict howan ?average?
wizard would behave in that contextwith an accuracy of 84.6% (wf-score of 85.3%,which is a 25.5% improvement over a one rule-based baseline).
We explained the learned strate-gies and showed that they can be implemented in665IF screenUser>1 AND (userSpeechAct=command OR screenHist>0.5) THEN graphic=yesELSE graphic=noFigure 3: Reformulation of the rules learnt by JRIPrule-based dialogue systems based on domain in-dependent features.
We also showed that featureengineering is essential for achieving significantperformance gains when using large feature spaceswith the small data sets which are typical of di-alogue WOZ studies.
By interpreting the learntstrategies we found them to be sub-optimal.
Incurrent research, RL is applied to optimise strate-gies and has been shown to lead to dialogue strate-gies which are better than those present in the orig-inal data (Henderson et al, 2005).
The next steptowards a RL-based system is to add task-level andreward-level annotations to calculate reward func-tions, as discussed in (Rieser et al, 2005).
Wefurthermore aim to learn more refined clarifica-tion strategies indicating the problem source andits severity.AcknowledgementsThe authors would like thank the ACL reviewers,Alissa Melinger, and Joel Tetreault for help and dis-cussion.
This work is supported by the TALK project,www.talk-project.org, and the International Post-Graduate College for Language Technology and CognitiveSystems, Saarbru?cken.ReferencesWilliam W. Cohen.
1995.
Fast effective rule induction.In Proceedings of the 12th ICML-95.Walter Daelemans, Ve?ronique Hoste, Fien De Meul-der, and Bart Naudts.
2003.
Combined optimizationof feature selection and algorithm parameter interac-tion in machine learning of language.
In Proceed-ings of the 14th ECML-03.Usama Fayyad and Keki Irani.
1993.
Multi-interval discretization of continuousvalued attributesfor classification learning.
In Proc.
IJCAI-93.Mark Hall.
2000.
Correlation-based feature selectionfor discrete and numeric class machine learning.
InProc.
17th Int Conf.
on Machine Learning.James Henderson, Oliver Lemon, and KallirroiGeorgila.
2005.
Hybrid Reinforcement/SupervisedLearning for Dialogue Policies from COMMUNI-CATOR data.
In IJCAI workshop on Knowledge andReasoning in Practical Dialogue Systems,.George John and Pat Langley.
1995.
Estimating con-tinuous distributions in bayesian classifiers.
In Pro-ceedings of the 11th UAI-95.
Morgan Kaufmann.Ivana Kruijff-Korbayova?, Nate Blaylock, Ciprian Ger-stenberger, Verena Rieser, Tilman Becker, MichaelKaisser, Peter Poller, and Jan Schehl.
2005.
An ex-periment setup for collecting data for adaptive out-put planning in a multimodal dialogue system.
In10th European Workshop on NLG.Pat Langley and Stephanie Sage.
1994.
Induction ofselective bayesian classifiers.
In Proceedings of the10th UAI-94.Zhang Le.
2003.
Maximum entropy modeling toolkitfor Python and C++.Oliver Lemon, Kallirroi Georgila, James Henderson,Malte Gabsdil, Ivan Meza-Ruiz, and Steve Young.2005.
Deliverable d4.1: Integration of learning andadaptivity with the ISU approach.Sharon Oviatt, Rachel Coulston, and RebeccaLunsford.
2004.
When do we interact mul-timodally?
Cognitive load and multimodalcommunication patterns.
In Proceedings of the 6thICMI-04.Sharon Oviatt.
2002.
Breaking the robustness bar-rier: Recent progress on the design of robust mul-timodal systems.
In Advances in Computers.
Aca-demic Press.Tim Paek and David Maxwell Chickering.
2005.The markov assumption in spoken dialogue manage-ment.
In Proceedings of the 6th SIGdial Workshopon Discourse and Dialogue.Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.Verena Rieser and Johanna Moore.
2005.
Implica-tions for Generating Clarification Requests in Task-oriented Dialogues.
In Proceedings of the 43rd ACL.Verena Rieser, Ivana Kruijff-Korbayova?, and OliverLemon.
2005.
A corpus collection and annota-tion framework for learning multimodal clarificationstrategies.
In Proceedings of the 6th SIGdial Work-shop on Discourse and Dialogue.David Traum and Pierre Dillenbourg.
1996.
Mis-communication in multi-modal collaboration.
InProceedings of the Workshop on Detecting, Repair-ing, and Preventing Human-Machine Miscommuni-cation.
AAAI-96.Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical Machine Learning Tools and Techniques(Second Edition).
Morgan Kaufmann.666
