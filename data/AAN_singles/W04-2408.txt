Modeling Category Structures with a Kernel FunctionHiroya TakamuraPrecision and Intelligence LaboratoryTokyo Institute of Technology4259 Nagatsuta Midori-ku Yokohama,226-8503 Japantakamura@pi.titech.ac.jpYuji MatsumotoDepartment of Information TechnologyNara Institute of Science and Technology8516-9 Takayama Ikoma Nara,630-0101 Japanmatsu@is.aist-nara.ac.jpHiroyasu YamadaSchool of Information ScienceJapan Advanced Institute of Science and Technology1-1 Asahidai Tatsunokuchi Ishikawa, 923-1292 Japanh-yamada@jaist.ac.jpAbstractWe propose one type of TOP (Tangent vectorOf the Posterior log-odds) kernel and apply it totext categorization.
In a number of categoriza-tion tasks including text categorization, nega-tive examples are usually more common thanpositive examples and there may be several dif-ferent types of negative examples.
Therefore,we construct a TOP kernel, regarding the prob-abilistic model of negative examples as a mix-ture of several component models respectivelycorresponding to given categories.
Since eachcomponent model of our mixture model is ex-pressed using a one-dimensional Gaussian-typefunction, the proposed kernel has an advantagein computational time.
We also show that thecomputational advantage is shared by a moregeneral class of models.
In our experiments,the proposed kernel used with Support VectorMachines outperformed the linear kernel andthe Fisher kernel based on the Probabilistic La-tent Semantic Indexing model.1 IntroductionRecently, Support Vector Machines (SVMs) have beenactively studied because of their high generalization abil-ity (Vapnik, 1998).
In the formulation of SVMs, func-tions which measure the similarity of two examples takean important role.
These functions are called kernel func-tions.
The usual dot-product of two vectors respectivelycorresponding to two examples is often used.
Althoughsome variants to the usual dot-product are sometimesused (for example, higher-order polynomial kernels andRBF kernels), the distribution of examples is not takeninto account in such kernels.However, new types of kernels have more recentlybeen proposed; they are based on the probability distri-bution of examples.
One is Fisher kernels (Jaakkola andHaussler, 1998).
The other is TOP (Tangent vector Of thePosterior log-odds) kernels (Tsuda et al, 2002).
WhileFisher kernels are constructed on the basis of a genera-tive model of data, TOP kernels are based on the class-posterior probability, that is, the probability that the pos-itive class occurs given an example.
However, in order touse those kernels, we have to select a probabilistic modelof data.
The selection of a model will affect categoriza-tion result.
The present paper provides one solution tothis issue.
Specifically, we proposed one type of TOPkernel, because it has been reported that TOP kernels per-form better than Fisher kernels in terms of categorizationaccuracy.We briefly explain our kernel.
We focus on negativeexamples in binary classification.
Negative examples areusually more common than positive examples.
Theremay be several different types of negative examples.
Fur-thermore, the categories of negative examples are some-times explicitly given (for example, the situation wherewe are given documents, each of which has one of threecategories ?sports?,?politics?
and ?economics?, and weare to extract documents with ?politics?).
In such a situa-tion, the probabilistic model of negative examples can beregarded as a mixture of several component models.
Weeffectively use this property.
Although many other mod-els can be used, we propose a model based on the sepa-rating hyperplanes in the original feature space.
Specif-ically, a one-dimensional Gaussian-type function normalto a hyperplane corresponds to a category.
The negativeclass is then expressed as a kind of Gaussian mixture.The reason for the selection of this model is that the re-sulting kernel has an advantage in computational time.The kernel based on this mixture model, what we callHyperplane-based TOP (HP-TOP) kernel, can be com-puted efficiently in spite of its high dimensionality.
Welater show that the computational advantage is shared bya more general class of models.In the experiments of text categorization, in whichSVMs are used as classifiers, our kernel outperformedthe linear kernel and the Fisher kernel based on the Prob-abilistic Latent Semantic Indexing model proposed byHofmann (2000) in terms of categorization accuracy.2 SVMs and Kernel MethodIn this section, we explain SVMs and the kernel method,which are the basis of our research.
SVMs have achievedhigh accuracy in various tasks including text categoriza-tion (Joachims, 1998; Dumais et al, 1998).Suppose a set Dl of ordered pairs consisting of a fea-ture vector and its labelDl = {(x1, y1), (x2, y2), ?
?
?
, (xl, yl)},(?i, xi ?
R|I|, yi ?
{?1, 1}) (1)is given.
Dl is called training data.
I is the set of featureindices.
In SVMs, a separating hyperplane (f(x) = w ?x ?
b) with the largest margin (the distance between thehyperplane and its nearest vectors) is constructed.Skipping the details of SVMs?
formulation, here wejust show the conclusion that, using some real numbers?
?i (?i) and b?, the optimal hyperplane is expressed asfollows:f(x) =?i?
?i yixi ?
x ?
b?.
(2)We should note that only dot-products of examples areused in the above expression.Since SVMs are linear classifiers, their separating abil-ity is limited.
To compensate for this limitation, thekernel method is usually combined with SVMs (Vapnik,1998).In the kernel method, the dot-products in (2) are re-placed with more general inner-products K(xi, x) (kernelfunctions).
The polynomial kernel (xi ?xj+1)d (d ?
N+)and the RBF kernel exp{?
?xi ?
xj?2/2?2} are oftenused.
Using the kernel method means that feature vectorsare mapped into a (higher dimensional) Hilbert space andlinearly separated there.
This mapping structure makesnon-linear separation possible, although SVMs are basi-cally linear classifiers.Another advantage of the kernel method is that al-though it deals with a high dimensional (possibly infinite)space, explicit computation of high dimensional vectorsis not required.
Only the general inner-products of twovectors need to be computed.
This advantage leads to arelatively small computational overhead.3 Kernels from Probabilistic ModelsRecently new type of kernels which connect genera-tive models of data and discriminative classifiers such asSVMs, have been proposed: the Fisher kernel (Jaakkolaand Haussler, 1998) and the TOP (Tangent vector Of thePosterior log-odds) kernel (Tsuda et al, 2002).3.1 Fisher KernelSuppose we have a probabilistic generative model p(x|?
)of the data (we denote an example by x).
The Fisher scoreof x is defined as ??
log p(x|?
), where ??
means par-tial differentiation with respect to the parameters ?.
TheFisher information matrix is denoted by I(?)
(this ma-trix defines the geometric structure of the model space).Then, the Fisher kernel at an estimate ??
is given by:K(x1, x2)= (??
log p(x1|??))tI?1(??)(??
log p(x2|??))
(3)The Fisher score of an example approximately indicateshow the model will change if the example is added to thetraining data used in the estimation of the model.
Thatmeans, the Fisher kernel between two examples will belarge, if the influences of the two examples to the modelare similar and large (Tsuda and Kawanabe, 2002).The matrix I(?)
is often approximated by the identitymatrix to avoid large computational overhead.3.2 TOP KernelOn the basis of a probabilistic model of the data, TOPkernels are designed to extract feature vectors f??
whichare considered to be useful for categorization with a sep-arating hyperplane.We begin with the proposition that, between the gener-alization error R(f??)
and the expected error of the poste-rior probability D(f??
), the relation R(f??)?L?
?
2D(f??
)holds, where L?
is the Bayes error.
This inequality meansthat minimizing D(f??)
leads to reducing the generaliza-tion error R(f??).
D(f??)
is expressed, using a logisticfunction F (t) = 1/(1 + exp(?t)), asD(f??
)= minw,bEx|F (w ?
f??
?
b)?
P (y = +1|x, ??
)|, (4)where ??
denotes the actual parameters of the model.The TOP kernel consists of features which can minimizeD(f??).
In other words, we would like to have feature vec-tors f??
that satisfy the following:?x, w ?
f??(x)?
b = F?1(P (y = +1|x, ??)).
(5)for certain values of w and b.For that purpose, we first define a function v(x, ?
):v(x, ?)
?
F?1(P (y = +1|x, ?
))= logP (y = +1|x, ?)?
logP (y = ?1|x, ?).
(6)The first-order Taylor expansion of v(x, ??)
around theestimate ??
isv(x, ??)
?
v(x, ??)
+?i(?
?i ?
?
?i)?v(x, ??)?
?i .
(7)If f??
is of the following form:f??
(x) =(v(x, ??
), ?v(x, ??)/?
?1, ?
?
?
, ?v(x, ??)/?
?p),(8)and if w and b are properly chosen asw = (1, ?
?1 ?
?
?1, ?
?
?
, ?
?p ?
?
?p), b = 0, (9)then (5) is approximately satisfied.
Thus, the TOP kernelis defined asK(x1, x2) = f??
(x1) ?
f??(x2).
(10)A detailed discussion of the TOP kernel and its theoreti-cal analysis have been given by Tsuda et al(Tsuda et al,2002).4 Related WorkHofmann (2000) applied Fisher kernels to text catego-rization under the Probabilistic Latent Semantic Indexing(PLSI) model (Hofmann, 1999).In PLSI, the joint probability of document d and wordw is :P (d, w) =?kP (zk)P (d|zk)P (w|zk), (11)where variables zk correspond to latent classes.
Afterthe estimation of the model using the EM algorithm, theFisher kernel for this model is computed.
The averagelog-likelihood of document d normalized by the docu-ment length is given byl(d) =?jP?
(wj |d) log?kP (wj |zk)P (zk|d), (12)whereP?
(wj |d) = freq(wj , d)?m freq(wm,d).
(13)They use spherical parameterization (Kass and Vos,1997) instead of the original parameters in the model.They define parameters ?jk = 2?P (wj |zk) and ?k =2?P (zk), and obtained?l(d)?
?jk =P?
(wj |d)P (zk|d, wj)?P (wj |zk), (14)?l(d)?
?k ?P (zk|d)?P (zk).
(15)Thus, the Fisher kernel for this model is obtained as de-scribed in Appendix A.The first term of (31) corresponds to the similaritythrough latent spaces.
The second term corresponds tothe similarity through the distribution of each word.
Thenumber of latent classes zk can affect the value of thekernel function.
In the experiment of (Hofmann, 2000),they computed the kernels with the different numbers (1to 64) of zk and added them together to make a robustkernel instead of deciding one specific number of latentclasses zk.They concluded that the Fisher kernel based on PLSIis effective when a large amount of unlabeled examplesare available for the estimation of the PLSI model.5 Hyperplane-based TOP KernelIn this section, we explain our TOP kernel.5.1 Derivation of HP-TOP kernelSuppose we have obtained the parameters wc and bcof the separating hyperplane for each category c ?Ccategory in the original feature space, where Ccategorydenotes the set of categories.We assume that the class-posteriors Pc(+1|d) andPc(?1|d) are expressed as1Pc(+1|d) = P (c)q(d|c)?c?
P (c?)q(d|c?
), (16)Pc(?1|d) =?e 6=c P (e)q(d|e)?c?
P (c?)q(d|c?
)(17)where, for any category x, component function q(d|x) isof Gaussian-type:q(d|x) = 1?2pi?2xexp{?
((wx ?
d ?
bx)?
?x)22?2x},(18)with the mean ?x of a random variable wx ?
d ?
bx andthe variance ?x.
Those parameters are estimated with themaximum likelihood estimation, as follows:?x =?
(d,y)?Dl,y=x{wx?d?bx}|{(d,y)?Dl|y=x}| , (19)?x =?(d,y)?Dl,y=x{wx?d?bx?
?x}2|{(d,y)?Dl|y=x}| .
(20)We choose the Gaussian-type function as an exam-ple.However, this choice is open to argument, since someother models also have the same computational advan-tage as described in Section 5.4.We set ?x1 = ?x/?2x, ?x2 = ?1/2?2x.
Although ?x1and ?x2 are not the natural parameters of this model,1We cannot say q(d|x) is a generative probability of d givenclass x, because it is one-dimensional and not valid as a proba-bility density in the original feature space.we parameterize this model using the parameters ?x1,?x2, wx, bx and P (x) (?x ?
Ccategory) for simplic-ity.
Using this probabilistic model,we compute func-tion v(d, ?)
as described in Appendix B (?
denotes{wx, bx, ?x1, ?x2|x ?
Ccategory} and wxi denotes the i-th element of the weight vector wx).The partial derivatives of this function with respect tothe parameters are in Appendix C.Then we can follow the definition (10) to obtain ourversion of the TOP kernel.
We call this new kernel ahyperplane-based TOP (HP-TOP) kernel.5.2 Properties of HP-TOP kernelIn the derivatives (39), which provide the largest numberof features, original features di are accompanied by otherfactors computed from probability distributions.
Thisform suggests that two vectors are considered to be moresimilar, if they have similar distributions over categories.In other words, an occurrence of a word can have dif-ferent contribution to the classification result, dependingon the context (i.e., the other words in the document).This property of the HP-TOP kernel can lead to the ef-fect of word sense disambiguation, because ?bank?
in afinancial document is treated differently from ?bank?
in adocument related to a river-side park.The derivatives (34) and (35) correspond to the first-order differences, respectively for the positive class andthe negative class.
Similarly, the derivatives (36) and (37)for the second-order differences.
The derivatives (40) and(41) are for the first-order differences normalized by thevariances.The derivatives other than (38) and (38) directly de-pend on the distance from a hyperplane, rather than onthe value of each feature.
These derivatives enrich thefeature set, when there are few active words, by whichwe mean the words that do not occur in the training data.For this reason, we expect that the HP-TOP kernel workswell for a small training dataset.5.3 Computational issueComputing the kernel in this form is time-consuming, be-cause the number of components of type (39) can be verylarge:O(|I| ?
|Ccategory|), (21)where I denotes the set of indices for original features.However, we can avoid this heavy computational costas follows.
Let us compute the dot-product of deriva-tives (39) of two vectors d1 and d2, which is shown inAppendix D. The last expression (45) is regarded as thescalar product of two dot-products.
Thus, by preservingvectors d and(?P (e)q(d|e)P?c(d)?e ?
(we ?
d ?
be)?2e)e 6=c,e?Ccategory, (22)we can efficiently compute the dot-product in (39); thecomputational complexity of a kernel function isO(|I|), (23)on the condition that the original dimension is larger thanthe number of categories.
Thus, from the viewpoint ofcomputational time, our kernel has an advantage oversome other kernels such as the PLSI-based Fisher kernelin Section 4, which requires the computational complex-ity of O(|I| ?
|Ccluster|), where Ccluster denotes the setof clusters.In the PLSI-based Fisher kernel, each word has a prob-ability distribution over latent classes.
In this sense, thePLSI-based Fisher kernel is more detailed, but detailedmodels are sometimes suffer overfitting to the trainingdata and have the computational disadvantage as men-tioned above.The PLSI-based Fisher kernel can be extended to aTOP kernel by using given categories as latent classes.However, the problem of computational time still re-mains.5.4 General statement about the computationaladvantageSo far, we have discussed the computational time forthe kernel constructed on the Gaussian mixture.
How-ever, the computational advantage of the kernel, in fact,is shared by a more general class of models.We examine the required conditions for the computa-tional advantage.
Suppose the class-posteriors have themixture form as Equations (16) and (17), but functionq(d|x) does not have to be a Gaussian-type function.
In-stead, function q(d|x) is supposed to be represented usingsome function r parametrized by we and b, as:q(d|x) = r(fx(d)|x), (24)where fx is a scalar function.
Then, let us obtain thederivative of v(d, ?)
with respect to wei, which is the bot-tleneck of kernel computation:?v(d, ?
)?wei= ?P (e)q(d|e)P?c(d)?r(fe(d)|e)?wei= ?P (e)q(d|e)P?c(d)?r(fe(d)|e)?fe(d)?fe(d)?wei .
(25)The first two factors of (25) do not depend on i. There-fore, if the last factor of (25) is variable-separable withrespect to e and i:?fe(d)?wei = S(e)T (i), (26)where S and T are some function, then the derivative(25) is also variable-separable.
In such cases, the effi-cient computation described in Section 5.3 is possible bypreserving the vectors:(T (i))i?I , (27)(?P (e)q(d|e)P?c(d)?r(fe(d)|e)?fe(d) S(e))e 6=c,e?Ccategory.
(28)We have now obtained the required conditions for theefficient computation: Equation (24) and the variable-separability.In case of Gaussian-type functions, function fe and itsderivative with respect to wei arefe(d) = we ?
d ?
be, (29)?fe(d)?wei = di.
(30)Thus, the conditions are satisfied.6 ExperimentsThrough experiments of text categorization, we empiri-cally compare the HP-TOP kernel with the linear kerneland the PLSI-based Fisher kernel.
We use Reuters-21578dataset2 with ModApte-split (Dumais et al, 1998).
In ad-dition, we delete some texts from the result of ModApte-split, because those texts have no text body.
After thedeletion, we obtain 8815 training examples and 3023 testexamples.
The words that occur less than five times in thewhole training set are excluded from the original featureset.We do not use all the 8815 training examples.
Thesize of the actual training data ranges from 1000 to 8000.For each dataset size, experiments are executed 10 timeswith different training sets.The result is evaluated with F-measures for the most frequent 10 categories (Table 1).The total number of categories is actually 116.
How-ever, for small categories, reliable statistics cannot be ob-tained.
For this reason, we regard the remaining cate-gories other than the 10 most frequent categories as onecategory.
Therefore, the model for negative examples isa mixture of 10 component models (9 out of the 10 mostfrequent categories and the new category consisting of theremaining categories).We assume uniform priors for categories as in (Tsudaet al, 2002).
We computed the Fisher kernels with differ-ent numbers (10, 20 and 30) of latent classes and addedthem together to make a robust kernel (Hofmann, 2000).After the learning in the original feature space, the param-eters for the probability distributions are estimated with2Available fromhttp://www.daviddlewis.com/resources/.Table 1: The categories and their sizes of Reuters-21578category training texts test textsearn 2725 1051acq 1490 644money-fx 464 141grain 399 135crude 353 164trade 339 133interest 291 100ship 197 87wheat 199 66corn 161 48maximum likelihood estimation as in Equations (19) and(20), followed by the learning with the proposed kernel.We used an SVM package, TinySVM3, for SVM com-putation.
The soft-margin parameter C was set to 1.0(other values of C showed no significant changes in re-sults).The result is shown in Figure 1 (for macro-average)and Figure 2 (for micro-average).
The HP-TOP kerneloutperforms the linear kernel and the PLSI-based Fisherkernel for every number of examples.At each number of examples, we conducted aWilcoxon Signed Rank test with 5% significance-level,for the HP-TOP kernel and the linear kernel, since thesetwo are better than the other.
The test shows that the dif-ference between the two methods is significant for thetraining data sizes 1000 to 5000.
The superiority of theHP-TOP kernel for small training datasets supports ourexpectation that the enrichment of feature set will lead tobetter performance for few active words.
Although wealso expected that the effect of word sense disambigua-tion would improve accuracy for large training datasets,the experiments do not provide us with an empirical ev-idence for the expectation.
One possible reason is thatGaussian-type functions do not reflect the actual distribu-tion of data.
We leave its further investigation as futureresearch.In this experimental setting, the PLSI-based Fisher ker-nel did not work well in terms of categorization accuracy.However, this Fisher kernel will perform better when thenumber of labeled examples is small and a number ofunlabeled examples are available, as reported by Hof-mann (2000).We also measured computational time of each method(Figure 3).
The vertical axis indicates the average com-putational time over 100 runs of experiments (10 runs foreach category).
Please note that training time in this fig-3Available fromhttp://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM/.646668707274767880821000 2000 3000 4000 5000 6000 7000 8000F-measureNumber of Labeled ExamplesHP-TOP KernelLinear KernelPLSI-based Fisher KernelFigure 1: Macro-average of F-measure848586878889901000  2000  3000  4000  5000  6000  7000  8000F-measureNumber of Labeled ExamplesHP-TOP KernelLinear KernelPLSI-based Fisher KernelFigure 2: Micro-average of F-measure0.11101001000100001000  2000  3000  4000  5000  6000  7000  8000Computational Time (seconds)Number of Labeled ExamplesHP-TOP KernelLinear KernelPLSI-based Fisher KernelFigure 3: Computational time of each methodure does not include the computational time required forfeature extraction4.
This result empirically shows that theHP-TOP kernel outperforms the PLSI-based Fisher ker-nel in terms of computational time as theoretically ex-pected in Section 5.3.7 ConclusionWe proposed a TOP kernel based on separating hy-perplanes.
The proposed kernel is created from one-dimensional Gaussians along the normal directions of thehyperplanes.
We showed that the computational advan-tage that the proposed kernel has is shared by a moregeneral class of models.
We empirically showed that theproposed kernel outperforms the linear kernel in text cat-egorization.Although the superiority of the proposed method to thelinear kernel was shown, the proposed method has to befurther investigated.
Firstly, for large data sizes (namely7000 and 8000), the proposed method was not signifi-cantly better than the linear kernel.
The effectiveness ofthe proposed method should be confirmed by more ex-periments and theoretical analysis.
Secondly, we have tocompare the proposed method with other kernels in or-der to check the effectiveness of the kernel function con-sisting of one-dimensional Gaussians normal to the hy-perplanes.
The use of Gaussians is open to argument,because their symmetric form is somewhat against our4If the computational time required for feature extraction isincluded, the HP-TOP kernel cannot be faster than the linearkernel.intuition.This model can be extended to incorporate unlabeledexamples, for example, using the EM algorithm.
In thatsense, the combination of PLSI and the semi-supervisedEM algorithm is also one promising model.
When thecategory structure of the negative examples is not given,the proposed method is not applicable.
We should inves-tigate whether unsupervised clustering can substitute forthe category structure.ReferencesSusan T. Dumais, John Platt, David Heckerman, andMehran Sahami.
1998.
Inductive learning algo-rithms and representations for text categorization.
InProceedings of the Seventh International Conferenceon Information and Knowledge Management (ACM-CIKM98), pages 148?155.Thomas Hofmann.
1999.
Probabilistic Latent Seman-tic Indexing.
In Proceedings of the 22nd Annual ACMConference on Research and Development in Informa-tion Retrieval, pages 50?57, Berkeley, California, Au-gust.Thomas Hofmann.
2000.
Learning the similarity of doc-uments: An information geometric approach to docu-ment retrieval and categorization.
In Advances in Neu-ral Information Processing Systems, 12, pages 914?920.Tommi Jaakkola and David Haussler.
1998.
Exploitinggenerative models in discriminative classifiers.
In Ad-vances in Neural Information Processing Systems 11,pages 487?493.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many relevantfeatures.
In Proceedings of the 10th European Con-ference on Machine Learning, pages 137?142.Robert E. Kass and Paul W. Vos.
1997.
Geometricalfoundations of asymptotic inference.
New York : Wi-ley.Koji Tsuda and Motoaki Kawanabe.
2002.
The leave-one-out kernel.
In Proceedings of International Con-ference on Artificial Neural Networks, pages 727?732.Koji Tsuda, Motoaki Kawanabe, Gunnar Ra?tsch, So?renSonnenburg, and Klaus-Robert Mu?ller.
2002.
A newdiscriminative kernel from probabilistic models.
Neu-ral Computation, 14(10):2397?2414.Vladimir Vapnik.
1998.
Statistical Learning Theory.John Wiley, New York.A Fisher Kernel based on PLSIK(d1, d2) =?kP (zk|d1)P (zk|d2)P (zk) +?jP?
(wj |d1)P?
(wj |d2)?kP (zk|d1, wj)P (zk|d2, wj)P (wj |zk) , (31)where P (zk|d, wj) = P (zk)P (d|zk)P (wj |zk)?l P (zl)P (d|zl)P (wj |zl)(= P (zk)P (d|zk)P (wj |zk)P (d, wj)).
(32)B Function v for HP-TOP Kernelv(d, ?,w, b) = logP (+1|d)?
logP (?1|d)= log P (c)q(d|c)?c?
P (c?)q(d|c?)?
log?e6=c P (e)q(d|e)?c?
P (c?)q(d|c?
)= logP (c)q(d|c)?
log?e6=cP (e)q(d|e)= logP (c) exp{?c1(wc ?
d) + ?c2(wc ?
d)2 + ?2c14?c2 ?12 log?pi?c2 }?
log?e6=cP (e) exp{?e1(we ?
d) + ?e2(we ?
d)2 + ?2e14?e2 ?12 log?pi?e2 }, (33)where ?x1 = ?x/?2x, ?x2 = ?1/2?2x.C Partial Derivatives?v(d, ?)?
?c1 = wc ?
d ?
bc ?
?c, (34)?v(d, ?)?
?e1 = ?P (e)q(d|e)?c?
6=c P (c?)q(d|c?
)(we ?
d ?
be ?
?e), (35)?v(d, ?)?
?c2 = (wc ?
d ?
bc)2 ?
?2c ?
?2c , (36)?v(d, ?)?
?e2 = ?P (e)q(d|e)?c?
6=c P (c?)q(d|c?
){(we ?
d ?
be)2 ?
?2e ?
?2e}, (37)?v(d, ?
)?wci =?c ?
(wc ?
d ?
bc)?2c di, (38)?v(d, ?
)?wei = ?P (e)q(d|e)?c?
6=c P (c?)q(d|c?
)?e ?
(we ?
d ?
be)?2e di, (39)?v(d, ?
)?bc =wc ?
d ?
bc ?
?c?2c , (40)?v(d, ?
)?be = ?P (e)q(d|e)?c?
6=c P (c?)q(d|c?
)we ?
d ?
be ?
?e?2e , (41)?v(d, ?
)P (c) =1P (c) , (42)?v(d, ?
)P (e) = ?P (d|e)?c?
6=c P (c?)q(d|c?).
(43)D Dot-product of Derivatives (39) in Appendix C?e6=c?i?v(d1, ?
)?wei?v(d2, ?
)?wei =?e6=c?iP (e)2q(d1|e)q(d2|e)P?c(d1)P?c(d2)?e ?
(we ?
d ?
be)?2e?e ?
(we ?
d ?
be)?2e d1i d2i (44)=(?e6=cP (e)2q(d1|e)q(d2|e)P?c(d1)P?c(d2)?e ?
(we ?
d ?
be)?2e?e ?
(we ?
d ?
be)?2e)d1 ?
d2, (45)where P?c(d) denotes?c?
6=c P (c?)q(d|c?
).
