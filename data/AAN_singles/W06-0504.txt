Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 26?32,Sydney, July 2006. c?2006 Association for Computational LinguisticsOntology Population from Textual Mentions:Task Definition and BenchmarkBernardo Magnini, Emanuele Pianta, Octavian Popescu andManuela SperanzaITC-irst, Istituto per la Ricerca Scientifica e TecnologicaVia Sommarive 18, 38050 Povo (TN), Italy{magnini, pianta, popescu, manspera}@itc.itAbstractIn this paper we propose and investigateOntology Population from Textual Mentions(OPTM), a sub-task of Ontology Populationfrom text where we assume that mentions forseveral kinds of entities (e.g.
PERSON,O R G A N I Z A T I O N , LO C A T I O N , GEO-POLITICAL_ ENTITY) are already extractedfrom a document collection.
On the onehand, OPTM simplifies the general OntologyPopulation task, limiting the input textualmaterial; on the other hand, it introduceschallenging extensions to Ontology Popula-tion restricted to named entities, being opento a wider spectrum of linguistic phenomena.We describe a manually created benchmarkfor OPTM and discuss several factors whichdetermine the difficulty of the task.1 IntroductionMentions are portions of text which refer to enti-ties1.
As an example, given a particular textualcontext, both the mentions ?George W. Bush?and ?the U.S.
President.?
refer to the same entity,i.e.
a particular instance of Person whose firstname is ?George?, whose middle initial is ?W.
?,whose family name is ?Bush?
and whose role is?U.S.
President?.In this paper we propose and investigate Ontol-ogy Population from Textual Mentions (OPTM),a sub-task of Ontology Learning and Population1The terms ?mention?
and ?entity?
have been intro-duced within the ACE Program (Linguistic Data Con-sortium, 2004).
?Mentions?
are equivalent to ?refer-ring expressions?
and ?entities?
are equivalent to?referents?, as widely used in computational linguis-tics.
In this paper, we use italics for ?mentions?
andsmall caps for ENTITY and ENTITY_ATTRIBUTE.
(OLP) from text where we assume that mentionsfor several kinds of entities (e.g.
PERSON,ORGANIZATION, LO C A T I O N , GEO-POLITICAL_ENTITY) are already extracted from a documentcollection.We assume an ontology with a set of classesC={c1, ?, cn} with each class c1being describedby a set of attribute value pairs [a1, v1].
Given aset of mentions M={m1,c1, ?,  mn,cn}, where eachmention mjis classified into a class ciin C, theOPTM task is defined in three steps: Recognitionand Classification of Entity Attributes, Normali-zation, and Resolution of inter-text Entity Co-reference.
(i) Recognition and Classification of EntityAttributes (RCEA).
The textual materialexpressed in a mention is extracted and dis-tributed along the attribute-value pairs al-ready defined for the class ciof the mention;as an example, given the PERSON mention?U.S.
President Bush?, we expect that theattribute LAST_NAME is filled with the value?Bush?
and the attribute ROLE is filled withthe value ?U.S.
President?.
Note that fillers,at this step, are still portions of text.
(ii) Normalization.
The textual material ex-tracted at step (i) is assigned to concepts andrelations already defined in the ontology; forexample, the entity BUSH is created as an in-stance of COUNTRY_PRESIDENT, and an in-stance of the relation PRESIDENT_OF is cre-ated between BUSH and U.S.A. At this stepdifferent instances are created for co-referring mentions.
(iii) Resolution of inter-text Entity Co-reference (REC).
Each mention mjhas to beassigned to a single individual entity be-longing to a class in C .
For example, werecognize that the instances created at step(i) for ?U.S.
President Bush?
and ?GeorgeW.
Bush?
actually refer to the same entity.26In this paper we address steps (i) and (iii),while step (ii) is work in progress.
The input ofthe OPTM task consists of classified mentionsand the output consists of individual entitiesfilled with textual material (i.e.
there is no nor-malization) with their co-reference relations.
Thefocus is on the definition of the task and on anempirical analysis of the aspects that determineits complexity, rather than on approaches andmethods for the automatic solution of OPTM.There are several advantages of OPTM whichmake it appealing for OLP.
First, mentions pro-vide an obvious simplification with respect to themore general Ontology Population from text (cf.Buitelaar et al 2005); in particular, mentions arewell defined and there are systems for automaticmention recognition.
Although there is no univo-cally accepted definition for the OP task, a usefulapproximation has been suggested by(Bontcheva and Cunningham, 2005) as OntologyDriven Information Extraction with the goal ofextracting and classifying instances of conceptsand relations defined in a Ontology, in place offilling a template.
A similar task has been ap-proached in a variety of perspectives, includingterm clustering (Lin, 1998 and Almuhareb andPoesio, 2004) and term categorization (Avanciniet al 2003).
A rather different task is OntologyLearning, where new concepts and relations aresupposed to be acquired, with the consequence ofchanging the definition of the Ontology itself(Velardi et al 2005).
However, since mentionshave been introduced as an evolution of the tra-ditional Named Entity Recognition task (seeTanev and Magnini, 2006), they guarantee a rea-sonable level of difficulty, which makes OPTMchallenging both for the Computational Linguis-tic side and the Knowledge Representationcommunity.
Second, there already exist anno-tated data with mentions, delivered under theACE (Automatic Content Extraction) initiative(Ferro et al 2005, Linguistic Data Consortium2004), which makes the exploitation of machinelearning based approaches possible.
Finally,having a limited scope with respect to OLP, theOPTM task allows for a better estimation of per-formance; in particular, it is possible to evaluatemore easily the recall of the task, i.e.
the propor-tion of information correctly assigned to an en-tity out of the total amount of information pro-vided by a certain mention.In the paper we both define the OPTM taskand describe an OPTM benchmark, i.e.
a docu-ment collection annotated with mentions as wellas an ontology where information from mentionshas been manually extracted.
The general archi-tecture of the OPTM task has been sketchedabove, considering three sub tasks.
The docu-ment collection we use consists of about 500Italian news items.
Currently, mentions referringto PE R S O N , ORGANIZATION and GEO-POLITICAL_ ENTITY have been annotated and co-references among such mentions have been es-tablished.
As for the RCEA sub task, we haveconsidered mentions referring to PERSON andhave built a knowledge base of instances, eachdescribed with a number of attribute-value pairs.The paper is structured as follows.
Section 2provides the useful background as far as men-tions and entities are concerned.
Section 3 de-fines the OPTM task and introduces the datasetwe have used, as well as the annotation proce-dures and guidelines we have defined for the re-alization of the OPTM benchmark corpus.
Sec-tion 4 reports on a number of quantitative andqualitative analyses of the OPTM benchmarkaimed at determining the difficulty of the task.Finally, Section 5 proposes future extensions anddevelopments of our work.2 Mentions and EntitiesAs indicated in the ACE Entity Detectiontask, the annotation of entities (e.g.
PERSON,ORGANIZAT I O N , LOCAT I O N  a n d  GEO-POLITICAL_ENTITY) requires that the entitiesmentioned in a text be detected, their syntactichead marked, their sense disambiguated, and thatselected attributes of these entities be extractedand merged into a unified representation for eachentity.As it often happens that the same entity ismentioned more than once in the same text, twointer-connected levels of annotation have beendefined: the level of the entity, which provides arepresentation of an object in the world, and thelevel of the entity mention, which provides in-formation about the textual references to thatobject.
For instance, if  the entityGEORGE_W._BUSH (e.g.
the individual in theworld who is the current president of the U.S.) ismentioned in two different sentences of a text as?the U.S. president?
and as ?the president?, thesetwo expressions are considered as two co-referring entity mentions.The kinds of reference made by entities tosomething in the world are described by the fol-lowing four classes:?
specific referential entities are those wherethe entity being referred to is a unique object27or set of objects (e.g.
?The president ofthecompany is here?)2;?
generic referential entities refer to a kind ortype of entity and not to a particular object (orset of objects) in the world (e.g.
?The presi-dent is elected every 5 years?);?
under-specified referential entities are non-generic non-specific references, including im-precise quantifications (e.g.
?everyone?)
andestimates (e.g.
?more than 10.000 people?);?
negatively quantified entities refer to theempty set of the mentioned type of object (e.g.
?No lawyer?
).The textual extent of mentions is defined asthe entire nominal phrase used to refer to an en-tity, thus including modifiers (e.g.
?a big fam-ily?
), prepositional phrases (e.g.
?the President ofthe Republic?)
and dependent clauses (e.g.
?thegirl who is working in the garden?
).The classification of entity mentions is basedon syntactic features; among the most significantcategories defined by LDD (Linguistic DataConsortium 2004) there are:- NAM: proper names (e.g.
?Ciampi?, ?theUN?
);- NOM: nominal constructions (e.g.
?good chil-dren?, ?the company?
);- PRO: pronouns, e.g.
personal (?you?)
and in-definite (?someone?
);- WHQ: wh-words, such as relatives and inter-rogatives (e.g.
?Who?s there??
);- PTV: partitive constructions (e.g.
?some ofthem?, ?one of the schools?
);- APP: appositive constructions (e.g.
?Dante,famous poet?
, ?Juventus, Italian footballclub?
).Since the dataset presented in this paper hasbeen developed for Italian, some new types ofmentions have been added to those listed in theLDC guidelines; for instance, we have created aspecific tag, ENCLIT, to annotate the cliticswhose extension can not be identified at word-level (e.g.
?veder[lo]?/?to see him?).
Some typesof mentions, on the other hand, have been elimi-nated; this is the case for pre-modifiers, due tosyntactic differences between English, whereboth adjectives and nouns can be used as pre-modifiers, and Italian, which only admits adjec-tives in that position.In extending the annotation guidelines, wehave decided to annotate all conjunctions of en-tities, not only those which share the same modi-fiers as indicated in the ACE guidelines, and tomark them using a specific new tag, CONJ (e.g.2Notice that the corpus is in Italian, but we present Englishexamples for the sake of readability.
?mother and child?
)3.According to the ACE standards, each dis-tinct person or set of people mentioned in adocument refers to an entity of type PERSON.
Forexample, people may be specified by name(?John Smith?
), occupation (?the butcher?
),family relation (?dad?
), pronoun (?he?
), etc., orby some combination of these.PERSON (PE), the class we have consideredfor the Ontology Population from Textual Men-tion task, is further classified with the followingsubtypes:?
INDIVIDUAL_PERSON: PES which refer to asingle person (e.g.
?George W.
Bush?);?
GROUP_PERSON: PES which refer to more thanone person (e.g.
?my parents?, ?your family?,etc.);?
INDEFINITE_PERSON: a PE is classified as in-definite when it is not possible to judge fromthe context whether it refers to one or morepersons (e.g.
?I wonder who came to see me?
).3 Task definitionIn Section 3.1 we first describe the documentcollection we have used for the creation of theOPTM benchmark.
Then, Section 3.2 providesdetails about RCEA, the first step in OPTM.3.1 Document collectionThe OPTM benchmark is built on top of adocument collection (I-CAB, Italian ContentAnnotated Bank)4annotated with entity men-tions.
I-CAB (Magnini et al 2006) consists of525 news documents taken from the local news-paper ?L?Adige?5.
The selected news stories be-long to four different days (September, 7th and8th 2004 and October, 7th and 8th 2004) and aregrouped into five categories: News Stories, Cul-tural News, Economic News, Sports News andLocal News (see Table 1).09/07 09/08 10/07 10/08 TotalNews 23 25 18 21 87Culture 20 18 16 18 72Economy 13 15 12 14 54Sport 29 41 27 26 123Local 46 43 49 51 189TOTAL 131 142 122 130 525Table 1: Number of news stories per category.3Appositive and conjoined mentions are complex construc-tions.
Although LDC does not identify heads for complexconstructions, we have decided to annotate all the extent ashead.4A demo is available at http://ontotext.itc.it/webicab5http://www.ladige.it/28I-CAB is further divided into training andtest sections, which contain 335 and 190 docu-ments respectively.
In total, I-CAB consists ofaround 182,500 words: 113,500 and 69,000words in the training and the test sections re-spectively (the average length of a news story isaround 339 words in the training section and 363words in the test section).The annotation of I-CAB is being carried outmanually, as we intend I-CAB to become abenchmark for various automatic InformationExtraction tasks, including recognition and nor-malization of temporal expressions, entities, andrelations between entities (e.g.
the relation af-filiation connecting a person to the organizationto which he or she is affiliated).3.2 Recognition and ClassificationAs stated in Section 1, we assume that foreach type of entity there is a set of attribute-valuepairs, which typically are used for mentioningthat entity type.
The same entity may have dif-ferent values for the same attribute and, at thispoint no normalization of the data is made, sothere is no way to differentiate between differentvalues of the same attribute, e.g.
there is nostipulation regarding the relationship between?politician?
and ?political leader?.
Finally, wecurrently assume a totally flat structure amongthe possible values for the attributes.The work we describe in this Section and inthe next one concerns a pilot study on entities oftype PERSON.
After an empirical investigation onthe dataset described in Section 3.1 we have as-sumed that the attributes listed in the first columnof Table 2 constitute a proper set for this type ofentity.
The second column lists some possiblevalues for each attribute.The textual extent of a value is defined as themaximal extent containing pertinent information.For instance, if we have a person mentioned as?the thirty-year-old sport journalist?, we willselect ?sport journalist?
as value for the attributeACTIVITY.
In fact, the age of the journalist in notpertinent to the activity attribute and is left out,whereas ?sport?
contributes to specifying theactivity performed.As there are always less paradigmatic valuesfor a given attribute, we shortly present furtherthe guidelines in making a decision in thosecases.
Generally, articles and prepositions are notadmitted at the beginning of the textual extent ofa value, an exception being made in the case ofarticles in nicknames.Attributes Possible valuesFIRST_NAME Ralph, GregMIDDLE_NAME J., W.LAST_NAME McCarthy, NewtonNICKNAME Spider, EnigmistaTITLE prof., Mr.SEX actressACTIVITYAFFILIATIONROLEjournalist, doctorThe New York Timesdirector, presidentPROVENIENCE South AmericanFAMILY_RELATION father, cousinAGE_CATEGORY boy, girlMISCELLANEA The men with red shoesTable 2.
Attributes for PERSON.Typical examples for the TITLE attribute are?Mister?, ?Miss?, ?Professor?, etc.
We consideras TITLE the words which are used to addresspeople with special status, but which do not referspecifically to their activity.
In Italian, profes-sions are often used to address people (e.g.
?av-vocato/lawyer?, ?ingegnere/engineer?).
In orderto avoid a possible overlapping between theTITLE attribute and the ACTIVITY attribute, pro-fessions are considered values for title only ifthey appear in abbreviated forms (?avv.
?, ?ing.?etc.)
before a proper name.With respect to the SEX attribute, we con-sider as values all the portions of text carryingthis information.
In most cases, first and middlenames are relevant.
In addition, the values of theSEX attribute can be gendered words (e.g.
?Mis-ter?
vs.
?Mrs.
?, ?husband?
vs.
?wife?)
and wordsfrom grammatical categories carrying informa-tion about gender (e.g.
adjectives).The attributes A CTIVITY, RO L E , AF -FILIATION are three strictly connected attributes.ACTIVITY refers to the actual activity performedby a person, while ROLE refers to the positionthey occupy.
So, for instance, ?politician?
is apossible value for ACTIVITY, while ?leader of theLabour Party?
refers to a ROLE.
Each group ofthese three attributes is associated with a mentionand all the information within a group has to bederived from the same mention.
If differentpieces of information derive from distinct men-tions, we will have two separate groups.
Con-sider the following three mentions of the sameentity:29(1) ?the journalist of Radio Liberty?
(2) ?the redactor of breaking news?
(3) ?a spare time astronomer?These three mentions lead to three differentgroups of ACTIVITY, ROLE and AFFILIATION.The obvious inference that the first two mentionsconceptually belong to the same group is notdrawn.
This step is to be taken at a further stage.The PROVENIENCE attribute can have asvalues all phrases denoting geographical/racialorigin or provenience and religious affiliation.The attribute AGE_CATEGORY can have eithernumerical values, such as ?three years old?, orwords indicating age, such as ?middle-aged?, etc.In the next section we will analyze the occur-rences of the values of these attributes in a newscorpus.4 Data analysisThe difficulty of the OPTM task is directly cor-related to four factors: (i) the extent to which thelinguistic form of mentions varies; (ii) the per-plexity of the values of the attributes; (iii) thesize of the set of the potential co-references and(iv) the number of different mentions per entity.In this section we present the work we have un-dertaken so far and the results we have obtainedregarding the above four factors.We started with a set of 175 documents be-longing to the I-CAB corpus (see Section 3.1).Each document has been manually annotatedobserving the specifications described in Section3.2.
We focused on mentions referring toINDIVIDUAL PERSON (Mentions in Table 3), ex-cluding from the dataset both mentions referringto different entity types (e.g.
ORGANIZATION)and PERSON GROUP.
In addition, for the pur-poses of this work we decided to filter out thefollowing mentions: (i) mentions consisting of asingle pronoun; (ii) nested mentions, (in particu-lar in the case where a larger mention, e.g.
?President Ciampi?, contained a smaller one, e.g.
?Ciampi?, only the larger mention was consid-ered).
The total number of remaining mentions(Meaningful mentions in Table 3) is 2343.
Fi-nally, we filtered out repetitions of mentions (i.e.string equal) that co-refer inside the same docu-ment, obtaining a set of 1139 distinct mentions.The average number of mentions for an entityin a document is 2.09, while the mentions/entityproportion within the whole collection is 2.68.The detailed distribution of mentions with re-spect to document entities is presented in Table4.
Columns 1 and 3 list the number of mentionsand columns 2 and 4 list the number of entitieswhich are mentioned for the respective numberof times (from 1 to 9 and more than 10).
For in-stance, in the dataset there are 741 entities which,within a single document, have just one mention,while there are 27 entities which are mentionedmore than 10 times in the same document.
As anindication of variability, only 14% of documententities have been mentioned in two differentways.Documents 175Words 57 033Words in mentions 8116Mentions 3157Meaningful mentions 2343Distinct mentions 1139Document entities 1117Collection entities 873Table 3.
Documents, mentions and entities in theOPTM dataset.#M/E #occ #M/E #occ1 741 6 152 164 7 113 64 8 124 47 9 55 31 ?10 27Table 4.
Distribution of mentions per entity.4.1 Co-reference densityWe can estimate the a priori probability that twoentities selected from different documents co-refer.
Actually, this is the estimate of the prob-ability that two entities co-refer conditioned bythe fact that they have been correctly identifiedinside the documents.
We can compute suchprobability as the complement of the ratio be-tween the number of different entities and thenumber of the document entities in the collec-tion.entitiesdocumententitiescollectioncorefcrossP??
?=?##1)(From Table 3 we read these values as 873and 1117 respectively, therefore, for this corpus,the probability of intra-document co-reference isapproximately 0.22.30A cumulative factor in estimating the diffi-culty of the co-reference task is the ratio betweenthe number of different entities and the numberof mentions.
We call this ratio the co-referencedensity and it shows the a priori expectation thata correct identified mention refers to a new en-tity.mentionsentitiescollectiondensitycoref## ?=?The co-reference density takes values in theinterval with limits [0-1].
The case where the co-reference density tends to 0 means that all thementions refer to the same entity, while wherethe value tends to 1 it means that each mention inthe collection refers to a different entity.
Bothlimits render the co-reference task superfluous.The figure for co-reference density we found inour corpus is 873/2343 ?
0.37, and it is far frombeing close to one of the extremes.A last measure we introduce is the ratiobetween the number of different entities and thenumber of distinct mentions.
Let?s call it pseudoco-reference density.
In fact it shows the value ofco-reference density conditioned by the fact thatone knows in advance whether two mentions thatare identical also co-refer.mentionsdistinctentitiescollectiondensitypcoref?
?=?##The pseudo co-reference for our corpus is873/1139 ?
0.76.
This information is not directlyexpressed in the collection, so it should be ap-proximated.
The difference between co-referencedensity and pseudo co-reference density (see Ta-ble 5) shows the increase in recall, if one consid-ers that two identical mentions refer to the sameentity with probability 1.
On the other hand, theloss in accuracy might be too large (consider forexample the case when two different people hap-pen to have the same first name).co-reference density 0.37pseudo co-reference density 0.76cross co-reference 0.22Table 5.
A priori estimation of difficulty of co-reference4.2 Attribute variabilityThe estimation of the variability of the values fora certain attribute is given in Table 6.
The firstcolumn indicates the attribute under considera-tion; the second column lists the total number ofmentions of the attribute found in the corpus; thethird column lists the number of different valuesthat the attribute actually takes and, between pa-rentheses, its proportion over the total number ofvalues; the fourth column indicates the propor-tion of the occurrences of the attribute with re-spect to the total number of mentions (distinctmentions are considered).Table 6.
Variability of values for attributes.In Table 7 we show the distribution of the at-tributes inside one mention.
That is, we calculatehow many times one entity contains more thanone attribute.
Columns 1 and 3 list the number ofattributes found in a mention, and columns 2 and4 list the number of mentions that actually con-tain that number of values for attributes.#attributes #mentions #attributes #mentions1 398 5 552 220 6 253 312 7 84 117 8 4Table 7.
Number of attributes inside a mention.An example of a mention from our dataset thatincludes values for eight attributes is the follow-ing:The correspondent of Al Jazira, Amr AbdelHamid, an Egyptian of Russian nationality?We conclude this section with a statistic re-garding the coverage of attributes (miscellaneaexcluded).
There are 7275 words used in 1139Attributes totalocc.distinctocc.
(%)occ.prob.FIRST_NAME 535 303 (44%) 27,0%MIDDLE_NAME 25 25 (100%) 2,1%LAST_NAME 772 690 (11%) 61,0%NICKNAME 14 14 (100%) 1,2%TITLE 12 10 (17%) 0,8%SEX 795 573 (23%) 51,0%ACTIVITY 145 88 (40%) 7,0%AFFILIATION 134 121 (10%) 11,0%ROLE 155 92 (42%) 8,0%PROVENIENCE 120 80 (34%) 7,3%FAMILY_REL.
17 17(100%) 1,4%AGE_CATEGORY 31 31(100%) 2,7%MISCELLANEA 106 106 (100%) 9,3%31distinct mentions, out of which 3606, approxi-mately 49%, are included in the values of theattributes.5 Conclusion and future workWe have presented work in progress aiming ata better definition of the general OLP task.
Inparticular we have introduced Ontology Popula-tion from Textual Mentions (OPTM) as a simpli-fication of OLP, where the source textual mate-rial are already classified mentions of entities.An analysis of the data has been conducted overa OPTM benchmark manually built from a cor-pus of Italian news.
As a result a number of indi-cators have been extracted that suggest the com-plexity of the task for systems aiming at auto-matic resolution of OPTM.Our future work is related to the definition andextension of the OPTM benchmark for the nor-malization step (see Introduction).
For this step itis crucial the construction and use of a large-scale ontology, including the concepts and rela-tions referred by mentions.
A number of inter-esting relations between mentions and ontologyare likely to emerge.The work presented in this paper is part of theONTOTEXT project, a larger initiative aimed atdeveloping text mining technologies to be ex-ploited in the perspective of the Semantic Web.The project focuses on the study and develop-ment of innovative knowledge extraction tech-niques for producing new or less noisy informa-tion to be made available to the Semantic Web.ONTOTEXT addresses three key research as-pects: annotating documents with semantic andrelational information, providing an adequatedegree of interoperability of such relational in-formation, and updating and extending the on-tologies used for Semantic Web annotation.
Theconcrete evaluation scenario in which algorithmswill be tested with a number of large-scale ex-periments is the automatic acquisition of infor-mation about people from newspaper articles.6 AcknowledgementsThis work was partially funded the three-year project ONTOTEXT6funded by the Provin-cia Autonoma di Trento.
We would like to thankNicola Tovazzi for his contribution to the anno-tation of the dataset.6http://tcc.itc.it/projects/ontotextReferencesAlmuhareb, A. and Poesio, M.. 2004.
Attribute-based and value-based clustering: An evalua-tion.
In Proceedings of EMNLP 2004, pages158--165, Barcelona, Spain.Avancini, H., Lavelli, A., Magnini, B.,Sebastiani, F., Zanoli, R. (2003).
ExpandingDomain-Specific Lexicons by Term Categori-zation.
In: Proceedings of SAC 2003, 793-79.Cunningham, H. and Bontcheva, K. KnowledgeManagement and Human Language: Crossingthe Chasm.
Journal of Knowledge Manage-ment, 9(5), 2005.Buitelaar, P., Cimiano, P. and Magnini, B.
(Eds.
)Ontology Learning from Text: Methods,Evaluation and Applications.
IOS Press, 2005.Ferro, L., Gerber, L., Mani, I., Sundheim, B. andWilson, G. (2005).
TIDES 2005 Standard forthe Annotation of Temporal Expressions.Technical report, MITRE.Lavelli, A., Magnini, B., Negri, M., Pianta, E.,Speranza, M. and Sprugnoli, R. (2005).
ItalianContent Annotation Bank (I-CAB): TemporalExpressions (V.
1.0.).
Technical Report T-0505-12.
ITC-irst, Trento.Lin, D. (1998).
Automatic Retrieval and Clus-tering of Similar Words.
In: Proceedings ofCOLING-ACL98, Montreal, Canada, 1998.Linguistic Data Consortium (2004).
ACE(Automatic Content Extraction) English An-notation Guidelines for Entities, version 5.6.12005.05.23.http://projects.ldc.upenn.edu/ace/docs/English-Entities-Guidelines_v5.6.1.pdfMagnini, B., Pianta, E., Girardi, C., Negri, M.,Romano, L., Speranza, M., Bartalesi Lenzi, V.and Sprugnoli, R. (2006).
I-CAB: the ItalianContent Annotation Bank.
Proceedings ofLREC-2006, Genova, Italy, 22-28 May, 2006.Tanev, H. and Magnini, B. Weakly SupervisedApproaches for Ontology Population.
Pro-ceedings of EACL-2006, Trento, Italy, 3-7April, 2006.Velardi, P., Navigli, R., Cuchiarelli, A., Neri, F.(2004).
Evaluation of Ontolearn, a Methodol-ogy for Automatic Population of Domain On-tologies.
In: Buitelaar, P., Cimiano, P.,Magnini, B.
(eds.
): Ontology Learning fromText: Methods, Evaluation and Applications,IOS Press, Amsterdam, 2005.32
