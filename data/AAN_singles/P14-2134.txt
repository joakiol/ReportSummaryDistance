Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828?834,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDistributed Representations of Geographically Situated LanguageDavid Bamman Chris Dyer Noah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{dbamman,cdyer,nasmith}@cs.cmu.eduAbstractWe introduce a model for incorporatingcontextual information (such as geogra-phy) in learning vector-space representa-tions of situated language.
In contrast toapproaches to multimodal representationlearning that have used properties of theobject being described (such as its color),our model includes information about thesubject (i.e., the speaker), allowing us tolearn the contours of a word?s meaningthat are shaped by the context in whichit is uttered.
In a quantitative evaluationon the task of judging geographically in-formed semantic similarity between repre-sentations learned from 1.1 billion wordsof geo-located tweets, our joint model out-performs comparable independent modelsthat learn meaning in isolation.1 IntroductionThe vast textual resources used in NLP ?newswire, web text, parliamentary proceedings ?can encourage a view of language as a disembod-ied phenomenon.
The rise of social media, how-ever, with its large volume of text paired with in-formation about its author and social context, re-minds us that each word is uttered by a particularperson at a particular place and time.
In short: lan-guage is situated.The coupling of text with demographic infor-mation has enabled computational modeling oflinguistic variation, including uncovering wordsand topics that are characteristic of geographicalregions (Eisenstein et al, 2010; O?Connor et al,2010; Hong et al, 2012; Doyle, 2014), learningcorrelations between words and socioeconomicvariables (Rao et al, 2010; Eisenstein et al, 2011;Pennacchiotti and Popescu, 2011; Bamman et al,2014); and charting how new terms spread geo-graphically (Eisenstein et al, 2012).
These modelscan tell us that hella was (at one time) used mostoften by a particular demographic group in north-ern California, echoing earlier linguistic studies(Bucholtz, 2006), and that wicked is used mostoften in New England (Ravindranath, 2011); andthey have practical applications, facilitating taskslike text-based geolocation (Wing and Baldridge,2011; Roller et al, 2012; Ikawa et al, 2012).One desideratum that remains, however, is how themeaning of these terms is shaped by geographicalinfluences ?
while wicked is used throughout theUnited States to mean bad or evil (?he is a wickedman?
), in New England it is used as an adverbialintensifier (?my boy?s wicked smart?).
In lever-aging grounded social media to uncover linguisticvariation, what we want to learn is how a word?smeaning is shaped by its geography.In this paper, we introduce a method that ex-tends vector-space lexical semantic models tolearn representations of geographically situatedlanguage.
Vector-space models of lexical seman-tics have been a popular and effective approachto learning representations of word meaning (Lin,1998; Turney and Pantel, 2010; Reisinger andMooney, 2010; Socher et al, 2013; Mikolov et al,2013, inter alia).
In bringing in extra-linguistic in-formation to learn word representations, our workfalls into the general domain of multimodal learn-ing; while other work has used visual informa-tion to improve distributed representations (An-drews et al, 2009; Feng and Lapata, 2010; Bruniet al, 2011; Bruni et al, 2012a; Bruni et al,2012b; Roller and im Walde, 2013), this workgenerally exploits information about the object be-ing described (e.g., strawberry and a picture of astrawberry); in contrast, we use information aboutthe speaker to learn representations that vary ac-cording to contextual variables from the speaker?sperspective.
Unlike classic multimodal systemsthat incorporate multiple active modalities (suchas gesture) from a user (Oviatt, 2003; Yu and828...WXMainAlabama AlaskaArizonaArkansashoFigure 1: Model.
Illustrated are the input dimensions that fire for a single sample, reflecting a particular word (vocabulary item#2) spoken in Alaska, along with a single output.
Parameter matrixW consists of the learned low-dimensional embeddings.Ballard, 2004), our primary input is textual data,supplemented with metadata about the author andthe moment of authorship.
This information en-ables learning models of word meaning that aresensitive to such factors, allowing us to distin-guish, for example, between the usage of wickedin Massachusetts from the usage of that word else-where, and letting us better associate geographi-cally grounded named entities (e.g, Boston) withtheir hypernyms (city) in their respective regions.2 ModelThe model we introduce is grounded in the distri-butional hypothesis (Harris, 1954), that two wordsare similar by appearing in the same kinds of con-texts (where ?context?
itself can be variously de-fined as the bag or sequence of tokens around a tar-get word, either by linear distance or dependencypath).
We can invoke the distributional hypothe-sis for many instances of regional variation by ob-serving that such variants often appear in similarcontexts.
For example:?
my boy?s wicked smart?
my boy?s hella smart?
my boy?s very smartHere, all three variants can often be seen in an im-mediately pre-adjectival position (as is commonwith intensifying adverbs).Given the empirical success of vector-space rep-resentations in capturing semantic properties andtheir success at a variety of NLP tasks (Turian etal., 2010; Socher et al, 2011; Collobert et al,2011; Socher et al, 2013), we use a simple, butstate-of-the-art neural architecture (Mikolov et al,2013) to learn low-dimensional real-valued repre-sentations of words.
The graphical form of thismodel is illustrated in figure 1.This model corresponds to an extension ofthe ?skip-gram?
language model (Mikolov et al,2013) (hereafter SGLM).
Given an input sentences and a context window of size t, each word siisconditioned on in turn to predict the identities ofall of the tokens within t words around it.
For avocabulary V , each input word siis representedas a one-hot vector wiof length |V |.
The SGLMhas two sets of parameters.
The first is the rep-resentation matrix W ?
R|V |?k, which encodesthe real-valued embeddings for each word in thevocabulary.
A matrix multiply h = w>W,?
Rkserves to index the particular embedding for wordw, which constitutes the model?s hidden layer.
Topredict the value of the context word y (again, aone-hot vector of dimensionality |V |), this hiddenrepresentation h is then multiplied by a second pa-rameter matrix X ?
R|V |?k.
The final predictionover the output vocabulary is then found by pass-ing this resulting vector through the softmax func-tion o = softmax(Xh), giving a vector in the |V |-dimensional unit simplex.
Backpropagation using(input x, output y) word tuples learns the valuesof W (the embeddings) and X (the output param-eter matrix) that maximize the likelihood of y (i.e.,the context words) conditioned on x (i.e., the si?s).During backpropagation, the errors propagated arethe difference between o (a probability distribu-tion with k outcomes) and the true (one-hot) out-put y.Let us define a set of contextual variablesC; in the experiments that follow, C is com-prised solely of geographical state Cstate={AK,AL, .
.
.
,WY}) but could in principle in-clude any number of features, such as calendar829month, day of week, or other demographic vari-ables of the speaker.
Let |C| denote the sum of thecardinalities of all variables in C (i.e., 51 states,including the District of Columbia).
Rather thanusing a single embedding matrix W that containslow-dimensional representations for every word inthe vocabulary, we define a global embedding ma-trix Wmain?
R|V |?kand an additional |C| suchmatrices (each again of size |V | ?
k, which cap-ture the effect that each variable value has on eachword in the vocabulary.
Given an input word wand set of active variable values A (e.g., A ={state = MA}), we calculate the hidden layerh as the sum of these independent embeddings:h = w>Wmain+?a?Aw>Wa.
While the wordwicked has a common low-dimensional represen-tation in Wmain,wickedthat is invoked for everyinstance of its use (regardless of the place), thecorresponding vector WMA,wickedindicates howthat common representation should shift in k-dimensional space when used in Massachusetts.Backpropagation functions as in standard SGLM,with gradient updates for each training example{x, y} touching not onlyWmain(as in SGLM), butall active WAas well.The additional W embeddings we add lead toan increase in the number of total parameters bya factor of |C|.
To control for the extra degreesof freedom this entails, we add squared `2regu-larization to all parameters, using stochastic gra-dient descent for backpropagation with minibatchupdates for the regularization term.
As in Mikolovet al (2013), we speed up computation using thehierarchical softmax (Morin and Bengio, 2005) onthe output matrix X .This model defines a joint parameterization overall variable values in the data, where informationfrom data originating in California, for instance,can influence the representations learned for Wis-consin; a naive alternative would be to simply trainindividual models on each variable value (a ?Cal-ifornia?
model using data only from California,etc.).
A joint model has three a priori advantagesover independent models: (i) sharing data acrossvariable values encourages representations acrossthose values to be similar; e.g., while city may becloser to Boston in Massachusetts and Chicago inIllinois, in both places it still generally connotesa municipality; (ii) such sharing can mitigate datasparseness for less-witnessed areas; and (iii) witha joint model, all representations are guaranteed tobe in the same vector space and can therefore becompared to each other; with individual models(each with different initializations), word vectorsacross different states may not be directly com-pared.3 EvaluationWe evaluate our model by confirming its facevalidity in a qualitative analysis and estimatingits accuracy at the quantitative task of judginggeographically-informed semantic similarity.
Weuse 1.1 billion tokens from 93 million geolocatedtweets gathered between September 1, 2011 andAugust 30, 2013 (approximately 127,000 tweetsper day evenly sampled over those two years).This data only includes tweets that have been ge-olocated to state-level granularity in the UnitedStates using high-precision pattern matching onthe user-specified location field (e.g., ?new yorkny?
?
NY, ?chicago?
?
IL, etc.).
As a pre-processing step, we identify a set of target mul-tiword expressions in this corpus as the maximalsequence of adjectives + nouns with the highestpointwise mutual information; in all experimentsdescribed below, we define the vocabulary V asthe most frequent 100,000 terms (either unigramsor multiword expressions) in the total data, and setthe dimensionality of the embedding k = 100.
Inall experiments, the contextual variable is the ob-served US state (including DC), so that |C| = 51;the vector space representation of word w in states is w>Wmain+ w>Ws.3.1 Qualitative EvaluationTo illustrate how the model described above canlearn geographically-informed semantic represen-tations of words, table 1 displays the terms withthe highest cosine similarity to wicked in Kansasand Massachusetts after running our joint modelon the full 1.1 billion words of Twitter data; whilewicked in Kansas is close to other evaluative termslike evil and pure and religious terms like gods andspirit, in Massachusetts it is most similar to otherintensifiers like super, ridiculously and insanely.Table 2 likewise presents the terms with thehighest cosine similarity to city in both Califor-nia and New York; while the terms most evokedby city in California include regional locationslike Chinatown, Los Angeles?
South Bay and SanFrancisco?s East Bay, in New York the most sim-ilar terms include hamptons, upstate and borough830Kansas Massachusettsterm cosine term cosinewicked 1.000 wicked 1.000evil 0.884 super 0.855pure 0.841 ridiculously 0.851gods 0.841 insanely 0.820mystery 0.830 extremely 0.793spirit 0.830 goddamn 0.781king 0.828 surprisingly 0.774above 0.825 kinda 0.772righteous 0.823 #sarcasm 0.772magic 0.822 sooooooo 0.770Table 1: Terms with the highest cosine similarity to wickedin Kansas and Massachusetts.California New Yorkterm cosine term cosinecity 1.000 city 1.000valley 0.880 suburbs 0.866bay 0.874 town 0.855downtown 0.873 hamptons 0.852chinatown 0.854 big city 0.842south bay 0.854 borough 0.837area 0.851 neighborhood 0.835east bay 0.845 downtown 0.827neighborhood 0.843 upstate 0.826peninsula 0.840 big apple 0.825Table 2: Terms with the highest cosine similarity to city inCalifornia and New York.
(New York City?s term of administrative division).3.2 Quantitative EvaluationAs a quantitative measure of our model?s perfor-mance, we consider the task of judging semanticsimilarity among words whose meanings are likelyto evoke strong geographical correlations.
In theabsence of a sizable number of linguistically in-teresting terms (like wicked) that are known to begeographically variable, we consider the proxy ofestimating the named entities evoked by specificterms in different geographical regions.
As notedabove, geographic terms like city provide one suchexample: in Massachusetts we expect the term cityto be more strongly connected to grounded namedentities like Boston than to other US cities.
Weconsider seven categories for which we can rea-sonably expect the connotations of each term tovary by geography; in each case, we calculate thedistance between two terms x and y using repre-sentations learned for a given state (?state(x, y)).1. city.
For each state, we measure the distancebetween the word city and the state?s mostpopulous city; e.g., ?AZ(city , phoenix ).2. state.
For each state, the distance betweenthe word state and the state?s name; e.g.,?WI(state,wisconsin).3.
football.
For all NFL teams, the distance be-tween the word football and the team name;e.g., ?IL(football , bears).4. basketball.
For all NBA teams froma US state, the distance between theword basketball and the team name; e.g.,?FL(basketball , heat).5. baseball.
For all MLB teams from a USstate, the distance between the word baseballand the team name; e.g., ?IL(baseball , cubs),?IL(baseball ,white sox ).6. hockey.
For all NHL teams from a US state,the distance between the word hockey and theteam name; e.g., ?PA(hockey , penguins).7. park.
For all US national parks, the distancebetween the word park and the park name;e.g., ?AK(park , denali).Each of these questions asks the following:what words are evoked for a given target word(like football)?
While football may everywhereevoke similar sports like baseball or soccer ormore specific football-related terms like touch-down or field goal, we expect that particular sportsteams will be evoked more strongly by the wordfootball in their particular geographical region: inWisconsin, football should evoke packers, whilein Pennsylvania, football evokes steelers.
Notethat this is not the same as simply asking whichsports team is most frequently (or most character-istically) mentioned in a given area; by measuringthe distance to a target word (football), we are at-tempting to estimate the varying strengths of asso-ciation between concepts in different regions.For each category, we measure similarity as theaverage cosine similarity between the vector forthe target word for that category (e.g., city) and thecorresponding vector for each state-specific an-swer (e.g., chicago for IL; boston for MA).
Wecompare three different models:1.
JOINT.
The full model described in section2, in which we learn a global representationfor each word along with deviations from thatcommon representation for each state.2.
INDIVIDUAL.
For comparison, we also parti-tion the data among all 51 states, and train asingle model for each state using only datafrom that state.
In this model, there is nosharing among states; California has the most8310.000.250.500.75city state baseball basketball football hockey parksimilarityModelJointIndividual?GeoFigure 2: Average cosine similarity for all models across all categories, with 95% confidence intervals on the mean.data with 11,604,637 tweets; Wyoming hasthe least with 47,503 tweets.3.
?GEO.
We also train a single model on all ofthe training data, but ignore any state meta-data.
In this case the distance ?
between twoterms is their overall distance within the en-tire United States.As one concrete example of these differencesbetween individual data points, the cosine similar-ity between city and seattle in the ?GEO modelis 0.728 (seattle is ranked as the 188th most sim-ilar term to city overall); in the INDIVIDUALmodel using only tweets from Washington state,?WA(city, seattle) = 0.780 (rank #32); and inthe JOINT model, using information from the en-tire United States with deviations for Washington,?WA(city, seattle) = 0.858 (rank #6).
The over-all similarity for the city category of each model isthe average of 51 such tests (one for each city).Figure 2 present the results of the full evalua-tion, including 95% confidence intervals for eachmean.
While the two models that include ge-ographical information naturally outperform themodel that does not, the JOINT model generallyfar outperforms the INDIVIDUAL models trainedon state-specific subsets of the data.1A model thatcan exploit all of the information in the data, learn-ing core vector-space representations for all wordsalong with deviations for each contextual variable,is able to learn more geographically-informed rep-resentations for this task than strict geographicalmodels alone.1This result is robust to the choice of distance metric; anevaluation measuring the Euclidean distance between vectorsshows the JOINT model to outperform the INDIVIDUAL and?GEO models across all seven categories.4 ConclusionWe introduced a model for leveraging situationalinformation in learning vector-space representa-tions of words that are sensitive to the speaker?ssocial context.
While our results use geographicalinformation in learning low-dimensional represen-tations, other contextual variables are straightfor-ward to include as well; incorporating effects fortime ?
such as time of day, month of year and ab-solute year ?
may be a powerful tool for reveal-ing periodic and historical influences on lexical se-mantics.Our approach explores the degree to which ge-ography, and other contextual factors, influenceword meaning in addition to frequency of usage.By allowing all words in different regions (or moregenerally, with different metadata factors) to ex-ist in the same vector space, we are able com-pare different points in that space ?
for example,to ask what terms used in Chicago are most simi-lar to hot dog in New York, or what word groupsshift together in the same region in comparisonto the background (indicating the shift of an en-tire semantic field).
All datasets and software tosupport these geographically-informed represen-tations can be found at: http://www.ark.cs.cmu.edu/geoSGLM.5 AcknowledgmentsThe research reported in this article was supportedby US NSF grants IIS-1251131 and CAREER IIS-1054319, and by an ARCS scholarship to D.B.This work was made possible through the use ofcomputing resources made available by the OpenCloud Consortium, Yahoo and the Pittsburgh Su-percomputing Center.832ReferencesMark Andrews, Gabriella Vigliocco, and David Vin-son.
2009.
Integrating experiential and distribu-tional data to learn semantic representations.
Psy-chological Review, 116(3):463?498.David Bamman, Jacob Eisenstein, and Tyler Schnoe-belen.
2014.
Gender identity and lexical variationin social media.
Journal of Sociolinguistics, 18(2).Elia Bruni, Giang Binh Tran, and Marco Baroni.
2011.Distributional semantics from text and images.
InProc.
of the Workshop on Geometrical Models ofNatural Language Semantics.Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran.
2012a.
Distributional semantics intechnicolor.
In Proc.
of ACL.Elia Bruni, Jasper Uijlings, Marco Baroni, and NicuSebe.
2012b.
Distributional semantics with eyes:Using image analysis to improve computational rep-resentations of word meaning.
In Proc.
of the ACMInternational Conference on Multimedia.Mary Bucholtz.
2006.
Word up: Social meanings ofslang in California youth culture.
In Jane Goodmanand Leila Monaghan, editors, A Cultural Approachto Interpersonal Communication: Essential Read-ings, Malden, MA.
Blackwell.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Gabriel Doyle.
2014.
Mapping dialectal variation byquerying social media.
In Proc.
of EACL.Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2010.
A latent variable model forgeographic lexical variation.
In Proc.
of EMNLP.Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.2011.
Discovering sociolinguistic associations withstructured sparsity.
In Proc.
of ACL.Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2012.
Mapping the geographicaldiffusion of new words.
arXiv, abs/1210.5268.Yansong Feng and Mirella Lapata.
2010.
Visual in-formation in semantic representation.
In Proc.
ofNAACL.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Liangjie Hong, Amr Ahmed, Siva Gurumurthy,Alexander J. Smola, and Kostas Tsioutsiouliklis.2012.
Discovering geographical topics in the Twit-ter stream.
In Proc.
of WWW.Yohei Ikawa, Miki Enoki, and Michiaki Tatsubori.2012.
Location inference using microblog mes-sages.
In Proc.
of WWW.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proc.
of COLING-ACL.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In Proc.
of ICLR.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InRobert G. Cowell and Zoubin Ghahramani, editors,Proc.
of AISTATS.Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, andNoah A. Smith.
2010.
Discovering demographiclanguage variation.
In NIPS Workshop on MachineLearning and Social Computing.Sharon Oviatt.
2003.
Multimodal interfaces.In Julie A. Jacko and Andrew Sears, editors,The Human-computer Interaction Handbook, pages286?304, Hillsdale, NJ, USA.
L. Erlbaum Asso-ciates Inc.Marco Pennacchiotti and Ana-Maria Popescu.
2011.Democrats, Republicans and Starbucks afficionados:User classification in Twitter.
In Proc.
of KDD.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in Twitter.
In Proc.
of the Workshop onSearch and Mining User-generated Contents.Maya Ravindranath.
2011.
A wicked good reason tostudy intensifiers in New Hampshire.
In NWAV 40.Joseph Reisinger and Raymond J. Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In Proc.
of NAACL.Stephen Roller and Sabine Schulte im Walde.
2013.
Amultimodal LDA model integrating textual, cogni-tive and visual modalities.
In Proc.
of EMNLP.Stephen Roller, Michael Speriosu, Sarat Rallapalli,Benjamin Wing, and Jason Baldridge.
2012.
Super-vised text-based geolocation using language modelson an adaptive grid.
In Proc.
of EMNLP-CoNLL.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proc.
of EMNLP.Richard Socher, John Bauer, Christopher D. Manning,and Ng Andrew Y.
2013.
Parsing with composi-tional vector grammars.
In Proc.
of ACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proc.
of ACL.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188, January.833Benjamin P. Wing and Jason Baldridge.
2011.
Sim-ple supervised document geolocation with geodesicgrids.
In Proc.
of ACL.Chen Yu and Dana H. Ballard.
2004.
A multimodallearning interface for grounding spoken language insensory perceptions.
ACM Transactions on AppliedPerception, 1(1):57?80.834
