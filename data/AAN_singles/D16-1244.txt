Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2249?2255,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA Decomposable Attention Model for Natural Language InferenceAnkur P. ParikhGoogleNew York, NYOscar Ta?ckstro?mGoogleNew York, NYDipanjan DasGoogleNew York, NYJakob UszkoreitGoogleMountain View, CA{aparikh,oscart,dipanjand,uszkoreit}@google.comAbstractWe propose a simple neural architecture for nat-ural language inference.
Our approach uses at-tention to decompose the problem into subprob-lems that can be solved separately, thus makingit trivially parallelizable.
On the Stanford Natu-ral Language Inference (SNLI) dataset, we ob-tain state-of-the-art results with almost an orderof magnitude fewer parameters than previouswork and without relying on any word-order in-formation.
Adding intra-sentence attention thattakes a minimum amount of order into accountyields further improvements.1 IntroductionNatural language inference (NLI) refers to the prob-lem of determining entailment and contradiction re-lationships between a premise and a hypothesis.
NLIis a central problem in language understanding (Katz,1972; Bos and Markert, 2005; van Benthem, 2008;MacCartney and Manning, 2009) and recently thelarge SNLI corpus of 570K sentence pairs was cre-ated for this task (Bowman et al, 2015).
We presenta new model for NLI and leverage this corpus forcomparison with prior work.A large body of work based on neural networksfor text similarity tasks including NLI has been pub-lished in recent years (Hu et al, 2014; Rockta?schelet al, 2016; Wang and Jiang, 2016; Yin et al, 2016,inter alia).
The dominating trend in these models isto build complex, deep text representation models,for example, with convolutional networks (LeCun etal., 1990, CNNs henceforth) or long short-term mem-ory networks (Hochreiter and Schmidhuber, 1997,LSTMs henceforth) with the goal of deeper sen-tence comprehension.
While these approaches haveyielded impressive results, they are often computa-tionally very expensive, and result in models havingmillions of parameters (excluding embeddings).Here, we take a different approach, arguing thatfor natural language inference it can often suffice tosimply align bits of local text substructure and thenaggregate this information.
For example, considerthe following sentences:?
Bob is in his room, but because of the thunderand lightning outside, he cannot sleep.?
Bob is awake.?
It is sunny outside.The first sentence is complex in structure and itis challenging to construct a compact representationthat expresses its entire meaning.
However, it is fairlyeasy to conclude that the second sentence followsfrom the first one, by simply aligning Bob with Boband cannot sleep with awake and recognizing thatthese are synonyms.
Similarly, one can concludethat It is sunny outside contradicts the first sentence,by aligning thunder and lightning with sunny andrecognizing that these are most likely incompatible.We leverage this intuition to build a simpler andmore lightweight approach to NLI within a neuralframework; with considerably fewer parameters, ourmodel outperforms more complex existing neural ar-chitectures.
In contrast to existing approaches, ourapproach only relies on alignment and is fully com-putationally decomposable with respect to the inputtext.
An overview of our approach is given in Fig-ure 1.
Given two sentences, where each word is repre-2249sented by an embedding vector, we first create a softalignment matrix using neural attention (Bahdanauet al, 2015).
We then use the (soft) alignment todecompose the task into subproblems that are solvedseparately.
Finally, the results of these subproblemsare merged to produce the final classification.
In ad-dition, we optionally apply intra-sentence attention(Cheng et al, 2016) to endow the model with a richerencoding of substructures prior to the alignment step.Asymptotically our approach does the same totalwork as a vanilla LSTM encoder, while being triv-ially parallelizable across sentence length, which canallow for considerable speedups in low-latency set-tings.
Empirical results on the SNLI corpus show thatour approach achieves state-of-the-art results, whileusing almost an order of magnitude fewer parameterscompared to complex LSTM-based approaches.2 Related WorkOur method is motivated by the central role played byalignment in machine translation (Koehn, 2009) andprevious approaches to sentence similarity modeling(Haghighi et al, 2005; Das and Smith, 2009; Changet al, 2010; Fader et al, 2013), natural languageinference (Marsi and Krahmer, 2005; MacCartneyet al, 2006; Hickl and Bensley, 2007; MacCartneyet al, 2008), and semantic parsing (Andreas et al,2013).
The neural counterpart to alignment, atten-tion (Bahdanau et al, 2015), which is a key partof our approach, was originally proposed and hasbeen predominantly used in conjunction with LSTMs(Rockta?schel et al, 2016; Wang and Jiang, 2016) andto a lesser extent with CNNs (Yin et al, 2016).
Incontrast, our use of attention is purely based on wordembeddings and our method essentially consists offeed-forward networks that operate largely indepen-dently of word order.3 ApproachLet a = (a1, .
.
.
, a`a) and b = (b1, .
.
.
, b`b) bethe two input sentences of length `a and `b, re-spectively.
We assume that each ai, bj ?
Rdis a word embedding vector of dimension d andthat each sentence is prepended with a ?NULL?token.
Our training data comes in the form oflabeled pairs {a(n),b(n),y(n)}Nn=1, where y(n) =(y(n)1 , .
.
.
, y(n)C ) is an indicator vector encoding thelabel and C is the number of output classes.
At testH (                 )+ +?+=y?intheparkaliceplayssomeoneplayingmusic outsidefluteasolo G (    ,    )G (    ,    )park outsidealice someoneflute+?solo music?G (    ,    )===flute musicF (    ,    )Figure 1: Pictoral overview of the approach, showing the Attend(left), Compare (center) and Aggregate (right) steps.time, we receive a pair of sentences (a,b) and ourgoal is to predict the correct label y.Input representation.
Let a?
= (a?1, .
.
.
, a?`a) andb?
= (b?1, .
.
.
, b?`b) denote the input representation ofeach fragment that is fed to subsequent steps of thealgorithm.
The vanilla version of our model simplydefines a?
:= a and b?
:= b.
With this input rep-resentation, our model does not make use of wordorder.
However, we discuss an extension using intra-sentence attention in Section 3.4 that uses a minimalamount of sequence information.The core model consists of the following threecomponents (see Figure 1), which are trained jointly:Attend.
First, soft-align the elements of a?
and b?using a variant of neural attention (Bahdanau et al,2015) and decompose the problem into the compari-son of aligned subphrases.Compare.
Second, separately compare eachaligned subphrase to produce a set of vectors{v1,i}`ai=1 for a and {v2,j}`bj=1 for b.
Each v1,i isa nonlinear combination of ai and its (softly) alignedsubphrase in b (and analogously for v2,j).Aggregate.
Finally, aggregate the sets {v1,i}`ai=1and {v2,j}`bj=1 from the previous step and use theresult to predict the label y?.3.1 AttendWe first obtain unnormalized attention weights eij ,computed by a function F ?, which decomposes as:eij := F ?
(a?i, b?j) := F (a?i)TF (b?j) .
(1)This decomposition avoids the quadratic complexitythat would be associated with separately applying F ?`a ?
`b times.
Instead, only `a + `b applications ofF are needed.
We take F to be a feed-forward neuralnetwork with ReLU activations (Glorot et al, 2011).2250These attention weights are normalized as follows:?i :=`b?j=1exp(eij)?`bk=1 exp(eik)b?j ,?j :=`a?i=1exp(eij)?`ak=1 exp(ekj)a?i .
(2)Here ?i is the subphrase in b?
that is (softly) alignedto a?i and vice versa for ?j .3.2 CompareNext, we separately compare the aligned phrases{(a?i, ?i)}`ai=1 and {(b?j , ?j)}`bj=1 using a function G,which in this work is again a feed-forward network:v1,i := G([a?i, ?i]) ?i ?
[1, .
.
.
, `a] ,v2,j := G([b?j , ?j ]) ?j ?
[1, .
.
.
, `b] .
(3)where the brackets [?, ?]
denote concatenation.
Notethat since there are only a linear number of terms inthis case, we do not need to apply a decompositionas was done in the previous step.
Thus G can jointlytake into account both a?i, and ?i.3.3 AggregateWe now have two sets of comparison vectors{v1,i}`ai=1 and {v2,j}`bj=1.
We first aggregate overeach set by summation:v1 =`a?i=1v1,i , v2 =`b?j=1v2,j .
(4)and feed the result through a final classifier H , thatis a feed forward network followed by a linear layer:y?
= H([v1,v2]) , (5)where y?
?
RC represents the predicted (unnormal-ized) scores for each class and consequently the pre-dicted class is given by y?
= argmaxiy?i.For training, we use multi-class cross-entropy losswith dropout regularization (Srivastava et al, 2014):L(?F , ?G, ?H) =1NN?n=1C?c=1y(n)c logexp(y?c)?Cc?=1 exp(y?c?
).Here ?F , ?G, ?H denote the learnable parameters ofthe functions F, G and H, respectively.3.4 Intra-Sentence Attention (Optional)In the above model, the input representations aresimple word embeddings.
However, we can augmentthis input representation with intra-sentence attentionto encode compositional relationships between wordswithin each sentence, as proposed by Cheng et al(2016).
Similar to Eqs.
1 and 2, we definefij := Fintra(ai)TFintra(aj) , (6)where Fintra is a feed-forward network.
We then cre-ate the self-aligned phrasesa?i :=`a?j=1exp(fij + di?j)?`ak=1 exp(fik + di?k)aj .
(7)The distance-sensitive bias terms di?j ?
R providesthe model with a minimal amount of sequence infor-mation, while remaining parallelizable.
These termsare bucketed such that all distances greater than 10words share the same bias.
The input representationfor subsequent steps is then defined as a?i := [ai, a?i]and analogously b?i := [bi, b?i].4 Computational ComplexityWe now discuss the asymptotic complexity of ourapproach and how it offers a higher degree of par-allelism than LSTM-based approaches.
Recall thatd denotes embedding dimension and ` means sen-tence length.
For simplicity we assume that all hid-den dimensions are d and that the complexity ofmatrix(d?
d)-vector(d?
1) multiplication is O(d2).A key assumption of our analysis is that ` < d,which we believe is reasonable and is true of theSNLI dataset (Bowman et al, 2015) where ` < 80,whereas recent LSTM-based approaches have usedd ?
300.
This assumption allows us to bound thecomplexity of computing the `2 attention weights.Complexity of LSTMs.
The complexity of anLSTM cell is O(d2), resulting in a complexity ofO(`d2) to encode the sentence.
Adding attention asin Rockta?schel et al (2016) increases this complexityto O(`d2 + `2d).Complexity of our Approach.
Application of afeed-forward network requires O(d2) steps.
Thus,the Compare and Aggregate steps have complexityO(`d2) and O(d2) respectively.
For the Attend step,2251Method Train Acc Test Acc #ParametersLexicalized Classifier (Bowman et al, 2015) 99.7 78.2 ?300D LSTM RNN encoders (Bowman et al, 2016) 83.9 80.6 3.0M1024D pretrained GRU encoders (Vendrov et al, 2015) 98.8 81.4 15.0M300D tree-based CNN encoders (Mou et al, 2015) 83.3 82.1 3.5M300D SPINN-PI encoders (Bowman et al, 2016) 89.2 83.2 3.7M100D LSTM with attention (Rockta?schel et al, 2016) 85.3 83.5 252K300D mLSTM (Wang and Jiang, 2016) 92.0 86.1 1.9M450D LSTMN with deep attention fusion (Cheng et al, 2016) 88.5 86.3 3.4MOur approach (vanilla) 89.5 86.3 382KOur approach with intra-sentence attention 90.5 86.8 582KTable 1: Train/test accuracies on the SNLI dataset and number of parameters (excluding embeddings) for each approach.Method N E CBowman et al (2016) 80.6 88.2 85.5Wang and Jiang (2016) 81.6 91.6 87.4Our approach (vanilla) 83.6 91.3 85.8Our approach w/ intra att.
83.7 92.1 86.7Table 2: Breakdown of accuracy with respect to classes on SNLIdevelopment set.
N=neutral, E=entailment, C=contradiction.F is evaluated O(`) times, giving a complexity ofO(`d2).
Each attention weight eij requires one dotproduct, resulting in a complexity of O(`2d).Thus the total complexity of the model is O(`d2 +`2d), which is equal to that of an LSTM with atten-tion.
However, note that with the assumption that` < d, this becomes O(`d2) which is the same com-plexity as a regular LSTM.
Moreover, unlike theLSTM, our approach has the advantage of being par-allelizable over `, which can be useful at test time.5 ExperimentsWe evaluate our approach on the Stanford NaturalLanguage Inference (SNLI) dataset (Bowman et al,2015).
Given a sentences pair (a,b), the task is topredict whether b is entailed by a, b contradicts a,or whether their relationship is neutral.5.1 Implementation DetailsThe method was implemented in TensorFlow (Abadiet al, 2015).Data preprocessing: Following Bowman et al(2015), we remove examples labeled ???
(no goldlabel) from the dataset, which leaves 549,367 pairsfor training, 9,842 for development, and 9,824 fortesting.
We use the tokenized sentences from thenon-binary parse provided in the dataset and prependeach sentence with a ?NULL?
token.
During training,each sentence was padded up to the maximum lengthof the batch for efficient training (the padding wasexplicitly masked out so as not to affect the objec-tive/gradients).
For efficient batching in TensorFlow,we semi-sorted the training data to first contain ex-amples where both sentences had length less than20, followed by those with length less than 50, andthen the rest.
This ensured that most training batchescontained examples of similar length.Embeddings: We use 300 dimensional GloVeembeddings (Pennington et al, 2014) to representwords.
Each embedding vector was normalized tohave `2 norm of 1 and projected down to 200 di-mensions, a number determined via hyperparametertuning.
Out-of-vocabulary (OOV) words are hashedto one of 100 random embeddings each initializedto mean 0 and standard deviation 1.
All embeddingsremain fixed during training, but the projection ma-trix is trained.
All other parameter weights (hiddenlayers etc.)
were initialized from random Gaussianswith mean 0 and standard deviation 0.01.Each hyperparameter setting was run on a sin-gle machine with 10 asynchronous gradient-updatethreads, using Adagrad (Duchi et al, 2011) for opti-mization with the default initial accumulator value of0.1.
Dropout regularization (Srivastava et al, 2014)was used for all ReLU layers, but not for the finallinear layer.
We additionally tuned the followinghyperparameters and present their chosen values in2252ID Sentence 1 Sentence 2 DA (vanilla) DA (intra att.)
SPINN-PI mLSTM GoldA Two kids are standing in the ocean huggingeach other.
Two kids enjoy their day at the beach.
N N E E NB A dancer in costumer performs on stage whilea man watches.
the man is captivated N N E E NC They are sitting on the edge of a fountain The fountain is splashing the persons seated.
N N C C ND Two dogs play with tennis ball in field.
Dogs are watching a tennis match.
N C C C CE Two kids begin to make a snowman on a sunnywinter day.
Two penguins making a snowman.
N C C C CF The horses pull the carriage, holding peopleand a dog, through the rain.
Horses ride in a carriage pulled by a dog.
E E C C CG A woman closes her eyes as she plays hercello.
The woman has her eyes open.
E E E E CH Two women having drinks and smokingcigarettes at the bar.
Three women are at a bar.
E E E E CI A band playing with fans watching.
A band watches the fans play E E E E CTable 3: Example wins and losses compared to other approaches.
DA (Decomposable Attention) refers to our approach whileSPINN-PI and mLSTM are previously developed methods (see Table 1).parentheses: network size (2-layers, each with 200neurons), batch size (4), 1 dropout ratio (0.2) andlearning rate (0.05?vanilla, 0.025?intra-attention).All settings were run for 50 million steps (each stepindicates one batch) but model parameters were savedfrequently as training progressed and we chose themodel that did best on the development set.5.2 ResultsResults in terms of 3-class accuracy are shown inTable 1.
Our vanilla approach achieves state-of-the-art results with almost an order of magnitude fewerparameters than the LSTMN of Cheng et al (2016).Adding intra-sentence attention gives a considerableimprovement of 0.5 percentage points over the ex-isting state of the art.
Table 2 gives a breakdown ofaccuracy on the development set showing that mostof our gains stem from neutral, while most lossescome from contradiction pairs.Table 3 shows some wins and losses.
Examples A-C are cases where both variants of our approach arecorrect while both SPINN-PI (Bowman et al, 2016)and the mLSTM (Wang and Jiang, 2016) are incor-rect.
In the first two cases, both sentences containphrases that are either identical or highly lexicallyrelated (e.g.
?Two kids?
and ?ocean / beach?)
and ourapproach correctly favors neutral in these cases.
InExample C, it is possible that relying on word-ordermay confuse SPINN-PI and the mLSTM due to how?fountain?
is the object of a preposition in the firstsentence but the subject of the second.The second set of examples (D-F) are cases where116 or 32 also work well and are a bit more stable.our vanilla approach is incorrect but mLSTM andSPINN-PI are correct.
Example F requires sequen-tial information and neither variant of our approachcan predict the correct class.
Examples D-E are in-teresting however, since they don?t require word or-der information, yet intra-attention seems to help.We suspect this may be because the word embed-dings are not fine-grained enough for the algorithmto conclude that ?play/watch?
is a contradiction, butintra-attention, by adding an extra layer of composi-tion/nonlinearity to incorporate context, compensatesfor this.Finally, Examples G-I are cases that all methodsget wrong.
The first is actually representative of manyexamples in this category where there is one criticalword that separates the two sentences (close vs openin this case) and goes unnoticed by the algorithms.Examples H requires inference about numbers andExample I needs sequence information.6 ConclusionWe presented a simple attention-based approach tonatural language inference that is trivially paralleliz-able.
The approach outperforms considerably morecomplex neural methods aiming for text understand-ing.
Our results suggest that, at least for this task,pairwise comparisons are relatively more importantthan global sentence-level representations.AcknowledgementsWe thank Slav Petrov, Tom Kwiatkowski, Yoon Kim,Erick Fonseca, Mark Neumann for useful discussionand Sam Bowman and Shuohang Wang for providingus their model outputs for error analysis.2253References[Abadi et al2015] Mart?
?n Abadi, Ashish Agarwal, PaulBarham, Eugene Brevdo, Zhifeng Chen, Craig Citro,Greg S Corrado, Andy Davis, Jeffrey Dean, MatthieuDevin, et al 2015.
TensorFlow: Large-scale machinelearning on heterogeneous systems.
Software availablefrom tensorflow.
org.
[Andreas et al2013] Jacob Andreas, Andreas Vlachos,and Stephen Clark.
2013.
Semantic parsing as ma-chine translation.
In Proceedings of ACL.
[Bahdanau et al2015] Dzmitry Bahdanau, KyunghyunCho, and Yoshua Bengio.
2015.
Neural machine trans-lation by jointly learning to align and translate.
InProceedings of ICLR.
[Bos and Markert2005] Johan Bos and Katja Markert.2005.
Recognising textual entailment with logical in-ference.
In Proceedings of EMNLP.
[Bowman et al2015] Samuel R. Bowman, Gabor Angeli,Christopher Potts, and Christopher D. Manning.
2015.A large annotated corpus for learning natural languageinference.
In Proceedings of EMNLP.
[Bowman et al2016] Samuel R. Bowman, Jon Gauthier,Abhinav Rastogi, Raghav Gupta, Christopher D. Man-ning, and Christopher Potts.
2016.
A fast unified modelfor parsing and sentence understanding.
In Proceedingsof ACL.
[Chang et al2010] Ming-Wei Chang, Dan Goldwasser,Dan Roth, and Vivek Srikumar.
2010.
Discrimina-tive learning over constrained latent representations.
InProceedings of HLT-NAACL.
[Cheng et al2016] Jianpeng Cheng, Li Dong, and MirellaLapata.
2016.
Long short-term memory-networks formachine reading.
In Proceedings of EMNLP.
[Das and Smith2009] Dipanjan Das and Noah A. Smith.2009.
Paraphrase identification as probabilistic quasi-synchronous recognition.
In Proceedings of ACL-IJCNLP.
[Duchi et al2011] John Duchi, Elad Hazan, and YoramSinger.
2011.
Adaptive subgradient methods for onlinelearning and stochastic optimization.
The Journal ofMachine Learning Research, 12:2121?2159.
[Fader et al2013] Anthony Fader, Luke S Zettlemoyer,and Oren Etzioni.
2013.
Paraphrase-driven learningfor open question answering.
In Proceedings of ACL.
[Glorot et al2011] Xavier Glorot, Antoine Bordes, andYoshua Bengio.
2011.
Deep sparse rectifier neuralnetworks.
In Proceedings of AISTATS.
[Haghighi et al2005] Aria D. Haghighi, Andrew Y. Ng,and Christopher D. Manning.
2005.
Robust textualinference via graph matching.
In Proceedings of HLT-NAACL.
[Hickl and Bensley2007] Andrew Hickl and Jeremy Bens-ley.
2007.
A discourse commitment-based frameworkfor recognizing textual entailment.
In Proceedings ofthe ACL-PASCAL Workshop on Textual Entailment andParaphrasing.
Association for Computational Linguis-tics.
[Hochreiter and Schmidhuber1997] Sepp Hochreiter andJu?rgen Schmidhuber.
1997.
Long short-term memory.Neural computation, 9(8):1735?1780.
[Hu et al2014] Baotian Hu, Zhengdong Lu, Hang Li, andQingcai Chen.
2014.
Convolutional neural networkarchitectures for matching natural language sentences.In Advances in NIPS.
[Katz1972] Jerrold J. Katz.
1972.
Semantic theory.Harper & Row.
[Koehn2009] Philipp Koehn.
2009.
Statistical machinetranslation.
Cambridge University Press.
[LeCun et al1990] Y. LeCun, B. Boser, J.S.
Denker,D.
Henderson, R.E.
Howard, W. Hubbard, and L.D.Jackel.
1990.
Handwritten digit recognition with aback-propagation network.
In Advances in NIPS.
[MacCartney and Manning2009] Bill MacCartney andChristopher D. Manning.
2009.
An extended model ofnatural logic.
In Proceedings of the IWCS.
[MacCartney et al2006] Bill MacCartney, TrondGrenager, Marie-Catherine de Marneffe, Daniel Cer,and Christopher D Manning.
2006.
Learning torecognize features of valid textual entailments.
InProceedings of HLT-NAACL.
[MacCartney et al2008] Bill MacCartney, Michel Galley,and Christopher D Manning.
2008.
A phrase-basedalignment model for natural language inference.
InProceedings of EMNLP.
[Marsi and Krahmer2005] Erwin Marsi and Emiel Krah-mer.
2005.
Classification of semantic relations byhumans and machines.
In Proceedings of the ACLworkshop on Empirical Modeling of Semantic Equiva-lence and Entailment.
[Mou et al2015] Lili Mou, Men Rui, Ge Li, Yan Xu,Lu Zhang, Rui Yan, and Zhi Jin.
2015.
Natural lan-guage inference by tree-based convolution and heuristicmatching.
In Proceedings of ACL (short papers).
[Pennington et al2014] Jeffrey Pennington, RichardSocher, and Christopher D. Manning.
2014.
GloVe:Global vectors for word representation.
In Proceedingsof EMNLP.
[Rockta?schel et al2016] Tim Rockta?schel, EdwardGrefenstette, Karl Moritz Hermann, Toma?s?
Koc?isky`,and Phil Blunsom.
2016.
Reasoning about entailmentwith neural attention.
In Proceedings of ICLR.
[Srivastava et al2014] Nitish Srivastava, Geoffrey Hinton,Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-dinov.
2014.
Dropout: A simple way to prevent neuralnetworks from overfitting.
The Journal of MachineLearning Research, 15(1):1929?1958.2254[van Benthem2008] Johan van Benthem.
2008.
A briefhistory of natural logic.
College Publications.
[Vendrov et al2015] Ivan Vendrov, Ryan Kiros, Sanja Fi-dler, and Raquel Urtasun.
2015.
Order-embeddings ofimages and language.
In Proceedings of ICLR.
[Wang and Jiang2016] Shuohang Wang and Jing Jiang.2016.
Learning natural language inference with LSTM.In Proceedings of NAACL.
[Yin et al2016] Wenpeng Yin, Hinrich Schu?tze, Bing Xi-ang, and Bowen Zhou.
2016.
ABCNN: Attention-based convolutional neural network for modeling sen-tence pairs.
In Transactions of the Association of Com-putational Linguistics.2255
