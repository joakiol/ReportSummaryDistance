Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1314?1324,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsLeveraging lexical cohesion and disruption for topic segmentationAnca S?imonUniversite?
de Rennes 1IRISA & INRIA RennesGuillaume GravierCNRSIRISA & INRIA Rennesanca-roxana.simon@irisa.frguillaume.gravier@irisa.frpascale.sebillot@irisa.frPascale Se?billotINSA de RennesIRISA & INRIA RennesAbstractTopic segmentation classically relies on oneof two criteria, either finding areas with co-herent vocabulary use or detecting discontinu-ities.
In this paper, we propose a segmenta-tion criterion combining both lexical cohesionand disruption, enabling a trade-off betweenthe two.
We provide the mathematical formu-lation of the criterion and an efficient graphbased decoding algorithm for topic segmenta-tion.
Experimental results on standard textualdata sets and on a more challenging corpusof automatically transcribed broadcast newsshows demonstrate the benefit of such a com-bination.
Gains were observed in all condi-tions, with segments of either regular or vary-ing length and abrupt or smooth topic shifts.Long segments benefit more than short seg-ments.
However the algorithm has proven ro-bust on automatic transcripts with short seg-ments and limited vocabulary reoccurrences.1 IntroductionTopic segmentation consists in evidentiating the se-mantic structure of a document: Algorithms devel-oped for this task aim at automatically detectingfrontiers which define topically coherent segmentsin a text.Various methods for topic segmentation of tex-tual data are described in the literature, e.g., (Rey-nar, 1994; Hearst, 1997; Ferret et al 1998; Choi,2000; Moens and Busser, 2001; Utiyama and Isa-hara, 2001), most of them relying on the notion oflexical cohesion, i.e., identifying segments with aconsistent use of vocabulary, either based on wordsor on semantic relations between words.
Reoccur-rences of words or related words and lexical chainsare two popular methods to evidence lexical cohe-sion.
This general principle of lexical cohesion isfurther exploited for topic segmentation with tworadically different strategies.
On the one hand, ameasure of the lexical cohesion can be used to deter-mine coherent segments (Reynar, 1994; Moens andBusser, 2001; Utiyama and Isahara, 2001).
On theother hand, shifts in the use of vocabulary can besearched for to directly identify the segment fron-tiers by measuring the lexical disruption (Hearst,1997).Techniques based on the first strategy yield moreaccurate segmentation results, but face a problem ofover-segmentation which can, up to now, only besolved by providing prior information regarding thedistribution of segment length or the expected num-ber of segments.
In this paper, we propose a segmen-tation criterion combining both cohesion and dis-ruption along with the corresponding algorithm fortopic segmentation.
Such a criterion ensures a co-herent use of vocabulary within each resulting seg-ment, as well as a significant difference of vocabu-lary between neighboring segments.
Moreover, thecombination of these two strategies enables regular-izing the number of segments found without resort-ing to prior knowledge.This piece of work uses the algorithm of Utiyamaand Isahara (2001) as a starting point, a versatile andperforming topic segmentation algorithm cast in astatistical framework.
Among the benefits of this al-gorithm are its independency to any particular do-main and its ability to cope with thematic segments1314of highly varying lengths, two interesting featuresto obtain a generic solution to the problem of topicsegmentation.
Moreover, the algorithm has provento be up to the state of the art in several studies, withno need of a priori information about the number ofsegments (contrary to algorithms in (Malioutov andBarzilay, 2006; Eisenstein and Barzilay, 2008) thatcan attain a higher segmentation accuracy).
It alsoprovides an efficient graph based implementation ofwhich we take advantage.To account both for cohesion and disruption, weextend the formalism of Isahara and Utiyama usinga Markovian assumption between segments in placeof the independence assumption of the original algo-rithm.
Keeping unchanged their probabilistic mea-sure of lexical cohesion, the Markovian assumptionenables to introduce the disruption between two con-secutive segments.
We propose an extended graphbased decoding strategy, which is both optimal andefficient, exploiting the notion of generalized seg-ment model or semi hidden Markov models.
Testsare performed on standard textual data sets and ona more challenging corpus of automatically tran-scribed broadcast news shows.The seminal idea of this paper was partially pub-lished in (Simon et al 2013) in the French language.The current paper significantly elaborates on the lat-ter, with a more detailed description of the algo-rithm and additional contrastive experiments includ-ing more data sets.
In particular, new experimentsclearly demonstrate the benefit of the method in arealistic setting with statistically significant gains.The organization of the article is as follows.
Ex-isting work on topic segmentation is presented inSection 2, emphasizing the motivations of the modelwe propose.
Section 3 details the baseline methodof Utiyama and Isahara before introducing our algo-rithm.
Experimental protocol and results are givenin Section 4.
Section 5 summarizes the finding andconcludes with a discussion of future work.2 Related workDefining the concept of theme precisely is not trivialand a large number of definitions have been given bylinguists.
Brown and Yule (1983) discuss at lengththe difficulty of defining a topic and note: ?Thenotion of ?topic?
is clearly an intuitively satisfac-tory way of describing the unifying principle whichmakes one stretch of discourse ?about?
somethingand the next stretch ?about?
something else, for itis appealed to very frequently in the discourse anal-ysis literature.
Yet the basis for the identification of?topic?
is rarely made explicit?.
To skirt the issue ofdefining a topic, they suggest to focus on topic-shiftmarkers and to identify topic changes, what mostcurrent topic segmentation methods do.Various characteristics can be exploited to iden-tify thematic changes in text data.
The most popularones rely either on the lexical distribution informa-tion to measure lexical cohesion (i.e., word reoccur-rences, lexical chains) or on linguistic markers suchas discourse markers which indicate continuity ordiscontinuity (Grosz and Sidner, 1986; Litman andPassonneau, 1995).
Linguistic markers are howeveroften specific to a type of text and cannot be consid-ered in a versatile approach as the one we are target-ing, where versatility is achieved relying on the solelexical cohesion.The key point with lexical cohesion is that a sig-nificant change in the use of vocabulary is consid-ered to be a sign of topic shift.
This general ideatranslates into two families of methods, local onestargeting a local detection of lexical disruptions andglobal ones relying on a measure of the lexical cohe-sion to globally find segments exhibiting coherencein their lexical distribution.Local methods (Hearst, 1997; Ferret et al 1998;Hernandez and Grau, 2002; Claveau and Lefe`vre,2011) locally compare adjacent fixed size regions,claiming a boundary when the similarity betweenthe adjacent regions is small enough, thus identify-ing points of high lexical disruption.
In the seminalwork of Hearst (1997), a fixed size window dividedinto two adjacent blocks is used, consecutively cen-tered at each potential boundary.
Similarity betweenthe adjacent blocks is computed at each point, the re-sulting similarity profile being analyzed to find sig-nificant valleys which are considered as topic bound-aries.On the contrary, global methods (Reynar, 1994;Choi, 2000; Utiyama and Isahara, 2001; Ji and Zha,2003; Malioutov and Barzilay, 2006; Misra et al2009) seek to maximize the value of the lexical co-hesion on each segment resulting from the segmen-tation globally on the text.
Several approaches have1315been taken relying on self-similarity matrices, suchas dot plots, or on graphs.
A typical and state-of-the-art algorithm is that of Utiyama and Isahara (2001)whose principle is to search globally for the bestpath in a graph representing all possible segmenta-tions and where edges are valued according to thelexical cohesion measured in a probabilistic way.When the lengths of the respective topic segmentsin a text (or between two texts) are very differ-ent from one another, local methods are challenged.Finding out an appropriate window size and extract-ing boundaries become critical with segments ofvarying length, in particular when short segmentsare present.
Short windows will render compari-son of adjacent blocks difficult and unreliable whilelong windows cannot handle short segments.
Thelack of a global vision also makes it difficult to nor-malize properly the similarities between blocks andto deal with statistics on segment length.
Whileglobal methods override these drawbacks, they facethe problem of over-segmentation due to the fact thatthey mainly rely on the sole lexical cohesion.
Shortsegments are therefore very likely to be coherentwhich calls for regularization introduced as priorson the segments length.These considerations naturally lead to the idea ofmethods combining lexical cohesion and disruptionto make the best of both worlds.
While the two cri-teria rely on the same underlying principle of lex-ical coherence (Grosz et al 1995) and might ap-pear as redundant, the resulting algorithms are quitedifferent in their philosophy.
A first (and, to thebest of our knowledge, unique) attempt at captur-ing a global view of the local dissimilarities is de-scribed in Malioutov and Barzilay (2006).
However,this method assumes that the number of segments tofind is known beforehand which makes it difficultfor real-world usage.3 Combining lexical cohesion anddisruptionWe extend the graph-based formalism of Utiyamaand Isahara to jointly account for lexical cohesionand disruption in a global approach.
Clearly, otherformalisms than the graph-based one could havebeen considered.
However, graph-based probabilis-tic topic segmentation has proven very accurate andversatile, relying on very minimal prior knowledgeon the texts to segment.
Good results at the state-of-the-art have also been reported in difficult conditionswith this approach (Misra et al 2009; Claveau andLefe`vre, 2011; Guinaudeau et al 2012).We briefly recall the principle of probabilisticgraph-based segmentation before detailing a Marko-vian extension to account for disruption.3.1 Probabilistic graph-based segmentationThe idea of the probabilistic graph-based segmen-tation algorithm is to find the segmentation into themost coherent segments constrained by a prior dis-tribution on segments length.
This problem is castinto finding the most probable segmentation of a se-quence of t basic units (i.e., sentences or utterancescomposed of words) W = ut1 among all possiblesegmentations, i.e.,S?
= arg maxSP [W |S]P [S] .
(1)Assuming that segments are mutually independentand assuming that basic units within a segment arealso independent, the probability of a text W for asegmentation S = Sm1 is given byP [W |Sm1 ] =m?i=1ni?j=1P [wij |Si] , (2)where ni is the number of words in the segmentSi, wij is the jth word in Si and m the number ofsegments.
The probability P [wij |Si] is given by aLaplace law where the parameters are estimated onSi, i.e.,P [wij |Si] =fi(wij) + 1ni + k, (3)where fi(wij) is the number of occurrences of wijin Si and k is the total number of distinct words inW , i.e., the size of the vocabulary V .
This probabil-ity favors segments that are homogeneous, increas-ing when words are repeated and decreasing consis-tently when they are different.
The prior distribu-tion on segment length is given by a simple model,P [Sm1 ] = n?m, where n is the total number ofwords, exhibiting a large value for a small numberof segments and conversely.The optimization of Eq.
1 can be efficiently im-plemented as the search for the best path in a1316weighted graph which represents all the possiblesegmentations.
Each node in the graph correspondsto a possible frontier placed between two utterances(i.e., we have a node between each pair of utter-ances), the arc between nodes i and j representing asegment containing utterances ui+1 to uj .
The cor-responding arc weight is the generalized probabilityof the words within segment Si?j according tov(i, j) =j?k=i+1ln(P [uk|Si?j ])?
?ln(n)where the probability is given as in Eq.
3.
The factor?
is introduced to control the trade-off between thesegments length and the lexical cohesion.3.2 Introduction of the lexical disruptionEq.
2 derives from the assumption that each segmentSi is independent from the others, which makes itimpossible to consider disruption between two con-secutive segments.
To do so, the weight of an arccorresponding to a segment Si should take into ac-count how different this segment is from Si?1.
Thisis typically handled using a Markovian assumptionof order 1.
Under this assumption, Eq.
2 is reformu-lated asP [W |Sm1 ] = P [W |S1]m?i=2P [W |Si, Si?1] ,where the notion of disruption can be embedded inthe term P [W |Si, Si?1] which explicitly mentionsboth segments.
Formally, P [W |Si, Si?1] is definedas a probability.
However, arbitrary scores which donot correspond to probabilities can be used insteadas the search for the best path in the graph of possi-ble segmentations makes no use of probability the-ory.
In this study, we define the score of a segmentSi given Si?1 aslnP [W |Si, Si?1] = lnP [Wi|Si]?
??
(Wi,Wi?1)(4)where Wi designates the set of utterances in Siand the rightmost part reflects the disruption be-tween the content of Si and of Si?1.
Eq.
4 clearlycombines the measure of lexical cohesion with ameasure of the disruption between consecutive seg-ments: ?
(Wi,Wi?1) > 0 measures the coherencebetween Si and Si?1, the substraction thus account-ing for disruption by penalizing consecutive coher-ent segments.
The underlying assumption is that thebigger ?
(Wi,Wi?1), the weaker the disruption be-tween the two segments.
Parameter ?
controls therespective contributions of cohesion and disruption.We initially adopted a probabilistic measureof disruption based on cross probabilities, i.e.,P [Wi|Si?1] and P [Wi?1|Si], which proved to havelimited impact on the segmentation.
We thereforeprefer to rely on a cosine similarity measure be-tween the word vectors representing two adjacentsegments, building upon a classical strategy of lo-cal methods such as TextTiling (Hearst, 1997).
Thecosine similarity measure is calculated between vec-tors representing the content of resp.
Si and Si?1,denoted vi and vi?1, where vi is a vector contain-ing the (tf-idf) weight of each term of V in Si.
Thecosine similarity is classically defined ascos(vi?1,vi) =?v?Vvi?1(v) vi(v)?
?v?Vv2i?1(v)?v?Vv2i (v).
(5)?
(Wi,Wi?1) is calculated from the cosine similar-ity measure as?
(Wi,Wi?1) = (1?
cos(vi?1,vi))?1 , (6)thus yielding a small penalty in Eq.
4 for highly dis-rupting boundaries, i.e., corresponding to low simi-larity measure.Given the quantities defined above, the algorithmboils down to finding the best scoring segmentationas given byS?
= arg maxSm?i=1ln(P [Wi|Si])??m?i=2?(Wi,Wi?1)?
?mln(n) .
(7)3.3 Segmentation algorithmTranslating Eq.
7 into an efficient algorithm is notstraightforward since all possible combinations ofadjacent segments need be considered.
To do so in agraph based approach, one needs to keep separatedthe paths of different lengths ending in a given node.In other words, only paths of the same length ending1317at a given point, with different predecessors, shouldbe recombined so that disruption can be consideredproperly in subsequent steps of the algorithm.
Notethat, in standard decoding as in Utiyama and Isa-hara?s algorithm, only one of such paths, the bestscoring one, would be retained.
We employ a strat-egy inspired from the decoding strategy of segmentmodels or semi-hidden Markov model with explicitduration model (Ostendorf et al 1996; Delakis etal., 2008).Search is performed through a lattice L ={V,E}, with V the set of nodes representing poten-tial boundaries and E the set of edges representingsegments, i.e., a set of consecutive utterances.
Theset V is defined asV = {nij |0 ?
i, j ?
N} ,where nij represents a boundary after utterance uireached by a segment of length j utterances andN = t+1.
In the lattice example of Fig.
1, it is trivialto see that for a given node, all incoming edges coverthe same segment.
For example, the node n42 is po-sitioned after u4 and all incoming segments containthe two utterances u3 and u4.
Edges are defined asE = {eip,jl|0 ?
i, p, j, l ?
N ;i < j; i = j ?
l;Lmin ?
l ?
Lmax} ,where eip,jl connects nip and njl with the constraintthat l = j ?
i and Lmin ?
l ?
Lmax.
Thus, an edgeeip,jl represents a segment of length l containing ut-terances from ui+1 to uj , denoted Si?j .
In Fig.
1,e01,33 represents a segment of length 3 from n01 ton33, covering utterances u1 to u3.
To avoid explo-sion of the lattice, a maximum segment length Lmaxis defined.
Symmetrically, a minimum segment sizecan be used.The property of this lattice, where, by construc-tion, all edges out of a node have the same segmentas a predecessor, makes it possible to weight eachedge in the lattice according to Eq.
4.
Consider anode nij for which all incoming edges encompassutterances ui?j to ui.
For each edge out of nij ,whatever the target node (i.e., the edge length), onecan therefore easily determine the lexical cohesionas defined by the generalized probability of Eq.
3and the disruption with respect to the previous seg-ment as defined by Eq.
6.Algorithm 1 Maximum probability segmentationStep 0.
Initializationq[0][j] = 0 ?j ?
[Lmin, Lmax]q[i][j] = ??
?i ?
[1, N ], j ?
[Lmin, Lmax]Step 1.
Assign best score to each nodefor i = 0?
t dofor j = Lmin ?
Lmax dofor k = Lmin ?
Lmax do/* extend path ending after ui with asegment of length j with an arc of length k */q[i+k][k] = max??????????
?q[i+ k][k],q[i][j]+Cohesion(ui+1 ?
ui+k)???
(ui?j ?
ui;ui+1 ?
ui+k)end forend forend forStep 2.
Backtrack from nNj with best scoreq[N ][j]Given the weighted decoding graph, the solutionto Eq.
7 is obtained by finding out the best path inthe decoding lattice, which can be done straightfor-wardly by scanning nodes in topological order.
Thedecoding algorithm is summarized in Algorithm 1with an efficient implementation in o(NL2max) whichdoes not require explicit construction of the lattice.4 ExperimentsExperiments are performed on three distinct corporawhich exhibit different characteristics, two contain-ing textual data and one spoken data.
We firstpresent the corpora before presenting and discussingresults on each.4.1 CorporaThe artificial data set of Choi (2000) is widely usedin the literature and enables comparison of a newsegmentation method with existing ones.
Choi?sdata set consist of 700 documents, each created byconcatenating the first z sentences of 10 articles ran-domly chosen from the Brown corpus, assumingeach article is on a different topic.
Table 1 provides1318Figure 1: An example of a lattice L.z = 3?11 3?5 6?8 9?11# samples 400 100 100 100Table 1: Number of documents in Choi?s corpus (Choi,2000).the corpus statistics, where z=3?11 means z is ran-domly chosen in the range [3, 11].
Hence, Choi?scorpus is adapted to test the ability of our modelto deal with variable segments length, z=3?11 be-ing the most difficult condition.
Moreover, Choi?scorpus provides a direct comparison with results re-ported in the literature.One of the main criticism of Choi?s data set is thepresence of abrupt topic changes due to the artifi-cial construction of the corpus.
We therefore re-port results on a textual corpus with more naturaltopic changes, also used in (Eisenstein and Barzi-lay, 2008).
The data set consists of 277 chaptersselected from (Walker et al 1990), a medical text-book, where each chapter?considered here as adocument?was divided by its author into themat-ically coherent sections.
The data set has a total of1,136 segments with an average of 5 segments perdocument and an average of 28 sentences per seg-ment.
This data set is used to study the impact ofsmooth, natural, topic changes.Finally, results are reported on a corpus of au-tomatic transcripts of TV news spoken data.
Thedata set consists of 56 news programs (?1/2 houreach), broadcasted in February and March 2007 onthe French television channel France 2, and tran-scribed by two different automatic speech recogni-tion (ASR) systems, namely IRENE (Huet et al2010) and LIMSI (Gauvain et al 2002), with re-spective word error rates (WER) around 36 % and30 %.
Each news program consists of successivereports of short duration (2-3 min), possibly withconsecutive reports on different facets of the samenews.
The reference segmentation was establishedby associating a topic with each report, i.e., plac-ing a boundary at the beginning of a report?s in-troduction (and hence at the end of the closing re-marks).
The TV transcript data set, which corre-sponds to some real-world use cases in the multi-media field, is very challenging for several reasons.On the one hand, segments are short, with a reducednumber of repetitions, synonyms being frequentlyemployed.
Moreover, smooth topic shifts can befound, in particular at the beginning of each pro-gram with different reports dedicated to the head-line.
On the other hand, transcripts significantly dif-fer from written texts: no punctuation signs or capi-tal letters; no sentence structure but rather utteranceswhich are only loosely syntactically motivated; pres-ence of transcription errors which may imply an ac-centuated lack of word repetitions.All data were preprocessed in the same way:Words were tagged and lemmatized with TreeTag-1319ger1 and only the nouns, non modal verbs and adjec-tives were retained for segmentation.
Inverse docu-ment frequencies used to measure similarity in Eq.
5are obtained on a per document basis, referring tothe number of sentences in textual data and of utter-ances in spoken data.4.2 ResultsPerformance is measured by comparison of hypoth-esized frontiers with reference ones.
Alignment as-sumes a tolerance of 1 sentence on texts and of 10seconds on transcripts, which corresponds to stan-dard values in the literature.
Results are reported us-ing recall, precision and F1-measure.
Recall refersto the proportion of reference frontiers correctly de-tected; Precision corresponds to the ratio of hypoth-esized frontiers that belong to the reference seg-mentation; F1-measure combines recall and preci-sion in a single value.
These evaluation measureswere selected because recall and precision are notsensitive to variations of segment length contraryto the Pk measure (Beeferman et al 1997) anddo not favor segmentations with a few number offrontiers as WindowDiff (Pevzner and Hearst, 2002)(see (Niekrasz and Moore, 2010) for a rigorous an-alytical explanation of the biases of Pk and Win-dowDiff ).Several configurations were considered in the ex-periments; due to space constraints, only the mostsalient experiments are presented here.
In Eq.
7, theparameter ?, which controls the contribution of theprior model with respect to the lexical cohesion anddisruption, allows for different trade-offs betweenprecision and recall.
For any given value of ?, ?is thus varied, providing the range of recall/precisionvalues attainable.
Results are compared to a baselinesystem corresponding to the application of the orig-inal algorithm of Utiyama and Isahara (i.e., setting?
= 0).
This baseline has been shown to be a high-performance algorithm, in particular with respect tolocal methods that exploit lexical disruption.
Differ-ences in F1-measure between this baseline and oursystem presented below are all statistically signifi-cant at the level of p < 0.01 (paired t-test).Choi?s corpus.
Figure 2 reports results obtainedon Choi?s data set, each graphic corresponding to1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTaggerz ?F1 Confidence interval 95 %gain UI Combined3-5 0 -0.2 [66.6,74.26] [75.23,78.08]3-5 1 0.7 [72.25,83.4] [87.88,92.13]3-11 1 0.23 [68.5,79.3] [86.6,87.43]6-8 1 0.4 [68.48,80.99] [76.9,85.17]9-11 0 1.6 [64.35,75.16] [81.31,84.86]9-11 1 1.4 [68.39,80.39] [84.37,88.9]Table 2: Gain in F1-measure for Choi?s corpus when us-ing lexical cohesion and disruption, and the correspond-ing 95 % confidence intervals for the F1-measure.
Re-sults are reported for different tolerance ?
.
UI denotesthe baseline and Combined the proposed model.a specific variation in the size of the thematic seg-ments forming the documents (e.g., 9 to 11 sen-tences for the top left graphic).
Results are providedfor different values of ?
in terms of F1-measureboxplots, i.e., variations of the F1-measure when ?varies (same range of variation for ?
considered foreach plot), where the leftmost boxplot, denoted byUI , corresponds to the baseline.
Box and whiskerplots graphically depicts the distribution of the F1-measures that can be attained by varying ?, plottingthe median value, the first and third quartile and theextrema.Figure 2 shows that, whatever the segmentslength, results globally improve according to the im-portance given to the disruption (?
variable).
More-over, the variation in F1-measure diminishes whendisruption is considered, thus indicating the influ-ence of the prior model diminishes.
When the seg-ments size decreases (see Figs.
2(b), 2(c), 2(d)), thedifference in the maximum F1-measure between ourresults and that of the baseline lowers, however stillin favor of our model.
This can be explained by thefact that our approach is based on the distribution ofwords, thus more words better help discriminate be-tween potential thematic frontiers.
Finally, using toolarge values for ?
can lead to under-segmentation, ascan be seen in Fig.
2(d) where, for ?
= 3, the varia-tion of F1-measure increases and the distribution be-comes negatively skewed (i.e., the median is closerto the third quartile than to the first).These results are confirmed by Table 2 whichpresents the gain in F1-measure (i.e., the differ-ence between the highest F1-measure obtained when1320(a) (b)(c) (d)Figure 2: F1-measure variation obtained on Choi?s corpus.
In each graphic, the leftmost boxplot UI corresponds toresults obtained by using the sole lexical cohesion (baseline), while the ?
value is the importance given to the lexicaldisruption in our approach.
Results are provided for the same range of variation of factor ?, allowing a tolerance of 1sentence between the hypothesized and reference frontiers.combining lexical cohesion and disruption and thehighest value for the baseline) for each of the foursets of documents in Choi?s corpus, together withthe 95 % confidence intervals: The effect of usingthe disruption is higher when segment size is longer,whether evaluation allows or not for a tolerance ?between the hypothesized frontiers and the referenceones.
A qualitative analysis of the segmentationsobtained confirmed that employing disruption helpseliminate wrong hypothesis and shift hypothesizedfrontiers closer to the reference ones (explaining thehigher gain at tolerance 0 for 9-11 data set).
Whensmaller segments?thus few word repetitions?andno tolerance are considered (e.g., 3?5), disruptioncannot improve segmentation.
Our model is glob-ally stable with respect to segment length, with rel-atively similar gain for 3?11 and 6?8 data sets inwhich the average number of words (distinct or re-peated) is close.Results discussed up to now are optimistic as theycorrespond to the best F1 value attainable computeda posteriori.
Stability of the results was confirmedz = 3?5 3?11 6?8 9?11UI 91.9 87.0 93.1 92.8Combined 92.9 87.5 93.5 94.0Table 3: F1 results using cross-validation on Choi?s dataset.using cross-validation with 5 folds (10 folds forz=3?11): Parameters ?
and ?
maximizing the F1-measure are determined on all but one fold, this lastfold being used for evaluation.
Results, averagedover all folds, are reported in Tab.
3 for the baselineand the method combining cohesion and disruption.Medical textbook corpus.
The medical textbookcorpus was previously used for topic segmentationby Eisenstein and Barzilay (2008) with their algo-rithm BayesSeg2.
We thus compare our results withthose obtained by BayesSeg and by the baseline.When considering the best F1-measure (i.e., the bestF1-measure which can be achieved by varying ?
and2The code and the data set are available athttp://groups.csail.mit.edu/rbg/code/bayesseg/1321(a) (b)Figure 3: Boxplots showing F1-measure variation on transcripts obtained using IRENE and LIMSI automatic speechrecognition systems.?
), we achieved an improvement of 2.2 with respectto BayesSeg when no tolerance is allowed, and of0.5 when the tolerance is of 1 sentence.
The corre-sponding figures with respect to the baseline are 0.6and 0.4.
When considering the F1-measure valuefor which the number of hypothesized frontiers isthe closest to the number of reference boundaries,improvement is of resp.
1.5 and 0.5 with respect toBayesSeg, -0.1 and 0.4 with respect to the baseline.These results show that our model combining lexi-cal cohesion and disruption is also able to deal withtopic segmentation of corpora from a homogeneousdomain, with smooth topic changes and segments ofregular size.One can argue that the higher number of free pa-rameters in our method explains most of the gainwith respect to BayesSeg.
While BayesSeg has onlyone free parameter (as opposed to two in our case),the number of segments is assumed to be providedas prior knowledge.
This assumption can be seenas an additional free parameter, i.e., the number ofsegments, and is a much stronger constraint than weare using.
Moreover, cross-validation experimentson the Choi data set show that improvement is notdue to over-fitting of the development data thanks toan additional parameter.
Gains on development setwith parameters tuned on the development set itselfand with parameters tuned on a held-out set in cross-validation experiments are in the same range.TV news transcripts corpus Figure 3 providesresults, in terms of F1-measure variation, for TVnews transcripts obtained with the two ASR sys-tems.
On this highly challenging corpus, with shortsegments, wrongly transcribed spoken words, andthus few word repetitions, the capabilities of ourmodel to overcome the baseline system are reduced.Yet, an improvement of the quality of the segmen-tation of these noisy data is still observed, andgeneral conclusions are quite similar?though a bitweaker?to those already made for Choi?s corpus.Results are confirmed in Table 4 which presents thegain in F1-measure of our model together with the95 % confidence interval, where F1-measure valuescorrespond to that of segmentations with a num-ber of hypothesized frontiers the closest to the ref-erence.
The two first lines show that the gain issmaller for IRENE transcripts which have a higherWER, thus fewer words available to discriminate be-tween segments belonging to different topics.
Theimpact of transcription errors is illustrated in thelast three lines, when segmenting six TV news forwhich manual reference transcripts were available(line 3), where the higher the WER, the smaller theF1-measure gain.5 ConclusionsWe have proposed a method to combine lexical co-hesion and disruption for topic segmentation.
Ex-perimental results on various data sets with variouscharacteristics demonstrate the impact of taking intoaccount disruption in addition to lexical cohesion.We observed gains both on data sets with segmentsof regular length and on data sets exhibiting seg-ments of highly varying length within a document.Unsurprisingly, bigger gains were observed on doc-1322CorpusF1 Confidence interval 95 %gain UI CombinedIRENE 0.3 [54.4,57.6] [56.92,59]LIMSI 0.86 [56.7,60.2] [59.44,61.95]MANUAL (6) 0.77 [70.39,72.29] [71.7,73.29]IRENE (6) 0.2 [56.81,60.94] [59.51,63.43]LIMSI (6) 0.5 [64.27,68.64] [67.7,71.56]Table 4: Gain in F1-measure for TV news corpus auto-matic and manual transcripts when using lexical cohesionand disruption, and the corresponding 95 % confidenceintervals.
Last three rows report results on only 6 showsfor which manual reference transcripts are available.uments containing relatively long segments.
How-ever the segmentation algorithm has proven to berobust on automatic transcripts with short segmentsand limited vocabulary reoccurrences.
Finally, wetested both abrupt topic changes and smooth oneswith good results on both.
Further work can be con-sidered to improve segmentation of documents char-acterized by small segments and few words repe-titions, such as using semantic relations or vector-ization techniques to better exploit implicit relationsnot considered by lexical reoccurrence.ReferencesDoug Beeferman, Adam Berger, and John Lafferty.1997.
Text segmentation using exponential models.In 2nd Conference on Empirical Methods in NaturalLanguage Processing, pages 35?46.Gillian Brown and George Yule.
1983.
Discourse analy-sis.
Cambridge University Press.Freddy Y. Y. Choi.
2000.
Advances in domain inde-pendent linear text segmentation.
In 1st InternationalConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 26?33.Vincent Claveau and Se?bastien Lefe`vre.
2011.
Topicsegmentation of TV-streams by mathematical mor-phology and vectorization.
In 12th International Con-ference of the International Speech CommunicationAssociation, pages 1105?1108.Manolis Delakis, Guillaume Gravier, and Patrick Gros.2008.
Audiovisual integration with segment modelsfor tennis video parsing.
Computer Vision and ImageUnderstanding, 111(2):142?154.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Conference onEmpirical Methods in Natural Language Processing,pages 334?343.Olivier Ferret, Brigitte Grau, and Nicolas Masson.
1998.Thematic segmentation of texts: Two methods for twokinds of texts.
In 36th Annual Meeting of the As-sociation for Computational Linguistics and 17th In-ternational Conference on Computational Linguistics,pages 392?396.Jean-Luc Gauvain, Lori Lamel, and Gilles Adda.
2002.The LIMSI broadcast news transcription system.Speech Communication, 37(1?2):89?108.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational Linguistics, 12(3):175?204.Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.1995.
Centering: a framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225, June.Camille Guinaudeau, Guillaume Gravier, and PascaleSe?billot.
2012.
Enhancing lexical cohesion measurewith confidence measures, semantic relations and lan-guage model interpolation for multimedia spoken con-tent topic segmentation.
Computer Speech and Lan-guage, 26(2):90?104.Marti A. Hearst.
1997.
TextTiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64.Nicolas Hernandez and Brigitte Grau.
2002.
Analysethe?matique du discours : segmentation, structuration,description et repre?sentation.
In 5e colloque interna-tional sur le document e?lectronique, pages 277?285.Ste?phane Huet, Guillaume Gravier, and Pascale Se?billot.2010.
Morpho-syntactic post-processing of n-best listsfor improved French automatic speech recognition.Computer Speech and Language, 24(4):663?684.Xiang Ji and Hongyuan Zha.
2003.
Domain-independenttext segmentation using anisotropic diffusion and dy-namic programming.
In 26th Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval, pages 322?329.Diane J. Litman and Rebecca J. Passonneau.
1995.
Com-bining multiple knowledge sources for discourse seg-mentation.
In 33rd Annual Meeting of the Associationfor Computational Linguistics, pages 108?115.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In 21st In-ternational Conference on Computational Linguisticsand the 44th Annual Meeting of the Association forComputational Linguistics, pages 25?32.Hemant Misra, Franc?ois Yvon, Joemon M. Jose, andOlivier Cappe.
2009.
Text segmentation via topicmodeling: an analytical study.
In Proc.
ACM con-ference on Information and knowledge management,pages 1553?1556.Marie-Francine Moens and Rik De Busser.
2001.Generic topic segmentation of document texts.
In 24th1323International Conference on Research and Develope-ment in Information Retrieval, pages 418?419.John Niekrasz and Johanna D. Moore.
2010.
Unbiaseddiscourse segmentation evaluation.
In Spoken Lan-guage Technology, pages 43?48.Mari Ostendorf, Vassilios V. Digalakis, and Owen A.Kimball.
1996.
From HMM?s to segment models: aunified view of stochastic modeling for speech recog-nition.
IEEE Transactions on Speech and Audio Pro-cessing, 4(5):360?378.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28:19?36.Jeffrey C. Reynar.
1994.
An automatic method of findingtopic boundaries.
In 32nd Annual Meeting on Associ-ation for Computational Linguistics, pages 331?333.Anca Simon, Guillaume Gravier, and Pascale Se?billot.2013.
Un mode`le segmental probabiliste combinantcohe?sion lexicale et rupture lexicale pour la segmen-tation the?matique.
In 20e confe?rence Traitement Au-tomatique des Langues Naturelles, pages 202?214.Masao Utiyama and Hitoshi Isahara.
2001.
A statisticalmodel for domain-independent text segmentation.
In39th Annual Meeting on the Association for Computa-tional Linguistics, pages 499?506.Kenneth H. Walker, Dallas W. Hall, and Willis J. Hurst.1990.
Clinical Methods: The History, Physical, andLaboratory Examinations.
Butterworths.1324
