BUPT Systems in the SIGHAN Bakeoff 2007Ying Qin   Caixia Yuan   Jiashen Sun   Xiaojie WangCenter of Intelligent Science and Technology ResearchBeijing University of Posts and TelecommunicationsBeijing, 100876, Chinaqinyingmail@163.com, yuancx@gmail.com,b.bigart911@gmail.com, xjwang@bupt.edu.cnAbstractChinese Word Segmentation(WS), NameEntity Recognition(NER) and Part-Of-Speech(POS) are three important ChineseCorpus annotation tasks.
With the greatimprovement in these annotations on somecorpus, now, the robustness, a capability ofkeeping good performances for a system byautomatically fitting the different corpusand standards, become a focal problem.This paper introduces the work onrobustness of WS and POS annotationsystems from Beijing University of Postsand Telecommunications(BUPT), and twoNER systems.
The WS system combines abasic WS tagger with an adaptor used to fita specific standard given.
POS taggers arebuilt for different standards under a twostep frame, both steps use ME but withincremental features.
A multipleknowledge source system and a lessknowledge Conditional Random Field(CRF) based systems are used for NER.Experiments show that our WS and POSsystems are robust.1 IntroductionIn the last SIGHAN bakeoff, there is no singlesystem consistently outperforms the others ondifferent test standards of Chinese WS and NERstandards(Sproat and Emerson, 2003).Performances of some systems varied significantlyon different corpus and different standards, thiskind of systems can not satisfy demands inpractical applications.
The robustness, a capabilityof keeping good performances for a system byautomatically fitting the different corpus andstandard, thus become a focal problem in WS andNER, it is the same for Chinese Part-of-Speech(POS) task which is new in the SIGHANbakeoff 2007.It is worthy to distinguish two kinds of differentrobustness, one is for different corpus (fromdifferent sources or different domain and so on)under a same standard, we call it corpus robustness,and another is for different standards (for differentapplication goals or demands and so on) for a samecorpus.
We call it standard robustness.
TheSIGHAN bakeoff series seems to focus more onlater.
We think corpus robustness should bereceived more attentions in the near future.We participant all simplified Chinese track onWS, NER and POS task in the SIGHAN bakeoff2007.
There are more than two tracks for WS andPOS.
This gives us a chance to test the robustnessof our systems.
This paper reports our WS, NERand POS systems in the SIGHAN Bakeoff 2007,especially on the work of achieving robustness ofWS and POS systems.This paper is arranged as follows, we introduceour WS, NER and POS system separately insection 2, section 3 and section 4, experiments andresults are listed in section 5, finally we draw someconclusions.2 Word SegmentationWS system includes three sequent steps, which arebasic segmentation, disambiguation and out-ofvocabulary (OOV) recognition.
In each step, weconstruct a basic work unit first, and then have anadaptor to tune the basic unit to fit differentstandards.94Sixth SIGHAN Workshop on Chinese Language Processing2.1 Basic SegmentationFor constructing a basic work unit for WS, acommon wordlist containing words ratified by fourdifferent segmentation standards (from SXU, NCC,PKU and CTB separately) are built.
We finally get64,000 words including about 1500 known entitywords as the common wordlist.
A forward-backward maximum matching algorithm with thecommon wordlist is employed as the common unitof our basic segmentor.To cater for different characteristics in differentsegmentation standards, we construct anotherwordlist containing words for each specification.A wordlist based adaptor is built to implement thetuning task after basic segmentation.2.2 DisambiguationDisambiguation of overlapping Ambiguity (OA) isa major task in this step.Strings with OA are also detected during basicforward-backward maximum matching in basicWS step.
These strings are common OA strings fordifferent standards.
Class-based bigram model isapplied to resolve the ambiguities.
In class-basedbigram, all named entities, all punctuation andfactoids is one class respectively and each word isone class.
We train the bigram transitionprobability based on the corpus of ChinesePeople?s Daily 2000 newswire.For corpus from different standards, overlappingambiguity strings with less than 3 overlappingchain are extracted from each train corpus.
We donot work on all of them but on some strings with afrequency that is bigger than a given value.
Adisambiguation adaptor using the highestprobability segmentations is built for OA stringsfrom each different standard.2.3 OOV RecognitionIn OOV recognition, we have a similar modelwhich consists of a common part based oncommon characteristics and an individual partautomatically constructed for each standard.We divide OOV into factoid which containsnon-Chinese characters like date, time, ordinalnumber, cardinal number, phone number, emailaddress and non-factoid.Factoid is recognized by an automaton.
Tocompatible to different standards, we also builtcore automata and several adaptors.Non-factoid is tackled by a unified character-based segmentation model based on CRF.
We firsttransform the WS training dataset into character-based two columns format as the training dataset inNER task.
The right column is a boundary tag ofeach character.
The boundary tags are B I and S,which B is the tag of the first character of a wordwhich contains more than two characters, I is theother non-initial characters in a word, S is for thesingle character word.
Then the transformedtraining data is used to train the CRF model.Features in the model are current character andother three characters within the context andbigrams.The trigger of non-factoid recognition iscontinual single character string excluding all thepunctuations in a line after basic word matching,disambiguation and factoid incorporation.
Themodel will tell whether these consecutivecharacters can form multi-character words in agiven context.At last, several rules are used to recognize someproper names separated by coordinate characterslike ??
?, ??
?, ???
and symbol ???
in foreignperson names.3 Named Entity RecognitionWe built two NER systems separately.
One is aunified named entity model based on CRF.
It usedonly a little knowledge include a small scale ofentity dictionary, a few linguistic rules to processsome special cases such as coordinate relation incorpus and some special symbols like dot among atransliteration foreign person name.Another one is an individual model for eachkind of entity based on Maximum Entropy wheremore rules found from corpus are used on entityboundary detection.
Some details on this modelcan be found in Suxiang Zhang et al2006.4 POS TaggingIn POS, we construct POS taggers for differentstandards under a two steps frame, both steps useME but with incremental features.
First, we usenormal features based Maximum Entropy (ME) totrain a basic model, and then join someprobabilistic features acquired from error analysisto training a finer model.95Sixth SIGHAN Workshop on Chinese Language Processing4.1 Normal Features for MEIn the first step of feature selection for MEtagger, we select contextual syntactic features forall words basing on a series of incrementalexperiments.For shrinking the search space, the model onlyassigns each word a label occurred in the trainingdata.
That is, the model builds a subset of all POStags for each token and restricts all possible labelsof a word within a small candidate set, whichgreatly saves computing cost.We enlarged PKU training corpus by using onemonth of Peking University's China Daily corpus(June in 2003) and CTB training corpus by usingCTB 2.0 data which includes 325 passages.To adapt with the given training corpus, thesamples whose labels are not included in thestandard training data were omitted firstly.
Afterpreprocessing, we get two sets of training samplesfor PKU and CTB with 1178 thousands tokens and206 thousands tokens respectively.
But the NCCtest remains its original data due to we have nocorpus with this standard.4.2 Probabilistic feature for MEBy detecting the label errors when training andtesting using syntactic features such as wordsaround the current tokens and tags of previoustokens, words with multiple possible tags areobviously error-prone.
We thus define someprobabilistic features especially for multi-tagwords.We find labels of these tokens are most closelyrelated to POS tag of word immediately previousto them.
For instance, in corpus of PekingUniversity, word ?Report?
has three different tagsof ?n(noun), v(verb), vn(noun verb)?.
But when wetaken into account its immediately previous words,we can find that when previous word's label is?q(quantifier)?, ?Report?
is labeled as ?n?
with afrequency of 91.67%, ?v?
with a frequency of8.33% and ?vn?
with a frequency of 0.0%.
We canassume that ?Report?
is labeled as ?n?
with the91.67% probability when previous word's label is?q?, and so on.Such probability is calculated from the wholetraining data and is viewed as discriminatingprobabilistic feature when choosing among themultiple tags for each word.
But for words withonly one possible tag, no matter what the label ofprevious word is, the label for them is always thetag occurred in the training data.5 ExperimentsWe participant all simplified Chinese tracks on WS,NER and POS task in the SIGHAN bakeoff 2007.Our systems only deal with Chinese in GBK code.There are some mistakes in some results submittedto bakeoff organizer due to coding transform fromGBK to UTF-16.
We then use WS evaluationprogram in the SIGHAN bakeoff 2006 to re-evaluate WS system using same corpus, as for POS,since there is no POS evaluation in the SIGHANbakeoff 2006, we implement a evaluation usingourselves?
program using same corpus.Table 1 shows evaluation results of WS usingevaluation programs from both the SIGHANbakeoff 2007 and the SIGHAN bakeoff 2006.Table 2 lists evaluation results of NER usingevaluation program from the SIGHAN bakeoff2007.
Table 3 gives evaluation results of POSusing evaluation programs from both the SIGHANbakeoff 2007 and ourselves(BUPT).Track UTF-16(SIGHAN4)GBK(SIGHAN 3)CTB 0.9256 0.950SXU 0.8741 0.969NCC 0.9592 0.972Table 1.
WS results (F-measure)SIGHAN 4 R P FSystem-1 0.8452 0.872 0.8584System-2 0.8675 0.9163 0.8912Table 2.
NER results (F-measure)Track UTF-16(SIGHAN 4)GBK(BUPT)CTB 0.9689 0.9689NCC 0.9096 0.9096PKU 0.6649 0.9462Table 3.
POS Results (F-measure)From the table 1 and Table 3, we can find oursystem is robust enough.
WS system keeps at arelatively steady performance.
Difference in POS96Sixth SIGHAN Workshop on Chinese Language Processingbetween NCC and other two tracks is mainly dueto the difference of the training corpus.6 ConclusionRecently, the robustness, a capability of keepinggood performances for a system by automaticallyfitting the different corpus and standards, become afocal problem.
This paper introduces our WS, NERand POS systems, especially on how they can get arobust performance.The SIGHAN bakeoff series seems to focusmore on standard robustness.
We think corpusrobustness should be received more attentions inthe near future.AcknowledgementThanks to Zhang Yan, Zhang Bichuan, ZhangTaozheng, Liu Haipeng and Jiang Huixing for allthe work they done to make the WS, NER andPOS systems go on wheels in a very short time.ReferencesBerger, A., Della Pietra, S. and Della Pietra, V.: AMaximum Entropy Approach to NaturalLanguage Processing.
ComputationalLinguistics.
22(1): pp 39-71, 1996.Thomas Emerson.
2005.
The Second InternationalChinese Word Segmentation Bakeoff.
InProceedings of the Fourth SIGHAN Workshopon Chinese Language Processing, Jeju Island,Republic of Korea.NanYuan Liang.
1987 A Written ChineseSegmentation system?
CDWS.
Journal ofChinese Information Processing, Vol.2: 44-52YaJuan Lv, Tie-jun Zhao, et al 2001.
Leveledunknown Chinese Words resolution by dynamicprogramming.
Journal Information Processing,15(1): 28-33.Yintang Yan, XiaoQiang Zhou.
2000.
Study ofSegmentation Strategy on Ambiguous Phrasesof Overlapping Type  Journal of The ChinaSociety For Scientific and Technical InformationVol.
19 , ?6Richard Sproat and Thomas Emerson.
2003.
TheFirst International Chinese Word SegmentationBakeoff.
In Proceedings of the Second SIGHANWorkshop on Chinese Language Processing,Sapporo, Japan.Caixia Yuan, Xiaojie Wang, Yixin Zhong.
SomeImprovements on Maximum Entropy BasedChinese POS Tagging.
The Journal of ChinaUniversities of Posts and Telecommunications,Vol.
13, pp 99-103, 2006.Suxiang Zhang, Xiaojie Wang, Juan Wen, YingQin, Yixin Zhong.
A Probabilistic FeatureBased Maximum Entropy Model for ChineseNamed Entity Recognition, in proceedings of21st International Conference on the ComputerProcessing of Oriental Languages,December17-19, 2006, Singapore.97Sixth SIGHAN Workshop on Chinese Language Processing
