Proceedings of the SIGDIAL 2014 Conference, pages 171?180,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsIdentifying Narrative Clause Types in Personal StoriesReid Swanson, Elahe Rahimtoroghi, Thomas Corcoran and Marilyn A. WalkerNatural Language and Dialog Systems LabUniversity of California Santa CruzSanta Cruz, CA 95064, USA{reid,elahe,maw}@soe.ucsc.edu, tcorcora@ucsc.eduAbstractThis paper describes work on automaticallyidentifying categories of narrative clausesin personal stories written by ordinary peo-ple about their daily lives and experiences.We base our approach on Labov & Walet-zky?s theory of oral narrative which catego-rizes narrative clauses into subtypes, such asORIENTATION, ACTION and EVALUATION.We describe an experiment where we an-notate 50 personal narratives from weblogsand experiment with methods for achievinghigher annotation reliability.
We use the re-sulting annotated corpus to train a classi-fier to automatically identify narrative cat-egories, achieving a best average F-score of.658, which rises to an F-score of .767 onthe cases with the highest annotator agree-ment.
We believe the identified narrativestructure will enable new types of compu-tational analysis of narrative discourse.1 IntroductionSharing personal experiences by storytelling isa fundamental aspect of human social behavior(Fivush et al., 2005; Fivush and Nelson, 2004;Habermas and Bluck, 2000; Bamberg, 2006;Thorne, 2004; Bohanek et al., 2008; Thorne andNam, 2009; McLean and Thorne, 2003; Pratt andFiese, 2004).
Humans appear to be wired to en-gage with information that is narratively structured(Gerrig, 1993; Bamberg, 2006; Bruner, 1991), andtelling stories provides a critical developmental andsocietal function, by serving as a means to reinforcecommunity value systems and to define individualidentity (Thorne and Shapiro, 2011; Thorne et al.,2007).
This has led some theorists to claim that ?thestories they tell?
is the defining aspect of both indi-viduals and cultures.Unlike any prior time in human history, personalnarratives about many life experiences are beingtold online, and are widely available in social me-dia sources such as weblogs.
A personal narrativeabout an arrest is shown in Fig.
1, and one about aprotest is in Fig.
4.
Narratives such as these providea valuable resource for learning a wealth of com-monsense knowledge about people, the types of ac-tivities they engage in, and the attitudes they hold.They are also well suited to learning about causaland temporal relationships between events becausenarrative interpretation explicitly depends on the co-herence of these relationships (Graesser et al., 1994;Elson, 2012; Gordon et al., 2011; Hu et al., 2013).This paper applies and tests a narrative clause la-beling scheme to personal narratives.
Our schemeis derived from Labov & Waletzky?s (henceforthL&W) theory of oral narrative (Labov, 1997; Labovand Waletzky, 1967).
L&W?s theory distinguishes(1) clauses that indicate causal relationships (AC-TION), from (2) clauses that provide traits or prop-erties of the setting or characters (ORIENTATION),from (3) clauses describing the story characters?emotional reactions to the events (EVALUATION).We adopt L&W?s theory for three reasons.
First,we believe that the narrative structure of personalnarratives posted on weblogs will be more similarto oral narrative than they are to classical stories.Second, we believe that any narrative discourse ty-pology must at least distinguish ACTION, from ORI-ENTATION, and EVALUATION.
Third, personal sto-ries found on the web are often noisy and diffi-cult to interpret; they do not always clearly followwell defined narrative conventions.
A deep analysis171# Category Story Clause1 OrientationNow, on with this week?s story...2 OrientationThe last month has been hectic.3 OrientationTurbo charged.4 OrientationLot?s of work because I was learning from Tim, my partner in crime.5 OrientationThis hasn?t been helped by the intense pressure in town due to the political transition coming to an end.6 OrientationThis week things started alright and on schedule.7 ActionBut I managed to get myself arrested by the traffic police (rouleage) early last Wednesday.8 ActionAfter yelling excessively at their outright corrupted methods9 Actionand asking incessently for what law I actually broke,10 Actionthey managed to bring me in at the police HQ.11 ActionI was drawing too much of a curious crowd for the authorities.12 ActionIn about half an hour at police HQ I had charmed every one around.13 ActionI had prepared my ?gift?
as they wished.14 EvaluationDecision witheld, they decided that I neednt to bother,15 Evaluationthey liked me too much.16 EvaluationI should go free.17 ActionI even managed to meet famous Raus, the big chief.18 EvaluationHe was too happy to let me go when he realized I was no one.19 ActionBut then, a Major at his side noticed my Visa was expired.20 EvaluationDamn!21 OrientationMy current Visa is being renewed in my other passport at Immigration?s.22 EvaluationFuck.23 EvaluationIn custody, for real.Figure 1: An excerpt from an example story from our corpus annotated with the L&W categories.and annotation scheme, such as the one employedby DramaBank (Elson and McKeown, 2010; Elson,2012) that extends theories of narrative structure andplot units (Stein et al., 2000; Lehnert, 1981), offersmany advantages.
However, acquiring this level ofanalysis on user generated content, such as blog sto-ries, is resource intensive.Research on computational models of narrativestructure typically focus on inferring the causal andtemporal relationships between events (Goyal et al.,2010; Chambers and Jurafsky, 2009; Riaz and Girju,2010; Beamer and Girju, 2009; Do et al., 2011;Manshadi et al., 2008; Gordon et al., 2011; Hu et al.,2013).
Yet L&W point out that stories are not justabout the events that occur.
In fact, L&W say thatstories that are only about events are boring.
Currentmethods for inferring narrative structure, includingour own (Hu et al., 2013), do not distinguish eventclauses from other narrative clause types.
But notethat actions only constitute about one third of theclauses in the narratives in Fig.
1 and Fig.
4.Sec.
2 provides more detail about L&W?s the-ory.
Sec.
3 describes the annotation experimentsand efforts to improve annotation reliability.
Sec.
4presents experiments on learning to automaticallyclassify L&W categories, where we examine the themost predictive features, and the effect of annotatoragreement on classification accuracy.
We achieve abest average F-score of .658, which rises to an F-score of .767 on the cases with the highest annotatoragreement.
We analyze the types of errors the clas-sifier makes in Sec.
5.1 and conclude in Sec.
6.2 Labov & Waletzky?s Theory ofNarrative DiscourseL&W?s theory of oral narrative defines a story asa series of ACTION clauses (events), of which atleast two must be temporally joined (e.g., clauses7-13 in Fig.
1 and clauses 7-11 in Fig.
4) (Labovand Waletzky, 1967; Labov, 1997) Stories also con-tain ORIENTATIONS (setting the scene, describingthe characters), e.g.
utterances 1-6 in Fig.
1.
Anorientation clause introduces the time and place ofthe events of the story, and identifies the participantsof the story and their initial behavior.
To properlyunderstand narrative structure, orientations need tobe identified as a separate type of utterance distinctfrom events.
L&W define two other structural typescalled ABSTRACT and CODA.
The ABSTRACT is aninitial narrative clause summarizing the entire se-quence of events.
A CODA is final clause whichreturns the narrative to the time of speaking, indi-cating the end of the narrative.
The CODA often pro-vides the ?moral?
of the story.The final element of a story according to L&Wis EVALUATION, which L&W identify as essen-tial to every story.
According to L&W, evaluationgives the reason for telling the story, or the point ofthe story: without EVALUATION there is no story,merely a boring recitation of events.
L&W statethat the EVALUATION clauses may also provide in-formation on the consequences of the events as they172relate to the goals and desires of the participants,and can be used to describe the events that did notoccur, may have occurred, or could occur in the fu-ture in the story.
Clauses 14-16 and 18 in Fig.
1provide the narrator?s evaluation of the transpiringevents as well as introducing possible but unreal-ized alternative timelines.
In theories of narrativeidentity (McAdams, 2003; Thorne, 2004), evalua-tion performs two additional functions: (1) it ex-presses the tellers opinion and in doing so reflectsthe value system of that person and their commu-nity; (2) it constructs and maintains relations be-tween the teller and the listener.
Clauses 20 and 22illustrate these functions where the narrator directlyreveals his affective response to the prior events.3 DatasetCorpus of Personal Stories.
Previous work (Gor-don and Swanson, 2009) showed that about 5% ofall weblog entries are personal stories describing anevent in the author?s daily life.
They developed anautomatic classifier for identifying personal narra-tives from a random sample of 5,000 posts froma corpus of 44 million entries available as part ofthe ICWSM 2010 dataset challenge (Burton et al.,2009).
229 of these posts were manually labeled aspersonal stories.
Our experiments are based on 50of these 229 stories.Annotation Process.
L&W?s theory applies to sub-sentence discourse units in a narrative.
It is anopen question what level of phrasal granularity isappropriate to apply to written narratives.
Here, wetreat each independent clause as the basic unit ofdiscourse and manually segment each story in ourdataset using this definition.
This results in a collec-tion of 1,602 independent clauses.
We then dividedthe 50 stories into 4 groups and annotated them inbatches among 3 annotators in order to refine ourannotation guidelines and process.
This dataset isfreely available at https://nlds.soe.ucsc.edu/lw.Previous work has applied L&W?s theory to Ae-sop?s fables and achieved high levels of interanno-tator agreement and extremely high machine learn-ing accuracies (Rahimtoroghi et al., 2013).
How-ever personal narratives clearly provide a more chal-lenging context for annotation.
There was a highlevel of disagreement after the initial round of an-notation.
We found at least 6 primary sources ofdisagreements:?
Clauses of more than one category are commonwith rising action and evaluation, e.g.
a clausecontaining elements of orientation, action, andevaluation: After leaving the apartment at 6:45AM, flying 2 hours, taking a cab to Seattle, andthen driving seven hours up to Whistler includinga border crossing, it?s safe to say that I felt prettymuch like a dick with legs.?
Actions that are implied but not explicitly statedin the text.?
Stative descriptions of the world as a result of anaction that are not intuitively orientation.?
Stative descriptions of the world that are localizedto a specific place in the narrative, which is prob-lematic to L&W?s definition of orientation.?
Subjective clauses in scene setting are usually ori-entation, but are lexically similar to evaluation.?
Disambiguating the functional purpose of clausesthat describe actions, but may be intended to setthe scene as opposed to the rising action.?
Disambiguating the functional purpose of subjec-tive language in the description of an event orstate, e.g., The gigantic tree outside my window,The radiant blue of the sky.After several rounds of annotation we stabilizedon a labeling scheme that hierarchically extendsthe original L&W categories, along with annotationguidelines that annotators could use to disambiguaterecurring problematic cases.
The final set of ex-tended category labels along with two reduced hi-erarchical mappings are shown in Table 1.STATIVE-LOCAL CONTEXT is a category for dis-tinguishing stative descriptions of the world, that arenot intuitively orientation.
For example:?
I saw the sign I expected to turn south on Hwy138.
The traffic sign pointed to Sutherlin andRoseburg,The clause in italics is a stative that describes thesign seen in the previous action.
It is clearly not anaction or evaluation, but is not intuitively an orien-tation, because it is so locally dependent.STATIVE-IMPLIED ACTIONS are clauses, whichdo not explicitly mention an action or event, but im-ply one that is necessary to maintain the causal ortemporal coherence of the remaining story.
For ex-ample: After that, we decided to walk some more.In the context of the story it is necessary to knowthat they actually did walk some more in order tointerpret the other actions described in the narrative.Implied actions are often passive constructions thatdescribe a state of the world that could only be true173Label Set ?
LabelsExtended 0.582 ?
Story Orient Action Eval Local Context Implied Action ConsequenceStative 0.597 ?
Story Orient Action Eval Stative Stative StativeL&W 0.630 ?
Story Orient Action Eval Orient Action EvalTable 1: The extended L&W label categories and two reduced hierarchical mappings.if an action had taken place.
For example: We wereat the convention center in about 10 minutes.STATIVE-CONSEQUENCE is a category that de-scribes the state of the world that has resulted asa consequence of an action, but does not directlyevaluate the goals, intentions or desires of the par-ticipants.
For example, clause 23 in Fig.
1.Using this extended label set we were able toachieve an inter-annotator agreement between the3 annotators of 0.582 using Fleiss?
?
on assign-ing categories to clauses.
We also mapped the fullset of labels to a smaller subsets to see if the finergrained distinctions helped improve reliability onmore coarse grained labeling schemes.
The ex-tended labels we included were generally differenttypes of stative descriptions of the world, whichwere all mapped to a single category for the Stativelabel set.
Finally, we mapped each extended labelto an original L&W category that we thought bestfit the original definitions.
When mapping back tothese reduced label sets we were able to increase the?
to 0.597 for the stative set and 0.630 for the orig-inal L&W categories.
This result indicates that wecan achieve higher reliability by ensuring that theannotators think carefully about particular kinds ofdistinctions between different stative clauses.Gold standard labels were selected based on a sim-ple majority of the annotator assignments.
When noannotators agreed on a label, one of the selected la-bels was chosen at random.
Once completed therewere 424 action clauses, 702 evaluations, 26 not sto-ries, 306 orientation, 17 stative consequences, 12implied actions and 115 local contexts.
Note thatEVALUATION and ORIENTATION clauses that wouldnot be distinguished from ACTION by previous workconstitute two thirds of the clauses.4 ExperimentsThe triply annotated dataset described above wasused as training and test data for experiments onlearning to automatically label narrative clauses.
40narratives were randomly selected to be used astraining and development data and the remaining10 narratives for test data.
The average story inthe training data had 29.3 clauses with the shortestFeature Set DescriptionLinguistic Parts of Speech, Number of Charac-ters in post, Average Word Length,Unigrams, BigramsLexical and Senti-ment CategoriesLIWC counts and frequencies, nega-tionStory Position First Clause, Last Clause, Position inthe story binned into ten story regionsTable 2: Feature Sets for L&W Classification.story consisting of 4 and the maximum consistingof 100.
The average story in the test data had 43clauses with the shortest story consisting of 4 andthe maximum consisting of 114.To derive feature representations of each type ofnarrative clause we started with the features pre-sented in (Rahimtoroghi et al., 2013).
We refinedthese by examining L&W?s descriptions of distin-guishing features of each category.
Table 2 summa-rizes the features we automatically extracted fromall narrative clauses in the weblogs.First, we used the Stanford Parser to distinguishindependent and dependent clauses and kept trackseparately of features that occurred in both typesof clause.
This is because L&W state that theunit of analysis should be an independent clausewith its subordinate clauses, but we felt that thesewere exactly the cases that often caused difficul-ties during annotation.
However distinguishingbetween the features occurring in the two clausetypes would allow us to determine if and whenthe features of the subordinate clause were rele-vant or more informative for automatic classifica-tion.
Then, within both dependent and independentclauses, we distinguished the part-of-speech of themain verb (POS), whether the clause contained anegation (Negate), lexical semantic categories fromLIWC (Pennebaker et al., 2001), dependency rela-tions (DEP), lexical unigrams (STEM), and whetherthe verb was one of a class of verbs that are likely tobe stative.We also developed a set of features describing therelative position of the clause in the story (Bin-Position, FirstClause, LastClause), because differ-ent story regions are often associated with different174Feature Gain Act Ori Eval ?POS:IND-VBD 0.128 0.084 0.002 0.031 0.011BinPosition0 0.076 0.017 0.042 0.014 0.003FirstClause 0.044 0.010 0.019 0.011 0.003POS:IND-VBZ 0.042 0.029 0.008 0.002 0.003IND-Negate 0.040 0.025 0.000 0.013 0.002IND-Copula 0.039 0.030 0.004 0.005 0.001POS:IND-: 0.036 0.001 0.000 0.002 0.033IND-FirstPerson 0.035 0.017 0.004 0.002 0.013IND-LIWC Motion 0.034 0.021 0.003 0.006 0.004POS:IND-VBP 0.033 0.023 0.001 0.007 0.002Table 3: The 10 most highly correlated features witheach label and cumulatively over all the labels usingmutual information and information gain.clause types.
For example, in Fig.
1 and Fig.
4, thebeginning of the story contains more ORIENTATIONclauses, while ACTION clauses are concentrated inthe middle of the story.
The EVALUATION clausestypically occur part-way through the story wherethey provide the narrator?s reaction to story events.See Table 2.In total there were 3,510 unique binary valued fea-tures extracted from our training dataset.
We usedmutual information to find the features that had thehighest correlation with each category and the in-formation gain over all the labels.
The 10 highestvalued features are in Table 3, e.g.
the top feature iswhen the part-of-speech (POS) of the main verb inthe independent clause (IND) is past tense (VBD).This feature encoding was used for machine learn-ing experiments with classification algorithms fromMallet (McCallum, 2002): Naive Bayes (NB) (Wit-ten and Frank, 2005), Confidence Weighted LinearClassifier (CWLC) (Dredze et al., 2008), MaximumEntropy (ME)(Witten and Frank, 2005) and a se-quential classifier (CRF) (Lafferty et al., 2001).5 Evaluation and ResultsWe evaluate the performance of our classifiers withexperiments using the 50 annotated stories.
Us-ing the 40 stories in the training set we calculatedthe information gain for each feature (see Table 3).For each subset of the highest valued features (inthe range of 22-212), we performed a 10-fold cross-validation on the training data and assessed the per-formance of each classifier to find the right numberof features.
Within each fold of the cross-validationwe also perform a simple grid search for the optimalhyper-parameters of the model (e.g., the prior in theME and CRF models) using only the data within thetraining fold.The feature selection experimental results areExtended Stative L&WClassifier # F1 # F1 # F1CRF 270.61 290.61 270.65CWLC 2110.67 2110.68?2110.73?ME 2110.67 2100.68?2100.73?NB 290.68?290.70?2100.76?Table 4: The optimal number of features found foreach model and the average F-score obtained usinga 10-fold cross-validation on the training data.shown in Table 4.
We report the optimal numberof features and the corresponding macro F-score,weighted by the relative frequency of each category,for each algorithm and label set.
For all algorithms,performance increases for label sets with higher lev-els of abstraction.
The Naive Bayes and CRF mod-els perform better with a small subset of the fea-tures, while the ME and CWLC algorithms use amuch larger subset.
Surprisingly the sequential clas-sifier has the lowest F-score and Naive Bayes per-forms the best.
A?indicates a significant improve-ment over CRF at p < 0.05 using a two-sided t-test.No other differences were significant.Using the optimal number of features obtainedfrom this search we trained a model for each algo-rithm using the entire training dataset and selectingthe hyper-parameters as before.
We applied thesemodels to the unseen test data and evaluated the per-formance of each classifier as applied to the entireset of clauses and to individual narratives.We first computed the precision, recall and F-scoreaggregated over all the clauses in the test set.
Ta-ble 5 summarizes the results for each classifier andlabel set.
The left hand side of the table shows themacro precision, recall and F-score weighted by therelative frequency of each label.
The right handside of the table shows the F-score of each indi-vidual label separately.
On this evaluation, NaiveBayes outperforms all other methods on all labelsets.
Overall, precision and recall are relatively bal-anced achieving a maximum F-score of 0.689 whenthe labels are mapped back to the original L&W cat-egories.
The CRF does surprisingly well consider-ing its poor performance during the feature selectionsearch.
The classifiers perform the poorest on orien-tation clauses and the best on evaluation clauses.As mentioned above, the annotation task is highlysubjective, requiring interpreting the narrative andthe author?s intention, which prevents us from ob-taining high levels of inter-rater agreement.
Becauseof the noise in the annotations, the standard evalua-175Overall Per LabelL&W StativeLabel Set Model Prec Rec F1 ?
Ori Eva Act ?
Imp Loc ConExtendedCRF 0.568 0.626 0.593 0.419 0.532 0.727 0.651 0.000 0.000 0.041 0.000CWLC 0.567 0.616 0.582 0.398 0.377 0.763 0.612 0.000 0.000 0.087 0.000ME 0.597 0.649 0.614 0.450 0.496 0.767 0.667 0.000 0.000 0.085 0.000NB 0.625 0.656 0.623 0.459 0.478 0.781 0.650 0.000 0.000 0.174 0.000StativeCRF 0.563 0.591 0.574 0.370 0.412 0.695 0.628 0.000 0.235CWLC 0.572 0.621 0.587 0.403 0.417 0.760 0.614 0.000 0.077ME 0.610 0.644 0.611 0.441 0.464 0.759 0.673 0.000 0.118NB 0.650 0.667 0.638 0.477 0.496 0.779 0.676 0.000 0.226L&WCRF 0.650 0.665 0.656 0.468 0.556 0.742 0.640 0.000CWLC 0.624 0.647 0.632 0.424 0.480 0.747 0.609 0.000ME 0.681 0.700 0.688 0.517 0.580 0.780 0.670 0.000NB 0.687 0.705 0.689 0.514 0.565 0.780 0.687 0.000Table 5: The performance of each of classifier on the test set when all clauses are aggregated together.Agreement Total # Prec Rec F1 ?
Ori Eva Act ?1 of 3 15 0.333 0.400 0.339 0.069 0.000 0.625 0.333 0.0002 of 3 146 0.597 0.610 0.580 0.405 0.472 0.700 0.622 0.0003 of 3 269 0.770 0.773 0.767 0.607 0.667 0.824 0.746 0.000All 430 0.687 0.705 0.689 0.514 0.565 0.780 0.687 0.000Adjusted 430 0.646 0.660 0.643 0.447 0.516 0.745 0.623 0.000Table 6: Performance measures for different levels of agreement among the annotators.tion metrics may hide information about the abilityof the classifiers to learn from our feature set.
Forexample, the best performing classifier (NB) incor-rectly labeled 127 clauses out of 430 possible in thetest set.
However, 44 of these errors agreed withat least one annotator, but were counted as entirelyincorrect in the previous evaluations.To address these concerns we also evaluated theperformance of the the best performing classifierbased on the level of agreement of each instance us-ing two different approaches.
See Table 6.
The firstapproach was inspired by the approach in (Louisand Nenkova, 2011) where the clauses in the testset are binned based on the number of annotatorswho agreed with the gold standard label.
The per-formance is then calculated for each bin.
The firstthree rows of Table 6 show the performance for thedifferent levels of agreement in the dataset.
Therewere only 15 clauses in the test set where there wasno agreement at all.
It is unsurprising that whenthe annotators could not agree on a label the sys-tem performed near chance levels.
However, whenall three annotators agreed on the gold standardlabel the F-score improved to 0.767.
As a compar-ison, the F-score of the entire test set was 0.689 asshown in the row labeled All.Our second approach is based on the proposal ofTetreault et al.
(Tetreault et al., 2013).
They intro-Label Set Model Min Max Mean ?
CIExtendedCRF 0.333 0.763 0.540 ?
0.080CWLC 0.276 0.763 0.582 ?
0.099ME 0.333 0.753 0.572 ?
0.088NB 0.333 0.741 0.573 ?
0.093StativeCRF 0.298 0.762 0.521 ?
0.099CWLC 0.345 0.758 0.591 ?
0.090ME 0.333 0.753 0.562 ?
0.098NB 0.333 0.758 0.582 ?
0.088L&WCRF 0.333 0.837 0.609 ?
0.097CWLC 0.458 0.877 0.658 ?
0.081ME 0.333 0.830 0.649 ?
0.095NB 0.333 0.851 0.647 ?
0.096Table 7: Summary statistics of the F-score, with95% confidence intervals, when evaluating stories.duce a modification to the standard precision, re-call and F-scores that takes into account the levelof agreement of each instance, where the values oftrue-positives and false-negatives are assigned frac-tional counts based on the proportion of annotatorswho assigned that label.
The final row of Table 6provides the results using these adjusted values.We also investigated the performance of the classi-fiers when evaluating each story separately.
Table 7summarizes these results.
Each classifier was ap-plied to the clauses of the 10 narratives in the testset and the F-score was computed for each narrativeindividually.
The table shows the minimum, maxi-mum and average F-score with 95% confidence in-176ll ll ll l ll l l0.500.550.600.650.700.75250 500 750 1000# Training ClausesF?ScoreModell CRFCWLCMaxEntNBFigure 2: Learning curves of the Naive Bayes clas-sifier using the optimal number of features.tervals over the 10 narratives.The CWLC performed the best on this test andthe performance of all the algorithms generally im-proved using the higher-level label sets.
The resultsalso show that there is a high variance in perfor-mance between stories, with a minimum F-score of0.458 and a maximum of 0.877 for the CWLC onthe L&W label set.
This indicates that some clausesare ambiguous and difficult to label, but also thatsome stories are more difficult to classify.To assess whether more annotated data could im-prove performance, we ran a series of learningcurves in Fig.
2.
Only the training data was usedfor these experiments.
The curves were created byrandomly sampling 90% of the data for training and10% for testing.
A model was trained, using thesame process as above, on successively larger sub-sets of the data and applied to the 10% held outclauses.
This process was repeated 10 times and themean F-Score is reported.
In nearly all cases, theperformance of classifiers is still increasing whenall of the data is used indicating that we have notexhausted the expressive power of our features andmore annotated data would be useful.
However, wealso see we can reach about 93% of our maximumperformance with only a few hundred examples.
Weplan to increase the size of our annotated data set infuture work.5.1 Error AnalysisOur results to date indicate that we achieve an over-all F-score of 0.689, and that our classifiers are mostaccurate for the evaluation and action categories.See Table 6.
Fig.
3 presents a confusion matrix0036052124209682601712183EvaluationActionOrientationNot StoryEvaluation Action Orientation Not StoryGoldPredictedFigure 3: Confusion matrix for the best classifier.showing the frequency of predicted labels againstthe gold standard labels for the Naive Bayes classi-fier on the L&W label set.
With the exception of notstory there are cases of confusion between all cate-gories.
However, in the vast majority of cases bothaction and orientation are confused with evaluationand the classifier overpredicts evaluation.We also categorized errors for the Naive Bayesclassifier into the the 4 sources of errors in the pre-dictions shown in Table 8.
The most common errorsinvolved clauses with lexical INDICATORS that arehighly correlated with one category, but in the con-text and interpretation of the story actually functionas a different type.
For example, unfortunately,could and n?t are all strong indicators of evaluation,but in this case the primary function of the clauseis to set the scene for the rest of the story, i.e., ori-entation.
The interpretation of these clauses is clearto a human, despite the lexical items misleading theclassifier.Another source of error is when the function of theclause in the narrative is ambiguous (PURPOSE inTable 8).
While there may be some misleading lex-ical indicators in these clauses, there were often nostrongly correlated words, such as the adjectives andmodal verbs in EVALUATIONS.
The distinction inthese cases is that the primary function of the clausewithin the story is unclear, even to a human reader.Unsurprisingly, most of the examples in this cate-gory had low inter-rater agreement.Some of the clauses contain figurative languageor complex constructions that require a significantamount of world knowledge and INFERENCE to in-terpret.
For example, understanding the INFERENCEclause in Table 8 requires recognizing the metaphorabout rabbit food in order to identify the subjectiveevaluation the narrator is making.There are also cases of clauses that contain MULTI-PLE categories, at least partially because of the gran-ularity of our segmentation.
In the example in Ta-ble 8 a new character, Alejandrio, is introduced anda rising action is described, trekking to the waterfall.177Error Type Freq Gold Pred ExampleIndicators 57 Ori Eva So, unfortunately I couldn?t make the Gamesindustry.biz party tonight.Purpose 20 Ori Eva I know it is a remarkable haircut because on the way home a handsome young Mo-roccan man nearly died to tell me how beautiful I was.Inference 6 Eva Ori That?s that rabbit food that all of those Northeastern Kerry voters...Multiple 4 Act Ori We trekked to a waterfall in the park with the help of Alejandrio a 65 year oldHonduran guy who not only walked quicker than us but also carried all the water.Unclear 39 Ori Eva We have diners out east,Not Story 7 Not Act scroll down to the Hobbit post,Table 8: Several common sources of errors with a prototypical example.Our annotation guidelines instructed us to prefer ac-tions in these types of clauses, however, both ORI-ENTATION and ACTION are present in this situation.There were also 39 clauses that were labeled in-correctly that had no clear reason (UNCLEAR) forbeing mislabelled.
We also explicitly excluded the7 clauses marked not part of the story.The types of errors described above are not mutu-ally exclusive and in some cases are causally related.For example, the purpose of a clause may be am-biguous because it contains conflicting lexical indi-cators.
Similarly, a clause containing multiple cate-gories will likely have strong lexical indicators fromeach of these categories so that the classification al-gorithms cannot disambiguate among possible la-bels.
This might be improved by more data, moresophisticated semantic features, or possibly an anal-ysis focused on discourse relations, such as those inthe PDTB (Louis et al., 2010; Prasad et al., 2008),or Elson?s STORY INTENTION GRAPH (Rishes et al.,2013; Elson and McKeown, 2010; Elson, 2012).6 DiscussionThis paper describes work on categorization of nar-rative clauses based on Labov & Waletzky?s theoryof oral narrative, applied to personal narratives writ-ten by ordinary people.
We show that we can auto-matically classify narrative clauses with these cate-gories achieving an overall F-score of 0.689, whichis substantially higher than a random (0.250) or ma-jority class (0.437) baseline, which increases to anF-score of .767 on the cases where all three annota-tors agreed.
The learning curves plotted in Fig.
2clearly suggest that more training data would bebeneficial before we investigate more complex fea-tures and learning algorithms.We believe the ability to automatically perform thistype of simple narrative analysis will enable andimprove many other types of deeper narrative un-derstanding (Rahimtoroghi et al., 2014; Hu et al.,2013).
For example, causal and temporal relation-ship extraction methods that focus only on clauses inthe same action sequence be more accurate, becausethey exclude disconnected events from the orienta-tion or evaluation sections.
This type of analysiswill also enable new methods for learning attitudesand values of societal groups based on the differentaffective evaluations that are provided in responseto action clauses.Our results also highlight several properties of thedata.
Performance is different for results by storyrather than over all clauses.
This indicates thatsome stories are more difficult to classify than oth-ers and that ambiguous clauses are not uniformlydistributed but are likely to be correlated with par-ticular authors or writing styles.
In other work, wehave started to investigate whether we can automat-ically rate the temporal coherence of personal narra-tives (Ryan et al., 2014).
We can use this to identifystories with utterances that are likely to be difficultto classify because of the poor quality of the narra-tive input.
These cases are unlikely to have usablenarrative structure.AcknowledgmentsThis research was supported by NSF Grants IIS#1002921 and IIS #123855.
The content of thispublication does not necessarily reflect the positionor policy of the government, and no official en-dorsement should be inferred.Appendix ASee Fig.
4 for an additional example labelled withL&W Categories.178#CategoryStory Clause1AbstractToday was a very eventful work day.2OrientationToday was the start of the G20 summit.3OrientationIt happens every year4Orientationand it is where 20 of the leaders of the world come together to talk about how to run theirgovernments effectively and what not.5OrientationSince there are so many leaders coming together their are going to be a lot of people whohave different views on how to run the government they follow so they protest.6OrientationThis week things started alright and on schedule.7ActionThere was a protest that happened along the street where I work8Actionand at first it looked peaceful until a bunch of people started rebelling9Actionand creating a riot.10ActionPolice cars were burned11Actionand things were thrown at cops.12OrientationPolice were in full riot gear to alleviate the violence.13ActionAs things got worse tear gas and bean bag bullets were fired at the rioters14Actionwhile they smash windows of stores.15EvaluationAnd this all happened right in front of my store16Evaluationwhich was kind of scary17Evaluationbut it was kind of interesting18Codasince I?ve never seen a riot before.Figure 4: A personal narrative about a protest, with narrative categories of Labov & Waletzky, 1967.ReferencesMichael Bamberg.
2006.
Stories: Big or small: Why dowe care?
Narrative inquiry, 16(1):139?147.Brandon Beamer and Roxana Girju.
2009.
Using a bi-gram event model to predict causal potential.
In Com-putational Linguistics and Intelligent Text Processing,p.
430?441.
Springer.Jennifer G Bohanek, Kelly A Marin, and Robyn Fivush.2008.
Family narratives, self, and gender in earlyadolescence.
The Journal of Early Adolescence,28(1):153?176.Jerome Bruner.
1991.
The narrative construction of re-ality.
Critical Inquiry, 18:1?21.Kevin Burton, Akshay Java, and Ian Soboroff.
2009.The ICWSM 2009 spinn3r dataset.
In Proc.
of the ThirdAnnual Conf.
on Weblogs and Social Media.N.
Chambers and D. Jurafsky.
2009.
Unsupervisedlearning of narrative schemas and their participants.
InProc.
of the 47th Annual Meeting of the ACL, p. 602?610.Quang Xuan Do, Yee Seng Chan, and Dan Roth.
2011.Minimally supervised event causality identification.
InProc.
of the Conf.
on Empirical Methods in NaturalLanguage Processing, p. 294?303.Mark Dredze, Koby Crammer, and Fernando Pereira.2008.
Confidence-weighted linear classification.
InProc.
of the 25th international conference on Machinelearning, p. 264?271.
ACM.D.K.
Elson and K.R.
McKeown.
2010.
Building a bankof semantically encoded narratives.
In Proc.
of the Sev-enth International Conf.
on Language Resources andEvaluation (LREC 2010).David K Elson.
2012.
Detecting story analogies fromannotations of time, action and agency.
In Proc.
ofthe LREC 2012 Workshop on Computational Models ofNarrative, Istanbul, Turkey.Robyn Fivush and Katherine Nelson.
2004.
Culture andlanguage in the emergence of autobiographical mem-ory.
Psychological Science, 15(9):573?577.Robyn Fivush, Jennifer G Bohanek, and Marshall Duke.2005.
The intergenerational self: Subjective perspec-tive and family history.
Individual and collective self-continuity.
Mahwah, NJ: Erlbaum.R.J.
Gerrig.
1993.
Experiencing narrative worlds: Onthe psychological activities of reading.Andrew Gordon and Reid Swanson.
2009.
Identifyingpersonal stories in millions of weblog entries.
In ThirdInternational Conf.
on Weblogs and Social Media, DataChallenge Workshop.Andrew Gordon, Cosmin Bejan, and Kenji Sagae.
2011.Commonsense causal reasoning using millions of per-sonal stories.
In Twenty-Fifth Conf.
on Artificial Intelli-gence (AAAI-11).Amit Goyal, Ellen Riloff, and Hal Daum?e III.
2010.
Au-tomatically producing plot unit representations for nar-rative text.
In Proc.
of the 2010 Conf.
on EmpiricalMethods in Natural Language Processing, p. 77?86.Arthur C Graesser, Murray Singer, and Tom Trabasso.1994.
Constructing inferences during narrative textcomprehension.
Psychological review, 101(3):371.179T.
Habermas and S. Bluck.
2000.
Getting a life: theemergence of the life story in adolescence.
PsycholBull, 126(5):748?69.Zhichao Hu, Elahe Rahimtoroghi, Larissa Munishkina,Reid Swanson, and Marilyn A Walker.
2013.
Unsu-pervised induction of contingent event pairs from filmscenes.
In Proc.
of Conf.
on Empirical Methods in Nat-ural Language Processing, p. 370?379.W.
Labov and J. Waletzky.
1967.
Narrative analysis:Oral versions of personal experience.
In J.
Helm, ed.,Essays on the Verbal and Visual Arts, p. 12?44.W.
Labov.
1997.
Some further steps in narrative analy-sis.
Journal of narrative and life history, 7:395?415.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.Wendy G Lehnert.
1981.
Plot units and narrative sum-marization.
Cognitive Science, 5(4):293?331.Annie Louis and Ani Nenkova.
2011.
Automatic identi-fication of general and specific sentences by leveragingdiscourse annotations.
In International Joint Conf.
onNatural Language Processing, p. 605?613.Annie Louis, Aravind Joshi, Rashmi Prasad, and AniNenkova.
2010.
Using entity features to classify im-plicit relations.
In Proc.
of the 11th Annual SIGdialMeeting on Discourse and Dialogue.Mehdi Manshadi, Reid Swanson, and Andrew S Gor-don.
2008.
Learning a probabilistic model of eventsequences from internet weblog stories.
In Proc.
of the21st FLAIRS Conf.
.Dan P McAdams.
2003.
Identity and the life story.
Au-tobiographical memory and the construction of a nar-rative self: Developmental and cultural perspectives,9:187?207.Andrew Kachites McCallum.
2002.
MAL-LET: a machine learning for language toolkit.http://mallet.cs.umass.edu.K.C.
McLean and A. Thorne.
2003.
Adolescents?
self-defining memories about relationships.
DevelopmentalPsychology, (39):635?645.J.
W. Pennebaker, M. E. Francis, and R. J. Booth.
2001.Inquiry and Word Count: LIWC 2001.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and Bonnie Web-ber.
2008.
The Penn Discourse TreeBank 2.0.
In Proc.of 6th International Conf.
on Language Resources andEvaluation (LREC 2008).Michael W Pratt and Barbara H Fiese.
2004.
Families,Stories, and the Life Course: An Ecological Context.Elahe Rahimtoroghi, Reid Swanson, and Marilyn A.Walker.
2013.
Evaluation, orientation, and action in in-teractive storytelling.
In Proc.
of Intelligent NarrativeTechnologies 6.Elahe Rahimtoroghi, Thomas Corcoran, Reid Swanson,Marilyn A. Walker, Kenji Sagae, and Andrew S. Gor-don.
2014.
Minimal narrative annotation schemesand their applications.
In Proc.
of Intelligent NarrativeTechnologies 7.Mehwish Riaz and Roxana Girju.
2010.
Another lookat causality: Discovering scenario-specific contingencyrelationships with no supervision.
In Semantic Com-puting (ICSC), p. 361?368.
IEEE.Elena Rishes, Stephanie Lukin, David K. Elson, andMarilyn A. Walker.
2013.
Generating dierent storytellings from semantic representations of narrative.
InInt.
Conf.
on Interactive Digital Storytelling, ICIDS?13.James Owen Ryan, Marilyn A. Walker, and NoahWardrip-Fruin.
2014.
Toward recombinant dialoguein interactive narrative.
In 7th Workshop on IntelligentNarrative Technologies.Nancy L. Stein, Tom Trabasso, and Maria D. Liwag.2000.
A goal appraisal theory of emotional understand-ing: Implications for development and learning.
In M.Lewis and J. M. Haviland-Jones, ed, Handbook of emo-tions (2nd ed.
), p. 436?457.Joel Tetreault, Martin Chodorow, and Nitin Madnani.2013.
Bucking the trend: improved evaluation and an-notation practices for esl error detection systems.
Lan-guage Resources and Evaluation, p. 1?27.A.
Thorne and V. Nam.
2009.
The storied constructionof personality.
In Kitayama S. and Cohen D., ed, Hand-book of Cultural Psychology, p. 491?505.A.
Thorne and L. A. Shapiro.
2011.
Testing, testing:Everyday storytelling and the construction of adoles-cent identity.
Adolescent Vulnerabilities and Opportu-nities: Developmental and Constructivist Perspectives,38:117.A.
Thorne, N. Korobov, and E. M. Morgan.
2007.
Chan-neling identity: A study of storytelling in conversationsbetween introverted and extraverted friends.
Journal ofresearch in personality, 41(5):1008?1031.Avril Thorne.
2004.
Putting the person into social iden-tity.
Human Development, 47(6):361?365.Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical machine learning tools and techniques.
Mor-gan Kaufmann, San Francisco, CA.180
