Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1007?1016,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPLearning Linear Ordering Problems for Better Translation?Roy TrombleGoogle, Inc.4720 Forbes Ave.Pittsburgh, PA 15213royt@google.comJason EisnerDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218jason@cs.jhu.eduAbstractWe apply machine learning to the Lin-ear Ordering Problem in order to learnsentence-specific reordering models formachine translation.
We demonstrate thateven when these models are used as a merepreprocessing step for German-Englishtranslation, they significantly outperformMoses?
integrated lexicalized reorderingmodel.Our models are trained on automaticallyaligned bitext.
Their form is simple butnovel.
They assess, based on features ofthe input sentence, how strongly each pairof input word tokens wi, wjwould liketo reverse their relative order.
Combiningall these pairwise preferences to find thebest global reordering is NP-hard.
How-ever, we present a non-trivial O(n3) al-gorithm, based on chart parsing, that atleast finds the best reordering within a cer-tain exponentially large neighborhood.
Weshow how to iterate this reordering processwithin a local search algorithm, which weuse in training.1 IntroductionMachine translation is an important but difficultproblem.
One of the properties that makes it dif-ficult is the fact that different languages expressthe same concepts in different orders.
A ma-chine translation system must therefore rearrangethe source language concepts to produce a fluenttranslation in the target language.1This work is excerpted and adapted from the first au-thor?s Ph.D. thesis (Tromble, 2009).
Some of the ideas hereappeared in (Eisner and Tromble, 2006) without empiricalvalidation.
The material is based in part upon work sup-ported by the National Science Foundation under Grant No.0347822.Phrase-based translation systems rely heavilyon the target language model to ensure a fluentoutput order.
However, a target n-gram languagemodel alone is known to be inadequate.
Thus,translation systems should also look at how thesource sentence prefers to reorder.
Yet past sys-tems have traditionally used rather weak models ofthe reordering process.
They may look only at thedistance between neighboring phrases, or dependonly on phrase unigrams.
The decoders also relyon search error, in the form of limited reorderingwindows, for both efficiency and translation qual-ity.Demonstrating the inadequacy of such ap-proaches, Al-Onaizan and Papineni (2006)showed that even given the words in the referencetranslation, and their alignment to the sourcewords, a decoder of this sort charged with merelyrearranging them into the correct target-languageorder could achieve a BLEU score (Papineni etal., 2002) of at best 69%?and that only whenrestricted to keep most words very close to theirsource positions.This paper introduces a more sophisticatedmodel of reordering based on the Linear Order-ing Problem (LOP), itself an NP-hard permutationproblem.
We apply machine learning, in the formof a modified perceptron algorithm, to learn pa-rameters of a linear model that constructs a matrixof weights from each source language sentence.We train the parameters on orderings derived fromautomatic word alignments of parallel sentences.The LOP model of reordering is a completeordering model, capable of assigning a differentscore to every possible permutation of the source-language sentence.
Unlike the target languagemodel, it uses information about the relative posi-tions of the words in the source language, as wellas the source words themselves and their parts ofspeech and contexts.
It is therefore a language-pairspecific model.1007We apply the learned LOP model as a prepro-cessing step before both training and evaluation ofa phrase-based translation system, namely Moses.Our methods for finding a good reordering un-der the NP-hard LOP are themselves of interest,adapting algorithms from natural language parsingand developing novel dynamic programs.Our results demonstrate a significant improve-ment over translation using unreordered German.Using Moses with only distance-based reorderingand a distortion limit of 6, our preprocessing im-proves BLEU from 25.27 to 26.40.
Furthermore,that improvement is significantly greater than theimprovement Moses achieves with its lexicalizedreordering model, 25.55.Collins et al (2005) improved German-Englishtranslation using a statistical parser and severalhand-written rules for preprocessing the Germansentences.
This paper presents a similar improve-ment using fully automatic methods.2 A Linear Ordering ModelThis section introduces a model of word reorder-ing for machine translation based on the LinearOrdering Problem.2.1 FormalizationThe input sentence is w = w1w2.
.
.
wn.
To dis-tinguish duplicate tokens of the same word, we as-sume that each token is superscripted by its inputposition, e.g., w = die1Katze2hat3die4Frau5gekauft6(gloss: ?the cat has the woman bought?
).For a fixedw, a permutation pi = pi1pi2.
.
.
pinisany reordering of the tokens in w. The set ?nofall such permutations has size n!.
We would like todefine a scoring model that assigns a high score tothe permutationpi = die4Frau5hat3gekauft6die1Katze2(gloss: ?the woman has bought the cat?
),since that corresponds well to the desired Englishorder.To construct a function that scores permutationsof w, we first construct a pairwise preference ma-trix Bw?
Rn?n, whose entries areBw[`, r]def= ?
?
?
(w, `, r), (1)Here ?
is a vector of weights.
?
is a vector offeature functions, each considering the entire wordsequencew, as well as any functions thereof, suchas part of speech tags.We will hereafter abbreviate Bwas B.
Its inte-ger indices ` and r are identified with the input to-kensw`andwr, and it can be helpful to write themthat way; e.g., we will sometimes write B[2, 5] asB[Katze2,Frau5].The idea behind our reordering model isthat B[Katze2,Frau5] > B[Katze5,Frau2] ex-presses a preference to keep Katze2before Frau5,whereas the opposite inequality would express apreference?other things equal?for permutationsin which their order is reversed.
Thus, we define1score(pi)def=?i,j: 1?i<j?nB[pii, pij] (2)p(pi)def=1Zexp(?
?
score(pi)) (3)?pidef= argmaxpi?
?nscore(pi) (4)Note that i and j denote positions in pi, whereaspii, pij, `, and r denote particular input tokens suchas Katze2and Frau5.2.2 DiscussionTo the extent that the costs B generally discour-age reordering, they will particularly discouragelong-distance movement, as it swaps more pairsof words.We point out that our model is somewhat pecu-liar, since it does not directly consider whether thepermutation pi keeps die4and Frau5adjacent oreven close together, but only whether their orderis reversed.Of course, the model could be extended to con-sider adjacency, or more generally, the three-waycost of interposing k between i and j.
See (Eis-ner and Tromble, 2006; Tromble, 2009) for suchextensions and associated algorithms.However, in the present paper we focus on themodel in the simple form (2) that only considerspairwise reordering costs for all pairs in the sen-tence.
Our goal is to show that these unfamiliarpairwise reordering costs are useful, when mod-eled with a rich feature set via equation (1).
Evenin isolation (as a preprocessing step), without con-sidering any other kinds of reordering costs or lan-guage model, they can achieve useful reorderings1For any ` < r, we may assume without loss of gener-ality that B[r, `] = 0, since if not, subtracting B[r, `] frombothB[`, r] andB[r, `] (exactly one of which appears in eachscore(pi)) will merely reduce the scores of all permutationsby this amount, leaving equations (3) and (4) unchanged.Thus, in practice, we take B to be an upper triangular ma-trix.
We use equation (1) only to defineB[`, r] for ` < r, andtrain ?
accordingly.
However, we will ignore this point in ourexposition.1008of German that complement existing techniquesand thus improve state-of-the-art systems.
Ourpositive results in even this situation suggest thatin future, pairwise reordering costs should proba-bly be integrated into MT systems.The probabilistic interpretation (3) of thescore (2) may be useful when thus integrating ourmodel with language models or other reorderingmodels during translation, or simply when train-ing our model to maximize likelihood or minimizeexpected error.
However, in the present paper wewill stick to purely discriminative training and de-coding methods that simply try to maximize (2).2.3 The Linear Ordering ProblemIn the combinatorial optimization literature, themaximization problem (4) (with inputB) is knownas the Linear Ordering Problem.
It has numer-ous practical applications in fields including eco-nomics, sociology, graph theory, graph drawing,archaeology, and task scheduling (Gr?otschel etal., 1984).
Computational studies on real datahave often used ?input-output?
matrices represent-ing resource flows among economic sectors (Schi-avinotto and St?utzle, 2004).Unfortunately, the problem is NP-hard.
Further-more, it is known to be APX-complete, meaningthat there is no polynomial time approximationscheme unless P=NP (Mishra and Sikdar, 2004).However, there are various heuristic proceduresfor approximating it (Tromble, 2009).
We nowgive an attractive, novel procedure, which uses aCKY-parsing-like algorithm to search various sub-sets of ?nin polynomial time.3 Local Search?Local search?
refers to any hill-climbing proce-dure that iteratively improves a solution by mak-ing an optimal ?local?
change at each iteration.2In this case, we start with the identity permutation,find a ?nearby?
permutation with a better score (2),and repeat until we have reached a local maximumof the scoring objective.This section describes a local search procedurethat uses a very generous definition of ?local.?
Ateach iteration, it finds the optimal permutation ina certain exponentially large neighborhood N(pi)of the current permutation pi.2One can introduce randomness to obtain MCMC sam-pling or simulated annealing algorithms.
Our algorithms ex-tend naturally to allow this (cf.
Tromble (2009)).S ?
S0,nSi,k?
Si,jSj,kSi?1,i?
piiFigure 1: A grammar for a large neighborhood ofpermutations, given one permutation pi of lengthn.
The Si,krules are instantiated for each 0 ?i < j < k ?
n, and the Si?1,irules for each0 < i ?
n.We say that two permutations are neighbors iffthey can be aligned by an Inversion TransductionGrammar (ITG) (Wu, 1997), which is a familiarreordering device in machine translation.
Equiva-lently, pi??
N(pi) iff pi can be transformed intopi?by swapping various adjacent substrings of pi,as long as these swaps are properly nested.
Zensand Ney (2003) used a normal form to show thatthe size of the ITG neighborhood N(pi) is a largeSchr?oder number, which grows exponentially inn.
Asymptotically, the ratio between the size ofthe neighborhood for n + 1 and the size for n ap-proaches 3 + 2?2 ?
5.8.We show that equation (2) can be optimizedwithin N(pi) in O(n3) time, using dynamic pro-gramming.
The algorithm is based on CKY pars-ing.
However, a novelty is that the grammarweights must themselves be computed by O(n3)dynamic programming.Our grammar is shown in Figure 1.
Parsingthe ?input sentence?
pi with this grammar simplyconstructs all binary trees that yield the string pi.There is essentially only one nonterminal, S, butwe split it into O(n2) position-specific nontermi-nals such as Si,j, which can only yield the spanpii+1pii+2.
.
.
pij.
An example parse is shown inFigure 2.The important point is that we will place ascore on each binary grammar rule.
The scoreof the rule Si,k?
Si,jSj,kis max(0,?i,j,k),where ?i,j,kis the benefit to swapping the sub-strings pii+1pii+2.
.
.
pijand pij+1pij+2.
.
.
pik.
Therule is considered to be a ?swap rule?
if itsscore is positive, showing that a swap will bebeneficial (independent of the rest of the tree).If the parse in Figure 2 is the parse withthe highest total score, and its swap rules areS0,5?
S0,1S1,5and S3,5?
S3,4S4,5, thenour best permutation in the neighborhood of pimust be the (linguistically desirable) permutationdie4Frau5hat3gekauft6die1Katze2, obtained from1009SS0,6HHHHHHS0,5HHHHS0,1die1S1,5HHHHS1,3HHS1,2die4S2,3Frau5S3,5HHS3,4gekauft6S4,5hat3S5,6Katze2Figure 2: One parse of the current permutation pi.In this example, pi has somehow gotten the inputwords into alphabetical order (owing to previoushill-climbing steps).
We are now trying to furtherimprove this order.pi by two swaps.How do we find this solution?
Clearlythe benefit (positive or negative) to swappingpii+1pii+2.
.
.
pijwith pij+1pij+2.
.
.
pikis?i,j,k=j?`=i+1k?r=j+1B[pir, pi`]?B[pi`, pir] (5)We can evaluate all O(n3) possible swaps in to-tal time O(n3), using the dynamic programmingrecurrence?i,j,k= ?i,j,k?1+ ?i+1,j,k?
?i+1,j,k?1(6)+B[pik, pii+1]?B[pii+1, pik]with the base case ?i,j,k= 0 if i = j or j = k.This gives us the weights for the grammar rules,and then we can use weighted CKY parsing tofind the highest-scoring (Viterbi) parse in O(n3)time.
Extracting our new and improved permuta-tion pi??
N(pi) from this parse is a simple O(n)-time algorithm.Figure 3 gives pseudocode for our local searchalgorithm, showing how to compute the quan-tities (6) during parsing rather than beforehand.?
[i, k] holds the weight of the best permuta-tion (in the neighborhood) of the subsequencepii+1pii+1.
.
.
pik.33The use of ?
is intended to suggest an analogy to insideprobability?or more precisely, the Viterbi approximation toinside probability (since we are maximizing rather than sum-ming over parses).The next two sections describe how to use ourlocal search algorithm to discriminatively learn theweights of the parameters from Section 2, equa-tion (1).4 FeaturesOur objective function (2) works only to the extentthat we can derive a good pairwise preference ma-trix Bw.
We do this by using a rich feature set inequation (1).We adapt the features of McDonald et al(2005), introduced there for dependency parsing,to the task of machine translation reordering.
Be-cause both models construct features for pairs ofwords given the entire sentence, there is a closecorrespondence between the two tasks, althoughthe output is quite different.Each feature ?
(w, `, r) in equation (1) is a bi-nary feature that fires when (w, `, r) has someconjunction of properties.
The properties that areconsidered include the words w`and wr, the partsof speech of {w`?1, .
.
.
, wr+1}, and the distancer ?
`.
Table 1 shows the feature templates.We also tried features based on a dependencyparse of the German, with the idea of using LOPfeatures to reorder the dependents of each word,and thus model syntactic movement.
This didproduce better monolingual reorderings (as in Ta-ble 2), but it did not help final translation into En-glish (Table 3), so we do not report the details here.5 Learning to ReorderIdeally, we would have a large corpus of desir-able reorderings of source sentences?in our case,German sentences permuted into target Englishword order?from which to train the parameters ofour model.
Unfortunately, the alignments betweenGerman and English sentences are only infre-quently one-to-one.
Furthermore, human-alignedparallel sentences are hard to come by, and neverin the quantity we would like.Instead, we make do with automatically-generated word alignments, and we heuristi-cally derive an English-like word order forthe German sentence based on the alignment.We used GIZA++ (Och and Ney, 2003) toalign approximately 751,000 sentences from theGerman-English portion of the Europarl corpus(Koehn, 2005), in both the German-to-English andEnglish-to-German directions.
We combined the10101: procedure LOCALSEARCHSTEP(B,pi, n)2: for i?
0 to n?
1 do3: ?
[i, i+ 1]?
04: for k ?
i+ 1 to n do5: ?
[i, i, k]?
?
[i, k, k]?
06: end for7: end for8: for w ?
2 to n do9: for i?
0 to n?
w do10: k ?
i+ w11: ?
[i, k]?
?
?12: for j ?
i+ 1 to k ?
1 do13: ?
[i, j, k]?
?
[i, j, k ?
1] + ?
[i+ 1, j, k]??
[i+ 1, j, k ?
1] +B[pik, pii+1]?B[pii+1, pik]14: ?
[i, k]?
max(?
[i, k], ?
[i, j] + ?
[j, k] + max(0, ?
[i, j, k]))15: end for16: end for17: end for18: return ?
[0, n]19: end procedureFigure 3: Pseudocode for computing the score of the best permutation in the neighborhood of pi underthe Linear Ordering Problem specified by the matrix B. Computing the best neighbor is a simple matterof keeping back pointers to the choices of max and ordering them as implied.alignments using the ?grow-diag-final-and?
proce-dure provided with Moses (Koehn et al, 2007).For each of these German sentences, we derivedthe English-like reordering of it, which we callGerman?, by the following procedure.
Each Ger-man token was assigned an integer key, namelythe position of the leftmost of the English tokensto which it was aligned, or 0 if it was not alignedto any English tokens.
We then did a stable sort ofthe German tokens based on these keys, meaningthat if two German tokens had the same key, theirorder was preserved.This is similar to the oracle ordering used byAl-Onaizan and Papineni (2006), but differs in thehandling of unaligned words.
They kept unalignedwords with the closest preceding aligned word.4Having found the German?corresponding toeach German sentence, we randomly dividedthe sentences into 2,000 each for developmentand evaluation, and the remaining approximately747,000 for training.We used the averaged perceptron algorithm(Freund and Schapire, 1998; Collins, 2002) totrain the parameters of the model.
We ran the al-gorithm multiple times over the training sentences,4We tried two other methods for deriving English wordorder from word alignments.
The first alternative was toalign only in one direction, from English to German, withnull alignments disallowed, so that every German word wasaligned to a single English word.
The second alternativeused BerkeleyAligner (Liang et al, 2006; DeNero and Klein,2007), which shares information between the two alignmentdirections to improve alignment quality.
Neither alternativeproduced improvements in our ultimate translation quality.measuring the quality of the learned parameters byreordering the held-out development set after eachiteration.
We stopped when the BLEU score onthe development set failed to improve for two con-secutive iterations, which occurred after fourteenpasses over the data.Each perceptron update should compare the trueGerman?to the German?that would be predictedby the model (2).
As the latter is NP-hard to find,we instead substitute the local maximum found bylocal search as described in Section 3, starting atthe identity permutation, which corresponds to theoriginal German word order.During training, we iterate the local search asdescribed earlier.
However, for decoding, we onlydo a single step of local search, thus restricting re-orderings to the ITG neighborhood of the origi-nal German.
This restriction turns out to improveperformance slightly, even though it reduces thequality of our approximation to the LOP prob-lem (4).
In other words, it turns out that reorder-ings found outside the ITG neighborhood tend tobe poor German?even if our LOP-based objectivefunction thinks that they are good German?.This is not to say that the gold standard German?is always in the ITG neighborhood of the originalGerman?often it is not.
Thus, it might be bet-ter in future work to still allow the local search totake more than one step, but to penalize the secondstep.
In effect, score(pi) would then include a fea-ture indicating whether pi is in the neighborhoodof the original German.1011t`?1w`t`t`+1tbtr?1wrtrtr+1?
?
?
??
?
??
?
??
?
??
?
??
??
??
??
??????
?
??
?
?
??
?
??
?
?
??
?
??
?
?
??
?
??
?
?
??
?
?Table 1: Feature templates forB[`, r] (w`is the `thword, t`its part of speech tag, and b matches anyindex such that ` < b < r).
Each of the aboveis also conjoined with the distance between thewords, r ?
`, to form an additional feature tem-plate.
Distances are binned into 1, 2, 3, 4, 5, > 5,and > 10.The model is initialized at the start of train-ing using log-odds of the parameters.
Let ?m={(w, `, r) | ?m(w, `, r) = 1} be the set of wordpairs in the training data for which feature m fires.Let?
?mbe the subset of ?mfor which the wordsstay in order, and?
?mthe subset for which thewords reverse order.
Then in this model,?m= log(?????m???+12)?log(?????m???+12).
(7)This model is equivalent to smoothed na?
?ve Bayesif converted to probabilities.
The learned modelsignificantly outperforms it on the monolingual re-ordering task.Table 2 compares the model after perceptrontraining to the model at the start of training,measuring BLEU score of the predicted German?against the observed German?.
In addition to theseBLEU scores, we can measure precision and re-call of pairs of reordered words against the ob-Ordering p2p3p4BLEUGerman 57.4 38.3 27.7 49.65Log-odds 57.4 38.4 27.8 49.75Perceptron 58.6 40.3 29.8 51.51Table 2: Monolingual BLEU score on develop-ment data, measured against the ?true?
German?ordering that was derived from automatic align-ments to known English translations.
The tableevaluates three candidate orderings: the originalGerman, German reordered using the log-oddsinitialized model, and German reordered usingthe perceptron-learned model.
In addition to theBLEU score, the table shows bigram, trigram, and4-gram precisions.
The unigram precisions are al-ways 100%, because the correct words are given.served German?.
On the held out test set, the pre-dicted German?achieves a recall of only 21%, buta precision of 64%.
Thus, the learned model istoo conservative, but makes moderately good de-cisions when it does reorder.6 Reordering as PreprocessingThis section describes experiments using themodel introduced in Section 2 and learned in Sec-tion 5 to preprocess German sentences for trans-lation into English.
These experiments are similarto those of Collins et al (2005).We used the model learned in Section 5 to gen-erate a German?ordering of the training, develop-ment, and test sets.
The training sentences are thesame that the model was trained on, and the devel-opment set is the same that was used as the stop-ping criterion for the perceptron.
The test set wasunused in training.We used the resulting German?as the input tothe Moses training pipeline.
That is, Moses re-computed alignments of the German?training datato the English sentences using GIZA++, then con-structed a phrase table.
Moses used the develop-ment data for minimum error-rate training (Och,2003) of its small number of parameters.
Finally,Moses translated the test sentences, and we mea-sured performance against the English referencesentences.
This is the standard Moses pipeline, ex-cept German has been replaced by German?.Table 3 shows the results of translation, bothstarting with unreordered German, and startingwith German?, reordered using the learned LinearOrdering Problems.
Note that Moses may itself re-1012System Input Moses Reord.
p1p2p3p4BLEU METEOR TERbaseline German Distance 59.6 31.4 18.8 11.6 25.27 54.03 60.60(a) German Lexical 60.0 32.0 19.3 12.1 25.55 54.18 59.76(b) German?Distance 60.4 32.7 20.2 12.8 26.40 54.91 58.63(a)+(b) German?Lexical 59.9 32.4 20.0 12.8 26.44 54.61 59.23Table 3: Machine translation performance of several systems, measured against a single English refer-ence translation.
The results vary both the preprocessing?either none, or reordered using the learnedLinear Ordering Problems?and the reordering model used in Moses.
Performance is measured usingBLEU, METEOR (Lavie et al, 2004), and TER (Snover et al, 2006).
(For TER, smaller values arebetter.
)order whatever input that it receives, during trans-lation into English.
Thus, the results in the tablealso vary the reordering model used in Moses, setto either a single-parameter distance-based model,or to the lexicalized bidirectional msd model.
Thelatter model has six parameters for each phrasein the phrase table, corresponding to monotone,swapped, or discontinuous ordering relative to theprevious phrase in either the source or target lan-guage.How should we understand the results?
Thebaseline system is Moses phrase-based translationwith no preprocessing and only a simple distance-based reordering model.
There are two ways toimprove this: (a) ask Moses to use the lexicalizedbidirectional msd reordering model that is pro-vided with Moses and is integrated with the rest oftranslation, or (b) keep the simple distance-basedmodel within Moses, but preprocess its trainingand test data with our linear reordering model.Note that the preprocessing in (b) will obviouslychange the phrasal substrings that are learned byMoses, for better or for worse.First, remarkably, (b) is significantly better than(a) on BLEU, with p < 0.0001 according to apaired permutation test.Second, combining (a) with (b) produced no im-provement over (b) in BLEU score (the differencebetween 26.40 and 26.44 is not significant, evenat p < 0.2, according to the same paired per-mutation test).
Lexicalized reordering in Moseseven degraded translation performance accordingto METEOR and TER.
The TER change is sig-nificant according to the paired permutation test atp < 0.001.
(We did not perform a significance testfor METEOR.
)Our word-based model surpasses the lexical-ized reordering in Moses largely because of long-distance movement.
The 518 sentences (26%) inllllllllllllllllllllllllllllllllllllllllllll l l llllllllllllllllllllllllllllllllllllllllllllll l l ll0 10 20 30 40 50?0.0020.0000.0020.0040.0060.0080.010Word Pairs ReorderedCumulativeBLEU ChangeBLEU Improvement Aggregated by Amount of Reorderingvs.
baselinevs.
(a)Figure 4: Cumulative change in BLEU score of(b) relative to the baseline and (a), aggregated bythe number of reordered word pairs in each sen-tence.
For those sentences where our model re-orders fewer than five word pairs, the BLEU scoreof translation degrades.the test set for which our model moves a wordmore than six words away from its starting posi-tion account for more than 67% of the improve-ment in BLEU from (a) to (b).Figure 4 shows another view of the BLEU im-provement.
It shows that, compared to the base-line, our preprocessing has basically no effect forsentences where it does only a little reordering,changing the relative order of fewer than five pairsof words.
Compared to Moses with lexicalized re-ordering, these same sentences actually hurt per-formance.
This more than accounts for the differ-ence between the BLEU scores of (b) and (a)+(b).Going beyond preprocessing, our model couldalso be integrated into a phrase-based decoder.
Webriefly sketch that possibility here.1013Phrase-based decoders keep a source coveragevector with every partial translation hypothesis.That coverage vector allows us to incorporate thescores from a LOP matrix B directly.
Wheneverthe decoder extends the hypothesis with a newsource phrase, covering wi+1wi+2.
.
.
wj, it addsj?1?`=i+1j?r=`+1B[`, r] +j?`=i+1?r?UB[`, r].The first term represents the phrase-internal score,and the second the score of putting the words in thephrase before all the remaining uncovered wordsU .7 Comparison to Prior WorkPreprocessing the source language to improvetranslation is a common technique.
Xia and Mc-Cord (2004) improved English-French translationusing syntactic rewrite rules derived from SlotGrammar parses.
Collins et al (2005) reportedan improvement from 25.2% to 26.8% BLEUon German-English translation using six hand-written rules to reorder the German sentencesbased on automatically-generated phrase-structuretrees.
Our work differs from these approaches inproviding an explicit model that scores all pos-sible reorderings.
In this paper, our model wastrained and used only for 1-best preprocessing, butit could potentially be integrated into decoding aswell, where it would work together with the trans-lation model and target language model to find acongenial translation.Costa-juss`a and Fonollosa (2006) improvedSpanish-English and Chinese-English translationusing a two-step process, first reordering thesource language, then translating it, both using dif-ferent versions of a phrase-based translation sys-tem.
Many others have proposed more explicitreordering models (Tillmann, 2004; Kumar andByrne, 2005; Koehn et al, 2005; Al-Onaizan andPapineni, 2006).
The primary advantage of ourmodel is that it directly accounts for interactionsbetween distant words, leading to better treatmentof long-distance movement.Xiong et al (2006) proposed a constituentreordering model for a bracketing transductiongrammar (BTG) (Wu, 1995), which predicts theprobability that a pair of subconstituents will re-order when combined to form a new constituent.The features of their model look only at the firstsource and target word of each constituent, mak-ing it something like a sparse version of our model.However, because of the target word features, theirreordering model cannot be separated from theirtranslation model.8 Conclusions and Future WorkWe have presented an entirely new model of re-ordering for statistical machine translation, basedon the Linear Ordering Problem, and shown thatit can substantially improve translation from Ger-man to English.The model is demonstrably useful in this pre-processing setting?which means that it can bevery simply added as a preprocessing step to anyMT system.
German-to-English is a particularlyattractive use case, because the word orders aresufficiently different as to require a good reorder-ing model that requires long-distance reordering.Our preprocessing here gave us a BLEU gainof 0.9 point over the best Moses-based result.English-to-German would obviously be anotherpotential win, as would translating between En-glish and Japanese, for example.As mentioned in Section 6, our model couldalso be integrated into a phrase-based, or a syntax-based decoder.
That possibility remains futurework, but it is likely to lead to further improve-ments, because it allows the translation system toconsider multiple possible reorderings under themodel, as well as to tune the weight of the modelrelative to the other parts of the system duringMERT.Tromble (2009) covers this integration in moredetail, and proposes several other ways of integrat-ing our reordering model into machine translation.It also experiments with numerous other param-eter estimation procedures, including some thatuse the probabilistic interpretation of our modelfrom (3).
It presents numerous additional neigh-borhoods for search in the Linear Ordering Prob-lem.We mentioned several possible extensions to themodel, such as going beyond the scoring modelof equation (2), or considering syntax-based fea-tures.
Another extension would try to reorder notwords but phrases, following (Xiong et al, 2006),or segment choice models (Kuhn et al, 2006),which assume a single segmentation of the wordsinto phrases.
We would have to define the pair-wise preference matrix B over phrases rather than1014words (Eisner and Tromble, 2006).
This wouldhave the disadvantage of complicating the featurespace, but might be a better fit for integration witha phrase-based decoder.Finally, we gave a novel algorithm for ap-proximately solving the Linear Ordering Prob-lem, interestingly combining dynamic program-ming with local search.
Another novel contri-bution is that we showed how to parameterize afunction that constructs a specific Linear Order-ing Problem instance from an input sentence w,and showed how to learn those parameters froma corpus of parallel sentences, using the percep-tron algorithm.
Likelihood-based training usingequation (3) would also be possible, with modifi-cations to our algorithm, notably the use of normalforms to avoid counting some permutations multi-ple times (Tromble, 2009).It would be interesting to compare the speedand accuracy of our dynamic-programming local-search method with an exact algorithm for solvingthe LOP, such as integer linear programming withbranch and bound (cf.
Charon and Hudry (2006)).Exact solutions can generally be found in practicefor n ?
100.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Dis-tortion models for statistical machine translation.
InCOLING-ACL, pages 529?536, Sydney, July.Ir`ene Charon and Olivier Hudry.
2006.
A branch-and-bound algorithm to solve the linear ordering problemfor weighted tournaments.
Discrete Applied Mathe-matics, 154(15):2097?2116, October.Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.2005.
Clause restructuring for statistical machinetranslation.
In ACL, pages 531?540, Ann Arbor,Michigan, June.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and ex-periments with perceptron algorithms.
In EMNLP,pages 1?8, Philadelphia, July.Marta R. Costa-juss`a and Jos?e A. R. Fonollosa.
2006.Statistical machine reordering.
In EMNLP, pages70?76, Sydney, July.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In ACL,pages 17?24, Prague, June.Jason Eisner and Roy W. Tromble.
2006.
Local searchwith very large-scale neighborhoods for optimal per-mutations in machine translation.
In Workshop oncomputationally hard problems and joint inferencein speech and language processing, New York, June.Yoav Freund and Robert E. Schapire.
1998.
Largemargin classification using the perceptron algorithm.In COLT, pages 209?217, New York.
ACM Press.Martin Gr?otschel, Michael J?unger, and GerhardReinelt.
1984.
A cutting plane algorithm forthe linear ordering problem.
Operations Research,32(6):1195?1220, November?December.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.In IWSLT, Pittsburgh, October.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In ACL Demo and Poster Sessions, pages 177?180, Prague, June.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In MT SummitX, pages 79?86, Phuket, Thailand, September.Roland Kuhn, Denis Yuen, Michel Simard, PatrickPaul, George Foster, Eric Joanis, and Howard John-son.
2006.
Segment choice models: Feature-richmodels for global distortion in statistical machinetranslation.
In HLT-NAACL, pages 25?32, NewYork, June.Shankar Kumar and William Byrne.
2005.
Lo-cal phrase reordering models for statistical machinetranslation.
In HLT-EMNLP, pages 161?168, Van-couver, October.Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-man.
2004.
The signicance of recall in automaticmetrics for MT evaluation.
In Robert E. Frederkingand Kathryn B. Taylor, editors, Machine Transla-tion: From Real Users to Research, pages 134?143.AMTA, Springer, September?October.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In HLT-NAACL, pages 104?111, New York, June.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Spanning tree methods for discrim-inative training of dependency parsers.
TechnicalReport MS-CIS-05-11, UPenn CIS.Sounaka Mishra and Kripasindhu Sikdar.
2004.
Onapproximability of linear ordering and related NP-optimization problems on graphs.
Discrete AppliedMathematics, 136(2?3):249?269, February.1015Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL, pages 160?167, Sapporo, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In ACL, pages311?318, Philadelphia, July.Tommaso Schiavinotto and Thomas St?utzle.
2004.The linear ordering problem: Instances, searchspace analysis and algorithms.
Journal of Math-ematical Modelling and Algorithms, 3(4):367?402,December.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In AMTA.Christoph Tillmann.
2004.
A unigram orientationmodel for statistical machine translation.
In HLT-NAACL Short Papers, pages 101?104, Boston, May.Roy Wesley Tromble.
2009.
Search and Learning forthe Linear Ordering Problem with an Applicationto Machine Translation.
Ph.D. thesis, Johns Hop-kins University, Baltimore, April.
http://nlp.cs.jhu.edu/?royt/Dekai Wu.
1995.
An algorithm for simultaneouslybracketing parallel texts by aligning words.
In ACL,pages 244?251, Cambridge, Massachusetts, June.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404, Septem-ber.Fei Xia and Michael McCord.
2004.
Improvinga statistical MT system with automatically learnedrewrite patterns.
In COLING, pages 508?514,Geneva, August.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Max-imum entropy based phrase reordering model forstatistical machine translation.
In COLING-ACL,pages 521?528, Sydney, July.Richard Zens and Hermann Ney.
2003.
A comparativestudy on reordering constraints in statistical machinetranslation.
In ACL, pages 144?151, Sapporo, July.1016
