Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 27?35,Avignon, France, April 23 2012. c?2012 Association for Computational LinguisticsDescribing Video Contents in Natural LanguageMuhammad Usman Ghani KhanUniversity of SheffieldUnited Kingdomughani@dcs.shef.ac.ukYoshihiko GotohUniversity of SheffieldUnited Kingdomy.gotoh@dcs.shef.ac.ukAbstractThis contribution addresses generation ofnatural language descriptions for human ac-tions, behaviour and their relations withother objects observed in video streams.The work starts with implementation ofconventional image processing techniquesto extract high level features from video.These features are converted into naturallanguage descriptions using context freegrammar.
Although feature extraction pro-cesses are erroneous at various levels, weexplore approaches to putting them to-gether to produce a coherent description.Evaluation is made by calculating ROUGEscores between human annotated and ma-chine generated descriptions.
Further weintroduce a task based evaluation by humansubjects which provides qualitative evalua-tion of generated descriptions.1 IntroductionIn recent years video has established its domi-nance in communication and has become an in-tegrated part of our everyday life ranging fromhand-held videos to broadcast news video (fromunstructured to highly structured).
There is a needfor formalising video semantics to help users gainuseful and refined information relevant to theirdemands and requirements.
Human language isa natural way of communication.
Useful entitiesextracted from videos and their inter-relations canbe presented by natural language in a syntacticallyand semantically correct formulation.While literature relating to object recognition(Galleguillos and Belongie, 2010), human actionrecognition (Torralba et al, 2008), and emotiondetection (Zheng et al, 2010) are moving towardsmaturity, automatic description of visual scenesis still in its infancy.
Most studies in video re-trieval have been based on keywords (Bolle etal., 1998).
An interesting extension to a key-word based scheme is natural language textual de-scription of video streams.
They are more humanfriendly.
They can clarify context between key-words by capturing their relations.
Descriptionscan guide generation of video summaries by con-verting a video to natural language.
They can pro-vide basis for creating a multimedia repository forvideo analysis, retrieval and summarisation tasks.Kojima et al (2002) presented a method fordescribing human activities in videos based ona concept hierarchy of actions.
They describedhead, hands and body movements using naturallanguage.
For a traffic control application, Nagel(2004) investigated automatic visual surveillancesystems where human behaviour was presentedby scenarios, consisting of predefined sequencesof events.
The scenario was evaluated and auto-matically translated into a text by analysing thevisual contents over time, and deciding on themost suitable event.
Lee et al (2008) introduceda framework for semantic annotation of visualevents in three steps; image parsing, event infer-ence and language generation.
Instead of humansand their specific activities, they focused on ob-ject detection, their inter-relations and events thatwere present in videos.
Baiget et al (2007) per-formed human identification and scene modellingmanually and focused on human behaviour de-scription for crosswalk scenes.
Yao et al (2010)introduced their work on video to text descrip-tion which is dependent on the significant amountof annotated data, a requirement that is avoidedin this paper.
Yang et al (2011) presented a27framework for static images to textual descrip-tions where they contained to image with up totwo objects.
In contrast, this paper presents awork on video streams, handling not only objectsbut also other features such as actions, age, genderand emotions.The study presented in this paper is concernedwith production of natural language descriptionfor visual scenes in a time series using a bottom-up approach.
Initially high level features (HLFs)are identified in video frames.
They may be ?key-words?, such as a particular object and its posi-tion/moves, used for a semantic indexing task invideo retrieval.
Spatial relations between HLFsare important when explaining the semantics ofvisual scene.
Extracted HLFs are then presentedby syntactically and semantically correct expres-sions using a template based approach.
Imageprocessing techniques are far from perfect; therecan be many missing, misidentified and erro-neously extracted HLFs.
We present scenariosto overcome these shortcomings and to generatecoherent natural descriptions.
The approach isevaluated using video segments drafted manuallyfrom the TREC video dataset.
ROUGE scores iscalculated between human annotated and machinegenerated descriptions.
A task based evaluation isperformed by human subjects, providing qualita-tive evaluation of generated descriptions.2 Dataset CreationThe dataset was manually created from a sub-set of rushes and HLF extraction task videos in2007/2008 TREC video evaluations (Over et al,2007).
It consists of 140 segments, with each seg-ment containing one camera shot, spanning 10 to30 seconds in length.
There are 20 video segmentsfor each of the seven categories:Action: Human can be seen performing some action(e.g., sit, walk)Closeup: Facial expressions/emotions can be seen(e.g., happy, sad)News: Anchor/reporter may be seen; particular scenesettings (e.g., weather board in the background)Meeting: Multiple humans are seen interacting; pres-ence of objects such as chairs and a tableGrouping: Multiple humans are seen but not in meet-ing scenarios; chairs and table may not be presentTraffic: Vehicles (e.g., car, bus, truck) / traffic signalsare seenIndoor/Outdoor: Scene settings are more obviousthan human activities (e.g., park scene, office)13 human subjects individually annotated thesevideos in one to seven short sentences.
They arereferred to as hand annotations in the rest of thispaper.3 Processing High Level FeaturesIdentification of human face or body can provethe presence of human in a video.
The methodby Kuchi et al (2002) is adopted for face detec-tion using colour and motion information.
Themethod works against variations in lightning con-ditions, skin colours, backgrounds, face sizes andorientations.
When the background is close to theskin colour, movement across successive framesis tested to confirm the presence of a human face.Facial features play an important role in identify-ing age, gender and emotion information (Maglo-giannis et al, 2009).
Human emotion can be esti-mated using eyes, lips and their measures (gradi-ent, distance of eyelids or lips).
The same set offacial features and measures can be used to iden-tify a human gender1.To recognise human actions the approach basedon a star skeleton and a hidden Markov model(HMM) is implemented (Chen et al, 2006).
Com-monly observed actions, such as ?walking?, ?run-ning?, ?standing?, and ?sitting?, can be identified.Human body is presented in the form of sticks togenerate features such as torso, arm length and an-gle, leg angle and stride (Sundaresan et al, 2003).Further Haar features are extracted and classifiersare trained to identify non-human objects (Violaand Jones, 2001).
They include car, bus, motor-bike, bicycle, building, tree, table, chair, cup, bot-tle and TV-monitor.
Scene settings ?
indoor oroutdoor ?
can be identified based on the edgeoriented histogram (EOH) and the colour orientedhistogram (COH) (Kim et al, 2010).3.1 Performance of HLF ExtractionIn the experiments, video frames were extractedusing ffmpeg2, sampled at 1 fps (frame per sec-ond), resulting in 2520 frames in total.
Most of1www.virtualffs.co.uk/In a Nutshell.html2Ffmpeg is a command line tool composed of a col-lection of free software and open source libraries.
It canrecord, convert and stream digital audio and video in nu-merous formats.
The default conversion rate is 25 fps.
Seehttp://www.ffmpeg.org/28(ground truth) (ground truth)exist not exist male femaleexist 1795 29 male 911 216not exist 95 601 female 226 537(a) human detection (b) gender identificationTable 1: Confusion tables for (a) human detection and(b) gender identification.
Columns show the groundtruth, and rows indicate the automatic recognition re-sults.
The human detection task is biased towards exis-tence of human, while in the gender identification pres-ence of male and female are roughly balanced.HLFs required one frame to evaluate.
Human ac-tivities were shown in 45 videos and they weresampled at 4 fps, yielding 3600 frames.
Uponseveral trials, we decided to use eight frames(roughly two seconds) for human action recogni-tion.
Consequently tags were assigned for eachset of eight frames, totalling 450 sets of actions.Table 1(a) presents a confusion matrix for hu-man detection.
It was a heavily biased datasetwhere human(s) were present in 1890 out of 2520frames.
Of these 1890, misclassification occurredon 95 occasions.
On the other hand gender iden-tification is not always an easy task even for hu-mans.
Table 1(b) shows a confusion matrix forgender identification.
Out of 1890 frames inwhich human(s) were present, frontal faces wereshown in 1349 images.
The total of 3555 humanswere present in 1890 frames (1168 frames con-tained multiple humans), however the table showsthe results when at least one gender is correctlyidentified.
Female identification was often moredifficult due to make ups, variety of hair stylesand wearing hats, veils and scarfs.Table 2 shows the human action recognitionperformance tested with a set of 450 actions.
Itwas difficult to recognise ?sitting?
actions, proba-bly because HMMs were trained on postures of acomplete human body, while a complete posturewas often not available when a person was sit-ting.
?Hand waving?
and ?clapping?
were relatedto movements in upper body parts, and ?walking?and ?running?
were based on lower body move-ments.
In particular ?waving?
appeared an easyaction to identify because of its significant movesof upper body parts.
Table 3 shows the confu-sion for human emotion recognition.
?Serious?,?happy?
and ?sad?
were most common emotionsin this dataset, in particular ?happy?
emotion wasmost correctly identified.There were 15 videos where human or any(ground truth)stand sit walk run wave clapstand 98 12 19 3 0 0sit 0 68 0 0 0 0walk 22 9 105 8 0 0run 4 0 18 27 0 0wave 2 5 0 0 19 2clap 0 0 0 0 4 9Table 2: Confusion table for human action recogni-tion.
Columns show the ground truth, and rows indi-cate the automatic recognition results.
Some actions(e.g., ?standing?)
were more commonly seen than oth-ers (e.g., ?waving?).
(ground truth)angry serious happy sad surprisedangry 59 0 0 15 16serious 0 661 0 164 40happy 0 35 427 27 8sad 61 13 0 281 2surprised 9 19 0 0 53Table 3: Confusion table for human emotion recogni-tion.
Columns show the ground truth, and rows indi-cate the automatic recognition results.other moving HLF (e.g., car, bus) were absent.Out of these 15 videos, 12 were related to outdoorenvironments where trees, greenery, or buildingswere present.
Three videos showed indoor set-tings with objects such as chairs, tables and cups.All frames from outdoor scenes were correctlyidentified; for indoor scenes 80% of frames werecorrect.
Presence of multiple objects seems tohave caused negative impact on EOH and COHfeatures, hence resulted in some erroneous clas-sifications.
The recognition performances fornon-human objects were also evaluated with thedataset.
We found their average precision3 scoresranging between 44.8 (table) and 77.8 (car).3.2 Formalising Spatial RelationsTo develop a grammar robust for describing hu-man related scenes, there is a need for formalis-ing spatial relations among multiple HLFs.
Theireffective use leads to smooth description of visualscenes.
Spatial relations can be categorised intostatic: relations between not moving objects;dynamic: direction and path of moving objects;inter-static and dynamic: relations between movingand not moving objects.3defined by Everingham et al (2010).29Figure 1: Procedure for calculating the ?between?
rela-tion.
Obj 1 and 2 are the two reference objects, whileObj 3, 4 and 5 are the target objects.Static relations can establish the scene settings(e.g., ?chairs around a table?
may imply an indoorscene).
Dynamic relations are used for finding ac-tivities present in the video (e.g., ?a man is run-ning with a dog?).
Inter-static and dynamic rela-tions are a mixture of stationary and non station-ary objects; they explain semantics of the com-plete scene (e.g., ?persons are sitting on the chairsaround the table?
indicates a meeting scene).Spatial relations are estimated using positionsof humans and other objects (or their boundingboxes, to be more precise).
Following relation-ships can be recognised between two or three ob-jects: ?in front of?, ?behind?, ?to the left?, ?to theright?, ?beside?, ?at?, ?on?, ?in?, and ?between?.Figure 1 illustrates steps for calculating the three-place relationship ?between?.
Schirra et al (1987)explained the algorithm:?
Calculate the two tangents g1 and g2 betweenthe reference objects using their closed-rectanglerepresentation;?
If (1) both tangents cross the target or its rectan-gle representation (see Obj 4 in the figure), or (2)the target is totally enclosed by the tangents andthe references (Obj 3), the relationship ?between?is true.?
If only one tangent intersects the subject (Obj 5),the applicability depends on its penetration depthin the area between the tangents, thus calculate:max(a/(a+b), a/(a+c))?
Otherwise ?between?
relation does not hold.3.3 Predicates for Sentence GenerationFigure 2 presents a list of predicates to be used fornatural language generation.
Some predicates arederived by combining multiple HLFs extracted,e.g., ?boy?
may be inferred when a human is aHuman structure relatedhuman (yes, no)gender (male, female)age (baby, child, young, old)body parts (hand, head, body)grouping (one, two, many)Human actions and emotionsaction (stand, sit, walk, run, wave, clap)emotion (happy, sad, serious, surprise, angry)Objects and scene settingsscene setting (indoor, outdoor)objects (car, cup, table, chair, bicycle, TV-monitor)Spatial relations among objectsin front of, behind, to the left, to the right, beside,at, on, in, betweenFigure 2: Predicates for single human scenes.?male?
and a ?child?.
Apart from objects, only onevalue can be selected from candidates at one time,e.g., gender can be male or female, action canbe only one of those listed.
Note that predicateslisted in Figure 2 are for describing single humanscenes; combination of these predicates may beused if multiple humans are present.4 Natural Language GenerationHLFs acquired by image processing require ab-straction and fine tuning for generating syntacti-cally and semantically sound natural language ex-pressions.
Firstly, a part of speech (POS) tag isassigned to each HLF using NLTK4 POS tagger.Further humans and objects need to be assignedproper semantic roles.
In this study, a human istreated as a subject, performing a certain action.Other HLFs are treated as objects, affected by hu-man?s activities.
These objects are usually helpfulfor description of background and scene settings.A template filling approach based on contextfree grammar (CFG) is implemented for sentencegeneration.
A template is a pre-defined structurewith slots for user specified parameters.
Eachtemplate requires three parts for proper function-ing: lexicons, template rules and grammar.
Lex-icon is a vocabulary containing HLFs extractedfrom a video stream (Figure 3).
Grammar assuressyntactical correctness of the sentence.
Templaterules are defined for selection of proper lexicons4www.nltk.org/30Noun ?
man | woman | car | cup | table|chair | cycle | head | hand | bodyVerb ?
stand | walk | sit | run | waveAdjective ?
happy | sad | serious | surprise |angry | one | two | many | youngold | middle-aged | child | babyPronoun ?
me | i | you | it | she | heDeterminer ?
the | a | an | this | these | thatPreposition ?
from | on | to | near | whileConjunction ?
and | or | butFigure 3: Lexicons and their POS tags.with well defined grammar.4.1 Template RulesTemplate rules are employed for the selection ofappropriate lexicons for sentence generation.
Fol-lowings are some template rules used in this work:Base returns a pre-defined string (e.g., when no HLFis detected)If same as an if-then statement of programming lan-guages, returning a result when the antecedent ofthe rule is trueSelect 1 same as a condition statement of program-ming languages, returning a result when one ofantecedent conditions is trueSelect n is used for returning a result while more thanone antecedent conditions is trueConcatenation appends the the result of one templaterule with the results of a second ruleAlternative is used for selecting the most specifictemplate when multiple templates can be usedElaboration evaluates the value of a template slotFigure 4 illustrates template rules selection pro-cedure.
This example assumes human presencein the video.
If-else statements are used for fit-ting proper gender in the template.
Human canbe performing only one action at a time referredby Select 1.
There can be multiple objects whichare either part of background or interacting withhumans.
Objects are selected by Select n rule.These values can be directly attained from HLFsextraction step.
Elaboration rule is used forgenerating new words by joining multiple HLFs.?Driving?
is achieved by combing ?person is in-side car?
and ?car is moving?.4.2 GrammarGrammar is the body of rules that describe thestructure of expressions in any language.
WeIf (gender == male) then man else womanSelect 1 (Action == walk, run, wave, clap, sit, stand )Select n (Object == car, chair, table, bike)Elaboration (If ?the car is moving?
and ?person isinside the car?)
then ?person is driving the car?Figure 4: Template rules applied for creating a sen-tence ?man is driving the car?.make use of context free grammar (CFG) for thesentence generation task.
CFG based formulationenables us to define a hierarchical presentation forsentence generation; e.g., a description for multi-ple humans is comprised of single human actions.CFG is formalised by 4-tuple:G = (T,N, S,R)where T is set of terminals (lexicon) shown inFigure 3, N is a set of non-terminals (usually POStags), S is a start symbol (one of non-terminals).Finally R is rules / productions of the form X ?
?, where X is a non-terminal and ?
is a se-quence of terminals and non-terminals which maybe empty.For implementing the templates, simpleNLG isused (Gatt and Reiter, 2009).
It also performssome extra processing automatically: (1) the firstletter of each sentence is capitalised, (2) ?-ing?
isadded to the end of a verb as the progressive as-pect of the verb is desired, (3) all words are puttogether in a grammatical form, (4) appropriatewhite spaces are inserted between words, and (5)a full stop is placed at the end of the sentence.4.3 Hierarchical Sentence GenerationIn this work we define a CFG based presenta-tion for expressing activities by multiple humans.Ryoo and Aggarwal (2009) used CFG for hierar-chical presentation of human actions where com-plex actions were composed of simpler actions.In contrast we allow a scenario where there is nointeraction between humans, i.e., they perform in-dividual actions without a particular relation ?imagine a situation whereby three people are sit-ting around a desk while one person is passingbehind them.Figure 5 shows an example for sentence gen-eration related to a single human.
This mech-anism is built with three blocks when only onesubject5 is present.
The first block expresses a5Non human subject is also allowed in the mechanism.31Figure 5: A scenario with a single human.Figure 6: A scenario with two humans.human subject with age, gender and emotion in-formation.
The second block contains a verb de-scribing a human action, to explain the relationbetween the first and the third blocks.
Spatial re-lation between the subject and other objects canalso be presented.
The third block captures otherobjects which may be either a part of backgroundor a target for subject?s action.The approach is hierarchical in the sense thatwe start with creating a single human grammar,then build up to express interactions between twoor more than two humans as a combination of sin-gle human activities.
Figure 6 presents examplesinvolving two subjects.
There can be three scenar-ios; firstly two persons interact with each other togenerate some common single activity (e.g., ?handshake?
scene).
The second scenario involves tworelated humans performing individual actions butthey do not create a single action (e.g., both per-sons are walking together, sitting or standing).
Fi-nally two persons happen to be in the same sceneat the same time, but there is no particular relationbetween them (e.g., one person walks, passing be-hind the other person sitting on a chair).
Figure 7shows an example that involves an extension of aFigure 7: A scenario with multiple humans.Figure 8: Template selection: (a) subject + subject +verb: ?man and woman are waving hands?
; (b) subject+ subject + object: ?two persons around the table?
; (c)subject + verb, noun phrase / subject, noun phrase /subject: ?a man is standing; a person is present; thereare two chairs?
; (d) subject + subject + subject + verb:?multiple persons are present ?.single human scenario to more than two subjects.Similarly to two-human scenarios, multiple sub-jects can create a single action, separate actions,or different actions altogether.4.4 Application ScenariosThis section overviews different scenarios for ap-plication of the sentence generation framework.Figure 8 presents examples for template selec-tion procedure.
Although syntactically and se-mantically correct sentences can be generated inall scenes, immaturity of image processing wouldcause some errors and missing information.Missing HLFs.
For example, action (?sitting?
)was not identified in Figure 8(b).
Further, detec-Figure 9: Image processing can be erroneous: (a) onlythree cars are identified although there are many ve-hicles prominent, (b) five persons (in red rectangles)are detected although four are present; (c) one maleis identified correctly, other male is identified as ?fe-male?
; (d) detected emotion is ?smiling?
though heshows a serious face.32Figure 10: Closeup of a man talking to someone in the outdoorscene ?
seen in ?MS206410?
from the 2007 rushes summarisationtask.
Machine annotation: A serious man is speaking; There arehumans in the background.
Hand annotation 1: A man is talkingto someone; He is wearing a formal suit; A police man is standingbehind him; Some people in the background are wearing hats.
Handannotation 2: A man with brown hair is talking to someone; He isstanding at some outdoor place; He is wearing formal clothes; Helooks serious; It is windy.tion of food on the table might have led to moresemantic description of the scene (e.g., ?dinningscene?).
In 8(d), fourth human and actions bytwo humans (?raising hands?)
were not extracted.Recognition of the road and many more vehiclesin Figure 9(a) could have produced more semanticexpression (e.g., ?heavy traffic scene?
).Non human subjects.
Suppose a human is ab-sent, or failed to be extracted, the scene is ex-plained on the basis of objects.
They are treated assubjects for which sentences are generated.
Fig-ure 9(a) presents such a scenario; description gen-erated was ?multiple cars are moving?.Errors in HLF extraction.
In Figure 9(c), oneperson was found correctly but the other was er-roneously identified as female.
Description gen-erated was ?a smiling adult man is present witha woman?.
Detected emotion was ?smile?
in 9(d)though real emotion was ?serious?.
Descriptiongenerated was ?a man is smiling?.5 Experiments5.1 Machine Generated Annotation SamplesFigures 10 to 12 present machine generated an-notation and two hand annotations for randomlyselected videos related to three categories fromdataset.Face closeup (Figure 10).
Main interest wasto find human gender and emotion information.Machine generated description was able to cap-ture human emotion and background information.Hand annotations explained the sequence more,e.g., dressing, identity of a person as policeman,hair colour and windy outdoor scene settings.Traffic scene (Figure 11).
Humans were absentin most of traffic video.
Object detector was ableto identify most prominent objects (e.g., car, bus)Figure 11: A traffic scene with many vehicles ?
seen in?20041101 110000 CCTV4 NEWS3 CHN?
from the HLF extrac-tion task.
Machine annotation: Many cars are present; Cars aremoving; A bus is present.
Hand annotation 1: There is a red bus,one yellow and many other cars on the highway; This is a scene ofdaytime traffic; There is a blue road sign on the big tower; There isalso a bridge on the road.
Hand annotation 2: There are many cars;There is a fly-over; Some buses are running on the fly-over; There isvehicle parapet; This is a traffic scene on a highway.Figure 12: An action scene of two humans ?
seen in?20041101 160000 CCTV4 DAILY NEWS CHN?
from the HLFextraction task.
Machine annotation: A woman is sitting whilea man is standing; There is a bus in the background; There is a car inthe background.
Hand annotation 1: Two persons are talking; Oneis a man and other is woman; The man is wearing formal clothes;The man is standing and woman is sitting; A bus is travellings be-hind.
Hand annotation 2: Young woman is sitting on a chair in apark and talking to man who is standing next to her.for description.
Hand annotations produced fur-ther details such as colours of car and other ob-jects (e.g., flyover, bridge).
This sequence wasalso described as a highway.Action scene (Figure 12).
Main interest wasto find humans and their activities.
Successfulrecognition of man, woman and their actions (e.g.,?sitting?, ?standing?)
led to well phrased descrip-tion.
The bus and the car at the background werealso identified.
In hand annotations dressing wasnoted and location was reported as a park.5.2 Evaluation with ROUGEDifficulty in evaluating natural language descrip-tions stems from the fact that it is not a simpletask to define the criteria.
We adopted ROUGE,widely used for evaluating automatic summarisa-tion (Lin, 2004), to calculate the overlap betweenmachine generated and hand annotations.
Table4 shows the results where higher ROUGE scoreindicates closer match between them.In overall scores were not very high, demon-strating the fact that humans have different ob-servations and interests while watching the samevideo.
Descriptions were often subjective, de-33Action Closeup In/Outdoor Grouping Meeting News TrafficROUGE-1 0.4369 0.5385 0.2544 0.3067 0.3330 0.4321 0.3121ROUGE-2 0.3087 0.3109 0.1877 0.2619 0.2462 0.3218 0.1268ROUGE-3 0.2994 0.2106 0.1302 0.1229 0.2400 0.2219 0.1250ROUGE-L 0.4369 0.4110 0.2544 0.3067 0.3330 0.3321 0.3121ROUGE-W 0.4147 0.4385 0.2877 0.3619 0.3265 0.3318 0.3147ROUGE-S 0.3563 0.4193 0.2302 0.2229 0.2648 0.3233 0.3236ROUGE-SU 0.3686 0.4413 0.2544 0.3067 0.2754 0.3419 0.3407Table 4: ROUGE scores between machine generated descriptions (reference) and 13 hand annotations (model).ROUGE 1-3 shows n-gram overlap similarity between reference and model descriptions.
ROUGE-L is based onlongest common subsequence (LCS).
ROUGE-W is for weighted LCS.
ROUGE-S skips bigram co-occurrencewithout gap length.
ROUGE-SU shows results for skip bigram co-occurrence with unigrams.pendent on one?s perception and understanding,that might have been affected by their educa-tional and professional background, personal in-terests and experiences.
Nevertheless ROUGEscores were not hopelessly low for machine gen-erated descriptions; Closeup, Action and Newsvideos had higher scores because of presence ofhumans with well defined actions and emotions.Indoor/Outdoor videos show the poorest resultsdue to the limited capability of image processingtechniques.5.3 Task Based Evaluation by HumanSimilar to human in the loop evaluation (Nwoguet al, 2011), a task based evaluation was per-formed to make qualitative evaluation of the gen-erated descriptions.
Given a machine generateddescription, human subjects were instructed tofind a corresponding video stream out of 10 can-didate videos having the same theme (e.g., a de-scription of a Closeup against 10 Closeup videos).Once a choice was made, each subject was pro-vided with the correct video stream and a ques-tionnaire.
The first question was how well the de-scription explained the actual video, rating from?explained completely?, ?satisfactorily?, ?fairly?,?poorly?, or ?does not explain?.
The second ques-tion was concerned with the ranking of usefulnessfor including various visual contents (e.g., human,objects, their moves, their relations, background)in the description.Seven human subjects conducted this evalua-tion searching a corresponding video for each often machine generated descriptions.
They did notinvolve creation of the dataset, hence they sawthese videos for the first time.
On average, theywere able to identify correct videos for 53%6 of6It is interesting to note the correct identification ratedescriptions.
They rated 68%, 48%, and 40% ofdescriptions explained the actual video ?fairly?,?satisfactorily?, and ?completely?.
Because mul-tiple videos might have very similar text descrip-tions, it was worth testing meaningfulness of de-scriptions for choosing the corresponding video.Finally, usefulness of visual contents had mix re-sults.
For about 84% of descriptions, subjectswere able to identify videos based on informationrelated to humans, their actions, emotions and in-teractions with other objects.6 ConclusionThis paper explored the bottom up approach todescribing video contents in natural language.The conversion from quantitative information toqualitative predicates was suitable for conceptualdata manipulation and natural language genera-tion.
The outcome of the experiments indicatesthat the natural language formalism makes it pos-sible to generate fluent, rich descriptions, allow-ing for detailed and refined expressions.
Futureworks include detection of groups, extension ofbehavioural models, more complex interactionsamong humans and other objects.AcknowledgementsMuhammad Usman Ghani Khan thanks Univer-sity of Engineering & Technology, Lahore, Pak-istan for funding his work under the Faculty De-velopment Program.went up to 70% for three subjects who also conducted cre-ation of the dataset.34ReferencesP.
Baiget, C. Ferna?ndez, X. Roca, and J. Gonza`lez.2007.
Automatic learning of conceptual knowledgein image sequences for human behavior interpre-tation.
Pattern Recognition and Image Analysis,pages 507?514.R.M.
Bolle, B.L.
Yeo, and M.M.
Yeung.
1998.
Videoquery: Research directions.
IBM Journal of Re-search and Development, 42(2):233?252.H.S.
Chen, H.T.
Chen, Y.W.
Chen, and S.Y.
Lee.
2006.Human action recognition using star skeleton.
InProceedings of the 4th ACM international workshopon Video surveillance and sensor networks, pages171?178.
ACM.M.
Everingham, L. Van Gool, C.K.I.
Williams,J.
Winn, and A. Zisserman.
2010.
The pascal vi-sual object classes (voc) challenge.
InternationalJournal of Computer Vision, 88(2):303?338.C.
Galleguillos and S. Belongie.
2010.
Context basedobject categorization: A critical survey.
ComputerVision and Image Understanding, 114(6):712?722.A.
Gatt and E. Reiter.
2009.
SimpleNLG: A real-isation engine for practical applications.
In Pro-ceedings of the 12th European Workshop on Nat-ural Language Generation, pages 90?93.
Associa-tion for Computational Linguistics.W.
Kim, J.
Park, and C. Kim.
2010.
A novel methodfor efficient indoor?outdoor image classification.Journal of Signal Processing Systems, pages 1?8.A.
Kojima, T. Tamura, and K. Fukunaga.
2002.
Nat-ural language description of human activities fromvideo images based on concept hierarchy of ac-tions.
International Journal of Computer Vision,50(2):171?184.P.
Kuchi, P. Gabbur, P. SUBBANNA BHAT, et al2002.
Human face detection and tracking using skincolor modeling and connected component opera-tors.
IETE journal of research, 48(3-4):289?293.M.W.
Lee, A. Hakeem, N. Haering, and S.C. Zhu.2008.
Save: A framework for semantic annota-tion of visual events.
In Computer Vision and Pat-tern Recognition Workshops.
CVPRW?08, pages 1?8.
IEEE.C.Y.
Lin.
2004.
Rouge: A package for automatic eval-uation of summaries.
In WAS.I.
Maglogiannis, D. Vouyioukas, and C. Aggelopoulos.2009.
Face detection and recognition of natural hu-man emotion using markov random fields.
Personaland Ubiquitous Computing, 13(1):95?101.H.H.
Nagel.
2004.
Steps toward a cognitive visionsystem.
AI Magazine, 25(2):31.I.
Nwogu, Y. Zhou, and C. Brown.
2011.
Disco: De-scribing images using scene contexts and objects.In Twenty-Fifth AAAI Conference on Artificial In-telligence.P.
Over, W. Kraaij, and A.F.
Smeaton.
2007.
Trecvid2007: an introduction.
In TREC Video retrievalevaluation online proceedings.M.S.
Ryoo and J.K. Aggarwal.
2009.
Semantic rep-resentation and recognition of continued and recur-sive human activities.
International journal of com-puter vision, 82(1):1?24.J.R.J.
Schirra, G. Bosch, CK Sung, and G. Zimmer-mann.
1987.
From image sequences to naturallanguage: a first step toward automatic perceptionand description of motions.
Applied Artificial Intel-ligence an International Journal, 1(4):287?305.A.
Sundaresan, A. RoyChowdhury, and R. Chellappa.2003.
A hidden markov model based framework forrecognition of humans from gait sequences.
In In-ternational Conference on Image Processing, ICIP2003, volume 2.
IEEE.A.
Torralba, K.P.
Murphy, W.T.
Freeman, and M.A.Rubin.
2008.
Context-based vision system forplace and object recognition.
In Ninth IEEE In-ternational Conference on Computer Vision, pages273?280.
IEEE.P.
Viola and M. Jones.
2001.
Rapid object detectionusing a boosted cascade of simple features.
In IEEEComputer Society Conference on Computer Visionand Pattern Recognition, volume 1.Y.
Yang, C.L.
Teo, H. Daume?
III, C. Fermu?ller, andY.
Aloimonos.
2011.
Corpus-guided sentence ger-eration of natural images.
In EMNLP.B.Z.
Yao, X. Yang, L. Lin, M.W.
Lee, and S.C. Zhu.2010.
I2t: Image parsing to text description.
Pro-ceedings of the IEEE, 98(8):1485?1508.W.
Zheng, H. Tang, Z. Lin, and T. Huang.
2010.
Emo-tion recognition from arbitrary view facial images.Computer Vision?ECCV 2010, pages 490?503.35
