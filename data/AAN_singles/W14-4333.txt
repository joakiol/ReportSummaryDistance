Proceedings of the SIGDIAL 2014 Conference, pages 251?253,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsSAWDUST: a Semi-Automated Wizard Dialogue Utterance Selection Toolfor domain-independent large-domain dialogueSudeep Gandhe David TraumUniversity of Southern California, Institute for Creative Technologiessrgandhe@gmail.com, traum@ict.usc.eduAbstractWe present a tool that allows human wiz-ards to select appropriate response utter-ances for a given dialogue context froma set of utterances observed in a dia-logue corpus.
Such a tool can be usedin Wizard-of-Oz studies and for collectingdata which can be used for training and/orevaluating automatic dialogue models.
Wealso propose to incorporate such automaticdialogue models back into the tool as anaid in selecting utterances from a large di-alogue corpus.
The tool allows a user torank candidate utterances for selection ac-cording to these automatic models.1 MotivationDialogue corpora play an increasingly importantrole as a resource for dialogue system creation.In addition to its traditional roles, such as train-ing language models for speech recognition andnatural language understanding, the dialogue cor-pora can be directly used for the selection ap-proach to response formation (Gandhe and Traum,2010).
In the selection approach, the response isformulated by simply picking the appropriate ut-terance from a set of previously observed utter-ances.
This approach is used in many wizard ofoz systems, where the wizard presses a button toselect an utterance, as well as in many automateddialogue systems (Leuski et al., 2006; Zukermanand Marom, 2006; Sellberg and J?onsson, 2008)The resources required for the selection ap-proach are a set of utterances to choose from andoptionally, a set of pairs of ?context, responseutterance?
to train automatic dialogue models.
Awizard can generate such resources by performingtwo types of tasks.
First is the traditional Wizard-of-Oz dialogue collection, where a wizard inter-acts with a user of the dialogue system.
Here thewizard selects an appropriate response utterancefor a context that is being updated in a dynamicfashion as the dialogue proceeds (dynamic contextsetting).
The second task is geared towards gather-ing data for training/evaluating automatic dialoguemodels, where a wizard is required to select ap-propriate responses (perhaps more than one) for acontext which is extracted from a human-humandialogue.
The context does not change based onthe wizard?s choices (static context setting).A wizard tool should help with the challengespresented by these tasks.
A challenge for bothof these tasks is that if the number of utterancesin the corpus is large (e.g., more than the num-ber of buttons that can be placed on a computerscreen), it may be very difficult for a wizard to lo-cate appropriate utterances.
For the second task ofcreating human-verified training/evaluation data,tools like NPCEditor (Leuski and Traum, 2010)have been developed which, allow the tagging ofa many to many relationships between contexts(approximated simply as input utterance) and re-sponses.
In other cases, a corpus of dialogues isused to acquire the set of selectable utterances, inwhich each context is followed by a single nextutterance, and many utterances appear only once.This sparsity of data makes the selection task hard.Moreover, it may be the case that there are manypossible continuations of a context or contexts inwhich an utterance may be appropriate (DeVaultet al., 2011).We address these needs with a semi-automatedwizard tool that allows a wizard to engage in dy-namic or static context utterance selection, selectmultiple responses, and use several kinds of searchtools to locate promising utterances from a largeset that can?t all be displayed or remembered.
Inthe next section we describe the tool and how itcan be used.
Then we describe how this tool wasused to create evaluation data in the static contextsetting.251Figure 1: A screenshot of the interface for the wizard data collectionin static context setting.Figure 2: A Histogram for thenumber of selected appropriateresponses.Figure 3: Avg.
cardinality of theset for different values of |R|.2 Wizard ToolOur wizard tool consists of several different views(see figure 1), and is similar in some respects to theIORelator annotation tool (DeVault et al., 2010),but specialized to act as a wizard interface.
Thefirst view (left pane) is a dialogue context, thatshows the recent history of the dialogue, beforethe wizard?s decision point.
The second view (topright pane) shows a list of possible utterances thatcan be selected from.
This view can be orderedin several different ways, as described below.
Fi-nally, there is a view of selected utterances (bot-tom right pane).
In the case of dynamic context,the wizard will probably only select one utteranceand then a dialogue partner will respond with anew utterance that extends the previous context.In the case of static evaluation, however, used fortraining and/or evaluating automated selection al-gorithms, it is often helpful to select multiple ut-terances if more than one is appropriate.To help wizards explore the set of all possibleutterances, we provide the ability to rank the utter-ances by various automated scores.
Our configu-ration used in the static context task uses Score1 asthe score calculated using one of the automatic di-alogue models, specifically Nearest Context model(Gandhe and Traum, 2007) - this model orderscandidate utterances from the corpus by the sim-ilarity of their previous two utterances to the cur-rent dialogue context.
Score2 is surface text sim-ilarity, computed as the METEOR score (Lavieand Denkowski, 2009) between the candidate ut-terance and the actual response utterance presentat that location in original human-human dialogue(which is not available to the wizard).
Wizards canalso search the set of utterances for specific key-words and the third column, Relevance, shows thescore for the search string entered by the wizards.The last column RF stands for relevance feedbackand ranks the utterances by similarity to the utter-ances that have already been chosen by the wiz-ard.
This allows wizards to easily find paraphrasesof already selected response utterances.
Clickingthe header of any of these columns will reorder theutterance list by the automated score, by relevance(assuming a search term has been entered) or byrelevance feedback (assuming one or more utter-ances have already been chosen).3 EvaluationWe evaluated the tool by having four human vol-unteers (wizards) use it in order to establish an up-per baseline for human-level performance in thestatic context evaluation task described in (Gandheand Traum, 2013).
Wizards were instructed in howto use the search and relevance feedback features.In order to not bias the wizards, they were not toldexactly what score1 and score2 indicate, but justthat the scores can be useful in search.Each wizard is presented with a set of utter-ances (Utrain) (|Utrain| ?
500) and is asked toselect a subset from these that will be appropri-ate as a response for the presented dialogue con-text.
Each wizard was requested to select some-where between 5 to 10 (at-least one) appropriateresponses for each dialogue context extracted from252five different human-human dialogues.
There area total of 89 dialogue contexts for the role thatthe wizards were to play.
Figure 2 shows the his-togram for the number of utterances selected asappropriate responses by the four wizards.
As ex-pected, wizards frequently chose multiple utter-ances as appropriate responses (mean = 7.80, min= 1, max = 25).To get an idea about how much the wizardsagree among themselves for this task, we calcu-lated the overlap between the utterances selectedby a specific wizard and the utterances selected byanother wizard or a set of wizards.
Let UTcbe a setof utterances selected by a wizard T for a dialoguecontext c. Let R be a set of wizards (T /?
R) andURcbe the union of sets of utterances selected bythe set of wizards (R) for the same context c. Thenwe define the following overlap measures,Precisionc=|UTc?
URc||UTc|Recallc=|UTc?
URc||URc|Jaccardc=|UTc?
URc||UTc?
URc|Dicec=2|UTc?
URc||UTc|+ |URc|Meteorc=1|UTc|?utMETEOR (ut, URc) ?ut?
UTcWe compute the average values of these over-lap measures for all contexts and for all possiblesettings of test wizards and reference wizards.
Ta-ble 1 shows the results with different values for thenumber of wizards used as reference.#ref Prec.
Rec.
Jacc.
Dice Meteor1 0.145 0.145 0.077 0.141 0.2902 0.244 0.134 0.093 0.170 0.4123 0.311 0.121 0.094 0.171 0.478Table 1: Inter-wizard agreementPrecision can be interpreted as the probabilitythat a response utterance selected by a wizard isalso considered appropriate by at least one otherwizard.
Precision rapidly increases along withthe number of reference wizards used.
This hap-pens because the size of the set URcsteadily in-creases with more reference wizards.
Figure 3shows this observed increase and the expected in-crease if there were no overlap between the wiz-ards.
The near-linear increase in |URc| suggeststhat selecting appropriate responses is a hard taskand may require a lot more than four wizards toachieve convergence.Subjectively, the wizards reported no major us-ability problems with the tool, and were able touse all four utterance ordering techniques to findappropriate utterances.4 Future WorkFuture work involves performing some formalevaluations comparing this tool to other tools (thatare missing some of the features of this tool) interms of amount of time taken to make selectionsand quality of the selections, using the same eval-uation techniques as (Gandhe and Traum, 2013).We also see a promising future for semi-automated selection, which blurs the line betweena pure algorithmic response and pure wizard se-lection.
Here the wizard can select appropriate re-sponses, which can be used by algorithms as su-pervised training data, meanwhile the algorithmscan be used to seed the wizard?s selection.ReferencesDavid DeVault, Susan Robinson, and David Traum.
2010.IORelator: A graphical user interface to enable rapid se-mantic annotation for data-driven natural language under-standing.
In Proc.
of the 5th Joint ISO-ACL/SIGSEMWorkshop on Interoperable Semantic Annotation (ISA-5).David DeVault, Anton Leuski, and Kenji Sagae.
2011.
Anevaluation of alternative strategies for implementing dia-logue policies using statistical classification and rules.
InProceedings of the IJCNLP 2011, Nov.Sudeep Gandhe and David Traum.
2007.
Creating spokendialogue characters from corpora without annotations.
InProceedings of Interspeech-07, Antwerp, Belgium.Sudeep Gandhe and David Traum.
2010.
I?ve said it be-fore, and I?ll say it again: an empirical investigation ofthe upper bound of the selection approach to dialogue.
InProceedings of the SIGDIAL ?10, Tokyo, Japan.Sudeep Gandhe and David Traum.
2013.
Surface text baseddialogue models for virtual humans.
In Proceedings of theSIGDIAL 2013, Metz, France.A.
Lavie and M. J. Denkowski.
2009.
The meteor metricfor automatic evaluation of machine translation.
MachineTranslation, 23:105?115.Anton Leuski and David R. Traum.
2010.
NPCEditor: Atool for building question-answering characters.
In Pro-ceedings of LREC 2010, Valletta, Malta.Anton Leuski, Ronakkumar Patel, David Traum, and Bran-don Kennedy.
2006.
Building effective question answer-ing characters.
In Proc.
of SIGDIAL ?06, Australia.Linus Sellberg and Arne J?onsson.
2008.
Using random in-dexing to improve singular value decomposition for latentsemantic analysis.
In Proceedings of LREC?08, Morocco.Ingrid Zukerman and Yuval Marom.
2006.
A corpus-basedapproach to help-desk response generation.
In Computa-tional Intelligence for Modelling, Control and Automation(CIMCA 2006), IAWTIC 2006.253
