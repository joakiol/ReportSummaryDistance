Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1088?1097,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsEQUATION PARSING : Mapping Sentences to Grounded EquationsSubhro Roy Shyam Upadhyay Dan RothUniversity of Illinois, Urbana Champaign{sroy9, upadhya3, danr}@illinois.eduAbstractIdentifying mathematical relations expressedin text is essential to understanding a broadrange of natural language text from electionreports, to financial news, to sport commen-taries to mathematical word problems.
Thispaper focuses on identifying and understand-ing mathematical relations described within asingle sentence.
We introduce the problemof Equation Parsing ?
given a sentence, iden-tify noun phrases which represent variables,and generate the mathematical equation ex-pressing the relation described in the sentence.We introduce the notion of projective equationparsing and provide an efficient algorithm toparse text to projective equations.
Our systemmakes use of a high precision lexicon of math-ematical expressions and a pipeline of struc-tured predictors, and generates correct equa-tions in 70% of the cases.
In 60% of the time,it also identifies the correct noun phrase ?variables mapping, significantly outperform-ing baselines.
We also release a new annotateddataset for task evaluation.1 IntroductionUnderstanding text often involves reasoning with re-spect to quantities mentioned in it.
Understandingthe news article statement in Example 1 requiresidentifying relevant entities and the mathematical re-lations expressed among them in text, and determin-ing how to compose them.
Similarly, solving a mathword problem with a sentence like Example 2, re-quires realizing that it deals with a single number,knowing the meaning of ?difference?
and compos-Example 1 Emanuel?s campaign contribu-tions total three times those of his opponentsput together.Example 2 Twice a number equals 25 lessthan triple the same number.Example 3 Flying with the wind , a bird wasable to make 150 kilometers per hour.Example 4 The sum of two numbers is 80.Example 5 There are 54 5-dollar and 10-dollar notes.ing the right equation ?
?25?
needs to be subtractedfrom a number only after it is multiplied by 3.As a first step towards understanding such rela-tions, we introduce the Equation Parsing task - givena sentence expressing a mathematical relation, thegoal is to generate an equation representing the rela-tion, and to map the variables in the equation to theircorresponding noun phrases.
To keep the problemtractable, in this paper we restrict the final outputequation form to have at most two (possibly coref-erent) variables, and assume that each quantity men-tioned in the sentence can be used at most once inthe final equation.1 In example 1, the gold out-put of an equation parse should be V1 = 3 ?
V2,with V1 = ?Emanuel?s campaign contributions?
andV2 = ?those of his opponents put together?.The task can be seen as a form of semantic parsing(Goldwasser and Roth, 2011; Kwiatkowski et al,2013) where instead of mapping a sentence to a logi-cal form, we want to map it to an equation.
However,1We empirically found that around 97% of sentences de-scribing a relation have this property.1088there are some key differences that make this prob-lem very challenging in ways that differ from the?standard?
semantic parsing.
In Equation Parsing,not all the components of the sentence are mappedto the final equation.
There is a need to identifynoun phrases that correspond to variables in the rela-tions and determine that some are irrelevant and canbe dropped.
Moreover, in difference from seman-tic parsing into logical forms, in Equation Parsingmultiple phrases in the text could correspond to thesame variable, and identical phrases in the text couldcorrespond to multiple variables.We call the problem of mapping noun phrasesto variables the problem of grounding variables.Grounding is challenging for various reasons, keyamong them are that: (i) The text often does notmention ?variables?
explicitly, e.g., the sentencein example 3 describes a mathematical relation be-tween the speed of bird and the speed of wind, with-out mentioning ?speed?
explicitly.
(ii) Sometimes,multiple noun phrases could refer to the same vari-able.
For instance, in example 2, both ?a number?and ?the same number?
refer to the same variable.On the other hand, the same noun phrase might re-fer to multiple variables, as in example 4, where thenoun phrase ?two numbers?
refer to two variables.In addition, the task involves deciding which ofthe quantities identified in the sentence are relevantto the final equation generation.
In example 5, both?5?
and ?10?
are not relevant for the final equation?V1 + V2 = 54?.
Finally, the equation needs tobe constructed from a list of relevant quantities andgrounded variables.
Overall, the output space be-comes exponential in the number of quantities men-tioned in the sentence.Determining the final equation that correspondsto the text is an inference step over a very largespace.
To address this, we define the concept of?projectivity?
- a condition where the final equationcan be generated by combining adjacent numbers orvariables, and show that most sentences expressingmathematical relations exhibit the projectivity prop-erty.
Finally, we restrict our inference procedure toonly search over equations which have this property.Our approach builds on a pipeline of structuredpredictors that identify irrelevant quantities, recog-nize coreferent variables, and, finally, generate equa-tions.
We also leverage a high precision lexicon ofmathematical expressions and develop a greedy lex-icon matching strategy to guide inference.
We dis-cuss and exemplify the advantages of this approachand, in particular, explain where the ?standard?
NLPpipeline fails to support equation parsing, and ne-cessitates the new approach proposed here.
Anothercontribution of this work is the development of anew annotated data set for the task of equation pars-ing.
We evaluate our method on this dataset andshow that our method predicts the correct equationin 70% of the cases and that in 60% of the time wealso ground all variables correctly.The next section presents a discussion of relatedwork.
Next we formally describe the task of equa-tion parsing.
The following sections describe ourequation representation and the concept of projectiv-ity, followed by the description of our algorithm togenerate the equations and variable groundings fromtext.
We conclude with experimental results.2 Related WorkThe work most related to this paper is (Madaan et al,2016), which focuses on extracting relation tripleswhere one of the arguments is a number.
In contrast,our work deals with multiple variables and complexequations involving them.
There has been a lot of re-cent work in automatic math word problem solving(Kushman et al, 2014; Roy et al, 2015; Hosseiniet al, 2014; Roy and Roth, 2015).
These solverscannot handle sentences individually.
They requirethe input to be a complete math word problem, andeven then, they only focus on retrieving a set of an-swer values without mentioning what each answervalue corresponds to.
Our work is also conceptuallyrelated to work on semantic parsing ?
mapping natu-ral language text to a formal meaning representation(Wong and Mooney, 2007; Clarke et al, 2010; Caiand Yates, 2013; Kwiatkowski et al, 2013; Gold-wasser and Roth, 2011).
However, as mentionedearlier, there are some significant differences in thetask definition that necessitate the development of anew approach.3 The Equation Parsing TaskEquation parsing takes as input a sentence x describ-ing a single mathematical equation, comprising oneor two variables and other quantities mentioned in x.1089Let N be the set of noun phrases in the sentence x.The output of the task is the mathematical equationdescribed in x, along with a mapping of each vari-able in the equation to its corresponding noun phrasein N .
We refer to this mapping as the ?grounding?of the variable; the noun phrase represents what thevariable stands for in the equation.
Table 1 givesan example of an input and output for the equationparsing of the text in example 2.
Since an equationcan be written in various forms, we use the formwhich most agrees with text, as our target output.So, for example 1, we will choose V1 = 3?
V2 andnot V2 = V1 ?
3.
In cases where several equationforms seem to be equally likely to be the target equa-tion, we randomly choose one of them, and keep thischoice consistent across the dataset.The Equation Parsing TaskInput Twice a number equals 25 less thantriple the same number.Output 2?
V1 = (3?
V1)?
25 (Equation)V1 = ?a number?
(Grounding)Table 1: Input and output for Equation Parsing3.1 Equation Parse RepresentationIn this section, we introduce an equation parse fora sentence.
An equation parse of a sentence x isa pair (T,E), where T represents a set of triggersextracted from x, and E represents an equation treeformed with the set T as leaves.
We now describethese terms in detail.Trigger Given a sentence xmentioning a mathemat-ical relation, a trigger can either be a quantity triggerexpressed in x, or variable trigger which is a nounphrase in x corresponding to a variable.
A quantitytrigger is a tuple (q, s), where q is the numeric valueof the quantity mentioned in text, and s is the spanof text from the sentence x which refers to the quan-tity.
A variable trigger is a tuple (l, s), where l rep-resents the label of the variable, and s represents thenoun phrase representing the variable.
For example,for the sentence in Fig 1, the spans ?Twice?, ?25?,and ?triple?
generate quantity triggers, whereas ?anumber?
and ?the same number?
generate variabletriggers, with label V1.Trigger List The trigger list T for a sentence x con-tains one trigger for each variable mention and eachnumeric value used in the final equation expressedNotation DefinitionQuantity Trigger Mention of a quantity in textVariable Trigger Noun phrase coupled with variablelabelTrigger Quantity or variable triggerQuantity TriggerListList of quantity triggers, one for eachnumber mention in equationVariable TriggerListList of variable triggers, one for eachvariable mention in equationTrigger List Union of quantity and variable trig-ger listEquation Tree Binary tree representation of equa-tionlc(n), rc(n) Left and right child of node nEXPR(n) Expression represented by node n(n) Operation at node nORDER(n) Order of operation at node nLocation(n) Character offset of trigger represent-ing leaf node nSpan-Start(n),Span-End(n)Start and end character offsets ofspan covered by node nTable 2: Summary of notations used in the paperby the sentence x.
The trigger list might consist ofmultiple triggers having the same label, or extractedfrom the same span of text.
In the example sentencein Fig 1, the trigger list comprises two triggers hav-ing the same label V1.
The final trigger list for theexample in Fig 1 is {(2, ?2?
), (V1, ?a number?
), (25,?25?
), (3, ?triple?
), (V1, ?the same number?)}.
Notethat there can be multiple valid trigger lists.
In ourexample, we could have chosen both variable trig-gers to point to the same mention ?a number?.
Quan-tity triggers in the trigger list form the quantity trig-ger list, and the variable triggers in trigger list formthe variable trigger list.Equation Tree An equation tree of a sentence x isa binary tree whose leaves constitute the trigger listof x, and internal nodes (except the root) are labeledwith one of the following operations ?
addition, sub-traction, multiplication, division.
In addition, fornodes which are labeled with subtraction or division,we maintain a separate variable to determine orderof its children.
The root of the tree is always labeledwith the operation equal.An equation tree is a natural representation for anequation.
Each node n in an equation tree repre-sents an expression EXPR(n), and the label of theparent node determines how the expressions of itschildren are to be composed to construct its own ex-pression.
Let us denote the label for a non-leaf node1090Twice a number equals 25 less than triple the same number.SentenceTrigger ListEquation Tree2 V1 25 3 V1?=?r?Figure 1: A sentence with its trigger list and equation tree.
?r indicates subtraction with order rl.n to be (n), where (n) ?
{+,?,?,?,=} andthe order of a node n?s children by ORDER(n) (de-fined only for subtraction and division nodes), whichtakes values lr (Left-Right) or rl (Right-Left).
For aleaf node n, the expression EXPR(n) represents thevariable label, if n is a variable trigger, and the nu-meric value of the quantity, if it is a quantity trigger.Finally, we use lc(n) and rc(n) to represent the leftand right child of node n, respectively.
The equationrepresented by the tree can be generated as follows.For all non-leaf nodes n, we haveEXPR(n) =????????????????????
?EXPR(lc(n))(n) EXPR(rc(n))if (n) ?
{+,?,=}EXPR(lc(n))(n) EXPR(rc(n))if (n) ?
{?,?}
?
ORDER(n) = lrEXPR(rc(n))(n) EXPR(lc(n))if (n) ?
{?,?}
?
ORDER(n) = rl(1)Given an equation tree T of a sentence, the equationrepresented by it is the expression generated by theroot of T (following Equation 1).
Referring to theequation tree in Fig 1, the node marked ??r?
repre-sents (3?
V1)?
25, and the root represents the fullequation 2?
V1 = (3?
V1)?
25.4 ProjectivityFor each leaf n of an equation tree T , we de-fine a function Location(?
), to indicate the posi-tion of the corresponding trigger in text.
We alsodefine for each node n of equation tree T , func-tions Span-Start(n) and Span-End(n) to denotethe minimum span of text containing the leaves ofthe subtree rooted at n. We define them as follows:Span-Start(n) =????
?Location(n) if n is a leafmin(Span-Start(lc(n)), Span-Start(rc(n)))otherwiseSpan-End(n) =????
?Location(n) if n is a leafmax(Span-End(lc(n)), Span-End(rc(n)))otherwiseAn equation tree T is called projec-tive iff for every node n of T , eitherSpan-End(lc(n)) ?
Span-Start(rc(n)) orSpan-End(rc(n)) ?
Span-Start(lc(n)).
In otherwords, the span of the left child and the right childcannot intersect in a projective equation tree2.The key observation, as our corpus analysis indi-cates, is that for most sentences, there exists a trig-ger list, such that the equation tree representing therelation in the sentence is projective.
However thismight involve mapping two mentions of the samevariable to different noun phrases.
Figure 1 showsan example of a projective equation tree, which re-quires different mentions of V1 to be mapped to dif-ferent noun phrases.
If we had mapped both men-tions of V1 to same noun phrase ?a number?, theresulting equation tree would not have been projec-tive.
We collected 385 sentences which representan equation with one or two mentions of variables,and each number in the sentence used at most oncein the equation.
We found that only one sentenceamong these could not generate a projective equa-tion tree.
(See Section 6.1 for details on dataset2This is more general than the definition of projective treesused in dependency parsing (McDonald et al, 2005).1091creation).
Therefore, we develop an algorithmic ap-proach for predicting projective equation trees, andshow empirically that it compares favourably withones which do not make the projective assumption.5 Predicting Equation ParseEquation parsing of a sentence involves predictingthree components ?
Quantity Trigger List, VariableTrigger List and Equation Tree.
We develop threestructured prediction modules to predict each of theabove components.All our prediction modules take a similar form:given input x and output y, we learn a scoring func-tion fw(x, y), which scores how likely is the outputy given input x.
The scoring function fw(x, y) islinear, fw(y) = wT?
(x, y), where ?
(x, y) is a fea-ture vector extracted from x and y.
The inferenceproblem, that is, the prediction y?
for an input x isthen: y?
= argmaxy?Y fw(y), where Y is the set ofall allowed values of y.5.1 Predicting Quantity Trigger ListGiven input text and the quantities mentioned in it,the role of this step is to identify , for each quan-tity in the text, whether it should be part of the fi-nal equation.
For instance, in example 5 in Section1, both ?5?
and ?10?
are not relevant for the finalequation ?V1 + V2 = 54?.
Similarly, in example4, the number ?two?
is irrelevant for the equation?V1 + V2 = 80?.We define for each quantity q in the sentence, aboolean value Relevance(q), which is set to trueif q is relevant for the final equation, and to falseotherwise.
For the structured classification, the in-put x is the sentence along with a set of recognizedquantities mentioned in it, and the output y is therelevance values for all quantities in the sentence.We empirically found that predicting all relevancevalues jointly performs better than having a binaryclassifier predict each one separately.
The featurefunction ?
(x, y) used for the classification gener-ates neighborhood features (from neighborhood ofq) and quantity features (properties of the quantitymention).
Details added to the appendix.5.2 Predicting Variable Trigger ListThe goal of this step is to predict the variable triggerlist for the equation.
Our structured classifier takesas input the sentence x, and the output y is eitherone or two noun-phrases, representing variables inthe final equation.
As we pointed out earlier, mul-tiple groundings might be valid for any given vari-able, hence there can be multiple valid variable trig-ger lists.
For every sentence x, we construct a set Yof valid outputs.
Each element in Y corresponds toa valid variable trigger list.
Finally, we aim to outputonly one of the elements of Y .We modified the standard structured prediction al-gorithm to consider ?superset supervision?
and takeinto account multiple gold structures for an input x.We assume access to N training examples of theform : (x1, Y1), (x2, Y2), .
.
.
, (xN , YN ), where eachYi is a set of valid outputs for the sentence xi.
Sincewe want to output only one variable trigger list, wewant to score at least one y from Yi higher than allother possible outputs, for each xi.
We use a modi-fied latent structured SVM to learn the weight vectorw.
The algorithm treats the best choice among all ofYi as a latent variable.
At each iteration, for all xi,the algorithm chooses the best choice y?i from theset Yi, according to the weight vector w. Then, wis updated by learning on all (xi, y?i ) by a standardstructured SVM algorithm.
The details of the algo-rithm are in Algorithm 1.
The distinction from stan-Algorithm 1 Structural SVM with Superset Super-visionInput: Training data T ={(x1, Y1), (x2, Y2), .
.
.
, (xN , YN )}Output: Trained weight vector w1: w ?
w02: repeat3: T ?
?
?4: for all (xi, Yi) ?
T do5: y?i ?
argmaxy?Yi wT?
(xi, y)6: T ?
?
T ?
?
{(xi, y?i )}7: end for8: Update w by running standard StructuralSVM algorithm on T ?9: until convergence10: return wdard latent structural SVM is in line 5 of Algorithm1.
In order to get the best choice y?i for input xi, wesearch only inside Yi, instead of all of Y .
A similarformulation can be found in Bjo?rkelund and Kuhn1092(2014).
The features ?
(x, y) used for variable trig-ger prediction include variable features (propertiesof noun phrase indicating variable) and neighbor-hood features (lexical features from neighborhoodof variable mention).
Details added to the appendix.If the output of the classifier is a pair of nounphrases, we use a rule based variable coreferencedetector, to determine whether both noun phrasesshould have the same variable label or not.
The rulesfor variable coreference are as follows :1.
If both noun phrases are the same, and they donot have the token ?two?
or ?2?, they have thesame label.2.
If the noun phrases are different, and the nounphrase appearing later in the sentence containstokens ?itself?, ?the same number?, they havethe same label.3.
In all other cases, they have different labels.Finally, each noun phrase contributes one variabletrigger to the variable trigger list.5.3 Predicting Equation TreeIt is natural to assume that the syntactic parse of thesentence could be very useful in addressing all thepredictions we are making in the equation parsingtasks.
However, it turns out that this is not the case?
large portions of the syntactic parse will not bepart of the equation parse, hence we need the afore-mentioned modules to address this.
Nevertheless,in the next task of predicting the equation tree, weattempted to constraint the output space using guid-ance from the syntactic tree; we found, though, thateven enforcing this weak level of output expectationis not productive.
This was due to the poor perfor-mance of current syntactic parsers on the equationdata (eg., in 32% of sentences, the Stanford parsermade a mistake which does not allow recovering thecorrect equation).The tree prediction module receives the trigger listpredicted by the previous two modules, and the goalis to create an equation tree using the trigger list asthe leaves of that tree.
The input x is the sentenceand the trigger list, and the output y is the equationtree representing the relation described in the sen-tence.
We assume that the output will be a projectiveequation tree.
For features ?
(x, y), we extract foreach non-leaf node n of the equation tree y, neigh-borhood features (from neighborhood of node spansof n?s children), connecting text features (from textbetween the spans of n?s children) and number fea-tures (properties of number in case of leaf nodes).Details are included in the appendix.The projectivity assumption implies that the finalequation tree can be generated by combining onlyadjacent nodes, once the set of leaves is sorted basedon Span-Start(?)
values.
This allows us to use CKYalgorithm for inference.
A natural approach to fur-ther reduce the output space is to conform to theprojective structure of the syntactic parse of the sen-tence.
However, we found this to adversely affectperformance, due to the poor performance of syn-tactic parser on equation data.Lexicon To bootstrap the equation parsing process,we developed a high precision lexicon to translatemathematical expressions to operations and orders,like ?sum of A and B?
translates to ?A+B?, ?A minusB?
translates to ?A-B?, etc.
(where A and B denoteplaceholder numbers or expressions).
At each stepof CKY, while constructing a node n of the equationtree, we check for a lexicon text expression corre-sponding to node n. If found, we allow only thecorresponding operation (and order) for node n, anddo not explore other operations or orders.
We showempirically that reducing the space using this greedylexicon matching help improve performance.
Wefound that using the lexicon rules as features insteadof hard constraints do not help as much.
Note thatour lexicon comprises only generic math concepts,and around 50% of the sentences in our dataset donot contain any pattern from the lexicon.Finally, given input sentence, we first predict thequantity trigger and the variable trigger lists.
Giventhe complete trigger list, we predict the equation treerelating the components of the trigger list.5.4 AlternativesA natural approach could be to jointly learn to pre-dict all three components, to capture the dependen-cies among them.
To investigate this, we developeda structured SVM which predicts all componentsjointly, using the union of the features of each com-ponent.
We use approximate inference, first enumer-ating possible trigger lists, and then equation trees,1093and find the best scoring structure.
However, thismethod did not outperform the pipeline method.
Theworse performance of joint learning is due to: (1)search space being too large for the joint model to dowell given our dataset size of 385, and (2) our inde-pendent classifiers being good enough, thus support-ing better joint inference.
This tradeoff is stronglysupported in the literature (Punyakanok et al, 2005;Sutton and McCallum, 2007).Another option is to enforce constraints betweentrigger list predictions, such as, variable triggersshould not overlap with the quantity triggers.
How-ever, we noticed that often noun phrases returnedby the Stanford parser were noisy, and would in-clude neighboring numbers within the extractednoun phrases.
This prevented us from enforcingsuch constraints.6 Experimental ResultsWe now describe the data set, and the annotationprocedure used.
We then evaluate the system?s per-formance on predicting trigger list, equation tree,and the complete equation parse.6.1 DatasetWe created a new dataset consisting of 385 sen-tences extracted from algebra word problems and fi-nancial news headlines.
For algebra word problems,we used the MIT dataset (Kushman et al, 2014),and two high school mathematics textbooks, Ele-mentary Algebra (College of Redwoods) and Begin-ning and Intermediate Algebra (Tyler Wallace).
Fi-nancial news headlines were extracted from The Lat-est News feed of MarketWatch, over the month ofFebruary, 2015.
All sentences with information de-scribing a mathematical relation among at most two(possibly coreferent) variables, were chosen.
Next,we pruned sentences which require multiple uses ofa number to create the equation.
This only removeda few time related sentences like ?In 10 years, Johnwill be twice as old as his son.?.
We empiricallyfound that around 97% of sentences describing a re-lation fall under the scope of our dataset.The annotators were shown each sentence pairedwith the normalized equation representing the rela-tion in the sentence.
For each variable in the equa-tion, the annotators were asked to mark spans oftext which best describe what the variable repre-sents.
The annotation guidelines are provided inthe appendix.
We wanted to consider only nounphrase constituents for variable grounding.
There-fore, for each annotated span, we extracted the nounphrase with maximum overlap with the span, andused it to represent the variables.
Finally, a tu-ple with each variable being mapped to one of thenoun phrases representing it, forms a valid outputgrounding (variable trigger list).
We computed inter-annotator agreement on the final annotations whereonly noun phrases represent variables.
The agree-ment (kappa) was 0.668, indicating good agreement.The average number of mention annotations per sen-tence was 1.74.6.2 Equation Parsing ModulesIn this section, we evaluate the performance of theindividual modules of the equation parsing process.We report Accuracy - the fraction of correct predic-tions.
Table 3 shows the 5-fold cross validation ac-curacy of the various modules.
In each case, we alsoreport accuracy by removing each feature group, oneat a time.
In addition, for equation tree prediction,we also show the effect of lexicon, projectivity, con-forming to syntactic parse constraints, and using lex-icon as features instead of hard constraints.
For allour experiments, we use the Stanford Parser (Socheret al, 2013), the Illinois POS tagger (Roth and Ze-lenko, 1998) and the Illinois-SL structured predic-tion package (Chang et al, 2015).6.3 Equation Parsing ResultsIn this section, we evaluate the performance of oursystem on the overall equation parsing task.
We re-port Equation Accuracy - the fraction of sentencesfor which the system got the equation correct, andEquation+Grounding Accuracy - the fraction of sen-tences for which the system got both the equationand the grounding of variables correct.
Table 4shows the overall performance of our system, on a5-fold cross validation.
We compare against JointLearning - a system which jointly learns to predictall relevant components of an equation parse (Sec-tion 5.4).
We also compare with SPF (Artzi andZettlemoyer, 2013), a publicly available semanticparser, which can learn from sentence-logical formpairs.
We train SPF with sentence-equation pairs1094Quantity Trigger List Prediction AccuracyAll features 95.3No Neighborhood features 42.5No Quantity features 93.2Variable Trigger List Prediction AccuracyAll features 75.5No Variable features 58.6No Neighborhood features 70.3Equation Tree Prediction AccuracyAll features 78.9No Neighborhood features 64.3No Connecting Text features 70.2No Number features 77.6No Lexicon 72.7No Projectivity 72.8Conform with Syntactic Parse 70.2Lexicon as Features 74.5Table 3: Performance of system componentsSource EquationAccuracyEquation +GroundingAccuracyOur System 71.3 61.2Joint Learning 60.9 50.0SPF 3.1 N/ATable 4: Performance on equation parsingand a seed lexicon for mathematical terms (similar toours), and report equation accuracy.
Our structuredpredictors pipeline approach is shown to be superiorto both Joint Learning and SPF.SPF gets only a few sentences correct.
We at-tribute this to the inability of SPF to handle over-lapping mentions (like in Example 4), as well as itsapproach of parsing the whole sentence to the fi-nal output form.
The developers of SPF also con-firmed 3 that it is not suitable for equation parsingand that these results are expected.
Since equationparsing is a more involved process, a slight adapta-tion of SPF does not seem possible, necessitating amore involved process , of the type we propose.
Ourapproach, in contrast to SPF, can handle overlappingmentions, selects triggers from text, and parses thetrigger list to form equations.3Private communication6.4 Error AnalysisFor variable trigger list prediction, around 25% ofthe errors were due to the predictor choosing a spanwhich is contained within the correct span, e.g.,when the target noun phrase is ?The cost of a child?sticket?, our predictor chose only ?child?s ticket?.Although this choice might be sufficient for down-stream tasks, we consider it to be incorrect in ourcurrent evaluation.
Another 25% of the errors weredue to selection of entities which do not participatein the relation.
For example, in ?A rancher raises 5times as many cows as horses.
?, our predictor chose?A rancher?
and ?cows?
as variables, whereas therelation exists between ?cows?
and ?horses?.
Forthe prediction of the equation tree, we found that35% of the errors were due to rare math conceptsexpressed in text.
For example, ?7 dollars short ofthe price?
represents 7 dollars should be subtractedfrom the price.
These errors can be handled by care-fully augmenting the lexicon.
Another 15% of theerrors were due to lack of world knowledge, requir-ing understanding of time, speed, and distance.7 ConclusionThis paper investigates methods that identify andunderstand mathematical relations expressed intext.
We introduce the equation parsing task, whichinvolves generating an equation from a sentenceand identifying what the variables represent.
Wedefine the notion of projectivity, and construct ahigh precision lexicon, and use these to reduce theequation search space.
Our experimental results arequite satisfying and raise a few interesting issues.
Inparticular, it suggests that predicting equation parsesusing a pipeline of structured predictors performsbetter than jointly trained alternatives.
As discussed,it also points out the limitation of the current NLPtools in supporting these tasks.
Our current formu-lation has one key limitation; we only deal withexpressions that are described within a sentence.Our future work will focus on lifting this restriction,in order to allow relations expressed across multiplesentences and multiple relations expressed in thesame sentence.
Code and dataset are availableat http://cogcomp.cs.illinois.edu/page/publication_view/800.1095AcknowledgementsThis work is funded by DARPA under agree-ment number FA8750-13-2-0008, and a grant fromthe Allen Institute for Artificial Intelligence (al-lenai.org).A FeaturesA.1 Quantity Trigger List PredictionThe feature function ?
(x, y) used for the classifica-tion generates the following features :1.
Neighborhood features : For each quantity qin the input sentence, we add unigrams and bi-grams generated from a window around q, partof speech tags of neighborhood tokens of q. Weconjoin these features with Relevance(q).2.
Quantity Features : For each quantity q, weadd unigrams and bigrams of the phrase repre-senting the quantity.
Also, we add a feature in-dicating whether the number is associated withnumber one or two, and whether it is the onlynumber present in the sentence.
These featuresare also conjoined with Relevance(q).A.2 Variable Trigger List PredictionThe features ?
(x, y) used for variable trigger predic-tion are as follows:1.
Variable features : Unigrams and bigramsgenerated from the noun phrase representingvariables, part of speech tags of tokens in nounphrase representing variables.2.
Neighborhood Features : Unigrams and POStags from neighborhood of variables.All the above features are conjoined with two labels,one denoting whether y has two variables or one,and the second denoting whether y has two variablesrepresented by the same noun phrase.A.3 Equation Tree PredictionFor features ?
(x, y), we extract for each non-leafnode n of the equation tree y, the following:1.
Neighborhood Features : Unigrams, bi-grams and POS tags from neighborhoodof Span-Start(lc(n)), Span-Start(rc(n)),Span-End(lc(n)) and Span-End(rc(n)),conjoined with (n) and ORDER(n).2.
Connecting Text Features : Unigrams,bigrams and part of speech tags betweenmin(Span-End(lc(n)),Span-End(rc(n)))and max(Span-Start(lc(n)),Span-Start(rc(n))), conjoined with (n) andORDER(n).3.
Number Features : In case we are combiningtwo leaf nodes representing quantity triggers,we add a feature signifying whether one num-ber is larger than the other.B Annotation GuidelinesThe annotators were shown each sentence pairedwith the normalized equation representing the rela-tion in the sentence.
For each variable in the equa-tion, the annotators were asked to mark spans of textwhich best describe what the variable represents.They were asked to annotate associated entities ifexact variable description was not present.
For in-stance, in example 3 (Section 1), the relation holdsbetween the speed of bird and the speed of wind.However, ?speed?
is not explicitly mentioned in thesentence.
In such cases, the annotators were askedto annotate the associated entities ?the wind?
and ?abird?
as representing variables.The guidelines also directed annotators to choosethe longest possible mention, in case they feel themention boundary is ambiguous.
As a result, inthe sentence, ?City Rentals rent an intermediate-sizecar for 18.95 dollars plus 0.21 per mile.
?, the phrase?City Rentals rent an intermediate-size car?
was an-notated as representing variable.
We allow multiplementions to be annotated for the same variable.
Inexample 2 (Section 1), both ?a number?
and ?thesame number?
were annotated as representing thesame variable.References[Artzi and Zettlemoyer2013] Yoav Artzi and Luke Zettle-moyer.
2013.
UW SPF: The University of WashingtonSemantic Parsing Framework.
[Bjo?rkelund and Kuhn2014] Anders Bjo?rkelund andJonas Kuhn.
2014.
Learning structured perceptronsfor coreference resolution with latent antecedents1096and non-local features.
In Proceedings of the 52ndAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers).
[Cai and Yates2013] Qingqing Cai and Alexander Yates.2013.
Semantic Parsing Freebase: Towards Open-domain Semantic Parsing.
In Proceedings of the Sec-ond Joint Conference on Lexical and ComputationalSemantics (*SEM).
[Chang et al2015] Kai-Wei Chang, Shyam Upadhyay,Ming-Wei Chang, Vivek Srikumar, and Dan Roth.2015.
Illinoissl: A JAVA library for structured pre-diction.
In Arxiv Preprint, volume abs/1509.07179.
[Clarke et al2010] J. Clarke, D. Goldwasser, M. Chang,and D. Roth.
2010.
Driving semantic parsing fromthe world?s response.
In Proc.
of the Conference onComputational Natural Language Learning (CoNLL),7.
[Goldwasser and Roth2011] D. Goldwasser and D. Roth.2011.
Learning from natural instructions.
In Proc.
ofthe International Joint Conference on Artificial Intelli-gence (IJCAI).
[Hosseini et al2014] Mohammad Javad Hosseini, Han-naneh Hajishirzi, Oren Etzioni, and Nate Kushman.2014.
Learning to solve arithmetic word problemswith verb categorization.
In Proc.
of the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP) 2014.
[Kushman et al2014] N. Kushman, L. Zettlemoyer,R.
Barzilay, and Y. Artzi.
2014.
Learning toautomatically solve algebra word problems.
In ACL.
[Kwiatkowski et al2013] Tom Kwiatkowski, EunsolChoi, Yoav Artzi, and Luke Zettlemoyer.
2013.Scaling semantic parsers with on-the-fly ontologymatching.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing.
[Madaan et al2016] A. Madaan, A. Mittal, Mausam,G.
Ramakrishnan, and S. Sarawagi.
2016.
Numericalrelation extraction with minimal supervision.
In Proc.of the Conference on Artificial Intelligence (AAAI).
[McDonald et al2005] Ryan McDonald, FernandoPereira, Kiril Ribarov, and Jan Hajic?.
2005.
Non-projective dependency parsing using spanning treealgorithms.
In Proceedings of the Conference onHuman Language Technology and Empirical Methodsin Natural Language Processing.
[Punyakanok et al2005] V. Punyakanok, D. Roth, W. Yih,and D. Zimak.
2005.
Learning and inference overconstrained output.
In Proc.
of the International JointConference on Artificial Intelligence (IJCAI), pages1124?1129.
[Roth and Zelenko1998] D. Roth and D. Zelenko.
1998.Part of speech tagging using a network of linear sepa-rators.
In Coling-Acl, The 17th International Confer-ence on Computational Linguistics, pages 1136?1142.
[Roy and Roth2015] S. Roy and D. Roth.
2015.
Solv-ing general arithmetic word problems.
In Proc.
ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).
[Roy et al2015] S. Roy, T. Vieira, and D. Roth.
2015.Reasoning about quantities in natural language.
Trans-actions of the Association for Computational Linguis-tics, 3.
[Socher et al2013] Richard Socher, John Bauer, Christo-pher D. Manning, and Andrew Y. Ng.
2013.
ParsingWith Compositional Vector Grammars.
In ACL.
[Sutton and McCallum2007] C. Sutton and A. McCallum.2007.
Piecewise pseudolikelihood for efficient train-ing of conditional random fields.
In Zoubin Ghahra-mani, editor, Proceedings of the International Confer-ence on Machine Learning (ICML), pages 863?870.Omnipress.
[Wong and Mooney2007] Y.-W. Wong and R. Mooney.2007.
Learning synchronous grammars for semanticparsing with lambda calculus.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 960?967, Prague, Czech Re-public, June.
Association for Computational Linguis-tics.1097
