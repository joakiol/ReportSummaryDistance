Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1370?1381, Dublin, Ireland, August 23-29 2014.Hybrid Grammars for Discontinuous ParsingMark-Jan NederhofSchool of Computer ScienceUniversity of St AndrewsKY16 9SX, UKHeiko VoglerDepartment of Computer ScienceTechnische Universit?at DresdenD-01062 Dresden, GermanyAbstractWe introduce the concept of hybrid grammars, which are extensions of synchronous grammars,obtained by coupling of lexical elements.
One part of a hybrid grammar generates linear struc-tures, another generates hierarchical structures, and together they generate discontinuous struc-tures.
This formalizes and generalizes some existing mechanisms for dealing with discontinuousphrase structures and non-projective dependency structures.
Moreover, it allows us to separatethe degree of discontinuity from the time complexity of parsing.1 IntroductionDiscontinuous phrases occur frequently in languages with relatively free word order, and adequate de-scription of their structure requires special care (Kathol and Pollard, 1995; M?uller, 2004).
Even forlanguages such as English, with a relatively rigid word order, there is a clear need for discontinuousstructures (McCawley, 1982; Stucky, 1987).Early treebanks for English (Marcus et al., 1993) have often represented discontinuity in a way thatmakes it tempting to ignore it altogether, certainly for the purposes of parsing, whereas recent approachestend to represent discontinuity in a more overt form, sometimes after transformation of existing treebanks(Choi and Palmer, 2010; Evang and Kallmeyer, 2011).
In many modern treebanks, discontinuous struc-tures have been given a prominent status (B?ohmov?a et al., 2000).Classes of trees without discontinuity can be specified as the sets of parse trees of context-free gram-mars (CFGs).
Somewhat larger classes can be specified by tree substitution grammars (Sima?an et al.,1994) and regular tree grammars (Brainerd, 1969; G?ecseg and Steinby, 1997).
Practical parsers for thesethree formalisms have running time O(n3), where n is the length of the input sentence.
Discontinuousstructures go beyond their strong generative capacity however.
Similarly, non-projective dependencystructures cannot be obtained by traditional dependency grammars.
See (Rambow, 2010) for discussionof the relation between constituent and dependency structures and see (Maier and Lichte, 2009) for acomparison of discontinuity and non-projectivity.One way to solve the above problems has been referred to as pseudo-projectivity, i.e.
a parser producesa projective structure, which in a second phase is transformed into a non-projective structure (Kahaneet al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005).
In particular, this may involvelifting, whereby one end point of a dependency link moves across a path of nodes.
A related idea fordiscontinuous phrase structure is the reversible splitting conversion of (Boyd, 2007).
See also (Johnson,2002; Campbell, 2004; Gabbard et al., 2006).As shown by (Nivre, 2009), the second phase of pseudo-projective dependency parsing can be inter-leaved with the first, by replacing the usual one-way input tape by an additional stack, or buffer.
WhereThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1370non-topmost positions from the parsing stack are moved back to the buffer, input positions are effectivelyswapped and non-projective dependency structures arise.Tree adjoining grammars (TAGs) can describe strictly larger classes of word order phenomena thanCFGs (Rambow and Joshi, 1997).
TAG parsers have a time complexity of O(n6) (Vijay-Shankar andJoshi, 1985).
However, the derived trees they generate are still continuous.
Although their derivationtrees may be argued to be discontinuous, these by themselves are not normally the desired syntacticstructures.
It was argued by (Becker et al., 1991) that further additions to TAGs are needed to obtainadequate descriptions of scrambling phenomena.An alternative is proposed by (Kallmeyer and Kuhlmann, 2012): a transformation is added that turns aderivation tree of a (lexicalized) TAG into a non-projective dependency structure.
A very similar mech-anism is used to obtain non-projective dependency structures using linear context-free rewriting systems(LCFRSs) (Kuhlmann, 2013) that are lexicalized.
In a LCFRS the synthesis of strings is normally spec-ified by yield functions associated with rules.
By an additional interpretation of the templates of theseyield functions in the algebra of dependency trees (with the overt lexical items as roots), the LCFRSgenerates both strings and (possibly non-projective) dependency structures.However, the running time of LCFRS parsers is generally very high, still polynomial in the sentencelength, but with a degree determined by properties of the grammar; difficulties involved in runningLCFRS parsers for natural languages are described by (Kallmeyer and Maier, 2013).It follows from the above that there is considerable freedom in the design of parsers that producediscontinuous structures for given input sentences.
One can distinguish between two main issues.
Thefirst is the formalism that guides the parsing of the input.
This determines a class of input (string)languages, which can be that of the context-free languages, or tree adjoining languages, etc.
We assumeparsing with any of these formalisms results in derivations of some sort.
The second main issue is themechanism that translates such derivations into discontinuous structures.This leads to a number of open questions that are all related.
First, what is, or should be, the divisionof labor between the parser producing the derivations and the mechanism turning those derivations intodiscontinuous structures?
If we want to achieve high degrees of discontinuity in the output structures,should the formalism for the input language be much more powerful than, say, context-free?
Or canhighly discontinuous structures be obtained equally well through ordinary CFGs in combination with anadvanced mechanism producing discontinuous structures out of derivations?Second, how should one approach the problem of finding the grammar (and grammar class) for theinput language and the mapping from derivations to structures if the only thing that is given is a treebank?A third question is which formalisms are suitable to formally describe mappings from derivations todiscontinuous structures.
Lastly, can we characterize the classes of output (tree-)languages for variouscombinations of input grammars and derivation-to-structure mappings?In this paper we provide one possible answer to these questions by a new type of formalism, which wecall hybrid grammars.
Such a grammar consists of a string grammar and a tree grammar.
Derivations arecoupled so as to achieve synchronous rewriting.
The input string language and the output tree languageare thereby straightforwardly defined.
Different from synchronous grammars (Shieber and Schabes,1990; Satta and Peserico, 2005) is that occurrences of terminal symbols are also coupled.
Therebythe linear order of the symbols in a derived string imposes an order on the coupled symbols in thesynchronously derived tree; this allows a straightforward specification of a discontinuous structure.One can define a hybrid grammar consisting of a simple macro grammar (Fischer, 1968) and a simplecontext-free tree grammar (Rounds, 1970), but various other combinations of a string grammar and a treegrammar are possible as well.
Due to lack of space we will here concentrate on only one kind of hybridgrammar, namely that consisting of a LCFRS as string grammar and a form of definite clause program astree grammar.
We will show that hybrid grammars that induce (finite) sets of hybrid trees can always beconstructed, even if the allowable derivations are severely restricted, and we discuss experiments.
Lastly,a negative result will be given, which shows that a certain linguistic phenomenon cannot be handled ifthe string grammar is too restricted.We cast our definitions in terms of hybrid trees, of which discontinuous phrase structures and non-1371projective dependency structures are special cases.1Thereby the generality of the framework is demon-strated.2 PreliminariesLet N = {0, 1, 2, .
.
.}
and N+= N \ {0}.
For each n ?
N+, we let [n] stand for the set {1, .
.
.
, n}, andwe let [0] stand for ?.
We write [n]0to denote [n] ?
{0}.
We fix an infinite list x1, x2, .
.
.
of pairwisedistinct variables.
We let X = {x1, x2, x3, .
.
.}
and Xk= {x1, .
.
.
, xk} for each k ?
N.A ranked set ?
is a set of symbols, associated with a rank function assigning a number rk?(?)
?
Nto each symbol ?
?
?.
A ranked alphabet is a ranked set with a finite number of symbols.
We let ?
(k)denote {?
?
?
| rk?(?)
= k}.The following definitions were inspired by (Seki and Kato, 2008).
The sets of terms and sequence-terms (s-terms) over ranked set ?, with variables in some set Y ?
X , are denoted by T?
(Y ) and T??
(Y ),respectively, and defined inductively as follows:(i) Y ?
T?
(Y ),(ii) if k ?
N, ?
?
?
(k)and si?
T??
(Y ) for each i ?
[k], then ?
(s1, .
.
.
, sk) ?
T?
(Y ), and(iii) if n ?
N and ti?
T?
(Y ) for each i ?
[n], then ?t1, .
.
.
, tn?
?
T??
(Y ).We let T?
?and T?stand for T??(?)
and T?(?)
respectively.
Throughout this paper, we use variables suchas s and sifor s-terms and variables such as t and tifor terms.
The justification for using s-terms asdefined here is that they provide the required flexibility for dealing with both strings (?
= ?
(0)) andunranked trees (?
= ?
(1)), in combination with derivational nonterminals.Concatenation of s-terms is given by ?t1, .
.
.
, tn?
?
?tn+1, .
.
.
, tn+m?
= ?t1, .
.
.
, tn+m?.
Sequencessuch as s1, .
.
.
, skor x1, .
.
.
, xkwill typically be abbreviated to s1,kor x1,k, respectively.
For ?
?
?
(0)we sometimes abbreviate ?
() to ?.In examples we also abbreviate ?t1, .
.
.
, tn?
to t1?
?
?
tn, that is, omitting the angle brackets and com-mas.
Moreover, we sometimes abbreviate ?(??)
to ?.
Whether ?
then stands for ?(??)
or for ?
() dependson whether ?
?
?
(1)or ?
?
?
(0), which will be clear from the context.Subterms in terms or s-terms are identified by positions; these can be formalized by a suitable refine-ment of the familiar notion of Gorn address.
The set of all positions in term t or in s-term s is denotedby pos(t) or pos(s), respectively.
The subset of pos(t) consisting of all positions where the label is insome set ?
?
?
is denoted by pos?
(t).3 Hybrid treesThe purpose of this section is to unify existing notions of non-projective dependency structures anddiscontinuous phrase structures, formalized using s-terms.We fix an alphabet ?
= ?
(1)and a subset ?
?
?.
A hybrid tree over (?,?)
is a pair h = (s,?s),where s ?
T?
?and?sis a total order on pos?(s).
In words, a hybrid tree combines hierarchical structure,in the form of an s-term over the full alphabet ?, with a linear structure, which can be seen as a stringover ?
?
?.
This string will be denoted by str(h).For discontinuous phrase structures, the elements of ?
would typically represent lexical items, andthe elements of ?
\ ?
would typically represent syntactic categories.
For non-projective dependencystructures, ?
would be equal to ?.
Simple examples of discontinuous phrase structures are presented inFigures 1 and 2.4 Basic grammatical formalismsThe concept of hybrid grammars is illustrated in Section 5, by coupling a class of string grammars and aclass of tree grammars.1Moreover, we need to avoid any confusion with the term ?discontinuous tree?
from (Bunt, 1996), which is characterizedby the notion of ?context daughter?, which is absent from our framework.
The term ?hybrid tree?
was used before by (Lu etal., 2008), also for a mixture of a tree structure and a linear structure, generated by a probabilistic model.
However, the linear?surface?
structure was obtained by a simple left-to-right tree traversal, whereas a meaning representation was obtained by aslightly more flexible traversal of the same tree.
The emphasis in the current paper is rather on separating the linear structurefrom the tree structure.1372VPVhat gearbeitetADVschnellhat schnell gearbeitetFigure 1: Hybrid tree for German ?[...
]hat schnell gearbeitet?
(?[...]
has workedquickly?
), after (Seifert and Fischer, 2004).The bottom line indicates the word order inGerman.
(Alternative analyses exist that do notrequire discontinuity; we make no claim thestructure above is the most adequate.
)Sa Sa Sa bbbaaa bb bFigure 2: Abstract representation of cross-serial dependencies in Dutch (Bresnan et al.,1982).4.1 Linear context-free rewriting systemsMuch as in (Vijay-Shanker et al., 1987), we define a linear context-free rewriting system (LCFRS) as atuple G = (N,S,?, P ), where N is a ranked alphabet of nonterminals, S ?
N(1)is the start symbol,?
= ?
(0)is a ranked alphabet of terminals (?
?N = ?
), and P is a finite set of rules, each of the form:A0(s1,k0)?
?A1(x1,m1), A2(xm1+1,m2), .
.
.
, An(xmn?1+1,mn)?
(1)where n ?
N, Ai?
N(ki)for each i ?
[n]0, and mi=?j:1?j?ikjfor each i ?
[n], and sj?
T??
(Xmn)for each j ?
[k0].
In words, the right-hand side is an s-term consisting of nonterminals Ai(i ?
[n]), withdistinct variables as arguments; there are mnvariables altogether, which is the sum of the ranks kiof allAi(i ?
[n]).
The left-hand side is an occurrence of A0with each argument being a string of variablesand terminals.
Furthermore, we demand that each xj(j ?
[mn]) occurs exactly once in the left-handside.
The largest rank of any nonterminal is called the fanout of the grammar.A rule instance is obtained by choosing a rule of the above form, and consistently substituting variableswith s-terms in T??
(which are strings due to the terminals having rank 0).
The language induced is theset of s-terms s such that ?S(s)?
??G?
?, where?Gis the ?derives?
relation that uses rule instances.
Forgiven s, the set of all LCFRS derivations ?S(s)?
??G??
(in compact tabular form) can be obtained inpolynomial time in the length of s (Seki et al., 1991).Example 1An example of a LCFRS is presented on the S(x1x3x2x4) ?
A(x1, x2) B(x3, x4)A(ax1,bx2) ?
A(x1, x2)A(?
?, ??)
?
?
?B(cx1,dx2) ?
B(x1, x2)B(?
?, ??)
?
??right.
Terminals are lower case bold letters andnonterminals are upper-case italic letters.
Allderived strings are of the form amcnbmdnwithm,n ?
N. The linguistic relevance lies in cross-serial dependencies in Swiss German (Shieber,1985).4.2 Definite clause programsIn this section we describe a particular kind of definite clause programs.
Our definition is inspired by(Deransart and Ma?uszynski, 1985), which investigated the relation between logic programs and attributegrammars, together with the ?syntactic single use requirement?
from (Giegerich, 1988).
The valuesproduced are trees (or to be more precise s-terms).1373A simple definite clause program (sDCP) is a tuple G = (N,S,?, P ), where N is a ranked alphabetof nonterminals and ?
= ?
(1)is a ranked alphabet of terminals.2Moreover, each nonterminal A ?
Nhas a fixed i-rank (the number of inherited arguments) and a fixed s-rank (the number of synthesizedarguments), denoted by i-rk(A) and s-rk(A), respectively, satisfying i-rk(A) + s-rk(A) = rkN(A).
Inour notation, the inherited arguments precede the synthesized arguments.
The start symbol S has onlyone argument, which is synthesized, i.e.
rkN(S) = s-rk(S) = 1 and i-rk(S) = 0.A rule is of the form:A0(x(0)1,k0, s(0)1,k?0)?
?A1(s(1)1,k1, x(1)1,k?1), .
.
.
, An(s(n)1,kn, x(n)1,k?n)?
(2)where n ?
N, ki= i-rk(Ai) and k?i= s-rk(Ai), for i ?
[n]0.
The set of variables occurring in the listsx(0)1,k0and x(i)1,k?i(i ?
[n]) equals Xm, where m = k0+?i?[n]k?i.
In other words, every variable from Xmoccurs exactly once in all these lists together.
This is where values ?enter?
the rule.
Further, the s-termsin s(0)1,k?0and s(i)1,ki(i ?
[n]) are in T??
(Xm) and together contain each variable in Xmexactly once.
Thisis where values are combined and ?exit?
the rule.The ?derives?
relation?Gand other relevant notation are defined as for LCFRSs (where the s-termsin arguments are now trees due to the terminals having rank 1).
If the rules in a derivation are given, thenthe relevant rule instances are uniquely determined, and can be computed in linear time in the size ofthe derivation, provided the sDCP contains no cycles.
The existence of cycles is decidable, as we knowfrom the literature on attribute grammars.
There are sufficient conditions for absence of cycles, such asthe grammar being L-attributed (Bochmann, 1976).
In this article, we will assume that sDCPs containno cycles.Example 2An example of a sDCP is presented S(x2) ?
A(x1) B(x1, x2)A(a A(x1) b) ?
A(x1)A(??)
?
?
?B(x1, c B(x2) d) ?
B(x1, x2)B(x1, x1) ?
?
?on the right, where the first argument ofB is inherited and all other argumentsare synthesized.
A derived s-term ise.g.
c B(c B(a A(??)
b) d) d.5 Hybrid grammarsWe couple derivations in two grammars in a way similar to how this is commonly done for synchronousgrammars, namely by indexed symbols.
However, we apply the mechanism not only to derivationalnonterminals but also to terminals.Let ?
be a ranked alphabet.
We define the ranked set I(?)
= {?u| ?
?
?, u ?
N+}, with rkI(?
)(?u)= rk?(?).
Let ?
be another ranked alphabet (?
?
?
= ?)
and Y ?
X , with X as in Section 2.
We letI??,?
(Y ) be the set of all s-terms s ?
T?I(?)??
(Y ) in which each index occurs at most once.For an s-term s, let ind(s) be the set of all indices occurring in s. The deindexing function D removesall indices from an s-term s ?
I??,?
(Y ) to obtain D(s) ?
T????
(Y ).
The set I?,?
(Y ) ?
TI(?)??
(Y ) ofterms with indexed symbols is defined much as above.
We let I?
?,?= I??,?(?)
and I?,?= I?,?(?
).A LCFRS/sDCP hybrid grammar (HG) is a tuple G = ((N1, S1,?
), (N2, S2,?
), P ), subject to thefollowing restrictions.
The objects ?
and ?
are ranked alphabets with ?
= ?
(0)and ?
= ?(1).
As meresets of symbols, we demand ?
?
?
but the rank functions associated with ?
and ?
differ.
Let ?
be theranked alphabet ?
\ ?, with rk?(?)
= 1 for ?
?
?.The hybrid rules in P are of the form [?1, ?2] where ?1has the form in Equation (1) of an LCFRSrule except that si?
I??,?
(Xmn) (i ?
[k0]) and Ai?
I(N1) (i ?
[n]) and each index in ?1occursexactly once, and ?2has the form in Equation (2) of a sDCP rule except that the s-terms in s(0)1,k?0ands(i)1,ki(i ?
[n]) are in I??,?
(Xm) and Ai?
I(N2) (i ?
[n]) and each index in ?2occurs exactly once.
Werequire that ind(?1) = ind(?2) and each index either couples a pair of identical terminals or couples apair of (possibly distinct) nonterminals.2The term ?simple?
here has a more restrictive meaning than the term with the same name in (Deransart and Ma?uszynski,1985).1374Let P1and P2be the sets of all D(?1) and D(?2), respectively, of some hybrid rule [?1, ?2].
Then werefer to the LCFRS (N1, S1,?, P1) and the sDCP (N2, S2,?, P2) as the first and second components,respectively, of G.In order to define the ?derives?
relation?G, we need rule instantiation as before, in combination withreindexing, which is a common notion for synchronous grammars.
This allows specification of a set ofpairs [s1, s2] ?
I??,??
I?
?,?which are such that [?S11(s1)?, ?S12(s2)?]
??G[?
?, ??].
For each such pairwe can construct a hybrid tree (s,?s) over (?,?
), where s = D(s2), and ?sis defined as follows.
Ifthere is a combination of positions p1, p?1, p2, p?2such that at p1in s1we find the same label as at p2ins2(this label must then be in I(?
)), and at p?1in s1we find the same label as at p?2in s2, and p1occursto the left of p?1, then p2?sp?2.
The language induced by G is defined as the set of all such hybrid trees.Given an input string, the desired hybrid trees can be effectively enumerated.
To be exact, afterconstruction of the parse table by a LCFRS parser, which takes polynomial time in the length of thestring, synchronous derivations can be enumerated.
Extracting a single derivation from the table requireslinear time in the size of that derivation.
Given a derivation, an s-term can be constructed in linear timein the size of that derivation, applying sDCP rules in the second component.
This s-term, in combinationwith the input string and the indices linking the two is then easily extended to a hybrid tree as outlinedabove.Example 3The hybrid tree[VP(x1x2x3)?
V1(x1, x3) ADV2(x2),VP(VP(x1x2))?
V1(x1) ADV2(x2)][V(h1, g2)?
??,V(V(h1g2))?
??][ADV(s1)?
??,ADV(ADV(s1))?
??
]in Figure 1 is ob-tained by the HGon the right.
(Allarguments in thesecond component are synthesized.)
We derive:[VP1(h2s3g4),VP1(VP(V(h2g4) ADV(s3)))]?
[V1(h2, g4) ADV5(s3),V1(V(h2g4)) ADV5(ADV(s3))]?
[ADV5(s3), ADV5(ADV(s3))]?
[?
?, ??
]Note that in the LCFRS that[VP(x1)?
V1(x1), VP(VP(x1))?
V1(x1)][V(h1x1g2)?
ADV3(x1), V(V(h1g2) x1)?
ADV3(x1)][ADV(s1)?
?
?, ADV(ADV(s1))?
??
]is the first component of the HGabove, nonterminal V has rank 2.On the right is an alternative HGderiving the same hybrid tree, butnow with all LCFRS nonterminals having rank 1, by which we obtain a syntactic variant of a CFG.
Yetanother HG for the same hybrid tree will be discussed in the next section, where we will see that the firstand second components can be disconnected even further, departing from the traditional way of LCFRSparsing.Example 4Hybrid trees as in Figure 2[A(x1x2)?
S1(x1, x2), A(x1)?
S1(x1)][S(a1x1,b2x2)?
S3(x1, x2), S(S(a1x1b2)?
S3(x1)][S(?
?, ??)?
?
?, S(??)?
??
]can be obtained by the HG onthe right.6 Grammar inductionWe define a recursive partitioning of a string s = ?1?
?
?
?nas a tree whose nodes are labeled withsubsets of [n].
The root is labeled with [n].
Each leaf is labeled with a single element of [n].
Eachinternal node is labeled with the union of the labels of its children, which furthermore must be disjoint.We say a subset of [n] has fanout k if k is the smallest number such that it can be written as the union ofk sets of consecutive numbers.1375A derivation of an LCFRS relates straightforwardly to a recursive partitioning.
Consider for examplethe derivation of string h s g by the LCFRS that is the first component of the first HG in Example 3.The root would be labeled {1, 2, 3}, with children labeled {1, 3} and {2}.
The node labeled {1, 3} haschildren labeled {1} and {3}.
The fanout of {1, 3} is 2, whereas it is 1 for all other node labels.
Onemay also extract a recursive partitioning directly from a hybrid tree, by associating each node with theset of positions of terminals that it dominates.
For example, Figure 1 gives rise to the same recursivepartitioning as the one mentioned above.One central observation of this paper is that for any hybrid tree h = (s,?s) and any recursive par-titioning of str(h), not necessarily extracted from h, we can construct a hybrid grammar G allowing aderivation of h, and moreover, the first (LCFRS) component of that derivation parses str(h) according tothe given recursive partitioning.
This observation holds for both dependency structures and constituentstructures.
The proof for dependency structures is quite technical however, and requires that the second(sDCP) component of a hybrid grammar has rules with inherited arguments.
For lack of space, we canonly give an outline for constituent structures, or in other words, we consider only input hybrid trees over(?,?)
where labels from ?
occur exclusively at the leaves.
In the resulting hybrid grammars, all sDCPrules will have only synthesized arguments.The intuition is the following.
For each node of the given recursive partitioning, the numbers in itslabel correspond to leaves of s, for the given hybrid tree h = (s,?s).
There is a smallest number ofmaximal disjoint subtrees in s that together contain all those leaves and no others.
If we now relate aparent node of the recursive partitioning to its child nodes, then we see that the relevant disjoint subtreesin s for the children can be combined to give the relevant disjoint subtrees for the parent, possibly addingfurther internal nodes.
This process can be expressed in terms of a hybrid rule.
Each pair consisting ofa hybrid tree and a recursive partitioning gives rise to a number of hybrid rules.
For a collection of suchpairs, we can combine all the rules into a hybrid grammar.Example 5 Consider again the hybrid tree in Figure 1, in combination with a recursive partitioningwhose root has children labeled {1, 2} and {3}.
The relevant disjoint subtrees for {1, 2} are hat andADV(schnell) and for {3} there is the subtree gearbeitet.
(In a real-world grammar we would haveparts of speech occurring above all the words.)
An appropriate hybrid rule that both respects the recursivepartitioning (by the first component LCFRS rule) and puts together relevant parts of the hybrid tree (bythe second component sDCP rule) would be of the form:[A(x1x2)?
B1(x1) C2(x2), A(VP(V(x1x3)x2))?
B1(x1, x2) C2(x3)]Here A, B and C should to be chosen to be consistent with neighboring nodes in the recursive partition-ing, to be discussed next.
An alternative recursive partitioning whose root has children labeled {1, 3}and {2} leads to the first hybrid rule in Example 3 (apart from nonterminal names).We have experimented with two ways of naming nonterminals in the derived hybrid rules.
The firstencodes the list of labels of the roots of the relevant disjoint subtrees.
In the above example, we wouldhave a name such as ?hat,ADV?
for A.
For fanout greater than 1, the locations of the ?gaps?
are ex-plicitly indicated.
For example, we might have ?hat, gap, gearbeitet?.
We will call this strict labeling.The second, and less precise, way is to replace lists of labels of siblings by a single name of the formchildren-of(X), where X is the label of the parent.
We will call this child labeling.Because our construction of hybrid grammars works for all recursive partitionings, there is no need tolimit ourselves to those extracted directly from the hybrid trees.
Moreover, a given recursive partitioningcan be transformed into a similar but different one in which fanout is restricted to some given valuek ?
1.
One possible procedure is to start at the root.
If the label J of the present node is a singleton,then we stop.
Otherwise, we search breadth-first through the subtree of the present node to identify adescendant such that both its label J?and J \J?have fanout not exceeding k. (It is easy to see such a nodealways exists: ultimately breadth-first search will reach the leaves, which are labeled with singletons.
)The present node is now given two children, the first is the node labeled J?that we identified above, andthe second is a copy of the present subtree, but with J?subtracted from the label of every node.
(Nodes1376labeled with the empty set are removed, and if a node has the same label as its parent then the two arecollapsed.)
We repeat the procedure for both children recursively.
Note that with k = 1, we can inducea ?CFG/sDCP?
hybrid grammar, that is, with the first component having fanout 1.Example 6The recursive partition-{1, 2, 3, 5, 6, 7}{1, 3, 6, 7}{1, 6}{1} {6}{3, 7}{3} {7}{2, 5}{2} {5}=?
{1, 2, 3, 5, 6, 7}{3, 7}{3} {7}{1, 2, 5, 6}{1, 6}{1} {6}{2, 5}{2} {5}Figure 3: Transformation of recursive partitioning to restrict fanout to 2.ing in the left half of Fig-ure 3 has a node labeled{1, 3, 6, 7}, with fanout 3.With J = {1, 2, 3, 5, 6, 7}and k = 2, one possiblechoice for J?is {3, 7}, asthen both J?and J \ J?={1, 2, 5, 6} have fanout notexceeding 2.
This leads tothe partitioning in the righthalf of the figure.
Because now all node labels have fanout not exceeding 2, recursive traversal will makeno further changes.
Other valid choices for J?would be {2} and {5}.
Not a valid choice for J?would be{1, 6}, as J \ {1, 6} = {2, 3, 5, 7}, which has fanout 3.Our procedure ensures that subsequent grammar induction leads to binary grammars.
Note that thiscontrasts with binarization algorithms (G?omez-Rodr?
?guez and Satta, 2009; G?omez-Rodr?
?guez et al.,2009) that are applied after a grammar is obtained.
Unlike (van Cranenburgh, 2012), our objective isnot to obtain a ?coarse?
grammar for the purpose of coarse-to-fine parsing.In experiments we also considered the right-branching partitioning, whose internal node labels are{m,m+ 1, .
.
.
, n}, with children labeled {m} and {m+ 1, .
.
.
, n}.
Similarly, there is a left-branchingrecursive partitioning.
In this way, we can induce a ?FA/sDCP?
hybrid grammar, with the first componenthaving finite-state power, which means we can parse in linear time.7 ExperimentsThe theory developed above shows that hybrid grammars allow considerable flexibility in the first com-ponent, leading to a wide range of different time complexities of parsing while, at least potentially, thesame kinds of discontinuous structures can be obtained.
We have run experiments to measure whatimpact different choices of the first component have on recall/precision and the degree of discontinuity.The training data consisted of the first 7000 trees of the TIGER treebank (Brants et al., 2004).
Fromthese, recursive partitionings were straightforwardly obtained, and transformed for different values of k.Also the left-branching and right-branching recursive partitionings were considered.
Hybrid grammarswere then extracted using strict or child labeling.
Probabilities of rules were determined by relativefrequency estimation, without any smoothing techniques.Test sentences were taken from the next 500 trees, excluding sentences of length greater than 20 andthose where a single tree did not span the entire sentence, leaving 324 sentences.
Parsing was on (goldstandard) parts of speech rather than words.
All punctuation was ignored.
Labeled recall, precision andF-measure were computed on objects each consisting of the label of a node and a sequence of pairs ofinput positions delimiting substrings covered by that node.
The algorithms were implemented in Pythonand the experiments were carried out on a desktop with four 3.1GHz Intel Core i5 CPUs.Results are reported in Table 1.
The choice of k = 1 can be seen as a baseline, the first componentthen being restricted to context-free power.
Note that k = 1, 2, 3 imply parsing complexities O(n3),O(n6), O(n9), respectively.In the case of strict labeling, the change from k = 1 to k = 2 leads to significant changes in runningtime, but that from k = 2 to k = 3 less so, which can be explained by the smaller number of constituentsthat have two gaps, compared to those with zero or one gap.
There was no significant change, neither inrunning time nor in F-measure, for values of k greater than 4, and therefore these values were omitted1377here.
Note that for k = ?
one would obtain the conventional technique of discontinuous parsing usingLCFRSs.
For the right-branching recursive partitionings, the running time is significantly higher thanthat for the left-branching ones, although it is linear-time in both cases.
This is due to the directional biasof the implemented parsing strategy.
In order to allow a straightforward comparison we have taken thesame parsing strategy in all cases.
Note the large number of parse failures for the right-branching andleft-branching partitionings, which is explained by the large number of very specific nonterminals.Child labeling leads to much smallerfail R P F1 # gaps secsstrict labelingk = 1 16 73.0 70.4 71.2 0.0075 442k = 2 12 73.1 70.7 71.4 0.0111 2,580k = 3 12 73.1 70.7 71.4 0.0121 2,942k = 4 12 73.1 70.7 71.4 0.0127 2,828r-branch 151 65.6 62.4 63.2 0.0118 775l-branch 266 82.0 78.9 79.5 0.0124 24child labelingk = 1 4 74.3 74.2 73.9 0.0120 939k = 2 4 75.0 75.1 74.7 0.0125 58,164r-branch 15 73.1 73.0 72.6 0.0117 319l-branch 56 75.7 76.6 75.7 0.0114 183Table 1: Number of parse failures, recall, precision, F-measure, average number of gaps per constituent, and run-ning time.numbers of nonterminals, and therebyalso to more ambiguity, and as a re-sult the increase from time complexityO(n3) to O(n6) is more noticeable interms of the actual running time.
There-fore carrying out the experiment for k ?3 was outside our reach.
Surprisingly,the right-branching partitioning performedvery well in this case, with a relatively lownumber of parse failures, F-measure com-peting with k = 1, 2, 3, 4 and strict label-ing, although it is clearly worse than thatwith k = 1, 2 and child labeling, and run-ning time smaller than in the case of any ofthe hybrid grammars where the first com-ponent has power beyond that of finite au-tomata.Child labeling generally gave better F-measure than strict labeling (ignoring strict labeling and left-branching partitioning, where the many parse failures distort the recall and precision).
This seems to bedue to the more accurate parameter estimation that was possible for the smaller numbers of rules obtainedwith child labeling.The differences in F-measure are relatively small for varying k. This can be explained by the relativelysmall portion of discontinuous structures in the test set.
We have looked closer at discontinuity in thetest set in two ways.
First, we measured the average number of gaps per constituent, which in the goldstandard was 0.0171.
None of the hybrid grammars came close to achieving this, but we do observethat more discontinuity is obtained for higher values of k. Secondly, we reran the experiments for onlythe 75 sentences out of the aforementioned 324 where the gold structure had at least one discontinuousphrase.
For this smaller set, F1 increases from 59.5 (k = 1) to 61.9 (k = 2, 3, 4) for strict labeling, andit increases from 64.4 (k = 1) to 66.5 (k = 2) for child labeling.
This suggests that with higher k, theadditional discontinuous structures found have at least some overlap with those of the gold standard.
Noteagain that there is no a priori bound on the fanout of produced hybrid trees, even when the first componenthas finite-state power, but the ability to abstract away from discontinuous structures in the training setseems to be enhanced if the first component is more powerful.
This is consistent with observations madeby (van Cranenburgh, 2012).8 LimitationsThe theory from Section 6 does not necessarily mean that any language of hybrid trees can be inducedby a HG whose first-component LCFRS has arbitrarily low fanout.
We illustrate this by means of thelanguage of hybrid trees generated by the HG of Example 4, in which the LCFRS has fanout 2.
NoCFG/sDCP grammar in fact exists for the same language, or in other words, the fanout of the first-component LCFRS cannot be reduced to 1, regardless of how we choose the second-component sDCP.For a proof, assume that a CFG/sDCP grammar does exist.
Letm be the maximum number of membersin the right-hand side of any CFG rule.
Let k be the maximum rank of any nonterminal in the second-component sDCP.
Now consider a CFG/sDCP derivation for a hybrid tree with yield anbn, where n ?13782 ?
k ?m.
In a top-down traversal, identify the first CFG nonterminal occurrence that covers a substringof the input string that has a length smaller than or equal to n/2 and greater than k. This substringmay contain occurrences of a and of b, but because its length is at most n/2, there will not be any pairconsisting of an occurrence of a and an occurrence of b that are both part of that substring, and thathave a common parent labeled S in the hybrid tree.
This means that more than k tree fragments or treenodes with missing child nodes are involved, which translate to more than k synthesized or inheritedarguments, contradicting the assumptions.9 ConclusionsWe have presented hybrid grammars as a novel framework for describing languages of discontinuoussyntactic structures.
This framework sheds light on the relation between various existing techniques, butit also offers potential for development of novel techniques.
Much of what we have shown is merelyan illustration of particular instances of this framework.
For example, next to the hybrid grammarsdiscussed here, we can consider those with macro grammars as first component, or simple context-free tree grammars as second component.
Many variations exist on the illustrated grammar inductiontechnique.
For example, next to our strict labeling and child labeling, one can consider approaches usinglatent variables, combined with expectation-maximization.AcknowledgmentsWe thank the anonymous reviewers for many helpful comments.ReferencesT.
Becker, A.K.
Joshi, and O. Rambow.
1991.
Long-distance scrambling and Tree Adjoining Grammars.
In FifthEACL, pages 21?26.G.V.
Bochmann.
1976.
Semantic evaluation from left to right.
Communications of the ACM, 19(2):55?62.A.
B?ohmov?a, J. Haji?c, E. Haji?cov?a, and B. Hladk?a.
2000.
The Prague dependency treebank: A tree-level anno-tation scenario.
In A. Abeill?e, editor, Treebanks: Building and using syntactically annotated corpora, pages103?127.
Kluwer, Dordrecht.A.
Boyd.
2007.
Discontinuity revisited: An improved conversion to context-free representations.
In LinguisticAnnotation Workshop, at ACL 2007, pages 41?44.W.S.
Brainerd.
1969.
Tree generating regular systems.
Information and Control, 14:217?231.S.
Brants, S. Dipper, P. Eisenberg, S. Hansen-Schirra, E. K?onig, W. Lezius, C. Rohrer, G. Smith, and H. Uszkoreit.2004.
TIGER: Linguistic interpretation of a German corpus.
Research on Language and Computation, 2:597?620.J.
Bresnan, R.M.
Kaplan, S. Peters, and A. Zaenen.
1982.
Cross-serial dependencies in Dutch.
Linguistic Inquiry,13(4):613?635.H.
Bunt.
1996.
Formal tools for describing and processing discontinuous constituency structure.
In H. Bunt andA.
van Horck, editors, Discontinuous Constituency, pages 63?84.
Mouton de Gruyter.R.
Campbell.
2004.
Using linguistic principles to recover empty categories.
In 42nd Annual Meeting of the ACL,pages 645?652.J.D.
Choi and M. Palmer.
2010.
Robust constituent-to-dependency conversion for English.
In Ninth InternationalWorkshop on Treebanks and Linguistic Theories, pages 55?66.P.
Deransart and J. Ma?uszynski.
1985.
Relating logic programs and attribute grammars.
Journal of Logic Pro-gramming, 2:119?155.K.
Evang and L. Kallmeyer.
2011.
PLCFRS parsing of English discontinuous constituents.
In 12th InternationalConference on Parsing Technologies, pages 104?116.1379M.J.
Fischer.
1968.
Grammars with macro-like productions.
In IEEE Conference Record of 9th Annual Sympo-sium on Switching and Automata Theory, pages 131?142.R.
Gabbard, S. Kulick, and M. Marcus.
2006.
Fully parsing the Penn Treebank.
In Human Language TechnologyConference of the NAACL, Main Conference, pages 184?191.F.
G?ecseg and M. Steinby.
1997.
Tree languages.
In G. Rozenberg and A. Salomaa, editors, Handbook of FormalLanguages, Vol.
3, chapter 1, pages 1?68.
Springer, Berlin.R.
Giegerich.
1988.
Composition and evaluation of attribute coupled grammars.
Acta Informatica, 25:355?423.C.
G?omez-Rodr?
?guez and G. Satta.
2009.
An optimal-time binarization algorithm for linear context-free rewritingsystems with fan-out two.
In 47th ACL and 4th International Joint Conference on Natural Language Processingof the AFNLP, pages 985?993.C.
G?omez-Rodr?
?guez, M. Kuhlmann, G. Satta, and D. Weir.
2009.
Optimal reduction of rule length in linearcontext-free rewriting systems.
In Human Language Technologies: The 2009 Annual Conference of the NorthAmerican Chapter of the ACL, pages 539?547.M.
Johnson.
2002.
A simple pattern-matching algorithm for recovering empty nodes and their antecedents.
In40th ACL, pages 136?143.S.
Kahane, A. Nasr, and O. Rambow.
1998.
Pseudo-projectivity, a polynomially parsable non-projective depen-dency grammar.
In 36th ACL and 17th International Conference on Computational Linguistics, volume 1, pages646?652.K.
Kallmeyer and M. Kuhlmann.
2012.
A formal model for plausible dependencies in lexicalized tree adjoininggrammar.
In Eleventh International Workshop on Tree Adjoining Grammar and Related Formalisms, pages108?116.L.
Kallmeyer and W. Maier.
2013.
Data-driven parsing using probabilistic linear context-free rewriting systems.Computational Linguistics, 39(1):87?119.A.
Kathol and C. Pollard.
1995.
Extraposition via complex domain formation.
In 33rd ACL, pages 174?180.M.
Kuhlmann.
2013.
Mildly non-projective dependency grammar.
Computational Linguistics, 39(2):355?387.W.
Lu, H.T.
Ng, W.S.
Lee, and L.S.
Zettlemoyer.
2008.
A generative model for parsing natural language tomeaning representations.
In Conference on Empirical Methods in Natural Language Processing, pages 783?792.W.
Maier and T. Lichte.
2009.
Characterizing discontinuity in constituent treebanks.
In P. de Groote, M. Egg,and L. Kallmeyer, editors, 14th Conference on Formal Grammar, volume 5591 of Lecture Notes in ArtificialIntelligence, Bordeaux, France.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.
1993.
Building a large annotated corpus of English: ThePenn treebank.
Computational Linguistics, 19(2):313?330.J.D.
McCawley.
1982.
Parentheticals and discontinuous constituent structure.
Linguistic Inquiry, 13(1):91?106.R.
McDonald and F. Pereira.
2006.
Online learning of approximate dependency parsing algorithms.
In 11thEACL, pages 81?88.S.
M?uller.
2004.
Continuous or discontinuous constituents?
a comparison between syntactic analyses for con-stituent order and their processing systems.
Research on Language and Computation, 2:209?257.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective dependency parsing.
In 43rd ACL, pages 99?106.J.
Nivre.
2009.
Non-projective dependency parsing in expected linear time.
In Joint Conference of the 47th ACLand the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 351?359.O.
Rambow and A.K.
Joshi.
1997.
A formal look at dependency grammars and phrase structure grammars withspecial consideration of word-order phenomena.
In L. Wenner, editor, Recent Trends in Meaning-Text Theory.John Benjamin.O.
Rambow.
2010.
The simple truth about dependency and phrase structure representations: An opinion piece.In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL,Main Conference, pages 337?340.1380W.C.
Rounds.
1970.
Mappings and grammars on trees.
Mathematical Systems Theory, 4:257?287.G.
Satta and E. Peserico.
2005.
Some computational complexity results for synchronous context-free grammars.In Human Language Technology Conference and Conference on Empirical Methods in Natural Language Pro-cessing, pages 803?810.S.
Seifert and I. Fischer.
2004.
Parsing string generating hypergraph grammars.
In H. Ehrig, G. Engels, F. Parisi-Presicce, and G. Rozenberg, editors, 2nd International Conference on Graph Transformations, volume 3256 ofLecture Notes in Computer Science, pages 352?267.
Springer-Verlag.H.
Seki and Y. Kato.
2008.
On the generative power of multiple context-free grammars and macro grammars.IEICE Transactions on Information and Systems, E91-D:209?221.H.
Seki, T. Matsumura, M. Fujii, and T. Kasami.
1991.
On multiple context-free grammars.
Theoretical ComputerScience, 88:191?229.S.M.
Shieber and Y. Schabes.
1990.
Synchronous tree-adjoining grammars.
In Papers presented to the 13thInternational Conference on Computational Linguistics, volume 3, pages 253?258.S.M.
Shieber.
1985.
Evidence against the context-freeness of natural language.
Linguistics and Philosophy,8(3):333?343.K.
Sima?an, R. Bod, S. Krauwer, and R. Scha.
1994.
Efficient disambiguation by means of stochastic treesubstitution grammars.
In International Conference on New Methods in Language Processing, pages 50?58.S.
Stucky.
1987.
Configurational variation in English.
In G.J.
Huck and A.E.
Ojeda, editors, DiscontinuousConstituency, volume 20 of Syntax and Semantics, pages 377?404.
Academic Press.A.
van Cranenburgh.
2012.
Efficient parsing with linear context-free rewriting systems.
In 13th EACL, pages460?470.K.
Vijay-Shankar and A.K.
Joshi.
1985.
Some computational properties of tree adjoining grammars.
In 23rd ACL,pages 82?93.K.
Vijay-Shanker, D.J.
Weir, and A.K.
Joshi.
1987.
Characterizing structural descriptions produced by variousgrammatical formalisms.
In 25th ACL, pages 104?111.1381
