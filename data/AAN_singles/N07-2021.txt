Proceedings of NAACL HLT 2007, Companion Volume, pages 81?84,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsSemi-Supervised Learning for Semantic Parsingusing Support Vector MachinesRohit J. Kate and Raymond J. MooneyDepartment of Computer SciencesThe University of Texas at Austin1 University Station C0500Austin, TX 78712-0233, USAfrjkate,mooneyg@cs.utexas.eduAbstractWe present a method for utilizing unan-notated sentences to improve a semanticparser which maps natural language (NL)sentences into their formal meaning rep-resentations (MRs).
Given NL sentencesannotated with their MRs, the initial su-pervised semantic parser learns the map-ping by training Support Vector Machine(SVM) classifiers for every production inthe MR grammar.
Our new method ap-plies the learned semantic parser to theunannotated sentences and collects unla-beled examples which are then used toretrain the classifiers using a variant oftransductive SVMs.
Experimental resultsshow the improvements obtained overthe purely supervised parser, particularlywhen the annotated training set is small.1 IntroductionSemantic parsing is the task of mapping a natu-ral language (NL) sentence into a complete, for-mal meaning representation (MR) which a computerprogram can execute to perform some task, likeanswering database queries or controlling a robot.These MRs are expressed in domain-specific unam-biguous formal meaning representation languages(MRLs).
Given a training corpus of NL sentencesannotated with their correct MRs, the goal of a learn-ing system for semantic parsing is to induce an ef-ficient and accurate semantic parser that can mapnovel sentences into their correct MRs.Several learning systems have been developed forsemantic parsing, many of them recently (Zelle andMooney, 1996; Zettlemoyer and Collins, 2005; Geand Mooney, 2005; Kate and Mooney, 2006).
Thesesystems use supervised learning methods whichonly utilize annotated NL sentences.
However, itrequires considerable human effort to annotate sen-tences.
In contrast, unannotated NL sentences areusually easily available.
Semi-supervised learningmethods utilize cheaply available unannotated dataduring training along with annotated data and of-ten perform better than purely supervised learningmethods trained on the same amount of annotateddata (Chapelle et al, 2006).
In this paper we present,to our knowledge, the first semi-supervised learningsystem for semantic parsing.We modify KRISP, a supervised learning sys-tem for semantic parsing presented in (Kate andMooney, 2006), to make a semi-supervised systemwe call SEMISUP-KRISP.
Experiments on a real-world dataset show the improvements SEMISUP-KRISP obtains over KRISP by utilizing unannotatedsentences.2 BackgroundThis section briefly provides background needed fordescribing our approach to semi-supervised seman-tic parsing.2.1 KRISP: The Supervised Semantic ParsingLearning SystemKRISP (Kernel-based Robust Interpretation for Se-mantic Parsing) (Kate and Mooney, 2006) is a su-pervised learning system for semantic parsing which81takes NL sentences paired with their MRs as train-ing data.
The productions of the formal MRLgrammar are treated like semantic concepts.
Foreach of these productions, a Support-Vector Ma-chine (SVM) (Cristianini and Shawe-Taylor, 2000)classifier is trained using string similarity as the ker-nel (Lodhi et al, 2002).
Each classifier can thenestimate the probability of any NL substring rep-resenting the semantic concept for its production.During semantic parsing, the classifiers are called toestimate probabilities on different substrings of thesentence to compositionally build the most probablemeaning representation (MR) of the sentence.KRISP trains the classifiers used in semantic pars-ing iteratively.
In each iteration, for every produc-tion  in the MRL grammar, KRISP collects pos-itive and negative examples.
In the first iteration,the set of positive examples for production  con-tains all sentences whose corresponding MRs usethe production  in their parse trees.
The set of neg-ative examples includes all of the other training sen-tences.
Using these positive and negative examples,an SVM classifier is trained for each production using a string kernel.
In subsequent iterations, theparser learned from the previous iteration is appliedto the training examples and more refined positiveand negative examples, which are more specific sub-strings within the sentences, are collected for train-ing.
Iterations are continued until the classifiers con-verge, analogous to iterations in EM (Dempster etal., 1977).
Experimentally, KRISP compares favor-ably to other existing semantic parsing systems andis particularly robust to noisy training data (Kate andMooney, 2006).2.2 Transductive SVMsSVMs (Cristianini and Shawe-Taylor, 2000) arestate-of-the-art machine learning methods for clas-sification.
Given positive and negative training ex-amples in some vector space, an SVM finds themaximum-margin hyperplane which separates them.Maximizing the margin prevents over-fitting in veryhigh-dimensional data which is typical in naturallanguage processing and thus leads to better general-ization performance on test examples.
When the un-labeled test examples are also available during train-ing, a transductive framework for learning (Vapnik,1998) can further improve the performance on thetest examples.Transductive SVMs were introduced in(Joachims, 1999).
The key idea is to find thelabeling of the test examples that results in themaximum-margin hyperplane that separates thepositive and negative examples of both the trainingand the test data.
This is achieved by includingvariables in the SVM?s objective function repre-senting labels of the test examples.
Finding theexact solution to the resulting optimization problemis intractable, however Joachims (1999) gives anapproximation algorithm for it.
One drawback ofhis algorithm is that it requires the proportion ofpositive and negative examples in the test data beclose to the proportion in the training data, whichmay not always hold, particularly when the trainingdata is small.
Chen et al (2003) present anotherapproximation algorithm which we use in oursystem because it does not require this assumption.More recently, new optimization methods have beenused to scale-up transductive SVMs to large datasets (Collobert et al, 2006), however we did notface scaling problems in our current experiments.Although transductive SVMs were originally de-signed to improve performance on the test data byutilizing its availability during training, they can alsobe directly used in a semi-supervised setting (Ben-nett and Demiriz, 1999) where unlabeled data isavailable during training that comes from the samedistribution as the test data but is not the actual dataon which the classifier is eventually to be tested.This framework is more realistic in the context of se-mantic parsing where sentences must be processedin real-time and it is not practical to re-train theparser transductively for every new test sentence.
In-stead of using an alternative semi-supervised SVMalgorithm, we preferred to use a transductive SVMalgorithm (Chen et al, 2003) in a semi-supervisedmanner, since it is easily implemented on top of anexisting SVM system.3 Semi-Supervised Semantic ParsingWe modified the existing supervised system KRISP,described in section 2.1, to incorporate semi-supervised learning.
Supervised learning in KRISPinvolves training SVM classifiers on positive andnegative examples that are substrings of the anno-82function TRAIN SEMISUP KRISP(Annotated corpus A = f(si;mi)ji = 1::Ng, MRL grammar G,Unannotated sentences T = ftiji = 1::Mg)C  fCj 2 Gg = TRAIN KRISP(A,G) // classifiers obtained by training KRISPLetP = fp= Set of positive examples used in training Cj 2 GgN = fn= Set of negative examples used in training Cj 2 GgU = fu= j 2 Gg // set of unlabeled examples for each production, initially all emptyfor i = 1 to M dofuij 2 Gg =COLLECT CLASSIFIER CALLS(PARSE(ti; C))U = fu= u[ uij 2 Ggfor each  2 G doC=TRANSDUCTIVE SVM TRAIN(p; n; u) // retrain classifiers utilizing unlabeled examplesreturn classifiers C = fCj 2 GgFigure 1: SEMISUP-KRISP?s training algorithmtated sentences.
In order to perform semi-supervisedlearning, these classifiers need to be given appropri-ate unlabeled examples.
The key question is: Whichsubstrings of the unannotated sentences should begiven as unlabeled examples to which productions?classifiers?
Giving all substrings of the unannotatedsentences as unlabeled examples to all of the clas-sifiers would lead to a huge number of unlabeledexamples that would not conform to the underly-ing distribution of classes each classifier is trying toseparate.
SEMISUP-KRISP?s training algorithm, de-scribed below and shown in Figure 1, addresses thisissue.The training algorithm first runs KRISP?s exist-ing training algorithm and obtains SVM classifiersfor every production in the MRL grammar.
Sets ofpositive and negative examples that were used fortraining the classifiers in the last iteration are col-lected for each production.
Next, the learned parseris applied to the unannotated sentences.
During theparsing of each sentence, whenever a classifier iscalled to estimate the probability of a substring rep-resenting the semantic concept for its production,that substring is saved as an unlabeled example forthat classifier.
These substrings are representative ofthe examples that the classifier will actually need tohandle during testing.
Note that the MRs obtainedfrom parsing the unannotated sentences do not playa role during training since it is unknown whetheror not they are correct.
These sets of unlabeled ex-amples for each production, along with the sets ofpositive and negative examples collected earlier, arethen used to retrain the classifiers using transductiveSVMs.
The retrained classifiers are finally returnedand used in the final semantic parser.4 ExperimentsWe compared the performance of SEMISUP-KRISPand KRISP in the GEOQUERY domain for semanticparsing in which the MRL is a functional languageused to query a U.S. geography database (Kate etal., 2005).
This domain has been used in most ofthe previous work.
The original corpus contains 250NL queries collected from undergraduate studentsand annotated with their correct MRs (Zelle andMooney, 1996).
Later, 630 additional NL querieswere collected from real users of a web-based inter-face and annotated (Tang and Mooney, 2001).
Weused this data as unannotated sentences in our cur-rent experiments.
We also collected an additional407 queries from the same interface, making a totalof 1; 037 unannotated sentences.The systems were evaluated using standard 10-fold cross validation.
All the unannotated sentenceswere used for training in each fold.
Performancewas measured in terms of precision (the percent-age of generated MRs that were correct) and recall(the percentage of all sentences for which correctMRs were obtained).
An output MR is consideredcorrect if and only if the resulting query retrievesthe same answer as the correct MR when submit-ted to the database.
Since the systems assign confi-dences to the MRs they generate, the entire range ofthe precision-recall trade-off can be obtained for asystem by measuring precision and recall at variousconfidence levels.
We present learning curves for thebest F-measure (harmonic mean of precision and re-8301020304050607080901000  20  40  60  80  100  120  140  160  180  200  220  240BestF-measureNo.
of annotated training sentencesSEMISUP-KRISPKRISPGEOBASEFigure 2: Learning curves for the best F-measureson the GEOQUERY corpus.call) obtained across the precision-recall trade-off asthe amount of annotated training data is increased.Figure 2 shows the results for both systems.The results clearly show the improvementSEMISUP-KRISP obtains over KRISP by utilizingunannotated sentences, particularly when the num-ber of annotated sentences is small.
We also showthe performance of a hand-built semantic parserGEOBASE (Borland International, 1988) for com-parison.
From the figure, it can be seen that, onaverage, KRISP achieves the same performance asGEOBASE when it is given 126 annotated examples,while SEMISUP-KRISP reaches this level given only94 annotated examples, a 25:4% savings in human-annotation effort.5 ConclusionsThis paper has presented a semi-supervised ap-proach to semantic parsing.
Our method utilizesunannotated sentences during training by extractingunlabeled examples for the SVM classifiers it uses toperform semantic parsing.
These classifiers are thenretrained using transductive SVMs.
Experimentalresults demonstrated that this exploitation of unla-beled data significantly improved the accuracy of theresulting parsers when only limited supervised datawas provided.AcknowledgmentsThis research was supported by a Google researchgrant.
The experiments were run on the Mastodoncluster provided by NSF grant EIA-0303609.ReferencesK.
Bennett and A. Demiriz.
1999.
Semi-supervised supportvector machines.
Advances in Neural Information Process-ing Systems, 11:368?374.Borland International.
1988.
Turbo Prolog 2.0 ReferenceGuide.
Borland International, Scotts Valley, CA.O.
Chapelle, B. Scho?lkopf, and A. Zien, editors.
2006.
Semi-Supervised Learning.
MIT Press, Cambridge, MA.Y.
Chen, G. Wang, and S. Dong.
2003.
Learning with progres-sive transductive support vector machine.
Pattern Recogni-tion Letters, 24:1845?1855.R.
Collobert, F. Sinz, J. Weston, and L. Bottou.
2006.
Largescale transductive SVMs.
Journal of Machine Learning Re-search, 7(Aug):1687?1712.N.
Cristianini and J. Shawe-Taylor.
2000.
An Introduction toSupport Vector Machines and Other Kernel-based LearningMethods.
Cambridge University Press.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.
Maxi-mum likelihood from incomplete data via the EM algorithm.Journal of the Royal Statistical Society B, 39:1?38.R.
Ge and R. J. Mooney.
2005.
A statistical semantic parserthat integrates syntax and semantics.
In Proc.
of CoNLL-05,pages 9?16, Ann Arbor, MI, July.T.
Joachims.
1999.
Transductive inference for text classifica-tion using support vector machines.
In Proc.
of ICML-99,pages 200?209, Bled, Slovenia, June.R.
J. Kate and R. J. Mooney.
2006.
Using string-kernels forlearning semantic parsers.
In Proc.
of COLING/ACL-06,pages 913?920, Sydney, Australia, July.R.
J. Kate, Y. W. Wong, and R. J. Mooney.
2005.
Learning totransform natural to formal languages.
In Proc.
of AAAI-05,pages 1062?1068, Pittsburgh, PA, July.H.
Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, andC.
Watkins.
2002.
Text classification using string kernels.Journal of Machine Learning Research, 2:419?444.L.
R. Tang and R. J. Mooney.
2001.
Using multiple clause con-structors in inductive logic programming for semantic pars-ing.
In Proc.
of ECML-01, pages 466?477, Freiburg, Ger-many.V.
N. Vapnik.
1998.
Statistical Learning Theory.
John Wiley& Sons.J.
M. Zelle and R. J. Mooney.
1996.
Learning to parse databasequeries using inductive logic programming.
In Proc.
ofAAAI-96, pages 1050?1055, Portland, OR, August.L.
S. Zettlemoyer and M. Collins.
2005.
Learning to map sen-tences to logical form: Structured classification with proba-bilistic categorial grammars.
In Proc.
of UAI-05, Edinburgh,Scotland, July.84
