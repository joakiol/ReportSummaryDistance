Proceedings of the ACL 2010 Student Research Workshop, pages 7?12,Uppsala, Sweden, 13 July 2010.c?2010 Association for Computational LinguisticsTowards Relational POMDPs for Adaptive Dialogue ManagementPierre LisonLanguage Technology LabGerman Research Centre for Artificial Intelligence (DFKI GmbH)Saarbr?ucken, GermanyAbstractOpen-ended spoken interactions are typi-cally characterised by both structural com-plexity and high levels of uncertainty,making dialogue management in such set-tings a particularly challenging problem.Traditional approaches have focused onproviding theoretical accounts for eitherthe uncertainty or the complexity of spo-ken dialogue, but rarely considered thetwo issues simultaneously.
This paper de-scribes ongoing work on a new approachto dialogue management which attemptsto fill this gap.
We represent the interac-tion as a Partially Observable Markov De-cision Process (POMDP) over a rich statespace incorporating both dialogue, user,and environment models.
The tractabilityof the resulting POMDP can be preservedusing a mechanism for dynamically con-straining the action space based on priorknowledge over locally relevant dialoguestructures.
These constraints are encodedin a small set of general rules expressed asa Markov Logic network.
The first-orderexpressivity of Markov Logic enables usto leverage the rich relational structure ofthe problem and efficiently abstract overlarge regions of the state and action spaces.1 IntroductionThe development of spoken dialogue systems forrich, open-ended interactions raises a number ofchallenges, one of which is dialogue management.The role of dialogue management is to determinewhich communicative actions to take (i.e.
what tosay) given a goal and particular observations aboutthe interaction and the current situation.Dialogue managers have to face several issues.First, spoken dialogue systems must usually dealwith high levels of noise and uncertainty.
Theseuncertainties may arise from speech recognitionerrors, limited grammar coverage, or from variouslinguistic and pragmatic ambiguities.Second, open-ended dialogue is characteristi-cally complex, and exhibits rich relational struc-tures.
Natural interactions should be adaptive toa variety of factors dependent on the interactionhistory, the general context, and the user prefer-ences.
As a consequence, the state space necessaryto model the dynamics of the environment tends tobe large and sparsely populated.These two problems have typically been ad-dressed separately in the literature.
On the onehand, the issue of uncertainty in speech under-standing is usually dealt using a range of proba-bilistic models combined with decision-theoreticplanning.
Among these, Partially ObservableMarkov Decision Process (POMDP) models haverecently emerged as a unifying mathematicalframework for dialogue management (Williamsand Young, 2007; Lemon and Pietquin, 2007).POMDPs provide an explicit account for a widerange of uncertainties related to partial observabil-ity (noisy, incomplete spoken inputs) and stochas-tic action effects (the world may evolve in unpre-dictable ways after executing an action).On the other hand, structural complexity istypically addressed with logic-based approaches.Some investigated topics in this paradigm arepragmatic interpretation (Thomason et al, 2006),dialogue structure (Asher and Lascarides, 2003),or collaborative planning (Kruijff et al, 2008).These approaches are able to model sophisticateddialogue behaviours, but at the expense of robust-ness and adaptivity.
They generally assume com-plete observability and provide only a very limitedaccount (if any) of uncertainties.We are currently developing an hybrid approachwhich simultaneously tackles the uncertainty andcomplexity of dialogue management, based on a7POMDP framework.
We present here our ongo-ing work on this issue.
In this paper, we morespecifically describe a new mechanism for dy-namically constraining the space of possible ac-tions available at a given time.
Our aim is to usesuch mechanism to significantly reduce the searchspace and therefore make the planning problemglobally more tractable.
This is performed in twoconsecutive steps.
We first structure the state spaceusing Markov Logic Networks, a first-order prob-abilistic language.
Prior pragmatic knowledgeabout dialogue structure is then exploited to derivethe set of dialogue actions which are locally ad-missible or relevant, and prune all irrelevant ones.The first-order expressivity of Markov Logic Net-works allows us to easily specify the constraintsvia a small set of general rules which abstract overlarge regions of the state and action spaces.Our long-term goal is to develop an unifiedframework for adaptive dialogue management inrich, open-ended interactional settings.This paper is structured as follows.
Section 2lays down the formal foundations of our work,by describing dialogue management as a POMDPproblem.
We then describe in Section 3 our ap-proach to POMDP planning with control knowl-edge using Markov Logic rules.
Section 4 dis-cusses some further aspects of our approach andits relation to existing work, followed by the con-clusion in Section 5.2 Background2.1 Partially Observable Markov DecisionProcesses (POMDPs)POMDPs are a mathematical model for sequentialdecision-making in partially observable environ-ments.
It provides a powerful framework for con-trol problems which combine partial observability,uncertain action effects, incomplete knowledge ofthe environment dynamics and multiple, poten-tially conflicting objectives.Via reinforcement learning, it is possible toautomatically learn near-optimal action policiesgiven a POMDP model combined with real or sim-ulated user data (Schatzmann et al, 2007).2.1.1 Formal definitionA POMDP is a tuple ?S,A,Z, T,?, R?, where:?
S is the state space, which is the model ofthe world from the agent?s viewpoint.
It isdefined as a set of mutually exclusive states.ztstt?atzt+1st+1st+2zt+2at+1?r(at, st) r(at+1, st+1)Figure 1: Bayesian decision network correspond-ing to the POMDP model.
Hidden variables aregreyed.
Actions are represented as rectangles tostress that they are system actions rather than ob-served variables.
Arcs into circular nodes expressinfluence, whereas arcs into squared nodes are in-formational.
For readability, only one state isshown at each time step, but it should be notedthat the policy pi is function of the full belief staterather than a single (unobservable) state.?
A is the action space: the set of possible ac-tions at the disposal of the agent.?
Z is the observation space: the set of obser-vations which can be captured by the agent.They correspond to features of the environ-ment which can be directly perceived by theagent?s sensors.?
T is the transition function, defined as T :S ?
A ?
S ?
[0, 1], where T (s, a, s?)
=P (s?|s, a) is the probability of reaching states?from state s if action a is performed.?
?
is the observation function, defined as?
: Z ?
A ?
S ?
[0, 1], with ?
(z, a, s?)
=P (z|a, s?
), i.e.
the probability of observing zafter performing a and being now in state s?.?
R is the reward function, defined as R :S ?
A ?
<, R(s, a) encodes the utility forthe agent to perform the action a while instate s. It is therefore a model for the goals orpreferences of the agent.A graphical illustration of a POMDP model asa Bayesian decision network is provided in Fig.
1.In addition, a POMDP can include additionalparameters such as the horizon of the agent (num-8ber of look-ahead steps), and the discount factor(weighting scheme for non-immediate rewards).2.1.2 Beliefs and belief updateA key idea of POMDP is the assumption that thestate of the world is not directly accessible, andcan only be inferred via observation.
Such uncer-tainty is expressed in the belief state b, which isa probability distribution over possible states, thatis: b : S ?
[0, 1].
The belief state for a statespace of cardinality n is therefore represented in areal-valued simplex of dimension (n?1).This belief state is dynamically updated beforeexecuting each action.
The belief state update op-erates as follows.
At a given time step t, the agentis in some unobserved state st= s ?
S .
Theprobability of being in state s at time t is writ-ten as bt(s).
Based on the current belief state bt,the agent selects an action at, receives a rewardR(s, at) and transitions to a new (unobserved)state st+1= s?, where st+1depends only on stand at.
The agent then receives a new observationot+1which is dependent on st+1and at.Finally, the belief distribution btis updated,based on ot+1and atas follows1.bt+1(s?
)= P (s?|ot+1, at, bt) (1)=P (ot+1|s?, at, bt)P (s?|at, bt)P (ot+1|at, bt)(2)=P (ot+1|s?, at)?s?SP (s?|at, s)P (s|at, bt)P (ot+1|at, bt)(3)= ?
?
(ot+1, s?, at)?s?ST (s, at, s?
)bt(s) (4)where ?
is a normalisation constant.
An initialbelief state b0must be specified at runtime as aPOMDP parameter when initialising the system.2.1.3 POMDP policiesGiven a POMDP model ?S,A,Z, T, Z,R?, theagent should execute at each time-step the actionwhich maximises its expected cumulative rewardover the horizon.
The function pi : B ?
A definesa policy, which determines the action to performfor each point of the belief space.The expected reward for policy pi starting frombelief b is defined as:Jpi(b) = E[h?t=0?tR(st, at) | b, pi](5)1As a notational shorthand, we write P (st=s) as P (s)and P (st+1=s?)
as P (s?
).The optimal policy pi?is then obtained by optimiz-ing the long-term reward, starting from b0:pi?= argmaxpiJpi(b0) (6)The optimal policy pi?yields the highest expectedreward value for each possible belief state.
Thisvalue is compactly represented by the optimalvalue function, noted V?, which is a solution tothe Bellman optimality equation (Bellman, 1957).Numerous algorithms for (offline) policy opti-misation and (online) planning are available.
Forlarge spaces, exact optimisation is impossible andapproximate methods must be used, see for in-stance grid-based (Thomson and Young, 2009) orpoint-based (Pineau et al, 2006) techniques.2.2 POMDP-based dialogue managementDialogue management can be easily cast as aPOMDP problem, with the state space being acompact representation of the interaction, the ac-tion space being a set of dialogue moves, the ob-servation space representing speech recognitionhypotheses, the transition function defining thedynamics of the interaction (which user reactionis to be expected after a particular dialogue move),and the observation function describing a ?sensormodel?
between observed speech recognition hy-potheses and actual utterances.
Finally, the rewardfunction encodes the utility of dialogue policies ?it typically assigns a big positive reward if a long-term goal has been reached (e.g.
the retrieval ofsome important information), and small negativerewards for minor ?inconveniences?
(e.g.
prompt-ing the user to repeat or asking for confirmations).Our long-term aim is to apply such POMDPframework to a rich dialogue domain for human-robot interaction (Kruijff et al, 2010).
These inter-actions are typically open-ended, relatively long,include high levels of noise, and require complexstate and action spaces.
Furthemore, the dialoguesystem also needs to be adaptive to its user (at-tributed beliefs and intentions, attitude, attentionalstate) and to the current situation (currently per-ceived entities and events).As a consequence, the state space must be ex-panded to include these knowledge sources.
Be-lief monitoring is then used to continuously updatethe belief state based on perceptual inputs (seealso (Bohus and Horvitz, 2009) for an overview oftechniques to extract such information).
These re-quirements can only be fullfilled if we address the9?curse of dimensionality?
characteristic of tradi-tional POMDP models.
The next section providesa tentative answer.3 Approach3.1 Control knowledgeClassical approaches to POMDP planning oper-ate directly on the full action space and select thenext action to perform based on the maximisationof the expected cumulative reward over the spec-ified horizon.
Such approaches can be used insmall-scale domains with a limited action space,but quickly become intractable for larger ones, asthe planning time increases exponentially with thesize of the action space.
Significant planning timeis therefore spend on actions which should be di-rectly discarded as irrelevant2.
Dismissing theseactions before planning could therefore provideimportant computational gains.Instead of a direct policy optimisation over thefull action space, our approach formalises actionselection as a two-step process.
As a first step, aset of relevant dialogue moves is constructed fromthe full action space.
The POMDP planner thencomputes the optimal (highest-reward) action onthis reduced action space in a second step.Such an approach is able to significantly reducethe dimensionality of the dialogue managementproblem by taking advantage of prior knowledgeabout the expected relational structure of spokendialogue.
This prior knowledge is to be encodedin a set of general rules describing the admissibledialogue moves in a particular situation.How can we express such rules?
POMDPs areusually modeled with Bayesian networks whichare inherently propositional.
Encoding such rulesin a propositional framework requires a distinctrule for every possible state and action instance.This is not a feasible approach.
We therefore needa first order (probabilistic) language able to ex-press generalities over large regions of the stateaction spaces.
Markov Logic is such a language.3.2 Markov Logic Networks (MLNs)Markov Logic combines first-order logic andprobabilistic graphical models in a unified repre-sentation (Richardson and Domingos, 2006).
A2For instance, an agent hearing a user command such as?Please take the mug on your left?
might spent a lot of plan-ning time calculating the expected future reward of dialoguemoves such as ?Is the box green??
or ?Your name is John?, whichare irrelevant to the situation.Markov Logic Network L is a set of pairs (Fi, wi),where Fiis a formula in first-order logic and wiisa real number representing the formula weight.A Markov Logic Network L can be seen asa template for constructing markov networks3.To construct a markov network from L, one hasto provide an additional set of constants C ={c1, c2, ..., c|C|}.
The resulting markov networkis called a ground markov network and is writtenML,C.
The ground markov network contains onefeature for each possible grounding of a first-orderformula in L, with the corresponding weight.
Thetechnical details of the construction of ML,Cfromthe two sets L and C is explained in several pa-pers, see e.g.
(Richardson and Domingos, 2006).Once the markov network ML,Cis constructed,it can be exploited to perform inference over ar-bitrary queries.
Efficient probabilistic inferencealgorithms such as Markov Chain Monte Carlo(MCMC) or other sampling techniques can thenbe used to this end (Poon and Domingos, 2006).3.3 States and actions as relational structuresThe specification of Markov Logic rules apply-ing over complete regions of the state and actionspaces (instead of over single instances) requiresan explicit relational structure over these spaces.This is realised by factoring the state and ac-tion spaces into a set of distinct, conditionally in-dependent features.
A state s can be expanded intoa tuple ?f1, f2, ...fn?, where each sub-state fiisassigned a value from a set {v1, v2, ...vm}.
Suchstructure can be expressed in first-order logic witha binary predicate fi(s, vj) for each sub-state fi,where vjis the value of the sub-state fiin s. Thesame type of structure can be defined over actions.This factoring leads to a relational structure of ar-bitrary complexity, compactly represented by a setof unary and binary predicates.For instance, (Young et al, 2010) factors eachdialogue state into three independent parts s =?su, au, sd?, where suis the user goal, authe lastuser move, and sdthe dialogue history.
Thesecan be expressed in Markov Logic with predicatessuch as UserGoal(s, su), LastUserMove(s, au),or History(s, sd).3Markov networks are undirected graphical models.103.4 Relevant action spaceFor a given state s, the relevant action spaceRelMoves(A, s) is defined as:{am: am?
A ?
RelevantMove(am, s)} (7)The truth-value of the predicateRelevantMove(am, s) is determined using aset of Markov Logic rules dependent on both thestate s and the action am.
For a given state s,the relevant action space is constructed via prob-abilistic inference, by estimating the probabilityP (RelevantMove(am, s)) for each action am,and selecting the subset of actions for which theprobability is above a given threshold.Eq.
8 provides a simple example of suchMarkov Logic rule:LastUserMove(s, au) ?
PolarQuestion(au) ?YesNoAnswer(am)?
RelevantMove(am, s) (8)It defines an admissible dialogue move for a situ-ation where the user asks a polar question to theagent (e.g.
?do you see my hand??).
The rule speci-fies that, if a state s contains auas last user move,and if auis a polar question, then an answer amof type yes-no is a relevant dialogue move for theagent.
This rule is (implicitly) universally quanti-fied over s, auand am.Each of these Markov Logic rules has a weightattached to it, expressing the strength of the im-plication.
A rule with infinite weight and satisfiedpremises will lead to a relevant move with prob-ability 1.
Softer weights can be used to describemoves which are less relevant but still possible ina particular context.
These weights can either beencoded by hand or learned from data (how to per-form this efficiently remains an open question).3.5 Rules application on POMDP belief stateThe previous section assumed that the state s isknown.
But the real state of a POMDP is never di-rectly accessible.
The rules we just described musttherefore be applied on the belief state.
Ultimately,we want to define a function Rel : <n?
P(A),which takes as input a point in the belief spaceand outputs a set of relevant moves.
For efficiencyreasons, this function can be precomputed offline,by segmenting the state space into distinct regionsand assigning a set of relevant moves to each re-gion.
The function can then be directly called atruntime by the planning algorithm.Due to the high dimensionality of the beliefspace, the above function must be approximatedto remain tractable.
One way to perform this ap-proximation is to extract, for belief state b, a setSmof m most likely states, and compute the setof relevant moves for each of them.
We then de-fine the global probability estimate of a being arelevant move given b as such:P (RelevantMove(a) | b, a) ?
?s?SmP (RelevantMove(a, s) | s, a)?
b(s) (9)In the limit wherem?
|S|, the error margin onthe approximation tends to zero.4 Discussion4.1 General commentsIt is worth noting that the mechanism we justoutlined does not intend to replace the existingPOMDP planning and optimisation algorithms,but rather complements them.
Each step serves adifferent purpose: the action space reduction pro-vides an answer to the question ?Is this action rel-evant?
?, while the policy optimisation seeks to an-swer ?Is this action useful??.
We believe that suchdistinction between relevance and usefulness isimportant and will prove to be beneficial in termsof tractability.It is also useful to notice that the Markov Logicrules we described provides a ?positive?
definitionof the action space.
The rules were applied to pro-duce an exhaustive list of all admissible actionsgiven a state, all actions outside this list being defacto labelled as non-admissible.
But the rules canalso provide a ?negative?
definition of the actionspace.
That is, instead of generating an exhaustivelist of possible actions, the dialogue system caninitially consider all actions as admissible, and therules can then be used to prune this action spaceby removing irrelevant moves.The choice of action filter depends mainly onthe size of the dialogue domain and the availabil-ity of prior domain knowledge.
A ?positive?
filteris a necessity for large dialogue domains, as theaction space is likely to grow exponentially withthe domain size and become untractable.
But thepositive definition of the action space is also sig-nificantly more expensive for the dialogue devel-oper.
There is therefore a trade-off between thecosts of tractability issues, and the costs of dia-logue domain modelling.114.2 Related WorkThere is a substantial body of existing work inthe POMDP literature about the exploitation ofthe problem structure to tackle the curse of di-mensionality (Poupart, 2005; Young et al, 2010),but the vast majority of these approaches retaina propositional structure.
A few more theoreti-cal papers also describe first-order MDPs (Wanget al, 2007), and recent work on Markov Logichas extended the MLN formalism to include somedecision-theoretic concepts (Nath and Domingos,2009).
To the author?s knowledge, none of theseideas have been applied to dialogue management.5 ConclusionsThis paper described a new approach to exploit re-lational models of dialogue structure for control-ling the action space in POMDPs.
This approachis part of an ongoing work to develop a unifiedframework for adaptive dialogue management inrich, open-ended interactional settings.
The dia-logue manager is being implemented as part of alarger cognitive architecture for talking robots.Besides the implementation, future work willfocus on refining the theoretical foundations ofrelational POMDPs for dialogue (including howto specify the transition, observation and rewardfunctions in such a relational framework), as wellas investigating the use of reinforcement learningfor policy optimisation based on simulated data.ReferencesN.
Asher and A. Lascarides.
2003.
Logics of Conver-sation.
Cambridge University Press.R.
Bellman.
1957.
Dynamic Programming.
PrincetonUniversity Press.Dan Bohus and Eric Horvitz.
2009.
Dialog in the openworld: platform and applications.
In ICMI-MLMI?09: Proceedings of the 2009 international confer-ence on Multimodal interfaces, pages 31?38, NewYork, NY, USA.
ACM.G.J.M.
Kruijff, M. Brenner, and N.A.
Hawes.
2008.Continual planning for cross-modal situated clarifi-cation in human-robot interaction.
In Proceedings ofthe 17th International Symposium on Robot and Hu-man Interactive Communication (RO-MAN 2008),Munich, Germany.G.-J.
M. Kruijff, P. Lison, T. Benjamin, H. Jacobsson,H.
Zender, and I. Kruijff-Korbayova.
2010.
Situateddialogue processing for human-robot interaction.
InH.
I. Christensen, A. Sloman, G.-J.
M. Kruijff, andJ.
Wyatt, editors, Cognitive Systems.
Springer Ver-lag.
(in press).O.
Lemon and O. Pietquin.
2007.
Machine learn-ing for spoken dialogue systems.
In Proceedingsof the European Conference on Speech Commu-nication and Technologies (Interspeech?07), pages2685?2688, Anvers (Belgium), August.A.
Nath and P. Domingos.
2009.
A language for rela-tional decision theory.
In Proceedings of the Inter-national Workshop on Statistical Relational Learn-ing.J.
Pineau, G. Gordon, and S. Thrun.
2006.
Anytimepoint-based approximations for large pomdps.
Arti-ficial Intelligence Research, 27(1):335?380.H.
Poon and P. Domingos.
2006.
Sound and effi-cient inference with probabilistic and deterministicdependencies.
In AAAI?06: Proceedings of the 21stnational conference on Artificial intelligence, pages458?463.
AAAI Press.P.
Poupart.
2005.
Exploiting structure to efficientlysolve large scale partially observable markov deci-sion processes.
Ph.D. thesis, University of Toronto,Toronto, Canada.M.
Richardson and P. Domingos.
2006.
Markov logicnetworks.
Machine Learning, 62(1-2):107?136.Jost Schatzmann, Blaise Thomson, Karl Weilhammer,Hui Ye, and Steve Young.
2007.
Agenda-baseduser simulation for bootstrapping a POMDP dia-logue system.
In HLT ?07: Proceedings of the45th Annual Meeting of the Association for Compu-tational Linguistics on Human Language Technolo-gies, pages 149?152, Rochester, New York, April.Association for Computational Linguistics.R.
Thomason, M. Stone, and D. DeVault.
2006.
En-lightened update: A computational architecture forpresupposition and other pragmatic phenomena.
InDonna Byron, Craige Roberts, and Scott Schwenter,editors, Presupposition Accommodation.
Ohio StatePragmatics Initiative.B.
Thomson and S. Young.
2009.
Bayesian updateof dialogue state: A pomdp framework for spokendialogue systems.
Computer Speech & Language,August.Ch.
Wang, S. Joshi, and R. Khardon.
2007.
First orderdecision diagrams for relational mdps.
In IJCAI?07:Proceedings of the 20th international joint confer-ence on Artifical intelligence, pages 1095?1100, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.J.
Williams and S. Young.
2007.
Partially observablemarkov decision processes for spoken dialog sys-tems.
Computer Speech and Language, 21(2):231?422.S.
Young, M. Ga?si?c, S. Keizer, F. Mairesse, J. Schatz-mann, B. Thomson, and K. Yu.
2010.
The hiddeninformation state model: A practical framework forpomdp-based spoken dialogue management.
Com-puter Speech & Language, 24(2):150?174.12
