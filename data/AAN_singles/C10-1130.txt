Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1155?1163,Beijing, August 2010Exploiting Salient Patterns for Question Detection and QuestionRetrieval in Community-based Question AnsweringKai WangDepartment of Computer ScienceSchool of ComputingNational University of Singaporekwang@comp.nus.edu.sgTat-Seng ChuaDepartment of Computer ScienceSchool of ComputingNational University of Singaporechuats@comp.nus.edu.sgAbstractQuestion detection serves great purposesin the cQA question retrieval task.
Whiledetecting questions in standard languagedata corpus is relatively easy, it becomesa great challenge for online content.
On-line questions are usually long and infor-mal, and standard features such as ques-tion mark or 5W1H words are likely to beabsent.
In this paper, we explore ques-tion characteristics in cQA services, andpropose an automated approach to detectquestion sentences based on lexical andsyntactic features.
Our model is capableof handling informal online languages.The empirical evaluation results furtherdemonstrate that our model significantlyoutperforms traditional methods in de-tecting online question sentences, and itconsiderably boosts the question retrievalperformance in cQA.1 IntroductionCommunity-based Question Answering services(cQA) such as Yahoo!
Answers have emergedas popular means of information exchange on theweb.
They not only connect a network of peopleto freely ask and answer questions, but also allowinformation seekers to search for relevant histori-cal questions in the cQA archive (Agichtein et al,2008; Xue et al, 2008; Wang et al, 2009).Many research works have been proposed tofind similar questions in cQA.
The state-of-the-artretrieval models include the vector space model(Duan et al, 2008), language model (Duan et al,2008; Jeon et al, 2005), Okapi model (Jeon et al,2005), translation model (Jeon et al, 2005; Rie-zler et al, 2007; Xue et al, 2008), and syntac-tic tree matching model(Wang et al, 2009).
Al-though experimental studies in these works showthat the proposed models are capable of improv-ing question retrieval, they did not give clear ex-planation on which portion of the question thatthe user query is actually matched against.
Aquestion thread from cQA usually comprises sev-eral sub-questions conveying different informa-tion needs, and it is highly desirable to identifyindividual sub-questions and match each of themto the user query.
Getting sub-questions clearlyidentified not only helps the retrieval system tomatch user query to the most desirable content butalso improves the retrieval efficiency.However, the detection of sub-question is non-trivial.
Question sentences in cQA are usuallymixed with various description sentences, andthey usually employ informal languages, wherestandard features such as question mark or ut-terance are likely to be absent.
As such, simpleheuristics using question mark or 5W1H words(who, what, where, why, how) may become in-adequate.
The demand of special techniques indetecting question sentences online arises due tothree particular reasons.
First, the question markcould be missing at the end of a question1, ormight be used in cases other than questions suchas ?Really bad toothache??.
Second, some ques-tions such as ?I?d like to know the expense of re-moving wisdom teeth?
are expressed in a declar-ative form, which neither contains 5W1H wordsnor is neccessarily ended with ???.
Third, somequestion-like sentences do not carry any actual in-formation need, such as ?Please help me??.
Fig-ure 1 illustrates an example of a question thread1It is reported (Cong et al, 2008) that 30% of onlinequestions do not end with question marks.1155S1: What do you guys do when you find that the 'plasticprotection seal' is missing or disturbed.S2: Throw it out, buy a new one.. or just use it anyways?S3: Is it really possible or likely that the item you purchased wastampered with?
?S4: The box was in a plastic wrap but the item itself inside didnot having the protection seal (box says it should) so Icouldn't have inspected it before I bought it.S5: Please suggest??
thanks!Figure 1: An example of a question thread ex-tracted from Yahoo!
Answersfrom Yahoo!
Answers, where sub-questions S1and S2 are posted in non-standard forms, and S5is merely a question-like simple sentence.
To thebest of our knowledge, none of the existing ques-tion retrieval systems are equipped with a com-prehensive question detector module to handlevarious question forms online, and limited efforthas been devoted to this direction.In this paper, we extensively explore character-istics of questions in cQA, and propose a fullyautomated approach to detecting question sen-tences.
In particular, we complement lexical pat-terns with syntactic patterns, and use them as fea-tures to train a classification model that is capableof handling informal online languages.
To savehuman annotations, we further propose to employone-class SVM algorithm for model learning, inwhich only positive examples are used as opposedto requiring both positive and negative examples.The rest of the paper is organized as follows:Section 2 presents the lexical and syntactic pat-terns as used for question detection.
Section 3describes the learning algorithm for the classifi-cation model.
Section 4 shows our experimentalresults.
Section 5 reviews some related work andSection 6 concludes this paper.2 Pattern Mining for Question DetectionAs has been discussed, human generated contenton the Web are usually not well formatted, andnaive methods such as the use of question markand 5W1H words are not adequate to correctlydetect or capture all online questions.
Methodsbased on hand-crafted rules also fail to cope withvarious question forms as randomly appeared onthe Web.
To overcome the shortcomings of thesetraditional methods, we propose to extract a setof salient patterns from online questions and usethem as features to detect question sentences.In this study, we mainly focus on two kindsof patterns ?
sequential pattern at the lexicallevel and syntactic shallow pattern at the syntac-tic level.
Sequential patterns have been well dis-cussed in many literature, including the identifi-cation of comparative sentences (Jindal and Liu,2006), the detection of erroneous sentences (Sunet al, 2007) and question sentences (Cong et al,2008) etc.
However, works on syntactic patternshave only been partially explored (Zaki and Ag-garwal, 2003; Sun et al, 2007; Wang et al, 2009).Grounded on these previous works, we next ex-plain our mining approach of the sequential andsyntactic shallow patterns.2.1 Sequential Pattern MiningSequential Pattern is also referred to as LabeledSequential Pattern (LSP) in the literature.
It isin the form of S?C , where S is a sequence{t1, .
.
.
, tn}, and C is the class label that the se-quence S is classified to.
In the problem of ques-tion detection, a sequence is defined to be a se-ries of tokens from questions, and the class labelsare {Q,NQ}, which stand for question and non-question respectively.The purpose of sequential pattern mining is toextract a set of frequent subsequence of wordsthat are indicative of questions.
For example,the word subsequence ?anyone know what .
.
.to?
could be a good indication to characterize thequestion sentence ?anyone know what I can do tomake me less tired.?.
Note that the mined sequen-tial tokens need not to be contiguous as appearedin the original text.There is a handful of algorithms available forfrequent subsequence extraction.
Pei et al (2001)observed that all occurrences of a frequent patterncan be classified into groups (approximated pat-tern) and proposed a Prefixspan algorithm.
ThePrefixspan algorithm quickly finds out all rela-tive frequent subsequences by a pattern growthmethod, and determines the approximated pat-terns from those subsequences.
We adopt this al-gorithm in our work due to its high reported effi-ciency.
We impose the following additional con-straints for better control over the significance ofthe mined patterns:11561.
Maximum Pattern Length: It limits the maxi-mum number of tokens in a mined sequence.2.
Maximum Token Distance: The two adjacenttokens tn and tn+1 in the pattern need to bewithin a threshold window in the original text.3.
Minimum Support: The minimum percentageof sentences in Q containing the pattern p.4.
Minimum Confidence: The probability of apattern p?Q being true in the whole database.To overcome the word sparseness problem, wegeneralize each sentence by applying the Part-of-Speech (POS) tags to all tokens except some in-dicative keywords such as 5W1H words, modalwords, stopwords etc.
For instance, the questionsentence ?How can I quickly tell if my wisdomteeth are coming?
is converted to ?How can I RBVBP if my NN NNS VBP VBG?, on top of whichthe pattern mining is conducted.
To further cap-ture online language patterns, we mine a set offrequent tokens that are unique to cQA such as?any1?, ?im?
and ?whats?, and keep them frombeing generalized.
The reason to hold back thisset of tokens is twofold.
First, conventional POStaggers are trained from standard English corpus,and they could mis-tag these non-standard words.Second, the special online tokens are analogue tostandard stopwords, and having them properly ex-cluded could help reflect the online users?
textualquestioning patterns.It is expected that the converted patterns pre-serve the most representative features of onlinequestions.
Each discovered pattern makes up abinary feature for the classification model that wewill introduce in Section 3.2.2 Syntactic Shallow Pattern MiningThe sequential patterns represent features at thelexical level, but we found that lexical patternsmight not always be adequate to categorize ques-tions.
For example, the pattern {when, do} couldpresume the non-question ?Levator scapulae isused when you do the traps workout?
to be a ques-tion, whereas the question ?know someone withan eating disorder??
could be overlooked due tothe lack of indicative lexical patterns.These limitations, however, could be allevi-ated by syntactic features.
The syntactic pattern(SBAR(WHADVP(WRB))(S(NP)(VP))) extractedSNP VPNN VBP NPAnyone tryNPweightNNSwatchersSNP VPNN VBP NPSomeone needDTaNNPdietNNmotivator?.
?.Anyone try weight watchers?
Someone need a diet motivator?Figure 2: An example of common syntactic pat-terns observed in two different question sentencesfrom the former example has the order of NPand VP being switched, which could indicatethe sentence to be a non-question, whereas thepattern (VP(VB)(NP(NP)(PP))) may be evidencethat the latter example is indeed a question,because this pattern is commonly witnessed inthe archived questions.
Figure 2 shows an ex-ample that two questions bear very differentwordings but share the same questioning pat-tern (S(NP(NN))(VP(VPB)(NP))) at the syntacticlevel.
In view of the above, we argue that pat-terns at the syntactic level could complement lex-ical patterns in identifying question sentences.To our knowledge, the mining of salient pat-terns at the syntactic level was limited to a fewtasks.
Zaki and Aggarwal (2003) employed treepatterns to classify XML data, Sun et al (2007)extracted all frequent sub-tree structures for erro-neous sentences detection, and Wang et al (2009)decomposed the parsing tree into fragments andused them to match similar questions.
Our workdiffers from these previous works in that: (1) wealso utilize syntactic patterns for the question de-tection; and (2) we do not blindly extract all pos-sible sub-tree structures, but focus only on certainportions of the parsing tree for better pattern rep-resentation and extraction efficiency.Given a syntactic tree T , we define syntac-tic pattern as a part of sub-structures of T suchthat the production rule for each non-leaf node inthe patterns is intact.
For example, the pattern(S(NP(NN))(VP(VPB)(NP))) in Figure 2 is con-sidered to be a valid syntactic pattern, whereas(S(NP(NN))(VP(VPB))) is not, since the produc-tion rule VP?VPB?NP is not strictly complied.We take the following measures to mine salientsyntactic patterns: First, we limit the depth ofeach syntactic pattern to be within a certain range.1157SBARQWHADVP SQWRB MD NP ADVP VPQ: How can I quickly tell if my wisdom teeth are coming?SQMD NP ADVP VPRBPRP SBARVBGeneralizationDecomposition??
?VPVB SBARSSBARIN SVPNPSBARQWHADVP SQWRB MD NP VPSQMD NP VPPRP SBARVB??
?VPVB SBARSSBARIN SVPNP(a) (b) (c) (d)(a?)
(b?)
(c?)
(d?)?
?Figure 3: Illustration of syntactic pattern extrac-tion and generalization processIt is believed that the syntax structure will becometoo specific if it is extended to a deeper level ortoo general if the depth is too shallow, neither ofwhich produces good representative patterns.
Wetherefore set the depth D of each syntactic patternto be within a reasonable range (2?D?4).
Sec-ond, we prune away all leaf nodes as well as theproduction rules at the POS tag level.
We believethat nodes at the bottom levels do not carry muchuseful structural information favored by questiondetector.
For example, the simple grammar ruleNP?DT?NN does not give any insight to use-ful question structures.
Third, we relax the def-inition of syntactic pattern by allowing the re-moval of some nodes denoting modifiers, prepo-sition phrases, conjunctions etc.
The reason isthat these nodes are not essential in representingthe syntactic patterns and are better excluded forgeneralization purpose.
Figure 3 gives an illus-tration of the process for pattern extraction andgeneralization.
In this example, several syntac-tic patterns are generated from the question sen-tence ?How can I quickly tell if my wisdom teethare coming?
?, and the tree patterns (a) and (b) aregeneralized into (a?)
and (b?
), in which the redun-dant branch (ADVP(RB)) that represents the ad-verb ?quickly?
is detached.Contents on the Web are prone to noise, andmost off-the-shelf parsers are not well-trained toparse online questions.
For example, the parsingtree of the question ?whats the matter with it?
?will be very different from that of the question?what is the matter with it??.
It would certainlybe nice to know that ?whats?
is a widely usedshort form of the phrase ?what is?
on the Web,but we are lack of this kind of thesaurus.
Nev-ertheless, we argue that the parsing errors wouldnot hurt the question detector performance muchas long as the mining database is large enough.The reason is that if certain irregular forms fre-quently occur on the Web, there will be statisti-cal evidences that the syntactic patterns derivedfrom it, though not desired, will commonly occuras well.
In other words, we take the wrong pat-terns and utilize them to detect questions in theirregular forms.
Our approach differs from othersystems in that we do not intentionally try to rec-tify the grammatical errors, but leave the errors asthey are and use the statistical based approach tocapture those informal patterns.The pattern extraction process is outlined in Al-gorithm 1.
The overall mining strategy is analo-gous to the mining of sequential patterns, wheresupport and confidence measures are taken intoaccount to control the significance of the minedpatterns.
All mined syntactic patterns togetherwith the lexical patterns will be used as featuresfor learning the classification model.Algorithm 1 ExtractPattern(S, D)Input: A set of syntactic trees for sentences (S); the depthrange (D)Output: A set of sub-tree patterns extracted from S1: Patterns = {}2: for all Syntactic tree T ?
S do3: Nodes ?
Top-down level order traversal of T4: for all node n ?
Nodes do5: Extract subtree p rooted under node n, with depthwithin the range D6: p ?
generalize(p)7: Patterns.add(p)8: end for9: end for10: return Patterns3 Learning the Classification ModelAlthough Conditional Random Fields (CRF) isgood sequential learning algorithm and has beenused in other related work (Cong et al, 2008),here we select Support Vector Machines (SVM)as an alternative learner.
The reason is that ourtask not only deals with sequential patterns butalso involves syntactic patterns that possess nosequential criteria.
Additionally, SVM has beenwidely shown to provide superior results com-pared to other classifiers.1158The input to a SVM binary classifier normallyconsists of both positive and negative examples.While it is easy to discover certain patterns fromquestions, it is unnatural to identify character-istics for non-questions, as they usually do notshare such common lexical and syntactic patterns.The lack of good negative examples leads tra-ditional SVM to perform poorly.
To adapt theimbalanced input data, we proposed to employa one-class SVM method (Manevitz and Yousef,2002) for learning.
The basic idea of one-classSVM is to transform features from only positiveexamples via a kernel to a hyper-plane and treatsthe origin as the only member of the second class.It uses relaxation parameters to separate the posi-tive examples from the origin, and finally appliesthe standard two-class SVM techniques to learna decision boundary.
As a result, anything out-side the boundary are considered to be outliers(i.e.
non-questions in this problem).More formally, given n training samplesx1, .
.
.
, xn of one class, the hyperplane separatingthem from the origin is constructed by solvingmin 12?w?2 + 1?nn?i=1?i ?
?
(1)subject to: w ?
?
(xi) ?
?
?
?i, where ?
is a ker-nel function, ?i is the slack variable, and ?
is theparameter controlling the upper bound percentageof outliers.
If w and ?
solve this problem, the de-cision function f(x) = sign(w ??(x)??)
will bepositive for most examples xi in the training set.Supervised learning methods usually requiretraining data to be manually annotated.
To savelabeling efforts, we take a shortcut by treating allsentences ending with question marks as an initialpositive examples.
This assumption is acceptable,as Cong et al (2008) reported that the rule-basedmethod using only question mark achieves a veryhigh precision of over 97% in detecting questions.It in turn indicates that questions ending with ??
?are highly reliable to be real questions.However, the initial training data still containmany sentences ending with ???
but are not truequestions.
These possible outliers will shift thedecision boundary away from the optimal one,and we need to remove them from the trainingdataset for better classification.
Many prepro-cessing strategies are available for training dataGood positive examples(true questions)Bad positive examples(non-questions)Origin(i) (ii) (iii)Iterations for trainingdata refinement(i)DecisionBoundaryIterationsFigure 4: Illustration of one-class SVM classifi-cation with training data refinement (conceptualonly).
Three iterations (i) (ii) (iii) are presented.refinement, including bootstrapping, condensing,and editing etc.
In this work, we employ a SVM-based data editing and classification method pro-posed by Song et al (2008), which iteratively setsa small value to the parameter ?
of the one-classSVM so as to continuously refine the decisionboundary.
The algorithm could be better visual-ized with Figure 4.
In each iteration, a new de-cision boundary will be determined based on theexisting set of data points, and a portion of pos-sible outliers will be removed from the trainingset.
It is expected that the learned hyperplane willeventually be very close to the optimal one.We use the freely available software LIBSVM2to conduct the one-class SVM training and test-ing.
A linear kernel is used, as it is shown to besuperior in our experiments.
In each refinementiteration, the parameter ?
is conservatively set to0.02.
The number of iteration is dynamically de-termined according to the algorithm depicted in(Song et al, 2008).
Other parameters are all set todefault.
The refined decision boundary from thetraining dataset will be applied to classify ques-tions from non-questions.
The question detectormodel learned will serve as a component for thecQA question retrieval system in our experiments.4 ExperimentsIn this section, we present empirical evaluationresults to assess the effectiveness of our ques-tion detection model.
In particular, we first ex-amine the effects of the number of patterns onquestion detection performance.
We further con-duct experiments to show that our question de-2Available at: http://www.csie.ntu.edu.tw/?cjlin/libsvm1159# of Lexical Confidence # of Syntactic ConfidencePatterns 60% 65% 70% 75% 80% Patterns 60% 65% 70% 75% 80%Support 0.40% 1685 1639 1609 1585 1545Support 0.03% 916 758 638 530 4530.45% 1375 1338 1314 1294 1277 0.04% 707 580 488 402 3410.50% 1184 1151 1130 1113 1110 0.05% 546 450 375 308 2610.55% 1037 1007 989 975 964 0.06% 468 379 314 260 218Table 1: Number of lexical and syntactic patterns mined over different support and confidence valuesLexicalPatternsConfidence SyntacticPatternsConfidence65% 70% 75% 60% 65% 70%P R F1 P R F1 P R F1 P R F1 P R F1 P R F1Support 0.40% 85.7 90.7 88.1 86.9 88.6 87.7 87.8 86.6 87.2Support 0.03% 80.4 83.3 81.9 85.1 77.5 81.1 90.7 70.2 79.10.45% 86.6 90.2 88.4 88.9 88.5 88.7 89.6 86.7 88.2 0.04% 79.0 86.1 82.4 90.1 78.2 83.7 90.8 70.8 79.60.50% 88.5 91.6 88.4 86.4 89.0 87.7 86.2 87.9 87.0 0.05% 80.3 82.5 81.4 88.8 78.4 83.3 89.9 69.0 78.10.55% 86.5 89.9 88.1 88.1 87.5 87.8 88.0 89.2 88.6 0.06% 83.0 83.2 83.1 88.5 77.2 82.4 86.7 75.8 80.9Table 2: Question detection performance over different sets of lexical patterns and syntactic patternstection model combining both lexical and syntac-tic features outperforms traditional rule-based orlexical-based methods.
We finally demonstratethat our question detection model gives additionalperformance boosting to question matching.4.1 Performance Variation over DifferentPattern SetsThe performance of the question detection modelcan be sensitive to the number of features used forlearning.
To find the optimal number of featuresused for model training, we examine the perfor-mance variation over different amount of lexicaland syntactic patterns undertaken for training.Dataset: We collected a total of around 800kquestion threads from Yahoo!
Answers Health-care domain.
From the collected data, we gener-ated the following three datasets:- Pattern Mining Set: Comprising around 350ksentences from 60k question threads, wherethose ending with ???
are treated as questionsand others as non-questions.- Training Set: Positive examples comprisingaround 130k sentences ending with ???
fromanother 60k question threads for the one-classSVM learning algorithm.- Testing Set: Two annotators are asked to tagrandomly picked sentences from the remainingset.
A total of 2,004 question sentences and2,039 non-question sentences are annotated.Methods & Results: We use different combi-nations of support and confidence values to gen-erate different set of patterns.
The support valueranges from 0.40% to 0.55% for lexical patternswith a step size of 0.05%, and ranges from 0.03%to 0.06% for syntactic patterns with a step sizeof 0.01%.
The confidence value for both patternsranges from 60% to 80% with a step size of 5%.These value ranges are empirically determined.Table 1 presents the number of lexical and syn-tactic patterns mined against different support andconfidence value combinations.For each set of lexical or syntactic patternsmined, we use them as features for model train-ing.
We convert the training sentences into a setof feature vectors and employ the one-class SVMalgorithm to train a classifier.
The classifier willthen be applied to predict the question sentencesin the testing set.
To evaluate each question de-tection model, we employ Precision (P ), Recall(R), and F1 as performance metrics, and Table 2presents the results3.We observe from Table 2 that given a fixed sup-port level, the precision generally increases withthe confidence level for both lexical and syntacticpatterns, but the recall drops.
The lexical featureset comprising 1,314 sequential patterns as gen-erated with {sup=0.45%, conf=70%} gives thebest F1 score of 88.7%, and the syntactic featureset comprising 580 syntactic patterns generatedfrom {sup=0.04%, conf=65%} gives the best F1score of 83.7%.
It is noted that the sequentialpatterns give relatively high recall while the syn-tactic patterns give relatively high precision.
Ourreading is that the sequential patterns are capableof capturing most questions, but it may also givewrong predictions to non-questions such as ?Lev-3The results for certain confidence levels are not verypromising and are not shown in the table due to lack of space.1160ator scapulae is used when you do the traps work-out?
that bears the sequential pattern {when, do}.On the other hand, the syntactic patterns couldgive reliable predictions, but its coverage couldsuffer due to the limited number of syntactic pat-terns.
We conjecture that a combination of bothfeatures could further improve the performance.4.2 Performance Comparison withTraditional Question Detection MethodsWe next conduct experiments to compare the per-formance of our question detection model to tra-ditional rule-based or lexical-based methods.Methods & Results: We set up five differentsystems for meaningful comparisons:1.
5W1H (baseline1): a rule-based method using5W1H to determine a question sentence.2.
Question Mark (baseline2): a method using thequestion mark ???
to judge a question.3.
SeqPattern: Using only the set of 1,314 se-quential patterns as features.4.
SynPattern: Using only the set of 580 syntacticpatterns as features.5.
SeqPattern+SynPattern: Merging both lexicaland syntactic patterns and use them as a set offeatures for question detection.We again employ Precision (P ), Recall (R),and F1 as performance metrics to evaluate eachquestion detection system, and tabulate the com-parison results in Table 3.
From the Table, weobserve that 5W1H performs poorly in both preci-sion and recall, and question mark based methodgives relatively low recall although the precisionis the highest amongst all the methods evaluated.This is in line with the results as observed in(Cong et al, 2008).
SeqPattern outperforms thetwo baseline systems in both R and F1 scores,and its combination with SynPattern augmentsthe performance in both precision and recall bya lot.
It also achieves statistically significant im-proved results (t-test, p-value<0.05) as comparedto other four systems.
These results are consistentwith our intuition that syntactic patterns can lever-age sequential patterns in improving the questiondetection performance.It is noted that SeqPattern+SynPattern exhibitsthe highest recall (R) amongst all the systems.The significance test further suggests that manySystem Combination P (%) R(%) F1(%)(1) 5W1H 75.37 49.50 59.76(2) Question Mark 94.12 77.50 85.00(3) SeqPattern 88.92 88.47 88.69(4) SynPattern 90.06 78.19 83.71(5) SeqPattern+SynPattern 92.11 89.67 90.87Table 3: Performance comparisons for questiondetection on different system combinationsquestion sentences miss-detected by 5W1H orQuestion Mark method could be properly cap-tured by our model.
This improvement is mean-ingful, as the question coverage is also an im-portant factor in the cQA question retrieval task,where high recall implies that more similar ques-tions could be matched and returned, hence im-proving the question retrieval performance.4.3 Performance Evaluation on QuestionRetrieval with Question Detection ModelTo further demonstrate that our question detectionmodel can improve question retrieval, we incor-porate it into different question retrieval systems.Methods: We select a simple bag-of-word(BoW) system retrieving questions at the lexicallevel, and a syntactic tree matching (STM) modelmatching questions at the syntactic level (Wang etal., 2009) as two baselines.
For each baseline, wefurther set up two different combinations:- Baseline+QM: Using question mark to detectquestion sentences, and perform question re-trieval on top of the detected questions.- Baseline+QD: Using our proposed model todetect question sentences, and perform ques-tion retrieval on top of the detected questions.This gives rise to additional 4 different systemcombinations for comparison.Dataset: We divide the dataset from Yahoo!Answers into a question repository set (750k) anda test set (50k).
For the baseline systems, all therepository sentences containing both questionsand non-questions are indexed, whereas for sys-tems equipped with QM or QD, only the detectedquestion sentences are indexed for retrieval.
Werandomly select 250 single-sentence questionsfrom the test set as queries, and for each query, theretrieval system will return a list of top 10 ques-tion matches.
We combine the retrieved resultsfrom different systems and ask two annotators tolabel each result to be either ?relevant?
or ?irrel-1161System BoW BoW BoW STM STM STMCombination +QM +QD +QM +QDMAP (%) 58.07 59.89 60.68 66.53 68.41 69.85% improvementof MAP over:Baseline N.A.
+3.13 +4.49 N.A.
+2.83 +4.99Baseline+QM N.A.
N.A.
+1.32 N.A.
N.A.
+2.10P@1 (%) 59.81 61.21 63.55 63.08 64.02 65.42Table 4: Question retrieval performance on differ-ent system combinations measured by MAP andP@1 (Baseline is either BoW or STM)evant?
without telling them which system the re-sult is generated from.
By eliminating some queryquestions that have no relevant matches, the finaltesting set contains 214 query questions.Metrics & Results: We evaluate the questionretrieval performance using two metrics: MeanAverage Precision (MAP) and Top One Precision(P@1).
The results are presented in Table 4.We can see from Table 4 that STM outper-forms BoW.
Applying QM or QD over BoW andSTM boosts the system performance in terms ofboth MAP and P@1.
They also achieve statis-tical significance as judged by paired t-test (p-value<0.05).
More specifically, the MAP onQM coupled systems improves by 3.13% and2.83% respectively over BoW and STM.
This isevidence that having question sentences clearlyidentified could help to retrieve relevant ques-tions more precisely, as without question detec-tion, the user query is likely to be matched to ir-relevant description sentences.
Our question de-tection model (QD) further improves the MAPby 1.32% and 2.1% respectively over BoW+QMand STM+QM, and it also yields better top oneprecision by correctly retrieving questions at thefirst position on 136 and 140 questions respec-tively, out of a total of 214 questions.
These im-provements are in line with our expectation thatour model incorporating salient features at boththe lexical and syntactic levels is comprehensiveenough to capture various forms of questions on-line, and hence improve the performance of ques-tion matching.5 Related WorkResearch on detecting question sentences cangenerally be classified into two categories.
Thefirst category simply employs rule-based methodssuch as question mark, 5W1H words, or hand-crafted regular expressions to detect questions.As discussed, these conventional methods are notadequate to cope with online questions.The second category uses machine learning ap-proaches to detect question sentences.
Shresthaand McKeown (2004) proposed a supervised ruleinduction method to detect interrogative questionsin email conversations based on part-of-speechfeatures.
Yeh and Yuan (2003) used a statisticalapproach to extract a set of question-related wordsand derived some syntax and semantic rules todetect mandarin question sentences.
Cong et al(2008) extracted labeled sequential patterns andused them as features to learn a classifier for ques-tion detection in online forums.Question pattern mining is also closely relatedto the learning of answer patterns.
Work on an-swer patterns includes the web based pattern min-ing (Zhang and Lee, 2002; Du et al, 2005) and acombination of syntactic and semantic elements(Soubbotin and Soubbotin, 2002) etc.In contrast to previous work, we do not only fo-cus on standard language corpus, but extensivelyexplore characteristics of online questions.
Ourapproach exploits salient question patterns at boththe lexical and syntactic levels for question detec-tion.
In particular, we employ the one-class SVMalgorithm such that the learning process is weaklysupervised and no human annotation is involved.6 ConclusionThis paper proposed a new approach to detectingquestion sentences in cQA.Wemined both lexicaland syntactic question patterns, and used them asfeatures to build classification models.
The min-ing and leaning process is fully automated and re-quires no human intervention.
Empirical evalua-tion on the cQA archive demonstrated the effec-tiveness of our model as well as its usefulness inimproving question retrieval performance.We are still investigating other features that arehelpful to detect questions.
One promising direc-tion for future work is to also employ lexical andsyntactic patterns to other related areas such asquestion type classification etc.
It is also interest-ing to employ a hybrid of CRF and SVM learningmethods to boost the accuracy and scalability ofthe classifier.1162ReferencesAgichtein, Eugene, Carlos Castillo, Debora Donato,Aristides Gionis, and Gilad Mishne.
2008.
Find-ing high-quality content in social media.
In WSDM,pages 183?194.Cong, Gao, Long Wang, Chin-Yew Lin, Young-InSong, and Yueheng Sun.
2008.
Finding question-answer pairs from online forums.
In SIGIR, pages467?474.Du, Yongping, Helen Meng, Xuanjing Huang, andLide Wu.
2005.
The use of metadata, web-derived answer patterns and passage context toimprove reading comprehension performance.
InHLT, pages 604?611.Duan, Huizhong, Yunbo Cao, Chin-Yew Lin, andYong Yu.
2008.
Searching questions by identify-ing question topic and question focus.
In HLT-ACL,pages 156?164.Jeon, Jiwoon, W. Bruce Croft, and JoonHo Lee.
2005.Finding similar questions in large question and an-swer archives.
In CIKM, pages 84?90.Jindal, Nitin and Bing Liu.
2006.
Identifying compar-ative sentences in text documents.
In SIGIR, pages244?251.Manevitz, Larry M. and Malik Yousef.
2002.
One-class svms for document classification.
J. Mach.Learn.
Res., 2:139?154.Pei, Jian, Jiawei Han, Behzad Mortazavi-asl, HelenPinto, Qiming Chen, Umeshwar Dayal, and Meichun Hsu.
2001.
Prefixspan: Mining sequen-tial patterns efficiently by prefix-projected patterngrowth.
In ICDE, pages 215?224.Riezler, Stefan, Alexander Vasserman, IoannisTsochantaridis, Vibhu Mittal, and Yi Liu.
2007.Statistical machine translation for query expansionin answer retrieval.
In ACL, pages 464?471.Shrestha, Lokesh and Kathleen McKeown.
2004.
De-tection of question-answer pairs in email conversa-tions.
In COLING, page 889.Song, Xiaomu, Guoliang Fan, and M. Rao.
2008.Svm-based data editing for enhanced one-classclassification of remotely sensed imagery.
Geo-science and Remote Sensing Letters, IEEE,5(2):189?193.Soubbotin,Martin M. and Sergei M. Soubbotin.
2002.Use of patterns for detection of likely answerstrings: A systematic approach.
In TREC.Sun, Guihua, Gao Cong, Xiaohua Liu, Chin-Yew Lin,and Ming Zhou.
2007.
Mining sequential patternsand tree patterns to detect erroneous sentences.
InAAAI, pages 925?930.Wang, Kai, Zhaoyan Ming, and Tat-Seng Chua.
2009.A syntactic tree matching approach to finding sim-ilar questions in community-based qa services.
InSIGIR, pages 187?194.Xue, Xiaobing, Jiwoon Jeon, and W. Bruce Croft.2008.
Retrieval models for question and answerarchives.
In SIGIR, pages 475?482.Yeh, Ping-Jer and Shyan-MingYuan.
2003.
Mandarinquestion sentence detection: A preliminary study.In EPIA, pages 466?478.Zaki, Mohammed J. and Charu C. Aggarwal.
2003.Xrules: an effective structural classifier for xmldata.
In KDD, pages 316?325.Zhang, Dell and Wee Sun Lee.
2002.
Web based pat-tern mining and matching approach to question an-swering.
In TREC.1163
