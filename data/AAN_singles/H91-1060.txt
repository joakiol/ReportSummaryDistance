A Procedure for Quantitatively Comparing the SyntacticCoverage of English GrammarsE.
Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D.Hindle, R. Ingria, F. Jelinek, J. I(lavans, M. Liberman, M. Jl4arcus, S. Roukos, B.Santorini, T. StrzalkowskiIBM Research Division, Thomas J. Watson Research CenterYorktown Heights, NY 10598The problem of quantitatively comparing tile perfor-mance of different broad-coverage rammars of En-glish has to date resisted solution.
Prima facie, knownEnglish grammars appear to disagree strongly witheach other as to the elements of even tile simplestsentences.
For instance, the grammars of Steve Abney(Bellcore), Ezra Black (IBM), Dan Flickinger (IIewlettPackard), Claudia Gdaniec (Logos), Ralph Grishmanand Tomek Strzalkowski (NYU), Phil Harrison (Boe-ing), Don tfindle (AT&T), Bob Ingria (BBN), andMitch Marcus (U. of Pennsylvania) recognize in com-mon only the following constituents, when each gram-marian provides the single parse which he/she wouldideally want his/her grammar to specify for three sam-ple Brown Corpus sentences:The famed Yankee Clipper, now retired, has been as-sisting (as (a batting coach)).One of those cai)ital-gains ventures, ill fact, has sad-dled him (with Gore Court).lie said this constituted a (very serious) misuse (ofthe (Criminal court) processes).Specific differences among grammars which con-tribute to this apparent disparateness of analysis in-clude the treatmeat of punctuation as independent to-kens or, on the other hand, as parasites on the wordsto which they attach in writing; the recursive attach-ment of auxiliary elements to the right of Verb Phrasenodes, versus their incorporation there en bloc; thegrouping of pre-infinitiva,1 "to" either with the mainverb alone or with the entire Verb Phrase that it in-tro(luces; and the employment or non-employment of"null nodes" as a device in the grammar; as well as306other differences.
Despite the seeming intractabilityof this problem, it appears to us that a solution toit is now at hand.
We propose an evaluation pro-cedure with these characteristics: it judges a parsebased only on the constituent boundaries it stipulates(and not the names it assigns to these constituents);it compares the parse to a "hand-parse" of the samesentence from the University of Pennsylvania Tree-bank; and it yields two principal measures for eachparse submitted.The procedure has three steps.
For each parse tobe evaluated: (1) erase from the fully-parsed sentenceall instances of: auxiliaries, "not", pre-infinitival "to",null categories, possessive ndings (% and '), and allword-external punctuation (e.g. "
.
, ; - ) ;  (2) recur-sively erase all parenthesis pairs enclosing either a sin-gle constituent or word, or nothing at all; (3) computegoodness cores (Crossing Parentheses, and Recall)for the input parse, by comparing it to a similarly-reduced version of the Penn Treebank parse of thesame sentence.For example, for the Brown Corpus sentence:Miss Xydis was best when she did not need to be tooprobing, consider the candidate parse:(S(NP-s(PNP(PNP Miss) (PNP Xydis))) (VP(VPASTwas) (ADJP(ADJ best))) (S(COMP(WIIADVP(WI\[ADVwhen))) ( ie -s  (PRO she)) (VP ((VPAST did) (NEG,tot) (V need)) (VP((X to) (V be)) (ADJP(ADV too)(ADJ probing))))))(?
(FIN .
))After step-one rasures, this becomes:(S(NP-s(PNP(PNP Miss) (PNP Xydis))) (VP(VPASTwas) (ADJP(ADJ best))) (S(COMP(WIIADVP(WIIADVwheu))) (NP-s (PRO she)) (VP((VPAST) (NEG)(V need)) (VP((X)  (V be)) (ADJP(ADV too) (ADJprobing)))))) (?
(FIN))And after step-two erasures:(S(NP-s Miss Xydis) (VP was best) (S when she (VPneed (V be (ADJP too probing)))))The Uuiversity of Pennsylvania Treebank output forthis sentence, after steps one and two have been ap-plied to it, is:(S(NP Miss Xydis) (VP was best (SBAR when (S she(VP need (VP be (ADJP too probing)))))))Step three consists of comparing the candidateparse to the treebank parse and deriving two scores:(1) The Crossing Parentheses score is the number oftimes the treebank has a parenthesization such as, say,(A (B C)) and the parse being evaluated has a paren-thesization for the same input of ((A B) C)), i.e.
thereare parentheses which "cross".
(2) The Recall scoreis the number of parenthesis pairs in the intersectionof tlle candidate and treebank parses (T intersectionC) divided by the number of parenthesis pairs in thetreebank parse T, viz.
(T intersection C) / T. Thisscore provides an additional measure of the degree offit between the standard and tile candidate parses; intheory a RecMl of 1 certifies a candidate parse as in-cluding all constituent boundaries that are essentialto the analysis of the input sentence.
We applie d thismetric to 14 sentences selected from the Brown Cor-pus and analyzed by each of the grammarians namedabove in the manner that each wished his/her gram-mar to do.
Instead of using the UPenn Treebank as astandard, we used the automaticMly computed "ma-jority parse" of each sentence obtained from the setof candidate parses themselves.
The average CrossingParentheses rate over all our grammars was .4%, witha corresponding Recall score of 94%.
We have agreedon three additionM categories of systematic alterationto our input parses which we believe will significantlyimprove the correlation between our "ideal parses",i.e.
our individuM goals, and our standard.
Evenat the current level of fit, we feel comfortable Mlow-ing one of our number, the UPenn parse, to serve asthe standard parse, since, crucially, it.
is produced byhand.
Our intention is to apply the current metric tomore Brown Corpus data "ideally parsed" by us, andthen to employ it to measure the performance of ourgrammars, run automatically, on a 1)enchma.rk set ofsentences.APPENDIX:EVALUATION PROCEDURE FOR COMPUTERENGLISH GRAMMARSO.
Input formatA parse for evaluation shouldconsist initially of:(a) the input word string,tokenized as follows:(I) Any tokens containingpunctuation marks areenclosed by verticalbars, e.g.
~D'Albert~I~,oooI(2) Contracted forms inwhich the abbreviatedverb is used in thesentence under analysisas a main verb, asopposed to an auxiliary,are to be split:you 've  -> you l'vel(In "You've a goodreason for that.
"but not in "You've beenhere often.
")John's -> John l'sl(In "John's (i.e.
is) agood friend" or "John's(i.e.
has) a goodfriend" but not "John's(i.e.
is) leaving" andnot "John's (i.e.
has)been here"(3) Hyphenated words,numbers andmiscellaneous digitalexpressions are leftas is (i.e.
not split),i.e.
~co-signersl (andnot "co I-I signers")|12,0001 (and not"2 I , I  0 o 0")~lall-womanl~Ififty-threel:Ifree-for-alll| 56th~13/.1~ 1212-~88-9o271~(b) the parse of the input wordstring with respect to thegrammar under evaluation(I) Each grammaticalconstituent of the inputis grouped using a pairof parentheses, e.g.307"(((I)) ((see) ((Ed))))"(2) Constituent labels may,optionally, immediatelyfol low left parenthesesand~or immediatelyprecede rightparentheses, e.g.
(S (N' (N Sue))(V' (V sees)(N' (N Tom))))  =: ( ( (Sue)  )( ( sees)( (Tom)  ) ) )e tc .I.
Erasures of Input ElementsThe f irst of the two stepsnecessary to prepare init ialparsed input for evaluat ionconsists  of erasing thefo l lowing types of word (token)str ings from the parse:(a) Auxi l iar iesExamples are :"would go there"-?
"go there","has been laughing"- ?
"laughing","does sing it correct ly"- ?
"sing it correctly",but not: "is a cup","is blue", "has a dollar","does the laundry"(b) "Not"E.g.
"is not in here"-> "is in here","Not precisely asleep,John sort of dozed"- ?
"precisely asleep,John sort of dozed"(c ) Pre- inf init ival "to"E.g.
"she opted to retire"- ?
"she opted retire","how to construe it"- ?
"how construe it"(d) Null categoriesExample 1 :("getting more pro lettersthan con"):(NXc (Qr more )(NX (A pro )(Npl letters))(Than than )(NX (A con) (Npl ) ) )NOTA BENE- ?
(NXc (Qr more )(NX (A pro )(Npl letters))(Than than )(NX (A con) ( )));NOTA BENEExample 2 :("The lawyer with whomI studied law"):(NP (DET The )(N lawyer)(S-REL (PP (P with)(NP whom ) )(NP I)(VP (V studied)(NP (N law))(PP 0))))NOTA BENE- ?
(NP (DET The)(N lawyer )(S- REL (PP (P with)(NP whom) )(NP I)(VP (V studied)(NP (N law))(PP ) ) ))NOTA BENE(e) Possess ive endings ( 's, ' )E.g. "
ILori'sl mother"(i.e.
the mother  of Lori)- ?
"Lori mother"(f) Word-external  punctuat ion(quotes, commas, periods,dashes, etc.
)E.g.The "blue book" was there- ?
The blue book was thereYour f i rst  , second andthird ideas -?
Your f i rstsecond and third ideasThis is it.
-> This is itA l l - -or  almost al l - -of  them-> All or almost all of  themBut leave as is: 13,~5618.2gl 13/17/g01 111:301Ip.m.I I1)1 Ieh.D.IIU.N.
I Ine'er-do-welll3082.
Erasures of ConstituentDelimiters, i.e.
ParenthesesThe second of the two stepsnecessary to prepare initialparsed input for evaluationconsists of erasing parenthesispairs, proceeding recursively,from the most to the least deeplyembedded portion of theparenthesization, whenever theyenclose either a singleconstituent or word, or nothingat all.Example:"Miss Xydis was best when shedid not need to be too probing."I.
Original parse(S (NP-s (PNP (PNP Miss )(PNP Xydis )))(VP (VPAST was )(ADJP (ADJ best )))(S (COMP (WHADVP(WHADV when )))(NP-s (PRO she ))(VP ((VPAST did )(MEG not )(V need ))(vP ((x to )(V be ))(ADJP(ADV too )(ADJprobing ))))))(?
(FIN .
))2.
Parse with all erasuresperformed except those ofconst i tuentdel imiters(parentheses):(S (NP-s (PNP (PNP Miss )(PNP Xydis )))(VP (VPAST was )(ADJP (ADJ best )))(S (COMP (WHADVP(WHADV when )))(NP-s (PRO she ))(VP ((VPAST )(MEG )(V need ))(vP ((x )(V be ))(ADJP(ADV too )(ADJprobing ))))))(?
(FIN ))3.
Parse with all constituentdelimiters erased whichare superfluous by the abovedefinition:(S (NP-s MissXydis )(VP wasbest )(S whenshe(vPneed(vPbe(ADJP tooprobing)))))NOTE: Any single-word adverbswhich are left behind, as itwere, by the erasure of auxil iaryelements, are attached to thehighest node of the immediatelyfol lowing verb constituent.Example:(will probably have)(seen Milton) ->( probably )(seen Milton) ->(probably seen Milton)3.
Redefinit ion of SelectedConstituentsThe third step in the process ofpreparing initial parsed inputfor evaluation is necessary onlyif the parse submitted treats anyof three particular constructionsin a manner different from thecanonical analysis currentlyaccepted by the group.
This stepconsists of redrawing constituentboundaries in conformity with theadopted standard.
The threeconstructions involved areextraposition, modification ofnoun phrases, and sequences ofprepositions which occurconstituent-init ial ly and~or309particles which occurconstituent-finally.
(a) ExtrapositionThe treatment accepted atpresent attaches theextraposed clause to thetopmost node of the host(sentential) clause.Example:If initial analysis is:(It (is (necessary(for us to leave))))Then change to standard asfollows:(It (is necessary)(for us to leave))NOTE: The fol lowing is not anexample of extraposition, andtherefore not to be modified,although it seems to differonly minimally from a genuineextraposition sentence such as:"It seemed like a good idea tobegin early":(It (seemed (like ((a goodmeeting) (to begin early)))))(b) Modification of Noun PhrasesThe treatment accepted atpresent attaches themodified "core" noun phraseand all of its modifiersfrom a single (noun phrase)node:Example:If initial analysis is:((((the tree (that (we saw)))(with (orange leaves)))(that (was (very old))))Then change to standard asfollows:((the tree) (that (we saw))(with (orange leaves))(that (was (very old))))(c) Sequences ofConstituent-InitialPrepositions and~orConstituent-Final ParticlesFor sequences ofprepositions occurring atthe start of aprepositional phrase, thecurrently accepted practiceis to attach eachindividually to thepreposit ional-phrase node.For sequences of particleswhich come at the end of averb phrase or otherconstituent with a verbalhead, the adopted standardis, likewise, to attacheach individually to thetop node of theconstituent:Example:If initial analysis is:(We (were (out (of (oatmealcookies)))))Then change to standard asfollows:(We (were (out of (oatmealcookies))))~.
Computation of EvaluationStatistics(a) Number of ConstituentsIncompatible With StandardParseFor the sentence underanalysis, compare theconstituents as del imitedby the standard parse withthose del imited by theparse for evaluation.
Thefirst statistic computedfor each sentence is thenumber of constituents inthe parse being evaluatedwhich "cross", i.e.
areneither subsstrings norsuperstrings of, theconstituents of thestandard parse.Example:Standard parse:((The prospect) (of(cutting back spending)))Parse for evaluation:(The (prospect (of((cutting back)spending))))The (non-unary)constituents of the parsefor evaluation are:3101.
The prospect ofcutting backspend ing2.
prospect of cuttingback spending3.
of cutting backspending4.
cutting back spending5.
cutting backWhile both constituents 2and 5 differ from thestandard, only 2 qualif iesas a "crossing" violation,as 5 is merely a substringof a constituent of thestandard parse.
So the"Constituents IncompatibleWith Standard" score forthis sentence is I.
(b) "Recall" and "Precision" ofParse Being EvaluatedAs a preliminary tocomputing Recall:Number ofStandard-ParseConstituentsin CandidateTotal Number ofStandard-ParseConstituentsand Precision:Number ofCandidate-ParseConstituents in StandardTotal Number ofCandidate-ParseConstituentsthe total number ofconstituents in the standardparse, and in the candidateparse, are simply counted.Notice that "Number ofStandard-Parse Constituentsin Candidate" and "Number ofCandidate-Parse Constituentsin Standard" are merelydifferent names for the sameobject--the intersection ofthe set of standard-parseconstituents with the set ofcandidate-parseconstituents.
So the finalcount prel iminary to thecomputation of Recall andPrecision is the number ofelements in thatintersection.
To return tothe first example of thelast subsection:Standard parse:((The prospect) (of(cutting back spending)))Parse for evaluation:(The (prospect (of((cutting back)spending))))there are  4 standard-parseconstituents, if theconvention is adopted ofexcluding unaryconstituents~ and 5candidate-parseconstituents, under the sameconvention.
Three of theseare common to both sets,i.e.
the intersection hereis 3.Computing Recall andPrecision is accomplishedfor this parse as follows:Recall = 3 /Precision = 3 / 5 .
(C) Combining StatisticsGatheredIn order to evaluate a setof parses, first simplycompute a distribution over"Incompatible Constituents"scores for the parses inthe set, e.g.Incompatible Constituents:0 I 2Frequency:3 I I(Total = 5)Next, average the Recalland Precision scores forthe various parses in theset, e.g.Average Recall = (3 /4  + 7 /8+ 2/4  + 518 + 314)  / 5= .700Average Precision = (3 /5+ 7 /10  + 2 /5  + 5 /10+ 315)  / 5= .560311
