Unsupervised Relation Extraction from Web DocumentsKathrin Eichler, Holmer Hemsen and Gu?nter NeumannDFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken{FirstName.SecondName}@dfki.deAbstractThe IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system.
A user of the systemexpresses an information request in the form of a topic description, which is used for an initial search in order to retrievea relevant set of documents.
On basis of this set of documents, unsupervised relation extraction and clustering is done bythe system.
The results of these operations can then be interactively inspected by the user.
In this paper we describe therelation extraction and clustering components of the IDEX system.
Preliminary evaluation results of these components arepresented and an overview is given of possible enhancements to improve the relation extraction and clustering components.1.
IntroductionInformation extraction (IE) involves the process of au-tomatically identifying instances of certain relations ofinterest, e.g., produce(<company>, <product>, <lo-cation>), in some document collection and the con-struction of a database with information about eachindividual instance (e.g., the participants of a meet-ing, the date and time of the meeting).
Currently, IEsystems are usually domain-dependent and adaptingthe system to a new domain requires a high amountof manual labour, such as specifying and implement-ing relation?specific extraction patterns manually (cf.Fig.
1) or annotating large amounts of training cor-pora (cf.
Fig.
2).
These adaptations have to be madeoffline, i.e., before the specific IE system is actuallymade.
Consequently, current IE technology is highlystatical and inflexible with respect to a timely adapta-tion to new requirements in the form of new topics.Figure 1: A hand-coded rule?based IE?system (schemat-ically): A topic expert implements manually task?specificextraction rules on the basis of her manual analysis of arepresentative corpus.1.1.
Our goalThe goal of our IE research is the conception and im-plementation of core IE technology to produce a newFigure 2: A data?oriented IE system (schematically): Thetask?specific extraction rules are automatically acquired bymeans of Machine Learning algorithms, which are usinga sufficiently large enough corpus of topic?relevant docu-ments.
These documents have to be collected and costlyannotated by a topic?expert.IE system automatically for a given topic.
Here, thepre?knowledge about the information request is givenby a user online to the IE core system (called IDEX)in the form of a topic description (cf.
Fig.
3).
Thisinitial information source is used to retrieve relevantdocuments and extract and cluster relations in an un-supervised way.
In this way, IDEX is able to adaptmuch better to the dynamic information space, in par-ticular because no predefined patterns of relevant re-lations have to be specified, but relevant patterns aredetermined online.
Our system consists of a front-end,which provides the user with a GUI for interactively in-specting information extracted from topic-related webdocuments, and a back-end, which contains the rela-tion extraction and clustering component.
In this pa-per, we describe the back-end component and presentpreliminary evaluation results.1.2.
Application potentialHowever, before doing so we would like to motivatethe application potential and impact of the IDEX ap-Figure 3: The dynamic IE system IDEX (schematically):a user of the IDEX IE system expresses her informationrequest in the form of a topic description which is used foran initial search in order to retrieve a relevant set of doc-uments.
From this set of documents, the system extractsand collects (using the IE core components of IDEX) a setof tables of instances of possibly relevant relations.
Thesetables are presented to the user (who is assumed to be thetopic?expert), who will analyse the data further for her in-formation research.
The whole IE process is dynamic, sinceno offline data is required, and the IE process is interactive,since the topic expert is able to specify new topic descrip-tions, which express her new attention triggered by a novelrelationship she was not aware of beforehand.proach by an example application.
Consider, e.g., thecase of the exploration and the exposure of corruptionsor the risk analysis of mega construction projects.
Viathe Internet, a large pool of information resources ofsuch mega construction projects is available.
Theseinformation resources are rich in quantity, but alsoin quality, e.g., business reports, company profiles,blogs, reports by tourists, who visited these construc-tion projects, but also web documents, which onlymention the project name and nothing else.
One ofthe challenges for the risk analysis of mega construc-tion projects is the efficient exploration of the possiblyrelevant search space.
Developing manually an IE sys-tem is often not possible because of the timely needof the information, and, more importantly, is proba-bly not useful, because the needed (hidden) informa-tion is actually not known.
In contrast, an unsuper-vised and dynamic IE system like IDEX can be usedto support the expert in the exploration of the searchspace through pro?active identification and clusteringof structured entities.
Named entities like for exampleperson names and locations, are often useful indicatorsof relevant text passages, in particular, if the names arein some relationship.
Furthermore, because the foundrelationships are visualized using an advanced graph-ical user interface, the user can select specific namesand find associated relationships to other names, thedocuments they occur in or she can search for para-phrases of sentences.2.
System architectureThe back-end component, visualized in Figure 4, con-sists of three parts, which are described in detail in thissection: preprocessing, relation extraction and relationclustering.2.1.
PreprocessingIn the first step, for a specific search task, a topic ofinterest has to be defined in the form of a query.
Forthis topic, documents are automatically retrieved fromthe web using the Google search engine.
HTML andPDF documents are converted into plain text files.
Asthe tools used for linguistic processing (NE recogni-tion, parsing, etc.)
are language-specific, we use theGoogle language filter option when downloading thedocuments.
However, this does not prevent some doc-uments written in a language other than our targetlanguage (English) from entering our corpus.
In ad-dition, some web sites contain text written in severallanguages.
In order to restrict the processing to sen-tences written in English, we apply a language guessertool, lc4j (Lc4j, 2007) and remove sentences not clas-sified as written in English.
This reduces errors onthe following levels of processing.
We also remove sen-tences that only contain non-alphanumeric characters.To all remaining sentences, we apply LingPipe (Ling-Pipe, 2007) for sentence boundary detection, namedentity recognition (NER) and coreference resolution.As a result of this step database tables are created,containing references to the original document, sen-tences and detected named entities (NEs).2.2.
Relation extractionRelation extraction is done on the basis of parsing po-tentially relevant sentences.
We define a sentence to beof potential relevance if it at least contains two NEs.In the first step, so-called skeletons (simplified depen-dency trees) are extracted.
To build the skeletons, theStanford parser (Stanford Parser, 2007) is used to gen-erate dependency trees for the potentially relevant sen-tences.
For each NE pair in a sentence, the commonroot element in the corresponding tree is identified andthe elements from each of the NEs to the root are col-lected.
An example of a skeleton is shown in Figure 5.In the second step, information based on dependencytypes is extracted for the potentially relevant sen-tences.
Focusing on verb relations (this can be ex-tended to other types of relations), we collect for eachverb its subject(s), object(s), preposition(s) with ar-guments and auxiliary verb(s).
We can now extractverb relations using a simple algorithm: We define averb relation to be a verb together with its arguments(subject(s), object(s) and prepositional phrases) andconsider only those relations to be of interest where atleast the subject or the object is an NE.
We filter outrelations with only one argument.2.3.
Relation clusteringRelation clusters are generated by grouping relationinstances based on their similarity.web documents documentretrievaltopic specific documents plain text documentssentence/documents+NE tableslanguagefilteringsyntactic +typed dependencyparsingsov?relationsskeletons +clusteringconversionPreprocessingRelation extractionRelation clusteringsentencesrelevantfiltering ofrelationfilteringtable of clustered relationssentence boundaryresolutioncoreferencedetection,NE recognition,Figure 4: System architectureFigure 5: Skeleton for the NE pair ?Hohenzollern?
and ?Brandenburg?
in the sentence ?Subsequent members ofthe Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.
?The comparably large amount of data in the corpusrequires the use of an efficient clustering algorithm.Standard ML clustering algorithms such as k-meansand EM (as provided by the Weka toolbox (Wittenand Frank, 2005)) have been tested for clustering therelations at hand but were not able to deal with thelarge number of features and instances required for anadequate representation of our dataset.
We thus de-cided to use a scoring algorithm that compares a re-lation to other relations based on certain aspects andcalculates a similarity score.
If this similarity score ex-ceeds a predefined threshold, two relations are groupedtogether.Similarity is measured based on the output from thedifferent preprocessing steps as well as lexical informa-tion from WordNet (WordNet, 2007):?
WordNet: WordNet information is used to deter-mine if two verb infinitives match or if they are inthe same synonym set.?
Parsing: The extracted dependency information isused to measure the token overlap of the two sub-jects and objects, respectively.
We also comparethe subject of the first relation with the object ofthe second relation and vice versa.
In addition,we compare the auxiliary verbs, prepositions andpreposition arguments found in the relation.?
NE recognition: The information from this stepis used to count how many of the NEs occurringin the contexts, i.e., the sentences in which thetwo relations are found, match and whether theNE types of the subjects and objects, respectively,match.?
Coreference resolution: This type of informationis used to compare the NE subject (or object) ofone relation to strings that appear in the samecoreference set as the subject (or object) of thesecond relation.Manually analyzing a set of extracted relation in-stances, we defined weights for the different similaritymeasures and calculated a similarity score for each re-lation pair.
We then defined a score threshold and clus-tered relations by putting two relations into the samecluster if their similarity score exceeded this thresholdvalue.3.
Experiments and resultsFor our experiments, we built a test corpus of doc-uments related to the topic ?Berlin Hauptbahnhof?by sending queries describing the topic (e.g., ?BerlinHauptbahnhof?, ?Berlin central station?)
to Googleand downloading the retrieved documents specifyingEnglish as the target language.
After preprocessingthese documents as described in 2.1., our corpus con-sisted of 55,255 sentences from 1,068 web pages, fromwhich 10773 relations were automatically extractedand clustered.3.1.
ClusteringFrom the extracted relations, the system built 306 clus-ters of two or more instances, which were manuallyevaluated by two authors of this paper.
81 of our clus-ters contain two or more instances of exactly the samerelation, mostly due to the same sentence appearing inseveral documents of the corpus.
Of the remaining 225clusters, 121 were marked as consistent, 35 as partlyconsistent, 69 as not consistent.
We defined consis-tency based on the potential usefulness of a cluster tothe user and identified three major types of potentiallyuseful clusters:?
Relation paraphrases, e.g.,accused (Mr Moore, Disney, In letter)accused (Michael Moore, Walt DisneyCompany)?
Different instances of the same pattern, e.g.,operates (Delta, flights, from New York)offers (Lufthansa, flights, from DC)?
Relations about the same topic (NE), e.g.,rejected (Mr Blair, pressure, from LabourMPs)reiterated (Mr Blair, ideas, in speech, onMarch)created (Mr Blair, doctrine)...Of our 121 consistent clusters, 76 were classified as be-ing of the type ?same pattern?, 27 as being of the type?same topic?
and 18 as being of the type ?relation para-phrases?.
As many of our clusters contain two instancesonly, we are planning to analyze whether some clustersshould be merged and how this could be achieved.3.2.
Relation extractionIn order to evaluate the performance of the relation ex-traction component, we manually annotated 550 sen-tences of the test corpus by tagging all NEs and verbsand manually extracting potentially interesting verbrelations.
We define ?potentially interesting verb rela-tion?
as a verb together with its arguments (i.e., sub-ject, objects and PP arguments), where at least twoof the arguments are NEs and at least one of themis the subject or an object.
On the basis of this crite-rion, we found 15 potentially interesting verb relations.For the same sentences, the IDEX system extracted 27relations, 11 of them corresponding to the manuallyextracted ones.
This yields a recall value of 73% anda precision value of 41%.There were two types of recall errors: First, errors insentence boundary detection, mainly due to noisy in-put data (e.g., missing periods), which lead to parsingerrors, and second, NER errors, i.e., NEs that werenot recognised as such.
Precision errors could mostlybe traced back to the NER component (sequences ofwords were wrongly identified as NEs).In the 550 manually annotated sentences, 1300 NEswere identified as NEs by the NER component.
402NEs were recognised correctly by the NER, 588wrongly and in 310 cases only parts of an NE wererecognised.
These 310 cases can be divided into threegroups of errors.
First, NEs recognised correctly, butlabeled with the wrong NE type.
Second, only partsof the NE were recognised correctly, e.g., ?Touris-mus Marketing GmbH?
instead of ?Berlin TourismusMarketing GmbH?.
Third, NEs containing additionalwords, such as ?the?
in ?the Brandenburg Gate?.To judge the usefulness of the extracted relations, weapplied the following soft criterion: A relation is con-sidered useful if it expresses the main information givenby the sentence or clause, in which the relation wasfound.
According to this criterion, six of the elevenrelations could be considered useful.
The remainingfive relations lacked some relevant part of the sen-tence/clause (e.g., a crucial part of an NE, like the?ICC?
in ?ICC Berlin?).4.
Possible enhancementsWith only 15 manually extracted relations out of 550sentences, we assume that our definition of ?potentiallyinteresting relation?
is too strict, and that more inter-esting relations could be extracted by loosening the ex-traction criterion.
To investigate on how the criterioncould be loosened, we analysed all those sentences inthe test corpus that contained at least two NEs in orderto find out whether some interesting relations were lostby the definition and how the definition would have tobe changed in order to detect these relations.
The ta-ble in Figure 6 lists some suggestions of how this couldbe achieved, together with example relations and thenumber of additional relations that could be extractedfrom the 550 test sentences.In addition, more interesting relations could befound with an NER component extended by moretypes, e.g., DATE and EVENT.
Open domain NERmay be useful in order to extract NEs of additionaltypes.
Also, other types of relations could be inter-esting, such as relations between coordinated NEs,option example additional relationsextraction of relations,where the NE is not thecomplete subject, object orPP argument, but only partof itCo-operation with <ORG>M.A.X.2001<\ORG> <V>is<\V> clearly ofbenefit to <ORG>BTM<\ORG>.25extraction of relations witha complex VP<ORG>BTM<\ORG> <V>invited and orsupported<\V> more than 1,000 media rep-resentatives in <LOC>Berlin<\LOC>.7resolution of relative pro-nounsThe <ORG>Oxford Centre for MaritimeArchaeology<\ORG> [...] which will<V>conduct<\V> a scientific symposium in<LOC>Berlin<\LOC>.2combination of several of theoptions mentioned above<LOC>Berlin<\LOC> has <V>developed tobecome<\V> the entertainment capital of<LOC>Germany<\LOC>.7Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?could be loosened.
For each option, an example sentence from the test corpus is given, together with the numberof relations that could be extracted additionally from the test corpus.e.g., in a sentence like The exhibition [...] shows<PER>Clemens Brentano<\PER>, <PER>Achimvon Arnim<\PER> and <PER>Heinrich vonKleist<\PER>, and between NEs occurring in thesame (complex) argument, e.g., <PER>Hanns PeterNerger<\PER>, CEO of <ORG>Berlin TourismusMarketing GmbH (BTM) <\ORG>, sums it up [...].5.
Related workOur work is related to previous work on domain-independent unsupervised relation extraction, in par-ticular Sekine (2006), Shinyama and Sekine (2006) andBanko et al (2007).Sekine (2006) introduces On-demand information ex-traction, which aims at automatically identifyingsalient patterns and extracting relations based on thesepatterns.
He retrieves relevant documents from anewspaper corpus based on a query and applies a POStagger, a dependency analyzer and an extended NEtagger.
Using the information from the taggers, he ex-tracts patterns and applies paraphrase recognition tocreate sets of semantically similar patterns.
Shinyamaand Sekine (2006) apply NER, coreference resolutionand parsing to a corpus of newspaper articles to ex-tract two-place relations between NEs.
The extractedrelations are grouped into pattern tables of NE pairsexpressing the same relation, e.g., hurricanes and theirlocations.
Clustering is performed in two steps: theyfirst cluster all documents and use this information tocluster the relations.
However, only relations amongthe five most highly-weighted entities in a cluster areextracted and only the first ten sentences of each arti-cle are taken into account.Banko et al (2007) use a much larger corpus, namely9 million web pages, to extract all relations betweennoun phrases.
Due to the large amount of data, theyapply POS tagging only.
Their output consists of mil-lions of relations, most of them being abstract asser-tions such as (executive, hired by, company) ratherthan concrete facts.Our approach can be regarded as a combination ofthese approaches: Like Banko et al (2007), we extractrelations from noisy web documents rather than com-parably homogeneous news articles.
However, ratherthan extracting relations from millions of pages we re-duce the size of our corpus beforehand using a query inorder to be able to apply more linguistic preprocessing.Like Sekine (2006) and Shinyama and Sekine (2006),we concentrate on relations involving NEs, the assump-tion being that these relations are the potentially in-teresting ones.
The relation clustering step allows usto group similar relations, which can, for example, beuseful for the generation of answers in a Question An-swering system.6.
Future workSince many errors were due to the noisiness of the ar-bitrarily downloaded web documents, a more sophisti-cated filtering step for extracting relevant textual infor-mation from web sites before applying NE recognition,parsing, etc.
is likely to improve the performance ofthe system.The NER component plays a crucial role for the qual-ity of the whole system, because the relation extractioncomponent depends heavily on the NER quality, andthereby the NER quality influences also the results ofthe clustering process.
A possible solution to improveNER in the IDEX System is to integrate a MetaNERcomponent, combining the results of several NER com-ponents.
Within the framework of the IDEX projecta MetaNER component already has been developed(Heyl, to appear 2008), but not yet integrated into theprototype.
The MetaNER component developed usesthe results from three different NER systems.
The out-put of each NER component is weighted depending onthe component and if the sum of these values for a pos-sible NE exceeds a certain threshold it is accepted asNE otherwise it is rejected.The clustering step returns many clusters containingtwo instances only.
A task for future work is to in-vestigate, whether it is possible to build larger clus-ters, which are still meaningful.
One way of enlargingcluster size is to extract more relations.
This couldbe achieved by loosening the extraction criteria as de-scribed in section 4.
Also, it would be interesting to seewhether clusters could be merged.
This would requirea manual analysis of the created clusters.AcknowledgementThe work presented here was partially supported by aresearch grant from the?Programm zur Fo?rderung vonForschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-ment Fund (ERDF).7.
ReferencesMichele Banko, Michael J. Cafarella, Stephen Soder-land, Matthew Broadhead, and Oren Etzioni.
2007.Open information extraction from the web.
In Proc.of the International Joint Conference on ArtificialIntelligence (IJCAI).Andrea Heyl.
to appear 2008.
Unsupervised relationextraction.
Master?s thesis, Saarland University.Lc4j.
2007.
Language categorization library for Java.http://www.olivo.net/software/lc4j/.LingPipe.
2007. http://www.alias-i.com/lingpipe/.Satoshi Sekine.
2006.
On-demand information extrac-tion.
In ACL.
The Association for Computer Lin-guistics.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted re-lation discovery.
In Proc.
of the main conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics, pages 304?311.
Associationfor Computational Linguistics.Stanford Parser.
2007. http://nlp.stanford.edu/downloads/lex-parser.shtml.Ian H. Witten and Eibe Frank.
2005.
Data Min-ing: Practical machine learning tools and techniques.Morgan Kaufmann, San Francisco, 2nd edition.WordNet.
2007. http://wordnet.princeton.edu/.
