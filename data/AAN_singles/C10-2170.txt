Coling 2010: Poster Volume, pages 1489?1497,Beijing, August 2010Sentence Ordering with Event-Enriched Semantics and Two-Layered Clustering for Multi-Document News SummarizationRenxian Zhang                Wenjie Li                   Qin LuDepartment of Computing, the Hong Kong Polytechnic University{csrzhang,cswjli,csluqin}@comp.polyu.edu.hkAbstractWe propose an event-enriched model toalleviate the semantic deficiencyproblem in the IR-style text processingand apply it to sentence ordering formulti-document news summarization.The ordering algorithm is built on eventand entity coherence, both locally andglobally.
To accommodate the event-enriched model, a novel LSA-integratedtwo-layered clustering approach isadopted.
The experimental result showsclear advantage of our model overevent-agonistic models.1 IntroductionOne of the crucial steps in multi-documentsummarization (MDS) is information ordering,right after content selection and before sentencerealization (Jurafsky and Martin, 2009:832?834).
Problems with this step are the culprit formuch of the dissatisfaction with automaticsummaries.
While textual order may guide theordering in single-document summarization, nosuch guidance is available for MDS ordering.A sensible solution is ordering sentences byenhancing coherence since incoherence is thesource of disorder.
Recent researches in thisdirection mostly focus on local coherence bystudying lexical cohesion (Conroy et al, 2006)or entity overlap and transition (Barzilay andLapata, 2008).
But global coherence, i.e.,coherence between sentence groups with thewhole text in view, is largely unaccounted forand few efforts are made at levels higher thanentity or word in measuring sentence coherence.On the other hand, event as a high-levelconstruct has proved useful in MDS contentselection (Filatova and Hatzivassiloglou, 2004;Li et al, 2006).
But the potential of event insummarization has not been fully gauged andfew publications report using event in MDSinformation ordering.
We will argue that eventis instrumental for MDS information ordering,especially multi-document news summarization(MDNS).
Ordering algorithms based on eventand entity information outperform those basedonly on entity information.After related works are surveyed in section 2,we will discuss in section 3 the problem ofsemantic deficiency in IR-based text processing,which motivates building event information intosentence representation.
The details of suchrepresentation are provided in section 4.
Insection 5, we will explicate the orderingalgorithms, including layered clustering andcluster-based ordering.
The performance of theevent-enriched model will be extensivelyevaluated in section 6.
Section 7 will concludethe work with directions to future work.2 Related WorkIn MDS, information ordering is often realizedon the sentence level and treated as a coherenceenhancement task.
A simple ordering criterionis the chronological order of the eventsrepresented in the sentences, which is oftenaugmented with other ordering criteria such aslexical overlap (Conroy et al, 2006), lexicalcohesion (Barzilay et al, 2002) or syntacticfeatures (Lapata 2003).A different way to capture local coherence insentence ordering is the Centering Theory (CT,Grosz et al 1995)-inspired entity-transitionapproach, advocated by Barzilay and Lapata(2005, 2008).
In their entity grid model,syntactic roles played by entities and transitionsbetween these syntactic roles underlie thecoherence patterns between sentences and in the1489whole text.
An entity-parsed corpus can be usedto train a model that prefers the sentenceorderings that comply with the optimal entitytransition patterns.Another important clue to sentence orderingis the sentence positional information in asource document, or ?precedence relation?,which is utilized by Okazaki et al (2004) incombination with topical clustering.Those works are all relevant to the currentwork because we seek ordering clues fromchronological order, lexical cohesion, entitytransition, and sentence precedence.
But we alsoadd an important member to the panoply ?
event.Despite its intuitive and conceptual appeal,event is not as extensively used insummarization as term or entity.
Filatova andHatzivassiloglou (2004) use ?atomic events?
asconceptual representations in MDS contentselection, followed by Li et al (2006) who treatevent terms and named entities as graph nodesin their PageRank algorithm.
Yoshioka andHaraguchi (2004) report an event reference-based approach to MDS content selection forJapanese articles.
Although ?sentencereordering?
is a component of their model, itrelies merely on textual and chronological order.Few published works report using eventinformation in MDS sentence ordering.Our work will represent text content at twolevels: event vectors and sentence vectors.
Thisis close in spirit to Bromberg?s (2006) enrichedLSA-coherence model, where both sentence andword vectors are used to compute a centroid asthe topic of the text.3 Semantic Deficiency in IR-Style TextProcessingAs automatic summarization traces its root toInformation Retrieval (IR), it inherits the vectorspace model (VSM) of text representation,according to which a sentence is treated as a bagof words or stoplist-filtered terms.
The order orrelation among the terms is ignored.
Forexample,1a) The storm killed 120,000 people in Jamaicaand five in the Dominican Republic before movingwest to Mexico.1b) [Dominican, Mexico, Jamaica, Republic, five,kill, move, people, storm, west]1c) [Dominican Republic, Mexico, Jamaica,people, storm]1b) and 1c) are the term-based and entity-based representations of 1a) respectively.
Theyonly indicate what the sentence is about (i.e.,some happening, probably a storm, in someplace that affects people), but ?aboutness?
is afar cry from informativeness.
For instance, nomessage about ?people in which place, Mexicoor Jamaica, are affected?
or ?what moves towhere?
can be gleaned from 1b) although suchmessage is clearly conveyed in 1a).
In otherwords, the IR-style text representation issemantically deficient.We argue that a natural text, especially anews article, is not only about somebody orsomething.
It also tells what happened tosomebody or something in a temporal-spatialmanner.
A natural approach to meeting the?what happened?
requirement is to introduceevent.4 Event-Enriched SentenceRepresentationIn summarization, an event is an activity orepisode associated with participants, time, place,and manner.
Conceptually, event bridgessentence and term/entity and partially fills thesemantic gap in the sentence representation.4.1 Event Structure and ExtractionFollowing (Li et al 2006), we define an event Eas a structured semantic unit consisting of oneevent term Term(E) and a set of event entitiesEntity(E).
In the news domain, event terms aretypically action verbs or deverbal nouns.
Lightverbs such as ?take?, ?give?, etc.
(Tan et al,2006) are removed.Event entities include named entities andhigh-frequency entities.
Named entities denotepeople, locations, organizations, dates, etc.High-frequency entities are common nouns orNPs that frequently participate in news events.Filatova and Hatzivassiloglou (2004) take thetop 10 most frequent entities and Li et al (2006)take the entities with frequency > 10.
Ratherthan using a fixed threshold, we reformulate?high-frequency?
as relative statistics based on(assumed) Gaussian distribution of the entitiesand consider those with z-score > 1 as candidateevent entities.Event extraction begins with shallow parsingand named entity recognition, analyzing each1490sentence S into ordered lists of event terms {t1,t2, ?}.
Low-frequency common entities areremoved.
If a noun is decided to be an eventterm, it cannot be (the head noun of) an entity.The next step is to identify events with eventterms and entities.
Filatova andHatzivassiloglou (2003) treat events as tripletswith two event entities sandwiching oneconnector (event term).
But the numberrestriction on entities is counterintuitive and isdropped in our method.
We first identify n + 1Segi segmented by n event terms tj.?
t1 ?
?
tj-1 ?
tj ?
tj+1 ?
?
tn ?Figure 1.
Segments among Event TermsFor each tj, the corresponding event Ej areextracted by taking tj and the event entities in itsnearest entity-containing Segp and Segq.Ej = [tj, Entity(Segp)?Entity(Segq)]            (Eq.
1)where p = argmax?????????????(????)
?
?
and q= argmin?????????????(????)
?
?
if such p and qexist.
1d) is the event-extracted result of 1a).1d) {[killed, [storm, people, Jamaica, DominicanRepublic]], [moving, [people, Jamaica, DominicanRepublic, west, Mexico]]}From this representation, it is easy to identifythe two events in sentence 1a) led by the eventterms ?killed?
and ?moving?.
Unlike the triplets(two named entities and one connector) in(Filatova and Hatzivassiloglou 2003), an eventin our model can have an unlimited number ofevent entities, as is often the real case.Moreover, we can tell that the ?killing?
involves?people?, ?storm?, ?Jamaica?, etc.
and the?moving?
involves ?Jamaica?, ?DominiqueRepublic?, etc.The shallow parsing-based approach isadmittedly coarse-grade (e.g., ?storm?
ismissing from the ?moving?
event), but theextracted event-enriched representations help toalleviate the semantic deficiency problem in IR.4.2 Event RelationsThe relations between two events include eventterm relation and event entity relation.
Twoevents are similar if their event terms are similarand/or their event entities are similar.
Suchsimilarities are in turn defined on the word level.For event terms, we first find the root verbs ofdeverbal nouns and then measure verb similarityby using the fine-grained relations provided byVerbOcean (Chklovski and Pantel, 2004),which has proved useful in summarization (Liuet al, 2007).
But unlike (Liu et al, 2007), wecount in all the verb relations except antonymybecause considering two antonymous verbs assimilar is counterintuitive.
The other fourrelations ?
similarity, strength, enablement,before ?
are all considered in our measurementof verb similarity.
If we denote the normalizedscore of two verbs on relation i as VOi(V1, V2)with i = 1, 2, 3, 4 corresponding to the abovefour relations, the term similarity of two events?t(E1, E2) is defined as in Eq.
2, where ?
is asmall number to suppress zeroes.
?
= 0.01 ifVOi(V1, V2) = 1 and otherwise ?
= 0.?t(E1, E2) = ?t(Term(E1), Term(E2)) = 1 ??
(1 ?
?
??(???)???),???)???))
+ ????? )
(Eq.
2)Entity similarity is measured by the sharedentities between two events.
Li et al (2006)define entity similarity as the number of sharedentities, which may unfairly assign high scoresto events with many entities in our model.
Sowe decide to use the normalized result as shownin Eq.
3, where ?e(E1, E2) denotes the evententity-based similarity between events E1 and E2.
?e(E1, E2) =|??????(??)???????(??)||??????(??)???????(??)|(Eq.
3)?
(E1, E2), the score of event similarity, is alinear combination of ?t(E1, E2) and ?e(E1, E2).?
(E1, E2) = ?1 ?
?t(E1, E2) + (1 ?
?1) ?
?e(E1, E2) (Eq.
4)4.3 Statistical Evidence for News EventsIn this work, we introduce events as a middle-layer representation between words andsentences under the assumptions that 1) eventsare widely distributed in a text and that 2) theyare natural clusters of salient information in atext.
They guarantee the relevance of event toour task ?
summaries are condensed collectionsof salient information in source documents.In order to confirm them, we scan the wholedataset in our experiment, which consists of 42200w human extracts and 39 400w humanextracts for the DUC 02 multi-document extracttask.
Detailed information about the dataset canbe found in Section 6.
Table 1 lists the statistics.200w 400w200w +400wSourceDocsEntity/Sent 8.78 8.48 8.47 6.01Entity/Word 0.34 0.33 0.33 0.30Event/Sent 2.43 2.26 2.28 1.42SegnSegj-1 SegjSeg01491Event/Word 0.09 0.09 0.09 0.07Sents withevents/Sents86.9% 85.1% 84.6% 71.3%Table 1.
Statistics from DUC 02 DatasetThere are on average 1.42 events per sentencein the source documents, and more than 70% ofall the sentences contain events.
The high eventdensity confirms our first assumption about thedistribution of events.
For the 200w+400wcategory consisting of all the human-selectedsentences, there are on average 2.28 events persentence, a 60% increase from the same ratio inthe source documents.
The proportion of event-containing sentences reaches 84.6%, 13%higher than that in the source documents.
Suchis evidence that events count into the extract-worthiness of sentences, which confirms oursecond assumption about the relevance ofevents to summarization.
The data also showhigher entity density in the extracts than in thesource documents.
As entities are still reliableand domain-independent clues of salient content,we will consider both event and entity in thefollowing ordering algorithm.5 MDS Sentence Ordering with Eventand Entity CoherenceIn this section, we discuss how event canfacilitate MDS sentence ordering with layeredclustering on the event and sentence levels andthen how event and entity information can beintegrated in a coherence-based algorithm toorder sentences based on sentence clusters.5.1 Two-layered ClusteringAfter sentences are represented as collections ofevents, we need to vectorize events andsentences to facilitate clustering and cluster-based sentence ordering.For a document set, event vectorizationbegins with aggregating all the event terms andentities in a set of event units (eu).
Given mdistinct event terms, n distinct named entities,and p distinct high-frequency common entities,the m + n + p eu?s are a concatenation of theevent terms and entities such that eui is an eventterm for 1 ?
i ?
m, a named entity for m + 1 ?
i?
m + n, and a high-frequency entity for m + n +1 ?
i ?
m + n + p. The eu?s define the m + n + pdimensions of an event vector in an eu-by-eventmatrix E = [eij], as shown in Figure 2.??????????
?
????
?
????
?
????
?????,?
?
????,??
???????,?
?
??????,???????
?Figure 2. eu-by-Event MatrixWe further define EntityN(Ej) and EntityH(Ej)to be the set of named entities and set of high-frequency entities of Ej.
Then,??(???,???)???))
1 ?
i ?
meij =?
??(???,?)?????????(??)????????(??
)?m + 1 ?
i ?
m + n?
??(???,?)?????????(??)????????(??
)?m + n + 1 ?
i ?m + n + p (Eq.
5)2 w1 is identical to w2?n(w1, w2) =  1 w1 (w2) is a part of w2 (w1) or theyare in a hypernymy / holonymyrelationship0 otherwise                          (Eq.
6)1 w1 is identical to w2?h(w1, w2) = 0.5 w1 are w2 are synonyms0 otherwise                       (Eq.
7)In Eq.
5, ?t(w1, w2) is defined as in Eq.
2.Both the entity-based ?n(w1, w2) and ?h(w1, w2)are measured in terms of total equivalence(identity) and partial equivalence.
For namedentities, partial equivalence applies to structuralsubsumption (e.g., ?Britain?
and ?Great Britain?
)and hypernymy/holonymy (e.g., ?South Africa?and ?Zambia?).
For common entities, it appliesto synonymy (e.g., ?security?
and ?safety?
).Partial equivalence is considered because of thelexical variations frequently employed injournalist writing.
The named entity scores aredoubled because they represent the essentialelements of a news story.Since the events are represented as vectors,sentence vectorization based on events is not asstraightforward as on entities or terms.
In thiswork we propose a novel approach of two-layered clustering for the purpose.
The basicidea is clustering events at the first layer andthen using event clusters as a feature tovectorize and cluster sentences at the secondE1, E2, ?
Eqeu1?eum?eum+n...eum+n+p1492layer.
Hard clustering of events, such as K-means, not only results in binary values in eventvectors and data sparseness but also isinappropriate.
For example, if EC1 clustersevents all with event terms similar to t* and EC2clusters events all with event entity sets similarto e* (a set), what about event {t*, e*}?Assigning it to either EC1 or EC2 is problematicas it is partially similar to both.
So we decide todo soft clustering at the first layer.A well-studied soft clustering technique is theExpectation-Maximization (EM) algorithmwhich iteratively estimates the unknownparameters in a probability mixture model.
Weassume a Gaussian mixture model for the qevent vectors V1, V2, ?, Vq, with hiddenvariables Hi, initial means Mi, priors ?i, andcovariance matrix Ci.
The E-step is to calculatethe hidden variables ???
for each Vt and the M-step re-estimates the new priors ?i?, means Mi?,and covariance matrix Ci?.
We iterate the twosteps until the log-likelihood converges within athreshold = 10-6.
The performance of the EMalgorithm is sensitive to the initial means, whichare pre-computed by a conventional K-means.In a preliminary study, we found that theevent vectors display pronounced sparseness.
Asolution to this problem in an effort to leveragethe latent ?event topics?
among eu?s is theLatent Semantic Analysis (LSA, Landauer andDumais, 1997) approach.
We apply LSA-styledimensionality reduction to the eu-by-eventmatrix E by doing Singular ValueDecomposition (SVD).
A problem is with thenumber h of the largest singular values, whichaffects the performance of dimensionalityreduction.
In this work, we adopt a utility-basedmetric to find the best h* by maximizing intra-cluster similarity (?h) and minimizing inter-cluster similarity (?h) corresponding to the h-dimensionality reductionh* = argmax?
??
???
(Eq.
8)?h is defined as the mean of average clustersimilarities measured by cosine distance and ?his the mean of cluster centroid similarities.Because the EM clustering assigns a probabilityto every event vector, we also take thoseprobabilities into account when calculating ?hand ?h.Based on the EM clustering of events, wevectorize a sentence by summing up theprobabilities of its constituent event vectorsover all event clusters (ECs) and obtaining anEC-by-sentence (Sn) matrix S = [sij].????
?
????
?
????
?
???
?Figure 3.
EC-by-Sentence Matrixsij = ?
P(????????????)?????
where ??????
is Er?s vector.At the sentence layer, hard clustering issufficient because we need definitive, notprobabilistic, membership information for thenext step ?
sentence ordering.
We use K-meansfor the purpose.
The LSA-style dimensionalityreduction is still in order as possibleperformance gain is expected from thediscovery of latent EC ?topics?.
The decision ofthe best dimensionality is the same as before,except that no probabilities are included.5.2 Coherence-Based Sentence OrderingOur ordering algorithm is based on sentenceclusters, which is designed on the observationthat human writers and summarizers organizesentences by blocks (paragraphs).
Sentenceswithin a block are conceptually close to eachother and adjacent sentences cohere with eachother.
Local coherence is thus realized withinblocks.
On the other hand, blocks are notrandomly ordered.
Two blocks are put next toeach other if their contents are close enough toensure text-level coherence.
So text-level, orglobal coherence is realized among blocks.We believe in MDNS, the block-styleorganization is a sensible strategy taken byhuman extractors to sort sentences fromdifferent sources.
Sentence clusters aresimulations of such blocks and our orderingalgorithm will be based on local coherence andglobal coherence described above.First we have to pinpoint the leading sentencefor an extract.
Using the heuristic of time andtextual precedence, we first generate a set ofpossible leading sentences L = {Li} as theintersection of the document-leading extractsentence set LDoc and the time-leading sentenceset LTime.
Note that |LDoc| = the number ofdocuments, LTime is in fact a sentence collectionof time-leading documents, and LDoc ?
LTime ?
?.S1, S2, ?
SnEC1?ECm1493If L is a singleton, finding the leadingsentence SL is trivial.
If not, SL is decided to bethe sentence in L most similar to all the othersentences in the extract sentence set P so that itqualifies as a good topic sentence.SL = argmax????
?
??????(??
,??)????\{??}
(Eq.
9)where ??????(?
?, ??)
is the similarity between S1and S2 in terms of their event similarity ?
(S1, S2)and entity similarity ?
(S1, S2).
?
(S1, S2) is anextended version of ?
(E1, E2) (Eq.
4) byaveraging the ?t(Ei, Ej) and ?e(Ei, Ej) for all (Ei,Ej) pairs in S1 ?S2.?
(S1, S2) = ?2 ??
??(??,??)?????,?????|?????(??)??????(??
)|+(1 ?
?2)??
??(??,??)?????,?????|?????(??)??????(??)|(Eq.
10)where Event(S) is the set of all events in S.
Next,?
(S1, S2) is the cosine similarity between theirentity vectors ??????
and ??????
with entity weightsconstructed according to Eq.
6 and 7.
Then,??????(?
?, ??)
= ?3??
(S1, S2) +(1 ?
?3)??
(S1, S2) (Eq.
11)After the leading sentence is determined, weidentify the leading cluster it belongs to and ourlocal coherence-based ordering starts with thiscluster.
We adopt a greedy algorithm, whichselects each time from the unordered sentenceset a sentence that best coheres with thesentence just selected, called anchor sentence.Matching each candidate sentence with theanchor sentence only in terms of ??????
wouldassume that the sentences are isolated anddecontextualized.
But the anchor sentence didnot come from nowhere and in order to find itsbest successor, we should also seek clues fromits source context, which is inspired by the?sentence precedence?
by Okazaki et al (2004).More formally, given an anchor sentence Si atthe end of the ordered sentence list, we selectthe next best sentence Si+1 according to theirassociative similarity and substitutivesimilarity, two crucial measures invented by us.Associative similarity SimASS(Si, Sj) measureshow Si and Sj associate with each other in termsof their event and entity coherence, whichalmost is ????????
?, ???.
But to better capture thetransition between entities and the flow of topic,we also consider a topic-continuity score tc(Si,Sj) according to the Centering Theory.
If thetopic continuity is measured in terms of entitychange, local coherence can be captured by thecentering transitions (CB and CP) in adjacentsentences.
Based on (Taboada and Wiesemann,2009), we assign 0.2 to the Establish andContinue transitions, 0.1 to Smooth Shift andRetain, and 0 to other centering transitions.Since tc(Si, Sj) only applies to entities, it istreated as a bonus affiliated to ?
(Si, Sj).???????
?
?, ???
= ?4 ?
?
(Si, Sj) + (1 ?
?4) ?
?
(Si, Sj)?
(1 + tc(Si, Sj))                                                 (Eq.
12)Substitutive similarity accommodates whatwe earlier emphasized about the ?source context?of the extracted sentences by measuring to whatdegree Si and Sj resemble each other?s relevantsource context.
More formally, let LC(Si) andRC(Si) be the left and right source contexts of Sirespectively, and the substitutive similaritySimSUB(Si, Sj) is defined as follows.???????
?
?, S??
= ???????
?
?, ??
( ??)?
+?????????
( ??
), S??
(Eq.
13)In this work, we simply take LC(Si) and RC(Si)to be the left adjacent sentence and rightadjacent sentence of Si in the source document.Note that tc(Si, Sj) does not apply here.
In viewof the chronological order widely accepted inMDS ordering, a time penalty, tp(Si, Sj), is usedto discount the score by 0.8 if Si?s documentdate is later than Sj?s document date.
Finally, Eq.14 summarizes our intra-cluster orderingmethod in a sentence cluster SCk.Si+1 = argmax??????\{??}
???
?
???????
?
?, ???
+(1 ?
??)
?
???????
?
?, ????
?
??
( ?
?, ??)
(Eq.
14)After all the sentences in the current sentencecluster are ordered, we move on by consideringthe similarity of sentence clusters.
Given aprocessed sentence cluster SCi, the next bestsentence cluster SCi+1 is the one that maximizesthe cluster similarity SimCLU(SCi, SCj) amongthe set of all clusters U.
Since clusters arecollections of sentences, their similarity is themean of cross-cluster pairwise sentencesimilarities, each calculated according to Eq.
14.Eq.
15 shows how SCi+1 is computed.SCi+1=argmax?????\{???}??????(???
, ???)
(Eq.
15)This is how we incorporate (block-style)global coherence into MDS sentence ordering.Starting from the second chosen sentencecluster, we choose the first sentence in thecurrent cluster with reference to the lastsentence in the previous processed cluster andapply Eq.
14.
We continue the whole processuntil all the extract sentences are ordered.14946 EvaluationIn this section, we report the experimental resulton the DUC 02 dataset.6.1 DataWe use the dataset of the DUC 02summarization track for MDS because itincludes an extraction task for which modelextracts are provided.
For every document set, 2model extracts are provided each for the 200wand 400w length categories.
We use 1 randomlychosen model extract per document set perlength category as the gold standard.We intended to use all the 59 document setson DUC 02 but found that for some categories,both model extracts contain material fromsections such as the title, lead, or even byline.Those extracts are incompatible with our designtailored for news body extracts.
Therefore wehave to filter them and retain only those extractswith all units selected from the news body.
As aresult, we collect 42 200w extracts and 39 400wextracts as our experimental dataset.6.2 Peer OrderingsWe evaluate the role played by various keyelements in our approach, including event, topiccontinuity, time penalty, and LSA-styledimensionality reduction.
In addition, weproduce a random ordering and a baselineordering according to chronological and textualorder only.
Table 2 lists the 9 peer orderings tobe evaluated, with their codes.A RandomB Baseline (time order + textual order)C Entity only (no LSA)D Event only (no LSA)E Entity + Event ?
topic continuity (no LSA)F Entity + Event ?
time penalty (no LSA)G Entity + Event (no LSA)H Entity + Event (event clustering LSA)I Entity + Event (event + sentence clustering LSA)Table 2.
Peer Orderings6.3 MetricsA popular metric used in sequence evaluationis Kendall?s ?
(Lapata, 2006), which measuresordering differences in terms of the number ofadjacent sentence inversions necessary toconvert a test ordering to the reference ordering.?
= 4m/(n(n ?
1))             (Eq.
16)where m is the number of inversions describedabove and n is the total number of sentences.The second metric we use is the AverageContinuity (AC) developed by Bollegala et al(2006), which captures the intuition that theordering quality can be estimated by the numberof correctly arranged continuous sentences.??
= exp( ?????
log( ??
+ ?)????
(Eq.
17)where k is the maximum number of continuoussentences, ?
is a small value in case Pn = 1.
Pn,the proportion of continuous sentences of lengthn in an ordering, is defined as m/(N ?
n + 1)where m is the number of continuous sentencesof length n in both the test and referenceorderings and N is the total number of sentences.We set k = 4 and ?
= 0.01.6.4 ResultWe empirically determine all the parameters (?i)and produce all the peer orderings.
Table 3 liststhe result, where we also show the statisticalsignificance between the full model peerordering ?I?
and all other versions, marked by *(p < .05) and ** (p < .01) on a two-tailed t-test.PeerCode200w 400wKendall?s ?
AC Kendall?s ?
ACA 0.014** 0.009** -0.019** 0.004**B 0.387 0.151* 0.259** 0.151*C 0.369* 0.128* 0.264* 0.156*D 0.380 0.163 0.270* 0.158*E 0.375* 0.156* 0.267* 0.157*F 0.388 0.159* 0.264* 0.157*G 0.385 0.158* 0.269* 0.162H 0.384 0.164 0.292* 0.170I 0.395 0.170 0.350 0.176Table 3.
Evaluation ResultAlmost all versions with entity and eventinformation outperform the baseline.
The LSA-style dimensionality reduction proves effectivefor our task, as the full model (Peer I) ranks firstand significantly beats versions without eventinformation, topic continuity, or LSA.
ApplyingLSA to both event and sentence clustering isbetter than applying it only to event clustering(Peer H), which produces unstable results and issometimes outperformed by no-LSA versions(Peer G).Event (Peer D) proves to be more valuablethan entity (Peer C) as the event-only versionsoutperform the entity-only version in allcategories, which is predicable because events1495are high-level constructs that incorporate mostof the document-level important entities.When entity is used, extra bonus can begained from topic continuity concerns from CT(Peer E vs.
Peer G) because the centeringtransition effectively captures the coherencepattern between adjacent sentences.
The effectof the chronological order seems less clear (PeerF vs. P) as removing it hurts longer extractsrather than short extracts.
Thereforechronological clues are more valuable forarranging more sentences from the same sourcedocument.Our ordering algorithm achieves even betterresult with long extracts because the importanceof order and coherence grows with text length.Measured by Kendall?s ?, the full modelordering in the 400w category is significantlybetter than all other orderings.For a qualitative evaluation, we select the200w extract d080ae and list all the sentences inFigure 4.
The event terms are boldfaced and theevent entities are underlined.Limited by space, let?s focus on the baseline(1 2 3 4 5 6), entity-only (3 5 2 4 6 1), and full-model versions (3 5 4 2 1 6).
The news extractis about the acquitting of child molesters.
Boththe ?acquitting?
and ?molesting?
events arefound in 1) and 3) but only the latter qualifies asthe topic sentence because it contains importantevent entities.
Choosing 3) instead of 1) as theleading sentence shows the advantage of ourevent-enriched model over the baseline.
Thesame choice is made by the entity-only versionbecause 3) happens to be also entity-intensive.In order to see the advantage of the full modelover the entity-only model, let?s consider 2) and4).
2) is chosen by the entity-only model after 5)because of the heavy entity overlap between 5)abecause of the heavy entity overlap between 5)and 2).
But semantically, 2) is not as close to 5)as 4) because only 4) contains entities for boththe ?acquitting?
(?juror?)
and ?molesting?(?children?)
events and intuitively, 4) continuesthe main trial-acquittal event topic but 2)supplies only secondary information.
Weexamined the sentence clusters before theordering and found that 3), 5), and 4) areclustered together only by the full model,leading to better coherence, locally and globally.7 Conclusion and Future WorkWe set out by realizing the semantic deficiencyof IR and propose a low-cost approach ofbuilding event semantics into sentencerepresentation.
Event extraction relies onshallow parsing and external knowledge sources.Then we propose a novel approach of two-layered clustering to use event information,coupled with LSA-style dimensionalityreduction.
MDS sentence ordering is guided bylocal and global coherence to simulate theblock-style writing and is realized by a greedyalgorithm.
The evaluation shows clearadvantage of our event-enriched model overbaseline and event-agonistic models,quantitatively and qualitatively.The extraction approach can be refined bydeep parsing and rich verb (frame) semantics.
Ina follow-up project, we will expand our datasetand experiment with more data and incorporatehuman evaluation in comparative tasks.AcknowledgmentThe work described in this paper was partiallysupported by a grant from the HK RGC (ProjectNumber: PolyU5217/07E).1) Thursday's acquittals in the McMartin Pre-School molestation case outraged parents who said prosecutors botched it,while those on the defense side proclaimed a triumph of justice over hysteria and hype.2) Originally, there were seven defendants, including Raymond Buckey's sister, Peggy Ann Buckey, and Virginia McMartin,the founder of the school, mother of Mrs. Buckey and grandmother of Raymond Buckey.3) Seven jurors who spoke with reporters in a joint news conference after acquitting Raymond Buckey and his mother,Peggy McMartin Buckey, on 52 molestation charges Thursday said they felt some children who testified may have beenmolested _ but not at the family-run McMartin Pre-School.4) ``The children were never allowed to say in their own words what happened to them,'' said juror John Breese.5) Ray Buckey and his mother, Peggy McMartin Buckey, were found not guilty Thursday of molesting children at thefamily-run McMartin Pre-School in Manhattan Beach, a verdict which brought to a close the longest and costliest criminaltrial in history .6) As it becomes apparent that McMartin cases will stretch out for years to come, parents and the former criminal defendantsalike are trying to resign themselves to the inevitability that the matter may be one they can never leave behind.Figure 4.
Extract sentences of d80ae, 200w1496ReferencesBarzilay, R., Elhadad, N., and McKeown, K. 2002.Inferring Strategies for Sentence Ordering inMultidocument News Summarization.
Journal ofArtificial Intelligence Research, 17:35?55.Barzilay, R., and Lapata, M. 2005.
Modeling LocalCoherence: An Entity-based Approach.
InProceedings of the 43rd Annual Meeting of theACL, 141-148.
Ann Arbor.Barzilay, R., and Lapata, M. 2008.
Modeling LocalCoherence: An Entity-Based Approach.Computational Linguistics, 34:1?34.Bollegala, D, Okazaki, N., and Ishizuka, M. 2006.
ABottom-up Approach to Sentence Ordering forMulti-document Summarization.
In Proceedingsof the 21st International Conference onComputational Linguistics and 44th AnnualMeeting of the ACL, 385?392.
Sydney, Australia.Bromberg, I.
2006.
Ordering Sentences According toTopicality.
Presented at the MidwestComputational Linguistics Colloquium.Chklovski, T., and Pantel, P. 2004.
VerbOcean:Mining the Web for Fine-Grained Semantic VerbRelations.
In Proceedings of Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP-04).
11?13.
Barcelona,Spain.Conroy, J. M., Schlesinger, J. D., and Goldstein, J.2006.
CLASSY Tasked Based Summarization:Back to Basics.
In proceedings of the DocumentUnderstanding Conference (DUC-06).Filatova, E., and Hatzivassiloglou, V. 2003.
Domain-independent detection, extraction, and labeling ofatomic events.
In Proceedings of RANLP, 145?152, Borovetz, Bulgaria.Filatova, E., and Hatzivassiloglou, V. 2004.
Event-Based Extractive Summarization.
In Proceedingsof the ACL-04, 104?111.Grosz, B. J., Aravind K. J., and Scott W. 1995.Centering: A framework for Modeling the LocalCoherence of Discourse.
ComputationalLinguistics, 21(2):203?225.Jurafsky D., and Martin, J. H. 2009.
Speech andLanguage Processing, Second Edition.
UpperSaddle River, NJ: Pearson Education International.Landauer, T., and Dumais, S. 1997.
A solution toPlato?s problem: The latent semantic analysistheory of the acquisition, induction, andrepresentation of knowledge.
PsychologicalReview, 104.Lapata, M. 2003.
Probabilistic Text Structuring:Experiments with Sentence Ordering.
InProceedings of the Annual Meeting of ACL, 545-552.
Sapporo, Japan.Li, W., Wu, M., Lu, Q., Xu, W., and Yuan, C. 2006.Extractive Summarization Using Inter- and Intra-Event Relevance.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and 44th Annual Meeting of the ACL,369?376.
Sydney.Liu, M., Li, W., Wu, M., and Lu, Q.
2007.
ExtractiveSummarization Based on Event Term Clustering.In Proceedings of the ACL 2007 Demo and PosterSessions, 185?188.
Prague.Okazaki, N., Matsuo, Y., and Ishizuka, M. 2004.Improving Chronological Ordering by PrecedenceRelation.
In Proceedings of 20th InternationalConference on Computational Linguistics(COLING 04), 750?756.Taboada, M., and Wiesemann, L., Subjects andtopics in conversation.
Journal of Pragmatics(2009), doi:10.1016/j.pragma.2009.04.009.Tan, Y. F., Kan, M., and Cui, H. 2006.
Extendingcorpus-based identification of light verbconstructions using a supervised learningframework.
In Proceedings of the EACL 2006Workshop on Multi-word-expressions in amultilingual context, 49?56, Trento, Italy.Yoshioka, M., and Haraguchi, M. 2004.
MultipleNews Articles Summarization Based on EventReference Information.
In Working Notes ofNTCIR-4, Tokyo.1497
