Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 694?704,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsSubsentential Sentiment on a Shoestring:A Crosslingual Analysis of Compositional ClassificationMichael Haas and Yannick VersleyInstitute for Computational LinguisticsUniversity of Heidelberg{haas,versley}@cl.uni-heidelberg.deAbstractSentiment analysis has undergone a shift fromdocument-level analysis, where labels ex-presses the sentiment of a whole document orwhole sentence, to subsentential approaches,which assess the contribution of individualphrases, in particular including the composi-tion of sentiment terms and phrases such asnegators and intensifiers.Starting from a small sentiment treebank mod-eled after the Stanford Sentiment Treebank ofSocher et al (2013), we investigate suitablemethods to perform compositional sentimentclassification for German in a data-scarce set-ting, harnessing cross-lingual methods as wellas existing general-domain lexical resources.1 IntroductionIn sentiment classification, we find a general ten-dency from document-level classification towardsmore fine-grained approaches that yield a more de-tailed appraisal of the judgement performed in thetext - in particular, using composition over syntac-tic structure to get a more detailed approach overphrases.For English movie reviews, work using the Stan-ford Sentiment Treebank (SSTb) has shown thatsuch subsentential sentiment information can yieldapproaches with both very high accuracy (Socheret al, 2013; Dong et al, 2014; Hall et al, 2014) andprecise information about the role of each phrase?
information which can subsequently used for ex-tracting or summarizing the sentiment expressed inthe text.The effort for creating a sentiment treebank suchas the SSTb, however, seems prohibitive if wewanted to create such a resource for each pairof relevant domain and language: Compared todocument-level annotations for sentiment, which areeasy to come by (e.g., star ratings), annotating indi-vidual syntactic phrases requires considerable effort.The main focus of this paper is the questionif and how it is possible to reach sensible per-formance for compositional sentiment classifica-tion when we only have limited resources to spendon an in-language, in-domain sentiment treebank.For this goal, we use a new resource, the Hei-delberg Sentiment Treebank (HeiST), which is aGerman-language counterpart to the Stanford Senti-ment Treebank in the sense that it makes explicit thecomposition of sentiment expression over syntacticphrases.
Our experiments on HeiST provide a di-rect comparison of different techniques for harness-ing cross-lingual, cross-domain, or cross-task infor-mation, and are the first of this kind to specificallytarget compositional sentiment analysis.Figure 1 (next page) shows a schematic overviewof the experiments: beyond supervised baseline ex-periments using SVM classification and a super-vised RNTN model (section 3), we evaluated cross-lingual projection (section 4), lexicon-based ap-proaches (section 5), as well as semi-supervised ap-proaches based on word clusters (section 6).2 Related WorkThe starting point for our research is the idea thatthe sentiment of larger stretches of text can be calcu-lated through composition over smaller stretches oftext, which was investigated in a learning frameworkby both Yessenalina and Cardie (2011) and Socheret al (2011, 2012), both learning in a compositional694HeiST(train)Crowdsourcingof HeiST [3]SVM [3.2]RNTN baseline [3.2]RNTN + clustersGoogleTranslateEN modelSSTbEnglishHeiST(test)projection/tree align [4][6]DE model clusterslexicon [5]split [6]GermanevaluationFigure 1: Plan to the experiments described in this paperfashion from datasets that only have document-levelsentiment annotation.On the same dataset as Socher et al, Wang andManning (2012) later demonstrated that unigramand bigram features in an SVM-based classificationframework can reach a greater accuracy than theearlier recursive neural network approach of Socheret al (2011, 2012), which calls into question the as-sumption that sentiment composition can be learnedpurely from sentence-level annotations.Compositionality through Tensors In subse-quent work, Socher et al (2013) introduce the Stan-ford Sentiment Treebank, which contains detailedannotations of sentiment values for individual syn-tactic phrases in a binarized tree, and an approachbased on recursive neural tensor networks (RNTN)which yields significant improvements over the ear-lier approaches using token-level features.The RNTN represents the contribution of individ-ual nodes as vectors of reals and achieves its preci-sion by using a tensor V[1:d]?
R2d?2d?das wellas a matrix W ?
R2d?dto capture second-order de-pendencies between the two children of a node in thetree (with vectors a, b), yielding first a vector h byhi=[ab]TV[i][ab]+ Wi[ab]then using a monotonic nonlinear function on h(here: tanh) to yield the vector for this node.
Thesentiment label of a node is then gained by multi-plying these hidden vectors by a matrix Ws, yield-ing a five-dimensional vector with the classification.Using hidden vectors for each node and capturingsecond-order interaction between the two child vec-tors a and b, the RNTN model achieves descrip-tive power greater than that of TreeCRFs (Nakagawaet al, 2010), and similar to latent-variable modelsthat have been very successful in syntactic parsing(Petrov et al, 2006).In later work, Zhu et al (2014) show that theRNTN?s lexicalized modeling of negators and theirbehaviour leads to increased descriptive power ofthe model, which results in an improved treatmentof negation.
Dong et al (2014) introduce an ap-proach that chooses between multiple compositiontensors (AdaMC-RNTN), which yields further gainswith respect to RNTN performance.In contrast to the lexicalized and high-dimensional RNTN model, there are severallines of work that attempt to work in a moredata-scarce setting.Lexicon-based approaches The classical ap-proach for performing sentiment classification in asetting where training data is sparse can be seenin the SO-CAL approach of Taboada et al (2011):Using a manually curated dictionary with senti-ment values for multiple parts of speech, and aset of heuristics that predict how intensifiers, nega-695tors/shifters as well as nonveridical moods affect thesentiment of a phrase, they show that it is possible toreach good results across different domains.Choi and Cardie (2009) show that it is possibleto adapt an existing general-domain sentiment lexi-con to a specific domain using an approach that opti-mizes a joint objective of classification loss, sparsityof the changes made to the lexicon, and ambiguityof lexicon entries.
Their approach yields apprecia-ble gains over the general-domain lexicon, both withCRF-based machine learning classification and witha simpler ?vote & flip?
algorithm that is based onmajority voting and negators.Crosslingual Sentiment Analysis involves theusage of a dataset in one language to perform sen-timent analysis in another language; in their work,Banea et al (2013) show that translating text in thetarget language to the source language and applyinga well-tuned sentiment classification system worksbetter than either translating the training corpus orthe lexicon used by the system.In research by other groups, Wan (2009) ad-vocates a bootstrapping approach that combinessource-side and target-side features in one classi-fier; Duh et al (2011) note that crosslingual senti-ment analysis techniques always incur a loss due tothe shift in language from the source language textsto the target language even though the general do-main is the same.
Popat et al (2013) argue that fullmachine translation is not useful for resource-scarcelanguages, and propose to use cross-lingual clus-tering both to improve the generalization capabilitywithin a single language as well as for crosslingualprojection, which works better than machine trans-lation with the English-Hindi language pair.It should be noted that most of the work presentedin the last two paragraph works with document-levelsentiment, or (in the case of Choi and Cardie) withshallower annotations, and offers additional chal-lenges in the case of sentiment composition.3 Low-Budget Treebanking for SentimentFor both supervised training and for evaluation, wecreated a German dataset that is close in domainto the Stanford Sentiment treebank (Socher et al,2013), covering opinionated sentences from moviereviews with phrase-level sentiment annotations.--0Waswhat--0mirme--0schwerheavy--aufon00diethe0Nervennerves0gehtgoes?What really gets on my nerves .
.
.
?Figure 2: A multiword expression in HeiSTThe original Stanford Sentiment Treebank isbased on the dataset of Pang and Lee (2005), whichincludes 10,662 sentences from excerpts of moviereviews published on rottentomatoes.com.
Itshould be noted that these excerpts are much morelikely to express an opinion than general text or eventhe main body of a movie review since they containprecisely a summary of the opinion.In order to match both domain and role of thesesentences most precisely, we collected creative-commons-licensed reviews from a German moviereview site, filmrezensionen.de, and usedonly the summary part of these documents, yielding1184 sentences, for which we crowdsourced annota-tion for each individual phrase in the binary tree (seeFigure 2 for an example tree fragment).For the purpose of getting binary phrase trees,sentences were processed with the Berkeley Parser(Petrov et al, 2006), NP nodes were added insidePPs (Samuelsson and Volk, 2004) and the result-ing parse trees binarized using the head table inCoreNLP (Rafferty and Manning, 2008), yielding14,321 unique phrases.Annotation was outsourced via the CrowdFlowerservice, which collects three judgements for eachphrase and computes an end result through vot-ing, using unambiguous test items (which we com-posed from strongly positive or strongly negativeadjective-noun combinations) to filter out annotatorslacking the requisite understanding of German.696Experiment A: Features, Confidence Prec ReclSentiWS 0.882 0.959SentiWS+Regression 0.894 0.967SentiWS+Regression, @50% 0.935 0.985SentiWS+Regression+POS 0.912 0.960SentiWS+Regression+POS, @50% 0.978 0.997Experiment B: Classifier (@50%) Prec ReclLinear SVM 0.975 0.980Random Forest 0.984 0.992Gradient Boosting 0.978 0.997Table 1: Filtering out objective phrasesThe HeiST treebank, as well as the code used inthese experiments, are available for research pur-poses.13.1 Selecting Subjective PhrasesOne possible approach to reduce annotation effortwould be to annotate only those phrases that a clas-sification model deems to have sentiment content inthe first place.
As a more extreme example of suchan approach, consider the MLSA sentiment datasetfor German, where 270 sentences were selected thatalready contained two words from an existing senti-ment lexicon (Clematide et al, 2012), with the goalof getting sentences with interesting interactions be-tween sentiment words.
Given the potential benefits(getting more data for the same annotation effort), anapproach that filters out non-interesting (confidentlyobjective) phrases would be highly appealing.For the pre-classification experiment, we usedcross-validation on 20 to assess the potential im-pact of strategies for saving.
For the correspondingclassifier, we used features from a German general-domain sentiment lexicon, a regression model fordocument-level sentiment (see section 5.2), as wellas part-of-speech tag features in a gradient boostingclassifier.
As seen in table 1, the sentiment lexicon,especially in conjunction with the regression modeland a POS-based filter, would allow to detect unin-teresting (objective) phrases with high accuracy.
Welimit ourselves to the 50% of most confident clas-sifications, and as a measure of caution, the filteris bypassed for any phrase that contains a word inone of several sentiment dictionaries (see section 5).The classifier has a precision of 96.5% for objective1http://www.cl.uni-heidelberg.de/?versley/HeiST/System Node Acc.
Root Acc.HeiST, only pos+neg sentencessupervisedRNTN (tuned) 0.776 0.687SVM (unigrams, coarse) 0.850??0.774?
?SVM (unigrams, fine) 0.835?
?0.735cross-lingualCLSA (simple feat.)
0.823?
?0.737CLSA (complex feat.)
0.810?
?0.738?Comparison: HeiST, all sentencessupervisedRNTN (tuned) 0.803 0.703*/**: significantly better than RNTN (p < 0.05 / p < 0.005)Table 2: HeiST baseline, cross-lingual projection, SVM.System Node Acc.
Root Acc.Comparison: SSTb sample, pos+neg sentenceslexicon-basedGeneral Inquirer 0.824 0.715SubjectivityClues 0.820 0.695supervisedRNTN, 500 sent.
0.704 0.526RNTN, 1000 sent.
0.738 0.539RNTN, 1500 sent.
0.756 0.569SVM, 500 sent.
0.803 0.652SVM, 1000 sent.
0.814 0.675SVM, 1500 sent.
0.823 0.683RNTN, 6920 sent.a0.876 0.854SVM, 6920 sent.a0.846 0.794a: published figures from Socher et al (2013)Table 3: Comparison figures on subsets of the StanfordSentiment Treebankphrases while catching about 66.7% of all objectivenodes.
While this would correspond to substantialsavings (about a quarter of all nodes would be as-signed the ?neutral?
label and not annotated), wewould also lose a fraction of non-neutral phrase andintroduce an unwanted bias (towards lexicon-basedresources) into our dataset.3.2 Baseline resultsWe use the existing RNTN implementation ofSocher et al (2013) to train and test supervisedlearning for sentiment composition, using cross-validation.
For parameter tuning, we varied thenumber of vector dimensions as well as the size ofthe minibatches used in training, and found that theresulting classifier yields very sensible results com-pared to a similarly-sized sample from SSTb (see697Tables 2 and 3).
We evaluate our results as in Socheret al (2013): we consider the recall of positive andnegative nodes while ignoring both neutral nodesand the difference between positive (+) and stronglypositive (++) or between negative (-) and stronglynegative (--) nodes, respectively.
Socher et al re-move sentences with neutral overall sentiment intraining as well as in testing, which seems to worsenthe RNTN performance on our dataset (see Table2), although other methods seem to be less affectedby it.
For comparability reasons, all other reportedfigures are based on Socher?s non-neutral-sentences-only setting.
In comparison results on SSTb (see Ta-ble 3), classification experiments from the Englishdata also show poor results for the RNTN classifierat small data sizes, in parallel with anecdotal evi-dence on recurrent neural networks having troublewith small dataset sizes.24 Crosslingual Projection forCompositional SentimentOur crosslingual approach follows Banea et al(2013) in assuming that machine translation of thetarget documents to the source language, then ap-plying a source-language sentiment analysis, and fi-nally projecting the result back to the target side willyield usable sentiment classification.
In differenceto previous approaches for cross-lingual sentimentanalysis, however, our annotation transfer concernsnot just analysis results for the complete sentence,but for individual syntactic nodes.After translating the target-language trees us-ing the Google Translate API, we parsed the sen-tences using the English model of the Stanfordparser, and applied the RNTN model of Socheret al (2013) trained on the English Stanford Sen-timent Treebank, yielding a labeling for each syn-tactic node with a sentiment value.
We then per-formed word alignment using the PostCAT wordaligner (Ganchev et al, 2008) with a model trainedon the OPUS version of the EuroParl corpus (Tiede-mann, 2012), and alignment of syntactic nodes us-ing the Lingua::Align toolbox for tree align-ment Tiedemann (2010) with a model trained on theSmultron parallel treebank (Volk et al, 2010).2Alec Radford (2015): General Sequence Learn-ing using Recurrent Neural Nets, https://indico.io/blog/general-sequence-learning-using-recurrent-neural-nets/System Node Acc.
Root Acc.Vote-onlyKlenner et al 0.769 0.646GermanPolarityClues 0.815 0.648SentiWS 0.815 0.711SentiMerge [0.0] 0.660 0.577SentiMerge [0.23] 0.718 0.604SentiMerge [0.4] 0.724 0.604Amazon+Lasso 0.499 0.426Vote-and-flipKlenner et al 0.780 0.646GermanPolarityClues 0.802 0.680SentiWS 0.807 0.665SentiMerge [0.0] 0.653 0.582SentiMerge [0.23] 0.717 0.607SentiMerge [0.4] 0.723 0.603Amazon+Lasso 0.471 0.413Table 4: Lexicon-based phrase labelingUsing the word alignment and ourLingua::Align model, we are able to map98.6% of the target-language nodes to a correspond-ing node on the source (English) side, whereas theremaining nodes are assigned the same sentimentlabel as the root.
As can be seen in table 2, a modelthat uses simpler features for Lingua::Alignworks less well than the full feature model.
Con-sidering that the RNTN on the Stanford SentimentTreebank reaches 87.6% node accuracy and 85.4%root accuracy, we see that the crosslingual pro-jection step induces a loss in accuracy, but stillperforms well in comparison to the approaches thatuse the HeiST training data.5 Lexicon-based ApproachesConsidering that the size of HeiST creates a sparsedata problem for the RNTN learner, it is naturalto ask whether we can improve the generalizationcapabilities of the system by either using a less-supervised approach or by generalizing over individ-ual word forms to alleviate the sparse data problem.5.1 General-domain lexiconSeveral general-domain sentiment lexicons exist forGerman, including those of Klenner et al (2009),Waltinger (2010a), Remus et al (2010), and Emer-son and Declerck (2014).698Klenner et al (2009) created their polarity lexi-con by semiautomatic extension of an existing one:starting from a set of 2866 adjective seeds, theylooked for adjectives that often co-occur in coor-dinations with known sentiment-bearing adjectives,which were added to the lexicon after a manual fil-tering step.
The current version of Klenner et al?sPolArt lexicon also contains other parts of speech,and a list of shifters and intensifiers that interact withsubjective terms.The GermanPolarityClues lexicon of Waltinger(2010a) combines translation from English lexiconswith a semi-automatic approach for merging andmanually correcting lexicon entries.The SentiWS lexicon (Remus et al, 2010) con-tains translations of the English General Inquirer(Stone et al, 1966), which have been translated viaGoogle Translate, as well as a small number of termsthat were mined from positive and negative productreviews, expanded using a collocation dictionary.Finally, the SentiMerge lexicon (Emerson andDeclerck, 2014) has been constructed as a Bayesiancombination (i.e., averaging with imputation formissing entries) of the three resources abovetogether with the German SentiSpin resourceof Waltinger (2010b), which contains automatic(dictionary-based) translations of the SentiSpin lex-icon of Tamura et al (2005).We tested all lexicons using two approaches: Inthe vote-only approach, the sentiment of a phrase isdetermined by the sum of the scores of the wordsin that phrase as they are assigned in the sentimentlexicon.
In the vote-and-flip approach, we con-sider the average of the sentiment terms, but in-vert the sentiment value whenever a term from theshifter category of Klenner et al?s lexicon is foundwithin the yield of the node.
A similar strategy wasused in many papers on sentiment composition, usu-ally with a performance rather close to the best sys-tem (see e.g.
the CompoMC baseline in Choi andCardie, 2008, or the Vote-and-Flip baseline in Choiand Cardie, 2009).5.2 Near-domain lexicon constructionWhile the filmrezensionen web site offers a goodnumber of reviews, the final collection is rathersmall.
To complement our small in-domaindataset we use the most common way of get-ting text with document-level annotations, namelycustomer-written reviews from the movies section ofamazon.de web site.Perhaps expectedly, customer reviews do not fo-cus exclusively on the film and its performance.Rather, it often occurs that customer reviews includea discussion of the physical (or other) medium thatthe film came on:3(1) I am with Lovefilm (now Prime) and tried tostream the series.
Terrible!
Always [issueswith] loading time and loss of the stream.
Itseems that Amazon hasn?t come to terms withthe technology yet.Other reviews on Amazon match our domain fairlywell, as in the following:(2) If this is truly a sequel to ?Speed?, it onlyshows in the second hour of the film.
It?sonly then that deBont shows why he would bean action [film] specialist.
Admittedly, eventhen we don?t get the same tension as in thepredecessor, but in any case it?s better thanthe first hour of the film.While we found that a small quantity of data (20+20hand-classified sentences) together with a 300-classLDA representation was sufficient to reach 100% ac-curacy in separating content-related versus media-related text, we found that filtering out the irrelevanttexts made no difference for the mean square error,in sharp contrast to L1/Lasso regularization, whichallows to learn a sparse lexicon.6 Variants of the RNTN ModelWhile the RNTN model certainly performs well onthe full Stanford Sentiment Treebank, it is likely thatits performance on HeiST is suffering from sparsedata problems, and that both words and particularconstructions can be novel and unseen.In syntactic parsing, Koo et al (2008) and Canditoand Seddah (2010) have shown that using Brownclusters can be beneficial for alleviating sparse dataproblems in parsing.
In a similar vein, Popat et al(2013) have successfully applied crosslingual clus-tering to generalizing over potentially unseen words3German original text has been omitted for space reasons699System Node Acc.
Root Acc.supervised baselineRNTN, supervised 0.776 0.687RNTN + clustersnewspaper text+Brown 0.708 0.649movie reviews+Brown 0.780 0.677features+k-means 0.755 0.674RNTN + split movie-review clusterssplit SentiWS 0.774 0.676split GermanPolarityClues 0.807 0.689RNTN + lexicon-based replacementreplace-gold 0.844 0.730repl-GermanPolarityClues 0.789?
?0.681repl-SentiMerge[0.23] 0.780?0.648Table 5: Incorporating additional informationin (document-level) sentiment analysis for English,Hindi and Marathi.In our experiments, we follow Candito and Sed-dah (2010) in simply replacing words by clusters:in their experiments, even this simple procedure canyield an improvement, with improved results whenthe unlabeled data stems from the target domain.Since Brown clusters are mostly syntactic/semanticin nature and do not automatically distinguish pos-itive or negative sentiment, we additionally per-formed multiple experiments to use clusters whileincorporating additional sentiment information:On one hand, we try to incorporate the judge-ments on the Amazon near-domain dataset more di-rectly into the clusters by using the repeated bisect-ing K-Means algorithm as implemented in CLUTO(Zhao and Karypis, 2005), with previous/next word,part-of-speech tag, and the score of the containingreview as features.
On the other hand, we split theBrown clusters according to the sentiment value thatthey have in a particular sentiment lexicon (e.g.
Sen-tiMerge), yielding three clusters 01101+, 01101-and 01101?
instead of the original cluster 01101.As a final experiment, we consider replacing onlysentiment words by a concatenation of their part-of-speech and the sentiment class (turning ?a greatfilm?
into ?a JJ++ film?
), and leaving neutral wordsintact.
As an upper baseline for this approach,we can get words?
sentiment polarity directly fromtraining and testing data, which yields the replace-gold entry in table 5.rule type # in SSTb # in HeiSTAVG 119468 19228INV 2158 289INT 6614 646MWE 18235 1936Table 6: Rule types in SSTb and HeiST7 Results and Error analysisLooking at the results in tables 2, 4 and 5, we see thatsimple support vector machine classification is veryeffective for reproducing the positive/negative senti-ment of nodes and complete sentences, followed bycrosslingual projection and a simple averaging ap-proach; we also see that handling negation in thevote-and-flip approach seems to lower the score, justas the best model with word clusters and splitting(using the GermanPolarityClues lexicon) performsbetter than the word-based RNTN approach, but lesswell than the lexicon by itself.
Even the replace-gold upper baseline ?
replacing sentiment-carryingwords by their sentiment label, which raises the per-formance substantially ?
gives results below the sim-pler SVM approach, which is counterintuitive.7.1 Is it about Compositionality?One motivation for using sub-sentence structureboth in approaches for rule-based composition (as,e.g.
in Taboada et al (2011) and other lexicon-basedapproaches) as well as in more complex learningapproaches such as RNN (Socher et al, 2011) andRNTN (Socher et al, 2013) is the idea that such ap-proaches are able to model the interaction betweensentiment-bearing words and sentiment-modifyingwords.
An example for investigations based on thisassumption is the work by Zhu et al (2014), whocontrast different lexicon-based approaches for han-dling negation with an RNTN model of negation anda modification of said model.Given the results using a lexicon-based approachimplementing the vote-and-flip heuristic in compar-ison to the vote-only heuristic, we found it worth in-vestigating what specific types of interaction exist incompositional sentiment treebanks, also consideringthat Zhu et al?s investigations yielded a more precisepicture of the sentiment-shifting action of negatorsas a highly lexicalized phenomenon.700For our analysis, we grouped the production rulessp?
slsrin a sentiment treebank into one of thefollowing categories:AVG A production is said to be averaging if the par-ent category is within the range of either daugh-ter category.
(e.g.
mind-numbingly good wouldbe the composition of a negative term and apositive term to a positive term, which still fitsthe averaging heuristic).INV A production is said to be inverting if onedaughter category is neutral and the otherdaughter category is on the other side on thespectrum (e.g.
?not great?
landing on the nega-tive side)INT A production is said to be intensifying if theparent category is on the same side of the scaleas the daughters but more extreme.MWE A production is said to be a multi-word produc-tion if the daughter categories are classified asneutral while the parent category is not.4As can be seen in table 6, the number of invert-ing and intensifying productions is dwarfed, both forthe SST and for HeiST, by the number of multi-wordrules.
While it is likely that these counts are slightlydistorted by noise in the annotation (as both datasetsare the product of crowdsourcing), this fact is re-markable and merits further investigation.Types of multiword expressions If we try togroup the nodes with a ?multiword?
production, wecan distinguish at least the following categories:?
aspect descriptions: In some cases, an adjec-tive is specifically used to describe a (positiveor negative) aspect of the movie, such as anelaborate continuation, or an expanded vision,where individual words have a neutral senti-ment label (and conceivable could have beenused in a non-aspect-specific way to convey aneutral or negative sentiment, such as an elab-orate perversion, or an expanded nightshift).Similarly, wenig Handlung (not much action)4The MWE category also contains a small number ?
about5% of total MWE productions ?
of positive-to-neutral andnegative-to-neutral productions, which we found to be predom-inantly noise from the crowdsourcing process.has a negative meaning as a construction de-spite ?wenig?
(few/not much) not having a neg-ative meaning itself.?
expression strengthening is a phenomenonthat occurs when a term is judged as neutralby annotators by itself, but gains a sentimentvalue when paired with an intensifier or nega-tor.
For example, intrusive was labeled as neu-tral in SSTb, but simply intrusive as negative.?
comparatives are a very regular constructionwhere too much of something is almost alwaysbad: too long, too insistent, too much, too manyare all negative in SSTb, just as zu viel (toomany) and zu wenig (not enough) and othercounterparts in HeiST are negative.?
true constructions such as plot holes or histor-ically significant in SSTB, or ruhigen Gewis-sens (with a calm conscience) and Finger weg(don?t touch it) in HeiST are both a problemfor approaches relying purely on compositionand not regular enough that we would expect tomodel it as a regular construction.Some of the neutral-to-positive or neutral-to-negative transitions don?t seem well-motivated andmay be regarded as artifacts from the crowdsourc-ing, as does n?t, is n?t and are n?t are negative inSSTb whereas ?s not, do n?t and did n?t get a neutrallabel.
In HeiST, nicht immer (not always) as wellas nicht ganz (not quite) are negative, whereas auchnicht (neither) and nicht so (not as) or nicht unbed-ingt (not necessarily) are neutral.The MWE productions seem to overlap with well-known linguistic phenomena ?
consider Fahrni andKlenner (2008) and their claim that most adjec-tives have a polarity that is dependent on the tar-get they modify instead of having a ?prior?
polar-ity that holds independently of the target, or theobservation of Su and Markert (2009) that senti-ment should be dependent on word senses insteadof word forms (which would capture a large num-ber of examples within the expression strengtheningcategory).
Yet, others may be idiosyncracies intro-duced by the crowdsourcing process, and powerfullearners such as RNTN or the approach of Hall et al(2014) will gain performance from simply memo-rizing the idiosyncracies of the data when there is701SVM CLSA RNTN +replace-goldType Total Corr Prec Corr Prec Corr Prec Corr PrecAVG 6341 3408 0.546 3158 0.506 3604 0.577 4309 0.690MWE 1638 369 0.225 538 0.328 538 0.359 546 0.333INT 612 370 0.605 362 0.592 413 0.675 470 0.768ID 392 283 0.722 269 0.686 286 0.730 323 0.824INV 259 76 0.293 65 0.251 93 0.359 79 0.305Table 7: Precision of rules with non-neutral parent label (ID: daughters and parent have identical labels)enough of it ?
because of the way the Stanford Sen-timent Treebank is constructed, phrases always havethe same (context-independent) label, while we mayget a more accurate (and possibly different) picturefrom introducing additional means of quality control(which in turn increases the necessary investment forsuch a sentiment treebank).7.2 Contrasting SVM and RNTN behaviourIn table 7, we tabulated the classification accuracyfor the parent node in different types of productionsin HeiST.
In this evaluation, we counted a produc-tion as correct whenever the parent node has theright sentiment label (in parallel with the labeledrecall in syntactic evaluation), ignoring for the mo-ment the question whether the production producedby a system falls into the same category.
It is easyto see that AVG-type productions are the least error-prone for all classifiers, whereas MWE and INV pro-ductions pose a significant challenge for the models.We also see that on these challenging production, theRNTN performs better than the other methods.8 SummaryWe presented a novel dataset for subsentential sen-timent classification, which uses the same conven-tions as the Stanford Sentiment Treebank (SSTb),which is the only German resource of this typebesides the smaller (270 sentences) MLSA corpus(Clematide et al, 2012).
We performed a system-atic exploration into supervised, cross-lingual, andlexicon-based approaches on this dataset and foundthat, paradoxically, the performance of the state-of-the-art recursive neural tensor network (RNTN)models are severely impeded in this data-sparse situ-ation, unlike latent-variable models for syntax whichcan deal with such conditions quite well: Lavelli andCorazza (2009), for example, reports that the bestresults for parsing on the very small TUT treebank(slightly more than 2000 sentences) can be achievedusing a PCFG-LA model.We showed that a wide variety of models ?
fromlexicon-based sentiment prediction over SVM withunigram features to crosslingual classification ?
per-forms better than the RNTN, and that methods toimprove RNTN performance that work in other set-tings (syntax) do not offer any easy fix.In a second step, we took a closer look at thecrowdsourced data in order to explain certain coun-terintuitive results (such as the fact that most senti-ment lexicons do not benefit from negation handling,or that the upper baseline achievable with the RNTNby getting gold-standard information on positive andnegative words is at about the same level as our SVMclassifier), and found that SSTb-type resources showmarked differences from e.g., the MLSA dataset asthey incorporate multiword items, but seem to bechallenging for the study of compositionality dueto noise that is not present in expert-annotated re-sources.ReferencesBanea, C., Mihalcea, R., and Wiebe, J.
(2013).
Port-ing multilingual subjectivity resources across lan-guages.
IEEE Transactions on Affective Comput-ing, 2(4):211?225.Candito, M.-H. and Seddah, D. (2010).
Parsingword clusters.
In Proceedings of the First Work-shop on Statistical Parsing of Morphologically-Rich Languages (SPMRL 2010).Choi, Y. and Cardie, C. (2008).
Learning with com-positional semantics as structural inference forsubsentential sentiment analysis.
In Proceedingsof the 2008 Conference on Empirical Methods inNatural Language Processing (EMNLP 2008).702Choi, Y. and Cardie, C. (2009).
Adapting a polar-ity lexicon using integer linear programming fordomain-specific sentiment classification.
In Pro-ceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing.Clematide, S., Gindl, S., Klenner, M., Petrakis, S.,Remus, R., Ruppenhofer, J., Waltinger, U., andWiegand, M. (2012).
MLSA ?
a multi-layeredreference corpus for german sentiment analysis.In Proceedings of the Eigth International Con-ference on Language Resources and Evaluation(LREC 2012).Dong, L., Wei, F., Zhou, M., and Xu, K.(2014).
Adaptive multi-compositionality for re-cursive neural models with applications to senti-ment analysis.
In AAAI 2014.Duh, K., Fujino, A., and Nagata, M. (2011).
Ismachine translation ripe for cross-lingual senti-ment classification.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics:shortpapers.Emerson, G. and Declerck, T. (2014).
SentiMerge:Combining sentiment lexicons in a Bayesianframework.
In Proceedings of the Workshopon Lexical and Grammatical Resources for Lan-guage Processing.Fahrni, A. and Klenner, M. (2008).
Old wine orwarm beer: Target-specific sentiment analysis ofadjectives.
In Affective Language in Human andMachine (AISB 2008).Ganchev, K., Graca, J., and Taskar, B.
(2008).
Betteralignments = better translations?
In Procecedingsof the 46th Annual Meeting of the Association ofComputational Linguistics (ACL 2008).Hall, D., Durrett, G., and Klein, D. (2014).
Lessgrammar, more features.
In ACL 2014.Klenner, M., Petrakis, S., and Fahrni, A.
(2009).PolArt: A robust tool for sentiment analysis.
InNODALIDA 2009.Koo, T., Carreras, X., and Collins, M. (2008).
Sim-ple semi-supervised dependency parsing.
In ACL2008.Lavelli, A. and Corazza, A.
(2009).
The BerkeleyParser at the EVALITA constituency parsing task.In Proceedings of the Workshop on Evaluation ofNLP Tools for Italian (EVALITA 2009).Nakagawa, T., Inui, K., and Kurohashi, S. (2010).Dependency tree-based sentiment classificationusing crfs with hidden variables.
In Human Lan-guage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the ACL.Pang, B. and Lee, L. (2005).
Seeing stars: Exploit-ing class relationships for sentiment categoriza-tion with respect to rating scales.
In ACL 2005.Petrov, S., Barett, L., Thibaux, R., and Klein, D.(2006).
Learning accurate, compact, and inter-pretable tree annotation.
In COLING-ACL 2006.Popat, K., Balamurali, A. R., Bhattacharyya, P., andHaffari, G. (2013).
The haves and the have-nots:Leveraging unlabelled corpora for sentiment anal-ysis.
In Proceedings of ACL 2013.Rafferty, A. and Manning, C. D. (2008).
Parsingthree German treebanks: Lexicalized and unlexi-calized baselines.
In ACL?08 workshop on Pars-ing German.Remus, R., Quasthoff, U., and Heyer, G. (2010).SentiWS - a publicly-available German-languageresource for sentiment analysis.
In Proceedingsof the 7th International Language Resources andEvaluation Conference (LREC 2010).Samuelsson, Y. and Volk, M. (2004).
Automaticnode insertion for treebank deepening.
In Pro-ceedings of the 3rd Workshop on Treebanks andLinguistic Theories (TLT 2004).Socher, R., Hurval, B., Manning, C. D., and Ng,A.
Y.
(2012).
Semantic compositionality throughrecursive matrix-vector spaces.
In EMNLP 2012.Socher, R., Pennington, J., Huang, E. H., Ng, A. Y.,and Manning, C. D. (2011).
Semi-supervised re-cursive autoencoders for predicting sentiment dis-tributions.
In EMNLP 2011.Socher, R., Perelygin, A., Wu, J., Chuang, J., Man-ning, C., Ng, A., and Potts, C. (2013).
Recur-sive deep models for semantic compositionalityover a sentiment treebank.
In Conference on Em-pirical Methods in Natural Language Processing(EMNLP 2013).Stone, P. J., Dunphy, D. C., and Smith, M. S. (1966).703The General Inquirer: A Computer Approach toContent Analysis.
MIT Press.Su, F. and Markert, K. (2009).
Subjectivity recogni-tion on word senses via semi-supervised mincuts.In Proceedings of NAACL 2009.Taboada, M., Brooke, J., Tofilofski, M., Voll, K.,and Stede, M. (2011).
Lexicon-based methodsfor sentiment analysis.
Computational Linguis-tics, 37(2):267?307.Tamura, H., Inui, T., and Okumura, M. (2005).
Ex-tracting semantic orientation of words using spinmodel.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics (ACL 2005).Tiedemann, J.
(2010).
Lingua-align: An experimen-tal toolbox for automatic tree-to-tree alignment.In Proceedings of LREC 2010.Tiedemann, J.
(2012).
Parallel data, tools and inter-faces in OPUS.
In Proceedings of the 8th Inter-national Conference on Language Resources andEvaluation (LREC?2012).Volk, M., G?ohring, A., Marek, T., and Samuels-son, Y.
(2010).
SMULTRON (version3.0) ?
The Stockholm MULtilingual paral-lel TReebank.
http://www.cl.uzh.ch/research/parallelcorpora/paralleltreebanks_en.html.Waltinger, U.
(2010a).
GermanPolarityClues: Alexical resource for German sentiment analy-sis.
In Proceedings of the 7th InternationalLanguage Resources and Evaluation Conference(LREC 2010).Waltinger, U.
(2010b).
Sentiment analysis reloaded- an empirical study on machine learning.basedsentiment classification using polarity clues.
InProceedings of the 6th International Conferenceon Web Information Systems and Technologies(WEBIST 2010).Wan, X.
(2009).
Co-training for cross-lingual sen-timent classification.
In Proceedings of the 47thAnnual Meeting of the ACL and the 4th IJCNLPof the AFNLP (ACL-IJCNLP 2009).Wang, S. and Manning, C. D. (2012).
Baselines andbigrams: Simple, good sentiment and topic clas-sification.
In Proceedings of ACL 2012.Yessenalina, A. and Cardie, C. (2011).
Composi-tional matrix-space models for sentiment analy-sis.
In EMNLP 2011.Zhao, Y. and Karypis, G. (2005).
Hierarchical clus-tering algorithms for document datasets.
DataMining and Knowledge Discovery, 10:141?168.Zhu, X., Guo, H., Mohammad, S., and Kiritchenko,S.
(2014).
An empirical study on the effect ofnegation words on sentiment.
In ACL 2014.704
