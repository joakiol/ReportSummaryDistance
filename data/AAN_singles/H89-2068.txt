Establishing Performance Baselinesfor Text Understanding SystemsBeth SundheimNaval Ocean Systems CenterThe Naval Ocean Systems Center (NOSC) has been supporting DARPA's natural languageresearch program in the area of evaluation.
The objective of the work is to devise anevaluation plan that will satisfy the needs for (1) characterizing the current state of the artin theory- and implementation-independent ways and (2) setting baselines against whichprogress can be measured.A task-oriented evaluation of text understanding systems was prepared and conducted.
Ninedifferent NLP systems participated in the evaluation.
NOSC collected 150 texts to be used asdevelopment (i.e.
training) and test data and prepared explanatory documentation on them.The performance task--a simulated database update task--and the expected outputs for eachtext were defined.
A scoring system was devised and underwent considerable revision in thecourse of the evaluation.The evaluation was conducted in phases between March and June, 1989, concluding at NOSCwith the Second Message Understanding Conference (MUCK-II), at which each system wasrequired to participate in a small onsite test.
The results of the evaluation were encouraging.Some systems were able to fill a high proportion of the simulated database with very highaccuracy, indicating an ability to adapt to a new domain in a short time and carry out at least alimited real-life task.
A test report on the evaluation is being prepared, and tentative plansfor another evaluation are being made.452
