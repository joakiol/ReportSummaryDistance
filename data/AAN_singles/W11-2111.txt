Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108?115,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsE-rating Machine TranslationKristen Parton1 Joel Tetreault2 Nitin Madnani2 Martin Chodorow31Columbia University, NY, USAkristen@cs.columbia.edu2Educational Testing Service, Princeton, NJ, USA{jtetreault, nmadnani}@ets.org3Hunter College of CUNY, New York, NY, USAmartin.chodorow@hunter.cuny.eduAbstractWe describe our submissions to the WMT11shared MT evaluation task: MTeRater andMTeRater-Plus.
Both are machine-learnedmetrics that use features from e-rater R?, an au-tomated essay scoring engine designed to as-sess writing proficiency.
Despite using onlyfeatures from e-rater and without comparingto translations, MTeRater achieves a sentence-level correlation with human rankings equiva-lent to BLEU.
Since MTeRater only assessesfluency, we build a meta-metric, MTeRater-Plus, that incorporates adequacy by combin-ing MTeRater with other MT evaluation met-rics and heuristics.
This meta-metric has ahigher correlation with human rankings thaneither MTeRater or individual MT metricsalone.
However, we also find that e-rater fea-tures may not have significant impact on cor-relation in every case.1 IntroductionThe evaluation of machine translation (MT) systemshas received significant interest over the last decadeprimarily because of the concurrent rising interest instatistical machine translation.
The majority of re-search on evaluating translation quality has focusedon metrics that compare translation hypotheses to aset of human-authored reference translations.
How-ever, there has also been some work on methods thatare not dependent on human-authored translations.One subset of such methods is task-based in thatthe methods determine the quality of a translation interms of how well it serves the need of an extrin-sic task.
These tasks can either be downstream NLPtasks such as information extraction (Parton et al,2009) and information retrieval (Fujii et al, 2009) orhuman tasks such as answering questions on a read-ing comprehension test (Jones et al, 2007).Besides extrinsic evaluation, there is another setof methods that attempt to ?learn?
what makes agood translation and then predict the quality of newtranslations without comparing to reference trans-lations.
Corston-Oliver et al (2001) proposed theidea of building a decision tree classifier to sim-ply distinguish between machine and human transla-tions using language model (LM) and syntactic fea-tures.
Kulesza and Shieber (2004) attempt the sametask using an support vector machine (SVM) classi-fier and features derived from reference-based MTmetrics such as WER, PER, BLEU and NIST.
Theyalso claim that the confidence score for the classi-fier being used, if available, may be taken as an es-timate of translation quality.
Quirk (2004) took adifferent approach and examined whether it is pos-sible to explicitly compute a confidence measure foreach translated sentence by using features derivedfrom both the source and target language sides.
Al-brecht and Hwa (2007a) expanded on this idea andconducted a larger scale study to show the viabil-ity of regression as a sentence-level metric of MTquality.
They used features derived from severalother reference-driven MT metrics.
In other work(Albrecht and Hwa, 2007b), they showed that onecould substitute translations from other MT systemsfor human-authored reference translations and de-rive the regression features from them.Gamon et al (2005) build a classifier to distin-guish machine-generated translations from human108ones using fluency-based features and show that bycombining the scores of this classifier with LM per-plexities, they obtain an MT metric that has goodcorrelation with human judgments but not betterthan the baseline BLEU metric.The fundamental questions that inspired our pro-posed metrics are as follows:?
Can an operational English-proficiency mea-surement system, built with absolutely no fore-thought of using it for evaluation of translationquality, actually be used for this purpose??
Obviously, such a system can only assess thefluency of a translation hypothesis and not theadequacy.
Can the features derived from thissystem then be combined with metrics suchas BLEU, METEOR or TERp?measures ofadequacy?to yield a metric that performs bet-ter?The first metric we propose (MTeRater) is anSVM ranking model that uses features derived fromthe ETS e-rater R?
system to assess fluency of trans-lation hypotheses.
Our second metric (MTeRater-Plus) is a meta-metric that combines MTeRater fea-tures with metrics such as BLEU, METEOR andTERp as well as features inspired by other MT met-rics.Although our work is intimately related to someof the work cited above in that it is a trained regres-sion model predicting translation quality at the sen-tence level, there are two important differences:1.
We do not use any human translations ?
ref-erence or otherwise ?
for MTeRater, not evenwhen training the metric.
The classifier istrained using human judgments of translationquality provided as part of the shared evalua-tion task.2.
Most of the previous approaches use featuresets that are designed to capture both transla-tion adequacy and fluency.
However, MTeRateruses only fluency-based features.The next section provides some background onthe e-rater system.
Section 3 presents a discussionof the differences between MT errors and learner er-rors.
Section 4 describes how we use e-rater to buildour metrics.
Section 5 outlines our experiments andSection 5 discusses the results of these experiments.Finally, we conclude in Section 6.2 E-raterE-rater is a proprietary automated essay scoringsystem developed by Educational Testing Service(ETS) to assess writing quality.1 The system hasbeen used operationally for over 10 years in high-stakes exams such as the GRE and TOEFL givenits speed, reliability and high agreement with humanraters.E-rater combines 8 main features using linear re-gression to produce a numerical score for an es-say.
These features are grammar, usage, mechan-ics, style, organization, development, lexical com-plexity and vocabulary usage.
The grammar featurecovers errors such as sentence fragments, verb formerrors and pronoun errors (Chodorow and Leacock,2000).
The usage feature detects errors related toarticles (Han et al, 2006), prepositions (Tetreaultand Chodorow, 2008) and collocations (Futagi et al,2008).
The mechanics feature checks for spelling,punctuation and capitalization errors.
The style fea-ture checks for passive constructions and word rep-etition, among others.
Organization and develop-ment tabulate the presence or absence of discourseelements and the length of each element.
Finally,the lexical complexity feature details how complexthe writer?s words are based on frequency indicesand writing scales, and the vocabulary feature eval-uates how appropriate the words are for the giventopic).
Since many of the features are essay-specific,there is certainly some mismatch between what e-rater was intended for and the genres we are using itfor in this experiment (translated news articles).In our work, we separate e-rater features into twoclasses: sentence level and document level.
Thesentence level features consist of all errors markedby the various features for each sentence alone.
Incontrast, the document level features are an aggre-gation of the sentence level features for the entiredocument.1A detailed description of e-rater is outside the scope of thispaper and the reader is referred to (Attali and Burstein, 2006).1093 Learner Errors vs. MT ErrorsSince e-rater is trained on human-written text anddesigned to look for errors in usage that are com-mon to humans, one research question is whether itis even useful for assessing the fluency of machinetranslated text.
E-rater is unaware of the transla-tion context, so it does not look for common MTerrors, such as untranslated words, mistranslationsand deleted content words.
However, these may getflagged as other types of learner errors: spelling mis-takes, confused words, and sentence fragments.Machine translations do contain learner-like mis-takes in verb conjugations and word order.
In anerror analysis of SMT output, Vilar et al (2006) re-port that 9.9% - 11.7% of errors made by a Spanish-English SMT system were incorrect word forms, in-cluding incorrect tense, person or number.
Theseerror types are also account for roughly 14% of er-rors made by ESL (English as a Second Language)writers in the Cambridge Learner Corpus (Leacocket al, 2010).On the other hand, some learner mistakes are un-likely to be made by MT systems.
The Spanish-English SMT system made almost no mistakes inidioms (Vilar et al, 2006).
Idiomatic expressionsare strongly preferred by language models, but maybe difficult for learners to memorize (?kicked abucket?).
Preposition usage is a common problemin non-native English text, accounting for 29% oferrors made by intermediate to advanced ESL stu-dents (Bitchener et al, 2005) but language modelsare less likely to prefer local preposition errors e.g.,?he went to outside?.
On the other hand, a languagemodel will likely not prevent errors in prepositions(or in other error types) that rely on long-distancedependencies.4 E-rating Machine TranslationThe MTeRater metric uses only features from e-raterto score translations.
The features are produced di-rectly from the MT output, with no comparison toreference translations, unlike most MT evaluationmetrics (such as BLEU, TERp and METEOR).An obvious deficit of MTeRater is a measure ofadequacy, or how much meaning in the source sen-tence is expressed in the translation.
E-rater wasnot developed for assessing translations, and theMTeRater metric never compares the translation tothe source sentence.
To remedy this, we proposethe MTeRater-Plus meta-metric that uses e-rater fea-tures plus all of the hybrid features described below.Both metrics were trained on the same data usingthe same machine learning model, and differ only intheir feature sets.4.1 E-rater FeaturesEach sentence is associated with an e-rater sentence-level vector and a document-level vector as previ-ously described and each column in these vectorswas used a feature.4.2 Features for Hybrid ModelsWe used existing automatic MT metrics as baselinesin our evaluation, and also as features in our hybridmetric.
The metrics we used were:1.
BLEU (Papineni et al, 2002): Case-insensitiveand case-sensitive BLEU scores were pro-duced using mteval-v13a.pl, which calculatessmoothed sentence-level scores.2.
TERp (Snover et al, 2009): Translation EditRate plus (TERp) scores were produced usingterp v1.
The scores were case-insensitive andedit costs from Snover et al (2009) were usedto produce scores tuned for fluency and ade-quacy.3.
METEOR (Lavie and Denkowski, 2009): Me-teor scores were produced using Meteor-nextv1.2.
All types of matches were allowed (ex-act, stem, synonym and paraphrase) and scorestuned specifically to rank, HTER and adequacywere produced using the ?-t?
flag in the tool.We also implemented features closely related toor inspired by other MT metrics.
The set of theseauxiliary features is referred to as ?Aux?.1.
Character-level statistics: Based on the suc-cess of the i-letter-BLEU and i-letter-recallmetrics from WMT10 (Callison-Burch et al,2010), we added the harmonic mean of preci-sion (or recall) for character n-grams (from 1to 10) as features.1102.
Raw n-gram matches: We calculated the pre-cision and precision for word n-grams (up ton=6) and added each as a separate feature (fora total of 12).
Although these statistics are alsocalculated as part of the MT metrics above,breaking them into separate features gives themodel more information.3.
Length ratios: The ratio between the lengthsof the MT output and the reference translationwas calculated on a character level and a wordlevel.
These ratios were also calculated be-tween the MT output and the source sentence.4.
OOV heuristic: The percentage of tokens inthe MT that match the source sentence.
Thisis a low-precision heuristic for counting out ofvocabulary (OOV) words, since it also countsnamed entities and words that happen to be thesame in different languages.4.3 Ranking ModelFollowing (Duh, 2008), we represent sentence-levelMT evaluation as a ranking problem.
For a partic-ular source sentence, there are N machine transla-tions and one reference translation.
A feature vectoris extracted from each {source, reference, MT} tu-ple.
The training data consists of sets of translationsthat have been annotated with relative ranks.
Dur-ing training, all ranked sets are converted to sets offeature vectors, where the label for each feature vec-tor is the rank.
The ranking model is a linear SVMthat predicts a relative score for each feature vector,and is implemented by SVM-rank (Joachims, 2006).When the trained classifier is applied to a set of Ntranslations for a new source sentence, the transla-tions can then be ranked by sorting the SVM scores.5 ExperimentsAll experiments were run using data from threeyears of previous WMT shared tasks (WMT08,WMT09 and WMT10).
In these evaluations, anno-tators were asked to rank 3-5 translation hypothe-ses (with ties allowed), given a source sentence anda reference translation, although they were only re-quired to be fluent in the target language.Since e-rater was developed to rate English sen-tences only, we only evaluated tasks with Englishas the target language.
All years included sourcelanguages French, Spanish, German and Czech.WMT08 and WMT09 also included Hungarian andmultisource English.
The number of MT systemswas different for each language pair and year, fromas few as 2 systems (WMT08 Hungarian-English) toas many as 25 systems (WMT10 German-English).All years had a newswire testset, which was dividedinto stories.
WMT08 had testsets in two additionalgenres, which were not split into documents.All translations were pre-processed and runthrough e-rater.
Each document was treated as an es-say, although news articles are generally longer thanessays.
Testsets that were not already divided intodocuments were split into pseudo-documents of 20contiguous sentences or less.
Missing end of sen-tence markers were added so that e-rater would notmerge neighboring sentences.6 ResultsFor assessing our metrics prior to WMT11, wetrained on WMT08 and WMT09 and tested onWMT10.
The metrics we submitted to WMT11were trained on all three years.
One criticism ofmachine-learned evaluation metrics is that they maybe too closely tuned to a few MT systems, and thusnot generalize well as MT systems evolve or whenjudging new sets of systems.
In this experiment,WMT08 has 59 MT systems, WMT09 has 70 dif-ferent MT systems, and WMT10 has 75 differentsystems.
Different systems participate each year,and those that participate for multiple years oftenimprove from year to year.
By training and test-ing across years rather than within years, we hopeto avoid overfitting.To evaluate, we measure correlation between eachmetric and the human annotated rankings accordingto (Callison-Burch et al, 2010): Kendall?s tau is cal-culated for each language pair and the results areaveraged across language pairs.
This is preferableto averaging across all judgments because the num-ber of systems and the number of judgments varybased on the language pair (e.g., there were 7,911ranked pairs for 14 Spanish-English systems, and3,575 ranked pairs for 12 Czech-English systems).It is difficult to calculate the statistical signifi-cance of Kendall?s tau on these data.
Unlike the111Source language cz de es fr avgIndividual Metrics & BaselinesMTeRater .32 .31 .19 .23 .26bleu-case .26 .27 .28 .22 .26meteor-rank .33 .36 .33 .27 .32TERp-fluency .30 .36 .28 .28 .30Meta-Metric & BaselineBMT+Aux+MTeRater .38 .42 .37 .38 .39BMT .35 .40 .35 .34 .36Additional Meta-MetricsBMT+LM .36 .41 .36 .36 .37BMT+MTeRater .38 .42 .36 .38 .38BMT+Aux .38 .41 .38 .37 .39BMT+Aux+LM .39 .42 .38 .36 .39Table 1: Kendall?s tau correlation with human rankings.BMT includes bleu, meteor and TERp; Aux includes aux-iliary features.
BMT+Aux+MTeRater is MTeRater-Plus.Metrics MATR annotations (Przybocki et al, 2009),(Peterson and Przybocki, 2010), the WMT judg-ments do not give a full ranking over all systems forall judged sentences.
Furthermore, the 95% confi-dence intervals of Kendall?s tau are known to be verylarge (Carterette, 2009) ?
in Metrics MATR 2010,the top 7 metrics in the paired-preference single-reference into-English track were within the sameconfidence interval.To compare metrics, we use McNemar?s testof paired proportions (Siegel and Castellan, 1988)which is more powerful than tests of independentproportions, such as the chi-square test for indepen-dent samples.2 As in Kendall?s tau, each metric?srelative ranking of a translation pair is compared tothat of a human.
Two metrics, A and B, are com-pared by counting the number of times both A and Bagree with the human ranking, the number of timesA disagrees but B agrees, the number of times Aagrees but B disagrees, and the number of times bothA and B disagree.
These counts can be arranged ina 2 x 2 contingency table as shown below.A agrees A disagreesB agrees a bB disagrees c dMcNemar?s test determines if the cases of mis-match in agreement between the metrics (cells b andc) are symmetric or if there is a significant difference2See http://faculty.vassar.edu/lowry/propcorr.html for an ex-cellent description.in favor of one of the metrics showing more agree-ment with the human than the other.
The two-tailedprobability for McNemar?s test can be calculated us-ing the binomial distribution over cells b and c.6.1 Reference-Free Evaluation with MTeRaterThe first group of rows in Table 1 shows theKendall?s tau correlation with human rankings ofMTeRater and the best-performing version of thethree standard MT metrics.
Even though MTeR-ater is blind to the MT context and does not use thesource or references at all, MTeRater?s correlationwith human judgments is the same as case-sensitivebleu (bleu-case).
This indicates that a metric trainedto assess English proficiency in non-native speakersis applicable to machine translated text.6.2 Meta-MetricsThe second group in Table 1 shows the cor-relations of our second metric, MTeRater-Plus(BMT+Aux+MTeRater), and a baseline meta-metric(BMT) that combined BLEU, METEOR and TERp.MTeRater-Plus performs significantly better thanBMT, according to McNemar?s test.We also wanted to determine whether the e-rater features have any significant impact when usedas part of meta-metrics.
To this end, we firstcreated two variants of MTeRater-Plus: one thatremoved the MTeRater features (BMT+Aux) andanother that replaced the MTeRater features withthe LM likelihood and perplexity of the sentence(BMT+Aux+LM).3 Both models perform as wellas MTeRater-Plus, i.e., adding additional fluencyfeatures (either LM scores or MTeRater) to theBMT+Aux meta-metric has no significant impact.To determine whether this was generally the case,we also created two variants of the BMT baselinemeta-metric that added fluency features to it: one inthe form of LM scores (BMT+LM) and another inthe form of the MTeRater score (BMT+MTeRater).Based on McNemar?s test, both models are sig-nificantly better than BMT, indicating that thesereference-free fluency features indeed capture an as-pect of translation quality that is absent from thestandard MT metrics.
However, there is no signfi-cant difference between the two variants of BMT.3The LM was trained on English Gigaword 3.0, and wasprovided by WMT10 organizers.1121) Ref: Gordon Brown has discovered yet another hole to fall into; his way out of it remains the sameMT+: Gordon Brown discovered a new hole in which to sink; even if it resigned, the position would not change.Errors: None markedMT-: Gordon Brown has discovered a new hole in which could, Even if it demissionnait, the situation does not change not.Errors: Double negative, spelling, preposition2) Ref: Jancura announced this in the Twenty Minutes programme on Radiozurnal.MT+: Jancura said in twenty minutes Radiozurnal.
Errors: SpellingMT-: He said that in twenty minutes.
Errors: none markedTable 2: Translation pairs ranked correctly by MTeRater but not bleu-case (1) and vice versa (2).6.3 DiscussionTable 2 shows two pairs of ranked translations (MT+is better than MT-), along with some of the errors de-tected by e-rater.
In pair 1, the lower-ranked trans-lation has major problems in fluency as detected bye-rater, but due to n-gram overlap with the reference,bleu-case ranks it higher.
In pair 2, MT- is morefluent but missing two named entities and bleu-casecorrectly ranks it lower.One disadvantage of machine-learned metrics isthat it is not always clear which features caused onetranslation to be ranked higher than another.
Wedid a feature ablation study for MTeRater whichshowed that document-level collocation features sig-nificantly improve the metric, as do features forsentence-level preposition errors.
Discourse-levelfeatures were harmful to MT evaluation.
This is un-surprising, since MT sentences are judged one at atime, so any discourse context is lost.Overall, a metric with only document-level fea-tures does better than one with only sentence-levelfeatures due to data sparsity ?
many sentences haveno errors, and we conjecture that the document-levelfeatures are a proxy for the quality of the MT sys-tem.
Combining both document-level and sentence-level e-rater features does significantly better thaneither alone.
Incorporating document-level featuresinto sentence-level evaluation had one unforeseeneffect: two identical translations can get differentscores depending on how the rest of the documentis translated.
While using features that indicate therelative quality of MT systems can improve overallcorrelation, it fails when the sentence-level signal isnot strong enough to overcome the prior belief.7 ConclusionWe described our submissions to the WMT11 sharedevaluation task: MTeRater and MTeRater-Plus.MTeRater is a fluency-based metric that uses fea-tures from ETS?s operational English-proficiencymeasurement system (e-rater) to predict the qual-ity of any translated sentence.
MTeRater-Plus is ameta-metric that combines MTeRater?s fluency-onlyfeatures with standard MT evaluation metrics andheuristics.
Both metrics are machine-learned mod-els trained to rank new translations based on existinghuman judgments of translation.Our experiments showed that MTeRater, by it-self, achieves a sentence-level correlation as high asBLEU, despite not using reference translations.
Inaddition, the meta-metric MTeRater-Plus achieveshigher correlations than MTeRater, BLEU, ME-TEOR, TERp as well as a baseline meta-metric com-bining BLEU, METEOR and TERp (BMT).
How-ever, further analysis showed that the MTeRatercomponent of MTeRater-Plus does not contributesignificantly to this improved correlation.
How-ever, when added to the BMT baseline meta-metric,MTeRater does make a significant contribution.Our results, despite being a mixed bag, clearlyshow that a system trained to assess English-language proficiency can be useful in providing anindication of translation fluency even outside of thespecific WMT11 evaluation task.
We hope that thiswork will spur further cross-pollination between thefields of MT evaluation and grammatical error de-tection.
For example, we would like to explore usingMTeRater for confidence estimation in cases wherereference translations are unavailable, such as task-oriented MT.AcknowledgmentsThe authors wish to thank Slava Andreyev at ETSfor his help in running e-rater.
This research wassupported by an NSF Graduate Research Fellowshipfor the first author.113ReferencesJoshua Albrecht and Rebecca Hwa.
2007a.
A Re-examination of Machine Learning Approaches forSentence-Level MT Evaluation.
In Proceedings ofACL.Joshua Albrecht and Rebecca Hwa.
2007b.
Regressionfor Sentence-Level MT Evaluation with Pseudo Refer-ences.
In Proceedings of ACL.Yigal Attali and Jill Burstein.
2006.
Automated es-say scoring with e-rater v.2.0.
Journal of Technology,Learning, and Assessment, 4(3).John Bitchener, Stuart Young, and Denise Cameron.2005.
The effect of different types of corrective feed-back on esl student writing.
Journal of Second Lan-guage Writing.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar F. Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,WMT ?10, pages 17?53, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Ben Carterette.
2009.
On rank correlation and thedistance between rankings.
In Proceedings of the32nd international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?09, pages 436?443, New York, NY, USA.
ACM.Martin Chodorow and Claudia Leacock.
2000.
An unsu-pervised method for detecting grammatical errors.
InProceedings of the Conference of the North AmericanChapter of the Association of Computational Linguis-tics (NAACL), pages 140?147.Simon Corston-Oliver, Michael Gamon, and ChrisBrockett.
2001.
A Machine Learning Approach tothe Automatic Evaluation of Machine Translation.
InProceedings of the 39th Annual Meeting on Associa-tion for Computational Linguistics, pages 148?155.Kevin Duh.
2008.
Ranking vs. regression in machinetranslation evaluation.
In Proceedings of the ThirdWorkshop on Statistical Machine Translation, StatMT?08, pages 191?194, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2009.
Evaluating Effects of Ma-chine Translation Accuracy on Cross-lingual PatentRetrieval.
In Proceedings of SIGIR, pages 674?675.Yoko Futagi, Paul Deane, Martin Chodorow, and JoelTetreault.
2008.
A computational approach to de-tecting collocation errors in the writing of non-nativespeakers of English.
Computer Assisted LanguageLearning, 21:353?367.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-level MT Evaluation Without Refer-ence Translations: Beyond Language Modeling.
InProceedings of the European Association for MachineTranslation (EAMT).Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineering,12(2):115?129.Thorsten Joachims.
2006.
Training linear SVMs in lineartime.
In ACM SIGKDD International Conference OnKnowledge Discovery and Data Mining (KDD), pages217?226.Douglas Jones, Martha Herzog, Hussny Ibrahim, ArvindJairam, Wade Shen, Edward Gibson, and MichaelEmonts.
2007.
ILR-Based MT Comprehension Testwith Multi-Level Questions.
In HLT-NAACL (ShortPapers), pages 77?80.Alex Kulesza and Stuart M. Shieber.
2004.
A Learn-ing Approach to Improving Sentence-level MT Evalu-ation.
In Proceedings of the 10th International Con-ference on Theoretical and Methodological Issues inMachine Translation (TMI).Alon Lavie and Michael J. Denkowski.
2009.
The me-teor metric for automatic evaluation of machine trans-lation.
Machine Translation, 23:105?115, September.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
Morgan &Claypool Publishers.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Kristen Parton, Kathleen R. McKeown, Bob Coyne,Mona T. Diab, Ralph Grishman, Dilek Hakkani-Tu?r,Mary Harper, Heng Ji, Wei Yun Ma, Adam Meyers,Sara Stolbach, Ang Sun, Gokhan Tur, Wei Xu, andSibel Yaman.
2009. Who, What, When, Where, Why?Comparing Multiple Approaches to the Cross-Lingual5W Task.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP, pages 423?431.Kay Peterson and Mark Przybocki.
2010.
Nist2010 metrics for machine translation evalua-tion (metricsmatr10) official release of results.http://www.itl.nist.gov/iad/mig/tests/metricsmatr/2010/results.114Mark Przybocki, Kay Peterson, Se?bastien Bronsart, andGregory Sanders.
2009.
The nist 2008 metrics for ma-chine translation challenge?overview, methodology,metrics, and results.
Machine Translation, 23:71?103,September.Christopher Quirk.
2004.
Training a Sentence-level Ma-chine Translation Confidence Measure.
In Proceed-ings of LREC.Sidney Siegel and N. John Castellan.
1988.
Nonpara-metric statistics for the behavioral sciences.
McGraw-Hill, 2 edition.Matthew Snover, Nitin Madnani, Bonnie J. Dorr, andRichard Schwartz.
2009.
Fluency, adequacy, orhter?
: exploring different human judgments with a tun-able mt metric.
In Proceedings of the Fourth Work-shop on Statistical Machine Translation, StatMT ?09,pages 259?268, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Joel Tetreault and Martin Chodorow.
2008.
The ups anddowns of preposition error detection in ESL writing.In Proceedings of the 22nd International Conferenceon Computational Linguistics (COLING), pages 865?872.David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error analysis of machine transla-tion output.
In International Conference on LanguageResources and Evaluation, pages 697?702, Genoa,Italy, May.115
