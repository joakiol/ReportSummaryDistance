Collaborative Response Generation inPlanning DialoguesJennifer Chu-Carroll*Bell Laboratories, Lucent TechnologiesSandra Carberry tUniversity of DelawareIn collaborative planning dialogues, the agents have different beliefs about the domain and abouteach other; thus, it is inevitable that conflicts arise during the planning process.
In this pa-per, we present aplan-based model for response generation during collaborative planning, basedon a recursive Propose-Evaluate-Modify framework for modeling collaboration.
We focus onidentifying strategies for content selection when 1) the system initiates information-sharingto gather further information in order to make an informed decision about whether to accepta proposal from the user, and 2) the system initiates collaborative negotiation to negotiatewith the user to resolve a detected conflict in the user's proposal.
When our model determines thatinformation-sharing should be pursued, it selects a focus o f in formation-sharing from among mul-tiple uncertainties that might be addressed, chooses an appropriate information-sharing strategy,and formulates a response that initiates an information-sharing subdialogue.
When our modeldetermines that conflicts must be resolved, it selects the most effective conflicts to address in re-solving disagreement about he user's proposal, identifies appropriate justification for the system'sclaims, and formulates a response that initiates a negotiation subdialogue.1.
IntroductionIn task-oriented collaborative planning dialogues, two agents work together to developa plan for achieving their shared goal.
Such a goal may be for one agent to obtain aBachelor's degree in Computer Science or for both agents to go to a mutually desirablemovie.
Since the two agents each have private beliefs about the domain and aboutone another, it is inevitable that conflicts will arise between them during the planningprocess.
In order for the agents to effectively collaborate with one another, each agentmust attempt o detect such conflicts as soon as they arise, and to resolve them in anefficient manner so that the agents can continue with their task.Our analysis of naturally occurring collaborative planning dialogues hows thatagents initiate two types of subdialogues for the purpose of resolving (potential) con-flicts between the agents.
First, an agent may initiate information-sharing subdia-logues when she does not have sufficient information to determine whether to acceptor reject a proposal made by the other agent.
The purpose of such information-sharingsubdialogues i for the two agents to share their knowledge regarding the proposalso that each agent can then knowledgeably reevaluate the proposal and come to aninformed decision about its acceptance.
Second, an agent may initiate collaborativenegotiation subdialogues when she detects a conflict between the agents with respectto a proposal.
The purpose of such collaborative negotiation subdialogues i for the* 600 Mountain Avenue, Murray Hill, NJ 07974, U.S.A. E-mail: jencc@bell-labs.comt Department of Computer and Information Sciences, Newark, DE 19716, U.S.A. E-marl:carberry@cis.udel.edu(~) 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 3two agents to resolve the detected conflict and agree on accepting the original pro-posal or perhaps ome modification of it.
For example, consider the following dialoguesegment between a travel agent (T) and a customer (C) who is making reservationsfor two other agents(1) T: Can we put them on American?
(2) C: Why?
(3) T: We're having a lot of problems on the USAir seat maps so we maynot be able to get the seats they want.
(4) But American whatever we request pretty much we get.
(5) C: I don't know if they care about seats.
(6) Let's go with USAir.
(7) T: Are you sure they won't mind if they don't get seats next to each other?
(8) C: I don't think they would care.
(9) The USAir flight was recommended by the manager, so I think weshould stick with it.
(10) T: Okay.This dialogue segment illustrates how an agent may initiate an information-sharingsubdialogue (utterances (2)-(4)) or a collaborative negotiation subdialogue (utterances(5)-(10)) to resolve (potential) disagreements between the agents.
In utterance (2), Cemploys the Ask-Why strategy, one of four information-sharing strategies that weidentified based on our analysis of collaborative planning dialogues, to gather infor-mation from T in order to reevaluate T's proposal in (1).
When taking into accountthe information obtained in utterances (3) and (4), however, C's reevaluation of theproposal results in her rejecting the proposal, i.e., C detects a conflict with T regard-ing which airline they should book on.
Thus, in utterances (5) and (6), C initiates acollaborative negotiation subdialogue in an attempt to convince T that they should gowith USAir.
This negotiation subdialogue ventually leads to T accepting C's plan in(10).One very important aspect of natural anguage generation is identification of ap-propriate content during response generation.
Although negotiation and conflict reso-lution are an integral part of collaborative activity, previous research as not providedmechanisms that enable a system to effectively participate in dialogues uch as theabove.
This paper presents our strategies and algorithms for initiating and generatingresponses in information-sharing and negotiation subdialogues.
As will be noted in'Section 4, we view each utterance as making a proposal with respect o actions orbeliefs that should be adopted.
In this paper, we discuss proposals for beliefs andfocus on situations where there are (potential) conflicts between the system and the356Chu-Carroll and Carberry Response Generation i Planning Dialoguesuser regarding their beliefs about he domain.
The paper addresses the following mainissues: 1) the use of a recursive Propose-Evaluate-Modify c cle for modeling collabo-rative activity, 2) initiation of information-sharing subdialogues in situations where thesystem's existing knowledge is not sufficient to make an informed ecision about theacceptance of a user proposal, 3) the process for selecting an appropriate information-sharing strategy based on the system's private knowledge about he domain and aboutthe user, 4) initiation of collaborative negotiation subdialogues when a detected con-flict is relevant to the task at hand, 5) the process for selecting the aspect o addressduring conflict resolution when multiple conflicts arise, and 6) the process for selectingappropriate vidence to justify the system's claims.
Our implemented system, CORE(COnflict REsolver), produces responses in a university course advisement domain,where the system plays the role of an advisor who is helping a student develop aplan to achieve her domain goal.
1 The system is mutually presumed to have greaterexpertise in some aspects of the domain (for example, the system is presumed to be anauthority on requirements for degrees but to have less certain knowledge about otheraspects uch as individual professor's sabbatical plans), while the user is assumed tobe more knowledgeable about his particular likes and dislikes.2.
Related Work2.1 Modeling CollaborationAllen (1991) proposed a discourse model that differentiates among the shared andindividual beliefs that agents might hold during collaboration.
His model consists ofsix plan modalities, organized hierarchically with inheritance in order to accommo-date the different states of beliefs during collaboration.
The plan modalities includeplan fragments that are private to an agent, those proposed by an agent but not yetacknowledged by the other, those proposed by an agent and acknowledged but notyet accepted by the other agent, and a shared plan between the two agents.
Plan frag-ments move from the lower-level modalities (private plans) to the top-level sharedplans if appropriate acknowledgment/acceptance is given.
Although Allen's frame-work provides a good basis for representing the state of collaborative planning, itdoes not specify how the collaborative planning process hould be carried out andhow responses should be generated when disagreements arise in such planning dia-logues.Grosz and Sidner (1990) developed a formal model that specifies the beliefs andintentions that must be held by collaborative agents in order for them to constructa shared plan.
Their model, dubbed the SharedPlan model eliminates the "master-slave assumption" typically made by plan recognition work prior to their effort.
Thus,instead of treating collaborative planning as having one controlling agent and onereactive agent where the former has absolute control over the formation of the planand the latter is involved only in the execution of the plan, they view collaborativeplanning as "two agents develop\[ing\] a plan together rather than merely execut\[ing\]the existing plan of one of them" (page 427).
Lochbaum (1994) developed an algorithmfor modeling discourse using this SharedPlan model and showed how information-seeking dialogues could be modeled in terms of attempts to satisfy knowledge pre-1 Although the examples that illustrate CORE's response generation process in this paper are all takenfrom the university course advisement domain, the strategies that we identified can easily be appliedto other collaborative planning domains.
For examples of how the system can be applied to thefinancial advisement and library information retrieval domains, see Section 8.1, and to the air trafficcontrol domain, see Chu-Carroll and Carberry (1996).357Computational Linguistics Volume 24, Number 3conditions (Lochbaum 1995).
Grosz and Kraus (1996) extended the SharedPlan modelto handle actions involving groups of agents and complex actions that decompose intomultiagent actions.
They proposed a formalism for representing collaborative agents'SharedPlans using three sources of information: 1) the agents' intention to do someactions, 2) their intentions that other agents will carry out some actions, and 3) theirintention that the joint activity will be successful.
However, in their model the agentswill avoid adopting conflicting intentions, instead of trying to resolve them.Sidner analyzed multiagent collaborative planning discourse and formulated anartificial language for modeling such discourse using proposal/acceptance and pro-posal/rejection sequences (Sidner 1992, 1994).
In other words, a multiagent collabora-tive planning process is represented in her language as one agent making a proposal(of a certain action or belief) to the other agents, and the other agents either acceptingor rejecting this proposal.
Each action (such as Propose or Accept) is represented bya message sent from one agent o another, which corresponds to the natural anguageutterances in collaborative planning discourse.
Associated with each message is a set ofactions that modifies the stack of open beliefs, rejected beliefs, individual beliefs, andmutual beliefs, that facilitate the process of belief revision.
However, it was not Sidner'sintention to specify conflict detection and resolution strategies for agents involved incollaborative interactions.
Our Propose-Evaluate-Modify framework, to be discussedin Section 3.2, builds on this notion of proposal/acceptance and proposal/rejectionsequences during collaborative planning.Walker (1996b) also developed a model of collaborative planning in which agentspropose options, deliberate on proposals that have been made, and either accept orreject proposals.
Walker argues against what she terms the redundancy constraint indiscourse (the constraint hat redundant information should be omitted).
She notesthat this constraint erroneously assumes that a hearer will automatically accept claimsthat are presented to him, and would cause the speaker to believe that it is unnec-essary to present evidence that the hearer already knows or should be able to infer(even though this evidence may not currently be part of his attentional focus).
Walkerinvestigated the efficiency of different communicative strategies, particularly the useof informationally redundant utterances (IRU's), under different assumptions aboutresource limits and processing costs, and her work suggests that effective use of IRU'scan reduce effort during collaborative planning and negotiation.Heeman and Hirst (1995) investigated collaboration on referring expressions ofobjects copresent with the dialogue participants.
They viewed the processes of build-ing referring expressions and identifying their referents as a collaborative activity, andmodeled them in a plan-based paradigm.
Their model allows for negotiation in se-lecting amongst multiple candidate referents; however, such negotiation is restrictedto the disambiguation process, instead of a negotiation process in which agents try toresolve conflicting beliefs.Edmonds (1994) studied an aspect of collaboration similar to that studied by Hee-man and Hirst.
However, he was concerned with collaborating on references to ob-jects that are not mutually known to the dialogue participants (such as references tolandmarks in direction-giving dialogues).
Again, Edmonds captures referent identifi-cation as a collaborative process and models it within the planning/plan recognitionparadigms.
However, he focuses on situations in which an agent's first attempt at de-scribing a referent is considered insufficient by the recipient and the agents collaborateon expanding the description to provide further information, and does not considercases in which conflicts arise between the agents during this process.Traum (1994) analyzed collaborative task-oriented dialogues and developed a the-ory of conversational cts that models conversation using actions at four different358Chu-Carroll and Carberry Response Generation i Planning Dialogueslevels: turn-taking acts, grounding acts, core speech acts, and argumentation acts.However, his work focuses on the recognition of such actions, in particular ground-ing acts, and utilizes a simple dialogue management model to determine appropriateacknowledgments from the system.2.2 Cooperative Response GenerationMany researchers (McKeown, Wish, and Matthews 1985; Paris 1988; McCoy 1988;Sarner and Carberry 1990; Zukerman and McConachy 1993; Logan et al 1994) haveargued that information from the user model should affect a generation system's de-cision on what to say and how to say it.
One user model attribute with such an effectis the user's domain knowledge, which Paris (1988) argues not only influences theamount of information given (based on Grice's Maxim of Quantity \[Grice 1975\]), butalso the kind of information provided.
McCoy(1988) uses the system's model of theuser's domain knowledge to determine possible reasons for a detected misconceptionand to provide appropriate explanations to correct he misconception.
Cawsey (1990)also uses a model of user domain knowledge to determine whether or not a userknows a concept in her tutorial system, and thereby determine whether further expla-nation is required.
Sarner and Carberry (1990) take into account he user's possibleplans and goals to help the system determine the user's perspective and provide defi-nitions suitable to the user's needs.
McKeown, Wish, and Matthews (1985) inferred theuser's goal from her utterances and tailored the system's response to that particularviewpoint.
In addition, Zukerman and McConachy (1993) took into account a user'spossible inferences in generating concise discourse.Logan et al, in developing their automated librarian (Cawsey et al 1993; Loganet al 1994), introduced the idea of utilizing a belief revision mechanism (Galliers 1992)to predict whether a given set of evidence is sufficient o change a user's existing be-lief.
They argued that in the information retrieval dialogues they analyzed, "in nocases does negotiation extend beyond the initial belief conflict and its immediate res-olution" (Logan et al 1994, 141); thus they do not provide a mechanism for extendedcollaborative negotiation.
On the other hand, our analysis of naturally occurring col-laborative negotiation dialogues hows that conflict resolution does extend beyonda single exchange of conflicting beliefs; therefore we employ a recursive Propose-Evaluate-Modify ramework that allows for extended negotiation.
Furthermore, theirsystem deals with one conflict at a time, while our model is capable of selecting afocus in its pursuit of conflict resolution when multiple conflicts arise.Moore and Paris (1993) developed a text planner that captures both intentional ndrhetorical information.
Since their system includes a Persuade operator for convincinga user to perform an action, it does not assume that the hearer would perform arecommended action without additional motivation.
However, although they providea mechanism for responding to requests for further information, they do not identifystrategies for negotiating with the user if the user expresses conflict with the system'srecommendation.Raskutti and Zukerman (1994) developed a system that generates disambiguatingand information-seeking queries during collaborative planning activities.
In situationswhere their system infers more than one plausible goal from the user's utterances, itgenerates disambiguating queries to identify the user's intended goal.
In cases wherea single goal is recognized, but contains insufficient details for the system to constructa plan to achieve this goal, their system generates information-seeking queries to elicitadditional information from the user in order to further constrain the user's goal.Thus, their system focuses on cooperative response generation in scenarios wherethe user does not provide sufficient information in his proposal to allow the agents359Computational Linguistics Volume 24, Number 3to immediately adopt his proposed actions.
On the other hand, our system focuseson collaborative response generation in situations where insufficient information isavailable to determine the acceptance of an unambiguously recognized proposal andthose where a conflict is detected between the agents with respect o the proposal.
-3.
Modeling Collaborative Planning Dialogues3.1 Corpus AnalysisIn order to develop a response generation model that is capable of generating naturaland appropriate responses when (potential) conflicts arise, the first author analyzedsample dialogues from three corpora of collaborative planning dialogues to examinehuman behavior in such situations.
These dialogues are: the TRAINS 91 dialogues(Gross, Allen, and Traum 1993), a set of air travel reservation dialogues (SRI Tran-scripts 1992), and a set of collaborative negotiation dialogues on movie selections(Udel Transcripts 1995).The dialogues were analyzed based on Sidner's model, which captures collabo-rative planning dialogues as proposal/acceptance and proposal/rejection sequences(Sidner 1992, 1994).
Emphasis was given to situations where a proposal was not im-mediately accepted, indicating a potential conflict between the agents.
In our analysis,all cases involving lack of acceptance fall into one of two categories: 1) rejection,where one agent rejects a proposal made by the other agent, and 2) uncertainty inacceptance, where one agent cannot decide whether or not to accept he other agent'sproposal.
The former is indicated when an agent explicitly conveys rejection of a pro-posal and / or provides evidence that implies such rejection, while the latter is indicatedwhen an agent solicits further information (usually in the form of a question) to helpiher decide whether to accept he proposal.
2 Walker (1996a) analyzed a corpus of fi-nancial planning dialogues for utterances that conveyed acceptance or rejection.
Whileour rejection category is subsumed by her rejections, some of what she classifies asrejections would fall into our uncertainty in acceptance category since the speaker'sutterance indicates doubt but not complete rejection.
For example, one of the utter-ances that Walker treats as a rejection is "A: Well I thought hey just started this year,"in response to B's proposal that A should have been eligible for an IRA last year.Since A's utterance conveys uncertainty about whether IRA's were started this year,it indirectly conveys uncertainty about whether A was eligible for an IRA last year.Thus, we classify this utterance as uncertainty in acceptance.Our analysis confirmed both Sidner's and Walker's observations that collaborativeplanning dialogues can be modeled as proposal/acceptance and proposal/rejectionsequences.
However, we further observed that in the vast majority of cases wherea proposal is rejected, the proposal is not discarded in its entirety, but is modifiedto a form that will potentially be accepted by both agents.
This tendency towardmodification is summarized in Table 1 and is illustrated by the following example(the utterance that suggests modification of the original proposal is in boldface): 32 In the vast majority of cases where there is lack of acceptance of a proposal the agent's response to theproposal clearly indicates either a rejection or an uncertainty in acceptance.
In cases where there is noexplicit indication, the perceived strength of belief conveyed by the agent's response as well as thesubsequent dialogue were used to decide between rejection and uncertainty in acceptance.3 We consider a proposal modified if subsequent dialogue pursues the same subgoal that the rejectedproposal is intended to address and takes into account he constraints previously discussed (such asthe source and destination cities and approximate departure time, in the sample dialogue).360Chu-Carroll and Carberry Response Generation i  Planning DialoguesTable 1Summary of corpus analysis.# Turns Rejection of Proposal Uncertainty in AcceptanceExpress-Modified Discarded Invite-Attack Ask-Why Both UncertaintySRI 1,899 39 2 5 1 0 0TRAINS 1,000 44 1 3 0 0 0UDEL 478 45 2 7 6 1 6Total 3,377 128 5 15 7 1 6Proposal Modification Example (SRI Transcripts 1992)C: Delta has a four thirty arriving eight fifty five.T: That one's sold out.C: That's sold out?T: Completely sold out.
Now there's a Delta four ten connects with Dallasarrives eight forty.We will use the term collaborative negotiation (Sidner 1994) to refer to the kindsof negotiation reflected in our transcripts, in which each agent is driven by the goal ofdevising a plan that satisfies the interests of the agents as a group, instead of one thatmaximizes their own individual interests.
Further analysis hows that a couple of fea-tures distinguish collaborative negotiation from argumentation a d noncollaborativenegotiation (Chu-Carroll and Carberry 1995c).
First, an agent engaging in collabora-tive negotiation does not insist on winning an argument, and will not argue for thesake of arguing; thus she may change her beliefs if another agent presents convincingjustification for an opposing belief.
This feature differentiates collaborative negotiationfrom argumentation (Birnbaum, Flowers, and McGuire 1980; Reichman 1981; Flowersand Dyer 1984; Cohen 1987; Quilici 1992).
Second, agents involved in collaborativenegotiation are open and honest with one another; they will not deliberately presentfalse information to the other agents, present information i  such a way as to misleadthe other agents, or strategically hold back information from other agents for later use.This feature distinguishes collaborative negotiation from noncollaborative n gotiationsuch as labor negotiation (Sycara 1989).As shown in Table 1, our corpus analysis also found 29 cases in which an agenteither explicitly or implicitly indicated uncertainty about whether to accept or rejectthe other agent's proposal and solicited further information to help in her decisionmaking.
4 These cases can be grouped into four classes based on the strategy that theagent adopted.
In the first strategy, Invite-Attack, the agent presents evidence (usuallyin the form of a question) that caused her to be uncertain about whether to accept heproposal.
For example, in the following excerpt from the corpus, A inquired about apiece of evidence that would conflict with Crimson Tide not being B's type of movie:4 About wo-thirds ofthese xamples were found in the Udel movie selection dialogues.
We believe thisis because inthat corpus, the dialogue participants are peers and the criteria for accepting/rejecting aproposal re less clear-cut than in the other two domains.361Computational Linguistics Volume 24, Number 3Invite-Attack Example (Udel Transcripts 1995)A: Why don't you want to see Crimson Tide?B: It's supposed to be violent.
It doesn't seem like my type of movie.A: Didn't you like Red October?In the second strategy, Ask-Why, the agent requests further evidence from the otheragent hat will help her make a decision about whether to accept he proposal, as inthe following example:Ask-Why Example (SRI Transcripts 1992)T: Does carrier matter to them do you know?C: No.T: Can we put them on American?C: Why?The third strategy, Invite-Attack-and-Ask-Why, is a combination ofthe first and secondstrategies where the agent presents evidence that caused her to be uncertain aboutwhether to accept he proposal and also requests that the other agent provide furtherevidence to support he original proposal, as in the following example:Invite-Attack-and-Ask-Why Example (Udel Transcripts 1995)a~B:A:B:I'd like to know some inkling of information about the movie.P told you what was happening.Other than P's reviews.Why?
He's a good kid.
He could tell you.Our last strategy includes all other cases in which an agent is clearly uncertain aboutwhether to accept a proposal, but does not directly employ one of the above threestrategies to resolve the uncertainty.
In our analysis, the cases that fall into this categoryshare a common feature in that the agent explicitly indicates her uncertainty aboutwhether to accept he proposal, without suggesting what type of information willhelp resolve her uncertainty, as in the following example:Express-Uncertainty Example (Udel Transcripts 1995)A: I don't like violence.B: You don't like violence?In our corpus analysis, most responses to these questions provided information thatled the agent to eventually accept or reject he original proposal.
We argue that thisinterest in sharing beliefs and supporting information is another feature that distin-guishes collaborative negotiation from argumentation a d noncollaborative n gotia-tion.
Although agents involved in the latter kinds of interaction take other agents'beliefs into account, they do so mainly to find weak points in their opponents' beliefsand to attack them in an attempt to win the argument.362Chu-Carroll and Carberry Response Generation i Planning Dialogues3.2 The Overall Processing ModelThe results of our corpus analysis suggest hat when developing a computationalagent hat participates in collaborative planning, the behavior described below shouldbe modeled.
When presented with a proposal, the agent should evaluate the proposalbased on its private beliefs to determine whether to accept or reject he proposal.
If theagent does not have sufficient information to make a rational decision about accep-tance or rejection, it should initiate an information-sharing subdialogue to exchangeinformation with the other agent so that each agent can knowledgeably re-evaluatethe proposal.
However, if the agent rejects the proposal, instead of discarding the pro-posal entirely, it should attempt to modify the proposal by initiating a collaborativenegotiation subdialogue to resolve the agents' conflict about the proposal.
Thus, wecapture collaborative planning in a Propose-Evaluate-Modify cycle of actions (Chu-Carroll and Carberry 1994, 1995a).
In other words, we view collaborative planning asagent A proposing a set of actions and beliefs to be added to the shared plan beingdeveloped, agent B evaluating the proposal based on his private beliefs to determinewhether or not to accept he proposal, and, if not, agent B proposing a set of mod-ifications to the original proposal.
Notice that this model is a recursive one in thatthe modification process itself contains a full collaboration cycle---agent B's proposedmodifications will again be evaluated by A, and if conflicts arise, A may proposemodifications to the previously proposed modifications.To illustrate how the Propose-Evaluate-Modify framework models collaborativeplanning dialogues, consider the following dialogue segment, aken from the TRAINS91 corpus (Gross, Allen, and Traum 1993):(11) M: Load the tanker car with the oranges, and as soon as engine E2 getsthere, couple the cars and take it to uh(12) S: Well we need a boxcar to take the oranges.
(13) M: No we need a tanker car.
(14) S: No we need a tanker car to take the orange juice, we have to makethe orange juice first.
(15) M: Oh we don't have the orange juice yet.
Where are there oranges?In utterance (11), M proposes a partial plan of loading the tanker car with orangesand coupling it with engine E2.
S evaluates and rejects the proposal and in utterance(12) conveys to M the invalidity of the proposal as a means of implicitly conveyinghis intention to modify the proposal.
In utterance (13), M rejects the belief proposedby S in utterance (12), and addresses the conflict by restating his belief as a meansof modifying S's proposal.
This proposed belief is again evaluated and rejected by Swho, in utterance (14), again attempts to modify M's proposal by providing a pieceof supporting evidence different from that already presented in utterance (12).
Finallyin utterance (15), M accepts these proposed beliefs and thus S's original proposal thatthe partial plan proposed in utterance (11) is invalid.The empirical studies and models of collaboration proposed in Clark and Wilkes-Gibbs (1990) and Clark and Schaefer (1989) provide further support for our Propose-Evaluate-Modify ramework.
They show that participants collaborate in maintaining acoherent discourse and that contributions in conversation i volve a presentation phaseand an acceptance phase.
In the case of referring expressions, $1 presents a referringexpression as part of an utterance; $2 then evaluates the referring expression.
In the363Computational Linguistics Volume 24, Number 3acceptance phase, $2 provides evidence that he has identified the intended entity andthat it is now part of their common ground.
If there are deficits in understanding,the agents enter a phase in which the referring expression is refashioned.
Clark andWilkes-Gibbs note several kinds of refashioning actions, including $2 conveying hisuncertainty about the intended referent (and thereby requesting an elaboration of it)and $1 replacing the referring expression with a new one of her own (still with theintention of identifying the entity intended by Sl's original expression).
This notionof presentation-(evaluation)-acceptance for understanding is similar to our Propose-Evaluate-Modify framework for addition of actions and beliefs to the shared plan.Expressions of uncertainty and substitution actions in the repair phase correlate re-spectively with information-sharing and modification for conflict resolution in ourframework.The rest of this paper discusses our plan-based model for response generationin collaborative planning dialogues.
Our model focuses on communication and ne-gotiation between a computational gent and a human agent who are collaboratingon constructing a plan to be executed by the human agent at a later point in time.Throughout his paper, the user or executing agent (EA) will be used to refer to theagent who will eventually be executing the plan, and the system (CORE) or consult-ing agent (CA) will be used to refer to the computational gent who is collaboratingon constructing the plan.
Figure 1 shows a schematic diagram of the design of ourresponse generation model, where the algorithm used in each subprocess i shownin boldface.
However, before discussing the details of our response generation model,we first address the modeling of agent intentions, which forms the basis of our repre-sentation of agent proposals.4.
Modeling the DialogueIn task-oriented collaborative planning, the agents clearly collaborate on constructingtheir domain plan.
In the university course advisement domain, a domain action maybe agent A getting a Master's degree in CS (Get-Masters(A, CS)).
The agents may alsocollaborate on the strategies used to construct the domain plan, such as determiningwhether to investigate in parallel the different plans for an action or whether to firstconsider one plan in depth (Ramshaw 1991).
Furthermore, the agents may collaborateon establishing certain mutual beliefs that indirectly contribute to the construction oftheir domain plan.
For example, they may collaborate on a mutual belief about whethera particular course is offered next semester as a means of determining whether takingthe course is feasible.
Finally, the agents engage in communicative actions in order toexchange the above desired information.To represent the different ypes of knowledge necessary for modeling a collab-orative dialogue, we use an enhanced version of the tripartite model presented in(Lambert and Carberry 1991) to capture the intentions of the dialogue participants.The enhanced ialogue model (Chu-Carroll and Carberry 1994) has four levels: thedomain level, which consists of the domain plan being constructed to achieve theagents' shared domain goal(s); the problem-solving level which contains the actionsbeing performed to construct the domain plan; the belief level, which consists of themutual beliefs pursued to further the problem-solving intentions; and the discourselevel which contains the communicative actions initiated to achieve the mutual beliefs.Actions at the discourse level can contribute to other discourse actions and also estab-lish mutual beliefs.
Mutual beliefs can support other beliefs and also enable problem-solving actions.
Problem-solving actions can be part of other problem-solving actionsand also enable domain actions.364Chu-Carroll and Carberry Response Generation i  Planning DialoguesOutput: acceptance ,c acceptof user proposalInput: proplsal by userEvaluate user proposal -- Sec 5.1(Evaluate-BelieO~ uncertain Identify subset of uncertain beliefsfor which further information willbe exchanged -- Sec 5.2.1(Select-Focus-!
n fo-Sharing)reject I ?r select an information-sharingstrategy -- Sec 5.2.2Output: discourse acts initiatinginformation-sharingIdentify subset of beliefs to be addressedin conflict resolution -- Sec 6.1(Select-Focus-Modification)Select evidence for claims -- Sec 6.2(Select-Justification)Output: discourse acts presentingproposed modifications along withjustificationFigure 1Schematic diagram of Propose-Evaluate-Modify process.'
ProposeEvaluateModify--->-proposalto userEach utterance by an agent constitutes a proposal that is intended to affect theagents' shared model of domain and problem-solving intentions, as well as their mu-tual beliefs.
These proposals may be explicitly or implicitly conveyed by an agent'sutterances.
For example, consider the following utterances by EA:(16)(17)EA: I want to satisfy my seminar course requirement.Who is teaching CS689?The dialogue model that represents utterances (16) and (17) is shown in Figure 2.It shows the domain actions, problem-solving actions, mutual beliefs, and discourseactions inferred from these utterances, as well as the relationships among them.
Theactions and beliefs represented at the domain, problem-solving, and belief levels aretreated as proposals, and are not considered shared actions or beliefs until the otheragent accepts them.
The beliefs captured by the nodes in the tree may be of three forms:1) MB(_agentl,_agent2,_prop), representing that _agent1 and _agent2 come to mutuallybelieve _prop, 2) MknowrefCagentl,_agent2,_var,_prop), meaning that _agent1 and _agent2come to mutually know the referent of _var which will satisfy _prop, where _var is avariable in _prop, and 3) Mknowifl_agentl,_agent2,_prop), re resenting that _agent1 and_agent2 come to mutually know whether or not _prop is true.
Inform actions produce365Computational Linguistics Volume 24, Number 3Proposed Domain  Leve l. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.\[Satisfy-Seminar-Course(EA,CS) 1~,__IT (EA CS689) ~-c I " ake-Course , - - _ _ ,Proposed Prob lem-So lv ing  Leve l  ~'~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
W .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i .
.
.
.
q Build-Plan(EA,CA,Satisfy-Seminar-Course(EA,CS)) ~'~j.
- - I Build_Plan(EA,CA,Take_Course(EA,CS689)) ~ - - -I Instantiate-Vars(EA,CA,Learn-Material(EA,CS689,_fac),Take-Course(EA,CS689)) Ii Ilnstantiate-Single-Var(EA C fac Learn-Material(EA,CS689 _fac),Take-Course(EA,CS689))', .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~- .~.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
JProposed Be l ie f  Leve l  "~ - .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~'-~'2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
:,,~MB(EA,CA,want(EA,Satisfy-Seminar-Course(EA,CS))) I IMkn?wdf(EA,Cm,-fa?,Tea?hes(-ra~,CS689))\]i, ,  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~-~- := .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.t D iscourse  Leve l  j ,.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~.
~.
~..-.2" .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I ' I Obtain'lnf?-Ref(EA'CA'-fac'Teaches(-fac'Cs689)) Ii IAsk-Ref(EA,CA,-fac,Teaches(- fac,CS689)) I:: "Jlnform(EA,CA,wam(EA,Satisfy-Seminar-Course(EA,CS))) \]i ITelI(EA,CA want(EA Satisfy-Seminar-Course(EA CS))) I \[ Ref-Request(EA'CA'-fac'Teacbes(-fac'cs689)) \]Surface-Say-Prop(EA,CA,want(EA,Satisfy-Seminar-Course(EA,CS))) I I Surface-wH-Q(EA'CA'-fac'Teaches(-fac,CS689)) I. .
.
.
.
.
I .w.q.nLt.q :~.adi.sfy " ~y.q f.m../~.a.r cq.u.rLe " .r.e q.u./.r e.m. en.t.s .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Why..iLty..af.h_in.g.CS6.8.9j~.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Key: ~ subaction arc- - ~ enable arcF igure  2Dia logue mode l  for ut terances (16) and (17).proposals for beliefs of the first type, while wh-questions and yes-no questions produceproposals for the second and third types of beliefs, respectively, sIn order to provide the necessary information for performing proposal evaluationand response generation, we hypothesize a recognition algorithm, based on Lambertand Carberry (1991), that infers agents' intentions from their utterances.
This algorithmmakes use of linguistic knowledge, contextual knowledge, and world knowledge, andutilizes a library of generic recipes for performing domain, problem-solving, and dis-course actions.
The library of generic recipes (Pollack 1986) contains templates forperforming actions.
The recipes are also used by our response generation system inplanning its responses to user utterances, and will be discussed in further detail inSection 5.2.Our system is presented with a dialogue model capturing anew user proposal andits relation to the preceding dialogue.
Based on our Propose-Evaluate-Modify frame-work the system will evaluate the proposed omain and problem-solving actions, aswell as the proposed mutual beliefs, to determine whether to accept he proposal.
In5 Note that wh-quesfions propose  that the agents come to mutually know the referent of a variable.
Oncethe proposal is accepted, the agents will work toward achieving this.
Mutual knowledge is establishedwhen the other agent responds to the question by providing the referent of the variable and theresponse is accepted by the first agent.
Similarly for the case of yes -no  questions.366Chu-Carroll and Carberry Response Generation i  Planning Dialoguesthis paper, we focus on proposal evaluation and modification at the belief level.
Read-ers interested in issues regarding proposal evaluation and modification with respectto proposed actions should refer to Chu-Carroll and Carberry (1994, in press) andChu-Carroll (1996).5.
Determining Acceptance or Rejection of Proposed Beliefs5.1 Evaluating Proposed BeliefsPrevious research as noted that agents do not merely believe or disbelieve a propo-sition; instead, they often consider some beliefs to be stronger (less defeasible) thanothers (Lambert and Carberry 1992; Walker 1992; Cawsey et al 1993).
Thus, we as-sociate a strength with each belief by an agent; this strength indicates the agent'sconfidence in the belief being an accurate description of situations in the real world.The strength of a belief is modeled with endorsements, which are explicit records offactors that affect one's certainty in a hypothesis (Cohen 1985), following Cawsey et al(1993) and Logan et al (1994).
We adopt he endorsements proposed by Galliers (1992),based primarily on the source of the information, modified to include the strength ofthe informing agent's belief as conveyed by the surface form of the utterance used toexpress the belief.
These endorsements are grouped into five classes: warranted, verystrong, strong, weak, and very weak, based on the strength that each endorsementrepresents, in order for the strengths of multiple pieces of evidence for a belief tocombine and contribute to determining the overall strength of the belief.The belief level of a dialogue model consists of one or more belief trees.
Eachbelief tree includes a main belief, represented by the root node of the tree, and a set ofevidence proposed to support it, represented by the descendents of the tree.
6Given aproposed belief tree, the system must determine whether to accept or reject he beliefrepresented by the root node of the tree (henceforth referred to as the top-level pro-posed belief).
This is because the top-level proposed belief is the main belief that EA(the executing agent) is attempting to establish between the agents, while its descen-dents are only intended to provide support for establishing that belief (Young, Moore,and Pollack 1994).
The result of the system's evaluation may lead to acceptance of thetop-level proposed belief, rejection of it, or a decision that insufficient information isavailable to determine whether to accept or reject it.In evaluating a top-level proposed belief (_bel), the system first gathers its evi-dence for and against _bel.
The evidence may be obtained from three sources: 1) EA'sproposal of _bel, 2) the system's own private evidence pertaining to _bel, and 3) evi-dence proposed by EA as support for _bel.
However, the proposed evidence will onlyaffect he system's acceptance of _bel if the system accepts the proposed evidence itself;thus, as part of evaluating _bel, the system evaluates the evidence proposed to support_bel, resulting in a recursive process.
A piece of evidence (for _bel) consists of an an-tecedent belief and an evidential relationship between the antecedent belief and _bel..For example, one might support he claim that Dr. Lewis will not be teaching CS682by stating that Dr. Lewis will be going on sabbatical.
This piece of evidence consistsof the belief that Dr. Lewis will be going on sabbatical nd the evidential relationship6 In this paper, we only consider situations in which an agent's proposed pieces of evidence alluniformly support or attack abelief, but not situations where some of the proposed pieces of evidencesupport abelief and some of them attack the belief.
In cases where an agent proposes vidence toattack abelief, the proposed belief tree will be represented as the pieces of evidence supporting thenegation ofthe belief being attacked.367Computational Linguistics Volume 24, Number 3that Dr. Lewis being on sabbatical generally implies that he is not teaching courses.
7A piece of evidence is accepted if both the belief and the relationship are accepted,rejected if either the belief or the relationship is rejected, and uncertain otherwise.The system's ability to decide whether to accept or reject a belief _bel may beaffected by its uncertainty about whether to accept or reject evidence that EA proposedas support for _bel.
For instance, the system's private evidence pertaining to _bel maybe such that it will accept _bel only if it accepts the entire set of evidence proposedby EA.
In this case, if the system is uncertain about whether to accept some of theproposed evidence, then this uncertainty would prevent it from accepting _bel.
On theother hand, the system's own evidence against _bel may be strong enough to lead to itsrejection of _bel regardless of its acceptance of the evidence proposed to support _bel.In this case, if the system is uncertain about whether to accept some of the proposedevidence, this uncertainty will have no effect on its decision to accept or reject _belitself.
Thus when the system is uncertain about whether to accept some of the proposedevidence, it must first determine whether esolving its uncertainty in these pieces ofevidence has the potential to affect its decision about the acceptance of _bel.
To do this,the system must determine the range of its decision about _bel, where the range isidentified by two endpoints: the upperbound,  which represents the system's decisionabout _bel in the best-case scenario where it has accepted all the uncertain pieces ofevidence proposed to support _bel, and the lowerbound, which represents he system'sdecision about _bel in the worst-case scenario where it has rejected all the uncertainpieces of evidence.
The actual decision about _bel then falls somewhere in between theupperbound and lowerbound, depending on which pieces of evidence are eventuallyaccepted or rejected.
If the upperbound and the lowerbound are both accept, thenthe system will accept _bel and the uncertainty about the proposed evidence will notbe resolved since its acceptance or rejection will not affect the acceptance of _bel.
sSimilarly, if the upperbound and the lowerbound are both reject, the system will reject_bel and the uncertainty about the proposed evidence will again not be resolved.
Inother cases, the system will pursue information-sharing in order to obtain furtherinformation that will help resolve the uncertainty about these beliefs and then re-evaluate _bel.We developed an algorithm, Evaluate-Belief (Figure 3), for evaluating a proposalof beliefs based on the aforementioned principles.
Evaluate-Belief is invoked with_bel instantiated as the top-level belief of a proposed belief tree.
During the evaluationprocess, two sets of evidence are constructed: the evidence set, which contains thepieces of evidence pertaining to _bel that the system has accepted, and the potentialevidence set, which contains the pieces of evidence proposed by the user that thesystem cannot determine whether to accept or reject.
These two sets of evidence are7 In our model, we associate wo measures with an evidential relationship: 1)degree, which representsthe amount of support he antecedent _beli provides for the consequent, _bel, and 2) strength, whichrepresents an agent's trength of belief in the evidential relationship (Chu-Carroll 1996).
For instance,the system may have a very strong (strength) belief that a professor going on sabbatical provides verystrong (degree) support for him not teaching any courses.
In some sense, degree can be viewed ascapturing the relevance (Grice 1975) of a piece of evidence~the more support an antecedent providesfor _bel, the more relevant i is to _bel.
Because of space reasons, we will not make the distinctionbetween degree and strength in the rest of this paper.
We will use an agent's trength of belief in anevidential relationship torefer to the amount of support hat the agent believes the antecedent provides for theconsequent.
This strength of belief is obtained by taking the weaker of the degree and strengthassociated with the evidential relationship n the actual representation n our system.8 Young, Moore, and Pollack (1994) argued that if a parent belief is accepted even though a child beliefthat is intended to support it is rejected, the rejection of the child belief need not be addressed since itis no longer elevant to the agents' overall goal.
Our strategy extends this concept to uncertaininformation.368Chu-Carroll and Carberry Response Generation in P lanning DialoguesEvaluate-Belief(_bel):1. evidence set ~-- _bel (appropriately endorsed as conveyed by EA)gand the system's evidencepertaining to _bel 1?2.
If _bel is a leaf node in the belief tree,return Determine-Acceptance(_bel,evidence set)3.
Evaluate ach of _bel's children, -bell ..... _beln:3.13.23.33.43.5/* evaluate antecedent belief _bel i */bel-result ~-- Evaluate-Belief(_beli)/* evaluate vidential relationship between _beli and _bel */rel-result ~-- Evaluate-Belief(supports(_beli,_bel))If bel-result =rel-result = accept,add {_beli,supports(_beli,_bel)} to evidence setElse if bel-result =reject or rel-result = reject,ignore _beli and supports(_beli,_bel)Else add {beli,supports(_beli,_bel)} to potential evidence set4.
Evaluate _bel:4.14.24.3/* compute upperbound */_bel.upper ~ Determine-Acceptance(_bel,evidence set + potential evidence set)/* computer Iowerbound */_bel.lower ~-- Determine-Acceptance(_bel, evidence set)/* determine acceptance */If _bel.upper =_bel.lower = accept, return acceptElse if _bel.upper =_beLlower = reject, return rejectElse, _bel.evidence ~-- evidence set_bel.potential ~-- potential evidence setreturn uncertainFigure 3Algorithm for evaluating a proposed belief.then  used  to calculate the upperbound and  the lowerbound,  wh ich  in  tu rn  determinethe sys tem's  acceptance of _bel.In  ca lcu la t ing  whether  to accept  a belief,  Evaluate-Belief invokes Determine-Acceptance, which  per fo rms the fo l low ing  funct i6ns  (Chu-Car ro l l  1996): 1) it ut i l i zesa s impl i f ied  vers ion  of Gal l iers '  bel ief  rev is ion  mechan ism (Gal l iers 1992; Logal  et al1994) to determine  the sys tem's  s t rength  of bel ief  in  _bel (or its negat ion)  g iven  a setof ev idence,  by  compar ing  the s t rengths  of the p ieces of ev idence  suppor t ing  and  at-tack ing  _bel, ~1 and  2) it determines  whether  to accept, reject, or remain  uncer ta in  aboutthe acceptance of _bel based  on  the resu l t ing  st rength.
In  determin ing  the s t rength  ofa p iece of ev idence  cons is t ing  of an  antecedent  bel ief  and  an  ev ident ia l  re la t ionsh ip ,9 EA's proposal of _bel is endorsed according to EA's level of expertise in the subarea of _bel as well asher confidence in _bel as conveyed by the surface form of her utterance.10 In our implementation, CORE's knowledge base contains a set of evidential relationships.
Its evidencepertaining to _bel consists of its beliefs about _bel as well as those {_evid-rel,_evid-bel} pairs where 1)the consequent of _evid-rel is _bel, 2) the antecedent of _evid-rel is _evid-bel, and 3) _evid-bel is held byCORE.
Future work will investigate how evidence might be inferred and how resource limitations(Walker 1996b) affect the appropriate depth of inferencing.11 To implement our system, we needed a means of estimating the strength of a belief, and we havebased this estimation on endorsements such as those used in Galliers' belief revision system.
However,the focus of our work is not on a logic of belief, and the mechanisms that we have developed forevaluating proposed beliefs and for effectively resolving detected conflicts (Section 6) are independentof any particular belief logic.
Therefore we will not discuss further the details of how strength of beliefis determined.
Readers are welcome to substitute their favorite means for combining beliefs of variousstrengths.369Computational Linguistics Volume 24, Number 3~-Professor(CS682,Lewis) \]T supports\[ On-Sabbatical(Lewis, 1998) \[Figure 4Beliefs proposed in utterances (18) and (19).Determine-Acceptance follows Walker's weakest link assumption (Walker 1992) andcomputes the strength of the evidence as the weaker of the strengths of the antecedentbelief and the evidential relationship.5.1.1 Example of Evaluating Proposed Beliefs.
To illustrate the evaluation of proposedbeliefs, consider the following utterances by EA, in response to CORE's proposal thatthe professor of CS682 may be Dr. Lewis:(18) EA: The professor of CS682 is not Dr.
Lewis.
(19) Dr. Lewis is going on sabbatical in 1998.Figure 4 shows the beliefs proposed by utterances (18) and (19) as follows: 1) theprofessor of CS682 is not Dr. Lewis, 2) Dr. Lewis is going on sabbatical in 1998, and3) Dr. Lewis being on sabbatical provides support for him not being the professorof CS682.
Note that the second and third beliefs constitute a piece of evidence pro-posed as support for the first belief.
Given these proposed beliefs, CORE evaluates theproposal by invoking the Evaluate-Belief algorithm on the top-level proposed belief,-~Professor(CS682,Lewis).
As part of evaluating this belief, CORE evaluates the evidenceproposed by EA (step 3 in Figure 3), thus recursively invoking Evaluate-Belief onboth the proposed child belief, On-Sabbatical(Lewis,1998), in step 3.1 and the proposedevidential relationship, supports(On-Sabbatical(Lewis,1998),-~Professor(CS682,Lewis)), instep 3.2.
When evaluating On-Sabbatical(Lewis,1998), CORE first searches in its pri-vate beliefs for evidence relevant to it, which includes: 1) a weak piece of evidence forDr.
Lewis going on sabbatical in 1998, consisting of the belief that Dr. Lewis has beenat the university for 6 years and the evidential relationship that being at the universityfor 6 years provides upport for a professor going on sabbatical next year (1998), and2) a strong piece of evidence against Dr. Lewis going on sabbatical, consisting of thebelief that Dr. Lewis has not been given tenure and the evidential relationship thatnot having been given tenure provides upport for a professor not going on sabbat-ical.
These two pieces of evidence are incorporated into the evidence set, along withEA's proposal of the belief, endorsed {non-expert, direct-statement} which has a corre-sponding strength of strong.
CORE then invokes Determine-Acceptance to valuatehow strongly the evidence favors believing or disbelieving On-Sabbatical(Lewis,1998)(step 2).
Determine-Acceptance finds that the evidence weakly favors believing On-Sabbatical(Lewis,1998); since this strength does not exceed the predetermined thresholdfor acceptance (which in our implementation f CORE is strong), CORE reserves judg-ment about the acceptance of On-Sabbatical(Lewis,1998).
Since CORE has a very strongprivate belief that being on sabbatical provides upport for a professor not teaching370Chu-Carroll and Carberry Response Generation i Planning Dialoguesa course, CORE accepts the proposed evidential relationship.
Since CORE accepts theproposed evidential relationship but is uncertain about he acceptance of the proposedchild belief, the acceptance of this piece of evidence is undetermined; thus it is addedto the potential evidence set (step 3.5).CORE then evaluates the top-level proposed belief, ~Professor(CS682,Lewis).
Theevidence set consists of EA's proposal of the belief, endorsed {non-expert, direct-state-ment} whose corresponding strength is strong, and CORE's private weak belief thatthe professor of CS682 is Dr. Lewis.
CORE then computes the upperbound on itsdecision about accepting -Professor(CS682,Lewis) by considering evidence from boththe evidence set and the potential evidence set (step 4.1), resulting in the upperboundbeing accept.
It then computes the lowerbound by considering only evidence from theevidence set, resulting in the lowerbound being uncertain.
Since the upperbound isaccept and the lowerbound uncertain, CORE again reserves judgment about whetherto accept -~Professor(CS682,Lewis), leading it to defer its decision about its acceptanceof EA's proposal in (18)-(19).5.2 Initiating Information-Sharing SubdialoguesA collaborative agent, when facing a situation in which she is uncertain about whetherto accept or reject a proposal, should attempt o share information with the otheragent so that the agents can knowledgeably re-evaluate the proposal and perhapscome to agreement.
We call this type of subdialogue an information-sharing subdia-logue (Chu-Carroll and Carberry 1995b).
Information-sharing subdialogues differ frominformation-seeking or clarification subdialogues (van Beek, Cohen, and Schmidt 1993;Raskutti and Zukerman 1993; Logan et al 1994; Heeman and Hirst 1995).
The latterfocus strictly on how an agent should go about gathering information from anotheragent o resolve an ambiguous proposal.
In contrast, in an information-sharing subdia-logue, an agent may gather information from another agent, present her own relevantinformation (and invite the other agent to address it), or do both in an attempt oresolve her uncertainty about whether to accept or reject a proposal that has been un-ambiguously interpreted.
Since a collaborative agent should engage in effective andefficient dialogues, she should pursue the information-sharing subdialogue that shebelieves will most likely result in the agents coming to a rational decision about theproposal.
The process for initiating information-sharing subdialogues involves twosteps: selecting a subset of the uncertain beliefs that the agent will explicitly addressduring the information-sharing process (called the focus of information-sharing), andselecting an effective information-sharing strategy based on the agent's beliefs aboutthe selected focus.
This process is captured by the recipe for the Share-Info-Reeval-uate-Beliefs problem-solving action that is part of a recipe library used by CORE'smechanism for planning responses.A recipe includes a header specifying the action defined by the recipe, the recipetype, the applicability conditions and preconditions of the action, the subactions com-prising the body of the recipe, and the goal of performing the action.
The applicabilityconditions and preconditions are both conditions that must be satisfied before an ac-tion can be performed; however, while it is anomalous for an agent to attempt osatisfy an unsatisfied applicability condition, she may construct a plan to satisfy afailed precondition.
A recipe may be of two types: specialization or decomposition.
Ia specialization recipe, the body of the recipe contains a set of alternative actions thatwill each accomplish the header action.
In a decomposition recipe, the body consists of371Computational Linguistics Volume 24, Number 3Action:Recipe-Type:Appl Conds:Precondition:Body:Goal:Share-Info-Reevaluate-Beliefs(_agentl,_agent2,_proposed-belief-tree)Specializationuncertain(_agentl,_proposed-belief-tree)focus-of-info-sharing(docus,_proposed-belief-tree)Reevaluate-After-Invite-Attack(_agentl,_agent2,_focus,_proposed-belief-tree)Reevaluate-After-Ask-Why(_agentl,_agent2,_focus,_proposed-belief-tree)Reevaluate-After-Invite-Attack-and-Ask-Why(-agent1-agent2-fcus-proposed-belief-tree)Reevaluate-After-Express-Uncertainty( _ gent1, _agent2, _focus, _proposed-belief-tree )acceptance-determined(_proposed-belief-tree)Figure 5The Share-Info-Reevaluate-Beliefs recipe.a set of simpler subactions for performing the action encoded by the recipe.
12 Finally,the goal of an action is what the agent performing the action intends to achieve.As shown in Figure 5, Share-Info-Reevaluate-Beliefs is applicable only if _agent1is uncertain about he acceptance of a belief tree proposed by _agent2.
The preconditionof the action specifies that the focus of information-sharing be identified.
The recipefor Share-Info-Reevaluate-Beliefs is of type specialization and its body consists offour subactions that correspond to four alternative information-sharing strategies that_agent1 may adopt in attempting to resolve its uncertainty in the acceptance of the se-lected focus.
The selected subaction will be the one whose applicability conditions (asspecified in its recipe) are satisfied; since the applicability conditions for the four sub-actions are mutually exclusive, only one will be selected.
This subaction will initiate aninformation-sharing subdialogue and lead to _agentl's re-evaluation of _agent2's orig-inal proposal, taking into account he newly obtained information.
Next we describehow the focus of information-sharing is identified and how an information-sharingstrategy is selected.5.2.1 Selecting the Focus of Information-Sharing.
In situations where the system isuncertain about the acceptance of a top-level proposed belief, _bel, it may also havebeen uncertain about the acceptance of some of the evidence proposed to support it.Thus, when the system initiates an information-sharing subdialogue to resolve its un-certainty about _bel, it could either directly resolve the uncertainty about _bel itself, orresolve a subset of the uncertain pieces of evidence proposed to support _bel, therebyperhaps resolving its uncertainty about _bel.
We refer to the subset of uncertain beliefsthat will be addressed during information-sharing as the focus of information-sharing.Selection of the focus of information-sharing partly depends on the upperbound andthe lowerbound on the system's decision about accepting _bel.
The possible combina-tions of these values produced by the Evaluate-Belief algorithm are shown in Table 2.13In cases 1 and 2, the system accepts/rejects _bel regardless of whether the pieces of12 In Allen's formalism (Allen 1979), the body of a recipe could contain a set of goals to be achieved or aset of actions to be performed.
In our current system, the preconditions are goals that are matchedagainst he goals of recipes, and the body contains actions that are matched against he header actionin recipes.13 In our model, a child belief in a proposed belief tree is always intended to provide support for itsparent belief; thus the evidence in the potential evidence set contributes positively toward the system'sacceptance of _bel.
Since the upperbound is computed by taking into account evidence from both theevidence and potential evidence sets while the lowerbound is computed by considering evidence fromthe evidence set alne, the upperbound will always be greater than or equal to the lowerbound (on thescale of reject, uncertain, and accept).
Thus only six out of the nine theoretically possible combinationscan occur.372Chu-Carroll and Carberry Response Generation i  Planning DialoguesTable 2Possible combinations ofupperbounds and lowerbounds.Upperbound Lowerbound Action1 accept accept2 reject reject3 uncertain uncertain4 accept uncertain5 uncertain reject6 accept rejectaccept _belreject _belresolve uncertainty regarding _bel itselfattempt to accept uncertain evidenceattempt to reject uncertain evidenceaction in cases 4and/or 5evidence in the potential evidence set, if ann are accepted or rejected.
In these cases,the uncertainty about he proposed evidence does not affect he system's acceptance of_bel, and therefore need not be resolved.
In case 3, the system remains uncertain aboutthe acceptance of _bel regardless of whether the uncertain pieces of evidence, if any,are accepted or rejected, i.e., resolving the uncertainty about he evidence will not helpresolve the uncertainty about _bel.
Thus, the system should focus on sharing informa-tion about _bel itself.
TM In cases 4 and 6 where the upperbound is accept, acceptanceof a large-enough subset of the uncertain evidence will result in the system accepting_bel, and in cases 5 and 6 where the lowerbound is reject, rejection of a large-enoughsubset of the uncertain evidence can lead the system to reject _bel.
Thus in all threecases, the system should initiate information-sharing to resolve the uncertainty aboutthe proposed evidence in an attempt to resolve the uncertainty about _bel.
is However,when there is more than one piece of evidence in the potential evidence set, the systemshould select a minimum subset of these pieces of evidence to address based on thelikelihood of each piece of evidence affecting the system's resolution of the uncertaintyabout _bel.In selecting the focus of information-sharing, we take into account he followingthree factors: 1) the number factor: the number of pieces of uncertain evidence thatwill be addressed uring information-sharing, since one would prefer to address asfew pieces of evidence as possible, 2) the effort factor: the effort involved in resolvingthe uncertainty in a piece of evidence, since one would prefer to address the piecesof evidence that require the least amount of effort to resolve, and 3) the contribu-tion factor: the contribution of each uncertain piece of evidence toward resolving theuncertainty about _bel, since one would prefer to address the uncertain pieces of ev-idence predicted to have the most impact on resolving the uncertainty about _bel.
Incases 4 and 6 in Table 2, where the system will accept _bel if it accepts a sufficientsubset of the uncertain evidence, the goal is to select as focus a minimum subset of theuncertain pieces of evidence 1) whose uncertainty requires the least effort to resolve,and 2) which, if accepted, are predicted to lead the system to accept _bel.
Similarly,in cases 5 and 6, where the system will reject _bel if it can reject a sufficient subset14 It might be the case that the system gathers further information about _bel, re-evaluates _bel taking intoaccount he newly-obtained information, and is still uncertain about whether  to accept or reject _bel.
Ifthis reevaluation of _bel with additional evidence falls into case 4, 5, or 6, then the uncertainty aboutthe proposed evidence becomes relevant and will be pursued.15 Based on our algorithm (to be shown in Figure 6), in case 6, the system will perform the actions inboth cases 4 and 5, i.e., try and gather both information that may lead to the acceptance of _bel andinformation that may lead to the rejection of _bel, and leave it up to the user to determine which one toaddress.
Alternatively, the system could be designed to select between the actions in cases 4 and 5, i.e.,determine whether attempting to accept _bel or attempting to reject _bel is more efficient, and pursuethe more promising path.
We leave this for future work.373Computational Linguistics Volume 24, Number 3Select-Focus-Info-Sharing(_bel):/* _bel has been previously annotated with two features by Evaluate-Belief:_bel.evidence: vidence pertaining to _bel which the system accepts_bel.potentiah evidence proposed by the user for _bel and about which the system is uncertain */1.
/* Cases I & 2 */If _bel.upper = _beLlower = accept or if _bel.upper = _beLlower = reject,focus ~- (}; return focus.2.
/* Case 3 */If _bel.upper = _beLlower = uncertain, focus ~-- (_bel}; return focus.3.
If _bel has no uncertain children, focus ~-- (_bel}; return focus.4.
/* Cases 4 & 6 */If _beLupper = accept,4.1 /* The effort factor */Assign each piece of uncertain evidence in _bel.potential to a set, and order the setsaccording to how close the evidence in each set was to being accepted.
Call them_set1,...,-setm._set-size ~-- 14.2 /* The contribution factor */For each set in ranked order, do until new-resulti=accept:new-result/ *- Determine-Acceptance(_bel, _bel.evidence + _set/)4.3 If new-resul t /~ accept,/* The number factor */_set-size ~-- _set-size + 1,form new sets of evidence of size _set-size from _bel.potential, 16rank new sets according to how close the evidence in each set was to being accepted,goto 4.2.4.4 Else, focus ~ U_elj~_seti Select-Focus-Info-Sharing(_elj); return focus.5.
/* Cases 5 & 6 */If _beLlower = reject,5.1 /* The effort factor */Assign each piece of uncertain evidence in _bel.potential to a set, and order the setsaccording to how close the evidence in each set was to being rejected.
Call them_set1, .
.
.
,  _setm._set-size ~-- 15.2 /* The contribution factor */For each set in ranked order, do until new-resulti = reject:new-resulti ~-- Determine-Acceptance(_bel, _bel.evidence + _bel.potential - set/)5.3 If new-resulti ~6 reject,/* The number factor */_set-size ~ _set-size + 1,form new sets of evidence of size _set-size from _bel.potential,rank new sets according to how close the evidence in each set was to being rejected,goto 5.2.5.4 Else, focus ~ U_elj~-seti Select-Focus-Info-Sharing(_el/); return focus.Figure 6Algor i thm for se lect ing the  focus of in fo rmat ion -shar ing .of the uncertain evidence, the system's goal is to select as focus a minimum subsetof the uncertain pieces of evidence 1) whose uncertainty requires the least effort toresolve, and 2) which, if rejected, are predicted to cause the system to reject _bel.
Oncethe system has identified this subset of uncertain evidence, it has to determine thefocus of information-sharing for resolving the uncertainty regarding these pieces ofevidence, leading to a recursive process.Our algorithm Select-Focus-Info-Sharing, shown in Figure 6, carries out this pro-374Chu-Carroll and Carberry Response Generation i  Planning Dialoguescess.
It is invoked with _bel instantiated as the uncertain top-level proposed belief.Steps 4 and 5 of the algorithm capture the above principles for identifying a set ofuncertain beliefs as the focus of information-sharing.
Our algorithm guarantees thatthe fewest pieces of uncertain evidence for _bel will be addressed, and that the belief(s)selected as focus are those that requires the least effort to achieve among those thatare strong enough to affect the acceptance of _bel, thus satisfying the above criteria.5.2.2 Selecting an Information-Sharing Strategy.
The focus of information-sharing,produced by the Select-Focus-Info-Sharing al orithm, is a set of one or more pro-posed beliefs that the system cannot decide whether to accept and whose acceptance(or rejection) will affect the system's acceptance of the top-level proposed belief.
Foreach of these uncertain beliefs, the system must select an information-sharing strategythat specifies how it will go about sharing information about the belief to resolve itsuncertainty.
Let _focus be one of the beliefs identified as the focus of information-sharing.
The selection of a particular information-sharing strategy should be based onthe system's existing beliefs about _focus as well as its beliefs about EA's beliefs about-focus.
As discussed in Section 3.1, our analysis of naturally occurring collaborativedialogues hows that human agents may adopt one of four information-sharing strate-gies.
The information-sharing strategies and the criteria under which we believe eachstrategy should be adopted are as follows:1.
Invite-Attack, in which agent A presents a piece of evidence againstXocus and (implicitly) invites the other agent (agent B) to attack it.
Thisstrategy focuses B's attention on the counterevidence and suggests that itis what keeps A from accepting _focus.
This strategy is appropriate whenA's counterevidence for _focus is critical, i.e., if convincing A that thecounterevidence is invalid will cause A to accept _focus.
This strategyalso allows for the possibility of B accepting the counterevidence andboth agents possibly adopting q_focus instead of _focus.2.
Ask-Why, in which A queries B about his reasons for believing in _focus.This strategy is appropriate when A does not know B's support for_focus, and intends to find out this information.
This will result either inA gathering evidence that contributes toward her accepting _focus, or inA discovering B's invalid justification for holding _focus and attemptingto convince B of its invalidity.3.
Ask-Why-and-Invite-Attack, in which A queries B for his evidence for-focus and also presents her evidence against it.
This strategy isappropriate when A does not know B's support for -focus, but does have(noncritical) evidence against it.
In this case B may provide his supportfor _focus, attack A's evidence against -focus, or accept A'scounterevidence and perhaps ubsequently adopt ~_focus.4.
Express-Uncertainty, in which A indicates her uncertainty aboutaccepting -focus and presents her evidence against _focus, if any.
Thisstrategy is appropriate when none of the previous three strategies apply.16 In the worst-case scenario, the algorithm will examine very superset of the elements in _bel.potential.However, _bel.potential contains only those proposed pieces of evidence whose acceptance is uncertain,which depends only on the number of utterances provided in a single turn, but not on the size ofCORE's or EA's knowledge base.
Thus, we believe that this combinatorial spect of the algorithmshould not affect the scalability of our system.375Computational Linguistics Volume 24, Number 3Action:Appl Conds:Preconditions:Body:Goal:Reevaluate-After-Invite-Attack( _agent1, _agent2, _focus, _proposed-belief-tree )~believe(_agentl,_focus)~believe(_agentl,~docus)believe(_agentl,_bel)believe(_agentl,supports(_bel,~_focus))results-in(believe(_agentl,~ _bel),believe(_agentl,_focus))MB(_agentl,_agent2,_bel) A MB(_agentl,_agent2,supports(_bel,~_focus)) VMB(_agentl,_agent2p_bel) VMB(_agentl,_agent2,~supports(_bel,-~_focus))Evaluate-Belief-Level(_agentl,_agent2,_proposed-belief-tree)belief-reevaluated(_proposed-belief-tree)Figure 7The Reevaluate-After-Invite-Attack recipe.In collaborative dialogues, A's indication of her uncertainty should leadB to provide information that he believes will help A re-evaluate heproposal.We have realized these four information-sharing strategies as problem-solvingrecipes in our system.
Figure 7 shows the recipe for the Reevaluate-After-Invite-Attackaction which corresponds tothe Invite-Attack strategy.
Reevaluate-After-Invite-Attacktakes four parameters: _agent1, the agent initiating information-sharing; _agent2, theagent who proposed the beliefs under consideration; _focus, a belief selected as thefocus of information-sharing; and _proposed-belief-tree, which is the belief tree from_agent2's original proposal.
The Reevaluate-After-Invite-Attack action is applicablewhen _agent1 is uncertain about the acceptance of _focus (captured by the first twoapplicability conditions).
Furthermore, _agent1 must hold another belief _bel that sat-isfies the following two conditions: 1) _agent1 believes that _bel provides upport for-l_focus, and 2) _agent1 disbelieving _bel will result in her accepting _focus, i.e., _bel isthe only reason that prevents _agent1 from accepting _focus.In the body of Reevaluate-After-Invite-Attack, _agent1 re-evaluates _proposed-belief-tree, _agent2's original proposal, by taking into account the information that shehas obtained since it was last evaluated.
This new information is obtained through aninformation-sharing subdialogue using the Invite-Attack strategy, and the dialogue isinitiated in an attempt to satisfy the preconditions of Reevaluate-After-Invite-Attack.Before performing the body of Reevaluate-After-Invite-AttacK one of three alternativepreconditions must hold: 1) the agents mutually believe _bel and that _bel providessupport for l_focus, i.e., _agent2 has accepted _agentl's counterevidence for _focus, 2)the agents mutually believe ~_bel, i.e., _agent1 has given up on her belief about _beland thus the counterevidence, or 3) the agents mutually believe that _bel does notprovide support for l-focus, i.e., _agent1 has changed her belief about the supports:relationship and thus the counterevidence.
Since _agent1 believes in both _bel andsupports(_bel,~_focus) when the action is initially invoked, she will attempt o satisfythe first precondition by adopting discourse actions to convey these beliefs to _agent2.This results in _agent1 initiating an information-sharing subdialogue to convey to_agent2 her critical evidence against _focus and (implicitly) inviting _agent2 to attackthis evidence.5.2.3 Example of Initiating Information-Sharing Subdialogues.
We now continuethe example in Section 5.1.1 where CORE has reserved judgment about two beliefs376Chu-Carroll and Carberry Response Generation i Planning DialoguesDialogue Model for Utterances (18) and (19).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,Proposed Problem-Solving Level i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,I Arbitrate(CA,EA,proposed) I\[Evaluate-aroposal(Cg,EA,proposed) \[I Evaluate-Belief-Level(CA,EA,<belief tr e>) \[I S hare'In f?-Reevaluate-Beli~fs( CA,EA,<belief tree> ) IReevaluate-After-Invite-Attack(CA,EA,On-Sabbatical(Lewis, 1998) <belief tree>) \[.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
-~= .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x~.< .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Prpposed Belief Level - "  ""  - .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.IMB(Cg,Eg,~Tenured(Lewis))l \[MB(Cg,EA,supports(-Tenured(Lewis),-On-Sabbatical(Lewis,1998))) \].
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
-~---..~-~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .~.;Z.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Discourse Levd"" - .  "
"r .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
- - .
-  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~- - -~-  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,\[ Express-Doubt(CA,EA,~Tenured(Lewis),~On-Sabbatical(Lewis, 1998)) \]I C?nvey-Uncertain'Belief(cA'EA'~Tenured(Lewis)) ISurface-Neg-YN-Q(CA,EA,-Tenured(Lewis)) I. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Isn "t it true that Dr. Lewis hasn't been given tenure?Figure 8Dialogue model for utterance (20).proposed by EA, namely ~Professor(CS682,Lewis) and On-Sabbatical(Lewis,1998).
Sincethe upperbound and lowerbound on the decision about whether to accept or re-ject ~Professor(CS682, Lewis) were accept and uncertain, CORE pursues information-sharing by invoking the Share-Info-Reevaluate-Beliefs action (Figure 5), which inturn invokes Select-Focus-Info-Sharing (Figure 6) on the top-level proposed belief-~Professor(CS682,Lewis).
Since the potential evidence set contains only one piece ofevidence (the only piece of evidence proposed by EA), and CORE's acceptance of thispiece of evidence will result in its acceptance of the topqevel proposed belief, the algo-rithm is applied recursively to the uncertain child belief On-Sabbatical(Lewis,1998).
Sincethe child belief has no children in the proposed belief tree, On-Sabbatical(Lewis,1998)is selected as the focus of information-sharing.CORE now performs the body of Share-Info-Reevaluate-Beliefs on the identifiedfocus, On-Sabbatical(Lewis,1998), by selecting an appropriate information-sharing strat-egy.
Since CORE's belief that Dr. Lewis not having been given tenure and its belief inthe evidential relationship that Dr. Lewis not having been given tenure implies thathe is not going on sabbatical constitute the only obstacle against its acceptance ofOn-Sabbatical(Lewis,1998), Reevaluate-After-Invite-Attack (Figure 7) is selected as thesubaction for Share-Info-Reevaluate-Beliefs.Figure 8 shows the dialogue model that will be constructed for this infor-mation-sharing process.
In order to satisfy the first precondition of Reevaluate-After-Invite-Attack, CORE posts MB(CA, EA, ~Tenured(Lewis)) and MB(CA, EA,supports(~Tenured(Lewis),~On-Sabbatical(Lewis,1998))) as mutual beliefs to be achieved.CORE applies the Express-Doubt discourse action (based on Lambert and Carberry\[1992\]) to simultaneously achieve these two goals, leading to the generation of the377Computational Linguistics Volume 24, Number 3Action:Appl Conds:Preconds:Body:Goal:Reevaluate-After-Invite-Attack(CA, EA, On-Sab(Lewis,1998), Kbelief tree~)~believe(CA, On-Sab(Lewis,1998))~believe(CA,~On-Sab(Lewis,1998))believe(CA,-~Tenured(Lewis))believe(CA, supports(~Tenured(Lewis),~On-Sab(Lewis,1998)))results-in(believe(CA, Tenured(Lewis)),believe(CA, On-Sab(Lewis,1998)))MB(CA, EA,~Tenured(Lewis)) A MB(CA,EA, supports(~Tenured(Lewis),~On-Sab(Lewis,1998)) VMB(CA, EA, Tenured(Lewis)) VMB(CA, EA,~supports(~Tenured(Lewis), ~On-Sab(Lewis,1998)))Evaluate-Belief-Level(CA, EA, Kbelief tree >)belief-reevaluated( Kbelief tree>)Figure 9Instantiated recipe for Reevaluate-After-Invite-Attack.semantic form of the following utterance:(20) CA: Isn't it true that Dr. Lewis hasn't been given tenure?5.2.4 Possible Follow-ups to Utterance (20).
Now consider how the alternative dis-juncts of the precondition for Reevaluate-After-Invite-Attack might be satisfied to en-able the execution of the body of the action.
Figure 9 shows the recipe for Reevaluate-After-Invite-Attack as instantiated in this example.
Consider the following alternativeresponses to utterance (20): 17(21) a. EA: Oh, you're right.
I guess that means he's not going on sabbatical.b.
EA: He told me that his tenure was approved yesterday.c.
EA: Yes, but he got special permission to take an early sabbatical.d.
EA: Really?
Are you sure of that?Utterance (21a) would be interpreted as EA accepting the beliefs proposed in (20).This indicates that EA now believes both ~Tenured(Lewis) and supports(-~Tenured(Lewis),~On-Sabbatical(Lewis,1998)), thus satisfying the first precondition of Reevaluate-After-Invite-Attack.
CORE will then reevaluate EA's original proposal, taking into accountthe new information obtained from utterance (21a).In utterance (21b), EA conveys rejection of CORE's proposed belief, ~Tenured(Lewis).If CORE accepts EA's proposal in (21b), then the mutual belief Tenured(Lewis) i  es-tablished between the agents.
This satisfies the second precondition in Figure 9 andleads CORE to reevaluate EA's original proposal.
In utterance (21c), on the other hand,EA conveys rejection of CORE's proposed evidential relationship, supports(~Tenured(Lewis),~On-Sabbatical(Lewis,1998)).
If CORE accepts EA's proposal in (21c), then themutual belief ~supports(~Tenured(Lewis), ~On-Sabbatical(Lewis,1998)) is established be-tween the agents.
This satisfies the third precondition i  Figure 9 and leads CORE to re-evaluate EA's original proposal.
Although in utterance (20), CORE attempted to satisfythe precondition that both agents believe ~Tenured(Lewis) and supports(~Tenured(Lewis),17 A reviewer suggested a fifth possible response of "So what?"
In such a case, the system would need torecognize that EA failed to comprehend the implied evidential relationship between ot being tenuredand not going on sabbatical.
Our current system cannot handle misunderstandings such as this.378Chu-Carroll and Carberry Response Generation i  Planning Dialogues~On-Sabbatical(Lewis,1998)), the precondition that is actually satisfied in (21b) and (21c)is different.
This illustrates how the preconditions of Reevaluate-After-Invite-Attackcapture situations in which EA presents counterevidence to CORE's critical evidenceand changes its beliefs.Utterance (21d), on the other hand, would be interpreted as EA being uncer-tain about whether to accept or reject CORE's proposal in (20), and initiating aninformation-sharing subdialogue to resolve this uncertainty.
This example illustrateshow an extended information-sharing process can be captured in our model as a recur-sive sequence of Propose and Evaluate actions.
CORE's first Evaluate action resultsin uncertainty about the acceptance of EA's proposal in (18) and (19), and leads tothe information-sharing subdialogue initiated by (20).
CORE's proposal in (20) is eval-uated by EA, whose uncertainty about whether to accept it leads her to initiate anembedded information-sharing subdialogue in utterance (21d).6.
Resolving Conflicts in Proposed BeliefsThe previous ection described our processes for evaluating proposed beliefs and ini-tiating information-sharing to resolve the system's uncertainty in its acceptance of theproposal.
The final outcome of the evaluation process is an informed decision aboutwhether the system should accept or reject EA's proposal.
When the system rejectsEA's proposal, it will attempt o modify the proposal instead of simply discarding it.This section describes algorithms for producing responses in negotiation subdialoguesinitiated as part of the modification process.The collaborative planning principle in Whittaker and Stenton (1988); Walker andWhittaker (1990); and Walker (1992) suggests that "conversants must provide evidenceof a detected iscrepancy in belief as soon as possible'(Walker 1992, 349).
Thus, oncean agent detects a relevant conflict, she must notify the other agent of the conflict andattempt to resolve it--to do otherwise is to fail in her responsibilities a  a collaborativeparticipant.
A conflict is "relevant" to the task at hand if it affects the domain planbeing constructed.
In terms of proposed beliefs, detected conflicts are relevant only ifthey contribute to resolving the agents' disagreement about a top-level proposed belief.This is because the top-level proposed belief contributes to problem-solving actionsthat in turn contribute to domain actions, while the other beliefs are proposed onlyas support for it.
If the agents agree on the top-level proposed belief, then whether ornot they agree on the evidence proposed to support it is no longer relevant (Young,Moore, and Pollack 1994).The negotiation process for conflict resolution is carried out by the Modify com-ponent of our Propose-Evaluate-Modify cycle.
The goal of the modification process isfor the agents to reach an agreement on accepting perhaps a variation of EA's originalproposal.
However, a collaborative agent should not modify a proposal without theother agent's consent.
This is captured by our Modify-Proposal action and its two spe-cializations: 1) Correct-Node (Figure 10), which is invoked when the agents attemptto resolve their conflict about a proposed belief, and 2) Correct-Relation, which is in-voked when the agents attempt to resolve their conflict about the proposed evidentialrelationship between two beliefs.
The recipes for the first subaction of each of these ac-tions, Modify-Node (Figure 10) and Modify-Relation, share a common preconditionthat both agents agree that the original proposal is faulty before any modification cantake place.
It is the attempt o satisfy this precondition that leads to the initiation of anegotiation subdialogue and the generation of natural anguage utterances to resolvethe agents' conflict.Communication for conflict resolution involves an agent (agent A) conveying to379Computational Linguistics Volume 24, Number 3Action:Type:Appl Cond:Body:Goal:Correct-Node(_agentl, _agent2, _belief, _proposed)Decompositionbelieve(_agentl, ~_belief)believe(_agent2, _belief)Modify-Node(_agentl, _agent2,_proposed, _belief)Insert-Correction(_agentl,_agent2,_proposed)acceptable(_proposed)Action: Modify-Node(_agentl,_agent2,_proposed,_belief)Precondition: MB(_agentl,_agent2, -~_belief)Body: Remove-Node(_agentl,_agent2,_proposed,_belief)Goal: modified(_proposed)Figure 10The Correct-Node and Modify-Node recipes.the other agent (agent B) the detected conflict and perhaps providing evidence tosupport her point of view.
If B accepts A's proposal for modification, the actual mod-ification process will be carried out.
On the other hand, if B does not immediatelyaccept A's claims, he may provide evidence to justify his point of view, leading toan extended negotiation subdialogue to resolve the detected conflict.
This negotiationsubdialogue may lead to 1) A accepting B's beliefs, thereby accepting B's originalproposal and abandoning her proposal to modify it, 2) B accepting A's beliefs, allow-ing A to carry out the modification of the proposal, 3) the agents accepting a furthermodification of the proposal, TM or 4) a disagreement between A and B that cannot beresolved.
The last case is beyond the scope of this work.As in the case of information-sharing, when a top-level proposed belief is rejectedby the system, the system may have also rejected some of the evidence proposed tosupport he top-level belief.
Thus, the system must first identify the subset of detectedconflicts it will explicitly address in its pursuit of conflict resolution.
Furthermore,it must determine what evidence it will present o EA in an attempt o resolve theagents' conflict about these beliefs.
The following sections address these two issues.6.1 Selecting the Focus of ModificationSince collaborative agents are expected to engage in effective and efficient dialoguesand not to argue for the sake of arguing, the system should address the rejectedbelief(s) that it predicts will most efficiently resolve the agents' conflict about the top-level proposed belief.
This subset of rejected beliefs will be referred to as the focus ofmodification.The process for selecting the focus of modification operates on a proposed belieftree evaluated using the Evaluate-Belief algorithm in Figure 3 and involves two steps.First, the system constructs a candidate foci tree consisting of the top-level proposedbelief along with the pieces of evidence that, if refuted, might resolve the agents' con-flict about he top-level proposed belief.
These pieces of evidence satisfy the followingtwo criteria: First, the evidence must have been rejected by the system, since a collabo-rative agent should only refute those beliefs about which the agents disagree.
Second,the evidence must be intended to support a rejected belief or evidential relationshipin the candidate foci tree.
This is because successful refutation of such evidence will18 This possibility iscaptured by the recursive nature of our Propose-Evaluate-Modify framework asnoted in Section 3.2, but will not be discussed further in this paper.380Chu-Carroll and Carberry Response Generation i  Planning Dialogues(r) a~(r) bA  O c (a) (r) a /(a 7 (ry(r) dO ?
e (a) (r) b A0 f (r) (r) dO 0 e (a)(a) (b)Figure 11An evaluated belief tree (a) and its corresponding candidate foci tree (b).lessen the support for the rejected belief or relationship it was intended to supportand thus indirectly further efutation of the piece of evidence that it is part of; by tran-sitivity, this refutation indirectly furthers refutation of the top-level proposed belief.Our algorithm for constructing the candidate foci tree first enters the top-levelbelief from the proposed belief tree into the candidate foci tree, since successful refu-tation of this belief will resolve the agents' conflict about the belief.
It then performsa depth-first search on the proposed belief tree to determine the nodes and links thatshould be included in the candidate foci tree.
When a node in the belief tree is visited,both the belief and the evidential relationship between the belief and its parent areexamined.
If either the belief or the relationship was rejected by the system during theevaluation process, this piece of evidence satisfies the two criteria noted above and isincluded in the candidate foci tree.
The system then continues to search through theevidence proposed to support the rejected belief and/or evidential relationship.
Onthe other hand, if neither the belief nor the relationship was rejected, the search on thecurrent branch terminates, ince the evidence itself does not satisfy the first criterion,and none of its descendents would satisfy the second criterion.Given the evaluated belief tree in Figure 11(a), Figure 11(b) shows its correspondingcandidate foci tree.
The parenthesized letters indicate whether a belief or evidentialrelationship was accepted (a) or rejected (r) during the evaluation process.
Notice thatthe evidence {c, supports(c,a)} is not included in the candidate foci tree because thefirst criterion is not satisfied.
In addition, {Lsupports(f,e)} is not incorporated into thecandidate foci tree because the evidence does not satisfy the second criterion.The second step in selecting the focus of modification is to select from the candi-date foci tree a subset of the rejected beliefs and/or evidential relationships that thesystem will explicitly refute.
The system could attempt to change EA's belief about hetop-level belief &el by 1) explicitly refuting _bel, 2) explicitly refuting the proposedevidence for _beL thereby causing him to accept ~_bel, or 3) refuting both _bel andits rejected evidence.
A collaborative agent's first preference should be to address therejected evidence, since McKeown's focusing rules suggest hat continuing a newlyintroduced topic is preferable to returning to a previous topic (McKeown 1985).
Whena piece of evidence for _bel is refuted, both the evidence and _bel are considered openbeliefs and can be addressed naturally in subsequent dialogues.
On the other hand,if the agent addresses _bel directly, thus implicitly closing the pieces of evidence pro-posed to support _beL then it will be less coherent o return to these rejected pieces381Computational Linguistics Volume 24, Number 3of evidence later on in the dialogue.
Furthermore, in addressing a piece of rejectedevidence to refute _bel, an agent conveys disagreement regarding both the evidenceand _bel.
If this refutation succeeds, then the agents not only have resolved their con-flict about _bel, but have also eliminated a piece of invalid support for _bel.
Althoughthe agents' goal is only to resolve their conflict about _bel, removing support for _belhas the beneficial side effect of strengthening acceptance of -~_bel, i.e., removing anylingering doubts that EA might have about accepting -~_bel.
If the system chooses torefute the rejected evidence, then it must identify a minimally sufficient subset hatit will actually address, and subsequently identify how it will go about refuting eachpiece of evidence in this subset.
This potentially recursive process produces a set ofbeliefs, called the focus of modification, that the system will explicitly refute.In deciding whether to refute the rejected evidence proposed as support for _bel,to refute _bel directly, or to refute both the rejected evidence and _bel, the system mustconsider which strategy will be successful in changing EA's beliefs about _bel.
Thesystem should first predict whether efuting the rejected evidence alone will producethe desired belief revision.
This prediction process involves the system first selectinga subset of the rejected evidence that it predicts it can successfully refute, and thenpredicting whether eliminating this subset of the rejected evidence is sufficient o causeEA to accept ~_bel.
If refuting the rejected evidence is predicted to fail to resolve theagents' conflict about _bel, the system should predict whether directly attacking _belwill resolve the conflict.
If this is again predicted to fail, the system should considerwhether attacking both _bel and its rejected evidence will cause EA to reject _bel.
Ifnone of these is predicted to succeed, then the system does not have sufficient evidenceto convince EA of -~_bel.Our algorithm, Select-Focus-Modification (Figure 12), is based on the above prin-ciples and is invoked with _bel instantiated as the root node of the candidate foci tree.To select he focus of modification, the system must be able to predict the effect thatpresenting a set of evidence will have on EA's acceptance of a belief.
Logan et al(1994) proposed a mechanism for predicting how a hearer's beliefs will be altered bysome communicated beliefs.
They utilize Galliers' belief revision mechanism (Galliers11992) to predict he hearer's belief in _bel based on: 1) the speaker's beliefs about thehearer's evidence pertaining to _bel, which can include beliefs previously conveyedby the hearer and stereotypical beliefs that the hearer is thought o hold, and 2) theevidence that the speaker is planning on presenting to the hearer.
Thus the predictionis based on the speaker's beliefs about what the hearer's evidence for and against_bel will be after the speaker's evidence has been presented to the hearer.
Our Predictfunction in Figure 12 utilizes this strategy to predict whether the hearer will accept,reject, or remain uncertain about his acceptance of _bel after evidence is presented tohim.In our algorithm, if resolving the conflict about _bel involves refuting its rejectedevidence (steps 4.2 and 4.4), Select-Min-Set is invoked to select a minimally sufficientset to actually address.
Select-Min-Set first ranks the pieces of evidence in _cand-setin decreasing order of the impact that each piece of evidence is believed to have onEA's belief in _bel.
The system then predicts whether changing EA's belief about thefirst piece of evidence (_evidl) is sufficient.
If not, then merely addressing one piece ofevidence will not be sufficient o change EA's belief about _bel (since the other piecesof evidence contribute less to EA's belief in _bel); thus the system predicts whetheraddressing the first two pieces of evidence in the ordered set is sufficient.
This processcontinues until the system finds the first n pieces of evidence which it predicts, whendisbelieved by EA, will cause him to accept -~_bel.
The rejected components of these npieces of evidence are then returned by Select-Min-Set.
This process guarantees that382Chu-Carroll and Carberry Response Generation in Planning DialoguesSelect-Focus-Modification(&el):1.
_bel.u-evid ~-- system's beliefs about EA's evidence pertaining to _bel&el.s-attack ~-~ system's own evidence against _bel2.
If _bel is a leaf node in the candidate foci tree,2.12.2If Predict(&el,_bel.u-evid + &el.s-attack) =reject,then &el.focus ~-- {&el}; returnElse &el.focus ~-- nil; return3.
/* Select /ocus /or each o/_bel" s children in the candidate /oci tree, _bell ..... _beln *!3.13.23.3If supports(&eli,&el) is accepted but &eli is not, Select-Focus-Modification(&eli).Else if _beli is accepted but supports(&eli,&el) is not,Select-Focus-Modification(supports(&eli,_bel)).Else Select-Focus-Modification(&eli) and Select-Focus-Modification(supports(_beli,_bel))4.
/* Choose between attacking the proposed evidence~or _bel and attacking _bel itself*~4.14.24.34.44.5/* Form a candidate set consisting o/the pieces o/evidence that the system rejected and which itpredicts it can successfully refute */_cand-set ~- { {_beli, supports( &eli,&el ) } \[ rejected( {_beli,supports(_beli,_bel ) } ) A(-~rejected(&eli) V _beli.focus~6nil) A(-~rejected(supports(_beli, &el)) Vsupports(&eli, _bel).focus#nil)}/* Check i/addressing _bel ~ rejected evidence is sufficient */If Predict(_bel, &el.u-evid - _cand-set) =reject,rain-set ~-- Select-Min-Set(_bel,_cand-set)&el.focus *- U&eli~_rnin-set &eli.focus/* Check i/addressing _bel itsel/ is sufficient */Else if Predict(&el,_bel.u-evid + &el.s-attack) =reject,&el.focus ~ {&el}/* Check i/addressing both ..bel and its rejected evidence is sufficient */Else if Predict(&el, &el.s-attack + &el.u-evid - _cand-set) =reject,min-set ~ Select-Min-Set(&el, _cand-set U {&el})&el.focus ~- U&eli~min-set - {_bel} &eli'f?cus U {&el}Else &el.focus ~-- nilFigure 12Algorithm for selecting the focus of modification._rain-set is the min imum subset of evidence proposed to support  _bel that the systembelieves it must  address in order to change EA's belief in _bel.After the Select-Focus-Modification process is completed, each rejected top-levelproposed belief (_bel) will be annotated with a set of beliefs that the system shouldrefute (_bel.focus) when attempting to change EA's v iew of _bel.
The negat ions of thesebeliefs are then posted by the system as mutua l  beliefs to be achieved in order to carryout the modif icat ion process.
The next step is for the system to select an appropr iateset of evidence to provide as justification for these proposed mutua l  beliefs.6.2 Selecting the Justification for a ClaimStudies in communicat ion  and social psycho logy have shown that evidence improvesthe persuasiveness of a message (Luchok and McCroskey 1978; Reynolds and Burgoon1983; Petty and Cac ioppo 1984; Hample  1985).
Research on the quantity of evidence in-dicates that there is no opt imal  amount  of evidence, but  that the use of high-qual i ty ev-idence is consistent with persuasive ffects (Reinard 1988).
On  the other hand,  Grice's383Computational Linguistics Volume 24, Number 3maxim of quantity (Grice 1975) argues that one should not contribute more informa-tion than is required.
Thus, it is important hat a collaborative agent select sufficientand effective, but not excessive, evidence to justify an intended mutual belief.The first step in selecting the justification for a claim is to identify the alternativepieces of evidence that the system can present o EA.
Since the components of thesepieces of evidence may again need to be justified (Cohen and Perrault 1979), thesealternative choices will be referred to as the candidate justification chains.
The systemwill then select a subset of these justification chains to present o EA.The most important aspect in selecting among these justification chains is that thesystem believes that the selected justification chains will achieve the goal of convincingEA of the claim.
Thus our system first selects the minimum subsets of the candidatejustification chains that are predicted to be sufficient o convince EA of the claim.
Ifmore than one such subset exists, selection heuristics will be applied.
Luchok andMcCroskey (1978) argued that high-quality evidence produces more attitude changethan any other evidence form, suggesting that justification chains for which the systemhas the greatest confidence should be preferred.
This also allows the system to betterjustify the evidence should questions about its validity arise.
Wyer (1970) and Morley(1987) argued that evidence is most persuasive if it is previously unknown to thehearer, suggesting that the system should select evidence that it believes is novel toEA.
19 Finally, Grice's maxim of quantity (Grice 1975) states that one should not make acontribution more informative than is needed; thus the system should select evidencechains that contain the fewest beliefs.Our algorithm Select-Justification (Figure 13) is based on these principles and isinvoked on a claim _rob that the system intends to make.
When justification chainshave been constructed for an antecedent belief _beli and the evidential relationshipbetween _beli and _rob, the algorithm uses a function Make-Evidence to constructa justification chain with _rob as its root node, the root node of _beli-chain as itschild node, and the root node of _reli-chain as the relationship between _beli and _mb(step 2.3).
Thus, Make-Evidence returns a justification chain for _rnb, which includes apiece of evidence that provides direct support for _mb, namely {_beli,_reli}, as well asspecifying how _beli and _reli should be justified.
2?
This justification chain is then addedto _evid-set, which contains alternative justification chains that the system can presentto EA as support for _rob.
The selection criteria discussed earlier are applied to theelements in _evid-set o produce _selected-set.
If _selected-set has only one element,then this justification chain will be selected as support for _rob; if _selected-set hasmore than one element, then a random justification chain will be selected as supportfor _rob; if _selected-set is empty, then no justification chain will be returned, thusindicating that the system does not have sufficient evidence to convince EA of _rob.
21Thus the Select-Justification algorithm returns a justification chain needed to supportan intended mutual belief, whenever possible, based on both the system's prediction ofthe strength of each candidate justification chain as well as a set of heuristics motivatedby research in communication and social psychology.19 Walker (1996b) has shown the importance of IRU's (Inforinationally Redundant  Utterances) in efficientdiscourse.
We leave including appropriate IRU's for future work.20 As can be seen from this construction process, a justification chain can be more than simple chainssuch as A --~ B ~ C. In fact, it can be a complex tree-like structure in which both nodes and l inks arefurther justified.
In our current system, a fact appears mult iple t imes in a justification chain if it is usedto justify more than one claim.21 In practice this should never be the case, because the Select-Focus-Modification algorithm only selectsas focus a set of beliefs that it believes the system can successfully refute.384Chu-Car ro l l  and  Carber ry  Response  Generat ion  in P lann ing  D ia loguesSelect-Justification(_mb):1.
If Predict(_mb, EA's evidence pertaining to _rob + system's claim of _rnb) = accept, return _rob.2.
/* Construct aset of candidate justification chains for _rob */_rob.evidence ~-- system's evidence for _rob_evid-set ~-- {}For each piece of evidence in ~nb.evidence, {_mbi,supports(_mbi,_mb)}:_beli ~-- -mbi~reli ~ supports(_mbi,_mb)2.1 _beli-chain ~-- Select-Jusfification(_beli)2.2 _reli-chain ~-- Select-Jusfification(_reli)2.3 _evid-set ~ _evid-set U Make-Evidence({_beli-chain,_reli-chain},,mb)3.
/* Select justification chains that are strong enough to convince EA of_rob */3.1 If _evid-set = nil; return nil.3.2 _set-size ~-- 13.3 _selected-set ~-- {}3.4 _candidate-set ~-- the set of all sets of justification chains constructed from _evid-set suchthat each element in _candidate-set contains _set-size lements from _evid-setFor each element in _candidate-set, _candl .
.
.
.
.
.
candm:3.4.1 If Predict( rob, EA's evidence pertaining to _rnb + system's claim of _rob +_candi) = accept_selected-set ~-- _selected-set U {_candi}3.5 If _selected-set = {}_set-size ~-- _set-size + 1If _set-size _~ number of elements in _evid-set, goto step 3.4;Else return nil.4.
/* Apply first heuristic */_selected-set ~ evidence in _selected-set about which the system is most confident5.
/* Apply second heuristic */_selected-set ~-~ evidence in _selected-set most novel to EA6.
/* Apply third heuristic */_selected-set ,-- evidence in _selected-set that contains the fewest beliefs7.
Return first element in _selected-setFigure 13Algor i thm for ident i fy ing  just i f icat ion for a belief.6.3 Example of Resolving a Detected ConflictTo illustrate how CORE initiates collaborative negotiation to resolve a detected conflict,consider the following utterances by EA:(22)(23)EA: Dr. Smith isn't the professor of CS821, is he?Isn't Dr. Jones the professor of CS821?In utterances (22)~(23), EA proposes three mutual beliefs: 1) the professor of CS821 isnot Dr. Smith, 2) the professor of CS821 is Dr. Jones, and 3) Dr. Jones being the professorof CS821 provides upport for Dr. Smith not being the professor of CS821.
22 CORE's22 Utterances (22) and (23) are both expressions of doubt.
In the former case, the speaker conveys a strongbut uncertain belief that the professor of CS821 is not Dr. Smith, while in the latter, the speaker conveysa strong but uncertain belief that the professor of CS821 is Dr. Jones (Lambert and Carberry 1992).385Computational Linguistics Volume 24, Number 3evaluation of this proposal is very similar to that discussed in Section 5.1.1, and willnot be repeated here.
The result is that CORE rejects both -~Professor(CS821,Smith) andProfessor(CS821,Jones), but accepts the evidential relationship between them.Since the top-level proposed belief, -~Professor(CS821,Smith) is rejected by CORE,the modification process is invoked.
The Modify-Proposal action specifies that thefocus of modification first be identified.
Thus CORE constructs the candidate foci treeand applies the Select-Focus-Modification algorithm to its root node.
In this example,the candidate foci tree is identical to the proposed belief tree since both the top-levelproposed belief and the evidence proposed to support it were rejected.
The Select-Focus-Modification algorithm (Figure 12) is then invoked on ~Professor(CS821,Smith).The algorithm specifies that the focus of modification for the rejected evidence firstbe determined; thus the algorithm is recursively applied to the rejected child belief,Professor(CS821,Jones) (step 3.1).CORE has two pieces of evidence against Dr. Jones being the professor of CS821:1) a very strong piece of evidence consisting of the beliefs that Dr. Jones is going onsabbatical in 1998 and that professors on sabbatical do not teach courses, and 2) astrong piece of evidence consisting of the beliefs that Dr. Jones' expertise is compilers,that CS821 is a database course, and that professors generally do not teach coursesoutside of their areas of expertise.
CORE predicts that its two pieces of evidence,when presented to EA, will lead EA to accept ~Professor(CS821,Jones); thus the focusof modification for Professor(CS821,Jones) is the belief itself.Having selected the focus of modification for the rejected child belief, CORE se-lects the focus of modification for the top-level proposed belief -~Professor(CS821,Smith).Since the only reason that CORE knows of for EA believing ~Professor(CS821,Smith)is the proposed piece of evidence, it predicts that eliminating EA's belief in the ev-idence would result in EA rejecting ~Professor(CS821,Smith).
T erefore, the focus ofmodification for ~Professor(CS821,Smith) is Professor(CS821,Jones).Once the focus of modification is identified, the subactions of Modify-Proposal reinvoked on the selected focus.
The dialogue model constructed for this modificationprocess is shown in Figure 14.
Since the selected focus is represented by a beliefnode, Correct-Node is selected as the subaction of Modify-Proposal.
To satisfy theprecondition of Modify-Node, CORE posts MB(CA, EA,-~Professor(CS821,Jones)) as amutual belief to be achieved.
CORE then adopts the Inform discourse action to achievethe mutual belief.
Inform has two subactions: Tell which conveys abelief to EA, andAddress-Acceptance, which invokes the Select-Justification algorithm (Figure 13) toselect justification for the intended mutual belief.Since the surface form of EA's utterance in (23) conveyed a strong belief in Pro-fessor(CS821,Jones), CORE predicts that merely informing EA of the negation of thisproposition isnot sufficient to change his belief; therefore CORE constructs justificationchains from the available pieces of evidence.
Figure 15 shows the candidate justificationchains constructed from CORE's two pieces of evidence for ~Professor(CS821,Jones).When constructing the justification chain in Figure 15(a), CORE predicts that merelyinforming EA of On-Sabbatical(Jones,1998) is not sufficient o convince him to acceptthis belief because of EA's previously conveyed strong belief that Dr. Jones will be oncampus in 1998 and the stereotypical belief that being on campus generally impliesnot being on sabbatical.
Thus further evidence is given to support On-Sabbatical(Jones,1998).Given the two alternative justification chains, CORE first selects those that arestrong enough to convince EA to accept ~Professor(CS821,Jones).
If the justificationchain in Figure 15(a) is presented to EA, CORE predicts that EA will have the fol-lowing pieces of evidence pertaining to Professor(CS821,Jones): 1) a strong belief in386Chu-Carroll and Carberry Response Generation in Planning Dialoguesi D ia logue  Mode l  fo r  Ut terances  (22)  - (23)  ,P roposed  Prob lem-So lv ing"  Eeve i  ............... ~" ......................... "r .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I Arbitrate(CA,EA,Proposed.Model) \[\[Evaluate-Proposal(CA,EA,Proposed-Model) \[ I Modify-Proposal~CA,EA,Proposed-Model) \[ itI C?rrect-N?de(CA'EA'-Pr?fess?r(CS 821 'J?nes)'Pr?p?sed-M?del) \[iI Modify-Node(CA,EA,Proposed-Model,~Professor(CS821,Jones)) \] i. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ ,  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,Proposed  Be l ie f  Leve l  , , "7 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
;.
.~MB(CA,EA,-Pr?fess?r(CSS21,J?nes)) I :s/ supports ~- - -~ \[i b~-2::"-.\-.. \] I MB(CA,EA,On-Sabbatical(Jones, 1998))' t , ', supports < .
.
.
.
k .
.
.
.
.~ i MB(CA,EA,Given-Tenure(Jones,1997)) J-~-_ ---~r~ -~x.__~.~,...Discourse  Leve l  ~ ~ ~ x ~ .~ _. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
x .
.
.
-  ~ .~.~__lnform~CA,EA,-Pr fosse or(CS821,Jones)) ', I ~' i ".
- - ' - - "7  ~ , , , i!Tell(CA,EA,-Professor(CS821,J?nes))l IAddress-Acceptance(CA,EA,~Professor(CS821,Jones)) \] ,' ', iThe proJessor ~f CS821 is not Dr. Jones.
~ ."
, ii ;:I Inform(CA,EA,On-Sabbatical(Jones, 1998)) ~- .
.
.
.
- ,,.
.
.
------~ ~ , :\[Tell(CA,EA,On-Sabbatical(Jones,1998)) \[ IAddress'Acceptance(CA,EA,On'Sabbatical(J?nes,1998)) ' \]iDr.
Jones is going on sabbatical in 1998.
~ ;~I In f?nn(CA,EA,Given-Tenure(J?nes, 1997)) l- .
.
.
.
"iI Tell(CA,EA,Given-Tenure(Jones, 1997)) \] "............................................................................. 0 5 d?ny.~ ..w~.~ ~ (vy....ty_+~re.
!
(9__9_7."
.. ............. !Figure 14Dialogue model for utterances (24) to (26).%I\]i II-Professor(CS821,Jones)On-Sabbatical(Jones, 1998)TGiven-Tenure(Jones, 1997)(a) Evidence: very strong-Professor(CS821 ,Jones)TExpertise(Jones,compilers)Content(CS821,database)(b) Evidence: strongFigure 15Alternative justification chains for -~Professor(CS821,Jones).Pro fessor (CS821, Jones) ,  conveyed by  ut terance  (23), 2) a very  s t rong piece of  ev idenceagainst  Professor (CS821, Jones) ,  prov ided  by  CORE's  proposa l  of the belief, 23 and  3) a23 EA should treat this as a very strong piece of evidence since CORE will convey it in a direct statementand is presumed to have very good (although imperfect) knowledge about prospective teachingassigmnents.387Computational Linguistics Volume 24, Number 3very strong piece of evidence against Professor(CS821,Jones), provided by CORE's pro-posed evidence in Figure 15(a).
CORE then predicts that EA will have an overall beliefin -~Professor(CS821,Jones) of trength (very strong, strong).
24 Similarly, CORE predictsthat when the evidence in Figure 15(b) is presented to EA, EA will have a very strongbelief in -~Professor(CS821,Jones).
Hence, both candidate justification chains are pre-dicted to be strong enough to change EA's belief about Professor(CS821,Jones).
Sincemore than one justification chain is produced, the selection heuristics are applied.
Thefirst heuristic prefers justification chains in which CORE is most confident; thus thejustification chain in Figure 15(a) is selected as the evidence that CORE will presentto EA, leading to the generation of the semantic forms of the following utterances:(24)(25)(26)CA: The professor of CS821 is not Dr. Jones.Dr.
Jones is going on sabbatical in 1998.Dr.
Jones was given tenure in 1997.7.
Implementation and Evaluation7.1 System ImplementationWe have implemented a prototype of our conflict resolution system, CORE, for a uni-versity course advisement domain; the implementation was done in Common Lispwith the Common Lisp Object System under SunOS.
CORE realizes the response gen-eration process for conflict resolution by utilizing the response generation strategiesdetailed in this paper.
Given the dialogue model constructed from EA's proposal, itperforms the evaluation and modification processes in our Propose-Evaluate-Modifyframework.
Domain knowledge used by CORE includes 1) knowledge about objectsin the domain, their attributes and corresponding values, such as the professor ofCS681 being Dr. Rogers, 2) knowledge about a hierarchy of concepts in the domain;for instance, computer science can be divided into hardware, software, and theory, and3) knowledge about evidential inference rules in the domain, such as a professor be-ing on sabbatical normally implies that he is not teaching courses.
CORE also makes use ofa model of its beliefs about EA's beliefs.
This knowledge helps CORE tailor its re-sponses to the particular EA by taking into account CORE's beliefs about what EAalready believes.
In addition, CORE maintains a library of generic recipes in order toplan its actions.
In our implementation, CORE has knowledge about 29 distinct objects,14 evidential rules, and 43 domain, problem-solving, and discourse recipes.
Since thefocus of this work is on the evaluation and modification processes that are captured asproblem-solving actions, 25 of the 43 recipes are domain-independent problem-solvingrecipes.CORE takes as input a four-level dialogue model that represents intentions in-ferred from EA's utterances, uch as that in Figure 2.
It then evaluates the proposal todetermine whether to accept he proposal to reject he proposal and attempt to modifyit, or to pursue information-sharing.
As part of the information-sharing and conflictresolution processes, CORE determines the discourse acts that should be adopted torespond to EA's utterances, and generates the semantic forms of the utterances that24 When the strength of a belief is represented as a list of values, it indicates that the net result ofcombining the strengths of all pieces of evidence pertaining to the belief is equivalent to having onepiece of positive evidence of each of the strengths listed.388Chu-Carroll and Carberry Response Generation i  Planning Dialoguesrealize these discourse acts.
Realization of these logical forms as natural languageutterances i discussed in the section on future work.7.2 Evaluation of CORE7.2.1 Methodology.
In order to obtain an initial assessment of the quality of CORE'sresponses, we performed an evaluation to determine whether or not the strategiesadopted by CORE are reasonable strategies that a system should employ when par-ticipating in collaborative planning dialogues and whether other options should beconsidered.
The evaluation, however, was not intended to address the completenessof the types of responses generated by CORE, nor was it intended to be a full scaleevaluation such as would be provided by integrating CORE's strategies into an actualinteractive advisement system.The evaluation was conducted via a questionnaire in which human judges rankedCORE's responses to EA's utterances among a set of alternative responses, and alsorated their level of satisfaction with each individual response.
The questionnaire con-tained a total of five dialogue segments that demonstrated CORE's ability to pur-sue information-sharing and to resolve detected conflicts in the agents' beliefs; otherdialogue segments included in the questionnaire addressed aspects of CORE's per-formance that are not the topic of this paper.
Each dialogue segment was selectedto evaluate a particular algorithm used in the response generation process.
For eachdialogue segment, the judges were given the following information:Input to CORE: this included EA's utterances (for illustrative purposes),the beliefs that would be inferred from each of these utterances and therelationships among them.
In effect, this is a textual description of thebelief level of the dialogue model that would be inferred from EA'sutterances.CORE's relevant knowledge: CORE's knowledge relevant o itsevaluation of each belief given in the input, along with CORE's strengthof belief in each piece of knowledge.Responses: for each dialogue segment, five alternative responses weregiven, one of which was the actual response generated by CORE (theresponses were presented in random order so that the judges were notaware of which response was actually generated by the system).
Theother four responses were obtained by altering CORE's responsegeneration strategies.
For instance, instead of invoking ourSelect-Justification algorithm, an alternative response can be generatedby including every piece of evidence that CORE believes will providesupport for its claim.
Alternatively, the preference for addressing rejectedevidence in Select-Focus-Modification can be altered to allow CORE toconsider directly refuting a parent belief before considering refuting itsrejected child beliefs.Appendix A shows a sample dialogue segment in the questionnaire, annotatedbased on how CORE's response generation mechanism was altered to produce eachof the four alternative responses.
In evaluating alternative responses, the judges wereexplicitly instructed not to pay attention to the phrasing of CORE's responses, butto evaluate the responses based on their conciseness, coherence, and effectiveness,since it was the quality of the content of CORE's responses that was of interest in this389Computational Linguistics Volume 24, Number 3Table 3Evaluation results.Mean of Median of Mean ofCORE's Responses CORE's Responses All Other ResponsesIS1 3.5 4 2.43IS2 3.9 4 2.58CN1 3.0 3 2.85CN2 3.6 4 2.95CN3 3.8 4 2.65(a) Satisfaction RatingMean of Median of Rank ofCORE's Responses CORE's Responses CORE's MeanIS1 2.1 2 2IS2 1.9 2 1CN1 2.9 3 3CN2 2.1 2 2CN3 1.8 2 2(b) Rankingevaluation.
Based on this principle, the judges were asked to rate the five responsesin the following two ways:..Level of Satisfaction: the goal of this portion of the evaluation was toassess the level of satisfaction that a user interacting with CORE is likelyto have based on CORE's responses.
Each alternative response was ratedon a scale of very good, good, fair, poor, and terrible.Ranking: the goal of this ranking was to compare our responsegeneration strategies with other alternative strategies that might beadopted in designing a response generation system.
The judges wereasked to rank in numerical order the five responses based on their orderof preference.Twelve judges, all of whom were undergraduate or graduate students in computerscience or linguistics, were asked to participate in this evaluation; evaluation formswere returned anonymously by 10 judges by the established eadline date.
Note thatthe judges had not been taught about he CORE system and its processing mechanismsprior to the evaluation.7.2.2 Results.
Two sets of results were computed for the judges' level of satisfactionwith CORE's responses, and for the ranking of CORE's responses as compared withthe alternative responses.
The results of our evaluation are shown in Tables 3(a) and3(b).
In order to assess the judges' level of satisfaction with CORE's responses, weassigned a value of I to 5 to each of the satisfaction ratings where I is terrible and 5 isvery good.
The mean and median of CORE's actual response in each dialogue segmentwere then computed, as well as the mean of all alternative r sponses provided for eachdialogue segment, which was used as a basis for comparison.
Table 3(a) shows that inthe two dialogue segments in which CORE initiated information-sharing (IS1 and IS2),the means of CORE's responses are both approximately one level of satisfaction higher390Chu-Carroll and Carberry Response Generation i  Planning DialoguesTable 4Comparison of CORE's responses with other esponses.Evaluate-Belief Select-Focus-Modification Select-JustificationOther Response CORE Other Response CORE Other Response CORECNI.1 reject reject all child N / A N / ACN1.2 reject reject child child all subsetCN2 reject reject main main evidence chain evidenceCN3 reject reject child child subset althan the average score given to all other responses (columns 1 and 3 in Table 3(a)).Furthermore, in both cases the median of the score is 4, indicating that at least half ofthe judges considered CORE's responses to be good or very good.
The three dialoguesegments in which CORE initiated collaborative negotiation (CN1, CN2, and CN3),however, yielded less uniform results.
The means of CORE's responses range frombeing slightly above the average score for other responses to being one satisfactionlevel higher.
However, in two out of the three responses, at least half of the judgesconsidered CORE's responses to be either good or very good.To assess the ranking of CORE's responses as compared with alternative r sponses,we again computed the means and medians of the rankings given to CORE's re-sponses, as well as the mean of the rankings given to each alternative response.
Thefirst column in Table 3(b) shows the mean rankings of CORE's responses.
This set ofresults is consistent with that in Table 3(a) in that the dialogue segments where CORE'sresponses received ahigher mean satisfaction rating also received a lower mean rank-ing (thus indicating ahigher preference).
The last column in Table 3(b) shows how themean of CORE's response in a dialogue segment ranks when compared to the meansof the alternative r sponses in the same dialogue segment.
The second column, on theother hand, shows the medians of the rankings for CORE's responses.
A comparisonof these two columns indicates that they agree in all but one case.
The disagreementoccurs in dialogue IS2; although more than half of the judges consider an alternativeresponse better than CORE's actual response (because the median of CORE's responseis 2), the judges do not agree on what this better esponse is (because the mean ofCORE's response ranks highest among all alternatives).
Thus, CORE's response in IS2can be considered the most preferred response among all judges.Next, we examine the alternative responses that are consistently ranked higherthan CORE's responses in the dialogue segments.
In dialogue IS1, EA proposed amainbelief and provided supporting evidence for it.
CORE initiated information sharingusing the Ask-Why strategy, focusing on an uncertain child belief.
The preferred alter-native response also adopted the Ask-Why strateg.~ but focused on the main belief.We tentatively assumed that this was because of the judges' preference for addressingthe main belief directly instead of being less direct by addressing the uncertain evi-dence.
However, this assumption was shown to be invalid by the result in IS2 wherethe most preferred response (which is CORE's actual response) addresses an uncer-tain child belief.
A factor that further complicates the problem is the fact that EA hasalready proposed evidence to support he main belief in IS1; thus applying Ask-Whyto the main belief would seem to be ineffective.To evaluate our collaborative negotiation strategies, we analyzed the responsesin dialogues CN1, CN2, and CN3 that were ranked higher than CORE's actual re-sponses.
We compared these preferred responses to CORE's responses based on theiragreement on the outcome of the Evaluate-Belief, Select-Focus-Modification, and391Computational Linguistics Volume 24, Number 3Select-Justification processes, as shown in Table 4.
For instance, the second row inthe table shows that the second preferred response in dialogue CN1 (listed as CN1.2)was produced as a result of Evaluate-Belief having rejected the proposal (which is inagreement with CORE), of Select-Focus-Modification having selected a child belief asits focus (again in agreement with CORE), and of Select-Justification having selectedall available vidence to present as justification (as opposed to CORE, which selecteda subset of such evidence).
These results indicate that, in the examples we tested, alljudges agreed with the "outcome of CORE's proposal evaluation mechanism, and inall but one case, the judges agreed with the belief(s) CORE chose to refute.
However,disagreements arose with respect o CORE's process for selecting justification.
In dia-logue CN1.2, the judges preferred providing all available vidence, which may be theresult of one of two assumptions.
First, the judges may believe that providing all avail-able evidence is a better strategy in general, or second, they may have reasoned aboutthe impact that potential pieces of evidence have on EA's beliefs and concluded thatthe subset of evidence that CORE selected is insufficient to convince EA of its claims.In dialogue CN2, the judges preferred a response of the form B ~ A, while COREgenerated a response of the form C --~ B ~ A, even though the judges were explicitlygiven CORE's belief that EA believes -~B.
This result invalidates the second assump-tion above, since if that assumption were true, it is very unlikely that the judges wouldhave concluded that no further evidence for B is needed in this case.
However, thefirst assumption above is also invalidated because an alternative response in dialogueCN2, which enumerated all available pieces of evidence, was ranked second last.
This,along with the fact that in dialogue CN3, the judges preferred a response that includesa subset of the evidence selected by CORE, leads us to conclude that further researchis needed to determine the reasons that led the judges to make seemingly contradic-tory judgments, and how these factors can be incorporated into CORE's algorithmsto improve its performance.
Although the best measure of performance would be toevaluate how our response generation strategies contribute to task success within arobust natural anguage advisement system, which is beyond our current capability,note that CORE's current collaborative negotiation and information-sharing strategiesresult in responses that most of our judges consider concise, coherent, and effective,and thus provide an excellent basis for future work.8.
Discussion8.1 Generality of the ModelThe response generation strategies presented in this paper are independent of the ap-plication domain and can be applied to other collaborative planning applications.
Wewill illustrate the generality of our model by showing how, with appropriate domainknowledge, it can generate the turns of dialogues that have been analyzed by otherresearchers.First, consider the following dialogue segment, where H (a financial advisor) andJ (an advice-seeker) are discussing whether J is eligible for an IRA for 1981 (Walker\[1996a\], in tum taken from Harry Gross Transcripts \[1982\]):(27) H: There's no reason why you shouldn't have an IRA for last year(1981).
(28) J: Well I thought hey just started this year.
(29) H: Oh no.
(30) IRA's were available as long as you are not a participant in anexisting pension.392Chu-Carroll and Carberry Response Generation i  Planning DialoguesSpeaker Belief StrengthH: expert HI: J is eligible for an IRA in 1981 strongH2: IRA is available as long as no pension warrantedJ: advisee Jl: IRA started in 1982 weakJ2: J worked for a company with a pension in 1981 warrantedFigure 16Assumed knowledge of dialogue participants in utterances (27) to (33)(31) J: Oh I see.
(32) Well I did work I do work for a company that has a pension.
(33) H: Ahh.
Then you're not eligible for 81.Let us suppose that H's and J's private beliefs are as shown in Figure 16, whichwe believe to be reasonable assumptions given the roles of the participants and thecontent and form of the utterances in the dialogue.
In utterance (27), H proposes thebelief that J should be eligible for an IRA in 1981.
J's weak belief that IRA's started in1982 resulted in J being uncertain about her acceptance of H's proposal in (27); thusJ initiates information-sharing using the Invite-Attack strategy and presents belief J1in utterance (28).
H rejects J's proposal from (28) because of his warranted belief H2;this rejection is conveyed in (29) and H provides counterevidence in (30).
J acceptsH's modification of her proposal in (31), and re-evaluates H's original proposal fromutterance (27) taking into account he new information from (30).
This leads J to rejectH's original proposal by stating her evidence for rejection in (32).
25 In utterance (33),H accepts J's proposal from utterance (32), and both agents come to agreement that Jis not eligible for an IRA in 1981.As we noted in Section 3.1, Walker classified utterance (28) as a rejection.
Webelieve that our treatment of utterance (28) as conveying uncertainty and initiatinginformation-sharing better accounts for the overall dialogue.
In our model, utter-ances (28)-(31) constitute an information-sharing subdialogue, with utterances (29)-(31) forming an embedded negotiation subdialogue.Next, consider the following dialogue segment between a user and a librarian,from Logan et al (1994):(34) U: I am looking for books on the architecture of Michelangelo.
(35) L: I thought Michelangelo was an artist.
(36) U: He was also an architect.
(37) He designed St. Peter's in Rome.
(38) U: Ok, ...25 Using CORE's current response generation mechanism, it would have explicitly stated its rejection ofthe main belief as follows: I am not eligible for an IRA for last year, since I work for a company that has apension.
However, it will be a very minor alteration to CORE's algorithms to allow for exclusivegeneration of implicit rejection of proposals.
On the other hand, to allow for both implicit and explicitrejection of proposals and to select between them during the generation process requires furtherreasoning, and we leave this for future work.393Computational Linguistics Volume 24, Number 3Here we assume that L has a weak belief that Michelangelo was an artist (L1), anda very strong belief that if a person is an artist, he is not an architect (L2), whileU has a very strong belief that Michelangelo is both an artist and an architect (U1).These beliefs are consistent with those expressed in utterances (34)-(38).
L initiatesinformation-sharing after U's proposal in (34) because of a weak piece of evidenceagainst it, which consists of beliefs L1 and L2; thus in utterance (35) L invites U toaddress her counterevidence.
U accepts Us proposal that Michelangelo was an artist,but rejects the implicit proposal that Michelangelo being an artist implies that he isnot an architect.
Thus U initiates collaborative negotiation by presenting a modifiedbelief in (36) and justifying it in (37), which leads to L accepting these proposed beliefsin (38).8.2 ContributionsAs illustrated by the dialogues in the previous ection, our work provides a domain-independent overall framework for modeling collaborative planning dialogues.
In-stead of treating each proposal as either accepted (and incorporated into the agents'shared plan/beliefs) or rejected (and deleted from the stack of open beliefs), our frame-work allows a proposal to be under negotiation.
Furthermore, this model is recursivein that the Modify action in itself contains a full Propose-Evaluate-Modify cycle, al-lowing the model to capture situations in which embedded negotiation subdialoguesarise in a natural and elegant fashion.Our work also addresses the following two issues: 1) how should the systemgo about determining whether to accept or reject a proposal made by the user, andwhat should it do when it remains uncertain about whether to accept, and 2) whena relevant conflict is detected in a proposal from the user, how should the system goabout resolving the conflict.
Our information-sharing mechanism allows the systemto focus on those beliefs that it believes will most effectively resolve its uncertaintyabout the proposal and to select an appropriate information-sharing strategy.
To ourknowledge, our model is the only response generation system to date that allows thesystem to postpone its decision about the acceptance of a proposal and to initiateinformation-sharing in an attempt o arrive at a decision.In order to address the second issue, we developed a conflict resolution mechanismthat allows the system to initiate collaborative negotiation with the user to resolve theirdisagreement about he proposal.
Our conflict resolution mechanism allows the systemto focus on those beliefs that it believes will most effectively and efficiently resolve theagents' conflict about the proposal and to select what it believes to be sufficient, butnot excessive, evidence to justify its claims.
Logan et al (Logan et al 1994; Cawseyet al 1993) developed a dialogue system that is capable of determining whether ornot evidence should be included to justify rejection of a single proposed belief.
Oursystem improves upon theirs by providing a means of dealing with situations in whichmultiple conflicts arise and those in which multiple pieces of evidence are availableto justify a claim.8.3 Future WorkThere are several directions in which our response generation framework must beextended.
First, we have focused on identifying information-sharing and conflict res-olution strategies for content selection in the response generation process.
For textstructuring, we used the simple strategy of presenting claims before their justification.However, Cohen analyzed argumentative t xts and found variation in the order inwhich claims and their evidence are presented (Cohen 1987).
Furthermore, we do notconsider situations in which a piece of evidence may simultaneously provide support394Chu-CarroU and Carberry Response Generation i  Planning DialoguesA BIX I \C D EFigure 17Example of a belief playing multiple roles.for two claims.
Since text structure can influence coherence and focus, we must inves-tigate appropriate mechanisms for determining the structure of a response containingmultiple propositions.
In addition, we must identify appropriate syntactic forms forexpressing each utterance (such as a surface negative question versus a declarativestatement), identify when cue words should be employed, and use a sentence realizerto produce actual English utterances.Our Select-Justification algorithm assumes that all information known to the usercan be accessed by the user without difficulty during his interaction with the system;thus it prefers selecting evidence that is novel to the user over selecting evidence al-ready known to the user.
However, Walker has argued that, when taking into accountresource limitations and processing costs, effective use of IRU's (informationally re-dundant utterances) can reduce effort during collaborative planning and negotiation(Walker 1996b).
It is thus important o investigate how resource limitations and pro-cessing costs may affect our process for conflict resolution in terms of both the selectionof the belief(s) to address, and the selection of evidence needed to refute the belief(s).In addition, we must investigate when to convey propositions implicitly rather thanexplicitly, as was the case in utterance (32) of the IRA dialogue in Section 8.1.Two assumptions made in this paper regarding the relationships between pro-posed beliefs are 1) proposed beliefs can always be represented in a tree structure, i.e.,each time a belief is proposed, it is intended as support for only one other belief, and2) an agent cannot provide both evidence to support a belief and evidence to attack itin the same turn during the dialogue.
Relaxing the first assumption complicates theselection of focus during both the modification and information-sharing processes.
Forinstance, consider the proposed belief structure in Figure 17.
Suppose that the systemevaluates the proposal and rejects all proposed beliefs A, B, C, D, and E. In select-ing the focus of modification, should the system now prefer addressing D because itsresolution will potentially resolve the conflict about both A and B?
What if D is thebelief which the system has the least amount of evidence against?
We are interestedin investigating how the current algorithms for conflict resolution and information-sharing will need to be modified to accommodate such belief structures.
Relaxing thesecond assumption, on the other hand, affects the evaluation and information-sharingprocesses.
For instance, in the following dialogue segment, he speaker utilizes a gen-eralized version of the Invite-Attack strategy to present evidence both for and againstthe main belief:A: I think Dr. Smith is going on sabbatical next year.I heard he was offered a visiting position at Bell Labs,but then again I heard he's going to be teaching AI next semester.Further research is needed to determine how the current evaluation process shouldbe altered to handle dialogues uch as the above.
In particular, we are interested ininvestigating how uncertainty about a piece of proposed evidence should affect theevaluation of the belief that it is intended to support, as well as how the selection of the395Computational Linguistics Volume 24, Number 3focus of information-sharing should be affected when a single turn can simultaneouslyprovide evidence both for and against a belief.Finally, in our current work, we have focused on task-oriented collaborative plan-ning dialogues where the agents explored only one plan at a time, and have shownhow our Propose-Evaluate-Modify framework is capable of modeling such dialogues.Although in the collaborative planning dialogues we analyzed, this constraint didnot seem to pose any problems, in certain other domains, such as the appointmentscheduling domain, the agents may be more likely to explore several options at onceinstead of focusing on only one option at a time (Ros6 et al 1995).
We are inter-ested in investigating how our Propose-Evaluate-Modify framework can be extendedto account for such discourse with multiple threads.
In particular, we are interestedin finding out whether the Propose-Evaluate-Modify framework should be revised sothat a single instance of the cycle (allowing for recursion) may model such discourse,or whether each thread should be modeled by an instance of the Propose-Evaluate-Modify cycle and an overarching structure developed to model interaction among themultiple cycles.8.4 Concluding RemarksThis paper has presented a model for response generation in collaborative planningdialogues.
Our model improves upon previous response generation systems by spec-ifying strategies for content selection for response generation in order to resolve (po-tential) conflict.
It includes both algorithms for information-sharing when the systemis uncertain about whether to accept a proposal by the user and algorithms for con-flict resolution when the system rejects a proposal.
The overall model is captured ina recursive Propose-Evaluate-Modify framework that can handle embedded subdia-logues.A.
Appendix: Sample Dialogue from Evaluation QuestionnaireIn this section, we include a sample dialogue from the questionnaire given to ourjudges for the evaluation of CORE, discussed in Section 7.2.
The dialogue is annotatedto indicate the primary purpose for its inclusion in the questionnaire, CORE's responsein each dialogue segment, as well as how CORE's response generation strategies aremodified to generate ach alternative response.
These annotations are included ascomments (surrounded by/* and */) and were not available to the judges during theevaluation process.Question 1/* This dialogue corresponds toCN1 in Section 7.2.
The primary purpose of this dialogue segmentis to evaluate the strategies adopted by the Select-Focus-Modification algorithm */Suppose that in previous dialogue, CORE has proposed that the professor of CS481(an AI course) is Dr. Seltzer, and that the user responds by giving the following 4utterances in a single turn:(utt i.i) U: The professor of CS481 is not Dr.
Seltzer.
(utt 1.2) Dr. Seltzer is going on sabbatical in 1998.
(utt 1.3) Dr. Seltzer has been at the university for 6 years.
(utt 1.4) Also, I think Dr. Seltzer's expertise is computer networks.The user's utterances are interpreted as follows:?
Main belief: a strong belief in ~professor(CS481,Seltzer) (utt1.1).396Chu-Carroll and Carberry Response Generation i  Planning Dialogues?
Two pieces of supporting evidence:- -  A strong piece of evidence consisting ofOn-Sabbatical(Seltzer,1998) andsupports(On-Sabbatical(Seltzer,1998),~professor(CS481,Seltzer)) (utt1.2), where On-Sabbatical(Seltzer,1998) is in turn supported by astrong piece of evidence consisting ofBeen-At-Univ(Seltzer,6 years) andsupports( Been-At-Univ( Sel tzer,6 years) ,On- Sabbatical( Seltzer,1998 ) )(utt 1.3).- -  A very weak piece of evidence consisting ofExpertise(Seltzer, Networks) andsupports( Expertise( Seltzer, Networks), ~professor( CS481,Seltzer) )(utt 1.4).CORE's private knowledge that affects its acceptance/rejection of the beliefs proposedin each user utterance (noted in boldface below) is as follows:(Utt 1.1) : a certain belief that the professor of CS481 is Dr. Seltzer.
/* certain =warranted */(Utt 1.2) : (1) a very strong belief that Dr. Seltzer has not been given tenure (andtherefore is not going on sabbatical in 1998), (2) a very-strong belief thatDr.
Seltzer will be going on sabbatical in 1999 (and hence is not goingon sabbatical in 1998), and (3) a strong belief that Dr. Seltzer will be theCS graduate program chair in 1998 (and thus will not go on sabbatical in1998).
(Utt 1.3) : a certain belief that Dr. Seltzer has only been at the university for 5years.
(Utt 1.4) : a very strong belief that Dr. Seltzer's expertise is AI.Based on CORE's beliefs, please judge the following alternative responses tothe user's utterances.
Note again that the criteria for your judgment should be theconciseness, coherence, and effectiveness of the response resolving CORE and theuser's conflict about he main belief (Dr. Seltzer not teaching CS481, in this example).Response 1-1:/* Select-Focus-Modification returns all rejected beliefs; this is response CNI.1 in Table 4 */CORE: Dr. Seltzer is the professor of CS481.Dr.
Seltzer is not going on sabbatical in 1998, since hehas not been given tenure.Furthermore, Dr. Seltzer has only been at the universityfor 5 years.Finally, Dr. Seltzer's expertise is in AI, not computernetworks.Response 1-2:/* CORE's actual response */CORE: Dr. Seltzer is not going on sabbatical in 1998, since hehasn't been given tenure.Also, Dr. Seltzer's expertise is not computer networks.Response 1-3:/* Select-Focus-Modification returns belie/in the lea/node of the tree that can best be refuted,397Computational Linguistics Volume 24, Number 3i.e., algorithm does not infer whether or not refuting a child belief will contribute to refutation ofits parent belief*~CORE: Dr. Seltzer hasn't been at the university for 6 years.Response 1-4:/* Select-Focus-Modification remains unchanged, but Select-Justification returns all avail-able evidence against selected foci; this is response CN1.2 in Table 4 */CORE: Dr. Seltzer is not going on sabbatical in 1998, because hehasn't been given tenure, because he is planning on goingon sabbatical in 1999, and because he is the CS graduateprogram chair in 1998.Also, Dr. Seltzer's expertise is AI, not computer networks.Response 1-5:/* Preference ordering in Select-Focus-Modification is changed to prefer addressing a parentbelief to refuting a child belief*~CORE: Dr. Seltzer is the professor of CS481.AcknowledgmentsWe would like to thank Candy Sidner,Stephanie Elzer, Kathy McCoy, Lance Ram-shaw, and Suzanne Mannes for their help-ful comments and discussions.
We wouldalso like to thank the anonymous review-ers of this special issue for providingmany useful suggestions.
In addition, weare grateful to Rachel Sacher for her helpin recording and transcribing a portionof the dialogues used in this researchand for her assistance in the implemen-tation of CORE.
This material is basedupon work supported by the NationalScience Foundation under Grant No.
IRI-9122026.ReferencesAllen, James F. 1979.
A Plan-Based Approachto Speech Act Recognition.
Ph.D. thesis,University of Toronto.Allen, James.
1991.
Discourse structure inthe TRAINS project.
In Darpa Speech andNatural Language Workshop.Birnbaum, Lawrence, Margot Flowers, andRod McGuire.
1980.
Towards an AI modelof argumentation.
I  Proceedings oftheNational Conference on Artificial Intelligence,pages 313-315.Cawsey, Alison.
1990.
Generatingexplanatory discourse.
In R. Dale,C.
Mellish, and M. Zock, editors, CurrentResearch in Natural Language Generation.Academic Press, chapter 4, pages 75-101.Cawsey, Alison, Julia Galliers, Brian Logan,Steven Reece, and Karen Sparck Jones.1993.
Revising beliefs and intentions: Aunified framework for agent interaction.In The Ninth Biennial Conference oftheSociety for the Study of Artificial Intelligenceand Simulation of Behaviour, pages 130-139.Chu-Carroll, Jennifer.
1996.
A Plan-BasedModel for Response Generation iCollaborative Consultation Dialogues.
Ph.D.thesis, University of Delaware.
Alsoavailable as Department of Computer andInformation Sciences, Laboratories forNLP/AI/HCI, Technical Report 97-01.Chu-Carroll, Jennifer and Sandra Carberry.1994.
A plan-based model for responsegeneration i collaborative task-orienteddialogues.
In Proceedings ofthe TwelfthNational Conference on Artificial Intelligence,pages 799-805.Chu-Carroll, Jennifer and Sandra Carberry.1995a.
Communication for conflictresolution in multi-agent collaborativeplanning.
In Proceedings ofthe FirstInternational Conference on MultiagentSystems, pages 49-56.Chu-Carroll, Jennifer and Sandra Carberry.1995b.
Generating information-sharingsubdialogues in expert-user consultation.In Proceedings ofthe 14th International JointConference on Artificial Intelligence, pages1243-1250.Chu-Carroll, Jennifer and Sandra Carberry.1995c.
Response generation icollaborative negotiation.
In Proceedings ofthe 33rd Annual Meeting, pages 136-143.Association for ComputationalLinguistics.Chu-Carroll, Jennifer and Sandra Carberry.1996.
Conflict detection and resolution incollaborative'planning.
I  IntelligentAgents: Agent Theories, Architectures, andLanguages, Volume II, Springer-Verlag398Chu-Carroll and Carberry Response Generation i  Planning DialoguesLecture Notes.
Springer-Verlag, pages111-126.Chu-Carroll, Jennifer and Sandra Carberry.In press.
Conflict resolution Incollaborative planning dialogues.International Journal of Human-ComputerStudies.Clark, Herbert and Edward Schaefer.
1989.Contributing to Discourse.
CognitiveScience, pages 259-294.Clark, Herbert and Deanna Wilkes-Gibbs.1990.
Referring as a Collaborative Process.In Philip Cohen, Jerry Morgan, andMartha Pollack, editors, Intentions inCommunication.
MIT Press, Cambridge,MA, pages 463-493.Cohen, Paul R. 1985.
Heuristic Reasoningabout Uncertainty: An Art~cial IntelligenceApproach.
Pitman Publishing Company.Cohen, Philip R. and C. Raymond Perrault.1979.
Elements of a plan-based theory ofspeech acts.
Cognitive Science, 3:177-212.Cohen, Robin.
1987.
Analyzing the structureof argumentative discourse.
ComputationalLinguistics, 13(1-2):11-24.Edmonds, Philip G. 1994.
Collaboration onreference to objects that are not mutuallyknown.
In Proceedings ofthe 15thInternational Conference on ComputationalLinguistics, pages 1118-1122.Flowers, Margot and Michael Dyer.
1984.Really arguing with your computer.
InProceedings ofthe National ComputerConference, pages 653-659.Galliers, Julia R. 1992.
Autonomous beliefrevision and communication.
IGardenfors, editor, Belief Revision.Cambridge University Press.Grice, H. Paul.
1975.
Logic andconversation.
In Peter Cole and Jerry L.Morgan, editors, Syntax and Semantics 3:Speech Acts.
Academic Press, Inc., NewYork, pages 41-58.Gross, Derek, James F. Allen, and David R.Traum.
1993.
The TRAINS 91 dialogues.Technical Report TN92-1, Department ofComputer Science, University ofRochester.Grosz, Barbara nd Sarit Kraus.
1996.Collaborative plans for complex groupactions.
Artificial Intelligence, 86(2):269-357.Grosz, Barbara J. and Candace L. Sidner.1990.
Plans for discourse.
In Cohen,Morgan, and Pollack, editors, Intentions inCommunication.
M1T Press, chapter 20,pages 417-444.Hample, Dale.
1985.
Refinements on thecognitive model of argument:Concreteness, involvement and groupscores.
The Western Journal of SpeechCommunication, 49:267-285.Harry Gross Transcripts.
1982.
Transcriptsderived from tapes of the radio talk showHarry Gross: Speaking of your money.Provided by the Dept.
of ComputerScience at the University of Pennsylvania.Heeman, Peter A. and Graeme Hirst.
1995.Collaborating on referring expressions.Computational Linguistics, 21(3):351-382.Lambert, Lynn and Sandra Carberry.
1991.A tripartite plan-based model of dialogue.In Proceedings ofthe 29th Annual Meeting,pages 47-54.
Association forComputational Linguistics.Lambert, Lynn and Sandra Carberry.
1992.Modeling negotiation dialogues.
InProceedings ofthe 30th Annual Meeting,pages 193-200.
Association forComputational Linguistics.Lochbaum, Karen E. 1994.
UsingCollaborative Plans to Model the IntentionalStructure of Discourse.
Ph.D. thesis,Harvard University.Lochbaurn, Karen.
1995.
The use ofknowledge preconditions in languageprocessing.
In Proceedings oftheInternational Joint Conference on ArtificialIntelligence, pages 1260-1266.Logan, Brian, Steven Reece, Alison Cawsey,Julia Galliers, and Karen Sparck Jones.1994.
Belief revision and dialoguemanagement i  information retrieval.Technical Report 339, University ofCambridge, Computer Laboratory.Luchok, Joseph A. and James C. McCroskey.1978.
The effect of quality of evidence onattitude change and source credibility.
TheSouthern Speech Communication Journal,43:371-383.McCoy, Kathleen F. 1988.
Reasoning on ahighlighted user model to respond tomisconceptions.
Computational Linguistics,14(3):52-63.McKeown, Kathleen R. 1985.
Text Generation:Using Discourse Strategies and FocusConstraints to Generate Natural LanguageText.
Cambridge University Press.McKeown, Kathleen R., Myron Wish, andKevin Matthews.
1985.
Tailoringexplanations for the user.
In Proceedings ofthe 9th International Joint Conference onArtiJi"cial Intelligence, pages 794-798, LosAngeles, CA.Moore, Johanna and Cecile Paris.
1993.Planning text for advisory dialogues:Capturing intentional and rhetoricalinformation.
Computational Linguistics,19(4):651-695.Morley, Donald D. 1987.
Subjective messageconstructs: A theory of persuasion.Communication Monographs, 54:183-203.Paris, CEcile L. 1988.
Tailoring objectdescriptions to a user's level of expertise.Computational Linguistics, 14(3):64-78.Petty, Richard E. and John T. Cacioppo.1984.
The effects of involvement on399Computational Linguistics Volume 24, Number 3responses to argument quantity andquality: Central and peripheral routes topersuasion.
Journal of Personality and SocialPsychology, 46(1):69-81.Pollack, Martha E. 1986.
A model of planinference that distinguishes between thebeliefs of actors and observers.
InProceedings ofthe 24th Annual Meeting,pages 207-214.
Association forComputational Linguistics.Quilici, Alex.
1992.
Arguing about planningalternatives.
In Proceedings ofthe 14thInternational Conference on ComputationalLinguistics, pages 906-910.Ramshaw, Lance A.
1991.
A Three-LevelModel for Plan Exploration.
In Proceedingsof the 29th Annual Meeting, pages 36-46,Berkeley, CA.
Association forComputational Linguistics.Raskutti, Bhavani and Ingrid Zukerman.1993.
Eliciting additional informationduring cooperative consultations.
InProceedings ofthe 15th Annual Meeting of theCognitive Science Society.Raskutti, Bhavani and Ingrid Zukerman.1994.
Query and response generationduring information-seeking teractions.In Proceedings ofthe 4th InternationalConference on User Modeling, pages 25-30.Reichman, Rachel.
1981.
Modeling informaldebates.
In Proceedings ofthe 7thInternational Joint Conference on ArtificialIntelligence, pages 19-24.Reinard, John C. 1988.
The empirical studyof the persuasive ffects of evidence, thestatus after fifty years of research.
HumanCommunication Research, 15(1):3-59.Reynolds, Rodney A. and Michael Burgoon.1983.
Belief processing, reasoning, andevidence.
In Bostrom, editor,Communication Yearbook 7.
SagePublications, chapter 4, pages 83-104.RosG Carolyn P., Barbara Di Eugenio,Lori S. Levin, and Carol Van Ess-Dykema.1995.
Discourse processing of dialogueswith multiple threads.
In Proceedings ofthe33rd Annual Meeting, pages 31-38.Association for ComputationalLinguistics.Sarner, Margaret H. and Sandra Carberry.1990.
Tailoring explanations using amultifaceted user model.
In Proceedings ofthe Second International Workshop on UserModels, Honolulu, Hawaii, March.Sidner, Candace L. 1992.
Using discourse tonegotiate in collaborative activity: Anartificial anguage.
In AAAI-92 Workshop:Cooperation Among Heterogeneous IntelligentSystems, pages 121-128.Sidner, Candace L. 1994.
An artificialdiscourse language for collaborativenegotiation.
In Proceedings ofthe TwelfthNational Conference on Artificial Intelligence,pages 814-819.SRI Transcripts.
1992.
Transcripts derivedfrom audiotape conversations made atSRI International, Menlo Park, CA.Prepared by Jacqueline Kowtko under thedirection of Patti Price.Sycara, Katia.
1989.
Argumentation:Planning other agents' plans.
InProceedings ofthe 11th International JointConference on Artificial Intelligence, pages517-523.Traum, David R. 1994.
A ComputationalTheory of Grounding in Natural LanguageConversation.
Ph.D. thesis, University ofRochester.Udel Transcripts.
1995.
Transcripts derivedfrom audiotape conversations made at theUniversity of Delaware.
Recorded andtranscribed by Rachel Sacher.van Beek, Peter, Robin Cohen, and KenSchmidt.
1993.
From plan critiquing toclarification dialogue for cooperativeresponse generation.
ComputationalIntelligence, 9(2):132-154.Walker, Marilyn A.
1992.
Redundancy incollaborative dialogue.
In Proceedings ofthe15th International Conference onComputational Linguistics, pages 345-351.Walker, Marilyn.
1996a.
Inferring acceptanceand rejection in dialog by default rules ofinference.
Language and Speech,39(2-3):265-304.Walker, Marilyn A.
1996b.
The effect ofresource limits and task complexity oncollaborative planning in dialogue.Artificial Intelligence, 85:181-243.Walker, Marilyn and Steve Whittaker.
1990.Mixed initiative in dialogue: Aninvestigation i to discourse segmentation.In Proceedings ofthe 28th Annual Meeting,pages 70-78.
Association forComputational Linguistics.Whittaker, Steve and Phil Stenton.
1988.Cues and control in expert-clientdialogues.
In Proceedings ofthe 26th AnnualMeeting, pages 123-130, Association forComputational Linguistics.Wyer, Jr., Robert S. 1970.
Informationredundancy, inconsistency, and noveltyand their role in impression formation.Journal of Experimental Social Psychology,6:111-127.Young, R. Michael, Johanna D. Moore, andMartha E. Pollack.
1994.
Towards aprincipled representation f discourseplans.
In Proceedings ofthe Sixteenth AnnualMeeting of the Cognitive Science Society,pages 946-951.Zukerman, Ingrid and Richard McConachy.1993.
Generating concise discourse thataddresses a user's inferences.
InProceedings ofthe 1993 International JointConference on Artificial Intelligence.400
