UNSUPERVISED WORD SENSE D ISAMBIGUATIONR IVAL ING SUPERVISED METHODSDav id  YarowskyDepar tment  of Computer  and In format ion ScienceUnivers i ty of Pennsy lvaniaPhi ladelphia,  PA 19104, USAyarowsky~unagi, ci s. upenn, eduAbst rac tThis paper presents an unsupervised learn-ing algorithm for sense disambiguationthat, when trained on unannotated Englishtext, rivals the performance of supervisedtechniques that require time-consuminghand annotations.
The algorithm is basedon two powerful constraints - that wordstend to have one sense per discourse andone sense per collocation - exploited in aniterative bootstrapping procedure.
Testedaccuracy exceeds 96%.1 In t roduct ionThis paper presents an unsupervised algorithm thatcan accurately disambiguate word senses in a large,completely untagged corpus) The algorithm avoidsthe need for costly hand-tagged training data by ex-ploiting two powerful properties of human language:1.
One sense per collocation: 2 Nearby wordsprovide strong and consistent clues to the senseof a target word, conditional on relative dis-tance, order and syntactic relationship.2.
One sense per discourse: The sense of a tar-get word is highly consistent within any givendocument.Moreover, language is highly redundant, so thatthe sense of a word is effectively overdetermined by(1) and (2) above.
The algorithm uses these prop-erties to incrementally identify collocations for tar-get senses of a word, given a few seed collocations1Note that the problem here is sense disambiguation:assigning each instance of a word to established sensedefinitions (such as in a dictionary).
This differs fromsense induction: using distributional similarity to parti-tion word instances into clusters that may have no rela-tion to standard sense partitions.2Here I use the traditional dictionary definition ofcollocation - "appearing in the same location; a juxta-position of words".
No idiomatic or non-compositionalinterpretation is implied.for each sense, This procedure is robust and self-correcting, and exhibits many strengths of super-vised approaches, including sensitivity to word-orderinformation lost in earlier unsupervised algorithms.2 One  Sense  Per  D iscourseThe observation that words strongly tend to exhibitonly one sense in a given discourse or document wasstated and quantified in Gale, Church and Yarowsky(1992).
Yet to date, the full power of this propertyhas not been exploited for sense disambiguation.The work reported here is the first to take advan-tage of this regularity in conjunction with separatemodels of local context for each word.
Importantly,I do not use one-sense-per-discourse as a hard con-straint; it affects the classification probabilisticallyand can be overridden when local evidence is strong.In this current work, the one-sense-per-discoursehypothesis was tested on a set of 37,232 examples(hand-tagged over a period of 3 years), the samedata studied in the disambiguation experiments.
Forthese words, the table below measures the claim'saccuracy (when the word occurs more than once ina discourse, how often it takes on the majority sensefor the discourse) and applicability (how often theword does occur more than once in a discourse).The one-sense-per-discourse hypothesis:WordplanttankpoachpalmaxessakebassspacemotioncraneSensesliving/factoryvehicle/contnrsteal/boiltree/handgrid/toolsbenefit/drinkfish/musicvolume/outerlegal/physicalbird/machineAverageAccuracy99.8 %99.6 %100.0 %99.8 %I00.0 %100.0 %100.0 %99.2 %99.9 %100.0 %99.8 %Applicblty72.8 %50.5 %44.4 %38.5 %35.5 %33.7 %58.8 %67.7 %49.8 %49.1%50.1%Clearly, the claim holds with very high reliabilityfor these words, and may be confidently exploited189as another source of evidence in sense tagging.
33 One Sense  Per  Co l locat ionThe strong tendency for words to exhibit only onesense in a given collocation was observed and quan-tified in (Yarowsky, 1993).
This effect varies de-pending on the type of collocation.
It is strongestfor immediately adjacent collocations, and weakenswith distance.
It is much stronger for words in apredicate-argument relationship than for arbitraryassociations at equivalent distance.
It is very muchstronger for collocations with content words thanthose with function words.
4 In general, the high reli-ability of this behavior (in excess of 97% for adjacentcontent words, for example) makes it an extremelyuseful property for sense disambiguation.A supervised algorithm based on this property isgiven in (Yarowsky, 1994).
Using a decisien listcontrol structure based on (Rivest, 1987), this al-gorithm integrates a wide diversity of potential ev-idence sources (lemmas, inflected forms, parts ofspeech and arbitrary word classes) in a wide di-versity of positional relationships (including localand distant collocations, trigram sequences, andpredicate-argument association).
The training pro-cedure computes the word-sense probability distri-butions for all such collocations, and orders them byr 0 /Pr(SenseAlColloeationi~x 5 the log-likelihood ratio ~ gt prISenseBlColloeationi~),with optional steps for interpolation and pruning.New data are classified by using the single mostpredictive piece of disambiguating evidence that ap-pears in the target context.
By not combining prob-abilities, this decision-list approach avoids the prob-lematic complex modeling of statistical dependencies3It is interesting to speculate on the reasons for thisphenomenon.
Most of the tendency is statistical: twodistinct arbitrary terms of moderate corpus frequencyaxe quite unlikely to co-occur in the same discoursewhether they are homographs or not.
This is particu-larly true for content words, which exhibit a "bursty"distribution.
However, it appears that human writersalso have some active tendency to avoid mixing senseswithin a discourse.
In a small study, homograph pairswere observed to co-occur oughly 5 times less often thanarbitrary word pairs of comparable frequency.
Regard-less of origin, this phenomenon is strong enough to beof significant practical use as an additional probabilisticdisambiguation constraint.4This latter effect is actually a continuous functionconditional on the burstiness of the word (the tendencyof a word to deviate from a constant Poisson distributionin a corpus).SAs most ratios involve a 0 for some observed value,smoothing is crucial.
The process employed here is sen-sitive to variables including the type of collocation (ad-jacent bigrams or wider context), coliocational distance,type of word (content word vs. function word) and theexpected amount of noise in the training data.
Detailsaxe provided in (Yarowsky, to appear).encountered in other frameworks.
The algorithm isespecially well suited for utilizing a large set of highlynon-independent evidence such as found here.
Ingeneral, the decision-list algorithm is well suited forthe task of sense disambiguation and will be used as .a component of the unsupervised algorithm below.4 Unsuperv ised  Learn ing  A lgor i thmWords not only tend to occur in collocations thatreliably indicate their sense, they tend to occur inmultiple such collocations.
This provides a mecha-nism for bootstrapping a sense tagger.
If one beginswith a small set of seed examples representative oftwo senses of a word, one can incrementally aug-ment these seed examples with additional examplesof each sense, using a combination of the one-sense-per-collocation and one-sense-per-discourse tenden-cies.Although several algorithms can accomplish sim-ilar ends, 6 the following approach has the advan-tages of simplicity and the ability to build on anexisting supervised classification algorithm withoutmodification.
~ As shown empirically, it also exhibitsconsiderable effectiveness.The algorithm will be illustrated by the disam-biguation of 7538 instances of the polysemous wordplant in a previously untagged corpus.STEP  1:In a large corpus, identify all examples of the givenpolysemous word, storing their contexts as lines inan initially untagged training set.
For example:Sense??????????????????????
?T ra in ing  Examples  (Keyword  in Context )... company  sa id  the  plant is st i l l  operat ingA l though thousands  o f  plant and an ima l  spec ies... zona l  d i s t r ibut ion  o f  plant l i fe .
.
.
.... to  s t ra in  mic roscop ic  plant l i fe f rom the  ...v inyl  ch lo r ide  monomer  plant, which  is ...and  Go lg i  apparatus  o f  plant and an ima l  cel ls... computer  d isk  dr ive  plant l ocated  in ...... d iv ide  life in to  plant and an ima l  k ingdom... c lose -up  s tud ies  o f  plant l i fe and  natura l... N i ssan  car  and  t ruck  plant in Japan  is ...... keep  a manufactur ing... mo lecu les  found  in... un ion  responses  to... an ima l  ra ther  than... many  dangers  tocompany manufacturing... g rowth  o f  aquat icautomated  manufactur ing... An ima l  andd i scovered  at  a St.  Lou isplant pro f i tab le  w i thoutplant and an ima l  t issueplant c losures  .
.
.
.plant t i ssues  can  beplant and an ima l  lifeplant is in Or lando  ...plant l i fe in water  ...plant in F remont  ,plant l i fe a re  de l i ca te lyplant manufactur ingcomputer  manufactur ing  plant and ad jacent  ...... the  pro l i fe ra t ion  o f  plant and an ima l  l ife?Including variants of the EM algorithm (Bantu,1972; Dempster et al, 1977), especially as applied inGale, Church and Yarowsky (1994).7Indeed, any supervised classification algorithm thatreturns probabilities with its classifications may poten-tially be used here.
These include Bayesian classifiers(Mosteller and Wallace, 1964) and some implementa-tions of neural nets, but not BrK!
rules (Brill, 1993).190STEP  2:For each possible sense of the word, identify a rel-atively small number of training examples represen-tative of that sense, s This could be accomplishedby hand tagging a subset of the training sentences.However, I avoid this laborious procedure by iden-tifying a small number of seed collocations repre-sentative of each sense and then tagging all train-ing examples containing the seed collocates with theseed's sense label.
The remainder of the examples(typically 85-98%) constitute an untagged residual.Several strategies for identifying seeds that requireminimal or no human participation are discussed inSection 5.In the example below, the words life and manufac-turing are used as seed collocations for the two majorsenses of plant (labeled A and B respectively).
Thispartitions the training set into 82 examples of livingplants (1%), 106 examples of manufacturing plants(1%), and 7350 residual examples (98%).Sense  Tra in ing  ExamplesA used  to  s t ra in  mic roscop icA ... zona l  d i s t r ibut ion  o fA c lose -up  s tud ies  o fA too  rap id  growth  o f  aquat icA ... the  pro l i fe ra t ion  ofA es tab l i shment  phase  o f  theA ... that  d iv ide  l i fe  in toA ... many  dangers  toA mammals  .
An ima l  andA beds  too  sa l ty  to  suppor tA heavy  seas,  damage , andA?
... v iny l  ch lo r ide  monomer?
... mo lecu les  found  in?
... N i ssan  car  and  truck?
... and  Go lg i  apparatus  o f?
... un ion  responses  to???
... cell types  found  in the?
... company  sa id  the?
... A l though thousands  o f?
... an ima l  ra ther  than?
... computer  d isk  dr ive?
(Keyword  in Context )plant l i f e  f rom the  ...plant l i f e  .
.
.
.plant l i f e  and  natura l  ...plant l i f e  in water  ...plant and an ima l  l l fe  ...plant v i rus  l i fe  cyc le  ...plant and an ima l  k ingdomplant and an ima l  l i fe  ...plant l i f e  a re  de l i ca te lyplant l i f e  .
R iver  ...plant l i f e  g rowing  on ...plant, which  is ...plant and an ima l  t i ssueplant in Japan  is ...plant and an ima l  celia ...plant c losures  .
.
.
.plant k ingdom are  ...plant is st i l l  operat ing  ...plant and an ima l  spec iesplant t i ssues  can  be  ...plant l ocated  in ...S .
.
.
.
.
.
.
.B automated  manufactur ing  plant in F remont  ...B ... vas t  manufactur ing  plant and d is t r ibut ion  ...B chemica l  manufactur ing  plant, produc ing  v i scoseB ... keep  a manufactur ing  plant pro f i tab le  w i thoutB computer  manufactur ing  plant and ad jacent  ...B d i scovered  at  a St.  Lou is  plant manufactur ingB ... copper  manufactur ing  plant found  that  theyB copper  w i re  manufactur ing  plant, for  example  ...B 's cement  manufactur ing  plant in A lpena  ...B po lys ty rene  manufactur ing  plant at  i ts  Dew ...B company  manufactur ing  plant is in Or lando  ...It is useful to visualize the process of seed de-velopment graphically.
The following figure illus-trates this sample initial state.
Circled regions arethe training examples that contain either an A or Bseed collocate.
The bulk of the sample points "?
"constitute the untagged residual.SFor the purposes of exposition, I will assume abinarysense partition.
It is straightforward to extend this to ksenses using k sets of seeds.?
_?
?
_ ?
7 ?
?
"t ?z  .71 ?
?
???
?
, ?
t ??
??
?
??
??
?7  ??
A A AAA ?
?
7 ??
?7  77?
?
?
~ AAAAA A A A ???
?
?
??
?
??
?A A AA A AAAA AA ??
?
???
??
A AAA A A AA ?
?
7 ?
~ A ~  ?
?77  ?
?777 ?
?
?
??
??
?7 ?
7?
?
- -  ?
?
????
????
?
??
?
?
??
?
??77?
?
??
??
??
?
??
?
??7??
?
??
?
?
?
??
?
?
?
?
?
?77  ????
???
?
?
?
77 ?
??
??
?
?
?~ 4 ??
?
?7??
77  77 ?
?
7777 ??
?
?~ 7 ?
7 77  ??
77, ,  ,7 , , ,7  77 , ,  7 ,~: ;~ 7 77777 77 ,-7717 ?77?7  7777 77777 ?
77 97  77 77 ??
r  77 7 77 77 77 7 ?7  7 7?777 77  ?
77~ ~ : .
, _ : .
.
: .
f f .
?
.
; : .7 .
: .
: : .
: .
: .~ .
.
,  .
.
.
.
.
.
7. .
.
.
.
: .
.
.
.
: .
, .
; .7777 7 777~ ~777~7 7 ~777 77777~?~77 77~7 77 77 77 ~ 77 77 77 7 7 7~ 77 7 ,  7 77 ,7 7v 7 7 ,7 7 77 77 , , ,  ?
77 7' 77 ,~,  '7 '77  7 ,777  , ,  7 7 7 7 ,7 7 ,7  7 7 ,7  7 7 77 7777 77 77 , ,?
?
77 ?
7 ?
??
?
7777 77 7 7777 7 7 77 7 7 77 ?
?7I 7 ~ 7 v ~ ~7 ~ v I ~?
7 '~7 7 7 7 ?
?
?
7 7 7 ?
7 ~,I ? "
7 7 77  7 ,~ 7"?
7 77  77 77 7 ~ , ?
- 'I 7 ~'?77 77 : ,?
7777 77 : ,7 777  7 7?
7?777 ?
7 7 7 7 77?
?
77 7 77 7 77 77 ?7 7 77 7 7 ?
?
77 7 ?
7 7 7 1~ 77 ?
7?I '  7 7 , '  ?7 .
7 7 ~,i,~o,.o,~,'.
~ : .
- -~7 ~ ~7.
I ?
?
7~ ?"
77 I , Sl ?
??
?My ?
7?
?
7 ??
777?
7 t77777?
?
77  7 ?7 ?7  ?
?
~F igure  1: Sample  In i t ia l  S ta teA = SENSE-A training exampleB = SENSE-B training example.~ur rent ly  unclassified training example\[ Life \] = Set of training examples containing thecollocation "life".STEP  3a:Train the supervised classification algorithm onthe SENSE-A/SENSE-B seed sets.
The decision-list al-gorithm used here (Yarowsky, 1994) identifies othercollocations that reliably partition the seed trainingdata, ranked by the purity of the distribution.
Be-low is an abbreviated example of the decision listtrained on the plant seed data.
9In i t ia l  decis ion l ist for plant (abbrev ia ted)LogL8.107.587.397.206.274.704.394.304.103.523.483.45Collocation Senseplant l ife =~ Amanufactur ing  plant ~ Bl ife (within 4-2-10 words) ~ Amanufactur ing  (in 4-2-10 words) =~ Banimal (within -I-2-10 words) =~ Aequipment (within -1-2-10 words) =?, Bemployee (within 4-2-10 words) =~ Bassembly plant ~ Bplant closure =~ Bplant species =~ Aautomate (within 4-2-10 words) ::~ Bmicroscopic plant ~ A9Note that a given collocate such as life may appearmultiple times in the list in different collocations1 re-lationships, including left-adjacent, right-adjacent, co-occurrence at other positions in a +k-word window andvarious other syntactic associations.
Different positionsoften yield substantially different likelihood ratios and incases such as pesticide plant vs. plant pesticide indicateentirely different classifications.191STEP 3b:Apply the resulting classifier to the entire sam-ple set.
Take those members in the residual thatare tagged as SENSE-A or SENSE-B with proba-bility above a certain threshold, and add thoseexamples to the growing seed sets.
Using thedecision-list algorithm, these additions will containnewly-learned collocations that are reliably indica-tive of the previously-trained seed sets.
The acquisi-tion of additional partitioning collocations from co-occurrence with previously-identified ones is illus-trated in the lower portion of Figure 2.STEP 3c:Optionally, the one-sense-per-discourse constraintis then used both to filter and augment this addition.The details of this process are discussed in Section 7.In brief, if several instances of the polysemous wordin a discourse have already been assigned SENSE-A ,this sense tag may be extended to all examples inthe discourse, conditional on the relative numbersand the probabilities associated with the tagged ex-amples.Label ing prev ious ly  untagged contextsusing the one-sense-per-discourse proper tyChange Disc .in tag  Numb.~.
-~ A 724A --* A 724?
--* A 724A --* A 348A --* A 348?
--* A i 348?
--* A 348Tra in ing  Examples  ( f rom same d iscourse)... the existence of plant and an ima l  l ife ...... c lass i f ied  as either plant or  an ima l  ...A l thoul~h bacter ia l  and  plant cells are  enc losed... the  l ife o f  the plant, producing stem... an  aspect  o f  plant l i fe , for  example... t i ssues  ; because plant egg cells havephotosynthes is ,  and  so plant growth  is a t tunedThis augmentation f the training data can oftenform a bridge to new collocations that may not oth-erwise co-occur in the same nearby context with pre-viously identified collocations.
Such a bridge to theSENSE-A collocate "cell" is illustrated graphically inthe upper half of Figure 2.Similarly, the one-sense-per-discourse constraintmay also be used to correct erroneously abeled ex-amples.
For example:Er ror  Cor rec t ion  us ing  the  one-sense-per -d i scourse  proper tyChange Disc .in tag  Numb.A ---* A 525A ---* A 525A ---* A 525B ~ A 525"l~raining Examples  ( f rom same d iscourse)conta ins  a var ied  plant and an ima l  lifethe  most  common plant l ife , the  ...s l ight  w i th in  Arc t i c  plant spec ies  ...are protected by plant par ts  remain ing  f rom?
?
L/re -A ' "  a " AA . '
?
~?
?77  ?? '
' ?
?
???
IrX'li'~A . '
" .^^~At22~f~- -P .
, ,~ :~ ' lM l~o~w~opic  I ?
1'?'
~??
,?
? '
?
; : ,?
??
?
?
??
?
??,^~-*~'.
,/2"~A=,I ,~: ' - ;  , , ,  , ,  , , ,L~I I  I ?
3?
?
?
?2 '  ?
???
?? '
t  " "  ? "
?????
?
?~77 ? '
t  ?
?~?-777 ????
?
77 ?
?7 ?77 ?
?
re  ?
??
?
??
?
??
?
77?
,7???
??
??
: .
: .
: .
, .
'.
.
.
.
.
.
.
: .
.
.
.
: .
.
.
.
.
.
'.:?
??
?
2?
?
???
?27  ?
?
?
??
??
27  ?
?
??
?
??
?????
?
?
?
?
?
?
???
?
7 "7 7 1Eouimr~nt I .
- \ [  a~l~ B u %~.~i,.lL.~ B -n~|.
; , ?
- ?~??
?
???
?
'~ .~.f: ' l~,, ,~/m,, = D B ~h l  ?
?
'~ B~-.I  I?
??
?
?
?
??
?
?
'7'., ?Figure 2: Sample In termediate  State(following Steps 3b and 3c)STEP 4:Stop.
When the training parameters are held con-stant, the algorithm will converge on a stable resid-ual set.Note that most training examples will exhibit mul-tiple collocations indicative of the same sense (as il-lustrated in Figure 3).
The decision list algorithmresolves any conflicts by using only the single mostreliable piece of evidence, not a combination of allmatching collocations.
This circumvents many ofthe problemz associated with non-independent evi-dence sources.STEP 3d:Repeat Step 3 iteratively.
The training sets (e.g.SENSE-A  seeds plus newly added examples) will tendto grow, while the residual will tend to shrink.
Addi-tional details aimed at correcting and avoiding mis-classifications will be discussed in Section 6.
F igure 3: Sample Final State192STEP 5:The classification procedure l arned from the finalsupervised training step may now be applied to newdata, and used to annotate the original untaggedcorpus with sense tags and probabilities.An abbreviated sample of the final decision listfor plant is given below.
Note that the original seedwords are no longer at the top of the list.
They havebeen displaced by more broadly applicable colloca-tions that better partition the newly learned classes.In cases where there are multiple seeds, it is evenpossible for an original seed for SENSE-A to becomean indicator for SENSE-B if the collocate ismore com-patible with this second class.
Thus the noise intro-duced by a few irrelevant or misleading seed wordsis not fatal.
It may be corrected if the majority ofthe seeds forms a coherent collocation space.Final decision list for plant (abbreviated)LogL Collocation Sense10.12 plant growth :=~ A9.68 car (within q-k words) =~ B9.64 plant height ~ A9.61 union (within 4-k words) =~ B9.54 equipment (within +k words) =?, B9.51 assembly plant ~ B9.50 nuclear plant =~ B9.31 flower (within =t:k words) =~ A9.24 job (within q-k words) =~ B9.03 fruit (within :t:k words) =?, A9.02 plant species =~ AWhen this decision list is applied to a new test sen-tence,... the loss of animal and plant species throughextinction .. .
,the highest ranking collocation found in the targetcontext (species) is used to classify the example asSENSW-A (a living plant).
If available, informationfrom other occurrences of "plant" in the discoursemay override this classification, as described in Sec-tion 7.5 Options for Training SeedsThe algorithm should begin with seed words thataccurately and productively distinguish the possiblesenses.
Such seed words can be selected by any ofthe following strategies:?
Use words in dict ionary definitionsExtract seed words from a dictionary's entry forthe target sense.
This can be done automati-cally, using words that occur with significantlygreater frequency in the entry relative to theentire dictionary.
Words in the entry appearingin the most reliable collocational relationshipswith the target word are given the most weight,based on the criteria given in Yarowsky (1993).Use a single defining collocate for eachclassRemarkably good performance may be achievedby identifying a single defining collocate for eachclass (e.g.
bird and machine for the word crane),and using for seeds only those contexts contain-ing one of these words.
WordNet (Miller, 1990)is an automatic source for such defining terms.Label salient corpus collocatesWords that co-occur with the target word inunusually great frequency, especially in certaincollocational relationships, will tend to be reli-able indicators of one of the target word's enses(e.g.
\]lock and bulldozer for "crane").
A humanjudge must decide which one, but this can bedone very quickly (typically under 2 minutes fora full list of 30-60 such words).
Co-occurrenceanalysis elects collocates that span the spacewith minimal overlap, optimizing the efforts ofthe human assistant.
While not fully automatic,this approach yields rich and highly reliable seedsets with minimal work.6 Escaping from InitialMisclassificationsUnlike many previous bootstrapping approaches, thepresent algorithm can escape from initial misclassi-fication.
Examples added to the the growing seedsets remain there only as long as the probability ofthe classification stays above the threshold.
IIf theirclassification begins to waver because new exampleshave discredited the crucial collocate, they are re-turned to the residual and may later be classified if-ferently.
Thus contexts that are added to the wrongseed set because of a misleading word in a dictionarydefinition may be (and typically are) correctly re-classified as iterative training proceeds.
The redun-dancy of language with respect o collocation makesthe process primarily self-correcting.
However, cer-tain strong collocates may become ntrenched as in-dicators for the wrong class.
We discourage such be-havior in the training algorithm by two techniques:1) incrementally increasing the width of the contextwindow after intermediate convergence (which peri-odically adds new feature values to shake up the sys-tem) and 2) randomly perturbing the class-inclusionthreshold, similar to simulated annealing.7 Using the One-sense-per-discoursePropertyThe algorithm performs well using only local col-locational information, treating each token of thetarget word independently.
However, accuracy canbe improved by also exploiting the fact that all oc-currences of a word in the discourse are likely toexhibit the same sense.
This property may be uti-lized in two places, either once at the end of Step193\[ (1) I (2)WordplantspacetankmotionbasspalmpoachaxesdutydrugsakecraneAVG(3) 1 (4)  (5)%Samp.
Major SupvsdSenses Size Sense Algrtmliving/factory 7538 53.1 97.7volume/outer 5745 50.7 93.9vehicle/container 11420 58.2 97.1legal/physical 11968 57.5 98.0fish/music 1859 56.1 97.8tree/hand 1572 74.9 96.5steal/boil 585 84.6 97.1grid/tools 1344 71.8 95.5tax/obligation 1280 50.0 93.7medicine/narcotic 1380 50.0 93.0benefit/drink 407 82.8 96.3bird/machine 2145 78.0 96.63936 63.9 96.1(6) 1(7)Seed TrainingTwo Dict.Words Defn.97.1 97.389.1 92.394.2 94.693.5 97.496.6 97.293.9 94.796.6 97.294.0 94.390.4 92.190.4 91.459.6 95.892.3 93.690.6 94.8I (8) (9) 1(1?)
II (11)Options (7) + OSPDTop End Each SchiitzeColls.
only Iter.
Algrthm97.6 98.3 98.6 9293.5 93.3 93.6 9095.8 96.1 96.5 9597.4 97.8 97.9 9297.7 98.5 98.895.8 95.5 95.9 -97.7 98.4 98.5 -94.7 96.8 97.0 -93.2 93.9 94.1 -92.6 93.3 93.9 -96.1 96.1 97.5 -94.2 95.4 95.595.5 96.1 96.5 92.24 after the algorithm has converged, or in Step 3cafter each iteration.At the end of Step 4, this property is used forerror correction.
When a polysemous word such asplant occurs multiple times in a discourse, tokensthat were tagged by the algorithm with low con-fidence using local collocation information may beoverridden by the dominant ag for the discourse.The probability differentials necessary for such a re-classification were determined empirically in an earlypilot study.
The variables in this decision are the to-tal number of occurrences of plant in the discourse(n), the number of occurrences assigned to the ma-jority and minor senses for the discourse, and thecumulative scores for both (a sum of log-likelihoodratios).
If cumulative evidence for the majority senseexceeds that of the minority by a threshold (condi-tional on n), the minority cases are relabeled.
Thecase n = 2 does not admit much reclassification be-cause it is unclear which sense is dominant.
But forn > 4, all but the most confident local classificationstend to be overridden by the dominant tag, becauseof the overwhelming strength of the one-sense-per-discourse tendency.The use of this property after each iteration issimilar to the final post-hoe application, but helpsprevent initially mistagged collocates from gaining afoothold.
The major difference is that in discourseswhere there is substantial disagreement concerningwhich is the dominant sense, all instances in thediscourse are returned to the residual rather thanmerely leaving their current tags unchanged.
Thishelps improve the purity of the training data.The fundamental limitation of this property iscoverage.
As noted in Section 2, half of the exam-ples occur in a discourse where there are no otherinstances of the same word to provide corroboratingevidence for a sense or to protect against misclas-sification.
There is additional hope for these cases,however, as such isolated tokens tend to strongly fa-vor a particular sense (the less "bursty" one).
Wehave yet to use this additional information.8 Eva luat ionThe words used in this evaluation were randomlyselected from those previously studied in the litera-ture.
They include words where sense differences arerealized as differences in French translation (drug--* drogue/m~dicament, andduty --~ devoir/droit),a verb (poach) and words used in Schiitze's 1992disambiguation experiments (tank, space, motion,plant) J ?The data were extracted from a 460 million wordcorpus containing news articles, scientific abstracts,spoken transcripts, and novels, and almost certainlyconstitute the largest training/testing sets used inthe sense-disambiguation l terature.Columns 6-8 illustrate differences in seed trainingoptions.
Using only two words as seeds does surpris-ingly well (90.6 %).
This approach is least success-ful for senses with a complex concept space, whichcannot be adequately represented by single words.Using the salient words of a dictionary definition asseeds increases the coverage of the concept space, im-proving accuracy (94.8%).
However, spurious wordsin example sentences can be a source of noise.
Quickhand tagging of a list of algorithmically-identifiedsalient collocates appears to be worth the effort, dueto the increa3ed accuracy (95.5%) and minimal cost.Columns 9 and 10 illustrate the effect of addingthe probabilistic one-sense-per-discourse constraintto collocation-based models using dictionary entriesas training seeds.
Column 9 shows its effectiveness1?The number of words studied has been limited hereby the highly time-consuming constraint that full handtagging is necessary for direct comparison with super-vised training.194as a post-hoc onstraint.
Although apparently smallin absolute terms, on average this represents a 27%reduction in error rate.
11 When applied at each iter-ation, this process reduces the training noise, yield-ing the optimal observed accuracy in column 10.Comparat ive performance:Column 5 shows the relative performance of su-pervised training using the decision list algorithm,applied to the same data and not using any discourseinformation.
Unsupervised training using the addi-tional one-sense-per-discourse constraint frequentlyexceeds this value.
Column 11 shows the perfor-mance of Schiitze's unsupervised algorithm appliedto some of these words, trained on a New York TimesNews Service corpus.
Our algorithm exceeds this ac-curacy on each word, with an average relative per-formance of 97% vs. 92%.
1~9 Comparison with Previous WorkThis algorithm exhibits a fundamental dvantageover supervised learning algorithms (including Black(1988), Hearst (1991), Gale et al (1992), Yarowsky(1993, 1994), Leacock et al (1993), Bruce andWiebe (1994), and Lehman (1994)), as it does not re-quire costly hand-tagged training sets.
It thrives onraw, unannotated monolingual corpora - the morethe merrier.
Although there is some hope from usingaligned bilingual corpora as training data for super-vised algorithms (Brown et al, 1991), this approachsuffers from both the limited availability of such cor-pora, and the frequent failure of bilingual translationdifferences to model monolingual sense differences.The use of dictionary definitions as an optionalseed for the unsupervised algorithm stems from along history of dictionary-based approaches, includ-ing Lesk (1986), Guthrie et al (1991), Veronis andIde (1990), and Slator (1991).
Although these ear-lier approaches have used often sophisticated mea-sures of overlap with dictionary definitions, theyhave not realized the potential for combining the rel-atively limited seed information i  such definitionswith the nearly unlimited co-occurrence informationextractable from text corpora.Other unsupervised methods have shown greatpromise.
Dagan and Itai (1994) have proposed amethod using co-occurrence statistics in indepen-dent monolingual corpora of two languages to guidelexical choice in machine translation.
Translationof a Hebrew verb-object pair such as lahtom (signor seal) and h. oze (contract or treaty) is determinedusing the most probable combination of words inan English monolingual corpus.
This work shows11The maximum possible rror rate reduction is 50.1%,or the mean applicability discussed in Section 2.12This difference is even more striking given thatSchiitze's data exhibit a higher baseline probability (65%vs.
55%) for these words, and hence constitute an easiertask.that leveraging bilingual exicons and monolinguallanguage models can overcome the need for alignedbilingual corpora.Hearst (1991) proposed an early application ofbootstrapping to augment raining sets for a su-pervised sense tagger.
She trained her fully super-vised algorithm on hand-labelled sentences, appliedthe result to new data and added the most con-fidently tagged examples to the training set.
Re-grettably, this algorithm was only described in twosentences and was not developed further.
Our cur-rent work differs by eliminating the need for hand-labelled training data entirely and by the joint use ofcollocation and discourse constraints to accomplishthis.Schiitze (1992) has pioneered work in the hier-archical clustering of word senses.
In his disam-biguation experiments, Schiitze used post-hoc align-ment of clusters to word senses.
Because the top-level cluster partitions based purely on distributionalinformation do not necessarily align with standardsense distinctions, he generated up to 10 sense clus-ters and manually assigned each to a fixed sense label(based on the hand-inspection f 10-20 sentences percluster).
In contrast, our algorithm uses automati-cally acquired seeds to tie the sense partitions to thedesired standard at the beginning, where it can bemost useful as an anchor and guide.In addition, Schiitze performs his classificationsby treating documents as a large unordered bag ofwords.
By doing so he loses many important dis-tinctions, such as collocational distance, word se-quence and the existence of predicate-argument r la-tionships between words.
In contrast, our algorithmmodels these properties carefully, adding consider-able discriminating power lost in other relatively im-poverished models of language.10 ConclusionIn essence, our algorithm works by harnessing sev-eral powerful, empirically-observed properties of lan-guage, namely the strong tendency for words to ex-hibit only one sense per collocation and per dis-course.
It attempts to derive maximal leverage fromthese properties by modeling arich diversity of collo-cational relationships.
It thus uses more discriminat-ing information than available to algorithms treatingdocuments as bags of words, ignoring relative posi-tion and sequence.
Indeed, one of the strengths ofthis work is that it is sensitive to a wider range oflanguage detail than typically captured in statisticalsense-disambiguation algorithms.Also, for an unsupervised algorithm it works sur-prisingly well, directly outperforming Schiitze's un-supervised algorithm 96.7 % to 92.2 %, on a testof the same 4 words.
More impressively, it achievesnearly the same performance as the supervised al-gorithm given identical training contexts (95.5 %195vs.
96.1%) , and in some cases actually achievessuperior performance when using the one-sense-per-discourse constraint (96.5 % vs. 96.1%).
This wouldindicate that the cost of a large sense-tagged train-ing corpus may not be necessary to achieve accurateword-sense disambiguation.AcknowledgementsThis work was partially supported by an NDSEG Fel-lowship, ARPA grant N00014-90-J-1863 and ARO grantDAAL 03-89-C0031 PRI.
The author is also affiliatedwith the Information Principles Research Center AT&TBell Laboratories, and greatly appreciates the use of itsresources in support of this work.
He would like to thankJason Eisner, Mitch Marcus, Mark Liberman, AlisonMackey, Dan Melamed and Lyle Ungar for their valu-able comments.Re ferencesBaum, L.E., "An Inequality and Associated Maximiza-tion Technique in Statistical Estimation of Probabilis-tic Functions of a Markov Process," Inequalities, v 3,pp 1-8, 1972.Black, Ezra, "An Experiment in Computational Discrim-ination of English Word Senses," in IBM Journal ofResearch and Development, v 232, pp 185-194, 1988.BriU, Eric, "A Corpus-Based Approach to LanguageLearning," Ph.D. Thesis, University of Pennsylvania,1993.Brown, Peter, Stephen Della Pietra, Vincent DellaPietra, and Robert Mercer, "Word Sense Disambigua-tion using Statistical Methods," Proceedings of the29th Annual Meeting of the Association for Compu-tational Linguistics, pp 264-270, 1991.Bruce, Rebecca nd Janyce Wiebe, "Word-Sense Disam-biguation Using Decomposable Models," in Proceed-ings of the 32nd Annual Meeting of the Associationfor Computational Linguistics, Las Cruces, NM, 1994.Church, K.W., "A Stochastic Parts Program an NounPhrase Parser for Unrestricted Text," in Proceeding,IEEE International Conference on Acoustics, Speechand Signal Processing, Glasgow, 1989.Dagan, Ido and Alon Itai, "Word Sense DisambiguationUsing a Second Language Monolingual Corpus", Com-putational Linguistics, v 20, pp 563-596, 1994.Dempster, A.P., Laird, N.M, and Rubin, D.B., "Maxi-mum Likelihood From Incomplete Data via the EMAlgorithm," Journal of the Royal Statistical Society,v 39, pp 1-38, 1977.Gale, W., K. Church, and D. Yarowsky, "A Methodfor Disambiguating Word Senses in a Large Corpus,"Computers and the Humanities, 26, pp 415-439, 1992.Gale, W., K. Church, and D. Yarowsky.
"Discrimina-tion Decisions for 100,000-Dimensional Spaces."
In A.Zampoli, N. Calzolari and M. Palmer (eds.
), CurrentIssues in Computational Linguistics: In Honour ofDon Walker, Kluwer Academic Publishers, pp.
429-450, 1994.Guthrie, J., L. Guthrie, Y. Wilks and H. Aidinejad,"Subject Dependent Co-occurrence and Word SenseDisambiguation," in Proceedings of the 29th AnnualMeeting of the Association for Computational Linguis-tics, pp 146-152, 1991.Hearst, Marti, "Noun Homograph Disambiguation Us-ing Local Context in Large Text Corpora," in UsingCorpora, University of Waterloo, Waterloo, Ontario,1991.Leacock, Claudia, Geoffrey Towell and Ellen Voorhees"Corpus-Based Statistical Sense Resolution," in Pro-ceedings, ARPA Human Language Technology Work-shop, 1993.Lehman, Jill Fain, "Toward the Essential Nature of Sta-tistical Knowledge in Sense Resolution", in Proceed-ings of the Twelfth National Conference on ArtificialIntelligence, pp 734-471, 1994.Lesk, Michael, "Automatic Sense Disambiguation: Howto tell a Pine Cone from an Ice Cream Cone," Pro-ceeding of the 1986 SIGDOC Conference, Associationfor Computing Machinery, New York, 1986.Miller, George, "WordNet: An On-Line LexicalDatabase," International Journal of Lexicography, 3,4, 1990.Mosteller, Frederick, and David Wallace, Inference andDisputed Authorship: The Federalist, Addison-Wesley,Reading, Massachusetts, 1964.Rivest, R. L., "Learning Decision Lists," in MachineLearning, 2, pp 229-246, 1987.Schiitze, Hinrich, "Dimensions of Meaning," in Proceed-ings of Supercomputing '92, 1992.Slator, Brian, "Using Context for Sense Preference," inText-Based Intelligent Systems: Current Research inText Analysis, Information Extraction and Retrieval,P.S.
Jacobs, ed., GE Research and Development Cen-ter, Schenectady, New York, 1990.Veronis, Jean and Nancy Ide, "Word Sense Disam-biguation with Very Large Neural Networks Extractedfrom Machine Readable Dictionaries," in Proceedings,COLING-90, pp 389-394, 1990.Yarowsky, David "Word-Sense Disambiguation UsingStatistical Models of Roget's Categories Trained onLarge Corpora," in Proceedings, COLING-92, Nantes,France, 1992.Yaxowsky, David, "One Sense Per Collocation," in Pro-ceedings, ARPA Human Language Technology Work-shop, Princeton, 1993.Yarowsky, David, "Decision Lists for Lexical Ambigu-ity Resolution: Application to Accent Restoration inSpanish and French," in Proceedings of the 32nd An-nual Meeting of the Association .for ComputationalLinguistics, Las Cruces, NM, 1994.Yarowsky, David.
"Homograph Disambiguation inSpeech Synthesis."
In J. Hirschberg, R. Sproat andJ.
van Santen (eds.
), Progress in Speech Synthesis,Springer-Verlag, to appear.196
