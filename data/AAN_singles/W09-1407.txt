Proceedings of the Workshop on BioNLP: Shared Task, pages 50?58,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsHigh-precision biological event extraction with a concept recognizerK.
Bretonnel Cohen?, Karin Verspoor?, Helen L. Johnson, Chris Roeder,Philip V. Ogren, William A. Baumgartner Jr., Elizabeth White, Hannah Tipney, and Lawrence HunterCenter for Computational PharmacologyUniversity of Colorado Denver School of MedicinePO Box 6511, MS 8303, Aurora, CO 80045 USAkevin.cohen@gmail.com, karin.verspoor@ucdenver.edu, helen.linguist@gmail.com,chris.roeder@ucdenver.edu, philip@ogren.info, william.baumgartner@ucdenver.edu,elizabeth.white@colorado.edu, hannah.tipney@ucdenver.edu, larry.hunter@ucdenver.eduAbstractWe approached the problems of event detec-tion, argument identification, and negation andspeculation detection as one of concept recog-nition and analysis.
Our methodology in-volved using the OpenDMAP semantic parserwith manually-written rules.
We achievedstate-of-the-art precision for two of the threetasks, scoring the highest of 24 teams at pre-cision of 71.81 on Task 1 and the highest of 6teams at precision of 70.97 on Task 2.The OpenDMAP system and the rule set areavailable at bionlp.sourceforge.net.
*These two authors contributed equally to thepaper.1 IntroductionWe approached the problem of biomedical eventrecognition as one of concept recognition and anal-ysis.
Concept analysis is the process of taking atextual input and building from it an abstract rep-resentation of the concepts that are reflected in it.Concept recognition can be equivalent to the namedentity recognition task when it is limited to locat-ing mentions of particular semantic types in text, orit can be more abstract when it is focused on recog-nizing predicative relationships, e.g.
events and theirparticipants.2 BioNLP?09 Shared TaskOur system was entered into all three of theBioNLP?09 (Kim et al, 2009) shared tasks:?
Event detection and characterization Thistask requires recognition of 9 basic biologicalevents: gene expression, transcription, proteincatabolism, protein localization, binding, phos-phorylation, regulation, positive regulation andnegative regulation.
It requires identificationof the core THEME and/or CAUSE participantsin the event, i.e.
the protein(s) being produced,broken down, bound, regulated, etc.?
Event argument recognition This task buildson the previous task, adding in additional argu-ments of the events, such as the site (protein orDNA region) of a binding event, or the locationof a protein in a localization event.?
Recognition of negations and speculationsThis task requires identification of negations ofevents (e.g.
event X did not occur), and specu-lation about events (e.g.
We claim that event Xshould occur).3 Our approachWe used the OpenDMAP system developed at theUniversity of Colorado School of Medicine (Hunteret al, 2008) for our submission to the BioNLP?09 Shared Task on Event Extraction.
OpenDMAPis an ontology-driven, integrated concept analysissystem that supports information extraction fromtext through the use of patterns represented in aclassic form of ?semantic grammar,?
freely mixingtext literals, semantically typed basal syntactic con-stituents, and semantically defined classes of enti-ties.
Our approach is to take advantage of the high50quality ontologies available in the biomedical do-main to formally define entities, events, and con-straints on slots within events and to develop pat-terns for how concepts can be expressed in text thattake advantage of both semantic and linguistic char-acteristics of the text.
We manually built patterns foreach event type by examining the training data andby using native speaker intuitions about likely waysof expressing relationships, similar to the techniquedescribed in (Cohen et al, 2004).
The patterns char-acterize the linguistic expression of that event andidentify the arguments (participants) of the eventsaccording to (a) occurrence in a relevant linguisticcontext and (b) satisfaction of appropriate semanticconstraints, as defined by our ontology.
Our solutionresults in very high precision information extraction,although the current rule set has limited recall.3.1 The reference ontologyThe central organizing structure of an OpenDMAPproject is an ontology.
We built the ontologyfor this project by combining elements of severalcommunity-consensus ontologies?the Gene Ontol-ogy (GO), Cell Type Ontology (CTO), BRENDATissue Ontology (BTO), Foundational Model ofAnatomy (FMA), Cell Cycle Ontology (CCO), andSequence Ontology (SO)?and a small number ofadditional concepts to represent task-specific aspectsof the system, such as event trigger words.
Combin-ing the ontologies was done with the Prompt plug-infor Prote?ge?.The ontology included concepts representing eachevent type.
These were represented as frames, withslots for the various things that needed to be re-turned by the system?the trigger word and the var-ious slot fillers.
All slot fillers were constrained tobe concepts in some community-consensus ontol-ogy.
The core event arguments were constrained inthe ontology to be of type protein from the SequenceOntology (except in the case of regulation events,where biological events themselves could satisfy theTHEME role), while the type of the other event argu-ments varied.
For instance, the ATLOC argumentof a gene expression event was constrained to beone of tissue (from BTO), cell type (from CTO), orcellular component (from GO-Cellular Component),while the BINDING argument of a binding event wasconstrained to be one of binding site, DNA, domain,or chromosome (all from the SO and all tagged byLingPipe).
Table 1 lists the various types.3.2 Named entity recognitionFor proteins, we used the gold standard annota-tions provided by the organizers.
For other seman-tic classes, we constructed a compound named en-tity recognition system which consists of a LingPipeGENIA tagging module (LingPipe, (Alias-i, 2008)),and several dictionary look-up modules.
The dictio-nary lookup was done using a component from theUIMA (IBM, 2009; Ferrucci and Lally, 2004) sand-box called the ConceptMapper.We loaded the ConceptMapper with dictionar-ies derived from several ontologies, including theGene Ontology Cellular Component branch, CellType Ontology, BRENDA Tissue Ontology, andthe Sequence Ontology.
The dictionaries containedthe names and name variants for each concept ineach ontology, and matches in the input documentswere annotated with the relevant concept ID for thematch.
The only modifications that we made tothese community-consensus ontologies were to re-move the single concept cell from the Cell Type On-tology and to add the synonym nuclear to the GeneOntology Cell Component concept nucleus.The protein annotations were used to constrain thetext entities that could satisfy the THEME role in theevents of interest.
The other named entities wereadded for the identification of non-core event partic-ipants for Task 2.3.3 Pattern development strategies3.3.1 Corpus analysisUsing a tool that we developed for visualizing thetraining data (described below), a subset of the gold-standard annotations were grouped by event typeand by trigger word type (nominalization, passiveverb, active verb, or multiword phrase).
This orga-nization helped to suggest the argument structures ofthe event predicates and also highlighted the varia-tion within argument structures.
It also showed thenature of more extensive intervening text that wouldneed to be handled for the patterns to achieve higherrecall.Based on this corpus analysis, patterns were de-veloped manually using an iterative process in whichindividual patterns or groups of patterns were tested51Table 1: Semantic restrictions on Task 2 event arguments.
CCO = Cell Cycle Ontology, FMA = Foundational Modelof Anatomy, other ontologies identified in the text.Event Type Site AtLoc ToLocbinding protein domain (SO),binding site (SO), DNA(SO), chromosome (SO)gene expression gene (SO), biologicalentity (CCO)tissue (BTO), cell type(CTO), cellular compo-nent (GO)localization cellular component(GO)cellular component(GO)phosphorylation amino acid (FMA),polypeptide region (SO)protein catabolism cellular component(GO)transcription gene (SO), biologicalentity (CCO)on the training data to determine their impact on per-formance.
Pattern writers started with the most fre-quent trigger words and argument structures.3.3.2 Trigger wordsIn the training data, we were provided annotationsof all relevant event types occurring in the trainingdocuments.
These annotations included a triggerword specifying the specific word in the input textwhich indicated the occurrence of each event.
Weutilized the trigger words in the training set as an-chors for our linguistic patterns.
We built patternsaround the generic concept of, e.g.
an expressiontrigger word and then varied the actual strings thatwere allowed to satisfy that concept.
We then ran ex-periments with our patterns and these varying sets oftrigger words for each event type, discarding thosethat degraded system performance when evaluatedwith respect to the gold standard annotations.Most often a trigger word was removed from anevent type trigger list because it was also a trig-ger word for another event type and therefore re-duced performance by increasing the false positiverate.
For example, the trigger words ?level?
and?levels?
appear in the training data trigger word listsof gene expression, transcription, and all three regu-lation event types.The selection of trigger words was guided by afrequency analysis of the trigger words provided inthe task training data.
In a post-hoc analysis, we findthat a different proportion of the set of trigger wordswas finally chosen for each different event type.
Be-tween 10-20% of the top frequency-ranked triggerwords were used for simple event types, with theexception that phosphorylation trigger words werechosen from the top 30%.
For instance, for gene ex-pression all of the top 15 most frequent trigger wordswere used (corresponding to the top 16%).
For com-plex event types (the regulations) better performancewas achieved by limiting the list to between 5-10%of the most frequent trigger words.In addition, variants of frequent trigger wordswere included.
For instance, the nominalization ?ex-pression?
is the most frequent gene expression trig-ger word and the verbal inflections ?expressed?
and?express?
are also in the top 20%.
The verbal inflec-tion ?expresses?
is ranked lower than the top 30%,but was nonetheless included as a trigger word in thegene expression patterns.3.3.3 PatternsAs in our previous publications on OpenDMAP,we refer to our semantic rules as patterns.
Forthis task, each pattern has at a minimum an eventargument THEME and an event-specific triggerword.
For example, {phosphorylation} :=52[phosphorylation nominalization][Theme],where [phosphorylization nominalization]represents a trigger word.
Both elements are definedsemantically.
Event THEMEs are constrained byrestrictions placed on them in the ontology, asdescribed above.The methodology for creating complex event pat-terns such as regulation was the same as for sim-ple events, with the exception that the THEMEswere defined in the ontology to also include bio-logical processes.
Iterative pattern writing and test-ing was a little more arduous because these pat-terns relied on the success of the simple event pat-terns, and hence more in-depth analysis was re-quired to perform performance-increasing patternadjustments.
For further details on the pattern lan-guage, the reader is referred to (Hunter et al, 2008).3.3.4 NominalizationsNominalizations were very frequent in the train-ing data; for seven out of nine event types, the mostcommon trigger word was a nominalization.
In writ-ing our grammars, we focused on these nominaliza-tions.
To write grammars for nominalizations, wecapitalized on some of the insights from (Cohen etal., 2008).
Non-ellided (or otherwise absent) argu-ments of nominalizations can occur in three basicpositions:?
Within the noun phrase, after the nominaliza-tion, typically in a prepositional phrase?
Within the noun phrase, immediately precedingthe nominalization?
External to the noun phraseThe first of these is the most straightforward tohandle in a rule-based approach.
This is particu-larly true in the case of a task definition like thatof BioNLP ?09, which focused on themes, since anexamination of the training data showed that whenthemes were post-nominal in a prepositional phrase,then that phrase was most commonly headed by of.The second of these is somewhat more challeng-ing.
This is because both agents and themes canoccur immediately before the nominalization, e.g.phenobarbital induction (induction by phenobarbi-tal) and trkA expression (expression of trkA).
To de-cide how to handle pre-nominal arguments, we madeuse of the data on semantic roles and syntactic posi-tion found in (Cohen et al, 2008).
That study foundthat themes outnumbered agents in the prenominalposition by a ratio of 2.5 to 1.
Based on this obser-vation, we assigned pre-nominal arguments to thetheme role.Noun-phrase-external arguments are the mostchallenging, both for automatic processing and forhuman interpreters; one of the major problems isto differentiate between situations where they arepresent but outside of the noun phrase, and situationswhere they are entirely absent.
Since the current im-plementation of OpenDMAP does not have robustaccess to syntactic structure, our only recourse forhandling these arguments was through wildcards,and since they mostly decreased precision without acorresponding increase in recall, we did not attemptto capture them.3.3.5 Negation and speculationCorpus analysis of the training set revealed twobroad categories each for negation and speculationmodifications, all of which can be described in termsof the scope of modification.NegationBroadly speaking, an event itself can be negatedor some aspect of an event can be negated.
In otherwords, the scope of a negation modification can beover the existence of an event (first example below),or over an argument of an existing event (second ex-ample).?
This failure to degrade IkappaBalpha ...(PMID 10087185)?
AP-1 but not NF-IL-6 DNA binding activity ...(PMID 10233875)Patterns were written to handle both types ofnegation.
The negation phrases ?but not?
and ?butneither?
were appended to event patterns to catchthose events that were negated as a result of anegated argument.
For event negation, a more ex-tensive list of trigger words was used that includedverbal phrases such as ?failure to?
and ?absence of.
?The search for negated events was conducted intwo passes.
Events for which negation cues fall out-side the span of text that stretches from argument to53event trigger word were handled concurrently withthe search for events.
A second search was con-ducted on extracted events for negation cues that fellwithin the argument to event trigger word span, suchas.
.
.
IL-2 does not induce I kappa B alpha degrada-tion (PMID 10092783)This second pass allowed us to capture one addi-tional negation (6 rather than 5) on the test data.SpeculationThe two types of speculation in the training datacan be described by the distinction between ?de re?and ?de dicto?
assertions.
The ?de dicto?
assertionsof speculation in the training data are modificationsthat call into question the degree of known truth ofan event, as in.
.
.
CTLA-4 ligation did not appear to affect theCD28 - mediated stabilization (PMID 10029815)The ?de re?
speculation address the potential ex-istence of an event rather that its degree of truth.
Inthese cases, the event is often being introduced intext by a statement of intention to study the event, asin.
.
.
we investigated CTCF expression.
.
.
[10037138]To address these distinct types of speculation, twosets of trigger words were developed.
One set con-sisted largely of verbs denoting research activities,e.g.
research, study, examine investigate, etc.
Theother set consisted of verbs and adverbs that denoteuncertainty, and included trigger words such as sug-gests, unknown, and seems.3.4 Handling of coordinationCoordination was handled using the OpenNLP con-stituent parser along with the UIMA wrappers thatthey provide via their code repository.
We choseOpenNLP because it is easy to train a model, it in-tegrates easily into a UIMA pipeline, and becauseof competitive parsing results as reported by Buyko(Buyko et al, 2006).
The parser was trained using500 abstracts from the beta version of the GENIAtreebank and 10 full-text articles from the CRAFTcorpus (Verspoor et al, In press).
From the con-stituent parse we extracted coordination structuresinto a simplified data structure that captures eachconjunction along with its conjuncts.
These wereprovided to downstream components.
The coordi-nation component achieves an F-score of 74.6% atthe token level and an F-score of 57.5% at the con-junct level when evaluated against GENIA.
For bothmeasures the recall was higher than the precision by4% and 8%, respectively.We utilized the coordination analysis to identifyevents in which the THEME argument was expressedas a conjoined noun phrase.
These were assumed tohave a distributed reading and were post-processedto create an individual event involving each con-junct, and further filtered to only include given (A1)protein references.
So, for instance, analysis of thesentence in the example below should result in thedetection of three separate gene expression events,involving the proteins HLA-DR, CD86, and CD40,respectively.NAC was shown to down-regulate theproduction of cytokines by DC as wellas their surface expression of HLA-DR, CD86 (B7-2), and CD40 molecules.
.
.
(PMID 10072497)3.5 Software infrastructureWe took advantage of our existing infrastructurebased on UIMA (The Unstructured InformationManagement Architecture) (IBM, 2009; Ferrucciand Lally, 2004) to support text processing and dataanalysis.3.5.1 Development toolsWe developed a visualization tool to enable thelinguistic pattern writers to better analyze the train-ing data.
This tool shows the source text one sen-tence at a time with the annotated words highlighted.A list following each sentence shows details of theannotations.3.6 Errors in the training dataIn some cases, there were discrepancies between thetraining data and the official problem definitions.This was a source of problems in the pattern devel-opment phase.
For example, phosphorylation eventsare defined in the task definition as having only aTHEME and a SITE.
However, there were instancesin the training data that included both a THEME anda CAUSE argument.
When those events were identi-fied by our system and the CAUSE was labelled, they54were rejected during a syntactic error check by thetest server.4 Results4.1 Official ResultsWe are listed as Team 13.
Table 2 shows our re-sults on the official metrics.
Our precision was thehighest achieved by any group for Task 1 and Task2, at 71.81 for Task 1 and 70.97 for task 2.
Our re-calls were much lower and adversely impacted ourF-measure; ranked by F-measure, we ranked 19thout of 24 groups.We noted that our results for the exact match met-ric and for the approximate match metric were veryclose, suggesting that our techniques for named en-tity recognition and for recognizing trigger wordsare doing a good job of capturing the appropriatespans.4.2 Other analysis: Bug fixes and coordinationhandlingIn addition to our official results, we also report inTable 3 (see last page) the results of a run in whichwe fixed a number of bugs.
This represents our cur-rent best estimate of our performance.
The precisiondrops from 71.81 for Task 1 to 67.19, and from 70.97for Task 2 to 65.74, but these precisions are stillwell above the second-highest precisions of 62.21for Task 1 and 56.87 for Task 2.
As the table shows,we had corresponding small increases in our recallto 17.38 and in our F-measure to 27.62 for Task 1,and in our recall to 17.07 and F-measure to 27.10 forTask 2.We evaluated the effects of coordination handlingby doing separate runs with and without this ele-ment of the processing pipeline.
Compared to ourunofficial results, which had an overall F-measurefor Task 1 of 27.62 and for Task 2 of 27.10, a ver-sion of the system without handling of coordinationhad an overall F-measure for Task 1 of 24.72 and forTask 2 of 24.21.4.3 Error Analysis4.3.1 False negativesTo better understand the causes of our low recall,we performed a detailed error analysis of false neg-atives using the devtest data.
(Note that this sectionincludes a very small number of examples from thedevtest data.)
We found five major causes of falsenegatives:?
Intervening material between trigger words andarguments?
Coordination that was not handled by our coor-dination component?
Low coverage of trigger words?
Anaphora and coreference?
Appositive gene names and symbolsIntervening material For reasons that we detailin the Discussion section, we avoided the use ofwildcards.
This, and the lack of syntactic analy-sis in the version of the system that we used (notethat syntactic analyses can be incorporated into anOpenDMAP workflow), meant that if there was textintervening between a trigger word and an argument,e.g.
in to efficiently [express] in developing thymo-cytes a mutant form of the [NF-kappa B inhibitor](PMID 10092801), where the bracketed text is thetrigger word and the argument, our pattern wouldnot match.Unhandled coordination Our coordination systemonly handled coordinated protein names.
Thus, incases where other important elements of the utter-ance, such as the trigger word transcription in tran-scription and subsequent synthesis and secretionof galectin-3 (PMID 8623933) were in coordinatedstructures, we missed the relevant event arguments.Low coverage of trigger words As we discuss inthe Methods section, we did not attempt to coverall trigger words, in part because some less-frequenttrigger words were involved in multiple event types,in part because some of them were extremely low-frequency and we did not want to overfit to the train-ing data, and in part due to the time constraints of theshared task.Anaphora and coreference Recognition of someevents in the data would require the ability to doanaphora and coreference resolution.
For example,in Although 2 early lytic transcripts, [BZLF1] and[BHRF1], were also detected in 13 and 10 cases,respectively, the lack of ZEBRA staining in any caseindicates that these lytic transcripts are most likely55Tasks 1 and 3 Task 2Event class GS answer R P F R P FLocalization 174 (18) 18 (18) 10.34 100.00 18.75 9.77 94.44 17.71Binding 347 (44) 110 (44) 12.68 40.00 19.26 12.32 39.09 18.74Gene expression 722 (263) 306 (263) 36.43 85.95 51.17 36.43 85.95 51.17Transcription 137 (18) 20 (18) 13.14 90.00 22.93 13.14 90.00 22.93Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00Phosphorylation 135 (30) 30 (30) 22.22 100.00 36.36 20.14 93.33 33.14EVENT TOTAL 1529 (377) 490 (377) 24.66 76.94 37.35 24.30 76.12 36.84Regulation 291 (9) 19 (9) 3.09 47.37 5.81 3.08 47.37 5.79Positive regulation 983 (32) 65 (32) 3.26 49.23 6.11 3.24 49.23 6.08Negative regulation 379 (10) 22 (10) 2.64 45.45 4.99 2.37 40.91 4.49REGULATION TOTAL 1653 (51) 106 (51) 3.09 48.11 5.80 3.02 47.17 5.67Negation 227 (4) 76 (4) 1.76 5.26 2.64Speculation 208 (14) 105 (14) 6.73 13.33 8.95MODIFICATION TOTAL 435 (18) 181 (18) 4.14 9.94 5.84ALL TOTAL 3182 (428) 596 (428) 13.45 71.81 22.66 13.25 70.97 22.33Table 2: Official scores for Tasks 1 and 2, and modification scores only for Task 3, from the approximate spanmatching/approximate recursive matching table.
GS = gold standard (true positives) (given for Tasks 1/3 only), answer= all responses (true positives) (given for tasks 1/3 only), R = recall, P = precision, F = F-measure.
All results are ascalculated by the official scoring application.
[expressed] by rare cells in the biopsies enteringlytic cycle (PMID 8903467), where the bracketedtext is the arguments and the trigger word, the syn-tactic object of the verb is the anaphoric noun phrasethese lytic transcripts, so even with the addition ofa syntactic component to our system, we still wouldnot have recognized the appropriate arguments with-out the ability to do anaphora resolution.Appositives The annotation guidelines for proteinsapparently specified that when a gene name waspresent in an appositive with its symbol, the symbolwas selected as the gold-standard argument.
For thisreason, in examples like [expression] of Fas ligand[FasL] (PMID 10092076), where the bracketed textis the trigger word and the argument, the gene nameconstituted intervening material from the perspec-tive of our patterns, which therefore did not match.We return to a discussion of recall and its implica-tions for systems like ours in the Discussion section.4.3.2 False positivesAlthough our overall rate of false positives waslow, we sampled 45 false positive events distributedacross the nine event types and reviewed them witha biologist.We noted two main causes of error.
The mostcommon was that we misidentified a slot filler orwere missing a slot filler completely for an actualevent.
The other main reason for false positives waswhen we erroneously identified a (non)event.
Forexample, in coexpression of NF-kappa B/Rel andSp1 transcription factors (PMID 7479915), we mis-takenly identified Sp1 transcription as an event.5 DiscussionOur results demonstrate that it is possible to achievestate-of-the art precision over a broad range of tasksand event types using our approach of manuallyconstructed, ontologically typed rules?our preci-sion of 71.81 on Task 1 was ten points higher thanthe second-highest precision (62.21), and our preci-sion of 70.97 on Task 2 was 14 points higher thanthe second-highest precision (56.87).
It remains thecase that our recall was low enough to drop our F-measure considerably.
Will it be the case that a sys-tem like ours can scale to practical performance lev-els nonetheless?
Four factors suggest that it can.The first is that there is considerable redundancyin the data; although we have not quantified it forthis data set, we note that the same event is often56Tasks 1 and 3 Task 2Event class GS answer R P F R P FLocalization 174 (33) 41 (33) 18.97 80.49 30.70 16.67 69.05 26.85Binding 347 (62) 152 (62) 17.87 40.79 24.85 17.48 40.13 24.35Gene expression 722 (290) 344 (290) 40.17 84.30 54.41 40.17 84.30 54.41Transcription 137 (28) 31 (28) 20.44 90.32 33.33 20.44 90.32 33.33Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00Phosphorylation 135 (47) 48 (47) 34.81 97.92 51.37 32.37 84.91 46.88EVENT TOTAL 1529 (464) 622 (464) 30.35 74.60 43.14 29.77 72.77 42.26Regulation 291 (11) 31 (11) 3.78 35.48 6.83 3.77 35.48 6.81Positive regulation 983 (60) 129 (60) 6.10 46.51 10.79 6.08 46.51 10.75Negative regulation 379 (18) 41 (18) 4.75 43.90 8.57 4.49 41.46 8.10REGULATION TOTAL 1653 (89) 201 (89) 5.38 44.28 9.60 5.31 43.78 9.47Negation 227 (6) 129 (6) 2.64 4.65 3.37Speculation 208 (25) 165 (25) 12.02 15.15 13.40MODIFICATION TOTAL 435 (31) 294 (31) 7.13 10.54 8.50ALL TOTAL 3182 (553) 823 (553) 17.38 67.19 27.62 17.07 65.74 27.10Table 3: Updated results on test data for Tasks 1-3, with important bug fixes in the code base.
See key above.mentioned repeatedly, but for knowledge base build-ing and other uses of the extracted information, it isonly strictly necessary to recognize an event once(although multiple recognition of the same assertionmay increase our confidence in its correctness).The second is that there is often redundancyacross the literature; the best-supported assertionswill be reported as initial findings and then repeatedas background information.The third is that these recall results reflect an ap-proach that made no use of syntactic analysis be-yond handling coordination.
There is often textpresent in the input that cannot be disregarded with-out either using wildcards, which generally de-creased precision in our experiments and whichwe generally eschewed, or making use of syntac-tic information to isolate phrasal heads.
Syntacticanalysis, particularly when combined with analysisof predicate-argument structure, has recently beenshown to be an effective tool in biomedical infor-mation extraction (Miyao et al, 2009).
There isbroad need for this?for example, of the thirty lo-calization events in the training data whose triggerword was translocation, a full eighteen had inter-vening textual material that made it impossible forsimple patterns like translocationof [Theme] or[ToLoc]translocation to match.Finally, our recall numbers reflect a very short de-velopment cycle, with as few as four patterns writ-ten for many event types.
A less time-constrainedpattern-writing effort would almost certainly resultin increased recall.AcknowledgmentsWe gratefully acknowledge Mike Bada?s help inloading the Sequence Ontology into Prote?ge?.This work was supported by NIHgrants R01LM009254, R01GM083649, andR01LM008111 to Lawrence Hunter andT15LM009451 to Philip Ogren.ReferencesAlias-i.
2008.
LingPipe 3.1.2.Ekaterina Buyko, Joachim Wermter, Michael Poprat, andUdo Hahn.
2006.
Automatically mapping an NLPcore engine to the biology domain.
In Proceedingsof the ISMB 2006 joint BioLINK/Bio-Ontologies meet-ing.K.
B. Cohen, L. Tanabe, S. Kinoshita, and L. Hunter.2004.
A resource for constructing customized testsuites for molecular biology entity identification sys-tems.
BioLINK 2004, pages 1?8.K.
Bretonnel Cohen, Martha Palmer, and LawrenceHunter.
2008.
Nominalization and alternations inbiomedical language.
PLoS ONE, 3(9).57D.
Ferrucci and A. Lally.
2004.
Building an exampleapplication with the unstructured information manage-ment architecture.
IBM Systems Journal, 43(3):455?475, July.Lawrence Hunter, Zhiyong Lu, James Firby, WilliamA.
Baumgartner Jr., Helen L. Johnson, Philip V. Ogren,and K. Bretonnel Cohen.
2008.
OpenDMAP: Anopen-source, ontology-driven concept analysis engine,with applications to capturing knowledge regardingprotein transport, protein interactions and cell-specificgene expression.
BMC Bioinformatics, 9(78).IBM.
2009.
UIMA Java framework.
http://uima-framework.sourceforge.net/.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 shared task on event extraction.
InProceedings of Natural Language Processing inBiomedicine (BioNLP) NAACL 2009 Workshop.
Toappear.Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2009.
Evaluating contri-butions of natural language parsers to protein-proteininteraction extraction.
Bioinformatics, 25(3):394?400.Karin Verspoor, K. Bretonnel Cohen, and LawrenceHunter.
In press.
The textual characteristics of tradi-tional and Open Access scientific journals are similar.BMC Bioinformatics.58
