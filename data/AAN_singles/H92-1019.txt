SESS ION 4: STAT IST ICAL  LANGUAGE MODEL INGAravind K. Joshi, ChairDepartment ofComputer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104-63891.
IntroductionCorpus based Natural Language Processing (NLP) isnow a well established paradigm in NLP.
The availabil-ity of large corpora, often annotated in various way hasled to the development ofa variety of approaches to sta-tistical language modeling.
The papers in this sessionrepresent many of these important approaches.
I willtry to classify these papers along different dimensions,thus providing the reader an overview as well as someunderstanding of the future directions of the work inthis area.There are two major motivations for research in statisti-cal NLP, which are not necessarily independent of eachother.1.
Robust Parsing: For processing free texts, handcrafted grammars are neither practical nor reliable.Statistical techniques are necessary both for robust-ness and efficiency.
The use of statistical techniquesfor part of speech tagging and parsing is clearly mo-tivated by these considerations.2.
Automatic Acquisition of Linguistic Structure:Here the goal is to use statistical techniques to dis-cover linguistic structure by processing large corpus.The two motivations are clearly not independent,however the latter is more concerned with the ex-tent to which the structure can be discovered statis-tically and the extent o which it has to be providedfrom outside.2.
Adaptive Stochastic ModelingImprovements in stochastic language modeling can beobtained by using adaptive techniques.
Della Pietra etal.
describe an algorithm to adapt a n - g ram languagemodel to a document as it is dictated.
Rosenfeld andHuang describe an adaptive technique which uses infor-mation about within-document word sequence correla-tions, where one word sequence triggers another, caus-ing its estimated probability to be raised.
Such adaptivetechniques are essential as the vocabulary size increases.3.
Part of Speech TaggingStatistical techniques have been very successful in thetask of part of speech tagging.
There are two papersin this session dealing with part of speech tagging, rep-resenting two different perspectives.
Black et al de-scribe the use of decision trees to estimate probabilitiesof words appearing with various parts of speech, giventhe context in which the word appears.
Decision treesare used to take care of some of the problems of modelinglong-distance dependencies.Statistical techniques were introduced for part of speechhave been more successful than the rule based techniquesfor the task of part of speech tagging.
These rules are, ofcourse, hand crafted.
Brill presents a rule based taggerwhich automatically acquires its rules and tags from acorpus based analysis.
Its accuracy is comparable tostochastic taggers.
Brill's paper is an example of howstatistical techniques can be used to acquire structure,thus opening possibilities for overcoming the limitationsof usual rule based approaches to language processing.4.
Grammar Inference and Probabil isticParsingGrammar inference is a challenging problem for statis-tical approaches to natural anguage processing becausethe standard techniques based on finite-state models areincapable to represent hierarchical structure of naturallanguage.The parameter estimation methods have already beenextended by Baker to stochastic ontext-free grammars.Pereira nd Schabes describe some of the difficulties withthe inside-outside algorithm, in particular the growth oflocal maxima as the number of nonterminals increasesand the possible divergence between the structure in-ferred and the qualitative linguistic judgments.
Theypropose an extension of the inside-outside algorithm us-ing a partially parsed corpus in order to provide a tighterconnection between the hierarchical structure and the in-ferred grammar.Stochastic approaches togrammar inference and parsingi01are significantly enhanced by combining lexical, struc-tural, and contextual information.
Several papers inthis session describe different echniques for achievingthis combination.
Magerman and Weir describe a proba-bilistic agenda based chart parsing algorithm which usesa probabilistic technique for modeling where edges inthe chart are likely to occur in the agenda-based chart-parsing process, enabling the parser to avoid the worstcase behavior.Mark et al use a stochastic context-free grammar (CFG)combined with the n-  gram statistics, which providesome 'local' contextual information.
They then describetechniques for parameter stimation.Black et al describe a history based approach for com-bining some lexical, syntactic, semantic, and structuralinformation.
They use the leftmost derivation of theparse tree to specify the context.
Although they de-scribe their approach using a CFG, it appears that theapproach is more general and not necessarily imited toCFGs.Schabes describes tochastic Lexicalized Tree-adjoiningGrammars(LTAG).
He shows how the inside-outsidereestimation algorithm for stochastic CFGs can be ex-tended to stochastic LTAGs.
The LTAGs provide aframework for integrating hierarchical, syntactic, andlexical information in the grammar formalism itself,thereby allowing the specification of co-occurrence r -lationships directly.Hindle also presents a parser that combines lexical andgrammatical constraints into a uniform grammatical rep-resentation.
In this sense, the papers by Schabes andHindle are closely related~ A new aspect of Hindle'sparser is that it uses analogy to guess the likelihood ofconstructions outside the grammar.The paper by BrUl and Marcus, although I have classifiedit in the general category of grammar inference and prob-abilistic parsing, has a somewhat different flavor.
Brilland Marcus present an algorithm for the acquisition ofphrase structure grammar in an unsupervised manner.Their approach isbased on the well-known distributionalanalysis techniques proposed by Zellig Harris in the early50's.
These techniques were not actively pursued earlieras it was not possible to work with large corpora in thosedays.
Now it is possible to do so.
Brill and Marcus usethe entropy measure to evaluate the distributional sim-ilarity of items, something that can be carried out withthe help of large corpora.
The techniques as proposedby Harris were meant o be used by linguists doing thefield work, judging the distributional similarity by ques-tioning informants in the field.5.
SummaryI have identified the current trends in statistical languagemodeling by classifying the papers in the categories de-scribed above.
The trend of combining statistical andgrammatical information in some uniform manner willdefinitely continue and we should expect both theoreti-cal and experimental results in the near future.There is no reason to suppose that these statistical tech-niques are applicable only at the sentence l vel.
It is verylikely that these techniques will be applicable to certainaspects of discourse also.
Again, it is important here tocombine statistical information with information aboutdiscourse structure in a uniform fashion.
Unlike gram-matical structure, we still know little about discoursestructure.
Hence, research on discourse structure is cru-cial if successful application of statistical techniques ito be achieved.102
