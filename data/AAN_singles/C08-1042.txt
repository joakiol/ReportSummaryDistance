Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 329?336Manchester, August 2008Evaluating Unsupervised Part-of-Speech Tagging for Grammar InductionWilliam P. Headden III, David McClosky, Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{headdenw,dmcc,ec}@cs.brown.eduAbstractThis paper explores the relationship be-tween various measures of unsupervisedpart-of-speech tag induction and the per-formance of both supervised and unsuper-vised parsing models trained on inducedtags.
We find that no standard taggingmetrics correlate well with unsupervisedparsing performance, and several metricsgrounded in information theory have nostrong relationship with even supervisedparsing performance.1 IntroductionThere has been a great deal of recent interest inthe unsupervised discovery of syntactic structurefrom text, both parts-of-speech (Johnson, 2007;Goldwater and Griffiths, 2007; Biemann, 2006;Dasgupta and Ng, 2007) and deeper grammaticalstructure like constituency and dependency trees(Klein and Manning, 2004; Smith, 2006; Bod,2006; Seginer, 2007; Van Zaanen, 2001).
Whilesome grammar induction systems operate on rawtext, many of the most successful ones presumeprior part-of-speech tagging.
Meanwhile, most re-cent work in part-of-speech induction focuses onincreasing the degree to which their tags matchhand-annotated ones such as those in the PennTreebank.In this work our goal is to evaluate how im-provements in part-of-speech tag induction affectsgrammar induction.
Using several different unsu-pervised taggers, we induce tags and train threegrammar induction systems on the results.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.We then explore the relationship between theperformance on common unsupervised taggingmetrics and the performance of resulting grammarinduction systems.
Disconcertingly we find thatthey bear little to no relationship.This paper is organized as follows.
In Section 2we discuss unsupervised part-of-speech inductionsystems and common methods of evaluation.
InSection 3, we describe grammar induction in gen-eral and discuss the systems with which we evalu-ate taggings.
We present our experiments in Sec-tion 4, and finally conclude in Section 5.2 Part-of-speech Tag InductionPart-of-speech tag induction can be thought of as aclustering problem where, given a corpus of words,we aim to group word tokens into syntactic classes.Two tasks are commonly labeled unsupervisedpart-of-speech induction.
In the first, tag inductionsystems are allowed the use of a tagging dictionary,which specifies for each word a set of possibleparts-of-speech (Merialdo, 1994; Smith and Eis-ner, 2005; Goldwater and Griffiths, 2007).
In thesecond, only the word tokens and sentence bound-aries are given.
In this work we focus on this lattertask to explore grammar induction in a maximallyunsupervised context.Tag induction systems typically focus on twosorts of features: distributional and morphologi-cal.
Distributional refers to what sorts of wordsappear in close proximity to the word in question,while morphological refers to modeling the inter-nal structure of a word.
All the systems belowmake use of distributional information, whereasonly two use morphological features.We primarily focus on the metrics used to evalu-ate induced taggings.
The catalogue of recent part-of-speech systems is large, and we can only test329the tagging metrics using a few systems.
Recentwork that we do not explore explicitly includes(Biemann, 2006; Dasgupta and Ng, 2007; Freitag,2004; Smith and Eisner, 2005).
We have selecteda few systems, described below, that represent abroad range of features and techniques to make ourevaluation of the metrics as broad as possible.2.1 Clustering using SVD and K-meansSchu?tze (1995) presents a series of part-of-speechinducers based on distributional clustering.
Weimplement the baseline system, which Klein andManning (2002) use for their grammar inductionexperiments with induced part-of-speech tags.
Foreach word type w in the vocabulary V , the systemforms a feature row vector consisting of the num-ber of times each of the F most frequent words oc-cur to the left of w and to the right of w. It normal-izes these row vectors and assembles them into a|V |?2F matrix.
It then performs a Singular ValueDecomposition on the matrix and rank reduces it todecrease its dimensionality to d principle compo-nents (d < 2F ).
This results in a representationof each word as a point in a d dimensional space.We follow Klein and Manning (2002) in using K-means to cluster the d dimensional word vectorsinto parts-of-speech.
We use the F = 500 mostfrequent words as left and right context features,and reduce to a dimensionality of d = 50.
We re-fer to this system as SVD in our experiments.The other systems described in Schu?tze (1995)make use of more complicated feature models.
Wechose the baseline system primarily to match pre-vious evaluations of grammar induction using in-duced tags (Klein and Manning, 2002).2.2 Hidden Markov ModelsOne simple family of models for part-of-speech in-duction are the Hidden Markov Models (HMMs),in which there is a sequence of hidden state vari-ables t1...tn(for us, the part-of-speech tags).
Eachstate tiis conditioned on the previous n ?
1 statesti?1...ti?n+1, and every tiemits an observed wordwiconditioned on ti.
There is a single start statethat emits nothing, as well as a single stop state,which emits an end-of-sentence marker with prob-ability 1 and does not transition further.
In our ex-periments we use the bitag HMM, in which eachstate tidepends only on state ti?1.The classic method of training HMMs for part-of-speech induction is the Baum-Welch (Baum,1972) variant of the Expectation-Maximization(EM) algorithm, which searches for a local max-imum in the likelihood of the observed words.Other methods approach the problem froma Bayesian perspective.
These methods placeDirichlet priors over the parameters of each transi-tion and emission multinomial.
For an HMM witha set of states T and a set of output symbols V :?t ?
T ?t?
Dir(?1, ...?|T |) (1)?t ?
T ?t?
Dir(?1, ...?|V |) (2)ti|ti?1, ?ti?1?
Multi(?ti?1) (3)wi|ti, ?ti?
Multi(?ti) (4)One advantage of the Bayesian approach is thatthe prior allows us to bias learning toward sparserstructures, by setting the Dirichlet hyperparame-ters ?, ?
to a value less than one (Johnson, 2007;Goldwater and Griffiths, 2007).
This increases theprobability of multinomial distributions which putmost of their mass on a few events, instead of dis-tributing them broadly across many events.
Thereis evidence that this leads to better performanceon some part-of-speech induction metrics (John-son, 2007; Goldwater and Griffiths, 2007).There are both MCMC and variational ap-proaches to estimating HMMs with sparse Dirich-let priors; we chose the latter (Variational Bayesor VB) due to its simple implementation as aminor modification to Baum-Welch.
Johnson(2007) evaluates both estimation techniques on theBayesian bitag model; Goldwater and Griffiths(2007) emphasize the advantage in the MCMC ap-proach of integrating out the HMM parameters in atritag model, yielding a tagging supported by manydifferent parameter settings.Following the setup in Johnson (2007), we ini-tialize the transition and emission distributions tobe uniform with a small amount of noise, and runEM and VB for 1000 iterations.
We label thesesystems as HMM-EM and HMM-VB respectivelyin our experiments.
In our VB experiments we set?i= ?j= 0.1,?i ?
{1, ..., |T |} , j ?
{1, ..., |V |},which yielded the best performance on most re-ported metrics in Johnson (2007).
We use max-imum marginal decoding, which Johnson (2007)reports performs better than Viterbi decoding.2.3 Systems with MorphologyClark (2003) presents several part-of-speech in-duction systems which incorporate morphologicalas well as distributional information.
We use the330implementation found on his website.12.3.1 Ney-Essen with MorphologyThe simplest model is based on work by (Ney etal., 1994).
It uses a bitag HMM, with the restric-tion that each word type in the vocabulary can onlybe generated by a single part-of-speech.
Thus thetag induction task here reduces to finding a multi-way partition of the vocabulary.
The learning al-gorithm greedily reassigns each word type to thepart-of-speech that results in the greatest increasein likelihood.In order to incorporate morphology, Clark(2003) associates with each part-of-speech a HMMwith letter emissions.
The vocabulary is gener-ated by generating a series of word types fromthe letter HMM of each part-of-speech.
These canmodel very basic concatenative morphology.
Theparameters of the HMMs are estimated by runninga single iteration of Forward-Backward after eachround of reassigning words to tags.
In our exper-iments we evaluate both the model without mor-phology (NE in our experiments), and the morpho-logical model, trying both 5 and 10 states in the let-ter HMM (NEMorph5, NEMorph10 respectively).2.3.2 Two-Level HMMThe final part-of-speech inducer we try fromClark (2003) is a two-level HMM.
This is similarto the previous model, except it lifts the restrictionthat a word appear under only one part-of-speech.Alternatively, one could think of this model as astandard HMM, whose emission distributions in-corporate a mixture of a letter HMM and a stan-dard multinomial.
Training uses a simple variationof Forward-Backward.
In the experiments in thispaper, we initialize the mixture parameters to .5,and try 5 states in the letter HMM.
We refer to thismodel as 2HMM.2.4 Tag EvaluationObjective evaluation in any clustering task is al-ways difficult, since there are many ways to de-fine good clusters.
Typically it involves a mix-ture of subjective evaluation and a comparison ofthe clusters to those found by human annotators.In the realm of part-of-speech induction, there areseveral common ways of doing the latter.
Thesesplit into two groups: accuracy and information-theoretic criteria.1http://www.cs.rhul.ac.uk/home/alexc/pos.tar.gzAccuracy, given some mapping between the setof induced classes and the gold standard labels, isthe number of words in the corpus that have beenmarked with the correct gold label divided by thetotal number of word tokens.
The main challengefacing these metrics is deciding how to to map eachinduced part-of-speech class to a gold tag.
Oneoption is what Johnson (2007) calls ?many-to-one?
(M-to-1) accuracy, in which each induced tag islabeled with its most frequent gold tag.
Althoughthis results in a situation where multiple inducedtags may share a single gold tag, it does not punisha system for providing tags of a finer granularitythan the gold standard.In contrast, ?one-to-one?
(1-to-1) accuracy re-stricts each gold tag to having a single inducedtag.
The mapping typically is made to try to givethe most favorable mapping in terms of accuracy,typically using a greedy assignment (Haghighi andKlein, 2006).
In cases where the number of goldtags is different than the number of induced tags,some must necessarily remain unassigned (John-son, 2007).In addition to accuracy, there are several infor-mation theoretic criteria presented in the literature.These escape the problem of trying to find an ap-propriate mapping between induced and gold tags,at the expense of perhaps being less intuitive.Let TIbe the tag assignments to the wordsin the corpus created by an unsupervised tag-ger, and let TGbe the gold standard tag as-signments.
Clark (2003) uses Shannon?s condi-tional entropy of the gold tagging given the in-duced tagging H(TG|TI).
Lower entropy indi-cates less uncertainty in the gold tagging if we al-ready know the induced tagging.
Freitag (2004)uses the similar ?cluster-conditional tag perplex-ity?
which is merely exp(H(TG|TI))2.
Sincecluster-conditional tag perplexity is a monotonicfunction of H(TG|TI), we only report the latter.Goldwater and Griffiths (2007) propose usingthe Variation of Information of Meila?
(2003):V I(TG;TI) = H(TG|TI) + H(TI|TG)VI represents the change in information when go-ing from one clustering to another.
It holds thenice properties of being nonnegative, symmetric,as well as fulfilling the triangle inequality.2Freitag (2004) measures entropy in nats, while we usebits.
The difference is a constant factor.3313 Grammar InductionIn addition to parts-of-speech, we also want to dis-cover deeper syntactic relationships.
Grammar in-duction is the problem of determining these re-lationships in an unsupervised fashion.
This canbe thought of more concretely as an unsupervisedparsing task.
As there are many languages and do-mains with few treebank resources, systems thatcan learn syntactic structure from unlabeled datawould be valuable.
Most work on this problem hasfocused on either dependency induction, which wediscuss in Section 3.2, or on constituent induction,which we examine in the next section.The Grammar Induction systems we use to eval-uate the above taggers are the Constituent-ContextModel (CCM), the Dependency Model with Va-lence (DMV), and a model which combines thetwo (CCM+DMV) outlined in (Klein and Man-ning, 2002; Klein and Manning, 2004).3.1 Constituent Grammar InductionKlein and Manning (2002) present a generativemodel for inducing constituent boundaries frompart-of-speech tagged text.
The model first gener-ates a bracketing B = {Bij}1?i?j?n, which spec-ifies whether each span (i, j) in the sentence is aconstituent or a distituent.
Next, given the con-stituency or distituency of the span Bij, the modelgenerates the part-of-speech yield of the spanti...tj, and the one-tag context window of the spanti?1, tj+1.
P (ti...tj|Bij) and P (ti?1, tj+1|Bij)are multinomial distributions.
The model is trainedusing EM.We evaluate induced constituency trees againstthose of the Penn Treebank using the versions ofunlabeled precision, recall, and F-score used byKlein and Manning (2002).
These ignore triv-ial brackets and multiple constituents spanning thesame bracket.
They evaluate their CCM systemon the Penn Treebank WSJ sentences of length 10or less, using part-of-speech tags induced by thebaseline system of Schu?tze (1995).
They reportthat switching to induced tags decreases the overallbracketing F-score from 71.1 to 63.2, although therecall of VP and S constituents actually improves.Additionally, they find that NP and PP recall de-creases substantially with induced tags.
They at-tribute this to the fact that nouns end up in manyinduced tags.There has been quite a bit of other work onconstituency induction.
Smith and Eisner (2004)present an alternative estimation technique forCCM which uses annealing to try to escape localmaxima.
Bod (2006) describes an unsupervisedsystem within the Data-Oriented-Parsing frame-work.
Several approaches try to learn structuredirectly from raw text.
Seginer (2007) has an in-cremental parsing approach using a novel repre-sentation called common-cover-links, which canbe converted to constituent brackets.
Van Zaanen(2001)?s ABL attempts to align sentences to deter-mine what sequences of words are substitutable.The work closest in spirit to this paper is Cramer(2007), who evaluates several grammar inductionsystems on the Eindhoven corpus (Dutch).
Oneof his experiments compares the grammar induc-tion performance of these systems starting withtags induced using the system described by Bie-mann (2006), to the performance of the systemson manually-marked tags.
However he does notevaluate to what degree better tagging performanceleads to improvement in these systems.3.2 Dependency Grammar InductionA dependency tree is a directed graph whose nodesare words in the sentence.
A directed edge existsbetween two words if the target word (argument) isa dependent of the source word (head).
Each wordtoken may be the argument of only one head, but ahead may have several arguments.
One word is thehead of the sentence, and is often thought of as theargument of a virtual ?Root?
node.Klein and Manning (2004) present their Depen-dency Model with Valence (DMV) for the un-supervised induction of dependencies.
Like theconstituency model, DMV works from parts-of-speech.
Under this model, for a given head, h,they first generate the parts-of-speech of the argu-ments to the right of h, and then those to the left.Generating the arguments in a particular directionbreaks down into two parts: deciding whether tostop generating in this direction, and if not, whatpart-of-speech to generate as the argument.
The ar-gument decision conditions on h and the direction.The stopping decision conditions on this and alsoon whether h has already generated an argument inthis direction, thereby capturing the limited notionof valence from which the model takes its name.It is worth noting that this model can only repre-sent projective dependency trees, i.e.
those withoutcrossing edges.Dependencies are typically evaluated using di-332Tagging Metrics Grammar Induction MetricsTagger No.
Tags CCM CCM+DMV DMV1-to-1 H(TG|TI) M-to-1 VI UF1 DA UA UF1 DA UAGold 1.00 0.00 1.00 0.00 71.50 52.90 67.60 56.50 45.40 63.80HMM-EM 10 0.39 2.67 0.41 4.39 58.89 40.12 59.26 59.43 36.77 57.37HMM-EM 20 0.43 2.28 0.48 4.54 57.31 51.16 64.66 61.33 38.65 58.57HMM-EM 50 0.36 1.83 0.58 4.92 56.56 48.03 63.84 58.02 39.30 58.84HMM-VB 10 0.40 2.75 0.41 4.42 39.05 27.72 52.84 58.64 23.94 51.64HMM-VB 20 0.40 2.63 0.43 4.65 37.60 33.77 55.97 40.30 30.36 51.53HMM-VB 50 0.38 2.70 0.42 5.01 34.68 37.29 57.72 39.82 29.03 50.50NE 10 0.34 2.74 0.40 4.32 28.80 20.70 50.60 32.70 26.20 48.90NE 20 0.48 2.02 0.55 3.76 32.50 36.00 59.30 40.60 32.80 54.00NEMorph10 10 0.44 2.46 0.47 3.74 29.03 25.99 53.80 34.58 26.98 48.72NEMorph10 20 0.48 1.94 0.56 3.65 31.95 35.85 57.93 38.22 30.45 50.72NEMorph10 50 0.47 1.24 0.72 3.60 31.07 36.29 57.76 39.28 31.50 52.83NEMorph5 10 0.45 2.50 0.47 3.76 29.04 22.72 51.58 32.67 23.62 47.89NEMorph5 20 0.44 2.02 0.56 3.80 31.94 24.17 52.43 32.90 22.41 47.17NEMorph5 50 0.47 1.27 0.72 3.64 31.39 38.63 59.44 40.23 34.26 54.632HMM 10 0.38 2.78 0.41 4.55 31.63 36.35 58.87 44.97 28.43 49.322HMM 20 0.41 2.35 0.48 4.71 42.39 43.91 60.74 50.85 29.32 50.692HMM 50 0.37 1.92 0.58 5.11 41.18 49.94 64.87 57.84 39.24 59.14SVD 10 0.31 3.07 0.34 4.99 37.77 27.64 49.56 36.46 20.74 45.52SVD 20 0.33 2.73 0.40 4.99 37.17 30.14 51.66 37.66 22.24 46.25SVD 50 0.34 2.37 0.47 5.18 36.87 37.66 56.49 52.83 22.50 46.52SVD 100 0.34 2.03 0.53 5.37 45.46 41.68 58.83 64.20 20.81 44.36SVD 200 0.32 1.72 0.59 5.59 61.90 34.79 52.25 59.93 22.66 42.30Table 1: The performance of the taggers regarding both tag and grammar induction metrics on WSJsections 0-10, averaged over 10 runs.
Bold indicates the result was within 10 percent of the best-scoringinduced system for a given metric.rected and undirected accuracy.
These are the to-tal number of proposed edges that appear in thegold tree divided by the total number of edges (thenumber of words in the sentence).
Directed accu-racy gives credit to a proposed edge if it is in thegold tree and is in the correct direction, while undi-rected accuracy ignores the direction.Klein and Manning (2004) also present a modelwhich combines CCM and DMV into a singlemodel, which we show as CCM+DMV.
In theirexperiments, this model performed better on boththe constituency and dependency induction tasks.As with CCM, Klein and Manning (2004) simi-larly evaluate the combined CCM+DMV systemusing tags induced with the same method.
Againthey find that overall bracketing F-score decreasesfrom 77.6 to 72.9 and directed dependency accu-racy measures decreases from 47.5 to 42.3 whenswitching to induced tags from gold.
However foreach metric, the systems still do quite well withinduced tags.As in the constituency case, Smith (2006)presents several alternative estimation proceduresfor DMV, which try to minimize the local maxi-mum problems inherent in EM.
It is thus possiblethese methods might yield better performance forthe models when run off of induced tags.4 ExperimentsWe induce tags with each system on the Penn Tree-bank Wall Street Journal (Marcus et al, 1994), sec-tions 0-10, which contain 20,260 sentences.
Wevary the number of tags (10, 20, 50) and run eachsystem 10 times for a given setting.
The result ofeach run is used as the input to the CCM, DMV,and CCM+DMV systems.
While the tags are in-duced from all sentences in the section, followingthe practice in (Klein and Manning, 2002; Kleinand Manning, 2004), we remove punctuation, andconsider only sentences of length not greater than10 in our grammar induction experiments.
Tag-gings are evaluated after punctuation is removed,but before filtering for length.To explore the relationship between taggingmetrics and the resulting performance of grammarinduction systems, we examine each pair of tag-ging and grammar induction metrics.
Consider thefollowing two examples: DMV directed accuracyvs.
H(TG|TI) (Figure 1), and CCM f-score vs.variation of information (Figure 2).
These were se-lected because they have relatively high magnitude?s.
From these plots it is clear that although there333may be a slight correspondence, the relationshipsare weak at best.Each tagging and grammar induction metricgives us a ranking over the set of taggings of thedata generated over the course of our experiments.These are ordered from best to worst according tothe metric, so for instance H(TG|TI) would givehighest rank to its lowest value.
We can com-pare the two rankings using Kendall?s ?
(see Lap-ata (2006) for an overview), a nonparametric mea-sure of correspondence for rankings.
?
measuresthe difference between the number of concordantpairs (items the two rankings place in the same or-der) and discordant pairs (those the rankings placein opposite order), divided by the total number ofpairs.
A value of 1 indicates the rankings have per-fect correspondence, -1 indicates they are in theopposite order, and 0 indicates they are indepen-dent.
The ?
values are shown in Table 2.
Thescatter-plot in Figure 1 shows the ?
with the great-est magnitude.
However, we can see that eventhese rankings have barely any relationship.An objection one might raise is that the lack ofcorrespondence reflects poorly not on these met-rics, but upon the grammar induction systems weuse to evaluate them.
There might be somethingabout these models in particular which yields theselow correlations.
For instance these grammar in-ducers all estimate their models using EM, whichcan get caught easily in a local maximum.To this possibility, we respond by pointing toperformance on gold tags, which is consistentlyhigh for all grammar induction metrics.
There isclearly some property of the gold tags which is ex-ploited by the grammar induction systems even inthe absence of better estimation procedures.
Thisproperty is not reflected in the tagging metrics.The scores for each system for tagging andgrammar induction, averaged over the 10 runs, areshown in Table 1.
Additionally, we included runsof the SVD-tagger for 100 and 200 tags, since run-ning this system is still practical with these num-bers of tags.
The Ney-Essen with Morphology tag-gers perform at or near the top on the various tag-ging metrics, but not well on the grammar induc-tion tasks on average.
HMM-EM seems to performon average quite well on all the grammar inductiontasks, while the SVD-based systems yield the topbracketing F-scores, making use of larger numbersof tags.Grammar Induction MetricsTagging CCM CCM+DMV DMVMetrics UF1 DA UA UF1 DA UA1-to-1 -0.22 -0.04 0.05 -0.13 0.13 0.12M-to-1 -0.09 0.17 0.24 0.03 0.26 0.25H(TG|TI) 0.01 0.21 0.27 0.07 0.29 0.28VI -0.25 -0.17 -0.06 -0.20 0.07 0.07Table 2: Kendall?s ?
, between tag and grammarinduction criteria.4.1 Supervised ExperimentsOne question we might ask is whether these tag-ging metrics capture information relevant to anyparsing task.
We explored this by experimentingwith a supervised parser, training off trees wherethe gold parts-of-speech have been removed andreplaced with induced tags.
Our expectation wasthat the brackets, the head propagation paths, andthe phrasal categories in the training trees wouldbe sufficient to overcome any loss in informationthat the gold tags might provide.
Additionally itwas possible the induced tags would ignore rareparts-of-speech such as FW, and make better useof the available tags, perhaps using new distribu-tional clues not in the original tags.To this end we modified the Charniak Parser(Charniak, 2000) to train off induced parts-of-speech.
The Charniak parser is a lexicalized PCFGparser for which the part-of-speech of a head wordis a key aspect of its model.
During training, thehead-paths from the gold part-of-speech tags areretained, but we replace the tags themselves.We ran experiments using the bitag HMM fromSection 2.2 trained using EM, as well as with theSchu?tze SVD tagger from Section 2.1.
The parserwas trained on sections 2-21 of the Penn Treebankfor training and section 24 was used for evaluation.As before we calculated ?
scores between eachtagging metric and supervised f-score.
Unlike theunsupervised evaluation where we used the metricUF1, we use the standard EVALB calculation ofunlabeled f-score.
The results are shown in Table3.The contrast with the unsupervised case is vast,with very high ?s for both accuracy metrics.
Con-sider f-score vs. many-to-one, plotted in Figure 3.The correspondence here is very clear: taggingswith high accuracy do actually reflect on betterparser performance.
Note, however, that the corre-spondence between the information theoretic mea-sures and parsing performance is still rather weak.3340.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5152025303540455055DMVDANEMorphHMM-VB2HMMNESVDHMM-EMGoldFigure 1: DMV Directed Accuracy vs. H(TG|TI)0 1 2 3 4 5 6VI20304050607080CCMUF1NEMorphHMM-VB2HMMNESVDHMM-EMGoldFigure 2: CCM fscore vs. tagging variation of in-formation.0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0M-to-10.780.800.820.840.860.880.90CharniakparserF1SVD 10 tagsSVD 50 tagsGoldHMM-EMbitag 10 tagsHMM-EMbitag 50 tagsFigure 3: Supervised parsing f-score vs. taggingmany-to-one accuracy.Tagging Metric Supervised F11-to-1 0.62M-to-1 0.83H(TG|TI) -0.19VI 0.25Table 3: Kendall?s ?
, between tag induction cri-teria and supervised parsing unlabeled bracketingF-score.Interestingly, parsing performance and speeddoes degrade considerably when training off in-duced tags.
We are not sure what causes this.
Onepossibility is in the lexicalized stage of the parser,where the probability of a head word is smoothedprimarily by its part-of-speech tag.
This requiresthat the tag be a good proxy for the syntactic roleof the head.
In any case this warrants further in-vestigation.5 Conclusion and Future WorkIn this work, we found that none of the most com-mon part-of-speech tagging metrics bear a strongrelationship to good grammar induction perfor-mance.
Although our experiments only involveEnglish, the poor correspondence we find betweenthe various tagging metrics and grammar induc-tion performance raises concerns about their re-lationship more broadly.
We additionally foundthat while tagging accuracy measures do corre-late with better supervised parsing, common infor-mation theoretic ones do not strongly predict bet-ter performance on either task.
Furthermore, thesupervised experiments indicate that informativepart-of-speech tags are important for good parsing.The next step is to explore better tagging met-rics that correspond more strongly to better gram-mar induction performance.
A good metric shoulduse all the information we have, including the goldtrees, to evaluate.
Finally, we should explore gram-mar induction schemes that do not rely on priorparts-of-speech, instead learning them from rawtext at the same time as deeper structure.AcknowledgmentsWe thank Dan Klein for his grammar inductioncode, as well as Matt Lease and other members ofBLLIP for their feedback.
This work was partiallysupported by DARPA GALE contract HR0011-06-2-0001 and NSF award 0631667.335ReferencesBaum, L.E.
1972.
An inequality and associated maxi-mization techniques in statistical estimation of prob-abilistic functions of Markov processes.
Inequali-ties, 3:1?8.Biemann, Chris.
2006.
Unsupervised part-of-speechtagging employing efficient graph clustering.
InProceedings of the COLING/ACL 2006 Student Re-search Workshop, pages 7?12, Sydney, Australia.Bod, Rens.
2006.
An all-subtrees approach to unsuper-vised parsing.
In Proceedings of Coling/ACL 2006,pages 865?872.Charniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the North Ameri-can Chapter of the ACL 2000, pages 132?139.Clark, Alexander.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of EACL 2003, pages 59?66,Budapest, Hungary.Cramer, Bart.
2007.
Limitations of current grammarinduction algorithms.
In Proceedings of the ACL2007 Student Research Workshop, pages 43?48.Dasgupta, Sajib and Vincent Ng.
2007.
Unsupervisedpart-of-speech acquisition for resource-scarce lan-guages.
In Proceedings of the EMNLP/CoNLL 2007,pages 218?227.Freitag, Dayne.
2004.
Toward unsupervised whole-corpus tagging.
In Proceedings of Coling 2004,pages 357?363, Aug 23?Aug 27.Goldwater, Sharon and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speechtagging.
In Proceedings of the ACL 2007, pages744?751.Haghighi, Aria and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofHLT/NAACL 2006, pages 320?327, New York, USA.Johnson, Mark.
2007.
Why doesn?t EM findgood HMM POS-taggers?
In Proceedings of theEMNLP/CoNLL 2007, pages 296?305.Klein, Dan and Christopher Manning.
2002.
A gener-ative constituent-context model for improved gram-mar induction.
In Proceedings of ACL 2002.Klein, Dan and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of ACL2004, pages 478?485, Barcelona, Spain, July.Lapata, Mirella.
2006.
Automatic evaluation of infor-mation ordering: Kendall?s tau.
Computational Lin-guistics, 32(4):1?14.Marcus, Mitchell, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre, Ann Bies,Mark Ferguson, Karen Katz, and Britta Schasberger.1994.
The Penn Treebank: Annotating PredicateArgument Structure.
In Proceedings of the 1994ARPA Human Language Technology Workshop.Meila?, Marina.
2003.
Comparing clusterings.
Pro-ceedings of the Conference on Computational Learn-ing Theory (COLT).Merialdo, Bernard.
1994.
Tagging english text witha probabilistic model.
Computational Linguistics,20(2):154?172.Ney, Herman, Ute Essen, and Renhard Knesser.
1994.On structuring probabilistic dependencies in stochas-tic language modelling.
Computer Speech and Lan-guage, 8:1?38.Schu?tze, Hinrich.
1995.
Distributional part-of-speechtagging.
In Proceedings of the 7th conference of theEACL, pages 141?148.Seginer, Yoav.
2007.
Fast unsupervised incrementalparsing.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 384?391, Prague, Czech Republic.Smith, Noah A. and Jason Eisner.
2004.
Anneal-ing techniques for unsupervised statistical languagelearning.
In Proceedings of ACL 2004, pages 487?494, Barcelona, Spain.Smith, Noah A. and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of ACL 2005, pages 354?362,Ann Arbor, Michigan.Smith, Noah A.
2006.
Novel Estimation Methods forUnsupervised Discovery of Latent Structure in Nat-ural Language Text.
Ph.D. thesis, Department ofComputer Science, Johns Hopkins University, Octo-ber.Van Zaanen, Menno M. 2001.
Bootstrapping Structureinto Language: Alignment-Based Learning.
Ph.D.thesis, University of Leeds, September.336
