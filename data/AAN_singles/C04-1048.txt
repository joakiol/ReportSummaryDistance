Generating Discourse Structures for Written TextsHuong LeThanh, Geetha Abeysinghe, and Christian HuyckSchool of Computing Science, Middlesex UniversityThe Burroughs, London, NW4 4BT, United Kingdom{H.Le, G.Abeysinghe, C.Huyck}@mdx.ac.ukAbstractThis paper presents a system for automati-cally generating discourse structures fromwritten text.
The system is divided into twolevels: sentence-level and text-level.
Thesentence-level discourse parser uses syntacticinformation and cue phrases to segment sen-tences into elementary discourse units and togenerate discourse structures of sentences.
Atthe text-level, constraints about textual adja-cency and textual organization are integratedin a beam search in order to generate best dis-course structures.
The experiments were donewith documents from the RST DiscourseTreebank.
It shows promising results in a rea-sonable search space compared to the dis-course trees generated by human analysts.1 IntroductionMany recent studies in Natural Language Proc-essing have paid attention to Rhetorical StructureTheory (RST) (Mann and Thompson 1988; Hovy1993; Marcu 2000; Forbes et al 2003), a methodof structured description of text.
Although rhe-torical structure has been found to be useful inmany fields of text processing (Rutledge et al2000; Torrance and Bouayad-Agha 2001), only afew algorithms for implementing discourse ana-lyzers have been proposed so far.
Most researchin this field concentrates on specific discoursephenomena (Schiffrin 1987; Litman and Hirsch-berg 1990).
The amount of research available indiscourse segmentation is considered small; indiscourse parsing it is even smaller.The difficulties in developing a discourseparser are (i) recognizing discourse relationsbetween text spans and (ii) deriving discoursestructures from these relations.
Marcu (2000)?sparser is based on cue phrases, and thereforefaces problems when cue phrases are not presentin the text.
This system can apply to unrestrictedtexts, but faces combinatorial explosion.
The dis-advantage of Marcu?s approach is that it pro-duces a great number of trees during its process,which is the essential redundancy in computa-tion.
As the number of relations increases, thenumber of possible discourse trees increases ex-ponentially.Forbes et al (2003) have a different approachof implementing a discourse parser for a Lexi-calized Tree Adjoining Grammar (LTAG).
Theysimplify discourse analysis by developing agrammar that uses cue phrases as anchors to con-nect discourse trees.
Despite the potential of thisapproach for discourse analysis, the case of nocue phrase present in the text has not been fullyinvestigated in their research.
Polanyi et al(2004) propose a far more complicated discoursesystem than that of Forbes et al (2003) , whichuses syntactic, semantic and lexical rules.
Polanyiet al have proved that their approach can providepromising results, especially in text summariza-tion.In this paper, different factors were investi-gated to achieve a better discourse parser, in-cluding syntactic information, constraints abouttextual adjacency and textual organization.
Witha given text and its syntactic information, thesearch space in which well-structured discoursetrees of a text are produced is minimized.The rest of this paper is organized as follows.The discourse analyzer at the sentence-level ispresented in Section 2.
A detailed description ofour text-level discourse parser is given in Section3.
In Section 4, we describe our experiments anddiscuss the results we have achieved so far.
Sec-tion 5 concludes the paper and proposes possiblefuture work on this approach.2 Sentence-level Discourse AnalyzingThe sentence-level discourse analyzer constructsdiscourse trees for each sentence.
In doing so,two main tasks need to be accomplished: dis-course segmentation and discourse parsing,which will be presented in Section 2.1 and Sec-tion 2.2.2.1 Discourse SegmentationThe purpose of discourse segmentation is to splita text into elementary discourse units (edus)1.This task is done using syntactic information andcue phrases, as discussed in Section 2.1.1 andSection 2.1.2 below.2.1.1 Segmentation by Syntax ?
Step 1Since an edu can be a clause or a simple sen-tence, syntactic information is useful for thesegmentation process.
One may argue that usingsyntactic information is complicated since a syn-tactic parser is needed to generate this informa-tion.
Since there are many advanced syntacticparsers currently available, the above problemcan be solved.
Some studies in this area werebased on regular expressions of cue phrases toidentify edus (e.g., Marcu 2000).
However, Re-deker (1990) found that only 50% of clausescontain cue phrases.
Segmentation based on cuephrases alone is, therefore, insufficient by itself.In this study, the segmenter?s input is a sen-tence and its syntactic structure; documents fromthe Penn Treebank were used to get the syntacticinformation.
A syntactic parser is going to beintegrated into our system (see future work).Based on the sentential syntactic structure, thediscourse segmenter checks segmentation rules tosplit sentences into edus.
These rules were cre-ated based on previous research in discoursesegmentation (Carlson et al 2002).
The segmen-tation process also provides initial informationabout the discourse relation between edus.
Forexample, the sentence ?Mr.
Silas Cathcart built ashopping mall on some land he owns?
maps withthe segmentation rule( NP|NP-SBJ  <text1> ( SBAR|RRC <text2> ) )In which, NP, SBJ, SBAR, and RRC stand fornoun phrase, subject, subordinate clause, and re-duce relative clause respectively.
This rule can bestated as, ?The clause attached to a noun phrasecan be recognized as an embedded unit.
?The system searches for the rule that mapswith the syntactic structure of the sentence, and1 For further information on ?edus?, see (Marcu 2000).then generates edus.
After that, a post process iscalled to check the correctness of discourseboundaries.
In the above example, the systemderives an edu ?he owns?
from the noun phrase?some land he owns?.
The post process detectsthat ?Mr.
Silas Cathcart built a shopping mallon?
is not a complete clause without the nounphrase ?some land?.
Therefore, these two textspans are combined into one.
The sentence isnow split into two edus ?Mr.
Silas Cathcart builta shopping mall on some land?
and ?he owns.?
Adiscourse relation between these two edus is theninitiated.
Its relation?s name and the nuclearityroles of its text spans are determined later on in arelation recognition-process (see Section 2.2).2.1.2 Segmentation by Cue Phrase?Step 2Several NPs are considered as edus when theyare accompanied by a strong cue phrase.
Thesecases cannot be recognized by syntactic informa-tion; another segmentation process is, therefore,integrated into the system.
This process seeksstrong cue phrases from the output of Step 1.When a strong cue phrase is found, this processdetects the end boundary of the NP.
This endboundary can be punctuation such as a semico-lon, or a full stop.
Normally, a new edu is createdfrom the begin position of the cue phrase to theend boundary of the NP.
However, this proceduremay create incorrect results as shown in the ex-ample below:(1) [In 1988, Kidder eked out a $46 millionprofit, mainly][ because of severe costcutting.
]The correct segmentation boundary for the sen-tence given in Example (1) should be the positionbetween the comma (?,?)
and the adverb?mainly?.
Such a situation happens when an ad-verb stands before a strong cue phrase.
The postprocess deals with this case by first detecting theposition of the NP.
After that, it searches for theappearance of adverbs before the position of thestrong cue phrase.
If an adverb is found, the newedu is segmented from the start position of theadverb to the end boundary of the NP.
Otherwise,the new edu is split from the start position of thecue phrase to the end boundary of the NP.
This isshown in the following example:(2) [According to a Kidder World story aboutMr.
Megargel,] [all the firm has to do is"position ourselves more in the deal flow.
"]Similar to Step 1, Step 2 also initiates discourserelations between edus that it derives.
The rela-tion name and the nuclearity role of edus areposited later in a relation recognition-process.2.2 Sentence-level Discourse ParsingThis module takes edus from the segmenter asthe input and generates discourse trees for eachsentence.
As mentioned in Section 2.1, manyedus have already been connected in an initialrelation.
The sentence-level discourse parserfinds a relation name for the existing relations,and then connects all sub-discourse-trees withinone sentence into one tree.
All leaves that corre-spond to another sub-tree are replaced by the cor-responding sub-trees, as shown in Example (3)below:(3) [She knows3.1] [what time you will come3.2][because I told her yesterday.3.3]The discourse segmenter in Step 1 outputs twosub-trees, one with two leaves ?She knows?
and?what time you will come?
; another with twoleaves ?She knows what time you will come?
and?because I told her yesterday?.
The system com-bines these two sub-trees into one tree.
This pro-cess is illustrated in Figure 1.Figure 1.
The discourse structure of text (3)Syntactic information is used to figure out whichdiscourse relation holds between text spans aswell as their nuclearity roles.
For example, thediscourse relation between a reporting clause anda reported clause in a sentence is an Elaborationrelation.
The reporting clause is the nucleus; thereported clause is the satellite in this relation.Cue phrases are also used to detect the con-nection between edus, as shown in (4):(4) [He came late] [because of the traffic.
]The cue phrase ?because of?
signals a Cause re-lation between the clause containing this cuephrase and its adjacent clause.
The clause con-taining ?because of?
is the satellite in a relationbetween this clause and its adjacent clause.To posit relation names, we combine severalfactors, including syntactic information, cuephrases, NP-cues, VP-cues2, and cohesive de-vices (e.g., synonyms and hyponyms derivedfrom WordNet) (Le and Abeysinghe 2003).
Withthe presented method of constructing sententialdiscourse trees based on syntactic informationand cue phrases, combinatorial explosions can beprevented and still get accurate analyses.3 Text-level Discourse Analyzing3.1 Search SpaceThe original search space of a discourse parser isenormous (Marcu 2000).
Therefore, a crucialproblem in discourse parsing is search-space re-duction.
In this study, this problem was solved byusing constraints about textual organization andtextual adjacency.Normally, each text has an organizationalframework, which consists of sections, para-graphs, etc., to express a communicative goal.Each textual unit completes an argument or atopic that the writer intends to convey.
Thus, atext span should have semantic links to text spansin the same textual unit before connecting withtext spans in a different one.
Marcu (2000) ap-plied this constraint by generating discoursestructures at each level of granularity (e.g., para-graph, section).
The discourse trees at one levelare used to build the discourse trees at the higherlevel, until the discourse tree for the entire text isgenerated.
Although this approach is good forderiving all valid discourse structures that repre-sent the text, it is not optimal when only somediscourse trees are required.
This is because theparser cannot determine how many discoursetrees should be generated for each paragraph orsection.
In this research, we apply a different ap-proach to control the levels of granularity.
In-stead of processing one textual unit at a time, weuse a block-level-score to connect the text spans2 An NP-cue (VP-cue) is a special noun (verb) in the NP(VP) that signals discourse relations.3.1-3.2Elaboration3.1-3.23.1-3.3Cause3.33.1 3.23.33.1-3.2Elaboration3.1-3.3Cause3.1 3.2that are in the same textual unit.
A detailed de-scription of the block-level-score is presented inSection 3.2.
The parser completes its task whenthe required number of discourse trees that coverthe entire text is achieved.The second factor that is used to reduce thesearch space is the textual adjacency constraint.This is one of the four main constraints in con-structing a valid discourse structure (Mann andThompson 1988).
Based on this constraint, weonly consider adjacent text spans in generatingnew discourse relations.
This approach reducesthe search space remarkably, since most of thetext spans corresponding to sub-trees in thesearch space are not adjacent.
This search spaceis much smaller than the one in Marcu?s (2000)because Marcu?s system generates all possibletrees, and then uses this constraint to filter theinappropriate ones.3.2 AlgorithmTo generate discourse structures at the text-level,the constraints of textual organization and textualadjacency are used to initiate all possible con-nections among text spans.
Then, all possiblediscourse relations between text spans are positedbased on cue phrases, NP-cues, VP-cues andother cohesive devices (Le and Abeysinghe2003).
Based on this relation set, the systemshould generate the best discourse trees, each ofwhich covers the entire text.
This problem can beconsidered as searching for the best solution ofcombining discourse relations.
An algorithm thatminimizes the search space and maximizes thetree?s quality needs to be found.
We apply abeam search, which is the optimization of thebest-first search where only a predeterminednumber of paths are kept as candidates.
This al-gorithm is described in detail below.A set called Subtrees is used to store sub-treesthat have been created during the constructingprocess.
This set starts with sentential discoursetrees.
As sub-trees corresponding to contiguoustext spans are grouped together to form biggertrees, Subtrees contains fewer and fewer mem-bers.
When Subtrees contains only one tree, thistree will represent the discourse structure of theinput text.
All possible relations that can be usedto construct bigger trees at a time t form a hy-pothesis set PotentialH.
Each relation in this set,which is called a hypothesis, is assigned a scorecalled a heuristic-score, which is equal to thetotal score of all discourse cues contributing tothis relation.
A cue?s score is between 0 and 100,depending on its certainty in signaling a specificrelation.
This score can be optimized by a train-ing process, which evaluates the correctness ofthe parser?s output with the discourse trees froman existing discourse corpus.
At present, thesescores are assigned by our empirical research.In order to control the textual block level, eachsub-tree is assigned a block-level-score, depend-ing on the block levels of their children.
Thisblock-level-score is added to the heuristic-score,aiming at choosing the best combination of sub-trees to be applied in the next round.
The value ofa block-level-score is set in a different value-scale, so that the combination of sub-trees in thesame textual block always has a higher prioritythan that in a different block.?
If two sub-trees are in the same paragraph,the tree that connects these sub-trees willhave the block-level-score = 0.?
If two sub-trees are in different paragraphs,the block-level-score of their parent tree isequal to -1000 * (Li-L0), in which L0 is theparagraph level, Li is the lowest block levelthat two sub-trees are in the same unit.
Forexample, if two sub-trees are in the samesection but in different paragraphs; and thereis no subsection in this section; then Li-L0 isequal to 1.
The negative value (-1000) meansthe higher distance between two text spans,the lower combinatorial priority they get.When selecting a discourse relation, the relationcorresponding to the node with a higher block-level-score has a higher priority than the nodewith a lower one.
If relations have the sameblock-level-score, the one with higher heuristic-score is chosen.To simplify the searching process, an accu-mulated-score is used to store the value of thesearch path.
The accumulated-score of a path atone step is the highest predicted-score of thispath at the previous step.
The predicted-score ofone step is equal to the sum of the accumulated-score, the heuristic-score and the block-level-score of this step.
The searching process nowbecomes the process of searching for the hy-pothesis with highest predicted-score.At each step of the beam search, we select themost promising nodes from PotentialH that havebeen generated so far.
If a hypothesis involvingtwo text spans <Ti> and <Tj> is used, the newsub-tree created by joining the two sub-trees cor-responding to these text spans is added to Sub-trees.
Subtrees is now updated so that it does notcontain overlapping sub-trees.
PotentialH is alsoupdated according to the change in Subtrees.
Therelations between the new sub-tree and its adja-cent sub-trees in Subtrees are created and addedto PotentialH.All hypotheses computed by the discourseparser are stored in a hypothesis set calledStoredH.
This set is used to guarantee that a dis-course sub-tree will not be created twice.
Whendetecting a relation between two text spans, theparser first looks for this relation in StoredH tocheck whether it has already been created or not.If it is not found, it will be generated by a dis-course relation recognizer.The most promising node from PotentialH isagain selected and the process continues.
A bit ofdepth-first searching occurs as the most promis-ing branch is explored.
If a solution is not found,the system will start looking for a less promisingnode in one of the higher-level branches that hadbeen ignored.
The last node of the old branch isstored in the system.
The searching process re-turns to this node when all the others get badenough that it is again the most promising path.In our algorithm, we limit the branches that thesearch algorithm can switch to by a number M.This number is chosen to be 10, as in experi-ments we found that it is large enough to derivegood discourse trees.
If Subtrees contains onlyone tree, this tree will be added to the tree?s set.3The searching algorithm finishes when the num-ber of discourse trees is equal to the number oftrees required by the user.
Since the parsersearches for combinations of discourse relationsthat maximize the accumulated-score, which rep-resents the tree?s quality, the trees being gener-ated are often the best descriptions of the text.4 EvaluationThe experiments were done by testing 20 docu-ments from the RST Discourse Treebank (RST-DT 2002), including ten short documents and ten3 If no relation is found between two discourse sub-trees, aJoint relation is assigned.
Thus, a discourse tree that coversthe entire text can always be found.long ones.
The length of the documents variesfrom 30 words to 1284 words.
The syntactic in-formation of these documents was taken fromPenn Treebank, which was used as the input ofthe discourse segmenter.
In order to evaluate thesystem, a set of 22 discourse relations (list, se-quence, condition, otherwise, hypothetical, an-tithesis, contrast, concession, cause, result, cause-result, purpose, solutionhood, circumstance,manner, means, interpretation, evaluation, sum-mary, elaboration, explanation, and joint) wasused.4 The difference among cause, result andcause-result is the nuclearity role of text spans.We also carried out another evaluation with theset of 14 relations, which was created by group-ing similar relations in the set of 22 relations.
TheRST corpus, which was created by humans, wasused as the standard discourse trees for ourevaluation.
We computed the output?s accuracyon seven levels shown below:?
Level 1 - The accuracy of discourse seg-ments.
It was calculated by comparing thesegment boundaries assigned by the dis-course segmenter with the boundaries as-signed in the corpus.?
Level 2 - The accuracy of text spans?
combi-nation at the sentence-level.
The system gen-erates a correct combination if it connects thesame text spans as the corpus.?
Level 3 - The accuracy of the nuclearity roleof text spans at the sentence-level.?
Level 4 - The accuracy of discourse relationsat the sentence-level, using the set of 22 rela-tions (level 4a), and the set of 14 relations(level 4b).?
Level 5 - The accuracy of text spans?
combi-nation for the entire text.?
Level 6 - The accuracy of the nuclearity roleof text spans for the entire text.?
Level 7 - The accuracy of discourse relationsfor the entire text, using the set of 22 rela-tions (level 7a), and the set of 14 relations(level 7b).The system performance when the output of asyntactic parser is used as the input of our dis-course segmenter will be evaluated in the future,when a syntactic parser is integrated with oursystem.
It is also interesting to evaluate the per-4 See (Le and Abeysinghe 2003) for a detailed description ofthis discourse relation set.formance of the discourse parser when the cor-rect discourse segments generated by an analystare used as the input, so that we can calculate theaccuracy of our system in determining discourserelations.
This evaluation will be done in our fu-ture work.In our experiment, the output of the previousprocess was used as the input of the process fol-lowing it.
Therefore, the accuracy of one level isaffected by the accuracies of the previous levels.The human performance was considered as theupper bound for our discourse parser?s perform-ance.
This value was obtained by evaluating theagreement between human annotators using 53double-annotated documents from the RST cor-pus.
The performance of our system and humanagreement are represented by precision, recall,and F-score5, which are shown in Table 1.The F-score of our discourse segmenter is86.9%, while the F-score of human agreement is98.7%.
The level 2?s F-score of our system is66.3%, which means the error in this case is28.7%.
This error is the accumulation of errorsmade by the discourse segmenter and errors indiscourse combination, given correct discoursesegments.
With the set of 14 discourse relations,the F-score of discourse relations at the sentence-level using 14 relations (53.0%) is higher thanthe case of using 22 relations (52.2%).The most recent sentence-level discourseparser providing good results is SPADE, which isreported in (Soricut and Marcu 2003).
SPADEincludes two probabilistic models that can beused to identify edus and build sentence-leveldiscourse parse trees.
The RST corpus was alsoused in Soricut and Marcu (S&M)?s experiment,in which 347 articles were used as the training set5 The F-score is a measure combining into a single figure.We use the F-score version in which precision (P) and recall(R) are weighted equally, defined as 2*P*R/(P+R).and 38 ones were used as the test set.
S&Mevaluated their system using slightly differentcriteria than those used in this research.
Theycomputed the accuracy of the discourse seg-ments, and the accuracy of the sentence-leveldiscourse trees without labels, with 18 labels andwith 110 labels.
It is not clear how the sentence-level discourse trees are considered as correct.The performance given by the human annotationagreement reported by S&M is, therefore, differ-ent than the one used in this paper.
To comparethe performance between our system and SPADEat the sentence-level, we calculated the differenceof F-score between the system and the analyst.Table 2 presents the performance of SPADEwhen syntactic trees from the Penn Treebankwere used as the input.DiscoursesegmentsUn-labelled110labels18 la-belsSPADE 84.7 73.0 52.6 56.4Human 98.3 92.8 71.9 77.0F-score(H)- F-score(S)13.6 19.8 19.3 20.6Table 2.
SPADE performance vs. human per-formanceTable 1 and Table 2 show that the discoursesegmenter in our study has a better performancethan SPADE.
We considered the evaluation ofthe ?Unlabelled?
case in S&M?s experiment asthe evaluation of Level 2 in our experiment.
Thevalues shown in Table 1 and Table 2 imply thatthe error generated by our system is consideredsimilar to the one in SPADE.To our knowledge, there is only one reportabout a discourse parser at the text-level thatmeasures accuracy (Marcu 2000).
When usingWSJ documents from the Penn Treebank,Marcu?s decision-tree-based discourse parser re-ceived 21.6% recall and 54.0% precision for theLevel 1 2 3 4a 4b 5 6 7a 7bPrecision 88.2 68.4 61.9 53.9 54.6 54.5 47.8 39.6 40.5Recall 85.6 64.4 58.3 50.7 51.4 52.9 46.4 38.5 39.3SystemF-score 86.9 66.3 60.0 52.2 53.0 53.7 47.1 39.1 39.9Precision 98.7 88.4 82.6 69.2 74.7 73.0 65.9 53.0 57.1Recall 98.8 88.1 82.3 68.9 74.4 72.4 65.3 52.5 56.6HumanF-score 98.7 88.3 82.4 69.0 74.5 72.7 65.6 52.7 56.9F-score(Human) ?F-score(System)11.8 22 22.4 16.8 21.5 19.0 18.5 13.7 17.0Table 1.
Our system performance vs. human performancespan nuclearity; 13.0% recall and 34.3% preci-sion for discourse relations.
The recall is moreimportant than the precision since we want dis-course relations that are as correct as possible.Therefore, the discourse parser presented in thispaper shows a better performance.
However,more work needs to be done to improve the sys-tem?s reliability.As shown in Table 1, the accuracy of the dis-course trees given by human agreement is nothigh, 52.7% in case of 22 relations and 56.9% incase of 14 relations.
This is because discourse istoo complex and ill defined to easily generaterules that can automatically derive discoursestructures.
Different people may create differentdiscourse trees for the same text (Mann andThompson 1988).
Because of the multiplicity ofRST analyses, the discourse parser should beused as an assistant rather than a stand-alonesystem.5 ConclusionsWe have presented a discourse parser and evalu-ated it using the RST corpus.
The presented dis-course parser is divided into two levels: sentence-level and text-level.
The experiment showed thatsyntactic information and cue phrases are quiteeffective in constructing discourse structures atthe sentence-level, especially in discourse seg-mentation (86.9% F-score).
The discourse trees atthe text-level were generated by combining thehypothesized discourse relations among non-overlapped text spans.
We concentrated on solv-ing the combinatorial explosion in searching fordiscourse trees.
The constraints of textual adja-cency and textual organization, and a beamsearch were applied to find the best-quality treesin a search space that is much smaller than theone given by Marcu (2000).
The experiment ondocuments from the RST corpus showed that theproposed approach could produce reasonable re-sults compared to human annotator agreements.To improve the system performance, future workincludes refining the segmentation rules and im-proving criteria to select optimal paths in thebeam search.
We would also like to integrate asyntactic parser to this system.
We hope this re-search will aid the development of text process-ing such as text summarization and textgeneration.ReferencesLynn Carlson, Daniel Marcu, and Mary Ellen Oku-rowski 2002.
Building a Discourse-Tagged Corpusin the Framework of Rhetorical Structure Theory.In Current Directions in Discourse and Dialogue,Kluwer Academic Publishers.Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad,Anoop Sarkar, Aravind Joshi and Bonnie Webber2003.
D-LTAG System: Discourse Parsing with aLexicalized Tree-Adjoining Grammar.
Journal ofLogic, Language and Information, 12(3), 261-279.Edward Hovy 1993.
Automated Discourse GenerationUsing Discourse Structure Relations.
Artificial In-telligence, 63: 341-386.Huong T. Le and Geetha Abeysinghe 2003.
A Study toImprove the Efficiency of a Discourse ParsingSystem.
In Proc of CICLing-03, 104-117.Diane Litman and Julia Hirschberg 1990.
Disambi-guating cue phrases in text and speech.
In Proc ofCOLING-90.
Vol 2: 251-256.William Mann and Sandra Thompson 1988.
Rhetori-cal Structure Theory: Toward a Functional Theoryof Text Organization.
Text, 8(3): 243-281.Daniel Marcu 2000.
The theory and practice of dis-course parsing and summarization.
MIT Press,Cambridge, Massachusetts, London, England.Livia Polanyi, Chris Culy, Gian Lorenzo Thione andDavid Ahn 2004.
A Rule Based Approach to Dis-course Parsing.
In Proc of SigDial2004.Gisela Redeker 1990.
Ideational and pragmatic mark-ers of discourse structure.
Journal of Pragmatics,367-381.RST-DT 2002.
RST Discourse Treebank .
LinguisticData Consortium.
http://www.ldc.upenn.edu/Cata-log/CatalogEntry.jsp?catalogId=LDC2002T07.Lloyd Rutledge, Brian Bailey, Jacco van Ossenbrug-gen, Lynda Hardman, and Joost Geurts 2000.
Gen-erating Presentation Constraints from RhetoricalStructure.
In Proc of HYPERTEXT 2000.Deborah Schiffrin 1987.
Discourse markers.
Cam-bridge: Cambridge University Press.Radu Soricut and Daniel Marcu 2003.
Sentence LevelDiscourse Parsing using Syntactic and Lexical In-formation.
In Proc of HLT-NAACL 2003.Mark Torrance, and Nadjet Bouayad-Agha 2001.Rhetorical structure analysis as a method for under-standing writing processes.
In Proc of MAD 2001.
