Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780?790,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAn Infinite Hierarchical Bayesian Model of Phrasal TranslationTrevor CohnDepartment of Computer ScienceThe University of SheffieldSheffield, United Kingdomt.cohn@sheffield.ac.ukGholamreza HaffariFaculty of Information TechnologyMonash UniversityClayton, Australiareza@monash.eduAbstractModern phrase-based machine translationsystems make extensive use of word-based translation models for inducingalignments from parallel corpora.
Thisis problematic, as the systems are inca-pable of accurately modelling many trans-lation phenomena that do not decomposeinto word-for-word translation.
This pa-per presents a novel method for induc-ing phrase-based translation units directlyfrom parallel data, which we frame aslearning an inverse transduction grammar(ITG) using a recursive Bayesian prior.Overall this leads to a model which learnstranslations of entire sentences, while alsolearning their decomposition into smallerunits (phrase-pairs) recursively, terminat-ing at word translations.
Our experimentson Arabic, Urdu and Farsi to Englishdemonstrate improvements over competi-tive baseline systems.1 IntroductionThe phrase-based approach (Koehn et al, 2003)to machine translation (MT) has transformed MTfrom a narrow research topic into a truly usefultechnology to end users.
Leading translation sys-tems (Chiang, 2007; Koehn et al, 2007; Marcu etal., 2006) all use some kind of multi-word transla-tion unit, which allows translations to be producedfrom large canned units of text from the trainingcorpus.
Larger phrases allow for the lexical con-text to be considered in choosing the translation,and also limit the number of reordering decisionsrequired to produce a full translation.Word-based translation models (Brown et al,1993) remain central to phrase-based model train-ing, where they are used to infer word-level align-ments from sentence aligned parallel data, fromwhich phrasal translation units are extracted us-ing a heuristic.
Although this approach demon-strably works, it suffers from a number of short-comings.
Firstly, many phrase-based phenomenawhich do not decompose into word translations(e.g., idioms) will be missed, as the underlyingword-based alignment model is unlikely to pro-pose the correct alignments.
Secondly, the rela-tionship between different phrase-pairs is not con-sidered, such as between single word translationsand larger multi-word phrase-pairs or where onelarge phrase-pair subsumes another.This paper develops a phrase-based translationmodel which aims to address the above short-comings of the phrase-based translation pipeline.Specifically, we formulate translation using in-verse transduction grammar (ITG), and seek tolearn an ITG from parallel corpora.
The noveltyof our approach is that we develop a Bayesianprior over the grammar, such that a nontermi-nal becomes a ?cache?
learning each productionand its complete yield, which in turn is recur-sively composed of its child constituents.
This isclosely related to adaptor grammars (Johnson etal., 2007a), which also generate full tree rewritesin a monolingual setting.
Our model learns trans-lations of entire sentences while also learning theirdecomposition into smaller units (phrase-pairs) re-cursively, terminating at word translations.
Themodel is richly parameterised, such that it can de-scribe phrase-based phenomena while also explic-itly modelling the relationships between phrase-pairs and their component expansions, thus ame-liorating the disconnect between the treatment ofwords versus phrases in the current MT pipeline.We develop a Bayesian approach using a Pitman-Yor process prior, which is capable of modellinga diverse range of geometrically decaying distri-butions over infinite event spaces (here translationphrase-pairs), an approach shown to be state of theart for language modelling (Teh, 2006).780We are not the first to consider this idea; Neu-big et al (2011) developed a similar approach forlearning an ITG using a form of Pitman-Yor adap-tor grammar.
However Neubig et al?s work wasflawed in a number of respects, most notably interms of their heuristic beam sampling algorithmwhich does not meet either of the Markov ChainMonte Carlo criteria of ergodicity or detailed bal-ance.
Consequently their approach does not con-stitute a valid Bayesian model.
In contrast, thispaper provides a more rigorous and theoreticallysound method.
Moreover our approach results inconsistent translation improvements across a num-ber of translation tasks compared to Neubig et al?smethod, and a competitive phrase-based baseline.2 Related WorkInversion transduction grammar (or ITG) (Wu,1997) is a well studied synchronous grammar for-malism.
Terminal productions of the form X ?e/f generate a word in two languages, and non-terminal productions allow phrasal movement inthe translation process.
Straight productions, de-noted by their non-terminals inside square brack-ets [...], generate their symbols in the given or-der in both languages, while inverted productions,indicated by angled brackets ?...
?, generate theirsymbols in the reverse order in the target language.In the context of machine translation, ITGhas been explored for statistical word alignmentin both unsupervised (Zhang and Gildea, 2005;Cherry and Lin, 2007; Zhang et al, 2008; Pauls etal., 2010) and supervised (Haghighi et al, 2009;Cherry and Lin, 2006) settings, and for decoding(Petrov et al, 2008).
Our paper fits into the re-cent line of work for jointly inducing the phrase ta-ble and word alignment (DeNero and Klein, 2010;Neubig et al, 2011).
The work of DeNero andKlein (2010) presents a supervised approach tothis problem, whereas our work is unsupervisedhence more closely related to Neubig et al (2011)which we describe in detail below.A number of other approaches have been de-veloped for learning phrase-based models frombilingual data, starting with Marcu and Wong(2002) who developed an extension to IBM model1 to handle multi-word units.
This pioneer-ing approach suffered from intractable inferenceand moreover, suffers from degenerate solutions(DeNero and Klein, 2010).
Our approach is simi-lar to these previous works, except that we imposeadditional constraints on how phrase-pairs can betiled to produce a sentence pair, and moreover,we seek to model the embedding of phrase-pairsin one another, something not considered by thisprior work.
Another strand of related research isin estimating a broader class of synchronous gram-mars than ITGs, such as SCFGs (Blunsom et al,2009b; Levenberg et al, 2012).
Conceptually, ourwork could be readily adapted to general SCFGsusing similar techniques.This work was inspired by adaptor grammars(Johnson et al, 2007a), a monolingual grammarformalism whereby a non-terminal rewrites in asingle step as a complete subtree.
The model priorallows for trees to be generated as a mixture of acache and a base adaptor grammar.
In our case,we have generalised to a bilingual setting using anITG.
Additionally, we have extended the model toallow recursive nesting of adapted non-terminals,such that we end up with an infinitely recursiveformulation where the top-level and base distribu-tions are explicitly linked together.As mentioned above, ours is not the first workattempting to generalise adaptor grammars for ma-chine translation; (Neubig et al, 2011) also devel-oped a similar approach based around ITG using aPitman-Yor Process prior.
Our approach improvesupon theirs in terms of the model and inference,and critically, this is borne out in our experimentswhere we show uniform improvements in transla-tion quality over a baseline system, as comparedto their almost entirely negative results.
We be-lieve that their approach had a number of flaws:For inference they use a beam-search, which mayspeed up processing but means that they are nolonger sampling from the true distribution, nor adistribution with the same support as the posterior.Moreover they include a Metropolis-Hastings cor-rection step, which is required to correct the sam-ples to account for repeated substructures whichwill be otherwise underrepresented.
Consequentlytheir approach does not constitute a Markov ChainMonte Carlo sampler, but rather a complex heuris-tic.The other respect in which this work differsfrom Neubig et al (2011) is in terms of model for-mulation.
They develop an ITG which generatesphrase-pairs as terminals, while we employ a morerestrictive word-based model which forces the de-composition of every phrase-pair.
This is an im-portant restriction as it means that we jointly learn781a word and phrase based model, such that wordbased phenomena can affect the phrasal struc-tures.
Finally our approach models separately thethree different types of ITG production (mono-tone, swap and lexical emission), allowing for aricher parameterisation which the model exploitsby learning different hyper-parameter values.3 ModelThe generative process of the model follows thatof ITG with the following simple grammarX ?
[X X] | ?X X?X ?
e/f | e/?
| ?/f ,where [?]
denotes monotone ordering and ???
de-notes a swap in one language.
The symbol ?
de-notes the empty string.
This corresponds to a sim-ple generative story, with each stage being a non-terminal rewrite starting with X and terminatingwhen there are no frontier non-terminals.A popular variant is a phrasal ITG, where theleaves of the ITG tree are phrase-pairs and thetraining seeks to learn a segmentation of the sourceand target which yields good phrases.
We wouldnot expect this model to do very well as it cannotconsider overlapping phrases, but instead is forcedinto selecting between many competing ?
and of-ten equally viable ?
options.
Our approach im-proves over the phrasal model by recursively gen-erating complete phrases.
This way we don?t insiston a single tiling of phrases for a sentence pair, butexplicitly model the set of hierarchically nestedphrases as defined by an ITG derivation.
This ap-proach is closer in spirit to the phrase-extractionheuristic, which defines a set of ?atomic?
terminalphrase-pairs and then extracts every combinationof these atomic phase-pairs which is contiguous inthe source and target.1The generative process is that we draw a com-plete ITG tree, t ?
P2(?
), as follows:1. choose the rule type, r ?
R, where r ?
{mono, swap, emit}2. for r = mono(a) draw the complete subtree expansion,t = X ?
[.
.
.]
?
TM3.
for r = swap(a) draw the complete subtree expansion,t = X ?
?.
.
.?
?
TS1Our technique considers the subset of phrase-pairs whichare consistent with the ITG tree.4.
for r = emit(a) draw a pair of strings, (e, f) ?
E(b) set t = X ?
e/fNote that we split the problem of drawing a treeinto two steps: first choosing the top-level ruletype and then drawing a rule of that type.
Thisgives us greater control than simply drawing a treeof any type from one distribution, due to our pa-rameterisation of the priors over the model param-eters TM , TS and E.To complete the generative story, we need tospecify the prior distributions for TM , TS andE.
First, we deal with the emission distribu-tion, E which we drawn from a Dirichlet Pro-cess prior E ?
DP(bE , P0).
We restrict the emis-sion rules to generate word pairs rather than phrasepairs.2 For the base distribution, P0, we use a sim-ple uniform distribution over word pairs,P0(e, f) =?????
?2 1VEVF e 6= ?, f 6= ??(1?
?)
1VF e = ?, f 6= ??(1?
?)
1VE e 6= ?, f = ?,where the constant ?
denotes the binomial proba-bility of a word being aligned.3We use Pitman-Yor Process priors for the TMand TS parametersTM ?
PYP(aM , bM , P1(?|r = mono))TS ?
PYP(aS , bS , P1(?|r = swap))where P1(t1, t2|r) is a distribution over a pair oftrees (the left and right children of a monotone orswap production).
P1 is defined as follows:1. choose the complete left subtree t1 ?
P2,2.
choose the complete right subtree t2 ?
P2,3.
set t = X ?
[t1 t2] or t = X ?
?t1 t2?depending on rThis generative process is mutually recursive: P2makes draws from P1 and P1 makes draws fromP2.
The recursion is terminated when the rule typer = emit is drawn.Following standard practice in Bayesian mod-els, we integrate out R, TM , TS and E. Thismeans draws from P2 (or P1) are no longer iid:for any non-trivial tree, computing its probabil-ity under this model is complicated by the fact2Note that we could allow phrases here, but given themodel can already reason over phrases by way of its hier-archical formulation, this is an unnecessary complication.3We also experimented with using word translation prob-abilities from IBM model 1, based on the prior used by Lev-enberg et al (2012), however we found little empirical differ-ence compared with this simpler uniform model.782that the probability of its two subtrees are inter-dependent.
This is best understood in terms ofthe Chinese Restaurant Franchise (CRF; Teh et al(2006)), which describes the posterior distributionafter integrating out the model parameters.
In ourcase we can consider the process of drawing a treefrom P2 as a customer entering a restaurant andchoosing where to sit, from an infinite set of ta-bles.
The seating decision is based on the numberof other customers at each table, such that populartables are more likely to be joined than unpopularor empty ones.
If the customer chooses an occu-pied table, the identity of the tree is then set tobe the same as for the other customers also seatedthere.
For empty tables the tree must be sampledfrom the base distribution P1.
In the standard CRFanalogy, this leads to another customer enteringthe restaurant one step up in the hierarchy, andthis process can be chained many times.
In ourcase, however, every new table leads to new cus-tomers reentering the original restaurant ?
thesecorrespond to the left and right child trees of amonotone or swap rule.
The recursion terminateswhen a table is shared, or a new table is labelledwith a emit rule.3.1 InferenceThe probability of a tree (i.e., a draw from P2) un-der the model isP2(t) = P (r)P2(t|r) (1)where r is the rule type, one of mono, swap oremit.
The distribution over types, P (r), is de-fined asP (r) = nT,?r + bT 13nT,?
+ bTwhere nT,?
are the counts over rules of types.4The second component in (1), P2(t|r), is de-fined separately for each rule type.
For r = monoor r = swap rules, it is defined asP2(t|r) =n?t,r ?K?t,rarn?r + br+ K?r ar + brn?r + brP1(t1, t2|r) ,(2)where n?t,r is the count for tree t in the other train-ing sentences, K?t,r is the table count for t and n?r4The conditioning on event and table counts, n?,K?
isomitted for clarity.and K?r are the total count of trees and tables, re-spectively.
Finally, the probability for r = emitis given byP2(t|r = emit) =n?t,E + bEP0(e, f)n?r + br,where t = X ?
e/f .To complete the derivation we still need to de-fine P1, which is formulated asP1(t1, t2) = P2(t1)P2(t2|t1) ,where the conditioning of the second recursive callto P2 reflects that the counts n?
and K?
maybe affected by the first draw from P2.
Althoughthese two draws are assumed iid in the prior, aftermarginalising out T they are no longer indepen-dent.
For this reason, evaluating P2(t) is computa-tionally expensive, requiring tracking of repeatedsubstructures in descendent sub-trees of t, whichmay affect other descendants.
This results in anasymptotic complexity exponential in the numberof nodes in the tree.
For this reason we considertrees annotated with binary values denoting theirtable assignment, namely whether they share a ta-ble or are seated alone.
Given this, the calculationis greatly simplified, and has linear complexity.5We construct an approximating ITG followingthe technique used for sampling trees from mono-lingual tree-substitution grammars (Cohn et al,2010).
To do so we encode the first term from(2) separately from the second term (correspond-ing to draws from P1).
Summing together thesetwo alternate paths ?
i.e., during inside inference ?we recover P2 as shown in (2).
The full grammartransform for inside inference is shown in Table 1.The sampling algorithm closely follows theprocess for sampling derivations from BayesianPCFGs (Johnson et al, 2007b).
For each sentence-pair, we first decrement the counts associated withits current tree, and then sample a new deriva-tion.
This involves first constructing the insidelattice using the productions in Table 1, and thenperforming a top-down sampling pass.
Aftersampling each derivation from the approximatinggrammar, we then convert this into its correspond-ing ITG tree, which we then score with the fullmodel and accept or reject the sample using the5To support this computation, we track explicit table as-signments for every training tree and their component sub-trees.
We also sample trees labelled with seating indicatorvariables.783Type X ?M P (r = mono)X ?
S P (r = swap)X ?
E P (r = emit)Base M ?
[XX] K?MaM+bMn?M+bMS ?
?XX?
K?S aS+bSn?S +bSCountFor every tree, t, of type r = mono, with nt,M > 0:M ?
sig(t) n?t,M?K?t,Marn?M+bMsig(t)?
yield(t) 1For every tree, t, of type r = swap, with nt,S > 0:S ?
sig(t) n?t,S?K?t,SaSn?S +bSsig(t)?
yield(t) 1Emit For every word pair, e/f in sentence pair,where one of e, f can be ?
:E ?
e/f P2(t)Table 1: Grammar transformation rules for MAPinside inference.
The function sig(t) returns aunique identifier for the complete tree t, andthe function yield(t) returns the pair of terminalstrings from the yield of t.Metropolis-Hastings algorithm.6 Accepted sam-ples then replace the old tree (otherwise the oldtree is retained) and the model counts are incre-mented.
This process is then repeated for eachsentence pair in the corpus in a random order.4 ExperimentsDatasets We train our model across threelanguage pairs: Urdu?English (UR-EN),Farsi?English (FA-EN), and Arabic?English(AR-EN).
The corpora statistics of these trans-lation tasks are summarised in Table 2.
TheUR-EN corpus comes from NIST 2009 translationevaluation.7 The AR-EN training data consistsof the eTIRR corpus (LDC2004E72), the Ara-bic news corpus (LDC2004T17), the Ummahcorpus (LDC2004T18), and the sentences withconfidence c > 0.995 in the ISI automaticallyextracted web parallel corpus (LDC2006T02).For FA-EN, we use TEP8 Tehran English-PersianParallel corpus (Pilevar and Faili, 2011), whichconsists of conversational/informal text extracted6The full model differs from the approximating grammarin that it accounts for inter-dependencies between subtreesby recursively tracking the changes in the customer and tablecounts while scoring the tree.
Around 98% of samples wereaccepted in our experiments.7http://www.itl.nist.gov/iad/mig/tests/mt/20098http://ece.ut.ac.ir/NLP/resources.htmsource target sentencesUR-EN 745K 575K 148KFA-EN 4.7M 4.4M 498KAR-EN 1.94M 2.08M 113KTable 2: Corpora statistics showing numbers ofparallel sentences and source and target words forthe training sets.from 1600 movie subtitles.
We tokenized thiscorpus, removed noisy single-word sentences,randomly selected the development and test sets,and used the rest of the corpus as the training set.We discard sentences with length above 30 fromthe datasets for all experiments.9Sampler configuration Samplers are initialisedwith trees created from GIZA++ alignmentsconstructed using a SCFG factorisation method(Blunsom et al, 2009a).
This algorithm repre-sents the translation of a sentence as a large SCFGrule, which it then factorises into lower rank SCFGrules, a process akin to rule binarisation com-monly used in SCFG decoding.
Rules that can-not be reduced to a rank-2 SCFG are simplifiedby dropping alignment edges until they can befactorised, the net result being an ITG derivationlargely respecting the alignments.10The blocked sampler was run 1000 iterationsfor UR-EN, 100 iterations for FA-EN and AR-EN.
After each full sampling iteration, we resam-ple all the hyper-parameters using slice-sampling,with the following priors: a ?
Beta(1, 1),b ?
Gamma(10, 0.1).
Figure 1 shows the poste-rior probability improves with each full samplingiterations.
The alignment probability was set to?
= 0.99.
The sampling was repeated for 5 in-dependent runs, and we present results where wecombine the outputs of these runs.
This is a formof Monte Carlo integration which allows us to rep-resent the uncertainty in the posterior, while alsorepresenting multiple modes, if present.The time complexity of our inference algorithmis O(n6), which can be prohibitive for large scalemachine translation tasks.
We reduce the com-plexity by constraining the inside inference toconsider only derivations which are compatible9Hence the BLEU scores we get for the baselines mayappear lower than what reported in the literature.10Using the factorised alignments directly in a translationsystem resulted in a slight loss in BLEU versus using the un-factorised alignments.
Our baseline system uses the latter.7840 100 200 300 400 500?9100000?9050000?9000000?8950000iterationlog posteriorFigure 1: Training progress on the UR-EN corpus,showing the posterior probability improving witheach full sampling iteration.
Different colours de-note independent sampling runs.llllllllllllllllllllllllllllllll ll lll lllllllllllllllllllllllllllllllll lllllllllllllllllllll lllllll lllllllllllllll lllllll lll llllll lllllllllll llll lll llll llll llllllll0 5 10 15 20 25 301e?051e?031e?01average sentence lengthtime(s)Figure 2: The runtime cost of bottom-up inside in-ference and top-down sampling as a function ofsentence length (UR-EN), with time shown on alogarithmic scale.
Full ITG inference is shownwith red circles, and restricted inference using theintersection constraints with blue triangles.
Theaverage time complexity for the latter is roughlyO(l4), as plotted in green t = 2?
10?7l4.with high confidence alignments from GIZA++.11Figure 2 shows the sampling time with respectto the average sentence length, showing that ouralignment-constrained sampling algorithm is bet-ter than the unconstrained algorithm with empir-ical complexity of n4.
However, the time com-plexity is still high, so we set the maximum sen-tence length to 30 to keep our experiments practi-cable.
Presumably other means of inference maybe more efficient, such as Gibbs sampling (Lev-enberg et al, 2012) or auxiliary variable sampling(Blunsom and Cohn, 2010); we leave these exten-sions to future work.Baselines.
Following (Levenberg et al, 2012;Neubig et al, 2011), we evaluate our model byusing its output word alignments to construct aphrase table.
As a baseline, we train a phrase-based model using the moses toolkit12 based onthe word alignments obtained using GIZA++ inboth directions and symmetrized using the grow-diag-final-and heuristic13 (Koehn et al, 2003).This alignment is used as input to the rule fac-torisation algorithm, producing the ITG trees withwhich we initialise our sampler.
To put our resultsin the context of the previous work, we also com-pare against pialign (Neubig et al, 2011), an ITGalgorithm using a Pitman-Yor process prior, as de-scribed in Section 2.14In the end-to-end MT pipeline we use a stan-dard set of features: relative-frequency and lexicaltranslation model probabilities in both directions;distance-based distortion model; language modeland word count.
We set the distortion limit to6 and max-phrase-length to 7 in all experiments.We train 3-gram language models using modifiedKneser-Ney smoothing.
For AR-EN experimentsthe language model is trained on English data as(Blunsom et al, 2009a), and for FA-EN and UR-EN the English data are the target sides of thebilingual training data.
We use minimum errorrate training (Och, 2003) with nbest list size 100to optimize the feature weights for maximum de-velopment BLEU.11These are taken from the final model 4 word alignments,using the intersection of the source-target and target-sourcemodels.
These alignments are very high precision (but havelow recall), and therefore are unlikely to harm the model.12http://www.statmt.org/moses13We use the default parameter settings in both moses andGIZA++.14http://www.phontron.com/pialign785Baselines This paperGIZA++ pialign individual combinationUR-EN 16.95 15.65 16.68 ?
.12 16.97FA-EN 20.69 21.41 21.36 ?
.17 21.50AR-ENMT03 44.05 43.30 44.8 ?
.28 45.10MT04 38.15 37.78 38.4 ?
.08 38.4MT05 42.81 42.18 43.13 ?
.23 43.45MT08 32.43 33.00 32.7 ?
.15 32.80Table 3: The BLEU scores for the translation tasks of three language pairs.
The individual column showthe average and 95% confidence intervals for 5 independent runs, whereas the combination column showthe results for combining the phrase tables of all these runs.
The baselines are GIZA++ alignments andthose generated by the pialign (Neubig et al, 2011) bold: the best result.1 2 5 10 20 50 1001e?051e?031e?01rule frequencyfraction ofgrammarmonotoneswapemitFigure 3: Fraction of rules with a given frequency,using a single sample grammar (UR-EN).4.1 ResultsTable 3 shows the BLEU scores for the three trans-lation tasks UR/AR/FA?EN based on our methodagainst the baselines.
For our models, we reportthe average BLEU score of the 5 independent runsas well as that of the aggregate phrase table gen-erated by these 5 independent runs.
There area number of interesting observations in Table 3.Firstly, combining the phrase tables from indepen-dent runs results in increased BLEU scores, possi-bly due to the representation of uncertainty in theoutputs, and the representation of different modescaptured by the individual models.
We believe thistype of Monte Carlo model averaging should beconsidered in general when sampling techniquesare employed for grammatical inference, e.g.
inparsing and translation.
Secondly, our approachconsistently improves over the Giza++ baselineoften by a large margin, whereas pialign under-performs the GIZA++ baseline in many cases.Thirdly, our model consistently outperforms pi-align (except in AR-EN MT08 which is veryclose).
This highlights the modeling and inferencedifferences between our method and the pialign.5 AnalysisIn this section, we present some insights about thelearned grammar and the model hyper-parameters.Firstly, we start by presenting various statisticsabout different learned grammars.
Figure 3 showsthe fraction of rules with a given frequency foreach of the three rule types.
The three types of ruleexhibit differing amounts of high versus low fre-quency rules, and all roughly follow power laws.As expected, there is a higher tendency to reusehigh-frequency emissions (or single-word transla-tion) compared to other rule types, which are thebasic building blocks to compose larger rules (orphrases).
Table 4 lists the high frequency mono-tone and swap rules in the learned grammar.
Weobserve the high frequency swap rules capture re-ordering in verb clusters, preposition-noun inver-sions and adjective-noun reordering.
Similar pat-terns are seen in the monotone rules, along withsome common canned phrases.
Note that ?in Iraq?appears twice, once as an inversion in UR-EN andanother time in monotone order for AR-EN.Secondly, we analyse the values learned forthe model hyper-parameters; Figure 4.
(a) showsthe posterior distribution over the hyper-parametervalues.
There is very little spread in the inferredvalues, suggesting the sampling chains may haveconverged.
Furthermore, there is a large differ-ence between the learned hyper-parameters for themonotone rules versus the swap rules.
For thePitman-Yor Process prior, the values of the hyper-7866`bB@1M;HBb?TBHB;M bm`2fKhK M-+?B27f` vb- vQm bm`2fKhK Mv- BK bm`2fKhK MK- #Qbbf` vb- KF2 bm`2fKhK M-`2 vQm bm`2fKhK Mv- Mvrvf?` >H- T`2bB/2Mif` vb DK?r`- MQi bm`2fKhK M MvbiK- BKbm`2fKhK MK F?- B?K bm`2fKhK MK- bm`2fKhK MQm` K2i?Q/ bm`2fKhK }- ?
p2f/ $ i?- #2f# $- ?
p2f/ $ i?
# $- H2i K2f `- #2+mb2 Q7fth`-bm`2fKhK } M- /QfF` `- +QK2 QMfxr/ #- 2t+mb2 K2f##t- FBHHf` #F- +QK2 QMfxr/#-KQ`2 i?
Mf$ i`- #2?BM/fT $- r?
i /QfKMwr`i- r?
i /Q vQmfKMwr`i - FBHHfF $- /QMirQ``vfM;`M M#- Bb Bif$ /?- r2H+QK2ftr $ %- +?B27f` } vb- KF2 bm`2fKhK- BbfKv $-KF2 bm`2fKhK }- KF2 bm`2fKhK} M- BK bQ``vf##t- H27if;  - B7f;` $`#B+@1M;HBb?TBHB;M bB/ Xf???
?- bii2bf??????
?- mMBi2/f???????
?- H@r7/f ?
????
?- 2zQ`ibf??
?- Q7 Kbb/2bi`m+iBQMf??????
?????
?- vQmKf????
?- DBMiQf???
??
?- HK H vQmKf?????
?????
?- H@BiiB?
/-f??????
?- i?2 }2H/ Q7f???
?- BbHK#/f????
?- b+?2/mH2/f??????
?
?- H@HK H@vQmKf?????
??????
?- f H@?
vif?
?????
?- T2MBMbmHf???????
??
?- K2Mr?BH2f????
????
?- T`BK2f????
?- i?2BMpBiiBQMf??
???
?- f H@?
vi -f?
?????
?- FQ`2M T2MBMbmHf???????
???????
??
?- H@M?
` ?f?????
?DD- /2T`iK2Mif????????
????
?- +Qi2f??
?- b TQbbB#H2f????
??
?- H HK H vQmKf?????
?????
?- -H@HK H@vQmK -f?????
?????
?- i i?2 BMpBiiBQMf??
????
?- D+[m2bf???
??????
?- r2HH bf???-TQBMibf??
?- pH/BKB` TmiBMf?????
????????
?????
?- ;2Q`;2 rX #mb?f???
?Qm` K2i?Q/ i?2 mMBi2/f??????
?- mb /QHH`bf?????
?- T`BK2f???????
???
?- +?BM ?f????
?- bTQF2bKMf??????
??- KMvf?
?- Bb 2tT2+i2/f??????
?- Bb 2tT2+i2/ iQf??????
?- i H2bif????
?- QM im2b/vf??
?- 2;vTi?f??
?- i?m`b/vf??
?- i?2 mMf??????
?- QM i?m`b/vf??
?- 7`B/vf??
?- QM 7`B/vf??
?- iQf??
?-H@r7/ -f?
????
?- i?2 mbf??????
?- 7Q`f?
?????
?- }`bi iBK2f??????
????
?- 7m`i?2`f?
?- B`[?f?????
?- Bb`2HB T`BK2f??????????
???????
???
?- i?2 irQf??????
?- QM bim`/vf??
?- QM bmM/vf???-mXbXf??????
?- pB2rbf????
?- b?
`QM ?f????
?- +QmMi`v ?f?????
?- ?2 bB/f??
?- Bb`2H ?f??????
?- T2QTH2?f????
?- ?2`2f?????
??
?- +?BM ?f????
?- - ?2 bB/f????
?- 2`HB2`f???
?
?- +?BM ?f??????
?-i H2bif???
??
?
?- i?2 mXbXf??????
?- i?2 ;xf???
?- i?2 ;x bi`BTf???
?- ?2 //2/f??
?- `22tT2+i2/f??????
?- `2 2tT2+i2/ iQf??????
?- `2 2tT2+i2/ iQf???????
?
?- KBHHBQM mXbXf?????-++Q`/BM;f??
?- iQf????
?- Q`/2`f?
?- BM Q`/2`f?
?- ?2 TQBMi2/f???
?- K7 - b?
`[ Hf?
?????
?- K7- b?
`[ H rbif?
?????
?- `7i ?f????
?kTable 5: Good phrase pairs in the top-100 high frequency phrase pairs specific to the phrase tablescoming from our method vs that of pialign for FA-EN and AR-EN translation tasks.parameters affects the rate at which the number oftypes grows compared to the number of tokens.Specifically, as the discount a or the concentra-tion b parameters increases we expect for a rela-tive increase in the number of types.
If the numberof observed monotone and swap rules were equal,then there would be a higher chance in reusing themonotone rules.
However, the number of observedmonotone and swap rules are not equal, as plottedin Figure 4.(b).
Similar results were observed forthe other language pairs (figures omitted for spacereasons).Thirdly, we performed a manual evaluation forthe quality of the phrase-pairs learned exclusivelyby our method vs pialign.
For each method,we considered the top-100 high frequency phrase-pairs which are specific to that method.
Then weasked a bilingual human expert to identify rea-sonably well phrase-pairs among these top-100phrase-pairs.
The results are summarized in Ta-ble 5, and show that we learn roughly twice asmany reasonably good phrase-pairs for AR-ENand FA-EN compared to pialign.ConclusionsWe have presented a novel method for learn-ing a phrase-based model of translation directlyfrom parallel data which we have framed as learn-ing an inverse transduction grammar (ITG) us-ing a recursive Bayesian prior.
This has ledto a model which learns translations of en-tire sentences, while also learning their decom-position into smaller units (phrase-pairs) recur-sively, terminating at word translations.
We havepresented a Metropolis-Hastings sampling algo-rithm for blocked inference in our non-parametricITG.
Our experiments on Urdu-English, Arabic-English, and Farsi-English translation tasks alldemonstrate improvements over competitive base-line systems.AcknowledgementsThe first author was supported by the EPSRC(grant EP/I034750/1) and an Erasmus-Mundusscholarship funding a research visit to Melbourne.The second author was supported by an early ca-reer research award from Monash University.7870.905 0.910 0.915 0.920 0.92502004006008001000am and asDensity1000 2000 3000 40000.00000.00100.0020bm and bsDensity0 5 10 15 20 25 300.000.010.020.030.040.050.06beDensity65000 65500 660000.00000.00050.00100.0015btDensity(a)291000 292000 2930000.00000.00040.00080.0012monotone176000 1770000.00000.00040.00080.0012swapDensity(b)Figure 4: (a) Posterior over the hyper-parameters,aM , aS , bM , bS , bE , bT , measured for UR-EN us-ing samples 400?500 for 3 independent samplingchains, and the intersection constraints.
(b) Poste-rior over the number of monotone and swap rulesin the resultant grammars.
The distribution foremission rules was also peaked about 147k rules.key ( ( ( ?2fM?rL ) ( ?fMu ) ) ( bB/fF? ) )kkk ( ?
( bB/fF? ) ( ?fMu ) ?
( i?
ifF? )
)kRN ( ( ( ( ?2fM?rL ) ( ?fMu ) )( bB/fF? ) ) ( i?
ifF? )
)R93 ( ( ( B7f;` ) ( ?f% ) ) ( vQmfT ) )Ry3 ( ( ?2fM?rL ) ?
( bB/fF? ) ( ?fMu ) ?
)R3k ?
( rBHHf; ) ( #2f?r ) ?RkN ?
( Bbf?u ) ( MQifM?vL ) ?Rkj ?
( ?
bf?u ) ( #22Mf;v ) ?Ry9 ?
( rBHHf; ) ( #2fDvu ) ?Ryj ?
( BMfKvL ) ( B`[f1`[ ) ?l`/m@1M;HBb?3Ny ( ( QM2fvFv ) ( Q7fx ) )39j ( ?
( v2?f`? )
( ?f% ) ?
( XfX ) )dj3 ( ( rBi?f# ) ( K2fKM ) )e99 ( ( ( ( QFvf# ) ( ?f$ ) ) ( ?f? )
) ( XfX ) )ey3 ( ( iQf#? )
( K2fKM ) )k8R ?
( Bbf/? )
( Bif$ ) ?kky ?
( i2HHf#;r ) ( K2fKM ) ?RNN ?
I ( Bf? )
( +MfirMK ) ?
( ?ifMKv ) =RNy ?
( ( r?QfFv ) ( `2f?biv ) ) ( vQmfir ) ?R3d ?
( iQH/f;7i ) ( K2fKM ) ?6`bB@1M;HBb?8ee ( ( BMf?? )
( B`[f?????? )
)9R9 ( ( BMf?? )
( 2;vTif??? )
)jNR ( ( i?Bbf??? )
( v2`f????? )
)j8e ( ( b?
`[f????? )
( H@rbif?????? )
)jyy ( ( BMf?? )
( B`[f?????? )
)9yk9 ?
( Xf ) ( ?f ) ?RjRk ?
I ( i?2f ) ( mMBi2/f??????? )
?
( bii2bf???????? )
=ee8 ?
( mMBi2/f??????? )
( bii2bf???????? )
?e8y ?
( Hbif?????? )
( v2`f????? )
?9ed ?
I ( i?2f ) ( mMBi2/f??????? )
?
( MiBQMbf????? )
=`#B+@1M;HBb?aQK2 HiBM i2ti M/ BMHBM2 `#B+, ?????
?????
?l`/m- 6`bB- `#B+Table 4: Top 5 monotone and swap productionsand their counts.
Rules with mostly punctuationor encoding 1:many or many:1 alignments wereomitted.788ReferencesPhil Blunsom and Trevor Cohn.
2010.
Inducing syn-chronous grammars with slice sampling.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 238?241,Los Angeles, California, June.
Association for Com-putational Linguistics.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009a.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In ACL2009, Singa-pore, August.Phil Blunsom, Trevor Cohn, and Miles Osborne.2009b.
Bayesian synchronous grammar induction.In D. Koller, D. Schuurmans, Y. Bengio, and L. Bot-tou, editors, Advances in Neural Information Pro-cessing Systems 21, pages 161?168.
MIT Press.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?311.Colin Cherry and Dekang Lin.
2006.
Soft syntacticconstraints for word alignment through discrimina-tive training.
In Proceedings of COLING/ACL.
As-sociation for Computational Linguistics.Colin Cherry and Dekany Lin.
2007.
Inversion trans-duction grammar for joint phrasal translation mod-eling.
In Proc.
of the HLT-NAACL Workshop onSyntax and Structure in Statistical Translation (SSST2007), Rochester, USA.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree-substitution grammars.
Journalof Machine Learning Research, pages 3053?3096.John DeNero and Dan Klein.
2010.
Discriminativemodeling of extraction sets for machine translation.In The 48th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies (ACL).Aria Haghighi, John Blitzer, and Dan Klein.
2009.Better word alignments with supervised itg models.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, Suntec, Singapore.
Association forComputational Linguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Gold-water.
2007a.
Adaptor grammars: A framework forspecifying compositional nonparametric bayesianmodels.
In B. Scho?lkopf, J. Platt, and T. Hoffman,editors, Advances in Neural Information ProcessingSystems 19, pages 641?648.
MIT Press, Cambridge,MA.Mark Johnson, Thomas L Griffiths, and Sharon Gold-water.
2007b.
Bayesian inference for PCFGs viaMarkov chain Monte Carlo.
In Proc.
of the 7th Inter-national Conference on Human Language Technol-ogy Research and 8th Annual Meeting of the NAACL(HLT-NAACL 2007), pages 139?146.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of the 3rd International Conference on Human Lan-guage Technology Research and 4th Annual Meet-ing of the NAACL (HLT-NAACL 2003), pages 81?88,Edmonton, Canada, May.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
of the 45th Annual Meeting of the ACL (ACL-2007), Prague.Abby Levenberg, Chris Dyer, and Phil Blunsom.
2012.A Bayesian model for learning SCFGs with discon-tiguous rules.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 223?232, Jeju Island, Korea, July.Association for Computational Linguistics.Daniel Marcu and William Wong.
2002.
A phrase-based, joint probability model for statistical machinetranslation.
In Proc.
of the 2002 Conference onEmpirical Methods in Natural Language Processing(EMNLP-2002), pages 133?139, Philadelphia, July.Association for Computational Linguistics.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In Proc.
of the 2006 Conference on EmpiricalMethods in Natural Language Processing (EMNLP-2006), pages 44?52, Sydney, Australia, July.Graham Neubig, Taro Watanabe, Eiichiro Sumita,Shinsuke Mori, and Tatsuya Kawahara.
2011.
Anunsupervised model for joint phrase alignment andextraction.
In The 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (ACL-HLT), pages 632?641,Portland, Oregon, USA, 6.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of the 41stAnnual Meeting of the ACL (ACL-2003), pages 160?167, Sapporo, Japan.Adam Pauls, Dan Klein, David Chiang, and KevinKnight.
2010.
Unsupervised syntactic alignmentwith inversion transduction grammars.
In Proceed-ings of the North American Conference of the Asso-ciation for Computational Linguistics (NAACL).
As-sociation for Computational Linguistics.789Slav Petrov, Aria Haghighi, and Dan Klein.
2008.Coarse-to-fine syntactic machine translation usinglanguage projections.
In Proceedings of EMNLP.Association for Computational Linguistics.M.
T. Pilevar and H. Faili.
2011.
Tep: Tehran english-persian parallel corpus.
In Proc.
International Con-ference on Intelligent Text Processing and Computa-tional Linguistics (CICLing).Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M.Blei.
2006.
Hierarchical Dirichlet processes.Journal of the American Statistical Association,101(476):1566?1581.Y.
W. Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages985?992.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Hao Zhang and Daniel Gildea.
2005.
Stochastic lex-icalized inversion transduction grammar for align-ment.
In Proceedings of the 43rd Annual Confer-ence of the Association for Computational Linguis-tics (ACL).
Association for Computational Linguis-tics.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.
InProc.
of the 46th Annual Conference of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies (ACL-08:HLT), pages 97?105,Columbus, Ohio, June.790
