In: Proceedings Of CoNLL-2000 and LLL-2000, pages 43-48, Lisbon, Portugal, 2000.Memory-Based Learning for Article GenerationGuido  Minnen*  Franc is  Bond tCognitive and Computing Sciences MT Research GroupUniversity of Sussex NTT Communication Science LabsPalmer BN1 9QH, Brighton, UK 2-4 Hikari-dai, Kyoto 619-0237, JAPANGuido.
Minnen?cogs.
susx.
ac.
uk bond?cslab, kecl.
ntt.
co. jpAnn CopestakeCSLIStanford UniversityStanford CA 94305-2150, USAaac?csli, stanford, eduAbst ractArticle choice can pose difficult problems in ap-plications uch as machine translation and auto-mated summarization.
In this paper, we investi-gate the use of corpus data to collect statisticalgeneralizations about article use in English inorder to be able to generate articles automati-cally to supplement a symbolic generator.
Weuse data from the Penn Treebank as input to amemory-based learner (TiMBL 3.0; Daelemanset al, 2000) which predicts whether to gener-ate an article with respect o an English basenoun phrase.
We discuss competitive r sults ob-tained using a variety of lexical, syntactic andsemantic features that play an important role inautomated article generation.1 In t roduct ionArticle choice can pose difficult problems in nat-ural language applications.
Machine transla-tion (MT) is an example of such an applica-tion.
When translating from a source languagethat lacks articles, such as Japanese or Rus-sian, to one that requires them, such as Englishor German, the system must somehow generatethe source language articles (Bond and Ogura,1998).
Similarly in automated summarization:when sentences or fragments are combined orreduced, it is possible that the form of a nounphrase (NP) is changed such that a change ofthe article associated with the NP's head be-comes necessary.
For example, consider the sen-tences A talk will be given on Friday about NLP;The talk will last .for one hour which might getsummarized as Friday's NLP talk will last one* Visiting CSLI, Stanford University (2000).t Visiting CSLI, Stanford University (1999-2000).hour.
However, given the input sentences, it isnot clear how to decide not to generate an arti-cle for the subject NP in the output sentence.Another important application is in the fieldknown as augmentative and alternative com-munication (AAC).
In particular, people whohave lost the ability to speak sometimes usea text-to-speech generator as a prosthetic de-vice.
But most disabilities which affect speech,such as stroke or amyotrophic lateral sclerosis(ALS or Lou Gehrig's disease), also cause somemore general motor impairment, which meansthat prosthesis users cannot achieve a text in-put rate comparable to normal typing speedseven if they are able to use a keyboard.
Manyhave to rely on a slower physical interface (head-stick, head-pointer, eye-tracker tc).
We are at-tempting to use a range of NLP technology toimprove text input speed for such users.
Articlechoice is particularly important for this applica-tion: many AAC users drop articles and resortto a sort of telegraphese, but this causes degra-dation in comprehension f synthetic speech andcontributes to its perception as unnatural androbot-like.
Our particular goal is to be able touse an article generator in conjunction with asymbolic generator for AAC (Copestake, 1997;Carroll et al, 1999).In this paper we investigate the use of corpusdata to collect statistical generalizations aboutarticle use in English so as to be able to gen-erate them automatically.
We use data fromthe Penn Treebank as input to a memory-basedlearner (TiMBL 3.0; Daelemans et al, 2000)that is used to predict whether to generate theor alan or no article.
1 We discuss a varietyof lexical, syntactic and semantic features that1We assume a postprocessor to determine whether togenerate a or an as  described in Minnen et al (2000).43play an important role in automated article gen-eration, and compare our results with other re-searchers'.The paper is structured as follows.
Section 2relates our work to that of others.
Section 3introduces the features we use.
Section 4 intro-duces the learning method we use.
We discussour results in Section 5 and suggest some di-rections for future research, then conclude withsome final remarks in Section 6.2 Re la ted  WorkThere has been considerable research on gen-erating articles in machine translation sys-tems (Gawrofiska, 1990; Murata and Nagao,1993; Bond and Ogura, 1998; Heine, 1998).These systems use hand-written rules and lex-ical information to generate articles.
The bestcited results, 88% accuracy, are quoted by Heine(1998) which were obtained with respect o avery small corpus of 1,000 sentences in a re-stricted omain.Knight and Chander (1994) present an ap-proach that uses decision trees to determinewhether to generate the or alan.
They do notconsider the possibility that no article shouldbe generated.
On the basis of a corpus of 400KNP instances derived from the Wall Street Jour-nal, they construct decision trees for the 1,600most frequent nouns by considering over 30,000lexical, syntactic and semantic features.
Theyachieve an accuracy of 81% with respect to thesenouns.
By guessing the for the remainder of thenouns, they achieve an overall accuracy of 78%.3 Features  Determin ing  AutomatedAr t i c le  Generat ionWe have extracted 300K base noun phrases(NPs) from the Penn Treebank Wall StreetJournal data (Bies et al, 1995) using the tgreptool.
The distribution of these NP instanceswith respect o articles is as follows: the 20.6%,a/an 9.4% and 70.0% with no article.We experimented with a range of features:1.
Head of the NP: We consider as the headof the NP the rightmost noun in the NP.
If anNP does not contain a noun, we take the lastword in the NP as its head.2.
Part-of-speech (PoS) tag of the head ofthe NP: PoS labels were taken from the PennTreebank.
We list the tags that occurred with(PP-DIR to/T0(NP the/DT problem/NN))Figure 1: An example ofa  prepositional phraseannotated with a functionaltagthe heads of theNPs in Table 1.PoS Tag the alan noNN 42,806 27,160 53,855NNS 10,705 446 58,118NNP 6,938 271 47,721NNPS 536 2 1,329CD 382 180 13,368DT 18 0 3,045PRP 0 0 21,214PRP$ 0 0 25EX 0 0 1,073IN 0 1 502JJ 388 143 931JJR 11 1 310JJS 184 0 282RB 15 41 498VBG 43 12 210VB 0 1 89WDT 2 0 4,812WP 0 0 2,759Misc.
40 8 269Total: 62,068 28,266 210,410Table 1: Distribution of NP instances in WallStreet Journal data (300,744 NPs in all)3.
Functional tag of the head of the NP: Inthe Penn Treebank each syntactic ategory canbe associated with up to four functional tags aslisted in Table 2.
We consider the sequence offunctional tags associated with the category ofthe NP as a feature; if a constituent has no func-tional tag, we give the feature the value NONE.4.
Category of the constituent embedding theNP: We looked at the category of the embeddingconstituent.
See Figure 1: The category of theconstituent embedding the NP the problem isPP.5.
Functional tag of the constituentembedding the NP: If the category of the con-stituent embedding the NP is associated withone or more functional tags, they are used asfeatures.
The functional tag of the constituentembedding the problem in Figure 1 is DIR.6.
Other determiners of the NP: We lookedat the presence of a determiner in the NP.
Bydefinition, an NP in the Penn Treebank can only44Functional Marks:Tag (ft) Text categoriesHLN headlines and datelinesLST list markersTTL titlesGrammat ica l  functionsCLFN0MADVLGSPRDSUBJTPCCLRBNFDTVtrue cleftsnon NPs that function as NPsclausal and NP adverbialslogical subjects in passivesnonVP predicatessurface subjecttopicalized/fronted constituentsclosely relatedbeneficiary of actiondative objectSemantic rolesV0C vocativesDIR direction and trajectoryL0C locationMNR mannerPRP purpose and reasonTMP temporal phrasesPUT locative complement of putEXT spatial extent of activityTable 2: Functional tags and their mean-ing (Santorini, 1990)have one determiner (Bies et al, 1995), so weexpect it to be a good predictor of situationswhere we should not generate an article.7.
Head countability preferences of the headof the NP: In case the head of an NP is a nounwe also use its countability as a feature.
We an-ticipate that this is a useful feature because sin-gular indefinite countable nouns normally takethe article a/n, whereas singular indefinite un-countable nouns normally take no article: a dogvs water.
We looked up the countability fromthe transfer lexicon used in the Japanese-to-English machine translation system ALT- J /E(Ikehara et al, 1991).
We used six values forthe countability feature: FC (fully countable) fornouns that have both singular and plural formsand can be directly modified by numerals andmodifiers such as many; UC (uncountable) fornouns that have no plural form and can be mod-ified by much; SC (strongly countable) for nounsthat are more often countable than uncount-able; WC (weakly countable) for nouns that aremore often uncountable than countable; and PT(pluralia tantum) for nouns that only have plu-ral forms, such as for example, scissors (Bondet al, 1994).
Finally, we used the value UNKNOWNif the lexicon did not provide countability infor-mation for a noun or if the head of the NP wasnot a noun.
41.4% of the NP instances receivedthe value UNKNOWN for this feature.8.
Semantic classes of the head of the NP: Ifthe head of the NP is a noun we also take intoaccount its semantic lassification in a large se-mantic hierarchy.
The underlying idea is thatthe semantic lass of the noun can be used as away to back off in case of unknown head nouns.The 2,710 node semantic hierarchy we used wasalso developed in the context of the ALT- J /Esystem (Ikehara et al, 1991).
Edges in this hi-erarchy represent IS-A or HAS-A relationships.In case the semantic lasses associated with twonodes stand in the IS-A relation, the semanticclass associated with the node highest in the hi-erarchy subsumes the semantic lass associatedwith the other node.Each of the nodes in this part of the hierarchyis represented by a boolean feature which is setto 1 if that node lies on the path from the rootof the hierarchy to a particular semantic class.Thus, for example, the semantic features of anoun in the semantic class organization con-sists of a vector of 30 features where the featurescorresponding to the nodes noun, concrete ,agent and organization are set to I and allother features are set to 0.
24 Memory-based  learn ingWe used the Tilburg memory based learnerTiMBL 3.0.1 (Daelemans et al, 2000) to learnfrom examples for generating articles using thefeatures discussed above.
Memory-based learn-ing reads all training instances into memory andclassifies test instances by extrapolating a classfrom the most similar instance(s) in memory.Daelemans et al (1999) have shown thatfor typical natural language tasks, this ap-proach has the advantage that it also extrap-olates from exceptional and low-frequency in-stances.
In addition, as a result of automat-ically weighing features in the similarity func-tion used to determine the class of a test in-stance, it allows the user to incorporate large2If a noun has multiple senses, we collapse them bytaking the semantic lasses of a noun to be the union ofthe semantic lasses of all its senses.45numbers of features from heterogeneous sources:When data is sparse, feature weighing embod-ies a smoothing-by-similarity effect (Zavrel andDaelemans, 1997).5 Evaluat ion and Discuss ionWe tested the features discussed in section 3with respect o a number of different memory-based learning methods as implemented in theTiMBL system (Daelemans et al, 2000).We considered two different learning algo-rithms.
The first, IB1 is a k-nearest neighbouralgorithm.
3 This can be used with two differ-ent metrics to judge the distance between theexamples: overlap and modified value differencemetric (MVDM).
TiMBL automatically learnsweights for the features, using one of five dif-ferent weighting methods: no weighting, gainratio, information gain, chi-squared and sharedvariance.
The second algorithm, IGTREE, storesexamples in a tree which is pruned accordingto the weightings.
This makes it much fasterand of comparable accuracy.
The results forthese different methods, for k = 1, 4, 16 are dis-played in Table 3.
IB1 is tested with leave-one-out cross-validation, IGTREE with ten-fold crossvalidation.The best results were (82.6%) for IB1 withthe MVDM metric, and either no weighting orweighting by gain ratio.
IGTREE did not per-form as well.
We investigated more values of k,from 1 to 200, and found they had little influ-ence on the accuracy results with k = 4 or 5performing slightly better.We also tested each of the features describedin Section 3 in isolation and then all together.We used the best performing algorithm from ourearlier experiment: IB1 with MVDM, gain ratioand k = 4.
The results of this are given inTable 4.When interpreting these results it is impor-tant to recall the figures provided in Table 1.The most common article, for any PoS, was noand for many PoS, including pronouns, gener-ating no article is always correct.
There is morevariation in NPs headed by common ouns andadjectives, and a little in NPs headed by propernouns.
Our baseline therefore consists of never3Strictly speaking, it is a k nearest distance algo-rithm, which looks at all examples in the nearest k dis-tances, the number of which may be greater than k.Feature Accuracyhead 80.3%head's part-of-speech 70.0%NP's functional tag 70.5%embedding category 70.0%embedding functional tag 70.0%determiner present or not 70.0%head's countability 70.0%head's semantic lasses 72.9%hlineTable 4: Accuracy results by featuregenerating an article: this will be right in 70.0%of all cases.Looking at the figures in Table 4, we see thatmany of the features investigated id not im-prove results above the baseline.
Using the headof the NP itself to predict the article gave thebest results of any single feature, raising the ac-curacy to 79.4%.
The functional tag of the headof the NP itself improved results slightly.
Theuse of the semantic lasses (72.1%) clearly im-proves the results over the baseline thereby indi-cating that they capture useful generalizations.The results from testing the features in com-bination are shown in Table 5.
Interestingly,features which were not useful on their own,proved useful in combination with the headnoun.
The most useful features appear to be thecategory of the embedding constituent (81.1%)and the presence or absence of a determiner(80.9%).
Combining all the features gave anaccuracy of 82.9%.Feature Accuracyhead+its part-of-speech 80.8%head+functional t g of NP 81.1%head+embedding category 80.8%head+embedding functional tag 81.4%head+determiner present or not 81.7%head+countability 80.8%head+semantic classes 80.8%hline all features 83.6%all features-semantic classes 83.6%Table 5: Accuracy with combined featuresOur best results (82.6%), which used all fea-tures are significantly better than the baselineof generating no articles (70.0%) or using onlythe head of the NP for training (79.4%).
We46Algor i thmkFeature  Weight ingNone Gain ratio Information gain X 2 Shared varianceIB1 1 83.5% 83.5% 83.3% 83.2% 83.3%(MVDM) 4 83.5% 83.6% 83.3% 83.3% 83.3%16 83.6% 83.5% 83.2% 83.2% 83.2%IB1 1 83.1% 83.5% 83.3% 83.2% 83.3%(overlap) 4 82.9% 83.1% 83.1% 83.1% 83.1%16 82.9% 83.0% 82.9% 82.9% 82.9%IGTREE - -  - -  82.9% 82.5% 82.4% 82.6%Table 3: Accuracy results broken down with respect o memory-based learning methods usedalso improve significantly upon earlier esults of78% as reported by Knight and Chander (1994),which in any case is a simpler task since it onlyinvolved choice between the and alan.
Further,our results are competitive with state of the artrule-based systems.
Because different corporaare used to obtain the various results reportedin the literature and the problem is often de-fined differently, detailed comparison is difficult.However, the accuracy achieved appears to ap-proach the accuracy results achieved with hand-written rules.In order to test the effect of the size of thetraining data, we tested used the best perform-ing algorithm from our earlier experiment (IB1with MVDM, gain ratio and k = 4) on varioussubsets of the corpus: the first 10%, the first20%, the first 30% and so on to the whole cor-pus.
The results are given in Table 6.Size Accuracy10% 80.95%20% 81.67%30% 82.14%4O% 82.45%50% 82.69%60% 83.04%70% 83.17%80% 83.24%90% 83.45%100% 83.58%(100% is 300,744 NPs)Table 6: Accuracy versus Size of Training DataThe accuracy is still improving even with300,744 NPs, an even larger corpus should giveeven better results.
It is important to keep inmind that we, like most other researchers, havebeen training and testing on a relatively homo-geneous corpus.
Furthermore, we took as giveninformation about the number of the NP.
Inmany applications we will have neither a largeamount of homogeneous training data nor infor-mation about number.5.1 Future  WorkIn the near future we intend to further ex-tend our approach in various directions.
First,we plan to investigate other lexical and syn-tactic features that might further improve ourresults, such as the existence of pre-modifierslike superlative and comparative adjectives, andpost-modifiers like prepositional phrases, rela-tive clauses, and so on.
We would also like to in-vestigate the effect of additional discourse-basedfeatures uch as one that incorporates informa-tion about whether the referent of a noun phrasehas been mentioned before.Second, we intend to make sure that the fea-tures we are using in training and testing willbe available in the applications we consider.
Forexample, in machine translation, the input nounphrase may be all dogs, whereas the outputcould be either all dogs or all the dogs.
Atpresent, words such as all, both, half in our in-put are tagged as pre-determiners if there is afollowing determiner (it can only be the or apossessive), and determiners if there is no arti-cle.
To train for a realistic application we needto collapse the determiner and pre-determinerinputs together in our training data.Furthermore, we are interested in trainingon corpora with less markup, like the BritishNational Corpus (Burnard, 1995) or even nomarkup at all.
By running a PoS tagger andthen an NP chunker, we should be able to geta lot more training data, and thus significantlyimprove our coverage.
If we can use plain text47to train on, then it will be easier to adapt ourtool quickly to new domains, for which there areunlikely to be fully marked up corpora.6 Conc lud ing  remarksWe described a memory-based approach to au-tomated article generation that uses a variety oflexical, syntactic and semantic features as pro-vided by the Penn Treebank Wall Street Jour-nal data and a large hand-encoded MT dictio-nary.
With this approach we achieve an accu-racy of 82.6%.
We believe that this approachis an encouraging first step towards a statisticaldevice for automated article generation that canbe used in a range of applications uch as speechprosthesis, machine translation and automatedsummarization.AcknowledgmentsThe authors would like to thank the Stanford NLPreading group, the LinGO project at CSLI, Timo-thy Baldwin, Kevin Knight, Chris Manning, Wal-ter Daelemans and two anonymous reviewers fortheir helpful comments.
This project is in part sup-ported by the National Science Foundation undergrant number IRI-9612682.ReferencesAnn Bies, Mark Fergusona, Karen Katz, and RobertMacIntyre, 1995.
Bracketing Guidelines for Tree-bank H Style.
Penn Treebank Project, Universityof Pennsylvania.Francis Bond and Kentaro Ogura.
1998.
Referencein Japanese-to-English machine translation.
Ma-chine Translation, 13(2-3):107-134.Francis Bond, Kentaro Ogura, and Satoru Ikehara.1994.
Countability and number in Japanese-to-English machine translation.
In 15th Interna-tional Conference on Computational Linguistics:COLING-94, pages 32-38, Kyoto.
(http: / /xxx.lanl.
gov/abs/cmp- ig/951 i001).Lou Burnard.
1995.
User reference guide for theBritish National Corpus.
Technical report, Ox-ford University Computing Services.John Carroll, Ann Copestake, Dan Flickinger, andVictor Poznanski.
1999.
An efficient chart gen-erator for (semi-)lexicalist grammars.
In Proceed-ings o/ the 7th European Workshop on NaturalLanguage Generation (EWNLG'99), pages 86-95,Toulouse, France.Ann Copestake.
1997.
Augmented and alternativeNLP techniques for augmentative and alternativecommunication.
In Proceedings of the ACL work-shop on Natural Language Processing for Commu-nication Aids, pages 37-42, Madrid.Walter Daelemans, Antal van den Bosch, and JakubZavrel.
1999.
Forgetting exceptions i harmful inlanguage learning.
Machine Learning, 34.Walter Daelemans, Jakub Zavrel, Ko van der Sloot,and Antal van den Bosch.
2000.
TiMBL: Tilburgmemory based learner, version 3.0, referenceguide.
ILK Technical Report 00-01, ILK, Tilburg,The Netherlands.
(ILK-0001; h t tp : / / i l k .kub .nl).Barbara Gawrofiska.
1990.
"Translation GreatProblem" on the problem of inserting articleswhen translating from Russian into Swedish.
In13th International Conference on ComputationalLinguistics: COLING-90, Helsinki.Julia E. Heine.
1998.
Definiteness predictions forJapanese noun phrases.
In 36th Annual Meetingo\] the Association \]or Computational Linguisticsand 17th International Conference on Computa-tional Linguistics: COLING/A CL-98, pages 519-525, Montreal, Canada.Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hi-romi Nakaiwa.
1991.
Toward an MT system with-out pre-editing - effects of new methods in ALT-J /E - .
In Third Machine Translation Summit:MT Summit III, pages 101-106, Washington DC.
(http ://xxx.
lanl.
gov/abs/cmp- ig/9510008).Kevin Knight and Ishwar Chander.
1994.
Au-tomated postediting of documents.
In Proceed-ings of the 12th National Conference on ArtificialIntelligence: AAAI-9~, pages 779-784, Seattle.
(http ://xxx.
lanl.
gov/abs/cmp-ig/9407028).Guido Minnen, John Carroll, and Darren Pearce.2000.
Robust, applied morphological generation.In Proceedings of the first International NaturalLanguage Genration Conference, Mitzpe Ramon,Israel.Masaki Murata and Makoto Nagao.
1993.
Deter-mination of referential property and number ofnouns in Japanese sentences for machine transla-tion into English.
In Fifth International Confer-ence on Theoretical and Methodological Issues inMachine Translation: TMI-93, pages 218-25, Ky-oto, July.
(ht tp : / /xxx.
laa l .
gov/abs/cmp-lg/9405019).Beatrice Santorini.
1990.
Part-of-speech taggingguidelines for the Penn Treebank Project.
Tech-nical Report MS-CIS-90-47, Department of Com-puter and Information Science, University ofPennsylvania.Jakub Zavrel and Walter Daelemans.
1997.Memory-based learning: Using similarity forsmoothing.
In Proceedings of the 35th AnnualMeeting of the Association for ComputationalLinguistics, Madrid, Spain.48
