Proceedings of the ACL 2010 Student Research Workshop, pages 13?18,Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational LinguisticsWSD as a Distributed Constraint Optimization ProblemSiva ReddyIIIT HyderabadIndiagvsreddy@students.iiit.ac.inAbhilash InumellaIIIT HyderabadIndiaabhilashi@students.iiit.ac.inAbstractThis work models Word Sense Disam-biguation (WSD) problem as a Dis-tributed Constraint Optimization Problem(DCOP).
To model WSD as a DCOP,we view information from various knowl-edge sources as constraints.
DCOP al-gorithms have the remarkable property tojointly maximize over a wide range of util-ity functions associated with these con-straints.
We show how utility functionscan be designed for various knowledgesources.
For the purpose of evaluation,we modelled all words WSD as a simpleDCOP problem.
The results are competi-tive with state-of-art knowledge based sys-tems.1 IntroductionWords in a language may carry more than onesense.
The correct sense of a word can be iden-tified based on the context in which it occurs.
Inthe sentence, He took all his money from the bank,bank refers to a financial institution sense insteadof other possibilities like the edge of river sense.Given a word and its possible senses, as definedby a dictionary, the problem of Word Sense Dis-ambiguation (WSD) can be defined as the task ofassigning the most appropriate sense to the wordwithin a given context.WSD is one of the oldest problems in com-putational linguistics which dates back to early1950?s.
A range of knowledge sources have beenfound to be useful for WSD.
(Agirre and Steven-son, 2006; Agirre and Mart?
?nez, 2001; McRoy,1992; Hirst, 1987) highlight the importance ofvarious knowledge sources like part of speech,morphology, collocations, lexical knowledge base(sense taxonomy, gloss), sub-categorization, se-mantic word associations, selectional preferences,semantic roles, domain, topical word associations,frequency of senses, collocations, domain knowl-edge.
etc.
Methods for WSD exploit informationfrom one or more of these knowledge sources.Supervised approaches like (Yarowsky and Flo-rian, 2002; Lee and Ng, 2002; Mart?
?nez et al,2002; Stevenson and Wilks, 2001) used collec-tive information from various knowledge sourcesto perform disambiguation.
Information from var-ious knowledge sources is encoded in the form ofa feature vector and models were built by trainingon sense-tagged corpora.
These approaches poseWSD as a classification problem.
They cruciallyrely on hand-tagged sense corpora which is hardto obtain.
Systems that do not need hand-tagginghave also been proposed.
Agirre and Martinez(Agirre and Mart?
?nez, 2001) evaluated the contri-bution of each knowledge source separately.
How-ever, this does not combine information from morethan one knowledge source.In any case, little effort has been made in for-malizing the way in which information from var-ious knowledge sources can be collectively usedwithin a single framework: a framework that al-lows interaction of evidence from various knowl-edge sources to arrive at a global optimal solution.Here we present a way for modelling informa-tion from various knowledge sources in a multiagent setting called distributed constraint opti-mization problem (DCOP).
In DCOP, agents haveconstraints on their values and each constraint hasa utility associated with it.
The agents communi-cate with each other and choose values such that aglobal optimum solution (maximum utility) is at-tained.
We aim to solve WSD by modelling it as aDCOP.To the best of our knowledge, ours is the firstattempt to model WSD as a DCOP.
In DCOPframework, information from various knowledgesources can be used combinedly to perform WSD.In section 2, we give a brief introduction of13DCOP.
Section 3 describes modelling WSD asa DCOP.
Utility functions for various knowledgesources are described in section 4.
In section 5,we conduct a simple experiment by modelling all-words WSD problem as a DCOP and perform dis-ambiguation on Senseval-2 (Cotton et al, 2001)and Senseval-3 (Mihalcea and Edmonds, 2004)data-set of all-words task.
Next follow the sec-tions on related work, discussion, future work andconclusion.2 Distributed Constraint OptimizationProblem (DCOP)A DCOP (Modi, 2003; Modi et al, 2004) consistsof n variables V = x1, x2, ...xneach assignedto an agent, where the values of the variables aretaken from finite, discrete domains D1, D2, ..., Dnrespectively.
Only the agent has knowledge andcontrol over values assigned to variables associ-ated to it.
The goal for the agents is to choosevalues for variables such that a given global objec-tive function is maximized.
The objective functionis described as the summation over a set of utilityfunctions.DCOP can be formalized as a tuple (A, V, D, C,F) where?
A = {a1, a2, .
.
.
an} is a set of n agents,?
V = {x1, x2, .
.
.
xn} is a set of n variables,each one associated to an agent,?
D = {D1, D2, .
.
.
Dn} is a set of finite anddiscrete domains each one associated to thecorresponding variable,?
C = {fk: Di?Dj?
.
.
.
Dm?
?}
is a set ofconstraints described by various utility func-tions fk.
The utility function fkis definedover a subset of variables V .
The domainof fkrepresent the constraints Cfkand fk(c)represents the utility associated with the con-straint c, where c ?
Cfk.?
F =?kzk?
fkis the objective function to bemaximized where zkis the weight of the cor-responding utility function fkAn agent is allowed to communicate only withits neighbours.
Agents communicate with eachother to agree upon a solution which maximizesthe objective function.3 WSD as a DCOPGiven a sequence of words W= {w1, w2, .
.
.
wn}with corresponding admissible senses Dwi={s1wi, s2wi.
.
.
}, we model WSD as DCOP as fol-lows.3.1 AgentsEach word wiis treated as an agent.
The agent(word) has knowledge and control of its values(senses).3.2 VariablesSense of a word varies and it is the one to be deter-mined.
We define the sense of a word as its vari-able.
Each agent wiis associated with the variableswi.
The value assigned to this variable indicatesthe sense assigned by the algorithm.3.3 DomainsSenses of a word are finite in number.
The set ofsenses Dwi, is the domain of the variable swi.3.4 ConstraintsA constraint specifies a particular configuration ofthe agents involved in its definition and has a util-ity associated with it.
For e.g.
If cijis a constraintdefined on agents wiand wj, then cijrefers to aparticular instantiation of wiand wj, say wi= spwiand wj= sqwj.A utility function fk: Cfk?
?
denote a set ofconstraints Cfk= {Dwi?Dwj.
.
.
Dwm}, definedon the agents wi, wj.
.
.
wmand also the utilitiesassociated with the constraints.
We model infor-mation from each knowledge source as a utilityfunction.
In section 4, we describe in detail aboutthis modelling.3.5 Objective functionAs already stated, various knowledge sources areidentified to be useful for WSD.
It is desirable touse information from these sources collectively,to perform disambiguation.
DCOP provides suchframework where an objective function is definedover all the knowledge sources (fk) as belowF =?kzk?
fkwhere F denotes the total utility associated witha solution and zkis the weight given to a knowl-edge source i.e.
information from various sources14can be weighted.
(Note: It is desirable to nor-malize utility functions of different knowledgesources in order to compare them.
)Every agent (word) choose its value (sense) in asuch a way that the objective function (global solu-tion) is maximized.
This way an agent is assigneda best value which is the target sense in our case.4 Modelling information from variousknowledge sourcesIn this section, we discuss the modelling of infor-mation from various knowledge sources.4.1 Part-of-speech (POS)Consider the word play.
It has 47 senses out ofwhich only 17 senses correspond to noun category.Based on the POS information of a word wi, itsdomain Dwiis restricted accordingly.4.2 MorphologyNoun orange has at least two senses, one corre-sponding to a color and other to a fruit.
But plu-ral form of this word oranges can only be used inthe fruit sense.
Depending upon the morphologi-cal information of a word wi, its domain Dwicanbe restricted.4.3 Domain informationIn the sports domain, cricket likely refers to agame than an insect.
Such information can be cap-tured using a unary utility function defined for ev-ery word.
If the sense distributions of a word wiare known, a function f : Dwi?
?
is definedwhich return higher utility for the senses favouredby the domain than to the other senses.4.4 Sense RelatednessSense relatedness between senses of two wordswi, wjis captured by a function f : Dwi?Dwj??
where f returns sense relatedness (utility) be-tween senses based on sense taxonomy and glossoverlaps.4.5 DiscourseDiscourse constraints can be modelled using an-ary function.
For instance, to the extent onesense per discourse (Gale et al, 1992) holds true,higher utility can be returned to the solutionswhich favour same sense to all the occurrencesof a word in a given discourse.
This informationcan be modeled as follows: If wi, wj, .
.
.
wmarethe occurrences of a same word, a function f :Di?
Dj?
.
.
.
Dm?
?
is defined which returnshigher utility when swi= swj= .
.
.
swmand forthe rest of the combinations it returns lower utility.4.6 CollocationsCollocations of a word are known to providestrong evidence for identifying correct sense of theword.
For example: if in a given context bank co-occur with money, it is likely that bank refers tofinancial institution sense rather than the edge ofa river sense.
The word cancer has at least twosenses, one corresponding to the astrological signand the other a disease.
But its derived form can-cerous can only be used in disease sense.
Whenthe words cancer and cancerous co-occur in a dis-course, it is likely that the word cancer refers todisease sense.Most supervised systems work through colloca-tions to identify correct sense of a word.
If a wordwico-occurs with its collocate v, collocational in-formation from v can be modeled by using the fol-lowing functioncoll infrm vwi: Dwi?
?where coll infrm vwireturns high utility tocollocationally preferred senses of withan othersenses.Collocations can also be modeled by assigningmore than one variable to the agents or by adding adummy agent which gives collocational informa-tion but in view of simplicity we do not go intothose details.Topical word associations, semantic word asso-ciations, selectional preferences can also be mod-eled similar to collocations.
Complex informationinvolving more than two entities can be modelledby using n-ary utility functions.5 Experiment: DCOP based All WordsWSDWe carried out a simple experiment to test the ef-fectiveness of DCOP algorithm.
We conductedour experiment in an all words setting and usedonly WordNet (Fellbaum, 1998) based relatednessmeasures as knowledge source so that results canbe compared with earlier state-of-art knowledge-based WSD systems like (Agirre and Soroa, 2009;Sinha and Mihalcea, 2007) which used similarknowledge sources as ours.15Our method performs disambiguation on sen-tence by sentence basis.
A utility function basedon semantic relatedness is defined for every pairof words falling in a particular window size.
Re-stricting utility functions to a window size reducesthe number of constraints.
An objective function isdefined as sum of these restricted utility functionsover the entire sentence and thus allowing infor-mation flow across all the words.
Hence, a DCOPalgorithm which aims to maximize this objectivefunction leads to a globally optimal solution.In our experiments, we used the best similaritymeasure settings of (Sinha and Mihalcea, 2007)which is a sum of normalized similarity mea-sures jcn, lch and lesk.
We used used DistributedPseudotree Optimization Procedure (DPOP) algo-rithm (Petcu and Faltings, 2005), which solvesDCOP using linear number of messages amongagents.
The implementation provided with theopen source toolkit FRODO1 (Le?aute?
et al, 2009)is used.5.1 DataTo compare our results, we ran our experimentson SENSEVAL-2 and SENSEVAL -3 English all-words data sets.5.2 ResultsTable 1 shows results of our experiments.
Allthese results are carried out using a window sizeof four.
Ideally, precision and recall values are ex-pected to be equal in our setting.
But in certaincases, the tool we used, FRODO, failed to find asolution with the available memory resources.Results show that our system performs con-sistently better than (Sinha and Mihalcea, 2007)which uses exactly same knowledge sources asused by us (with an exception of adverbs inSenseval-2).
This shows that DCOP algorithmperform better than page-rank algorithm used intheir graph based setting.
Thus, for knowledge-based WSD, DCOP framework is a potential al-ternative to graph based models.Table 1 also shows the system (Agirre andSoroa, 2009), which obtained best results forknowledge based WSD.
A direct comparisonbetween this and our system is not quantita-tive since they used additional knowledge suchas extended WordNet relations (Mihalcea and1http://liawww.epfl.ch/frodo/Moldovan, 2001) and sense disambiguated glosspresent in WordNet3.0.Senseval-2 All Words data setnoun verb adj adv allP dcop 67.85 37.37 62.72 56.87 58.63R dcop 66.44 35.47 61.28 56.65 57.09F dcop 67.14 36.39 61.99 56.76 57.85P Sinha07 67.73 36.05 62.21 60.47 58.83R Sinha07 65.63 32.20 61.42 60.23 56.37F Sinha07 66.24 34.07 61.81 60.35 57.57Agirre09 70.40 38.90 58.30 70.1 58.6MFS 71.2 39.0 61.1 75.4 60.1Senseval-3 All Words data setP dcop 62.31 43.48 57.14 100 54.68R dcop 60.97 42.81 55.17 100 53.51F dcop 61.63 43.14 56.14 100 54.09P Sinha07 61.22 45.18 54.79 100 54.86R Sinha07 60.45 40.57 54.14 100 52.40F Sinha07 60.83 42.75 54.46 100 53.60Agirre09 64.1 46.9 62.6 92.9 57.4MFS 69.3 53.6 63.7 92.9 62.3Table 1: Evaluation results on Senseval-2 andSenseval-3 data-set of all words task.5.3 Performance analysisWe conducted our experiment on a computer withtwo 2.94 GHz process and 2 GB memory.
Ouralgorithm just took 5 minutes 31 seconds onSenseval-2 data set, and 5 minutes 19 seconds onSenseval-3 data set.
This is a singable reductioncompared to execution time of page rank algo-rithms employed in both Sinha07 and Agirre09.
InAgirre09, it falls in the range 30 to 180 minutes onmuch powerful system with 16 GB memory hav-ing four 2.66 GHz processors.
On our system,time taken by the page rank algorithm in (Sinhaand Mihalcea, 2007) is 11 minutes when executedon Senseval-2 data set.Since DCOP algorithms are truly distributed innature the execution times can be further reducedby running them parallely on multiple processors.6 Related workEarlier approaches to WSD which encoded infor-mation from variety of knowledge sources can beclassified as follows:?
Supervised approaches: Most of the super-vised systems (Yarowsky and Florian, 2002;16Lee and Ng, 2002; Mart?
?nez et al, 2002;Stevenson and Wilks, 2001) rely on the sensetagged data.
These are mainly discrimina-tive or aggregative models which essentiallypose WSD a classification problem.
Dis-criminative models aim to identify the mostinformative feature and aggregative modelsmake their decisions by combining all fea-tures.
They disambiguate word by word anddo not collectively disambiguate whole con-text and thereby do not capture all the rela-tionships (e.g sense relatedness) among allthe words.
Further, they lack the ability todirectly represent constraints like one senseper discourse.?
Graph based approaches: These approachescrucially rely on lexical knowledge base.Graph-based WSD approaches (Agirre andSoroa, 2009; Sinha and Mihalcea, 2007) per-form disambiguation over a graph composedof senses (nodes) and relations between pairsof senses (edges).
The edge weights encodeinformation from a lexical knowledge basebut lack an efficient way of modelling in-formation from other knowledge sources likecollocational information, selectional prefer-ences, domain information, discourse.
Also,the edges represent binary utility functionsdefined over two entities which lacks the abil-ity to encode ternary, and in general, any N-ary utility functions.7 DiscussionThis framework provides a convenient way ofintegrating information from various knowledgesources by defining their utility functions.
Infor-mation from different knowledge sources can beweighed based on the setting at hand.
For exam-ple, in a domain specific WSD setting, sense dis-tributions play a crucial role.
The utility functioncorresponding to the sense distributions can beweighed higher in order to take advantage of do-main information.
Also, different combination ofweights can be tried out for a given setting.
Thusfor a given WSD setting, this framework allows usto find 1) the impact of each knowledge source in-dividually 2) the best combination of knowledgesources.Limitations of DCOP algorithms: SolvingDCOPs is NP-hard.
A variety of search algorithmshave therefore been developed to solve DCOPs(Mailler and Lesser, 2004; Modi et al, 2004;Petcu and Faltings, 2005) .
As the number ofconstraints or words increase, the search space in-creases thereby increasing the time and memorybounds to solve them.
Also DCOP algorithms ex-hibit a trade-off between memory used and num-ber of messages communicated between agents.DPOP (Petcu and Faltings, 2005) use linear num-ber of messages but requires exponential memorywhereas ADOPT (Modi et al, 2004) exhibits lin-ear memory complexity but exchange exponentialnumber of messages.
So it is crucial to choose asuitable algorithm based on the problem at hand.8 Future WorkIn our experiment, we only used relatedness basedutility functions derived from WordNet.
Effect ofother knowledge sources remains to be evaluatedindividually and in combination.
The best possiblecombination of weights of knowledge sources isyet to be engineered.
Which DCOP algorithm per-forms better WSD and when has to be explored.9 ConclusionWe initiated a new line of investigation into WSDby modelling it in a distributed constraint opti-mization framework.
We showed that this frame-work is powerful enough to encode informationfrom various knowledge sources.
Our experimen-tal results show that a simple DCOP based modelencoding just word similarity constraints performscomparably with the state-of-the-art knowledgebased WSD systems.AcknowledgementWe would like to thank Prof. Rajeev Sangal andAsrar Ahmed for their support in coming up withthis work.ReferencesEneko Agirre and David Mart??nez.
2001.
Knowledgesources for word sense disambiguation.
In Text,Speech and Dialogue, 4th International Conference,TSD 2001, Zelezna Ruda, Czech Republic, Septem-ber 11-13, 2001, Lecture Notes in Computer Sci-ence, pages 1?10.
Springer.Eneko Agirre and Aitor Soroa.
2009.
Personaliz-ing pagerank for word sense disambiguation.
InEACL ?09: Proceedings of the 12th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, pages 33?41, Morristown, NJ,USA.
Association for Computational Linguistics.17Eneko Agirre and Mark Stevenson.
2006.
Knowledgesources for wsd.
In Word Sense Disambiguation:Algorithms and Applications, volume 33 of Text,Speech and Language Technology, pages 217?252.Springer, Dordrecht, The Netherlands.Scott Cotton, Phil Edmonds, Adam Kilgarriff, andMartha Palmer.
2001.
Senseval-2.
http://www.sle.sharp.co.uk/senseval2.Christiane Fellbaum, editor.
1998.
WordNet An Elec-tronic Lexical Database.
The MIT Press, Cam-bridge, MA ; London, May.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One sense per discourse.
In HLT?91: Proceedings of the workshop on Speech andNatural Language, pages 233?237, Morristown, NJ,USA.
Association for Computational Linguistics.Graeme Hirst.
1987.
Semantic interpretation andthe resolution of ambiguity.
Cambridge UniversityPress, New York, NY, USA.Thomas Le?aute?, Brammert Ottens, and Radoslaw Szy-manek.
2009.
FRODO 2.0: An open-sourceframework for distributed constraint optimization.In Proceedings of the IJCAI?09 Distributed Con-straint Reasoning Workshop (DCR?09), pages 160?164, Pasadena, California, USA, July 13. http://liawww.epfl.ch/frodo/.Yoong Keok Lee and Hwee Tou Ng.
2002.
An em-pirical evaluation of knowledge sources and learn-ing algorithms for word sense disambiguation.
InEMNLP ?02: Proceedings of the ACL-02 conferenceon Empirical methods in natural language process-ing, pages 41?48, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Roger Mailler and Victor Lesser.
2004.
Solvingdistributed constraint optimization problems usingcooperative mediation.
In AAMAS ?04: Proceed-ings of the Third International Joint Conference onAutonomous Agents and Multiagent Systems, pages438?445, Washington, DC, USA.
IEEE ComputerSociety.David Mart?
?nez, Eneko Agirre, and Llu?
?s Ma`rquez.2002.
Syntactic features for high precision wordsense disambiguation.
In COLING.Susan W. McRoy.
1992.
Using multiple knowledgesources for word sense discrimination.
COMPUTA-TIONAL LINGUISTICS, 18:1?30.Rada Mihalcea and Phil Edmonds, editors.
2004.Proceedings Senseval-3 3rd International Workshopon Evaluating Word Sense Disambiguation Systems.ACL, Barcelona, Spain.Rada Mihalcea and Dan I. Moldovan.
2001. ex-tended wordnet: progress report.
In in Proceedingsof NAACL Workshop on WordNet and Other LexicalResources, pages 95?100.Pragnesh Jay Modi, Wei-Min Shen, Milind Tambe, andMakoto Yokoo.
2004.
Adopt: Asynchronous dis-tributed constraint optimization with quality guaran-tees.
Artificial Intelligence, 161:149?180.Pragnesh Jay Modi.
2003.
Distributed constraint opti-mization for multiagent systems.
PhD Thesis.Adrian Petcu and Boi Faltings.
2005.
A scalablemethod for multiagent constraint optimization.
InIJCAI?05: Proceedings of the 19th internationaljoint conference on Artificial intelligence, pages266?271, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Ravi Sinha and Rada Mihalcea.
2007.
Unsupervisedgraph-basedword sense disambiguation using mea-sures of word semantic similarity.
In ICSC ?07: Pro-ceedings of the International Conference on Seman-tic Computing, pages 363?369, Washington, DC,USA.
IEEE Computer Society.Mark Stevenson and Yorick Wilks.
2001.
The inter-action of knowledge sources in word sense disam-biguation.
Comput.
Linguist., 27(3):321?349.David Yarowsky and Radu Florian.
2002.
Evaluat-ing sense disambiguation across diverse parameterspaces.
Natural Language Engineering, 8:2002.18
