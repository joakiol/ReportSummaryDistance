Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 432?441,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Parallel-Hierarchical Model for Machine Comprehension on SparseDataAdam Trischler?adam.trischlerZheng Ye?jeff.yeXingdi Yuaneric.yuanJing Hejing.hePhilip Bachmanphil.bachmanMaluuba ResearchMontreal, Qu?ebec, CanadaKaheer Sulemank.suleman@maluuba.comAbstractUnderstanding unstructured text is a ma-jor goal within natural language process-ing.
Comprehension tests pose questionsbased on short text passages to evaluatesuch understanding.
In this work, we in-vestigate machine comprehension on thechallenging MCTest benchmark.
Partlybecause of its limited size, prior workon MCTest has focused mainly on engi-neering better features.
We tackle thedataset with a neural approach, harness-ing simple neural networks arranged in aparallel hierarchy.
The parallel hierarchyenables our model to compare the pas-sage, question, and answer from a vari-ety of trainable perspectives, as opposedto using a manually designed, rigid fea-ture set.
Perspectives range from the wordlevel to sentence fragments to sequencesof sentences; the networks operate only onword-embedding representations of text.When trained with a methodology de-signed to help cope with limited trainingdata, our Parallel-Hierarchical model setsa new state of the art for MCTest, outper-forming previous feature-engineered ap-proaches slightly and previous neural ap-proaches by a significant margin (over 15percentage points).1 IntroductionHumans learn in a variety of ways?by communi-cation with each other and by study, the readingof text.
Comprehension of unstructured text bymachines, at a near-human level, is a major goalfor natural language processing.
It has garnered?A.
Trischler and Z. Ye contributed equally to this work.significant attention from the machine learning re-search community in recent years.Machine comprehension (MC) is evaluated byposing a set of questions based on a text pas-sage (akin to the reading tests we all took inschool).
Such tests are objectively gradable andcan be used to assess a range of abilities, frombasic understanding to causal reasoning to infer-ence (Richardson et al, 2013).
Given a text pas-sage and a question about its content, a system istested on its ability to determine the correct an-swer (Sachan et al, 2015).
In this work, we focuson MCTest, a complex but data-limited compre-hension benchmark, whose multiple-choice ques-tions require not only extraction but also infer-ence and limited reasoning (Richardson et al,2013).
Inference and reasoning are important hu-man skills that apply broadly, beyond language.We present a parallel-hierarchical approach tomachine comprehension designed to work well ina data-limited setting.
There are many use-cases inwhich comprehension over limited data would behandy: for example, user manuals, internal doc-umentation, legal contracts, and so on.
More-over, work towards more efficient learning fromany quantity of data is important in its own right,for bringing machines more in line with the wayhumans learn.
Typically, artificial neural networksrequire numerous parameters to capture complexpatterns, and the more parameters, the more train-ing data is required to tune them.
Likewise, deepmodels learn to extract their own features, but thisis a data-intensive process.
Our model learns tocomprehend at a high level even when data issparse.The key to our model is that it compares thequestion and answer candidates to the text usingseveral distinct perspectives.
We refer to a ques-tion combined with one of its answer candidatesas a hypothesis (to be detailed below).
The seman-432tic perspective compares the hypothesis to sen-tences in the text viewed as single, self-containedthoughts; these are represented using a sum andtransformation of word embedding vectors, sim-ilarly to Weston et al (2014).
The word-by-wordperspective focuses on similarity matches betweenindividual words from hypothesis and text, at var-ious scales.
As in the semantic perspective, weconsider matches over complete sentences.
Wealso use a sliding window acting on a subsententialscale (inspired by the work of Hill et al (2015)),which implicitly considers the linear distance be-tween matched words.
Finally, this word-levelsliding window operates on two different viewsof story sentences: the sequential view, wherewords appear in their natural order, and the depen-dency view, where words are reordered based on alinearization of the sentence?s dependency graph.Words are represented throughout by embeddingvectors (Bengio et al, 2000; Mikolov et al, 2013).These distinct perspectives naturally form a hierar-chy that we depict in Figure 1.
Language is hierar-chical, so it makes sense that comprehension relieson hierarchical levels of understanding.The perspectives of our model can be consid-ered a type of feature.
However, they are im-plemented by parametric differentiable functions.This is in contrast to most previous efforts onMCTest, whose numerous hand-engineered fea-tures cannot be trained.
Our model, significantly,can be trained end-to-end with backpropagation.To facilitate learning with limited data, we alsodevelop a unique training scheme.
We initializethe model?s neural networks to perform specificheuristic functions that yield decent (though notimpressive) performance on the dataset.
Thus, thetraining scheme gives the model a safe, reasonablebaseline from which to start learning.
We call thistechnique training wheels.Computational models that comprehend (inso-far as they perform well on MC datasets) havebeen developed contemporaneously in several re-search groups (Weston et al, 2014; Sukhbaatar etal., 2015; Hill et al, 2015; Hermann et al, 2015;Kumar et al, 2015).
Models designed specifi-cally for MCTest include those of Richardson etal.
(2013), and more recently Sachan et al (2015),Wang et al (2015), and Yin et al (2016).
In exper-iments, our Parallel-Hierarchical model achievesstate-of-the-art accuracy on MCTest, outperform-ing these existing methods.Below we describe related work, the mathemat-ical details of our model, and our experiments,then analyze our results.2 The ProblemIn this section, we borrow from Sachan et al(2015), who laid out the MC problem nicely.
Ma-chine comprehension requires machines to answerquestions based on unstructured text.
This canbe viewed as selecting the best answer from a setof candidates.
In the multiple-choice case, can-didate answers are predefined, but candidate an-swers may also be undefined yet restricted (e.g., toyes, no, or any noun phrase in the text) (Sachan etal., 2015).For each question q, let T be the unstructuredtext and A = {ai} the set of candidate answersto q.
The machine comprehension task reduces toselecting the answer that has the highest evidencegiven T .
As in Sachan et al (2015), we combinean answer and a question into a hypothesis, hi=f(q, ai).
To facilitate comparisons of the text withthe hypotheses, we also break down the passageinto sentences tj, T = {tj}.
In our setting, q,ai, and tjeach represent a sequence of embeddingvectors, one for each word and punctuation markin the respective item.3 Related WorkMachine comprehension is currently a hot topicwithin the machine learning community.
In thissection we will focus on the best-performing mod-els applied specifically to MCTest, since it is some-what unique among MC datasets (see Section 5).Generally, models can be divided into two cate-gories: those that use fixed, engineered features,and neural models.
The bulk of the work onMCTest falls into the former category.Manually engineered features often require sig-nificant effort on the part of a designer, and/orvarious auxiliary tools to extract them, and theycannot be modified by training.
On the otherhand, neural models can be trained end-to-end andtypically harness only a single feature: vector-representations of words.
Word embeddings arefed into a complex and possibly deep neural net-work which processes and compares text to ques-tion and answer.
Among deep models, mecha-nisms of attention and working memory are com-mon, as in Weston et al (2014) and Hermann et al(2015).4333.1 Feature-engineering modelsSachan et al (2015) treated MCTest as a structuredprediction problem, searching for a latent answer-entailing structure connecting question, answer,and text.
This structure corresponds to the bestlatent alignment of a hypothesis with appropri-ate snippets of the text.
The process of (latently)selecting text snippets is related to the attentionmechanisms typically used in deep networks de-signed for MC and machine translation (Bahdanauet al, 2014; Weston et al, 2014; Hill et al,2015; Hermann et al, 2015).
The model usesevent and entity coreference links across sentencesalong with a host of other features.
These includespecifically trained word vectors for synonymy;antonymy and class-inclusion relations from ex-ternal database sources; dependencies and seman-tic role labels.
The model is trained using a latentstructural SVM extended to a multitask setting, sothat questions are first classified using a pretrainedtop-level classifier.
This enables the system to usedifferent processing strategies for different ques-tion categories.
The model also combines questionand answer into a well-formed statement using therules of Cucerzan and Agichtein (2005).Our model is simpler than that of Sachan etal.
(2015) in terms of the features it takes in, thetraining procedure (stochastic gradient descent vs.alternating minimization), question classification(we use none), and question-answer combination(simple concatenation or mean vs. a set of rules).Wang et al (2015) augmented the baseline fea-ture set from Richardson et al (2013) with fea-tures for syntax, frame semantics, coreferencechains, and word embeddings.
They combinedfeatures using a linear latent-variable classifiertrained to minimize a max-margin loss function.As in Sachan et al (2015), questions and answersare combined using a set of manually written rules.The method of Wang et al (2015) achieved theprevious state of the art, but has significant com-plexity in terms of the feature set.Space does not permit a full description of allmodels in this category, but we refer the readerto the contributions of Smith et al (2015) andNarasimhan and Barzilay (2015) as well.Despite its relative lack of features, the Parallel-Hierarchical model improves upon the feature-engineered state of the art for MCTest by a smallamount (about 1% absolute) as detailed in Sec-tion 5.3.2 Neural modelsNeural models have, to date, performed relativelypoorly on MCTest.
This is because the dataset issparse and complex.Yin et al (2016) investigated deep-learningapproaches concurrently with the present work.They measured the performance of the AttentiveReader (Hermann et al, 2015) and the Neural Rea-soner (Peng et al, 2015), both deep, end-to-endrecurrent models with attention mechanisms, andalso developed an attention-based convolutionalnetwork, the HABCNN.
Their network operateson a hierarchy similar to our own, providing fur-ther evidence of the promise of hierarchical per-spectives.
Specifically, the HABCNN processestext at the sentence level and the snippet level,where the latter combines adjacent sentences (aswe do through an n-gram input).
Embedding vec-tors for the question and the answer candidatesare combined and encoded by a convolutional net-work.
This encoding modulates attention over sen-tence and snippet encodings, followed by max-pooling to determine the best matches betweenquestion, answer, and text.
As in the present work,matching scores are given by cosine similarity.The HABCNN also makes use of a question clas-sifier.Despite the conceptual overlap between theHABCNN and our approach, the Parallel-Hierarchical model performs significantly betteron MCTest (more than 15% absolute) as detailedin Section 5.
Other neural models tested in Yin etal.
(2016) fare even worse.4 The Parallel-Hierarchical ModelLet us now define our machine comprehensionmodel in full.
We first describe each of the per-spectives separately, then describe how they arecombined.
Below, we use subscripts to index el-ements of sequences, like word vectors, and su-perscripts to indicate whether elements come fromthe text, question, or answer.
In particular, we usethe subscripts k,m, n, p to index sequences fromthe text, question, answer, and hypothesis, respec-tively, and superscripts t, q, a, h. We depict themodel schematically in Figure 1.4.1 Semantic PerspectiveThe semantic perspective is similar to the Mem-ory Networks approach for embedding inputs intomemory space (Weston et al, 2014).
Each sen-434Semantic SententialSW-sequential SW-dependencyMLPWord-by-wordtop Ntjtj |tj+1unigrambigram tj | tj+1tj-1 |trigramMLP+Sum MLPEmbeddingq aiMiFigure 1: Schematic of the Parallel-Hierarchicalmodel.
SW stands for ?sliding window.?
MLPrepresents a fully connected neural network.tence of the text is a sequence of d-dimensionalword vectors: tj= {tk}, tk?
Rd.
The semanticvector stis computed by embedding the word vec-tors into a D-dimensional space using a two-layernetwork that implements weighted sum followedby an affine tranformation and a nonlinearity; i.e.,st= f(At?k?ktk+ btA).
(1)The matrix At?
RD?d, the bias vector btA?RD, and for f we use the leaky ReLU function.The scalar ?kis a trainable weight associatedwith each word in the vocabulary.
These scalarweights implement a kind of exogenous or bottom-up attention that depends only on the input stimu-lus (Mayer et al, 2004).
They can, for example,learn to perform the function of stopword lists ina soft, trainable way, to nullify the contribution ofunimportant filler words.The semantic representation of a hypothesis isformed analogously, except that we concatenatethe question word vectors qmand answer wordvectors anas a single sequence {hp} = {qm, an}.For semantic vector shof the hypothesis, we usea unique transformation matrix Ah?
RD?dandbias vector bhA?
RD.These transformations map a text sentence anda hypothesis into a common space where they canbe compared.
We compute the semantic match be-tween text sentence and hypothesis using the co-sine similarity,Msem= cos(st, sh).
(2)4.2 Word-by-Word PerspectiveThe first step in building the word-by-word per-spective is to transform word vectors from atext sentence, question, and answer through re-spective neural functions.
For the text,?tk=f(Bttk+ btB), where Bt?
RD?d, btB?
RDand f is again the leaky ReLU.
We transform thequestion and the answer to?qmand?ananalogouslyusing distinct matrices and bias vectors.
In con-trast to the semantic perspective, we keep the ques-tion and answer candidates separate in the word-by-word perspective.
This is because matchesto answer words are inherently more importantthan matches to question words, and we want ourmodel to learn to use this property.4.2.1 SententialInspired by the work of Wang and Jiang (2015)in paraphrase detection, we compute matches be-tween hypotheses and text sentences at the wordlevel.
This computation uses the cosine similarityas before:cqkm= cos(?tk,?qm), (3)cakn= cos(?tk,?an).
(4)The word-by-word match between a text sen-tence and question is determined by taking themaximum over k (finding the text word that bestmatches each question word) and then taking aweighted mean over m (finding the average matchover the full question):Mq=1Z?m?mmaxkcqkm.
(5)Here, ?mis the word weight for the question wordandZ normalizes these weights to sum to one overthe question.
We define the match between a sen-tence and answer candidate, Ma, analogously.
Fi-nally, we combine the matches to question and an-swer according toMword= ?1Mq+ ?2Ma+ ?3MqMa.
(6)Here, the ?
are trainable parameters that controlthe relative importance of the terms.4354.2.2 Sequential Sliding WindowThe sequential sliding window is related to theoriginal MCTest baseline by Richardson et al(2013).
Our sliding window decays from its focusword according to a Gaussian distribution, whichwe extend by assigning a trainable weight to eachlocation in the window.
This modification en-ables the window to use information about the dis-tance between word matches; the original base-line (Richardson et al, 2013) used distance infor-mation through a predefined function.The sliding window scans over the words ofthe text as one continuous sequence, without sen-tence breaks.
Each window is treated like a sen-tence in the previous subsection, but we include alocation-based weight ?(k).
This weight is basedon a word?s position in the window, which, givena window, depends on its global position k. Thecosine similarity is adapted assqkm= ?
(k) cos(?tk,?qm), (7)for the question and analogously for the answer.We initialize the location weights with a Gaus-sian and fine-tune them during training.
The finalmatching score, denoted as Msws, is computed asin (5) and (6) with sqkmreplacing cqkm.4.2.3 Dependency Sliding WindowThe dependency sliding window operates identi-cally to the linear sliding window, but on a differ-ent view of the text passage.
The output of thiscomponent is Mswdand is formed analogously toMsws.The dependency perspective uses the StanfordDependency Parser (Chen and Manning, 2014) asan auxiliary tool.
Thus, the dependency graph canbe considered a fixed feature.
Moreover, lineariza-tion of the dependency graph, because it relieson an eigendecomposition, is not differentiable.However, we handle the linearization in data pre-processing so that the model sees only reorderedword-vector inputs.Specifically, we run the Stanford DependencyParser on each text sentence to build a dependencygraph.
This graph has nwvertices, one for eachword in the sentence.
From the dependency graphwe form the Laplacian matrix L ?
Rnw?nwanddetermine its eigenvectors.
The second eigenvec-tor u2of the Laplacian is known as the Fiedlervector.
It is the solution to the minimizationminimizegN?i,j=1?ij(g(vi)?
g(vj))2, (8)where viare the vertices of the graph and ?ijisthe weight of the edge from vertex i to vertexj (Golub and Van Loan, 2012).
The Fiedler vectormaps a weighted graph onto a line such that con-nected nodes stay close, modulated by the connec-tion weights.1This enables us to reorder the wordsof a sentence based on their proximity in the de-pendency graph.
The reordering of the words isgiven by the ordered index setI = arg sort(u2).
(9)To give an example of how this works, con-sider the following sentence from MCTest and itsdependency-based reordering:Jenny, Mrs. Mustard ?s helper, called thepolice.the police, called Jenny helper, Mrs. ?sMustard.Sliding-window-based matching on the originalsentence will answer the question Who called thepolice?
with Mrs. Mustard.
The dependency re-ordering enables the window to determine the cor-rect answer, Jenny.4.3 Combining Distributed EvidenceIt is important in comprehension to synthesize in-formation found throughout a document.
MCTestwas explicitly designed to ensure that it could notbe solved by lexical techniques alone, but wouldinstead require some form of inference or limitedreasoning (Richardson et al, 2013).
It thereforeincludes questions where the evidence for an an-swer spans several sentences.To perform synthesis, our model also takes in n-grams of sentences, i.e., sentence pairs and triplesstrung together.
The model treats these exactlyas it treats single sentences, applying all func-tions detailed above.
A later pooling operationcombines scores across all n-grams (including thesingle-sentence input).
This is described in thenext subsection.1We experimented with assigning unique edge weights tounique relation types in the dependency graph.
However, thishad negligible effect.
We hypothesize that this is becausedependency graphs are trees, which do not have cycles.436With n-grams, the model can combine infor-mation distributed across contiguous sentences.In some cases, however, the required evidence isspread across distant sentences.
To give our modelsome capacity to deal with this scenario, we takethe top N sentences as scored by all the preced-ing functions, and then repeat the scoring compu-tations, viewing these top N as a single sentence.The reasoning behind these approaches can beexplained well in a probabilistic setting.
If we con-sider our similarity scores to model the likelihoodof a text sentence given a hypothesis, p(tj| hi),then the n-gram and top N approaches model ajoint probability p(tj1, tj2, .
.
.
, tjk| hi).
We can-not model the joint probability as a product of in-dividual terms (score values) because distributedpieces of evidence are likely not independent.4.4 Combining PerspectivesWe use a multilayer perceptron (MLP) to combineMsem, Mword, Mswd, and Msws, as well as thescores for separate n-grams, as a final matchingscore Mifor each answer candidate.
This MLPhas multiple layers of staged input, because thedistinct scores have different dimensionality: thereis one Msemand one Mwordfor each story sen-tence, and oneMswdand oneMswsfor each appli-cation of the sliding window.
The MLP?s activa-tion function is linear.Our overall training objective is to minimize theranking lossL(T, q, A) = max(0, ?+ maxiMi 6=i??Mi?
),(10)where ?
is a constant margin, i?indexes the cor-rect answer.
We take the maximum over i so thatwe are ranking the correct answer over the best-ranked incorrect answer (of which there are three).This approach worked better than comparing thecorrect answer to the incorrect answers individu-ally as in Wang et al (2015).Our implementation of the Parallel-Hierarchicalmodel, built in Theano (Bergstra et al, 2010) us-ing the Keras framework (Chollet, 2015), is avail-able on Github.24.5 Training WheelsBefore training, we initialized the neural-networkcomponents of our model to perform sensibleheuristic functions.
Training did not converge onthe small MCTest without this vital approach.2https://github.com/Maluuba/mctest-modelEmpirically, we found that we could achieveabove 50% accuracy on MCTest using a simplesum of word vectors followed by a dot product be-tween the story-sentence sum and the hypothesissum.
Therefore, we initialized the network for thesemantic perspective to perform this sum, by ini-tializing Axas the identity matrix and bxAas thezero vector, x ?
{t, h}.
Recall that the activationfunction is aReLU so that positive outputs are un-changed.We also found basic word-matching scores tobe helpful, so we initialized the word-by-wordnetworks likewise.
The network for perspective-combination was initialized to perform a sum ofindividual scores, using a zero bias-vector and aweight matrix of ones, since we found that eachperspective contributed positively to the overall re-sult.This training wheels approach is related toother techniques from the literature.
For in-stance, Socher et al (2013) proposed the identity-matrix initialization in the context of parsing,and Le et al (2015) proposed it in the contextof recurrent neural networks (to preserve the er-ror signal through backpropagation).
In residualnetworks (He et al, 2015), shortcut connectionsbypass certain layers in the network so that a sim-pler function can be trained in conjunction withthe full model.5 Experiments5.1 The DatasetMCTest is a collection of 660 elementary-levelchildren?s stories and associated questions, writ-ten by human subjects.
The stories are fictional,ensuring that the answer must be found in the textitself, and carefully limited to what a young childcan understand (Richardson et al, 2013).The more challenging variant consists of 500stories with four multiple-choice questions each.Despite the elementary level, stories and questionsare more natural and more complex than thosefound in synthetic MC datasets like bAbI (Westonet al, 2014) and CNN (Hermann et al, 2015).MCTest is challenging because it is both com-plicated and small.
As per Hill et al (2015), ?itis very difficult to train statistical models only onMCTest.?
Its size limits the number of parame-ters that can be trained, and prevents learning anycomplex language modeling simultaneously withthe capacity to answer questions.4375.2 Training and Model DetailsIn this section we describe important details of thetraining procedure and model setup.
For a com-plete list of hyperparameter settings, our stopwordlist, and other minuti?, we refer interested readersto our Github repository.For word vectors we use Google?s publiclyavailable embeddings, trained with word2vec onthe 100-billion-word News corpus (Mikolov et al,2013).
These vectors are kept fixed throughouttraining, since we found that training them wasnot helpful (likely because of MCTest?s size).
Thevectors are 300-dimensional (d = 300).We do not use a stopword list for the text pas-sage, instead relying on the trainable word weightsto ascribe global importance ratings to words.These weights are initialized with the inverse doc-ument frequency (IDF) statistic computed over theMCTest corpus.3However, we do use a short stop-word list for questions.
This list nullifies querywords such as {who, what, when, where, how},along with conjugations of the verbs to do and tobe.Following earlier methods, we use a heuris-tic to improve performance on negation ques-tions (Sachan et al, 2015; Wang et al, 2015).When a question contains the words which andnot, we negate the hypothesis ranking scores sothat the minimum becomes the maximum.
Thisheuristic leads to an improvement around 6% onthe validation set.The most important technique for training themodel was the training wheels approach.
With-out this, training was not effective at all (see theablation study in Table 2).
The identity initializa-tion requires that the network weight matrices aresquare (d = D).We found dropout (Srivastava et al, 2014) to beparticularly effective at improving generalizationfrom the training to the test set, and used 0.5 asthe dropout probability.
Dropout occurs after allneural-network transformations, if those transfor-mations are allowed to change with training.
Ourbest performing model held networks at the word-by-word level fixed.For combining distributed evidence, we usedup to trigrams over sentences and our best-performing model reiterated over the top two sen-tences (N = 2).3We override the IDF initialization for words like not,which are frequent but highly informative.We used the Adam optimizer with the standardsettings (Kingma and Ba, 2014) and a learningrate of 0.003.
To determine the best hyperpa-rameters we performed a search over 150 settingsbased on validation-set accuracy.
MCTest?s orig-inal validation set is too small for reliable hy-perparameter tuning, so, following Wang et al(2015), we merged the training and validation setsof MCTest-160 and MCTest-500, then split themrandomly into a 250-story training set and a 200-story validation set.
This repartition of the datadid not affect overall performance per se; rather,the larger validation set made it easier to choosehyperparameters because validation results weremore consistent.5.3 ResultsTable 1 presents the performance of feature-engineered and neural methods on the MCTest testset.
Accuracy scores are divided among questionswhose evidence lies in a single sentence (single)and across multiple sentences (multi), and amongthe two variants.
Clearly, MCTest-160 is easier.The first three rows represent feature-engineered methods.
Richardson et al (2013) +RTE is the best-performing variant of the originalbaseline published along with MCTest.
It uses alexical sliding window and distance-based mea-sure, augmented with rules for recognizing textualentailment.
We described the methods of Sachanet al (2015) and Wang et al (2015) in Section 3.On MCTest-500, the Parallel Hierarchical modelsignificantly outperforms these methods on singlequestions (> 2%) and slightly outperforms thelatter two on multi questions (?
0.3%) and overall(?
1%).
The method of Wang et al (2015)achieves the best overall result on MCTest-160.We suspect this is because our neural methodsuffered from the relative lack of training data.The last four rows in Table 1 are neural methodsthat we discussed in Section 3.
Performance mea-sures are taken from Yin et al (2016).
Here wesee our model outperforming the alternatives by alarge margin across the board (> 15%).
The Neu-ral Reasoner and the Attentive Reader are large,deep models with hundreds of thousands of pa-rameters, so it is unsurprising that they performedpoorly on MCTest.
The specifically-designedHABCNN fared better, its convolutional architec-ture cutting down on the parameter count.
Becausethere are similarities between our model and the438MethodMCTest-160 accuracy (%) MCTest-500 accuracy (%)Single (112) Multiple (128) All Single (272) Multiple (328) AllRichardson et al (2013) + RTE 76.78 62.50 69.16 68.01 59.45 63.33Sachan et al (2015) - - - 67.65 67.99 67.83Wang et al (2015) 84.22 67.85 75.27 72.05 67.94 69.94Attentive Reader 48.1 44.7 46.3 44.4 39.5 41.9Neural Reasoner 48.4 46.8 47.6 45.7 45.6 45.6HABCNN-TE 63.3 62.9 63.1 54.2 51.7 52.9Parallel-Hierarchical 79.46 70.31 74.58 74.26 68.29 71.00Table 1: Experimental results on MCTest.Ablated component Validation accuracy (%)- 70.13n-gram 66.25Top N 66.63Sentential 65.00SW-sequential 68.88SW-dependency 69.75Word weights 66.88Trainable embeddings 63.50Training wheels 34.75Table 2: Ablation study on MCTest-500 (all).HABCNN, we hypothesize that the performancedifference is attributable to the greater simplicityof our model and our training wheels methodol-ogy.6 Analysis and DiscussionWe measure the contribution of each componentof the model by ablating it.
Results on the vali-dation set are given in Table 2.
Not surprisingly,the n-gram functionality is important, contribut-ing almost 4% accuracy improvement.
Withoutthis, the model has almost no means for synthe-sizing distributed evidence.
The top N functioncontributes similarly to the overall performance,suggesting that there is a nonnegligible numberof multi questions that have their evidence dis-tributed across noncontiguous sentences.
Ablatingthe sentential component made a significant differ-ence, reducing performance by about 5%.
Sim-ple word-by-word matching is obviously usefulon MCTest.
The sequential sliding window con-tributes about 1.3%, suggesting that word-distancemeasures are not overly important.
Similarly, thedependency-based sliding window makes a veryminor contribution.
We found this surprising.
Itmay be that linearization of the dependency graphremoves too much of its information.
The ex-ogenous word weights make a significant contri-bution of over 3%.
Allowing the embeddings tochange with training reduced performance fairlysignificantly, almost 8%.
As discussed, this is acase of having too many parameters for the avail-able training data.
Finally, we see that the trainingwheels methodology had enormous impact.
With-out heuristic-based initialization of the model?svarious weight matrices, accuracy goes down toabout 35%, which is only ten points over randomchance.Analysis reveals that most of our system?s testfailures occur on questions about quantity (e.g.,How many...? )
and temporal order (e.g., Whowas invited last?
).
Quantity questions make up9.5% of our errors on the validation set, while or-der questions make up 10.3%.
This weakness isnot unexpected, since our architecture lacks anycapacity for counting or tracking temporal order.Incorporating mechanisms for these forms of rea-soning is a priority for future work (in contrast,the Memory Network model (Weston et al, 2014)is quite good at temporal reasoning).The Parallel-Hierarchical model is simple.
Itdoes no complex language or sequence modeling.Its simplicity is a response to the limited data ofMCTest.
Nevertheless, the model achieves state-of-the-art results on the multi questions, which(putatively) require some limited reasoning.
Ourmodel is able to handle them reasonably well justby stringing important sentences together.
Thus,the model imitates reasoning with a heuristic.
Thissuggests that, to learn true reasoning abilities,MCTest is too simple a dataset?and it is almostcertainly too small for this goal.However, it may be that human language pro-cessing can be factored into separate processes ofcomprehension and reasoning.
If so, the Parallel-Hierarchical model is a good start on the former.Indeed, if we train the method exclusively on sin-gle questions then its results become even moreimpressive: we can achieve a test accuracy of79.1% on MCTest-500.
Note that this boost inperformance comes from training on only abouthalf the data.
The ?single?
questions can be con-439sidered a close analogue of the RTE task, at whichour model becomes very adept even with less data.Incorporating the various views of our modelamounts to encoding prior knowledge about theproblem structure.
This is similar to the purposeof feature engineering, except that the views canbe fully trained.
Encoding problem structure intothe structure of neural networks is not new: as an-other example, the convolutional architecture hasled to large gains in vision tasks.7 ConclusionWe have presented the novel Parallel-Hierarchicalmodel for machine comprehension, and evalu-ated it on the small but complex MCTest.
Ourmodel achieves state-of-the-art results, outper-forming several feature-engineered and neural ap-proaches.Working with our model has emphasized tous the following (not necessarily novel) concepts,which we record here to promote further empiricalvalidation.?
Good comprehension of language is sup-ported by hierarchical levels of understand-ing (cf.
Hill et al (2015)).?
Exogenous attention (the trainable wordweights) may be broadly helpful for NLP.?
The training wheels approach, that is, ini-tializing neural networks to perform sensibleheuristics, appears helpful for small datasets.?
Reasoning over language is challenging, buteasily simulated in some cases.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2014.
Neural machine translation by jointlylearning to align and translate.
arXiv preprintarXiv:1409.0473.Yoshua Bengio, R?ejean Ducharme, and Pascal Vincent.2000.
A neural probabilistic language model.
In Ad-vances in Neural Information Processing Systems,pages 932?938.J.
Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pas-canu, G. Desjardins, J. Turian, D. Warde-Farley, andY.
Bengio.
2010.
Theano: a CPU and GPU mathexpression compiler.
In In Proc.
of SciPy.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In EMNLP, pages 740?750.Franc?ois Chollet.
2015. keras.https://github.com/fchollet/keras.Silviu Cucerzan and Eugene Agichtein.
2005.
Factoidquestion answering over unstructured and structuredweb content.
In TREC, volume 72, page 90.Gene H Golub and Charles F Van Loan.
2012.
Matrixcomputations, volume 3.
JHU Press.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and JianSun.
2015.
Deep residual learning for image recog-nition.
arXiv preprint arXiv:1512.03385.Karl Moritz Hermann, Tomas Kocisky, EdwardGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-leyman, and Phil Blunsom.
2015.
Teaching ma-chines to read and comprehend.
In Advances in Neu-ral Information Processing Systems, pages 1684?1692.Felix Hill, Antoine Bordes, Sumit Chopra, and JasonWeston.
2015.
The goldilocks principle: Readingchildren?s books with explicit memory representa-tions.
arXiv preprint arXiv:1511.02301.Diederik Kingma and Jimmy Ba.
2014.
Adam: Amethod for stochastic optimization.
arXiv preprintarXiv:1412.6980.Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad-bury, Robert English, Brian Pierce, Peter Ondruska,Ishaan Gulrajani, and Richard Socher.
2015.Ask me anything: Dynamic memory networksfor natural language processing.
arXiv preprintarXiv:1506.07285.Quoc V Le, Navdeep Jaitly, and Geoffrey E Hin-ton.
2015.
A simple way to initialize recurrentnetworks of rectified linear units.
arXiv preprintarXiv:1504.00941.Andrew R Mayer, Jill M Dorflinger, Stephen MRao, and Michael Seidenberg.
2004.
Neuralnetworks underlying endogenous and exogenousvisual?spatial orienting.
Neuroimage, 23(2):534?541.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Karthik Narasimhan and Regina Barzilay.
2015.
Ma-chine comprehension with discourse relations.
In53rd Annual Meeting of the Association for Com-putational Linguistics.Baolin Peng, Zhengdong Lu, Hang Li, and Kam-FaiWong.
2015.
Towards neural network-based rea-soning.
arXiv preprint arXiv:1508.05508.Matthew Richardson, Christopher JC Burges, and ErinRenshaw.
2013.
Mctest: A challenge dataset forthe open-domain machine comprehension of text.
InEMNLP, volume 1, page 2.440Mrinmaya Sachan, Avinava Dubey, Eric P Xing, andMatthew Richardson.
2015.
Learning answerentail-ing structures for machine comprehension.
In Pro-ceedings of ACL.Ellery Smith, Nicola Greco, Matko Bosnjak, and An-dreas Vlachos.
2015.
A strong lexical matchingmethod for the machine comprehension test.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 1693?1698, Lisbon, Portugal, September.
Association forComputational Linguistics.Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013.
Parsing with composi-tional vector grammars.
In ACL (1), pages 455?465.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al2015.
End-to-end memory networks.
In Advancesin Neural Information Processing Systems, pages2431?2439.Shuohang Wang and Jing Jiang.
2015.
Learning nat-ural language inference with lstm.
arXiv preprintarXiv:1512.08849.Hai Wang, Mohit Bansal, Kevin Gimpel, and DavidMcAllester.
2015.
Machine comprehension withsyntax, frames, and semantics.
In Proceedings ofACL, Volume 2: Short Papers, page 700.Jason Weston, Sumit Chopra, and Antoine Bor-des.
2014.
Memory networks.
arXiv preprintarXiv:1410.3916.Wenpeng Yin, Sebastian Ebert, and Hinrich Sch?utze.2016.
Attention-based convolutional neural net-work for machine comprehension.
arXiv preprintarXiv:1602.04341.441
