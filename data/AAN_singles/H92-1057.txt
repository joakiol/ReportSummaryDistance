Experimental Results for Baseline Speech Recognition Performanceusing Input Acquired from a Linear Microphone ArrayHarvey F. Silverman, Smart E. Kirtman, John E. Adcock and Paul C. MeuseLaboratory  for  Engineer ing Man/Mach ine  Systems (LEMS)ABSTRACTIn this paper, baseline speech recognition performance is determinedboth for a single remote microphone and for a signal derived from adelay-and-sum beamformer using an eight-microphone linear array.An HMM-based, connected-speech, 38-word vocabulary (alphabet,digits, 'space', 'period'), talker-independent speech recognitionsystem is used for testing performance.
Normal performance, withno language model, i.e., raw word-level performance, is currentlyabout 81% for a set of talkers not in the training set and about91% for training set data.
The system has been trained andtested using a close-talking bead-mounted microphone.
Since ameaningful comparison requires using the same speech, the existingspeech database was appropriately pre-filtered, played out througha transducer (speaker) in the room environment, picked-up by themicrophone array, and re-stored as a digital file.
The resultingfile was post-processed and used as input to the recognizer; therecognition performance indicates the effect of the input device.
Thebaseline xperiment showed that both a single remote microphoneand the beamformed signal reduced performance by 12% in a roomwith no other talkers.
For the array tested, the error is generallyattributable toreverberation ff the floor and ceiling.1.
In t roduct ionIt is widely accepted that appropriate data-acquisition tech-nology must be available in order to make speech-recognitiona viable computer input mode \[1, 2, 3\].
While work hasbeen done in the area of signal conditioning \[4\], for thelast three years, research at Brown University has been inprogress to develop hardware, software and algorithms as ameans to make non-intrusive speech acquisition a practicalreality \[5, 6\] Principal focus to date has been to use thephase relationships among a group of microphones spacedin a line - hence a linear array - for the remote, real-timeacquisition of a talker's data.
Various beamforming and talkerlocation/tracking al orithms have been studied, reported, andevaluated relative to listening quality \[7, 8, 9, 10, 11, 12\]The quality of a speech data acquisition system may beassessed in several ways.
For many applications, evaluationis usually given, quantitatively, in terms of some signal-to-noise measure or human-listening experiment score, orqualitatively in terms of human evaluation.
However, fora system whose output is fed to a speech recognizer, the*Supported principally by NSF/DARPA Grant No.
IRI-8901882Div is ion of  Engineer ingBrown Univers i tyProv idence,  R102912recognition performance is an excellent, quantifiable measure;this approach and its results make up the body of this paper.A key problem for such systems to overcome is that of rever-beration.
Acoustic reflections in a normal room environmentmake the output of a remote microphone quite different fromthat taken from the normal, close-talking, recognizer micro-phone.
Several ways have been suggested to alleviate thisproblem:?
A more focused array system will attenuate r flectionscoming from a wider off-axis volume\[13\].
Many mi-crophones are required to do this, and a system withbeana-width control over a broad spectrum and in two orthree directions i  essential.
This is the spatial-filteringapproach to solving the problem..
The acoustic environment ear the microphones i  verycritical.
New ways of mounting the microphones inan appropriately sound absorbent material substantiallyimprove performance, without necessarily limiting thepracticality of the array.
More directional elements canalso be used.
This is an acoustical pproach to helpingto resolve the problem.?
One form or another of deconvolution can be usedto undo the effects of reverberations \[3, 14, 15, 16,17, 18, 19\].
Either directly or indirectly, some char-acterization of the room is obtained, usually as somespatially-dependent impulse response.
After this non-trivial problem is solved, some processing "art" is oftenessential to overcome nulls in the spectrum and performinverse filtering.This project investigates all of the above methods.
It mightbe added that, when working with real acoustic systems,mechanisms for reducing reverberations must be carefullyapplied; it is a hard problem.
However, the purpose ofthis paper is not to deal with the improvements achieved byemploying various means to dereverberate th  output signal ofthe array; rather, it is to set a baseline standard against whichto compare future developments.
The problem is posed: howbadly does recognizer performance degrade when the inputsignal is from 1) a single remote omnidirectional microphone,285or from 2) the beam:formed output from a linear microphonearray?
This experiment quantifies the acceptability (or lackthereof) of using relatively straightforward implementationsof remote microphone t chnology for speech recognition.2.
The LEMS Speech RecognizerAn HMM-based, connected-speech, 38-word vocabulary (al-phabet, digits, 'space', 'period'), talker-independent speechrecognition system has been running for two years in theLEMS facility \[20, 21\].
This small, but very difficult vocab-ulary has many of the problems associated with a phonemerecognizer.Speech, sampled at 16kHz from a close-talking microphone,is tnmcated through a 40ms Hamming window every 10ms.Twelve cepstral coefficients, twelve delta cepstral coeffi-cients, overall energy and delta overall energy comprise the26 element feature vector.
Three 256-entry codebooks areused to vector quantize the data from cepstral, delta cepstral,and energy/delta energy features respectively 1.
The recog-nizer differs from standard HMM models in that durationalprobabilities are handled explicitly \[22\].
For each state, selftransitions are disallowed.
During training, nonparametricstatistics are gathered for each of 30 potential durations inthe state, i.e., 10ms to 300ms.
In the base system used forthis experiment, a gamma distribution was fitted to the non-parametric statistics.
The models used are word-level modelshaving from five to twelve states.
Only forward transitionsand skips of a single state were allowed.The best available recognizer at the time was used for theexperiment, except hat the amount of speech normally usedto develop the vector quantizafion codebooks was reducedfrom one and one-half hours to 15 minutes.
This made itfeasible to do several full k-means re-trainings of the system;VQ training took but two days (elapsed time) on a SUNSPARCstation 2 while VQ training for the one and one-halfhour case would have taken an unacceptable twelve days2!The change to the VQ training degraded performance forthe close-talking microphone data by 1.5%, i.e., the 79%performance of the system for 1) new talkers and 2) nogrammar was reduced to 77.5%.About four hours of speech (2400 connected strings, ornearly 40,000 vocabulary items) from 80 talkers, half male,half female, were used to train the hidden Markov models.Currently, the training procedure requires 60 hours of CPUtime from each of eight SPARC 1+/2 workstations linkedin a loosely-coupled fashion through sockets.
Well-knownmechanisms for speeding up the process, such as doing theeat the time this experiment was initiated, semi-continuous modelingof output probabilities and better word models were not yet a part of thesystem.
Current improvements have increased overall performance for thehead-mounted microphone input by about 3%.2We are optimizing this program now.m2rnSMALL  ROOMN I Te?hn ioe  EAS-12PM216SMike I M ike  8I_ _!Approx.
2mFigure 1: Acoustical Geometry of Array/Sourcescomputation i the logarithm domain using integers and alookup table \[23\], as well as some detailed new programmingspeedups \[24\] are being used to reduce the training time.3.
Data DevelopmentThe original speech data were recorded in a large, generallynot-too-noisy room through an Audio Technica ATM73ahead-mounted, close-talking microphone.
The speech wassampled through a Sony DAT- 16 bits at 48kHz samplingrate.
It was then digitally decimated to16kHz and fed directlyto a SUN workstation to build a high-fidelity database \[25\].The signal-to-noise ratio is about 50dB.It would not have been possible, let alne feasible, to recordanother large dataset from the same talkers using the micro-phone array system for acquisition.
Thus, a mechanism hadto be developed to use the high-fidelity database as input tothe array recording system.
A high-quality transducer wasused to play out the speech; the geometry isshown in Figure1.
The resulting real-time system for the data conversionis schematically shown in Figure 2.
Three SPARC 1+/2workstations are used.
The first converts the digital speechdata in speech recognition format into digital data acceptablefor playback through the microphone array hardware.
Thisinvolves changing the sampling rate from 16kHz to 20kHzand then applying an FIR inverse filter to undo the coloringthat will come from the output ransducer.
This filter wasobtained by running digital, band-limited white noise withDFr spectrum W(r) through the transducer and recordingthe output hrough an ultra-flat frequency response Briiel& Kjaer (B&K) condenser microphone system placed a few286-'H ??
N--Veo~r  ~v.
(Disk) OuanL.
.
.
.
.
.
.
.
.
.
.
.
.
P : .~-~, .
.
l , _  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.SPARC Wo~kstaUons \[S~allah H ="  N FIIlllr iSignal Condltlonlr I l k  i., .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
l _ __~_- , _~ i  .
.
.
.
.
.
.
.
.
.
.
.
Iltllo mphcl~l Ar rl~# SyslllmOU.I(x2)I L~vpass FilterArnpllflr IAooustloal E~lvlronment3 .0 - -2520  .
- -i 1 .5 - -1 .0 - -0 .5-i ii,I, i  1 tE r) -TrenoduGer SpectrumY(r )  - Inverae SpectrumI1000 2000 3000 4000I//Ih I I5000 6000 7000FREQUENCY~Figure 2: The Data Conversion System Figure 3: Spectra of the Output Transducer System andInverse Filtercentimeters in front of the middle of the output ransducer.After accumulating an average magnitude spectrum of theB&K's output via multiple 128-point DFT's, the spectrumS(r) was inverted, i.e., Y(r) = W(r ) /S ( r ) ,  and inversetransformed toproduce azero-phase FIR filter 3.
Any spectralenergy attenuated by the anti-aliasing filter i.e., frequenciesabove 7kHz, were forced to unity gain.
S(r) and Y(r) areshown in Figure 3.
The subjective audible ffects as wellas the flattened white-noise response indicate that this proce-dure was successful in removing the 'boominess' potentiallyintroduced by the transducer system?Initially, small, omnidirectional electret microphones weremounted at the edge of a 5cmx 10cm board containing am-plifier/filter electronics and the board was plugged verticallyinto a (2.5m) horizontal cage?
Recent work disclosed thatthis system formed resonant cavities that impacted the per-formance of the linear microphone array.
When the samemicrophones with the same spacing (18?4 cm) were insertedinto a (180cmx 30cmx 15cm) block of six pound ester foam,the degradations due to the cavities disappeared asmay beseen in Figure 4.
Note that the data shown are for thetransducer output after the noise has been inverse filtered.The remainder of the data conversion system is straightfor-ward.
Twenty kilohertz sampling interrupts are used bothto produce the speech output(s) and to digitize the analogsignals from the eight microphones.
Sufficient memory isavailable for about 10 second utterances.
Upon comple-tion of an utterance, the microphone data are sent to a third3Non-zero-pha~zinverse filters are also being investigated.SPARCstation for sample-rate conversion, signal processingfor recognition, and archiving on hard disk as feattwe vectorsfor the recognition system?4.
Exper iment  and Resul tsThe system was trained, both for VQ and for the hiddenMarkov model parameters, three different imes: 1) for thehigh-fidelity data, 2) for the output of a single microphoneof the array (a. central one), and 3) for the simple delay-and-sum beamfonned output of the 8 microphone array.
Therecognizer was tested using 20 new talkers, again half maleand half female, for a total of an hour of speech, or about 4800vocabulary items.
The data conversion system was run under'quiet' conditions.
Not including noise due to reverberations,the signal-to-noise ratios were significantly degraded by theacoustical noise to 24dB for the single remote microphoneand 26dB for the beamformed signal.
The results as a functionof talker number are plotted in Figure 5.
From the Figure,one may deduce that:For all cases, variation with respect to talker is far greaterthan variations due to other effects.Recognition performance is approximately the same forthe single microphone as it is for the beamformed case,given no other point 'noise' sources..Performance for the high-fidelity signal is consistentlyabout 12% better than for the acoustically degradedsignal.2874.
(1*~04.o-6.041.0-10.0-12.0-14.0-16.0-18.0Figure 4:and Foam.,,;i t l I' ti ?
iJ Army In Cage2000 4000 6000 8000FREQUENCY (Hi), DLmmI-lO,OOoSpectra of Array Microphone Response in Cagei80.0-70.0-60.0-50.0-40.0-30.0-20.0-J Ii , L x /  ?
~ - -t!/ i /,; '/l ',.
.
.
.
./ I I~', ~Bmmformed Array1--10 15 20Tedker NumberFigure 5: RecognitionPerformance for the Three AcquisitionSystemsFor completeness, each of the test datasets was run againsteach of the three systems.
The results are given in Table 1.Test Data J Model Trained fromfrom I Hi-Fi Remote Mike BeamformedHi-Fi 77.5% 50.0% 53.6%Remote Mike 38.8% 65.6% 64.6%Beam:formed 32.8% 57.6% 65.3%Table 1Averaged Results for Direct and Cross-Trained Systems5.
DiscussionGiven the degraded acoustical environment, i  was not sur-prising that performance for the converted ata was reducedusing remote-microphone i put.
However, it was somewhatsurprising that his very carefully done experiment indicatesno performance advantage when simple beamforming is usedto generate the input.
This could be due to the following:?
Low-frequency background noise is not effectively elim-inated by an acoustic array of this type and size.
Somefiltering, perhaps combined with sub-band types of en-hancements, should help.?
The major reverberations in the room come from theceiling and floor.
They have been measured as beingas much as 25% of the original wavefront in intensity.Even if the reflections average 10%, implying a 14dBsignal-to-noise ratio, 'quiet' room conditions no longerhold.
A focused two or three dimensional rray couldattenuate hese reflections and thus address the problem.Altematively, pressure gradient microphones could beused in a one-dimensional array as done in \[13\].There is always some variability in an acoustical exper-iment regarding equipment positioning, overall ampli-tudes, microphone calibration etc.
While great care wastaken, certainly the beamformer output would be moresusceptible tothese variabilities than would be the singleremote microphone.In order to determine the impact of beamforming, the testingdata were run through the data conversion system (source at(1 m, 2m)) several additional times, each with a second trans-ducer located at (2m, 2m).
This second transducer repeateda few seconds of speech at various, controlled levels as thetesting data were being recorded.
This procedure permitsthe assessment ofthe effects of beamforming with respect ospatial filtering of off-axis noise.
The test datasets for boththe single remote microphone and the beam:formed data wererun through their respective quiet-room recognizers.
As thepurpose of this test was to check the simple beamformer, moreelaborate beamformers were not used to generate the data ofFigure 6.
Also, note that no background noise processing(such as high-pass filtering the signals) was used to removethe low-frequency 'rumble' of the room.288i+t3eeo.50.0 - -40.0 - -30.0 - -20.0 - -10.0 - -0.0-10.0 - -//!
/JJ/ - J7JJ t///s'/j J "J/Ji 8Ingle Mlcroph0ne J0.0 S.0 10.0 15.0 20.0Signal to Nolam Ratio (dB)Figure 6: Performance ina Noisy EnvironmentAs the graph indicates, there is an appreciable performancegain using the array for acoustic data collection in a noisyenvironment.
The simple beamformer consistently scored10-15% higher than a single microphone for SNR's less than16dB.
Note that in one case the recognition result is negative.This is a consequence of the method employed for calculatingthe performance score.6.
ConclusionAn intricate experiment has been developed to quantifythe effects of alternative acoustic environments on speech-recognition systems.
The performance of an HMM-basedalphadigit recognizer was reduced about 12% when input wasconverted from high-fidelity, close-talking input to either asingle remote microphone or the output of a delay-and-sumbeamformer using an eight-microphone linear array underquiet conditions.
Beamforming did significantly improveperformance over that of a single microphone for low signal-to-noise ratios and is thus advantageous in the presence ofacoustic interference.More importantly, though, the work establishes an automatedprocedure for reconstructing a given database in a new envi-ronment, permitting the evaluation of acoustic-input devices.Such a structured methodology has allowed the determinationof baseline performance and now future improvements canbe appropriately measured.289References\[1\] J. L. Flanagan.
Bandwidth design for speech-seekingmicrophone arrays.
Proceedings of 1985 ICASSP, pages732-735, March 1985.\[2\] J. L. Flanagan, J. D. Johnson, R. Zahn, and G. W.Elko.
Computer-steered microphone arrays for soundtransduction in large rooms.
J. Accoust.
Soc.
Am.,78(5): 1508-1518, November 1985.\[3\] J.
B. Allen, D. A. Berkley, and J. Blauert.
Multimi-crophone signal-processing technique to remove roomreverberation from speech signals.
Journal of the Acous-tical Society of America, 62(4):912-915, October 1977.\[4\] A. Acero and R. M. Stem.
Towards environment-ir~ependent spoken language systems.
In Proceedingsof the Speech and Natural Language Workshop, ages157 - 162, Hidden Valley, Pennsylvania, June 1990.\[5\] H. E Silverman.
Some analysis of microphone arraysfor speech data acquisition.
IEEE Trans.
on Acous-tics, Speech, and Signal Processing, ASSP-35(2): 1699-1712, December 1987.\[6\] M. S. Brandstein.
Design and implementation of theLEMS microphone array for the acquisition of highquality speech signals.
Brown University Honor's The-sis, May 1988.\[7\] V. M. Alvarado and H. E Silverman.
Experimentalresults showing the effects of optimal spacing betweenelements of a linear microphone array.
In Proceedingsof 1990 ICASSP, pages 837-840, Albuquerque, NM,April 1990.\[8\] V. M. Alvarado.
Talker Localization and Optimal Place-ment of Microphones for a Linear Microphone Arrayusing Stochastic Region Contraction.
PhD thesis, BrownUniversity, May 1990.\[9\] H. E Silverman.
DSP beamforming and talker trackingwith a linear microphone array.
In Proceedings of JASA119th Meeting, page $3, State College, PA, May 1990.\[I0\] H. F. Silverman.
An algorithm for determining talkerlocation using a linear microphone array and optimalhyperbolic fit.
In Proceedings DARPA Speech andNatural Language Workshop, ages 151-156, HiddenValley,PA, June 1990.\[11\] M. Berger and H. E Silverman.
Microphone arrayoptimization by stochastic region contraction (SRC).IEEE Transactions on Signal Processing, 39(11):2377-2386, 1991.\[12\] H. E Silverman and S. E. Kirtman.
A two-stage al-gorithm for determining talker location from linearmicrophone-array data.
LEMS Technical Report 97,LEMS,Division of Engineering, Brown University,Providence, RI 02912, November 1991.\[13\] J. L. Flanagan, R. Mammone, and G. W. Elko.
Autodi-rective microphone systems for natural communicationwith speech recognizers.
In Proceedings of the FourthDARPA Workshop on Speech and Natural Language,pages 4.8 - 4.13, Asilomar, CA, February 1991.\[14\] S. T. Neely and J.
B. Allen.
Invertability of a roomimpulse response.
Journal of the Acoustical Society ofAmerica, 66(1): 165-169, July 1979.\[15\] R. Zelinski.
A microphone array with adaptive post-filtering for noise reduction i reverberent rooms.
In Pro-ceedings oflCASSP88, pages 2578-2580, New York,NY, April 1988.\[16\] D. Van Compemolle, W. Ma, E Xie, and M. VanDiest.
Speech recognition i noisy environments withthe aid of microphone arrays.
Speech Communication,9(5/6):433-442, December 1990.\[17\] D. Bees, M. Blostein, and P. Kabal.
Reverberent speechenhancement using cepstral processing.
In Proceedingsof ICASSP91, pages 977- 980, Toronto, Ont, Canada,April 1991.\[18\] H. Wang and F. Itakura.
An approach ofdereverberationusing multi-microphone sub-band envelope stimation.In Proceedings oflCASSP91, pages 953- 956, Toronto,Ont, Canada, April 1991.\[19\] H. Yamada, H. Wang, and F. Itakura.
Recovering ofbroad band reverberent speech signal by sub-band mintmethod.
In Proceedings oflCASSP91, pages 969- 972,Toronto, Ont, Canada, April 1991.\[20\] L. T. Niles and H. F. Silverman.
Combining hiddenMarkov model and neural network classifiers.
In Pro-ceedings of 1990 ICASSP, pages 417-420, Albuquerque,NM, April 1990.\[21\] M. M. Hochberg, J. T. Foote, and H. F. Silverman.The LEMS talker-independent speech recognition sys-tem.
LEMS Technical Report 82, LEMS, Division ofEngineering, Brown University, Providence, RI 02912,March 1991.\[22\] M. M. Hochberg, J. T. Foote, and H. F. Silverman.Explicit state duration modeling for HMM-based con-nected speech recognition.
In 1991 Arden House Work-shop on Speech Recognition, Harriman, NY, December1992.
Accepted for Presentation.290\[231\[24\]\[25\]E E Brown.
The Acoustic Modeling Problem in Auto-matic Speech Recognition.
PhD thesis, Carnegie-MellonUniversity, Pittsburgh, PA, May 1987.J.
T. Foote, M. M. Hochberg, P. M. Athanas, A. T.Smith, M. E. Waslowski, and H. E Silverman.
Dis-tributed hidden Markov model training on loosely-coupled mulfiprocessor networks.
In Proceedings of1992 lCASSP(Accepted), San Francisco, CA, March1992.J.
T. Foote.
The LEMS DAT/SCSI interface: User'sguide and technical reference.
LEMS Technical Re-port 95, LEMS, Division of Engineering, Brown Uni-versity, Providence, RI 02912, November 1991.
