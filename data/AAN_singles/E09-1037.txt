Proceedings of the 12th Conference of the European Chapter of the ACL, pages 318?326,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsCube Summing, Approximate Inference with Non-Local Features,and Dynamic Programming without SemiringsKevin Gimpel and Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{kgimpel,nasmith}@cs.cmu.eduAbstractWe introduce cube summing, a techniquethat permits dynamic programming algo-rithms for summing over structures (likethe forward and inside algorithms) to beextended with non-local features that vio-late the classical structural independenceassumptions.
It is inspired by cube prun-ing (Chiang, 2007; Huang and Chiang,2007) in its computation of non-localfeatures dynamically using scored k-bestlists, but also maintains additional resid-ual quantities used in calculating approx-imate marginals.
When restricted to lo-cal features, cube summing reduces to anovel semiring (k-best+residual) that gen-eralizes many of the semirings of Good-man (1999).
When non-local features areincluded, cube summing does not reduceto any semiring, but is compatible withgeneric techniques for solving dynamicprogramming equations.1 IntroductionProbabilistic NLP researchers frequently make in-dependence assumptions to keep inference algo-rithms tractable.
Doing so limits the features thatare available to our models, requiring featuresto be structurally local.
Yet many problems inNLP?machine translation, parsing, named-entityrecognition, and others?have benefited from theaddition of non-local features that break classicalindependence assumptions.
Doing so has requiredalgorithms for approximate inference.Recently cube pruning (Chiang, 2007; Huangand Chiang, 2007) was proposed as a way to lever-age existing dynamic programming algorithmsthat find optimal-scoring derivations or structureswhen only local features are involved.
Cube prun-ing permits approximate decoding with non-localfeatures, but leaves open the question of how thefeature weights or probabilities are learned.
Mean-while, some learning algorithms, like maximumlikelihood for conditional log-linear models (Laf-ferty et al, 2001), unsupervised models (Pereiraand Schabes, 1992), and models with hidden vari-ables (Koo and Collins, 2005; Wang et al, 2007;Blunsom et al, 2008), require summing over thescores of many structures to calculate marginals.We first review the semiring-weighted logicprogramming view of dynamic programming al-gorithms (Shieber et al, 1995) and identify an in-tuitive property of a program called proof localitythat follows from feature locality in the underlyingprobability model (?2).
We then provide an analy-sis of cube pruning as an approximation to the in-tractable problem of exact optimization over struc-tures with non-local features and show how theuse of non-local features with k-best lists breakscertain semiring properties (?3).
The primarycontribution of this paper is a novel technique?cube summing?for approximate summing overdiscrete structures with non-local features, whichwe relate to cube pruning (?4).
We discuss imple-mentation (?5) and show that cube summing be-comes exact and expressible as a semiring whenrestricted to local features; this semiring general-izes many commonly-used semirings in dynamicprogramming (?6).2 BackgroundIn this section, we discuss dynamic programmingalgorithms as semiring-weighted logic programs.We then review the definition of semirings and im-portant examples.
We discuss the relationship be-tween locally-factored structure scores and proofsin logic programs.2.1 Dynamic ProgrammingMany algorithms in NLP involve dynamic pro-gramming (e.g., the Viterbi, forward-backward,318probabilistic Earley?s, and minimum edit distancealgorithms).
Dynamic programming (DP) in-volves solving certain kinds of recursive equationswith shared substructure and a topological order-ing of the variables.Shieber et al (1995) showed a connectionbetween DP (specifically, as used in parsing)and logic programming, and Goodman (1999)augmented such logic programs with semiringweights, giving an algebraic explanation for theintuitive connections among classes of algorithmswith the same logical structure.
For example, inGoodman?s framework, the forward algorithm andthe Viterbi algorithm are comprised of the samelogic program with different semirings.
Goodmandefined other semirings, including ones we willuse here.
This formal framework was the basisfor the Dyna programming language, which per-mits a declarative specification of the logic pro-gram and compiles it into an efficient, agenda-based, bottom-up procedure (Eisner et al, 2005).For our purposes, a DP consists of a set of recur-sive equations over a set of indexed variables.
Forexample, the probabilistic CKY algorithm (run onsentence w1w2...wn) is written asCX,i?1,i = pX?wi (1)CX,i,k = maxY,Z?N;j?
{i+1,...,k?1}pX?Y Z ?
CY,i,j ?
CZ,j,kgoal = CS,0,nwhere N is the nonterminal set and S ?
N is thestart symbol.
Each CX,i,j variable corresponds tothe chart value (probability of the most likely sub-tree) of an X-constituent spanning the substringwi+1...wj .
goal is a special variable of greatest in-terest, though solving for goal correctly may (ingeneral, but not in this example) require solvingfor all the other values.
We will use the term ?in-dex?
to refer to the subscript values on variables(X, i, j on CX,i,j).Where convenient, we will make use of Shieberet al?s logic programming view of dynamic pro-gramming.
In this view, each variable (e.g., CX,i,jin Eq.
1) corresponds to the value of a ?theo-rem,?
the constants in the equations (e.g., pX?Y Zin Eq.
1) correspond to the values of ?axioms,?and the DP defines quantities corresponding toweighted ?proofs?
of the goal theorem (e.g., find-ing the maximum-valued proof, or aggregatingproof values).
The value of a proof is a combi-nation of the values of the axioms it starts with.Semirings define these values and define two op-erators over them, called ?aggregation?
(max inEq.
1) and ?combination?
(?
in Eq.
1).Goodman and Eisner et al assumed that the val-ues of the variables are in a semiring, and that theequations are defined solely in terms of the twosemiring operations.
We will often refer to the?probability?
of a proof, by which we mean a non-negative R-valued score defined by the semanticsof the dynamic program variables; it may not be anormalized probability.2.2 SemiringsA semiring is a tuple ?A,?,?,0,1?, in which Ais a set, ?
: A ?
A ?
A is the aggregationoperation, ?
: A ?
A ?
A is the combina-tion operation, 0 is the additive identity element(?a ?
A, a ?
0 = a), and 1 is the multiplica-tive identity element (?a ?
A, a ?
1 = a).
Asemiring requires ?
to be associative and com-mutative, and ?
to be associative and to distributeover?.
Finally, we require a?0 = 0?a = 0 forall a ?
A.1 Examples include the inside semir-ing, ?R?0,+,?, 0, 1?, and the Viterbi semiring,?R?0,max,?, 0, 1?.
The former sums the prob-abilities of all proofs of each theorem.
The lat-ter (used in Eq.
1) calculates the probability of themost probable proof of each theorem.
Two moreexamples follow.Viterbi proof semiring.
We typically need torecover the steps in the most probable proof inaddition to its probability.
This is often done us-ing backpointers, but can also be accomplished byrepresenting the most probable proof for each the-orem in its entirety as part of the semiring value(Goodman, 1999).
For generality, we define aproof as a string that is constructed from stringsassociated with axioms, but the particular formof a proof is problem-dependent.
The ?Viterbiproof?
semiring includes the probability of themost probable proof and the proof itself.
LettingL ?
??
be the proof language on some symbolset ?, this semiring is defined on the set R?0 ?
Lwith 0 element ?0, ?
and 1 element ?1, ?.
Fortwo values ?u1, U1?
and ?u2, U2?, the aggregationoperator returns ?max(u1, u2), Uargmaxi?
{1,2} ui?.1When cycles are permitted, i.e., where the value of onevariable depends on itself, infinite sums can be involved.
Wemust ensure that these infinite sums are well defined underthe semiring.
So-called complete semirings satisfy additionalconditions to handle infinite sums, but for simplicity we willrestrict our attention to DPs that do not involve cycles.319Semiring A Aggregation (?)
Combination (?)
0 1inside R?0 u1 + u2 u1u2 0 1Viterbi R?0 max(u1, u2) u1u2 0 1Viterbi proof R?0 ?
L ?max(u1, u2), Uargmaxi?
{1,2} ui?
?u1u2, U1.U2?
?0, ?
?1, ?k-best proof (R?0 ?
L)?k max-k(u1 ?
u2) max-k(u1 ?
u2) ?
{?1, ?
}Table 1: Commonly used semirings.
An element in the Viterbi proof semiring is denoted ?u1, U1?, where u1 is the probabilityof proof U1.
The max-k function returns a sorted list of the top-k proofs from a set.
The ?
function performs a cross-producton two k-best proof lists (Eq.
2).The combination operator returns ?u1u2, U1.U2?,where U1.U2 denotes the string concatenation ofU1 and U2.2k-best proof semiring.
The ?k-best proof?semiring computes the values and proof strings ofthe k most-probable proofs for each theorem.
Theset is (R?0 ?
L)?k, i.e., sequences (up to lengthk) of sorted probability/proof pairs.
The aggrega-tion operator ?
uses max-k, which chooses the khighest-scoring proofs from its argument (a set ofscored proofs) and sorts them in decreasing order.To define the combination operator ?, we requirea cross-product that pairs probabilities and proofsfrom two k-best lists.
We call this ?, defined ontwo semiring values u = ?
?u1, U1?, ..., ?uk, Uk?
?and v = ?
?v1, V1?, ..., ?vk, Vk??
by:u ?
v = {?uivj , Ui.Vj?
| i, j ?
{1, ..., k}} (2)Then, u ?
v = max-k(u ?
v).
This is similar tothe k-best semiring defined by Goodman (1999).These semirings are summarized in Table 1.2.3 Features and InferenceLet X be the space of inputs to our logic program,i.e., x ?
X is a set of axioms.
Let L denote theproof language and let Y ?
L denote the set ofproof strings that constitute full proofs, i.e., proofsof the special goal theorem.
We assume an expo-nential probabilistic model such thatp(y | x) ?
?Mm=1 ?hm(x,y)m (3)where each ?m ?
0 is a parameter of the modeland each hm is a feature function.
There is a bijec-tion between Y and the space of discrete structuresthat our model predicts.Given such a model, DP is helpful for solvingtwo kinds of inference problems.
The first prob-lem, decoding, is to find the highest scoring proof2We assume for simplicity that the best proof will neverbe a tie among more than one proof.
Goodman (1999) han-dles this situation more carefully, though our version is morelikely to be used in practice for both the Viterbi proof andk-best proof semirings.y?
?
Y for a given input x ?
X:y?
(x) = argmaxy?Y?Mm=1 ?mhm(x,y) (4)The second is the summing problem, whichmarginalizes the proof probabilities (without nor-malization):s(x) =?y?Y?Mm=1 ?mhm(x,y) (5)As defined, the feature functions hm can dependon arbitrary parts of the input axiom set x and theentire output proof y.2.4 Proof and Feature LocalityAn important characteristic of problems suited forDP is that the global calculation (i.e., the value ofgoal ) depend only on local factored parts.
In DPequations, this means that each equation connectsa relatively small number of indexed variables re-lated through a relatively small number of indices.In the logic programming formulation, it meansthat each step of the proof depends only on the the-orems being used at that step, not the full proofsof those theorems.
We call this property proof lo-cality.
In the statistical modeling view of Eq.
3,classical DP requires that the probability modelmake strong Markovian conditional independenceassumptions (e.g., in HMMs, St?1 ?
St+1 | St);in exponential families over discrete structures,this corresponds to feature locality.For a particular proof y of goal consisting oft intermediate theorems, we define a set of proofstrings `i ?
L for i ?
{1, ..., t}, where `i corre-sponds to the proof of the ith theorem.3 We canbreak the computation of feature function hm intoa summation over terms corresponding to each `i:hm(x, y) =?ti=1 fm(x, `i) (6)This is simply a way of noting that feature func-tions ?fire?
incrementally at specific points in the3The theorem indexing scheme might be based on a topo-logical ordering given by the proof structure, but is not im-portant for our purposes.320proof, normally at the first opportunity.
Any fea-ture function can be expressed this way.
For localfeatures, we can go farther; we define a functiontop(`) that returns the proof string correspondingto the antecedents and consequent of the last infer-ence step in `.
Local features have the property:hlocm (x, y) =?ti=1 fm(x, top(`i)) (7)Local features only have access to the most re-cent deductive proof step (though they may ?fire?repeatedly in the proof), while non-local featureshave access to the entire proof up to a given the-orem.
For both kinds of features, the ?f?
termsare used within the DP formulation.
When tak-ing an inference step to prove theorem i, the value?Mm=1 ?fm(x,`i)m is combined into the calculationof that theorem?s value, along with the values ofthe antecedents.
Note that typically only a smallnumber of fm are nonzero for theorem i.When non-local hm/fm that depend on arbitraryparts of the proof are involved, the decoding andsumming inference problems are NP-hard (theyinstantiate probabilistic inference in a fully con-nected graphical model).
Sometimes, it is possibleto achieve proof locality by adding more indices tothe DP variables (for example, consider modify-ing the bigram HMMViterbi algorithm for trigramHMMs).
This increases the number of variablesand hence computational cost.
In general, it leadsto exponential-time inference in the worst case.There have been many algorithms proposed forapproximately solving instances of these decod-ing and summing problems with non-local fea-tures.
Some stem from work on graphical mod-els, including loopy belief propagation (Sutton andMcCallum, 2004; Smith and Eisner, 2008), Gibbssampling (Finkel et al, 2005), sequential MonteCarlo methods such as particle filtering (Levy etal., 2008), and variational inference (Jordan et al,1999; MacKay, 1997; Kurihara and Sato, 2006).Also relevant are stacked learning (Cohen andCarvalho, 2005), interpretable as approximationof non-local feature values (Martins et al, 2008),and M-estimation (Smith et al, 2007), which al-lows training without inference.
Several other ap-proaches used frequently in NLP are approximatemethods for decoding only.
These include beamsearch (Lowerre, 1976), cube pruning, which wediscuss in ?3, integer linear programming (Rothand Yih, 2004), in which arbitrary features can actas constraints on y, and approximate solutions likeMcDonald and Pereira (2006), in which an exactsolution to a related decoding problem is foundand then modified to fit the problem of interest.3 Approximate DecodingCube pruning (Chiang, 2007; Huang and Chi-ang, 2007) is an approximate technique for decod-ing (Eq.
4); it is used widely in machine transla-tion.
Given proof locality, it is essentially an effi-cient implementation of the k-best proof semiring.Cube pruning goes farther in that it permits non-local features to weigh in on the proof probabili-ties, at the expense of making the k-best operationapproximate.
We describe the two approximationscube pruning makes, then propose cube decoding,which removes the second approximation.
Cubedecoding cannot be represented as a semiring; wepropose a more general algebraic structure that ac-commodates it.3.1 Approximations in Cube PruningCube pruning is an approximate solution to the de-coding problem (Eq.
4) in two ways.Approximation 1: k < ?.
Cube pruning usesa finite k for the k-best lists stored in each value.If k = ?, the algorithm performs exact decodingwith non-local features (at obviously formidableexpense in combinatorial problems).Approximation 2: lazy computation.
Cubepruning exploits the fact that k < ?
to use lazycomputation.
When combining the k-best prooflists of d theorems?
values, cube pruning does notenumerate all kd proofs, apply non-local featuresto all of them, and then return the top k. Instead,cube pruning uses a more efficient but approxi-mate solution that only calculates the non-localfactors on O(k) proofs to obtain the approximatetop k. This trick is only approximate if non-localfeatures are involved.Approximation 2 makes it impossible to formu-late cube pruning using separate aggregation andcombination operations, as the use of lazy com-putation causes these two operations to effectivelybe performed simultaneously.
To more directlyrelate our summing algorithm (?4) to cube prun-ing, we suggest a modified version of cube prun-ing that does not use lazy computation.
We callthis algorithm cube decoding.
This algorithm canbe written down in terms of separate aggregation321and combination operations, though we will showit is not a semiring.3.2 Cube DecodingWe formally describe cube decoding, show thatit does not instantiate a semiring, then describea more general algebraic structure that it does in-stantiate.Consider the set G of non-local feature functionsthat map X ?
L ?
R?0.4 Our definitions in ?2.2for the k-best proof semiring can be expanded toaccommodate these functions within the semiringvalue.
Recall that values in the k-best proof semir-ing fall inAk = (R?0?L)?k.
For cube decoding,we use a different set Acd defined asAcd = (R?0 ?
L)?k?
??
?Ak?G?
{0, 1}where the binary variable indicates whether thevalue contains a k-best list (0, which we call an?ordinary?
value) or a non-local feature functionin G (1, which we call a ?function?
value).
Wedenote a value u ?
Acd byu = ??
?u1, U1?, ?u2, U2?, ..., ?uk, Uk???
??
?u?, gu, us?where each ui ?
R?0 is a probability and eachUi ?
L is a proof string.We use ?k and ?k to denote the k-best proofsemiring?s operators, defined in ?2.2.
We let g0 besuch that g0(`) is undefined for all ` ?
L. For twovalues u = ?u?, gu, us?,v = ?v?, gv, vs?
?
Acd,cube decoding?s aggregation operator is:u?cd v = ?u?
?k v?, g0, 0?
if ?us ?
?vs (8)Under standard models, only ordinary values willbe operands of?cd, so?cd is undefined when us?vs.
We define the combination operator ?cd:u?cd v = (9)????????????u?
?k v?, g0, 0?
if ?us ?
?vs,?max-k(exec(gv, u?
)), g0, 0?
if ?us ?
vs,?max-k(exec(gu, v?
)), g0, 0?
if us ?
?vs,??
?, ?z.(gu(z)?
gv(z)), 1?
if us ?
vs.where exec(g, u?)
executes the function g uponeach proof in the proof list u?, modifies the scores4In our setting, gm(x, `) will most commonly be definedas ?fm(x,`)m in the notation of ?2.3.
But functions in G couldalso be used to implement, e.g., hard constraints or other non-local score factors.in place by multiplying in the function result, andreturns the modified proof list:g?
= ?`.g(x, `)exec(g, u?)
= ??u1g?
(U1), U1?, ?u2g?
(U2), U2?,..., ?ukg?
(Uk), Uk?
?Here, max-k is simply used to re-sort the k-bestproof list following function evaluation.The semiring properties fail to hold when in-troducing non-local features in this way.
In par-ticular, ?cd is not associative when 1 < k < ?.For example, consider the probabilistic CKY algo-rithm as above, but using the cube decoding semir-ing with the non-local feature functions collec-tively known as ?NGramTree?
features (Huang,2008) that score the string of terminals and nonter-minals along the path from word j to word j + 1when two constituents CY,i,j and CZ,j,k are com-bined.
The semiring value associated with sucha feature is u = ??
?,NGramTreepi(), 1?
(for aspecific path pi), and we rewrite Eq.
1 as fol-lows (where ranges for summation are omitted forspace):CX,i,k =?cd pX?Y Z ?cdCY,i,j ?cdCZ,j,k?cduThe combination operator is not associativesince the following will give different answers:5(pX?Y Z ?cd CY,i,j)?cd (CZ,j,k ?cd u) (10)((pX?Y Z ?cd CY,i,j)?cd CZ,j,k)?cd u (11)In Eq.
10, the non-local feature function is ex-ecuted on the k-best proof list for Z, while inEq.
11, NGramTreepi is called on the k-best prooflist for the X constructed from Y and Z. Further-more, neither of the above gives the desired re-sult, since we actually wish to expand the full setof k2 proofs of X and then apply NGramTreepito each of them (or a higher-dimensional ?cube?if more operands are present) before selecting thek-best.
The binary operations above retain onlythe top k proofs of X in Eq.
11 before applyingNGramTreepi to each of them.
We actually wouldlike to redefine combination so that it can operateon arbitrarily-sized sets of values.We can understand cube decoding through analgebraic structure with two operations ?
and ?,where ?
need not be associative and need not dis-tribute over?, and furthermore where?
and?
are5Distributivity of combination over aggregation fails forrelated reasons.
We omit a full discussion due to space.322defined on arbitrarily many operands.
We will re-fer here to such a structure as a generalized semir-ing.6 To define ?cd on a set of operands with N ?ordinary operands and N function operands, wefirst compute the full O(kN?)
cross-product of theordinary operands, then apply each of the N func-tions from the remaining operands in turn upon thefull N ?-dimensional ?cube,?
finally calling max-kon the result.4 Cube SummingWe present an approximate solution to the sum-ming problem when non-local features are in-volved, which we call cube summing.
It is an ex-tension of cube decoding, and so we will describeit as a generalized semiring.
The key addition is tomaintain in each value, in addition to the k-best listof proofs from Ak, a scalar corresponding to theresidual probability (possibly unnormalized) of allproofs not among the k-best.7 The k-best proofsare still used for dynamically computing non-localfeatures but the aggregation and combination op-erations are redefined to update the residual as ap-propriate.We define the set Acs for cube summing asAcs = R?0 ?
(R?0 ?
L)?k ?
G?
{0, 1}A value u ?
Acs is defined asu = ?u0, ?
?u1, U1?, ?u2, U2?, ..., ?uk, Uk???
??
?u?, gu, us?For a proof list u?, we use ?u??
to denote the sumof all proof scores,?i:?ui,Ui??u?
ui.The aggregation operator over operands{ui}Ni=1, all such that uis = 0,8 is defined by:?Ni=1 ui = (12)?
?Ni=1 ui0 +??
?Res(?Ni=1 u?i)???
,max-k(?Ni=1 u?i), g0, 0?6Algebraic structures are typically defined with binary op-erators only, so we were unable to find a suitable term for thisstructure in the literature.7Blunsom and Osborne (2008) described a related ap-proach to approximate summing using the chart computedduring cube pruning, but did not keep track of the residualterms as we do here.8We assume that operands ui to ?cs will never be suchthat uis = 1 (non-local feature functions).
This is reasonablein the widely used log-linear model setting we have adopted,where weights ?m are factors in a proof?s product score.where Res returns the ?residual?
set of scoredproofs not in the k-best among its arguments, pos-sibly the empty set.For a set ofN+N ?
operands {vi}Ni=1?
{wj}N ?j=1such that vis = 1 (non-local feature functions) andwjs = 1 (ordinary values), the combination oper-ator ?
is shown in Eq.
13 Fig.
1.
Note that thecase where N ?
= 0 is not needed in this applica-tion; an ordinary value will always be included incombination.In the special case of two ordinary operands(where us = vs = 0), Eq.
13 reduces tou?
v = (14)?u0v0 + u0 ?v?
?+ v0 ?u?
?+ ?Res(u?
?
v?)?
,max-k(u?
?
v?
), g0, 0?We define 0 as ?0, ?
?, g0, 0?
; an appropriate def-inition for the combination identity element is lessstraightforward and of little practical importance;we leave it to future work.If we use this generalized semiring to solve aDP and achieve goal value of u, the approximatesum of all proof probabilities is given by u0+?u?
?.If all features are local, the approach is exact.
Withnon-local features, the k-best list may not containthe k-best proofs, and the residual score, while in-cluding all possible proofs, may not include all ofthe non-local features in all of those proofs?
prob-abilities.5 ImplementationWe have so far viewed dynamic programmingalgorithms in terms of their declarative speci-fications as semiring-weighted logic programs.Solvers have been proposed by Goodman (1999),by Klein and Manning (2001) using a hypergraphrepresentation, and by Eisner et al (2005).
Be-cause Goodman?s and Eisner et al?s algorithms as-sume semirings, adapting them for cube summingis non-trivial.9To generalize Goodman?s algorithm, we sug-gest using the directed-graph data structure knownvariously as an arithmetic circuit or computationgraph.10 Arithmetic circuits have recently drawninterest in the graphical model community as a9The bottom-up agenda algorithm in Eisner et al (2005)might possibly be generalized so that associativity, distribu-tivity, and binary operators are not required (John Blatz, p.c.
).10This data structure is not specific to any particular set ofoperations.
We have also used it successfully with the insidesemiring.323N?i=1vi ?N ?
?j=1wj =????B?P(S)?b?Bwb0?c?S\B?w?c???
(13)+ ?Res(exec(gv1 , .
.
.
exec(gvN , w?1 ?
?
?
?
?
w?N ?)
.
.
.))?
,max-k(exec(gv1 , .
.
.
exec(gvN , w?1 ?
?
?
?
?
w?N ?)
.
.
.
)), g0, 0?Figure 1: Combination operation for cube summing, where S = {1, 2, .
.
.
, N ?}
and P(S) is the power set of S excluding ?.tool for performing probabilistic inference (Dar-wiche, 2003).
In the directed graph, there are ver-tices corresponding to axioms (these are sinks inthe graph), ?
vertices corresponding to theorems,and ?
vertices corresponding to summands in thedynamic programming equations.
Directed edgespoint from each node to the nodes it depends on;?
vertices depend on?
vertices, which depend on?
and axiom vertices.Arithmetic circuits are amenable to automaticdifferentiation in the reverse mode (Griewankand Corliss, 1991), commonly used in back-propagation algorithms.
Importantly, this permitsus to calculate the exact gradient of the approx-imate summation with respect to axiom values,following Eisner et al (2005).
This is desirablewhen carrying out the optimization problems in-volved in parameter estimation.
Another differen-tiation technique, implemented within the semir-ing, is given by Eisner (2002).Cube pruning is based on the k-best algorithmsof Huang and Chiang (2005), which save timeover generic semiring implementations throughlazy computation in both the aggregation and com-bination operations.
Their techniques are not asclearly applicable here, because our goal is to sumover all proofs instead of only finding a small sub-set of them.
If computing non-local features is acomputational bottleneck, they can be computedonly for the O(k) proofs considered when choos-ing the best k as in cube pruning.
Then, the com-putational requirements for approximate summingare nearly equivalent to cube pruning, but the ap-proximation is less accurate.6 Semirings Old and NewWe now consider interesting special cases andvariations of cube summing.6.1 The k-best+residual SemiringWhen restricted to local features, cube pruningand cube summing can be seen as proper semir-k-best proof(Goodman, 1999)k-best + residualViterbi proof(Goodman, 1999)all proof(Goodman, 1999)Viterbi(Viterbi, 1967)ignoreproofinside(Baum et al, 1970)ignore residualk = 0k = ?k =1Figure 2: Semirings generalized by k-best+residual.ings.
Cube pruning reduces to an implementationof the k-best semiring (Goodman, 1998), and cubesumming reduces to a novel semiring we call thek-best+residual semiring.
Binary instantiations of?
and ?
can be iteratively reapplied to give theequivalent formulations in Eqs.
12 and 13.
We de-fine 0 as ?0, ???
and 1 as ?1, ?1, ??.
The ?
opera-tor is easily shown to be commutative.
That ?
isassociative follows from associativity of max-k,shown by Goodman (1998).
Showing that ?
isassociative and that ?
distributes over ?
are lessstraightforward; proof sketches are provided inAppendix A.
The k-best+residual semiring gen-eralizes many semirings previously introduced inthe literature; see Fig.
2.6.2 VariationsOnce we relax requirements about associativityand distributivity and permit aggregation and com-bination operators to operate on sets, several ex-tensions to cube summing become possible.
First,when computing approximate summations withnon-local features, we may not always be inter-ested in the best proofs for each item.
Since thepurpose of summing is often to calculate statistics324under a model distribution, we may wish insteadto sample from that distribution.
We can replacethe max-k function with a sample-k function thatsamples k proofs from the scored list in its argu-ment, possibly using the scores or possibly uni-formly at random.
This breaks associativity of ?.We conjecture that this approach can be used tosimulate particle filtering for structured models.Another variation is to vary k for different theo-rems.
This might be used to simulate beam search,or to reserve computation for theorems closer togoal , which have more proofs.7 ConclusionThis paper has drawn a connection between cubepruning, a popular technique for approximatelysolving decoding problems, and the semiring-weighted logic programming view of dynamicprogramming.
We have introduced a generaliza-tion called cube summing, to be used for solv-ing summing problems, and have argued that cubepruning and cube summing are both semirings thatcan be used generically, as long as the under-lying probability models only include local fea-tures.
With non-local features, cube pruning andcube summing can be used for approximate decod-ing and summing, respectively, and although theyno longer correspond to semirings, generic algo-rithms can still be used.AcknowledgmentsWe thank three anonymous EACL reviewers, John Blatz, Pe-dro Domingos, Jason Eisner, Joshua Goodman, and membersof the ARK group for helpful comments and feedback thatimproved this paper.
This research was supported by NSFIIS-0836431 and an IBM faculty award.A k-best+residual is a SemiringIn showing that k-best+residual is a semiring, we will restrictour attention to the computation of the residuals.
The com-putation over proof lists is identical to that performed in thek-best proof semiring, which was shown to be a semiring byGoodman (1998).
We sketch the proofs that ?
is associativeand that ?
distributes over ?
; associativity of ?
is straight-forward.For a proof list a?, ?a??
denotes the sum of proof scores,Pi:?ai,Ai??a?ai.
Note that:?Res(a?
)?+ ?max-k(a?)?
= ?a??
(15)??a?
?
b???
= ?a????b???
(16)Associativity.
Given three semiring values u, v, and w, weneed to show that (u?v)?w = u?(v?w).
After expand-ing the expressions for the residuals using Eq.
14, there are10 terms on each side, five of which are identical and cancelout immediately.
Three more cancel using Eq.
15, leaving:LHS = ?Res(u?
?
v?)?
?w?
?+ ?Res(max-k(u?
?
v?)
?
w?
)?RHS = ?u??
?Res(v?
?
w?
)?+ ?Res(u?
?
max-k(v?
?
w?
))?If LHS = RHS, associativity holds.
Using Eq.
15 again, wecan rewrite the second term in LHS to obtainLHS = ?Res(u?
?
v?)?
?w?
?+ ?max-k(u?
?
v?)
?
w???
?max-k(max-k(u?
?
v?)
?
w?
)?Using Eq.
16 and pulling out the common term ?w?
?, we haveLHS =(?Res(u?
?
v?
)?+ ?max-k(u?
?
v?)?)
?w???
?max-k(max-k(u?
?
v?)
?
w?
)?= ?(u?
?
v?)
?
w??
?
?max-k(max-k(u?
?
v?)
?
w?
)?= ?(u?
?
v?)
?
w??
?
?max-k((u?
?
v?)
?
w?
)?The resulting expression is intuitive: the residual of (u?v)?w is the difference between the sum of all proof scores andthe sum of the k-best.
RHS can be transformed into this sameexpression with a similar line of reasoning (and using asso-ciativity of ?).
Therefore, LHS = RHS and ?
is associative.Distributivity.
To prove that ?
distributes over ?, we mustshow left-distributivity, i.e., thatu?
(v?w) = (u?v)?
(u?w), and right-distributivity.
We show left-distributivity here.As above, we expand the expressions, finding 8 terms on theLHS and 9 on the RHS.
Six on each side cancel, leaving:LHS = ?Res(v?
?
w?)?
?u?
?+ ?Res(u?
?
max-k(v?
?
w?
))?RHS = ?Res(u?
?
v?
)?+ ?Res(u?
?
w?
)?+ ?Res(max-k(u?
?
v?)
?max-k(u?
?
w?
))?We can rewrite LHS as:LHS = ?Res(v?
?
w?)?
?u?
?+ ?u?
?
max-k(v?
?
w?)??
?max-k(u?
?
max-k(v?
?
w?
))?= ?u??
(?Res(v?
?
w?
)?+ ?max-k(v?
?
w?)?)?
?max-k(u?
?
max-k(v?
?
w?
))?= ?u??
?v?
?
w??
?
?max-k(u?
?
(v?
?
w?
))?= ?u??
?v?
?
w??
?
?max-k((u?
?
v?)
?
(u?
?
w?
))?where the last line follows because ?
distributes over ?
(Goodman, 1998).
We now work with the RHS:RHS = ?Res(u?
?
v?
)?+ ?Res(u?
?
w?
)?+ ?Res(max-k(u?
?
v?)
?max-k(u?
?
w?
))?= ?Res(u?
?
v?
)?+ ?Res(u?
?
w?
)?+ ?max-k(u?
?
v?)
?max-k(u?
?
w?)??
?max-k(max-k(u?
?
v?)
?max-k(u?
?
w?
))?Since max-k(u?
?
v?)
and max-k(u?
?
w?)
are disjoint (weassume no duplicates; i.e., two different theorems can-not have exactly the same proof), the third term becomes?max-k(u?
?
v?
)?+ ?max-k(u?
?
w?)?
and we have= ?u?
?
v?
?+ ?u?
?
w???
?max-k(max-k(u?
?
v?)
?max-k(u?
?
w?
))?= ?u??
?v?
?+ ?u??
?w???
?max-k((u?
?
v?)
?
(u?
?
w?
))?= ?u??
?v?
?
w??
?
?max-k((u?
?
v?)
?
(u?
?
w?))?
.325ReferencesL.
E. Baum, T. Petrie, G. Soules, and N. Weiss.
1970.A maximization technique occurring in the statis-tical analysis of probabilistic functions of Markovchains.
Annals of Mathematical Statistics, 41(1).P.
Blunsom and M. Osborne.
2008.
Probabilistic infer-ence for machine translation.
In Proc.
of EMNLP.P.
Blunsom, T. Cohn, and M. Osborne.
2008.
A dis-criminative latent variable model for statistical ma-chine translation.
In Proc.
of ACL.D.
Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.W.
W. Cohen and V. Carvalho.
2005.
Stacked sequen-tial learning.
In Proc.
of IJCAI.A.
Darwiche.
2003.
A differential approach to infer-ence in Bayesian networks.
Journal of the ACM,50(3).J.
Eisner, E. Goldlust, and N. A. Smith.
2005.
Com-piling Comp Ling: Practical weighted dynamic pro-gramming and the Dyna language.
In Proc.
of HLT-EMNLP.J.
Eisner.
2002.
Parameter estimation for probabilisticfinite-state transducers.
In Proc.
of ACL.J.
R. Finkel, T. Grenager, and C. D. Manning.
2005.Incorporating non-local information into informa-tion extraction systems by gibbs sampling.
In Proc.of ACL.J.
Goodman.
1998.
Parsing inside-out.
Ph.D. thesis,Harvard University.J.
Goodman.
1999.
Semiring parsing.
ComputationalLinguistics, 25(4):573?605.A.
Griewank and G. Corliss.
1991.
Automatic Differ-entiation of Algorithms.
SIAM.L.
Huang and D. Chiang.
2005.
Better k-best parsing.In Proc.
of IWPT.L.
Huang and D. Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProc.
of ACL.L.
Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
of ACL.M.
I. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul.1999.
An introduction to variational methods forgraphical models.
Machine Learning, 37(2).D.
Klein and C. Manning.
2001.
Parsing and hyper-graphs.
In Proc.
of IWPT.T.
Koo and M. Collins.
2005.
Hidden-variable modelsfor discriminative reranking.
In Proc.
of EMNLP.K.
Kurihara and T. Sato.
2006.
Variational Bayesiangrammar induction for natural language.
In Proc.
ofICGI.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
ofICML.R.
Levy, F. Reali, and T. Griffiths.
2008.
Modeling theeffects of memory on human online sentence pro-cessing with particle filters.
In Advances in NIPS.B.
T. Lowerre.
1976.
The Harpy Speech RecognitionSystem.
Ph.D. thesis, Carnegie Mellon University.D.
J. C. MacKay.
1997.
Ensemble learning for hiddenMarkov models.
Technical report, Cavendish Labo-ratory, Cambridge.A.
F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.2008.
Stacking dependency parsers.
In Proc.
ofEMNLP.R.
McDonald and F. Pereira.
2006.
Online learningof approximate dependency parsing algorithms.
InProc.
of EACL.F.
C. N. Pereira and Y. Schabes.
1992.
Inside-outsidereestimation from partially bracketed corpora.
InProc.
of ACL, pages 128?135.D.
Roth and W. Yih.
2004.
A linear programmingformulation for global inference in natural languagetasks.
In Proc.
of CoNLL.S.
Shieber, Y. Schabes, and F. Pereira.
1995.
Principlesand implementation of deductive parsing.
Journal ofLogic Programming, 24(1-2):3?36.D.
A. Smith and J. Eisner.
2008.
Dependency parsingby belief propagation.
In Proc.
of EMNLP.N.
A. Smith, D. L. Vail, and J. D. Lafferty.
2007.
Com-putationally efficient M-estimation of log-linearstructure models.
In Proc.
of ACL.C.
Sutton and A. McCallum.
2004.
Collective seg-mentation and labeling of distant entities in infor-mation extraction.
In Proc.
of ICML Workshop onStatistical Relational Learning and Its Connectionsto Other Fields.A.
J. Viterbi.
1967.
Error bounds for convolutionalcodes and an asymptotically optimal decoding algo-rithm.
IEEE Transactions on Information Process-ing, 13(2).M.
Wang, N. A. Smith, and T. Mitamura.
2007.
Whatis the Jeopardy model?
a quasi-synchronous gram-mar for QA.
In Proc.
of EMNLP-CoNLL.326
