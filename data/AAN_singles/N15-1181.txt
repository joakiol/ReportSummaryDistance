Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1576?1586,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsA Bayesian Model for Joint Learning of Categories and their FeaturesLea Frermann and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABl.frermann@ed.ac.uk, mlap@inf.ed.ac.ukAbstractCategories such as ANIMAL or FURNITUREare acquired at an early age and play an impor-tant role in processing, organizing, and con-veying world knowledge.
Theories of cat-egorization largely agree that categories arecharacterized by features such as functionor appearance and that feature and categoryacquisition go hand-in-hand, however previ-ous work has considered these problems inisolation.
We present the first model thatjointly learns categories and their features.The set of features is shared across categories,and strength of association is inferred in aBayesian framework.
We approximate thelearning environment with natural languagetext which allows us to evaluate performanceon a large scale.
Compared to highly engi-neered pattern-based approaches, our modelis cognitively motivated, knowledge-lean, andlearns categories and features which are per-ceived by humans as more meaningful.1 IntroductionCategorization is one of the most basic cognitivefunctions.
It allows individuals to organize theirsubjective experience of their environment by struc-turing its contents.
This ability to group differentobjects into the same category based on their com-mon characteristics underlies major cognitive activ-ities such as perception, learning, and the use of lan-guage.
Global categories (such as FURNITURE orANIMAL) are shared among members of societies,and influence how we perceive, interact with, andargue about the world.Given its fundamental importance, categoriza-tion is one of the most studied problems in cog-nitive science.
The literature is rife with theoret-ical and experimental accounts, as well as model-ing simulations focusing on the emergence, repre-sentation, and learning of categories.
Most theo-ries assume that basic level concepts such as dog orchair are characterized by features such as barksor used-for-sitting, and are grouped into cate-gories based on those features.
Although the pre-cise grouping mechanism has been subject to con-siderable debate (including arguments in favor of ex-emplars (Nosofsky, 1988), prototypes (Reed, 1972),and category utility (Corter and Gluck, 1992)), it isfairly uncontroversial that categories are associatedwith featural representations.Experimental studies show that the developmentof categories and feature learning mutually influ-ence each other (Goldstone et al, 2001; Schynsand Rodet, 1997): concepts are categorized basedon their features, but the perception of features isinfluenced by already established categories, and,like categories, features evolve over time.
There isalso evidence that features such as barks or runsare grouped into types like behavior (Ahn, 1998;McRae et al, 2005; Spalding and Ross, 2000), andthe distribution of feature types varies across cat-egories.
For instance, living-things such as ANI-MALS have characteristic behavior, whereas arti-facts such as TOOLS have characteristic functions,and both categories have characteristic appearance.In this paper, we investigate the problem of jointlylearning categories and their feature types.
Previousmodeling work has largely considered these prob-lems in isolation, focusing either on category learn-ing with a fixed set of simplistic features (Ander-son, 1991; Sanborn et al, 2006) or feature learning(Austerweil and Griffiths, 2013; Baroni et al, 2010;1576Kelly et al, 2014), but not both.We present a Bayesian model which induces (se-mantic) categories and feature types from naturallanguage text.
Although language is one of manyfactors influencing category formation (others in-clude the physical world, how we perceive it, andinteract with it), large text corpora encode a surpris-ing amount of extralinguistic information (Riordanand Jones, 2011), and can thus be viewed as an ap-proximation of the learning environment.
Moreover,focusing on textual data, allows us to build catego-rization models with theoretically unlimited scope,and evaluate categories and their features on a muchlarger scale than previous work in the cognitive sci-ence literature.Our model induces categories (e.g., ANIMALS)and their feature types (e.g., behavior) from ob-servations of target concepts (e.g., lion, cow) andtheir co-occurring contexts (e.g., eats, sleeps, large).While we can directly evaluate learnt categoriesthrough comparison against behavioral data, eval-uating feature types is less straightforward.
Previ-ous work has shown that the kinds of features learn-able from text are qualitatively different from thoseproduced by humans, which makes direct com-parison difficult (Baroni et al, 2010; Kelly et al,2014).
We circumvent this problem by assessing ina crowd-sourcing experiment whether the inducedfeature types are relevant for a given category andwhether they form a coherent class.
Evaluation re-sults show that our joint model learns accurate cat-egories and feature types achieving results competi-tive with highly engineered approaches focusing ex-clusively on feature learning.2 Related WorkThe problems of category formation and featurelearning have been considered largely independentlyin the literature.
Bayesian categorization modelswere pioneered by Anderson (1991) and recently re-formalized by Sanborn et al (2006).
These mod-els are aimed at replicating human behavior in smallscale category acquisition studies, where a fixed setof simple (e.g., binary) features is assumed.
Fr-ermann and Lapata (2014) propose a model simi-lar in spirit, which they apply to large scale corpora,while investigating incremental learning in the con-text of child category acquisition (see also Fountainand Lapata (2011) for a non-Bayesian approach).Their model associates sets of features with cate-gories as a by-product of the learning process, how-ever these feature sets are independent across cate-gories and are not optimized during learning.Previous approaches on feature learning haveprimarily focused on emulating or complementingnorming studies by automatically extracting norm-like properties from textual corpora (e.g., elephanthas-trunk, scissors used-for-cutting).
A com-mon theme in this line of research is the use ofpre-defined syntactic patterns (Baroni et al, 2010),or manually created rules specifying possible con-nection paths of concepts to features in dependencytrees (Devereux et al, 2009; Kelly et al, 2014).Once extracted, the features are typically weightedin order to filter out noisy instances.
Features arelearnt for individual concepts rather than categories.Austerweil and Griffiths (2013) also focus exclu-sively on feature learning, however from sensorydata.
They develop a nonparametric Bayesian modelwhich is able to infer unlimited features, based ondistributional patterns as well as category informa-tion.To our knowledge, we propose the first Bayesianmodel that jointly learns categories and their fea-tures, arguing that the two tasks are mutually de-pendent.
Our model is knowledge-lean, it learnsfrom raw text in a single process, without rely-ing on parsing resources, manually crafted rule pat-terns, or post-processing steps.
Our work also dif-fers from approaches which combine topic mod-els with human-produced feature norms (Steyvers,2010).
Our aim is not to boost the generalizationperformance of a topic model, rather we investi-gate how both categories and features can be jointlylearnt from data.3 The BCF ModelIn this section we present our Bayesian model ofcategory and feature induction (henceforth, BCF).BCF jointly learns categories, feature types, andtheir associations.
Specifically, it infers one globalset of feature types which is shared across cate-gories (e.g., ANIMALS and VEHICLES can be de-scribed in terms of colors).
However, categories1577Generate category distribution, ??
Dir(?
)for concept type ` doGenerate category, k`?Mult(?
)for category k doGenerate feature type distribution, ?k?
Dir(?
)for feature type g doGenerate feature distribution, ?g?
Dir(?
)for stimulus d doObserve concept cdand retrieve category kcdGenerate a feature type, gd?Mult(?kcd)for feature position i doGenerate a feature fd,i?Mult(?gd)Figure 1: The generative story of the BCF model.Observations f and latent labels k and g are drawnfrom Multinomial distributions (Mult).
Parametersfor the multinomial distributions are drawn fromDirichlet distributions (Dir).differ in their strength of association with featuretypes (e.g., the feature type function will be highlyassociated with TOOLS but less so with ANIMALS).BCF jointly optimizes categories and their featuralrepresentation: the learning objective is to obtain aset of meaningful categories, each characterized byrelevant and coherent feature types.The generative story and plate diagram for theBCF model are shown in Figures 1 and 2, respec-tively.
The input to the model is a collection ofstimuli d ?
{1..D} extracted from a large text cor-pus.
Each stimulus consists of a target conceptc ?
{1..L} and its context f ?
{1..F}.
We adopt asimple representation of context as the set of wordsmaking up the sentence c occurs in (except c).
Themodel assigns concepts to categories k ?
{1..K} andfeatures to feature types g ?
{1..G}.
It learns aset of concept clusters (i.e., categories), as well asa clustering over features (i.e., feature types), anda distribution over those feature clusters for eachcategory (i.e., category-feature type associations).Specifically, the occurrences of a concept will beassigned a category, based on how similar the con-cept?s feature types are compared to the featuretypes of all other potential categories.
Simultane-ously, upon observing a stimulus (i.e., a concept incontext), the model assigns the context to a particu-lar feature type based on its probability under all po-gcfk`???kc??
?IDLKGFigure 2: The plate diagram of the BCF model.Shaded nodes indicate observed variables, and dot-ted nodes indicate hyperparameters.tential feature types, and the prior probability of ob-serving that feature type with the concept?s assignedcategory.More formally, we can describe the modelthrough the generative story given in Figure 1.
Weassume a global multinomial distribution over cate-gories Mult(?
), drawn from a symmetric Dirichletdistribution with hyperparameter ?.
For each cate-gory k, we assume an independent set of multino-mial parameters over feature types ?k, drawn from asymmetric Dirichlet distribution with hyperparam-eter ?.
For each concept type `, we draw a cat-egory k`from Mult(?).
Finally, for each featuretype g, we draw a multinomial distribution over fea-tures Mult(?g) from a symmetric Dirichlet distribu-tion with hyperparameter ?.
With these global as-signments in place, we can generate stimuli d asfollows: we first retrieve the category kcdof theobserved concept cd; we then generate a featuretype gdfrom the category?s feature type distribu-tion Mult(?kcd); and finally, for each feature posi-tion i we generate feature fd,ifrom the feature type?sdistribution Mult(?gd).
The joint probability of themodel over latent categories, latent feature types,model parameters, and data can be factorized as:P(g, f ,?,?,?,k|c,?,?,?)
= (1)P(?|?)?`P(k`|?)?kP(?k|?)?gP(?g|?
)?dP(gd|?kcd)?iP( fd,i|?gd).Since we use conjugate priors throughout, we canintegrate out the model parameters analytically, andperform inference only over the latent variables,namely the category and feature type labels associ-1578Categories (k1) bouquet scarf slipper coathat veil hair cape glove cap fur...(k2) buzzard penguin toad emu duckbird pheasant chickadee crocodile...(k3) broccoli cantaloupe caulifloweryam potato blueberry spinach...(k4) dresser apartment shack gatebasement garage curtain cabinet...Featuretypes(g1) wear cover veilwoman coat glovehair cap face head(g2) white black colorbrown dark spot redhair colour yellow(g3) bird eat animalfood rodent rabbit ratmouse mammal dog(g4) ant insect butterflywasp larva nest beetleegg caterpillar mothFigure 3: Example of categories (top) and feature types (bottom) inferred by the BCF model.
Connectinglines indicate a strong association between the category and the respective feature type.ated with the stimuli.Exact inference in the BCF model is intractable,so we turn to approximate posterior inference to dis-cover the assignments of latent variables that bestexplain our data.
We construct a Gibbs sampler (Ge-man and Geman, 1984) which iteratively re-assignssingle variables based on the current assignmentsof all other variables.
One Gibbs iteration for ourmodel consists of one sweep through the input stim-uli, resampling feature type assignments from:P(gdkcd= i|g?dkcd, f?,kcd,?,?)
(2)?
P(gdkcd= i|g?dkcd,kcd,?
)?P( fd|f?,gdkcd= i,?
),followed by one sweep through the concept types,resampling category assignments from:P(k`= j|gk`,k?,?,?)
(3)?
P(k`= j|k?,?
)?P(gk`|g?k`,k`= j,?
),where gdkcddenotes the feature type assignment tostimulus d given the category kcdof d?s observed tar-get concept cd.
k`refers to the category assignmentof concept type `, gk`refers to the feature type asso-ciations of category k`, and fdrefers to the observedfeatures in stimulus d. The superscript?indicatesthe absence of the variable assignment(s) which arecurrently resampled from the current representationof the model state.Figure 3 illustrates example output produced byour model, in terms of learnt categories, learnt fea-ture types and their associations.
Connecting linesindicate category-feature type associations.
Featuretypes are shared across categories, e.g., categoriesCLOTHING (k1), BIRDS (k2), and FOOD (k3) are allassociated with feature type color (g2).4 Experimental DesignIn this section we outline our experimental set-upfor assessing the performance of the BCF model de-scribed above.
We present our data set, briefly intro-duce the models used for comparison with our ap-proach, and explain how system output was evalu-ated.
We then report results on a series of experi-ments which evaluate the quality of the categoriesand feature types learnt by BCF.Data Our experiments used basic-level target con-cepts (e.g., cat or chair) from two norming studies(McRae et al, 2005; Vinson and Vigliocco, 2008).In these studies, humans were presented with con-cepts and asked for each concept to produce a set ofcharacteristic features.
In a subsequent study (Foun-tain and Lapata, 2010), the concepts were classi-fied into 41 categories (with possible multi-categorymembership), 34 of which we use as a goldstandardin our categorization experiments (comprising 492concepts in total).
We excluded very general cate-gories such as THING or STRUCTURE, based on theintuition that it is difficult to identify characteristicfeatures for them.
As a heuristic concepts were ex-cluded if they were close to the root of WordNet(e.g., with depth 2 or 4).To obtain the input stimuli for the BCF model,we used a subset of the Wackypedia corpus (Baroniet al, 2009), an automatically extracted and POStagged dump of the English Wikipedia.
For each tar-get concept, we identified one corresponding articlein Wackypedia.
Next, we extracted a set of stimuliwhich consists of (a) every sentence from the con-cept?s corresponding article, and (b) any sentence ina different article which mentions the concept.
Thisresulted in a data set of 63,076 stimuli which we splitinto 60% training, 20% development and 20% test.1579We removed stopwords as well as words with a partof speech other than noun, verb, and adjective.
Fur-thermore, we discarded words with an age of acqui-sition above 10 years (Kuperman et al, 2012) to re-strict the vocabulary to frequent and generally famil-iar words.Models and Parameters We compared the per-formance of BCF against BayesCat, a Bayesianmodel of category acquisition (Frermann and Lap-ata, 2014) and Strudel, a pattern-based model whichextracts concept features from text (Baroni et al,2010).BayesCat induces categories, which are repre-sented through a distribution over target concepts,and a distribution over features (i.e., individual con-text words).
In contrast to BCF, it does not learntypes of features.
In addition, while BCF induces ahard assignment of concepts to categories, BayesCatlearns soft distributions over target concepts for eachcategory.
Soft assignments can be converted intohard assignments by assigning each concept to itsmost probable category.
We ran BayesCat on thesame input stimuli as BCF, with the following pa-rameters: the number of categories was set to K =40, and the hyperparameters to ?= 0.7,?= 0.1,?=0.1.
For the BCF model, we used the same numberof categories, namely K = 40.
The number of fea-ture types was set to G = 75, and the hyperparam-eters to ?
= 0.5,?
= 0.5, and ?
= 0.1.
Parameterswere tuned on the development set.
For both mod-els, we report results averaged over 10 Gibbs runs,each time we ran the sampler for 1,000 iterations.We used annealing during learning which proved ef-fective for avoiding local optima.Strudel automatically extracts features for con-cepts from text collections following a pattern-basedapproach.
It takes as input a set of target conceptsand a set of patterns, and extracts a list of featuresfor each concept, where each concept-feature pair isweighted with a log-likelihood ratio expressing thepair?s strength of association.
Baroni et al (2010)show that the learnt representations can be used as abasis for various tasks such as typicality rating, cat-egorization, or clustering of features into types.
Inour experiments we obtained Strudel representationsfrom the same Wackypedia corpus used for extract-ing the input stimuli for BCF (and BayesCat).
Notethat Strudel, unlike the two Bayesian models, is not acognitively motivated acquisition model, but an op-timized system developed with the aim of obtainingthe best possible features from data.4.1 Experiment 1: Evaluation of CategoriesIn our first experiment we evaluate the quality ofthe categories induced by the three models presentedabove.
The models produce hard categorizations,however, the cognitive gold standard we use forevaluation (Fountain and Lapata, 2010) representssoft categories.
We obtained a hard categorizationby assigning members of multiple categories to theirmost typical category (typicality scores are providedwith the data).1Method BCF and BayesCat learn a set of cate-gories which we can directly compare to the goldstandard.
For Strudel, we produce a categorizationas follows: we represent each concept as a vectorover features (obtained from Wackypedia), whereeach component corresponds to the concept-featurelog-likelihood ratios provided by Strudel; followingBaroni et al (2010), we then cluster the vectors us-ing K-means and the Cluto toolkit.2As for the othermodels, we set the number of categories to K = 40.Metrics To assess the quality of the clusters pro-duced by the models, we measure purity (pur; theextent to which each learnt cluster corresponds toa single gold class) as well as its inverse, colloca-tion (col; the extent to which all items of a particu-lar gold class are represented in a single learnt clus-ter).
Both measures are based on set-overlap, andwe also report their harmonic mean ( f 1; Lang andLapata 2011).
In addition, we report the V-measure(v1; Rosenberg and Hirschberg 2007) and its fac-tors measuring the homogeneity of clusters (hom)and their completeness (com).
The two factors intu-itively correspond to purity and collocation, but arebased on information-theoretic measures.Results Our results are summarized in Table 1.They show that BCF and Strudel perform almostidentically, and both outperform BayesCat.
BCFlearns the categories from data, whereas for Strudel1http://homepages.inf.ed.ac.uk/s0897549/data/.2http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview1580hom com v1 pur col f 1BCF 0.68 0.64 0.66 0.59 0.52 0.55BayesCat 0.65 0.59 0.62 0.57 0.45 0.50Strudel 0.70 0.62 0.66 0.61 0.48 0.54Table 1: Model performance on the category induc-tion task.we construct the categories post-hoc after a highlyinformed feature extraction process (relying ongrammatical patterns).
It is therefore not surpris-ing that Strudel performs well, and it is encourag-ing to see that BCF does too.
Also, note that Strudeltends to learn very clean clusters at the cost of re-call, whereas the tradeoff is less extreme for BCF.Again, this is expected given Strudel?s pattern-basedapproach.
While BCF and Strudel are constrained toassign each concept to only one category, BayesCatinduces a soft categorization which is turned into ahard categorization in a post-learning step.
Whilethis setting allows for more flexibility, it also in-duces more uncertainty and results in categoriza-tions which resemble the gold standard less closelycompared to the two other models.4.2 Experiment 2: Evaluation of FeaturesWe next investigate the quality of the features ourmodel learns.
We do this by letting the model pre-dict the right concept solely from a set of features.If the model has acquired informative features, theywill be predictive of the unknown concept.
Specifi-cally, the model is presented with a set of previouslyunseen test stimuli with the target concept removed.For each stimulus, the model ranks all possible tar-get concepts based on the features f (i.e., contextwords).Method In our experiments we compared theranking performance of BCF, BayesCat, andStrudel.
For the Bayesian models, we directly ex-ploit the learnt distributions.
For BCF, we computethe score of a target concept c given a set of featuresas:Score(c|f) =?gP(g|c)P(f|g).
(4)pr@1 pr@10 pr@20 avgBCFfull 0.12 0.50 0.63 56.1?tgt 0.09 0.40 0.53 78.5BayesCatfull 0.11 0.49 0.64 37.7?tgt 0.09 0.39 0.53 52.4Strudelfull 0.07 0.33 0.47 64.4?tgt 0.07 0.35 0.49 62.2Table 2: Model performance on the concept predic-tion task.
Precision at rank 1, 10, 20, and averagerank assigned (avg).
?tgt refers to the conditionwhere we remove context words which are identi-cal to the target concept as opposed to using the fullcontext.Similarly, for BayesCat we compute the score of aconcept c given a set of features as follows:Score(c|f) =?kP(c|k)P(f|k).
(5)For Strudel, we rank concepts according to thecumulative log-likelihood ratio-based associationscore over all observed features for a particular con-cept c:Score(c|f) =?f?fassociation(c, f ).
(6)Metrics Since we can directly compare modelpredictions against the actual target concept of thestimulus, we report precision at rank 1, 10, and 20.We also report the average rank assigned to the cor-rect concept.
All results are based on a randomtest set of 2,000 previously unseen stimuli.
To con-trol for the possibility that the models are learninga strong (yet trivial) correlation between target con-cepts and identical words occurring as features, wealso report results on a modification of our test setwhere we remove any mention of the target conceptfrom the context, if present (the ?tgt condition).Results Our results on the concept prediction taskare shown in Table 2.
The Bayesian models out-perform Strudel across all metrics and conditions.Strudel?s extraction algorithm, which relies on pre-defined patterns, might be too restrictive with re-spect to the set of features it extracts and as a re-sult they are not discriminative.
BayesCat and BCF158100.10.20.30.40.50.60.70  2  4  6  8  10  12  14  16  18  20CumulativeRelativeFrequencyRankBCFBayesCatStrudelFigure 4: Number of times the correct target con-cept was placed within the top 20 ranks by BCF,BayesCat, and Strudel.perform comparably given that they learn from ex-actly the same data and exploit local co-occurrencerelations in similar ways.
BayesCat produces bet-ter average rank scores than BCF, while achievinglower precision scores.
This can be explained by thefact that BCF assigns low ranks to correct conceptsmore reliably than BayesCat.
Figure 4 shows the rel-ative cumulative frequencies of the ranks assignedby the three models.
We display the top ranks 1through 20 (out of 492).
As can be seen, BCF per-forms slightly better than BayesCat.
Pairwise differ-ences between the systems are all statistically sig-nificant (p 0.01); using a one-way ANOVA withpost-hoc Tukey HSD test).Note that performance decreases for the Bayesianmodels in the?tgt condition, i.e., when occurrencesof the target concept are removed from the context.Strudel is less affected by this given its pattern-basedlearning mechanism which is not prone to associ-ating target word types with themselves.
However,repetitions are a natural phenomenon from a cogni-tive standpoint and it seems reasonable to considermultiple occurrences of a concept as a canonical fea-ture of the learning environment.Overall, the precision scores may seem low.
How-ever, the models rank a set of 492 target con-cepts; a random baseline would achieve a pr@1 ofonly 0.002%.
In addition, the target concepts we areconsidering are by design highly confusable: theywere selected so that they form categories and arethus bound to share some features which makes thesalmon journey move hundred mile strongcurrent reproduceBCF salmon tuna goldfish lobster fishBayesCat fish radio goldfish salmon clockStrudel train house apartment ship carfinger avoid cut quick claw tip painfulBCF tent ski peg curtain hutBayesCat eye ear spider leg hairStrudel finger toe hair tail handTable 3: Model output on the concept predictiontask for salmon (top) and finger (bottom): the toppart of each table shows the true concept (left) andthe context provided to the model as input (right).The bottom part of the table shows the five mosthighly ranked concepts (left to right) for each model.prediction task harder.
Example output for all threemodels is shown in Table 3.
The models take contextfeatures ?journey move hundred mile strong?
and?avoid cut quick claw tip?
as input and are expectedto predict salmon and finger, respectively.
UnlikeStrudel, BCF and BayesCat rank salmon almost cor-rectly and the other high ranked concepts are reason-able in the given context as well.
For the second ex-ample, only Strudel predicts the correct concept cor-rectly, but again the top-ranked concepts of the othertwo models are reasonable in the given context.4.3 Experiment 3: Evaluation of Feature TypesIn this suite of experiments we evaluate two as-pects of the feature types induced by our model:(1) Are they relevant to their associated category?and (2) Do they form a coherent class?
Our evalu-ation followed the intrusion paradigm originally in-troduced to assess the output of topic models (Changet al, 2009).
We performed two intrusion studiesusing Amazon?s Mechanical Turk crowd-sourcingplatform.In the feature intrusion study, participants wereshown examples of categories and their feature typesboth of which were represented as word clusters (seeFigure 6 top).
They were asked to detect the fea-ture type which did not belong to the category.
If amodel creates relevant feature types, we would ex-pect participants to be able to identify the intruderrelatively easily.
We also conducted a word intrusion1582Strudel white change panttrouser replacepaint fasten thicklayer applyhole shirt lieneck finishpattern hood covercrimson woolenman occasion seesteal stripedBCF wear cover veilwoman coatwhite black colorbrown darkeye tooth earskin lipwear suit trouserwoman garmentanimal feather skinwool materialFigure 5: Example feature types learnt for the category CLOTHING by Strudel (top) and BCF (bottom).
?Select intruder feature type (right) wrt category (left).
?ant hornetmoth fleabeetle waspcockroach?
egg female food young bird?
ant insect butterfly wasp larva?
wear cover veil woman coat?
body air fish blood muscle?Select the intruder word.??
?
?
?
?egg female box young birdFigure 6: Illustration of the feature type intrusiontask (top); and the word intrusion task (bottom).study, where participants were shown a single fea-ture type (again represented as a word cluster) andasked to detect the intruder feature/word (see Fig-ure 6 bottom).
If the features are overall coherentand meaningful, it should be relatively straightfor-ward to identify the intruder.Method We compared the feature types learnt byBCF and Strudel.
We omitted BayesCat from thisevaluation as it does not naturally produce featuretypes, rather it associates unstructured lists of fea-tures with categories.
As mentioned earlier, Strudeldoes not induce feature types either, however, it as-sociates concepts with features which can be post-processed to obtain feature types as follows.
Given acategory induced by Strudel (as explained in Experi-ment 1), we collected the features associated with atleast half of the concepts in the category with a loglikelihood score no less than 19.51.3We then clus-tered these features with K-means (using the Clutotoolkit) into K = 5 feature types.For BCF, for each category k, we select the five3Following Baroni et al (2010), this number corresponds toa probability of co-occurrence below 0.00001, assuming inde-pendence.feature types g with highest association P(g|k), to-gether with one intruder feature type g?which ishighly associated with some other category k?butnot with k. For Strudel we took the five feature typeselicited through the procedure described above, andone random feature type from the global set of fea-ture types.
Each feature type was represented by acluster of five words.With respect to the word intrusion task, partici-pants were only shown feature types (i.e., word clus-ters) irrespectively of the associated category.
BCFfeature types g were represented as the set of thefive words w with highest probability P( f |g).
In ad-dition, we added one intruder word which had lowprobability under g but high probability under someother feature type.
For Strudel, we represented fea-ture types as a random subset of five words, andadded an additional intruder word from the globalset of features.For the feature type intrusion task, We evaluateda total of 40 categories for each model.
Each par-ticipant assessed 10 categories per session (5 permodel).
Categories and feature types were presentedin random order.
For the word intrusion task, weevaluated a total of 66 feature types for each model.Participants saw 11 feature types per session, in ran-domized order.
In both cases, we collected 10 re-sponses per item.Metrics We evaluated feature type relevance andcoherence by measuring precision (the proportionof intruders identified correctly).
We also use theKappa coefficient to measure inter-subject agree-ment (Fleiss, 1981) on our two tasks.Results Our results are presented in Table 4.
Par-ticipants identify the intruder feature type correctlymore than 50% of the time.
The performance ofStrudel is slightly better compared to BCF, bothin terms of accuracy and Kappa (however the dif-1583Feat Type Intrusion Word IntrusionPrec Kappa Prec KappaBCF 0.52 0.23 0.78 0.60Strudel 0.56 0.26 0.36 0.21Table 4: Performance of Strudel and BCF on thefeature type and word intrusion tasks.
We reportprecision (Prec) and inter-subject agreement (Fleiss?Kappa; all Kappa values are statistically significantat p 0.05).ferences are not statistically significant, using a t-test).
Again this is not surprising considering thatStrudel?s feature types were elicited through a highlyinformed, pipelined process.
The results show thatthe simpler and cognitively plausible BCF modellearns feature types of a quality comparable to ahighly engineered, competitive system.
Examplesof feature types discovered by BCF and Strudel areshown in Figure 5, for the category CLOTHING.As can be seen, Strudel obtains a large number ofaction-related features (e.g., replace, change, steal ).BCF creates more varied feature types.
For exam-ple, the second cluster refers to external properties(e.g., color), and the last cluster contains CLOTHINGmaterials.Concerning the word intrusion task, we observethat participants are able to detect the intruder moreaccurately when presented with BCF feature typesas compared to Strudel feature types (differences be-tween Strudel and BCF are statistically significantat p 0.05, again using a t-test).
The results sug-gest that the feature types learnt by BCF are morecoherent, and indeed express meaningful propertiesshared by concepts belonging to the same category.While being relevant to the category, Strudel?s fea-ture types do not seem to exhibit internal coherenceto a similar extent.
The mutual dependence of cat-egory formation and feature learning allows BCF tolearn feature types which are both relevant and indi-vidually interpretable.5 DiscussionIn this paper we presented a cognitively motivatedBayesian model which jointly learns categories andtheir features, arguing that the two tasks are co-dependent.
Our model learns from raw text with-out relying on elaborate post-processing and high-precision patterns.
Evaluation of the inferred cat-egories and their features shows that BCF per-forms competitively compared to a system specif-ically engineered to extract high quality features,despite the more complex learning objective, andthe knowledge-lean approach.
We approximate thecognitive learning environment with large text cor-pora.
However, we do not claim to learn fea-tures qualitatively similar to features produced in hu-man elicitation studies.
Instead, we show, througha crowdsourcing-based human evaluation, that thelearnt features are meaningful in that they are rele-vant to their associated category and form a coherentclass.An interesting direction for future work would beto learn feature types from multiple modalities (notonly text) and to investigate how different informa-tion sources (e.g., visual or pragmatic input) influ-ence feature learning.
The BCF model learns de-scriptive feature types represented as a collection offeature values.
In addition to such descriptive fea-tures (e.g., behavior) categories also possess defin-ing features (e.g., animate) which are bound to oneparticular value.
Extending the model in a way thatallows to learn qualitatively different types of fea-tures is desirable from a cognitive perspective.
Wewill also develop an incremental learning algorithmfor joint category and feature learning (e.g., usingsequential Monte Carlo methods such as Particle Fil-tering).
In addition, it would be interesting to inves-tigate the emergence of feature types with nonpara-metric Bayesian methods.Finally, the BCF model can be applied to tasksbeyond those discussed here.
For example, onecould learn definitions (aka features) of terms (akaconcepts) in specialist fields (e.g., finance, law,medicine) or monitor how the meaning of words orconcepts as represented by their features changesover time.Acknowledgments We thank Micha Elsner andCharles Sutton for helpful discussions, WilliamSchuler for his comments, and Carina Silbererfor providing the Strudel features.
We acknowl-edge the support of EPSRC through project grantEP/I037415/1.1584ReferencesAhn, Woo-Kyoung.
1998.
Why are different fea-tures central for natural kinds and artifacts?
: therole of causal status in determining feature cen-trality.
Cognition 69:135.Anderson, John R. 1991.
The adaptive natureof human categorization.
Psychological Review98:409?429.Austerweil, Joseph L. and Thomas L. Griffiths.2013.
A nonparametric Bayesian framework forconstructing flexible feature representations.
Psy-chological Review 120(4):817?851.Baroni, Marco, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky wide web:a collection of very large linguistically processedweb-crawled corpora.
Language Resources andEvaluation 43(3):209?226.Baroni, Marco, Brian Murphy, Eduard Barbu, andMassimo Poesio.
2010.
Strudel: A corpus-basedsemantic model based on properties and types.Cognitive Science 34(2):222?254.Chang, Jonathan, Jordan Boyd-Graber, ChongWang, Sean Gerrish, and David M. Blei.
2009.Reading tea leaves: How humans interpret topicmodels.
In Neural Information Processing Sys-tems.
pages 288?296.Corter, James E. and Mark A. Gluck.
1992.
Explain-ing basic categories - feature predictability andinformation.
Psychological Bulletin 111(2):291?303.Devereux, Barry, Nicholas Pilkington, TherryPoibeau, and Anna Korhonen.
2009.
Towards un-restricted, large-scale acquisition of feature-basedconceptual representations from corpus data.
Re-search on Language & Computation 7(2-4):137?170.Fleiss, Joseph L. 1981.
Statistical Methods for Ratesand Proportions.
Wiley, New York.Fountain, Trevor and Mirella Lapata.
2010.
Meaningrepresentation in natural language categorization.In Proceedings of the 32nd Annual Conference ofthe Cognitive Science Society.
Portland, Oregon,pages 1916?1921.Fountain, Trevor and Mirella Lapata.
2011.
In-cremental models of natural language categoryacquisition.
In Proceedings of the 33nd An-nual Conference of the Cognitive Science Society.Boston, Massachusetts, pages 255?260.Frermann, Lea and Mirella Lapata.
2014.
Incremen-tal Bayesian learning of semantic categories.
InProceedings of the 14th Conference of the Eu-ropean Chapter of the Association for Computa-tional Linguistics, EACL 2014, April 26-30, 2014,Gothenburg, Sweden.
pages 249?258.Geman, Stuart and Donald Geman.
1984.
Stochasticrelaxation, Gibbs distributions and the Bayesianrestoration of images.
IEEE Transactions on Pat-tern Analysis and Machine Intelligence 6(6):721?741.Goldstone, Robert L., Yvonne Lippa, andRichard M. Shiffrin.
2001.
Altering objectrepresentations through category learning.Cognition 78:27?43.Kelly, Colin, Barry Devereux, and Anna Korhonen.2014.
Automatic extraction of property norm-likedata from large text corpora.
Cognitive Science38(4):638?682.Kuperman, Victor, Hans Stadthagen-Gonzalez, andMarc Brysbaert.
2012.
Age-of-acquisition rat-ings for 30,000 english words.
Behavior ResearchMethods 44(4):978?990.Lang, Joel and Mirella Lapata.
2011.
Unsupervisedsemantic role induction with graph partitioning.In Proceedings of the 2011 Conference on Em-pirical Methods in Natural Language Processing.Edinburgh, Scotland, UK., pages 1320?1331.McRae, Ken, George S. Cree, Mark S. Seidenberg,and Chris McNorgan.
2005.
Semantic featureproduction norms for a large set of living andnonliving things.
Behavioral Research Methods37(4):547?59.Nosofsky, Robert M. 1988.
Exemplar-based ac-counts of relations between classification, recog-nition, and typicality.
Journal of ExperimentalPsychology: Learning, Memory, and Cognition14:700?708.Reed, Stephen K. 1972.
Pattern recognition and cat-egorization.
Cognitive psychology 3(3):382?407.Riordan, Brian and Michael N. Jones.
2011.
Re-dundancy in perceptual and linguistic experience:1585Comparing feature-based and distributional mod-els of semantic representation.
Topics in Cogni-tive Science 3(2):303?345.Rosenberg, Andrew and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based externalcluster evaluation measure.
In Proceedings of the2007 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning.
Prague, Czech Re-public, pages 410?420.Sanborn, Adam N., Thomas L. Griffiths, andDaniel J. Navarro.
2006.
A more rational modelof categorization.
In Proceedings of the 28th An-nual Conference of the Cognitive Science Society.Vancouver, Canada, pages 726?731.Schyns, Philippe G and Luc Rodet.
1997.
Catego-rization creates functional features.
Journal of Ex-perimental Psychology: Learning, Memory, andCognition 23:681?696.Spalding, T. L. and B. H. Ross.
2000.
Concept learn-ing and feature interpretation.
Memory & Cogni-tion 28:439?451.Steyvers, Mark.
2010.
Combining feature norms andtext data with topic models.
Acta Psychologica133(3):234?243.Vinson, David and Gabriella Vigliocco.
2008.
Se-mantic feature production norms for a large set ofobjects and events.
Behavior Research Methods40(1):183?190.1586
