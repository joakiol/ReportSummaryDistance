IIIIIIIIIIIIIIIIIIICentroid-based summarization of multiple documents: sentenceextraction, utility-based evaluation, and user studiesDragomir R. RadevSchool of InformationUniversity of MichiganAnn Arbor, MI 48103radev@umich.eduHongyan JingDepartment o f  Computer Sc ienceColumbia UniversityNew York, NY 10027hjing@cs.columbia.eduMalgorzata BudzikowskaIBM TJ Watson Research Center30 Saw Mill River RoadHawthorne, NY 10532sm I@us.ibm.comAbstractWe present a multi-document summarizer, calledMEAD, which generates summaries usingcluster centroids produced by a topic detectionand tracking system.
We also describe two newtechniques, based on sentence utility andsubsumption, which we have applied to theevaluation of both single and multiple documentsummaries.
Finally, we describe two user studiesthat test our models of multi-documentsummarization.1 IntroductionOn October 12, 1999, a relatively small number ofnews sources mentioned in passing that PakistaniDefense Minister Gen. Pervaiz Musharraf was awayvisiting Sri Lanka.
However, all world agencieswould be actively reporting on the major events thatwere to happen in Pakistan in the following days:Prime Minister Nawaz Sharif announced that in Gen.Musharrafs absence, the Defense Minister had been-sacked and replaced by General Zia Addin.
Largenumbers of  messages from various ources tarted toinundate the newswire: about the army's occupationof the capital, the Prime Minister's ouster and hissubsequent placement under house arrest, Gen.Musharrafs return to his country, his ascendancy topower, and the imposition of military control overPakistan.The paragraph above summarizes a large amount ofnews from different sources.
While it was notautomatically generated, one can imagine the use ofsuch automatically generated summaries.
In thispaper we will describe how multi-documentsummaries are built and evaluated.1.1 Topic detection and multi-documentsummarizationThe process of identifying all articles on an emergingevent is called Topic Detection and Tracking (TDT).A large body of research in TDT has been createdover the past two years \[Allan et al, 98\].
We willpresent an extension of our own research on TDT\[Radev et al, 1999\] to cover summarization f multi-document clusters.Our entry in the official TDT evaluation, calledCIDR ~adev et al, 1999\], uses modified TF*IDF toproduce clusters of  news articles on the same event.We developed a new technique for multi-documentsummarization (or MDS), called centroid-basedsummarization (CBS) which uses as input thecentroids of the clusters produced by CIDR toidentify which sentences are central to the topic ofthe cluster, rather than the individual articles.
Wehave implemented CBS in a system, named MEAD.The main contributions of this paper are: thedevelopment of a centroid-based multi-documentsummarizer, the use of cluster-based sentence utility(CBSU) and cross-sentence informationalsubsumption (CSIS) for evaluation of single andmulti-document summaries, two user studies thatsupport our findings, and an evaluation of MEAD.An event cluster, produced by a TDT system,consists of chronologically ordered news articlesfrom multiple sources, which describe an event as itdevelops over time.
Event clusters range from2 to 10documents from which MEAD produces ummariesin the form of sentence xtracts.A key feature of MEAD is its use of  cluster centroids,which consist of words which are central not only toone article in a cluster, but to all the articles.MEAD is significantly different from previous workon multi-document summarization \[Radev &McKeown, 1998; Carbonell and Goldstein, 1998;Mani and Bloedorn, 1999; MeKeown et aI., 1999\],21which use techniques such as graph matching,maximal marginal relevance, or language generation.Finally, evaluation of  multi-document summaries i adifficult problem.
There is not yet a widely acceptedevaluation scheme.
We propose a utility-basedevaluation scheme, which can be used to evaluateboth single-document and multi-documentsummaries.2 Informational content of sentences2.1 Cluster-based sentence utility (CBSU)Cluster-based sentence utility (CBSU, or utility)refers to the degree of relevance (from 0 to 10) of a "particular sentence to the general topic of the entirecluster (for a dis cussion of  what is a topic, see \[Allanet al 1998\]).
A utility of 0 means that the sentence isnot relevant to the cluster and a 10 inarks an essentialsentence.2.2 Cross -sentence  in fo rmat iona lsubsumption (CS IS)A related notion to CBSU is cross-sentenceinformational subsumption (CSIS, or subsumption),which reflects that certain sentences repeat some ofthe information present in other sentences and may,therefore, be omitted during summarization.
If theinformation content of  sentence a (denoted as i(a)) iscontained within sentence b, then a becomesinformationally redundant and the content of b is saidto subsume that of a:i(a) c: i(b)In the example below, (2) subsumes (1) because thecrucial information in (1) is also included in (2)which presents additional content: "the court", "last.August", and "sentenced him to life".
(1) John Doe was found guilty of the murder.
(2) The court found John Doe guilty of the murderof Jane Doe last August and sentenced him to life.The cluster shown in Figure I shows subsumptionlinks across two articles ~ about recent terroristactivities in Algeria (ALG 18853 and ALG 18854).An arrow from sentence A to sentence B indicatesthat the information content of A is subsumed by theinformation content of B. Sentences 2, 4, and 5 fromthe first article repeat he information from sentenceI The full text of these articles is shown in theAppendix.2 in the second article, while sentence' 9 from theformer article is later repeated in sentences 3 and 4 ofthe latter article.Figure 1: Subsumption links across two articles:ALG 18853 and ALG 18854.2.3 Equivalence classes of sentencesSentences subsuming each other are said to belong tothe same equivalence class.
An equivalence classmay contain more than two sentences within thesame or different articles.
In the following example,although sentences (3) and (4) are not exactparaphrases of each other, they can be substituted foreach other without crucial loss of information andtherefore belong to the same equivalence class, i.e.i(3) c i(4) and i(4) c i(3).
In the user study sectionwe will take a look at the way humans perceive CSISand equivalence class.
(3) Eighteen decapitated bodies have been foundin a mass grave in northern Algeria, press reportssaid Thursday.
(4) Algerian newspapers have reported onThursday that 18 decapitated bodies have beenfound by the authorities.2.4 Comparison with MMRMaximal marginal relevance (or MMR) is atechnique similar to CSIS and was introduced in\[Carbonell and Goldstein, 1998\].
In that paper, MMRis used to produce summaries of single documentsthat avoid redundancy.
The authors mention that theirpreliminary results indicate that multiple documentson the same topic also contain redundancy but theyfall short of using MMR for multi-documentsummarization.
Their metric is used as anenhancement to a query-based summary whereasCSIS is designed for query-independent (a.k.a.,generic) summaries.22IIII3 MEAD: a centroid-based multi-document summarizerWe now describe the corpus used for the evaluationof MEAD, and later in this section we presentMEAD's algorithm.Cluster # does # sent source news sources topicclari.world.africa.northwestem AFP, UPI Algerian terrorists threaten Belgium AB'CD2 253 45 clari.world.terrorism2 65 clari.wodd.europe.russia7 189 clari.world.europe.russiaI 0 151 TDT-3 corpus topic 783 83 TDT-3 corpus topic 67AFP, UP!AP, AFPAP, AFP, UPIAP, PRI, VOAAP, NYTThe FB1 puts Osama bin Laden onthe most wanted listExplosion in a Moscow apartmentbuilding (September 9, 1999)Explosion in a Moscow apartmentbuilding (September 13, 1999)General strike in DenmarkToxic spill in SpainTable 1: Corpm comi~osition3.1 Descr ip t ion  o f  the  corpusFor our experiments, we prepared, a small corpusconsisting of a total of 558 sentences in 27documents, organized in 6 clusters (Table 1), allextracted by CIDR.
Four of the clusters are fromUsenet newsgroups.
The remaining two clusters arefrom the official TDT corpus 2.
Among the factors forour selection of clusters are: coverage of as manynews sources as possible, coverage of both TDT andnon-TDT data, coverage of different ypes of news(e.g., terrorism, internal affairs, and environment),and diversity in cluster sizes (in our case, from 2 to10 articles).
The test corpus is used in the evaluationin such a way that each cluster is summarized at 9different compression rates, thus giving nine times asmany sample points as one would expect from thesize of the corpus.3.2 Cluster centroidsTable 2 shows a sample centroid, produced by CIDR\[Radev et al, 1999\] from cluster A.
The "count"column indicates the average number of occurrencesof a word'across the entire cluster.
The IDF valueswere computed from the TDT corpus.
A centroid, inthis context, is a pseudo-document which consists ofwords which have Count*IDF scores above a pre-defined threshold in the documents that constitute thecluster.
CIDR computes Count*IDF in an iterativefashion, updating its values as more articles areinserted in a given cluster.
We hypothesize thatsentences that contain the words from the centroidare more indicative of the topic of the cluster.2 The selection of Cluster E is due to an idea by theparticipants in the Novelty Detection Workshop, ledby James Allan.Word Count IDF  Count  * IDFbelgiumgiaalgerianhayatalgeriaislamicmeloukarabicbattalion15.50 4.96 76.867.50 8.39 62.906.00 6.36 38.153.00 8.90 26.694.50 5.63 25.326.00 4.13 24.762.00 10.00 19.993.00 5.99 17.972.50 7.16 17.91Table 2: Sample c*ntroid produced by CIDR3.3 Cent ro id -based  a lgor i thmMEAD decides which sentences to include in theextract by ranking them according to a set ofparameters.
The input to MEAD is a cluster ofarticles (e.g., extracted by CIDR) and a value for thecompression rate r. For example, if  the clustercontains a total of 50 sentences (n = 50) and thevalue of r is 20%, the output of MEAD will contain10 sentences.
Sentences are laid in the same order asthey appear in the original documents withdocuments ordered chronologically.
We benefit herefrom the time stamps associated with each document.SCORE (s) = Z i  (wcC, + + wpJwhere i (1 ~ i ~_ n) is the sentence number withinthe cluster.INPUT: Cluster of d documents 3 with n sentences(compression rate = r)3 Note that currently, MEAD requires that sentenceboundaries be marked.234.2.3 System performance (S)The system performance S is one of the numbers 6described in the previous subsection.
For { 13}, thevalue of S is 0.627 (which is lower than random).
For{14}, S is 0.833, which is between R and J.
In theexample, only two of the six possible sentenceselections, {14} and {24} are between R and J. Threeothers, {13}, {23}, and {34} are below R. while {12}is better than J.4.2.4.
Normalized system performance (1))To restrict system performance (mostly) between 0and 1, we use a mapping between R and J in such away that when S ffi R, the normalized systemperformance, D, is equal to 0 and when S = J, Dbecomes 1.
The corresponding linear function 7is:D = (S-R) / (J-R)Figure 2 shows the mapping .between systemperformance S on the left (a) and normalized systemperformance D on the fight Co).
A small part of the 0-i segment is mapped to the entire 0-1 segment;therefore the difference between two systems,performing at e.g., 0.785 and 0.812 can besignificant!I J9~.
i :0.$414S-0.8331R - f lT J2  ~05 --09(a):.-...-.:.
--.
: -.
.
.
.
.
...:......:...-.'.'.
"%%- r - l .0S" .
0.9"Ze/- DO5I t ' -  0.0Ib)Figure 2: Performance mappingExample: the normalized system performance for the{14} system then becomes (0.833 - 0.732)/(0.841 -0.732) or 0.927.
Since the score is close to I, the{14} system is almost as good as the interjudgeagreement.
The normalized system performance forthe {24} system is similarly (0.837 - 0.732) / (0.8417 The formula is valid when J > R (that is, the judgesagree among each other better than randomly).- 0.732) or 0.963.
Of the two systems, {24}outperforms { 14}.4.3 Using CSIS to evaluate multi-documentsummariesTo use CSIS in the evaluation, we introduce a newparameter, E, which tells us how much to penalize asystem that includes redundant information.
In theexample from Table 7 (arrows indicate subsumption),a summarizer with r = 20% needs to pick 2 out of 12sentences.
Suppose that it picks 1/I and 2/1 (in bold).l fE  = 1, it should get full credit of 20 utility points.
IfE = 0, it should get no credit for the second sentenceas it is subsumed by the first sentence.
By varying Ebetween 0 and 1, the evaluation may favor or ignoresubsumption.SentiSent2Sent3Sent4Article l Article2 Article310 ~10 58 9 85 6Table 7: Sample subsumption table (12 sentences,3 articles)5 User studies and system evaluationWe ran two user experiments.
First, six judges wereeach given six clusters and asked to ascribe animportance score from 0 to 10 to each sentencewithin a particular cluster.
Next, five judges had toindicate for each sentence which other sentence(s), ifany, it subsumes s.5.1 CBSU: interjudge agreementUsing the techniques described in Section 0, wecomputed the cross-judge agreement (J) for the 6clusters for various r (Figure 3).
Overall, interjudgeagreement was quite high.
An interesting drop ininterjudge agreement occurs for 20-30% summaries.The drop most likely results from the fact that 10%summaries are typically easier to produce because thefew most imporiant sentences in a cluster are easierto identify.s We should note that both annotation tasks werequite time consuming and frustrating for the userswho took anywhere from 6 to 10 hours each tocomplete their part.26IIIIIIIlIIiota.J SFigure 3: Cross-judge agreement (J) on the CBSUannotation task.5.2 CSIS:  in ter judge  agreementIn the second experiment, we asked users to indicateall cases when within a cluster, a sentence issubsumed by another.
The judges' data on the firstseven sentences of cluster A are shown in Table 8.The "+ score" indicates the number of judges whoagree on the most frequent subsumption.
The '-*score" indicates that the consensus was nosubsumption.
We found relatively low interjudgeagreement on the cases in which at least one judgeindicated evidence of subsumption.
Overall, out of558 sentences, there was full agreement (5judges) on292 sentences (Table 9).
Unfortunately, h 291 ofthese 292 sentences the agreement was that there isno subsumption.
When the bar of agreement waslowered to four judges, 23 out of 406 agreements areon sentences with subsumption.
Overall, out of 80sentences with subsumption, only 24 had anagreement of four or more judges.
However, in 54eases at least hree judges agreed on the presence of aparticular instance of subsumption.SentenceAI-IA1-2AI-3AI-4AI-5A1-6A1-7Judge1A2-5A2-10Judge2 Judge3 Judge4 Judge5 + score - scoreA2-1 A2-1 A2-1 3A2-5 A2-5 3- A2-10A2-10 A2-10 A2-10 4A2-1 - A2-2 A2-4- - A2-7- A2-8Table 8: Judges' indication for subsumption for the first seven sentences in cluster A# iudszes a~reeinfCluster .4-60 71 63 6I IClusterB Cluster C ClusterD ClusterE+ + + +0 24 0 45 0 88 I 733 6 1 10 9 37 8 354 5 4 4 28 20 5 232 1 1 0 7 0 7 0 iTable 9: lnterjudge CSIS agreementIn conclusion, we found very high interjudgeagreement in the first experiment and moderatelylow agreement in the second experiment.
Weconcede that the time necessary to do a proper jobat the second task is partly to blame.5.3 Eva luat ion  o f  MEADSince the baseline of random sentence selection isalready included in the evaluation formulae, weused the Lead-based method (selecting theCluster F+0 610 I13 71 '0positionally first (n'r/e) sentences from each clusterwhere c -- number of clusters) as the baseline toevaluate our syt;tem.In Table 10 we show the normalized performance(D) of MEAD, for the six clusters at ninecompression rates.
MEAD performed better thanLead in 29 (in bold) out of 54 cases.
Note that forthe largest cluster, Cluster D, MEAD outperformedLead at all compression rates.27Cluster ACluster BCluster CCluster DCluster ECluster F10% 20% 30% 40% 50% 60% 70% 80% 90%0.855 0.572 0.427 0,759 0.862 0.910 0.554 1.001 0.5840.365 0A02 0.690 0.714 0.867 0.640 0.845 0.713 1.3170.753 0,938 0.841 1.029 0.751 0.819 0.595 0.611 0.6830.739 0.764 0.683 ?0.723 0.614 0.568 0.668 0.719 1.1001.083 0.937 0.581 0.373 0.438 0.369 0A29 0A87 0.2611.064 0.893 0.928 1.000 0.732 0,805 0,910 0.689 0.199Table 10: Normalized performance (D) of MEADIIIIIWe then modified the MEAD algorithm to includelead information as well as centroids (see Section 0).In this case, MEAD+Lead performed better than theLead baseline in 41 cases.
We are in the process ofrunning experiments with other SCORE formulas.5.4 D iscuss ionIt may seem that utility-based evaluation requires toomuch effort and is prone to low interjudge agreement.We believe that our results show that interjudgeagreement is quite high.
As far as the amount ofeffort required, we believe that the larger effort onthe part of the judges is more or less compensatedwith the ability to evaluate summaries off-line and atvariable compression rates.
Alternative valuationsdon't make such evaluations possible.
We shouldconcede that a utility-based approach is probably notfeasible for query-based summaries as these aretypically done only on-line.We discussed the possibility of  a sentencecontributing negatively to the utility of anothersentence due to redundancy.
We should also point outthat sentences can also reinforce one anotherpositively.
For example, if  a sentence mentioning anew entity is included in a summary, one might alsowant to include a sentence that puts the entity in thecontext of the re?t of the article or cluster.6 Contributions and future workWe presented a new multi-document summarizer,MEAD.
It summarizes clusters of news articlesautomatically grouped by a topic detection system.MEAD uses information from the centroids of theclusters to select sentences that are most likely to berelevant to the cluster topic.We used a new utility-based technique, CBSU, forthe evaluation of  MEAD and of summarizers ingeneral.
We found that MEAD produces ummariesthat are similar in quality to the ones produced byhumans.
We also compared MEAD's performance toan alternative method, multi-document lead, and28showed how MEAD's sentence scoring weights canbe modified to produce summaries significantlybetter than the alternatives.We also looked at a property of multi-documentchisters, namely cross-sentence informationsubsumption (which is related to the MMR metricproposed in \[Carbonell and Goldstein, 1998\]) andshowed how it can be used in evaluating multi-document summaries.All our findings are backed by the analysis of twoexperiments hat we performed with human subjects.We found that the interjudge agreement on sentenceutility is very high while the agreement on cross-sentence subsumption is moderately low, althoughpromising.In the future, we would like to test ourmultidocument summarizer on a larger corpus andimprove the summarization algorithm.
We wouldalso like to explore how the techniques we proposedhere can be used for multiligual multidocumentsummarization.7 AcknowledgmentsWe would like to thank Inderjeet Mani, WlodekZadrozny, Rie Kubota Ando, Joyce Chai, and NandaKambhatla for their valuable feedback.
We wouldalso like to thank Carl Sable, Min-Yen Kan, DaveEvans, Adam Budzikowski, and Veronika Horvathfor their help with the evaluation.ReferencesJames Allan, Jaime Carbonell, George Doddington,Jonathan Yamron, and Yiming Yang, Topicdetection and tracking pilot study: final report, InProceedings of the Broadcast News Understandingand Transcription Workshop, 1998.Jaime Carbonell and Jade Goldstein.
The use ofMMR, diversity-based reranking for reorderingdocuments and producing summaries.
InProceedings of ACM-SIGIR'98, Melbourne,Australia, August 1998.Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, andJaime Carbonell, Summarizing Text Documents:Sentence Selection and Evaluation Metrics, InProceedings of ACM-SIGIR'99, Berkeley, CA,August 1999.Th6r6se Hand.
A Proposal for Task-Based Evaluationof Text Summarization Systems, in Mani, I., andMaybury, M., eds., Proceedings of theACL/EACL'97 Workshop on Intelligent ScalableText Summarization, Madrid, Spain, July 1997.Hongyan Jing, Regina Barzilay, Kathleen McKeown,and Michael Elhadad, Summarization EvaluationMethods: Experiments and Analysis, In WorkingNotes, AAAI Spring Symposium on IntelligentText Summarization, Stanford, CA, April 1998.Inderjeet Mani and Eric Bloedorn, SummarizingSimilarities and Differences Among RelatedDocuments, Information Retrieval 1 (l-2), pages35-67, June 1999.29Inderjeet Mani, David House, Gary Klein, LynetteHirschman, Leo Orbst, Th6r6se Firmin, MichaelChrzanowski, and Beth Sundheim The TIPSTERSUMMAC text summarization evaluation.Technical Report MTR98W0000138, MITRE,McLean, Virginia, October 1998.Inderjeet Mani and Mark Maybury.
Advances inAutomatic Text Summarization.
MIT Press, 1999.Kathleen McKeown, Judith Klavans, VasileiosHatzivassiloglou, Regina Barzilay, and EleazarEskin, Towards Multidocument Summarization byReformulation: Progress and Prospects, InProceedings of AAAI'99, Orlando, FL, July 1999.Dragomir R. Radev and Kathleen McKeown.Generating natural language summaries frommultiple on-line sources.
ComputationalLinguistics, 24 (3), pages 469-500, September1998.Dragomir R. Radev, Vasileios Hatzivassiloglou, andKathleen R. McKeown.
A description of the CIDRsystem as used for TDT-2.
In DARPA BroadcastNews Workshop, Herndon, VA, February 1999.AppendixARTICLE 18853: ALGIERS, May 20 (AFP)I. Eighteen decapitated bodies have been found !n a "'1mass grave in northern Algeria, press reports aid I ~,Thursday, adding that two shepherds were murdered \['~earlier this week.
12.
Security forces found the mass grave on Wednesdayat Chbika, near Djelfa, 275 kilometers (170 miles)south of the capital.3.
It contained the bodies of people killed last yearduring a wedding ceremony, according to Le QuotidienLiberte.4.
The victims included women, children and old men.5.
Most of them had been decapitated and their headsthrown on a road, reported the Es Sahara.6.
Another mass grave containing the bodies of around10 people was discovered recently near Algiers, in theEucalyptus district.7.
The two shepherds were killed Monday evening by agroup of nine armed lslamists near the Moulay Slissenforest.8.
After being injured in a hail of automatic weaponsfire, the pair were finished offwith machete blowsbefore being decapitated, Le Quotidien d'Oran reported.9.
Seven people, six of them children, were killed andtwo injured Wednesday by armed lslamists nearMedea, 120 kilometers (75 miles) south of Algiers,security forces said.10.
The same day a parcel bomb explosion injured 17people in Algiers itself.11.
Since early March, violence linked to armedIslamists has claimed more than 500 lives, according topress tallies.ARTICLE 18854: ALGIERS, May 20 (UPI)J !.
Algerian ewspapers have reported that 18F\] decapitated bodies have been found by authoritiesin the south of the country.2.
Police found the "decapitated bodies of women,rchildren and old men,with their heads thrown on af road" near the town of Jelfa, 275 kilometers (170miles) south of the capital Algiers.3.
In another incident on Wednesday, seven people-- including six children -- were killed by terrorists,Algerian security forces said.4.
Extremist Muslim militants were responsible forthe slaughter of the seven people in the province ofMedea, 120 kilometers (74 miles) south of Algiers.5.
The killers also kidnapped three girls during thesame attack, authorities said, and one of the girlswas found wounded on a nearby road.6.
Meanwhile, the Algerian daily Le Matin todayquoted Interior Minister Abdul Malik Silal assaying that "terrorism has not been eradicated, butthe movement of the terrorists has significantlydeclined"7.
Algerian violence has claimed the lives of morethan 70,000 people since the army cancelled the1992 general elections that Islamic parties werelikely to win.8.
Mainstream Islamic groups, most of which arebanned in the country, insist heir members are notresponsible for the violence against civilians.9.
Some Muslim groups have blamed the army,while others accuse "foreign elements conspiringagainst Algeria.
"30IIIIIIIIIIlIIIIIIII
