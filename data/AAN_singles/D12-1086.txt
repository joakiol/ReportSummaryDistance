Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 940?951, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLearning Syntactic Categories Using Paradigmatic Representations ofWord ContextMehmet Ali Yatbaz Enis Sert Deniz YuretArtificial Intelligence LaboratoryKoc?
University, I?stanbul, Turkey{myatbaz,esert,dyuret}@ku.edu.trAbstractWe investigate paradigmatic representationsof word context in the domain of unsupervisedsyntactic category acquisition.
Paradigmaticrepresentations of word context are based onpotential substitutes of a word in contrast tosyntagmatic representations based on prop-erties of neighboring words.
We comparea bigram based baseline model with severalparadigmatic models and demonstrate signif-icant gains in accuracy.
Our best model basedon Euclidean co-occurrence embedding com-bines the paradigmatic context representationwith morphological and orthographic featuresand achieves 80% many-to-one accuracy on a45-tag 1M word corpus.1 IntroductionGrammar rules apply not to individual words (e.g.dog, eat) but to syntactic categories of words (e.g.noun, verb).
Thus constructing syntactic categories(also known as lexical or part-of-speech categories)is one of the fundamental problems in language ac-quisition.Syntactic categories represent groups of wordsthat can be substituted for one another without alter-ing the grammaticality of a sentence.
Linguists iden-tify syntactic categories based on semantic, syntac-tic, and morphological properties of words.
There isalso evidence that children use prosodic and phono-logical features to bootstrap syntactic category ac-quisition (Ambridge and Lieven, 2011).
Howeverthere is as yet no satisfactory computational modelthat can match human performance.
Thus identify-ing the best set of features and best learning algo-rithms for syntactic category acquisition is still anopen problem.Relationships between linguistic units can beclassified into two types: syntagmatic (concerningpositioning), and paradigmatic (concerning substitu-tion).
Syntagmatic relations determine which unitscan combine to create larger groups and paradig-matic relations determine which units can be sub-stituted for one another.
Figure 1 illustrates theparadigmatic vs syntagmatic axes for words in asimple sentence and their possible substitutes.In this study, we represent the paradigmatic axisdirectly by building substitute vectors for each wordposition in the text.
The dimensions of a substi-tute vector represent words in the vocabulary, andthe magnitudes represent the probability of occur-rence in the given position.
Note that the substitutevector for a word position (e.g.
the second word inFig.
1) is a function of the context only (i.e.
?thecried?
), and does not depend on the word thatdoes actually appear there (i.e.
?man?).
Thus substi-Figure 1: Syntagmatic vs. paradigmatic axes for wordsin a simple sentence (Chandler, 2007).940tute vectors represent individual word contexts, notword types.
We refer to the use of features based onsubstitute vectors as paradigmatic representations ofword context.Our preliminary experiments indicated that usingcontext information alone without the identity or thefeatures of the target word (e.g.
using dimension-ality reduction and clustering on substitute vectors)has limited success and modeling the co-occurrenceof word and context types is essential for inducingsyntactic categories.
In the models presented in thispaper, we combine paradigmatic representations ofword context with features of co-occurring wordswithin the co-occurrence data embedding (CODE)framework (Globerson et al 2007; Maron et al2010).
The resulting embeddings for word types aresplit into 45 clusters using k-means and the clustersare compared to the 45 gold tags in the 1M wordPenn Treebank Wall Street Journal corpus (Mar-cus et al 1999).
We obtain many-to-one accura-cies up to .7680 using only distributional informa-tion (the identity of the word and a representation ofits context) and .8023 using morphological and or-thographic features of words improving the state-of-the-art in unsupervised part-of-speech tagging per-formance.The high probability substitutes reflect both se-mantic and syntactic properties of the context asseen in the example below (the numbers in paren-theses give substitute probabilities):?Pierre Vinken, 61 years old, will join theboard as a nonexecutive director Nov.
29.?the: its (.9011), the (.0981), a (.0006), .
.
.board: board (.4288), company (.2584),firm (.2024), bank (.0731), .
.
.Top substitutes for the word ?the?
consist ofwords that can act as determiners.
Top substitutesfor ?board?
are not only nouns, but specificallynouns compatible with the semantic context.This example illustrates two concerns inherent inall distributional methods: (i) words that are gener-ally substitutable like ?the?
and ?its?
are placed inseparate categories (DT and PRP$) by the gold stan-dard, (ii) words that are generally not substitutablelike ?do?
and ?put?
are placed in the same category(VB).
Freudenthal et al(2005) point out that cat-egories with unsubstitutable words fail the standardlinguistic definition of a syntactic category and chil-dren do not seem to make errors of substituting suchwords in utterances (e.g.
?What do you want??
vs.*?What put you want??).
Whether gold standardpart-of-speech tags or distributional categories arebetter suited to applications like parsing or machinetranslation can be best decided using extrinsic eval-uation.
However in this study we follow previouswork and evaluate our results by comparing them togold standard part-of-speech tags.Section 2 gives a detailed review of related work.Section 3 describes the dataset and the constructionof the substitute vectors.
Section 4 describes co-occurrence data embedding, the learning algorithmused in our experiments.
Section 5 describes ourexperiments and compares our results with previ-ous work.
Section 6 gives a brief error analysisand Section 7 summarizes our contributions.
All thedata and the code to replicate the results given inthis paper is available from the authors?
website athttp://goo.gl/RoqEh.2 Related WorkThere are several good reviews of algorithmsfor unsupervised part-of-speech induction(Christodoulopoulos et al 2010; Gao and Johnson,2008) and models of syntactic category acquisition(Ambridge and Lieven, 2011).This work is to be distinguished from supervisedpart-of-speech disambiguation systems, which uselabeled training data (Church, 1988), unsuperviseddisambiguation systems, which use a dictionary ofpossible tags for each word (Merialdo, 1994), orprototype driven systems which use a small setof prototypes for each class (Haghighi and Klein,2006).
The problem of induction is important forstudying under-resourced languages that lack la-beled corpora and high quality dictionaries.
It is alsoessential in modeling child language acquisition be-cause every child manages to induce syntactic cat-egories without access to labeled sentences, labeledprototypes, or dictionary constraints.Models of unsupervised part-of-speech inductionfall into two broad groups based on the informationthey utilize.
Distributional models only use word941types and their context statistics.
Word-feature mod-els incorporate additional morphological and ortho-graphic features.2.1 Distributional modelsDistributional models can be further categorized intothree subgroups based on the learning algorithm.The first subgroup represents each word type with itscontext vector and clusters these vectors accordingly(Schu?tze, 1995).
Work in modeling child syntac-tic category acquisition has generally followed thisclustering approach (Redington et al 1998; Mintz,2003).
The second subgroup consists of proba-bilistic models based on the Hidden Markov Model(HMM) framework (Brown et al 1992).
A thirdgroup of algorithms constructs a low dimensionalrepresentation of the data that represents the empir-ical co-occurrence statistics of word types (Glober-son et al 2007), which is covered in more detail inSection 4.Clustering: Clustering based methods representcontext using neighboring words, typically a sin-gle word on the left and a single word on the rightcalled a ?frame?
(e.g., the dog is; the cat is).
Theycluster word types rather than word tokens based onthe frames they occupy thus employing one-tag-per-word assumption from the beginning (with the ex-ception of some methods in (Schu?tze, 1995)).
Theymay suffer from data sparsity caused by infrequentwords and infrequent contexts.
The solutions sug-gested either restrict the set of words and set of con-texts to be clustered to the most frequently observed,or use dimensionality reduction.
Redington et al(1998) define context similarity based on the num-ber of common frames bypassing the data sparsityproblem but achieve mediocre results.
Mintz (2003)only uses the most frequent 45 frames and Biemann(2006) clusters the most frequent 10,000 words us-ing contexts formed from the most frequent 150-200words.
Schu?tze (1995) and Lamar et al(2010b)employ SVD to enhance similarity between less fre-quently observed words and contexts.
Lamar et al(2010a) represent each context by the currently as-signed left and right tag (which eliminates data spar-sity) and cluster word types using a soft k-meansstyle iterative algorithm.
They report the best clus-tering result to date of .708 many-to-one accuracyon a 45-tag 1M word corpus.HMMs: The prototypical bitag HMM model max-imizes the likelihood of the corpus w1 .
.
.
wnexpressed as P (w1|c1)?ni=2 P (wi|ci)P (ci|ci?1)where wi are the word tokens and ci are their (hid-den) tags.
One problem with such a model is its ten-dency to distribute probabilities equally and the re-sulting inability to model highly skewed word-tagdistributions observed in hand-labeled data (John-son, 2007).
To favor sparse word-tag distributionsone can enforce a strict one-tag-per-word solution(Brown et al 1992; Clark, 2003), use sparse pri-ors in a Bayesian setting (Goldwater and Griffiths,2007; Johnson, 2007), or use posterior regulariza-tion (Ganchev et al 2010).
Each of these techniquesprovide significant improvements over the standardHMM model: for example Gao and Johnson (2008)show that sparse priors can gain from 4% (.62 to .66with a 1M word corpus) in cross-validated many-to-one accuracy.
However Christodoulopoulos et al(2010) show that the older one-tag-per-word modelssuch as (Brown et al 1992) outperform the moresophisticated sparse prior and posterior regulariza-tion methods both in speed and accuracy (the Brownmodel gets .68 many-to-one accuracy with a 1Mword corpus).
Given that close to 95% of the wordoccurrences in human labeled data are tagged withtheir most frequent part of speech (Lee et al 2010),this is probably not surprising; one-tag-per-word isa fairly good first approximation for induction.2.2 Word-feature modelsOne problem with the algorithms in the previoussection is the poverty of their input features.
Of thesyntactic, semantic, and morphological informationlinguists claim underlie syntactic categories, con-text vectors or bitag HMMs only represent limitedsyntactic information in their input.
Experimentsincorporating morphological and orthographic fea-tures into HMM based models demonstrate signifi-cant improvements.
(Clark, 2003; Berg-Kirkpatrickand Klein, 2010; Blunsom and Cohn, 2011) incor-porate similar orthographic features and report im-provements of 3, 7, and 10% respectively over thebaseline Brown model.
Christodoulopoulos et al(2010) use prototype based features as described in(Haghighi and Klein, 2006) with automatically in-942duced prototypes and report an 8% improvementover the baseline Brown model.
Christodoulopou-los et al(2011) define a type-based Bayesian multi-nomial mixture model in which each word instanceis generated from the corresponding word type mix-ture component and word contexts are representedas features.
They achieve a .728 MTO score by ex-tending their model with additional morphologicaland alignment features gathered from parallel cor-pora.
To our knowledge, nobody has yet tried toincorporate phonological or prosodic features in acomputational model for syntactic category acquisi-tion.2.3 Paradigmatic representationsSahlgren (2006) gives a detailed analysis of paradig-matic and syntagmatic relations in the context ofword-space models used to represent word mean-ing.
Sahlgren?s paradigmatic model represents wordtypes using co-occurrence counts of their frequentneighbors, in contrast to his syntagmatic model thatrepresents word types using counts of contexts (doc-uments, sentences) they occur in.
Our substitutevectors do not represent word types at all, but con-texts of word tokens using probabilities of likely sub-stitutes.
Sahlgren finds that in word-spaces built byfrequent neighbor vectors, more nearest neighborsshare the same part-of-speech compared to word-spaces built by context vectors.
We find that rep-resenting the paradigmatic axis more directly usingsubstitute vectors rather than frequent neighbors im-prove part-of-speech induction.Our paradigmatic representation is also related tothe second order co-occurrences used in (Schu?tze,1995).
Schu?tze concatenates the left and right con-text vectors for the target word type with the left con-text vector of the right neighbor and the right con-text vector of the left neighbor.
The vectors from theneighbors include potential substitutes.
Our methodimproves on his foundation by using a 4-gram lan-guage model rather than bigram statistics, using thewhole 78,498 word vocabulary rather than the mostfrequent 250 words.
More importantly, rather thansimply concatenating vectors that represent the tar-get word with vectors that represent the context weuse S-CODE to model their co-occurrence statistics.2.4 EvaluationWe report many-to-one and V-measure scores forour experiments as suggested in (Christodoulopou-los et al 2010).
The many-to-one (MTO) evaluationmaps each cluster to its most frequent gold tag andreports the percentage of correctly tagged instances.The MTO score naturally gets higher with increas-ing number of clusters but it is an intuitive met-ric when comparing results with the same numberof clusters.
The V-measure (VM) (Rosenberg andHirschberg, 2007) is an information theoretic met-ric that reports the harmonic mean of homogeneity(each cluster should contain only instances of a sin-gle class) and completeness (all instances of a classshould be members of the same cluster).
In Sec-tion 6 we argue that homogeneity is perhaps moreimportant in part-of-speech induction and suggestMTO with a fixed number of clusters as a more in-tuitive metric.3 Substitute VectorsIn this study, we predict the part of speech of a wordin a given context based on its substitute vector.
Thedimensions of the substitute vector represent wordsin the vocabulary, and the entries in the substitutevector represent the probability of those words be-ing used in the given context.
Note that the substi-tute vector is a function of the context only and isindifferent to the target word.
This section detailsthe choice of the data set, the vocabulary and the es-timation of substitute vector probabilities.The Wall Street Journal Section of the Penn Tree-bank (Marcus et al 1999) was used as the test cor-pus (1,173,766 tokens, 49,206 types).
The tree-bank uses 45 part-of-speech tags which is the set weused as the gold standard for comparison in our ex-periments.
To compute substitute probabilities wetrained a language model using approximately 126million tokens of Wall Street Journal data (1987-1994) extracted from CSR-III Text (Graff et al1995) (we excluded the test corpus).
We usedSRILM (Stolcke, 2002) to build a 4-gram languagemodel with Kneser-Ney discounting.
Words thatwere observed less than 20 times in the languagemodel training data were replaced by UNK tags,which gave us a vocabulary size of 78,498.
The per-plexity of the 4-gram language model on the test cor-943pus is 96.It is best to use both left and right context whenestimating the probabilities for potential lexical sub-stitutes.
For example, in ?He lived in San Franciscosuburbs.
?, the token San would be difficult to guessfrom the left context but it is almost certain look-ing at the right context.
We define cw as the 2n ?
1word window centered around the target word posi-tion: w?n+1 .
.
.
w0 .
.
.
wn?1 (n = 4 is the n-gramorder).
The probability of a substitute word w in agiven context cw can be estimated as:P (w0 = w|cw) ?
P (w?n+1 .
.
.
w0 .
.
.
wn?1)(1)= P (w?n+1)P (w?n+2|w?n+1).
.
.
P (wn?1|wn?2?n+1) (2)?
P (w0|w?1?n+1)P (w1|w0?n+2).
.
.
P (wn?1|wn?20 ) (3)where wji represents the sequence of wordswiwi+1 .
.
.
wj .
In Equation 1, P (w|cw) is pro-portional to P (w?n+1 .
.
.
w0 .
.
.
wn+1) because thewords of the context are fixed.
Terms without w0are identical for each substitute in Equation 2 there-fore they have been dropped in Equation 3.
Finally,because of the Markov property of n-gram languagemodel, only the closest n ?
1 words are used in theexperiments.Near the sentence boundaries the appropriateterms were truncated in Equation 3.
Specifically, atthe beginning of the sentence shorter n-gram con-texts were used and at the end of the sentence termsbeyond the end-of-sentence token were dropped.For computational efficiency only the top 100substitutes and their unnormalized probabilitieswere computed for each of the 1,173,766 positionsin the test set1.
The probability vectors for each po-sition were normalized to add up to 1.0 giving us thefinal substitute vectors used in the rest of this study.1The substitutes with unnormalized log probabilities can bedownloaded from http://goo.gl/jzKH0.
For a descrip-tion of the FASTSUBS algorithm used to generate the substitutesplease see http://arxiv.org/abs/1205.5407v1.FASTSUBS accomplishes this task in about 5 hours, a naivealgorithm that looks at the whole vocabulary would take morethan 6 days on a typical 2012 workstation.4 Co-occurrence Data EmbeddingThe general strategy we follow for unsupervisedsyntactic category acquisition is to combine featuresof the context with the identity and features of thetarget word.
Our preliminary experiments indicatedthat using the context information alone (e.g.
clus-tering substitute vectors) without the target wordidentity and features had limited success.2 It is theco-occurrence of a target word with a particular typeof context that best predicts the syntactic category.In this section we review the unsupervised meth-ods we used to model co-occurrence statistics: theCo-occurrence Data Embedding (CODE) method(Globerson et al 2007) and its spherical extension(S-CODE) introduced by (Maron et al 2010).Let X and Y be two categorical variables with fi-nite cardinalities |X| and |Y |.
We observe a set ofpairs {xi, yi}ni=1 drawn IID from the joint distribu-tion of X and Y .
The basic idea behind CODE andrelated methods is to represent (embed) each valueof X and each value of Y as points in a commonlow dimensional Euclidean space Rd such that val-ues that frequently co-occur lie close to each other.There are several ways to formalize the relationshipbetween the distances and co-occurrence statistics,in this paper we use the following:p(x, y) =1Zp?(x)p?
(y)e?d2x,y (4)where d2x,y is the squared distance between the em-beddings of x and y, p?
(x) and p?
(y) are empiricalprobabilities, and Z =?x,y p?(x)p?
(y)e?d2x,y is anormalization term.
If we use the notation ?x forthe point corresponding to x and ?y for the pointcorresponding to y then d2x,y = ?
?x ?
?y?2.
Thelog-likelihood of a given embedding `(?, ?)
can be2A 10-nearest-neighbor supervised baseline using cosinedistance between substitute vectors gives .7213 accuracy.
Clus-tering substitute vectors using various distance metrics and di-mensionality reduction methods give results inferior to this up-per bound.944expressed as:`(?, ?)
=?x,yp?
(x, y) log p(x, y) (5)=?x,yp?
(x, y)(?
logZ + log p?(x)p?(y)?
d2x,y)= ?
logZ + const ??x,yp?
(x, y)d2x,yThe likelihood is not convex in ?
and ?.
We usegradient ascent to find an approximate solution fora set of ?x, ?y that maximize the likelihood.
Thegradient of the d2x,y term pulls neighbors closer inproportion to the empirical joint probability:???x?x,y?p?
(x, y)d2x,y =?y2p?
(x, y)(?y ?
?x)(6)The gradient of the Z term pushes neighbors apartin proportion to the estimated joint probability:???x(?
logZ) =?y2p(x, y)(?x ?
?y) (7)Thus the net effect is to pull pairs together if theirestimated probability is less than the empirical prob-ability and to push them apart otherwise.
The gradi-ents with respect to ?y are similar.S-CODE (Maron et al 2010) additionally re-stricts all ?x and ?y to lie on the unit sphere.
Withthis restriction, Z stays around a fixed value dur-ing gradient ascent.
This allows S-CODE to sub-stitute an approximate constant Z?
in gradient calcu-lations for the real Z for computational efficiency.In our experiments, we used S-CODE with its sam-pling based stochastic gradient ascent algorithm andsmoothly decreasing learning rate.5 ExperimentsIn this section we present experiments that evaluatesubstitute vectors as representations of word con-text within the S-CODE framework.
Section 5.1replicates the bigram based S-CODE results from(Maron et al 2010) as a baseline.
The S-CODEalgorithm works with discrete inputs.
The substi-tute vectors as described in Section 3 are high di-mensional and continuous.
We experimented withtwo approaches to use substitute vectors in a dis-crete setting.
Section 5.2 presents an algorithm thatpartitions the high dimensional space of substitutevectors into small neighborhoods and uses the par-tition id as a discrete context representation.
Sec-tion 5.3 presents an even simpler model which pairseach word with a random substitute.
When the left-word ?
right-word pairs used in the bigram modelare replaced with word ?
partition-id or word ?
sub-stitute pairs we see significant gains in accuracy.These results support our running hypothesis thatparadigmatic features, i.e.
potential substitutes ofa word, are better determiners of syntactic categorycompared to left and right neighbors.
Section 5.4explores morphologic and orthographic features asadditional sources of information and its results im-prove the state-of-the-art in the field of unsupervisedsyntactic category acquisition.Each experiment was repeated 10 times with dif-ferent random seeds and the results are reportedwith standard errors in parentheses or error bars ingraphs.
Table 1 summarizes all the results reportedin this paper and the ones we cite from the literature.5.1 Bigram modelIn (Maron et al 2010) adjacent word pairs (bi-grams) in the corpus are fed into the S-CODE algo-rithm as X,Y samples.
The algorithm uses stochas-tic gradient ascent to find the ?x, ?y embeddings forleft and right words in these bigrams on a single 25-dimensional sphere.
At the end each word w in thevocabulary ends up with two points on the sphere,a ?w point representing the behavior of w as theleft word of a bigram and a ?w point representingit as the right word.
The two vectors for w are con-catenated to create a 50-dimensional representationat the end.
These 50-dimensional vectors are clus-tered using an instance weighted k-means algorithmand the resulting groups are compared to the cor-rect part-of-speech tags.
Maron et al(2010) reportmany-to-one scores of .6880 (.0016) for 45 clustersand .7150 (.0060) for 50 clusters (on the full PTB45tag-set).
If only ?w vectors are clustered withoutconcatenation we found the performance drops sig-nificantly to about .62.To make a meaningful comparison we re-ran thebigram experiments using our default settings andobtained a many-to-one score of .7314 (.0096) andthe V-measure is .6558 (.0052) for 45 clusters.
Thefollowing default settings were used: (i) each word945Distributional Models MTO VM(Lamar et al 2010a) .708 -(Brown et al 1992)* .678 .630(Goldwater et al 2007) .632 .562(Ganchev et al 2010)* .625 .548(Maron et al 2010) .688 (.0016) -Bigrams (Sec.
5.1) .7314 (.0096) .6558 (.0052)Partitions (Sec.
5.2) .7554 (.0055) .6703 (.0037)Substitutes (Sec.
5.3) .7680 (.0038) .6822 (.0029)Models with Additional Features MTO VM(Clark, 2003)* .712 .655(Christodoulopoulos et al 2011) .728 .661(Berg-Kirkpatrick and Klein, 2010) .755 -(Christodoulopoulos et al 2010) .761 .688(Blunsom and Cohn, 2011) .775 .697Substitutes and Features (Sec.
5.4) .8023 (.0070) .7207 (.0041)Table 1: Summary of results in terms of the MTO and VM scores.
Standard errors are given in parentheses whenavailable.
Starred entries have been reported in the review paper (Christodoulopoulos et al 2010).
Distributionalmodels use only the identity of the target word and its context.
The models on the right incorporate orthographic andmorphological features.was kept with its original capitalization, (ii) thelearning rate parameters were adjusted to ?0 =50, ?0 = 0.2 for faster convergence in log likeli-hood, (iii) the number of s-code iterations were in-creased from 12 to 50 million, (iv) k-means initial-ization was improved using (Arthur and Vassilvit-skii, 2007), and (v) the number of k-means restartswere increased to 128 to improve clustering and re-duce variance.5.2 Random partitionsInstead of using left-word ?
right-word pairs as in-puts to S-CODE we wanted to pair each word with aparadigmatic representation of its context to get a di-rect comparison of the two context representations.To obtain a discrete representation of the context,the random?partitions algorithm first designates arandom subset of substitute vectors as centroids topartition the space, and then associates each contextwith the partition defined by the closest centroid incosine distance.
Each partition thus defined gets aunique id, and word (X) ?
partition-id (Y ) pairs aregiven to S-CODE as input.
The algorithm cyclesthrough the data until we get approximately 50 mil-lion updates.
The resulting ?x vectors are clusteredusing the k-means algorithm (no vector concatena-tion is necessary).
Using default settings (64K ran-dom partitions, 25 s-code dimensions, Z = 0.166)the many-to-one accuracy is .7554 (.0055) and theV-measure is .6703 (.0037).To analyze the sensitivity of this result to our spe-cific parameter settings we ran a number of experi-ments where each parameter was varied over a rangeof values.Figure 2 gives results where the number of initial0.70.710.720.730.740.750.760.770.780.790.810000  100000number of random partitionsm2oFigure 2: MTO is not sensitive to the number of partitionsused to discretize the substitute vector space within ourexperimental range.random partitions is varied over a large range andshows the results to be fairly stable across two ordersof magnitude.Figure 3 shows that at least 10 embedding dimen-sions are necessary to get within 1% of the best re-sult, but there is no significant gain from using morethan 25 dimensions.Figure 4 shows that the constant Z?
approximationcan be varied within two orders of magnitude with-out a significant performance drop in the many-to-one score.
For uniformly distributed points on a 25dimensional sphere, the expected Z ?
0.146.
In theexperiments where we tested we found the real Z al-ways to be in the 0.140-0.170 range.
When the con-stant Z?
estimate is too small the attraction in Eq.
6dominates the repulsion in Eq.
7 and all points tendto converge to the same location.
When Z?
is toohigh, it prevents meaningful clusters from coalesc-9460.710.20.210.10.110.30.310.40.410.56  60  60089numb er ofaedm dpnm8ope8onteFigure 3: MTO falls sharply for less than 10 S-CODEdimensions, but more than 25 do not help.0.70.710.720.730.740.750.760.770.780.790.80.01  0.1  1number o faadbptifs tbi2bFigure 4: MTO is fairly stable as long as the Z?
constantis within an order of magnitude of the real Z value.ing.We find the random partition algorithm to befairly robust to different parameter settings and theresulting many-to-one score significantly better thanthe bigram baseline.5.3 Random substitutesAnother way to use substitute vectors in a dis-crete setting is simply to sample individual substi-tute words from them.
The random-substitutes al-gorithm cycles through the test data and pairs eachword with a random substitute picked from the pre-computed substitute vectors (see Section 3).
We ranthe random-substitutes algorithm to generate 14 mil-lion word (X) ?
random-substitute (Y ) pairs (12substitutes for each token) as input to S-CODE.Clustering the resulting ?x vectors yields a many-to-one score of .7680 (.0038) and a V-measure of.6822 (.0029).This result is close to the previous result by therandom-partition algorithm, .7554 (.0055), demon-strating that two very different discrete represen-tations of context based on paradigmatic featuresgive consistent results.
Both results are significantlyabove the bigram baseline, .7314 (.0096).
Figure 5illustrates that the random-substitute result is fairlyrobust as long as the training algorithm can observemore than a few random substitutes per word.0.70.710.720.730.740.750.760.770.780.790.81  10  100number of random pubptitutep ser ordm2oFigure 5: MTO is not sensitive to the number of randomsubstitutes sampled per word token.5.4 Morphological and orthographic featuresClark (2003) demonstrates that using morpholog-ical and orthographic features significantly im-proves part-of-speech induction with an HMMbased model.
Section 2 describes a number other ap-proaches that show similar improvements.
This sec-tion describes one way to integrate additional fea-tures to the random-substitute model.The orthographic features we used are similar tothe ones in (Berg-Kirkpatrick et al 2010) with smallmodifications:?
Initial-Capital: this feature is generated for cap-italized words with the exception of sentenceinitial words.?
Number: this feature is generated when the to-ken starts with a digit.?
Contains-Hyphen: this feature is generated forlowercase words with an internal hyphen.947?
Initial-Apostrophe: this feature is generated fortokens that start with an apostrophe.We generated morphological features using theunsupervised algorithm Morfessor (Creutz and La-gus, 2005).
Morfessor was trained on the WSJ sec-tion of the Penn Treebank using default settings, anda perplexity threshold of 300.
The program induced5 suffix types that are present in a total of 10,484word types.
These suffixes were input to S-CODEas morphological features whenever the associatedword types were sampled.In order to incorporate morphological and ortho-graphic features into S-CODE we modified its in-put.
For each word ?
random-substitute pair gen-erated as in the previous section, we added word ?feature pairs to the input for each morphological andorthographic feature of the word.
Words on averagehave 0.25 features associated with them.
This in-creased the number of pairs input to S-CODE from14.1 million (12 substitutes per word) to 17.7 mil-lion (additional 0.25 features on average for each ofthe 14.1 million words).Using similar training settings as the previoussection, the addition of morphological and ortho-graphic features increased the many-to-one score ofthe random-substitute model to .8023 (.0070) andV-measure to .7207 (.0041).
Both these results im-prove the state-of-the-art in part-of-speech inductionsignificantly as seen in Table 1.6 Error AnalysisFigure 6 is the Hinton diagram showing the rela-tionship between the most frequent tags and clustersfrom the experiment in Section 5.4.
In general theerrors seem to be the lack of completeness (multi-ple large entries in a row), rather than lack of ho-mogeneity (multiple large entries in a column).
Thealgorithm tends to split large word classes into sev-eral clusters.
Some examples are:?
Titles like Mr., Mrs., and Dr. are split from therest of the proper nouns in cluster (39).?
Auxiliary verbs (10) and the verb ?say?
(22)have been split from the general verb clusters(12) and (7).?
Determiners ?the?
(40), ?a?
(15), and capital-ized ?The?, ?A?
(6) have been split into theirown clusters.?
Prepositions ?of?
(19), and ?by?, ?at?
(17) havebeen split from the general preposition cluster(8).Nevertheless there are some homogeneity errors aswell:?
The adjective cluster (5) also has some nounmembers probably due to the difficulty of sep-arating noun-noun compounds from adjectivemodification.?
Cluster (6) contains capitalized words that spana number of categories.Most closed-class items are cleanly separated intotheir own clusters as seen in the lower right handcorner of the diagram.
The completeness errors arenot surprising given that the words that have beensplit are not generally substitutable with the othermembers of their Penn Treebank category.
Thus itcan be argued that metrics that emphasize homo-geneity such as MTO are more appropriate in thiscontext than metrics that average homogeneity andcompleteness such as VM as long as the number ofclusters is controlled.7 ContributionsOur main contributions can be summarized as fol-lows:?
We introduced substitute vectors as paradig-matic representations of word context anddemonstrated their use in syntactic category ac-quisition.?
We demonstrated that using paradigmatic rep-resentations of word context and modeling co-occurrences of word and context types withthe S-CODE learning framework give superiorresults when compared to a baseline bigrammodel.?
We extended the S-CODE framework to in-corporate morphological and orthographic fea-tures and improved the state-of-the-art in un-supervised part-of-speech induction to 80%many-to-one accuracy.948Figure 6: Hinton diagram comparing most frequent tags and clusters.?
All our code and data, including the sub-stitute vectors for the one million wordPenn Treebank Wall Street Journal dataset,is available at the authors?
website athttp://goo.gl/RoqEh.ReferencesB.
Ambridge and E.V.M.
Lieven, 2011.
Child LanguageAcquisition: Contrasting Theoretical Approaches,chapter 6.1.
Cambridge University Press.D.
Arthur and S. Vassilvitskii.
2007. k-means++: Theadvantages of careful seeding.
In Proceedings of theeighteenth annual ACM-SIAM symposium on Discretealgorithms, pages 1027?1035.
Society for Industrialand Applied Mathematics.Taylor Berg-Kirkpatrick and Dan Klein.
2010.
Phyloge-netic grammar induction.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 1288?1297, Uppsala, Sweden, July.Association for Computational Linguistics.Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero, and Dan Klein.
2010.
Painless unsu-pervised learning with features.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 582?590, Los Angeles,California, June.
Association for Computational Lin-guistics.C.
Biemann.
2006.
Unsupervised part-of-speech taggingemploying efficient graph clustering.
In Proceedingsof the 21st International Conference on computationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics: Student ResearchWorkshop, pages 7?12.
Association for ComputationalLinguistics.Phil Blunsom and Trevor Cohn.
2011.
A hierarchi-cal pitman-yor process hmm for unsupervised part ofspeech induction.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 865?874,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Comput.Linguist., 18:467?479, December.D.
Chandler.
2007.
Semiotics: the basics.
The BasicsSeries.
Routledge.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsupervisedpos induction: how far have we come?
In Proceedingsof the 2010 Conference on Empirical Methods in Nat-ural Language Processing, EMNLP ?10, pages 575?584, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2011.
A bayesian mixture modelfor pos induction using multiple features.
In Proceed-ings of the 2011 Conference on Empirical Methods inNatural Language Processing, pages 638?647, Edin-burgh, Scotland, UK., July.
Association for Computa-tional Linguistics.949Kenneth Ward Church.
1988.
A stochastic parts pro-gram and noun phrase parser for unrestricted text.
InProceedings of the second conference on Applied nat-ural language processing, ANLC ?88, pages 136?143,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of the tenth conference on Eu-ropean chapter of the Association for ComputationalLinguistics - Volume 1, EACL ?03, pages 59?66,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Mathias Creutz and Krista Lagus.
2005.
Inducingthe morphological lexicon of a natural language fromunannotated text.
In Proceedings of AKRR?05, Inter-national and Interdisciplinary Conference on Adap-tive Knowledge Representation and Reasoning, pages106?113, Espoo, Finland, June.D.
Freudenthal, J.M.
Pine, and F. Gobet.
2005.
On theresolution of ambiguities in the extraction of syntacticcategories through chunking.
Cognitive Systems Re-search, 6(1):17?25.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
J. Mach.
Learn.
Res.,99:2001?2049, August.Jianfeng Gao and Mark Johnson.
2008.
A comparison ofbayesian estimators for unsupervised hidden markovmodel pos taggers.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?08, pages 344?352, Stroudsburg, PA,USA.
Association for Computational Linguistics.Amir Globerson, Gal Chechik, Fernando Pereira, andNaftali Tishby.
2007.
Euclidean embedding of co-occurrence data.
J. Mach.
Learn.
Res., 8:2265?2295,December.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages744?751, Prague, Czech Republic, June.
Associationfor Computational Linguistics.David Graff, Roni Rosenfeld, and Doug Paul.
1995.
Csr-iii text.
Linguistic Data Consortium, Philadelphia.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe main conference on Human Language TechnologyConference of the North American Chapter of the As-sociation of Computational Linguistics, HLT-NAACL?06, pages 320?327, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers?
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 296?305,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Michael Lamar, Yariv Maron, and Elie Bienenstock.2010a.
Latent-descriptor clustering for unsupervisedpos induction.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?10, pages 799?809, Stroudsburg, PA,USA.
Association for Computational Linguistics.Michael Lamar, Yariv Maron, Mark Johnson, and ElieBienenstock.
2010b.
Svd and clustering for unsuper-vised pos tagging.
In Proceedings of the ACL 2010Conference Short Papers, pages 215?219, Uppsala,Sweden, July.
Association for Computational Linguis-tics.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.2010.
Simple type-level unsupervised pos tagging.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 853?861, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3.Linguistic Data Consortium, Philadelphia.Yariv Maron, Michael Lamar, and Elie Bienenstock.2010.
Sphere embedding: An application to part-of-speech induction.
In J. Lafferty, C. K. I. Williams,J.
Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors,Advances in Neural Information Processing Systems23, pages 1567?1575.Bernard Merialdo.
1994.
Tagging english text with aprobabilistic model.
Comput.
Linguist., 20:155?171,June.T.H.
Mintz.
2003.
Frequent frames as a cue for gram-matical categories in child directed speech.
Cognition,90(1):91?117.M.
Redington, N. Crater, and S. Finch.
1998.
Distribu-tional information: A powerful cue for acquiring syn-tactic categories.
Cognitive Science, 22(4):425?469.A.
Rosenberg and J. Hirschberg.
2007.
V-measure: Aconditional entropy-based external cluster evaluationmeasure.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 410?420.Magnus Sahlgren.
2006.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, StockholmUniversity.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of the seventh conference950on European chapter of the Association for Compu-tational Linguistics, EACL ?95, pages 141?148, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In Proceedings International Con-ference on Spoken Language Processing, pages 257?286, November.951
