Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 426?429,Prague, June 2007. c?2007 Association for Computational LinguisticsUPC: Experiments with Joint Learning within SemEval Task 9Llu?
?s Ma`rquez, Llu?
?s Padro?, Mihai Surdeanu, Luis VillarejoTechnical University of Catalonia{lluism,padro,surdeanu,luisv}@lsi.upc.edu1 IntroductionThis paper describes UPC?s participation in theSemEval-2007 task 9 (Ma`rquez et al, 2007).1 Weaddressed all four subtasks using supervised learn-ing.
The paper introduces several novel issues:(a) for the SRL task, we propose a novel re-ranking algorithm based on the re-ranking Percep-tron of Collins and Duffy (2002); and (b) for thesame task we introduce a new set of global featuresthat extract information not only at proposition levelbut also from the complete set of frame candidates.We show that in the SemEval setting, i.e., smalltraining corpora, this approach outperforms previ-ous work.
Additionally, we added NSD and NERinformation in the global SRL model but this exper-iment was unsuccessful.2 Named Entity RecognitionFor the NER subtask we recognize first strong NEs,followed by weak NE identification.
Any single to-ken with the np0000, W, or Z PoS tag is consid-ered a strong entity and is classified using the (At-serias et al, 2006) implementation of a multi-labelAdaBoost.MH algorithm, with a configuration sim-ilar to the NE classification module of Carreras etal.
(2003).
The classifier yields predictions for fourclasses (person, location, organization, misc).
En-tities with NUM and DAT are detected separatelysolely based on POS tags.The features used by the strong NE classifiermodel a [-3,+3] context around the focus word, andinclude bag-of-words, positional lexical features,1Two of the authors of this paper, Llu?
?s Ma`rquez and LuisVillarejo, are organizers of the SemEval-2007 task 9.PoS tags, orthographic features, as well as featuresindicating whether the focus word, some of its com-ponents, or some word in the context are included inexternal gazetteers or trigger words files.The second step starts by selecting all nounphrases (np) that cover a span of more than one to-ken and include a strong NE as weak entity candi-dates.
This strategy covers more than 95% of theweak NEs.
A second AdaBoost.MH classifier isthen applied to decide the right class for the nounphrase among the possible six (person, location, or-ganization, misc, number, date) plus a NONE classindicating that the noun phrase is not a weak NE.The features used for weak NE classification are:(1) simple features ?
length in tokens, head word,lemma, and POS of the np, syntactic function of thenp (if any), minimum and maximum number of npnodes in the path from the candidate noun phrase toany of the strong NEs included in it, and number andtype of the strong NEs predicted by the first?levelclassifier that fall inside the candidate; (2) bag ofcontent words inside the candidate; and (3) pattern-based features, consisting in codifying the sequenceof lexical tokens spanned by the candidate accordingto some generalizations.
When matching, tokens aregeneralized to: the POS tag (in case of np0000,W, Z, and punctuation marks), trigger-word of classX, word-in-gazetteer of class X, and strong-NE oftype X, predicted by the first level classifier.
Therest of words are abstracted to a common form (?w?standing for a single word and ?w+?
standing for asequence of n > 1 words).
Beginning and end of thespan are also codified explicitly in the pattern?basedfeatures.
Finally, to avoid sparsity, only paths of up426to length 6 are codified as features.
Also, for eachpath, n?grams of length 2, 3 and 4 are considered.We filter out features that occur less than 10 times.3 Noun Sense DisambiguationWe have approached the NSD subtask using su-pervised learning.
In particular, we used SVMlight(Joachims, 1999), which is a freely available imple-mentation of Support Vector Machines (SVM).We trained binary SVM classifiers for every senseof words with more than 15 examples in the trainingset and a probability distribution over its senses inwhich no sense is above 90%.
The words not cov-ered by the SVM classifiers are disambiguated usingthe most frequent sense (MFS) heuristic.
The MFSwas calculated from the relative frequencies in thetraining corpus.
To the words that do not appear inthe training corpus we assigned the first WordNetsense.We used a fairly regular set of features from theWSD literature.
We included: (1) a bag of con-tent words appearing in a ?10-word window; (2) abag of content words appearing in the clause of thetarget word; (3) {1, .
.
.
, n}?grams of POS tags andlemmas in a ?n-word window (n is 3 for POS and2 for lemmas); (4) unigrams and bigrams of (POS-tag,lemma) pairs in a?2-word window; and (5) syn-tactic features, i.e., label of the syntactic constituentfrom which the target noun is the head, syntacticfunction of that constituent (if any), and the verb.Regarding the empirical setting, we filtered outfeatures occurring less than 3 times, we used linearSVMs with a 0.5 value for the C regularization pa-rameter (trade-off between training error and mar-gin), and we applied one-vs-all binarization.4 Semantic Role LabelingThe SRL approach deployed here implements a re-ranking strategy that selects the best argument framefor each predicate from the top N frames generatedby a base model.
We describe the two models next.4.1 The Local ModelThe local (i.e., base) model is an adaption of Model3 of Ma`rquez et al (2005).
This SRL approachmaps each frame argument to one syntactic con-stituent and trains one-vs-all AdaBoost (Schapireand Singer, 1999) classifiers to jointly identify andclassify constituents in the full syntactic tree of thesentence as arguments.
The model was adapted tothe languages and corpora used in the SemEval eval-uations by removing the features that were specificeither to English or PropBank (governing category,content word, and temporal cue words) and addingseveral new features: (a) syntactic function features?
the syntactic functions available in the data oftenpoint to specific argument labels (e.g., SUJ usuallyindicates an Arg0); and (b) back-off features forsyntactic labels and POS tags ?
for the features thatinclude POS tags or syntactic labels we add a back-off version of the feature where the POS tags andsyntactic labels are reduced to a small set.In addition to feature changes we modified thecandidate filtering heuristic: we select as candidatesonly syntactic constituents that are immediate de-scendents of S phrases that include the correspond-ing predicate (for both languages, over 99.6% of thecandidates match this constraint).4.2 The Global ModelWe base our re-ranking approach on a variant of there-ranking Perceptron of Collins and Duffy (2002).We modify the original algorithm in two ways tomake it more robust to the small training set avail-able: (a) instead of comparing the score of the cor-rect frame only with that of the best candidate foreach frame, we sequentially compare it with thescore of each candidate in order to acquire more in-formation, and (b) we learn not only when the pre-diction is incorrect but also when the prediction isnot confident enough.The algorithm is listed in Algorithm 1: w is thevector of model parameters, h generates the featurevector for one example, and xij denotes the jth can-didate for the ith frame in the training data.
xi1,which denotes the ?correct?
candidate for frame i, isselected to maximize the F1 score for each frame.The algorithm sequentially inspects all candidatesfor each frame and learns when the difference be-tween the scores of the correct and the current candi-date is less than a threshold ?
.
During testing we usethe average of all acquired model vectors, weightedby the number of iterations they survived in train-ing.
We tuned all system parameters through cross-validation on the training data.
For both languageswe set ?
= 10 (we do not normalize feature vectors)427Algorithm 1: Re-ranking Perceptronw = ~0for i = 1 to n dofor j = 2 to ni doif w ?
h(xij) > w ?
h(xi1)?
?
thenw?
w + h(xi1)?
h(xij)and the number of training epochs to 2.With respect to the features used, we focus onlyon global features that can be extracted indepen-dently of the local models.
We show in Section 6that this approach performs better on the smallSemEval corpora than approaches that include fea-tures from the local models.
We group the featuresinto two sets: (a) features that extract informationfrom the whole candidate set, and (b) features thatmodel the structure of each candidate frame:Features from the whole candidate set:(1) Position of the current candidate in the whole set.Frame candidates are generated using the dynamicprogramming algorithm of Toutanova et al (2005),and then sorted in descending order of the log prob-ability of the whole frame (i.e., the sum of all ar-gument log probabilities as reported by the localmodel).
Hence, smaller positions indicate candi-dates that the local model considers better.
(2) For each argument in the current frame, we storeits number of repetitions in the whole candidate set.The intuition is that an argument that appears inmany candidate frames is most likely correct.Features from each candidate frame:(3) The complete sequence of argument labels, ex-tended with the predicate lemma and voice, similarto Toutanova et al (2005).
(4) Maximal overlap with a frame from the verb lex-icon.
Both the Spanish and Catalan TreeBanks con-tain a static lexicon that lists the accepted sequencesof arguments for the most common verbs.
For eachcandidate frame, we measure the maximal overlapwith the lexicon frames for the given verb and usethe precision, recall, and F1 scores as features.
(5) Average probability (from the local model) of allarguments in the current frame.
(6) For each argument label that repeats in the cur-rent frame, we add combinations of the predicatelemma, voice, argument label, and the number oflabel repetitions as features.
The intuition is that ar-gument repetitions typically indicate an error (evenif allowed by the domain constraints).5 Semantic Class DetectionThe semantic class detection subtask has been per-formed using a naive cascade of heuristics: (1) thepredicted frame for each verb is compared with theframes present in the provided verbal lexicon, andthe class of the lexicon frame with the largest num-ber of matching arguments is chosen; (2) if there ismore than one verb with the maximum score, thefirst one in the lexicon (i.e., the most frequent) isused; (3) if the focus verb is not found in the lexicon,its most frequent class in the training corpus is used;(4) if the verb does not appear in the training data,the most frequent class overall (D2) is assigned.
Theresults obtained on the training corpus are 81.1% F1for Spanish and 86.6% for Catalan.
As a baseline,assigning the most frequent class for each verb (orD2 if not seen in training), yields F1 values of 48.1%for Spanish and 64.0% for Catalan.6 Results and DiscussionTable 1 lists the results of our system on the Se-mEval test data.
Our results are encouraging con-sidering the size of the training corpus (e.g., the En-glish PropBank is 10 times larger than the corpusused here) and the complexity of the problem (e.g.,the NER task includes both weak and strong entities;the SRL task contains 33 core arguments for Span-ish vs. 6 for English).
We analyze the behavior ofour system next.The first issue that deserves further analysis is thecontribution of our global SRL model.
We list theresults of this analysis in Table 2 as improvementsover the local SRL model.
We report results for 6corpora: the 4 test corpora and the 2 training cor-pora, where the results are generated through 5-foldcross validation.
The first block in the table showsthe contribution of our best re-ranking model.
Thesecond block shows the results of a re-ranking modelusing our best feature set but the original re-rankingPerceptron of Collins and Duffy (2002).
The thirdblock shows the performance of our re-ranking al-gorithm configured with the features proposed byToutanova et al (2005).
We draw several conclu-sions from this experiment: (a) our re-ranking model428NER NSD SRL SCP R F1 P R F1 P R F1 F1ca.CESS-ECE 79.92% 76.63% 78.24 87.47% 87.47% 87.47 82.16% 70.05% 75.62 85.71es.CESS-ECE 72.53% 68.48% 70.45 83.30% 83.30% 83.30 86.24% 75.58% 80.56 87.74ca.3LB 82.04% 79.42% 80.71 85.69% 85.53% 85.61 86.36% 85.30% 85.83 87.35es.3LB 62.03% 53.85% 57.65 88.14% 88.14% 88.14 82.23% 80.78% 81.50 76.01Table 1: Official results on the test data.
Due to space constraints, we show only the F1 score for SC.Re-ranking Collins ToutanovaP R F1 P R F1 P R F1ca.train +1.87 +1.79 +1.83 +1.56 +1.48 +1.52 -6.81 -6.67 -6.73es.train +3.16 +3.12 +3.14 +2.96 +2.93 +2.95 -6.51 -6.96 -6.75ca.CESS-ECE +0.77 +0.66 +0.71 +0.99 +0.84 +0.91 -8.11 -6.29 -7.10es.CESS-ECE +1.85 +1.94 +1.91 +1.45 +1.85 +1.68 -10.84 -8.46 -9.54ca.3LB +1.58 +1.47 +1.53 +1.48 +1.39 +1.44 -7.71 -7.57 -7.64es.3LB +2.57 +2.83 +2.71 +2.71 +2.91 +2.82 -10.53 -11.95 -11.26Table 2: Analysis of the re-ranking model for SRL.using only global information always outperformsthe local model, with F1 score improvements rang-ing from 0.71 to 3.14 points; (b) the re-ranking Per-ceptron proposed here performs better than the orig-inal algorithm, but the improvement is minimal; and(c) the feature set proposed here achieve significantbetter performance on the SemEval corpora than theset proposed by Toutanova et al, which never im-proves over the local model.
The model configuredwith the Toutanova et al feature set performs mod-estly because the features are too sparse for the smallSemEval corpora (e.g., all features from the localmodel are included, concatenated with the label ofthe corresponding argument).
On the other hand, wereplicate the behavior of the local model just withfeature (1), and furthermore, all the other 5 globalfeatures proposed have a positive contribution.In a second experiment we investigated simplestrategies for model combination.
We incorporatedNER and NSD information in the re-ranking modelfor SRL as follows: for each frame argument, weadd features that concatenate the predicate lemma,the argument label, and the NER or NSD labels forthe argument head word (we add features both withand without the predicate lemma).
We used only thebest NER/NSD labels from the local models.
To re-duce sparsity, we converted word senses to coarserclasses based on the corresponding WordNet seman-tic files.
This new model boosts the F1 score of ourbest re-ranking SRL model with an average of 0.13points on two corpora (es.3LB and ca.CESS-ECE),but it reduces the F1 of our best SRL model with anaverage of 0.17 points on the other 4 corpora.
Wecan conclude that, in the current setting, NSD andNER do not bring useful information to the SRLproblem.
However, it is soon to state that problemcombination is not useful.
To have a conclusive an-swer one will have to investigate true joint learningof the three subtasks.ReferencesJ.
Atserias, B. Casas, E. Comelles, M. Gonza`lez, L.
Padro?, andM.
Padro?.
2006.
Freeling 1.3: Syntactic and semantic ser-vices in an open-source NLP library.
In Proc.
of LREC.X.
Carreras, L. Ma`rquez, and L. Padro?.
2003.
A simple namedentity extractor using AdaBoost.
In CoNLL 2003 SharedTask Contribution.M.
Collins and N. Duffy.
2002.
New ranking algorithms forparsing and tagging: Kernels over discrete structures, andthe voted perceptron.
In Proc.
of ACL.T.
Joachims.
1999.
Making large-scale SVM learning practi-cal, Advances in Kernel Methods - Support Vector Learning.MIT Press, Cambridge, MA.L.
Ma`rquez, M. Surdeanu, P. Comas, and J. Turmo.
2005.
Arobust combination strategy for semantic role labeling.
InProc.
of EMNLP.L.
Ma`rquez, M.A.
Mart?
?, M.
Taule?, and L. Villarejo.
2007.SemEval-2007 task 09: Multilevel semantic annotation ofCatalan and Spanish.
In Proc.
of SemEval-2007, the 4thWorkshop on Semantic Evaluations.
Association for Com-putational Linguistics.R.E.
Schapire and Y.
Singer.
1999.
Improved boosting algo-rithms using confidence-rated predictions.
Machine Learn-ing, 37(3).K.
Toutanova, A. Haghighi, and C. Manning.
2005.
Joint learn-ing improves semantic role labeling.
In Proc.
of ACL.429
