Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 48?57,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsA Report on the First Native Language Identification Shared TaskJoel Tetreault?, Daniel Blanchard?
and Aoife Cahill??
Nuance Communications, Inc., 1198 E. Arques Ave, Sunnyvale, CA 94085, USAJoel.Tetreault@nuance.com?
Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA{dblanchard, acahill}@ets.orgAbstractNative Language Identification, or NLI, is thetask of automatically classifying the L1 of awriter based solely on his or her essay writ-ten in another language.
This problem areahas seen a spike in interest in recent yearsas it can have an impact on educational ap-plications tailored towards non-native speak-ers of a language, as well as authorship pro-filing.
While there has been a growing bodyof work in NLI, it has been difficult to com-pare methodologies because of the differentapproaches to pre-processing the data, differ-ent sets of languages identified, and differentsplits of the data used.
In this shared task, thefirst ever for Native Language Identification,we sought to address the above issues by pro-viding a large corpus designed specifically forNLI, in addition to providing an environmentfor systems to be directly compared.
In thispaper, we report the results of the shared task.A total of 29 teams from around the worldcompeted across three different sub-tasks.1 IntroductionOne quickly growing subfield in NLP is the taskof identifying the native language (L1) of a writerbased solely on a sample of their writing in an-other language.
The task is framed as a classifica-tion problem where the set of L1s is known a priori.Most work has focused on identifying the native lan-guage of writers learning English as a second lan-guage.
To date this topic has motivated several pa-pers and research projects.Native Language Identification (NLI) can be use-ful for a number of applications.
NLI can be used ineducational settings to provide more targeted feed-back to language learners about their errors.
Itis well known that speakers of different languagesmake different kinds of errors when learning a lan-guage (Swan and Smith, 2001).
A writing tutorsystem which can detect the native language of thelearner will be able to tailor the feedback about theerror and contrast it with common properties of thelearner?s language.
In addition, native language isoften used as a feature that goes into authorship pro-filing (Estival et al 2007), which is frequently usedin forensic linguistics.Despite the growing interest in this field, devel-opment has been encumbered by two issues.
Firstis the issue of data.
Evaluating an NLI system re-quires a corpus containing texts in a language otherthan the native language of the writer.
Because ofa scarcity of such corpora, most work has used theInternational Corpus of Learner English (ICLEv2)(Granger et al 2009) for training and evaluationsince it contains several hundred essays written bycollege-level English language learners.
However,this corpus is quite small for training and testingstatistical systems which makes it difficult to tellwhether the systems that are developed can scalewell to larger data sets or to different domains.Since the ICLE corpus was not designed with thetask of NLI in mind, the usability of the corpus forthis task is further compromised by idiosyncrasiesin the data such as topic bias (as shown by Brookeand Hirst (2011)) and the occurrence of characterswhich only appear in essays written by speakers ofcertain languages (Tetreault et al 2012).
As a result,it is hard to draw conclusions about which features48actually perform best.
The second issue is that therehas been little consistency in the field in the use ofcross-validation, the number of L1s, and which L1sare used.
As a result, comparing one approach toanother has been extremely difficult.The first Shared Task in Native Language Identifi-cation is intended to better unify this community andhelp the field progress.
The Shared Task addressesthe two deficiencies above by first using a new cor-pus (TOEF11, discussed in Section 3) that is largerthan the ICLE and designed specifically for the taskof NLI and second, by providing a common set ofL1s and evaluation standards that everyone will usefor this competition, thus facilitating direct compar-ison of approaches.
In this report we describe themethods most participants used, the data they eval-uated their systems on, the three sub-tasks involved,the results achieved by the different teams, and somesuggestions and ideas about what we can do for thenext iteration of the NLI shared task.In the following section, we provide a summaryof the prior work in Native Language Identification.Next, in Section 3 we describe the TOEFL11 cor-pus used for training, development and testing in thisshared task.
Section 4 describes the three sub-tasksof the NLI Shared Task as well as a review of thetimeline.
Section 5 lists the 29 teams that partici-pated in the shared task, and introduce abbreviationsthat will be used throughout this paper.
Sections 6and 7 describe the results of the shared task and aseparate post shared task evaluation where we askedteams to evaluate their system using cross-validationon a combination of the training and developmentdata.
In Section 8 we provide a high-level view ofthe common features and machine learning methodsteams tended to use.
Finally, we offer conclusionsand ideas for future instantiations of the shared taskin Section 9.2 Related WorkIn this section, we provide an overview of some ofthe common approaches used for NLI prior to thisshared task.
While a comprehensive review is out-side the scope of this paper, we have compiled abibliography of related work in the field.
It can bedownloaded from the NLI Shared Task website.1To date, nearly all approaches have treated thetask of NLI as a supervised classification problemwhere statistical models are trained on data from thedifferent L1s.
The work of Koppel et al(2005) wasthe first in the field and they explored a multitudeof features, many of which are employed in severalof the systems in the shared tasks.
These featuresincluded character and POS n-grams, content andfunction words, as well as spelling and grammati-cal errors (since language learners have tendenciesto make certain errors based on their L1 (Swan andSmith, 2001)).
An SVM model was trained on thesefeatures extracted from a subsection of the ICLEcorpus consisting of 5 L1s.N-gram features (word, character and POS) havefigured prominently in prior work.
Not only are theyeasy to compute, but they can be quite predictive.However, there are many variations on the features.Past reseach efforts have explored different n-gramwindows (though most tend to focus on unigramsand bigrams), different thresholds for how many n-grams to include as well as whether to encode thefeature as binary (presence or absence of the partic-ular n-gram) or as a normalized count.The inclusion of syntactic features has been a fo-cus in recent work.
Wong and Dras (2011) exploredthe use of production rules from two parsers andSwanson and Charniak (2012) explored the use ofTree Substitution Grammars (TSGs).
Tetreault etal.
(2012) also investigated the use of TSGs as wellas dependency features extracted from the Stanfordparser.Other approaches to NLI have included the use ofLatent Dirichlet Analysis to cluster features (Wonget al 2011), adaptor grammars (Wong et al 2012),and language models (Tetreault et al 2012).
Ad-ditionally, there has been research into the effects oftraining and testing on different corpora (Brooke andHirst, 2011).Much of the aforementioned work takes the per-spective of optimizing for the task of Native Lan-guage Identification, that is, what is the best way ofmodeling the problem to get the highest system ac-curacy?
The problem of Native Language Identifica-1http://nlisharedtask2013.org/bibliography-of-related-work-in-nli49tion is also of interest to researchers in Second Lan-guage Acquisition where they seek to explain syn-tactic transfer in learner language (Jarvis and Cross-ley, 2012).3 DataThe dataset for the task was the new TOEFL11corpus (Blanchard et al 2013).
TOEFL11 con-sists of essays written during a high-stakes college-entrance test, the Test of English as a Foreign Lan-guage (TOEFL R?).
The corpus contains 1,100 es-says per language sampled as evenly as possiblefrom 8 prompts (i.e., topics) along with score lev-els (low/medium/high) for each essay.
The 11 na-tive languages covered by our corpus are: Ara-bic (ARA), Chinese (CHI), French (FRE), German(GER), Hindi (HIN), Italian (ITA), Japanese (JAP),Korean (KOR), Spanish (SPA), Telugu (TEL), andTurkish (TUR).The TOEFL11 corpus was designed specificallyto support the task of native language identifica-tion.
Because all of the essays were collectedthrough ETS?s operational test delivery system forthe TOEFL R?
test, the encoding and storage of alltexts in the corpus is consistent.
Furthermore, thesampling of essays was designed to ensure approx-imately equal representation of native languagesacross topics, insofar as this was possible.For the shared task, the corpus was split intothree sets: training (TOEFL11-TRAIN), development(TOEFL11-DEV), and test (TOEFL11-TEST).
Thetrain corpus consisted of 900 essays per L1, the de-velopment set consisted of 100 essays per L1, andthe test set consisted of another 100 essays per L1.Although the overall TOEFL11 corpus was sampledas evenly as possible with regard to language andprompts, the distribution for each language is not ex-actly the same in the training, development and testsets (see Tables 1a, 1b, and 1c).
In fact, the distri-bution is much closer between the training and testsets, as there are several languages for which thereare no essays for a given prompt in the developmentset, whereas there are none in the training set, andonly one, Italian, for the test set.It should be noted that in the first instantiation ofthe corpus, presented in Tetreault et al(2012), weused TOEFL11 to denote the body of data consistingof TOEFL11-TRAIN and TOEFL11-DEV.
However,in this shared task, we added 1,100 sentences for atest set and thus use the term TOEFL11 to now de-note the corpus consisting of the TRAIN, DEV andTEST sets.
We expect the corpus to be releasedthrough the the Linguistic Data Consortium in 2013.4 NLI Shared Task DescriptionThe shared task consisted of three sub-tasks.
Foreach task, the test set was TOEFL11-TEST and onlythe type of training data varied from task to task.?
Closed-Training: The first and main taskwas the 11-way classification task using onlythe TOEFL11-TRAIN and optionally TOEFL11-DEV for training.?
Open-Training-1: The second task allowedthe use of any amount or type of training data(as is done by Brooke and Hirst (2011)) exclud-ing any data from the TOEFL11, but still evalu-ated on TOEFL11-TEST.?
Open-Training-2: The third task allowed theuse of TOEFL11-TRAIN and TOEFL11-DEVcombined with any other additional data.
Thismost closely reflects a real-world scenario.Additionally, each team could submit up to 5 dif-ferent systems per task.
This allowed a team to ex-periment with different variations of their core sys-tem.The training data was released on January 14,with the development data and evaluation script re-leased almost one month later on February 12.
Thetrain and dev data contained an index file with the L1for each essay in those sets.
The previously unseenand unlabeled test data was released on March 11and teams had 8 days to submit their system predic-tions.
The predictions for each system were encodedin a CSV file, where each line contained the file IDof a file in TOEFL11-TEST and the correspondingL1 prediction made by the system.
Each CSV filewas emailed to the NLI organizers and then evalu-ated against the gold standard.5 TeamsIn total, 29 teams competed in the shared task com-petition, with 24 teams electing to write papers de-scribing their system(s).
The list of participating50Lang.
P1 P2 P3 P4 P5 P6 P7 P8ARA 113 113 113 112 112 113 112 112CHI 113 113 113 112 112 113 112 112FRE 128 128 76 127 127 60 127 127GER 125 125 125 125 125 26 125 124HIN 132 132 132 71 132 38 132 131ITA 142 70 122 141 141 12 141 131JAP 108 114 113 113 113 113 113 113KOR 113 113 113 112 112 113 112 112SPA 124 120 38 124 123 124 124 123TEL 139 139 139 41 139 26 139 138TUR 132 132 72 132 132 37 132 131Total 1369 1299 1156 1210 1368 775 1369 1354(a) Training SetLang.
P1 P2 P3 P4 P5 P6 P7 P8ARA 12 13 13 13 14 7 14 14CHI 14 14 0 15 15 14 13 15FRE 17 18 0 14 19 0 13 19GER 15 15 16 10 13 0 15 16HIN 16 17 17 0 17 0 16 17ITA 18 0 0 30 31 0 21 0JAP 0 14 15 14 15 14 14 14KOR 15 8 15 2 13 15 16 16SPA 7 0 0 21 7 21 21 23TEL 16 17 17 0 17 0 16 17TUR 22 4 0 22 7 0 22 23Total 152 120 93 141 168 71 181 174(b) Dev SetLang.
P1 P2 P3 P4 P5 P6 P7 P8ARA 13 11 12 14 10 13 12 15CHI 13 14 13 13 7 14 14 12FRE 13 14 11 15 14 8 11 14GER 15 14 16 16 12 2 12 13HIN 13 13 14 15 7 15 10 13ITA 13 19 16 16 15 0 11 10JAP 8 14 12 11 10 15 14 16KOR 12 12 8 14 12 14 13 15SPA 10 13 16 14 4 12 15 16TEL 10 10 11 14 13 15 11 16TUR 15 9 18 16 8 6 13 15Total 135 143 147 158 112 114 136 155(c) Test SetTable 1: Number of essays per language per prompt in each data setteams, along with their abbreviations, can be foundin Table 2.6 Shared Task ResultsThis section summarizes the results of the sharedtask.
For each sub-task, we have tables listing the51Team Name AbbreviationBobicev BOBChonger CHOCMU-Haifa HAICologne-Nijmegen CNCoRAL Lab @ UAB CORCUNI (Charles University) CUNcywu CYWdartmouth DAReurac EURHAUTCS HAUItaliaNLP ITAJarvis JARkyle, crossley, dai, mcnamara KYLLIMSI LIMLTRC IIIT Hyderabad HYDMichigan MICMITRE ?Carnie?
CARMQ MQNAIST NAINRC NRCOslo NLI OSLToronto TORTuebingen TUEUalberta UABUKP UKPUnibuc BUCUNT UNTUTD UTDVTEX VTXTable 2: Participating Teams and Team Abbrevia-tionstop submission for each team and its performanceby overall accuracy and by L1.2Table 3 shows results for the Closed sub-taskwhere teams developed systems that were trainedsolely on TOEFL11-TRAIN and TOEFL11-DEV.
Thiswas the most popular sub-task with 29 teams com-peting and 116 submissions in total for the sub-task.Most teams opted to submit 4 or 5 runs.The Open sub-tasks had far fewer submissions.Table 4 shows results for the Open-1 sub-task whereteams could train systems using any training data ex-cluding TOEFL11-TRAIN and TOEFL11-DEV.
Threeteams competed in this sub-task for a total of 13 sub-2For those interested in the results of all submissions, pleasecontact the authors.missions.
Table 5 shows the results for the third sub-task ?Open-2?.
Four teams competed in this task fora total of 15 submissions.The challenge for those competing in the Opentasks was finding enough non-TOEFL11 data foreach L1 to train a classifier.
External corpora com-monly used in the competition included the:?
ICLE: which covered all L1s except for Ara-bic, Hindi and Telugu;?
FCE: First Certificate in English Corpus(Yannakoudakis et al 2011): a collection ofessay written for an English assessment exam,which covered all L1s except for Arabic, Hindiand Telugu?
ICNALE: International Corpus Network ofAsian Learners of English (Ishikawa, 2011):a collection of essays written by Chinese,Japanese and Korean learners of English alongwith 7 other L1s with Asian backgrounds.?
Lang8: http://www.lang8.com: a social net-working service where users write in the lan-guage they are learning, and get correctionsfrom users who are native speakers of that lan-guage.
Shared Task participants such as NAIand TOR scraped the website for all writngsamples from English language learners.
Allof the L1s in the shared task are represented onthe site, though the Asian L1s dominate.The most challenging L1s to find data for seemedto be Hindi and Telugu.
TUE used essays writtenby Pakastani students in the ICNALE corpus to sub-stitute for Hindi.
For Telugu, they scraped mate-rial from bilingual blogs (English-Telugu) as wellas other material for the web.
TOR created cor-pora for Telugu and Hindi by scraping news articles,tweets which were geolocated in the Hindi and Tel-ugu speaking areas, and translations of Hindi andTelugu blogs using Google Translate.We caution directly comparing the results of theClosed sub-task to the Open ones.
In the Open-1sub-task most teams had smaller training sets thanused in the Closed competition which automaticallyputs them at a disadvantage, and in some cases there52L1 F-ScoreTeamNameRun OverallAcc.ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TURJAR 2 0.836 0.785 0.856 0.860 0.893 0.775 0.905 0.854 0.813 0.798 0.802 0.854OSL 2 0.834 0.816 0.850 0.874 0.912 0.792 0.873 0.828 0.806 0.783 0.792 0.840BUC 5 0.827 0.840 0.866 0.853 0.931 0.736 0.873 0.851 0.812 0.779 0.760 0.796CAR 2 0.826 0.859 0.847 0.810 0.921 0.762 0.877 0.825 0.827 0.768 0.802 0.790TUE 1 0.822 0.810 0.853 0.806 0.897 0.768 0.883 0.842 0.776 0.772 0.824 0.812NRC 4 0.818 0.804 0.845 0.848 0.916 0.745 0.903 0.818 0.790 0.788 0.755 0.790HAI 1 0.815 0.804 0.842 0.835 0.903 0.759 0.845 0.825 0.806 0.776 0.789 0.784CN 2 0.814 0.778 0.845 0.848 0.882 0.744 0.857 0.812 0.779 0.787 0.784 0.827NAI 1 0.811 0.814 0.829 0.828 0.876 0.755 0.864 0.806 0.789 0.757 0.793 0.802UTD 2 0.809 0.778 0.846 0.832 0.892 0.731 0.866 0.846 0.819 0.715 0.784 0.784UAB 3 0.803 0.820 0.804 0.822 0.905 0.724 0.850 0.811 0.736 0.777 0.792 0.786TOR 1 0.802 0.754 0.827 0.827 0.878 0.722 0.850 0.820 0.808 0.747 0.784 0.798MQ 4 0.801 0.800 0.828 0.789 0.885 0.738 0.863 0.826 0.780 0.703 0.782 0.802CYW 1 0.797 0.769 0.839 0.782 0.833 0.755 0.842 0.815 0.770 0.741 0.828 0.788DAR 2 0.781 0.761 0.806 0.812 0.870 0.706 0.846 0.788 0.776 0.730 0.723 0.767ITA 1 0.779 0.738 0.775 0.832 0.873 0.711 0.860 0.788 0.742 0.708 0.762 0.780CHO 1 0.775 0.764 0.835 0.798 0.888 0.721 0.816 0.783 0.670 0.688 0.786 0.758HAU 1 0.773 0.731 0.820 0.806 0.897 0.686 0.830 0.832 0.763 0.703 0.702 0.736LIM 4 0.756 0.737 0.760 0.788 0.886 0.654 0.808 0.775 0.756 0.712 0.701 0.745COR 5 0.748 0.704 0.806 0.783 0.898 0.670 0.738 0.794 0.739 0.616 0.730 0.741HYD 1 0.744 0.680 0.778 0.748 0.839 0.693 0.788 0.781 0.735 0.613 0.770 0.754CUN 1 0.725 0.696 0.743 0.737 0.830 0.714 0.838 0.676 0.670 0.680 0.697 0.684UNT 3 0.645 0.667 0.682 0.635 0.746 0.558 0.687 0.676 0.620 0.539 0.667 0.609BOB 4 0.625 0.513 0.684 0.638 0.751 0.612 0.706 0.647 0.549 0.495 0.621 0.608KYL 1 0.590 0.589 0.603 0.643 0.634 0.554 0.663 0.627 0.569 0.450 0.649 0.507UKP 2 0.583 0.592 0.560 0.624 0.653 0.558 0.616 0.631 0.565 0.456 0.656 0.489MIC 3 0.430 0.419 0.386 0.411 0.519 0.407 0.488 0.422 0.384 0.400 0.500 0.396EUR 1 0.386 0.500 0.390 0.277 0.379 0.487 0.522 0.441 0.352 0.281 0.438 0.261VTX 5 0.319 0.367 0.298 0.179 0.297 0.159 0.435 0.340 0.370 0.201 0.410 0.230Table 3: Results for closed taskL1 F-ScoreTeamNameRun OverallAcc.ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TURTOR 5 0.565 0.410 0.776 0.692 0.754 0.277 0.680 0.660 0.650 0.653 0.190 0.468TUE 2 0.385 0.114 0.502 0.420 0.430 0.167 0.611 0.485 0.348 0.385 0.236 0.314NAI 2 0.356 0.329 0.450 0.331 0.423 0.066 0.511 0.426 0.481 0.314 0.000 0.207Table 4: Results for open-1 taskL1 F-ScoreTeamNameRun OverallAcc.ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TURTUE 1 0.835 0.798 0.876 0.844 0.883 0.777 0.883 0.836 0.794 0.846 0.826 0.818TOR 4 0.816 0.770 0.861 0.840 0.900 0.704 0.860 0.834 0.800 0.816 0.804 0.790HYD 1 0.741 0.677 0.782 0.755 0.829 0.693 0.784 0.777 0.728 0.613 0.766 0.744NAI 3 0.703 0.676 0.695 0.708 0.846 0.618 0.830 0.677 0.610 0.663 0.726 0.688Table 5: Results for open-2 task53was a mismatch in the genre of corpora (for exam-ple, tweets by Telugu speakers are different in com-position than essays written by Telugu speakers).TUE and TOR were the only two teams to partic-ipate in all three sub-tasks, and their Open-2 sys-tems outperformed their respective best systems inthe Closed and Open-1 sub-tasks.
This suggests, un-surprisingly, that adding more data can benefit NLI,though quality and genre of data are also importantfactors.7 Cross Validation ResultsUpon completion of the competition, we asked theparticipants to perform 10-fold cross-validation on adata set consisting of the union of TOEFL11-TRAINand TOEFL11-DEV.
This was the same set of dataused in the first work to use any of the TOEFL11data (Tetreault et al 2012), and would allow anotherpoint of comparison for future NLI work.
For directcomparison with Tetreault et al(2012), we providedthe exact folds used in that work.The results of the 10-fold cross-validation areshown in Table 6.
Two teams had systems that per-formed at 84.5 or better, which is just slightly higherthan the best team performance on the TOEFL11-TEST data.
In general, systems that performed wellin the main competition also performed similarly(in terms of performance and ranking) in the cross-validation experiment.
Please note that we reportresults as they are reported in the respective papers,rounding to just one decimal place where possible.8 Discussion of ApproachesWith so many teams competing in the shared taskcompetition, we investigated whether there were anycommonalities in learning methods or features be-tween the teams.
In this section, we provide a coarsegrained summary of the common machine learningmethods teams employed as well as some of thecommon features.
Our summary is based on the in-formation provided in the 24 team reports.While there are many machine learning algo-rithms to choose from, the overwhelming majorityof teams used Support Vector Machines.
This maynot be surprising given that most prior work has alsoused SVMs.
Tetreault et al(2012) showed that onecould achieve even higher performance on the NLITeam AccuracyCN 84.6JAR 84.5OSL 83.9BUC 82.6MQ 82.5TUE 82.4CAR 82.2NAI 82.1Tetreault et al(2012) 80.9HAU 79.9LIM 75.9CUN 74.2UNT 63.8MIC 63Table 6: Results for 10-fold cross-validation onTOEFL11-TRAIN + TOEFL11-DEVtask using ensemble methods for combining classi-fiers.
Four teams also experimented with differentways of using ensemble methods.
Three teams usedMaximum Entropy methods for their modeling.
Fi-nally, there were a few other teams that tried differ-ent methods such as Discriminant Function Analysisand K-Nearest Neighbors.
Possibly the most distinctmethod employed was that of string kernels by theBUC team (who placed third in the closed compe-tition).
This method only used character level fea-tures.
A summary of the machine learning methodsis shown in Table 7.A summary of the common features used acrossteams is shown in Table 8.
It should be noted thatthe table does not detail the nuanced differences inhow the features are realized.
For example, in thecase of n-grams, some teams used only the top kmost frequently n-grams while others used all of then-grams available.
If interested in more informationabout the particulars of a system and its feature, werecommend reading the team?s summary report.The most common features were word, characterand POS n-gram features.
Most teams used n-gramsranging from unigrams to trigrams, in line with priorliterature.
However several teams used higher-ordern-grams.
In fact, four of the top five teams (JAR,OSL, CAR, TUE) generally used at least 4-grams,54Machine Learning TeamsSVM CN, UNT, MQ, JAR, TOR, ITA, CUN, TUE, COR, NRC, HAU, MIC, CARMaxEnt / logistic regression LIM, HAI, CAREnsemble MQ, ITA, NRC, CARDiscriminant Function Analysis KYLString Kernels / LRD BUCPPM BOBk-NN VTXTable 7: Machine Learning algorithms used in Shared Taskand some, such as OSL and JAR, went as high 7 and9 respectively in terms of character n-grams.Syntactic features, which were first evaluated inWong and Dras (2011) and Swanson and Char-niak (2012) were used by six teams in the competi-tion, with most using dependency parses in differentways.
Interestingly, while Wong and Dras (2011)showed some of the highest performance scores onthe ICLE corpus using parse features, only two ofthe six teams which used them placed in the top tenin the Closed sub-task.Spelling features were championed by Koppel etal.
(2005) and in subsequent NLI work, howeveronly three teams in the competition used them.There were several novel features that teams tried.For example, several teams tried skip n-grams, aswell as length of words, sentences and documents;LIM experimented with machine translation; CUNhad different features based on the relative frequen-cies of the POS and lemma of a word; HAI triedseveral new features based on passives and contextfunction; and the TUE team tried a battery of syn-tactic features as well as text complexity measures.9 SummaryWe consider the first edition of the shared task asuccess as we had 29 teams competing, which weconsider a large number for any shared task.
Alsoof note is that the task brought together researchersnot only from the Computational Linguistics com-munity, but also those from other linguistics fieldssuch as Second Language Acquisition.We were also delighted to see many teams buildon prior work but also try novel approaches.
It isour hope that finally having an evaluation on a com-mon data set will allow researchers to learn fromeach other on what works well and what does not,and thus the field can progress more rapidly.
Theevaluation scripts are publicly available and we ex-pect that the data will become available through theLinguistic Data Consortium in 2013.For future editions of the NLI shared task, wethink it would be interesting to expand the scope ofNLI from identifying the L1 of student essays to beable to identify the L1 of any piece of writing.
TheICLE and TOEFL11 corpora are both collections ofacademic writing and thus it may be the case thatcertain features or methodologies generalize betterto other writing genres and domains.
For those in-terested in robust NLI approaches, please refer to theTOR team shared task report as well as Brooke andHirst (2012).In addition, since the TOEFL11 data contains pro-ficiency level one could include an evaluation byproficiency level as language learners make differ-ent types of errors and may even have stylistic differ-ences in their writing as their proficiency progresses.Finally, while this may be in the periphery of thescope of an NLI shared task, one interesting evalua-tion is to see how well human raters can fare on thistask.
This would of course involve knowledgeablelanguage instructors who have years of experiencein teaching students from different L1s.
Our think-ing is that NLI might be one task where computerswould outperform human annotators.AcknowledgmentsWe would like to thank Derrick Higgins and mem-bers of Educational Testing Service for assisting usin making the TOEFL11 essays available for thisshared task.
We would also like to thank PatrickHoughton for assisting the shared task organizers.55Feature Type TeamsWord N-Grams 1 CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, UAB,CYW, NAI, NRC, MIC, CAR2 CN, UNT, JAR, TOR, KYL, ITA, CUN, BOB, OSL, TUE, COR,UAB, CYW, NAI, NRC, HAU, MIC, CAR3 UNT, MQ, JAR, KYL, CUN, COR, HAU, MIC, CAR4 JAR, KYL, CAR5 CARPOS N-grams 1 CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, HAI, CAR2 CN, UNT, JAR, TOR, ITA, LIM, CUN, BOB, TUE, COR, HAI,NAI, NRC, MIC, CAR3 CN, UNT, JAR, TOR, LIM, CUN, TUE, COR, HAI, NAI, NRC,CAR4 CN, JAR, TUE, HAI, NRC, CAR5 TUE, CARCharacter N-Grams 1 CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, HAI, CAR2 CN, UNT, MQ, JAR, TOR, ITA, LIM, BOB, OSL, COR, HAI, NAI,HAU, MIC, CAR3 CN, UNT, MQ, JAR, TOR, LIM, BOB, OSL, VTX, COR, HAI,NAI, NRC, HAU, MIC, CAR4 CN, JAR, LIM, BOB, OSL, HAI, HAU, MIC, CAR5 CN, JAR, BOB, OSL, HAU, CAR6 CN, JAR, OSL,7 JAR, OSL8-9 JARFunction N-Grams MQ, UABSyntactic Features Dependencies MQ, TOR, ITA, TUE, NAI, NRCTSG MQ, TOR, NAI,CF Productions TOR,Adaptor Grammars MQSpelling Features LIM,CN, HAITable 8: Common Features used in Shared TaskIn addition, thanks goes to the BEA8 Organizers(Joel Tetreault, Jill Burstein and Claudia Leacock)for hosting the shared task with their workshop.
Fi-nally, we would like to thank all the teams for partic-ipating in this first shared task and making it a suc-cess.
Their feedback, patience and enthusiasm madeorganizing this shared task a great experience.ReferencesDaniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: ACorpus of Non-Native English.
Technical report, Ed-ucational Testing Service.Julian Brooke and Graeme Hirst.
2011.
Native languagedetection with ?cheap?
learner corpora.
In Conferenceof Learner Corpus Research (LCR2011), Louvain-la-Neuve, Belgium.
Presses universitaires de Louvain.Julian Brooke and Graeme Hirst.
2012.
Robust, Lexical-ized Native Language Identification.
In Proceedingsof COLING 2012, pages 391?408, Mumbai, India, De-cember.
The COLING 2012 Organizing Committee.Dominique Estival, Tanja Gaustad, Son Bao Pham, WillRadford, and Ben Hutchinson.
2007.
Author profilingfor English emails.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics, pages 263?272, Melbourne, Australia.Sylviane Granger, Estelle Dagneaux, and Fanny Meunier.2009.
The International Corpus of Learner English:Handbook and CD-ROM, version 2.
Presses Universi-taires de Louvain, Louvain-la-Neuve, Belgium.Shin?ichiro Ishikawa.
2011.
A New Horizon in LearnerCorpus Studies: The Aim of the ICNALE Projects.
InG.
Weir, S. Ishikawa, and K. Poonpon, editors, Cor-56pora and Language Technologies in Teaching, Learn-ing and Research.
University of Strathclyde Publish-ing.Scott Jarvis and Scott Crossley, editors.
2012.
Approach-ing Language Transfer Through Text Classification:Explorations in the Detection-based Approach, vol-ume 64.
Multilingual Matters Limited, Bristol, UK.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Determining an author?s native language by mining atext for errors.
In Proceedings of the eleventh ACMSIGKDD international conference on Knowledge dis-covery in data mining, pages 624?628, Chicago, IL.ACM.Michael Swan and Bernard Smith, editors.
2001.Learner English: A teacher?s guide to interference andother problems.
Cambridge University Press, 2 edi-tion.Benjamin Swanson and Eugene Charniak.
2012.
Na-tive Language Detection with Tree Substitution Gram-mars.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 193?197, Jeju Island, Ko-rea, July.
Association for Computational Linguistics.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native tongues, lost andfound: Resources and empirical evaluations in nativelanguage identification.
In Proceedings of COLING2012, pages 2585?2602, Mumbai, India, December.The COLING 2012 Organizing Committee.Sze-Meng Jojo Wong and Mark Dras.
2011.
ExploitingParse Structures for Native Language Identification.In Proceedings of the 2011 Conference on Empiri-cal Methods in Natural Language Processing, pages1600?1610, Edinburgh, Scotland, UK., July.
Associa-tion for Computational Linguistics.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2011.
Topic Modeling for Native Language Identifi-cation.
In Proceedings of the Australasian LanguageTechnology Association Workshop 2011, pages 115?124, Canberra, Australia, December.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2012.
Exploring Adaptor Grammars for Native Lan-guage Identification.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 699?709, Jeju Island, Korea,July.
Association for Computational Linguistics.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A New Dataset and Method for Automati-cally Grading ESOL Texts.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 180?189, Portland, Oregon, USA, June.
Associ-ation for Computational Linguistics.57
