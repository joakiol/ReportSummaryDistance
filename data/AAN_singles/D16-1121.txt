Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1131?1141,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsUnderstanding Language Preference for Expression of Opinion andSentiment: What do Hindi-English Speakers do on Twitter?Koustav RudraIIT Kharagpur, Indiakoustav.rudra@cse.iitkgp.ernet.inShruti Rijhwani?Carnegie Mellon University,Pittsburgh, Pennsylvaniasrijhwan@cs.cmu.eduRafiya BegumMicrosoft Research Labs,Bangalore, Indiat-rafbeg@microsoft.comKalika BaliMicrosoft Research Labs,Bangalore, Indiakalikab@microsoft.comMonojit ChoudhuryMicrosoft Research Labs,Bangalore, Indiamonojitc@microsoft.comNiloy GangulyIIT Kharagpur, Indianiloy@cse.iitkgp.ernet.inAbstractLinguistic research on multilingual societieshas indicated that there is usually a preferredlanguage for expression of emotion and sen-timent (Dewaele, 2010).
Paucity of data haslimited such studies to participant interviewsand speech transcriptions from small groupsof speakers.
In this paper, we report a studyon 430,000 unique tweets from Indian users,specifically Hindi-English bilinguals, to un-derstand the language of preference, if any,for expressing opinion and sentiment.
To thisend, we develop classifiers for opinion detec-tion in these languages, and further classifyingopinionated tweets into positive, negative andneutral sentiments.
Our study indicates thatHindi (i.e., the native language) is preferredover English for expression of negative opin-ion and swearing.
As an aside, we exploresome common pragmatic functions of code-switching through sentiment detection.1 IntroductionThe pattern of language use in a multilingual soci-ety is a complex interplay of socio-linguistic, dis-cursive and pragmatic factors.
Sometimes speakershave a preference for a particular language for cer-tain conversational and discourse settings; on otheroccasions, there is fluid alteration between two ormore languages in a single conversation, also knownas Code-switching (CS) or Code-mixing1.
Under-?
* This work was done when the author was a Research Fel-low at Microsoft Research Lab India.1Although some linguists differentiate between Code-switching and Code-mixing, this paper will use the two termsinterchangeably.standing and characterizing language preference inmultilingual societies has been the subject matter oflinguistic inquiry for over half a century (see Milroyand Muysken (1995) for an overview).Conversational phenomena such as CS were ob-served only in speech and therefore, all previousstudies are based on data collected from a smallset of speakers or from interviews.
With the grow-ing popularity of social media, we now have anabundance of conversation-like data that exhibit CSand other speech phenomena, hitherto unseen intext (Bali et al, 2014).
Leveraging such data fromTwitter, we conduct a large-scale study on languagepreference, if any, for the expression of opinion andsentiment by Hindi-English (Hi-En) bilinguals.We first build a corpus of 430,000 unique India-specific tweets across four domains (sports, enter-tainment, politics and current events) and automati-cally classify the tweets by their language: English,Hindi and Hi-En CS.
We then develop an opinion de-tector for each language class to further categorizethem into opinionated and non-opinionated tweets.Sentiment detectors further classify the opinionatedtweets as positive, negative or neutral.
Our studyshows that there is a strong preference towards Hindi(i.e.
the native language or L1) over English (L2) forexpression of negative opinion.
The effect is clearlyvisible in CS tweets, where a switch from English toHindi is often correlated with a switch from a pos-itive to negative sentiment.
This is referred to asthe polarity?switch function of CS (Sanchez, 1983).Using the same experimental technique, we also ex-plore other pragmatic functions of CS, such as rein-forcement and narrative?evaluative.1131Apart from being the first large-scale quantita-tive study of language preference in multilingualsocieties, this work also has several other contri-butions: (a) We develop one of the first opin-ion and sentiment classifiers for Romanized Hindiand CS Hi-En tweets with higher accuracy thanthe only known previous attempt (Sharma et al,2015b).
(b) We present a novel methodology for au-tomatically detecting pragmatic functions of code-switching through opinion and sentiment detection.The rest of the paper is organized as follows:Sec.
2 introduces language preference, functions ofCS and Hindi-English bilingualism on the web.
Sec.3 formulates the problem and presents the funda-mental questions that this paper seeks to answer.Sec.
4 and 5 discuss dataset creation and opinion andsentiment detection techniques respectively.
Sec.
6evaluates the hypotheses in light of the observationson the tweet corpus.
We conclude in Sec.
7, andraise some interesting sociolinguistic questions forfuture studies.2 Background and Related WorkIn order to situate the questions addressed in ourwork in existing literature, we present a briefoverview of the past research in pragmatic and dis-cursive analysis of code-switching, and specifically,on language preference for emotional expression.
Aprimer to Hi-En bilingualism and its presence in so-cial media shall follow.2.1 CS Functions and Language PreferenceIn multilingual communities, where there are morethan one linguistic channels for information ex-change, the choice of the channel depends on a vari-ety of factors, and is usually unpredictable (Auer,1995).
Nevertheless, linguistic studies point outcertain frequently-observed patterns.
For instance,certain speech activities might be exclusively ormore commonly related to a certain language choice(e.g.
Fishman (1971) reports use of English for pro-fessional purposes and Spanish for informal chatfor English-Spanish bilinguals from Puerto Rico).Apart from association between such conversationalcontexts and language preference, language alter-ation is often found to be used as a signaling de-vice to imply certain pragmatic functions (Barredo,1997; Sanchez, 1983; Nishimura, 1995; Maschler,1991; Maschler, 1994) such as: (a) reported speech(b) narrative to evaluative switch (c) reiterations oremphasis (d) topic shift (e) puns and language play(f) topic/comment structuring etc.
Attempts of pre-dicting the preferred language, or even exhaustivelylisting such functions, have failed.
However, lin-guists agree that language alteration in multilingualcommunities is not a random process.Of specific interest to us are the studies onlanguage preference for expression of emotions.Through large-scale interviews and two decades ofresearch, Dewaele (2004; 2010) argued that for mostmultilinguals, L1 (the dominant language, which isoften, but not always, the native or mother tongue)is the language preference for emotions, which in-clude emotional inner speech, swearing and evenemotional conversations.
Dewaele argues that emo-tionally charged words in L1 elicit stronger emo-tions than those in other languages, and hence L1is preferred for emotion expression.2.2 Hindi-English BilingualismAround 125 million people in India speak English,half of whom have Hindi as their mother-tongue.The large proportion of the remaining half, espe-cially those residing in the metropolitan cities, alsoknow at least some Hindi.
This makes Hi-En code-switching, commonly called Hinglish, extremelywidespread in India.
There is historical attesta-tion, as well as recent studies on the growing use ofHinglish in general conversation, and in entertain-ment and media (see Parshad et al (2016) and ref-erences therein).
Several recent studies (Bali et al,2014; Barman et al, 2014; Solorio et al, 2014; Se-quiera et al, 2015) also provide evidence of Hinglishand other instances of CS on online social mediasuch as Twitter and Facebook.
In a Facebook datasetanalyzed by Bali et al (2014), almost all sufficientlylong conversation threads were found to be multi-lingual, and as much as 17% of the comments hadCS.
This study also indicates that on online socialmedia, Hindi is seldom written in the Devanagariscript.
Instead, loose Roman transliteration, or Ro-manized Hindi, is common, especially when userscode-switch between Hindi and English.While there has been some effort towards com-putational processing of CS text (Solorio and Liu,2008; Solorio and Liu, 2010; Vyas et al, 2014; Peng1132et al, 2014), to the best of our knowledge, there hasbeen no study on automatic identification of func-tional aspects of CS or any large-scale, data-drivenstudy of language preference.
The current studyadds to the growing repertoire of work on quantita-tive analysis of social media data for understandingsocio-linguistic and pragmatic issues, such as de-tection of depression (De Choudhury et al, 2013),politeness (Danescu-Niculescu-Mizil et al, 2013),speech acts (Vosoughi and Roy, 2016), and socialstatus (Tchokni et al, 2014).3 Problem FormulationAlong the lines of (Dewaele, 2010), we ask the fol-lowing question: Is there a preferred language forexpression of opinion and sentiment by the Hi-Enbilinguals on Twitter?3.1 DefinitionsMore formally, let ?
= {h, e,m} be the set oflanguages: Hindi (h), English (e) and Mixed (m),i.e., code-switched.
Let ?
= {d, r}, be the setof scripts:2 Devanagari (d) and Roman (r).
Letus further introduce a set of sentiments, 3 ={+,?, 0,?
}, where +, ?
and 0 respectively denoteutterances with positive, negative and neutral opin-ions.
?
denote non-opinionated (like factual) texts.Let T = {t1, t2, .
.
.
t|T |} be a set of tweets (or anytext) generated by Hi-En bilinguals.
We define:?
?
(T ), ?
(T ) and (T ) as the subsets of T thatrespectively contain all tweets in language ?,script ?
and sentiment .?
?
? (T ) = ?
(T )?
?
(T )?
(T ).
Likewise, wealso define ? (T ) = ?
(T ) ?
(T ), ??
(T ) =?
(T ) ?
?
(T ) and ? (T ) = ?
(T ) ?
(T ).The preference towards a language-script pair ??
forexpressing a type of sentiment  is given by the prob-abilitypr(?
?|;T ) = pr(|??
;T )pr(?
?|T )pr(|T ) (1)However, pr(??
), which defines the prior probabilityof choosing ??
for a tweet is dependent on a large2Tweets in mixed script are rare and hence we do not includea symbol for it, though the framework does not preclude suchpossibilities.number of socio-linguistic parameters beyond sen-timent.
For instance, on social media, English isoverwhelmingly more common than any Indic lan-guage (Bali et al, 2014).
This is because (a) En-glish tweets come from a large number of users apartfrom Hi-En bilinguals and (b) English is the pre-ferred language for tweeting even for Hi-En bilin-guals because it expands the target audience of thetweet by manifolds.
The preference of ??
for ex-pressing , therefore, can be quantified as:pr(|??
;T ) = |?
? (T )||??
(T )| (2)We say ??
is the preferred language-script choiceover ????
for expressing sentiment  if and only ifpr(|??
;T ) > pr(|????
;T ) (3)The strength of the preference is directlyproportionate the ratio of the probabilities:pr(|??
;T )/pr(|????
;T ).
An alternative butrelated way of characterizing the preference isthrough comparing the odds of choosing a senti-ment type  to its polar opposite - ?.
We say, ??
isthe preferred language-script pair for expressing ,ifpr(|??
;T )pr(?|??
;T ) >pr(|????
;T )pr(?|????
;T ) (4)3.2 HypothesesNow we can formally define the two hypotheses, weintend to test here.Hypothesis I: For Hi-En bilinguals, Hindi is the pre-ferred language for expression of opinion on Twitter.Therefore, we expectpr({+,?, 0}|hd;T ) > pr({+,?, 0}|er;T ) (5)i.e., pr(?|hd;T ) < pr(?|er;T ) (6)And similarly,pr(?|hr;T ) < pr(?|er;T ) (7)Hypothesis II: For Hi-En bilinguals, Hindi is thepreferred language for expression of negative senti-ment.
Therefore,pr(?|hd;T ) ?
pr(?|hr;T ) > pr(?|er;T ) (8)1133In particular, we would like to hypothesize that theodds of choosing Hindi for negative over positive isreally high compared to the odds for English.
I.e.,pr(?|hd;T )pr(+|hd;T ) ?pr(?|hr;T )pr(+|hr;T ) >pr(?|er;T )pr(+|er;T ) (9)A special case of the above hypotheses arise inthe context of code-mixing, i.e., for the set mr(T ).Since the mixed tweets certainly come from profi-cient bilinguals and have both Hi and En fragments,we can reformulate our hypotheses at a tweet level.Let mhr(T ) and mer(T ) respectively denote the setof Hi and En fragments in mr(T ).Hypothesis Ia: Hindi is the preferred language forexpression of opinion in Hi-En code-mixed tweets.Therefore, we expecti.e., pr(?|mhr;T ) < pr(?|mer;T ) (10)Hypothesis IIa: Hindi is the preferred languagefor expression of negative sentiment in Hi-En code-switched tweets.
Therefore,pr(?|mhr;T ) > pr(?|mer;T ) (11)pr(?|mhr;T )pr(+|mhr;T ) >pr(?|mer;T )pr(+|mer;T ) (12)Likewise, the above hypotheses also apply for theDevanagari script, though for technical reasons, wedo not test them here.Besides comparing aggregate statistics onmr(T ),it is also interesting to look at the sentiment ofmhr(ti) andmer(ti) for each tweet ti.
In particular,for every pair of  6=?, we want to study the fractionof tweets in mr(T ) where mhr(ti) has sentiment and mer(ti) has ?.
Let this fraction be pr(h ?e?
;mr(T )).
Under ?no-preference for language?
(i.e., the null) hypothesis, we would expect pr(h ?e?
;mr(T )) ?
pr(h?
?
e;mr(T )).
However,if pr(h ?
?
;mr(T )) is significantly higher thanpr(h?
?
e;mr(T )), it means that speakers preferto switch from English to Hindi when they want toexpress a sentiment  and vice versa.Pragmatic Functions of Code-Switching: Whennative speakers tend to switch from Hindi to Englishwhen they switch from an expression with sentiment to one with ?, or in other words  ?
?, weTopic (# tweets): HashtagsSports (188K): #IndvsPak, #IndvsUae, #IndvsSaMovies (82K): #MSG3successfulweeks, #MS-Gincinemas, #BlockbusterMSG, #Shamitabh, #PKPolitics (92K): #DelhiDecides, #RahulonlLeave,#AAPStorm, #AAPSweepCurrent Events (68K): #RailBudget2015, #Beef-ban, #LandAcquisitionBill, #UnionBudget2015Table 1: Hashtags used and number of tweets collectedsay this is an observed pragmatic function of code-switching between Hindi and English (note that theorder of the languages is important), if and only ifpr(h ?
e?
;mr(T ))pr(h?
?
e;mr(T )) > 1 (13)3.3 A Note on Statistical SignificanceAll the statistics defined here are likelihoods; Equa-tions 9, 12 and 13, in particular, state our hypothesisin the form of the Likelihood Ratio Test.
However,the true classes ?
and  are unknown; we predictthe class labels using automatic language and senti-ment detection techniques that have non-negligibleerrors.
Under such a situation, the likelihoods can-not be considered as true test statistics, and conse-quently, hypothesis testing cannot be done per se.Nevertheless, we can use these as descriptive statis-tics and investigate the status of the aforementionedhypotheses.4 DatasetsWe collected tweets with certain India-specific hash-tags (Table 1) using the Twitter Search API (Twi,2015b) over three months (December 2014 ?
Febru-ary 2015).
In this paper, we use tweets in De-vanagari script Hindi (hd), and Roman script En-glish (er), Hindi (hr) and Hi-En Mixed (mr).
En-glish and mixed tweets written in Devanagari are ex-tremely rare (Bali et al, 2014) and we do not studythem here.
We filter out tweets labeled by the Twit-ter API (Twi, 2015a) as German, Spanish, French,Portuguese, Turkish, and all non-Roman script lan-guages (except Hindi).We experiment on the following different corpora:TAll: All tweets after filtering.
This corpuscontains 430,000 unique tweets posted by 1,25,396unique users.1134TBL: Tweets from users who are certainly Hi-Enbilinguals, which are approximately 55% (240,000)of the tweets in TAll.
We define a user to be a Hi-Enbilingual if there is at least one mr tweet from theuser, or if the user has tweeted at least once in Hindi(hd or hr) and once in English (er).Tspo,Tmov,Tpol,Teve: Topic-wise corpora forsports, movies, politics and events (Table 1).TCS: Tweets with inter-sentential CS.
We definethese as tweets containing at least one sequence of 5contiguous Hindi words and one sequence of 5 con-tiguous English words.
The corpus has 3,357 tweets.SAC: 1000 monolingual tweets (er, hr, hd) and260 mixed (mr) tweets manually annotated withsentiment and opinion labels.
These were annotatedby two linguists, both fluent Hi-En speakers.
The an-notators first checked whether the tweet is opinion-ated or?
and then identified polarity of the opinion-ated tweets (+, ?
or 0).
Thus, the tweets are classi-fied into the four classes in the set 3.
If a tweet con-tains both opinion and ?, each fragment was indi-vidually annotated.
The inter-annotator agreement is77.5% (?
= 0.59) for opinion annotation and 68.4%(?
= 0.64) over all four classes.
A third linguistindependently corrected the disagreements.LLCTest: 141 er, 137 hr, and 241 mr tweetsannotated by a Hi-En bilingual form the test set forthe Language Labeling system (Sec.
5.1).SAC and LLCTest can be downloaded and usedfor research purposes3.Note that apart from SAC and LLCTest, all cor-pora are subsets of TAll.
For generalizability ofour observations, it is important to ensure that thetweets in TAll come from a large number of usersand the datasets do not over-represent a small set ofusers.
In Figure 1, we plot the minimum fraction ofusers required (x-axis) to cover a certain percentageof the tweets in TAll (y-axis).
Tweets from at least10%, i.e., 12.5K users are needed to cover 50% ofthe corpus.
As expected, we do observe a power-law-like distribution, where a few users contribute alarge number of tweets, and a large number of userscontribute a few tweets each.
We believe that 12.5Kusers is sufficient to ensure an unbiased study.Further, we classify the users into three specificgroups (i) news channels, (ii) general users (having3http://www.cnergres.iitkgp.ac.in/codemixing0 20 40 60 80 1005060708090100% of top users% of tweetscoveredFigure 1: Distribution of cumulative % of tweets and # ofusers (sorted in descending order by number of tweets).?
10,000 followers), (iii) popular users or celebrities(having > 10,000 followers).
Interestingly, for bothTAll, and TBL corpora, we observe that around98% of all users are general, and 96% of all tweetscome from such users.
Hence, most observationsfrom these corpora are expected to be representativeof the average online linguistic behavior of a Hi-Enbilingual.5 MethodFig.
2 diagrammatically summarizes our experimen-tal method.
We identify the language used in eachtweet before detecting opinion and sentiment.5.1 Language LabelingTweets in Devanagari script are accurately detectedby the Twitter API as Hindi tweets ?
we label theseas hd, though a small fraction of them could also bemd.
To classify Roman script tweets as er, hr ormr, we use the system that performed best in theFIRE 2013 shared task for word-level language de-tection of Hi-En text (Gella et al, 2013).
This sys-tem uses character n-gram features with a MaximumEntropy model for labeling each input word with alanguage label (either English or Hindi).
We designminor modifications to the system to improve its per-formance on Twitter data, which are omitted heredue to paucity of space.5.2 Opinion and Sentiment DetectionMost of the existing research in opinion detec-tion (Qadir, 2009; Brun, 2012; Rajkumar et al,1135Figure 2: Overview of the experimental method.2014) and sentiment analysis (Mohammad, 2012;Mohammad et al, 2013; Mittal et al, 2013; Rosen-thal et al, 2015) focus on monolingual tweets andsentences.
Recently, there has been a couple ofstudies on sentiment detection of code-switchedtweets (Vilares et al, 2015; Sharma et al, 2015b).Sharma et al (2015b) use Hindi SentiWordNet andnormalization techniques to detect sentiment in Hi-En CS tweets.We propose a two-step classification model.
Wefirst identify whether a tweet is opinionated or non-opinionated (?).
If the tweet is opinionated, we fur-ther classify it according to its sentiment (+,?
or 0).Fig.
2 shows the architecture of the proposed model.Two-step classification was empirically found to bebetter than a single four-class classifier.We develop individual classifiers for each lan-guage class (er, hr, hd, mr) using an SVM withRBF kernel from Scikit-learn (Pedregosa et al,2011).
We use the SAC dataset (Sec.
4) as train-ing data and features as described in Sec.
5.3.5.3 Classifier FeaturesFor opinion classification (opinion or ?
), we pro-pose a set of event-independent lexical features andTwitter-specific features.
(i) Subjective words: Ex-pected to be present in opinion tweets.
We use lexi-cons from Volkova et al (2013) for er and Bakliwalet al (2012) for hd.
We Romanize the hd lexiconfor the hr classifiers (ii) Elongated words: Wordswith one character repeated more than two times,e.g.
sooo, naaahhhhi (iii) Exclamations: Presenceof contiguous exclamation marks (iv) Emoticons4(v) Question marks: Queries are generally non-opinionated.
(vi) Wh-words: These are used toform questions (vii) Modal verbs: e.g.
should,could, would, cud, shud (viii) Excess hashtags:Presence of more than two hashtags (ix) Intensi-fiers: Generally used to emphasize sentiment, e.g.,we shouldn?t get too comfortable (x) Swear words5:Prevalent in opinionated tweets, e.g.
that was af ing no ball!!!!
#indvssa (xi) Hashtags: Hash-tags might convey user sentiment (Barbosa et al,2012).
We manually identify hashtags in our corpusthat represent explicit opinion.
(xii) Domain lexi-con: For hr, & hd category tweets, we constructsentiment lexicons from 1000 manually annotatedtweets.
Each word or phrase in this lexicon repre-sents +, or ?, or 0 sentiment.
(xiii) Twitter usermentions (xiv) Pronouns: Opinion is often in firstperson using pronouns like I and we.For sentiment classification, we use emoticons,swear words, exclamation marks and elongatedwords as described above.
We also use subjec-tive words from various lexicons (Mohammad andTurney, 2013; Volkova et al, 2013; Bakliwal etal., 2012; Sharma et al, 2015a).
Additionally, weuse ?
(i) Sentiment words: From Hashtag Senti-ment and Sentiment140 lexicons (Mohammad et al,2013).
We also manually annotate hashtags from ourdataset that represent sentiment.
(ii) Negation: Anegated context is tweet segment that begins witha negation word and ends with a punctuation mark(Pang et al, 2002).
The list of negation words are4The list of emoticons was extracted from Wikipedia5Swear word lexicons from noswearing.com, youswear.com1136Classifier er hd hr mrOpinion 72.6 72.0 79.9 73.5Sentiment 64.4 61.5 62.7 63.4Table 2: Accuracy of the opinion and sentiment classi-fiers.
All values are in %.taken from Christopher Potts?
sentiment tutorial6.Themr opinion classifier uses the output from theer and hr classifiers as features (Fig.
2), along withan additional feature that represents whether the ma-jority of the words in the tweet are Hindi or not.
Asimilar strategy is used for mr sentiment detection.5.4 EvaluationWe evaluated the language labeling system on theLLCTest corpus, on which the precision (recall)values were 0.93(0.91), 0.90(0.85) and 0.88(0.92)for er, hr and mr classes respectively.
The tweet-level classification accuracy was 89.8%.The opinion and sentiment classifiers were eval-uated using 10-fold cross validation on the SACdataset.
Table 2 details the class-wise accuracy.
Forcomparison, we also reimplemented the dictionaryand dependency-based method by Qadir (2009).The accuracy of the opinion classifier on the ertweets was found to be 65.7%, 7% lower than oursystem.
We also compared our mr sentiment clas-sifier with that of Sharma et al (2015b).
As theirmethod performs two class sentiment detection (+and ?
), we select such tweets from SAC.
Theirsystem achieves an accuracy of 68.2%, which is 4%lower than the accuracy of our system.An analysis of the errors showed more false nega-tives (i.e., opinions labeled?)
than false positives inopinion classification.
Sentiment misclassification isuniformly distributed.Table 3 reports the accuracy of the opinion clas-sifier for feature ablation experiments.
For all threelanguage-script pairs, lexicon and non-word (emoti-cons, elongated words, hashtags, exclamation) fea-tures are the most effective, though all features havesome positive contribution towards the final accu-racy of opinion detection.
For hr and hd tweets, do-main knowledge is significant, as shown by the 4%accuracy drop with removing the domain lexicon.6http://sentiment.christopherpotts.net/lingstruc.htmlAblated Feature(s) er hr hdNONE 72.6 79.9 72.0mention 70.1 79.3 70.8lexicon 68.1 75.9 66.6subjective 69.7 79.8 70.3wh-words 71.0 79.3 70.1modal verb 71.1 79.3 71.3intensifier 71.3 76.6 69.6slang 70.0 79.2 70.6pronoun 71.6 79.7 70.3domain lex.
N.A.
77.0 67.7non-word 67.7 75.6 68.9Table 3: Feature ablation experiments for the opinionclassifiers.
NONE represents the case when all featureswere used.
The two smallest values (pertaining to thetwo most effective features) are shown in bold.Corpus TBL TAll Tpol Tmov|er(T )|/|T | 0.65 0.79 0.76 0.70|hd(T )|/|T | 0.12 0.08 0.13 0.04|hr(T )|/|T | 0.08 0.05 0.05 0.09|mr(T )|/|T | 0.15 0.08 0.06 0.17Table 4: Distribution across classes in ?6 Experiments and ObservationsIn this section, we report our experiments on430,000 unique tweets (TAll), and its various sub-sets as defined in Sec 4.
First, we run the languagedetection system on the corpora.
Table 4 shows thelanguage-wise distribution.
We see that languagepreference varies by topic, which is not surprising.Due to paucity of space, the correlation between lan-guage usage and topic will not be discussed at lengthhere, but we will highlight cases where the differ-ences are striking.We apply the language-specific opinion and senti-ment classifiers to tweets detected as the correspond-ing language class.
In the following subsections, weempirically investigate the hypotheses.6.1 Status of Hypotheses I and IITable 5 shows pr(?|??
;T ), pr(?|??
;T ) andpr(?|??
;T )/pr(+|??
;T ) for TAll, TBL and tworandomly selected topics ?
Movie and Politics.
Thestatistics are fairly consistent over the corpora, withslight differences but similar trends in Tmov.1137Statistic ??
TBL TAll Tpol Tmover 0.34 0.35 0.37 0.29pr(?|??
;T ) hd 0.45 0.47 0.48 0.49hr 0.38 0.39 0.37 0.49er 0.16 0.17 0.22 0.07pr(?|??
;T ) hd 0.18 0.17 0.19 0.16hr 0.24 0.25 0.27 0.13pr(?|??
;T )pr(+|??
;T )er 0.35 0.38 0.59 0.11hd 3.00 3.27 5.67 1.90hr 1.46 1.60 1.96 0.55Table 5: Sentiment across languages: Statistics concern-ing hypotheses I and II.We need the first statistic in order to investigateHypothesis I (Eqs.
6 and 7), and the two latter onesfor verifying Hypothesis II (Eqs.
8 and 9).Contrary to Eqs.
6 and 7, for all corpora exceptTmov, we observe the following trend:pr(?|hd;T ) > pr(?|hr;T ) ?
pr(?|er;T )In other words, hd is more commonly preferred forexpressing non-opinions than hr and er.
Hypothe-sis I is clearly untrue for these corpora, though due tothe small differences between hr and er, we cannotclaim that English is the preferred language for ex-pressing opinions.
A closer scrutiny of the corporarevealed that hd tweets mostly come from officialsources (news channels, political parties, productionhouses) and celebrities, which are mostly factual.hr tweets are from general users and show similartrends as English.
Thus, in general, there seems tobe no preferred language for expressing opinion bythe Hi-En bilinguals on Twitter.In the context of Hypothesis II, we see the gen-eral pattern (with some topic specific variations):pr(?|hr;T ) > pr(?|hd;T ) ?
pr(?|er;T )The pattern emerges even more strongly, when welook at pr(?|??
;T )/pr(+|??
;T ).
The odds of ex-pressing a negative opinion over positive opinion inHindi is between 1.5 and 6 (Tmov exhibits a slightlydifferent pattern but similar preference, Tpol shows astronger preference towards Hindi for negative senti-ment), whereas the same for English is between 0.1and 0.6.
In other words, English is more preferredStatistic mhr merpr(?|??
;TCS) 0.39 0.45pr(?|??
;TCS) 0.22 0.14pr(?|??;TCS)/pr(+|??
;TCS) 2.2 0.34Table 6: TCS statistics for testing hypotheses Ia and IIafor expressing positive opinion, and Hindi for nega-tive opinion.
These observations provide very strongevidence in favor of Hypothesis II.6.2 Status of Hypotheses Ia and IIaRecall that Hypothesis Ia and Hypothesis IIa areessentially same as Hypotheses I and II, but appliedon mhr and mer fragments from the TCS corpus.Table 6 reports the three statistics necessaryfor testing these hypotheses.
pr(?|mer;TCS) isslightly greater than pr(?|mhr;TCS), which iswhat we would expect if Hypothesis Ia was true.However, since the difference is small, we view itas a trend rather than a proof of Hypothesis Ia.The statistics clearly show that Hypothesis IIaholds true for TCS .
The fraction of negative senti-ment in mhr is over 1.5 times higher than that ofmer.
Further, the odds of expressing a negative sen-timent in Hindi over positive sentiment in Hindi ina code-switched tweet is 6.5 times higher than thesame odds for English.6.3 Switching FunctionsRecall that using Eq.
13 (Sec.
3), we can estimatethe preference, if any, for switching to a particularlanguage while changing the sentiment.
In particu-lar, research in socio-linguistics has shown that usersoften switch between languages when they switchfrom non-opinion (?)
to opinion ({+,?, 0}).
Thisis called the Narrative-Evaluative function of CS(Sanchez, 1983).
This function appears in 46.1%of the tweets in TCS .
We find thatpr(h{+,?, 0} ?
e?;TCS)pr(h?
?
e{+,?, 0};TCS)= 0.86which indicates that there is no preference forswitching to Hindi (or English) while switching be-tween opinion and non-opinion.
This is also con-firmed above in the context of hypotheses I and Ia.While switching between opinion and non-opinionin a tweet, users do switch language.
However, we1138Sports Movies Politics Events051015202530Percentage of tweets containingslang termser hd hr mr(a) Abusive tweetsSports Movies Politics Events012345678Percentage of code?switched tweetscontaining slang termser hr(b) Swearing pref.
in TCSFigure 3: Distribution of swear words by languageobserve no particular preference for the languageschosen for each part.We also report two other pragmatic functions:pr(h?
?
e{+, 0,?
};TCS)pr(h{+, 0,?}
?
e?
;TCS)= 1.98pr(h?
?
e+;TCS)pr(h+?
e?
;TCS)= 10.27The latter function is called polarity switch.
The ex-tremely high value for these ratios is an evidencefor a strong preference towards switching languagefrom English to Hindi while switching to negativesentiment (and switching to English when sentimentchanges from negative to positive).We also observe cases where there is a languageswitch, but no sentiment switch and hence, we can-not evaluate language preference using Eq.
13 (be-cause  = ?).
In TCS , 15.3% of the tweets showPositive Reinforcement, where both fragments are ofpositive sentiment.
Negative Reinforcement is de-fined similarly and is seen in 8.7% of the tweets.Other tweets in TCS likely have pragmatic functionsthat cannot be identified based on sentiment.6.4 Language Preference for SwearingSince there is evidence that the native language(Hindi, in this case) is preferred for swearing (De-waele, 2004), we computed the fraction of tweetsthat contain swear words in each language class.Fig.
3a shows the distribution across topics.
Thelanguages hr and mr have a much higher fractionof abusive tweets than er and hd.
Fig.
3b shows thedistribution of abusive mhr and mer fragments fortweets in TCS .
Interestingly, over 90% of the swearwords occur in mhr.
Both distributions stronglysuggest a preference for swearing in Hindi.7 ConclusionIn this paper, through a large scale empirical studyof nearly half a million tweets, we tried to answera fundamental question regarding multilingualism,namely, is there a preferred language for expressionof sentiment.
We also looked at some of the prag-matic functions of code-switching.
Our results indi-cate a strong preference for using Hindi, L1 for theusers from whom these tweets come, for expressingnegative sentiment, including swearing.
However,we do not observe any particular preference towardsHindi for expressing opinions.Previous linguistic studies (Dewaele, 2004; De-waele, 2010) have already shown a preference forL1 for expressing emotion and swearing.
However,we observe that for expressing positive emotion, En-glish (which would be L2) is the language of pref-erence.
This raises some intriguing socio-linguisticquestions.
Is it the case that English being the lan-guage of aspiration in India, it is preferred for posi-tive expression?
Or is it because Hindi is specificallypreferred for swearing and therefore, is the languageof preference for negative emotion?
How do suchpreferences vary across topics, users and other mul-tilingual communities?
How representative of thesociety is this kind of social media study?
We planto explore some of these questions in the future.Our study also indicates that inferences drawnon multilingual societies by analyzing data in justone language (usually English), which has been thenorm so far, are likely to be incorrect.AcknowledgementKoustav Rudra was supported by a fellowship fromTata Consultancy Services.1139ReferencesPeter Auer.
1995.
The pragmatics of code-switching:a sequential approach.
In Lesley Milroy and PieterMuysken, editors, One speaker, two languages, pages115?135.
Cambridge University Press.Akshat Bakliwal, Piyush Arora, and Vasudeva Varma.2012.
Hindi subjective lexicon : A lexical resource forhindi polarity classification.
In Proc.
LREC, Austin,Texas, USA, May.Kalika Bali, Yogarshi Vyas, Jatin Sharma, and Mono-jit Choudhury.
2014.
?i am borrowing ya mixing?
?an analysis of English-Hindi code mixing in Face-book.
In Proc.
First Workshop on Computational Ap-proaches to Code Switching, EMNLP.Glivia A. R. Barbosa, Wagner Meira Jr, Ismael S. Silva,Raquel O. Prates, Mohammed J. Zaki, and AdrianoVeloso.
2012.
Characterizing the effectiveness oftwitter hashtags to detect and track online populationsentiment.
In Proc.
ACM CHI, Austin, Texas, USA,May.Utsab Barman, Amitava Das, Joachim Wagner, and Jen-nifer Foster.
2014.
Code mixing: A challenge for lan-guage identification in the language of social media.In The 1st Workshop on Computational Approaches toCode Switching, EMNLP 2014.Inma Mun?oa Barredo.
1997.
Pragmatic functionsof code-switching among Basque-Spanish bilinguals.Retrieved on October, 26:528?541.Caroline Brun.
2012.
Learning opinionated patterns forcontextual opinion detection.
In COLING (Posters),pages 165?174.
Citeseer.Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, DanJurafsky, Jure Leskovec, and Christopher Potts.
2013.A computational approach to politeness with applica-tion to social factors.
Proceedings of ACL.Munmun De Choudhury, Michael Gamon, Scott Counts,and Eric Horvitz.
2013.
Predicting depression via so-cial media.
In ICWSM.Jean-Marc Dewaele.
2004.
Blistering barnacles!
Whatlanguage do multilinguals swear in?!
Estudios de So-ciolinguistica, 5:83?105.Jean-Marc Dewaele.
2010.
Emotions in multiple lan-guages.
Palgrave Macmillan, Basingstoke, UK.J.
A. Fishman.
1971.
Sociolinguistics.
Rowley, New-bury, MA.Spandana Gella, Jatin Sharma, and Kalika Bali.
2013.Query word labeling and back transliteration for indianlanguages: Shared task system description.Yael Maschler.
1991.
The language games bilingualsplay: language alternation at language boundaries.Language and communication, 11(2):263?289.Yael Maschler.
1994.
Appreciation ha?araxa ?oha?arasta?
[valuing or admiration].
Negotiating con-trast in bilingual disagreement talk, 14(2):207?238.Lesley Milroy and Pieter Muysken, editors.
1995.
Onespeaker, two languages: Cross-disciplinary perspec-tives on code-switching.
Cambridge University Press.Namita Mittal, Basant Agarwal, Garvit Chouhan, NitinBania, and Prateek Pareek.
2013.
Sentiment analysisof hindi review based on negation and discourse rela-tion.
In proceedings of International Joint Conferenceon Natural Language Processing, pages 45?50.Saif M. Mohammad and Peter D. Turney.
2013.Crowdsourcing a Word-Emotion Association Lexicon.29(3):436?465.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceedings ofthe seventh international workshop on Semantic Eval-uation Exercises (SemEval-2013), Atlanta, Georgia,USA, June.Saif M Mohammad.
2012.
# emotional tweets.
In Pro-ceedings of the First Joint Conference on Lexical andComputational Semantics-Volume 1: Proceedings ofthe main conference and the shared task, and Volume2: Proceedings of the Sixth International Workshop onSemantic Evaluation, pages 246?255.
Association forComputational Linguistics.Miwa Nishimura.
1995.
A functional analysis ofJapanese/English code-switching.
Journal of Prag-matics, 23(2):157?181.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment classification usingmachine learning techniques.
In Proc.
EMNLP, pages79?86.Rana D. Parshad, Suman Bhowmick, Vineeta Chand,Nitu Kumari, and Neha Sinha.
2016.
What is Indiaspeaking?
Exploring the ?Hinglish?
invasion.
PhysicaA, 449:375?389.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.Nanyun Peng, Yiming Wang, and Mark Dredze.2014.
Learning polylingual topic models from code-switched social media documents.
In ACL (2), pages674?679.Ashequl Qadir.
2009.
Detecting opinion sentences spe-cific to product features in customer reviews usingtyped dependency relations.
In Proceedings of theWorkshop on Events in Emerging Text Types, pages38?43.
Association for Computational Linguistics.1140Pujari Rajkumar, Swara Desai, Niloy Ganguly, andPawan Goyal.
2014.
A novel two-stage frameworkfor extracting opinionated sentences from news arti-cles.
TextGraphs-9, page 25.Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,Saif M Mohammad, Alan Ritter, and Veselin Stoy-anov.
2015.
Semeval-2015 task 10: Sentiment analy-sis in twitter.
Proceedings of SemEval-2015.Rosaura Sanchez.
1983.
Chicano discourse.
Rowley,Newbury House.Royal Sequiera, Monojit Choudhury, Parth Gupta,Paolo Rosso, Shubham Kumar, Somnath Banerjee,Sudip Kumar Naskar, Sivaji Bandyopadhyay, GokulChittaranjan, Amitava Das, and Kunal Chakma.
2015.Overview of fire-2015 shared task on mixed script in-formation retrieval.
In Working Notes of FIRE, pages21?27.Raksha Sharma, Pushpak Bhattacharyya, Ultimate Goal,and Hindi Senti Lexicon Statistics.
2015a.
A senti-ment analyzer for hindi using hindi senti lexicon.Shashank Sharma, Pykl Srinivas, and Rakesh ChandraBalabantaray.
2015b.
Text normalization of code mixand sentiment analysis.
In Advances in Computing,Communications and Informatics (ICACCI), 2015 In-ternational Conference on, pages 1468?1473.
IEEE.Thamar Solorio and Yang Liu.
2008.
Part-of-speech tag-ging for english-spanish code-switched text.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 1051?1060.
As-sociation for Computational Linguistics.Thamar Solorio and Yang Liu.
2010.
Learning to PredictCode-Switching Points.
In Proc.
EMNLP.Thamar Solorio, Elizabeth Blair, Suraj Maharjan, StevenBethard, Mona Diab, Mahmoud Gohneim, AbdelatiHawwari, Fahad AlGhamdi, Julia Hirschberg, AlisonChang, et al 2014.
Overview for the first shared taskon language identification in code-switched data.
Pro-ceedings of The First Workshop on Computational Ap-proaches to Code Switching, EMNLP, pages 62?72.Simo Tchokni, D.O.
Se?aghdha, and Daniele Quercia.2014.
Emoticons and phrases: Status symbols in so-cial media.
In Eighth International AAAI Conferenceon Weblogs and Social Media.2015a.
GET help/languages ?
Twitter Developers, 8.2015b.
GET search/tweets ?
Twitter Developers, 8.David Vilares, Miguel A Alonso, and Carlos Go?mez-Rodr?guez.
2015.
Sentiment analysis on monolingual,multilingual and code-switching twitter corpora.
In6th Workshop on Computational Approaches to Sub-jectivity, Sentiment and Social Media Analysis.Svitlana Volkova, Theresa Wilson, and David Yarowsky.2013.
Exploring Sentiment in Social Media: Boot-strapping Subjectivity Clues from Multilingual TwitterStreams.
In Proc.
ACL (Vol2: Short Papers).Soroush Vosoughi and Deb Roy.
2016.
Tweet acts: Aspeech act classifier for twitter.
In Tenth InternationalAAAI Conference on Web and Social Media.Yogarshi Vyas, Spandana Gella, Jatin Sharma, KalikaBali, and Monojit Choudhury.
2014.
POS Taggingof English-Hindi Code-Mixed Social Media Content.In Proc.
EMNLP, pages 974?979.1141
