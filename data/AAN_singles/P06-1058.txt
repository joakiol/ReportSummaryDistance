Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 457?464,Sydney, July 2006. c?2006 Association for Computational LinguisticsAn Equivalent Pseudoword Solution to ChineseWord Sense DisambiguationZhimao Lu+    Haifeng Wang++    Jianmin Yao+++    Ting Liu+    Sheng Li++ Information Retrieval Laboratory, School of Computer Science and Technology,Harbin Institute of Technology, Harbin, 150001, China{lzm, tliu, lisheng}@ir-lab.org++ Toshiba (China) Research and Development Center5/F., Tower W2, Oriental Plaza, No.
1, East Chang An Ave., Beijing, 100738, Chinawanghaifeng@rdc.toshiba.com.cn+++ School of Computer Science and TechnologySoochow University, Suzhou, 215006, Chinajyao@suda.edu.cnAbstractThis paper presents a new approachbased on Equivalent Pseudowords (EPs)to tackle Word Sense Disambiguation(WSD) in Chinese language.
EPs are par-ticular artificial ambiguous words, whichcan be used to realize unsupervised WSD.A Bayesian classifier is implemented totest the efficacy of the EP solution onSenseval-3 Chinese test set.
The per-formance is better than state-of-the-artresults with an average F-measure of 0.80.The experiment verifies the value of EPfor unsupervised WSD.1 IntroductionWord sense disambiguation (WSD) has been ahot topic in natural language processing, which isto determine the sense of an ambiguous word ina specific context.
It is an important techniquefor applications such as information retrieval,text mining, machine translation, text classifica-tion, automatic text summarization, and so on.Statistical solutions to WSD acquire linguisticknowledge from the training corpus using ma-chine learning technologies, and apply theknowledge to disambiguation.
The first statisticalmodel of WSD was built by Brown et al (1991).Since then, most machine learning methods havebeen applied to WSD, including decision tree,Bayesian model, neural network, SVM, maxi-mum entropy, genetic algorithms, and so on.
Fordifferent learning methods, supervised methodsusually achieve good performance at a cost ofhuman tagging of training corpus.
The precisionimproves with larger size of training corpus.Compared with supervised methods, unsuper-vised methods do not require tagged corpus, butthe precision is usually lower than that of thesupervised methods.
Thus, knowledge acquisi-tion is critical to WSD methods.This paper proposes an unsupervised methodbased on equivalent pseudowords, which ac-quires WSD knowledge from raw corpus.
Thismethod first determines equivalent pseudowordsfor each ambiguous word, and then uses theequivalent pseudowords to replace the ambigu-ous word in the corpus.
The advantage of thismethod is that it does not need parallel corpus orseed corpus for training.
Thus, it can use a large-scale monolingual corpus for training to solvethe data-sparseness problem.
Experimental re-sults show that our unsupervised method per-forms better than the supervised method.The remainder of the paper is organized as fol-lows.
Section 2 summarizes the related work.Section 3 describes the conception of EquivalentPseudoword.
Section 4 describes EP-based Un-supervised WSD Method and the evaluation re-sult.
The last section concludes our approach.2 Related WorkFor supervised WSD methods,  a knowledge ac-quisition bottleneck is to prepare the manually457tagged corpus.
Unsupervised method is an alter-native, which often involves automatic genera-tion of tagged corpus, bilingual corpus alignment,etc.
The value of unsupervised methods lies inthe knowledge acquisition solutions they adopt.2.1 Automatic Generation of Training CorpusAutomatic corpus tagging is a solution to WSD,which generates large-scale corpus from a smallseed corpus.
This is a weakly supervised learningor semi-supervised learning method.
This rein-forcement algorithm dates back to Gale et al(1992a).
Their investigation was based on a 6-word test set with 2 senses for each word.Yarowsky (1994 and 1995), Mihalcea andMoldovan (2000), and Mihalcea (2002) havemade further research to obtain large corpus ofhigher quality from an initial seed corpus.
Asemi-supervised method proposed by Niu et al(2005) clustered untagged instances with taggedones starting from a small seed corpus, whichassumes that similar instances should have simi-lar tags.
Clustering was used instead of boot-strapping and was proved more efficient.2.2 Method Based on Parallel CorpusParallel corpus is a solution to the bottleneck ofknowledge acquisition.
Ide et al (2001 and2002), Ng et al (2003), and Diab (2003, 2004a,and 2004b) made research on the use of align-ment for WSD.Diab and Resnik (2002) investigated the feasi-bility of automatically annotating large amountsof data in parallel corpora using an unsupervisedalgorithm, making use of two languages simulta-neously, only one of which has an availablesense inventory.
The results showed that word-level translation correspondences are a valuablesource of information for sense disambiguation.The method by Li and Li (2002) does not re-quire parallel corpus.
It avoids the alignmentwork and takes advantage of bilingual corpus.In short, technology of automatic corpus tag-ging is based on the manually labeled corpus.That is to say, it still need human interventionand is not a completely unsupervised method.Large-scale parallel corpus; especially word-aligned corpus is highly unobtainable, which haslimited the WSD methods based on parallel cor-pus.3 Equivalent PseudowordThis section describes how to obtain equivalentpseudowords without a seed corpus.Monosemous words are unambiguous prioriknowledge.
According to our statistics, they ac-count for 86%~89% of the instances in a diction-ary and 50% of the items in running corpus, theyare potential knowledge source for WSD.A monosemous word is usually synonymousto some polysemous words.
For example thewords "??
, ??
, ??
??
??
?
?, , , ,??"
has similar meaning as one of the sensesof the ambiguous word "??
", while "?
?, ?
?, ??
??
?
?, , ??
??
??
?
?, , , , ,??
??
??
?
?, , , " are the same for "??
".This is quite common in Chinese, which can beused as a knowledge source for WSD.3.1 Definition of Equivalent PseudowordIf the ambiguous words in the corpus are re-placed with its synonymous monosemous word,then is it convenient to acquire knowledge fromraw corpus?
For example in table 1, the ambigu-ous word "??"
has three senses, whose syn-onymous monosemous words are listed on theright column.
These synonyms contain some in-formation for disambiguation task.An artificial ambiguous word can be coinedwith the monosemous words in table 1.
Thisprocess is similar to the use of general pseu-dowords (Gale et al, 1992b; Gaustad, 2001; Na-kov and Hearst, 2003), but has some essentialdifferences.
This artificial ambiguous word needto simulate the function of the real ambiguousword, and to acquire semantic knowledge as thereal ambiguous word does.
Thus, we call it anequivalent pseudoword (EP) for its equivalencewith the real ambiguous word.
It's apparent thatthe equivalent pseudoword has provided a newway to unsupervised WSD.S1 ??/??
?S2 ??/??/??/??/????
(ba3 wo4)S3 ??/??/??/??/?
?Table 1.
Synonymous Monosemous Words forthe Ambiguous Word "??
"The equivalence of the EP with the real am-biguous word is a kind of semantic synonym orsimilarity, which demands a maximum similaritybetween the two words.
An ambiguous word hasthe same number of EPs as of senses.
Each EP'ssense maps to a sense of ambiguous word.The semantic equivalence demands furtherequivalence at each sense level.
Every corre-458sponding sense should have the maximum simi-larity, which is the strictest limit to the construc-tion of an EP.The starting point of unsupervised WSD basedon EP is that EP can substitute the original wordfor knowledge acquisition in model training.Every instance of each morpheme of the EP canbe viewed as an instance of the ambiguous word,thus the training set can be enlarged easily.
EP isa solution to data sparseness for lack of humantagging in WSD.3.2 Basic Assumption for EP-based WSDIt is based on the following assumptions that EPscan substitute the original ambiguous word forknowledge acquisition in WSD model training.Assumption 1: Words of the same meaningplay the same role in a language.
The sense is animportant attribute of a word.
This plays as thebasic assumption in this paper.Assumption 2: Words of the same meaningoccur in similar context.
This assumption iswidely used in semantic analysis and plays as abasis for much related research.
For example,some researchers cluster the contexts of ambigu-ous words for WSD, which shows good perform-ance (Schutze, 1998).Because an EP has a higher similarity with theambiguous word in syntax and semantics, it is auseful knowledge source for WSD.3.3 Design and Construction of EPsBecause of the special characteristics of EPs, it'smore difficult to construct an EP than a generalpseudo word.
To ensure the maximum similaritybetween the EP and the original ambiguous word,the following principles should be followed.1) Every EP should map to one and only oneoriginal ambiguous word.2) The morphemes of an EP should map oneby one to those of the original ambiguous word.3) The sense of the EP should be the same asthe corresponding ambiguous word, or has themaximum similarity with the word.4) The morpheme of a pseudoword stands fora sense, while the sense should consist of one ormore morphemes.5) The morpheme should be a monosemousword.The fourth principle above is the biggest dif-ference between the EP and a general pseudoword.
The sense of an EP is composed of one orseveral morphemes.
This is a remarkable featureof the EP, which originates from its equivalentlinguistic function with the original word.
Toconstruct the EP, it must be ensured that thesense of the EP maps to that of the original word.Usually, a candidate monosemous word for amorpheme stands for part of the linguistic func-tion of the ambiguous word, thus we need tochoose several morphemes to stand for one sense.The relatedness of the senses refers to thesimilarity of the contexts of the original ambigu-ous word and its EP.
The similarity between thewords means that they serve as synonyms foreach other.
This principle demands that both se-mantic and pragmatic information should betaken into account in choosing a morpheme word.3.4 Implementation of the EP-based SolutionAn appropriate machine-readable dictionary isneeded for construction of the EPs.
A Chinesethesaurus is adopted and revised to meet this de-mand.Extended Version of TongYiCiCiLinTo extend the TongYiCiCiLin (Cilin) to holdmore words, several linguistic resources areadopted for manually adding new words.
An ex-tended version of the Cilin is achieved, whichincludes 77,343 items.A hierarchy of three levels is organized in theextended Cilin for all items.
Each node in thelowest level, called a minor class, contains sev-eral words of the same class.
The words in oneminor class are divided into several groups ac-cording to their sense similarity and relatedness,and each group is further divided into severallines, which can be viewed as the fifth level ofthe thesaurus.
The 5-level hierarchy of the ex-tended Cilin is shown in figure 1.
The lower thelevel is, the more specific the sense is.
The fifthlevel often contains a few words or only oneword, which is called an atom word group, anatom class or an atom node.
The words in thesame atom node hold the smallest semantic dis-tance.From the root node to the leaf node, the senseis described more and more detailed, and thewords in the same node are more and more re-lated.
Words in the same fifth level node havethe same sense and linguistic function, whichensures that they can substitute for each otherwithout leading to any change in the meaning ofa sentence.459?
????
???
?
?
??
?
??
?
?
?Level 1Level 2Level 3Level 4Level 5?
?Figure 1.
Organization of Cilin (extended)The extended version of extended Cilin isfreely downloadable from the Internet and hasbeen used by over 20 organizations in the world1.Construction of EPsAccording to the position of the ambiguous word,a proper word is selected as the morpheme of theEP.
Almost every ambiguous word has its corre-sponding EP constructed in this way.The first step is to decide the position of theambiguous word starting from the leaf node ofthe tree structure.
Words in the same leaf nodeare identical or similar in the linguistic functionand word sense.
Other words in the leaf node ofthe ambiguous word are called brother words ofit.
If there is a monosemous brother word, it canbe taken as a candidate morpheme for the EP.
Ifthere does not exist such a brother word, trace tothe fourth level.
If there is still no monosemousbrother word in the fourth level, trace to the thirdlevel.
Because every node in the third level con-tains many words, candidate morpheme for theambiguous can usually be found.In most cases, candidate morphemes can befound at the fifth level.
It is not often necessaryto search to the fourth level, less to the third.
Ac-cording to our statistics, the extended Cilin con-tains about monosemous words for 93% of theambiguous words in the fifth level, and 97% inthe fourth level.
There are only 112 ambiguouswords left, which account for the other 3% andmainly are functional words.
Some of the 3%words are rarely used, which cannot be found ineven a large corpus.
And words that lead to se-mantic misunderstanding are usually contentwords.
In WSD research for English, only nouns,verbs, adjectives and adverbs are considered.1 It is located at http://www.ir-lab.org/.From this aspect, the extended version of Cilinmeets our demand for the construction of EPs.If many monosemous brother words are foundin the fourth or third level, there are many candi-date morphemes to choose from.
A further selec-tion is made based on calculation of sense simi-larity.
More similar brother words are chosen.Computing of EPsGenerally, several morpheme words are neededfor better construction of an EP.
We assume thatevery morpheme word stands for a specific senseand does not influence each other.
It is morecomplex to construct an EP than a commonpseudo word, and the formulation and statisticalinformation are also different.An EP is described as follows:iikiiiikkWWWWSWWWWSWWWWSLMMMMMMLL,,,:,,,:,,,:321223222121131211121WEP?????????
?Where WEP is the EP word, Si is a sense of theambiguous word, and Wik is a morpheme word ofthe EP.The statistical information of the EP is calcu-lated as follows:1?
stands for the frequency of the S)( iSC i :?=kiki WCSC )()(2?
stands for the co-occurrence fre-quency of S),( fi WSCi and the contextual word Wf :?=kfikfi WWCWSC ),(),(460Ambiguous word citation (Qin and Wang, 2005) Ours Ambiguous wordcitation (Qin andWang, 2005) Ours??
(ba3 wo4) 0.56 0.87 ??
(mei2 you3) 0.75 0.68?
(bao1) 0.59 0.75 ??
(qi3 lai2) 0.82 0.54??
(cai2 liao4) 0.67 0.79 ?
(qian2) 0.75 0.62??
(chong1 ji1) 0.62 0.69 ??
(ri4 zi3) 0.75 0.68?
(chuan1) 0.80 0.57 ?
(shao3) 0.69 0.56??
(di4 fang1) 0.65 0.65 ??
(tu1 chu1) 0.82 0.86??
(fen1 zi3) 0.91 0.81 ??
(yan2 jiu1) 0.69 0.63??
(yun4 dong4) 0.61 0.82 ??
(huo2 dong4) 0.79 0.88?
(lao3) 0.59 0.50 ?
(zou3) 0.72 0.60?
(lu4) 0.74 0.64 ?
(zuo4) 0.90 0.73Average 0.72 0.69 Note: Average of the 20 wordsTable 2.
The F-measure for the Supervised WSD4 EP-based Unsupervised WSD MethodEP is a solution to the semantic knowledge ac-quisition problem, and it does not limit thechoice of statistical learning methods.
All of themathematical modeling methods can be appliedto EP-based WSD methods.
This section focuseson the application of the EP concept to WSD,and chooses Bayesian method for the classifierconstruction.4.1 A Sense Classifier Based on the Bayes-ian ModelBecause the model acquires knowledge from theEPs but not from the original ambiguous word,the method introduced here does not need humantagging of training corpus.In the training stage for WSD, statistics of EPsand context words are obtained and stored in adatabase.
Senseval-3 data set plus unsupervisedlearning method are adopted to investigate intothe value of EP in WSD.
To ensure the compara-bility of experiment results, a Bayesian classifieris used in the experiments.Bayesian ClassifierAlthough the Bayesian classifier is simple, it isquite efficient, and it shows good performanceon WSD.The Bayesian classifier used in this paper isdescribed in (1)???????
?+= ??
ijkcvkjkSi SvPSPwS )|(log)(logmaxarg)( (1)Where wi is the ambiguous word,  is theoccurrence probability of the sense S)( kSPk,is the conditional probability of the context wordv)|( kj SvPj, and ci is the set of the context words.To simplify the experiment process, the NaiveBayesian modeling is adopted for the sense clas-sifier.
Feature selection and ensemble classifica-tion are not applied, which is both to simplify thecalculation and to prove the effect of EPs inWSD.Experiment Setup and ResultsThe Senseval-3 Chinese ambiguous words aretaken as the testing set, which includes 20 words,each with 2-8 senses.
The data for the ambiguouswords are divided into a training set and a testingset by a ratio of 2:1.
There are 15-20 traininginstances for each sense of the words, and occursby the same frequency in the training and test set.Supervised WSD is first implemented usingthe Bayesian model on the Senseval-3 data set.With a context window of (-10, +10), the opentest results are shown in table 2.The F-measure in table 2 is defined in (2).RPRPF +?
?= 2  (2)461Where P and R refer to the precision and recallof the sense tagging respectively, which are cal-culated as shown in (3) and (4))tagged()correct(CCP =  (3))all()correct(CCR =  (4)Where C(tagged) is the number of tagged in-stances of senses, C(correct) is the number ofcorrect tags, and C(all) is the number of tags inthe gold standard set.
Every sense of the am-biguous word has a P value, a R value and a Fvalue.
The F value in table 2 is a weighted aver-age of all the senses.In the EP-based unsupervised WSD experi-ment, a 100M corpus (People's Daily for year1998) is used for the EP training instances.
TheSenseval-3 data is used for the test.
In our ex-periments, a context window of (-10, +10) istaken.
The detailed results are shown in table 3.4.2 Experiment Analysis and DiscussionExperiment Evaluation MethodTwo evaluation criteria are used in the experi-ments, which are the F-measure and precision.Precision is a usual criterion in WSD perform-ance analysis.
Only in recent years, the precision,recall, and F-measure are all taken to evaluatethe WSD performance.In this paper, we will only show the f-measurescore because it is a combined score of precisionand recall.Result Analysis on Bayesian Supervised WSDExperimentThe experiment results in table 2 reveals that theresults of supervised WSD and those of (Qin andWang, 2005) are different.
Although they are allbased on the Bayesian model, Qin and Wang(2005) used an ensemble classifier.
However, thedifference of the average value is not remarkable.As introduced above, in the supervised WSDexperiment, the various senses of the instancesare evenly distributed.
The lower bound as Galeet al (1992c) suggested should be very low andit is more difficult to disambiguate if there aremore senses.
The experiment verifies this reason-ing, because the highest F-measure is less than90%, and the lowest is less than 60%, averagingabout 70%.With the same number of senses and the samescale of training data, there is a big differencebetween the WSD results.
This shows that otherfactors exist which influence the performanceother than the number of senses and training datasize.
For example, the discriminability among thesenses is an important factor.
The WSD task be-comes more difficult if the senses of the ambigu-ous word are more similar to each other.Experiment Analysis of the EP-based WSDThe EP-based unsupervised method takes thesame open test set as the supervised method.
Theunsupervised method shows a better performance,with the highest F-measure score at 100%, low-est at 59% and average at 80%.
The resultsshows that EP is useful in unsupervised WSD.SequenceNumber Ambiguous word F-measureSequenceNumber Ambiguous wordF-measure(%)1 ??
(ba3 wo4) 0.93 11 ??
(mei2 you3) 1.002 ?
(bao1) 0.74 12 ??
(qi3 lai2) 0.593 ?
(cai2 liao4) 0.80 13 ?
(qian2) 0.714 ??
(chong1 ji1) 0.85 14 ??
(ri4 zi3) 0.625 ?
(chuan1) 0.79 15 ?
(shao3) 0.826 ??
(di4 fang1) 0.78 16 ??
(tu1 chu1) 0.937 ??
(fen1 zi3) 0.94 17 ??
(yan2 jiu1) 0.718 ??
(yun4dong4)0.94 18 ??
(huo2 dong4) 0.899 ?
(lao3) 0.85 19 ?
(zou3) 0.6810 ?
(lu4) 0.81 20 ?
(zuo4) 0.67Average 0.80 Note: Average of the 20 wordsTable 3.
The Results for Unsupervised WSD based on EPs462From the results in table 2 and table 3, it canbe seen that 16 among the 20 ambiguous wordsshow better WSD performance in unsupervisedSWD than in supervised WSD, while only 2 ofthem shows similar results and 2 performs worse .The average F-measure of the unsupervisedmethod is higher by more than 10%.
The reasonlies in the following aspects:1) Because there are several morpheme wordsfor every sense of the word in construction of theEP, rich semantic information can be acquired inthe training step and is an advantage for sensedisambiguation.2) Senseval-3 has provided a small-scale train-ing set, with 15-20 training instances for eachsense, which is not enough for the WSD model-ing.
The lack of training information leads to alow performance of the supervised methods.3) With a large-scale training corpus, the un-supervised WSD method has got plenty of train-ing instances for a high performance in disam-biguation.4) The discriminability of some ambiguousword may be low, but the corresponding EPscould be easier to disambiguate.
For example,the ambiguous word "?"
has two senses whichare difficult to distinguish from each other, butits Eps' senses of "??/??/??"
and "?/?/?/?
"can be easily disambiguated.
It is the samefor the word "??
", whose Eps' senses are "??/??
/??"
and "??/??".
EP-basedknowledge acquisition of these ambiguous wordsfor WSD has helped a lot to achieve high per-formance.5 ConclusionAs discussed above, the supervised WSD methodshows a low performance because of its depend-ency on the size of the training data.
This revealsits weakness in knowledge acquisition bottleneck.EP-based unsupervised method has overcamethis weakness.
It requires no manually taggedcorpus to achieve a satisfactory performance onWSD.
Experimental results show that EP-basedmethod is a promising solution to the large-scaleWSD task.
In future work, we will examine theeffectiveness of EP-based method in other WSDtechniques.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1991.
Word-Sense Disambiguation Using Statistical Methods.In Proc.
of the 29th Annual Meeting of the Associa-tion for Computational Linguistics (ACL-1991),pages 264-270.Mona Talat Diab.
2003.
Word Sense DisambiguationWithin a Multilingual Framework.
PhD thesis,University of Maryland College Park.Mona Diab.
2004a.
Relieving the Data AcquisitionBottleneck in Word Sense Disambiguation.
In Proc.of the 42nd Annual Meeting of the Association forComputational Linguistics (ACL-2004), pages 303-310.Mona T. Diab.
2004b.
An Unsupervised Approach forBootstrapping Arabic Sense Tagging.
In Proc.
ofArabic Script Based Languages Workshop at COL-ING 2004, pages 43-50.Mona Diab and Philip Resnik.
2002.
An Unsuper-vised Method for Word Sense Tagging Using Par-allel Corpora.
In Proc.
of the 40th Annual Meetingof the Association for Computational Linguistics(ACL-2002), pages 255-262.William Gale, Kenneth Church, and David Yarowsky.1992a.
Using Bilingual Materials to Develop WordSense Disambiguation Methods.
In Proc.
of the 4thInternational Conference on Theoretical and Meth-odolgical Issues in Machine Translation(TMI-92),pages 101-112.William Gale, Kenneth Church, and David Yarowsky.1992b.
Work on Statistical Methods for WordSense Disambiguation.
In Proc.
of AAAI Fall Sym-posium on Probabilistic Approaches to NaturalLanguage, pages 54-60.William Gale, Kenneth Ward Church, and DavidYarowsky.
1992c.
Estimating Upper and LowerBounds on the Performance of Word Sense Disam-biguation Programs.
In Proc.
of the 30th AnnualMeeting of the Association for Computational Lin-guistics (ACL-1992), pages 249-256.Tanja Gaustad.
2001.
Statistical Corpus-Based WordSense Disambiguation: Pseudowords vs. Real Am-biguous Words.
In Proc.
of the 39th ACL/EACL,Student Research Workshop, pages 61-66.Nancy Ide, Tomaz Erjavec, and Dan Tufi?.
2001.Automatic Sense Tagging Using Parallel Corpora.In Proc.
of the Sixth Natural Language ProcessingPacific Rim Symposium, pages 83-89.Nancy Ide, Tomaz Erjavec, and Dan Tufis.
2002.Sense Discrimination with Parallel Corpora.
InWorkshop on Word Sense Disambiguation: RecentSuccesses and Future Directions, pages 54-60.Cong Li and Hang Li.
2002.
Word Translation Dis-ambiguation Using Bilingual Bootstrapping.
InProc.
of the 40th Annual Meeting of the Association463for Computational Linguistics (ACL-2002), pages343-351.Rada Mihalcea and Dan Moldovan.
2000.
An IterativeApproach to Word Sense Disambiguation.
In Proc.of Florida Artificial Intelligence Research SocietyConference (FLAIRS 2000), pages 219-223.Rada F. Mihalcea.
2002.
Bootstrapping Large SenseTagged Corpora.
In Proc.
of the 3rd InternationalConference on Languages Resources and Evalua-tions (LREC 2002), pages 1407-1411.Preslav I. Nakov and Marti A. Hearst.
2003.
Cate-gory-based Pseudowords.
In Companion Volume tothe Proceedings of HLT-NAACL 2003, Short Pa-pers, pages 67-69.Hwee Tou.
Ng, Bin Wang, and Yee Seng Chan.
2003.Exploiting Parallel Texts for Word Sense Disam-biguation: An Empirical Study.
In Proc.
of the 41stAnnual Meeting of the Association for Computa-tional Linguistics (ACL-2003), pages 455-462.Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.2005.
Word Sense Disambiguation Using LabelPropagation Based Semi-Supervised Learning.
InProc.
of the 43th Annual Meeting of the Associationfor Computational Linguistics (ACL-2005), pages395-402.Ying Qin and Xiaojie Wang.
2005.
A Track-basedMethod on Chinese WSD.
In Proc.
of Joint Sympo-sium of Computational Linguistics of China (JSCL-2005), pages 127-133.Hinrich.
Schutze.
1998.
Automatic Word Sense Dis-crimination.
Computational Linguistics, 24(1): 97-123.David Yarowsky.
1994.
Decision Lists for LexicalAmbiguity Resolution: Application to Accent Res-toration in Spanish and French.
In Proc.
of the 32ndAnnual Meeting of the Association for Computa-tional Linguistics(ACL-1994), pages 88-95.David Yarowsky.
1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Methods.
InProc.
of the 33rd Annual Meeting of the Associationfor Computational Linguistics (ACL-1995), pages189-196.464
