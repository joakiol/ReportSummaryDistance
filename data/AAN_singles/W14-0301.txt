Workshop on Humans and Computer-assisted Translation, pages 1?9,Gothenburg, Sweden, 26 April 2014.c?2014 Association for Computational LinguisticsWord Confidence Estimation for SMT N-best List Re-rankingNgoc-Quang Luong Laurent BesacierLIG, Campus de Grenoble41, Rue des Math?ematiques,UJF - BP53, F-38041 Grenoble Cedex 9, France{ngoc-quang.luong,laurent.besacier,benjamin.lecouteux}@imag.frBenjamin LecouteuxAbstractThis paper proposes to use Word Confi-dence Estimation (WCE) information toimprove MT outputs via N-best list re-ranking.
From the confidence label as-signed for each word in the MT hypoth-esis, we add six scores to the baseline log-linear model in order to re-rank the N-bestlist.
Firstly, the correlation between theWCE-based sentence-level scores and theconventional evaluation scores (BLEU,TER, TERp-A) is investigated.
Then, theN-best list re-ranking is evaluated over dif-ferent WCE system performance levels:from our real and efficient WCE system(ranked 1st during last WMT 2013 QualityEstimation Task) to an oracle WCE (whichsimulates an interactive scenario where auser simply validates words of a MT hy-pothesis and the new output will be auto-matically re-generated).
The results sug-gest that our real WCE system slightly (butsignificantly) improves the baseline whilethe oracle one extremely boosts it; and bet-ter WCE leads to better MT quality.1 IntroductionA number of methods to improve MT hypothe-ses after decoding have been proposed in the past,such as: post-editing, re-ranking or re-decoding.Post-editing (Parton et al., 2012) is a human-inspired task where the machine post edits trans-lations in a second automatic pass.
In re-ranking(Zhang et al., 2006; Duh and Kirchhoff, 2008;Bach et al., 2011), more features are used alongwith the multiple model scores for re-determiningthe 1-best among N-best list.
Meanwhile, re-decoding process (Venugopal et al., 2007) inter-venes directly into the decoder?s search graph (e.g.adds more reward or penalty scores), driving it toanother better path.This work aims at re-ranking the N-best list to im-prove MT quality.
Generally, during the transla-tion task, the decoder traverses through paths inits search space, computes the objective functionvalues for them and outputs the one with high-est score as the best hypothesis.
Besides, thosewith lower scores can also be generated in a so-called N-best list.
The decoder?s function consistsof parameters from different models, such as trans-lation, distortion, word penalties, reordering, lan-guage models, etc.
In the N-best list, although thecurrent 1-best beats the remains in terms of modelscore, it might not be exactly the closest to the hu-man reference.
Therefore, adding more decoderindependent features would be expected to raiseup a better candidate.
In this work, we build sixadditional features based on the labels predictedby our Word Confidence Estimation (WCE) sys-tem, then integrate them with the existing decoderscores for re-ranking hypotheses in the N-bestlist.
More precisely, in the second pass, our re-ranker aggregates over decoder and WCE-basedweighted scores and utilizes the obtained sum tosort out the best candidate.
The novelty of this pa-per lies on the following contributions: the corre-lation between WCE-based sentence-level scoresand conventional evaluation scores (BLEU, TER,TERp-A) is first investigated.
Then, we conductthe N-best list re-ranking over different WCE sys-tem performance levels: starting by a real WCE,passing through several gradually improved (sim-ulated) systems and finally the ?oracle?
one.
Fromthese in-depth experiments, the role of WCE inimproving MT quality via re-ranking N-best listis confirmed and reinforced.The remaining parts of this article are organizedas follows: in section 2 we summarize some out-standing approaches in N-best list re-ranking aswell as in WCE.
Section 3 describes our WCE sys-tem construction, followed by proposed features.1The experiments along with results and in-depthanalysis of WCE scores?
contribution (as WCEsystem gets better) are presented in Section 4 andSection 5.
The last section concludes the paperand points out some ongoing work.2 Related Work2.1 N-best List Re-rankingWalking through various related work concern-ing this issue, we observe some prominent ideas.The first attempt focuses on proposing additionalLanguage Models.
Kirchhoff and Yang (2005)train one word-based 4-gram model (with modi-fied Kneser-Ney smoothing) and one factored tri-gram one, then combine them with seven decoderscores for re-ranking N-best lists of several SMTsystems.
Their proposed LMs increase the transla-tion quality of the baselines (measured by BLEUscore) from 21.6 to 22.0 (Finnish - English), orfrom 30.5 to 31.0 (Spanish - English).
Meanwhile,Zhang et al.
(2006) experiment a distributed LMwhere each server, among the total of 150, hosts aportion of the data and responses its client, allow-ing them to exploit an extremely large corpus (2.7billion word English Gigaword) for estimating N-gram probability.
The quality of their Chinese- English hypotheses after the re-scoring processby using this LM is improved 4.8% (from BLEU31.44 to 32.64, oracle score = 37.48).In one other direction, several authors propose toreplace the current linear scoring function used bythe decoder by more efficient functions.
Sokolovet al.
(2012) learn their non-linear scoring functionin a learning-to-rank paradigm, applying Boostingalgorithm.
Their gains on the WMT?
{10, 11, 12}are shown modest yet consistent and higher thanthose based on linear scoring functions.
Duh andKirchhoff (2008) use Minimum Error Rate Train-ing (MERT) (Och, 2003) as a weak learner andbuild their own solution, BoostedMERT, a highly-expressive re-ranker created by voting among mul-tiple MERT ones.
Their proposed model dramat-ically beats the decoder?s log-linear model (43.7vs.
42.0 BLEU) in IWSLT 2007 Arabic - Englishtask.
Applying solely goodness (the sentence con-fidence) scores, Bach et al.
(2011) obtain very con-sistent TER reductions (0.7 and 0.6 on the dev andtest set) after a 5-list re-ranking for their Arabic -English SMT hypotheses.
This latter work is theone that is the most related to our paper.
However,the major differences are: (1) our proposed sen-tence scores are computed based on word confi-dence labels; and (2) we perform an in-depth studyof the use of WCE for N-best reranking and assessits usefulness in a simulated interactive scenario.2.2 Word Confidence EstimationConfidence Estimation (CE) is the task of iden-tifying the correct parts and detecting the trans-lation errors in MT output.
If the error is pre-dicted for each word, this becomes WCE.
The in-teresting uses of WCE include: pointing out thewords that need to be corrected by the post-editor,telling readers about the reliability of a specificportion, and selecting the best segments amongoptions from multiple translation systems for com-bination.Dealing with this problem, various approacheshave been proposed: Blatz et al.
(2003) combineseveral features using neural network and naiveBayes learning algorithms.
One of the most ef-fective feature combinations is the Word PosteriorProbability (WPP) as suggested by Ueffing et al.
(2003) associated with IBM-model based features(Blatz et al., 2004).
Ueffing and Ney (2005)propose an approach for phrase-based translationmodels: a phrase is a sequence of contiguouswords and is extracted from the word-alignedbilingual training corpus.
The confidence valueof each word is then computed by summing overall phrase pairs in which the target part containsthis word.
Xiong et al.
(2010) integrate targetword?s Part-Of-Speech (POS) and train them byMaximum Entropy Model, allowing significativegains in comparison to WPP features.
The novelfeatures from source side, alignment context, anddependency structure (Bach et al., 2011) help toaugment marginally in F-score as well as the Pear-son correlation with human judgment.
Other ap-proaches are based on external features (Soricutand Echihabi, 2010; Felice and Specia, 2012) al-lowing to cope with various MT systems (e.g.
sta-tistical, rule based etc.).
Among the numerousWCE applications, we consider its contribution ina specific step of SMT pipeline: N-best list re-ranking.
Our WCE system and the proposed re-ranking features are presented in the next section.3 Our ApproachOur approach can be expressed in three steps: in-vestigate the potential of using word-level score inN-best list re-ranking, build the WCE system and2extract additional features to integrate with the ex-isting log-linear model.3.1 Investigating the correlation between?word quality?
scores and other metricsFirstly, we investigate the correlation betweensentence-level scores (obtained from WCE labels)and conventional evaluation scores (BLEU (Pa-pineni et al., 2002), TER and TERp-A (Snoveret al., 2008)).
For each sentence, a word qualityscore (WQS) is calculated by:WQS =#??G??
(good) words#words(1)In other words, we are trying to answer the fol-lowing question: can the high percentage of ?G?
(good) words (predicted by WCE system) in aMT output ensure its possibility of having a betterBLEU and low TER (TERp-A) value ?
This inves-tigation is a strong prerequisite for further exper-iments in order to check that WCE scores do notbring additional ?noise?
to the re-ranking process.In this experiment, we compute WQS over our en-tire French - English data set (total of 10,881 1-best translations) for which WCE oracle labels areavailable (see Section 3.2 to see how they were ob-tained).
The results are plotted in Figure 1, wherethe y axis shows the ?G?
(good) word percent-age, and the x axis shows BLEU (1a), TER (1b) orTERp-A (1c) scores.
It can be seen from Figure 1that the major parts of points (the densest areas) inall three cases conform the common tendency: InFigure 1a, the higher ?G?
percentage, the higherBLEU is; on the contrary, in Figure 1b (Figure1c), the higher ?G?
percentage, the lower TER(TERp-A) is.
We notice some outliers, i.e.
sen-tences with most or almost words labeled ?good?,yet still have low BLEU or high TER (TERp-A)scores.
This phenomenon is to be expected whenmany (unknown) source words are not translatedor when the (unique) reference is simply too farfrom the hypothesis.
Nevertheless, the informa-tion extracted from oracle WCE labels seems use-ful to build an efficient re-ranker.3.2 WCE System PreparationEssentially, a WCE system construction consistsof two pivotal elements: the features (the SMTsystem dependent or independent informationextracted for each word to represent its char-acteristics) and the machine learning method(to train the prediction model).
MotivatedFigure 1: The correlation between WQS in a sen-tence and its overall quality measured by : (a)BLEU, (b) TER and (c) TERp-A metricsby the idea of addressing WCE problem asa sequence labeling process, we employ theConditional Random Fields (CRFs) for our modeltraining, with WAPITI toolkit (Lavergne et al.,2010).
Basically, CRF computes the probabil-ity of the output sequence Y = (y1, y2, ..., yN)given the input sequenceX = (x1, x2, ..., xN) by:3p?
(Y |X) =1Z?
(X)exp{K?k=1?kFk(X,Y )}(2)where Fk(X,Y ) =?Tt=1fk(yt?1, yt, xt);{fk} (k = 1,K) is a set of feature functions;{?k} (k = 1,K) are the associated parameter val-ues; and Z?
(x) is the normalization function.In terms of features, a number of knowledgesources are employed for extracting them, result-ing in the major types listed below.
We brieflysummarize them in this work, further details abouttotal of 25 features can be referred in (Luong et al.,2013a).?
Target Side: target word; bigram (trigram)backward sequences; number of occurrences?
Source Side: source word(s) aligned to thetarget word?
Alignment Context: the combinations of thetarget (source) word and all aligned source(target) words in the window ?2?
Word posterior probability?
Pseudo-reference (Google Translate):whether the current word appears in thepseudo reference or not1??
Graph topology: number of alternative pathsin the confusion set, maximum and minimumvalues of posterior probability distribution?
Language model (LM) based: length of thelongest sequence of the current word and itsprevious ones in the target (resp.
source) LM.For example, with the target word wi: if thesequence wi?2wi?1wiappears in the targetLM but the sequence wi?3wi?2wi?1widoesnot, the n-gram value for wiwill be 3.?
Lexical Features: word?s Part-Of-Speech(POS); sequence of POS of all its alignedsource words; POS bigram (trigram) back-ward sequences; punctuation; proper name;numerical?
Syntactic Features: Null link; constituent la-bel; depth in the constituent tree?
Semantic Features: number of word senses inWordNet.Interestingly, this feature set was also used in ourEnglish - Spanish WCE system which got the first1This is our first-time experimented feature and does notappear in (Luong et al., 2013a)rank in WMT 2013 Quality Estimation SharedTask (Luong et al., 2013b).For building the WCE training and test sets, weuse a dataset of 10,881 French sentences (Potetet al., 2012) , and apply a baseline SMT systemto generate hypotheses (1000-best list).
Our base-line SMT system (presented for WMT 2010 eval-uation) keeps the Moses?s default setting (Koehnet al., 2007): log-linear model with 14 weightedfeature functions.
The translation model is trainedon the Europarl and News parallel corpora ofWMT102evaluation campaign (1,638,440 sen-tences).
The target language model is trained bythe SRI language modeling toolkit (Stolcke, 2002)on the news monolingual corpus (48,653,884 sen-tences).Translators were then invited to correct MT out-puts, giving us the same amount of post editions(Potet et al., 2012).
The set of triples (source,hypothesis, post edition) is then divided into thetraining set (10000 first triples) and test set (881remaining).
To train the WCE model, we ex-tract all above features for words of the 1-best hy-potheses of the training set.
For the test set, thefeatures are built for all 1000 best translations ofeach source sentence.
Another essential elementis the word?s confidence labels (or so-called WCEoracle labels) used to train the prediction modelas well as to judge the WCE results.
They areset by using TERp-A toolkit (Snover et al., 2008)in one of the following classes: ?I?
(insertions),?S?
(substitutions), ?T?
(stem matches), ?Y?
(syn-onym matches), ?P?
(phrasal substitutions), ?E?
(exact matches) and then simplified into binaryclass: ?G?
(good word) or ?B?
(bad word) (Lu-ong et al., 2013a).Once having the prediction model built with allfeatures, we apply it on the test set (881 x 1000best = 881000 sentences) and get needed WCE la-bels.
Figure 2 shows an example about the classi-fication results for one sentence.
Comparing withthe reference labels, we can point out easily thecorrect classifications for ?G?
words (e.g.
in caseof operation, added) and for ?B?
words (e.g.
is,have), as well as classification errors (e.g.
a, com-bat).
According to the Precision (Pr), Recall (Rc)and F-score (F) shown in Table 1, our WCE sys-tem reaches very promising performance in pre-dicting ?G?
label, and acceptable for ?B?
label.These labels will be used to calculate our proposed2http://www.statmt.org/wmt10/4Figure 2: Example of our WCE classification results for one MT hypothesisfeatures (section 3.3).Label Pr(%) Rc(%) F(%)Good (G) 84.36 91.22 87.65Bad (B) 51.34 35.95 42.29Table 1: Pr, Rc and F for ?G?
and ?B?
labels ofour WCE system3.3 Proposed FeaturesSince the scores resulted from the WCE systemare for words, we have to synthesize them in sen-tence level scores for integrating with the 14 de-coder scores.
Six proposed scores involve:?
The ratio of number of good words to totalnumber of words.
(1 score)?
The ratio of number of good nouns (verbs) tototal number of nouns (verbs)3.
(2 scores)?
The ratio of number of n consecutive goodword sequences to the total number of con-secutive word sequences ; n=2, n=3 and n=4.
(3 scores)For instance, in case of the hypothesis in Figure 2:among the total of 18 words, we have 12 labeledas ?G?
; and 7 out of 17 word pairs (bigram) arelabeled as ?GG?, etc.
Hence, some of the above3We decide not to experiment with adjectives, adverbs andconjunctions since their number can be 0 in many cases.scores can be written as:#good words#words=1218= 0.667#good bigrams#bigrams=717= 0.4118#good trigrams#trigrams=316= 0.1875(3)With the features simply derived from WCE labelsand not from CRF model scores (i.e.
the probabil-ity p(G), p(B)) , we expect to spread out the eval-uation up to the ?oracle?
setting, where the usersvalidate a word as ?G?
or ?B?
without providingany confidence score.4 Experiments4.1 Experimental SettingsAs described in Section 3.2, our SMT system gen-erates 1000-best list for each source sentence, andamong them, the best hypothesis was determinedby using the objective function based on 14 de-coder scores, including: 7 reordering scores, 1 lan-guage model score, 5 translation model scores and1 word penalty score.
Initially, all six additionalWCE-based scores are weighted as 1.0.
Then,two optimization methods: MERT and MarginInfused Relaxed Algorithm (MIRA) (Watanabeet al., 2007) are applied to optimize the weights ofall 20 scores of the re-ranker.
In both methods, wecarry out a 2-fold cross validation on the N-best5Systems MERT MIRABLEU TER TERp-A BLEU TER TERp-ABL 52.31 0.2905 0.3058 50.69 0.3087 0.3036BL+OR 58.10 0.2551 0.2544 55.41 0.2778 0.2682BL+WCE 52.77 0.2891 0.3025 51.01 0.3055 0.3012WCE + 25% 53.45 0.2866 0.2903 51.33 0.3010 0.2987WCE + 50% 55.77 0.2730 0.2745 53.63 0.2933 0.2903WCE + 75% 56.40 0.2687 0.2669 54.35 0.2848 0.2822Oracle BLEU score BLEU=60.48Table 2: Translation quality of the baseline system (only decoder scores) and that with additional scoresfrom real ?WCE?
or ?oracle?
WCE systemSystem MERTBetter Equivalent WorseBL+WCE 159 601 121BL+OR 517 261 153WCE+25% 253 436 192WCE+50% 320 449 112WCE+75% 461 243 177Table 3: Quality comparison (measured by TER) between the baseline and two integrated systems indetails (How many sentences are improved, kept equivalent or degraded, out of 881 test sentences?test set.
In other words, we split our N-best testset into two equivalent subsets: S1 and S2.
Play-ing the role of a development set, S1 will be usedto optimize the 20 weights for re-ranking S2 (andvice versa).
Finally two result subsets (new 1-bestafter re-ranking process) are merged for evalua-tion.
To better acknowledge the impact of the pro-posed scores, we calculate them not only using ourreal WCE system, but also using an oracle WCE(further called ?WCE scores?
and ?oracle scores?,respectively).
To summarize, we experiment withthe three following systems:?
BL: Baseline SMT system with 14 above de-coder scores?
BL+WCE: Baseline + 6 real WCE scores?
BL+OR: Baseline + 6 oracle WCE scores(simulating an interactive scenario).4.2 Results and AnalysisThe translation quality of BL, BL+WCE andBL+OR, optimized by MERT and MIRA methodare reported in Table 2.
Meanwhile, Table 3depicts in details the number of sentences inthe two integrated systems which outperform, re-main equivalent or degrade the baseline hypoth-esis (when match against the references, mea-sured by TER).
It can be observed from Table2 that the integration of oracle scores signifi-cantly boosts the MT output quality, measuredby all three metrics and optimized by both meth-ods employed.
We gained 5.79 and 4.72 pointsin BLEU score, by MERT and MIRA (respec-tively).
With TER, BL+OR helps to gain 0.03point in both two methods.
Meanwhile, in case ofTERp-A, the improvement is 0.05 point for MERTand 0.03 point for MIRA.
It is worthy to mentionthat the possibility of obtaining such oracle labelsis definitely doable through a human-interactionscenario (which could be built from a tool likePET (Post-Editing Tool) (Aziz et al., 2012) forinstance).
In such an environment, once havingthe hypothesis produced by the first pass (trans-lation task), the human editor could simply clickon words considered as bad (B), the other wordsbeing implicitly considered as correct (G).Breaking down the analysis into sentence level,as described on Table 3, BL+OR (MERT) yieldsnearly 59% (517 over 881) better outputs than thebaseline and only 17% of worse ones.
Further-more, Table 2 shows that in case of our test set, op-timizing by MERT is pretty more beneficial thanMIRA (we do not have a clear explanation of thisyet).For more insightful understanding about WCEscores?
acuteness, we make a comparison with6the most possible optimal BLEU score that couldbe obtained from the N-best list.
Applying thesentence-level BLEU+1 (Nakov et al., 2012) met-ric over candidates in the list, we are able to se-lect the one with highest score and aggregate allof them in an oracle-best translation; the result-ing performance obtained is 60.48.
This scoreaccounts for a fact that the simulated interactivescenario (BL+OR) lacks only 2.38 points (in caseof MERT) to be optimal and clearly overpass thebaseline (8.17 points below the best score).The contribution of a real WCE system seemsmore modest: BL+WCE marginally increasesBLEU scores of BL (0.46 gain in case of opti-mizing by MERT and 0.32 by MIRA).
For bothTER and TERp-A metric, the progressions arealso negligible.
To verify the significance of thisresult, we estimate the p-value between BLEU ofBL+WCE system and BLEU of baseline BL rely-ing on Approximate Randomization (AR) method(Clark et al., 2011) which indicates if the improve-ment yielded by the optimized system is likelyto be generated again by some random processes(randomized optimizers).
After various optimizerruns, we selected randomly 5 optimizer outputs,perform the AR test and obtain a p-value of 0.01.This result reveals that the improvement yieldedby BL+WCE is significative although small, orig-inated from the contribution of WCE score, notby any optimizer variance.
This modest but pos-itive change in BLEU score using WCE features,encourages us to investigate and analyze furtherabout WCE scores?
impact, supposing WCE per-formance is getting better.
More in-depth analysisis presented in the next section.5 Further Understanding of WCE scoresrole in N-best Re-ranking viaImprovement SimulationWe think it would be very interesting and usefulto answer the following question: do WCE scoresreally effectively help to increase MT output qual-ity when the WCE system is getting better andbetter?
To do this, our proposition is as follows:firstly, by using the oracle labels, we filter out allwrongly classified words in the test set and pushthem into a temporary set, called T. Then, we cor-rect randomly a percentage (25%, 50%, or 75%)of labels in T. Finally, the altered T will be inte-grated back with the correctly predicted part (bythe WCE system) in order to form a new ?simu-lated?
result set.
This strategy results in three ?vir-tual?
WCE systems called ?WCE+N%?
(N=25,50 or 75), which use 14 decoder scores and 6 ?sim-ulated?
WCE scores.
Table 4 shows the perfor-mance of these systems in term of F score (%).From each of the above systems, the whole exper-System F(?G?)
F(?B?)
Overall FWCE+25% 89.87 58.84 63.51WCE+50% 93.21 73.09 76.11WCE+75% 96.58 86.87 88.33Oracle labels 100 100 100Table 4: The performances (Fscore) of simulatedWCE systemsimental setting is identical to what we did with theoriginal WCE and oracle systems: six scores arebuilt and combined with existing 14 system scoresfor each hypothesis in the N-best list.
After that,MERT and MIRA methods are invoked to opti-mize their weights, and finally the reordering isperformed thanks to these scores and appropriateoptimal weights.
The translation quality measuredby BLEU, TER and TERp-A after re-ranking us-ing ?WCE+N%?
(N=25,50,75) can be seen alsoin Table 2.
The number of translations which out-perform, keep intact and decline in comparison tothe baseline are shown in Table 3 for MERT opti-mization.We note that all obtained scores fit our guess andexpectation: the better performance WCE systemreaches, the clearer its role in improving MT out-put quality.
Diminishing 25% of the wrongly pre-dicted words leads to a gain 0.68 point (by MERT)and 0.32 (by MIRA) in BLEU score.
More sig-nificant increases of BLEU 3.00 and BLEU 3.63(MERT) can be achieved when prediction errorsare cut off up to 50% and 75%.
Figure 3 presentsan overview of the results obtained and helps usto predict the MT improvements expected if theWCE system improves in the future.
Table 5shows several examples where WCE scores driveSMT system to better reference-correlated hypoth-esis.
In the first example, the baseline generatesthe hypothesis in which the source phrase ?poursa part?
remains untranslated.
On the contrary,WCE+50% overcomes this drawback by result-ing in a correct translation phrase: ?for his part?.The latter translation needs only one edit opera-tion (shift for ?Bettencourt-Meyers?)
to becomeits reference.
In example 2, BL+OR selects the7Example 1 (from WCE+50%)Source Pour sa part , l?
avocat de Franc?oise Bettencourt-Meyers , OlivierMetzner , s?
est f?elicit?e de la d?ecision du tribunal .Hypothesis (Baseline SMT) The lawyer of Bettencourt-Meyers Franc?oise , Olivier Metzner ,welcomed the court ?s decision .Hypothesis (SMT+WCEscores)For his part , the lawyer of Bettencourt-Meyers Franc?oise ,Olivier Metzner , welcomed the court ?s decision .Post-edition For his part , the lawyer of Franc?oise Bettencourt-Meyers ,Olivier Metzner , welcomed the court ?s decision .Example 2 (from BL+OR)Source Pour l?
otre , l?
accord risque ?
de creuser la tombe d?
un tr`esgrand nombre de pme du secteur dans les 12 prochains mois ?
.Hypothesis (Baseline MT) For the otre the agreement is likely to deepen the grave of a verylarge number of smes in the sector in the next 12 months ?
.Hypothesis (SMT+WCEscores)For the otre agreement , the risk ?
digging the grave of a verylarge number of medium-sized businesses in the next 12 months ?.Post-edition For the otre , the agreement risks ?
digging the grave of a verylarge number of small- and medium-sized businesses in the next12 months ?
.Table 5: Examples of MT hypothesis before and after reranking using the additional scores fromWCE+50% (Example 1) and BL+OR (Example 2) systemFigure 3: Comparison of the performance of var-ious systems: the integrations of WCE features,which the quality increases gradually, lead to thelinear improvement of translation outputs.better hypothesis, in which the phrases ?creuserla tombe?
and ?
?pme du secteur?
are translatedinto ?digging the grave?
and ?medium-sized busi-nesses?, respectively, better than those of the base-line (?deepen the grave?
and ?smes in the sec-tor?
).6 Conclusions And PerspectivesSo far, the word confidence scores have beenexploited in several applications, e.g.
post-editing, sentence quality assessment or multipleMT-system combination, yet very few studies (ex-cept Bach et al.
(2011) ) propose to investigatethem for boosting MT quality.
Thus, this pa-per proposed several features extracted from aWCE system and combined them with existing de-coder scores for re-ranking N-best lists.
Our WCEmodel is built using CRFs, on a variety of types offeatures for the French - English SMT task.
Dueto its limitations in predicting translation errors(?B?
label), WCE scores ensure only a modest im-provement in translation quality over the baselineSMT.
Nevertheless, further experiments about thesimulation of WCE performance suggest that suchtypes of score contribute dramatically if they arebuilt from an accurate WCE system.
They alsoshow that with the help of an ?ideal?
WCE, theMT system reaches quite close to its most optimalpossible quality.
These scores are totally indepen-dent from the decoder, they can be seen as a wayto introduce lexical, syntactic and semantic infor-mation (used for WCE) in a SMT pipeline.As future work, we plan to focus on augmentingour WCE performance using more linguistic fea-tures as well as advanced techniques (feature se-lection, Boosting method...).
In the same time, wewould like to integrate the WCE scores in the de-coder?s search graph to redirect the decoding pro-cess (preliminary experiments, not reported hereyet, have shown that this is a very promising av-enue of research).8ReferencesWilker Aziz, Sheila C. M. de Sousa, and Lucia Specia.
Pet:a tool for post-editing and assessing machine translation.In Proceedings of the Eight International Conference onLanguage Resources and Evaluation (LREC?12), Istanbul,Turkey, May 23-25 2012.Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
Goodness:A method for measuring machine translation confidence.In Proceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics, pages 211?219, Port-land, Oregon, June 19-24 2011.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, andNicola Ueffing.
Confidence estimation for machine trans-lation.
Technical report, JHU/CLSP Summer Workshop,2003.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, andNicola Ueffing.
Confidence estimation for machine trans-lation.
In Proceedings of COLING 2004, pages 315?321,Geneva, April 2004.Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.Better hypothesis testing for statistical machine transla-tion: Controlling for optimizer instability.
In Proceedingsof the Association for Computational Lingustics, 2011.Kevin Duh and Katrin Kirchhoff.
Beyond log-linear models:Boosted minimum error rate training for n-best re-ranking.In Proc.
of ACL, Short Papers, 2008.Mariano Felice and Lucia Specia.
Linguistic features forquality estimation.
In Proceedings of the 7th Workshop onStatistical Machine Translation, pages 96?103, Montreal,Canada, June 7-8 2012.Katrin Kirchhoff and Mei Yang.
Improved language model-ing for statistical machine translation.
In Proceedings ofthe ACL Workshop on Building and Using Parallel Texts,pages 125?128, Ann Arbor, Michigan, June 2005.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
Moses: Open source toolkit for statisti-cal machine translation.
In Proceedings of the 45th AnnualMeeting of the Association for Computational Linguistics,pages 177?180, Prague, Czech Republic, June 2007.Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.
Practi-cal very large scale crfs.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguistics,pages 504?513, 2010.Ngoc Quang Luong, Laurent Besacier, and Benjamin Lecou-teux.
Word confidence estimation and its integration insentence quality estimation for machine translation.
InProceedings of The Fifth International Conference onKnowledge and Systems Engineering (KSE 2013), Hanoi,Vietnam, October 17-19 2013a.Ngoc Quang Luong, Benjamin Lecouteux, and Laurent Be-sacier.
LIG system for WMT13 QE task: Investigating theusefulness of features in word confidence estimation forMT.
In Proceedings of the Eighth Workshop on Statisti-cal Machine Translation, pages 396?391, Sofia, Bulgaria,August 2013b.
Association for Computational Linguistics.Preslav Nakov, Francisco Guzman, and Stephan Vogel.
Op-timizing for sentence-level bleu+1 yields short transla-tions.
In Proceedings of COLING 2012, pages 1979?1994,Mumbai, India, December 8 -15 2012.Franz Josef Och.
Minimum error rate training in statisticalmachine translation.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguistics,pages 160?167, July 2003.Kishore Papineni, Salim Roukos, Todd Ard, and Wei-JingZhu.
Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics, 2002.Kristen Parton, Nizar Habash, Kathleen McKeown, GonzaloIglesias, and Adri`a de Gispert.
Can automatic post-editingmake mt more meaningful?
In Proceedings of the 16thEAMT, pages 111?118, Trento, Italy, 28-30 May 2012.M Potet, R Emmanuelle E, L Besacier, and H Blanchon.Collection of a large database of french-english smt out-put corrections.
In Proceedings of the eighth interna-tional conference on Language Resources and Evaluation(LREC), Istanbul, Turkey, May 2012.Matthew Snover, Nitin Madnani, Bonnie Dorr, and RichardSchwartz.
Terp system description.
In MetricsMATRworkshop at AMTA, 2008.Artem Sokolov, Guillaume Wisniewski, and Francois Yvon.Non-linear n-best list reranking with few features.
In Pro-ceedings of AMTA, 2012.Radu Soricut and Abdessamad Echihabi.
Trustrank: Inducingtrust in automatic translations via ranking.
In Proceedingsof the 48th ACL (Association for Computational Linguis-tics), pages 612?621, Uppsala, Sweden, July 2010.Andreas Stolcke.
Srilm - an extensible language model-ing toolkit.
In Seventh International Conference on Spo-ken Language Processing, pages 901?904, Denver, USA,2002.Nicola Ueffing and Hermann Ney.
Word-level confidenceestimation for machine translation using phrased-basedtranslation models.
In Proceedings of Human Lan-guage Technology Conference and Conference on Empiri-cal Methods in Natural Language Processing, pages 763?770, Vancouver, 2005.Nicola Ueffing, Klaus Macherey, and Hermann Ney.
Con-fidence measures for statistical machine translation.
InProceedings of the MT Summit IX, pages 394?401, NewOrleans, LA, September 2003.Ashish Venugopal, Andreas Zollmann, and Stephan Vogel.An efficient two-pass approach to synchronous-cfg drivenstatistical mt.
In Proceedings of Human Language Tech-nologies 2007: The Conference of the North AmericanChapter of the Association for Computational Linguistics,April 2007.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
Online large-margin training for statistical ma-chine translation.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learning,pages 64?773,, Prague, Czech Republic, June 2007.Deyi Xiong, Min Zhang, and Haizhou Li.
Error detectionfor statistical machine translation using linguistic features.In Proceedings of the 48th Association for ComputationalLinguistics, pages 604?611, Uppsala, Sweden, July 2010.Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.Distributed language modeling for n-best list re-ranking.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing (EMNLP 2006),pages 216?223, Sydney, July 2006.9
