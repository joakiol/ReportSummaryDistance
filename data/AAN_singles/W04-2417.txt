A transformation-based approach to argument labelingDerrick HigginsEducational Testing ServiceMail Stop 12-RRosedale RoadPrinceton, NJ 08541dhiggins@ets.orgAbstractThis paper presents the results of applyingtransformation-based learning (TBL) to theproblem of semantic role labeling.
The greatadvantage of the TBL paradigm is that it pro-vides a simple learning framework in which theparallel tasks of argument identification and ar-gument labeling can mutually influence one an-other.
Semantic role labeling nevertheless dif-fers from other tasks in which TBL has beensuccessfully applied, such as part-of-speechtagging and named-entity recognition, becauseof the large span of some arguments, the de-pendence of argument labels on global infor-mation, and the fact that core argument labelsare largely arbitrary.
Consequently, some careis needed in posing the task in a TBL frame-work.1 OverviewIn the closed challenge of the CoNLL shared task, thesystem is charged with both identifying argument bound-aries, and correctly labeling the arguments with the cor-rect semantic role, without using a parser to suggestcandidate phrases.
Transformation-based learning (Brill,1995) is well-suited to simultaneously addressing thisdual task of identifying and labeling semantic argumentsof a predicate, because it allows intermediate hypothe-ses to influence the ultimate decisions made.
More con-cretely, the category of an argument may decisively in-fluence how the system places its boundaries, and con-versely, the shape of an argument is an important factorin predicting its category.We treat the task as a word-by-word tagging problem,using a variant of the IOB2 labeling scheme.2 Transformation-based learningTBL is a general machine learning tool for assigningclasses to a sequence of observations.
TBL induces aset of transformational rules, which apply in sequence tochange the class assigned to observations which meet therules?
conditions.We use the software package fnTBL to designthe model described here.
This package, and theTBL framework itself, are described in detail byNgai and Florian (2001).3 Task DefinitionDefining the task of semantic role labeling in TBL termsrequires four basic steps.
First, the problem has to be re-duced to that of assigning an appropriate tag to each wordin a sentence.
Second, we must define the features asso-ciated with each word in the sentence, on which the trans-formational rules will operate.
Third, we must decide onthe exact forms the transformational rules will be allowedto take (the rule templates).
Finally, we must determinea mapping from our word-by-word tag assignment to thelabeled bracketing used to identify semantic arguments inthe test data.
Each of these steps is addressed below.3.1 Tagging schemeThe simplest way of representing the chunks of textwhich correspond to semantic arguments is to usesome variant of the IOB tagging scheme (Sang andVeenstra, 1999).
This is the approach taken byHacioglu et al (2003), who apply the IOB2 taggingscheme in their word-by-word models, as shown in thesecond row of Figure 1.However, two aspects of the problem at hand make thistag assignment difficult to use for TBL.
First, semanticargument chunks can be very large in size.
An argu-ment which contains a relative clause, for example, caneasily be longer than 20 words.
Second, the label an ar-gument is assigned is largely arbitrary, in the sense thatcore argument labels (A0, A1, etc.)
generally cannot beassigned without some information external to the con-stituent, such as the class of the predicate, or the identityof other arguments which have already been assigned.
Sousing the IOB2 format, it might take a complicated se-quence of TBL rules to completely re-tag, say, an A0 ar-gument as A1.
If this re-tagging is imperfectly achieved,we are left with the difficult decision of how to interpretthe stranded I-A0 elements, and the problem that theymay incorrectly serve as an environment for other trans-formational rules.For this reason, we adopt a modified version of theIOB2 scheme which is a compromise between addressingthe tasks of argument identification and argument label-ing.
The left boundary (B) tags indicate the label of theargument, but the internal (I) tags are non-specific as toargument label, as in the last row of Figure 1.
This al-lows a a single TBL rule to re-label an argument, whilestill allowing for interleaving of TBL rules which affectargument identification and labeling.3.2 Feature CodingWith each word in a sentence, we associate the followingfeatures:Word The word itself, normalized to lower-case.Tag The word?s part-of-speech tag, as predicted by thesystem of Gime?nez and Ma`rquez (2003).Chunk The chunk label of the word, as predicted by thesystem of Carreras and Ma`rquez (2003).Entity The named-entity label of the word, as predictedby the system of Chieu and Ng (2003).L/R A feature indicating whether the word is to the left(L) or right (R) of the target verb.Indent This feature indicates the clause level of the cur-rent word with respect to the target predicate.
Us-ing the clause boundaries predicted by the systemof Carreras and Ma`rquez (2003), we compute a fea-ture based on the linguistic notion of c-command.1If both the predicate and the current word are inthe same basic clause, Indent=0.
If the predicate c-commands the current word, and the current word isone clause level lower, Indent=1.
If it is two clauselevels lower, Indent=2, and so on.
If the c-commandrelations are reversed, the indent levels are negative,and if neither c-commands the other, Indent=?NA?.
(Figure 2 illustrates how this feature is defined.)
Theabsolute value of the Indent feature is not permittedto exceed 5.is-PP A boolean feature indicating whether the word isincluded within a base prepositional phrase.
This is1A node ?
(reflexively) c-commands a node ?
iff there is anode ?
such that ?
directly dominates ?, and ?
dominates ?.Note that only clauses (S nodes) are considered in our applica-tion described above.true if its chunk tag is B-PP or I-PP, or if it is withinan NP chunk directly following a PP chunk.PP-head If is-PP is true, this is the head of the preposi-tional phrase; otherwise it is zero.N-head The final nominal element of the next NP chunkat the same indent level as the current word, if itexists.
For purposes of this feature, a possessive NPchunk is combined with the following NP chunk.Verb The target predicate under consideration.V-Tag The POS tag of the target predicate.V-Passive A boolean feature indicating whether the tar-get verb is in the passive voice.
This is determinedusing a simple regular expression over the sentence.Path As in (Pradhan et al, 2003), this feature is an or-dered list of the chunk types intervening between thetarget verb and the current word, with consecutiveNP chunks treated as one.3.3 Rule TemplatesIn order to define the space of rules searched by the TBLalgorithm, we must specify a set of rule templates, whichdetermine the form transformational rules may take.
Therule templates used in our system are 130 in number, andfall into a small number of classes, as described below.These rules all take the form f 1 .
.
.
fn ?
labelw,where f1 through fn are features of the current word w orwords in its environment, and usually include the current(semantic argument) label assigned to w. The categoriza-tion of rule templates below, then, basically amounts to alist of the different feature sets which are used to predictthe argument label of each word.The initial assignment of tags which is given to theTBL algorithm is a very simple chunk-based assignment.Every word is given the tag O (outside all semantic argu-ments), except if it is within an NP chunk at Indent levelzero.
In that case, the word is assigned the tag I if itschunk label is I-NP, B-A0 if its chunk label is B-NP andit is to the left of the verb, and B-A1 if its chunk label isB-NP and it is to the right of the verb.3.3.1 Basic rules (10 total)The simplest class of rules simply change the currentword?s argument label based on its own local features,including the current label, and the features L/R, Indent,and Chunk.3.3.2 Basic rules using local context (29)An expanded set of rules using all features of the cur-rent word, as well as the argument labels of the currentand previous words.
For example, the following rule willchange the label O to I within an NP chunk, if the initialArgument boundaries [A1 The deal] [V collapsed] [AM-TMP on Friday] .IOB2 [B-A1 The] [I-A1 deal] [B-V collapsed] [B-AM-TMP on] [I-AM-TMP Friday] [O .
]Modified scheme [B-A1 The] [I deal] [B-V collapsed] [B-AM-TMP on] [I Friday] [O .
]Figure 1: Tag assignments for word-by-word semantic role assignmentWV WV W Vindent = NAindent = ?1indent = 1Figure 2: Sample values of Indent feature for different clause embeddings of a word W and target verb Vportion of the chunk has already been marked as within asemantic argument:labelw0 = Oindentw0 = 0chunkw0 = I-NPL/Rw0 = Rlabelw?1 = I?
labelw0 = I.3.3.3 Lexically conditioned rules (14)These rules change the argument label of the currentword based on the Word feature of the current or sur-rounding words, in combination with argument labels andchunk labels from the surrounding context.
For example,this rule marks the adverb back as a directional modifierwhen it follows the target verb:labelw0 = Ochunkw0 = B-ADVPwordw0 = backlabelw?1 = B-Vchunkw?1 = B-VP?
labelw0 = B-AM-DIR.3.3.4 Entity (24)These rules further add the named-entity tag of the cur-rent, preceding, or following word to the basic and local-context rules above.3.3.5 Verb tag (15)These rules add the POS tag of the predicate to thebasic and simpler local-context rules above.3.3.6 Verb-Noun dependency (9)These rules allow the argument label of the currentword to be changed, based on its Verb and N-head fea-tures,as well as other local features.3.3.7 Word-Noun dependency (3)These rules allow the argument label of the currentword to be changed, based on its Word, N-head, Indent,L/R, and Chunk features, as well as the argument labelsof adjacent words.3.3.8 Long-distance rules (6)Because many of the dependencies involved in the se-mantic role labeling task hold over the domain of the en-tire sentence, we include a number of long-distance rules.These rules allow the argument label to be changed de-pending on the word?s current label, the features L/R, In-dent, Verb, and the argument label of a word within 50 or100 words of the current word.
These rules are intendedto support generalizations like ?if the current word is la-beled A0, but there is already an A0 further to the left,change it to I?.3.3.9 ?Smoothing?
rules (15)Finally, there are a number of ?smoothing?
rules,which are designed primarily to prevent I tags frombecoming stranded, so that arguments which contain alarge number of words can successfully be identified.These rules allow the argument label of a word to bechanged based on the argument labels of the previous twowords, the next two words, and the chunk tags of thesewords.
This sample rule marks a word as being argument-internal, if both its neighbors are already so marked:labelw?1 = Ilabelw0 = Olabelw1 = I?
labelw0 = I.3.3.10 Path rules (5)Finally, we include a number of rule templates usingthe highly-specific Path feature.
These rules allow the ar-gument label of a word to be changed based on its currentvalue, as well as the value of the feature Path in combi-nation with L/R, Indent, V-Tag, Verb, and Word.3.4 Tag interpretationThe final step in our transformation-based approach tosemantic role labeling is to map the word-by word IOBtags predicted by the TBL model back to the format of theoriginal data set, which marks only argument boundaries,so that we can calculate precision and recall statistics foreach argument type.
The simplest method of performingthis mapping is to consider an argument as consisting ofan initial labeled boundary tag (such as B-A0, followedby zero or more argument-internal (I) tags, ignoring any-thing which does not conform to this structure (in partic-ular, strings of Is with no initial boundary marker).In fact, this method works quite well, and it is used forthe results reported below.Finally, there is a post-processing step in which adjuctsmay be re-labeled if the same sequence of words is foundas an adjunct in the training data, and always bears thesame role.
This affected fewer than twenty labels on thedevelopment data, and added only about 0.1 to the overallf-measure.4 ResultsThe results on the test section of the CoNLL 2004 dataare presented in Table 1 below.
The overall result, an f-score of 60.66, is considerably below results reported forsystems using a parser on a comparable data set.
How-ever, it is a reasonable result given the simplicity of oursystem, which does not make use of the additional infor-mation found in the PropBank frames themselves.It is an interesting question to what extent our re-sults depend on the use of the Path feature (whichPradhan et al (2003) found to be essential to their mod-els?
performance).
Since this Path feature is also likelyto be one of the model?s most brittle features, depend-ing heavily on the accuracy of the syntactic analysis, wemight hope that the system does not depend too heav-ily on it.
In fact, the overall f-score on the developmentset drops from 62.75 to 61.33 when the Path feature isremoved, suggestig that it is not essential to our model,though it does help performance to some extent.ReferencesEric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: A case studyin part-of-speech tagging.
Computational Linguistics,21(4):543?565.Xavier Carreras and Llu?
?s Ma`rquez.
2003.
Phrase recog-nition by filtering and ranking with perceptrons.
InProceedings of RANLP 2003.Hai Leong Chieu and Hwee Tou Ng.
2003.
Named en-tity recognition with a maximum entropy approach.
InProceedings of CoNLL 2003.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2003.
Fast and accu-rate part-of-speech tagging: the SVM approach revis-ited.
In Proceedings of RANLP 2003.Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.Martin, and Dan Jurafsky.
2003.
Shallow semanticPrecision Recall F?=1Overall 64.17% 57.52% 60.66A0 72.48% 68.94% 70.67A1 63.57% 61.88% 62.72A2 51.32% 40.90% 45.52A3 51.58% 32.67% 40.00A4 36.07% 44.00% 39.64A5 0.00% 0.00% 0.00AM-ADV 41.08% 32.25% 36.13AM-CAU 63.33% 38.78% 48.10AM-DIR 31.58% 24.00% 27.27AM-DIS 56.93% 53.99% 55.42AM-EXT 70.00% 50.00% 58.33AM-LOC 26.34% 21.49% 23.67AM-MNR 46.90% 26.67% 34.00AM-MOD 96.24% 91.10% 93.60AM-NEG 90.98% 95.28% 93.08AM-PNC 37.93% 12.94% 19.30AM-PRD 0.00% 0.00% 0.00AM-TMP 51.81% 38.42% 44.12R-A0 82.00% 77.36% 79.61R-A1 78.26% 51.43% 62.07R-A2 100.00% 22.22% 36.36R-A3 0.00% 0.00% 0.00R-AM-LOC 50.00% 25.00% 33.33R-AM-MNR 0.00% 0.00% 0.00R-AM-PNC 0.00% 0.00% 0.00R-AM-TMP 100.00% 7.14% 13.33V 98.15% 98.15% 98.15Table 1: Results on test set: closed challengeparsing using support vector machines.
Technical Re-port CSLR-2003-01, Center for Spoken Language Re-search, University of Colorado at Boulder.Grace Ngai and Radu Florian.
2001.
Transformation-based learning in the fast lane.
In Proceedings ofNAACL 2001, pages 40?47, June.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin, and Daniel Jurafsky.2003.
Support vector learning for semantic argumentclassification.
Technical Report CSLR-2003-03, Cen-ter for Spoken Language Research, University of Col-orado at Boulder.Erik F. Tjong Kim Sang and Jorn Veenstra.
1999.
Rep-resenting text chunks.
In Proceedings of EACL 1999,pages 173?179.
