Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 17?21,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsTemporal Text Ranking and Automatic Dating of TextsVlad Niculae1, Marcos Zampieri2, Liviu P. Dinu3, Alina Maria Ciobanu3Max Planck Institute for Software Systems, Germany1Saarland University, Germany2Center for Computational Linguistics, University of Bucharest, Romania3vniculae@mpi-sws.org, marcos.zampieri@uni-saarland.de,ldinu@fmi.unibuc.ro, alina.ciobanu@my.fmi.unibuc.roAbstractThis paper presents a novel approach tothe task of temporal text classificationcombining text ranking and probability forthe automatic dating of historical texts.The method was applied to three histor-ical corpora: an English, a Portugueseand a Romanian corpus.
It obtained per-formance ranging from 83% to 93% ac-curacy, using a fully automated approachwith very basic features.1 IntroductionTemporal text classification is an underexploredproblem in NLP, which has been tackled as amulti-class problem, with classes defined as timeintervals such as months, years, decades or cen-turies.
This approach has the drawback of havingto arbitrarily delimit the intervals, and often leadsto a model that is not informative for texts writtenwithin such a window.
If the predefined window istoo large, the output is not useful for most systems;if the window is too small, learning is impracticalbecause of the large number of classes.
Particu-larly for the problem of historical datasets (as theone we propose here), learning a year-level classi-fier would not work, because each class would berepresented by a single document.Our paper explores a solution to this drawbackby using a ranking approach.
Ranking amounts toordering a set of inputs with respect to some mea-sure.
For example, a search engine ranks returneddocuments by relevance.
We use a formalizationof ranking that comes from ordinal regression, theclass of problems where samples belong to inher-ently ordered classes.This study is of interest to scholars who dealwith text classification and NLP in general; his-torical linguists and philologists who investigatelanguage change; and finally scholars in the dig-ital humanities who often deal with historicalmanuscripts and might take advantage of temporaltext classification applications in their research.2 Related WorkModelling temporal information in text is a rele-vant task for a number of NLP tasks.
For example,in Information Retrieval (IR) research has beenconcentrated on investigating time-sensitivity doc-ument ranking (Dakka and Gravana, 2010).
Evenso, as stated before, temporal text classificationmethods were not substantially explored as othertext classification tasks.One of the first studies to model temporal infor-mation for the automatic dating of documents isthe work of de Jong et al.
(2005).
In these exper-iments, authors used unigram language models toclassify Dutch texts spanning from January 1999to February 2005 using normalised log-likelihoodratio (NLLR) (Kraaij, 2004).
As to the featuresused, a number of approaches proposed to auto-matic date take into account lexical features (Dalliand Wilks, 2006; Abe and Tsumoto, 2010; Ku-mar et al., 2011) and a few use external linguisticknowledge (Kanhabua and N?rv?ag, 2009).A couple of approaches try to classify texts notonly regarding the time span in which the textswere written, but also their geographical locationsuch as (Mokhov, 2010) for French and, more re-cently, (Trieschnigg et al., 2012) for Dutch.
At theword level, two studies aim to model and under-stand how word usage and meaning change overtime (Wijaya and Yeniterzi, 2011), (Mihalcea andNastase, 2012).The most recent studies in temporal text classifi-cation to our knowledge are (Ciobanu et al., 2013)for Romanian using lexical features and (?Stajnerand Zampieri, 2013) for Portuguese using stylisticand readability features.173 Methods3.1 CorporaTo evaluate the method proposed here we usedthree historical corpora.
An English historicalcorpus entitled Corpus of Late Modern EnglishTexts (CLMET)1(de Smet, 2005), a Portuguesehistorical corpus entitled Colonia2(Zampieri andBecker, 2013) and a Romanian historical corpus(Ciobanu et al., 2013).CLMET is a collection of English texts derivedfrom the Project Gutenberg and from the OxfordText Archive.
It contains around 10 million to-kens, divided over three sub-periods of 70 years.The corpus is available for download as raw textor annotated with POS annotation.For Portuguese, the aforementioned Colonia(Zampieri and Becker, 2013) is a diachronic col-lection containing a total of 5.1 million tokens and100 texts ranging from the 16thto the early 20thcentury.
The texts in Colonia are balanced be-tween European and Brazilian Portuguese (it con-tains 52 Brazilian texts and 48 European texts) andthe corpus is annotated with lemma and POS in-formation.
According to the authors, some textspresented edited orthography prior to their com-pilation but systematic spelling normalisation wasnot carried out.The Romanian corpus was compiled to portraitdifferent stages in the evolution of the Romanianlanguage, from the 16thto the 20thcentury in atotal of 26 complete texts.
The methodology be-hind corpus compilation and the date assignmentare described in (Ciobanu et al., 2013).3.2 Temporal classification as rankingWe propose a temporal model that learns a linearfunction g(x) = w ?
x to preserve the temporal or-dering of the texts, i.e.
if document3xipredatesdocument xj, which we will henceforth denote asxi?
xj, then g(xi) < g(xj).
Such a problem isoften called ranking or learning to rank.
When thegoal is to recover contiguous intervals that corre-spond to ordered classes, the problem is known asordinal regression.We use a pairwise approach to ranking that re-duces the problem to binary classification using a1https://perswww.kuleuven.be/?u0044428/clmet2http://corporavm.uni-koeln.de/colonia/3For brevity, we use xito denote both the document itselfand its representation as a feature vector.linear model.
The method is to convert a datasetof the form D = {(x, y) : x ?
Rd, y ?
Y} into apairwise dataset:Dp= {((xi, xj), I[yi< yj]) :(xi, yi), (xj, yj) ?
D}Since the ordinal classes only induce a partial or-dering, as elements from the same class are notcomparable, Dpwill only consist of the compara-ble pairs.The problem can be turned into a linear classifi-cation problem by noting that:w ?
xi< w ?
xj??
w ?
(xi?
xj) < 0In order to obtain probability values for the or-dering, we use logistic regression as the linearmodel.
It therefore holds that:P(xi?
xj;w) =11 + exp(?w ?
(xi?
xj))While logistic regression usually fits an inter-cept term, in our case, because the samples consistof differences of points, the model operates in anaffine space and therefore gains an extra effectivedegree of freedom.
The intercept is therefore notneeded.The relationship between pairwise ranking andpredicting the class from an ordered set {r1, ...rk}is given by assigning to a document x the class risuch that?
(ri?1) ?
g(x) < ?
(ri) (1)where ?
is an increasing function that does notneed to be linear.
(Pedregosa et al., 2012), whoused the pairwise approach to ordinal regressionon neuroimaging prediction tasks, showed usingartificial data that ?
can be accurately recoveredusing non-parametric regression.
In this work, weuse a parametric estimation of ?
that can be usedin a probabilistic interpretation to identify the mostlikely period when a text was written, as describedin section 3.3.3.3 Probabilistic dating of uncertain textsThe ranking model described in the previous sec-tion learns a direction along which the temporalorder of texts is preserved as much as possible.This direction is connected to the chronologicalaxis through the ?
function.
For the years t for18which we have an unique attested document xt,we have thatx ?
xt??
g(x) < g(xt) < ?
(t)This can be explained by seeing that equation 2gives ?
(t) as an upper bound for the projections ofall texts written in year t, and by transitivity for allprevious texts as well.Assuming we can estimate the function ?
withanother function?
?, the cumulative densitiy func-tion of the distribution of the time when an unseendocument was written can be expressed.P (x ?
t) ?11 + exp(w ?
x???
(t))(2)Setting the probability to12provides a point es-timate of the time when x was written, and confi-dence intervals can be found by setting it to p and1?
p.3.4 FeaturesOur ranking and estimation model can work withany kind of numerical features.
For simplicitywe used lexical and naive morphological features,pruned using ?2feature selection with tunablegranularity.The lexical features are occurrence counts of allwords that appear in at least plexdocuments.
Themorphological features are counts of character n-grams of length up to wmphin final positions ofwords, filtered to occur in at least nmphdocuments.Subsequently, a non-linear transformation ?
isoptionally applied to the numerical features.
Thisis one of ?sqrt(z) =?z, ?log(z) = log(z) or?id(z) = z (no transformation).The feature selection step is applied before gen-erating the pairs for classification, in order for the?2scoring to be applicable.
The raw target val-ues used are year labels, but to avoid separatingalmost every document in its own class, we in-troduce a granularity level that transforms the la-bels into groups of ngranyears.
For example, ifngran= 10 then the features will be scored ac-cording to how well they predict the decade a doc-ument was written in.
The features in the top pfselpercentile are kept.
Finally, C is the regulariza-tion parameter of the logistic regression classifier,as defined in liblinear (Fan et al., 2008).0.2 0.4 0.6 0.8 1.00.720.740.760.780.800.820.84RidgeRanking0.6 0.7 0.8 0.9 1.00.780.790.800.810.820.83RidgeRankingFigure 1: Learning curves for English (top) andPortuguese (bottom).
Proportion of training setused versus score.4 ResultsEach corpus is split randomly into training and testsets with equal number of documents.
The bestfeature set is chosen by 3-fold cross-validated ran-dom search over a large grid of possible configu-rations.
We use random search to allow for a moreefficient exploration of the parameter space, giventhat some parameters have much less impact to thefinal score than others.The evaluation metric we used is the percentageof non-inverted (correctly ordered) pairs, follow-ing (Pedregosa et al., 2012).We compare the pairwise logistic approach toa ridge regression on the same feature set, andtwo multiclass SVMs, at century and decade level.While the results are comparable with a slight ad-vantage in favour of ranking, the pairwise rankingsystem has several advantages.
On the one hand, itprovides the probabilistic interpretation describedin section 3.3.
On the other hand, the model cannaturally handle noisy, uncertain or wide-range la-bels, because annotating whether a text was writ-ten before another can be done even when the textsdo not correspond to punctual moments in time.While we do not exploit this advantage, it can leadto more robust models of temporal evolution.
Thelearning curves in Figure 1 further show that thepairwise approach can better exploit more data andnonlinearity.The implementation is based on the scikit-learnmachine learning library for Python (Pedregosa etal., 2011) with logistic regression solver from (Fanet al., 2008).
The source code will be available.4.1 Uncertain textsWe present an example of using the method fromSection 3.3 to estimate the date of uncertain, held-out texts of historical interest.
Figure 2 shows theprocess used for estimating ?
as a linear, and inthe case of Portuguese, quadratic function.
The19size plexnmphwmph?
ngranpfselC score ridge century decade MAEen 293 0.9 0 3 ?log100 0.15 290.838 0.837 0.751 0.813 22.8pt 87 0.9 25 4 ?sqrt5 0.25 2?50.829 0.819 0.712 0.620 58.7ro 42 0.8 0 4 ?log5 0.10 2280.929 0.924 0.855 0.792 28.8Table 1: Test results of the system on the three datasets.
The score is the proportion of pairs of docu-ments ranked correctly.
The column ridge is a linear regression model used for ranking, while centuryand decade are linear SVMs used to predict the century and the decade of each text, but scored as pair-wise ranking, for comparability.
Chance level is 0.5.
MAE is the mean absolute error in years.
Thehyperparameters are described in section 3.4.1650 1700 1750 1800 1850 1900 1950Year3002001000100200w?xLinear (33.54)TrainTest1400 1500 1600 1700 1800 1900 2000 2100Year40200204060w?xLinear (17.27)Quadratic (15.44)TrainTest1400 1500 1600 1700 1800 1900 2000 2100Year10050050100w?xLinear (1.87)TrainTestFigure 2: Estimating the function ?
that defines the relationship between years and projections of docu-ments to the direction of the model, for English, Portuguese and Romanian (left to right).
In parantheses,the normalized residual of the least squares fit is reported on the test set.15401560158016001620164016601680170017201740176017801800182018401860188019001920194019601980200020200.20.00.20.40.60.81.01.2Figure 3: Visualisation of the probability esti-mation for the dating of C. Cantacuzino?s Isto-ria T,?arii Rum?anes,ti.
The horizontal axis is thetime, the points are known texts with a heightequal to the probability predicted by the classifier.The dashed line is the estimated probability fromEquation 2.estimation is refit on all certain documents prior toplugging into the probability estimation.The document we use to demonstrate the pro-cess is Romanian nobleman and historian Con-stantin Cantacuzino?s Istoria T,?arii Rum?anes,ti.The work is believed to be written in 1716, theyear of the author?s death, and published in sev-eral editions over a century later (Stahl, 2001).This is an example of the system being reasonablyclose to the hypothesis, thus providing linguisticsupport to it.
Our system gives an estimated dat-ing of 1744.7 with a 90% confidence interval of1736.2 ?
1753.2.
As publications were signifi-cantly later, the lexical pull towards the end of 18thcentury that can be observed in Figure 3 could bedriven by possible editing of the original text.5 ConclusionWe propose a ranking approach to temporal mod-elling of historical texts.
We show how the modelcan be used to produce reasonable probabilisticestimates of the linguistic age of a text, using avery basic, fully-automatic feature extraction stepand no linguistic or historical knowledge injected,apart from the labels, which are possibly noisy.Label noise can be atenuated by replacing un-certain dates with intervals that are more certain,and only generating training pairs out of non-overlapping intervals.
This can lead to a morerobust model and can use more data than wouldbe possible with a regression or classification ap-proach.
The problem of potential edits that a texthas suffered still remains open.Finally, better engineered and linguistically-motivated features, such as syntactic, morphologi-cal or phonetic patterns that are known or believedto mark epochs in the evolution of a language, canbe plugged in with no change to the fundamentalmethod.20ReferencesH.
Abe and S. Tsumoto.
2010.
Text categorizationwith considering temporal patterns of term usages.In Proceedings of ICDM Workshops, pages 800?807.
IEEE.A.
Ciobanu, A. Dinu, L. Dinu, V. Niculae, andO.
Sulea.
2013.
Temporal text classification forromanian novels set in the past.
In Proceedings ofRANLP2013, Hissar, Bulgaria.W.
Dakka and C. Gravana.
2010.
Answering gen-eral time-sensitive queries.
IEEE Transactions onKnowledge and Data Engineering.A.
Dalli and Y. Wilks.
2006.
Automatic dating of doc-uments and temporal text classification.
In Proceed-ings of the Workshop on Annotating and Reasoningabout Time and Events, pages 17?22, Sidney, Aus-tralia.F.
de Jong, H. Rode, and D. Hiemstra.
2005.
Temporallanguage models for the disclosure of historical text.In Proceedings of AHC 2005 (History and Comput-ing).H.
de Smet.
2005.
A corpus of late modern english.ICAME-Journal.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.N.
Kanhabua and P. N?rv?ag.
2009.
Using tem-poral language models for document dating.
InECML/PKDD, pages 738?741.W.
Kraaij.
2004.
Variations on language modelingfor information retrieval.
Ph.D. thesis, Universityof Twente.A.
Kumar, M. Lease, and J. Baldridge.
2011.
Super-vised language modelling for temporal resolution oftexts.
In Proceedings of CIKM11 of the 20th ACMinternational conference on Information and knowl-edge management, pages 2069?2072.R.
Mihalcea and V. Nastase.
2012.
Word epoch dis-ambiguation: Finding how words change over time.In Proceedings of ACL, pages 259?263.
Associationfor Computational Linguistics.S.
Mokhov.
2010.
A marf approach to deft2010.
InProceedings of TALN2010, Montreal, Canada.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Fabian Pedregosa, Alexandre Gramfort, Ga?el Varo-quaux, Elodie Cauvet, Christophe Pallier, andBertrand Thirion.
2012.
Learning to rank frommedical imaging data.
CoRR, abs/1207.3598.H.H.
Stahl.
2001.
G?anditori s?i curente de istoriesocial?a rom?aneasc?a.
Biblioteca Institutului SocialRom?an.
Ed.
Univ.
din Bucures?ti.S.
?Stajner and M. Zampieri.
2013.
Stylistic changesfor temporal text classification.
In Proceedings ofthe 16th International Conference on Text Speechand Dialogue (TSD2013), Lecture Notes in ArtificialIntelligence (LNAI), pages 519?526, Pilsen, CzechRepublic.
Springer.D.
Trieschnigg, D. Hiemstra, M. Theune, F. de Jong,and T. Meder.
2012.
An exploration of lan-guage identification techniques for the dutch folktaledatabase.
In Proceedings of LREC2012.D.
Wijaya and R. Yeniterzi.
2011.
Understanding se-mantic change of words over centuries.
In Proc.
ofthe Workshop on Detecting and Exploiting CulturalDiversity on the Social Web (DETECT).M.
Zampieri and M. Becker.
2013.
Colonia: Corpus ofhistorical portuguese.
ZSM Studien, Special Volumeon Non-Standard Data Sources in Corpus-Based Re-search, 5.21
