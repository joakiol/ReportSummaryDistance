Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 21?28,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsSpanish DAL: A Spanish Dictionary of Affect in LanguageMat?
?as G. Dell?
Amerlina R?
?os and Agust?
?n GravanoDepartamento de Computacio?n, FCEyNUniversidad de Buenos Aires, Argentina{mamerlin,gravano}@dc.uba.arAbstractThe topic of sentiment analysis in text hasbeen extensively studied in English for thepast 30 years.
An early, influential work byCynthia Whissell, the Dictionary of Affect inLanguage (DAL), allows rating words alongthree dimensions: pleasantness, activation andimagery.
Given the lack of such tools in Span-ish, we decided to replicate Whissell?s work inthat language.
This paper describes the Span-ish DAL, a knowledge base formed by morethan 2500 words manually rated by humansalong the same three dimensions.
We evalu-ated its usefulness on two sentiment analysistasks, which showed that the knowledge basemanaged to capture relevant information re-garding the three affective dimensions.1 IntroductionIn an attempt to quantify emotional meaning in writ-ten language, Whissell developed the Dictionary ofAffect in Language (DAL), a tool for rating wordsand texts in English along three dimensions ?
pleas-antness, activation and imagery (Whissell et al1986; Whissell, 1989, inter alia).
DAL works bylooking up individual words in a knowledge basecontaining 8742 words.
All words in this lexiconwere originally rated by 200 na?
?ve volunteers alongthe same three dimensions.Whissell?s DAL has subsequently been used in di-verse research fields, for example as a keystone forsentiment analysis in written text (Yi et al 2003,e.g.)
and emotion recognition in spoken language(Cowie et al 2001).
DAL has also been used to aidthe selection of emotionally balanced word stimulifor Neuroscience and Psycholinguistics experiments(Gray et al 2002).
Given the widespread impact ofDAL for the English language, it would be desirableto create similar lexicons for other languages.In recent years, there have been efforts to buildcross-lingual resources, such as using sentimentanalysis tools in English to score Spanish texts af-ter performing machine translation (Brooke et al2009) or to automatically derive sentiment lexiconsin Spanish (Pe?rez-Rosas et al 2012).
The purposeof the present work is to create a manually anno-tated lexicon for the Spanish language, replicatingWhissell?s DAL, aiming at alleviating the scarcityof resources for the Spanish language, and at deter-mining if the lexicon-based approach would workin Spanish as well as it does in English.
We leavefor future work the comparison of the different ap-proaches mentioned here.
This paper describes thethree steps performed to accomplish that goal: i)creating a knowledge base which is likely to havea good word coverage on arbitrary texts from anytopic and genre (Section 2); ii) having a number ofvolunteers annotate each word for the three affectivedimensions under study (Section 3); and iii) evaluat-ing the usefulness of our knowledge base on simpletasks (Section 4).2 Word selectionThe first step in building a Spanish DAL consists inselecting a list of content words that is representa-tive of the Spanish language, in the sense that it willhave a good coverage of the words in arbitrary inputtexts from potentially any topic or genre.
To accom-plish this we decided to use texts downloaded fromWikipedia in Spanish1 and from an online collectionof short stories called Los Cuentos.2 Articles fromWikipedia cover a wide range of topics and are gen-1http://es.wikipedia.org2http://www.loscuentos.net21erally written in encyclopedia style.
We downloadedthe complete set of articles in March, 2012, consist-ing of 834,460 articles in total.
Short stories fromLos Cuentos were written by hundreds of differentauthors, both popular and amateur, on various gen-res, including tales, essays and poems.
We down-loaded the complete collection from Los Cuentos inApril, 2012, consisting of 216,060 short stories.2.1 Filtering and lemmatizing wordsWe extracted all words from these texts, sorted themby frequency, and filtered out several word classesthat we considered convey no affect by themselves(and thus it would be unnecessary to have them ratedby the volunteers).
Prepositions, determinants, pos-sessives, interjections, conjunctions, numbers, datesand hours were tagged and removed automaticallyusing the morphological analysis function includedin the Freeling toolkit (Padro?
et al 2010).3 Wealso excluded the following adverb subclasses forthe same reason: place, time, mode, doubt (e.g.,quiza?s, maybe), negation, affirmation and amount.Nouns and verbs were lemmatized using Freel-ing as well, except for augmentative and diminu-tive terminations, which were left intact due to theirpotential effect on a word?s meaning and/or affect(e.g., burrito is either a small donkey, burro, or atype of Mexican food).
Additionally, proper nounswere excluded.
Names of cities, regions, countriesand nationalities were marked and removed usingGeoWorldMap,4 a freely-available list of locationnames from around the world.
Names of peoplewere also filtered out.
Proper names were manu-ally inspected to avoid removing those with a lexicalmeaning, a common phenomenon in Spanish (e.g.,Victoria).
Other manually removed words includewords in foreign languages (mainly in English), ro-man numbers (e.g., XIX) and numbers in textualform, such as seis (six), sexto (sixth), etc.
Wordswith one or two characters were removed automat-ically, since we noticed that they practically alwayscorresponded to noise in the downloaded texts.2.2 Counting ?word, word-class?
pairsWe implemented a small refinement over Whissell?swork, which consisted in considering ?word, word-3http://nlp.lsi.upc.edu/freeling/4http://www.geobytes.com/FreeServices.htmclass?
pairs, rather than single words, since in Span-ish the same lexical form may have different senses.Thus, to each word (in its lemmatized form) we at-tached one of four possible word classes ?
noun,verb, adjective or adverb.
For example, bajoprep (un-der) or bajonoun (bass guitar).For each input word w, Freeling?s morphologicalanalysis returns a sequence of tuples ?lemma, POS-tag, probability?, which correspond to the possiblelemmas and part-of-speech tags for w, together withtheir prior probability.
For example, the analysisfor the word bajo returns four tuples: ?bajo, SPS00(i.e, preposition), 0.879?, ?bajo, AQ0MS0 (adjec-tive), 0.077?, ?bajo, NCMS000 (noun), 0.040?,and ?bajar, VMIP1S0 (verb), 0.004?.
This meansthat bajo, considered without context, has 87.9%chances of being a noun, or 0.04% of being a verb.Using this information, we computed the countsof all ?word, word-class?
pairs, taking into accounttheir prior probabilities.
For example, assuming theword bajo appeared 1000 times in the texts, it wouldcontribute with 1000?0.879 = 879 to the frequencyof bajoprep (i.e., bajo as a preposition), 77 to bajoadj,40 to bajonoun, and 4 to bajarverb.2.3 Merging Wikipedia and Los CuentosThis process yielded 163,071 ?word, word-class?pairs from the Wikipedia texts, and 30,544 from LosCuentos.
To improve readability, hereafter we willrefer to ?word, word-class?
pairs simply as words.Figure 1 shows the frequency of each word countin our two corpora.
We note that both graphics arepractically identical, with a majority of low-countwords and a long tail with few high-count words.To create our final word list to be rated by vol-unteers, we needed to merge our two corpora fromWikipedia and Los Cuentos.
To accomplish this, weFigure 1: Frequency of word counts in texts taken fromWikipedia and Los Cuentos.22normalized all word counts for corpus size (normal-ized count(w) = count(w) / corpus size), combinedboth lists and sorted the resulting list by the normal-ized word count (for the words that appeared in bothlists, we used its average count instead).
The result-ing list contained 175,413 words in total.The top 10 words from Wikipedia were ma?sadv,an?onoun, ciudadnoun, poblacio?nnoun, estadonoun, nom-brenoun, veznoun, municipionoun, gruponoun and his-torianoun (more, year, city, population, state, name,time, as in ?first time?, municipality, group and his-tory, respectively).
The 10 words most commonfrom Los Cuentos were ma?sadv, veznoun, vidanoun,d?
?anoun, tanadv, tiemponoun, ojonoun, manonoun,amornoun and nochenoun (more, time, life, day, so,time, eye, hand, love and night).2.4 Assessing word coverageNext we studied the coverage of the top k wordsfrom our list on texts from a third corpus formedby 3603 news stories downloaded from Wikinews inSpanish in April, 2012.5 We chose news stories forthis task because we wanted a different genre forstudying the evolution of coverage.Formally, let L be a word list, T any text, andW (T ) the set of words occurring at least once in T .We define the coverage of L on T as the percentageof words in W (T ) that appear in L. Figure 2 showsthe evolution of the mean coverage on Wikinews ar-ticles of the top k words from our word list.
Inthis figure we can observe that the mean coveragegrows rapidly, until it reaches a plateau at aroundFigure 2: Mean coverage of the top k words from our liston Wikinews articles.5http://es.wikinews.org80%.
This suggests that even a low number of wordsmay achieve a relatively high coverage on new texts.The 20% that remains uncovered, independently ofthe size of the word list, may be explained by thefunction words and proper names that were removedfrom our word list.
Note that news articles normallycontain many proper names, days, places and otherwords that we intentionally discarded.3 Word ratingAfter selecting the words, the next step consisted inhaving them rated by a group of volunteers.
For thispurpose we created a web interface, so that volun-teers could complete this task remotely.3.1 Web interfaceOn the first page of the web interface, volunteerswere asked to enter their month and year of birth,their education level and their native language, andwas asked to complete a reCAPTCHA6 to avoidbots.
Subsequently, volunteers were taken to a pagewith instructions for the rating task.
They wereasked to rate each word along the three dimensionsshown in Table 1.
These are the same three dimen-Pleasantness Activation Imagery1 Desagradable Pasivo Dif?
?cil de imaginar(Unpleasant) (Passive) (Hard to imagine)2 Ni agradable Ni activo Ni dif?
?cil ni fa?cilni desagradable ni pasivo de imaginar(In between) (In between) (In between)3 Agradable Activo Fa?cil de imaginar(Pleasant) (Active) (Easy to imagine)Table 1: Possible values for each of the three dimensions.sions used in Whissell?s work.
Importantly, theseconcepts were not defined, to avoid biasing the judg-ments.
Volunteers were also encouraged to followtheir first impression, and told that there were no?correct?
answers.
Appendix A shows the actual lo-gin and instructions pages used in the study.After reading the instructions, volunteers pro-ceeded to judge two practice words, intended to helpthem get used to the task and the interface, followedby 20 target words.
Words were presented one perpage.
Figure 3 shows a screenshot of the page forrating the word navegarverb.
Note that the word class6http://www.recaptcha.net23Figure 3: Screenshot of the web page for rating a word.
(verb in this example) is indicated right below theword.
After completing the first batch of 20 words,volunteers were asked if they wanted to finish thestudy or do a second batch, and then a third, a fourth,and so on.
This way, they were given the chance todo as many words as they felt comfortable with.
Ifa volunteer left before completing a batch, his/herratings so far were also recorded.3.2 Volunteers662 volunteers participated in the study, with a meanage of 33.3 (SD = 11.2).
As to their level of educa-tion, 76% had completed a university degree, 23%had finished only secondary school, and 1% hadcompleted only primary school.
Only volunteerswhose native language was Spanish were allowedto participate in the study.
Each volunteer was as-signed 20 words following this procedure: (1) The175,413 words in the corpus were sorted by wordcount.
(2) Words that had already received 5 or moreratings were excluded.
(3) Words that had alreadybeen rated by a volunteer with the same month andyear of birth were excluded, to prevent the same vol-unteer from rating twice the same word.
(4) The top20 words were selected.Each volunteer rated 52.3 words on average (SD= 34.0).
Roughly 30% completed 20 words orfewer; 24% completed 21-40 words; 18%, 41-60words; and the remaining 28%, more than 60 words.3.3 Descriptive statisticsA total of 2566 words were rated by at least 5 volun-teers.
Words with fewer annotations were excludedfrom the study.
We assigned each rating a numericvalue from 1 to 3, as shown in Table 1.
Table 2shows some basic statistics for each of the three di-mensions.Mean SD Skewness KurtosisPleasantness 2.23 0.47 ?0.47 ?0.06Activation 2.33 0.48 ?0.28 ?0.84Imagery 2.55 0.42 ?0.90 0.18Table 2: Descriptive statistics for the three dimensions.The five most pleasant words, according to thevolunteers, were jugarverb, besonoun, sonrisanoun,compan??
?anoun and reirverb (play, kiss, smile, com-pany and laugh, respectively).
The least pleas-ant ones were asesinatonoun, caroadj, ahogarverb,heridanoun and cigarronoun (murder, expensive,drown, wound and cigar).Among the most active words appear ideanoun,publicarverb, violentoadj, sexualadj and talentonoun(idea, publish, violent, sexual and talent).
Amongthe least active, we found yacerverb, espiritualadj,quietoadj, esperarverb and cada?veradj (lay, spiritual,still, wait and corpse).The easiest to imagine include sucioadj, silen-cionoun, darverb, peznoun and pensarverb (dirty, si-lence, give, fish and think).
Finally, the hardestto imagine include consistirverb, constarverb, mor-folog?
?anoun, piedadnoun and tendencianoun (consist,consist, morphology, compassion and tendency).We conducted Pearson?s correlation tests betweenthe different dimensions.
Table 3 shows the correla-tion matrix.
Correlations among rating dimensionswere very weak, which supports the assumption thatpleasantness, activation and imagery are three inde-pendent affective dimensions.
These numbers arevery similar to the ones reported in Whissell?s work.Pleasantness Activation ImageryPleasantness 1.00 0.14 0.10Activation 0.14 1.00 0.11Imagery 0.10 0.11 1.00Table 3: Correlation between the different dimensionsNext, we computed Cohen?s ?
to measure the de-gree of agreement above chance between volunteers(Cohen, 1968).7 Given that we used a three-pointscale for rating each affective dimension, we used7This measure of agreement above chance is interpreted asfollows: 0 = None, 0 - 0.2 = Small, 0.2 - 0.4 = Fair, 0.4 - 0.6 =Moderate, 0.6 - 0.8 = Substantial, 0.8 - 1 = Almost perfect.24a weighted version of ?, thus taking into accountthe distance on that scale between disagreements.For example, the distance between pleasant and un-pleasant was 2, and the distance between pleasantand in-between was 1.
We obtained a weighted ?measure of 0.42 for pleasantness, 0.30 for activation,and 0.14 for imagery.
Considering that these werehighly subjective rating tasks, the agreement lev-els for pleasantness and activation were quite high.The imagery task seemed somewhat more difficult,although we still observed some agreement abovechance.
These results indicate that our knowledgebase managed to, at least partially, capture informa-tion regarding the three affective dimensions.4 EvaluationNext we proceeded to evaluate the usefulness of ourknowledge base.
For this purpose, we developed asimple system for estimating affect along our threeaffective dimensions, and evaluated it on two differ-ent sentiment-analysis tasks.
The first task consistedin a set of texts labeled by humans, and served tocompare the judgments of human labelers with thepredictions of our system.
The second task consistedin classifying a set of user product reviews into ?pos-itive?
or ?negative?
opinions, a common applicationfor online stores.4.1 Simple system for estimating affectWe created a simple computer program for automat-ically estimating the degree of pleasantness, acti-vation and imagery of an input text, based on theknowledge base described in the previous sections.For each word in the knowledge base, we cal-culated its mean rating for each dimension.
Sub-sequently, for an input text T we used Freeling togenerate a full syntactic parsing, from which we ex-tracted all ?word, word-class?
pairs in T .
The systemcalculates the value for affective dimension d usingthe following procedure:score?
0count?
0for each word w in T (counting repetitions):if w is included in KB:score?
score+KBd(w)count?
count+ 1return score/countwhere KB is our knowledge base, and KBd(w) isthe value for w in KB for dimension d.For example, given the sentence ?Mi amiga espe-raba terminar las pruebas a tiempo?
(?My female-friend was hoping to finish the tests on time?
), andassuming our knowledge base contains the numbersshown in Table 4, the three values are computed asfollows.
First, all words are lemmatized (i.e., miamigo esperar terminar el prueba a tiempo).
Sec-ond, the mean of each dimension is calculated withthe described procedure, yielding a pleasantness of2.17, activation of 2.27 and imagery of 2.53.word word-class mean P mean A mean Iamigo noun 3.0 2.4 3esperar verb 1.2 1 2.8poder verb 2.8 2.8 2.2terminar verb 2.2 3 2.8prueba noun 1.8 2.4 2.2tiempo noun 2 2 2.2mean: 2.17 2.27 2.53Table 4: Knowledge base for the example text (P = pleas-antness; A = activation; I = imagery).It is important to mention that this system is just aproof of concept, motivated by the need to evaluatethe effectiveness of our knowledge base.
It could beused as a baseline system against which to comparemore complex affect estimation systems.
Also, ifresults are good enough with such a simple system,this would indicate that the information containedin the knowledge base is useful, and in the future itcould help create more complex systems.4.2 Evaluation #1: Emotion estimationThe first evaluation task consisted in comparing pre-dictions made by our simple system against rat-ings assigned by humans (our gold standard), on anumber of sentences and paragraphs extracted fromWikipedia and Los Cuentos.4.2.1 Gold standardFrom each corpus we randomly selected 15 sen-tences with 10 or more words, and 5 paragraphs withat least 50 words and two sentences ?
i.e.
30 sen-tences and 10 paragraphs in total.
These texts weresubsequently rated by 5 volunteers (2 male, 3 fe-male), who were instructed to rate each entire text(sentence or paragraph) for pleasantness, activation25and imagery using the same three-point scale shownin Table 1.
The weighted ?measure for these ratingswas 0.17 for pleasantness, 0.17 for activation and0.22 for imagery.
Consistent with the subjectivityof these tasks, the degree of inter-labeler agreementwas rather low, yet still above chance level.
Notealso that for pleasantness and activation the agree-ment level was lower for texts than for individualwords, while the opposite was true for imagery.4.2.2 ResultsTo evaluate the performance of our system, weconducted Pearson?s correlation test for each affec-tive dimension, in order to find the degree of cor-relation between the system?s predictions for the 40texts and their corresponding mean human ratings.Table 5 shows the resulting ?
coefficients.System \ GS Pleasantness Activation ImageryPleasantness 0.59 * 0.15 * ?0.18 *Activation 0.13 * 0.40 * 0.14 *Imagery 0.16 0.19 0.07Table 5: Correlations between gold standard and system?spredictions.
Statistically significant results are markedwith ?*?
(t-tests, p < 0.05).The coefficient for pleasantness presented a highvalue at 0.59, which indicates that the system?s esti-mation of pleasantness was rather similar to the rat-ings given by humans.
For activation the correlationwas weaker, although still significant.
On the otherhand, for imagery this simple system did not seemable to successfully emulate human judgments.These results suggest that, at least for pleasant-ness and activation, our knowledge base success-fully captured useful information regarding how hu-mans perceive those affective dimensions.
For im-agery, it is not clear whether the information basedid not capture useful information, or the estimationsystem was too simplistic.4.2.3 Effect of word count on performanceNext we studied the evolution of performance asa function of the knowledge base size, aiming at as-sessing the potential impact of increasing the num-ber of words annotated by humans.
Figure 4 sum-marizes the results of a simulation, in which succes-sive systems were built and evaluated using the top250, 350, 450, ..., 2350, 2450 and 2566 words in ourknowledge base.The green line (triangles) represents the meancoverage of the system?s knowledge base on the goldstandard texts; the corresponding scale is shown onthe right axis.
Similarly to Figure 2, the coveragegrew rapidly, starting at 18% when using 250 wordsto 44% when using all 2566 words.The blue (circles), red (squares) and purple (di-amonds) lines correspond to the correlations of thesystem?s predictions and the gold standard ratingsfor pleasantness, activation and imagery, respec-tively; the corresponding scale is shown on the leftaxis.
The black lines are a logarithmic function fit toeach of the three curves (?2 = 0.90, 0.72 and 0.68,respectively).Figure 4: Evolution of the correlation between systempredictions and Gold Standard, with respect to the knowl-edge base size.These results indicate that the system perfor-mance (measured as the correlation with humanjudgments) grew logarithmically with the number ofwords in the knowledge base.
Interestingly, the per-formance grew at a slower pace than word cover-age.
In other words, an increase in the proportionof words in a text that were known by the systemdid not lead to a similar increase in the accuracy ofthe predictions.
An explanation may be that, oncean emotion had been established based on a percent-age of words in the text, the addition of a few extrawords did not significantly change the outcome.In consequence, if we wanted to do a substantialimprovement to our baseline system, it would prob-ably not be a good idea to simply annotate more26words.
Instead, it may be more effective to workon how the system uses the information contained inthe knowledge base.4.3 Evaluation #2: Classification of reviewsThe second evaluation task consisted in using ourbaseline system for classifying user product reviewsinto positive or negative opinions.4.3.1 CorpusFor this task we used a corpus of 400 user reviewsof products such as cars, hotels, dishwashers, books,cellphones, music, computers and movies, extractedfrom the Spanish website Ciao.es.8 This is the samecorpus used by Brooke (2009), who employed senti-ment analysis tools in English to score Spanish textsafter performing machine translation.On Ciao.es, users may enter their written reviewsand associate a numeric score to them, ranging from1 to 5 stars.
For this evaluation task, we made theassumption that there was a strong relation betweenthe written reviews and their corresponding numericscores.
Following this assumption, we tagged re-views with 1 or 2 stars as ?negative?
opinions, andreviews with 4 or 5 stars as ?positive?.
Reviews with3 stars were considered neutral, and ignored.4.3.2 ResultsWe used our system in a very simple way for pre-dicting the polarity of opinions.
First we computedM , the mean pleasantness score on 80% of the re-views.
Subsequently, for each review in the remain-ing 20%, if its pleasantness score was greater thanM , then it was classified as ?positive?
; otherwise, itwas classified as ?negative?.After repeating this procedure five times using5-fold cross validation, the overall accuracy was62.33%.
Figure 5 shows the evolution of the sys-tem?s accuracy with respect to the number of wordsin the knowledge base.
The green line (triangles)represents the mean coverage of the system?s knowl-edge base on user review texts; the correspondingscale is shown on the right axis.
The blue line (cir-cles) corresponds to the classification accuracy; thecorresponding scale is shown on the left axis.
Theblack line is a logarithmic function fit to this curve(?2 = 0.80).8http://ciao.esFigure 5: Evolution of the classification accuracy withrespect to the size of the knowledge base.Notably, with as few as 500 words the accuracyis already significantly above chance level, which is50% for this task.
This indicates that our knowl-edge base managed to capture information on pleas-antness that may aid the automatic classification ofpositive and negative user reviews.Also, similarly to our first evaluation task, weobserve that the accuracy increased as more wordswere added to the knowledge base.
However, it didso at a logarithmic pace slower than the growth ofthe word coverage on the user reviews.
This sug-gests that adding more words labeled by humans tothe knowledge base would only have a limited im-pact on the performance of this simple system.5 ConclusionIn this work we presented a knowledge base of Span-ish words labeled by human volunteers for threeaffective dimensions ?
pleasantness, activation andimagery, inspired by the English DAL created byWhissell (1986; 1989).
The annotations of thesethree dimensions were weakly intercorrelated, indi-cating a high level of independence of each other.Additionally, the agreement between volunteers wasquite high, especially for pleasantness and activa-tion, given the subjectivity of the labeling task.To evaluate the usefulness of our lexicon, we builta simple emotion prediction system.
When used forpredicting the same three dimensions on new texts,its output significantly correlated with human judg-ments for pleasantness and activation, but the results27for imagery were not satisfactory.
Also, when usedfor classifying the opinion polarity of user productreviews, the system managed to achieve an accuracybetter than random.
These results suggest that ourknowledge base successfully captured useful infor-mation of human perception of, at least, pleasant-ness and activation.
For imagery, either it failed tocapture any significant information, or the systemwe created was too simple to exploit it accordingly.Regarding the evolution of the system?s perfor-mance as a function of the size of the lexicon, theresults were clear.
When more words were included,the system performance increased only at a loga-rithmic pace.
Thus, working on more complex sys-tems seems to be more promising than adding morehuman-annotated words.In summary, this work presented a knowledgebase that may come handy to researchers and de-velopers of sentiment analysis tools in Spanish.
Ad-ditionally, it may be useful for disciplines that needto select emotionally balanced word stimuli, such asNeuroscience or Psycholinguistics.
In future workwe will compare the usefulness of our manuallyannotated lexicon and cross-linguistic approaches(Brooke et al 2009; Pe?rez-Rosas et al 2012).AcknowledgmentsThis work was funded in part by ANPCYT PICT-2009-0026 and CONICET.
The authors thank Carlos ?Greg?Diuk and Esteban Mocskos for valuable suggestions andcomments, and Julian Brooke, Milan Tofiloski and MaiteTaboada for kindly sharing the Ciao corpus.ReferencesJ.
Brooke, M. Tofiloski, and M. Taboada.
2009.
Cross-linguistic sentiment analysis: From English to Span-ish.
In International Conference on Recent Advancesin NLP, Borovets, Bulgaria, pages 50?54.J.
Cohen.
1968.
Weighted kappa: Nominal scaleagreement provision for scaled disagreement or partialcredit.
Psychological bulletin, 70(4):213.R.
Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis,S.
Kollias, W. Fellenz, and J.G.
Taylor.
2001.
Emo-tion recognition in human-computer interaction.
Sig-nal Processing Magazine, IEEE, 18(1):32?80.J.R.
Gray, T.S.
Braver, and M.E.
Raichle.
2002.
Integra-tion of emotion and cognition in the lateral prefrontalcortex.
Proceedings of the National Academy of Sci-ences, 99(6):4115.L.
Padro?, M. Collado, S. Reese, M. Lloberes, andI.
Castello?n.
2010.
Freeling 2.1: Five years of open-source language processing tools.
In InternationalConf.
on Language Resources and Evaluation (LREC).V.
Pe?rez-Rosas, C. Banea, and R. Mihalcea.
2012.Learning sentiment lexicons in spanish.
In Int.
Conf.on Language Resources and Evaluation (LREC).C.
Whissell, M. Fournier, R. Pelland, D. Weir, andK.
Makarec.
1986.
A dictionary of affect in language:Iv.
reliability, validity, and applications.
Perceptualand Motor Skills, 62(3):875?888.Cynthia Whissell.
1989.
The dictionary of affect in lan-guage.
Emotion: Theory, research, and experience,4:113?131.J.
Yi, T. Nasukawa, R. Bunescu, and W. Niblack.
2003.Sentiment analyzer: Extracting sentiments about agiven topic using NLP techniques.
In 3rd IEEE Int.Conf.
on Data Mining, pages 427?434.
IEEE.A Login and instructions pagesFigures 6 and 7 show the screenshots of the login andinstructions pages of our web interface for rating words.Figure 6: Screenshot of the login page.Figure 7: Screenshot of the instructions page.28
