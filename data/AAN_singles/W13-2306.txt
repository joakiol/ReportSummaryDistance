Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 42?50,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsEntailment: An Effective Metric for Comparing and EvaluatingHierarchical and Non-hierarchical Annotation SchemesRohan Ramanath?R.
V. College of Engineering, Indiaronramanath@gmail.comMonojit Choudhury Kalika BaliMicrosoft Research Lab India{monojitc, kalikab}@microsoft.comAbstractHierarchical or nested annotation of lin-guistic data often co-exists with simplernon-hierarchical or flat counterparts, aclassic example being that of annotationsused for parsing and chunking.
In thiswork, we propose a general strategy forcomparing across these two schemes ofannotation using the concept of entailmentthat formalizes a correspondence betweenthem.
We use crowdsourcing to obtainquery and sentence chunking and showthat entailment can not only be used asan effective evaluation metric to assess thequality of annotations, but it can also beemployed to filter out noisy annotations.1 IntroductionLinguistic annotations at all levels of linguistic or-ganization ?
phonological, morpho-syntactic, se-mantic, discourse and pragmatic, are often hierar-chical or nested in nature.
For instance, syntac-tic dependencies are annotated as phrase structureor dependency trees (Jurafsky and Martin, 2000).Nevertheless, the inherent cognitive load associ-ated with nested segmentation and the sufficiencyof simpler annotation schemes for building NLPapplications have often lead researchers to definenon-hierarchical or flat annotation schemes.
Theflat annotation, in essence, is a ?flattened?
ver-sion of the tree.
For instance, chunking of Natu-ral Language (NL) text, which is often consideredan essential preprocessing step for many NLP ap-plications (Abney, 1991; Abney, 1995), is, looselyspeaking, a flattened version of the phrase struc-ture tree.
The closely related task of Query Seg-mentation is of special interest to us here, as it is?The work was done during author?s internship at Mi-crosoft Research Lab India.f Pipe representation Boundary var.3 barbie dress up | games 0 0 13 barbie dress | up games 0 1 02 barbie | dress up | games 1 0 12 barbie | dress up games 1 0 0Table 1: Example of flat segmentations from 10Turkers.
f is the frequency of annotations; seg-ment boundaries are represented by |.the first step in further analysis and understandingof Web search queries (Hagen et al 2011).The task in both query and sentence chunking isto divide the string of words into contiguous sub-strings of words (commonly refered to as segmentsor chunks) such that the words from a segmentare related to each other more strongly than wordsfrom different segments.
It is typically assumedthat the segments are syntactically and semanti-cally coherent.
Table 1 illustrates the concept ofsegmentation of a query.
The crowdsourced an-notations for this data were obtained from 10 an-notators, the experimental details of which will bedescribed in Sec.
5.
We shall refer to this style oftext chunking as flat segmentation.Nested segmentation of a query or a sentence,on the other hand, is a recursive application of flatsegmentation, whereby the longer flat segmentsare further divided into smaller chunks recursively.The process stops when a segment consists of lessthan three words or is a multiword entity that can-not be segmented further.
This style of segmenta-tion can be represented through nested parenthe-sization of the text, as illustrated in Table 2.
Theseannotations were also obtained through the samecrowdsourcing experiment (Sec.
5).
Fig.
1 showsan alternative visualization of a nested segmenta-tion in the form of a tree.An important problem that arises in the con-text of flat segmentation is the issue of granular-42f Bracket representation Boundary var.4 ((barbie dress)( up games)) 0 1 03 (barbie ((dress up) games)) 2 0 12 (barbie (dress (up games))) 2 1 01 ((barbie (dress up)) games) 1 0 2Table 2: Example of nested segmentation from 10Turkers.
f is the frequency of annotations.2barbie 10dress upgamesFigure 1: Tree representation of the nested seg-mentation: (barbie ((dress up) games))ity.
For instance, in the case of NL chunking, itis not clear whether the chunk boundaries shouldcorrespond to the innermost parentheses in thenested segmentation marking very short chunks,or should one annotate the larger chunks corre-sponding to clausal boundaries.
For this reason,Inter-Annotator Agreement (IAA) for flat annota-tion tasks is often poor (Bali et al 2009; Hagenet al 2011; Saha Roy et al 2012).
However, lowIAA does not necessarily imply low quality anno-tation, and could as well be due to the inherent am-biguity in the task definition with respect to gran-ularity.
Although we have illustrated the conceptand problems of flat and nested annotations usingthe examples of sentence and query segmentation,these issues are generic and typical of any flat an-notation scheme which tries to flatten or approx-imate an underlying hierarchical structure.
Thereare three important research questions pertainingto the linguistic annotations of this kind:?
How to measure the true IAA and the qualityof the flat annotations??
How to compare the agreement between theflat and the nested annotations??
How can we identify or construct the opti-mal or error-free flat annotations from a noisymixture of nested and flat annotations?In this paper, we introduce the concept of ?en-tailment of a flat annotation by a nested annota-tion?.
For a given linguistic unit (a query or a sen-tence, for example), a nested annotation is said toentail a flat annotation if the structure of the lat-ter does not contradict the more specific structurerepresented by the former.
Based on this simplenotion, which will be formalized in Sec.
3, wedevelop effective techniques for comparing acrossand evaluating the quality of flat and nested an-notations, and identifying the optimal flat annota-tion.
We validate our theoretical framework on thetasks of query and sentence segmentation.
In par-ticular, we conduct crowdsourcing based flat andnested segmentation experiments for Web searchqueries and sentences using Amazon MechanicalTurk (AMT)1.
We also obtain annotations for thesame datasets by trained experts which are ex-pected to be of better quality than the AMT-basedannotations.
Various statistical analyses of the an-notated data bring out the effectiveness of entail-ment as a metric for comparison and evaluation offlat and nested annotations.The rest of the paper is organized as fol-lows.
Sec.
2 provides some background on theannotation tasks and related work on IAA.
InSec.
3, we introduce the notion of entailmentand develop theoretical models and relatedstrategies for assessing the quality of annotation.In Sec.
4, we introduce some strategies basedon entailment for the identification of error-freeannotations from a given set of noisy annotations.Sec.
5 describes the annotation experimentsand results.
Sec.
6 concludes the paper bysummarizing the work and discussing futureresearch directions.
All the annotated datasetsused in this research can be obtained freely fromhttp://research.microsoft.com/apps/pubs/default.aspx?id=192002and used for non-commercial research purposes.2 BackgroundSegmentation or chunking of NL text is a well-studied problem.
Abney (1991; 1992; 1995)defines a chunk as a sub-tree within a syntac-tic phrase structure tree corresponding to Noun,Prepositional, Adjectival, Adverbial and VerbPhrases.
Similarly, Bharati et al1995) define itas Noun Group and Verb Group based only on lo-cal surface information.
Chunking is an importantpreprocessing step towards parsing.Like chunking, query segmentation is an im-portant step towards query understanding and isgenerally believed to be useful for Web search1https://www.mturk.com/mturk/welcome43(see Hagen et al(2011) for a survey).
Auto-matic query segmentation algorithms are typicallyevaluated against a small set of human-annotatedqueries (Bergsma and Wang, 2007).
The reportedlow IAA for such datasets casts serious doubts onthe reliability of annotation and the performanceof the algorithms evaluated on them (Hagen et al2011; Saha Roy et al 2012).
To address the is-sue of data scarcity, Hagen et al(2011) createda large set of manually segmented queries throughcrowdsourcing2.
However, their approach has cer-tain limitations because the crowd is already pro-vided with a few possible segmentations of a queryto choose from.
Nevertheless, if large scale datahas to be procured crowdsourcing seems to be theonly efficient and effective model for the task, andhas been proven to be so for other IR and lin-guistic annotations (see Lease et al(2011) forexamples).
It should be noted that almost all thework on query segmentation, except (Huang et al2010), has considered only flat segments.An important problem that arises in the contextof flat annotations is the issue of granularity.
In theabsence of a set of guidelines that explicitly statethe granularity expected, Inter-Annotator Agree-ment (IAA) for flat annotation tasks are often poor.Bali et al(2009) showed that for NL chunking,annotators typically agree on major (i.e., clausal)boundaries but do not agree on minor (i.e., phrasalor intra-phrasal) boundaries.
Similarly, for querysegmentation, low IAA remains an issue (Hagenet al 2011; Saha Roy et al 2012).The issue of granularity is effectively addressedin nested annotation, because the annotator is ex-pected to mark the most atomic segments (suchas named entities and multiword expressions) andthen recursively combine them to obtain largersegments.
Certain amount of ambiguity, that mayarise because of lack of specific guidelines on thenumber of valid segments at the last level (i.e., top-most level of the nested segmentation tree), canalso be resolved by forcing the annotator to recur-sively divide the sentence/query always into ex-actly two parts (Abney, 1992; Bali et al 2009).The present study is an extension of our recentwork (Ramanath et al 2013) on analysis of theeffectiveness of crowdsourcing for query and sen-tence segmentation.
We introduced a novel IAAmetric based on Kripendorff?s ?, and showed thatwhile the apparent agreement between the annota-2http://www.webis.de/research/corporators in a crowdsourced experiment might be high,the chance corrected agreement is actually low forboth flat and nested segmentations (as comparedto gold annotations obtained from three experts).The reason for the apparently high agreement isdue to an inherent bias of the crowd to dividea piece of text in roughly two equal parts.
Thepresent study extends this work by introducing ametric to compare across flat and nested segmen-tations that enables us to further analyze the relia-bility of the crowdsourced annotations.
This met-ric is then employed to identify the optimal flatsegmentation(s) from a set of noisy annotations.The study uses the same experimental setup andannotated datasets as described in (Ramanath etal., 2013).
Nevertheless, for the sake of readabilityand self-containedness, the relevant details will bementioned here again.We do not know of any previous work that com-pares flat and nested schemes of annotation.
Infact, Artstein and Poesio (2008), in a detailed sur-vey of IAA metrics and their usage in NLP, men-tion that defining IAA metrics for trees (hierarchi-cal annotations) is a difficult problem due to theexistence of overlapping annotations.
Vadas andCurran (2011) and Brants (2000) discuss measur-ing IAA of nested segmentations employing theconcepts of precision, recall, and f-score.
How-ever, neither of these studies apply statistical cor-rection for chance agreement.3 Entailment: Definition and ModelingIn this section, we shall introduce certain notationsand use them to formalize the notion of entail-ment, which in turn, is used for the computation ofagreement between flat and nested segmentations.Although we shall develop the whole frameworkin the context of queries, it is applicable to sen-tence segmentation and, in fact, more generally toany flat and nested annotations.3.1 Basic DefinitionsLet Q be the set of all queries.
A query q ?
Qcan be represented as a sequence of |q| words:w1w2 .
.
.
w|q|.
We introduce |q| ?
1 random vari-ables, b1, b2, .
.
.
b|q|?1, such that bi represents theboundary between the words wi and wi+1.
A flatand nested segmentation of q, represented by F jqand N jq respectively, j varying from 1 to totalnumber of annotations, c, is a particular instan-tiation of these boundary variables as follows.44Definition.
Flat Segmentation: A flat segmen-tation, F jq , can be uniquely defined by a binaryassignment of the boundary variables bji , wherebji = 1 iff wi and wi+1 belong to two different flatsegments.
Otherwise, bji = 0.
Thus, q has 2|q|?1possible flat segmentations.Definition.
Nested Segmentation: A nested seg-mentation, N jq , is defined as an assignment ofnon-negative integers to the boundary variablessuch that bji = 0 iff words wi and wi+1 form anatomic segment (i.e., they are grouped together),else bji = 1 + max(lefti, righti), where leftiand righti are the heights of the largest subtreesending at wi and beginning at wi+1 respectively.This numbering scheme can be understoodthrough Fig.
1.
Every internal node of the binarytree corresponding to the nested segmentation isnumbered according to its height.
The lowest in-ternal nodes, both of whose children are querywords, are assigned a value of 0.
Other internalnodes get a value of one greater than the heightof its higher child.
Since every internal node cor-responds to a boundary, we assign the height ofthe node to the corresponding boundary variables.The number of unique nested segmentations of qis the corresponding Catalan number3 C|q|?1.Note that, following Abney?s (1992) suggestionfor nested chunking, we define nested segmenta-tion as a strict binary tree or binary bracketing ofthe query.
This is not only helpful for theoreticalanalysis, but also necessary to ensure that thereis no ambiguity related to the granularity of seg-ments.3.2 EntailmentGiven a nested segmentation N jq , there are severalpossible ways to ?flatten?
it.
Flat segmentations ofq, where bi = 0 for all i (i.e., the whole query isone segment) and bi = 1 for all i (i.e., all words arein different segments) are trivially obtainable fromN jq , and therefore, are not neither informative norinteresting.
Intuitively, any flat segmentation, F kq ,can be said to agree with N jq if for every flat seg-ment in F kq there is a corresponding internal nodein N jq , such that the subgraph rooted at that nodespans (contains) all and only those words presentin the flat segment (Abney, 1991).Let us take the examples of flat and nestedsegmentations shown in Tables 1 and 2 to illus-3http://en.wikipedia.org/wiki/Catalan\_numbertrate this notion.
Consider two nested segmenta-tions, N1q = ((barbie (dress up)) games), N2q =(barbie ((dress up) games)) and three flat seg-mentations, F 1q = barbie | dress up | games,F 2q = barbie | dress up games, F3q =barbie dress | up games.
Figure 2 diagram-matically compares the two nested segmentations(the two rows) with the three flat segmentations(columns A, B and C).
There are three flat seg-ments in F 1q , of which the two single wordsegments barbie and games trivially coincidewith the corresponding leaf nodes.
The segmentdressup coincides exactly with the words spannedby the node marked 0 of N1q (Fig.
2, top row, col-umn A).
Hence, F 1q can be said to be in agree-ment withN1q .
On the other hand, there is no nodein N1q , which exactly coincides with the segmentdressupgames of F 2q (Fig.
2, top row, column B).Hence, we say that N1q does not agree with F2q .We formalize this notion of agreement in termsof entailment, which is defined as follows.Definition: Entailment.
A nested segmentation,N jq is said to entail a flat segmentation, Fkq , (orequivalently, F kq is entailed by Njq ) if and only iffor every multiword segment wi+1, wi+2, ..., wi+lin F kq , the corresponding boundary variables inN jq follows the constraint: bi > bi+m and bi+l >bi+m for all 1 ?
m < l.It can be proved that this definition of entail-ment is equivalent to the intuitive description pro-vided earlier.
Yet another equivalent definition ofentailment is presented in the form of Algorithm 1.Due to paucity of space, the proofs of equivalenceare omitted.Definition: Average Observed Entailment.
Forthe set of queries Q, and corresponding sets ofc flat and nested segmentations, there are |Q|c2pairs of flat and nested segmentations that can becompared for entailment.
We define the averageobserved entailment for this annotation set as thefraction of these |Q|c2 annotation pairs for whichthe flat segmentation is entailed by the correspond-ing nested segmentation.
We shall express thisfraction as percentage.3.3 Entailment by Random ChanceAverage observed Entailment can be consideredas a measure of the IAA, and hence, an indica-tor of the quality of the annotations.
However,in order to interpret the significance of this value,we need an estimate of the average entailment that45Figure 2: Every node of the tree represent boundary values, nested(flat).
Column A: F 1q is entailed byboth N1q and N2q , Column B: F2q is entailed by N2q but not N1q , Column C: F3q is entailed by neitherN1q nor N2q .
The nodes (or equivalently the boundaries) violating the entailment constraint are marked across, and those agreeing are marked with ticks.Algorithm 1 Algorithm: isEntail1: procedure ISENTAIL(flat, nested) .
flat,nested are lists containing boundary values2: if len(nested) ?
1 or len(flat) ?
1 then3: return True4: end if5: h?
largest element in nested6: i?
index of h7: if flat[i] = 1 then8: if !
isEntail(flat[: i], nested[: i]) or!
isEntail(flat[i+1 :], nested[i+1 :]) then9: return False10: else11: return True12: end if13: else14: while h 6= 0 do15: nested[i]?
?nested[i]16: h?
largest element in nested17: i?
index of h18: if flat[i] = 1 then19: return False20: end if21: end while22: return True23: end if24: end procedureone would expect if the annotations, both flat andnested, were drawn uniformly at random from theset of all possible annotations.
From our exper-iments we observe that trivial flat segmentationsare, in fact, extremely rare, and a very large frac-tion of the flat annotations have two or three seg-ments.
Therefore, for computing the chance en-tailment, we assume that the number of segmentsin the flat segmentation is known and fixed, whichis either 2 or 3, but all segmentations with thesemany segments are equally likely to be chosen.We also assume that all nested segmentations areequally likely.When there are 2 segments: For a query q, thenumber of flat segmentations with two segments,i.e., one boundary, is(|q|?11)= |q| ?
1.
Notethat for any nested segmentation N jq , all flat seg-mentations that have at least one boundary and isentailed by it must have a boundary between wi?and wi?+1, where bi?
has the highest value in N jq .In other words, bi?
is the boundary correspondingto the root of the nested tree (the proof is intu-itive and is omitted).
Therefore, there is exactlyone ?flat segmentation with one boundary?
that isentailed by a given N jq .
Therefore, the randomchance that a nested segmentation N jq will entaila flat segmentation with one boundary is given by(|q| ?
1)?1 (for |q| > 1).When there are 3 segments: Number of flatsegmentations with two boundaries is(|q|?12).
Theflat segmentation(s) entailed by N jq can be gener-ated as follows.
As argued above, every flat seg-mentation entailed by N jq must have a boundary46at position i?.
The second boundary can be eitherin the left or right of i?.
But in either case, thechoice of the boundary is unique which will corre-spond to the highest node in the left or right sub-tree of the root node.
Thus, every nested segmen-tation entails at most 2 flat segmentations.
How-ever, if i?
= 1 or |q| ?
1 for a N jq , then, respec-tively, the left or right subtrees do not exist.
Insuch cases, there is only one flat segmentation en-tailed by N jq .
Note that there are exactly C|q|?2nested segmentations for which the i?
= 1, andsimilarly another C|q|?2 for which i?
= |q| ?
1.Therefore, out of C|q|?1 ?
(|q|?12)pairs, exactly2C|q|?1?2C|q|?2 pairs satisfy the entailment con-ditions.
Thus, the expected probability of entail-ment by random chance when there are exactlytwo boundaries in the flat segmentation of q is:2(C|q|?1 ?
C|q|?2)C|q|?1(|q|?12) = 2(|q| ?
12)?1(1?C|q|?2C|q|?1)The values of the probability of observing a ran-dom nested segmentation entailing a flat segmen-tation with exactly two boundaries for |q| =3, 4, 5, 6, 7 and 8 are 1, 0.4, 0.213.
0.133, 0.091and 0.049 respectively.3.4 Other IAA MetricsAlthough entailment can be used as a measure ofagreement between flat and nested segmentations,IAA within flat or within nested segmentationscannot be computed using this notion.
In (Ra-manath et al 2013), we have extensively dealtwith the issue of computing IAA for these cases.Krippendorff?s ?
(Krippendorff, 2004), which isan extremely versatile agreement coefficient, hasbeen appropriately modified to be applicable to acrowdsourced annotation scenario.
?
= 1 im-plies perfect agreement, ?
= 0 implies that theobserved agreement is just as good as that by ran-dom chance, whereas ?
< 0 implies that the ob-served agreement is less than that one would ex-pect by random chance.
Due to paucity of spacewe omit any further discussion on this and referthe reader to (Ramanath et al 2013).
Here, wewill use the ?
values as an alternative indicator ofIAA and therefore, the quality of annotation.4 Optimal SegmentationSuppose that we have a large number of flat andnested annotations coming from a noisy sourcesuch as crowdsourcing; is it possible to employthe notion of entailment to identify the annota-tions which are most likely to be correct?
Here,we describe two such strategies to obtain the opti-mal (error-free) flat segmentation.Flat Entailed by Most Nested (FEMN): Theintuition behind this approach is that if a flat seg-mentation F kq is entailed by most of the nestedsegmentations of q, then it is very likely that F kqis correct.
Therefore, for each flat segmentationsof q, we count the number of nested segmentationsof q that entail it, and the one with highest count isdeclared as the optimal FEMN segmentation.
It isinteresting to note that while computing the opti-mal FEMN segmentation, we never encountered atie between two flat segmentations.
The trivial flatsegmentations (i.e., if the whole query is one seg-ment or every word is in different segments) arefiltered as a preprocessing step.Iterative Voting (IV): FEMN assumes that thenested segmentations are relatively noise-free.
Ifmost of the nested segmentations are erroneous,FEMN would select an erroneous optimal flat seg-mentation.
To circumvent this issue, we propose amore sophisticated iterative voting process, wherewe count the number of flat segmentations entailedby each nested segmentation of q, and similarly,number of nested segmentations that entail eachflat segmentation.
The flat and nested segmenta-tions with the least scores are then removed fromthe dataset.
Then we recursively apply the IV pro-cess on the reduced set of annotations until we areleft with a single flat segmentation.5 Experiments and ResultsWe obtained nested and flat segmentation of Websearch queries through crowdsourcing as well asfrom trained experts.
Furthermore, we also con-ducted similar crowdsourcing experiments for NLsentences, which helped us understand the specificchallenges in annotating queries because of theirapparent lack of a well-defined syntactic structure.In this section, we first describe the experimen-tal setup and datasets, and then present the obser-vations and results.5.1 Crowdsourcing ExperimentIn this study we use the same set of crowd-sourced annotations as described in (Ramanathet al 2013).
For the sake of completeness, webriefly describe the annotation procedure here as47well.
We used Amazon Mechanical Turk for thecrowdsourcing experiments.
Two separate Hu-man Intelligence Tasks were designed for flat andnested segmentation.
The concept of flat andnested segmentation was introduced to the Turk-ers with the help of two short videos4.When in doubt regarding the meaning of aquery, the Turkers were advised to issue the queryon a search engine of their choice and find out itspossible interpretation(s).
Only Turkers who hadcompleted more than 100 tasks at an acceptancerate of ?
60% were allowed to participate in thetask and were paid $0.02 for a flat and $0.06 for anested segmentation.
Every query was annotatedby 10 different annotators.5.2 DatasetThe following sets of queries and sentences wereused for annotations:Q500, QG500: Saha Roy et al(2012) re-leased a dataset of 500 queries, 5 to 8 words long,for the evaluation of various segmentation algo-rithms.
This dataset has flat segmentations fromthree annotators obtained under controlled exper-imental settings, and could be considered as Goldannotation.
Hence, we selected this set for our ex-periments as well.
We procured the correspond-ing nested segmentation for these queries fromtwo human experts who are regular search engineusers.
They annotated the data under supervisionand were trained and paid for the task.
We shallrefer to the set of flat and nested gold annotationsas QG500, whereas Q500 will be reserved for thedataset procured through the AMT experiments.Q700: As 500 queries are not enough for mak-ing reliable conclusions and also, since the queriesmay not have been chosen specifically for the pur-pose of annotation experiments, we expanded theset with another 700 queries sampled from thelogs of a popular commercial search engine.
Wepicked, uniformly at random, queries that were 4to 8 words long.S300: We randomly selected 300 English sen-tences from a collection of full texts of public do-main books5 that were 5 to 15 words long, andmanually checked them for well-formedness.4Flat: http://youtu.be/eMeLjJIvIh0, Nested:http://youtu.be/xE3rwANbFvU5http://www.gutenberg.org5.3 Entailment StatisticsTable 3 reports two statistics ?
the values ofKripendorff?s ?
and the average observed entail-ment (expressed as %) for flat and nested segmen-tations along with the corresponding expected val-ues for entailment by chance.
For nested segmen-tation, the ?
values were computed for two differ-ent distance metrics6 d1 and d2.As expected, the highest value of ?
for bothflat and nested segmentation is observed for thegold annotations.
An ?
> 0.6 indicates a rea-sonably good7 IAA, and thus, reliable annota-tions.
We note that the entailment statistics fol-low a very similar trend as ?, and for all the cases,the observed average entailment is much higherthan what we would expect by random chance.These two observations clearly point to the factthat entailment is indeed a good indicator of theagreement between the nested and flat segmenta-tions, and consequently, the reliability of the an-notations.
We also observe that the average en-tailment for S300 is in the same ballpark as forthe queries.
This indicates that the apparent lackof structure in queries does not specifically influ-ence the annotations.
Along the same lines, onecan also argue that the length of a text, whichis higher for sentences than queries, does not af-fect the crowdsourced annotations.
In fact, in ourprevious study (Ramanath et al 2013), we showthat it is the bias of the Turkers to divide a textin approximately two segments of equal size (ir-respective of other factors, like syntactic structureor length), that leads to very similar IAA acrossdifferent types of texts.
Our current study on en-tailment further strengthens this fact.Figure 3 plots the distribution of the entailmentvalues for the three datasets.
The distributions arenormal-like implying that entailment is a robustmetric and its average value is a usable statistic.In order to analyze the agreement between theTurkers and the experts, we computed the av-erage entailment between Q500 flat annotations(from AMT) with QG500 nested annotations, andsimilarly, Q500 nested annotations with QG5006Intuitively, for d1 disagreements between segmentboundaries are equally penalized at all the levels of nestedtree, whereas for d2 disagreements higher up the tree (i.e.,close to the root) are penalized more than those at lower lev-els.7It should be noted that there is no consensus on what isa good value of ?
for linguistic annotations, partly becauseit is dependent on the nature of the annotation task and thedemand of the end applications that use the annotated data.48Dataset Krippendorff?s ?
Entailment StatisticsFlat Nested Observed Chanced1 d1 d2Q700 0.21 0.21 0.16 49.68 12.63Q500 0.22 0.15 0.15 56.69 19.08QG500 0.61 0.66 0.67 87.07 11.91S300 0.27 0.18 0.14 52.86 19.12Table 3: ?
and Average Entailment StatisticsFigure 3: Distribution of the entailment values (x-axis) plotted as the % of comparable flat-nestedannotation pairs.Figure 4: Distribution of percentage of entailedpairs using QG500 as reference.flat annotations, which turned out to be 70.42%and 63.24% respectively.
The corresponding dis-tributions are shown as Nested and Flat in Fig.4.
Thus, the flat segmentations from the Turkersseem to be more accurate than their nested seg-mentations, a fact also supported by the ?
values.This could be due to the much higher cognitiveload associated with nested segmentation that de-mands more time and concentration that an ordi-nary Turker may not be willing to invest.5.4 Optimal Segmentation ResultsIn order to evaluate the optimal flat segmentationselection strategies, FEMN and IV, we computedthe percentage of queries in Q500 for which theoptimal flat segmentation (as obtained by apply-ing these strategies on AMT annotations) is en-tailed by the corresponding nested segmentationsin QG500.
The average entailment values forFEMN and IV turns out to be 79.60% and 82.80%respectively.
This shows that the strategies are in-deed able to pull out the more accurate flat seg-mentations from the set, though, as one would ex-pect, IV performs better than FEMN, and its cho-sen segmentations are almost as good as that byexpert annotators.Another experiment was conducted to preciselycharacterize the effectiveness of these strategieswhereby we mixed the annotations from the Q500and QG500, and then applied FEMN and IV topull out the optimal flat segmentations.
We ob-served that for 63.71% and 91.44% of the queries,the optimal segmentation chosen by FEMN and IVrespectively was indeed one of the three gold flatannotations in QG500.
This reinforces our con-clusion that IV can effectively identify the optimalflat segmentation of a query from a noisy set of flatand nested segmentations.6 ConclusionIn this paper, we proposed entailment as a theo-retical model for comparing hierarchical and non-hierarchical annotations.
We present a formaliza-tion of the notion of entailment and use it for de-vising two strategies, FEMN and IV, for identify-ing the optimal flat segmentation in a noisy set ofannotations.
One of the main contributions of thiswork resides in our following experimental find-ing: Even though annotations obtained throughcrowdsourcing for a difficult task like query seg-mentation might be very noisy, a small fraction ofthe annotations are nevertheless correct; it is pos-sible to filter out these correct annotations usingthe Iterative Voting strategy when both hierarchi-cal and non-hierarchical segmentations are avail-able from the crowd.The proposed model is generic and we be-lieve that the experimental findings extend beyondquery and sentence segmentation to other kinds oflinguistic annotations where hierarchical and non-hierarchical schemes co-exist.AcknowledgmentThanks to Rishiraj Saha Roy, IIT Kharagpur, forhis valuable inputs during this work.49ReferencesSteven P. Abney.
1991.
Parsing By Chunks.
KluwerAcademic Publishers.Steven P. Abney.
1992.
Prosodic Structure, Perfor-mance Structure And Phrase Structure.
In Proceed-ings 5th Darpa Workshop on Speech and NaturalLanguage, pages 425?428.
Morgan Kaufmann.Steven P. Abney.
1995.
Chunks and dependencies:Bringing processing evidence to bear on syntax.Computational Linguistics and the Foundations ofLinguistic Theory, pages 145?164.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Kalika Bali, Monojit Choudhury, Diptesh Chatterjee,Sankalan Prasad, and Arpit Maheswari.
2009.
Cor-relates between Performance, Prosodic and PhraseStructures in Bangla and Hindi: Insights from a Psy-cholinguistic Experiment.
In ICON ?09, pages 101?
110.Shane Bergsma and Qin Iris Wang.
2007.
Learn-ing Noun Phrase Query Segmentation.
In EMNLP-CoNLL ?07, pages 819?826.Akshar Bharati, Vineet Chaitanya, and Rajeev Sangal.1995.
Natural Language Processing: A PaninianPerspective.
Prentice.Thorsten Brants.
2000.
Inter-annotator agreement fora German newspaper corpus.
In In Proceedings ofSecond International Conference on Language Re-sources and Evaluation LREC-2000.Matthias Hagen, Martin Potthast, Benno Stein, andChristof Bra?utigam.
2011.
Query segmentation re-visited.
In WWW ?11, pages 97?106.Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li,Kuansan Wang, Fritz Behr, and C. Lee Giles.
2010.Exploring Web Scale Language Models for SearchQuery Processing.
In WWW ?10, pages 451?460.Dan Jurafsky and James H Martin.
2000.
Speech &Language Processing.
Pearson Education India.Klaus Krippendorff.
2004.
Content Analysis: AnIntroduction to Its Methodology.
Sage,ThousandOaks, CA.Matthew Lease, Vaughn Hester, Alexander Sorokin,and Emine Yilmaz, editors.
2011.
Proceedings ofthe ACM SIGIR 2011 Workshop on Crowdsourcingfor Information Retrieval (CIR 2011).Rohan Ramanath, Monojit Choudhury, Kalika Bali,and Rishiraj Saha Roy.
2013.
Crowd Prefers theMiddle Path: A New IAA Metric for Crowdsourc-ing Reveals Turker Biases in Query Segmentation.In Proceedings of ACL.
ACL.Rishiraj Saha Roy, Niloy Ganguly, Monojit Choud-hury, and Srivatsan Laxman.
2012.
An IR-basedEvaluation Framework for Web Search Query Seg-mentation.
In SIGIR ?12, pages 881?890.
ACM.David Vadas and James R. Curran.
2011.
ParsingNoun Phrases in the Penn Treebank.
Comput.
Lin-guist., 37(4):753?809, December.50
