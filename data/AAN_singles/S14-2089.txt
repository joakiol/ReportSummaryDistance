Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 512?516,Dublin, Ireland, August 23-24, 2014.SAIL: Sentiment Analysis using Semantic Similarity and ContrastFeaturesNikolaos Malandrakis, Michael Falcone, Colin Vaz, Jesse Bisogni,Alexandros Potamianos, Shrikanth NarayananSignal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA{malandra,mfalcone,cvaz,jbisogni}@usc.edu,potam@telecom.tuc.gr, shri@sipi.usc.eduAbstractThis paper describes our submission to Se-mEval2014 Task 9: Sentiment Analysis inTwitter.
Our model is primarily a lexi-con based one, augmented by some pre-processing, including detection of Multi-Word Expressions, negation propagationand hashtag expansion and by the use ofpairwise semantic similarity at the tweetlevel.
Feature extraction is repeated forsub-strings and contrasting sub-string fea-tures are used to better capture complexphenomena like sarcasm.
The resultingsupervised system, using a Naive Bayesmodel, achieved high performance in clas-sifying entire tweets, ranking 7th on themain set and 2nd when applied to sarcastictweets.1 IntroductionThe analysis of the emotional content of text isrelevant to numerous natural language process-ing (NLP), web and multi-modal dialogue appli-cations.
In recent years the increased popularityof social media and increased availability of rele-vant data has led to a focus of scientific efforts onthe emotion expressed through social media, withTwitter being the most common subject.Sentiment analysis in Twitter is usually per-formed by combining techniques used for relatedtasks, like word-level (Esuli and Sebastiani, 2006;Strapparava and Valitutti, 2004) and sentence-level (Turney and Littman, 2002; Turney andLittman, 2003) emotion extraction.
Twitter how-ever does present specific challenges: the breadthof possible content is virtually unlimited, the writ-ing style is informal, the use of orthography andThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/grammar can be ?unconventional?
and there areunique artifacts like hashtags.
Computation sys-tems, like those submitted to SemEval 2013 task2 (Nakov et al., 2013) mostly use bag-of-wordsmodels with specific features added to model emo-tion indicators like hashtags and emoticons (Davi-dov et al., 2010).This paper describes our submissions to Se-mEval 2014 task 9 (Rosenthal et al., 2014), whichdeals with sentiment analysis in twitter.
The sys-tem is an expansion of our submission to the sametask in 2013 (Malandrakis et al., 2013a), whichused only token rating statistics as features.
Weexpanded the system by using multiple lexica andmore statistics, added steps to the pre-processingstage (including negation and multi-word expres-sion handling), incorporated pairwise tweet-levelsemantic similarities as features and finally per-formed feature extraction on substrings and usedthe partial features as indicators of irony, sarcasmor humor.2 Model Description2.1 PreprocessingPOS-tagging / Tokenization was performedusing the ARK NLP tweeter tagger (Owoputi etal., 2013), a Twitter-specific tagger.Negations were detected using the list fromChristopher Potts?
tutorial.
All tokens up to thenext punctuation were marked as negated.Hashtag expansion into word strings was per-formed using a combination of a word insertionFinite State Machine and a language model.
Anormalized perplexity threshold was used todetect if the output was a ?proper?
English stringand expansion was not performed if it was not.Multi-word Expressions (MWEs) were detectedusing the MIT jMWE library (Kulkarni andFinlayson, 2011).
MWEs are non-compositionalexpressions (Sag et al., 2002), which should be512handled as a single token instead of attempting toreconstruct their meaning from their parts.2.2 Lexicon-based featuresThe core of the system was formed by the lexicon-based features.
We used a total of four lexica andsome derivatives.2.2.1 Third party lexicaWe used three third party affective lexica.SentiWordNet (Esuli and Sebastiani, 2006) pro-vides continuous positive, negative and neutral rat-ings for each sense of every word in WordNet.We created two versions of SentiWordNet: onewhere ratings are averaged over all senses of aword (e.g., one ratings for ?good?)
and one whereratings are averaged over lexeme-pos pairs (e.g.,one rating for the adjective ?good?
and one for thenoun ?good?
).NRC Hashtag (Mohammad et al., 2013) Senti-ment Lexicon provides continuous polarity ratingsfor tokens, generated from a collection of tweetsthat had a positive or a negative word hashtag.Sentiment140 (Mohammad et al., 2013) Lexi-con provides continuous polarity ratings for to-kens, generated from the sentiment140 corpus of1.6 million tweets, with emoticons used as posi-tive and negative labels.2.2.2 Emotiword: expansion and adaptationTo create our own lexicon we used an automatedalgorithm of affective lexicon expansion based onthe one presented in (Malandrakis et al., 2011;Malandrakis et al., 2013b), which in turn is an ex-pansion of (Turney and Littman, 2002).We assume that the continuous (in [?1, 1]) va-lence, arousal and dominance ratings of any termtjcan be represented as a linear combination ofits semantic similarities dijto a set of seed wordswiand the known affective ratings of these wordsv(wi), as follows:v?
(tj) = a0+N?i=1aiv(wi) dij, (1)where aiis the weight corresponding to seed wordwi(that is estimated as described next).
For thepurposes of this work, dijis the cosine similaritybetween context vectors computed over a corpusof 116 million web snippets (up to 1000 for eachword in the Aspell spellchecker) collected usingthe Yahoo!
search engine.Given the starting, manually annotated, lexi-con Affective Norms for English Words (Bradleyand Lang, 1999) we selected 600 out of the 1034words contained in it to serve as seed words andall 1034 words to act as the training set and usedLeast Squares Estimation to estimate the weightsai.
Seed word selection was performed by a sim-ple heuristic: we want seed words to have extremeaffective ratings (high absolute value) and the setto be close to balanced (sum of seed ratings equalto zero).
The equation learned was used to gener-ate ratings for any new terms.The lexicon created by this method is task-independent, since both the starting lexicon andthe raw text corpus are task-independent.
To cre-ate task-specific lexica we used corpus filtering onthe 116 million sentences to select ones that matchour domain, using either a normalized perplex-ity threshold (using a maximum likelihood trigrammodel created from the training set tweets) or acombination of pragmatic constraints (keywordswith high mutual information with the task) andperplexity threshold (Malandrakis et al., 2014).Then we re-calculated semantic similarities on thefiltered corpora.
In total we created three lexica: atask-independent (base) version and two adaptedversions (filtered by perplexity alone and filteredby combining pragmatics and perplexity), all con-taining valence, arousal and dominance token rat-ings.2.2.3 Statistics extractionThe lexica provide up to 17 ratings for each to-ken.
To extract tweet-level features we used sim-ple statistics and selection criteria.
First, all tokenunigrams and bigrams contained in a tweet werecollected.
Some of these n-grams were selectedbased on a criterion: POS tags, whether a token is(part of) a MWE, is negated or was expanded froma hashtag.
The criteria were applied separatelyto token unigrams and token bigrams (POS tagsonly applied to unigrams).
Then ratings statisticswere extracted from the selected n-grams: length(cardinality), min, max, max amplitude, sum, av-erage, range (max minus min), standard deviationand variance.
We also created normalized versionsby dividing by the same statistics calculated overall tokens, e.g., the maximum of adjectives overthe maximum of all unigrams.
The results of thisprocess are features like ?maximum of Emotiwordvalence over unigram adjectives?
and ?average ofSentiWordNet objectivity among MWE bigrams?.5132.3 Tweet-level similarity ratingsOur lexicon was formed under the assumptionthat semantic similarity implies affective similar-ity, which should apply to larger lexical units likeentire tweets.
To estimate semantic similarityscores between tweets we used the publicly avail-able TakeLab semantic similarity toolkit (?Sari?c etal., 2012) which is based on a submission to Se-mEval 2012 task 6 (Agirre et al., 2012).
We usedthe data of SemEval 2012 task 6 to train threesemantic similarity models corresponding to thethree datasets of that task, plus an overall model.Using these models we created four similarity rat-ings between each tweet of interest and each tweetin the training set.
These similarity ratings wereused as features of the final model.2.4 Character featuresCapitalization features are frequencies and rela-tive frequencies at the word and letter level, ex-tracted from all words that either start with a capi-tal letter, have a capital letter in them (but the firstletter is non-capital) or are in all capital letters.Punctuation features are frequencies, relative fre-quencies and punctuation unigrams.Character repetition features are frequencies,relative frequencies and longest string statistics ofwords containing a repetition of the same letter.Emoticon features are frequencies, relative fre-quencies, and emoticon unigrams.2.5 Contrast featuresCognitive Dissonance is an important phe-nomenon associated with complex linguistic caseslike sarcasm, irony and humor (Reyes et al., 2012).To estimate it we used a simple approach, inspiredby one-liner joke detection: we assumed that thefinal few tokens of each tweet (the ?suffix?)
con-trast the rest of the tweet (the ?prefix?)
and createdsplit versions of the tweet where the last N tokensare the suffix and all other tokens are the prefix,for N = 2 and N = 3.
We repeated the fea-ture extraction process for all features mentionedabove (except for the semantic similarity features)for the prefix and suffix, nearly tripling the totalnumber of features.2.6 Feature selection and TrainingThe extraction process lead to tens of thousandsof candidate features, so we performed forwardstepwise feature selection using a correlation crite-Table 1: Performance and rank achieved by oursubmission for all datasets of subtasks A and B.task dataset avg.
F1 rankALJ2014 70.62 16SMS2013 74.46 16TW2013 78.47 14TW2014 76.89 13TW2014SC 65.56 15BLJ2014 69.34 15SMS2013 56.98 24TW2013 66.80 10TW2014 67.77 7TW2014SC 57.26 2rion (Hall, 1999) and used the resulting set of 222features to train a model.
The model chosen is aNaive Bayes tree, a tree with Naive Bayes clas-sifiers on each leaf.
The motivation comes fromconsidering this a two stage problem: subjectivitydetection and polarity classification, making a hi-erarchical model a natural choice.
The feature se-lection and model training/classification was con-ducted using Weka (Witten and Frank, 2000).Table 2: Selected features for subtask B.Features numberLexicon-derived 178By lexiconEwrd / S140 / SWNet / NRC 71 / 53 / 33 / 21By POS tagall (ignore tag) 103adj / verb / proper noun 25 / 11 / 11other tags 28By functionavg / min / sum / max 45 / 40 / 38 / 26other functions 29Semantic similarity 29Punctuation 7Emoticon 5Other features 3Contrast 72prefix / suffix 54 / 183 ResultsWe took part in subtasks A and B of SemEval2014 task 9, submitting constrained runs trainedwith the data the task organizers provided.
Sub-task B was the priority and the subtask A modelwas created as an afterthought: it only uses thelexicon-based and morphology features for the tar-get string and the entire tweet as features of an NBTree.The overall performance of our submissionon all datasets (LiveJournal, SMS, Twitter 2013,Twitter 2014 and Twitter 2014 Sarcasm) can beseen in Table 1.
The subtask A system performed514Table 3: Performance on all data sets of subtask B after removing 1 set of features.
Performance differ-ence with the complete system listed if greater than 1%.Features removedLJ2014 SMS2013 TW2013 TW2014 TW2014SCavg.
F1 diff avg.
F1 diff avg.
F1 diff avg.
F1 diff avg.
F1 diffNone (Submitted) 69.3 57.0 66.8 67.8 57.3Lexicon-derived 43.6 -25.8 38.2 -18.8 49.5 -17.4 51.5 -16.3 43.5 -13.8Emotiword 67.5 -1.9 56.4 63.5 -3.3 66.1 -1.7 54.8 -2.5Base 68.4 56.3 65.0 -1.9 66.4 -1.4 59.6 2.3Adapted 69.3 57.4 66.7 67.5 50.8 -6.5Sentiment140 68.1 -1.3 54.5 -2.5 64.4 -2.4 64.2 -3.6 45.4 -11.9NRC Tag 70.6 1.3 58.5 1.6 66.3 66.0 -1.7 55.3 -2.0SentiWordNet 68.7 56.0 66.2 68.1 52.7 -4.6per Lexeme 69.3 56.7 66.1 68.0 52.7 -4.5per Lexeme-POS 68.8 57.1 66.7 67.4 55.0 -2.2Semantic Similarity 69.0 58.2 1.2 64.9 -2.0 65.5 -2.2 52.2 -5.0Punctuation 69.7 57.4 66.6 67.1 53.9 -3.4Emoticon 69.3 57.0 66.8 67.8 57.3Contrast 69.2 57.5 66.7 67.0 51.9 -5.4Prefix 69.5 57.2 66.8 67.2 47.4 -9.9Suffix 68.6 57.2 66.5 67.9 56.3badly, ranking near the bottom (among 20 submis-sions) on all datasets, a result perhaps expectedgiven the limited attention we gave to the model.The subtask B system did very well on the threeTwitter datasets, ranking near the top (among 42teams) on all three sets and placing second on thesarcastic tweets set, but did notably worse on thetwo non-Twitter sets.A compact list of the features selected by thesubtask B system can be seen in Table 2.
The ma-jority of features (178 of 222) are lexicon-based,29 are semantic similarities to known tweets andthe rest are mainly punctuation and emoticon fea-tures.
The lexicon-based features mostly comefrom Emotiword, though that is probably becauseEmotiword contains a rating for every unigramand bigram in the tweets, unlike the other lexica.The most important part-of-speech tags are adjec-tives and verbs, as expected, with proper nounsbeing also highly important, presumably as indi-cators of attribution.
Still, most features are cal-culated over all tokens (including stop words).
Fi-nally it is worth noting the 72 contrast features se-lected.We also conducted a set of experiments usingpartial feature sets: each time we use all featuresminus one set, then apply feature selection andclassification.
The results are presented in Ta-ble 3.
As expected, the lexicon-based features arethe most important ones by a wide margin thoughthe relative usefulness of the lexica changes de-pending on the dataset: the twitter-specific NRClexicon actually hurts performance on non-tweets,while the task-independent Emotiword hurts per-formance on the sarcastic tweets set.
Overallthough using all is the optimal choice.
Among theother features only semantic similarity provides arelatively consistent improvement.A lot of features provide very little benefit onmost sets, but virtually everything is important forthe sarcasm set.
Lexica, particularly the twitterspecific ones like Sentiment 140 and the adaptedversion of Emotiword make a big difference, per-haps indicating some domain-specific aspects ofsarcasm expression (though such assumptions areshaky at best due to the small size of the testset).
The contrast features perform their intendedfunction well, providing a large performance boostwhen dealing with sarcastic tweets and perhapsexplaining our high ranking on that dataset.Overall the subtask B system performed verywell and the semantic similarity features and con-trast features provide potential for further growth.4 ConclusionsWe presented a system of twitter sentiment anal-ysis combining lexicon-based features with se-mantic similarity and contrast features.
The sys-tem proved very successful, achieving high ranksamong all competing systems in the tasks of senti-ment analysis of generic and sarcastic tweets.Future work will focus on the semantic similar-ity and contrast features by attempting more accu-rately estimate semantic similarity and using somemore systematic way of identifying the ?contrast-ing?
text areas.515ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6: Apilot on semantic textual similarity.
In proc.
Se-mEval, pages 385?393.Margaret Bradley and Peter Lang.
1999.
AffectiveNorms for English Words (ANEW): Stimuli, in-struction manual and affective ratings.
technical re-port C-1.
The Center for Research in Psychophysi-ology, University of Florida.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using Twitter hashtagsand smileys.
In Proc.
COLING, pages 241?249.Andrea Esuli and Fabrizio Sebastiani.
2006.
SENTI-WORDNET: A publicly available lexical resourcefor opinion mining.
In Proc.
LREC, pages 417?422.Mark A.
Hall.
1999.
Correlation-based feature selec-tion for machine learning.
Ph.D. thesis, The Univer-sity of Waikato.Nidhi Kulkarni and Mark Alan Finlayson.
2011.jMWE: A java toolkit for detecting multi-word ex-pressions.
In proc.
Workshop on Multiword Expres-sions, pages 122?124.Nikolaos Malandrakis, Alexandros Potamianos, EliasIosif, and Shrikanth Narayanan.
2011.
Kernel mod-els for affective lexicon creation.
In Proc.
Inter-speech, pages 2977?2980.Nikolaos Malandrakis, Abe Kazemzadeh, AlexandrosPotamianos, and Shrikanth Narayanan.
2013a.SAIL: A hybrid approach to sentiment analysis.
Inproc.
SemEval, pages 438?442.Nikolaos Malandrakis, Alexandros Potamianos, EliasIosif, and Shrikanth Narayanan.
2013b.
Distri-butional semantic models for affective text analy-sis.
Audio, Speech, and Language Processing, IEEETransactions on, 21(11):2379?2392.Nikolaos Malandrakis, Alexandros Potamianos,Kean J. Hsu, Kalina N. Babeva, Michelle C. Feng,Gerald C. Davison, and Shrikanth Narayanan.
2014.Affective language model adaptation via corpusselection.
In proc.
ICASSP, pages 4871?4874.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In proc.
Se-mEval, pages 321?327.Preslav Nakov, Zornitsa Kozareva, Alan Ritter, SaraRosenthal, Veselin Stoyanov, and Theresa Wilson.2013.
SemEval-2013 Task 2: Sentiment analysis inTwitter.
In Proc.
SemEval, pages 312?320.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
Inproc.
NAACL, pages 380?390.Antonio Reyes, Paolo Rosso, and Davide Buscaldi.2012.
From humor recognition to irony detection:The figurative language of social media.
Data &Knowledge Engineering, 74(0):1 ?
12.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment analysis in Twitter.
In Proc.
SemEval.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann A.Copestake, and Dan Flickinger.
2002.
Multiwordexpressions: A pain in the neck for NLP.
In Compu-tational Linguistics and Intelligent Text Processing,volume 2276 of Lecture Notes in Computer Science,pages 189?206.Carlo Strapparava and Alessandro Valitutti.
2004.WordNet-Affect: an affective extension of WordNet.In Proc.
LREC, volume 4, pages 1083?1086.Peter D. Turney and Michael L. Littman.
2002.
Un-supervised learning of semantic orientation from ahundred-billion-word corpus.
technical report ERC-1094 (NRC 44929).
National Research Council ofCanada.Peter D. Turney and Michael L. Littman.
2003.
Mea-suring praise and criticism: Inference of semanticorientation from association.
ACM Transactions onInformation Systems, 21:315?346.Frane?Sari?c, Goran Glava?s, Mladen Karan, Jan?Snajder,and Bojana Dalbelo Ba?si?c.
2012.
Takelab: Systemsfor measuring semantic text similarity.
In proc.
Se-mEval, pages 441?448.Ian H. Witten and Eibe Frank.
2000.
Data Mining:Practical Machine Learning Tools and Techniques.Morgan Kaufmann.516
