SpokenDialogueforVirtualAdvisersinasemi-imme rsiveCommandandControlenvironmentDominiqueEstival,MichaelBroughton,AndrewZschor n,ElizabethProngerHumanSystemsIntegrationGroup,CommandandContro lDivisionDefenceScienceandTechnologyOrganisationPOBox1500,EdinburghSA5111AUSTRALIA{Dominique.Estival,Michael.Broughton,AndrewZscho rn}@dsto.defence.gov.auAbstractWe present the spoken dialogue systemdesigned and implemented for VirtualAdvisers in the FOCAL environment.
Itsarchitectureisbasedon:DialogueAgentsusing propositional attitudes, a NaturalLanguage Understanding componentusing typed unification grammar, and acommercial speaker-independent speechrecognition system.
The currentapplication aims to facilitate the multi-media presentation of military planninginformation in a semi-immersiveenvironment.1 IntroductionIn this paper, we present the spoken dialoguesystem implemented for communicating with thevirtual advisers (VAs) in the Future OperationsCentre Analysis Laboratory (FOCAL) at theAustralian Defence Science and TechnologyOrganisation(DSTO).Weareexperimentingwiththe use of spoken dialogue with virtualconversational characters to access multi-mediainformation during the conduct of militaryoperations and in particular to facilitate theplanningofsuchoperations.Unlike telephone-based dialogue systems(Estival, 2002),whicharemainlycreated fornewcommercial applications, dialogue systems forCommand and Control applications (Moore et al1997) generally seek to simulate the militarydomain and therefore require an understanding ofthatdomain.2 UsingVirtualAdvisersinFOCALFOCAL was established to "pioneer a paradigmshiftincommandenvironmentsthroughasuperioruseof capability andgreater situationawareness".The facility was designed to experiment withinnovativetechnologiestosupportthisgoal,andi thasnowbeenrunningfortwoyears.FOCAL contains a large-screen, semi-immersive virtual reality environment as itsprimary display, allowing vast quantities ofinformationtobedisplayed.
OurcurrentVAscanbe described as 3-dimensional "Talking Heads",i.e.
only the head and upper portions of the bodyare represented.
Theycandisplayexpression, lip-synchronisation and head movement, along withcertain autonomous behaviours such as blinkingand gaze (Taplin et al, 2001).
These factors allcombinetoaddlife-likenesstotheVAsandcreatemoreengaginginteractionwithusers.Presenting information via aTalkingHeadhasbeen commercially demonstrated by the virtualnewscaster ?Ananova?
(Ananova, 2002).Embodiedcharactersarealsobeingdevelopedandinclude the PPP (Andre, Rist and Muller, 1998)and Rea (Cassell, 2000).
PPP is a cartoon stylePersonalized Plan-based Presenter that combinespointing, head movements and facial expressionsto draw the viewer?s attention to the informationbeingpresented.
Rea isa virtual real-estateagen tthat takesanactive role inconversation, shenodsherheadtoindicateunderstandingofspokeninput,orcanraiseherhandtoindicateadesiretospeak .Several VAs have been implemented forFOCAL, each having a particular role orknowledge expertise.
For example, one advisermay have specialist knowledge relating to legalissues, another may have information relating tothe geography of a region.
Each VA has adifferentfacialappearance,voiceandmannerisms.To demonstrate and evaluate the performanceof VAs (and of the other FOCAL projects), afictitious scenario has been developed thatincorporates key elements of military planning atthe operational level (see section 8).
The VAsprovide information rich briefs through thecombineduseofspokenoutputviaText-to-Speech(TTS)andmultimedia.
Relevantquestionscanbeasked at the end of the briefs through the use ofspokendialogue.3 Previousimplementation:FrancoAsdescribedin(Taplinetal.,2001)thefirstVA inFOCAL, named Franco, was also an animated 3-dimensional "Talking Head" model, intended toeither deliver prepared information, such as abriefing or slide show, or to interactconversationally with users.
To demonstrate theconversational functionality (Broughton et al,2002), it was implemented with a commercialspeaker-dependent automated speech recogniser(ASR),DragonNaturallySpeaking?.TheNaturalLanguage understanding component wasimplemented in NatLink (Gould, 2001) and asimpleuser-drivendialoguemanagement,basedonkey-word recognition and nesting of dialoguestatestoprovidecontext,wasalsoimplementedinPython.Franco has been successful in demonstratingthe proof-of-concept of a VA in the FOCALenvironment.
Answering spoken questions aboutspecific military assets and platforms, it alsopermits the display of other types of informationsuch as pictures, animated video clips, tabularinformation from a database, and location detailsondigitalmaps.4 ImprovementsAlthough Francowas successful in demonstratingthe potential usefulness of a VA in a CommandandControlenvironment foroperationalplanning,it suffers from certain limitations which we arenowaddressinginafollow-upproject.The first limitation, and the easiest to remedy,was the unnaturalness of the synthetic voice wehad given Franco.
For greater effectiveness, wehad to provide ourVAwith amore natural voiceandwithanAustralianaccent.
WechosethenewAustralian TTS voice from Rhetorical, developedbyAppen (rVoice,2002).
This requiredmakingsomechanges, some of them relatively important,to the interface with the talking head model toachieve lip-synchronisation, but that aspect of theworkwillnotbeaddressedinthispaper.The second limitation was the relative rigidityof the dialogue management strategy we wereusing.
The alternative approach we havedeveloped is to create Dialogue Agentsimplemented in A TTITUDE.
This is described insection6.The third limitation was due to the speaker-dependent nature of the ASR.
While a speaker-dependent ASR allows greater flexibility in theinput towhich theVAcanrespond,wewantedtodevelop a system which could not only bedemonstratedby the fewpeoplewhohave trainedthe speech recogniser, but where visitorsthemselvescouldbeparticipantsandcouldinteractwith theVA.
Switching toa speaker-independentASR led us to radically modify our SpokenLanguage Understanding component, and this isdescribedinsection7.The new implementationwe describe here hasallowed us not only to address those threelimitations, but also to alter fundamentally thearchitectureofthesystem,openingupthedialoguemanagementcomponentstocontrolandinteractionby other tools and agents in the FOCALenvironment.
The resulting system is now fullymodular and provides scalability as well asflexibility.Thisnewimplementationallowsustofocusourresearch into dialogue management issue, toinvestigate the use of A TTITUDE  for dialoguemanagement and to experimentwithmore naturallanguageinput.5 IntegrationCommunication between the various componentsofthesystem(speechrecogniser,dialoguecontrol,virtual adviser control andmultimedia display) isnowachievedwith theCoABS (ControlofAgentBased Systems) Grid infrastructure (GlobalInfoTek,2002).TheCoABSGridwasdesignedtoallowalargenumberofheterogeneousprocedural,object-oriented and agent-based systems tocommunicate.
Using the CoABS Grid as ourinfrastructure has allowed us to integrate all thecomponents of the dialogue system and it willprovideaneasywaytointegrateotheragentsandavariety of input and output devices.Communication between CoABS agents isaccomplishedviastringmessages.6 DialogueManagementwithA TTITUDEATTITUDE  is a multi-agent architecture developedat DSTO, capable of representing and reasoningboth with uncertainty and about multiplealternative scenarios (Lambert, 1999).
It is amulti-agent extension of the MetaCon reactiveplanner developed for control of phased arrayradars on the Swedish Airborne Early Warningaircraft(LambertandRelbe,1998).
A TTITUDE hassome similarities with Prolog and other logicprogramming languages as well as with AIresearch on blackboard and multi-agentarchitectures.
Because A TTITUDE  was designedspecificallytosupporttheprogrammingofreactivesystems, it possesses powerful facilities forhandling interactions of the internal systementities,bothwitheachotherandwiththeexterna lworld.ATTITUDE isveryhigh-level,weakly-typed,andthanks to the agent paradigm, it produces looselycoupled and modularised systems.
For thesereasons, and because A TTITUDE  implementsreasoningabout propositionalattitudes ,itprovidesa very attractive framework in which to developand express dialogue management controlstrategies.
It is worth emphasizing here thatATTITUDE is not merely a notation to representspeech acts or  communicative acts betweenagents, but that it is actually the programminglanguage and environment in which both theagents themselves and the control structure forinteraction between the agents are implementedandexecuted.BecauseA TTITUDE hasneverbeenusedforthispurpose before, this is an interesting area ofresearch in itself, and one of the goals of theprojecthasbeentoseehowA TTITUDE needstobeextended to implement dialogue management.Further, this allows us to investigate how farattitude programming  (see section 6.2) can gotowardsexpressingspeechactsandcommunicativeacttype.
However,wedonotclaimtoemploythefull power of propositional attitudes in ourimplementation yet.
This is another area ofresearchwhichwearenowexploring.
Neitherarewe yet at the stage where we could performautomatic detection of utterance type (Wright,1998) or of dialogue act (Carberry and Lambert,1999;PrasadandWalker,2002).6.1 PropositionalattitudesThe A TTITUDE  programming environment is sonamed because it utilises propositional attitudeinstructions  as programming instructions (this hasbeendubbed attitudeprogramming ).
Propositionalattitudesareallegedmentalstatescharacterisedb ypropositional attitude expressions, which are themeans by which individuals relate their ownmentalbehaviourtoothers'.Propositional attitude instructions are of the formshownin(1).
(1)[subject][attitude][propositionalexpression]In(1):-[subject]denotesthe individualwhosementalstateisbeingcharacterised;- [propositional expression] describes somepropositionalclaimabouttheworld;and- [attitude]expresses the subject'sdispositionalattitudetowardthatclaimabouttheworld.6.2 ATTITUDEprogrammingWhen software agent Mary  encounters thepropositionalattitudeinstruction" Fred  desire [thedoor is closed]", Mary  will issue a message tosoftwareagent Fred instructing Fred  todesirethatthe door be closed.
Similarly, when encounteringthepropositionalattitudeinstruction" I believe [thesky isblue]", Mary  herselfwillattempt to believethattheskyisblue.An important characteristic of A TTITUDEprogramming is that each propositional attitudeinstruction either succeeds or fails, possibly withsideeffects,dependinguponwhetherthe recipientagentisabletosatisfytheinstructionalrequest.
Aseach propositional attitude instruction eithersucceeds or fails, the execution path selectedthrough a network of propositional attitudeinstructions (routine) is determined by thesuccessesandfailuresof thepropositionalattitud einstructionsattemptedalong theway.
Thecontrolstructure is therefore governed by a semantics ofsuccess.Computational routines for a software agentarise by linking together particular choices ofpropositional attitude instructions.Thesenetworksofpropositionalattitudeinstructionsthenprescri berecipesdefiningthepossiblementalbehaviourofasoftwareagent.6.3 The  ATTITUDEDialogueAgentsWe have implemented a number of A TTITUDEDialogueAgents.
ThemainagentinourDialogueManagement architecture (shown in Figure 1) isthe Conductor.
It is the agent responsible for theflowofinformationbetweentheotheragentsanditmanages multi-modal interactions.
The otheragents, also described further in this section, arethe Speaker, the NLG (Natural LanguageGenerator), the MMP (Multimedia Presenter) andseveral IS(InformationSource)agents.Inadditionto theseagents,eachdialoguestate (seesection8 )isalsoimplementedasanA TTITUDE agent,withitsownsetofroutines.As explained in section 6.2, each A TTITUDEagent?s behaviour is programmed as a set ofroutinesFigure1.DialoguewithA TTITUDEThe interaction between the A TTITUDEDialogueagentsisshowninFigure1,inwhichtheframe around the A TTITUDE  agents can beinterpretedasrepresentingtheCoABSgrid.SpeakerAgentWhenspeechfromtheuserhasbeendetectedandrecognised, the attribute-value pairs for thatutterance (see section 7) are sent to Speaker.Speaker takes that information and produces acorrespondingA TTITUDE expression,whichisthenforwardedto Conductor.The linguistic coverage of the system isdeterminedbythegrammarswhichareavailableateach dialogue state.
For now, the coverage islimited to a set of utterances appropriate for thebriefing scenario described in section 8.
Thesewere used to define the Regulus1 grammars fromwhichtheNuancegrammarsarecompiled.WearenowplanningtomovefromRegulus1toRegulus2,which will allow us to derive dialogue stategrammarsfromalargeEnglishgrammarusingtheEBLstrategydescribedin(Rayneretal.,2002b)ConductorThisagentisresponsiblefordialogueflowcontrol andallothe  rdialogueagentsmustregisterwithit.ConductorreceivescommunicativeactsfromSpeaker.Forexample:( whquestion  (property mig- 29flying -range?value?units))Thisqueryisforwardedontoallregisteredagents .Conductorchoosesthemostappropriateresponsereceivedandsendsthist oMMPtopresenttheanswer.SpeakerSpeakerreceivesspeechrecognitionresultsinthe formofattribute-valuepairs,andtranslatestheseintoAttitudeexpressionstosendtoConductor.InformationSource(IS)Thiscategoryofagentseachregisterwith  Conductor  and interfacewithabackgrounddatasource,forexampl e,adatabaseofaircraftproperties.Eachusestheirdatasourcetorespondtoqueriesf romConductor.MultimediaPresenter(MMP)Thisagentreceivesalistofexpressionsfrom  Conductor anddirectstheappropriateservicestopresent multimediadatatotheuser.Forexample:((whanswer (property mig -29flying - range810nautical - miles))(image mig- 29))Inthiscase,MMPrequestsanEnglishformofthewhanswer expressionandsendstheresulttotheTTSapplication.Similarly,anappropriateapplication isdirectedtodisplaytherequestedimage.NaturalLanguageGenerator(NLG)Receivesexpressionsfrom  MMP andusestemplatestoreturn correspondingEnglishsentences.EnglishquestionfromuserNuance/RegulusAttitudeexpressionQueryResponsePresentationdirectivesNLGdirectiveTTSEnglishstringVirtualAdvisorspeakingEnglishstringAttribute/ValuepairsMultimediadisplayedConductorAgentConductor  takes an A TTITUDE  expression fromSpeakerandforwardsitontoallthe IS agentsthathave registered with it.
It then waits for all theresponses to come back from those agents, in theformoflistsofexpressions.Every response Conductor  receives is put intoits knowledge base, along with some extrainformation:-Sender:whichISagentsenttheresponse.- In-Reply-To: which previous communicativeactthisisaresponseto.- Strength: whether every expression of theresponse is 'strong' (the sender believes it iseither absolute truthor absolutenegation)or ifoneormore is 'weak' (the senderbelieves it isneitherabsolutetruthnorabsolutenegation).-Bound-State: if thereareanyfreevariablesintheresponse,orifitisfullyground.- Unifiability: whether one or more of theexpressionsintheresponseisofthesameformas Speaker?s initial expression.The final expression in Conductor?s knowledgebaseisasshownin(2).
(2)(response?in-reply-to?sender?strength?bound_state?unifiability?content)Given the initial expression from Speaker and thereplies it receives from the IS agents, Conductorchooses the 'best' response.
For example, aresponse that is strong, fully ground and unifieswith Speaker?s expression is deemed to be morerelevant and informative than a response that isweak and contains free variables.
Conductorforwardsthisresponseto MMP.MultimediaPresenter(MMP)MMP  iterates through the list of expressions sentby Conductor  andpresents eachexpression to theuser.
MMP recognises classes of expressionsandchooses topresent themusingcertainmedia.
Forexample, some expressions are instructions tochangetheVAheadmodel,whileothersaretobetranslatedintoEnglishsentencesandspokenbytheVA.
For the latter function MMP  uses NLG (seebelow).Othermediathroughwhich MMP canchoosetopresent the information contained in theexpressionsinclude:imageryfromadatabase(e.g.pictures of military platforms, or of strategiclocations), video clips, images from weather orradar information sources, virtual video, 3-dimensional virtual battle space maps, textualinformationandaudio.NaturalLanguageGenerator(NLG)For now, NLG  uses templates to transformATTITUDE expressionsintoEnglish.
Forexample,the instruction in (3) provides two possibleresponsesfortheA TTITUDE expressionspecified: 1(3)(property?assetoverview?valuetext)whanswerpriority10((response1("The"?asset"isa"?value".")
)((response2("Iunderstandthatthe"?asset "isa"?value".
"))))When NLG  is first requested to generate theEnglishoutputfortheexpressionin(4.a),intende dto be a communicative act of type whanswer, ituses the template given in (4.b), corresponding to"response1"in(3), toproducetheEnglishanswergivenin(4.c).(4.a)(propertymig-29overview"Russianmulti-rolefighter"text)b.("The"?asset"isa"?value".
")c.TheMig-29isaRussianmulti-rolefighter.When NLG  isrequestedasecond time togeneratethe output for (3), it uses the template in (5.a),corresponding to "response 2" in (3), to producetheEnglishanswergivenin(5.b).
(5.a)("Iunderstandthatthe"?asset"isa"?valu e".
")b.IunderstandthattheMig-29isaRussianm ulti-rolefighter.Thus NLG  cycles through the list of templates forappropriateresponses.
Prioritiescanalsobegive nto templates, enabling NLG  to use generaltemplates togetherwithmore specificand tailoredones.It is clear that template-based languagegeneration is too rigid for fully natural dialogues ,andweintendtoexploremoreflexible techniquesafter we implement a wider coverage Englishgrammar;however,ithassofarbeensufficientfor1Variablesaredenotedwith"?
",whiletextstrings (tobesenttospeechsynthesis,ordisplayedonaslide)areb etweendoublequotes,"".our purposes, namely to demonstrate andinvestigateagent-baseddialoguemanagement.InformationSourceAgent(IS)The ISagents,e.g.aWeatherAgentor aPlatformCapabilities Agent, can answer users' questions,eitherby using their own internalknowledgebaseorbyaccessingexternalInformationSources,suchas a weather information server, or a database ofmilitary assets.
All IS agents register withConductor, and when an expression is sent bySpeaker,all IS agentstrytorespondtoit.ByusingtheCoABSGridastheinfrastructureandimplementing the agentwithA TTITUDE, we leavethe architecture extremely flexible and scalable(Kahn and Della Torre Cicalese, 2001).
Forinstance, it is possible to increase the amount ofinformation at the system?s disposal during run-time by launching a new IS agent and by addingsometemplatesto NLG.6.4 DialoguedesignFornow, thedialogue is specifiedasa finite stat emachineandisstillverymuchsystemdirected.
Inthebriefingapplication (seesection8.1), theVAsfirst "push" the information that needs to bepresented, as briefing officers do in a normalbriefing.Someoftheinformationisalsopresente dusing visual aids, such as power point slides andmaps for specifying location  information.
Theinformation to be presented and the media to beusedaredeterminedbytheagentforthatparticula rdialoguestate.The VA then allows users to ask questions torepeat or clarify particular points, or to gainadditionalinformation.7 SpokenLanguageProcessing7.1 Speaker-independentspeechrecognitionAsstatedinsection4,oneofthemainmotivationsformovingfromaspeaker-dependenttoaspeaker-independentASRwastoallowvisitorsinFOCALthe possibility of using the system themselves,rather than relying on a small set of trainedindividuals to run demonstrations.
We chose tousetheNuanceToolkit(Nuance,2002)forseveralreasons:  besides its reliability as a speaker-independent ASR for both telephone andmicrophone speech, Nuance 8.0 providesAustralian-New Zealand English, as well as USandUKEnglish,acousticlanguagemodels.
Evenmore importantly for our purposes, NuancegrammarscanbecompiledfromRegulus,ahigher-level language processing component which hasalready been used to develop several spokendialogue systems in different domains (Rayner etal.,2001,RaynerandBouillon,2002).7.2 SpokenLanguageUnderstandingFollowing our decision to move from a speaker-dependent to a speaker-independent ASR, wedecided to use Regulus to implement our NaturalLanguage Understanding component.
Regulus isan Open Source environment which compilestyped unification grammars into context-freegrammar language models compatible with theNuance Toolkit.
It is "written in a Prolog-basedfeature-value notation and compiles into NuanceGSLgrammars."(Rayneretal.,2002a).
Regulusisalsodescribedindetailin(Rayneretal.,2001 ).The main motivation for using Regulus is theusual one of greater efficiency due to the morecompact nature of a unification grammarrepresentation compared with a context-freegrammar.
In addition, using Regulus to define ahigherlevelgrammar,weareabletoobtainasoursemantic representation a list of attribute-valuepairs, and this permits a more sophisticatedprocessingoftheinformationbytheotheragents.Regulus also allows the development of bi-directional grammars, and we intend tomake useofthisfunctionalityinlaterimplementationsoft heNLG  agent.
However, fornow, thegrammarswehave developed have been limited to recognitionandunderstanding.8 Currentapplicationimplementation8.1 DialoguescenarioThe scenario for the current application wasdeveloped by members of the Human SystemsIntegration (HSI) group and is grounded on theirexperience with, and observations of, militaryoperational planning.
It is based on a fictitiousscenario developed for training (the examplesgivenherehaveallbeenmodified)andexemplifiestheJointMilitaryAppreciationProcess(JMAP)formilitaryplanningacross the three services (Army,Navy and Air Force).
A sub-scenario was chosenfor the development of the spoken dialogue withtheVAs.
28.2 DialogueflowThe structured nature of a military planning tasksuch as this onemakes it very easy to partition itintodifferentstages,whichcanthenbemappedtodifferent dialogue states.
In our dialogue script,each top-level dialogue state corresponds to asectionoftheplanningexercise,givenin(6).
(6)Commander'sInitialGuidance-CDF(ChiefofDefenceForces)Intent-PlanningGuidance-Constraints-Restrictions-LegalIssues-CommandandControlThese6topleveldialoguestatesarethenfollowedbyanOverallQuestionTime.The mixed-initiative nature of the system canbemodelledinafinitestatediagram,allowingfora) briefing-like system ?pushes?, b) confirmationqueriesfromthesystemandc)questionsfromtheuser.
However, because the system is primarilyagent-based, the dialogue can also evolvedynamically.
Forinstance,oncethesystemisina?question?
state,  the dialogue flow then allowsusers to ask anumberofquestions,until they aresatisfied,and thedialoguecanmove toadifferentstate.Each of the top level dialogue states alsocorresponds to an IS agent with its own set ofATTITUDE  routines.
These agents register withConductor  and act as experts in their particularfields (e.g., the Legal Issues adviser).
Theagent scontain knowledge which they use to answerquestionsposedtothemby Conductor.
Allagentshave the ability to keep track of which state (ortopic)theyarein.Thisallowsnotonly Conductor,but also the other dialogue agents, to distinguishbetween providing the user with new informationorinformationthathasalreadybeenpresented.2ThisistheCommander?sinitialguidancetotheTh eatrePlanningGroup(TPG),whichispartoftheMission AnalysissectionofJMAP.8.3 KnowledgeRepresentationThecurrentontologydevelopedforthisapplicationis only a small part of the larger KnowledgeRepresentationontologytobeusedthroughout thewhole FOCAL system.
For now, we onlyrepresenttheconceptsneededinoursmalldomain,and their relationships are translated intoATTITUDE  statements, allowing agents to drawinferences.
For example, if a user can ask thequestion given  in (7.a), it will be translated int othe listof  attributevaluepairsgiven in (7.b)a ndsent to Speaker.
Speaker then translates theseattributevaluepairsinto theA TTITUDE expressionin(7.c)andforwardsitonto Conductor.
(7.a) Whatdepartmentoverseesnegotiationswithunionsandindustry?b.
[questionwhatquestion,conceptnegotiation,attributeoversee,obj1department]c. conductordesire(comm_act(negotiationoversee?department)fromspeakertypewhatquestionin-response-tonull)As described in section 6, when Conductor  posesthequestiontotheappropriateagents,theyrespon dwith the information in their knowledge base orinformation they can extract from a database.Agentsstoreknowledgeas believe statementssuchastheoneshownin(8):(8) Ibelieve(negotiationoversee?departmentofworkplacerelations?
)These believe  statementsarethenunifiedwith thepropositions translated by Speaker, and ifunification is successful, a reply is sent back toConductor.
Finally, Conductor  passes the answeron to NLG  to match a template and produce anEnglishanswer,forinstance(9).
(9) TheDepartmentofWorkplaceRelationsoverseesnegotiationswithunionsandindustry.An agentwhich has access to a databasecan alsotranslate a user's question into the relevantdatabasequerytoobtaintheanswer.
Animportantissue under research concerns the automaticderivation of A TTITUDE  statements from a pre-existingdatabase.8.4 SeveraldifferentVAsAs explained above, each stage of the planningprocessispresentedtotheuserbyaparticularVAwithitsassociated IS agentandtheVAthenallowsusers to ask further questions.
Besides theirspecialised knowledge, theVAs are differentiatedthrough different head models, different TTSvoices (maleorfemale,differentregionalaccents)anddifferentpersonalities.Onceadialoguestateiscompletedandtheuserhas no further questions, the VA for that statesendsamessage to Conductor  tomovetothenextstate.
Conductor can then initiate the change inrecognition grammar, voice for the next VA andmodelforthenextVAhead.Having several VAs coming on at differentstagestopresentdifferent informationallowsfor aVA tobe specialised in a particular domain,  justas real briefing officers are during a realmilitar yplanningexercise.Fornow,weonlydisplayoneVAatatime,butweintendtoexperimentwithhavingmultipleVAsat the same time.
The final state of the dialogueflowallowsuserstoaskquestionsaboutanyaspectof the planning process, and questions can beposedtoalltheVAs,soitwouldbenaturalforth euserstoseealltheVAsatthatstage.8.5 RapidPrototypingandEvaluationThe key word version developed previously (seeBroughton et al, 2002) has been maintained as arapid prototyping environment for evaluating newscripts and dialogues.
It allows newdialogues tobe quickly tested by entering suitable key words,sufficient to discriminate one question fromanother.
Thissystemprovesfasterfortestingtha nthe more precise method of grammar building.Multiple response strings can be generated,providing more naturalness for those interactingwith the VAs on a regular basis.
By rapidlyprototyping questions and responses, we can testthe intuitiveness of expected questions and thesmoothness and timeliness of responses,particularly when presented combined withmultimedia.The implemented system described here hasso faronlybeen testedwithothermembersof thegroup,butdemonstrations tovisitorsandpotentialusers will provide a more rigorous  form ofevaluation on an on-going basis.
An evaluationphase for the project is scheduled for 2003-2004,during which time we will have access to moreusers andwill be able to conductmore structuredexperiments.9 NaturalInteractionwithVAsIn addition to the ASR and TTS systemspreviously discussed, other technologies can becombined into the overall system to increasenaturalnessofinteraction,andweareinvestigatin gspeaker recognitionaswellasa rangeofpointingtechnologies.Theneed for a speaker recognition systemhasemerged with the move to a speaker independentASR.WithaspeakerdependentASR,userswouldload their individual profile before use, thusenabling the system to know who was using it.With a speaker-independent ASR, a speakerrecognition system would allow the VAs torecognisewho is talking to themandenable themto address known users by name.
We plan tointegrate within FOCAL the speaker recognitionsystem which has been developed at DSTO(Roberts, 1998).
This system uses statisticalmodelling techniques and is capable of bothspeaker identification (recognising users from adatabase of stored speech profiles) and speakerverification (verifying the identity of a particula ruser).We are also proposing to use pointingtechniques in combination with the speech andlanguage technologies to build a multimodalsystem.
Multimodal systems were originallydemonstrated by Bolts (1980) and research iscontinuing across varied applications (e.g., Oviattet al, 2000 and Gibbon et al, 2000).
However,unlike systems such as MATCH (Johnston et al,2002), where the issue is allowing multimodalinteraction on portable devices with very smallscreens, in FOCAL we are concerned withensuring thatusersget the full benefit of the ver ylarge screen and with allowing several users tointeract at a distance from the screen.
It is alsoworth mentioning that, unlike the interactivesystemdescribedin (Rickeletal.,2002),whichi sconcernedwithtraining inamilitaryenvironment,we are not trying to simulate a complete virtualworldwithembodiedagents.However, we propose to include traditionalpointingtechnologies,suchasthestandarddesktopmouse, through to3-dimensional trackingsystemsfor gaze, gesture and user tracking.
This willinvolve integrating more complex languageunderstanding, as information will need to bederived from both the user's utterance and fromwhatisbeingpointedto.Forexample,tointerpre tan utterance such as (10) uttered while the userpoints toa locationonamap,weneed toperformreference resolution on "this region", and matchthatreferenttotheitembeingpointedat.
(10)Whatdoweknowaboutthisregion?10 ConclusionWe have now implemented in FOCAL theinfrastructure needed to perform spoken andmultimodaldialoguewithseveralVAs.
This isofinterestinitself,asitwillallowustocontinue ourresearch on spoken language understanding andspokendialoguesystemsandalsotoaddressissuesof language generation which have for now beenleft aside.
Already we have been able to movefrom a rigid dialogue control structure, with veryconstrained input, to a more flexible and scalablecontrol structure allowing real connectivitybetweenagents.Havingmoved to a speaker-independent ASR,and takingadvantageof theopensourcenatureofRegulus, we intend to pursue research issuesregarding robustprocessing of spoken input, suchasusinggrammarspecialisationfromacorpusanddevisingtechniquesforignoringpartsoftheinput .We have implementeda dialoguemanagementarchitecture based on A TTITUDE  agents whichcommunicate with each other using propositionalattitude expressions.
Other agents can now bedeveloped to  perform additional functions, inparticular to launch the display of other types ofinformationandtointerpretothertypesofinput.This will allow us to explore how spokendialogue with VAs can be combined with othervirtual interaction technologies (e.g., gesture,pointing, gaze tracking).
In this respect, the nextstep in our project is the development of a fullfledge MMP  agent based on the frameworkdescribedin(ColineauandParis,2003).However,theworkwehavereportedheremustalso be seen as part of the larger researchprogrammeundertakenwithinFOCAL.
Fromthisperspective, this work is of interest because itallows othermembers of theHSI group topursueresearch in the usability of new technologies toperform the paradigm shift in commandenvironments.
In particular, this project isproviding the support for further research intowhether this way of presenting information ishelpful in an operational command environment.It allows us to devise experiments to explore thecrucial issue of trust in the information beingpresented,andhowtheway theinformationbeingpresentedcanaffectthattrust.IntegratingspokendialoguewithplanningtoolswillalsoallowustoexplorewhetherVAscanhelpinmilitaryoperationplanning,andhowbesttousethesetools.AcknowledgementsWe wish to thank the Chief of C2D, and theDirector of Information Sciences Laboratory, forsponsoring and funding this work.
We wish toacknowledge the work of Paul Taplin inintegrating speech synthesis and lip-synchronisation, and the work of Benjamin Fryfrom the University of South Australia indeveloping the Regulus/Nuance grammars.FinallywewishtothanktheothermembersoftheHSIgroupinC2DfortheirconstantandinvaluablehelpwiththeFOCALproject.ReferencesAnanova.2002.
http://www.ananova.com.E.
Andre, T. Rist, and J. Muller.
1998.
IntegratingReactive and Scripted Behaviours in a Life-LikePresentation Agent, Proceedings of the SecondInternational Conference on Autonomous Agents ,261-268.Appen.2002.
http://www.appen.com.au.R.A.Bolt.1980.
"Put-that-there":voiceandgestu reatthe graphics interface .
Proceedings of theSIGGRAPH, July,262-270.Michael Broughton, Oliver Carr, Dominique Estival,Paul Taplin, Steven Wark, Dale Lambert.
2002.
"Conversing with Franco, FOCAL?s VirtualAdviser".
Conversation Characters Workshop,HumanFactors2002 ,Melbourne,Australia.Sandra Carberry and Lynn Lambert.
1999.
"A ProcessModel for Recognizing Communicative Acts andModelingNegotiationSubdialogues".
ComputationalLinguistics.25,1,pp.1-53Justine Cassell.
2000.
Embodied ConversationalInterfaceAgents, Communicationsof theACM ,Vol.43,No.4,70-78.Nathalie Colineau and C?cile Paris.
2003.
FrameworkfortheDesignofIntelligentMultimediaPresentati onSystems: An architecture proposal for FOCAL.CMISTechnicalReport03/92,CSIRO,May2003.Dominique Estival.
2002.
"The Syrinx SpokenLanguage System".
International Journal of  SpeechTechnology.
vol.5.no.1.pp.85-96.Michael Johnston, Srinivas Bangalore, GunaranjanVasireddy, Amanda Stent, Patrick Ehlen, MarilynWalker, Steve Whittaker, Preetam Maloor.
2002.
"MATCH: anArchitecture forMultimodalDialogueSystems".
Proceedingsofthe40thAnnualMeetingofthe Association for Computational Linguistics(ACL'02).pp.376-383.Philadelphia..DafyddGibbon, IngeMertins,RogerK.Moore (Eds.).2000.
Handbook of Multimodal and SpokenDialogue Systems: Resources, Terminology andProductEvaluation.
KluwerAcademicPublishers.Global InfoTek Inc. 2002.
Control of Agent BasedSystems.
http://coabs.globalinfotek.com.JoelGould.
2001.
"Implementation and Acceptance ofNatLink, aPython-BasedMacroSystem forDragonNaturallySpeaking", The Ninth International PythonConference,March5-8,CaliforniaMartha L. Kahn and Cynthia Della Torre Cicalese.2001.
"CoABS Grid Scalability Experiments".Proceedings of the Second International Workshopon Infrastructure for Agents, MAS, and ScalableMAS, AutonomousAgents2001Conference.Dale A. Lambert and Mikael G. Relbe.
1998.
"Reasoning with Tolerance".
2nd  InternationalConference on Knowledge-Based IntelligentElectronicSystems .IEEE.pp.418-427.DaleA.Lambert.1999.
"AdvisersWithA TTITUDE forSituation Awareness".
Proceedings of the 1999Workshop on Defence Applications of SignalProcessing.
pp.113-118, Edited A. Lindsey, B.Moran, J. Schroeder, M. Smith and L. White.LaSalle,Illinois.Dale A. Lambert.
2003.
"Automating CognitiveRoutines", accepted for publication in the 6thInternationalConferenceonInformationFusion.R.Moore, J.Dowding,H.Bratt, J. Gawron,Y.Gorfu ,A. Cheyer.
1997.
"CommandTalk: A spoken-language interface for battlefield simulations".
InProceedings of the Fifth Conference on AppliedNaturalLanguageProcessing,pp1-7.Nuance.2002.http://www.nuance.com/ .Oviatt, S., Cohen, P., Wu, L., Vergo, J., Duncan, L .,Suhm, B., Bers, J., Holzman, T., Winograd, T.,Landay,J.,Larson,J.,Ferro,D.2000.
"Designing theuser interface formultimodal speech and pen-basedgesture applications: state-of-the-art systems andfuture research directions".
Human ComputerInteraction.RashmiPrasad andMarilynWalker.
2002.
"Training aDialogue Act Tagger for Human-Human andHuman-ComputerTravelDialogues".
Proceedingsof3rdSIGDIALWorkshop .Philadelphia.pp.162-173.Manny Rayner, John Dowding, Beth Ann Hockey.2001.
"A Baseline method for compiling typedunification grammars into context free languagemodels".
In Proceedings of Eurospeech 2001, pp729-732.Aalborg,Denmark.Manny Rayner, John Dowding, Beth Ann Hockey.2002a.
"RegulusDocumentation".Manny Rayner, Beth Ann Hockey, John Dowding.2002b.
"Grammar Specialisation meets LanguageModelling".
ICSLP2002.
Denver.Manny Rayner and Pierrette Bouillon.
2002.
"AFlexible Speech to Speech Phrasebook Translator".Proceedings of the ACL-02 Speech-SpeechTranslationWorkshop ,pp69-76.Jeff Rickel, Stacy Marsella, Jonathan Gratch, Randa llHill,DavidTraum,WilliamSwartout.2002.TowardaNewGenerationofVirtualHumans for InteractiveExperiences.
IEEE Intelligent Systems, 1094-7167,pp.32-38.William Roberts.
1998.
"Automatic SpeakerRecognition Using Statistical Models".
DSTOResearchReport,DSTO-RR-0131 ,DSTOElectronicsandSurveillanceResearchLaboratory.rVoice.
2002.
Rhetorical Systems,http://www.rhetoricalsystems.com/rvoice.html.Paul Taplin, Geoffrey Fox, Michael Coleman, StevenWark, Dale Lambert.
2001.
"Situation AwarenessUsing a Virtual Adviser", Talking HeadWorkshop,OzCHI2001 ,Fremantle,Australia.Helen Wright.
1998.
"Automatic utterance typedetection using suprasegmental features".Proceedings of the 5th International Conference onSpokenLanguageProcessing(ICSLP'98).Sydney.
