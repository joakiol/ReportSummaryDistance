Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 30?40,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsReally?
Well.
Apparently Bootstrapping Improves the Performance ofSarcasm and Nastiness Classifiers for Online DialogueStephanie LukinNatural Language and Dialogue SystemsUniversity of California, Santa Cruz1156 High Street, Santa Cruz, CA 95064slukin@soe.ucsc.eduMarilyn WalkerNatural Language and Dialogue SystemsUniversity of California, Santa Cruz1156 High Street, Santa Cruz, CA 95064maw@soe.ucsc.eduAbstractMore and more of the information on the webis dialogic, from Facebook newsfeeds, to fo-rum conversations, to comment threads onnews articles.
In contrast to traditional, mono-logic Natural Language Processing resourcessuch as news, highly social dialogue is fre-quent in social media, making it a challengingcontext for NLP.
This paper tests a bootstrap-ping method, originally proposed in a mono-logic domain, to train classifiers to identifytwo different types of subjective language indialogue: sarcasm and nastiness.
We exploretwo methods of developing linguistic indica-tors to be used in a first level classifier aimedat maximizing precision at the expense of re-call.
The best performing classifier for the firstphase achieves 54% precision and 38% recallfor sarcastic utterances.
We then use generalsyntactic patterns from previous work to cre-ate more general sarcasm indicators, improv-ing precision to 62% and recall to 52%.
Tofurther test the generality of the method, wethen apply it to bootstrapping a classifier fornastiness dialogic acts.
Our first phase, usingcrowdsourced nasty indicators, achieves 58%precision and 49% recall, which increases to75% precision and 62% recall when we boot-strap over the first level with generalized syn-tactic patterns.1 IntroductionMore and more of the information on the web isdialogic, from Facebook newsfeeds, to forum con-versations, to comment threads on news articles.
Incontrast to traditional, monologic Natural LanguageProcessing resources such as news, highly social di-alogue is very frequent in social media, as illustratedin the snippets in Fig.
1 from the publicly avail-able Internet Argument Corpus (IAC) (Walker et alQuote Q, Response R Sarc NastyQ1: I jsut voted.
sorry if some people actu-ally have, you know, LIVES and don?t sit aroundall day on debate forums to cater to some athe-ists posts that he thiks they should drop every-thing for.
emoticon-rolleyes emoticon-rolleyesemoticon-rolleyes As to the rest of your post, well,from your attitude I can tell you are not Christianin the least.
Therefore I am content in knowingwhere people that spew garbage like this will endup in the End.R1: No, let me guess .
.
.
er .
.
.
McDonalds.
No,Disneyland.
Am I getting closer?1 -3.6Q2: The key issue is that once children are bornthey are not physically dependent on a particularindividual.R2 Really?
Well, when I have a kid, I?ll be sure tojust leave it in the woods, since it can apparentlycare for itself.1 -1Q3: okay, well i think that you are just findingreasons to go against Him.
I think that you hadsome bad experiances when you were younger ora while ago that made you turn on God.
You arelooking for reasons, not very good ones i mightadd, to convince people.....either way, God lovesyou.
:)R3: Here come the Christians, thinking they canknow everything by guessing, and commiting thegenetic fallacy left and right.0.8 -3.4Figure 1: Sample Quote/Response Pairs from4forums.com with Mechanical Turk annotationsfor Sarcasm and Nasty/Nice.
Highly negative valuesof Nasty/Nice indicate strong nastiness and sarcasm isindicated by values near 1.2012).
Utterances are frequently sarcastic, e.g., Re-ally?
Well, when I have a kid, I?ll be sure to justleave it in the woods, since it can apparently carefor itself (R2 in Fig.
1 as well as Q1 and R1), and areoften nasty, e.g.
Here come the Christians, thinkingthey can know everything by guessing, and commit-ing the genetic fallacy left and right (R3 in Fig.
1).Note also the frequent use of dialogue specific dis-course cues, e.g.
the use of No in R1, Really?
Wellin R2, and okay, well in Q3 in Fig.
1 (Fox Treeand Schrock, 1999; Bryant and Fox Tree, 2002; FoxTree, 2010).30The IAC comes with annotations of differenttypes of social language categories including sarcas-tic vs not sarcastic, nasty vs nice, rational vs emo-tional and respectful vs insulting.
Using a conser-vative threshold of agreement amongst the annota-tors, an analysis of 10,003 Quote/Response pairs(Q/R pairs) from the 4forums portion of IAC sug-gests that social subjective language is fairly fre-quent: about 12% of posts are sarcastic, 23% areemotional, and 12% are insulting or nasty.
We selectsarcastic and nasty dialogic turns to test our methodon more than one type of subjective language andexplore issues of generalization; we do not claim anyrelationship between these types of social languagein this work.Despite their frequency, expanding this corpus ofsarcastic or nasty utterances at scale is expensive:human annotation of 100% of the corpus would beneeded to identify 12% more examples of sarcasmor nastiness.
An explanation of how utterances areannotated in IAC is detailed in Sec.
2.Our aim in this paper is to explore whether it ispossible to extend a method for bootstrapping a clas-sifier for monologic, subjective sentences proposedby Riloff & Wiebe, henceforth R&W (Riloff andWiebe, 2003; Thelen and Riloff, 2002), to automat-ically find sarcastic and nasty utterances in unanno-tated online dialogues.
Sec.
3 provides an overviewof R&W?s bootstrapping method.
To apply boot-strapping, we:1.
Explore two different methods for identifyingcue words and phrases in two types of subjec-tive language in dialogues: sarcasm and nasty(Sec.
4);2.
Use the learned indicators to train a sarcastic(nasty) dialogue act classifier that maximizesprecision at the expense of recall (Sec.
5);3.
Use the classified utterances to learn generalsyntactic extraction patterns from the sarcastic(nasty) utterances (Sec.
6);4.
Bootstrap this process on unannotated text tolearn new extraction patterns to use for classifi-cation.We show that the Extraction Pattern Learner im-proves the precision of our sarcasm classifier by17% and the recall by 24%, and improves the pre-cision of the nastiness classifier by 14% and recallby 13%.
We discuss previous work in Sec.
2 andcompare to ours in Sec.
7 where we also summarizeour results and discuss future work.2 Previous WorkIAC provides labels for sarcasm and nastiness thatwere collected with Mechanical Turk on Q/R pairssuch as those in Fig.
1.
Seven Turkers per Q/R pairanswered a binary annotation question for sarcasmIs the respondent using sarcasm?
(0,1) and a scalarannotation question for nastiness Is the respondentattempting to be nice or is their attitude fairly nasty?
(-5 nasty .
.
.
5 nice).
We selected turns from IACTable 1 with sarcasm averages above 0.5, and nastyaverages below -1 and nice above 1.
Fig.
1 includedexample nastiness and sarcasm values.Previous work on the automatic identificationof sarcasm has focused on Twitter using the#sarcasm (Gonza?lez-Iba?n?ez et al 2011) and#irony (Reyes et al 2012) tags and a combinedvariety of tags and smileys (Davidov et al 2010).Another popular domain examines Amazon productreviews looking for irony (Reyes and Rosso, 2011),sarcasm (Tsur et al 2010), and a corpus collec-tion for sarcasm (Filatova, 2012).
(Carvalho et al2009) looks for irony in comments in online newpa-pers which can have a thread-like structure.
Thisprimary focus on monologic venues suggests thatsarcasm and irony can be detected with a relativelyhigh precision but have a different structure from di-alogues (Fox Tree and Schrock, 1999; Bryant andFox Tree, 2002; Fox Tree, 2010), posing the ques-tion, can we generalize from monologic to dialogicstructures?
Each of these works use methods in-cluding LIWC unigrams, affect, polarity, punctua-tion and more, and achieve on average a precision of75% or accuracy of between 45% and 85%.Automatically identifying offensive utterances isalso of interest.
Previous work includes identifyingflames in emails (Spertus, 1997) and other messag-ing interfaces (Razavi et al 2010), identifying in-sults in Twitter (Xiang et al 2012), as well as com-ments from new sites (Sood et al 2011).
Theseapproaches achieve an accuracy between 64% and83% using a variety of approaches.
The accuraciesfor nasty utterances has a much smaller spread andhigher average than sarcasm accuracies.
This sug-gests that nasty language may be easier to identifythan sarcastic language.3 Method OverviewOur method for bootstrapping a classifier for sarcas-tic (nasty) dialogue acts uses R&W?s model adaptedto our data as illustrated for sarcasm in Fig.
2.
The31Figure 2: Bootstrapping Flow for Classifying SubjectiveDialogue Acts, shown for sarcasm, but identical for nas-tiness.overall idea of the method is to find reliable cues andthen generalize.
The top of Fig.
2 specifies the inputto the method as an unannotated corpus of opiniondialogues, to illustrate the long term aim of buildinga large corpus of the phenomenon of interest with-out human annotation.
Although the bootstrappingmethod assumes that the input is unannotated text,we first need utterances that are already labeled forsarcasm (nastiness) to train it.
Table 1 specifies howwe break down into datasets the annotations on theutterances in IAC for our various experiments.The left circle of Fig.
2 reflects the assump-tion that there are Sarcasm or Nasty Cues that canidentify the category of interest with high preci-sion (R&W call this the ?Known Subjective Vocab-ulary?).
The aim of first developing a high preci-sion classifier, at the expense of recall, is to selectutterances that are reliably of the category of inter-est from unannotated text.
This is needed to ensurethat the generalization step of ?Extraction PatternLearner?
does not introduce too much noise.R&W did not need to develop a ?Known Sub-jective Vocabulary?
because previous work providedone (Wilson et al 2005; Wiebe et al 1999; Wiebeet al 2003).
Thus, our first question with applyingR&W?s method to our data was whether or not it ispossible to develop a reliable set of Sarcasm (Nas-tiness) Cues (O1 below).
Two factors suggest thatit might not be.
First, R&W?s method assumes thatthe cues are in the utterance to be classified, but ithas been claimed that sarcasm (1) is context depen-dent, and (2) requires world knowledge to recognize,SARCASM #sarc #notsarc totalMT exp dev 617 NA 617HP train 1407 1404 2811HP dev test 1614 1614 3228PE eval 1616 1616 3232All 5254 4635 9889NASTY #nasty #nice totalMT exp dev 510 NA 510HP train 1147 1147 2294HP dev test 691 691 1382PE eval 691 691 1382All 3039 2529 5568Table 1: How utterances annotated for sarcasm (top) andnastiness (bottom) in IAC were used.
MT = MechanicalTurk experimental development set.
HP train = utter-ances used to test whether combinations of cues could beused to develop a High precision classifier.
HP dev test= ?Unannotated Text Collection?
in Fig.
2.
PE eval =utterances used to train the Pattern Classifier.at least in many cases.
Second, sarcasm is exhibitedby a wide range of different forms and with differ-ent dialogue strategies such as jocularity, understate-ment and hyberbole (Gibbs, 2000; Eisterhold et al2006; Bryant and Fox Tree, 2002; Filatova, 2012).In Sec.
4 we devise and test two different methodsfor acquiring a set of Sarcasm (Nastiness) Cues onparticular development sets of dialogue turns calledthe ?MT exp dev?
in Table 1.The boxes labeled ?High Precision Sarcastic PostClassifier?
and ?High Precision Not Sarcastic PostClassifier?
in Fig.
2 involves using the Sarcasm(Nastiness) Cues in simple combinations that max-imize precision at the expense of recall.
R&Wfound cue combinations that yielded a High Preci-sion Classifier (HP Classifier) with 90% precisionand 32% recall on their dataset.
We discuss our testof these steps in Sec.
5 on the ?HP train?
develop-ment sets in Table 1 to estimate parameters for theHigh Precision classifier, and then test the HP classi-fier with these parameters on the test dataset labeled?HP dev test?
in Table 1.R&W?s Pattern Based classifier increased recallto 40% while losing very little precision.
The openquestion with applying R&W?s method to our data,was whether the cues that we discovered, by what-ever method, would work at high enough precisionto support generalization (O2 below).
In Sec.
6 we32describe how we use the ?PE eval?
development set(Table 1) to estimate parameters for the ExtractionPattern Learner, and then test the Pattern Based Sar-castic (Nasty) Post classifier on the newly classifiedutterances from the dataset labeled ?HP dev test?
(Table 1).
Our final open question was whether theextraction patterns from R&W, which worked wellfor news text, would work on social dialogue (O3below).
Thus our experiments address the followingopen questions as to whether R&W?s bootstrappingmethod improves classifiers for sarcasm and nasti-ness in online dialogues:?
(O1) Can we develop a ?known sarcastic(nasty) vocabulary??
The LH circle of Fig.
2illustrates that we use two different methods toidentify Sarcasm Cues.
Because we have ut-terances labeled as sarcastic, we compare a sta-tistical method that extracts important featuresautomatically from utterances, with a methodthat has a human in the loop, asking annotatorsto select phrases that are good indicators of sar-casm (nastiness) (Sec.
5);?
(O2) If we can develop a reliable set of sarcasm(nastiness) cues, is it then possible to developan HP classifier?
Will our precision be highenough?
Is the fact that sarcasm is often con-text dependent an issue?
(Sec.
5);?
(O3) Will the extraction patterns used inR&W?s work allow us to generalize sarcasmcues from the HP Classifiers?
Are R&W?s pat-terns general enough to work well for dialogueand social language?
(Sec.
6).4 Sarcasm and Nastiness CuesBecause there is no prior ?Known Sarcastic Vocabu-lary?
we pilot two different methods for discoveringlexical cues to sarcasm and nastiness, and experi-ment with combinations of cues that could yield ahigh precision classifier (Gianfortoni et al 2011).The first method uses ?2 to measure whether a wordor phrase is statistically indicative of sarcasm (nasti-ness) in the development sets labeled ?MT exp dev?
(Table 1).
This method, a priori, seems reasonablebecause it is likely that if you have a large enoughset of utterances labeled as sarcastic, you could beable to automatically learn a set of reliable cues forsarcasm.The second method introduces a step of humanannotation.
We ask Turkers to identify sarcastic(nasty) indicators in utterances (the open questionunigram?2 MT IA FREQright ah .95 2oh relevant .85 2we amazing .80 2same haha .75 2all yea .73 3them thanks .68 6mean oh .56 56bigram?2 MT IA FREQthe same oh really .83 2mean like oh yeah .79 2trying to so sure .75 2that you no way .72 3oh yeah get real .70 2I think oh no .66 4we should you claim .65 2trigram?2 MT IA FREQyou mean to I get it .97 3mean to tell I?m so sure .65 2have to worry then of course .65 2sounds like a are you saying .60 2to deal with well if you .55 2I know I go for it .52 2you mean to oh, sorry .50 2Table 2: Mechanical Turk (MT) and ?2 indicators forSarcasmO1) from the development set ?MT exp dev?
(Ta-ble 1).
Turkers were presented with utterances pre-viously labeled sarcastic or nasty in IAC by 7 dif-ferent Turkers, and were told ?In a previous study,these responses were identified as being sarcastic by3 out of 4 Turkers.
For each quote/response pair,we will ask you to identify sarcastic or potentiallysarcastic phrases in the response?.
The Turkers thenselected words or phrases from the response they be-lieved could lead someone to believing the utterancewas sarcastic or nasty.
These utterances were notused again in further experiments.
This crowdsourc-ing method is similar to (Filatova, 2012), but wheretheir data is monologic, ours is dialogic.4.1 Results from Indicator CuesSarcasm is known to be highly variable in form, andto depend, in some cases, on context for its inter-pretation (Sperber and Wilson, 1981; Gibbs, 2000;Bryant and Fox Tree, 2002).
We conducted an ini-tial pilot on 100 of the 617 sarcastic utterances in33unigram?2 MT IA FREQlike idiot .90 3them unfounded .85 2too babbling .80 2oh lie .72 11mean selfish .70 2just nonsense .69 9make hurt .67 3bigram?2 MT IA FREQof the don?t expect .95 2you mean get your .90 2yes, you?re an .85 2oh, what?s your .77 4you are prove it .77 3like a get real .75 2I think what else .70 2trigram?2 MT IA FREQto tell me get your sick .75 2would deny a your ignorance is .70 2like that?
make up your .70 2mean to tell do you really .70 2sounds like a do you actually .65 2you mean to doesn?t make it .63 3to deal with what?s your point .60 2Table 3: Mechanical Turk (MT) and ?2 indicators forNastyFigure 3: Interannotator Agreement for sarcasm trigramsthe development set ?MT exp dev?
to see if this wasnecessarily the case in our dialogues.
(Snow et al2008) measures the quality of Mechanical Turk an-notations on common NLP tasks by comparing themto a gold standard.
Pearson?s correlation coefficientshows that very few Mechanical Turk annotatorswere required to beat the gold standard data, oftenless than 5.
Because our sarcasm task does not havegold standard data, we ask 100 annotators to partic-ipate in the pilot.
Fig.
3 plots the average interan-notator agreement (ITA) as a function of the numberof annotators, computed using Pearson correlationcounts, for 40 annotators and for trigrams which re-quire more data to converge.
In all cases (unigrams,bigrams, trigrams) ITA plateaus at around 20 anno-tators and is about 90% with 10 annotators, showingthat the Mechanical Turk tasks are well formed andthere is high agreement.
Thus we elicited only 10annotations for the remainder of the sarcastic and allthe nasty utterances from the development set ?MTexp dev?.We begin to form our ?known sarcastic vocab-ulary?
from these indicators, (open question O1).Each MT indicator has a FREQ (frequency): thenumber of times each indicator appears in the train-ing set; and an IA (interannotator agreement): howmany annotators agreed that each indicator was sar-castic or nasty.
Table 2 shows the best unigrams,bigrams, and trigrams from the ?2 test and from thesarcasm Mechanical Turk experiment and Table 3shows the results from the nasty experiment.
Wecompare the MT indicators to the ?2 indicators aspart of investigating open question O1.As a pure statistical method, ?2 can pick outthings humans might not.
For example, if it just hap-pened that the word ?we?
only occurs in sarcasticutterances in the development set, then ?2 will se-lect it as a strong sarcastic word (row 3 of Table 2).However, no human would recognize this word ascorresponding to sarcasm.
?2 could easily be over-trained if the ?MT exp dev?
development set is notlarge enough to eliminate such general words fromconsideration, ?MT exp dev?
only has 617 sarcasticutterances and 510 nasty utterances (Table 1).Words that the annotators select as indicators(columns labeled MT in Table 2 and Table 3) aremuch more easily identifiable although they do notappear as often.
For example, the IA of 0.95 for ?ah?in Table 2 means that of all the annotators who saw?ah?
in the utterance they annotated, 95% selected itto be sarcastic.
However the FREQ of 2 means that?ah?
only appeared in 2 utterances in the ?MT expdev?
development set.We test whether any of the methods for select-ing indicators provide reliable cues that generalizeto a larger dataset in Sec.
5.
The parameters thatwe estimate on the development sets are exactly howfrequent (compared to a ?1) and how reliable (com-34pared to a ?2) a cue has to be to be useful in R&W?sbootstrapping method.5 High-Precision ClassifiersR&W use their ?known subjective vocabulary?
totrain a High Precision classifier.
R&W?s HP classi-fier searches for exact surface matches of the sub-jective indicators and classifies utterances as sub-jective if two subjective indicators are present.
Wefollow similar guidelines to train HP Sarcasm andNasty Classifiers.
To test open question O1, weuse a development set called ?HP train?
(Table 1)to test three methods for measuring the ?goodness?of an indicator that could serve as a high precisioncue: (1) interannotator agreement based on anno-tators consensus from Mechanical Turk, on the as-sumption that the number of annotators that selecta cue indicates its strength and reliability (IA fea-tures); (2) percent sarcastic (nasty) and frequencystatistics in the HP train dataset as R&W do (percentfeatures); and (3) the ?2 percent sarcastic (nasty)and frequency statistics (?2 features).The IA features use the MT indicators and the IAand FREQ calculations introduced in Sec.
4 (seeTables 2 and 3).
First, we select indicators suchthat ?1 <= FREQ where ?1 is a set of possiblethresholds.
Then we introduce two new parameters?
and ?
to divide the indicators into three ?good-ness?
groups that reflect interannotator agreement.indicatorstrength ={weak if 0 ?
IA < ?medium if ?
?
IA < ?strong if ?
?
IA < 1For IA features, an utterance is classified as sar-castic if it contains at least one strong or two mediumindicators.
Other conditions were piloted.
We firsthypothesized that weak cues might be a way ofclassifying ?not sarcastic?
utterances.
But HP trainshowed that both sarcastic and not sarcastic utter-ances contain weak indicators yielding no informa-tion gain.
The same is true for Nasty?s counter-class Nice.
Thus we specify that counter-class utter-ances must have no strong indicators or at most onemedium indicator.
In contrast, R&W?s counter-classclassifier looks for a maximum of one subjective in-dicator.The percent features also rely on the FREQ ofeach MT indicator, subject to a ?1 threshold, aswell as the percentage of the time they occur ina sarcastic utterance (%SARC) or nasty utterance(%NASTY).
We select indicators with various pa-rameters for ?1 and ?2 ?
%SARC.
At least two in-dicators must be present and above the thresholds tobe classified and we exhaust all combinations.
Lessthan two indicators are needed to be classified as thecounter-class, as in R&W.Finally, the ?2 features use the same method aspercent features only using the ?2 indicators insteadof the MT indicators.After determining which parameter settings per-forms the best for each feature set, we ran the HPclassifiers, using each feature set and the best param-eters, on the test set labeled ?HP dev test?.
The HPClassifiers classify the utterances that it is confidenton, and leave others unlabeled.5.1 Results from High Precision ClassifiersThe HP Sarcasm and Nasty Classifiers were trainedon the three feature sets with the following parame-ters: IA features we exhaust all combinations of ?
=[.70, .75, .80, .85, .90, .95, 1.00], ?
= [.35, .40, .45,.50, .55, .60, .65, .7], and ?1 = [2, 4, 6, 8, 10]; for thepercent features and ?2 features we again exhaust ?1= [2, 4, 6, 8, 10] and ?2 = [.55, .60, .65, .70, .75, .80,.85, .90, .95, 1.00].Tables 4 and 5 show a subset of the experimentswith each feature set.
We want to select parame-ters that maximize precision without sacrificing toomuch recall.
Of course, the parameters that yieldthe highest precision also have the lowest recall, e.g.Sarcasm percent features, parameters ?1 = 4 and?2 = 0.75 achieve 92% precision but the recall is1% (Table 4), and Nasty percent features with pa-rameters ?1 = 8 and ?2 = 0.8 achieves 98% preci-sion but a recall of 3% (Table 5).
On the other end ofthe spectrum, the parameters that achieve the highestrecall yield a precision equivalent to random chance.Examining the parameter combinations in Ta-bles 4 and 5 shows that percent features do betterthan IA features in all cases in terms of precision.Compare the block of results labeled % in Tables 4and 5 with the IA and ?2 blocks for column P. Nastyappears to be easier to identify than Sarcasm, espe-cially using the percent features.
The performanceof the ?2 features is comparable to that of percentfeatures for sarcasm, but lower than percent featuresfor Nasty.The best parameters selected from each featureset are shown in the PARAMS column of Table 6.With the indicators learned from these parameters,we run the Classifiers on the test set labeled ?HP35SARC PARAMS P R N (tp)% ?1 =4, ?2 =.55 62% 55% 7684, .6 72% 32% 4584, .65 84% 12% 1704, .75 92% 1% 23IA ?1 =2, ?
=.90, ?
=.35 51% 73% 1,0262, .95, .55 62% 13% 1892, .9, .55 54% 34% 4724, .75, .5 64% 7% 1024, .75, .6 78% 1% 22?2 ?1 =8, ?2 =.55 59% 64% 8938, .6 67% 31% 4348, .65 70% 12% 1708, .75 93% 1% 14Table 4: Sarcasm Train results; P: precision, R: recall, tp:true positive classificationsNASTY PARAMS P R N (tp)% ?1 =2, ?2 =.55 65% 69% 7984, .65 80% 44% 5098, .75 95% 11% 1258, .8 98% 3% 45IA ?1 =2, ?
=.95, ?
=.35 50% 96% 1,1262, .95, .45 60% 59% 6934, .75, .45 60% 50% 5802, .7, .55 73% 12% 1492, .9, .65 85% 1% 17?2 ?1 =2, ?2 =.55 73% 15% 1872, .65 78% 8% 1042, .7 86% 3% 32Table 5: Nasty Train results; P: precision, R: recall, tp:true positive classificationsdev test?
(Table 1).
The performance on test set ?HPdev test?
(Table 6) is worse than on the training set(Tables 4 and 5).
However we conclude that boththe % and ?2 features provide candidates for sar-casm (nastiness) cues that are high enough precision(open question O2) to be used in the Extraction Pat-tern Learner (Sec.
6), even if Sarcasm is more con-text dependent than Nastiness.PARAMS P R FSarc % ?1 =4, ?2 =.55 54% 38% 0.46Sarc IA ?1 =2, ?
=.95, ?
=.55 56% 11% 0.34Sarc ?2 ?1 =8, ?2 =.60 60% 19% 0.40Nasty % ?1 =2, ?2 =.55 58% 49% 0.54Nasty IA ?1 =2, ?
=.95, ?
=.45 53% 35% 0.44Nasty ?2 ?1 =2, ?2 =.55 74% 14% 0.44Table 6: HP Dev test results; PARAMS: the best pa-rameters for each feature set P: precision, R: recall, F:f-measure6 Extraction PatternsR&W?s Pattern Extractor searches for instances ofthe 13 templates in the first column of Table 7 in ut-terances classified by the HP Classifier.
We reim-plement this; an example of each pattern as in-stantiated in test set ?HP dev test?
for our data isshown in the second column of Table 7.
The tem-plate <subj> active-verb <dobj> matches ut-terances where a subject is followed by an activeverb and a direct object.
However, these matchesare not limited to exact surface matches as the HPClassifiers required, e.g.
this pattern would matchthe phrase ?have a problem?.
Table 10 in the Ap-pendix provides example utterances from IAC thatmatch the instantiated template patterns.
For exam-ple, the excerpt from the first row in Table 10 ?Itis quite strange to encounter someone in this dayand age who lacks any knowledge whatsoever of themechanism of adaptation since it was explained 150years ago?
matches the <subj> passive-verbpattern.
It appears 2 times (FREQ) in the test setand is sarcastic both times (%SARC is 100%).
Row11 in Table 10 shows an utterance matching theactive-verb prep <np> pattern with the phrase?At the time of the Constitution there weren?t ex-actly vast suburbs that could be prowled by thieveslooking for an open window?.
This phrase appears14 times (FREQ) in the test set and is sarcastic(%SARC) 92% of the time it appears.Synactic Form Example Pattern<subj> passive-verb <subj> was explained<subj> active-verb <subj> appears<subj> active-verb dobj <subj> have problem<subj> verb infinitive <subj> have to do<subj> aux noun <subj> is nothingactive-verb <dobj> gives <dobj>infinitive <dobj> to force <dobj>verb infinitive <dobj> want to take <dobj>noun aux <dobj> fact is <dobj>noun prep <np> argument against <np>active-verb prep <np> looking for <np>passive-verb prep <np> was put in <np>infinitive prep <np> to go to <np>Table 7: Syntactic Templates and Examples of Patternsthat were Learned for Sarcasm.
Table.
10 in the Appendixprovides example posts that instantiate these patterns.The Pattern Based Classifiers are trained on a de-velopment set labeled ?PE eval?
(Table 1).
Utter-ances from this development set are not used again36Figure 4: Recall vs.
Precision for Sarcasm PE evalin any further experiments.
Patterns are extractedfrom the dataset and we again compute FREQ and%SARC and %NASTY for each pattern subject to?1 ?
FREQ and ?2 ?
%SARC or % NASTY.Classifications are made if at least two patterns arepresent and both are above the specified ?1 and ?2,as in R&W.
Also following R&W, we do not learn?not sarcastic?
or ?nice?
patterns.To test the Pattern Based Classifiers, we use as in-put the classifications made by the HP Classifiers.Using the predicted labels from the classifiers as thetrue labels, the patterns from test set ?HP test dev?are extracted and compared to those patterns foundin development set ?PE eval?.
We have two featuresets for both sarcasm and nastiness: one using thepredictions from the MT indicators in the HP clas-sifier (percent features) and another using those in-stances from the ?2 features.6.1 Results from Pattern ClassifierThe Pattern Classifiers classify an utterance as Sar-castic (Nasty) if at least two patterns are present andabove the thresholds ?1 and ?2, exhausting all com-binations of ?1 = [2, 4, 6, 8, 10] and ?2 = [.55, .60,.65, .70, .75, .80, .85, .90, .95, 1.00].
The counter-classes are predicted when the utterance containsless than two patterns.
The exhaustive classifica-tions are first made using the utterances in the de-velopment set labeled ?PE eval?.
Fig.
4 shows theprecision and recall trade-off for ?1 = [2, 10] and all?2 values on sarcasm development set?PE eval?.
Asrecall increases, precision drops.
By including pat-terns that only appear 2 times, we get better recall.Limiting ?1 to 10 yields fewer patterns and lowerrecall.Table 8 shows the results for various parameters.The PE dev dataset learned a total of 1,896 sarcas-tic extraction patterns above a minimum threshold of?1 < 2 and ?2 < 0.55, and similarly 847 nasty ex-traction patterns.
Training on development set ?PEdev?
yields high precision and good recall.
To se-lect the best parameters, we again look for a balancebetween precision and recall.
Both Classifiers havevery high precision.
In the end, we select parame-ters that have a better recall than the best parame-ter from the HP Classifiers which is recall = 38%for sarcasm and recall = 49% for nastiness.
Thebest parameters and their test results are shown inTable 9.PARAMS P R F N (tp)SARC ?1 =2, ?2 =.60 65% 49% 0.57 7922, .65 71% 44% 0.58 7172, .70 80% 38% 0.59 6162, 1.0 97% 24% 0.60 382NASTY ?1 =2, ?2 =.65 71% 49% 0.60 3352, .75 83% 42% 0.62 2892, .90 96% 30% 0.63 209Table 8: Pattern Classification Training; P: precision, R:recall, F: F-measure, tp: true positive classificationsThe Pattern Classifiers are tested on ?HP dev test?with the labels predicted by our HP Classifiers, thuswe have two different sets of classifications for bothSarcasm and Nastiness: percent features and ?2 fea-tures.
Overall, the Pattern Classification performsbetter on Nasty than Sarcasm.
Also, the percent fea-tures yield better results than ?2 features, possiblybecause the precision for ?2 is high from the HPClassifiers, but the recall is very low.
We believethat ?2 selects statistically predictive indicators thatare tuned to the dataset, rather than general.
Havinga human in the loop guarantees more general fea-tures from a smaller dataset.
Whether this remainstrue on the size as the dataset increases to 1000 ormore is unknown.
We conclude that R&W?s patternsgeneralize well on our Sarcasm and Nasty datasets(open question O3), but suspect that there may bebetter syntactic patterns for bootstrapping sarcasmand nastiness, e.g.
involving cue words or semanticcategories of words rather than syntactic categories,as we discuss in Sec.
7.This process can be repeated by taking the newlyclassified utterances from the Pattern Based Clas-sifiers, then applying the Pattern Extractor to learnnew patterns from the newly classified data.
This37PARAMS P R FSarc % ?1 =2, ?2 =.70 62% 52% 0.57Sarc ?2 ?1 =2, ?2 =.70 31% 58% 0.45Nasty % ?1 =2, ?2 =.65 75% 62% 0.69Nasty ?2 ?1 =2, ?2 =.65 30% 70% 0.50Table 9: The results for Pattern Classification on HP devtest dataset ; PARAMS: the best parameters for each fea-ture set P: precision, R: recall, F: f-measurecan be repeated for multiple iterations.
We leave thisfor future work.7 Discussion and Future WorkIn this work, we apply a bootstrapping method totrain classifiers to identify particular types of subjec-tive utterances in online dialogues.
First we createa suite of linguistic indicators for sarcasm and nas-tiness using crowdsourcing techniques.
Our crowd-sourcing method is similar to (Filatova, 2012).
Fromthese new linguistic indicators we construct a classi-fier following previous work on bootstrapping sub-jectivity classifiers (Riloff and Wiebe, 2003; Thelenand Riloff, 2002).
We compare the performance ofthe High Precision Classifier that was trained basedon statistical measures against one that keeps humanannotators in the loop, and find that Classifiers us-ing statistically selected indicators appear to be over-trained on the development set because they do notgeneralize well.
This first phase achieves 54% preci-sion and 38% recall for sarcastic utterances using thehuman selected indicators.
If we bootstrap by usingsyntactic patterns to create more general sarcasm in-dicators from the utterances identified as sarcastic inthe first phase, we achieve a higher precision of 62%and recall of 52%.We apply the same method to bootstrapping aclassifier for nastiness dialogic acts.
Our first phase,using crowdsourced nasty indicators, achieves 58%precision and 49% recall, which increases to 75%precision and 62% recall when we bootstrap withsyntactic patterns, possibly suggesting that nastiness(insults) are less nuanced and easier to detect thansarcasm.Previous work claims that recognition of sarcasm(1) depends on knowledge of the speaker, (2) worldknowledge, or (3) use of context (Gibbs, 2000; Eis-terhold et al 2006; Bryant and Fox Tree, 2002;Carvalho et al 2009).
While we also believe thatcertain types of subjective language cannot be de-termined from cue words alone, our Pattern BasedClassifiers, based on syntactic patterns, still achieveshigh precision and recall.
In comparison to previousmonologic works whose sarcasm precision is about75%, ours is not quite as good with 62%.
While thenasty works do not report precision, we believe thatthey are comparable to the 64% - 83% accuracy withour precision of 75%.Open question O3 was whether R&W?s patternsare fine tuned to subjective utterances in news.
How-ever R&W?s patterns improve both precision and re-call of our Sarcastic and Nasty classifiers.
In fu-ture work however, we would like to test whethersemantic categories of words rather than syntacticcategories would perform even better for our prob-lem, e.g.
Linguistic Inquiry and Word Count cat-egories.
Looking again at row 1 in Table 10, ?Itis quite strange to encounter someone in this dayand age who lacks any knowledge whatsoever of themechanism of adaptation since it was explained 150years ago?, the word ?quite?
matches the ?cogmech?and ?tentative?
categories, which might be interest-ing to generalize to sarcasm.
In row 11 ?At the timeof the Constitution there weren?t exactly vast sub-urbs that could be prowled by thieves looking for anopen window?, the phrase ?weren?t exactly?
couldalso match the LIWC categories ?cogmech?
and ?cer-tain?
or, more specifically, certainty negated.We also plan to extend this work to other cate-gories of subjective dialogue acts, e.g.
emotionaland respectful as mentioned in the Introduction, andto expand our corpus of subjective dialogue acts.
Wewill experiment with performing more than one iter-ation of the bootstrapping process (R&W completetwo iterations) as well as create a Hybrid Classifiercombining the subjective cues and patterns into asingle Classifier that itself can be bootstrapped.Finally, we would like to extend our method todifferent dialogue domains to see if the classifierstrained on our sarcastic and nasty indicators wouldachieve similar results or if different social mediasites have their own style of displaying sarcasm ornastiness not comparable to those in forum debates.ReferencesG.A.
Bryant and J.E.
Fox Tree.
2002.
Recognizing ver-bal irony in spontaneous speech.
Metaphor and sym-bol, 17(2):99?119.P.
Carvalho, L. Sarmento, M.J. Silva, and E. de Oliveira.2009.
Clues for detecting irony in user-generated con-38tents: oh...!!
it?s so easy;-).
In Proc.
of the 1st inter-national CIKM workshop on Topic-sentiment analysisfor mass opinion, p. 53?56.
ACM.D.
Davidov, O. Tsur, and A. Rappoport.
2010.
Semi-supervised recognition of sarcastic sentences in twitterand amazon.
In Proc.
of the Fourteenth Conference onComputational Natural Language Learning, p. 107?116.
Association for Computational Linguistics.J.
Eisterhold, S. Attardo, and D. Boxer.
2006.
Reactionsto irony in discourse: Evidence for the least disruptionprinciple.
Journal of Pragmatics, 38(8):1239?1256.E.
Filatova.
2012.
Irony and sarcasm: Corpus genera-tion and analysis using crowdsourcing.
In LanguageResources and Evaluation Conference, LREC2012.J.E.
Fox Tree and J.C. Schrock.
1999.
Discourse Mark-ers in Spontaneous Speech: Oh What a Differencean Oh Makes.
Journal of Memory and Language,40(2):280?295.J.
E. Fox Tree.
2010.
Discourse markers across speak-ers and settings.
Language and Linguistics Compass,3(1):1?13.P.
Gianfortoni, D. Adamson, and C.P.
Rose?.
2011.
Mod-eling of stylistic variation in social media with stretchypatterns.
In Proc.
of the First Workshop on Algo-rithms and Resources for Modelling of Dialects andLanguage Varieties, p. 49?59.
ACL.R.W.
Gibbs.
2000.
Irony in talk among friends.Metaphor and Symbol, 15(1):5?27.R.
Gonza?lez-Iba?n?ez, S. Muresan, and N. Wacholder.2011.
Identifying sarcasm in twitter: a closer look.In Proc.
of the 49th Annual Meeting of the ACL: Hu-man Language Technologies: short papers, volume 2,p.
581?586.A.
Razavi, D. Inkpen, S. Uritsky, and S. Matwin.
2010.Offensive language detection using multi-level classi-fication.
Advances in Artificial Intelligence, p. 16?27.A.
Reyes and P. Rosso.
2011.
Mining subjective knowl-edge from customer reviews: a specific case of ironydetection.
In Proc.
of the 2nd Workshop on Computa-tional Approaches to Subjectivity and Sentiment Anal-ysis (WASSA 2.011), ACL, p. 118?124.A.
Reyes, P. Rosso, and D. Buscaldi.
2012.
From humorrecognition to irony detection: The figurative languageof social media.
Data & Knowledge Engineering.E.
Riloff and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In Proc.
of the 2003conference on Empirical methods in Natural Lan-guage Processing-V. 10, p. 105?112.
ACL.R.
Snow, B. O?Conner, D. Jurafsky, and A.Y.
Ng.
2008.Cheap and fast?but is it good?
: evaluating non-expertannotations for natural language tasks In Proc.
ofthe Conference on Empirical Methods in Natural Lan-guage Processing, p. 254?263.
ACM.S.O.
Sood, E.F. Churchill, and J. Antin.
2011.
Auto-matic identification of personal insults on social newssites.
Journal of the American Society for InformationScience and Technology.Dan Sperber and Deidre Wilson.
1981.
Irony and theuse-mention distinction.
In Peter Cole, editor, RadicalPragmatics, p. 295?318.
Academic Press, N.Y.E.
Spertus.
1997.
Smokey: Automatic recognition ofhostile messages.
In Proc.
of the National Conferenceon Artificial Intelligence, p. 1058?1065.M.
Thelen and E. Riloff.
2002.
A bootstrapping methodfor learning semantic lexicons using extraction patterncontexts.
In Proc.
of the ACL-02 conference on Empir-ical methods in natural language processing-Volume10, p. 214?221.
ACL.O.
Tsur, D. Davidov, and A. Rappoport.
2010.
Icwsm?a great catchy name: Semi-supervised recognition ofsarcastic sentences in online product reviews.
In Proc.of the fourth international AAAI conference on we-blogs and social media, p. 162?169.Marilyn Walker, Pranav Anand, , Robert Abbott, andJean E. Fox Tree.
2012.
A corpus for research ondeliberation and debate.
In Language Resources andEvaluation Conference, LREC2012.J.M.
Wiebe, R.F.
Bruce, and T.P.
O?Hara.
1999.
Devel-opment and use of a gold-standard data set for subjec-tivity classifications.
In Proc.
of the 37th annual meet-ing of the Association for Computational Linguistics,p.
246?253.
ACL.J.
Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,B.
Fraser, D. Litman, D. Pierce, E. Riloff, T. Wilson,et al2003.
Recognizing and organizing opinions ex-pressed in the world press.
In Working Notes-New Di-rections in Question Answering (AAAI Spring Sympo-sium Series).T.
Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,J.
Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-wardhan.
2005.
Opinionfinder: A system for subjec-tivity analysis.
In Proc.
of HLT/EMNLP on InteractiveDemonstrations, p. 34?35.
ACL.G.
Xiang, B.
Fan, L. Wang, J. Hong, and C. Rose.
2012.Detecting offensive tweets via topical feature discov-ery over a large scale twitter corpus.
In Proc.
ofthe 21st ACM international conference on Informationand knowledge management, p. 1980?1984.
ACM.8 Appendix A.
Instances of LearnedPatterns39Pattern Instance FREQ %SARC Example Utterance<subj> was explained 2 100% Well, I incorrectly assumed that anyone attempting to enter the discus-sion would at least have a grasp of the most fundamental principles.
Itis quite strange to encounter someone in this day and age who lacks anyknowledge whatsoever of the mechanism of adaptation since it was ex-plained 150 years ago.<subj> appears 1 94% It appears this thread has been attacked by the ?line item ?
poster.<subj> have problem 4 50% I see your point, Iangb but I?m not about to be leaving before you?ve hada chance to respond.
I won?t be ?leaving ?
at all.
You challenged meto produce an argument, so I?m going to produce my argument.
I willthen summarize the argument, and you can respond to it and we can thendiscuss / debate those specifics that you have a problem with.<subj> have to do 15 86% How does purchasing a house have to do with abortion?
Ok, so what ifthe kid wants to have the baby and the adults want to get rid of it?
Whatif the adults want her to have the baby and the kid wants to get rid of it?You would force the kid to have a child (that doesn?t seem responsible atall), or you would force the kid to abort her child (thereby taking awayher son or daughter).
Both of those decisions don?t sound very consitentor responsible.
The decision is best left up to the person that is pregnant,regardless of their age.<subj> is nothing 10 90% Even though there is nothing but ad hoc answers to the questions, cre-ationists touted the book as ?proof ?
that Noahs?
ark was possible.
Theynever seem to notice that no one has ever tried to build and float an ark.They prefer to put the money into creation museums and amusementparks.gives <dobj> 25 88% Just knowing that there are many Senators and Congressmen who wouldlike to abolish gun rights gives credence to the fact that government couldactually try to limit or ban the 2nd Amendment in the future.to force <dobj> 9 89% And I just say that it would be unjust and unfair of you to force meta-physical belief systems of your own which constitute religious beliefupon your follows who may believe otherwise than you.
Get pregnantand treat your fetus as a full person if you wish, nobody will force youto abort it.
Let others follow their own beliefs differing or the same.Otherwise you attempt to obtain justice by doing injusticewant to take <dobj> 5 80% How far do you want to take the preemptive strike thing?
Should wemake it illegal for people to gather in public in groups of two or largerbecause anything else might be considered a violent mob assembly forthe basis of creating terror and chaos?fact is <dobj> 6 83% No, the fact is PP was founded by an avowed racist and staunch supporterof Eugenics.argument against <np> 4 75% Perhaps I am too attached to this particular debate that you are havingbut if you actually have a sensible argument against gay marriage thenplease give it your best shot here.
I look forward to reading your com-ments.looking for <np> 14 92% At the time of the Constitution there weren?t exactly vast suburbs thatcould be prowled by thieves looking for an open window.was put in <np> 3 66% You got it wrong Daewoo.
The ban was put in place by the 1986 FirearmOwners Protection Act, designed to correct the erronius Gun Control Actof 1968.
The machinegun ban provision was slipped in at the last minute,during a time when those that would oppose it werent?
there to debate it.to go to <np> 8 63% Yes that would solve the problem wouldn?t it,worked the first timearound,I say that because we (U.S.)are compared to the wild west.
Butbe they whites,Blacks,Reds,or pi** purple shoot a few that try to detainor threaten you, yeah I think they will back off unless they are preparedto go to war.Table 10: Sarcastic patterns and example instances40
