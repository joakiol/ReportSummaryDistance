Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 399?405,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsCan Markov Models Over Minimal Translation Units Help Phrase-BasedSMT?Nadir DurraniUniversity of Edinburghdnadir@inf.ed.ac.ukHieu Hoang Philipp KoehnUniversity of Edinburghhieu.hoang,pkoehn@inf.ed.ac.ukAlexander Fraser Helmut SchmidLudwig Maximilian University Munichfraser,schmid@cis.uni-muenchen.deAbstractThe phrase-based and N-gram-basedSMT frameworks complement each other.While the former is better able to memo-rize, the latter provides a more principledmodel that captures dependencies acrossphrasal boundaries.
Some work has beendone to combine insights from these twoframeworks.
A recent successful attemptshowed the advantage of using phrase-based search on top of an N-gram-basedmodel.
We probe this question in thereverse direction by investigating whetherintegrating N-gram-based translation andreordering models into a phrase-baseddecoder helps overcome the problematicphrasal independence assumption.
A largescale evaluation over 8 language pairsshows that performance does significantlyimprove.1 IntroductionPhrase-based models (Koehn et al, 2003; Ochand Ney, 2004) learn local dependencies such asreorderings, idiomatic collocations, deletions andinsertions by memorization.
A fundamental draw-back is that phrases are translated and reorderedindependently of each other and contextual infor-mation outside of phrasal boundaries is ignored.The monolingual language model somewhat re-duces this problem.
However i) often the languagemodel cannot overcome the dispreference of thetranslation model for nonlocal dependencies, ii)source-side contextual dependencies are still ig-nored and iii) generation of lexical translations andreordering is separated.The N-gram-based SMT framework addressesthese problems by learning Markov chains over se-quences of minimal translation units (MTUs) alsoknown as tuples (Marin?o et al, 2006) or over op-erations coupling lexical generation and reorder-ing (Durrani et al, 2011).
Because the mod-els condition the MTU probabilities on the previ-ous MTUs, they capture non-local dependenciesand both source and target contextual informationacross phrasal boundaries.In this paper we study the effect of integratingtuple-based N-gram models (TSM) and operation-based N-gram models (OSM) into the phrase-based model in Moses, a state-of-the-art phrase-based system.
Rather than using POS-basedrewrite rules (Crego and Marin?o, 2006) to forma search graph, we use the ability of the phrase-based system to memorize larger translation unitsto replicate the effect of source linearization asdone in the TSM model.We also show that using phrase-based searchwith MTU N-gram translation models helps to ad-dress some of the search problems that are non-trivial to handle when decoding with minimaltranslation units.
An important limitation of theOSM N-gram model is that it does not handle un-aligned or discontinuous target MTUs and requirespost-processing of the alignment to remove these.Using phrases during search enabled us to makenovel changes to the OSM generative story (alsoapplicable to the TSM model) to handle unalignedtarget words and to use target linearization to dealwith discontinuous target MTUs.We performed an extensive evaluation, carryingout translation experiments from French, Spanish,Czech and Russian to English and in the oppositedirection.
Our integration of the OSM model intoMoses and our modification of the OSM model todeal with unaligned and discontinuous target to-kens consistently improves BLEU scores over the399baseline system, and shows statistically significantimprovements in seven out of eight cases.2 Previous WorkSeveral researchers have tried to combine the ideasof phrase-based and N-gram-based SMT.
Costa-jussa` et al (2007) proposed a method for combin-ing the two approaches by applying sentence levelreranking.
Feng et al (2010) added a linearizedsource-side language model in a phrase-based sys-tem.
Crego and Yvon (2010) modified the phrase-based lexical reordering model of Tillman (2004)for an N-gram-based system.
Niehues et al (2011)integrated a bilingual language model based onsurface word forms and POS tags into a phrase-based system.
Zhang et al (2013) explored multi-ple decomposition structures for generating MTUsin the task of lexical selection, and to rerank theN-best candidate translations in the output of aphrase-based.
A drawback of the TSM model isthe assumption that source and target informationis generated monotonically.
The process of re-ordering is disconnected from lexical generationwhich restricts the search to a small set of precom-puted reorderings.
Durrani et al (2011) addressedthis problem by coupling lexical generation andreordering information into a single generativeprocess and enriching the N-gram models to learnlexical reordering triggers.
Durrani et al (2013)showed that using larger phrasal units during de-coding is superior to MTU-based decoding in anN-gram-based system.
However, they do not usephrase-based models in their work, relying onlyon the OSM model.
This paper combines insightsfrom these recent pieces of work and show thatphrase-based search combined with N-gram-basedand phrase-based models in decoding is the over-all best way to go.
We integrate the two N-gram-based models, TSM and OSM, into phrase-basedMoses and show that the translation quality is im-proved by taking both translation and reorderingcontext into account.
Other approaches that ex-plored such models in syntax-based systems usedMTUs for sentence level reranking (Khalilov andFonollosa, 2009), in dependency translation mod-els (Quirk and Menezes, 2006) and in target lan-guage syntax systems (Vaswani et al, 2011).3 Integration of N-gram ModelsWe now describe our integration of TSM andOSM N-gram models into the phrase-based sys-Figure 1: Example (a) Word Alignments (b) Un-folded MTU Sequence (c) Operation Sequence (d)Step-wise Generationtem.
Given a bilingual sentence pair (F,E) andits alignment (A), we first identify minimal trans-lation units (MTUs) from it.
An MTU is definedas a translation rule that cannot be broken downany further.
The MTUs extracted from Figure 1(a)are A ?
a,B ?
b, C .
.
.H ?
c1 and D ?
d.These units are then generated left-to-right in twodifferent ways, as we will describe next.3.1 Tuple Sequence Model (TSM)The TSM translation model assumes that MTUsare generated monotonically.
To achieve this ef-fect, we enumerate the MTUs in the target left-to-right order.
This process is also called sourcelinearization or tuple unfolding.
The resulting se-quence of monotonic MTUs is shown in Figure1(b).
We then define a TSM model over this se-quence (t1, t2, .
.
.
, tJ ) as:ptsm(F,E,A) =J?j=1p(tj |tj?n+1, ..., tj?1)where n indicates the amount of context used.
A4-gram Kneser-Ney smoothed language model istrained with SRILM (Stolcke, 2002).Search: In previous work, the search graph inTSM N-gram SMT was not built dynamicallylike in the phrase-based system, but instead con-structed as a preprocessing step using POS-basedrewrite rules (learned when linearizing the sourceside).
We do not adopt this framework.
We use1We use .
.
.
to denote discontinuous MTUs.400phrase-based search which builds up the decodinggraph dynamically and searches through all pos-sible reorderings within a fixed window.
Duringdecoding we use the phrase-internal alignments toperform source linearization.
For example, if dur-ing decoding we would like to apply the phrasepair ?C D H ?
d c?, a combination of t3 and t4 inFigure 1(b), then we extract the MTUs from thisphrase-pair and linearize the source to be in theorder of the target.
We then compute the TSMprobability given the n ?
1 previous MTUs (in-cluding MTUs occurring in the previous sourcephrases).
The idea is to replicate rewrite ruleswith phrase-pairs to linearize the source.
Previ-ous work on N-gram-based models restricted thelength of the rewrite rules to be 7 or less POS tags.We use phrases of length 6 and less.3.2 Operation Sequence Model (OSM)The OSM model represents a bilingual sentencepair and its alignment through a sequence of oper-ations that generate the aligned sentence pair.
Anoperation either generates source and target wordsor it performs reordering by inserting gaps andjumping forward and backward.
The MTUs aregenerated in the target left-to-right order just as inthe TSM model.
However rather than linearizingthe source-side, reordering operations (gaps andjumps) are used to handle crossing alignments.During training, each bilingual sentence pair is de-terministically converted to a unique sequence ofoperations.2 The example in Figure 1(a) is con-verted to the sequence of operations shown in Fig-ure 1(c).
A step-wise generation of MTUs alongwith reordering operations is shown in Figure 1(d).We learn a Markov model over a sequence of oper-ations (o1, o2, .
.
.
, oJ ) that encapsulate MTUs andreordering information which is defined as fol-lows:posm(F,E,A) =J?j=1p(oj |oj?n+1, ..., oj?1)A 9-gram Kneser-Ney smoothed language modelis trained with SRILM.3 By coupling reorder-ing with lexical generation, each (translation orreordering) decision conditions on n ?
1 previ-ous (translation and reordering) decisions span-ning across phrasal boundaries.
The reorderingdecisions therefore influence lexical selection and2Please refer to Durrani et al (2011) for a list of opera-tions and the conversion algorithm.3We also tried a 5-gram model, the performance de-creased slightly in some cases.vice versa.
A heterogeneous mixture of translationand reordering operations enables the OSM modelto memorize reordering patterns and lexicalizedtriggers unlike the TSM model where translationand reordering are modeled separately.Search: We integrated the generative story ofthe OSM model into the hypothesis extension pro-cess of the phrase-based decoder.
Each hypothesismaintains the position of the source word coveredby the last generated MTU, the right-most sourceword generated so far, the number of open gapsand their relative indexes, etc.
This informationis required to generate the operation sequence forthe MTUs in the hypothesized phrase-pair.
Afterthe operation sequence is generated, we computeits probability given the previous operations.
Wedefine the main OSM feature, and borrow 4 sup-portive features, the Gap, Open Gap, Gap-widthand Deletion penalties (Durrani et al, 2011).3.3 Problem: Target Discontinuity andUnaligned WordsTwo issues that we have ignored so far are the han-dling of MTUs which have discontinuous targets,and the handling of unaligned target words.
BothTSM and OSM N-gram models generate MTUslinearly in left-to-right order.
This assumption be-comes problematic in the cases of MTUs that havetarget-side discontinuities (See Figure 2(a)).
TheMTU A?
g .
.
.
a can not be generated because ofthe intervening MTUs B ?
b, C .
.
.H ?
c andD ?
d. In the original TSM model, such cases aredealt with by merging all the intervening MTUsto form a bigger unit t?1 in Figure 2(c).
A solu-tion that uses split-rules is proposed by Crego andYvon (2009) but has not been adopted in Ncode(Crego et al, 2011), the state-of-the-art TSM N-gram system.
Durrani et al (2011) dealt withthis problem by applying a post-processing (PP)heuristic that modifies the alignments to removesuch cases.
When a source word is aligned to adiscontinuous target-cept, first the link to the leastfrequent target word is identified, and the groupof links containing this word is retained while theothers are deleted.
The alignment in Figure 2(a),for example, is transformed to that in Figure 2(b).This allows OSM to extract the intervening MTUst2 .
.
.
t5 (Figure 2(c)).
Note that this problem doesnot exist when dealing with source-side disconti-nuities: the TSM model linearizes discontinuoussource-side MTUs such as C .
.
.H ?
c. The401Figure 2: Example (a) Original Alignments (b)Post-Processed Alignments (c) Extracted MTUs ?t?1 .
.
.
t?3 (from (a)) and t1 .
.
.
t7 (from (b))OSM model deals with such cases through InsertGap and Continue Cept operations.The second problem is the unaligned target-sideMTUs such as ?
?
f in Figure 2(a).
Insertingtarget-side words ?spuriously?
during decoding isa non-trival problem because there is no evidenceof when to hypothesize such words.
These casesare dealt with in N-gram-based SMT by mergingsuch MTUs to the MTU on the left or right basedon attachment counts (Durrani et al, 2011), lexicalprobabilities obtained from IBM Model 1 (Marin?oet al, 2006), or POS entropy (Gispert and Marin?o,2006).
Notice how ??
f (Figure 2(a)) is mergedwith the neighboring MTU E ?
e to form a newMTU E ?
ef (Figure 2 (c)).
We initially used thepost-editing heuristic (PP) as defined by Durrani etal.
(2011) for both TSM and OSM N-gram mod-els, but found that it lowers the translation quality(See Row 2 in Table 2) in some language pairs.3.4 Solution: Insertion and LinearizationTo deal with these problems, we made novel modi-fications to the generative story of the OSM model.Rather than merging the unaligned target MTUsuch as ?
?
f , to its right or left MTU, we gen-erate it through a new Generate Target Only (f)operation.
Orthogonal to its counterpart GenerateSource Only (I) operation (as used for MTU t7 inFigure 2 (c)), this operation is generated as soonas the MTU containing its previous target wordis generated.
In Figure 2(a), ?
?
f is generatedimmediately after MTU E ?
e is generated.
Ina sequence of unaligned source and target MTUs,unaligned source MTUs are generated before theunaligned target MTUs.
We do not modify the de-coder to arbitrarily generate unaligned MTUs buthypothesize these only when they appear withinan extracted phrase-pair.
The constraint providedby the phrase-based search makes the GenerateTarget Only operation tractable.
Using phrase-based search therefore helps addressing some ofthe problems that exist in the decoding frameworkof N-gram SMT.The remaining problem is the discontinuous tar-get MTUs such as A?
g .
.
.
a in Figure 2(a).
Wehandle this with target linearization similar to theTSM source linearization.
We collapse the targetwords g and a in the MTU A ?
g .
.
.
a to occurconsecutively when generating the operation se-quence.
The conversion algorithm that generatesthe operations thinks that g and a occurred adja-cently.
During decoding we use the phrasal align-ments to linearize such MTUs within a phrasalunit.
This linearization is done only to computethe OSM feature.
Other features in the phrase-based system (e.g., language model) work with thetarget string in its original order.
Notice again howmemorizing larger translation units using phraseshelps us reproduce such patterns.
This is achievedin the tuple N-gram model by using POS-basedsplit and rewrite rules.4 EvaluationCorpus: We ran experiments with data madeavailable for the translation task of the EighthWorkshop on Statistical Machine Translation.
Thesizes of bitext used for the estimation of translationand monolingual language models are reported inTable 1.
All data is true-cased.Pair Parallel Monolingual Langfr?en ?39 M ?91 M frcs?en ?15.6 M ?43.4 M cses?en ?15.2 M ?65.7 M esru?en ?2 M ?21.7 M ru?287.3 M enTable 1: Number of Sentences (in Millions) usedfor TrainingWe follow the approach of Schwenk and Koehn(2008) and trained domain-specific language mod-els separately and then linearly interpolated themusing SRILM with weights optimized on the held-out dev-set.
We concatenated the news-test setsfrom four years (2008-2011) to obtain a large dev-setin order to obtain more stable weights (Koehnand Haddow, 2012).
For Russian-English andEnglish-Russian language pairs, we divided thetuning-set news-test 2012 into two halves and used402No.
System fr-en es-en cs-en ru-en en-fr en-es en-cs en-ru1.
Baseline 31.89 35.07 23.88 33.45 29.89 35.03 16.22 23.882.
1+pp 31.87 35.09 23.64 33.04 29.70 35.00 16.17 24.053.
1+pp+tsm 31.94 35.25 23.85 32.97 29.98 35.06 16.30 23.964.
1+pp+osm 32.17 35.50 24.14 33.21 30.35 35.34 16.49 24.225.
1+osm* 32.13 35.65 24.23 33.91 30.54 35.49 16.62 24.25Table 2: Translating into and from English.
Bold: Statistically Significant (Koehn, 2004) w.r.t Baselinethe first half for tuning and second for test.
We testour systems on news-test 2012.
We tune with thek-best batch MIRA algorithm (Cherry and Foster,2012).Moses Baseline: We trained a Moses system(Koehn et al, 2007) with the following settings:maximum sentence length 80, grow-diag-final-and symmetrization of GIZA++ alignments, aninterpolated Kneser-Ney smoothed 5-gram lan-guage model with KenLM (Heafield, 2011) used atruntime, msd-bidirectional-fe lexicalized reorder-ing, sparse lexical and domain features (Hasleret al, 2012), distortion limit of 6, 100-besttranslation options, minimum bayes-risk decoding(Kumar and Byrne, 2004), cube-pruning (Huangand Chiang, 2007) and the no-reordering-over-punctuation heuristic.Results: Table 2 shows uncased BLEU scores(Papineni et al, 2002) on the test set.
Row 2 (+pp)shows that the post-editing of alignments to re-move unaligned and discontinuous target MTUsdecreases the performance in the case of ru-en, cs-en and en-fr.
Row 3 (+pp+tsm) shows that our in-tegration of the TSM model slightly improves theBLEU scores for en-fr, and es-en.
Results dropin ru-en and en-ru.
Row 4 (+pp+osm) shows thatthe OSM model consistently improves the BLEUscores over the Baseline systems (Row 1) givingsignificant improvements in half the cases.
Theonly result that is lower than the baseline systemis that of the ru-en experiment, because OSM isbuilt with PP alignments which particularly hurtthe performance for ru-en.
Finally Row 5 (+osm*)shows that our modifications to the OSM model(Section 3.4) give the best result ranging from[0.24?0.65] with statistically significant improve-ments in seven out of eight cases.
It also shows im-provements over Row 4 (+pp+osm) even in somecases where the PP heuristic doesn?t hurt.
Thelargest gains are obtained in the ru-en translationtask (where the PP heuristic inflicted maximumdamage).5 Conclusion and Future WorkWe have addressed the problem of the indepen-dence assumption in PBSMT by integrating N-gram-based models inside a phrase-based systemusing a log-linear framework.
We try to replicatethe effect of rewrite and split rules as used in theTSM model through phrasal alignments.
We pre-sented a novel extension of the OSM model tohandle unaligned and discontinuous target MTUsin the OSM model.
Phrase-based search helps usto address these problems that are non-trivial tohandle in the decoding frameworks of the N-gram-based models.
We tested our extentions and modi-fications by evaluating against a competitive base-line system over 8 language pairs.
Our integra-tion of TSM shows small improvements in a fewcases.
The OSM model which takes both reorder-ing and lexical context into consideration consis-tently improves the performance of the baselinesystem.
Our modification to the OSM model pro-duces the best results giving significant improve-ments in most cases.
Although our modificationsto the OSM model enables discontinuous MTUs,we did not fully utilize these during decoding, asMoses only uses continous phrases.
The discon-tinuous MTUs that span beyond a phrasal lengthof 6 words are therefore never hypothesized.
Wewould like to explore this further by extending thesearch to use discontinuous phrases (Galley andManning, 2010).AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful feedback and suggestions.
The re-search leading to these results has received fund-ing from the European Union Seventh FrameworkProgramme (FP7/2007-2013) under grant agree-ment n ?
287658.
Alexander Fraser was funded byDeutsche Forschungsgemeinschaft grant Modelsof Morphosyntax for Statistical Machine Transla-tion.
Helmut Schmid was supported by DeutscheForschungsgemeinschaft grant SFB 732.
Thispublication only reflects the authors views.403ReferencesColin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 427?436, Montre?al, Canada, June.
Associa-tion for Computational Linguistics.Marta R. Costa-jussa`, Josep M. Crego, David Vilar,Jose?
A.R.
Fonollosa, Jose?
B. Marin?o, and Her-mann Ney.
2007.
Analysis and System Combina-tion of Phrase- and N-Gram-Based Statistical Ma-chine Translation Systems.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics; Companion Volume, Short Pa-pers, pages 137?140, Rochester, New York, April.Josep M. Crego and Jose?
B. Marin?o.
2006.
ImprovingStatistical MT by Coupling Reordering and Decod-ing.
Machine Translation, 20(3):199?215.Josep M. Crego and Franc?ois Yvon.
2009.
GappyTranslation Units under Left-to-Right SMT Decod-ing.
In Proceedings of the Meeting of the EuropeanAssociation for Machine Translation (EAMT), pages66?73, Barcelona, Spain.Josep M. Crego and Franc?ois Yvon.
2010.
Improv-ing Reordering with Linguistically Informed Bilin-gual N-Grams.
In Coling 2010: Posters, pages 197?205, Beijing, China, August.
Coling 2010 Organiz-ing Committee.Josep M. Crego, Franc?ois Yvon, and Jose?
B. Marin?o.2011.
Ncode: an Open Source Bilingual N-gramSMT Toolkit.
The Prague Bulletin of MathematicalLinguistics, 96:49?58.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A Joint Sequence Translation Model with In-tegrated Reordering.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages1045?1054, Portland, Oregon, USA, June.Nadir Durrani, Alexander Fraser, and Helmut Schmid.2013.
Model With Minimal Translation Units, ButDecode With Phrases.
In The 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, Atlanta, Georgia, USA, June.
Associationfor Computational Linguistics.Minwei Feng, Arne Mauser, and Hermann Ney.
2010.A Source-side Decoding Sequence Model for Statis-tical Machine Translation.
In Conference of the As-sociation for Machine Translation in the Americas2010, Denver, Colorado, USA, October.Michel Galley and Christopher D. Manning.
2010.Accurate Non-Hierarchical Phrase-Based Transla-tion.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 966?974, Los Angeles, California, June.
As-sociation for Computational Linguistics.Adria` Gispert and Jose?
B. Marin?o.
2006.
Linguis-tic Tuple Segmentation in N-Gram-Based StatisticalMachine Translation.
In INTERSPEECH.Eva Hasler, Barry Haddow, and Philipp Koehn.
2012.Sparse Lexicalised Features and Topic Adaptationfor SMT.
In Proceedings of the seventh Interna-tional Workshop on Spoken Language Translation(IWSLT), pages 268?275.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 187?197, Edinburgh, Scotland, United King-dom, 7.Liang Huang and David Chiang.
2007.
Forest Rescor-ing: Faster Decoding with Integrated LanguageModels.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 144?151, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Maxim Khalilov and Jose?
A. R. Fonollosa.
2009.
N-Gram-Based Statistical Machine Translation VersusSyntax Augmented Machine Translation: Compar-ison and System Combination.
In Proceedings ofthe 12th Conference of the European Chapter of theACL (EACL 2009), pages 424?432, Athens, Greece,March.
Association for Computational Linguistics.Philipp Koehn and Barry Haddow.
2012.
Towards Ef-fective Use of Training Data in Statistical MachineTranslation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, pages 317?321, Montre?al, Canada, June.
Association for Com-putational Linguistics.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proceed-ings of HLT-NAACL, pages 127?133, Edmonton,Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In ACL 2007 Demonstrations, Prague, Czech Re-public.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.Shankar Kumar and William J. Byrne.
2004.
Mini-mum Bayes-Risk Decoding for Statistical MachineTranslation.
In HLT-NAACL, pages 169?176.404Jose?
B. Marin?o, Rafael E. Banchs, Josep M. Crego,Adria` de Gispert, Patrik Lambert, Jose?
A. R. Fonol-losa, and Marta R. Costa-jussa`.
2006.
N-gram-Based Machine Translation.
Computational Lin-guistics, 32(4):527?549.Jan Niehues, Teresa Herrmann, Stephan Vogel, andAlex Waibel.
2011.
Wider Context by Using Bilin-gual Language Models in Machine Translation.
InProceedings of the Sixth Workshop on StatisticalMachine Translation, pages 198?206, Edinburgh,Scotland, July.
Association for Computational Lin-guistics.Franz J. Och and Hermann Ney.
2004.
The AlignmentTemplate Approach to Statistical Machine Transla-tion.
Computational Linguistics, 30(1):417?449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Morristown, NJ, USA.Christopher Quirk and Arul Menezes.
2006.
Do WeNeed Phrases?
Challenging the Conventional Wis-dom in Statistical Machine Translation.
In HLT-NAACL.Holger Schwenk and Philipp Koehn.
2008.
Large andDiverse Language Models for Statistical MachineTranslation.
In International Joint Conference onNatural Language Processing, pages 661?666, Jan-uary 2008.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Intl.
Conf.
Spoken Lan-guage Processing, Denver, Colorado.Christoph Tillman.
2004.
A Unigram Orienta-tion Model for Statistical Machine Translation.
InHLT-NAACL 2004: Short Papers, pages 101?104,Boston, Massachusetts.Ashish Vaswani, Haitao Mi, Liang Huang, and DavidChiang.
2011.
Rule Markov Models for Fast Tree-to-String Translation.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 856?864, Portland, Oregon, USA, June.Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-feng Gao.
2013.
Beyond Left-to-Right: Multi-ple Decomposition Structures for SMT.
In The2013 Conference of the North American Chapterof the Association for Computational Linguistics:Human Language Technologies, Atlanta, Georgia,USA, June.
Association for Computational Linguis-tics.405
