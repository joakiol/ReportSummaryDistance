Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 230?234,Dublin, Ireland, August 23-24, 2014.DIT: Summarisation and Semantic Expansion in Evaluating SemanticSimilarityMagdalena KacmajorIBM Technology CampusDublin Software LabIrelandmagdalena.kacmajor@ie.ibm.comJohn D. KelleherApplied Intelligence Research CentreDublin Institute of TechnologyIrelandjohn.d.kelleher@dit.ieAbstractThis paper describes an approach to im-plementing a tool for evaluating seman-tic similarity.
We investigated the poten-tial benefits of (1) using text summarisa-tion to narrow down the comparison to themost important concepts in both texts, and(2) leveraging WordNet information to in-crease usefulness of cosine comparisonsof short texts.
In our experiments, textsummarisation using a graph-based algo-rithm did not prove to be helpful.
Se-mantic and lexical expansion based uponword relationships defined in WordNet in-creased the agreement of cosine similarityvalues with human similarity judgements.1 IntroductionThis paper describes a system that addresses theproblem of assessing semantic similarity betweentwo different-sized texts.
The system has been ap-plied to SemEval-2014 Task 3, Cross-Level Se-mantic Similarity (Jurgens et al, 2014).
The appli-cation is limited to a single comparison type, thatis, paragraph to sentence.The general approach taken can be charac-terised as text summarisation followed by a pro-cess of semantic expansion and finally similaritycomputation using cosine similarity.The rationale for applying summarisation is tofocus the comparison on the most important ele-ments of the text by selecting key words to be usedin the similarity comparison.
This summarisationapproach is based on the assumption that if sum-mary of a paragraph is similar to the summary sen-tence paired with the paragraph in the task dataset,then the original paragraph and sentence pair mustThis work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/have been similar and so should receive a highsimilarity rating.The subsequent semantic expansion is intendedto counteract the problem arising from the smallsize of both compared text units.
The similaritymetric used by the system is essentially a func-tion of word overlap.
However, because both theparagraphs and sentences being compared are rel-atively short, the probability of a word overlap -even between semantically similar texts - is quitesmall.
Therefore prior to estimating the similaritybetween the texts we extend the word vectors cre-ated by the summarisation process with the syn-onyms and other words semantically and lexicallyrelated to the words occurring in the text.By using cosine similarity measure, we nor-malize the lengths of word vectors representingdifferent-sized documents (paragraphs and sen-tences).The rest of the paper is organized as follows:section 2 describes the components of the sys-tem in more detail; section 3 describes parame-ters used in the experiments we conducted andpresents our results; and section 4 provides con-cluding remarks.2 System Description2.1 OverviewThere are four main stages in the system process-ing pipeline: (1) text pre-processing; (2) summari-sation; (3) semantic expansion; (4) computing thesimilarity scores.
In the following sections we de-scribe each of these stages in turn.2.2 Pre-processingParagraphs and sentences are tokenized and anno-tated using Stanford CoreNLP1.
The annotationsinclude part-of-speech (POS), lemmatisation, de-pendency parse and coreference resolution.
Then1http://nlp.stanford.edu/software/corenlp.shtml230the following processes are applied: (a) Token se-lection, the system ignores tokens with POS otherthan nouns, adjectives and verbs (in our exper-iments we tested various combinations of thesethree categories); (b) Token merging, the criteriafor merging can be more restrictive (same wordform) or less restrictive ?
based on same lemma,or even same lemma ignoring POS; (c) Stopwordremoval, we apply a customized stopword list toexclude verbs that have very general meaning.In this way, each text unit is processed to pro-duce a filtered set of tokens which at the next stepcan be directly transformed into nodes in the graphrepresentation of the text.
Dependency and coref-erence annotations will be used for defining edgesin the graph.2.3 Summarisation systemSummarisation has been implemented using Tex-tRank (Mihalcea and Tarau, 2004), an itera-tive graph-based ranking algorithm derived fromPageRank (Brin and Page, 1998).The ranking is based on the following principle:when node i links to node j, a vote is cast that in-creases the rank of node j.
The strength of the votedepends on the importance (rank) of the castingnode, thus the algorithm is run iteratively until theranks stop changing beyond a given threshold, oruntil a specified limit of iterations is reached.To apply this algorithm to paragraphs and sen-tences, our system builds a graph representationfor each of these text units, with nodes represent-ing the tokens selected and merged at the pre-ceding stage.
The nodes are connected by co-occurrence (Mihalcea and Tarau, 2004), depen-dency and/or coreference relations.
Next, a un-weighted or weighted version of the ranking algo-rithm is iterated until convergence.For each test unit, the output of the summariseris a list of words sorted by rank.
Depending onthe experimental setup, the summariser forwardson all processed words, or only a subset of top-ranked words.2.4 Lexico-semantic expansionFor each word returned from the summariser, weretrieve all (or a predetermined number of) synsetsthat have this word as a member.
For each re-trieved synset, we also identify synsets relatedthrough semantic and lexical relations.
Finally, us-ing all these synsets we create the synonym groupfor a word that includes all the members of thesesynsets.If a word has many different senses, then thesynonym group grows large, and the chances thatthe sense of a given member of this large groupwill match the sense of the original word areshrinking.
To account for this fact, each memberof the synonym group is assigned a weight usingEquation 1.
This weight is simply 1 divided bythe count of the number of words in the synonymgroup.synweight =1#SynonymGroup(1)At the end of this process for each documentwe have the set of words that occurred in the doc-ument, and each of these words has a synonymgroup associated with it.
All of the members ofthe synonym groups have a weight value.2.5 Similarity comparisonCosine similarity is used to compute the similar-ity for each paragraph-sentence pair.
For this cal-culation each text (paragraph or sentence) is rep-resented by a bag-of-words vector containing allthe words derived from the text together with theirsynonym groups.The bag-of-words can be binary or frequencybased, with the counts optionally modified by theword ranks.
The counts for words retrieved fromWordNet are weighted with synweights, whichmeans that they are usually represented by verysmall numbers.
However, if a match is found be-tween a WordNet word and a word observed inthe document, the weight of both is adjusted ac-cording to semantic match rules.
These rules havebeen established empirically, and are presented insection 3.3.1.The cosine values for each paragraph-sentencepair are not subject to any further processing.3 Experiments3.1 DatasetAll experiments were carried out on trainingdataset provided by SemEval-2014 Task 3 forparagraph to sentence comparisons.3.2 ParametersFor each stage in the pipeline, there is a set of pa-rameters whose values influence the final results.Each set of parameters will be discussed next.2313.2.1 Pre-processing parametersThe parameters used for pre-processing determinethe type and number of nodes included in thegraph:?
POS: Parts-of-speech that are allowed intothe graph, e.g.
only nouns and verbs, ornouns, verbs and adjectives.?
Merging criteria: The principle by whichwe decide whether two tokens should be rep-resented by the same node in the graph.?
Excluded verbs: The contents of the stop-word list.3.2.2 Summarisation parametersThese parameters control the structure of the graphand the results yielded by TextRank algorithm.The types of nodes in the graph are already de-cided at the pre-processing stage.?
Relation type: In order to link the nodes(words) in the graph representation of a docu-ment, we use co-occurrence relations (Mihal-cea and Tarau, 2004), dependency relationsand coreference relations.
The two latter aredefined based on the Stanford CoreNLP an-notations, whereas a co-occurrence edge iscreated when two words appear in the textwithin a word span of a specified length.
Theco-occurrence relation comes with two addi-tional parameters:?
Window size: Maximum number ofwords constituting the span.?
Window application: The window canbe applied before or after filtering awaytokens of unwanted POS, i.e.
we can re-quire either the co-occurrence within theoriginal text or in the filtered text.?
Graph type: A document can be representedas an unweighted or weighted graph.
Inthe second case we use a weighted versionof TextRank algorithm (Mihalcea and Tarau,2004) in which the strength of a vote dependsboth on the rank of the casting node and onthe weight of the link producing the vote.?
Edge weights: In general, the weightof an edge between any two nodes de-pends on the number of identified rela-tions, but we also experimented with as-signing different weights depending onthe relation type.?
Normalisation: This parameter refers to nor-malising word ranks computed for the longerand the shorter text unit.?
Word limit: The maximum number of top-ranked words included in vector representa-tion of the longer text.
May be equal to thenumber of words in the shorter of the twocompared texts, or fixed at some arbitraryvalue.3.2.3 Semantic extension parametersThe following factors regulate the impact of addi-tional words retrieved from WordNet:?
Synset limit: The maximum number ofsynsets (word senses) retrieved from Word-Net per each word.
Can be controlled byword ranks returned from the summariser.?
Synonym limit: The maximum number ofsynonyms (per synset) added to vector repre-sentation of the document.
Can be controlledby word ranks.?
WordNet relations: The types of semanticand lexical relations used to acquire addi-tional synsets.3.2.4 Similarity comparison parameters?
Bag-of-words model: The type of bag-of-word used for cosine comparisons.?
Semantic match weights: The rules for ad-justing weights of WordNet words that matchobserved words from the other vector.3.3 ResultsThe above parameters in various combinationswere applied in an extensive series of experiments.Contrary to our expectations, the results indicatethat the summariser has either no impact or has anegative effect.
Table 1 presents the set of param-eters that seem to have impact, and the values thatresulted in best scores, as calculated by SemEvalTask 3 evaluation tool against the training dataset.3.3.1 DiscussionIn the course of experiments we consistently ob-served higher performance when all words fromboth compared documents were included, as op-posed to selecting top-ranked words from thelonger document.
Furthermore, less restrictive cri-teria for merging tended to give better results.232Parameter ValueWord limit no limitPOS JJ, NN, VMerging criteria lemma, ignore POSCustom stopword list yesSynset limit 15Synonym limit no limitWordNet relations similar to, pertainym,hypernymBag-of-words model binaryTable 1: Parameter values yielding the best scores.We noticed clear improvement after extend-ing word vectors with synonyms and relatedwords.
WordNet relations that contributed mostare similar to, hypernym (ISA relation), pertainym(relational adjective) and derivationally relatedform.
The results obtained before and after apply-ing summarisation and lexico-semantic expansion(while keeping other parameters fixed at values re-ported in Table 1) are shown in Table 2.``````````````Word ranksExpansionNo YesIgnored 0.728 0.755Used to select top-rank words 0.690 0.716Used to control synset limit N/A 0.752Used to weight vector counts 0.694 N/ATable 2: The effects of applying text summarisa-tion and lexico-semantic expansion.Table 3 summarises the most efficient rules foradjusting weights in word vectors when a matchhas been found between an observed word fromone vector and a WordNet word in the other vec-tor.
The rules are as follows: (1) If the match is be-tween an observed word from the paragraph vectorand a WordNet word from the sentence vector, theweight of both is set to 0.25; (2) If the match is be-tween an observed word from the sentence vectorand the WordNet word from the paragraph vector,the weight of both is set to 0.75; (3) If the match isbetween two WordNet words, one from the para-graph and one from the sentence, the weight ofboth is set to whichever synweight is higher; (4)If the match is between two observed words, theweight of both is set to 1.We received slightly better results after setting alimit on the number of included word senses, andPPPPPPPPParagr.Sent.Obs.
word WordNet wordObserved word 1.0 0.25WordNet word 0.75 max(synweight)Table 3: Optimal weights for semantic match.after ignoring a few verbs with particularly broadmeaning.3.3.2 Break-down into categoriesPearson correlation between gold standard and thesubmitted results was 0.785.
Table 4 shows thecorrelations within each category, both for the testset and the train set.
The results are very con-sistent across datasets, except for Reviews whichscored much lower with the test data.
The over-all result was lower with the training data becauseof higher number of examples in Metaphoric cat-egory, where the performance of our system wasextremely poor.Category Test data Train datanewswire 0.907 0.926cqa 0.778 0.779metaphoric 0.099 -0.16scientific 0.856 -travel 0.880 0.887review 0.752 0.884overall 0.785 0.755Table 4: Break-down of the results.4 ConclusionsWe described our approach, parameters used in thesystem, and the results of experiments.
Text sum-marisation didn?t prove to be helpful.
One possi-ble explanation of the neutral or negative effect ofsummarisation is the small size of the texts units:with the limited number of words available forcomparison, any procedure reducing this alreadyscarce set may be disadvantageous.The results benefited from adding synonymsand semantically and lexically related words.Lemmatisation and merging same-lemma wordsregardless the POS, as well as ignoring very gen-eral verbs seem to be helpful.The best performance has been observed inNewswire category.
Finally, given that the simi-larity metric used by the system is essentially a233function of word overlap between the two texts,it is not surprising that the system struggled withmetaphorically related texts.ReferencesSergey Brin and Lawrence Page.
1998.
The Anatomyof Large-Scale Hypertextual Web Search Engine.In Computer Networks and ISDN Systems, 30(1-7):107?117.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Cambridge, MA: MIT Press.David Jurgens, Mohammad Taher Pilehvar, andRoberto Navigli.
2014.
SemEval-2014 Task 3:Cross-Level Semantic Similarity.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation (SemEval-2014).
August 23-24, 2014, Dublin,Ireland.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing Order into Texts.
In Proceedings ofEMNLP 2004:404?411, Barcelona, Spain.George A. Miller.
1995.
WordNet: A LexicalDatabase for English.
Communications of the ACM,Vol.
38, No.
11:39?41.234
