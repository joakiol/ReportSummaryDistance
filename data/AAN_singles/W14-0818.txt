Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 104?108,Gothenburg, Sweden, 26-27 April 2014.c?2014 Association for Computational LinguisticsFeature Norms of German Noun CompoundsStephen RollerDepartment of Computer ScienceThe University of Texas at Austinroller@cs.utexas.eduSabine Schulte im WaldeInstitut f?ur Maschinelle SprachverarbeitungUniversit?at Stuttgart, Germanyschulte@ims.uni-stuttgart.deAbstractThis paper presents a new data collectionof feature norms for 572 German noun-noun compounds.
The feature norms com-plement existing data sets for the sametargets, including compositionality rat-ings, association norms, and images.
Wedemonstrate that the feature norms are po-tentially useful for research on the noun-noun compounds and their semantic trans-parency: The feature overlap of the com-pounds and their constituents correlateswith human ratings on the compound?constituent degrees of compositionality,?
= 0.46.1 IntroductionFeature norms are short descriptions of typical at-tributes for a set of objects.
They often describethe visual appearance (a firetruck is red), functionor purpose (a cup holds liquid), location (mush-rooms grow in forests), and relationships betweenobjects (a cheetah is a cat).
The underlying fea-tures are usually elicited by asking a subject tocarefully describe a cue object, and recording theirresponses.Feature norms have been widely used in psy-cholinguistic research on conceptual representa-tions in semantic memory.
Prominent collectionshave been pursued by McRae et al.
(2005) for liv-ing vs. non-living basic-level concepts; by Vin-son and Vigliocco (2008) for objects and events;and by Wu and Barsalou (2009) for noun and nounphrase objects.
In recent years, feature norms havealso acted as a loose proxy for perceptual infor-mation in data-intensive computational models ofsemantic tasks, in order to bridge the gap betweenlanguage and the real world (Andrews et al., 2009;Silberer and Lapata, 2012; Roller and Schulte imWalde, 2013).In this paper, we present a new resource of fea-ture norms for a set of 572 concrete, depictableGerman nouns.
More specifically, these nouns in-clude 244 noun-noun compounds and their corre-sponding constituents.
For example, we includefeatures for ?Schneeball?
(?snowball?
), ?Schnee?(?snow?
), and ?Ball?
(?ball?).
Table 1 presentsthe most prominent features of this example com-pound and its constituents.
Our collection com-plements existing data sets for the same targets,including compositionality ratings (von der Heideand Borgwaldt, 2009); associations (Schulte imWalde et al., 2012; Schulte im Walde and Borg-waldt, 2014); and images (Roller and Schulte imWalde, 2013).The remainder of this paper details the col-lection process of the feature norms, discussestwo forms of cleansing and normalization we em-ployed, and performs quantitative and qualitativeanalyses.
We find that the normalization proce-dures improve quality in terms of feature tokensper feature type, that the normalized feature normshave a desirable distribution of features per cue,and that the feature norms are useful in semanticmodels to predict compositionality.2 Feature Norm CollectionWe employ Amazon Mechanical Turk (AMT)1fordata collection.
AMT is an online crowdsourc-ing platform where requesters post small, atomictasks which require manual completion by hu-mans.
Workers can complete these tasks, calledHITs, in order to earn a small bounty.2.1 Setup and DataWorkers were presented with a simple page askingthem to describe the typical attributes of a givennoun.
They were explicitly informed in Englishthat only native German speakers should complete1http://www.mturk.com104Schneeball ?snowball?
Schnee ?snow?
Ball ?ball?ist kalt ?is cold?
8 ist kalt ?is cold?
13 ist rund ?is round?
14ist rund ?is round?
7 ist wei?
?is white?
13 zum Spielen ?for playing?
4aus Schnee ?made from snow?
7 im Winter ?in the winter?
6 rollt ?rolls?
2ist wei?
?is white?
7 f?allt ?falls?
3 wird geworfen ?is thrown?
2formt man ?is formed?
2 schmilzt ?melts?
2 ist bunt ?is colorful?
2wirft man ?is thrown?
2 hat Flocken ?has flakes?
2 Fu?ball ?football?
2mit den H?anden ?with hands?
2 ist w?assrig ?is watery?
1 Basketball ?basketball?
2Table 1: Most frequent features for example compound Schneeball and its constituents.the tasks.
All other instructions were given in Ger-man.
Workers were given 7 example features forthe nouns ?Tisch?
(?table?)
and ?Katze?
(?cat?
), andinstructed to provide typical attributes per noun.Initially, workers were required to provide 6-10features per cue and were only paid $0.02 per hit,but very few workers completed the hits.
Afterlowering the requirements and increasing the re-ward, we received many more workers and col-lected the data more quickly.
Workers could alsomark a word as unfamiliar or provide additionalcommentary if desired.We collected responses from September 21,2012 until January 31, 2013.
Workers who wereobvious spammers were rejected and not rewardedpayment.
Typically spammers pasted text fromGoogle, Wikipedia, or the task instructions andwere easy to spot.
Users who failed to follow in-structions (responded in English, did not providethe minimum number of features, or gave nonsen-sical responses) were also rejected without pay-ment.
Users who put in a good faith effort andconsistently gave reasonable responses had all oftheir responses accepted and rewarded.In total, 98 different workers completed at leastone accepted hit, but the top 25 workers accountedfor nearly 90% of the responses.
We accepted28,404 different response tokens over 18,996 re-sponse types for 572 different cues, or roughly 50features per cue.3 Cleansing and NormalizationWe provide two cleaned and normalized versionsof our feature norms.2In the first version, we cor-rect primarily orthographic mistakes such as in-consistent capitalization, spelling errors, and sur-face usage, but feature norms remain otherwiseunchanged.
This version will likely be more usefulto researchers interested in more subtle variations2The norms can be downloaded fromwww.ims.uni-stuttgart.de/forschung/ressourcen/experiment-daten/feature-norms.en.html.and distinctions made by the workers.The second version of our feature norms aremore aggressively normalized, to reduce the quan-tity of unique and low frequency responses whilemaintaining the spirit of the original response.
Theresulting data is considerably less sparse than theorthographically normalized version.
This versionis likely to be more useful for research that ishighly affected by sparse data, such as multimodalexperiments (Andrews et al., 2009; Silberer andLapata, 2012; Roller and Schulte im Walde, 2013).3.1 Orthographic NormalizationOrthographic normalization is performed in fourautomatic passes and one manual pass in the fol-lowing order:Letter Case Normalization: Many workersinconsistently capitalize the first word of featurenorms as though they are writing a complete sen-tence.
For example, ?ist rund?
and ?Ist rund?
(?isround?)
were both provided for the cue ?Ball?.We cannot normalize capitalization by simply us-ing lowercase everywhere, as the first letter ofGerman nouns should always be capitalized.
Tohandle the most common instances, we lowercasethe first letter of features that began with articles,modal verbs, prepositions, conjunctions, or thehigh-frequency verbs ?kommt?, ?wird?, and ?ist?.Umlaut Normalization: The same Germanword may sometimes be spelled differently be-cause some workers use German keyboards(which have the letters ?a, ?o, ?u, and ?
), and oth-ers use English keyboards (which do not).
Weautomatically normalize to the umlaut form (i.e.?gruen?
to ?gr?un?, ?weiss?
to ?wei??)
whenever twoworkers gave both versions for the same cue.Spelling Correction: We automatically correctcommon misspellings (such as errecihen?
erre-ichen), using a list from previous collection exper-iments (Schulte im Walde et al., 2008; Schulte imWalde et al., 2012).
The list was created semi-automatically, and manually corrected.105Usage of ?ist?
and ?hat?
: Workers sometimesdrop the verbs ?ist?
(?is?)
and ?hat?
(?has?
), e.g.
theworker writes only ?rund?
(?round?)
instead of ?istrund?, or ?Obst?
(?fruit?)
instead of ?hat Obst?.
Wenormalize to the ?ist?
and ?hat?
forms when twoworkers gave both versions for the same cue.
Notethat we cannot automatically do this across sepa-rate cues, as the relationship may change: a treehas fruit, but a banana is fruit.Manual correction: Following the above auto-matic normalizations, we manually review all non-unique responses.
In this pass, responses are nor-malized and corrected with respect to punctuation,capitalization, spelling, and orthography.
Roughly170 response types are modified in this phase.3.2 Variant NormalizationThe second manual pass consists of more aggres-sive normalization of expression variants.
In thispass, features are manually edited to minimize thenumber of feature types while preserving as muchsemantic meaning as possible:?
Replacing plurals with singulars;?
Removing modal verbs, e.g.
?kann Kunstsein?
(?can be art?)
to ?ist Kunst?;?
Removing quantifiers and hedges, e.g.
?istmeistens blau?
(?is mostly blue?)
to ?ist blau?;?
Splitting into atomic norms, e.g.
?ist wei?oder schwarz?
(?is white or black?)
to ?istwei??
and ?ist schwarz?, or ?jagt im Wald?
(?hunts in forest?)
to ?jagt?
and ?im Wald?;?
Simplifying verbiage, e.g.
?ist in der Farbeschwarz?
(?is in the color black?)
to ?istschwarz?.These selected normalizations are by no meanscomprehensive or exhaustive, but do handle alarge portion of the cases.
In total, we modifyroughly 5400 tokens over 1300 types.4 Quantitative AnalysisIn the following two analyses, we explore the typeand token counts of our feature norms across thesteps in the cleansing process, and analyze the un-derlying distributions of the features per cues.Type and Token counts Table 2 shows the to-ken and type counts for all features in each stepof the cleansing process.
We also present thecounts for non-idiosyncratic features, or featureswhich are provided for at least two distinct cues.The orthographic normalizations generally lowerthe number of total and non-idiosyncratic types,and increase the number of non-idiosyncratic to-kens.
This indicates we are successfully identify-ing and correcting many simple orthographic er-rors, resulting in a less sparse matrix.
The nec-essary amount of manual correction is relativelylow, indicating we are able to catch the majorityof mistakes using simple, automatic methods.Data Version Total Non-idiosyncraticof Responses Types Tokens Types TokensRaw 18,996 28,404 2,029 10,675Case 18,848 28,404 2,018 10,801Umlaut 18,700 28,404 1,967 10,817Spelling 18,469 28,404 1,981 11,072ist/hat 18,317 28,404 1,924 11,075Manual 18,261 28,404 1,889 11,106Aggressive 17,503 28,739 1,374 11,848Table 2: Counts in the cleansing process.The more aggressively normalized norms areconsiderably different than the orthographicallynormalized norms.
Notably, the number of totaltokens increases from the atomic splits.
The datais also less sparse and more robust, as indicated bythe drops in both total and non-idiosyncratic types.Furthermore, the number of non-idiosyncratic to-kens also increases considerably, indicating wewere able to find numerous edge cases and placethem in existing, frequently-used bins.Number of Features per Cue Another impor-tant aspect of the data set is the number of featuresper cue.
An ideal feature norm data set would con-tain a roughly equal number of (non-idiosyncratic)features for every cue; if most of the features areunderrepresented, with a majority of the featureslying in only a few cues, then our data set mayonly properly represent for these few, heavily rep-resented cues.Figure 1 shows the number of features per cuefor (a) all features and (b) the non-idiosyncraticfeatures, for the aggressively normalized data set.In the first histogram, we see a clear bimodal dis-tribution around the number of features per cue.This is an artifact of the two parts of our collec-tion process: the shorter, wider distribution corre-sponds to the first part of collection, where work-ers gave more responses for less reward.
Thetaller, skinnier distribution corresponds to the sec-ond half of collection, when workers were re-warded more for less work.
The second collec-tion procedure was clearly effective in raising thenumber of hits completed, but resulted in fewerfeatures per cue.1060204060800 25 50 75 100 125Features / CueCount(a) All Norms02550750 20 40 60Features / CueCount(b) Non?idiosyncratic NormsFigure 1: Distribution of features per cue.In the second histogram, we see only the non-idiosyncratic features for each cue.
Unlike the firsthistogram, we see only one mode with a relativelylong tail.
This indicates that mandating more fea-tures per worker (as in the first collection process)often results in more idiosyncratic features, andnot necessarily a stronger representation of eachcue.
We also see that roughly 85% of the cues haveat least 9 non-idiosyncratic features each.
In sum-mary, our representations are nicely distributed forthe majority of cues.5 Qualitative AnalysisOur main motivation to collect the feature normsfor the German noun compounds and their con-stituents was that the features provide insight intothe semantic properties of the compounds andtheir constituents and should therefore represent avaluable resource for cognitive and computationallinguistics research on compositionality.
The fol-lowing two case studies demonstrate that the fea-ture norms indeed have that potential.Predicting the Compositionality The first casestudy relies on a simple feature overlap measureto predict the degree of compositionality of thecompound?constituent pairs of nouns: We use theproportion of shared features of the compound anda constituent with respect to the total number offeatures of the compound.
The degree of compo-sitionality of a compound noun is calculated withrespect to each constituent of the compound.For example, if a compound nounN0received atotal of 30 features (tokens), out of which it shares20 with the first constituent N1and 10 with thesecond constituent N2, the predicted degrees ofcompositionality are2030= 0.67 for N0?N1, and1030= 0.33 for N0?N2.
The predicted degrees ofcompositionality are compared against the meancompositionality judgments as collected by vonder Heide and Borgwaldt (2009), using the Spear-man rank-order correlation coefficient.
The result-ing correlations are ?
= 0.45, p < .000001 for thestandard normalized norms, and ?
= 0.46, p <.000001 for the aggressively normalized norms,which we consider a surprisingly successful resultconcerning our simple measure.
Focusing on thecompound?head pairs, the feature norms reached?
= 0.57 and ?
= 0.59, respectively.Perceptual Model Information As mentionedin the Introduction, feature norms have also actedas a loose proxy for perceptual information indata-intensive computational models of semantictasks.
The second case study is taken from Rollerand Schulte im Walde (2013), who integrated fea-ture norms as one type of perceptual informa-tion into an extension and variations of the LDAmodel by Andrews et al.
(2009).
A bimodal LDAmodel integrating textual co-occurrence featuresand our feature norms significantly outperformedthe LDA model that only relied on the textual co-occurrence.
The evaluation of the LDA modelswas performed on the same compositionality rat-ings as described in the previous paragraph.6 ConclusionThis paper presented a new collection of featurenorms for 572 German noun-noun compounds.The feature norms complement existing data setsfor the same targets, including compositionalityratings, association norms, and images.We have described our collection process, andthe cleaning and normalization, and we haveshown both the orthographically normalized andmore aggressively normalized feature norms to beof higher quality than the raw responses in termsof types per token, and that the normalized featurenorms have a desirable distribution of features percue.
We also demonstrated by two case studiesthat the norms represent a valuable resource forresearch on compositionality.107ReferencesMark Andrews, Gabriella Vigliocco, and David Vin-son.
2009.
Integrating experiential and distribu-tional data to learn semantic representations.
Psy-chological Review, 116(3):463.Ken McRae, George S. Cree, Mark S. Seidenberg, andChris McNorgan.
2005.
Semantic feature pro-duction norms for a large set of living and nonliv-ing things.
Behavior Research Methods, 37(4):547?559.Stephen Roller and Stephen Schulte im Walde.
2013.A multimodal LDA model integrating textual, cog-nitive and visual modalities.
In Proceedings ofthe 2013 Joint Conference on Empirical Meth-ods in Natural Language Processing and Compu-tational Natural Language Learning, pages 1146?1157, Seattle, Washington, USA.Sabine Schulte im Walde and Susanne Borgwaldt.2014.
Association norms for German noun com-pounds and their constituents.
Under review.Sabine Schulte im Walde, Alissa Melinger, MichaelRoth, and Andrea Weber.
2008.
An empirical char-acterisation of response types in German associationnorms.
Research on Language and Computation,6(2):205?238.Sabine Schulte im Walde, Susanne Borgwaldt, andRonny Jauch.
2012.
Association norms of Germannoun compounds.
In Proceedings of the 8th Interna-tional Conference on Language Resources and Eval-uation, pages 632?639, Istanbul, Turkey.Carina Silberer and Mirella Lapata.
2012.
Groundedmodels of semantic representation.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1423?1433, JejuIsland, Korea.David Vinson and Gabriella Vigliocco.
2008.
Se-mantic feature production norms for a large set ofobjects and events.
Behavior Research Methods,40(1):183?190.Claudia von der Heide and Susanne Borgwaldt.
2009.Assoziationen zu Unter-, Basis- und Oberbegrif-fen.
Eine explorative Studie.
In Proceedings ofthe 9th Norddeutsches Linguistisches Kolloquium,pages 51?74.Ling-ling Wu and Lawrence W. Barsalou.
2009.
Per-ceptual simulation in conceptual combination: Evi-dence from property generation.
Acta Psychologica,132:173?189.108
