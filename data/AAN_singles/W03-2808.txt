No-bureaucracy evaluationAdam KilgarriITRI, University of Brightonadam@itri.brighton.ac.ukSenseval is a series of evaluation exer-cises for Word Sense Disambiguation.
Thecore design is in accordance with the MUCand TREC model of quantitative, developer-oriented (rather than user-oriented) evalua-tion.
The rst was in 1998, with tasks forthree languages and 25 participating researchteams, the second in 2001, with tasks fortwelve languages, thirty-ve participating re-search teams and over 90 participating sys-tems.
The third is currently in planning.
Thescale of the resources developed is indicated inTable 1 (reproduced from (Edmonds and Kil-garri, 2002)).1In this paper we address ve of the workshopthemes from a Senseval perspective:1. organisational structure2.
re-use of corpus resources: pro and con3.
the web and evaluation4.
Senseval and Machine Translation eval-uation5.
re-use of metrics: a cautionary tale.1 OrganisationOne aspect of Senseval of interest here is itsorganizational structure.
It has no centralisedsponsor to fund or supply infrastructure.
Al-most all work was done by volunteer eortwith just modest local grant funding for par-ticular subtasks, with organisers answerableto no-one beyond the community of WSD re-searchers.
This was possible because of the1Senseval data sets and results are available athttp://www.senseval.orglevel of commitment.
People wanted the eval-uation framework, so they were willing to ndthe time, from whatever slack they were ableto concoct.At the Senseval-1 workshop, the possibil-ity of nding an o?cial sponsor {most likelythe EU or a branch of the US administration{was discussed at length and vigorously.
Theprevailing view was that, while it was niceto have more money around, it was not nec-essary and came at a cost.
Various experi-ences were cited where researchers felt theirenergies had been diverted from the researchitself to the processes of grant applications,cost statements, and the strange business ofwriting reports which in all likelihood no-onewill ever read.
My experience, as co-ordinatorof Senseval-1 and chair of Senseval-2, wasthat, without external funding but with greatgoodwill and energy for the task at various lo-cations round the globe, it was possible to geta vast amount done in a short time, at somecost to family life but with a minimum of mis-directed eort.At several points, potential funders havesaid \All you need to do is ll in our form..." Itis always worth asking whether this is a poi-soned chalice.
How much eort will it taketo ll in, and how much more to follow itthrough?
What is the cost to my engagementand enthusiasm of doing things their way (asI shall have to, if I take the king's shilling, asgood governance demands that procedures arefollowed, forms are lled, any changes to theoriginal plan are justied and documented ...).I should note that, possibly, my perspectivehere is atypical.
As the co-ordinator, withoutTable 1: Senseval-2, resources, participation, results.Language TaskaSystems Lemmas InstancesbIAAcBaselinedBest scoreCzech AW 1 {e277,986 { { 94Basque LS 3 40 5,284 75 65 76Dutch AW 1 1,168 16,686 { 75 84English AW 21 1,082 2,473 75 57 69English LS 26 73 12,939 86 48/16f64/40Estonian AW 2 4,608 11,504 72 85 67Italian LS 2 83 3,900 21 { 39Japanese LS 7 100 10,000 86 72 78Japanese TM 9 40 1,200 81 37 79Korean LS 2 11 1,733 { 71 74Spanish LS 12 39 6,705 64 48 65Swedish LS 8 40 10,241 95 { 70aAW: all-words task, LS: lexical sample, TM: translation memory.bTotal instances annotated in both training and test corpora.
In the default case, they were split 2:1between training and test sets.cInter-annotator agreement is generally the average percentage of cases where two (or more) anno-tators agree, before adjudication.
However there are various ways in which it can be calculated, so thegures in the table are not all directly comparable.dGenerally, choosing the corpus-attested most frequent sense, although this was not always possibleor straightforward.eA dash `{' indicates the data was unavailable.fSupervised and unsupervised scores are separated by a slash.a funder as taskmaster, I had a particularlyfree hand to ordain as I saw t. This wasmost agreeable, but it is quite possible thatothers involved saw me as their (more or lessreasonable, more or less benevolent) dictatorand bureaucracy, and did not share the plea-sures of autonomy that I experienced.I am not sure that I advocate the no-bureaucracy approach: clearly, it depends onthere being some slack somewhere which canbe redirected.
It is however a model well worthconsidering, if only because it is such fun work-ing with other committed volunteers for nobetter reason than that you all want to reachthe same goal.2 The re-use trapConsider the following position (Redux, 2001):As followers of the literature willhave noted, great strides have beenmade in statistical parsing.
In twodecades, system performance gureshave soared to over 90%.
Thisis a magnicent tale.
Parsing iscracked.
An enormous debt is owedto the producers of the Penn Tree-bank.
As anticipated by Don Walker,marked-up resources were what weneeded.
Once we had them, the al-gorithm boys could set to work, andwhoomph!The benets of concentrating on theone corpus have been enormous.
Theeld has focused.
It has been the mi-croscope under which the true natureof language has become apparent.Like Mendel unpacking the secretsof all species' genetics through as-siduous attention to sweet peas, andsweet peas alone, Charniak, Collins,and others have unpacked the secretsof grammatical structure throughrigorous attention to the Wall StreetJournal.We would now like to point out theunhelpfulness of comments appear-ing on the CORPORA mailing list,reporting low performance of variousstatistical POS-taggers when appliedto text of dierent types to the train-ing material, and also of a footnoteto a recent ACL paper, accordingto which a leading Penn-Treebank-trained parser was applied to literarytexts but then its performance "sig-nicantly degraded".
These resultshave not, I am glad to say, enteredbeyond that footnote into the scien-tic literature.
The authors shouldrealise that it is prima facie invalidto apply a resource trained on onetype of data, to another.
Anyonewishing to use a statistical parser ona text type for which a manually-parsed training corpus does not ex-ist, must rst create the training cor-pus.
If they are not willing to dothat, they may as well accept thatten years of dazzling progress is ofno use to them.. .
.So now, our proposal.
We are encour-aged to see the amount of work basedon the Wall Street Journal which ap-pears in ACL proceedings.
Howeverwe remain concerned about the quan-tity of papers appearing there whichfail to use a rigorous methodology,and fail to build on the progress out-lined above.
These papers tend tofall outside the domain which hasbecome the testing ground for ourunderstanding of the phenomenon oflanguage, viz, the Wall Street Jour-nal.
Outside the Wall Street Journal,we are benighted.
May I suggest thatACL adopt a policy of accepting onlypapers investigating the language ofthe Wall Street Journal.A similar position was discussed in relationto Senseval.
There was a move to use, in partor in whole, the same sample of words (ca 40items) for Senseval-2 (English lexical sampletask) as had been used in Senseval-1.
Thiswould have promoted comparability of resultsacross the two exercises.
However, we wereanxious about continuing to focus our eortson just 40 of the 10,000 ambiguous words ofthe language, as it seemed plausible that someissues had simply not arisen in the rst sample,and if we did not switch sample, there was nochance that they would ever be encountered.All Senseval resources are in the publicdomain and can be (and have been) used byresearchers wanting to compare their systemperformance with performance gures as inSenseval proceedings.
Of course such com-parison will never be fair, as systems compet-ing under the examination conditions of theevaluation exercise were operating under timepressure, and did not always have time to cor-rect even the most egregious of bugs.
Howeverit is hard to see how the evaluation series cankeep the sheer range and variety of languageuse on the agenda if samples are reused.3 Languageow and the webYou cannot step twice into the sameriver, for other waters are constantlyowing on.Heraclitus (c. 535-c. 475 BC)We are currently planning a Senseval-3 taskwhere the test data will be instances of wordsin web pages, as located by a search engine.Test data will be dened by URL, line num-ber and byte oset.
The goal is to explorewhat happens when laboratory conditions arechanged for web conditions.
It will support ex-ploration of how supervised-training systemsperform when test set and training set are nolonger subsets of the same whole.
Partipantswill be expected to rst retrieve the web pageand then apply WSD to it.
This will allowsystems to use a wider context than is possi-ble in the usual paradigm of short-context testinstances.
They could, for example, gather acorpus of the reference URL, plus any pages itlinks to, plus other pages close to it in its di-rectory tree, in order to identify the domainof the instance.
In general, it makes spacefor a range of techniques which the Sensevalparadigm to date has ruled out.Clearly, web pages may change or die be-tween selecting URLs for manual tagging atset-up time, and the evaluation period, re-sulting in wasted manual-tagging eort.
Weshall minimize the waste by, rst, drawingup a candidate list of URL's, then, checkingthem to see whether they are still availableand unchanged a month or so later.
The factthat some web pages have died will not in-validate the exercise.
It just means there willbe fewer usable test instances than test-URLsdistributed.One hypothesis to be explored is thatsupervised-training systems are less resilientthan other system-types, in the real world situ-ation where the data to be disambiguated \inanger" may not match the text type of thetraining corpus.
The relation between the per-formance of supervised-training systems in thelaboratory and in the wild is to my mind oneof the critical issues at the current point intime, given the ascendancy that the paradigmhas achieved in CL.It may also shed light on the relation be-tween a linguistic/collocational view of wordsenses and one dominated by domain.
In-evitably, for some words, there will be a poormatch between the domains of training-corpusinstances and the domains of web instances.While this might seem `unfair' and a problemfollowing from the biases of the web, it is a factof linguistic life.
The concept of an unbiasedcorpus has no theoretical credentials.
The taskwill explore the implications of working with acorpus whose biases are unknown, and in anycase forever changing.The web also happens to be the corpus thatmany potential customers for WSD need tooperate on, so the task will provide a pictureof whether WSD technology is yet ready forthese potential clients.4 Senseval and MachineTranslation evaluationAs noted above, overall Senseval design istaken from MUC.
We have also followed MUCand TREC discussions of the hub-and-spokesmodel and the need to forever look towardsupdating the task, to guard against partici-pants becoming expert at the task as denedbut not at anything else.WSD is not a task of interest in itself.
Onedoes WSD in order to improve performance onsome other task.
The critical end-to-end task,for WSD, is Machine Translation (Kilgarri,1997).In Senseval-2, for Japanese there was atranslation memory task, which took the formof an MT evaluation (Kurohashi, 2001).
Inthat experimental design, each system re-sponse potentially requires individual atten-tion from a human assessor.
As in assess-ing human or computer translation, one can-not specify a complete set of correct answersahead of time, so one must be open to thepossibility that the system response is cor-rect but dierent from all the responses seento date.
Thus the exercise is potentially farmore expensive than the MUC model.
In theMUC model, human attention is required foreach data instance.
In this model, human at-tention is potentially required for each data-instance/system combination.Another consequence is that there is no free-standing, system-independent gold standardcorpus of correct answers.
New or revised sys-tems cannot simply test against a gold stan-dard (unless they limit their range of possibleanswers to ones already encountered, whichwould introduce further biases).So it is a more complex and costly formof evaluation.
However it is also far moreclosely related to a real task.
It is a directionthat Senseval needs to take.2The MUC-style xed-sense-inventory should be seen aswhat was necessary to open the chapter onWSD evaluation: a graspable, manageabletask when we had no experience of the dif-culties we might encounter, which also pro-vided researchers with some objective datasetsfor their development work.
For the future the2It is also the route we have taken in the WASPSproject, which is geared towards WSD for MT (Koelinget al, 2003).emphasis needs to be on assessments such asthe Japanese one, related to real tasks.5 Metric re-use: kappaConsider the (ctional) game show \Couples".The idea is to establish which couples share thesame world view to the greatest extent.
Eachmember of the couple is put in a space wherethey cannot hear what the other is saying, andis then asked twenty multiple-choice questionslikeWhat is the greatest UK pop group of the1960s?The Beatles/The Rolling StonesorWhich month is your oldest nephew/niece'sbirthday?Jan/Feb/Mar/Apr/May/Jun/Jul/Aug/Sep/Oct/Nov/Dec /No-nephew-or-nieceThe couple that gives the same answer mostoften wins.Dierent couples get dierent questions,sometimes with dierent numbers of multiple-choice options, and this introduces a risk ofunfairness.
If one couple gets all two-waychoices, while another gets all 13-way choices,and both agree half the time, the 13-way cou-ple have really done much better.
Randomguessing would have got (on average) a 50%score for the couple who got the two-way ques-tions, whereas it would only have got a 1/13or 7.7% score for the others.One way to x the problem is to give, foreach question, not a full point but a score mod-ied to allow for what random guessing wouldhave given.
This can be dened as =P (A)  P (E)1  P (E)where P (A) is the proportion of times theyactually agree, and P (E) is the proportion oftimes they would agree by chance.This is called the Kappa statistic.
It was de-veloped within the discipline of Content Anal-ysis, and introduced into the HLT world byJean Carletta (Carletta, 1996).Inter-Annotator AgreementFor HLT, the issue arises in manual taggingtasks, such as manually identifying the wordclass or word sense of a word in the text, orthe discourse function of a clause.
In each ofthese cases, there will be a xed set of possibleanswers.
Consider two exercises, one where ateam of two human taggers tag a set of clausesfor discourse function using a set of four pos-sible functions, the other where another teamof two uses a set of fteen possible functions.If the rst team gave the same answers 77%of the time, and the second gave the same an-swers 71% of the time, then, at a rst pass,the rst team had a higher agreement level.However they were using a smaller tagset, andwe can use kappa to compensate for that.
Thekappa gure for the rst team is0:77  1=41  1=4=0:520:75= 0:69and that for the second team is0:71   1=151  1=15=0:640:93= 0:69The inter-annotator agreement (IAA) can bepresented as simple agreement gures of 77%and 71%, or as kappa values of 0.69 in bothcases.IAA matters to HLT evaluation because hu-man tagging is what is needed to produced`gold standard' datasets against which systemperformance can be judged.
The simplest ap-proach is for a person to mark up a text,and to evaluate the system against those tag-gings.
But the person might make mistakes,and there may be problems of interpretationand judgement calls where a dierent humanmay well have given a dierent answer.
So, forgold standard dataset development, each itemto be tagged should be tagged by at least twopeople.How condent can we be in the integrity ofthe gold standard?
Do we really know that itis correct?
A central consideration is IAA: iftaggers agreed with each other nearly all thetime, we can be condent that, rstly, the goldstandard corpus is not full of errors, and sec-ondly, that the system of categories, or tags,according to which the markup took place isadequate to the task.
If the tags are not well-suited to the task and adequately dened, itwill frequently be arbitrary which tag a taggerselects, and this will show up in low IAA.ReservationsCarletta presented kappa as a better mea-sure of IAA than uncorrected agreement.
Inthe specic cases she describes, this is certainlyvalid.Those cases are very specic. Kappa is rel-evant where the concern is that an IAA gurebased on a small tagset is being compared withone based on a large tagset.
Where that is thefocus of the investigation, kappa is an appro-priate statistic.Where it is not, there are arguments for andagainst the use of kappa.
In its favour is thatit builds in compensation for distortions thatmight otherwise go unnoticed resulting fromdierent tagset sizes.Against is, principally, the argument thatkappa gures are hard to interpret.
A simpleagreement gure is just that: it is clear whatit means, and the critical question of whether,say, 90% agreement is `good enough' is onefor the reader to form their own judgment on.With a kappa gure of .85, the reader needs to,rstly, understand the mathematics of kappa,and secondly, bear in mind the various com-plexities of how kappa might have been calcu-lated (see also below), before forming a judg-ment.
To \help" the reader with this task,there are various discussions in the literatureas to how dierent kappa gures are to be in-terpreted.
Sadly, these are contradictory (andeven if they weren't, it is the duty of any criti-cal reader to form their own judgment on whatis good enough.
)Complexities in the calculationAbove we present kappa in its simplest form.Naturally, when used in earnest additional is-sues arise.
The observations below arose prin-cipally from the consideration of how we mightuse kappa in Senseval.
The task was to pro-duce a gold standard corpus in which wordswere associated with their appropriate mean-ings, with the inventory of meanings takenfrom a dictionary.Firstly, tagset size is assumed to be xed.In the Senseval context, there were three is-sues here.1.
There were two variants of the task: `lex-ical sample' and `all-words'.
In the all-words variant, all content words in a textare tagged.
Some will be highly polyse-mous, others not polysemous at all.
Itis not clear how to present kappa guresthat are averages across datasets wherethe tagset size varies.In the lexical sample task, rst, a sampleof sentences containing a particular wordis identied, and then, only the instancesof that word are tagged, so the issue doesnot arise immediately.
It does still ariseif a kappa gure is to be computed whichdraws together data from more than onelexical-sample word.2.
In addition to the dictionary senses forthe word, there were two tags, U for`unassignable' and P for `proper name',which were always available as options forthe human taggers.
If included, for pur-poses of calculating kappa, a word thatonly has two dictionary senses is classi-ed as a four-way choice, which seems in-appropriate, particularly as U and P tagswere quite rare and absent entirely forsome words.3.
There were a number of other `marginal'senses which, if included in the tag count,extend it greatly (for some words).
Inthe Senseval-1, taggers largely workedwithin a given word class, so noun in-stances ofoat were treated separatelyfrom verb instances, but, in e.g., nouncases where none of the noun instancestted, they were instructed to considerwhether any of the verb senses were agood semantic match (even though theyevidently could not be a syntactic match).Also some words formed part of numer-ous multi-word units that were listed inthe dictionary.
Where a tagger foundthe lexical-sample word occurring withina listed multi-word unit, the instructionwas to assign that as a sense.One response to issues 2 and 3 is to use amore sophisticated model of random guessing,in which, rather than assuming all tags areequally likely for the random guesser, we usethe relative frequencies of the dierent tags asthe basis for a probability model .
The methodsucceeds in giving less weight to marginal tags,at the cost of making the maths of the calu-clation more complex and the output kappagures correspondingly harder to interpret.Secondly, the Senseval tagging schemeallowed human taggers to give multiple an-swers, and also allowed multiple answers in thetagging scheme.Thirdly, in Senseval the number of hu-mans tagging an instance varied (according towhether or not the instance was problematic).Fourthly, there is a distinction between twokinds of occasion on which two taggers givedierent tags.
It may be a problematic caseto tag, or it may be simple human error (suchas a typo).
Arguably, simple typos and sim-ilar are of no theoretical interest and shouldbe corrected before considering IAA.
A relatedpoint is the distinction between agreement lev-els (between individual taggers) and replica-bility (between teams of taggers).
Where theconcern is the integrity of a gold standard re-source, replicability is the real matter of in-terest: would another team of taggers, usingthe same data, guidelines and methods, arriveat the same taggings?
A tagging methodologywhich guards against simple errors, waywardindividuals, and wayward interpretations willtend to produce replicable datasets.All of these considerations can be addressedusing a variant of kappa.
My point is thatkappa becomes harder and harder to interpret,as more and more assumptions and intricaciesare built into its calculation.Kappa has been widely embraced as an ex-ample of an aspect of evaluation technologythat carries across dierent HLT evaluationtasks, giving a shimmer of statistical sophis-tication wherever it alights.
My sense is thatit is a bandwagon, which HLT researchers havefelt they ought to jump on in order to displaytheir scientic credentials and ability to usestatistics, which, in many places where it hasbeen used, has led to little but gratuitous ob-fuscation.6 ConclusionClearly, we would like new HLT evaluation ex-ercises to benet from evaluation work alreadydone.
This paper explores several issues thathave arisen from the Senseval experience.ReferencesJean Carletta.
1996.
Assessing agreement on clas-sication tasks: The kappa statistic.
Computa-tional Linguistics, 22(2):249{254.Philip Edmonds and Adam Kilgarri.
2002.
Intro-duction to the special issue on evaluating wordsense disambiguation systems.
Journal of Nat-ural Language Engineering, 8(4).Adam Kilgarri.
1997.
What is word sense dis-ambiguation good for?
In Proc.
Natural Lan-guage Processing in the Pacic Rim (NLPRS'97), pages 209{214, Phuket, Thailand, Decem-ber.Rob Koeling, Roger Evans, Adam Kilgarri, andDavid Tugwell.
2003.
An evaluation pf a lex-icographer's workbench: building lexicons formachine translation.
In EACL workshop on re-sources for Machine Translation, Budapest.Sadao Kurohashi.
2001. senseval-2 japanesetranslation task.
In Proc.
senseval-2: SecondInternational Workshop on Evaluating WSDSystems, pages 37{40, Toulouse, July.
ACL.Swift Redux.
2001.
A modest proposal.
ELSnews,10(2):7.
