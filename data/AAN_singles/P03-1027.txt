Recognizing Expressions of Commonsense Psychology in English TextAndrew Gordon, Abe Kazemzadeh, Anish Nair and Milena PetrovaUniversity of Southern CaliforniaLos Angeles, CA 90089 USAgordon@ict.usc.edu, {kazemzad, anair, petrova}@usc.eduAbstractMany applications of natural languageprocessing technologies involve analyzingtexts that concern the psychological statesand processes of people, including theirbeliefs, goals, predictions, explanations,and plans.
In this paper, we describe ourefforts to create a robust, large-scale lexi-cal-semantic resource for the recognitionand classification of expressions of com-monsense psychology in English Text.We achieve high levels of precision andrecall by hand-authoring sets of localgrammars for commonsense psychologyconcepts, and show that this approach canachieve classification performance greaterthan that obtained by using machinelearning techniques.
We demonstrate theutility of this resource for large-scale cor-pus analysis by identifying references toadversarial and competitive goals in po-litical speeches throughout U.S. history.1 Commonsense Psychology in LanguageAcross all text genres it is common to find wordsand phrases that refer to the mental states of people(their beliefs, goals, plans, emotions, etc.)
and theirmental processes (remembering, imagining, priori-tizing, problem solving).
These mental states andprocesses are among the broad range of conceptsthat people reason about every day as part of theircommonsense understanding of human psychol-ogy.
Commonsense psychology has been studiedin many fields, sometimes using the terms Folkpsychology or Theory of Mind, as both a set of be-liefs that people have about the mind and as a setof everyday reasoning abilities.Within the field of computational linguistics,the study of commonsense psychology has not re-ceived special attention, and is generally viewed asjust one of the many conceptual areas that must beaddressed in building large-scale lexical-semanticresources for language processing.
Although therehave been a number of projects that have includedconcepts of commonsense psychology as part of alarger lexical-semantic resource, e.g.
the BerkeleyFrameNet Project (Baker et al, 1998), none haveattempted to achieve a high degree of breadth ordepth over the sorts of expressions that people useto refer to mental states and processes.The lack of a large-scale resource for the analy-sis of language for commonsense psychologicalconcepts is seen as a barrier to the development ofa range of potential computer applications that in-volve text analysis, including the following:?
Natural language interfaces to mixed-initiativeplanning systems (Ferguson & Allen, 1993;Traum, 1993) require the ability to map ex-pressions of users?
beliefs, goals, and plans(among other commonsense psychology con-cepts) onto formalizations that can be ma-nipulated by automated planning algorithms.?
Automated question answering systems(Voorhees & Buckland, 2002) require the abil-ity to tag and index text corpora with the rele-vant commonsense psychology concepts inorder to handle questions concerning the be-liefs, expectations, and intentions of people.?
Research efforts within the field of psychologythat employ automated corpus analysis tech-niques to investigate developmental and men-tal illness impacts on language production, e.g.Reboul & Sabatier?s (2001) study of the dis-course of schizophrenic patients, require theability to identify all references to certain psy-chological concepts in order to draw statisticalcomparisons.In order to enable future applications, we un-dertook a new effort to meet this need for a lin-guistic resource.
This paper describes our efforts inbuilding a large-scale lexical-semantic resource forautomated processing of natural language textabout mental states and processes.
Our aim was tobuild a system that would analyze natural languagetext and recognize, with high precision and recall,every expression therein related to commonsensepsychology, even in the face of an extremely broadrange of surface forms.
Each recognized expres-sion would be tagged with an appropriate conceptfrom a broad set of those that participate in ourcommonsense psychological theories.Section 2 demonstrates the utility of a lexical-semantic resource of commonsense psychology inautomated corpus analysis through a study of thechanges in mental state expressions over the courseof over 200 years of U.S. Presidential State-of-the-Union Addresses.
Section 3 of this paper describesthe methodology that we followed to create thisresource, which involved the hand authoring oflocal grammars on a large scale.
Section 4 de-scribes a set of evaluations to determine the per-formance levels that these local grammars couldachieve and to compare these levels to those ofmachine learning approaches.
Section 5 concludesthis paper with a discussion of the relative meritsof this approach to the creation of lexical-semanticresources as compared to other approaches.2 Applications to corpus analysisOne of the primary applications of a lexical-semantic resource for commonsense psychology istoward the automated analysis of large text cor-pora.
The research value of identifying common-sense psychology expressions has beendemonstrated in work on children?s language use,where researchers have manually annotated largetext corpora consisting of parent/child discoursetranscripts (Barsch & Wellman, 1995) and chil-dren?s storybooks (Dyer et al, 2000).
While theseprevious studies have yielded interesting results,they required enormous amounts of human effortto manually annotate texts.
In this section we aimto show how a lexical-semantic resource for com-monsense psychology can be used to automate thisannotation task, with an example not from the do-main of children?s language acquisition, but ratherpolitical discourse.We conducted a study to determine how politi-cal speeches have been tailored over the course ofU.S.
history throughout changing climates of mili-tary action.
Specifically, we wondered if politi-cians were more likely to talk about goals havingto do with conflict, competition, and aggressionduring wartime than in peacetime.
In order toautomatically recognize references to goals of thissort in text, we used a set of local grammarsauthored using the methodology described in Sec-tion 3 of this paper.
The corpus we selected to ap-ply these concept recognizers was the U.S. State ofthe Union Addresses from 1790 to 2003.
The rea-sons for choosing this particular text corpus wereits uniform distribution over time and its easyavailability in electronic form from Project Guten-berg (www.gutenberg.
net).
Our set of local gram-mars identified 4290 references to these goals inthis text corpus, the vast majority of them beginreferences to goals of an adversarial nature (ratherthan competitive).
Examples of the references thatwere identified include the following:?
They sought to use the rights and privilegesthey had obtained in the United Nations, tofrustrate its purposes [adversarial-goal] andcut down its powers as an effective agent ofworld progress.
(Truman, 1953)?
The nearer we come to vanquishing [adver-sarial-goal] our enemies the more we inevita-bly become conscious of differences amongthe victors.
(Roosevelt, 1945)?
Men have vied [competitive-goal] with eachother to do their part and do it well.
(Wilson,1918)?
I will submit to Congress comprehensive leg-islation to strengthen our hand in combating[adversarial-goal] terrorists.
(Clinton, 1995)Figure 1 summarizes the results of applying ourlocal grammars for adversarial and competitivegoals to the U.S. State of the Union Addresses.
Foreach year, the value that is plotted represents thenumber of references to these concepts that wereidentified per 100 words in the address.
The inter-esting result of this analysis is that references toadversarial and competitive goals in this corpusincrease in frequency in a pattern that directly cor-responds to the major military conflicts that theU.S.
has participated in throughout its history.Each numbered peak in Figure 1 corresponds toa period in which the U.S. was involved in a mili-tary conflict.
These are: 1) 1813, War of 1812, USand Britain; 2) 1847, Mexican American War; 3)1864, Civil War; 4) 1898, Spanish American War;5) 1917, World War I; 6) 1943, World War II; 7)1952, Korean War; 8) 1966, Vietnam War; 9)1991, Gulf War; 10) 2002, War on Terrorism.The wide applicability of a lexical-semantic re-source for commonsense psychology will requirethat the identified concepts are well defined andare of broad enough scope to be relevant to a widerange of tasks.
Additionally, such a resource mustachieve high levels of accuracy in identifying theseconcepts in natural language text.
The remainder ofthis paper describes our efforts in authoring andevaluating such a resource.3 Authoring recognition rulesThe first challenge in building any lexical-semanticresource is to identify the concepts that are to berecognized in text and used as tags for indexing ormarkup.
For expressions of commonsense psy-chology, these concepts must describe the broadscope of people?s mental states and processes.
Anontology of commonsense psychology with a highdegree of both breadth and depth is described byGordon (2002).
In this work, 635 commonsensepsychology concepts were identified through ananalysis of the representational requirements of acorpus of 372 planning strategies collected from 10real-world planning domains.
These concepts weregrouped into 30 conceptual areas, corresponding tovarious reasoning functions, and full formal mod-els of each of these conceptual areas are beingauthored to support automated inference aboutcommonsense psychology (Gordon & Hobbs,2003).
We adopted this conceptual framework inour current project because of the broad scope ofthe concepts in this ontology and its potential forfuture integration into computational reasoningsystems.The full list of the 30 concept areas identified isas follows: 1) Managing knowledge, 2) Similaritycomparison, 3) Memory retrieval, 4) Emotions, 5)Explanations, 6) World envisionment, 7) Execu-tion envisionment, 8) Causes of failure, 9) Man-aging expectations, 10) Other agent reasoning, 11)Threat detection, 12) Goals, 13) Goal themes, 14)Goal management, 15) Plans, 16) Plan elements,17) Planning modalities, 18) Planning goals, 19)Plan construction, 20) Plan adaptation, 21) Design,22) Decisions, 23) Scheduling, 24) Monitoring, 25)Execution modalities, 26) Execution control, 27)Repetitive execution, 28) Plan following, 29) Ob-servation of execution, and 30) Body interaction.Our aim for this lexical-semantic resource wasto develop a system that could automatically iden-tify every expression of commonsense psychologyin English text, and assign to them a tag corre-sponding to one of the 635 concepts in this ontol-ogy.
For example, the following passage (fromWilliam Makepeace Thackeray?s 1848 novel,Vanity Fair) illustrates the format of the output ofthis system, where references to commonsensepsychology concepts are underlined and followedby a tag indicating their specific concept type de-limited by square brackets:00.20.40.60.811.21.41.61.817901798180618141822183018381846185418621870187818861898190619141922193019391947195519631971197919871997YearFeaturesper100words127654389 10Figure 1.
Adversarial and competitive goals in the U.S. State of the Union Addresses from 1790-2003Perhaps [partially-justified-proposition] shehad mentioned the fact [proposition] already toRebecca, but that young lady did not appear to[partially-justified-proposition] have remem-bered it [memory-retrieval]; indeed, vowed andprotested that she expected [add-expectation] tosee a number of Amelia's nephews and nieces.She was quite disappointed [disappointment-emotion] that Mr. Sedley was not married; shewas sure [justified-proposition] Amelia had saidhe was, and she doted so on [liking-emotion] lit-tle children.The approach that we took was to author (byhand) a set of local grammars that could be used toidentify each concept.
For this task we utilized theIntex Corpus Processor software developed by theLaboratoire d'Automatique Documentaire et Lin-guistique (LADL) of the University of Paris 7 (Sil-berztein, 1999).
This software allowed us to authora set of local grammars using a graphical user in-terface, producing lexical/syntactic structures thatcan be compiled into finite-state transducers.
Tosimplify the authoring of these local grammars,Intex includes a large-coverage English dictionarycompiled by Blandine Courtois, allowing us tospecify them at a level that generalized over nounand verb forms.
For example, there are a variety ofways of expressing in English the concept of reaf-firming a belief that is already held, as exemplifiedin the following sentences:1) The finding was confirmed by the newdata.
2) She told the truth, corroborating hisstory.
3) He reaffirms his love for her.
4) Weneed to verify the claim.
5) Make sure it is true.Although the verbs in these sentences differ intense, the dictionaries in Intex allowed us to recog-nize each using the following simple description:(<confirm> by | <corroborate> | <reaffirm> |<verify> | <make> sure)While constructing local grammars for each ofthe concepts in the original ontology of common-sense psychology, we identified several conceptualdistinctions that were made in language that werenot expressed in the specific concepts that Gordonhad identified.
For example, the original ontologyincluded only three concepts in the conceptual areaof memory retrieval (the sparsest of the 30 areas),namely memory, memory cue, and memory re-trieval.
English expressions such as ?to forget?
and?repressed memory?
could not be easily mappeddirectly to one of these three concepts, whichprompted us to elaborate the original sets of con-cepts to accommodate these and other distinctionsmade in language.
In the case of the conceptualarea of memory retrieval, a total of twelve uniqueconcepts were necessary to achieve coverage overthe distinctions evident in English.These local grammars were authored one con-ceptual area at a time.
At the time of the writing ofthis paper, our group had completed 6 of the origi-nal 30 commonsense psychology conceptual areas.The remainder of this paper focuses on the first 4of the 6 areas that were completed, which wereevaluated to determine the recall and precision per-formance of our hand-authored rules.
These fourareas are Managing knowledge, Memory, Expla-nations, and Similarity judgments.
Figure 2 pre-sents each of these four areas with a singlefabricated example of an English expression foreach of the final set of concepts.
Local grammarsfor the two additional conceptual areas, Goals (20concepts) and Goal management (17 concepts),were authored using the same approach as the oth-ers, but were not completed in time to be includedin our performance evaluation.After authoring these local grammars using theIntex Corpus Processor, finite-state transducerswere compiled for each commonsense psychologyconcept in each of the different conceptual areas.To simplify the application of these transducers totext corpora and to aid in their evaluation, trans-ducers for individual concepts were combined intoa single finite state machine (one for each concep-tual area).
By examining the number of states andtransitions in the compiled finite state graphs, someindication of their relative size can be given for thefour conceptual areas that we evaluated: Managingknowledge (348 states / 932 transitions), Memory(203 / 725), Explanations (208 / 530), and Similar-ity judgments (121 / 500).4 Performance evaluationIn order to evaluate the utility of our set of hand-authored local grammars, we conducted a study oftheir precision and recall performance.
In order tocalculate the performance levels, it was first neces-sary to create a test corpus that contained refer-ences to the sorts of commonsense psychologicalconcepts that our rules were designed to recognize.To accomplish this, we administered a survey to1.
Managing knowledge (37 concepts)He?s got a logical mind (managing-knowledge-ability).
She?s very gullible (bias-toward-belief).
He?s skepti-cal by nature (bias-toward-disbelief).
It is the truth (true).
That is completely false (false).
We need to knowwhether it is true or false (truth-value).
His claim was bizarre (proposition).
I believe what you are saying (be-lief).
I didn?t know about that (unknown).
I used to think like you do (revealed-incorrect-belief).
The assumptionwas widespread (assumption).
There is no reason to think that (unjustified-proposition).
There is some evidenceyou are right (partially-justified-proposition).
The fact is well established (justified-proposition).
As a rule, stu-dents are generally bright (inference).
The conclusion could not be otherwise (consequence).
What was the rea-son for your suspicion (justification)?
That isn?t a good reason (poor-justification).
Your argument is circular(circular-justification).
One of these things must be false (contradiction).
His wisdom is vast (knowledge).
Heknew all about history (knowledge-domain).
I know something about plumbing (partial-knowledge-domain).He?s got a lot of real-world experience (world-knowledge).
He understands the theory behind it (world-model-knowledge).
That is just common sense (shared-knowledge).
I?m willing to believe that (add-belief).
I stoppedbelieving it after a while (remove-belief).
I assumed you were coming (add-assumption).
You can?t make thatassumption here (remove-assumption).
Let?s see what follows from that (check-inferences).
Disregard the con-sequences of the assumption (ignore-inference).
I tried not to think about it (suppress-inferences).
I concludedthat one of them must be wrong (realize-contradiction).
I realized he must have been there (realize).
I can?t thinkstraight (knowledge-management-failure).
It just confirms what I knew all along (reaffirm-belief).2.
Memory (12 concepts)He has a good memory (memory-ability).
It was one of his fondest memories (memory-item).
He blocked outthe memory of the tempestuous relationship (repressed-memory-item).
He memorized the words of the song(memory-storage).
She remembered the last time it rained (memory-retrieval).
I forgot my locker combination(memory-retrieval-failure).
He repressed the memories of his abusive father (memory-repression).
The widowwas reminded of her late husband (reminding).
He kept the ticket stub as a memento (memory-cue).
He intendedto call his brother on his birthday (schedule-plan).
He remembered to set the alarm before he fell asleep (sched-uled-plan-retrieval).
I forgot to take out the trash (scheduled-plan-retrieval-failure).3.
Explanations (20 concepts)He?s good at coming up with explanations (explanation-ability).
The cause was clear (cause).
Nobody knewhow it had happened (mystery).
There were still some holes in his account (explanation-criteria).
It gave us theexplanation we were looking for (explanation).
It was a plausible explanation (candidate-explanation).
It wasthe best explanation I could think of (best-candidate-explanation).
There were many contributing factors (fac-tor).
I came up with an explanation (explain).
Let?s figure out why it was so (attempt-to-explain).
He came upwith a reasonable explanation (generate-candidate-explanation).
We need to consider all of the possible expla-nations (assess-candidate-explanations).
That is the explanation he went with (adopt-explanation).
We failed tocome up with an explanation (explanation-failure).
I can?t think of anything that could have caused it (explana-tion-generation-failure).
None of these explanations account for the facts (explanation-satisfaction-failure).Your account must be wrong (unsatisfying-explanation).
I prefer non-religious explanations (explanation-preference).
You should always look for scientific explanations (add-explanation-preference).
We?re not goingto look at all possible explanations (remove-explanation-preference).4.
Similarity judgments (13 concepts)She?s good at picking out things that are different (similarity-comparison-ability).
Look at the similaritiesbetween the two (make-comparison).
He saw that they were the same at an abstract level (draw-analogy).
Shecould see the pattern unfolding (find-pattern).
It depends on what basis you use for comparison (comparison-metric).
They have that in common (same-characteristic).
They differ in that regard (different-characteristic).
Ifa tree were a person, its leaves would correspond to fingers (analogical-mapping).
The pattern in the rug wasintricate (pattern).
They are very much alike (similar).
It is completely different (dissimilar).
It was an analogousexample (analogous).Figure 2.
Example sentences referring to 92 concepts in 4 areas of commonsense psychologycollect novel sentences that could be used for thispurpose.This survey was administered over the courseof one day to anonymous adult volunteers whostopped by a table that we had set up on our uni-versity?s campus.
We instructed the survey taker toauthor 3 sentences that included words or phrasesrelated to a given concept, and 3 sentences thatthey felt did not contain any such references.
Eachsurvey taker was asked to generate these 6 sen-tences for each of the 4 concept areas that we wereevaluating, described on the survey in the follow-ing manner:?
Managing knowledge: Anything about theknowledge, assumptions, or beliefs that peoplehave in their mind?
Memory: When people remember things, for-get things, or are reminded of things?
Explanations: When people come up with pos-sible explanations for unknown causes?
Similarity judgments: When people find simi-larities or differences in thingsA total of 99 people volunteered to take oursurvey, resulting in a corpus of 297 positive and297 negative sentences for each conceptual area,with a few exceptions due to incomplete surveys.Using this survey data, we calculated the preci-sion and recall performance of our hand-authoredlocal grammars.
Every sentence that had at leastone concept detected for the corresponding conceptarea was treated as a ?hit?.
Table 1 presents theprecision and recall performance for each conceptarea.
The results show that the precision of oursystem is very high, with marginal recall perform-ance.The low recall scores raised a concern over thequality of our test data.
In reviewing the sentencesthat were collected, it was apparent that some sur-vey participants were not able to complete the taskas we had specified.
To improve the validity of thetest data, we enlisted six volunteers (native Englishspeakers not members of our development team) tojudge whether or not each sentence in the corpuswas produced according to the instructions.
Thecorpus of sentences was divided evenly amongthese six raters, and each sentence that the raterjudged as not satisfying the instructions was fil-tered from the data set.
In addition, each rater alsojudged half of the sentences given to a differentrater in order to compute the degree of inter-rateragreement for this filtering task.
After filteringsentences from the corpus, a second preci-sion/recall evaluation was performed.
Table 2 pre-sents the results of our hand-authored localgrammars on the filtered data set, and lists the in-ter-rater agreement for each conceptual area amongour six raters.
The results show that the systemachieves a high level of precision, and the recallperformance is much better than earlier indicated.The performance of our hand-authored localgrammars was then compared to the performancethat could be obtained using more traditional ma-chine-learning approaches.
In these comparisons,the recognition of commonsense psychology con-cepts was treated as a classification problem,where the task was to distinguish between positiveConcept area Correct Hits(a)Wrong hits(b)Total positivesentences (c)Total negativesentencesPrecision(a/(a+b))Recall(a/c)Managing knowledge 205 16 297 297 92.76% 69.02%Memory 240 4 297 297 98.36% 80.80%Explanations 126 7 296 296 94.73% 42.56%Similarity judgments 178 18 296 297 90.81% 60.13%749 45 1186 1187 94.33% 63.15%Table 1.
Precision and recall results on the unfiltered data setConcept area Inter-rateragreement (K)CorrectHits (a)Wronghits (b)Total positivesentences (c)Total negativesentencesPrecision(a/(a+b))Recall(a/c)Managing knowledge 0.5636 141 12 168 259 92.15% 83.92%Memory 0.8069 209 0 221 290 100% 94.57%Explanations 0.7138 83 5 120 290 94.21% 69.16%Similarity judgments 0.6551 136 12 189 284 91.89% 71.95%0.6805 569 29 698 1123 95.15% 81.51%Table 2.
Precision and recall results on the filtered data set, with inter-rater agreement on filteringand negative sentences for any given concept area.Sentences in the filtered data sets were used astraining instances, and feature vectors for eachsentence were composed of word-level unigramand bi-gram features, using no stop-lists and byignoring punctuation and case.
By using a toolkitof machine learning algorithms (Witten & Frank,1999), we were able to compare the performanceof a wide range of different techniques, includingNa?ve Bayes, C4.5 rule induction, and SupportVector Machines, through stratified cross-validation (10-fold) of the training data.
The high-est performance levels were achieved using a se-quential minimal optimization algorithm fortraining a support vector classifier using polyno-mial kernels (Platt, 1998).
These performance re-sults are presented in Table 3.
The percentagecorrectness of classification (Pa) of our hand-authored local grammars (column A) was higherthan could be attained using this machine-learningapproach (column B) in three out of the four con-cept areas.We then conducted an additional study to de-termine if the two approaches (hand-authored localgrammars and machine learning) could be com-plimentary.
The concepts that are recognized byour hand-authored rules could be conceived as ad-ditional bimodal features for use in machinelearning algorithms.
We constructed an additionalset of support vector machine classifiers trained onthe filtered data set that included these additionalconcept-level features in the feature vector of eachinstance along side the existing unigram and bi-gram features.
Performance of these enhancedclassifiers, also obtained through stratified cross-validation (10-fold), are also reported in Table 3 aswell (column C).
The results show that these en-hanced classifiers perform at a level that is thegreater of that of each independent approach.5 DiscussionThe most significant challenge facing developersof large-scale lexical-semantic resources is comingto some agreement on the way that natural lan-guage can be mapped onto specific concepts.
Thischallenge is particularly evident in consideration ofour survey data and subsequent filtering.
Theabilities that people have in producing and recog-nizing sentences containing related words orphrases differed significantly across concept areas.While raters could agree on what constitutes asentence containing an expression about memory(Kappa=.8069), the agreement on expressions ofmanaging knowledge is much lower than wewould hope for (Kappa=.5636).
We would expectmuch greater inter-rater agreement if we hadtrained our six raters for the filtering task, that is,described exactly which concepts we were lookingfor and gave them examples of how these conceptscan be realized in English text.
However, this ap-proach would have invalidated our performanceresults on the filtered data set, as the task of theraters would be biased toward identifying exam-ples that our system would likely perform well onrather than identifying references to concepts ofcommonsense psychology.Our inter-rater agreement concern is indicativeof a larger problem in the construction of large-scale lexical-semantic resources.
The deeper wedelve into the meaning of natural language, the lesswe are likely to find strong agreement among un-trained people concerning the particular conceptsthat are expressed in any given text.
Even withlexical-semantic resources about commonsenseknowledge (e.g.
commonsense psychology), finerdistinctions in meaning will require the efforts oftrained knowledge engineers to successfully mapbetween language and concepts.
While this willcertainly create a problem for future preci-A.
Hand authored localgrammarsB.
SVM with word levelfeaturesC.
SVM with word andconcept featuresConcept area Pa K Pa K Pa KManaging knowledge 90.86% 0.8148 86.0789% 0.6974 89.5592% 0.7757Memory 97.65% 0.8973 93.5922% 0.8678 97.4757% 0.9483Explanations 89.75% 0.7027 85.9564% 0.6212 89.3462% 0.7186Similarity judgments 86.25% 0.7706 92.4528% 0.8409 92.0335% 0.8309Table 3.
Percent agreement (Pa) and Kappa statistics (K) for classification using hand-authored localgrammars (A), SVMs with word features (B), and SVMs with word and concept features (C)sion/recall performance evaluations, the concern iseven more serious for other methodologies thatrely on large amounts of hand-tagged text data tocreate the recognition rules in the first place.
Weexpect that this problem will become more evidentas projects using algorithms to induce local gram-mars from manually-tagged corpora, such as theBerkeley FrameNet efforts (Baker et al, 1998),broaden and deepen their encodings in conceptualareas that are more abstract (e.g.
commonsensepsychology).The approach that we have taken in our re-search does not offer a solution to the growingproblem of evaluating lexical-semantic resources.However, by hand-authoring local grammars forspecific concepts rather than inducing them fromtagged text, we have demonstrated a successfulmethodology for creating lexical-semantic re-sources with a high degree of conceptual breadthand depth.
By employing linguistic and knowledgeengineering skills in a combined manner we havebeen able to make strong ontological commitmentsabout the meaning of an important portion of theEnglish language.
We have demonstrated that theprecision and recall performance of this approachis high, achieving classification performancegreater than that of standard machine-learningtechniques.
Furthermore, we have shown thathand-authored local grammars can be used toidentify concepts that can be easily combined withword-level features (e.g.
unigrams, bi-grams) forintegration into statistical natural language proc-essing systems.
Our early exploration of the appli-cation of this work for corpus analysis (U.S. Stateof the Union Addresses) has produced interestingresults, and we expect that the continued develop-ment of this resource will be important to the suc-cess of future corpus analysis and human-computerinteraction projects.AcknowledgmentsThis paper was developed in part with funds fromthe U.S. Army Research Institute for the Behav-ioral and Social Sciences under ARO contractnumber DAAD 19-99-D-0046.
Any opinions,findings and conclusions or recommendations ex-pressed in this paper are those of the authors anddo not necessarily reflect the views of the Depart-ment of the Army.ReferencesBaker, C., Fillmore, C., & Lowe, J.
(1998) The Ber-keley FrameNet project.
in Proceedings of theCOLING-ACL, Montreal, Canada.Bartsch, K. & Wellman, H. (1995) Children talk aboutthe mind.
New York: Oxford University Press.Dyer, J., Shatz, M., & Wellman, H. (2000) Young chil-dren?s storybooks as a source of mental state infor-mation.
Cognitive Development 15:17-37.Ferguson, G. & Allen, J.
(1993) Cooperative Plan Rea-soning for Dialogue Systems, in AAAI-93 Fall Sym-posium on Human-Computer Collaboration:Reconciling Theory, Synthesizing Practice, AAAITechnical Report FS-93-05.
Menlo Park, CA: AAAIPress.Gordon, A.
(2002) The Theory of Mind in StrategyRepresentations.
24th Annual Meeting of the Cogni-tive Science Society.
Mahwah, NJ: Lawrence Erl-baum Associates.Gordon, A.
& Hobbs (2003) Coverage and competencyin formal theories: A commonsense theory of mem-ory.
AAAI Spring Symposium on Formal Theories ofCommonsense knowledge, March 24-26, Stanford.Platt, J.
(1998).
Fast Training of Support Vector Ma-chines using Sequential Minimal Optimization.
In B.Sch?lkopf, C. Burges, and A. Smola (eds.)
Advancesin Kernel Methods - Support Vector Learning, Cam-bridge, MA: MIT Press.Reboul A., Sabatier P., No?l-Jorand M-C. (2001) Lediscours des schizophr?nes: une ?tude de cas.
Revuefran?aise de Psychiatrie et de Psychologie M?dicale,49, pp 6-11.Silberztein, M. (1999) Text Indexing with INTEX.Computers and the Humanities 33(3).Traum, D. (1993) Mental state in the TRAINS-92 dia-logue manager.
In Working Notes of the AAAI SpringSymposium on Reasoning about Mental States: For-mal Theories and Applications, pages 143-149, 1993.Menlo Park, CA: AAAI Press.Voorhees, E. & Buckland, L. (2002) The Eleventh TextREtrieval Conference (TREC 2002).
Washington,DC: Department of Commerce, National Institute ofStandards and Technology.Witten, I.
& Frank, E. (1999) Data Mining: PracticalMachine Learning Tools and Techniques with JavaImplementations.
Morgan Kaufman.
