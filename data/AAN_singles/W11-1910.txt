Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 71?75,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsSupervised Coreference Resolution with SUCREHamidreza Kobdani and Hinrich Schu?tzeInstitute for Natural Language ProcessingUniversity of Stuttgart, Germanykobdani@ims.uni-stuttgart.deAbstractIn this paper we present SUCRE (Kobdaniand Schu?tze, 2010) that is a modular coref-erence resolution system participating in theCoNLL-2011 Shared Task: Modeling Unre-stricted Coreference in OntoNote (Pradhan etal., 2011).
The SUCRE?s modular architectureprovides a clean separation between data stor-age, feature engineering and machine learningalgorithms.1 IntroductionNoun phrase coreference resolution is the processof finding markables (noun phrase) referring to thesame real world entity or concept.
In other words,this process groups the markables of a documentinto entities (equivalence classes) so that all mark-ables in an entity are coreferent.
Examples of ap-plications of coreference resolution are Informa-tion Extraction, Question Answering and AutomaticSummarization.Coreference is an equivalence relation betweentwo markables, i.e., it is reflexive, symmetric andtransitive.
The first solution that intuitively comesto mind is binary classification of markable pairs(links).
Therefore at the heart of most existing ap-proaches there is a binary classifier that classifieslinks to coreferent/disreferent.
One can also use thetransitive property of coreference relation to buildthe entities; this is done using a clustering method.Our approach in this paper consist of the abovementioned steps, namely:1.
Classification of links to coreferent/disreferent.2.
Clustering of links which are classified ascoreferent.This paper is organized as follows.
In Section 2,we present our feature engineering approach.
Sec-tion 3 presents the system architecture.
Data set isdescribed in Section 4.
Sections 5 and 6 present re-sults and conclusions.2 Feature EngineeringIn recent years there has been substantial work onthe problem of coreference resolution.
Most meth-ods present and report on the benchmark data setsfor English.
The feature sets they use are based on(Soon et al, 2001).
These features consist of string-based features, distance features, span features, part-of-speech features, grammatical features, and agree-ment features.We defined a comprehensive set of features basedon previous coreference resolution systems for En-glish, e.g.
(Bengtson and Roth, 2008).
In the com-mon approach to coreference resolution we havechosen, features are link features, i.e., features aredefined over a pair of markables.
For link featuredefinition and extraction, the head words of mark-ables are usually used, but in some cases the headword is not a suitable choice.
For example, con-sider these two markables: the book and a book, inboth cases book is the head word but to distinguishwhich markable is definite and which indefinite ad-ditional information about the markables has to betaken into account.
Now consider these two mark-ables: the university students in Germany and theuniversity students in France in this case the headwords and the first four words of each markableare the same but they cannot be coreferent, and thiscould be detected only by looking at the entire nounphrase.
Some features require complex preprocess-71ing or complex definitions.
Consider the two mark-ables the members of parliament and the members ofthe European Union.
The semantic class ofmembersis person in the first case and country in the second.To cover all such cases, we introduced a feature defi-nition language (Kobdani et al, 2010).
With the fea-ture definition language we will be able to access allinformation that is connected to a markable, includ-ing the first, last and head words of the two mark-ables; all other words of the two markables; and thetwo markables as atomic elements.After defining new features (new definition fromscratch or definition by combination of existing fea-tures), we have to evaluate them.
In principle, wecould use any figure of merit to evaluate the useful-ness of a feature or to compare two similar features,including Gini coefficient, mutual information, andcorrelation coefficient.
In our current system, ex-pected information gain (IG) and information gainratio (IGR) are used.As an example, consider the following two fea-tures, which can be considered different attempts toformalize the same linguistic property:1.
The noun phrase has a subject role and is def-inite (e.g.
markable begins with a definite arti-cle)2.
The noun phrase has a subject role and is notindefinite (e.g.
markable begins with an indefi-nite article)The information gain ratios of the above men-tioned features are equal to 0.0026 for the first and0.0051 for the second one ?
this shows that the sec-ond one is a better choice.
We now define IG andIGR.The change in entropy from a prior state to a statethat takes some information is the expected informa-tion gain (Mitchell, 1997):IG (f) = H (C) ?
Hf (C) (1)Where f is the feature value, C its correspondingclass, and entropy is defined as follows:H (C) = ?
?iP (Ci) log2P (Ci) (2)Hf (C) =?f|Cf ||C| H (Cf ) (3)If a feature takes a large number of distinct values,the information gain would not be a good measurefor deciding its relevance.
In such cases the infor-mation gain ratio is used instead.
The informationgain ratio for a feature is calculated as follows:IGR (f) = IG (f)SInf (C) (4)SInf(C) = ?
?i|Ci||C| log2|Ci||C| (5)Equation (4) can be used as an indicator for whichfeatures are likely to improve classification accu-racy.3 System ArchitectureThe architecture of the system has two main parts:preprocessing and coreference resolution.In preprocessing the text corpus is converted toa relational data model.
The main purpose of therelational model in our system is the use of a fea-ture definition language (Kobdani et al, 2010).
Af-ter modeling the text corpus, coreference resolutioncan be performed.The main steps of the system are presented as fol-lows.3.1 Preliminary text conversionIn this step, tokens are extracted from the corpus.
Inthe CoNLL-2011 Shared Task this step is as simpleas reading each line of the input data set and extract-ing its corresponding token.3.2 Atomic attributes of tokensAtomic features of the tokens are extractedin this step.
The extracted atomic featuresare: part of speech, number, pronoun person(first, second and third), pronoun type (subjec-tive,,predeterminer,reflexive,objective and posses-sive), WordNet semantic class and gender.We use a rather simple method to extract semanticclass of each token from WordNet.
We look at thesynonyms of the token and if one of them is in thepredefined keyword set, we take it as its correspond-ing semantic class.
The example of the keywordsare person, time, abstraction, device, human action,organization, place and animal.723.3 Markable DetectionIn this step all noun phrases from the parse tree areextracted.
After clustering step all markables whichare not included in a chain are deleted from the listof markables.
In other word we will not have anycluster with less than 2 members.Figure 1 presents the simple markable detectionmethod which we used in the SUCRE.3.4 Atomic attributes of markablesIn this step, the atomic attributes of the markablesare extracted.
In the data set of the CoNLL-2011shared task the named entity property of a markablecan be used as its atomic attribute.3.5 Link GeneratorFor training, the system generates a positive train-ing instance for an adjacent coreferent markable pair(m, n) and negative training instances for the mark-able m and all markables disreferent with m that oc-cur before n (Soon et al, 2001).
For decoding itgenerates all the possible links inside a window of100 markables.3.6 Link feature definition and extractionThe output of the link generator, which is the list ofthe generated links, is the input to the link featureextractor for creating train and test data sets.
To dothis, the feature definitions are used to extract thefeature values of the links (Kobdani et al, 2011).3.7 LearningFor learning we implemented a decision tree classi-fier (Quinlan, 1993).
To achieve state-of-the-art per-formance, in addition to decision tree we also triedsupport vector machine and maximum entropy thatdid not perform better than decision tree.3.8 Classification and ClusteringIn this part, the links inside one document are clas-sified then the coreference chains are created.
Weuse best-first clustering for this purpose.
It searchesfor the best predicted antecedent from right to leftstarting from the end of the document.
For the docu-ments with more than a predefined number of mark-ables we apply a limit for searching.
In this way, inaddition to better efficiency, the results also improve.Markable Detection PSG A (W1, W2, .
.
.
, Wn)1.
A markable M is presented by a set ofthree words:Begin (Mb), End (Me) and Head (Mh).2.
Let DM be the set of detected markables.3.
Let Ti be the node i in the parse tree withlabel Li(if node is a word then Li is equal to Wi).4.
Start from parse tree root Tr:Find Markables(Tr,Lr,DM )Find Markables(T ,L,DM )1.
If L is equal to noun phrase, then extractthe markable M :(a) Set the begin word of the markable:Mb = Noun Phrase Begin(T ,L)(b) Set the end word of the markable:Me = Noun Phrase End(T ,L)(c) Set the head word of the markable:Mh = Noun Phrase Head(T ,L)(d) Add the markable M to the set of de-tected markables DM .2.
Repeat for all Ti the daughters of T :Find Markables(Ti,Li,DM )Noun Phrase Begin(T ,L)If T has no daughter then return L;else set Tb to the first daughter of T and returnNoun Phrase Begin(Tb,Lb).Noun Phrase End(T ,L)If T has no daughter then return L;else set Tb to the last daughter of T and returnNoun Phrase End(Tb,Lb).Noun Phrase Head(T ,L)If T has no daughter then return L;else set Th to the biggest noun phrase daughterof T and return Noun Phrase Head(Th,Lh).Figure 1: Markable Detection from Parse Tree (all possi-ble markables) .73Automatic GoldRec.
Prec.
F1 Rec.
Prec.
F1MD 60.17 60.92 60.55 62.50 61.62 62.06MUC 54.30 51.84 53.06 57.44 53.15 55.21B3 71.39 64.68 67.87 74.07 64.39 68.89CEAFM 46.36 46.36 46.36 47.07 47.07 47.07CEAFE 35.38 37.26 35.30 35.19 38.44 36.74BLANC 65.01 64.93 64.97 66.23 65.16 65.67Table 1: Results of SUCRE on the development data setfor the automatically detected markables.
MD: MarkableDetection.4 Data SetsOntoNotes has been used for the CoNLL-2011shared task.
The OntoNotes project 1 is to providea large-scale, accurate corpus for general anaphoriccoreference.
It aims to cover entities and events (i.e.it is not limited to noun phrases or a limited set ofentity types) (Pradhan et al, 2007).For training we used 4674 documents containinga total of 1909175 tokens, 190700 markables and50612 chains.SUCRE participated in the closed track of theshared task.
Experiments have been performed forthe two kind of documents, namely, the automati-cally preprocessed documents and the gold prepro-cessed documents.
In this paper, we report only thescores on the development data set using the offi-cial scorer of the shared task.
The automaticallypreprocessed part consists of 303 documents con-taining a total of 136257 tokens, 52189 automati-cally detected markables, 14291 true markables and3752 chains.
The gold preprocessed part consists of303 documents containing a total of 136257 tokens,52262 automatically detected markables, 13789 truemarkables and 3752 chains.5 ResultsWe report recall, precision, and F1 for MUC (Vi-lain et al, 1995), B3 (Bagga and Baldwin, 1998),CEAFM /CEAFE (Luo, 2005) and BLANC (Re-casens et al, 2010).Table 1 presents results of our system for theautomatically detected markables.
It is apparentfrom this table that the application of the gold pre-processed documents slightly improves the perfor-mance (MD-F1: +1.51; MUC-F1: +2.15; B3-F1:1http://www.bbn.com/ontonotes/Automatic GoldRec.
Prec.
F1 Rec.
Prec.
F1MUC 58.63 87.88 70.34 60.48 88.25 71.78B3 57.91 86.47 69.36 59.21 86.25 70.22CEAFM 59.81 59.81 59.81 60.91 60.91 60.91CEAFE 70.49 36.43 48.04 71.09 37.73 49.30BLANC 69.67 76.27 72.34 70.34 76.01 72.71Table 2: Results of SUCRE on the development data setfor the true markables (i.e.
no singletone is included).+1.02; CEAFM -F1: +0.71; CEAFE-F1: +1.44;BLANC-F1: +0.70 ).Table 2 presents results of our system for the truemarkables that were all and only part of coreferencechains.
Again the results show that the applicationof gold preprocessed documents slightly improvesthe performance (MUC-F1: +1.44; B3-F1: +0.86;CEAFM -F1: +1.1; CEAFE-F1: +1.26; BLANC-F1:+0.37 ).Comparing the results of tables 1 and 2, there is asignificant difference between the scores on the au-tomatically detected markables and the scores on thetrue markables (e.g.
for the automatically prepro-cessed documents: MUC-F1: +17.28; CEAFM -F1:+13.45; CEAFE-F1: +12.74; BLANC-F1: +7.37).No significant improvement in B3 is seen (auto-matic: +1.49; gold: +1.33).
We suspect that this ispartly due to the very sensitive nature of B3 againstthe singleton chains.
Because in the implementationof scorer for the CoNLL-2011 shared task the non-detected key markables are automatically includedinto the response as singletons.6 ConclusionIn this paper, we have presented our system SUCREparticipated in the CoNLL-2011 shared task.
Wetook a deeper look at the feature engineering of SU-CRE.
We presented the markable detection methodwe applied.We showed that the application of the gold pre-processed documents improves the performance.
Ithas been demonstrated that the availability of thetrue markables significantly improves the results.Also it has been shown that the singletons have alarge impact on the B3 scores.74ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In LREC Workshop onLinguistics Coreference ?98, pages 563?566.Eric Bengtson and Dan Roth.
2008.
Understandingthe value of features for coreference resolution.
InEMNLP ?08, pages 294?303.Hamidreza Kobdani and Hinrich Schu?tze.
2010.
Sucre:A modular system for coreference resolution.
In Se-mEval ?10, pages 92?95.Hamidreza Kobdani, Hinrich Schu?tze, Andre Burkovski,Wiltrud Kessler, and Gunther Heidemann.
2010.
Re-lational feature engineering of natural language pro-cessing.
In CIKM ?10.
ACM.Hamidreza Kobdani, Hinrich Schu?tze, MichaelSchiehlen, and Hans Kamp.
2011.
Bootstrap-ping coreference resolution using word associations.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics, ACL ?11.Association for Computational Linguistics.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In HLT ?05, pages 25?32.Tom M. Mitchell.
1997.
Machine Learning.
McGraw-Hill, New York.Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Unre-stricted Coreference: Identifying Entities and Eventsin OntoNotes.
In in Proceedings of the IEEE Inter-national Conference on Semantic Computing (ICSC),September 17-19.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
Conll-2011 shared task: Modeling unrestrictedcoreference in ontonotes.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning (CoNLL 2011), Portland, Oregon,June.J.
Ross Quinlan.
1993.
C4.5: Programs for machinelearning.
Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA.Marta Recasens, Llu?
?s Ma`rquez, Emili Sapena,M.Anto`nia Mart?
?, Mariona Taule?, Ve?ronique Hoste,Massimo Poesio, and Yannick Versley.
2010.SemEval-2010 Task 1: Coreference resolution inmultiple languages.
In SemEval ?10, pages 70?75.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to coref-erence resolution of noun phrases.
In CL ?01, pages521?544.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In MUC6 ?95,pages 45?52.75
