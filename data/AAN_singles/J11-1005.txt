Syntactic Processing Using the GeneralizedPerceptron and Beam SearchYue Zhang?University of CambridgeStephen Clark?
?University of CambridgeWe study a range of syntactic processing tasks using a general statistical framework that consistsof a global linear model, trained by the generalized perceptron together with a generic beam-search decoder.
We apply the framework to word segmentation, joint segmentation and POS-tagging, dependency parsing, and phrase-structure parsing.
Both components of the frameworkare conceptually and computationally very simple.
The beam-search decoder only requires thesyntactic processing task to be broken into a sequence of decisions, such that, at each stage inthe process, the decoder is able to consider the top-n candidates and generate all possibilitiesfor the next stage.
Once the decoder has been defined, it is applied to the training data, usingtrivial updates according to the generalized perceptron to induce a model.
This simple frameworkperforms surprisingly well, giving accuracy results competitive with the state-of-the-art on allthe tasks we consider.The computational simplicity of the decoder and training algorithm leads to significantlyhigher test speeds and lower training times than their main alternatives, including log-linearand large-margin training algorithms and dynamic-programming for decoding.
Moreover, theframework offers the freedom to define arbitrary features which can make alternative trainingand decoding algorithms prohibitively slow.
We discuss how the general framework is applied toeach of the problems studied in this article, making comparisons with alternative learning anddecoding algorithms.
We also show how the comparability of candidates considered by the beamis an important factor in the performance.
We argue that the conceptual and computational sim-plicity of the framework, together with its language-independent nature, make it a competitivechoice for a range of syntactic processing tasks and one that should be considered for comparisonby developers of alternative approaches.1.
IntroductionIn this article we study a range of syntactic processing tasks using a general frameworkfor structural prediction that consists of the generalized perceptron (Collins 2002) and?
University of Cambridge Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue,Cambridge, UK.
E-mail: yue.zhang@cl.cam.ac.uk.??
University of Cambridge Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue,Cambridge, UK.
E-mail: stephen.clark@cl.cam.ac.uk.Submission received: 10 November 2009; revised submission received: 12 August 2010; accepted forpublication: 20 September 2010.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 1beam-search.
We show that the framework, which is conceptually and computationallysimple, is practically effective for structural prediction problems that can be turned intoan incremental process, allowing accuracies competitive with the state-of-the-art to beachieved for all the problems we consider.The framework is extremely flexible and easily adapted to each task.
One advan-tage of beam-search is that it does not impose any requirements on the structure ofthe problem, for example, the optimal sub-problem property required for dynamic-programming, and can easily accommodate non-local features.
The generalized per-ceptron is equally flexible, relying only on a decoder for each problem and using atrivial online update procedure for each training example.
An advantage of the linearperceptron models we use is that they are global models, assigning a score to a completehypothesis for each problem rather than assigning scores to parts which are then com-bined under statistical independence assumptions.
Here we are following a recent lineof work applying global discriminative models to tagging and wide-coverage parsingproblems (Lafferty, McCallum, and Pereira 2001; Collins 2002; Collins and Roark 2004;McDonald, Crammer, and Pereira 2005; Clark and Curran 2007; Carreras, Collins, andKoo 2008; Finkel, Kleeman, and Manning 2008).The flexibility of our framework leads to competitive accuracies for each of the taskswe consider.
For word segmentation, we show how the framework can accommodatea word-based approach, rather than the standard and more restrictive character-basedtagging approaches.
For POS-tagging, we consider joint segmentation and POS-tagging,showing that a single beam-search decoder can be used to achieve a significant accuracyboost over the pipeline baseline.
For Chinese and English dependency parsing, weshow how both graph-based and transition-based algorithms can be implemented asbeam-search, and then combine the two approaches into a single model which out-performs both in isolation.
Finally, for Chinese phrase-structure parsing, we describe aglobal model for a shift-reduce parsing algorithm, in contrast to current deterministicapproaches which use only local models at each step of the parsing process.
For all thesetasks we present results competitive with the best results in the literature.In Section 2 we describe our general framework of the generic beam-search algo-rithm and the generalized perceptron.
Then in the subsequent sections we describeeach task in turn, based on conference papers including Zhang and Clark (2007, 2008a,2008b, 2009, 2010), presented in our single coherent framework.
We give an updatedset of results, plus a number of additional experiments which probe further into theadvantages and disadvantages of our framework.
For the segmentation task, we alsocompare our beam-search framework with alternative decoding algorithms includingan exact dynamic-programming method, showing that the beam-search method is sig-nificantly faster with comparable accuracy.
For the joint segmentation and POS-taggingtask, we present a novel solution using the framework in this article, and show thatit gives comparable accuracies to our previous work (Zhang and Clark 2008a), whilebeing more than an order of magnitude faster.In Section 7 we provide further discussion of the framework based on the studiesof the individual tasks.
We present the main advantages of the framework, and give ananalysis of the main reasons for the high speeds and accuracies achieved.
We also dis-cuss how this framework can be applied to a potential new task, and show that the com-parability of candidates in the incremental process is an important factor to consider.In summary, we study a general framework for incremental structural prediction,showing how the framework can be tailored to a range of syntactic processing problemsto produce results competitive with the state-of-the-art.
The conceptual and compu-tational simplicity of the framework, together with its language-independent nature,106Zhang and Clark Syntactic Processingmake it a competitive choice that should be considered for comparison by developersof alternative approaches.2.
The Decoding and Training FrameworkThe framework we study in this article addresses the general structural predictionproblem of mapping an input structure x ?
X onto an output structure y ?
Y, whereX is the set of possible inputs, and Y is the set of possible outputs.
For example, forthe problem of Chinese word segmentation, X is the set of raw Chinese sentences andY is the set of all possible segmented Chinese sentences.
For the problem of Englishdependency parsing, X is the set of all English sentences and Y is the set of all possibleEnglish dependency trees.Given an input sentence x, the output F(x) is defined as the highest scored amongthe possible output structures for x:F(x) = arg maxy?GEN(x)Score(y) (1)where GEN(x) denotes the set of possible outputs for an input sentence x, and Score(y)is some real-valued function on Y.To compute Score(y), the output structure y is mapped into a global feature vector?
(y) ?
N d. Here a feature is a count of the occurrences of a certain pattern in an outputstructure, extracted according to a set of feature templates, and d is the total number offeatures.
The term global feature vector is used by Collins (2002) to distinguish betweenfeature counts for whole sequences and the local feature vectors in maximum entropytagging models, which are boolean-valued vectors containing the indicator features forone element in the sequence (Ratnaparkhi 1998).
Having defined the feature vector,Score(y) is computed using a linear model:Score(y) = ?
(y) ?
~w (2)where ~w ?
Rd is the parameter vector of the model, the value of which is defined bysupervised learning using the generalized perceptron.For the general framework we study in this article, the output y is required to bebuilt through an incremental process.
Suppose that K incremental steps are taken intotal to build y.
The incremental change at the ith step (0 < i ?
K) can be written as?
(y, i).
For word segmentation, ?
(y, i) can be an additional character added to the output;for shift-reduce parsing, ?
(y, i) can be an additional shift-reduce action.
Denoting thechange to the global feature vector at the incremental step as ?(?
(y, i)), the global featurevector ?
(y) can be written as ?
(y) =?Ki=1 ?(?
(y, i)).
Hence, Score(y) can be computedincrementally byScore(y) =K?i=1?(?
(y, i)) ?
~w (3)In the following sections, we describe the two major components of the gen-eral framework, that is, the general beam-search algorithm for finding F(x) =arg maxy?GEN(x) Score(y) for a given x, and the generalized perceptron for training ~w.107Computational Linguistics Volume 37, Number 12.1 Beam-Search DecodingGiven an input x, the output structure y is built incrementally.
At each step, an incre-mental sub-structure is added to the partially built output.
Due to structural ambiguity,different sub-structures can be built.
Taking POS-tagging for example, the incrementalsub-structure for each processing step can be a POS-tag assigned to the next input word.Due to structural ambiguity, different POS-tags can be assigned to a word, and thedecoding algorithm searches for the particular path of incremental steps which buildsthe highest scored output.We present a generic beam-search algorithm for our decoding framework, whichuses an agenda to keep the B-best partial outputs at each incremental step.
The partiallybuilt structures, together with useful additional information, are represented as a set ofstate items.
Additional information in a state item is used by the decoder to organizethe current structures or keep a record of the incremental process.
For POS-taggingit includes the remaining input words yet to be assigned POS-tags; for a shift-reduceparser, it includes the stack structure for the shift-reduce process and the incomingqueue of unanalyzed words.The agenda is initialized as empty, and the state item that corresponds to the initialstructure is put onto it before decoding starts.
At each step during decoding, each stateitem from the agenda is extended with one incremental step.
When there are multiplechoices to extend one state item, multiple new state items are generated.
The new stateitems generated at a particular step are ranked by their scores, and the B-best are putback onto the agenda.
The process iterates until a stopping criterion is met, and thecurrent best item from the agenda is taken as the output.Pseudo code for the generic beam-search algorithm is given in Figure 1, where thevariable problem represents a particular task, such as word segmentation or dependencyparsing, and the variable candidate represents a state item, which has a different defini-tion for each task.
For example, for the segmentation task, a candidate is a pair, consistingof the partially segmented sentence and the remaining character sequence yet to besegmented.
The agenda is an ordered list, used to keep all the state items generated ateach stage, ordered by score.
The variable candidates is the set of state items that can beused to generate new state items, that is, the B-best state items from the previous stage.B is the number of state items retained at each stage.function BEAM-SEARCH(problem, agenda, candidates, B)candidates ?
{STARTITEM(problem)}agenda ?
CLEAR(agenda)loop dofor each candidate in candidatesagenda ?
INSERT(EXPAND(candidate, problem), agenda)best ?
TOP(agenda)if GOALTEST(problem, best)then return bestcandidates ?
TOP-B(agenda, B)agenda ?
CLEAR(agenda)Figure 1The generic beam-search algorithm.108Zhang and Clark Syntactic ProcessingSTARTITEM initializes the start state item according to the problem; for example,for the segmentation task, the start state item is a pair consisting of an empty seg-mented sentence and the complete sequence of characters waiting to be segmented.CLEAR removes all items from the agenda.
INSERT puts one or more state items ontothe agenda.
EXPAND represents an incremental processing step, which takes a stateitem and generates new state items from it in all possible ways; for example, for thesegmentation task, EXPAND takes the partially segmented sentence in a state item,and extends it in all possible ways using the first character in the remaining charactersequence in the state item.
TOP returns the highest scoring state item on the agenda.GOALTEST checks whether the incremental decoding process is completed; for example,for the segmentation task, the process is completed if the state item consists of a fullysegmented sentence and an empty remaining character sequence.
TOP-B returns the B-highest scoring state items on the agenda, which are used for the next incremental step.State items in the agenda are ranked by their scores.
Suppose that K incrementalsteps are taken in total to build an output y.
At the ith step (0 < i ?
K), a state item inthe agenda can be written as candidatei, and we haveScore(candidatei) =i?n=1?(?
(candidatei, n)) ?
~wFeatures for a state item can be based on both the partially built structure and theadditional information we mentioned earlier.The score of a state item can be computed incrementally as the item is built.
Thescore of the start item is 0.
At the ith step (0 < i ?
K), a state item candidatei is generatedby extending an existing state item candidatei?1 on the agenda with ?
(candidatei, i).
Inthis case, we haveScore(candidatei) = Score(candidatei?1) +?(?
(candidatei, i)) ?
~wTherefore, when a state item is extended, its score can be updated by adding theincremental score of the step ?(?
(candidatei, i)) ?
~w.
The nature of the scoring functionmeans that, given appropriately defined features, it can be computed efficiently for boththe incremental decoding and training processes.Because the correct item can fall out of the agenda during the decoding process, thegeneral beam-search framework is an approximate decoding algorithm.
Nevertheless,empirically this algorithm gives competitive results on all the problems in this article.2.2 The Generalized PerceptronThe perceptron learning algorithm is a supervised training algorithm.
It initializes theparameter vector as all zeros, and updates the vector by decoding the training examples.For each example, the output structure produced by the decoder is compared withthe correct structure.
If the output is correct, no update is performed.
If the outputis incorrect, the parameter vector is updated by adding the global feature vector ofthe training example and subtracting the global feature vector of the decoder output.Intuitively, the training process is effectively coercing the decoder to produce the correctoutput for each training example.
The algorithm can perform multiple passes over thesame training sentences.
In all experiments, we decide the number of training iterationsusing a set of development test data, by choosing the number that gives the highest109Computational Linguistics Volume 37, Number 1Inputs: training examples (xi, yi)Initialization: set ~w = 0Algorithm:for r = 1..P, i = 1..Ncalculate zi = decode(xi)if zi 6= yi~w = ~w +?
(yi) ?
?
(zi)Outputs: ~wFigure 2The generalized perceptron algorithm, adapted from Collins (2002).development test accuracy as the final number in testing.
Figure 2 gives the algorithm,where N is the number of training sentences and P is the number of passes over the data.The averaged perceptron algorithm (Collins 2002) is a standard way of reducingoverfitting on the training data.
It was motivated by the voted-perceptron algorithm(Freund and Schapire 1999) and has been shown to give improved accuracy over thenon-averaged perceptron on a number of tasks.
Let N be the number of training sen-tences, P the number of training iterations, and ~wi,r the parameter vector immediatelyafter the ith sentence in the rth iteration.
The averaged parameter vector ~?
?
Rd isdefined as~?
= 1PN?i=1..N,r=1..P~wi,rand it is used instead of ~w as the model parameters.
We use the averaged perceptron forall the tasks we consider.We also use the early-update strategy of Collins and Roark (2004), which is amodified version of the perceptron algorithm specifically for incremental decodingusing beam search.
At any step during the decoding process to calculate zi, if all partialcandidates in the agenda are incorrect, decoding is stopped and the parameter vector isupdated according to the current best candidate in the agenda and the correspondinggold-standard partial output.
To perform early-update, the decoder needs to keep aversion of the correct partial output for each incremental step, so that the parametervalues are adjusted as soon as the beam loses track of the correct state item.
The intuitionis to force the beam to keep the correct state item at every incremental step, rather thanlearning only the correct overall structure.
This strategy has been shown to improve theaccuracy over the original perceptron for beam-search decoding.In summary, our general framework consists of a global linear model, which istrained by the averaged perceptron, and a beam-search decoder.
When applied to aparticular task, the structure of a state item as well as some of the functions in thedecoder need to be instantiated.
In the following sections, we show how the generalframework can be applied to Chinese word segmentation, joint segmentation and POS-tagging, Chinese and English dependency parsing, and Chinese constituent parsing.3.
Word SegmentationChinese word segmentation (CWS) is the problem of finding word boundaries forChinese sentences, which are written as continuous character sequences.
Other lan-guages, including Japanese and Thai, also have the problem of word segmentation, andtypical statistical models for CWS can also be applied to them.110Zhang and Clark Syntactic ProcessingWord segmentation is a problem of ambiguity resolution, often requiring knowl-edge from a variety of sources.
Out-of-vocabulary (OOV) words are a major source ofambiguity.
For example, a difficult case occurs when an OOV word consists of characterswhich have themselves been seen as words; here an automatic segmentor may split theOOV word into individual single-character words.
Typical examples of unseen wordsinclude Chinese names, translated foreign names, and idioms.The segmentation of known words can also be ambiguous.
For example,should be (here) (flour) in the sentence (flour and rice are ex-pensive here) or (here) (inside) in the sentence (it?s cold insidehere).
The ambiguity can be resolved with information about the neighboring words.
Incomparison, for the sentence , possible segmentations include (thediscussion) (will) (very) (be successful) and (the discussion meeting)(very) (be successful).
The ambiguity can only be resolved with contextual infor-mation outside the sentence.
Human readers often use semantics, contextual informa-tion about the document, and world knowledge to resolve segmentation ambiguities.There is no fixed standard for Chinese word segmentation.
Experiments haveshown that there is only about 75% agreement among native speakers regarding thecorrect word segmentation (Sproat et al 1996).
Also, specific NLP tasks may requiredifferent segmentation criteria.
For example, could be treated as a singleword (Bank of Beijing) for machine translation, although it is more naturally segmentedinto (Beijing) (bank) for tasks such as text-to-speech synthesis.
Therefore,supervised learning with specifically defined training data has become the dominantapproach.Following Xue (2003), the standard approach for building a statistical CWS modelis to treat CWS as a sequence labeling task.
A tag is assigned to each character in theinput sentence, indicating whether the character is a single-character word or the start,middle, or end of a multi-character word.
The context for disambiguation is normally afive-character window with the current character in the middle.
We call these methodscharacter-based word segmentation.
The advantage of character-based segmentation isthat well-known tagging approaches can be applied directly to the CWS problem.There are various character-based models in the literature.
They differ mainly in thelearning algorithm and the features used.
Several discriminative learning algorithmshave been applied to the character-based systems.
Examples include Xue (2003), Peng,Feng, and McCallum (2004), and Wang et al (2006), which use maximum entropy andconditional random field models, and Jiang et al (2008), which uses the perceptronmodel.
The standard feature set is that defined by Ng and Low (2004), though otherfeature sets are reported to improve the accuracy (Zhao, Huang, and Li 2006).
Zhao,Huang, and Li (2006) also showed that the best accuracy for conditional random field(CRF) models is given by using a set of six character segmentation tags, rather thanthe standard set {beginning, middle, end, single} shown previously.
Standard searchalgorithms for sequence tagging have been applied to the decoding process, such asthe dynamic-programming algorithm and beam-search.A disadvantage of character-based models is the use of limited contextual infor-mation.
For these methods, context is confined to the neighboring characters.
Othercontextual information, in particular the surrounding words, is not included.
Considerthe sentence , which can be from (among which) (foreign)(companies), or (in China) (foreign companies) (business).
Note that thefive-character window surrounding is the same in both cases, making the taggingdecision for that character difficult given the local window.
The correct decision can bemade, however, by comparing the two three-word windows containing this character.111Computational Linguistics Volume 37, Number 1In Zhang and Clark (2007) we proposed a word-based approach to segmentation,which provides a direct solution to the problem.
In comparison with the character-basedapproach, our segmentor does not map the CWS problem into sequence labeling.
Byusing a global linear model, it addresses the segmentation problem directly, extractingword-based features from the output segmented structure.
Hence we call our wordsegmentation model the word-based approach.
In fact, word-based segmentors can beseen as a generalization of character-based segmentors, because any character-basedfeatures can be defined in a word-based model.In the following sections, we describe a word-based segmentor using the generalframework of this article, which is slightly different from the original system we pro-posed in Zhang and Clark (2007).
We compare the accuracies of this segmentor andthe 2007 segmentor, and report a set of improved results for our 2007 segmentor usinga better method to optimize the number of training iterations.
We then study alterna-tive decoders to the general framework, including a Viterbi inference algorithm and amultiple-beam search algorithm, and provide discussion on the general framework andword-based segmentation.3.1 Instantiating the General FrameworkIn this section we formulate our word-based segmentor as an instance of the generalframework of this article.
Our segmentor builds a candidate segmentation incremen-tally, one character at a time.
When each character is processed, it is either combinedwith the last word of the partial candidate that has been built so far, or added to thecandidate as the start of a new word.
The same process repeats for each input character,and therefore runs in linear time.For ambiguity resolution, we use a beam-search decoding algorithm to explore thesearch space.
Initially containing only an empty sentence, an agenda is used to keep aset of candidate items for each processing step.
When an input character is processed,it is combined with each candidate in the agenda in the two aforementioned ways, andtwo new candidates are generated.
At the end of each step, the B-best newly generatedcandidates are kept in the agenda for the next processing step.
When the input sentenceis exhausted, the top candidate from the agenda is taken as the output.This decoding process can be expressed as an instance of the generic algorithm inFigure 1.
For the word segmentation problem, a state item in the algorithm is a pair?S, Q?, where S contains part of the input that has been segmented, and Q containsthe rest of the input sentence as a queue of incoming characters.
The initial state itemSTARTITEM(word segmentation) contains an empty sentence, and an incoming queue ofthe whole input sentence.
EXPAND(candidate, word segmentation) pops the first characterfrom the incoming queue, and adds it to the partial segmented sentence in candidate intwo different ways to generate two new state items: It either appends the character tothe last word in the sentence or joins it as the start of a new word in the sentence.
Finally,GOALTEST(word segmentation, best) returns true if best contains a fully segmented inputsentence, and therefore an empty incoming queue, and false otherwise.The score of a segmented sentence is computed by the global linear model inEquation (2), where the parameter vector ~w for the model is computed by the early-update version of the perceptron training algorithm described in Section 2.2.
Our wordsegmentor computes the global feature vector ?
(y) incrementally according to Equa-tion (3), where for the ith character, ?(?
(y, i)) is computed using the feature templatesin Table 1, according to whether the character is appended to or separated from itsprevious character.112Zhang and Clark Syntactic ProcessingTable 1Feature templates for the word segmentor.Feature template When c0 is1 w?1 separated2 w?1w?2 separated3 w?1, where len(w?1) = 1 separated4 start(w?1)len(w?1) separated5 end(w?1)len(w?1) separated6 end(w?1)c0 separated7 c?1c0 appended8 begin(w?1)end(w?1) separated9 w?1c0 separated10 end(w?2)w?1 separated11 start(w?1)c0 separated12 end(w?2)end(w?1) separated13 w?2len(w?1) separated14 len(w?2)w?1 separatedw = word; c = character.
The index of the current character is 0.3.2 Comparisons with Zhang and Clark (2007)Both the segmentor of this article and our segmentor of Zhang and Clark (2007) usea global linear model trained discriminatively using the perceptron.
However, whencomparing state items in the agenda, our 2007 segmentor treated full words in thesame way as partial words, scoring them using the same feature templates.
This scoringmechanism can potentially have a negative effect on the accuracy.
In this article, we takea different strategy and apply full-word feature templates only when the next inputcharacter is separated from the word.
In fact, most of the feature templates in Table 1are related to full word information, and are applied when separating the next character.This method thus gives a clear separation of partial word and full word information.
Wealso applied early-update in this article, so that the training process is closely coupledwith beam-search decoding.
In Zhang and Clark (2007) we performed the standardglobal discriminative learning.3.3 Combining Word-Based and Character-Based SegmentationAs stated earlier, a character-based segmentor maps the segmentation problem intoa sequence labeling problem, where labels are assigned to each input character torepresent its segmentation.
Our word-based approach does not map the segmentationproblem into a labeling task but solves it directly.
In this article, we further show that theflexibility of the word-based approach, enabled by our general framework, allows thecombination of a character-based sub-system into our word-based system.
The intuitionis simple: Both perceptron learning and beam-search decoding allow arbitrary features,and therefore features from a typical character-based system can be incorporated intoour segmentor to provide further information.
Though character-based segmentors canalso leverage word-level features indirectly, the labeling nature prevents them fromdirect use of word information.We follow the convention of character-based segmentation, and define the set ofsegmentation tags as {B, E, M, S}.
The tags B, E, M represent the character being the113Computational Linguistics Volume 37, Number 1beginning, end, and middle of a multiple-character word, respectively, and the tag Srepresents the character being a single-character word.The character-based features that we incorporate into our segmentor are shownin Table 2, which consist of unigram, bigram, and trigram information in the three-character window surrounding the current character, paired with the segmentation tagof the current character.
To distinguish this system from our system without combina-tion of character-based information, we call our segmentor in Section 3.1 the pure word-based segmentor and the segmentor that uses character-based features the combinedsegmentor in our experimental sections.3.4 ExperimentsWe performed two sets of experiments.
In the first set of experiments, we used theChinese Treebank (CTB) data to study the speed/accuracy tradeoff by varying thesize of the beam.
In the second set of experiments, we used training and testing setsfrom the first and second international Chinese word segmentation bakeoffs (Sproatand Emerson 2003; Emerson 2005) to compare the accuracies to other models in theliterature, including our segmentor of Zhang and Clark (2007).F-score is used as the accuracy measure: 2pr/(p + r), where precision p is the per-centage of words in the decoder output that are segmented correctly, and recall r is thepercentage of gold-standard output words that are correctly segmented by the decoder.CWS systems are evaluated by two types of tests.
The closed tests require that thesystem is trained only with a designated training corpus.
Any extra knowledge is notallowed, including common surnames, Chinese and Arabic numbers, European letters,lexicons, parts-of-speech, semantics, and so on.
The open tests do not impose suchrestrictions.
Open tests measure a model?s capability to utilize extra information anddomain knowledge, which can lead to improved performance, but because this extrainformation is not standardized, direct comparison between open test results is lessinformative.
In this article, we focus only on the closed test.3.4.1 Speed/Accuracy Tradeoff.
We split CTB5 into training, development test, and testsets as shown in Table 3, where the development test data are used to determine thenumber of training iterations, which are used to obtain the final accuracies on the testdata.
We measure the accuracies on the test data with various beam-sizes, and plotthe speed/accuracy tradeoff graph in Figure 3.
Each point in the figure, from right toleft, corresponds to beam size B = 1, 2, 4, 8, 16, 32, and 64, respectively.
Speed is mea-sured in the number of thousand characters per second, and accuracy is calculated usingF-score.As the size of the beam increases, the speed of the segmentor decreases.
Becausea larger part of the search is explored with an increased beam size, the accuracy ofTable 2Feature templates of a typical character-based word segmentor.Feature template When c0 is1 cis0, i ?
{?1, 0, 1} separated, appended2 ci?1cis0, i ?
0, 1 separated, appended3 c?1c0c1s0 separated, appendedc = character; s = segmentation tag.
The index of the current character is 0.114Zhang and Clark Syntactic ProcessingTable 3Training, development, and test data for word segmentation on CTB5.Sections Sentences WordsTraining 1?270, 400?931, 1001?1151 18,085 493,892Dev 301?325 350 6,821Test 271?300 348 8,008the decoder has the potential to increase.
This explains the increased accuracies whenB increases from 1 to 16.
However, the amount of increase drops when the beam sizeincreases.3.4.2 Closed Test on the SIGHAN Bakeoffs.
Four training and testing corpora were used inthe first bakeoff (Sproat and Emerson 2003), including the Academia Sinica Corpus (AS),the Penn Chinese Treebank Corpus (CTB), the Hong Kong City University Corpus (CU),and the Peking University Corpus (PU).
However, because the testing data from thePenn Chinese Treebank Corpus is currently unavailable to us, we excluded this corpusfrom our experiments.
The corpora are encoded in GB (PU, CTB) and BIG5 (AS, CU).In order to test them consistently in our system, they are all converted to UTF8 withoutloss of information.The results are shown in Table 4.
We follow the format from Peng, Feng, andMcCallum (2004), where each row represents a CWS model.
The first three columnsrepresent tests with the AS, CU, and PU corpora, respectively.
The best score in eachcolumn is shown in bold.
The last two columns represent the average accuracy of eachmodel over the tests it participated in (SAV), and our average over the same tests (OAV),respectively.
The first eight rows represent models from Sproat and Emerson (2003) thatparticipated in at least one closed test from the table, row ?Peng?
represents the CRFmodel from Peng, Feng, and McCallum (2004), row ?Zhang 2007?
represents our modelas reported in Zhang and Clark (2007), and the last two rows represent our model inthis article, using only word-based features in Table 1 and combined features in Tables 1plus 2, respectively.In Zhang and Clark (2007) we fixed the number of training iterations to six forall experiments, according to a separate set of development data.
An alternative wayto decide the number of training iterations is to set apart 10% from the training dataFigure 3Speed/accuracy tradeoff of the segmentor.115Computational Linguistics Volume 37, Number 1Table 4The accuracies of various word segmentors over the first SIGHAN bakeoff data.AS CU PU SAV OAVS01 93.8 90.1 95.1 93.0 95.5S04 93.9 93.9 94.8S05 94.2 89.4 91.8 95.9S06 94.5 92.4 92.4 93.1 95.5S08 90.4 93.6 92.0 94.8S09 96.1 94.6 95.4 95.9S10 94.7 94.7 94.8S12 95.9 91.6 93.8 95.9Peng 95.6 92.8 94.1 94.2 95.5Zhang 2007 96.5 94.6 94.0 95.0 95.5Zhang 2007* 96.9 94.6 94.1 95.2 95.5this article pure 97.0 94.6 94.6 95.4 95.5this article combined 96.9 94.8 94.8The best score in each column and the best average in each row is in boldface.
*Zhang 2007 with the (Carreras, Surdeanu, and Marquez 2006) method applied (see text fordetails).as development test data, and use the rest for development training.
For testing, alltraining data are used for training, with the number of training iterations set to be thenumber which gave the highest accuracy during the development experiments.
Thismethod was used by Carreras, Surdeanu, and Marquez (2006) in their parsing model.We apply it to our segmentor model in this article.
Moreover, we also use this methodto decide the number of training iterations for our system of Zhang and Clark (2007),and show the accuracies in row ?Zhang 2007*?.For each row the best average is shown in bold.
We achieved the best accuracy in allthree corpora, and better overall accuracy than all the other models using the methodof this article.
Our new method to decide the number of training iterations also gaveimproved accuracies compared to our 2007 model.
The combination of character-basedfeatures and our original word-based features gave slight improvement in the overallaccuracy.Four training and testing corpora were used in the second bakeoff (Emerson 2005),including the Academia Sinica corpus (AS), the Hong Kong City University Corpus(CU), the Peking University Corpus (PK), and the Microsoft Research Corpus (MR).Different encodings were provided, and the UTF8 data for all four corpora were usedin our experiments.Following the format of Table 4, the results for this bakeoff are shown in Table 5.
Wechose the three models that achieved at least one best score in the closed tests fromEmerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita(2006) for comparison.
Row ?Zh-a?
and ?Zh-b?
represent the pure sub-word CRF modeland the confidence-based combination of the CRF and rule-based models, respectively.Again, our model achieved better overall accuracy than all the other models.
Thecombination of character-based features improved the accuracy slightly again.116Zhang and Clark Syntactic ProcessingTable 5The accuracies of various word segmentors over the second SIGHAN bakeoff data.AS CU PK MR SAV OAVS14 94.7 94.3 95.0 96.4 95.1 95.6S15b 95.2 94.1 94.1 95.8 94.8 95.6S27 94.5 94.0 95.0 96.0 94.9 95.6Zh-a 94.7 94.6 94.5 96.4 95.1 95.6Zh-b 95.1 95.1 95.1 97.1 95.6 95.6Zhang 2007 94.6 95.1 94.5 97.2 95.4 95.6Zhang 2007* 95.0 95.1 94.6 97.3 95.5 95.6This article pure 95.1 95.2 94.4 97.3 95.5 95.6This article combined 95.4 95.1 94.4 97.3The best score in each column and the best average in each row is in boldface.
*Zhang 2007 with the (Carreras, Surdeanu, and Marquez 2006) method applied (see text fordetails).3.5 Alternative Decoding AlgorithmsBesides the general framework of this article, there are various alternative learning anddecoding algorithms for a discriminative linear model applied to the word segmenta-tion problem, using the same feature templates we defined.
In this section, we studytwo alternative decoding algorithms to the beam-search decoder, including a multiple-beam search algorithm, which can be viewed as an alternative decoder specificallydesigned for the word segmentation and joint segmentation and tagging problems, anda dynamic-programming algorithm.
Both algorithms explore a larger part of the searchspace than the single beam-search algorithm, and we compare the accuracy and speedof these algorithms within the generalized perceptron learning framework.3.5.1 A Multiple-Beam Search Decoder.
In Zhang and Clark (2008a) we proposed amultiple-beam decoder for the problem of joint word segmentation and POS-tagging,in which state items only contain complete words.
This algorithm can be naturallyadapted for word segmentation.
Compared with the single-beam decoder, it explores alarger fraction of the search space.
Moreover, the multiple-beam decoder does not havethe problem of comparing partial words with full words in a single agenda, which oursegmentor of Zhang and Clark (2007) has.
We implement this decoder for segmentationand compare its accuracies with our single-beam decoder.Instead of a single agenda, the multiple-beam algorithm keeps an agenda for eachcharacter in the input sentence, recording the best partial candidates ending with thecharacter.
Like the single beam decoder, the input sentence is processed incremen-tally.
However, at each stage, partial sequence candidates are available at all previouscharacters.
Therefore, the decoder can examine all candidate words ending with thecurrent character.
These possible words are combined with the relevant partial candi-dates from the previous agendas to generate new candidates, which are then insertedinto the agenda for the current character.
The output of the decoder is the top candidatein the last agenda, representing the best segmentation for the whole sentence.
The117Computational Linguistics Volume 37, Number 1multiple-beam search decoder explores a larger number of candidate outputs comparedto the single-beam search.
To improve the running speed, a maximum word lengthrecord is kept to limit the length of candidate words.Because the multiple-beam decoder can also be applied to the joint segmentationand POS-tagging problem in Section 4, we describe this algorithm by extending thegeneric beam-search algorithm in Figure 1.
Three modifications are made to the originalalgorithm, shown in Figure 4.
First, the B-best candidates generated in each processingstep are kept in prev topBs.
Here prev topBs can be seen as a list of the candidates in theoriginal search algorithm, with prev topBs[k] containing the current items with size k,and prev topBs[0] containing only the start item.
An extra loop is used to enumerate allstate items in prev topBs for the generation of new state items.
Second, variable k is usedto represent the size of the state items to be generated, and EXPAND generates only stateitems with size k, taking k as an extra parameter.
Third, the B-best newly generated stateitems are appended to the back of prev topBs, without removing previous state items inprev topBs.
The algorithm thereby keeps track of kB state items instead of B at the kthprocessing stage, and explores a larger subset of the exponential search space.The rest of the algorithm is the same as the original algorithm in Figure 1.
To in-stantiate this generic algorithm for word segmentation, STARTITEM(word segmentation)consists of an empty sentence S and a queue Q containing the full input sentence;EXPAND(candidate, k, word segmentation) generates a single new state item by poppingcharacters on the incoming queue from the front until the kth character (k is the index inthe input sentence rather than the queue itself), and appending them as a new wordto candidate; and GOALTEST(word segmentation, best) returns true if best consists of acomplete segmented sentence and an empty incoming queue.As before, the linear model from Section 2 is applied directly to score state items,and the model parameters are trained with the averaged perceptron algorithm.
Thefeatures for a state item are extracted according to the feature templates in Table 1.3.5.2 A Dynamic-Programming Decoder.
Given the feature templates that we define,a dynamic-programming algorithm can be used to explore the whole search spacein cubic time.
The idea is to reduce the search task into overlapping sub-problems.function MULTIPLE-BEAM-SEARCH(problem, agenda, prev topBs, B)prev topBs ?
{{STARTITEM(problem)}}agenda ?
CLEAR(agenda)k ?
0loop dok ?
k + 1for each candidates in prev topBsfor each candidate in candidatesagenda ?
INSERT(EXPAND(candidate, k, problem), agenda)if GOALTEST(problem, agenda)then return TOP(agenda)candidates ?
TOP-B(agenda, B)prev topBs ?
APPEND(prev topBs, candidates)agenda ?
CLEAR(agenda)Figure 4The extended generic beam-search algorithm with multiple beams.118Zhang and Clark Syntactic ProcessingSuppose that the input has n characters; one way to find the highest-scored segmenta-tion is to first find the highest-scored segmentations with the last word being charactersb..n ?
1, where b ?
0..n ?
1, respectively, and then choose the highest-scored one fromthese segmentations.
In order to find the highest-scored segmentation with the lastword being characters b, ..n ?
1, the last word needs to be combined with all differentsegmentations of characters 0..b ?
1 so that the highest scored can be selected.
However,because the largest-range feature templates span only over two words (see Table 1),the highest scored among the segmentations of characters 0..b ?
1 with the last wordbeing characters b?..b ?
1 will also give the highest score when combined with the wordb..n ?
1.
As a result, the highest-scored segmentation with the last word being charac-ters b..n ?
1 can be found as long as the highest-scored segmentations of 0..b ?
1 withthe last word being b?..b ?
1 are found, where b?
?
0..b ?
1.
With the same reasoning, thehighest-scored segmentation of characters 0..b ?
1 with the last word being b?..b ?
1 canbe found by choosing the highest-scored one among the highest-scored segmentationsof 0..b?
?
1 with the last word being b??..b?
?
1, where b??
?
0..b?
?
1.
In this way, thesearch task is reduced recursively into smaller problems, where in the simplest casethe highest-scored segmentation of characters 0..e with the last word being characters0..e(e ?
0..n ?
1) are known.
And the final highest-scored segmentation can be found byincrementally finding the highest-scored segmentations of characters 0..e(e ?
0..n ?
1)with the last word being b..e(b ?
0..e).The pseudo code for this algorithm is shown in Figure 5.
It works by buildingan n by n table chart, where n is the number of characters in the input sentence sent.chart[b, e] records the highest scored segmentation from the beginning to character e,with the last word starting from character b and ending at character e. chart[0, e] can becomputed directly for e = 0..n ?
1, whereas chart[b, e] needs to be built by combing thebest segmentation on sent[0, b ?
1] and sent[b, e], for b > 0.
The final output is the bestamong chart[b, n ?
1], with b = 0..n ?
1.
The reason for recording partial segmentationswith different final words separately (leading to cubic running time) is the word bigramfeature template.
Note that with a larger feature range, exact inference with dynamic-programming can become prohibitively slow.Inputs: raw sentence sent (length n)Variables: an n by n table chart, where chart[b, e] stores the best scored (partial)segmentation of the characters from the begining of the sentence to charactere, with the last word spanning over the characters from b until e;character index b for the start of word;character index e for the end of word;character index p for the start of the previous word.Initialization:for e = 0..n ?
1:chart[0, e] ?
a single word sent[0..e]Algorithm:for e = 0..n ?
1:for b = 1..e:chart[b, e] ?
the highest scored segmentation among those derived by combiningchart[p, b ?
1] with sent[b, e], for p = 0..b ?
1Outputs: the highest scored segmentation among chart[b, n ?
1], for b = 0..n ?
1Figure 5A dynamic-programming algorithm for word segmentation.119Computational Linguistics Volume 37, Number 1Table 6Comparison between three different decoders for word segmentation.Bakeoff 1 Bakeoff 2AS CU PU AS CU PU MS AverageSB F-measure 96.9 94.6 94.1 95.0 95.1 94.6 97.3 95.4SB sent/sec 212 145 202 358 115 177 105 188SB char/sec 3,054 6,846 4,808 5,263 5,333 5,870 4,963 5,162SB # features 4.0M 0.5M 1.5M 3.9M 1.8M 1.5M 2.7M 2.3MMB F-measure 97.0 94.5 94.1 95.0 95.0 94.4 97.3 95.3MB sent/sec 147 7 13 167 7 11 5 51MB char/sec 2,118 331 187 2,455 325 365 236 859MB # features 4.0M 0.6M 1.5M 3.9M 1.8M 1.5M 2.7M 2.3MDP F-measure 97.1 94.6 94.3 95.0 95.0 94.5 97.2 95.4DP sent/sec 131 3 6 142 4 4 2 42DP char/sec 1,887 142 86 2,087 185 133 95 659DP # features 4.0M 0.5M 1.5M 3.9M 1.7M 1.4M 2.7M 2.3M3.5.3 Experiments.
Table 6 shows the comparison between the single-beam (SB), multiple-beam (MB), and dynamic-programming (DP) decoders by F-score and speed.1 Speed ismeasured by the number of sentences (sent) and characters (char) per second (excludingmodel loading time).
We also include the size of the models in each case.
The slightdifference in model size between different methods is due to different numbers ofnegative features generated during training.
The single-beam search algorithm achievedsignificantly higher speed than both the multiple-beam and the dynamic-programmingalgorithms, whereas the multiple-beam search algorithm ran slightly faster than thedynamic-programming algorithm.
Though addressing the comparability issue andexploring a larger number of candidate output segmentations, neither multiple-beamsearch nor dynamic programming gave higher accuracy than the single-beam searchalgorithm overall.
One of the possible reasons is that the perceptron algorithm adjustsits parameters according to the mistakes the decoder makes: Although the single-beammight make more mistakes than the multiple-beam given the same model, it does notnecessarily perform worse with a specifically tailored model.4.
Joint Segmentation and Part-of-Speech TaggingJoint word segmentation and POS-tagging is the problem of solving word segmen-tation and POS-tagging simultaneously.
Traditionally, Chinese word segmentation andPOS-tagging are performed in a pipeline.
The output from the word segmentor is takenas the input for the POS-tagger.
A disadvantage of pipelined segmentation and POS-tagging is that POS-tag information, which is potentially useful for segmentation, isnot used during the segmentation step.
In addition, word segmentation errors arepropagated to the POS-tagger, leading to lower quality of the overall segmented and1 The experiments were performed using the Zhang and Clark (2007) feature set and single-beam decoder,and our new way to decide the number of training iterations in this article.
The single-beam resultscorrespond to ?Zhang 2007*?
in Tables 4 and 5.120Zhang and Clark Syntactic ProcessingPOS-tagged output.
Joint word segmentation and POS-tagging is a method that ad-dresses these problems.
In Zhang and Clark (2008a) we proposed a joint word segmen-tor and POS-tagger using a multiple-beam decoder, and showed that it outperformed apipelined baseline.
We recently showed that comparable accuracies can be achieved by asingle-beam decoder, which runs an order of magnitude faster (Zhang and Clark 2010).In this section, we describe our single-beam system using our general framework, andprovide a detailed comparison with our multiple-beam and baseline systems of Zhangand Clark (2008a).4.1 Instantiating the General FrameworkGiven an input sentence, our joint segmentor and POS-tagger builds an output incre-mentally, one character at a time.
When a character is processed, it is either concatenatedwith the last word in the partially built output, or taken as a new word.
In the lattercase, a POS-tag is assigned to the new word.
When more characters are concatenated toa word, the POS-tag of the word remains unchanged.For the decoding problem, an agenda is used to keep B different candidates ateach incremental step.
Before decoding starts, the agenda is initialized with an emptysentence.
When a character is processed, existing candidates are removed from theagenda and extended with the current character in all possible ways, and the B-bestnewly generated candidates are put back onto the agenda.
After all the input charactershave been processed, the highest-scored candidate from the agenda is taken as output.Expressed as an instance of the generic algorithm in Figure 1, a state item is a pair?S, Q?, with S being a segmented and tagged sentence and Q being a queue of the next in-coming characters.
STARTITEM(joint tagging) contains an empty sentence and the wholeinput sentence as incoming characters; EXPAND(candidate, joint tagging) pops the firstcharacter from the incoming queue, adds it to candidate, and assigns POS-tags in theaforementioned way to generate a set of new state items; and GOALTEST( joint tagging,best) returns true if best contains a complete segmented and POS-tagged output and anempty queue.The linear model from Section 2 is applied to score state items, differentiatingpartial words from full words in the aforementioned ways, and the model parametersare trained with the averaged perceptron.
The features for a state item are extractedincrementally according to Equation (3), where for the ith character, ?(?
(y, i)) is com-puted according to the feature templates in both Table 1, which are related to wordsegmentation, and Table 7, which are related to POS-tagging.
During training, the early-update method of Collins and Roark (2004), as described in Section 2, is used.
It ensuresthat state items on the beam are highly probable at each incremental step, and is crucialto the high accuracy given by a single-beam.4.2 PruningWe use several pruning methods from Zhang and Clark (2008a), most of which serveto improve the accuracy by removing irrelevant candidates from the beam.
First, thesystem records the maximum number of characters that a word with a particular POS-tag can have.
For example, from the Chinese Treebank that we used for our experiments,most POS are associated only with one- or two-character words.
The only POS-tags thatare seen with words over ten characters long are NN (noun), NR (proper noun), andCD (numbers).
The maximum word length information is initialized as all ones, andupdated according to each training example before it is processed.121Computational Linguistics Volume 37, Number 1Table 7POS feature templates for the joint segmentor and POS-tagger.Feature template when c0 is1 w?1t?1 separated2 t?1t0 separated3 t?2t?1t0 separated4 w?1t0 separated5 t?2w?1 separated6 w?1t?1end(w?2) separated7 w?1t?1c0 separated8 c?2c?1c0t?1, where len(w?1) = 1 separated9 c0t0 separated10 t?1start(w?1) separated11 t0c0 separated or appended12 c0t0start(w0) appended13 ct?1end(w?1), where c ?
w?1 and c 6= end(w?1) separated14 c0t0cat(start(w0)) separated15 ct?1cat(end(w?1)), where c ?
w?1 and c 6= end(w?1) appended16 c0t0c?1t?1 separated17 c0t0c?1 appendedw = word; c = character; t = POS-tag.
The index of the current character is 0.Second, a tag dictionary is used to record POS-tags associated with each word.During decoding, frequent words and words with ?closed set?
tags2 are only assignedPOS-tags according to the tag dictionary, while other words are assigned every POS-tag to make candidate outputs.
Whether a word is a frequent word is decided by thenumber of times it has been seen in the training process.
Denoting the number of timesthe most frequent word has been seen by M, a word is a frequent word if it has beenseen more than M/5, 000 + 5 times.
The threshold value is taken from Zhang and Clark(2008a), and we did not adjust it during development.
Word frequencies are initializedas zeros and updated according to each training example before it is processed; thetag dictionary is initialized as empty and updated according to each training examplebefore it is processed.Third, we make an additional record of the initial characters for words with ?closedset?
tags.
During decoding, when the current character is added as the start of a newword, ?closed set?
tags are only assigned to the word if it is consistent with the record.This type of pruning is used in addition to the tag dictionary to prune invalid partialwords, while the tag dictionary is used to prune complete words.
The record for initialcharacter and POS is initially empty, and updated according to each training examplebefore it is processed.Finally, at any decoding step, we group partial candidates that are generated byseparating the current character as the start of a new word by the signature p0p?1w?1,and keep only the best among those having the same p0p?1w?1.
The signature p0p?1w?1is decided by the feature templates we use: it can be shown that if two candidates cand1and cand2 generated at the same step have the same signature, and the score of cand1is higher than the score of cand2, then at any future step, the highest scored candidate2 ?Closed set?
tags are the set of POS-tags which are only associated with a fixed set of words, according tothe Penn Chinese Treebank specifications (Xia 2000).122Zhang and Clark Syntactic Processinggenerated from cand1 will always have a higher score than the highest scored candidategenerated from cand2.From these four pruning methods, only the third was not used by our multiple-beam system (Zhang and Clark 2008a).
This was designed to help keep likely partialwords in the agenda and improve the accuracy, and does not give our system a speedadvantage over our multiple-beam system.4.3 Comparison with Multiple-Beam Search (Zhang and Clark 2008a)Our system of Zhang and Clark (2008a) was based on the perceptron and a multiple-beam decoder.
That decoder can be seen as a slower alternative of our decoder in this ar-ticle, but one which explores a larger part of the search space.
The comparison betweenour joint segmentation and tagging systems of Zhang and Clark (2008a) and this articleis similar to the comparison between our segmentors in sections 3.5.1 and 3.1.
In Zhangand Clark (2008a), we argued that the straightforward implementation of the single-beam decoder cannot give competitive accuracies to the multiple-beam decoder, andthe main difficulties for a single-beam decoder are in the representing of partial words,and the handling of an exponentially large combined search space using one beam.
Inthis section, we give a description of our system of Zhang and Clark (2008a), and discussthe reason we can achieve competitive accuracies using a single beam in this article.4.3.1 The Multiple-Beam System of Zhang and Clark (2008a).
The decoder of our multiple-beam system can be formulated as an instance of the multiple-beam decoder describedin Section 3.5.1.Similar to the multiple-beam search decoder for word segmentation, the decodercompares candidates only with complete tagged words, and enables the size of thesearch space to scale with the input size.
A set of state items is kept for each characterto record possible segmented and POS-tagged sentences ending with the character.Just as with the single-beam decoder, the input sentence is processed incrementally.However, when a character is processed, the number of previously built state items isincreased from B to kB, where B is the beam-size and k is the number of characters thathave been processed.
Moreover, partial candidates ending with any previous characterare available.
The decoder thus generates all possible tagged words ending with the cur-rent character, concatenating each with existing partial sentences ending immediatelybefore the word, and putting the resulting sentence onto the agenda.
After the characteris processed, the B-best items in the agenda are kept as the corresponding state itemsfor the character, and the agenda is cleared for the next character.
All input charactersare processed in the same way, and the final output is the best state item for the lastcharacter.To instantiate the generic multiple-beam algorithm in Figure 4 for joint segmenta-tion and POS-tagging, STARTITEM( joint tagging) consists of an empty sentence S anda queue Q containing the full input sentence; EXPAND(candidate, k, joint tagging) popscharacters on the incoming queue from the front until the kth character (k is the indexin the input sentence rather than the queue itself), appending them as a new wordto candidate, and assigning to the new word all possible POS-tags to generate a set ofnew items; and GOALTEST(joint tagging, best) returns true if best consists of a completesegmented and POS-tagged sentence and an empty incoming queue.The linear model from Section 2 is again applied directly to score state items, andthe model parameters are trained with the averaged perceptron algorithm.
The features123Computational Linguistics Volume 37, Number 1for a state item are extracted according to the union of the feature templates for thebaseline segmentor and the baseline POS-tagger.4.3.2 Discussion.
An important problem that we solve for a single-beam decoder for theglobal model is the handling of partial words.
As we pointed out in Zhang and Clark(2008a), it is very difficult to score partial words properly when they are compared withfull words, although such comparison is necessary for incremental decoding with asingle-beam.
To allow comparisons with full words, partial words can either be treatedas full words, or handled differently.We showed in Zhang and Clark (2008a) that a naive single-beam decoder whichtreats partial words in the same way as full words failed to give a competitive ac-curacy.
An important reason for the low accuracy is over-segmentation during beam-search.
Consider the three characters (tap water).
The first two characters do notmake sense when put together as a single word.
Rather, when treated as two single-character words, they can make sense in a sentence such as (please) (self) (come)(take).
Therefore, when using single-beam search to process (tap water), thetwo-character word candidate is likely to have been thrown off the agenda beforethe third character is considered, leading to an unrecoverable segmentation error.This problem is even more severe for a joint segmentor and POS-tagger than fora pure word segmentor, because the POS-tags and POS-tag bigram of and fur-ther supports them being separated when is considered.
The multiple-beam searchdecoder we proposed in Zhang and Clark (2008a) can be seen as a means to ensurethat the three characters always have a chance to be considered as a singleword.
It explores candidate segmentations from the beginning of the sentence until eachcharacter, and avoids the problem of processing partial words by considering only fullwords.
However, because it explores a larger part of the search space than a single-beamdecoder, its time complexity is correspondingly higher.In our single-beam system, we treat partial words differently from full words,so that in the previous example, the decoder can take the first two characters in(tap water) as a partial word, and keep it in the beam before the third character isprocessed.
One challenge is the representation of POS-tags for partial words.
The POS ofa partial word is undefined without the corresponding full word information.
Thougha partial word can make sense with a particular POS-tag when it is treated as a completeword, this POS-tag is not necessarily the POS of the full word which contains the partialword.
Take the three-character sequence as an example.
The first characterrepresents a single-character word ?below?, for which the POS can be LC or VV.
Thefirst two characters represent a two-character word ?rain?, for which the POS canbe VV.
Moreover, all three characters when put together make the word ?rainy day?, forwhich the POS is NN.
As discussed earlier, assigning POS tags to partial words as if theywere full words leads to low accuracy.An obvious solution to this problem is not to assign a POS to a partial word until itbecomes a full word.
However, lack of POS information for partial words makes themless competitive compared to full words in the beam, because the scores of full wordsare further supported by POS and POS n-gram information.
Therefore, not assigning POSto partial words potentially leads to over segmentation.
In our experiments, this methoddid not give comparable accuracies to our multiple-beam system.We take a different approach, and assign a POS-tag to a partial word when itsfirst character is separated from the final character of the previous word.
When morecharacters are appended to a partial word, the POS is not changed.
The idea is to usethe POS of a partial word as the predicted POS of the full word it will become.
Possible124Zhang and Clark Syntactic Processingpredictions are made with the first character of the word, and the likely ones will bekept in the beam for the next processing steps.
For example, with the three characters, we try to keep two partial words (besides full words) in the beam when thefirst word is processed, with the POS being VV and NN, respectively.
The first POSpredicts the two-character word , and the second the three-character word .Now when the second character is processed, we still need to maintain the possible POSNN in the agenda, which predicts the three-character word .We show that the mechanism of predicting the POS at the first character givescompetitive accuracy.
This mechanism can be justified theoretically.
Unlike alphabeticallanguages, each Chinese character represents some specific meanings.
Given a character,it is natural for a human speaker to know immediately what types of words it can start.This allows the knowledge of possible POS-tags of words that a character can start, usinginformation about the character from the training data.
Moreover, the POS of the previ-ous words to the current word are also useful in deciding possible POS for the word.3The mechanism of first-character decision of POS also boosts the efficiency, becausethe enumeration of POS is unnecessary when a character is appended to the end of anexisting word.
As a result, the complexity of each processing step is reduced by halfcompared to a method without POS prediction.Finally, an intuitive way to represent the status of a partial word is using a flagexplicitly, which means an early decision of the segmentation of the next incomingcharacter.
We take a simpler alternative approach, and treat every word as a partialword until the next incoming character is separated from the last character of this word.Before a word is confirmed as a full word, we only apply to it features that represent itscurrent partial status, such as character bigrams, its starting character, its part-of-speech,and so forth.
Full word features, including the first and last characters of a word, areapplied immediately after a word is confirmed as complete.An important component for our proposed system is the training process, whichneeds to ensure that the model scores a partial word with predicted POS properly.
Weapply the general framework and use the averaged perceptron for training, togetherwith the ?early update?
mechanism.4.4 ExperimentsWe performed two sets of experiments, using the Chinese Treebank 4 and 5, respectively.In the first set of experiments, CTB4 was separated into two parts: CTB3 (420K charactersin 150K words/10, 364 sentences) was used for the final 10-fold cross validation, andthe rest (240K characters in 150K words/4, 798 sentences) was used as training and testdata for development.
The second set of experiments was performed to compare withrelevant systems on CTB5 data.The standard F-scores are used to measure both the word segmentation accuracyand the overall segmentation and tagging accuracy, where the overall accuracy isJF = 2pr/(p + r)with the precision p being the percentage of correctly segmented and tagged wordsin the decoder output, and the recall r being the percentage of gold-standard tagged3 The next incoming characters are also a useful source of information for predicting the POS.
However, oursystem achieved competitive accuracy compared to our multiple-beam system without such characterlookahead features.125Computational Linguistics Volume 37, Number 1words that are correctly identified by the decoder.
For direct comparison with Ng andLow (2004), the POS-tagging accuracy is also calculated by the percentage of correct tagson each character.4.4.1 Development Experiments.
Our development data consists of 150K words in 4, 798sentences.
Eighty percent (80%) of the data were randomly chosen as the developmenttraining data, and the rest were used as the development test data.
Our developmenttests were mainly used to decide the size of the beam, the number of training iterations,and to observe the effect of early update.Figure 6 shows the accuracy curves for joint segmentation and POS-tagging by thenumber of training iterations, using different beam sizes.
With the size of the beamincreasing from 1 to 32, the accuracies generally increase, although the amount ofincrease becomes small when the size of the beam becomes 16.
After the tenth iteration,a beam size of 32 does not always give better accuracies than a beam size of 16.
Wetherefore chose 16 as the size of the beam for our system.The testing times for each beam size between 1 and 32 are 7.16 sec, 11.90 sec,18.42 sec, 27.82 sec, 46.77 sec, and 89.21 sec, respectively.
The corresponding speedsin the number of sentences per second are 111.45, 67.06, 43.32, 28.68, 17.06 and 8.95,respectively.Figure 6 also shows that the accuracy increases with an increased number of train-ing iterations, but the amount of increase becomes small after the 25th iteration.
Wechose 29 as the number of iterations to train our system.The effect of early update: We compare the accuracies by early update and normalperceptron training.
In the normal perceptron training case, the system reached the bestperformance at the 22nd iteration, with a segmentation F-score of 90.58% and jointFigure 6The influence of beam-sizes, and the convergence of the perceptron for the joint segmentor andPOS-tagger.126Zhang and Clark Syntactic ProcessingTable 8The accuracies of joint segmentation and POS-tagging by 10-fold cross validation.Baseline (pipeline) Joint single-beam Joint multiple-beam# SF JF JA SF JF JA SF JF JAAv.
95.2 90.3 92.2 95.8 91.4 93.0 95.9 91.3 93.0SF = segmentation F-score; JF = overall segmentation and POS-tagging F-score; JA = taggingaccuracy by character.F-score of 83.38%.
When using early update, the algorithm reached the best accuracyat the 30th training iteration, obtaining a segmentation F-score of 91.14% and a jointF-score of 84.06%.4.4.2 Cross-Validation Results.
Ten-fold cross validation is performed to test the accuracyof the joint word segmentor and POS-tagger, and to make comparisons with existingmodels in the literature.
Following Ng and Low (2004), we partition the sentences inCTB3, ordered by sentence ID, into 10 groups evenly.
In the nth test, the nth group isused as the testing data.Table 8 shows the cross-validation accuracies of the pipeline baseline system andthe joint system using single and multiple beam decoders.
SF, JF and JA representsegmentation F-score, tagging F-score, and tagging accuracy, respectively.
The jointsegmentor and tagger systems outperformed the baseline consistently, while the singlebeam-search decoder in this article gave comparable accuracies to our multiple-beamalgorithm of Zhang and Clark (2008a).Speed comparisons of the three systems using the same 10-fold cross-validation areshown in Table 9.
TE, ML, and SP represents the testing time (seconds), model loadingtime (seconds), and speed (number of sentences per second), respectively.
Speed is cal-culated as number of test sentences divided by the test time (excluding model loading).For the baseline system, test time and model loading time for both the segmentor andthe POS-tagger are recorded.
The joint system using a single beam decoder was over10 times faster than the multiple-beam system, and the baseline system was more thanthree times as fast as the single-beam joint system.
These tests were performed on a MacOSX platform with a 2.13GHz CPU and a gcc 4.0.1 compiler.Table 10 shows the overall accuracies of the baseline and joint systems, and com-pares them to two relevant models in the literature.
The accuracy of each model isTable 9The speeds of joint word segmentation and POS-tagging by 10-fold cross validation.Baseline (pipeline) Joint single-beam Joint multiple-beam# TE (s+p=total) ML (s+p=total) SP TE ML SP TE ML SPAv.
8.8+10.4=19.2 2.9+3.7=6.6 82.2 58.6 12.1 22.4 575.0 9.5 1.9TE = testing time (seconds); ML = model loading time (seconds); SP = speed by the number ofsentences per second, excluding loading time; (s) = segmentation in baseline; (p) = POS-tagging inbaseline.127Computational Linguistics Volume 37, Number 1Table 10The comparison of overall accuracies of various joint segmentor and POS-taggers by 10-foldcross validation using CTB.Model SF JF JABaseline+ (Ng) 95.1 ?
91.7Joint+ (Ng) 95.2 ?
91.9Baseline+* (Shi) 95.85 91.67 ?Joint+* (Shi) 96.05 91.86 ?Baseline (ours) 95.20 90.33 92.17Joint (our multiple-beam) 95.90 91.34 93.02Joint (our single-beam) 95.84 91.37 93.01SF = segmentation F-score; JF = joint segmentation and POS-tagging F-score; JA = tagging accuracyby character.+ Knowledge about special characters.
* Knowledge from semantic net outside CTB.shown in a row, where Ng represents the models from Ng and Low (2004), whichapplies a character tagging approach to perform word segmentation and POS-taggingsimultaneously, and Shi represents the models from Shi and Wang (2007), which is areranking system for segmentation and POS-tagging.
These two models are describedin more detail in the related work section.
Each accuracy measure is shown in a col-umn, including the segmentation F-score (SF), the overall tagging F-score ( JF), and thetagging accuracy by character ( JA).
As can be seen from the table, our joint modelsachieved the largest improvement over the baseline, reducing the segmentation errorby more than 14% and the overall tagging error by over 12%.The overall tagging accuracy of our joint model was comparable to but less thanthe joint model of Shi and Wang (2007).
Despite the higher accuracy improvementfrom the baseline, the joint system did not give higher overall accuracy.
One possi-ble reason is that Shi and Wang (2007) included knowledge about special charactersand semantic knowledge from Web corpora (which may explain the higher baselineaccuracy), whereas our system is completely data-driven.
However, the comparison isindirect because our partitions of the CTB corpus are different.
Shi and Wang (2007) alsochunked the sentences before doing 10-fold cross validation, but used an uneven split.We chose to follow Ng and Low (2004) and split the sentences evenly to facilitate furthercomparison.Compared with Ng and Low (2004), our baseline model gave slightly better accu-racy, consistent with our previous observations about the word segmentors in Section 3.Due to the large accuracy gain from the baseline, our joint model performed muchbetter.In summary, when compared with existing joint word segmentation and POS-tagging systems using cross-validation tests, our proposed model achieved the bestaccuracy boost from the pipelined baseline, and competitive overall accuracy.
Our sys-tem based on the general framework of this article gave comparable accuracies to ourmultiple-beam system in Zhang and Clark (2008a), and a speed that is over an order ofmagnitude higher than the multiple-beam algorithm.4.4.3 Test Results Using CTB5.
We follow Kruengkrai et al (2009) and split the CTB5 intotraining, development testing, and testing sets, as shown in Table 11.
The data are used128Zhang and Clark Syntactic ProcessingTable 11Training, development, and test data from CTB5 for joint word segmentation and POS-tagging.Sections Sentences WordsTraining 1?270, 400?931, 1001?1151 18,085 493,892Dev 301?325 350 6,821Test 271?300 348 8,008to compare the accuracies of our joint system with models in the literature, and to drawthe speed/accuracy tradeoff graph.Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals,symbols, alphabets, and Chinese and other characters.
In the previous experiments, oursystem did not use any knowledge beyond the training data.
To make the comparisonfairer, we included knowledge of English letters and Arabic numbers in this experiment.During both training and decoding, English letters and Arabic numbers are segmentedusing rules, treating consecutive English letters or Arabic numbers as a single word.The results are shown in Table 12, where row N07 refers to the model of Nakagawaand Uchimoto (2007), rows J08a and J08b refer to the models of Jiang et al (2008) andJiang, Mi, and Liu (2008), and row K09 refers to the models of Kruengkrai et al (2009).Columns SF and JF refer to segmentation and joint segmentation and tagging accuracies,respectively.
Our system gave comparable accuracies to these recent works, obtainingthe best (same as the error-driven version of K09) joint F-score.The accuracy/speed tradeoff graphs for the joint segmentor and POS-taggers, to-gether with the baseline pipeline system, are shown in Figure 7.
For each point in eachcurve, the development test data were used to decide the number of training iterations,and then the speed and accuracy were measured using test data.
No character knowl-edge is used in any system.
The baseline curve was drawn with B = 16 for the baselinesegmentor, because the baseline segmentation accuracy did not improve beyond B = 16in our experiments.
Each point in this curve corresponds to a different beam size ofthe baseline POS-tagger, which are 2, 4, 8, 16, 32, 64, 128, and 256, respectively, fromright to left.When the speed is over 2.5 thousand characters per second, the baseline systemperformed better than the joint single-beam and multiple-beam systems, due to thehigher segmentation accuracy brought by the fixed beam segmentor.
However, as theTable 12Accuracy comparisons between various joint segmentors and POS-taggers on CTB5.SF JFK09 (error-driven) 97.87 93.67our system 97.78 93.67Zhang 2008 97.82 93.62K09 (baseline) 97.79 93.60J08a 97.85 93.41J08b 97.74 93.37N07 97.83 93.32SF = segmentation F-score; JF = joint segmentation and POS-tagging F-score.129Computational Linguistics Volume 37, Number 1Figure 7The accuracy/speed tradeoff graph for the joint segmentor and POS-taggers and the two-stagebaseline.tagger beam size further increased, the accuracy of the baseline system did not improve.The highest F-score of the baseline (92.83%) was achieved when the beam size of thebaseline POS-tagger was 32.
In fact, we have shown in Section 3.5 that the accuracyof the baseline segmentor was similar to using a Viterbi decoder when the beam sizewas 16.
This was also true for the baseline POS-tagger, according to our experiments.Therefore, though being the most accurate when the speed is high, the baseline systemreaches the highest F-score at 2.63 thousand characters per second, and cannot furtherimprove the accuracy on this curve.The points in the single-beam curve correspond to beam sizes of 2, 4, 8, 16, and 32,respectively, from right to left.
When the speed is roughly between 0.5 and 2.0 thou-sand characters per second, the single-beam system gave the best F-score.
This isbecause the multiple-beam system did not reach such high speeds, and the baselinesystem could not produce a higher accuracy than 92.83%.
The single-beam joint systemgave the highest accuracy of 93.50% when the beam size was 16 and the speed was1.01 thousand characters per second, and the F-score dropped slightly when the beamfurther increased to 32.The points in the multiple-beam curve correspond to beam sizes of 1, 2, 4, 8, and 16,respectively, from right to left.
The multiple-beam system gave the highest F-score of93.62% when the beam size was 16, but the speed was down to 0.06 thousand sentencesper second.4.5 Related WorkNg and Low (2004) mapped the joint segmentation and POS-tagging task into asingle character sequence tagging problem.
Two types of tags are assigned to eachcharacter to represent its segmentation and POS.
For example, the tag b NN indicates acharacter at the beginning of a noun, and the tag e VV indicates a character at the endof a verb.
Using this method, POS features are allowed to interact with segmentation.Because tagging is restricted to characters, the search space is reduced to O((4T)n),where 4 is the number of segmentation tags and T is the size of the tag set.
Beam-searchdecoding is effective with a small beam-size.
However, the disadvantage of thismodel is the difficulty of incorporating whole word information into POS-tagging.
Forexample, the standard word + POS-tag feature is not applicable.130Zhang and Clark Syntactic ProcessingShi and Wang (2007) introduced POS information into segmentation by reranking.B-best segmentation outputs are passed to a separately-trained POS-tagger, and the bestoutput is selected using the overall POS-segmentation probability score.
In this system,the decoding for word segmentation and POS-tagging are still performed separately, andexact inference for both is possible.
However, the interaction between POS and segmen-tation is restricted by reranking: POS information is used to improve segmentation onlyfor the B segmentor outputs.
In comparison to the two systems described here, our jointsystem does not impose any hard constraints on the interaction between segmentationand POS information.Jiang, Mi, and Liu (2008) proposed a reranking system that builds a pruned wordlattice instead of generating a B-best list by the segmentor.
The advantage of thisreranking method compared to Shi and Wang?s (2007) method is that more ambiguity iskept in the reranking stage.
The reranking algorithm used a similar model to our jointsegmentor and POS-tagger.Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentationand POS tagging using an HMM-based approach.
Word information is used to processknown words, and character information is used for unknown words in a similarway to Ng and Low (2004).
In comparison, our model handles character and wordinformation simultaneously in a single perceptron model.
Recently, Kruengkrai et al(2009) developed this hybrid model further by scoring characters and words in the samemodel.
Their idea is similar to our joint segmentor and POS-tagger in Zhang and Clark(2008a).5.
Dependency ParsingDependency parsing is the problem of producing the syntactic structure for an inputsentence according to dependency grammar.
The output structure of a dependencyparser is called a dependency graph; in this article, only dependency trees are con-sidered.
As can be seen from Figure 8, a dependency tree consists of a set of verticesand directed arcs.
Each arc represents the relationship between a pair of words in thesentence; it points from the head word to its modifier.
For example, in the arc betweenthe word (I) and the word (like), (like) is the head word and (I) is thesubject that modifies (like); in the arc between the word (like) and the word(reading), (like) is the head word and (reading) is the object that modifies(like).
In a dependency tree, there is only one word that does not modify any otherword, and it is called the head word of the sentence.
The other words each modifyexactly one word, and no word can modify itself.A dependency tree is called projective if there are no crossing arcs when thesentence is represented linearly, in word order.
Though almost all languages are non-projective to some degree, the majority of sentences in most languages are projective.
InFigure 8An example Chinese dependency tree.131Computational Linguistics Volume 37, Number 1the CoNLL shared tasks on dependency parsing (Buchholz and Marsi 2006; Nivre et al2007) most data sets contain only 1?2% non-projective arcs, and projective dependencyparsing models can give reasonable accuracy in these tasks (Carreras, Surdeanu, andMarquez 2006).
Although non-projective dependency parsing can be solved directly byusing a different model from projective dependency parsing (McDonald et al 2005), itcan also be solved using a projective parsing model, with the help of a reversible trans-formation procedure between non-projective and projective dependency trees (Nivreet al 2006).
In this article we focus on projective dependency parsing.An unlabeled dependency tree is a dependency tree without dependency labelssuch as Subj and Obj in Figure 8.
The same techniques used by unlabeled dependencyparsers can be applied to labeled dependency parsing.
For example, a shift-reduceunlabeled dependency parser can be extended to perform labeled dependency parsingby splitting a single reduce action into a set of reduce actions each associated with adependency label.
Here we focus on unlabeled dependency parsing.Graph-based (McDonald, Crammer, and Pereira 2005; Carreras, Surdeanu, andMarquez 2006; McDonald and Pereira 2006) and transition-based (Yamada andMatsumoto 2003; Nivre et al 2006) parsing algorithms offer two different approachesto data-driven dependency parsing.
Given an input sentence, a graph-based algorithmfinds the highest scoring parse tree from all possible outputs, scoring each completetree, while a transition-based algorithm builds a parse by a sequence of actions, scoringeach action individually.
Although graph-based and transition-based parsers can bedifferentiated in various ways, we prefer to think in terms of the features used in thetwo approaches as the differentiating factor.In Zhang and Clark (2008b) we proposed a graph-based parser and a transition-based parser using the generalized perceptron and beam-search, showing that beam-search is a competitive choice for both graph-based and transition-based dependencyparsing.
In the same paper we used a single discriminative model to combine graph-based and transition-based parsing, showing that the combined parser outperformsboth graph-based and transition-based parsers individually.
All three parsers can beexpressed by our general framework.
Here we describe the transition-based and com-bined parsers, which share the same decoding process.5.1 Instantiating the General FrameworkOur dependency parser uses the incremental parsing process of MaltParser (Nivreet al 2006), which is characterized by the use of a stack and four transition actions:SHIFT, ARCRIGHT, ARCLEFT, and REDUCE.
An input sentence is formed into a queueof incoming words and processed from left to right.
Initially empty, the stack is usedthroughout the parsing process to store unfinished words, which are the words beforethe current word that may still be linked with the current or a future word.The SHIFT action pops the first word from the queue and pushes it onto the stack.The ARCRIGHT action pops the first word from the queue, adds a dependency linkfrom the stack top to the word (i.e., the stack top becomes the parent of the word), andpushes the word onto the stack.
The ARCLEFT action adds a dependency link from thefirst word on the queue to the stack top, and pops the stack.
The REDUCE action popsthe stack.
Among the four transition actions, SHIFT and ARCRIGHT push a word ontothe stack, and ARCLEFT and REDUCE pop the stack; SHIFT and ARCRIGHT read the nextinput word, and ARCLEFT and ARCRIGHT add a link to the output.The initial state contains an empty stack and the whole input sentence as incomingwords.
The final state contains a stack holding only the head word of the sentence and132Zhang and Clark Syntactic Processingan empty queue, with the input words having all been processed.
The incremental pars-ing process starts from the initial state, and builds a candidate parse tree by repeatedlyapplying one transition action out of the four, until the final state is reached.For the decoding problem, an agenda is used to find the output parse tree fromdifferent possible candidates.
Starting with an initial state item, a set of candidatestate items is used to generate new state items for each step.
At each step, each existingstate item is extended by applying all applicable actions from the four, and the generateditems are put onto the agenda.
After each step, the best B items are taken from theagenda to generate new state items for the next step, and the agenda is cleared.
After allincoming words have been processed, the corresponding parse of the top item from theagenda is taken as the output.The decoding process is an instance of the generalized algorithm in Figure 1.
A stateitem is a pair ?S, Q?, where S represents the stack with the partial parse, and Q representsthe queue of incoming words.
The initial state item STARTITEM(dependency parsing)consists of an empty stack, and an incoming queue of the whole input.
The functionEXPAND(candidate, dependency parsing) applies all possible actions to candidate and gen-erates a set of new state items.
GOALTEST(dependency parsing, best) returns true if besthas reached the final state, and false otherwise.The score of a parse tree is computed by the global linear model in Equation (2),where the parameter vector ~w for the model is computed by the averaged perceptrontraining algorithm described in Section 2.2.
During training of the dependency parser,the early-update strategy of Collins and Roark (2004) is used.
The intuition is to improvelearning by avoiding irrelevant information, as discussed earlier: When all the items inthe current agenda are incorrect, further parsing steps will be irrelevant because thecorrect partial output no longer exists in the candidate ranking.Both the transition-based and the combined parsers use this training and decod-ing framework.
The main difference between the two parsers is in the definition ofthe feature templates.
Whereas the transition-based parser uses only transition-basedfeatures, the combined parser applies features from the graph-based parser in additionto transition-based features.
Table 13 shows the templates for transition-based features.Individual features for a parse are extracted from each transition action that is used tobuild the parse, by first instantiating the templates according to the local context, andthen pairing the instantiated template with the transition action.
Shown in Figure 9, thecontextual information consists of the top of the stack (ST), the parent (STP) of ST, theleftmost (STLC) and rightmost child (STRC) of ST, the current word (N0), the next threeTable 13Transition-based feature templates for the dependency parser.1 stack top STwt; STw; STt2 current word N0wt; N0w; N0t3 next word N1wt; N1w; N1t4 ST and N0 STwtN0wt; STwtN0w; STwN0wt; STwtN0t; STtN0wt; STwN0w; STtN0t5 POS bigram N0tN1t6 POS trigrams N0tN1tN2t; STtN0tN1t; STPtSTtN0t;STtSTLCtN0t; STtSTRCtN0t; STtN0tN0LCt7 N0 word N0wN1tN2t; STtN0wN1t; STPtSTtN0w;STtSTLCtN0w; STtSTRCtN0w; STtN0wN0LCtw = word; t = POS-tag.133Computational Linguistics Volume 37, Number 1Figure 9Transition-based feature context for the dependency parser.words from the input (N1, N2, N3), and the leftmost child of N0 (N0LC).
Word and POSinformation from the context are manually combined, and the combination of featuretemplates is decided by development tests.Table 14 shows the templates used to extract graph-based features from partialparses.
Templates 1?6 are taken from MSTParser (McDonald and Pereira 2006), asecond-order graph-based parser.
They are defined in the context of a word, its parentand its sibling.
To give more templates, features from templates 1?5 are also conjoinedwith the arc direction and distance, whereas features from template 6 are also conjoinedwith the direction and distance between the child and its sibling.
Here ?distance?
refersto the difference between word indexes.
Templates 7?8 are two extra feature templatesthat capture information about grandchildren and arity (i.e., the number of childrento the left or right).
They are difficult to include in an efficient dynamic-programmingdecoder, but easy to include in a beam-search decoder.
During decoding, the graph-based feature templates are instantiated at the earliest possible situation.
For example,the first-order arc and second-order sibling feature templates are instantiated whenARCLEFT or ARCRIGHT is performed, with sibling information for the newly addedlink.
The arity features are added as soon as the left or right arity of a word becomesfixed, the left arity templates being instantiated when ARCLEFT or SHIFT is performed,with the right arity templates being instantiated when ARCLEFT or REDUCE is per-formed.
Similarly, the leftmost grandchild features are instantiated when ARCLEFT orARCRIGHT is performed, and the rightmost grandchild features are instantiated whenARCLEFT or REDUCE is performed.Table 14Graph-based feature templates for the dependency parser.1 Parent word (P) Pw; Pt; Pwt2 Child word (C) Cw; Ct; Cwt3 P and C PwtCwt; PwtCw; PwCwt; PwtCt; PtCwt; PwCw; PtCt4 Tag Bt between P, C PtBtCt5 Neighbor words PtPLtCtCLt; PtPLtCtCRt; PtPRtCtCLt; PtPRtCtCRt;of P and C, left (L) PtPLtCLt; PtPLtCRt; PtPRtCLt; PtPRtCRt; PLtCtCLt; PLtCtCRt;and right (R) PRtCtCLt; PRtCtCRt; PtCtCLt; PtCtCRt; PtPLtCt; PtPRtCt6 sibling (S) of C CwSw; CtSt; CwSt; CtSw; PtCtSt7 leftmost (CLC) and PtCtCLCt; PtCtCRCtrightmost (CRC)children of C8 left (la) and right (ra) Ptla; Ptra; Pwtla; Pwtraarity of Pw = word; t = POS-tag.134Zhang and Clark Syntactic Processing5.1.1 The Comparability of State Items.
Our dependency parsers are based on the incre-mental shift-reduce parsing process.
During decoding, state items built with the samenumber of transition actions are put onto the agenda and compared with each other.
Tobuild any full parse tree, each word in the input sentence must be put onto the stackonce, and each word except the root of the sentence must be popped off the stackonce.
Because the four transition actions are either stack-pushing or stack-popping,each full parse must be built with 2n ?
1 transition actions, where n denotes the sizeof the input.
Therefore, the decoding process consists of 2n ?
1 steps, and in each stepall state items in the agenda have been built using the same number of actions.
Ourexperiments showed that start-of-the-art accuracy can be achieved by this intuitivemethod of candidate comparison.5.2 Experiments for EnglishWe used Penn Treebank 3 for our experiments, which was separated into the training,development, and test sets in the same way as McDonald, Crammer, and Pereira(2005), shown in Table 15.
Bracketed sentences from the Treebank were translatedinto dependency structures using the head-finding rules from Yamada and Matsumoto(2003).Before parsing, POS-tags are assigned to the input sentence using our baselinePOS-tagger of Zhang and Clark (2008a), which can be seen as the perceptron taggerof Collins (2002) with beam-search.
Like McDonald, Crammer, and Pereira (2005), weevaluated the parsing accuracy by the precision of lexical heads (the percentage of inputwords, excluding punctuation, that have been assigned the correct parent) and by thepercentage of complete matches, in which all words excluding punctuation have beenassigned the correct parent.A set of development tests, including the convergence of the perceptron, can befound in Zhang and Clark (2008a).
In this article, we report only the final test accu-racies and a set of additional speed/accuracy tradeoff results.
The accuracies of ourtransition-based and combined parsers on English data are shown together with othersystems in Table 16.
In the table, each row represents a parsing model.
Rows Yamada2003 and MSTParser represent Yamada and Matsumoto (2003), and MSTParser withtemplates 1?6 from Table 14 (McDonald and Pereira 2006), respectively.
Rows Transitionand Combined represent our pure transition-based and combined parsers, respectively.Row Huang 2010 shows the recent work of Huang and Sagae (2010), which appliesdynamic-programming packing to transition-based dependency parsing.
Rows Koo2008 and Chen 2009 represent the models of Koo, Carreras, and Collins (2008) andChen et al (2009), which perform semi-supervised learning by word-clustering andself-training, respectively.
Columns Word and Complete show the precision of lexicalTable 15The training, development, and test data for English dependency parsing.Sections Sentences WordsTraining 2?21 39,832 950,028Development 22 1,700 40,117Test 23 2,416 56,684135Computational Linguistics Volume 37, Number 1Table 16Accuracy comparisons between various dependency parsers on English data.Word CompleteYamada 2003 90.3 38.4Transition 91.4 41.8MSTParser 91.5 42.1Combined 92.1 45.4Huang 2010 92.1 ?Koo 2008 93.2 ?Chen 2009 93.2 47.2See text for details.Figure 10The accuracy/speed tradeoff graph for the transition-based and combined dependency parsers.heads and complete matches, respectively.
The combined parser achieved 92.1% per-word accuracy, which was significantly higher than the pure transition-based parser.4These results showed the effectiveness of utilizing both graph-based and transition-based information in a single model.
Represented by features that are not available in apure transition-based system, graph-based information helped the combined parser toachieve higher accuracy.As in the previous sections, we plot the speed/accuracy tradeoff for the transition-based and combined parsers.
For each point in each curve in Figure 10, we run thedevelopment test to decide the number of training iterations, and read the speed andaccuracy from the final test.
Each point in the transition curve corresponds to B = 1, 2, 4,8, 16, 32, 64, and 128, respectively, and each point in the combined curve corresponds toB = 1, 2, 4, 8, 16, 32, and 64, respectively.
When the speed was above 100 sentences persecond, the pure transition-based parser outperformed the combined parser with thesame speed.
However, as the size of the beam increases, the accuracy of the combinedparser increased more rapidly.
The combined parser gave higher accuracies at the samespeed when the speed was below 50 sentences per second.
The accuracy of the pure4 In Zhang and Clark (2008a) we showed that the combined parser also significantly outperformed thegraph-based parser.136Zhang and Clark Syntactic ProcessingTable 17Training, development, and test data for Chinese dependency parsing.Sections Sentences WordsTraining 001?815 16,118 437,8591,001?1,136Dev 886?931 804 20,4531,148?1,151Test 816?885 1,915 50,3191,137?1,147transition parser did not increase beyond B = 64.
Our experiments were performed ona Linux platform with a 2.0GHz CPU and a gcc 4.0.1 compiler.When B = 1, the transition-based parser was similar to MaltParser, because ourtransition-based parser is built upon the arc-each transition process of MaltParser,and uses similar feature sets.
The main difference is that our transition-based parserperforms global training using the perceptron algorithm, whereas MaltParser performslocal training using a support vector machine (SVM) with a polynomial kernel.
Becauseglobal training optimizes the model for full sequences of transition actions, a small beamcan potentially have a negative impact on learning, and hence the overall performance.5.3 Experiments for ChineseWe used the Penn Chinese Treebank 5 for our experiments.
Following Duan, Zhao, andXu (2007), we split the corpus into training, development, and test data as shown inTable 17.
We used a set of head-finding rules to turn the bracketed sentences into depen-dency structures, and they can be found in Zhang and Clark (2008a).
Like Duan, Zhao,and Xu (2007), we used gold-standard POS-tags for the input.
The parsing accuracywas evaluated by the percentage of non-root words that have been assigned the correcthead, the percentage of correctly identified root words, and the percentage of completematches, all excluding punctuation.The accuracies are shown in Table 18.
Rows Transition and Combined show ourmodels in the same way as for the English experiments from Section 5.2.
Row Duan 2007represents the transition-based model from Duan, Zhao, and Xu (2007), which appliedbeam-search to the deterministic model from Yamada and Matsumoto (2003).
RowTable 18Test accuracies of various dependency parsers on CTB5 data.Non-root Root CompleteDuan 2007 84.36 73.70 32.70Transition 84.69 76.73 32.79Huang 2010 85.52 78.32 33.72Combined 86.21 76.26 34.41See text for details.137Computational Linguistics Volume 37, Number 1Huang 2010 represents the model of Huang and Sagae (2010), which applies dynamic-programming packing to transition-based parsing.
The observations were similar to theEnglish tests.
The combined parser outperformed the transition-based parsers.
It gavethe best accuracy we are aware of for supervised dependency parsing using the CTB.One last question we investigate for this article is the overall performance when theparser is pipelined with a POS-tagger, or with the joint segmentation and POS-taggingalgorithm in Section 4, forming a complete pipeline for Chinese inputs.
The results areshown in Table 19.
For these experiments, we tune the pipelined POS-tagger and thejoint segmentor and POS-tagger on the CTB5 corpus in Table 17, using the developmenttest data to decide the number of training iterations and reporting the final test accuracy.The overall accuracy is calculated in F-score: Defining nc as the number of output wordsthat have been correctly segmented and assigned the correctly segmented head word,no as the number of words in the output, and nr the number of words in the reference,precision is p = nc/no and recall is r = nc/nr.
When pipelined with a pure POS-taggerusing gold-standard segmentation, the pipelined system gave 93.89% POS accuracyand 81.21% joint tagging and parsing F-score for non-root words.
When combinedwith the joint segmentation and POS-tagging system, the segmentation F-score, jointsegmentation and POS-tagging F-score were 95.00% and 90.17%, respectively, and thejoint segmentation and parsing F-score for non-root words (excluding punctuations)was 75.09%, where the corresponding accuracy with gold-standard segmented and POS-tagged input was 86.21%, as shown in Table 18.5.4 Related WorkMSTParser (McDonald and Pereira 2006) is a graph-based parser with an exhaustivesearch decoder, and MaltParser (Nivre et al 2006) is a transition-based parser witha greedy search decoder.
Representative of each method, MSTParser and MaltParsergave comparable accuracies in the CoNLL-X shared task (Buchholz and Marsi 2006).However, they make different types of errors, which can be seen as a reflection of theirtheoretical differences (McDonald and Nivre 2007).
By combining graph-based andtransition-based information, our dependency parser achieved higher accuracy thanboth graph-based and transition-based baselines.
The combination of information isenabled by our general framework.
Our global model does not impose any limitationon the kind of features that can be used, and therefore allows both graph-basedand transition-based features.
Moreover, beam-search frees the decoder from localityrestrictions such as the ?optimal sub-problem?
requirement for dynamic-programming,which limits the range of features in order to achieve reasonable decoding speed.Compared to the greedy local search decoder for MaltParser, beam-search can reduceerror propagation by keeping track of a set of candidate items.Table 19The combined segmentation, POS-tagging, and dependency parsing F-scores using differentpipelined systems.Seg F Tag F Dep FGold seg tag 100.00 100.00 86.21Gold seg auto tag 100.00 93.89 81.21Auto seg tag 95.00 90.17 75.09138Zhang and Clark Syntactic ProcessingOur transition-based parser is based on the arc-eager parsing process of MaltParser(Nivre et al 2006), although our parser is different from MaltParser in two aspects.First, we applied beam-search in decoding, which helps to prevent error propagationof local search.
Johansson and Nugues (2007) also use beam search.
Second, we use theperceptron to train whole sequences of transition actions globally, whereas MaltParseruses SVM to train each transition action locally.
Our global training corresponds to beam-search decoding, which searches for a globally optimal sequence of transition actionsrather than an optimal action at each step.An existing method to combine multiple parsing algorithms is the ensemble ap-proach (Sagae and Lavie 2006), which was reported to be useful in improving depen-dency parsing (Hall et al 2007).
A more recent approach (Nivre and McDonald 2008)combined MSTParser and MaltParser by using the output of one parser for featuresin the other in a stacking framework.
Both Hall et al (2007) and Nivre and McDonald(2008) can be seen as methods to combine separately defined models.
In contrast, ourparser combines two components in a single model, in which all parameters are trainedconsistently.6.
Phrase-Structure ParsingPhrase-structure parsing is the problem of producing the syntactic structure of aninput sentence according to a phrase-structure grammar.
An example phrase-structureparse tree is shown in Figure 11.
Similar to dependency parsing, dominant approachesto phrase-structure parsing include the transition-based method (Briscoe and Carroll1993), which builds an output parse tree by choosing a series of transition actions such asSHIFT and REDUCE, and the graph-based method (Collins 1999; Charniak 2000), whichexplores the search space of possible parse trees to find the best output according tograph-based scores.For English constituent parsing using the Penn Treebank, the best performingtransition-based parser lags behind the current state-of-the-art (Sagae and Lavie 2005).However, for Chinese constituent parsing using the Chinese Treebank, Wang et al(2006) have shown that a shift-reduce parser can give competitive accuracy scores to-gether with high speeds by using an SVM to make a single decision at each point in theparsing process.In Zhang and Clark (2009) we proposed a transition-based constituent parser forChinese, which is based on the transition process of Wang et al (2006).
Rather thanmaking a single decision at each processing step, our parser uses a global linear modelFigure 11An example Chinese lexicalized phrase-structure parse tree.139Computational Linguistics Volume 37, Number 1and beam-search decoding, and achieved competitive accuracy.
This phrase-structureparser can be expressed as an instance of our general framework.6.1 Instantiating the General FrameworkThe incremental parsing process of our parser is based on the shift-reduce parsers ofSagae and Lavie (2005) and Wang et al (2006), with slight modifications.
The inputis assumed to be segmented and POS-tagged, and the word?POS pairs waiting to beprocessed are stored in a queue.
A stack holds the partial parse trees that are built duringthe parsing process.
The output of this process is a binarized parse tree.
The four typesof action used to build a parse are:r SHIFT, which pushes the next word-POS pair in the queue onto the stack.r REDUCE?unary?X, which makes a new unary-branching node withlabel X; the stack is popped and the popped node becomes the child ofthe new node; the new node is pushed onto the stack.r REDUCE?binary?
{L/R}?X, which makes a new binary-branching nodewith label X; the stack is popped twice, with the first popped nodebecoming the right child of the new node and the second popped nodebecoming the left child; the new node is pushed onto the stack.
The left (L)and right (R) versions of the rules indicate whether the head of the newnode is to be taken from the left or right child.r TERMINATE, which pops the root node off the stack and ends parsing.
Thisaction can only be applied when the input queue is empty, and the stackcontains only one item.
The popped node is taken as the output.Defining the start state as the stack being empty and the queue containing the inputsentence, and the final state as the state immediately after a TERMINATE action, theincremental process builds a parse tree by repeatedly applying actions from the startstate until the final state is reached.
Note that not all sequences of actions produce validbinarized trees.
In the deterministic parser of Wang et al (2006), the highest scoringaction predicted by the classifier may prevent a valid binary tree from being built.
Inthis case, Wang et al simply return a partial parse consisting of all the subtrees onthe stack.
In our parser a set of restrictions is applied which guarantees a valid parsetree.
For example, two simple restrictions are that a SHIFT action can only be appliedif the queue of incoming words is non-empty, and the binary reduce actions can onlybe performed if the stack contains at least two nodes.
Some of the restrictions are morecomplex than this; the full set can be found in Zhang and Clark (2009).
Wang et al(2006) give a detailed example showing how a segmented and POS-tagged sentence canbe incrementally processed using the shift-reduce actions to produce a binary tree.
Weshow this example in Figure 12.For the decoding problem, our parser performs beam-search using an agenda tofind the output parse from possible candidates.
Initially containing a start state item, aset of state items is used to generate new state items for each processing step.
At eachstep, each of the state items is extended using all applicable actions, generating a set ofnew state items, which are put onto the agenda.
After each step, the B-best items fromthe agenda are taken for the generation of new state items in the next step.
The same140Zhang and Clark Syntactic ProcessingFigure 12An example shift-reduce parsing process.process repeats until the highest scored item from the agenda is in the final state, and itis taken as the final parse.This decoding process can be seen as an instance of the generic algorithm in Fig-ure 1.
Here a state item is a pair ?S, Q?, where S represents the stack with partial parses,and Q represents the incoming queue.
The initial state item STARTITEM(constituentparsing) is the start state item, where the stack is empty and the queue contains the141Computational Linguistics Volume 37, Number 1whole input sentence, the function EXPAND(candidate, constituent parsing) extends can-didate by using all applicable actions to generate a set of new state items, andGOALTEST (constituent parsing, best) returns true if best reaches the final state, and falseotherwise.The score of a parse tree is computed by the global linear model in Equation (2),where the parameter vector ~w for the model is computed by the averaged perceptrontraining algorithm described in Section 2.2.
As in the training of the dependency parser,the early-update strategy of Collins and Roark (2004) is used.Table 20 shows the set of feature templates for the model.
Individual features aregenerated from these templates by first instantiating a template with particular labels,words, and tags, and then pairing the instantiated template with a particular action.In the table, the symbols S0, S1, S2, and S3 represent the top four nodes on the stack,and the symbols N0, N1, N2, and N3 represent the first four words in the incomingqueue.
S0L, S0R, and S0U represent the left and right child for binary branching S0,and the single child for unary branching S0, respectively; w represents the lexical headtoken for a node; and c represents the label for a node.
When the corresponding nodeis a terminal, c represents its POS-tag, whereas when the corresponding node is a non-terminal, c represents its constituent label; t represents the POS-tag for a word.The context S0, S1, S2, S3, N0, N1, N2, N3 for the feature templates is taken from Wanget al (2006).
However, Wang et al (2006) used a polynomial kernel function with anSVM and did not manually create feature combinations.
Because we used the linear per-ceptron algorithm we manually combined Unigram features into Bigram and Trigramfeatures.The Bracket row shows bracket-related features, which were inspired by Wang et al(2006).
Here brackets refer to left brackets including ??, ?fi?, and ??
and right bracketsincluding ?
?, ?fl?, and ?
?.
In the table, b represents the matching status of the lastleft bracket (if any) on the stack.
It takes three different values: 1 (no matching rightbracket has been pushed onto stack), 2 (a matching right bracket has been pushedonto stack), and 3 (a matching right bracket has been pushed onto stack, but thenpopped off).Table 20Feature templates for the phrase-structure parser.Description Feature templatesUnigrams S0tc, S0wc, S1tc, S1wc, S2tc, S2wc, S3tc, S3wc,N0wt, N1wt, N2wt, N3wt,S0lwc, S0rwc, S0uwc, S1lwc, S1rwc, S1uwc,Bigrams S0wS1w, S0wS1c, S0cS1w, S0cS1c,S0wN0w, S0wN0t, S0cN0w, S0cN0t,N0wN1w, N0wN1t, N0tN1w, N0tN1tS1wN0w, S1wN0t, S1cN0w, S1cN0t,Trigrams S0cS1cS2c, S0wS1cS2c, S0cS1wS2c, S0cS1cS2w,S0cS1cN0t, S0wS1cN0t, S0cS1wN0t, S0cS1cN0wBracket S0wb, S0cbS0wS1cb, S0cS1wb, S0cS1cb, S0wN0tb, S0cN0wb, S0cN0tbSeparator S0wp, S0wcp, S0wq, S0wcq, S1wp, S1wcp, S1wq, S1wcqS0cS1cp, S0cS1cqw = word; c = constituent label; t = POS-tag.142Zhang and Clark Syntactic ProcessingTable 21The standard split of CTB2 data for phrase-structure parsing.Sections Sentences WordsTraining 001?270 3,475 85,058Development 301?325 355 6,821Test 271?300 348 8,008The Separator row shows features that include one of the separator punctuations(i.e., ?
?, ??, ??, and ?ff?)
between the head words of S0 and S1.
These templates applyonly when the stack contains at least two nodes; p represents a separator punctuationsymbol.
Each unique separator punctuation between S0 and S1 is only counted oncewhen generating the global feature vector.
q represents the count of any separatorpunctuation between S0 and S1.6.1.1 The Comparability of State Items.
Just as in our dependency parsers, the phrase-structure parser is based on an incremental shift-reduce parsing process, and stateitems built with the same number of transition actions are compared with each otherduring decoding.
However, due to the possibility of unary-reduce actions, the numberof actions used to build full parse trees can vary given an input sentence.
This makes thecomparison of state items more difficult than for dependency parsing, because whensome state items on the agenda are completed (i.e., in the final state), others on theagenda may still need more actions to complete.
We choose to push completed stateitems back onto the agenda during the decoding process, without changing them.
Thedecoding continues until the highest scored state item on the agenda is completed.When decoding stops, the highest scored state item on the agenda is taken as the finaloutput.
In this process, completed state items that do not have the highest score in theagenda are kept unchanged and compared with incomplete state items, even thoughthey may have been built with different numbers of actions.
Our experiments showedthat this approach gave reasonable accuracies.6.2 ExperimentsThe experiments were performed using the Chinese Treebank 2 (Table 21) and ChineseTreebank 5 data.
Standard data preparation was performed before the experiments:Empty terminal nodes were removed; any non-terminal nodes with no children wereremoved; any unary X ?
X nodes resulting from the previous steps were collapsedinto one X node.For all experiments, we used the EVALB tool5 for evaluation, and used labeled recall(LR), labeled precision (LP) and F1 score (which is the harmonic mean of LR and LP) tomeasure parsing accuracy.6.2.1 Test Results on CTB2.
The following tests were performed using both gold-standardPOS-tags and POS-tags automatically assigned by a POS-tagger.
We used our base-line POS-tagger in Section 4 for automatic POS-tagging.
The results of various models5 http://nlp.cs.nyu.edu/evalb/.143Computational Linguistics Volume 37, Number 1Table 22Accuracies of various phrase-structure parsers on CTB2 with gold-standard POS-tags.Model LR LP F1Bikel Thesis 80.9% 84.5% 82.7%Wang 2006 SVM 87.2% 88.3% 87.8%Wang 2006 Stacked 88.3% 88.1% 88.2%Our parser 89.4% 90.1% 89.8%See text for details.evaluated on sentences with less than 40 words and using gold-standard POS-tags areshown in Table 22.
The rows represent the model from Bikel and Chiang (2000) andBikel (2004), the SVM and ensemble models from Wang et al (2006), and our parser,respectively.
The accuracy of our parser is competitive using this test set.The results of various models using automatically assigned POS-tags are shown inTable 23.
The rows in the table represent the models from Bikel and Chiang (2000), Levyand Manning (2003), Xiong et al (2005), Bikel (2004), Chiang and Bikel (2002), the SVMmodel from Wang et al (2006), the ensemble system from Wang et al (2006), and theparser of this article, respectively.
Our parser gave comparable accuracies to the SVMand ensemble models from Wang et al (2006).
However, comparison with Table 22shows that our parser is more sensitive to POS-tagging errors than some of the othermodels.
One possible reason is that some of the other parsers (e.g., Bikel 2004) use theparser model itself to resolve tagging ambiguities, whereas we rely on a POS-tagger toaccurately assign a single tag to each word.
In fact, for the Chinese data, POS-taggingaccuracy is not high, with the perceptron-based tagger achieving an accuracy of only93%.
The beam-search decoding framework we use could accommodate joint parsingand tagging, although the use of features based on the tags of incoming words com-plicates matters somewhat, because these features rely on tags having been assigned toall words in a pre-processing step.
One possible solution would be generating multiplePOS-tags for each word during tagging, and incorporating tag information into the shiftaction, so that the parser will resolve the POS-tag ambiguity.
We leave this problem forfuture work.Table 23Accuracies of various phrase-structure parsers on CTB2 with automatically assigned tags.?
40 words ?
100 words UnlimitedLR LP F1 POS LR LP F1 POS LR LP F1 POSBikel 2000 76.8% 77.8% 77.3% - 73.3% 74.6% 74.0% - - - - -Levy 2003 79.2% 78.4% 78.8% - - - - - - - - -Xiong 2005 78.7% 80.1% 79.4% - - - - - - - - -Bikel Thesis 78.0% 81.2% 79.6% - 74.4% 78.5% 76.4% - - - - -Chiang 2002 78.8% 81.1% 79.9% - 75.2% 78.0% 76.6% - - - - -W06 SVM 78.1% 81.1% 79.6% 92.5% 75.5% 78.5% 77.0% 92.2% 75.0% 78.0% 76.5% 92.1%W06 Stacked 79.2% 81.1% 80.1% 92.5% 76.7% 78.4% 77.5% 92.2% 76.2% 78.0% 77.1% 92.1%Our parser 80.2% 80.5% 80.4% 93.5% 76.5% 77.7% 77.1% 93.1% 76.1% 77.4% 76.7% 93.0%See text for details.144Zhang and Clark Syntactic ProcessingFigure 13The accuracy/speed tradeoff graph for the phrase-structure parser.Petrov and Klein (2007) reported LR and LP of 85.7% and 86.9% for sentenceswith less than 40 words and 81.9% and 84.8% for all sentences on the CTB2 test set,respectively.
These results are significantly better than any model from Table 23.However, we did not include their scores in the table because they used a differenttraining set from CTB5, which is much larger than the CTB2 training set used by allparsers in the table.
In order to make a comparison, we split the data in the same wayas Petrov and Klein and tested our parser using automatically assigned POS-tags.
Itgave LR and LP of 82.0% and 80.9% for sentences with less than 40 words and 77.8%and 77.4% for all sentences, significantly lower than Petrov and Klein.
Possible reasonsinclude the sensitivity of our parser to POS-tag errors, and perhaps the use of a latentvariable model by Petrov and Klein.As in the previous sections, we plot the speed/accuracy tradeoff for our phrase-structure parser.
For each point in each curve in Figure 13, we run the developmenttest to decide the number of training iterations, and draw the point with speed andaccuracy from the final test.
Each point in the curve corresponds to B = 1, 2, 4, 8, 16,and 32, respectively.
The accuracies increased when the beam increased from 1 to 4, butfluctuated when the beam increased beyond 4.
In contrast to the development tests, theaccuracy reached its maximum when the beam size was 4 rather than 16.
However,the general trend of increased accuracy as the speed decreases can still be observed,and the amount of increase diminishes as the speed decreases.
These experiments wereperformed on a Linux platform with a 2.0GHz CPU and a gcc 4.0.1 compiler.6.2.2 Test Accuracy Using CTB5.
Table 24 presents the performance of the parser on CTB5.We adopt the data split from the previous section, as shown in Table 17.
We used thesame parser configurations as Section 6.2.1.Table 24Accuracies of our phrase-structure parser on CTB5 using gold-standard and automaticallyassigned POS-tags.?
40 words UnlimitedLR LP F1 POS LR LP F1 POS87.9% 87.5% 87.7% 100% 86.9% 86.7% 86.8% 100%80.2% 79.1% 79.6% 94.1% 78.6% 78.0% 78.3% 93.9%145Computational Linguistics Volume 37, Number 1As an additional evaluation we also produced dependency output from the phrase-structure trees, using the head-finding rules, so that we can compare with dependencyparsers.
We compare the dependencies read off our constituent parser using CTB5 datawith the dependency parser from Section 5, which currently gives the best dependencyparsing accuracy on CTB5.
The same measures are taken and the accuracies with gold-standard POS-tags are shown in Table 25.
Our constituent parser gave higher accuracythan the combined dependency parser.
It is interesting that, though the constituentparser uses many fewer feature templates than the dependency parser, the features doinclude constituent information, which is unavailable to the dependency parser.6.3 Related WorkOur parser is based on the shift-reduce parsing process from Sagae and Lavie (2005)and Wang et al (2006), and therefore it can be classified as a transition-based parser(Nivre et al 2006).
An important difference between our parser and the Wang et al(2006) parser is that our parser is based on a discriminative learning model with globalfeatures, whereas the parser from Wang et al (2006) is based on a local classifier thatoptimizes each individual choice.
Instead of greedy local decoding, we used beam-search in the decoder.An early work that applies beam-search to constituent parsing is Ratnaparkhi(1999).
The main difference between our parser and Ratnaparkhi?s is that we use aglobal discriminative model, whereas Ratnaparkhi?s parser has separate probabilitiesof actions chained together in a conditional model.Both our parser and the parser from Collins and Roark (2004) use a global discrim-inative model and an incremental parsing process.
The major difference is that Collinsand Roark, like Roark (2001), follow a top?down derivation strategy, whereas we choseto use a shift-reduce process which has been shown to give state-of-the-art accuraciesfor Chinese (Wang et al 2006).
In addition, we did not include a generative baselinemodel in the discriminative model, as did Collins and Roark (2004).7.
DiscussionWe have demonstrated in the previous sections that accuracies competitive with thestate-of-the-art can be achieved by our general framework for Chinese word segmen-tation, joint word segmentation and POS-tagging, Chinese and English dependencyparsing, and Chinese phrase-structure parsing.
Besides these tasks, our baseline POS-tagger in Section 4 is also implemented in the framework.
When it is applied to EnglishPOS-tagging using feature templates from Collins (2002), it gave similar accuracy to theTable 25Comparison of dependency accuracies between phrase-structure parsing and dependencyparsing using CTB5 data.Non-root Root CompleteDependency parser 86.21% 76.26% 34.41%Constituent parser 86.95% 79.19% 36.08%146Zhang and Clark Syntactic Processingdynamic-programming decoder of Collins (2002).
All these experiments suggest that thegeneral yet efficient framework provides a competitive solution for structural predictionproblems with an incremental output-building process.
In this section, we discuss themain reasons for the effectiveness of the general framework, as well as its prerequisites,advantages, and limitations when applied to a general task.One of the main reasons for the high accuracies achieved by this framework is thefreedom in using arbitrary features to capture statistical patterns, including those thatlead to impractical time complexity with alternative learning and decoding frameworksand algorithms such as CRFs and dynamic programming.
This freedom was exploitedby our effort to incorporate larger sources of statistical information for the improve-ment of accuracy.
For example, our word-based segmentor extends the character-basedapproach by including word information; our joint word segmentor and POS-taggerutilizes POS information for word segmentation; our combined dependency parserincludes statistical information from two different methods in a single, consistentlytrained model.
The models gave state-of-the-art accuracies in these problems, demon-strating the advantage of using flexible information covering large parts of the outputstructure.Compared to alternative discriminative training algorithms such as structural SVM(Tsochantaridis et al 2004) and CRFs (Lafferty, McCallum, and Pereira 2001; Clark andCurran 2007), the perceptron has a simple parameter update process, which is oftenefficient in both memory usage and running time depending on the decoding algorithm.Consider joint word segmentation and POS-tagging for example; we found in ourexperiments that practical training times can be achieved by the perceptron algorithm,but not with structural SVM.
MIRA (Crammer and Singer 2003) and its simplifications(McDonald, Crammer, and Pereira 2005; Crammer et al 2006) are also commonly usedlearning algorithms to train a global linear model.
They can be treated as slower butpotentially more accurate alternatives to the perceptron for the general framework.
Weexperimented with these for joint segmentation and tagging but did not improve uponthe perceptron.Beam-search enables training to be performed efficiently for extremely large com-plex search spaces, for which dynamic programming algorithms may be impractical.For the joint word segmentation and POS-tagging problem, a dynamic-programmingdecoder is prohibitively slow but a beam-search decoder runs in reasonable time.A more important advantage of beam-search compared to dynamic-programmingis that beam-search allows arbitrary features, whereas the efficiency of a dynamic-programming decoder is restricted by the range of features, due to its requirementfor optimal substructure.
For our combined dependency parser, the feature set makesa dynamic-programming decoder infeasibly slow.
From this perspective, beam-searchis in line with other recent research on the improvement of accuracies by incorporat-ing non-local features via approximation, such as belief propagation for dependencyparsing (Smith and Eisner 2008), integer linear programming for dependency parsing(Martins, Smith, and Xing 2009), and forest reranking for phrase-structure parsing(Huang 2008).The only prerequisite of the framework is an incremental process, which consumesthe input sentence and builds the output structure using a sequence of actions.
Allfour problems studied in the article were first turned into an incremental process, andthen solved by applying the framework.
The number of distinct actions for a problemis dependent on the complexity of the output.
For word segmentation, there are onlytwo actions (append or separate).
For transition-based unlabeled dependency parsing,there are four actions (shift, arc-left, arc-right, and reduce).
For joint segmentation and147Computational Linguistics Volume 37, Number 1POS-tagging and constituent parsing, there are many more distinct actions accordingto the set of labels.
For example, the number of distinct unary-reduce actions for con-stituent parsing is equal to the number of distinct constituent labels that form unary-branching nodes.
The incremental processes for all four problems have linear timecomplexity, and a larger number of distinct actions leads to a slower decoder.One of the most important issues in using beam-search is the comparability of par-tially built structures at each incremental step during decoding.
For word segmentationand joint segmentation and POS-tagging, we compare partially built sentences that havethe same number of characters.
For word segmentation, partial words can be treated inthe same way as full words, without losing much accuracy.
The same approach wasnot effective when applied to joint segmentation and POS-tagging, for which partialwords must be treated differently, or avoided by alternative inference such as usingmultiple-beams.
For dependency parsing, we compared partial outputs that have beenbuilt using the same number of transition actions.
Because all parses for a sentence withsize n are built using exactly 2n ?
1 transition actions, this comparison is consistentthroughout the decoding process.
For constituent parsing, we initially compare partialoutputs that have been built using the same number of actions.
However, becausedifferent output parse trees can contain a different number of unary-reduce actions,some candidate outputs will be completed earlier than others.
When this happens, wechoose to retain fully built outputs in the agenda while continuing the decoding process,until the highest scored item in the agenda is a complete parse.
Therefore, there aresituations when different parses in the agenda have a different number of actions.
Thisdid not turn out to be a significant problem.
We believe that the most effective way toorganize output comparison is largely an empirical question.Finally, alternative components can be used to replace the learning or decodingalgorithms of the framework to give higher accuracies.
Huang and Sagae (2010) haverecently applied dynamic-programming to dependency parsing to pack items that havethe same signature in the beam, and obtained higher accuracy than our transition-baseddependency parser in Section 5.8.
ConclusionWe explored word segmentation, joint word segmentation and POS-tagging, depen-dency parsing, and phrase-structure parsing using a general framework of a globallinear model, trained by the perceptron algorithm and decoded with beam-search.We have chosen to focus on Chinese; the framework itself and our algorithms for thespecific problems are language-independent, however.
In Section 5 we reported resultson English dependency parsing.
Despite the rather simple nature of the decoding andtraining processes, the framework achieved accuracies competitive with the state-of-the-art for all the tasks we considered, by making use of a large range of statisticalinformation.
As further evidence of the effectiveness of our framework, we have re-cently adapted our phrase-structure parser in Section 6 to parsing with a lexicalizedgrammar formalism, Combinatory Categorial Grammar (CCG), and achieved higherF-scores than the state-of-the-art C&C CCG parser (Clark and Curran 2007).
The range ofproblems that we have studied suggests that our framework is a simple yet competitiveoption for structural prediction problems in general.Our source code can be found at www.sourceforge.net/projects/zpar.
It containsthe general framework, our implementations of the four tasks addressed in this article,and other tools for syntactic processing.148Zhang and Clark Syntactic ProcessingAcknowledgmentsThis work was largely carried out whileYue Zhang was a DPhil student at theOxford University Computing Laboratory,where he was supported by an ORSScholarship and the Clarendon Fund.Stephen Clark was supported by EPSRCgrant EP/E035698/1.ReferencesBikel, Daniel M. 2004.
On the Parameter Spaceof Generative Lexicalized Statistical ParsingModels.
Ph.D. thesis, University ofPennsylvania.Bikel, Daniel M. and David Chiang.
2000.Two statistical parsing models appliedto the Chinese Treebank.
In Proceedings ofSIGHAN Workshop, pages 1?6, Hong Kong.Briscoe, Ted and John Carroll.
1993.Generalized probabilistic LR parsingof natural language (corpora) withunification-based grammars.Computational Linguistics, 19(1):25?59.Buchholz, Sabine and Erwin Marsi.
2006.aonll-X shared task on multilingualdependency parsing.
In Proceedings ofCoNLL, pages 149?164, New York, NY.Carreras, Xavier, Michael Collins, and TerryKoo.
2008.
Tag, dynamic programming,and the perceptron for efficient,feature-rich parsing.
In Proceedings ofCoNLL, pages 9?16, Manchester.Carreras, Xavier, Mihai Surdeanu, and LluisMarquez.
2006.
Projective dependencyparsing with perceptron.
In Proceedings ofCoNLL, pages 181?185, New York, NY.Charniak, Eugene.
2000.
Amaximum-entropy-inspired parser.
InProceedings of NAACL, pages 132?139,Seattle, WA.Chen, Wenliang, Jun?ichi Kazama, KiyotakaUchimoto, and Kentaro Torisawa.
2009.Improving dependency parsing withsubtrees from auto-parsed data.
InProceedings of EMNLP, pages 570?579,Singapore.Chiang, David and Daniel M. Bikel.
2002.Recovering latent information intreebanks.
In Proceedings of COLING,pages 183?198, Taipei.Clark, Stephen and James R. Curran.2007.
Wide-coverage efficient statisticalparsing with CCG and log-linearmodels.
Computational Linguistics,33(4):493?552.Collins, Michael.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania.Collins, Michael.
2002.
Discriminativetraining methods for hidden Markovmodels: Theory and experiments withperceptron algorithms.
In Proceedings ofEMNLP, pages 1?8, Philadelphia, PA.Collins, Michael and Brian Roark.
2004.Incremental parsing with the perceptronalgorithm.
In Proceedings of ACL,pages 111?118, Barcelona.Crammer, Koby, Ofer Dekel, Joseph Keshet,Shai Shalev-Shwartz, and Yoram Singer.2006.
Online passive-aggressivealgorithms.
Journal of Machine LearningResearch, 7:551?585.Crammer, Koby and Yoram Singer.
2003.Ultraconservative online algorithms formulticlass problems.
Journal of MachineLearning Research, 3:951?991.Duan, Xiangyu, Jun Zhao, and Bo Xu.
2007.Probabilistic models for action-basedChinese dependency parsing.
InProceedings of ECML/ECPPKDD,pages 559?566, Warsaw.Emerson, Thomas.
2005.
The secondinternational Chinese word segmentationbakeoff.
In Proceedings of SIGHANWorkshop, pages 123?133, Jeju.Finkel, Jenny Rose, Alex Kleeman, andChristopher D. Manning.
2008.
Efficient,feature-based, conditional random fieldparsing.
In Proceedings of ACL/HLT,pages 959?967, Columbus, OH.Freund, Y. and R. Schapire.
1999.
Largemargin classification using the perceptronalgorithm.
In Rob Holte, editor, MachineLearning.
Kluwer, Boston, MA,pages 277?296.Hall, Johan, Jens Nilsson, Joakim Nivre,Gu?lsen Eryigit, Bea?ta Megyesi, MattiasNilsson, and Markus Saers.
2007.
Singlemalt or blended?
A study in multilingualparser optimization.
In Proceedings of theCoNLL Shared Task Session of EMNLP/CoNLL, pages 933?939, Prague.Huang, Liang.
2008.
Forest reranking:Discriminative parsing with non-localfeatures.
In Proceedings of ACL/HLT,pages 586?594, Columbus, OH.Huang, Liang and Kenji Sagae.
2010.Dynamic programming for linear-timeincremental parsing.
In Proceedings of ACL,pages 1077?1086, Uppsala.Jiang, Wenbin, Liang Huang, Qun Liu, andYajuan Lu?.
2008.
A cascaded linear modelfor joint Chinese word segmentation andpart-of-speech tagging.
In Proceedings ofACL/HLT, pages 897?904, Columbus, OH.Jiang, Wenbin, Haitao Mi, and Qun Liu.
2008.Word lattice reranking for Chinese word149Computational Linguistics Volume 37, Number 1segmentation and part-of-speech tagging.In Proceedings of COLING, pages 385?392,Manchester.Johansson, Richard and Pierre Nugues.
2007.Incremental dependency parsing usingonline learning.
In Proceedings of theCoNLL/EMNLP, pages 1134?1138, Prague.Koo, Terry, Xavier Carreras, and MichaelCollins.
2008.
Simple semi-superviseddependency parsing.
In Proceedings ofACL/HLT, pages 595?603, Columbus, OH.Kruengkrai, Canasai, Kiyotaka Uchimoto,Jun?ichi Kazama, Yiou Wang, KentaroTorisawa, and Hitoshi Isahara.
2009.
Anerror-driven word-character hybrid modelfor joint Chinese word segmentation andPOS tagging.
In Proceedings of ACL/AFNLP,pages 513?521, Suntec.Lafferty, J., A. McCallum, and F. Pereira.2001.
Conditional random fields:Probabilistic models for segmenting andlabeling sequence data.
In Proceedings ofICML, pages 282?289, Williamstown, MA.Levy, Roger and Christopher D. Manning.2003.
Is it harder to parse Chinese, or theChinese treebank?
In Proceedings of ACL,pages 439?446, Sapporo.Martins, Andre, Noah Smith, and Eric Xing.2009.
Concise integer linear programmingformulations for dependency parsing.
InProceedings of ACL/AFNLP, pages 342?350,Suntec.McDonald, Ryan, Koby Crammer, andFernando Pereira.
2005.
Onlinelarge-margin training of dependencyparsers.
In Proceedings of ACL, pages 91?98,Ann Arbor, MI.McDonald, Ryan and Joakim Nivre.
2007.Characterizing the errors of data-drivendependency parsing models.
In Proceedingsof EMNLP/CoNLL, pages 122?131, Prague.McDonald, Ryan and Fernando Pereira.2006.
Online learning of approximatedependency parsing algorithms.
InProceedings of EACL, pages 81?88, Trento.McDonald, Ryan, Fernando Pereira,Kiril Ribarov, and Jan Hajic.
2005.Non-projective dependency parsingusing spanning tree algorithms.
InProceedings of HLT/EMNLP, pages 523?530,Vancouver.Nakagawa, Tetsuji and Kiyotaka Uchimoto.2007.
A hybrid approach to wordsegmentation and POS tagging.
InProceedings of ACL Demo and PosterSession, Prague.Ng, Hwee Tou and Jin Kiat Low.
2004.Chinese part-of-speech tagging:One-at-a-time or all-at-once?
Word-basedor character-based?
In Proceedings ofEMNLP, pages 227?284, Barcelona.Nivre, Joakim, Johan Hall, Sandra Ku?bler,Ryan McDonald, Jens Nilsson, SebastianRiedel, and Deniz Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.In Proceedings of the CoNLL Shared TaskSession of EMNLP/CoNLL, pages 915?932,Prague.Nivre, Joakim, Johan Hall, Jens Nilsson,Gu?ls?en Eryig?it, and Svetoslav Marinov.2006.
Labeled pseudo-projectivedependency parsing with support vectormachines.
In Proceedings of CoNLL,pages 221?225, New York, NY.Nivre, Joakim and Ryan McDonald.2008.
Integrating graph-based andtransition-based dependency parsers.
InProceedings of ACL/HLT, pages 950?958,Columbus, OH.Peng, F., F. Feng, and A. McCallum.
2004.Chinese segmentation and new worddetection using conditional random fields.In Proceedings of COLING, pages 562?568,Geneva.Petrov, Slav and Dan Klein.
2007.
Improvedinference for unlexicalized parsing.
InProceedings of HLT/NAACL, pages 404?411,Rochester, NY.Ratnaparkhi, Adwait.
1998.
MaximumEntropy Models for Natural LanguageAmbiguity Resolution.
Ph.D. thesis,University of Pennsylvania.Ratnaparkhi, Adwait.
1999.
Learning toparse natural language with maximumentropy models.
Machine Learning,34(1-3):151?175.Roark, Brian.
2001.
Probabilistic top?downparsing and language modeling.Computational Linguistics, 27:249?276.Sagae, Kenji and Alon Lavie.
2005.
Aclassifier-based parser with linearrun-time complexity.
In Proceedings ofIWPT, pages 125?132, Vancouver.Sagae, Kenji and Alon Lavie.
2006.
Parsercombination by reparsing.
In Proceedingsof HLT/NAACL, Companion Volume: ShortPapers, pages 129?132, New York, NY.Shi, Yanxin and Mengqiu Wang.
2007.
Adual-layer CRF based joint decodingmethod for cascade segmentation andlabelling tasks.
In Proceedings of IJCAI,pages 1707?1712, Hyderabad.Smith, David and Jason Eisner.
2008.Dependency parsing by beliefpropagation.
In Proceedings of the 2008Conference on Empirical Methods in NaturalLanguage Processing, pages 145?156,Honolulu, HI.150Zhang and Clark Syntactic ProcessingSproat, R., C. Shih, W. Gail, and N. Chang.1996.
A stochastic finite-stateword-segmentation algorithm for Chinese.Computational Linguistics, 22(3):377?404.Sproat, Richard and Thomas Emerson.
2003.The first international Chinese wordsegmentation bakeoff.
In Proceedings of TheSecond SIGHAN Workshop, pages 282?289,Sapporo.Tsochantaridis, I., T. Hofmann, T. Joachims,and Y. Altun.
2004.
Support vectormachine learning for interdependent andstructured output spaces.
In Proceedingsof ICML, pages 102?114, Banff.Wang, Xinhao, Xiaojun Lin, Dianhai Yu,Hao Tian, and Xihong Wu.
2006.
Chineseword segmentation with maximumentropy and n-gram language model.In Proceedings of SIGHAN Workshop,pages 138?141, Sydney.Xia, Fei.
2000.
The Part-of-Speech TaggingGuidelines for the Chinese Treebank (3.0),University of Pennsylvania.Xiong, Deyi, Shuanglong Li, Qun Liu,Shouxun Lin, and Yueliang Qian.
2005.Parsing the Penn Chinese Treebank withsemantic knowledge.
In Proceedings ofIJCNLP, pages 70?81, Jeju.Xue, N. 2003.
Chinese word segmentation ascharacter tagging.
International Journal ofComputational Linguistics and ChineseLanguage Processing, 8(1):29?48.Yamada, H. and Y. Matsumoto.
2003.Statistical dependency analysis usingsupport vector machines.
In Proceedingsof IWPT, pages 195?206, Nancy.Zhang, Ruiqiang, Genichiro Kikui, andEiichiro Sumita.
2006.
Subword-basedtagging by conditional randomfields for Chinese word segmentation.In Proceedings of the HLT/NAACL,Companion, volume Short Papers,pages 193?196, New York, NY.Zhang, Yue and Stephen Clark.
2007.Chinese segmentation with a word-basedperceptron algorithm.
In Proceedings ofACL, pages 840?847, Prague.Zhang, Yue and Stephen Clark.
2008a.
Jointword segmentation and POS tagging usinga single perceptron.
In Proceedings ofACL/HLT, pages 888?896, Columbus, OH.Zhang, Yue and Stephen Clark.
2008b.A tale of two parsers: Investigatingand combining graph-based andtransition-based dependency parsingusing beam-search.
In Proceedings ofEMNLP, pages 562?571, Honolulu, HI.Zhang, Yue and Stephen Clark.
2009.Transition-based parsing of the ChineseTreebank using a global discriminativemodel.
In Proceedings of IWPT,pages 162?171, Paris.Zhang, Yue and Stephen Clark.
2010.
A fastdecoder for joint word segmentation andPOS-tagging using a single discriminativemodel.
In Proceedings of EMNLP,pages 843?852, Cambridge, MA.Zhao, Hai, Chang-Ning Huang, and Mu Li.2006.
An improved Chinese wordsegmentation system with conditionalrandom field.
In Proceedings of SIGHANWorkshop, pages 162?165, Sydney.151152
