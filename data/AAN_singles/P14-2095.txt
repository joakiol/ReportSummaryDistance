Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579?585,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsCross-lingual Model Transfer Using Feature Representation ProjectionMikhail KozhevnikovMMCI, University of SaarlandSaarbr?ucken, Germanymkozhevn@mmci.uni-saarland.deIvan TitovILLC, University of AmsterdamAmsterdam, Netherlandstitov@uva.nlAbstractWe propose a novel approach to cross-lingual model transfer based on featurerepresentation projection.
First, a com-pact feature representation relevant for thetask in question is constructed for eitherlanguage independently and then the map-ping between the two representations isdetermined using parallel data.
The tar-get instance can then be mapped intothe source-side feature representation us-ing the derived mapping and handled di-rectly by the source-side model.
This ap-proach displays competitive performanceon model transfer for semantic role label-ing when compared to direct model trans-fer and annotation projection and suggestsinteresting directions for further research.1 IntroductionCross-lingual model transfer approaches are con-cerned with creating statistical models for var-ious tasks for languages poor in annotated re-sources, utilising resources or models availablefor these tasks in other languages.
That includesapproaches such as direct model transfer (Ze-man and Resnik, 2008) and annotation projec-tion (Yarowsky et al, 2001).
Such methods havebeen successfully applied to a variety of tasks,including POS tagging (Xi and Hwa, 2005; Dasand Petrov, 2011; T?ackstr?om et al, 2013), syntac-tic parsing (Ganchev et al, 2009; Smith and Eis-ner, 2009; Hwa et al, 2005; Durrett et al, 2012;S?gaard, 2011), semantic role labeling (Pad?o andLapata, 2009; Annesi and Basili, 2010; Tonelliand Pianta, 2008; Kozhevnikov and Titov, 2013)and others.Direct model transfer attempts to find a sharedfeature representation for samples from the twolanguages, usually generalizing and abstract-ing away from language-specific representations.Once this is achieved, instances from both lan-guages can be mapped into this space and a modeltrained on the source-language data directly ap-plied to the target language.
If parallel data isavailable, it can be further used to enforce modelagreement on this data to adjust for discrepanciesbetween the two languages, for example by meansof projected transfer (McDonald et al, 2011).The shared feature representation depends onthe task in question, but most often each aspectof the original feature representation is handledseparately.
Word types, for example, may be re-placed by cross-lingual word clusters (T?ackstr?omet al, 2012) or cross-lingual distributed word rep-resentations (Klementiev et al, 2012).
Part-of-speech tags, which are often language-specific,can be converted into universal part-of-speechtags (Petrov et al, 2012) and morpho-syntacticinformation can also be represented in a unifiedway (Zeman et al, 2012; McDonald et al, 2013;Tsarfaty, 2013).
Unfortunately, the design of suchrepresentations and corresponding conversion pro-cedures is by no means trivial.Annotation projection, on the other hand, doesnot require any changes to the feature represen-tation.
Instead, it operates on translation pairs,usually on sentence level, applying the availablesource-side model to the source sentence andtransferring the resulting annotations through theword alignment links to the target one.
The qualityof predictions on source sentences depends heav-ily on the quality of parallel data and the domainit belongs to (or, rather, the similarity between thisdomain and that of the corpus the source-languagemodel was trained on).
The transfer itself alsointroduces errors due to translation shifts (Cyrus,2006) and word alignment errors, which may leadto inaccurate predictions.
These issues are gen-erally handled using heuristics (Pad?o and Lapata,2006) and filtering, for example based on align-ment coverage (van der Plas et al, 2011).579Figure 1: Dependency-based semantic role labeling example.
The top arcs depict dependency relations,the bottom ones ?
semantic role structure.
Rendered with https://code.google.com/p/whatswrong/.1.1 MotivationThe approach proposed here, which we will referto as feature representation projection (FRP), con-stitutes an alternative to direct model transfer andannotation projection and can be seen as a com-promise between the two.It is similar to direct transfer in that we alsouse a shared feature representation.
Instead ofdesigning this representation manually, however,we create compact monolingual feature represen-tations for source and target languages separatelyand automatically estimate the mapping betweenthe two from parallel data.
This allows us to makeuse of language-specific annotations and accountfor the interplay between different types of infor-mation.
For example, a certain preposition at-tached to a token in the source language mightmap into a morphological tag in the target lan-guage, which would be hard to handle for tradi-tional direct model transfer other than using somekind of refinement procedure involving paralleldata.
Note also that any such refinement procedureapplicable to direct transfer would likely work forFRP as well.Compared to annotation projection, our ap-proach may be expected to be less sensitive to par-allel data quality, since we do not have to com-mit to a particular prediction on a given instancefrom parallel data.
We also believe that FRPmay profit from using other sources of informa-tion about the correspondence between source andtarget feature representations, such as dictionaryentries, and thus have an edge over annotation pro-jection in those cases where the amount of paralleldata available is limited.2 EvaluationWe evaluate feature representation projection onthe task of dependency-based semantic role label-ing (SRL) (Haji?c et al, 2009).This task consists in identifying predicates andtheir arguments in sentences and assigning eachargument a semantic role with respect to its pred-icate (see figure 1).
Note that only a single word?
the syntactic head of the argument phrase ?
ismarked as an argument in this case, as opposedto constituent- or span-based SRL (Carreras andM`arquez, 2005).
We focus on the assignment ofsemantic roles to identified arguments.For the sake of simplicity we cast it as a multi-class classification problem, ignoring the interac-tion between different arguments in a predicate.
Itis well known that such interaction plays an impor-tant part in SRL (Punyakanok et al, 2008), but itis not well understood which kinds of interactionsare preserved across languages and which are not.Also, should one like to apply constraints on theset of semantic roles in a given predicate, or, forexample, use a reranker (Bj?orkelund et al, 2009),this can be done using a factorized model obtainedby cross-lingual transfer.In our setting, each instance includes the wordtype and part-of-speech and morphological tags (ifany) of argument token, its parent and correspond-ing predicate token, as well as their dependencyrelations to their respective parents.
This repre-sentation is further denoted ?0.2.1 ApproachWe consider a pair of languages (Ls, Lt) andassume that an annotated training set DsT={(xs, ys)} is available in the source language aswell as a parallel corpus of instance pairs Dst={(xs, xt)}and a target dataset DtE={xt}thatneeds to be labeled.We design a pair of intermediate compactmonolingual feature representations ?s1and ?t1and models Msand Mtto map source and targetsamples xsand xtfrom their original representa-tions, ?s0and ?t0, to the new ones.
We use the par-580allel instances in the new feature representation?Dst={(xs1, xt1)}={(Ms(xs),Mt(xt))}to determine the mapping Mts(usually, linear) be-tween the two spaces:Mts= argmaxM?(xs1,xt1??Dst)???xs1?M(xt1)??
?2Then a classification model Myis trained on thesource training data?DsT= {(xs1, ys)} = {(Ms(xs), ys)}and the labels are assigned to the target samplesxt?
DtEusing a composition of the models:yt= My(Mts(Mt(xt)))2.2 Feature RepresentationOur objective is to make the feature represen-tation sufficiently compact that the mapping be-tween source and target feature spaces could bereliably estimated from a limited amount of paral-lel data, while preserving, insofar as possible, theinformation relevant for classification.Estimating the mapping directly from raw cat-egorical features (?0) is both computationally ex-pensive and likely inaccurate ?
using one-hot en-coding the feature vectors in our experimentswould have tens of thousands of components.There is a number of ways to make this repre-sentation more compact.
To start with, we re-place word types with corresponding neural lan-guage model representations estimated using theskip-gram model (Mikolov et al, 2013a).
Thiscorresponds to Msand Mtabove and reduces thedimension of the feature space, making direct es-timation of the mapping practical.
We will refer tothis representation as ?1.To go further, one can, for example, applydimensionality reduction techniques to obtain amore compact representation of ?1by eliminatingredundancy or define auxiliary tasks and producea vector representation useful for those tasks.
Insource language, one can even directly tune an in-termediate representation for the target problem.2.3 BaselinesAs mentioned above we compare the performanceof this approach to that of direct transfer and an-notation projection.
Both baselines are using thesame set of features as the proposed model, as de-scribed earlier.The shared feature representation for di-rect transfer is derived from ?0by replacinglanguage-specific part-of-speech tags with univer-sal ones (Petrov et al, 2012) and adding cross-lingual word clusters (T?ackstr?om et al, 2012) toword types.
The word types themselves are left asthey are in the source language and replaced withtheir gloss translations in the target one (Zemanand Resnik, 2008).
In English-Czech and Czech-English we also use the dependency relation infor-mation, since the annotations are partly compati-ble.The annotation projection baseline implementa-tion is straightforward.
The source-side instancesfrom a parallel corpus are labeled using a classi-fier trained on source-language training data andtransferred to the target side.
The resulting anno-tations are then used to train a target-side classifierfor evaluation.
Note that predicate and argumentidentification in both languages is performed us-ing monolingual classifiers and only aligned pairsare used in projection.
A more common approachwould be to project the whole structure from thesource language, but in our case this may giveunfair advantage to feature representation projec-tion, which relies on target-side argument identifi-cation.2.4 ToolsWe use the same type of log-linear classifiersin the model itself and the two baselines toavoid any discrepancy due to learning proce-dure.
These classifiers are implemented usingPYLEARN2 (Goodfellow et al, 2013), based onTHEANO (Bergstra et al, 2010).
We also use thisframework to estimate the linear mapping Mtsbe-tween source and target feature spaces in FRP.The 250-dimensional word representations for?1are obtained using WORD2VEC tool.
Bothmonolingual data and that from the parallel cor-pus are included in the training.
In Mikolov et al(2013b) the authors consider embeddings of up to800 dimensions, but we would not expect to bene-fit as much from larger vectors since we are usinga much smaller corpus to train them.
We did nottune the size of the word representation to our task,as this would not be appropriate in a cross-lingualtransfer setup, but we observe that the classifieris relatively robust to their dimension when evalu-581ated on source language ?
in our experiments theperformance of the monolingual classifier does notimprove significantly if the dimension is increasedpast 300 and decreases only by a small margin(less than one absolute point) if it is reduced to100.
It should be noted, however, that the dimen-sion that is optimal in this sense is not necessarilythe best choice for FRP, especially if the amountof available parallel data is limited.2.5 DataWe use two language pairs for evaluation:English-Czech and English-French.
In the firstcase, the data is converted from Prague Czech-English Dependency Treebank 2.0 (Haji?c et al,2012) using the script from Kozhevnikov andTitov (2013).
In the second, we use CoNLL 2009shared task (Haji?c et al, 2009) corpus for Englishand the manually corrected dataset from van derPlas et al (2011) for French.
Since the size ofthe latter dataset is relatively small ?
one thou-sand sentences ?
we reserve the whole dataset fortesting and only evaluate transfer from English toFrench, but not the other way around.
Datasets forother languages are sufficiently large, so we take30 thousand samples for testing and use the restas training data.
The validation set in each exper-iment is withheld from the corresponding trainingcorpus and contains 10 thousand samples.Parallel data for both language pairs is de-rived from Europarl (Koehn, 2005), which we pre-process using MATE-TOOLS (Bj?orkelund et al,2009; Bohnet, 2010).3 ResultsThe classification error of FRP and the baselinesgiven varying amount of parallel data is reportedin figures 2, 3 and 4.
The training set for eachlanguage is fixed.
We denote the two baselines AP(annotation projection) and DT (direct transfer).The number of parallel instances in these exper-iments is shown on a logarithmic scale, the valuesconsidered are 2, 5, 10, 20 and 50 thousand pairs.Please note that we report only a single valuefor direct transfer, since this approach does not ex-plicitly rely on parallel data.
Although some ofthe features ?
namely, gloss translations and cross-lingual clusters ?
used in direct transfer are, in fact,derived from parallel data, we consider the effectof this on the performance of direct transfer to beindirect and outside the scope of this work.2 5 10 20 500.340.360.380.400.42Number of parallel instances, ?103ErrorFRPAPDTFigure 2: English-Czech transfer results2 5 10 20 500.320.340.360.380.40Number of parallel instances, ?103ErrorFRPAPDTFigure 3: Czech-English transfer resultsThe rather inferior performance of direct trans-fer baseline on English-French may be partiallyattributed to the fact that it cannot rely on depen-dency relation features, as the corpora we considermake use of different dependency relation inven-tories.
Replacing language-specific dependencyannotations with the universal ones (McDonaldet al, 2013) may help somewhat, but we wouldstill expect the methods directly relying on paral-lel data to achieve better results given a sufficientlylarge parallel corpus.Overall, we observe that the proposed methodwith ?1representation demonstrates performancecompetitive to direct transfer and annotation pro-jection baselines.5822 5 10 20 500.340.360.380.40Number of parallel instances, ?103ErrorFRPAPDTFigure 4: English-French transfer results4 Additional Related WorkApart from the work on direct/projected transferand annotation projection mentioned above, theproposed method can be seen as a more explicitkind of domain adaptation, similar to Titov (2011)or Blitzer et al (2006).It is also somewhat similar in spirit to Mikolovet al (2013b), where a small number of wordtranslation pairs are used to estimate a mappingbetween distributed representations of words intwo different languages and build a word transla-tion model.5 ConclusionsIn this paper we propose a new method of cross-lingual model transfer, report initial evaluation re-sults and highlight directions for its further devel-opment.We observe that the performance of this methodis competitive with that of established cross-lingual transfer approaches and its application re-quires very little manual adjustment ?
no heuris-tics or filtering and no explicit shared feature rep-resentation design.
It also retains compatibilitywith any refinement procedures similar to pro-jected transfer (McDonald et al, 2011) that mayhave been designed to work in conjunction withdirect model transfer.6 Future WorkThis paper reports work in progress and there isa number of directions we would like to pursuefurther.Better Monolingual Representations The rep-resentation we used in the initial evaluation doesnot discriminate between aspects that are relevantfor the assignment of semantic roles and those thatare not.
Since we are using a relatively small set offeatures to start with, this does not present much ofa problem.
In general, however, retaining only rel-evant aspects of intermediate monolingual repre-sentations would simplify the estimation of map-ping between them and make FRP more robust.For source language, this is relatively straight-forward, as the intermediate representation can bedirectly tuned for the problem in question usinglabeled training data.
For target language, how-ever, we assume that no labeled data is availableand auxiliary tasks have to be used to achieve this.Alternative Sources of Information Theamount of parallel data available for manylanguage pairs is growing steadily.
However,cross-lingual transfer methods are often appliedin cases where parallel resources are scarce or ofpoor quality and must be used with care.
In suchsituations an ability to use alternative sources ofinformation may be crucial.
Potential sourcesof such information include dictionary entries orinformation about the mapping between certainelements of syntactic structure, for example aknown part-of-speech tag mapping.The available parallel data itself may also beused more comprehensively ?
aligned argumentsof aligned predicates, for example, constitute onlya small part of it, while the mapping of vector rep-resentations of individual tokens is likely to be thesame for all aligned words.Multi-source Transfer One of the strong pointsof direct model transfer is that it naturally fits themulti-source transfer setting.
There are severalpossible ways of adapting FRP to such a setting.It remains to be seen which one would producethe best results and how multi-source feature rep-resentation projection would compare to, for ex-ample, multi-source projected transfer (McDonaldet al, 2011).AcknowledgementsThe authors would like to acknowledge thesupport of MMCI Cluster of Excellence andSaarbr?ucken Graduate School of Computer Sci-ence and thank the anonymous reviewers for theirsuggestions.583ReferencesPaolo Annesi and Roberto Basili.
2010.
Cross-lingualalignment of FrameNet annotations through hiddenMarkov models.
In Proceedings of the 11thinterna-tional conference on Computational Linguistics andIntelligent Text Processing, CICLing?10, pages 12?25.
Springer-Verlag.James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, David Warde-Farley, andYoshua Bengio.
2010.
Theano: a CPU andGPU math expression compiler.
In Proceedingsof the Python for Scientific Computing Conference(SciPy), Austin, TX.Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.2009.
Multilingual semantic role labeling.
In Pro-ceedings of the Thirteenth Conference on Computa-tional Natural Language Learning (CoNLL 2009):Shared Task, pages 43?48, Boulder, Colorado, June.Association for Computational Linguistics.John Blitzer, Ryan McDonal, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proc.
Conference on EmpiricalMethods in Natural Language Processing, Sydney,Australia.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rdInternational Conference on Computa-tional Linguistics (Coling 2010), pages 89?97, Bei-jing, China, August.Xavier Carreras and Llu?
?s M`arquez.
2005.
Introduc-tion to the CoNLL-2005 shared task: Semantic rolelabeling.
In Proceedings of CoNLL-2005, Ann Ar-bor, MI USA.Lea Cyrus.
2006.
Building a resource for studyingtranslation shifts.
CoRR, abs/cs/0606096.Dipanjan Das and Slav Petrov.
2011.
Unsuper-vised part-of-speech tagging with bilingual graph-based projections.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages600?609, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Greg Durrett, Adam Pauls, and Dan Klein.
2012.
Syn-tactic transfer using a bilingual lexicon.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 1?11,Jeju Island, Korea, July.
Association for Computa-tional Linguistics.Kuzman Ganchev, Jennifer Gillenwater, and BenTaskar.
2009.
Dependency grammar induction viabitext projection constraints.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages369?377, Suntec, Singapore, August.
Associationfor Computational Linguistics.Ian J. Goodfellow, David Warde-Farley, Pascal Lam-blin, Vincent Dumoulin, Mehdi Mirza, Razvan Pas-canu, James Bergstra, Fr?ed?eric Bastien, and YoshuaBengio.
2013.
Pylearn2: a machine learning re-search library.
CoRR, abs/1308.4214.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In Proceedings of theThirteenth Conference on Computational NaturalLanguage Learning (CoNLL 2009): Shared Task,pages 1?18, Boulder, Colorado.Jan Haji?c, Eva Haji?cov?a, Jarmila Panevov?a, PetrSgall, Ond?rej Bojar, Silvie Cinkov?a, Eva Fu?c?
?kov?a,Marie Mikulov?a, Petr Pajas, Jan Popelka, Ji?r?
?Semeck?y, Jana?Sindlerov?a, Jan?St?ep?anek, JosefToman, Zde?nka Ure?sov?a, and Zden?ek?Zabokrtsk?y.2012.
Announcing Prague Czech-English depen-dency treebank 2.0.
In Nicoletta Calzolari (Con-ference Chair), Khalid Choukri, Thierry Declerck,Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-iani, Jan Odijk, and Stelios Piperidis, editors, Pro-ceedings of the Eight International Conference onLanguage Resources and Evaluation (LREC?12), Is-tanbul, Turkey, May.
European Language ResourcesAssociation (ELRA).Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel text.Natural Language Engineering, 11(3):311?325.Alexandre Klementiev, Ivan Titov, and Binod Bhat-tarai.
2012.
Inducing crosslingual distributed rep-resentations of words.
In Proceedings of the Inter-national Conference on Computational Linguistics(COLING), Bombay, India.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Conference Pro-ceedings: the tenth Machine Translation Summit,pages 79?86, Phuket, Thailand.
AAMT.Mikhail Kozhevnikov and Ivan Titov.
2013.
Cross-lingual transfer of semantic role labeling models.In Proceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 1190?1200, Sofia, Bulgaria,August.
Association for Computational Linguistics.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?11, pages 62?72, Edinburgh, United King-dom.
Association for Computational Linguistics.584Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuz-man Ganchev, Keith Hall, Slav Petrov, HaoZhang, Oscar T?ackstr?om, Claudia Bedini, N?uriaBertomeu Castell?o, and Jungmee Lee.
2013.
Uni-versal dependency annotation for multilingual pars-ing.
In Proceedings of the 51st Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 92?97, Sofia, Bulgaria,August.
Association for Computational Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
CoRR, abs/1301.3781.Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.2013b.
Exploiting similarities among languages formachine translation.
CoRR, abs/1309.4168.Sebastian Pad?o and Mirella Lapata.
2006.
Optimalconstituent alignment with edge covers for semanticprojection.
In Proc.
44thAnnual Meeting of Associ-ation for Computational Linguistics and 21stInter-national Conf.
on Computational Linguistics, ACL-COLING 2006, pages 1161?1168, Sydney, Aus-tralia.Sebastian Pad?o and Mirella Lapata.
2009.
Cross-lingual annotation projection for semantic roles.Journal of Artificial Intelligence Research, 36:307?340.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceedings ofLREC, May.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2):257?287.David A Smith and Jason Eisner.
2009.
Parser adap-tation and projection with quasi-synchronous gram-mar features.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 822?831.
Association for Com-putational Linguistics.Anders S?gaard.
2011.
Data point selection for cross-language adaptation of dependency parsers.
In Pro-ceedings of the 49thAnnual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, volume 2 of HLT ?11, pages682?686, Portland, Oregon.
Association for Com-putational Linguistics.Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-reit.
2012.
Cross-lingual word clusters for directtransfer of linguistic structure.
In Proc.
of the An-nual Meeting of the North American Associationof Computational Linguistics (NAACL), pages 477?487, Montr?eal, Canada.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
Transactions of the Association for Computa-tional Linguistics, 1:1?12.Ivan Titov.
2011.
Domain adaptation by constraininginter-domain variability of latent feature representa-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 62?71, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.Sara Tonelli and Emanuele Pianta.
2008.
Frame infor-mation transfer from English to Italian.
In Proceed-ings of LREC 2008.Reut Tsarfaty.
2013.
A unified morpho-syntacticscheme of stanford dependencies.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics (Volume 2: Short Papers),pages 578?584, Sofia, Bulgaria, August.
Associa-tion for Computational Linguistics.Lonneke van der Plas, Paola Merlo, and James Hen-derson.
2011.
Scaling up automatic cross-lingualsemantic role annotation.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,HLT ?11, pages 299?304, Portland, Oregon, USA.Association for Computational Linguistics.Chenhai Xi and Rebecca Hwa.
2005.
A backoffmodel for bootstrapping resources for non-englishlanguages.
In Proceedings of Human LanguageTechnology Conference and Conference on Empiri-cal Methods in Natural Language Processing, pages851?858, Vancouver, British Columbia, Canada,October.
Association for Computational Linguistics.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2001.
Inducing multilingual text analysistools via robust projection across aligned corpora.In Proceedings of the first international conferenceon Human language technology research, pages 1?8.
Association for Computational Linguistics.Daniel Zeman and Philip Resnik.
2008.
Cross-language parser adaptation between related lan-guages.
In Proceedings of the IJCNLP-08 Workshopon NLP for Less Privileged Languages, pages 35?42, Hyderabad, India, January.
Asian Federation ofNatural Language Processing.Daniel Zeman, David Mare?cek, Martin Popel,Loganathan Ramasamy, Jan?St?ep?anek, Zden?ek?Zabokrtsk?y, and Jan Haji?c.
2012.
Hamledt: Toparse or not to parse?
In Nicoletta Calzolari (Con-ference Chair), Khalid Choukri, Thierry Declerck,Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-iani, Jan Odijk, and Stelios Piperidis, editors, Pro-ceedings of the Eight International Conference onLanguage Resources and Evaluation (LREC?12), Is-tanbul, Turkey, may.
European Language ResourcesAssociation (ELRA).585
