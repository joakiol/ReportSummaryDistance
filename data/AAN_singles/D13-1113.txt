Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1124?1135,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsIdentifying Multiple Userids of the Same AuthorTieyun Qian Bing LiuState Key Laboratory of Software Eng.Wuhan UniversityDepartment of Computer ScienceUniversity of Illinois at Chicago16 Luojiashan Road 851 South Morgan St., ChicagoWuhan, Hubei 430072, China  IL, USA, 60607qty@whu.edu.cn liub@cs.uic.eduAbstractThis paper studies the problem of identifyingusers who use multiple userids to post in so-cial media.
Since multiple userids may belongto the same author, it is hard to directly applysupervised learning to solve the problem.
Thispaper proposes a new method, which still usessupervised learning but does not require train-ing documents from the involved userids.
In-stead, it uses documents from other useridsfor classifier building.
The classifier can beapplied to documents of the involved userids.This is possible because we transform thedocument space to a similarity space andlearning is performed in this new space.
Ourevaluation is done in the online review do-main.
The experimental results using a largenumber of userids and their reviews show thatthe proposed method is highly effective.1 IntroductionIt is common knowledge that some users in socialmedia register multiple accounts/userids to postarticles, blogs, reviews, etc.
There are many rea-sons for doing this.
For example, due to past post-ings, a user may become despised by others.He/she then registers another userid in order toregain his/her status.
A user may also use multipleuserids to instigate controversy or debates to popu-larize a topic to make it ?hot?
or even just to pro-mote activities at a website.
Yet, a user may alsouse multiple userids to post fake or deceptive opin-ions to promote or demote some products (Liu,2012).
It is thus important to develop technologiesto identify such multi-id users.
This paper dealswith this problem based on writing style and otherlinguistic clues.Problem definition: Given a set of userids ID ={id1, ?, idn} and each idi has a set of documentsDi, we want to identify userids that belong to thesame physical author.The main related works to ours are in the area ofauthorship attribution (AA), which aims to identifyauthors of documents.
AA is often solved usingsupervised learning.
Let A = {a1, ?, ak} be a set ofauthors (or classes) and each author ai ?
A has aset of training documents Di.
A classifier is thenbuilt to decide the author a of each test documentd, where a ?
A.
We will discuss this and other re-lated works in Section 2.This supervised AA formulation, however, isnot suitable for our task because we only haveuserids but not real authors.
Since some of theuserids may belong to the same author, we cannottreat each userid as a class because in that case, wewill be classifying based on userids, which won?thelp us find authors with multiple userids (see Sec-tion 7 also).This paper proposes a novel algorithm.
To sim-plify the presentation, we assume that at most twouserids can belong to a single author, but the algo-rithm can be extended to handle more than twouserids from the same author.
Using this assump-tion, the algorithm works in two steps:1.
Candidate identification: For each userid idi,we first find the most likely userid idj (i ?
j) thatmay have the same author as idi.
We call idj thecandidate of idi.
We also call this function can-did-iden, i.e., idj = candid-iden(idi).
For easypresentation, here we only use one argument for*  The work was mainly done when the first author was visit-ing the University of Illinois at Chicago.1124candid-iden.
In the computation, it needs morearguments (see Section 4).2.
Candidate confirmation: In the reverse order,we apply the function candid-iden on idj, whichproduces idk, i.e., idk = candid-iden(idj).Decision making: If k = i, we conclude that idiand idj are from the same author.
Otherwise, idiand idj are not from the same author.The key of the algorithm is candid-iden.
An ob-vious approach for candid-iden is to use an infor-mation retrieval method.
We can first split thedocuments Di of each idi into two subsets, a queryset Qi and a sample set Si.
We then compare eachquery document in Qi with each sample documentin Sj from other userids idj (?
ID ?
{idi}).
Cosinecan be used here for similarity comparison.
All thesimilarity scores are then aggregated and used torank the userids in ID ?
{idi}.
The top rankeduserid is the candidate for idi.
Note that partition-ing the documents of a userid idi into the query setQi and the sample set Si is crucial here.
We cannotuse all documents in Di to compare with all docu-ments in Dj.
If so and we get candid-iden(idi) = idj,we will definitely get candid-iden(idj) = idi sincethe similarity function is symmetric.This cosine similarity based method, however,does not work well (see Section 7).
We propose asupervised learning method to compute the scores.For this, we need to reformulate the problem.The idea of this reformulation is to learn in asimilarity space rather than in the original docu-ment space as in traditional AA.
In the new formu-lation, each document d is still represented as afeature vector, but the vector no longer representsthe document d itself.
Instead, it represents a set ofsimilarities between the document d and a query q.We call this method learning in the similarityspace (LSS).Specifically, in LSS, each document d is firstrepresented with a document space vector (called ad-vector) based on the document itself as in thetraditional classification learning of AA.
Each fea-ture in the d-vector is called a d-feature (docu-ment-feature).
A query document q is representedin the same way.
We then produce a similarity vec-tor sv (called s-vector) for d. sv consists of a set ofsimilarity values between document d (in a d-vector) and query q (in a d-vector):sv =Sim(d, q),where Sim is a similarity function consists of a setof similarity measures.
Thus, the d-vector for doc-ument d in the document space is transformed toan s-vector sv for d in the similarity space.
Eachfeature in sv is called an s-feature.
For example,we have the following d-vector for query q:q: 1:1 2:1 6:2where x:z represents a d-feature x (a word) and itsfrequency z in q.
We also have two non-querydocuments, one is d1 which is written by the authorof query q and the other is d2 which is not writtenby query author q.
Their d-vectors are:d1:  1:2 2:1 3:1  d2:  2:2 3:1 5:2If we use cosine as the first similarity measure inSim, we can generate an s-feature 1:0.50 for d1(cosine(q, d1) = 0.50) and an s-feature 1:0.27 for d2(cosine(q, d2) = 0.27).
If we have more similaritymeasures more s-features can be produced.
Theresulting two s-vectors for d1 and d2 with theirclass labels, 1 and -1, are as follows:d1: 1 1:0.50 ?
d2:  -1  1:0.27 ?Class 1 means ?written by author of query q?, alsocalled q-positive, and class -1 means ?not writtenby author of query q?, also called q-negative.LSS gives us a two-class classification problem.In this formulation, a test userid and his/her docu-ments do not have to be seen in training as long asa set of known documents from this userid isavailable.
Any supervised learning method can beused to build a classifier.
We use SVM.
The result-ing classifier is employed to compute a score foreach review to be used in the two-step algorithmabove to find the candidate for each userid andthen the userids with the same authors.Due to the use of query documents, the LSSformulation has some resemblance to documentranking based on learning to rank (Li, 2011; Liu,2011).
However, LSS is very different because weturn the problem into a supervised classificationproblem.
The key difference between learning torank and classification is that ranking will alwaysput some documents at the top even if the desireddocuments do not exist.
However, classificationwill not return any document if the desired docu-ments do not exist in the test data (unless there areclassification errors).
Our Type II experiments inSection 7 were specifically designed for testingsuch non-existence situations.1125Using online review as the application domain,we conduct experiments on a large number of re-views and their author/reviewer userids from Am-azon.com.
The results show that the proposedalgorithm is highly accurate and outperforms threestrong baselines markedly.2 Related WorkA similar problem was attempted in (Chen et al2004) in the context of open forums where usersinteract with each other in their discussions.
Theirmethod is based on post relationships and intervalsbetween posts.
It does not use any linguistic clues.It is thus not applicable to domains like online re-views.
Reviews do not involve user interactionssince each review is independent of other reviews.Novak et alalso solved the same problem underthe name of ?Anti-aliasing?
(Novak et al 2004).They used a clustering based method which as-sumed the number of actual authors is known.
Thisis unrealistic in practice as there is no way to knowwhich author has and does not have multiple ids.Our work is also related to authorship attribu-tion (AA).
However, to our knowledge, our prob-lem has not been attempted in AA.
Existing worksfocused on two main themes: finding good writingstyle features, and developing effective classifica-tion methods.
On finding good features (d-featuresin our case), it was found that the most promisingfeatures are function words (Mosteller, 1964; Ar-gamon and Levitan, 2004; Argamon et al 2007)and rewrite rules (Halteren et al 1996).
Length(Gamon 2004; Graham et al 2005), richness (Hal-teren et al 1996; Koppel and Schler, 2004), punc-tuations (Graham et al 2005), character n-grams(Grieve, 2007; Hedegaard and Simonsen, 2011),word n-grams (Burrows, 1992; Sanderson andGuenter 2006), POS n-grams (Gamon, 2004; Hirstand Feiguina, 2007), syntactic category pairs (Na-rayanan et al 2012) are also useful.On classification, numerous methods have beentried, e.g., Bayesian analysis (Mosteller, 1964),discriminant analysis (Stamatatos et al 2000),PCA (Hoover, 2001), neural networks (Graham etal., 2005; Zheng et al 2006; Graham et al 2005),clustering (Sanderson and Guenter, 2006), decisiontrees (Uzuner and Katz, 2005; Zhao and Zobel,2005), regularized least squares classification(Narayanan et al 2012),   and SVM (Diederich etal., 2000; Gamon 2004; Koppel and Schler, 2004;Hedegaard and Simonsen, 2011).
Among them,SVM was found to be most accurate (Li et al2006; Kim et al 2011).
Although we also usesupervised learning, we do not learn in the originaldocument space as these existing methods do.
Thetransformation is important because it enables usto use documents from other authors in training.The traditional supervised learning (TSL) cannotdo that.
In our case, the only documents that TSLcan use for training are the queries in the testingset.
However, as we will see in our experiments,such a method performs poorly.Since we use online reviews as our experimentdomain, our work is related to fake review detec-tion (Jindal and Liu, 2008) as imposters can usemultiple userids to post fake reviews.
Existing re-search has proposed many methods to detect fakereviewers (Lim et al 2010; Wang et al 2011;Mukherjee et al 2012) and fake reviews (Jindaland Liu, 2008; Ott et al 2011, 2012; Li et al2011; Feng et al 2012).
However, none of themidentifies userids belonging to the same person.3 Learning in the Similarity SpaceWe now formulate the proposed supervisedlearning in the similarity space (LSS), which willbe used in the candid-iden function in our algo-rithm to be discussed in Section 4.The key difference between LSS and the classicdocument space learning is in the document repre-sentation.
Another difference is in the testingphase.
We discuss testing first.Test data: We are given:?
A query q from query author (userid) aq?
A set of test documents DT = {dt1, ?, dtm}.Goal: classify the test documents into those au-thored by aq and those not authored by aq.We note the following points:i)  This is like a retrieval scenario, but we use su-pervised learning to perform the task.ii) Unlike traditional supervised classification,here the test query author aq does not have tobe used in training.
But we are given a querydocument q from aq.
Clearly, in practice, wecan have multiple query documents from aq,which we will discuss in Section 4.Training document representation: As notedearlier, each document is represented with a simi-larity vector (s-vector) computed using a similarity1126function Sim.
Sim takes a query document and anon-query document and produces a vector of sim-ilarity values or s-features to represent the non-query document.
We present the detail below:Let the set of training authors be AR = {ar1, ..,arn}.
Each author ari has a set of documents DRi.Each document in DRi is first represented with adocument vector (or d-vector).
The algorithm forproducing the training set, called s-training set, isgiven in Figure 1.We randomly select a small set of queries Qifrom documents DRi of each author ari (lines 1,and 2).
For each query qij ?
Qi (line 3), it selects aset of documents DRij also from DRi (excluding qij)of the same author (line 4) to be the positive doc-uments for qij, called q-positive and labeled 1.Then, for each document drijk in DRij, a q-positives-training example with the label 1 is generated fordrijk by computing the similarities of qij and drijkusing the similarity function Sim (lines 5, 6).
Inline 7, it selects a set of documents DRij,rest fromother authors to be the negative documents for qij,called q-negative and labeled -1.
For each docu-ment drijk,rest in DRij,rest (line 8), a q-negative s-training example with label -1 is generated for drijkby computing the similarities of qij and drijk,rest us-ing Sim (line 9).
How to select Qi, DRij and DRij,rest(lines 2, 4 and 7) is left open intentionally to giveflexibility in implementation.This formulation gives us a two-class classifica-tion problem.
The classes are 1 (q-positive mean-ing ?written by author of query qij?)
and -1 (q-negative meaning ?not written by author of queryqij.?
Figure 2 shows what the s-training data lookslike.
For easy presentation, we assume that thereare k queries in every Qi, and p documents in everyDRij and u documents in every DRij,rest.
The num-ber of authors is n. Each author ari generatesk?
(p+u) s-training examples.
As we will see inSection 7, k can be very small, even 1.Complexity: In the worst case, every document1.
For each document set Di of idi ?
ID do2.
partition Di into two subsets:(1) query set Qi and (2) sample set Si;3.
For each document set Di of idi ?
ID do// step 1: candidate identification4.
idj = candid-iden(idi, ID), i < j;// step 2: candidate confirmation5.
idk = candid-iden(idj, ID), k ?
j;6.
If k = i then idi and idj are from the same author8.
else  idi and idj are not from the same authorFigure 3: Identifying userids from the same authorsFunction candidate-iden(idi, ID)1.
For each sample document set Sj of idj ?
ID-{idi} do2.
pcount[idj],  psum[idj], psqsum[idj], max[idj] = 0;3.
For each query qi ?
Qi do4.
For each sample sjf ?
Sj do5.
ssjf = <(idi, qi), (Sim(sjf, qi), ?)>;6.
Classify ssjf  using the classifier built earlier;7.
If ssjf is classified positive, i.e., 1 then8.
pcount[idj] = pcount[idj] + 1;9.  psum[idj] = psum[idj] + ssjf.score10 psqsum[idj] = psqsum[idj] + (ssjf.score)211.
If ssif.score > max[idj] then12.
max[idj] = srjf.score// Four methods to decide which idj is the candidate for idi13.
If for all idj ?
ID-{idi}, pcount[idi] = 0 then14.
])(max[maxarg }{ jidIDid idcid ij ???15.
Else)||][(maxarg}{ jjidIDid Sidpcountcidij ??
?// 1.
Voting16.
)||][(maxarg}{ jjidIDid Sidpsumcidij ??
?// 2.
ScoreSum17.
)||])[((maxarg2}{ jjidIDid Sidpsumcidij ??
?// 3.
ScoreSqSum18.
])(max[maxarg }{ jidIDid idcid ij ??
?// 4.
ScoreMax19.
return cid;Figure 4: Identifying the candidate1.
For each author ari ?
AR2.
select a set of query documents Qi  ?
DRi3.
For each query qij ?
Qi// produce positive s-training examples4.
select a set of documents from author ariDRij ?
DRi ?
{qij}5.
For each document drijk ?
DRij6.
produce an s-training example for drijk,(Sim(drijk, qij), 1)// produce negative s-training examples7.
select a set of documents from the rest of authorsDRij,rest ?
(DR1 ?
?
?
DRn) ?
DRi8.
For each document drijk,rest ?
DRij,rest9.
produce an s-training example for drijk,rest,(Sim(drijk,rest, qij), -1)Figure 1: Generating s-training examples//  Author ar1 ?// positive (1) s-training examples(Sim(dr111, q11), 1),  ?,  (Sim(dr11p, q11), 1)?
(Sim(dr1k1, q1k), 1),   ?,   (Sim(dr1kp, q1k), 1)// negative (-1) s-training examples(Sim(dr111.rest, q11), -1),  ?,  (Sim(dr11u.rest, q11), -1)?
(Sim(dr1k1.rest, q1k), -1),  ?,   (Sim(dr1ku.rest, q1k), -1)?Figure 2: s-training examples1127can serve as a query or a non-query document.Then we need to compute all pairwise documentsimilarities.
If the number of training documents ism, the complexity is O(m2), which is both spaceand computation expensive.
However, in practice,we don?t need all pairwise comparisons.
Only asmall subset is sufficient (see Section 7).Test document representation: Like trainingdocuments, test documents are represented as s-vectors as well in the similarity space.Given a query q from author aq and a set of testdocuments DT, each test document dti is convertedto a s-vector svi = Sim(dti, q).
To reflect svi is com-puted based on query q from author aq, a s-testcase is thus represented as <(aq, q), (svi, ?
)>.Training: A binary classifier is learned using thes-training data.
Each s-training example is repre-sented with (sv, y), where sv is an s-vector and y(?
{1, -1}) is its class.
Any supervised learningalgorithm, e.g., SVM, can be applied.Testing: The classifier is applied to each s-testcase <(aq, q), (svi, ?
)> (where svi = S(dti, q)) togive it a class q-positive or q-negative.
Note thatthe classifier is only applied on svi.In most cases, classification based on a single que-ry is inaccurate.
Using multiple queries of an au-thor can classify much more accurately.4 Identify Userids of the Same AuthorWe now expand the sketch of the two-step algo-rithm in Section 1 based on the problem statementin Section 1.
The algorithm is given in Figure 3.Lines 1-2 partitions the documents set Di ofeach idi in ID = {id1, id2, ?, idn}, the set of useridsthat we are working on.
How to do the partition isflexible (see Section 7).
Line 4 is the step 1 ofcandidate identification, and line 5 is the step2 ofcandidate confirmation.
Lines 6-8 is the decisionmaking of step 2 (see Section 1).
Line 6 produceda classification score using the classifier describedin Section 3.
The key function here is candid-iden.Its algorithm is in Figure 4.The candid-iden function takes two arguments:the query userid idi and the whole set of useridsID.
It classifies each sample ssjf in sample set Sj ofidj ?
ID-{idi} to positive (qi-positive) or negative(qi-negative) (lines 4, 5, 6).
We then aggregate theclassification results to determine which userid islikely to have the same author as idi.One simple aggregation method is voting.
Wecount the total number of positive classifications ofthe sample documents of each userid in ID-{idi}.The userid idj with the highest count is the candi-date cid which may share the same author as queryidi.
cid is returned as the candidate.There are also other methods, which can dependon what output value the classifier produces.
Herewe propose four methods including the votingmethod above.
The other three methods requiresthe classifier to produces a prediction score, whichreflects the positive and negative certainty.
Manyclassification algorithms produce such a score.Here we use SVM.
For each classification, SVMoutputs a positive or negative score indicating thecertainty that the test case is positive or negative.To save space, all four alternative methods aregiven in Figure 4.
Line 2 initializes some variablesfor recording the aggregated values for the finaldecision making.
The four methods are as follows:1).
Voting: For each sample from userid idj, if it isclassified as positive, one vote/count is addedto pcount[idj].
The userid with the highestpcount is regarded as the candidate userid, cid(line 15).
Note that the normalization is ap-plied because the sizes of the sample sets Sjcan be different for different userids.
Lines 13and 14 mean that if all documents of alluserids are classified as negative (pcount[idj] =0, which also implies psum[idj] = psqsum[idj]= 0), we use method 4).2).
ScoreSum: This method works similarly to thevoting method above except that instead ofcounting positive classifications, this methodsums up all scores of positive classifications inpsum[idj] for each userid (line 9).
The decisionis also made similarly (line 16).3).
ScoreSqSum: This method works similarly toScoreSum above except that it sums up thesquared scores of positive classifications inpsqsum[idj] for each userid (line 10).
The deci-sion is also made similarly (line 17).4).
ScoreMax: This method works similarly to thevoting method as well except that it finds themaximum classification score for the docu-ments of each userid (lines 11 and 12).
Thedecision is made in line 18.5 D-featuresWe now compute s-features (similarity features)1128for each non-query document based on a querydocument.
Since s-features are calculated using d-features of a non-query document and a querydocument, we thus discuss d-features first, whichare extracted from each document itself.
We em-ploy 26 d-features in four categories: length d-features, frequency based d-features, tf.idf based d-features, and richness d-features.
Although manyfeatures below have been used in various tasksbefore, our key contribution is solving a new prob-lem based on a new learning formulation (LSS).Length d-feature: We derive three length d-features from each raw document: (1) averagesentence length (in terms of word count); (2)average word length (in terms of character countin one word); (3) average document length (interms of word count in one document).Frequency based d-features: We extract lexical,syntactic, and stylistic tokens from the raw docu-ments and the parsed syntactic trees to produce thefollowing features:?
Lexical tokens: word unigrams?
Syntactic tokens: content-independent struc-tures: POS n-grams (1 ?
n ?
3) and rewrite rules(Halteren et al 1996; Hirst and Feiguina, 2007).A rewrite rule is a combination of a node andits immediate constituents in a syntactic tree.For example, the rewrite rule for "the bestbook" is NP->DT+JJS+NN.?
Common stylistic token: K-length word (1 ?
K ?15), punctuations, and 157 function words(www.flesl.net/Vocabulary/SinglewordLists/functionwordlist.php).?
Review specific stylistic tokens: These tokensreflect styles of reviews: all cap words, pairs ofquotation marks, pairs of brackets, exclamatorymarks, contractions, two or more consecutivenon-alphanumeric characters, model auxilia-ries (e.g., should, must), word ?recommend?
or?recommended?, sentences with the first lettercapitalized, sentences starting with This is (thisis) or This was (this was).
We then treat thesetokens as pseudo-words and count their fre-quency to form frequency d-features.TF-IDF based d-feature: For the tokens listed inthe frequency based features above, we also com-pute their tf.idf values.
We list these two kinds ofd-features separately because they will be used fordifferent s-features later.Richness d-features: This is a set of vocabularyrichness functions used to quantify the diversity ofvocabulary in text (Holmes and Forsyth, 1995).
Inthis paper, we apply them to the counts of wordunigrams, POS n-grams (1 ?
n ?
3), and rewriterules.
Here POS n-grams and rewrite rules aretreated as pseudo-words.
Let T be the total numberof tokens (words or pseudo-words), and V(T) bethe number of different tokens in a document, v bethe highest frequency of occurrence of a token, andV(m, T) be the number of tokens which occur mtimes in the document.
We use the following sixrichness measures (Yule, 1944; Burrows, 1992;Halteren et al 1996) given in Table 1: Yule?scharacteristic (K), Hapax dislegomena (S), Simp-son?s index (D), Honor?s measure (R), Brunet?smeasure (W), and Hapax legomena (H).
They giveus a set of richness d-features about word uni-grams, POS n-grams, and rewrite rules.Table 1.
Richness metrics24 12( * ( , ) )10 *vmm V m T TK T????
(2, )( )V TS V T?1( *( 1)* ( , ))*( 1)vmm m V m TD T T???
?
?100*log( )1 (1, ) / ( )TR V T V T?
?
(1, )H V T?
( ) , 0.17aV TW T a??
?6 S-FeaturesThe extracted d-features are transformed into s-features, which are a set of similarity functions ontwo documents.
We adopt five types of s-features.Sim4 Length s-features: This is a set of four simi-larity functions defined by us.
They are used for d-feature vectors of length.
The four formulae aregiven in Table 2, where lwq.
(lwd), lsq.
(lsd), and  lrq.
(lrd) denote the average word, sentence, and docu-ment length respectively, either in query q or non-query document d. They produce four s-features.Table 2.
Sim4 for computing length s-features1/ (1 log(1 | |))wq wdl l?
?
?1/ (1 log(1 | |))sq sdl l?
?
?1/ (1 log(1 | |))rq rdl l?
?
?
{ , , } { , , } { , , }22( * ) / ( ) * ( )m w s r m w s r m w s rmq md mq mdl l l l?
?
??
?
?1129Sim3 Sentence s-features: This is a set of threesentence similarity functions (Metzler et al 2005).We apply them (called Sim3) to documents.
Sim3s-features are used for frequency based d-features.The three formulae are given in Table 3, where f(t,s) is the frequency count of token t in a document s,and lq and ld are the average document length ofthe query and non-query document, respectively.Table 3.
Sim3 for computing sentence s-features( , ) / ( ( , ) ( , ) ( , ))t q d t q t d t q df t d f t q f t d f t d?
?
?
?
?
??
??
?
?
?
( , )log ( )*( , ) ( , ) ( , ) ( , )t q dt q dt q t d t q df t dNf t d f t q f t d f t d?
??
??
?
?
??
???
?
?1 * ( )*1 log(1 | |) 1 | ( , ) ( , ) |t q dq dN idf tl l f t q f t d?
??
?
?
?
?
?Sim7 Retrieval s-features: This is a set of sevensimilarity functions (Table 4) applicable to all fre-quency based d-features.
These functions wereused in information retrieval (Cao et al 2006).Table 4.
Sim7 for computing retrieval s-featureslog( ( , ) 1)t q df t d?
??
?| |log( 1)( , )t q dDf t d?
?
?
?log( ( ))t q didf t?
??
( , )log( 1)| |t q df t dd?
?
??
( , )log( * ( ) 1)| |t q df t d idf td?
?
?
?log( 25 )BM score( , ) | |log( * 1)| | ( , )t q df t d Dd f t d?
?
?
?In Table 4, f(t, d) denotes the frequency count oftoken t in a non-query document d, q denotes thequery, D is the entire collection, |.| is the size of aset, and idf is the inverse document frequency.These 7 formulae can produce 7 s-features.SimC tf-idf s-feature: This is the cosine similarityused for d-vectors represented by the tf.idf basedd-features.
SimC tf-idf produces one s-feature.SimC Richness s-feature: This is also cosine sim-ilarity.
However, it is applied to the richness d-feature vectors, and produces one s-feature.7 Experimental EvaluationWe now evaluate the proposed approach and com-pare it with baselines.
All our experiments use theSVMperf classifier (Joachims, 2006).7.1  Experiment SetupExperiment Data: We use a set of reviews andtheir authors/reviewers from Amazon.com as ourexperiment data.
We select the authors who haveposted more than 30 reviews in the book category.After cleaning, we have 831 authors, 731 authorsfor training and 100 authors for testing.
The num-bers of reviews in the training and test author setare 59256 and 14308, respectively.
We use theStanford parser (Klein and Manning, 2003) to gen-erate the grammar structure of review sentencesfor extracting syntactic d-features.
Note that theauthors here are in fact userids.
However, sincethey are randomly selected from a large number ofuserids, the probability that two sampled useridsbelong to the same person is very small.
Thus, itshould be safe to assume that each userid here rep-resents a unique author.Training data: We randomly choose 1 (one) re-view for each author as the query and all of his/herother reviews as q-positive reviews.
The q-negative reviews consist of reviews randomly se-lected from the other 730 authors, two reviews perauthor.
We also tried to use more queries fromeach author, but they make little difference.Test data: The test authors are all unseen, i.e.,their reviews have not been used in training.
Weprepare the test case for each author as follows.We first divide the reviews of each author intotwo equal subsets.
The purpose is to simulate thesituation where there are two userids idia and idibfrom the same author ai.
Our objective is that giv-en one userid idia and its query set, we want to findthe other userid idib from the same author.For the review subset of idia (or idib), we ran-domly select 9 reviews as the query set and anoth-er 10 reviews as the sample set for the userid.
Thetwo sets are disjoint.
We don?t use more queries orsample reviews from each author since in the re-view domain most authors do not have many re-views (Jindal and Liu, 2008).
In the experiments,we will vary the number of test userids, the num-ber of queries, and the number of samples.
We usethe following format to describe each test data:T<n>_Q<n>S<n>, where T denotes the total num-ber of test userids, Q the query set and S the sam-ple set, and <n> a number.
For example,T50_Q9S10 stands for a test data with 50 userids,and for each userid, 9 reviews are selected as que-ries and 10 reviews are selected as samples.
* rep-1130resents a wildcard whose value we can vary.Note that we use this ?artificial?
data rather thanmanually labeled data for our experiments becauseit is very hard to reliably label any gold-standarddata manually in this case.
The problem is similarto labeling fake reviews.
In the fake review detec-tion research, researchers have manually label fakereviews and reviewers (Yoo and Gretzel 2009;Lim et al 2010; Li et al 2011; Wang et al 2011).However, based on the actual fake reviews writtenusing Amazon Mechanical Turk, Ott et al(2011)have showed that the accuracy of human labelingof fake reviews is very poor.
We also believe thatour test data is realistic for evaluation as we canimage that the two sets of reviews are from twoaccounts (userids) of the same author (reviewer).Two types of experiments: For each author withtwo userids, we conduct two types of tests.?
Type I: Identify two userids belong to the sameauthor.
The experiment runs iteratively to testevery userid.
In each iteration, we plant oneuserid of an author in the test set and use theother userid of the same author as the queryuserid.
That is, in the ith run, the test data con-sist of the following two components:1.
Query userid idia and its query set Qia2.
Test userids {id1a, ?, id(i-1)a, idib, ?, idma}and their corresponding sample review sets{S1a, ?, S(i-1)a, Sib, ?, Sma}.Note that the query userid idia and the testuserid idib are from the same author.
Our objec-tive is to use Qia to find idib through Sib.Evaluation measure: We use precision, recall,and F1 score to evaluate Type I experiments aswe want to identify all matching pairs.
The er-rors are ?no pair?
and ?wrong pair?
found.?
Type II: Type II experiments test the caseswhen no pair exists.
That is, we do not plantany matching userid for the query userid.
Then,the algorithm should not find anything.
For theith run, the test data has these components:1.
Query userid idia and its query set Qia2.Test userids {id1a, ?, id(i-1)a, id(i+1)a, ?, idma}and their sample review sets {S1a, ?, S(i-1)a,S(i+1)a, ?, Sma}.
idib is not planted.Evaluation measure: Here we cannot use pre-cision and recall because we are not trying tofind any pairs.
We thus use accuracy as ourmeasure.
For each idi, if no pair is found, it iscorrect.
If a pair is found, it is wrong.Baseline methods:  As mentioned eariler, thereare  only two works that tried to identify multi-idusers.
The first is that in (Chen et al 2004).However, as we discussed in related work, theirapproach is not applicable to reviews.
The other isthat in (Novak et al 2004), which used clusteringbut assumed that the number of actual authors (orclusters) is known.
This is unrealistic in practice.Thus we designed three new baselines:TSL: This baseline is based on the traditional su-pervised learning (TSL).
We use it to evaluatehow the traditional approach performs in theoriginal feature space.
In this case, each docu-ment in TSL has to be represented as a vector ofd-features or traditional n-gram features.
Foreach test userid id, we build a SVM classifierbased on the one vs. all strategy.
That is, fortraining we use id?s queries in T*_Q*S10 as thepositive documents, and all queries of the othertest userids (e.g., 99 userids if the test data has100 userids) as the negative documents.
Notethat TSL cannot use the 731 userids for trainingas in LSS because they do not appear in the testdata.
In testing, userid id?s sample (non-query)documents in T*_Q*S10 are used as positivedocuments, and the sample documents of all oth-er test userids are used as negative documents.SimUG: It uses the word unigrams to compare thecosine similarity of queries and samples.
Cosinesimilarity with unigrams is the most widely useddocument similarity measure.SimAD: It uses all d-features to compare the cosinesimilarity of queries and samples.For both SimUG and SimAD, their cosine simi-larity values are used in place of SVM scores ofLSS or TSL.
We then apply the same 4 strategiesto decide the final author attribution except votingas cosine similarity cannot classify.7.2  Results and analysis1) Effects of positive/total ratio in training set:Since our data is highly skewed and too many neg-ative cases may not be good for classification, wethus performed this experiment to find a good ratio.Table 5 shows the results for Type I experiments.From Table 5, we can see that the results are high-ly accurate.
Even for 100 userids, our method cancorrectly identify 85% cases.
Here we use the datasets T*_Q9S10 and the decision method isScoreSqSum, which produces the best result.
The1131results for Type II experiments (Table 6) are alsoaccurate.
In most cases, the values of accuracy arehigher than 90%.
For all our experiments below,we use the model/classifier trained with 0.4 ratio.Table 5.
Positive(p)/total(t) ratio in training (Type I)F1p/t 10 30 50 80 1000.3 100.00 84.62 86.36 88.89 83.720.4 100.00 91.91 90.11 88.89 85.710.5 100.00 90.91 91.30 88.89 87.010.6 94.74 82.35 87.64 85.71 86.360.7 94.74 84.62 86.36 86.53 87.64Table 6.
Positive(p)/total(t) ratio in training (Type II)Accuracyp/t 10 30 50 80 1000.3 90.00 90.00 92.00 97.50 94.000.4 90.00 90.00 94.00 98.75 95.000.5 80.00 86.67 94.00 97.75 95.000.6 80.00 86.67 90.00 93.75 92.000.7 80.00 86.67 90.00 95.00 92.00(2) Effects of different decision methods: Weshow the results of the four proposed decisionmethods: Voting, ScoreSum, ScoreSqSum, andScoreMax, using our basic data of T*_Q9S10 withvaried number of test userids.
Figure 5(a) showsthat ScoreSqSum is the best for Type I experi-ments.
Figure 5(b) shows ScoreMax is the best forType II, but ScoreSqSum also does very well.
Be-low, ScoreSqSum is used as our default methodbecause Type I is more important than Type II.
(a)  Type I                   (b) Type IIFigure 5: Effect of different decision methods(3) Effects of number of queries per userid:Figure 6 shows the results of different numbers ofqueries.
We see that more queries give better re-sults, which is easy to understand because morequeries give more information.
We use 9 queriesper userid in all other experiments.
(a)  Type I                   (b) Type IIFigure 6: Effect of different numbers of queries(4) Effects of number of samples per userid: Wetried 2, 4, 6, 8, 10 samples per userid.
Althoughthere are some fluctuations for Type II (Fig.7(b)),we can see an upward trend for Type I in Fig.
7(a).This indicates that more sample documents givebetter results in general.
The main reason again isthat more samples from a userid give more identi-fying information about the userid.
We use 10 testdocuments (samples) per userid in all experiments.
(a)  Type I                   (b) Type IIFigure 7: Effect of different number of samples(5) Impact of individual s-feature sets: Here weshow the effectiveness of individual s-feature sets.From Table 7, we see that Sim7Retrieval s-features are extremely important for Type I test.Removing Sim7Retrieval causes about 10% to20% F1 score drop on different datasets.
SimCT-fidf s-features are also useful.
The impacts of others-features are small.
The same applies to Type IItest (Table 8).
On average, using all features is thebest.
Hence we use all features in all other experi-ments above.Table 7.
Using different s-features (Type I)T*_Q9S10 F110F130F150F180F1100All features 100.00 90.91 90.11 88.89 85.71No Sim4Len 100.00 88.89 86.36 87.32 85.06No SimCRichness 100.00 88.89 91.30 88.89 85.71No SimCTfidf 100.00 80.00 86.36 86.53 83.72No Sim7Retrieval 82.35 72.34 75.80 78.79 77.30No Sim3Sent 94.74 84.62 86.36 88.11 87.64Table 8.
Using different s-features (Type II)T*_Q9S10 Acc.10Acc30Acc.50Acc80Acc.100All features 90.00 90.00 94.00 98.75 99.00No Sim4Len 90.00 93.33 96.00 96.25 96.00No SimCRichness 90.00 90.00 94.00 96.25 96.00No SimCTfidf 90.00 86.67 94.00 93.75 97.00No Sim7Retrieval 80.00 90.00 94.00 94.00 96.00No Sim3Sent 90.00 93.33 92.00 98.75 93.00(6) Comparing with the three baselines: Similarto our method, the training data for TSL is highlyskewed as it uses a one-vs.-all strategy.
Hence wealso investigate the effect of p/t ratio in training forTSL.
Results show that 0.4 ratio is the best setting.1132Thus this setting is adopted for TSL in the follow-ing experiments.
Note that we cannot conduct p/tratio experiments for SimAD and SimUG as theyare unsupervised methods.
We use ScoreMax forTSL, ScoreSqSum for SimUG and SimAD, re-spectively, since they perform the best for theircorresponding approaches.
Tables 9 and 10 showthe results of our LSS method and the baselinemethods for Type I and II tests respectively.
ForTSL, we use all d-features.
Unigram features gaveTSL much worse results and are thus not includedhere.Table 9: Comparison with baselines (Type I)10 30 50 80 100LSS Pre 100.00 100.00 100.00 100.00 98.68Rec 100.00 83.33 82.00 80.00 75.76F1 100.00 90.91 90.11 88.89 85.71TSL Pre 50.00 50.00 33.33 0.00 0.00Rec 11.11 3.45 2.08 0.00 0.00F1 18.18 6.45 3.92 0.00 0.00SimUG Pre 100.00 100.00 100.00 100.00 100.00Rec 70.00 46.67 48.00 48.75 43.00F1 82.35 63.64 64.86 65.55 60.14SimAD Pre 100.00 75.00 100.00 33.33 0.00Rec 20.00 10.35 2.00 1.28 0.00F1 33.33 18.18 3.92 2.47 0.00Table 10: Comparison with baselines (Type II)Accuracy 10 30 50 80 100LSS 90.00 90.00 94.00 98.75 95.00TSL 90.00 96.67 98.00 98.75 99.00SimUG 96.00 93.33 96.00 96.25 97.00SimAD 90.00 96.67 98.00 98.75 99.00From Tables 9 and 10, we can make the follow-ing observations.?
For Type I, F1 scores of LSS are markedly bet-ter than those of the three baselines.
The resultsof SimUG also drop more quickly than LSSwith the increased number of userids.
SimAD?sresults are extremely poor.
These show thatLSS is much more superior to the unsupervisedmethods.
TSL performed the worst, indicatingthat traditional supervised learning is inappro-priate for this task.
There are two main reasons:First, for one vs. all learning, the negative train-ing data actually contain positive documentswhich are written by the same author using an-other userid as the positive data, which confus-es the classifier.
Second, TSL is unable to buildan accurate classifier using the small number ofqueries (which are training data).
In contrast,our LSS method can exploit a large number ofother authors who do not have to appear in test-ing and thus achieves the huge improvements.?
For Type II, LSS also performs very well.
Thebaselines perform well too and even better,which is not surprising because they have diffi-culty in finding matching pairs for Type I.Since Type II datasets have no author with mul-tiple userids, naturally the baselines will dowell for Type II.
But that is useless becausewhen there are authors with multiple usersids(Type I), they are unable to find them well.In summary, we can conclude that for Type I tests(there are authors with multiple userids), LSS isdramatically better than all baseline methods.
ForType II tests (there is no author with multipleuserids), it also performs very well.8 ConclusionThis paper proposed a novel method to identifyuserids that may be from the same author.
Thecore of the method is a supervised learning methodwhich learns in a similarity space rather than thedocument space.
This learning method is able tobetter determine whether a document may be writ-ten by a known author, although no documentfrom the author has been used in training (as longas we have some documents from the author toserve as queries).
To the best of our knowledge,there is no existing method based on linguisticanalysis for solving the problem.
Our experimentalresults based on a large number of reviewers andtheir reviews show that the proposed algorithm ishighly accurate.
It outperforms three baselinesmarkedly.AcknowledgementsWe are grateful to the anonymous reviewers fortheir thoughtful comments.
Tieyun Qian was sup-ported in part by the NSFC Projects (61272275,61272110, 61202036), and the 111 Project(B07037).
Bing Liu was supported in part by agrant from National Science Foundation (NSF)under no.
IIS-1111092.ReferencesShlomo Argamon and Shlomo Levitan.
2004.Measuring the usefulness of function words forauthorship attribution.
Literary and LinguisticComputing 1-3.1133Shlomo Argamon, Casey Whitelaw, Paul Chase,Sobhan Raj Hota, Navendu Garg, and ShlomoLevitan.
2007.
Stylistic text classification usingfunctional lexical features: Research articles.
J.Am.
Soc.
Inf.
Sci.
Technol.
58:802-822.John F. Burrows.
1992.
Not unless you ask nicely:The interpretative nexus between analysis andinformation.
Literary and Linguistic Computing7:91-109.Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, YalouHuang, and Hsiao-Wuen Hon.
2006.
Adaptingranking svm to document retrieval.
Proc.
ofSIGIR, Pages 186-193.Hung-Ching Chen, Mark K. Goldberg, MalikMagdon-Ismail.
2004.
Identifying multi-ID usersin open forums.
Intelligence and SecurityInformatics, Pages 176-186.Joachim Diederich, J?rg Kindermann, EddaLeopold, and Gerhard Paass, 2000.
Authorshipattribution with support vector machines.Applied Intelligence 19:109-123.Hugo Jair Escalante, Thamar Solorio, and ManuelMontes-y-G?mez.
2011.
Local histograms ofcharacter n-grams for authorship attribution.Proc.
of ACL-HLT, Volume I: 288-298.Song Feng, Longfei Xing, Anupam Gogar, andYejin Choi.
2012.
Distributional Footprints ofDeceptive Product Reviews.
Proc.
of ICWSM.Michael Gamon.
2004.
Linguistic correlates ofstyle: authorship classification with deeplinguistic analysis features.
Proc.
of Coling.Neil Graham, Graeme Hirst, and Bhaskara Marthi.2005.
Segmenting documents by stylisticcharacter.
Natural Language Engineering,11:397-415.Jack Grieve.
2007.
Quantitative authorshipattribution: An evaluation of techniques.
Literaryand Linguistic Computing 22:251-270.Hans van Halteren, Fiona Tweedie, and HaraldBaayen.
1996.
Outside the cave of shadows:using syntactic annotation to enhance authorshipattribution.
Literary and Linguistic Computing11:121-132.Steffen Hedegaard and Jakob Grue Simonsen.2011.
Lost in translation: authorship attributionusing frame semantics.
Proc.
of ACL-HLT, shortpapers - Volume 2, 65-70.Graeme Hirst and Ol?ga Feiguina.
2007.
Bigramsof syntactic labels for authorship discriminationof short texts.
Literary and Linguistic Computing22:405-417.David I. Holmes and R. S. Forsyth.
1995.
TheFederalist Revisited: New Directions inAuthorship Attribution, Literary and LinguisticComputing, 10(2): 111-127.David L. Hoover.
2001.
Statistical stylistics andauthorship attribution: an empiricalinvestigation.
Literary and Linguistic Computing16:421-424.Nitin Jindal and Bing Liu.
2008.
Opinion Spamand Analysis.
Proc.
of WSDM, California, USA.Thorsten Joachims.
2006.
Training linear svms inlinear time.
Proc.
of KDD.Sangkyum Kim, Hyungsul Kim, Tim Weninger,Jiawei Han, and Hyun Duk Kim.
2011.Authorship classification: a discriminativesyntactic tree mining approach.
Proc.
of SIGIR,Pages 455-464.Dan Klein, and Christopher D. Manning.
2003.Accurate unlexicalized parsing.
Proc.
of ACL,423-430.Moshe Koppel and Jonathan Schler.
2004.Authorship verification as a one-classclassification problem.
Proc.
of ICML.Moshe Koppel, Jonathan Schler, ShlomoArgamon.
2011.
Authorship attribution in thewild.
Lang Resources & Evaluation, 45:83-94Fangtao Li, Minlie Huang, Yi Yang and XiaoyanZhu.
2011.
Learning to identify review Spam.Proc.
of IJCAI.Hang Li.
2011.
Learning to Rank for InformationRetrieval and Natural Language Processing.Morgan & Claypool publishers.Jiexun Li, Rong Zheng, and Hsinchun Chen.
2006.From fingerprint to writeprint.
Communicationsof the ACM, 49:76-82.Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, BingLiu, Hady W. Lauw.
2010.
Detecting productreview spammers using rating behaviors.
Proc.of CIKM, 2010.Bing Liu.
2012.
Sentiment Analysis and OpinionMining, Morgan & Claypool publishers.1134Tieyan Liu.
2011.
Learning to Rank for Infor-mation Retrieval.
Springer.Kim Luyckx, Walter Daelemans.
2008.
AuthorshipAttribution and Verification with Many Authorsand Limited Data.
Proc.
of Coling, pages 513-520.David Madigan, Alexander Genkin, David D.Lewis, Shlomo Argamon, Dmitriy Fradkin, andLi Ye.
2005.
Author Identification on the LargeScale.
Proc.
of CSNA.Donald Metzler, Yaniv Bernstein, W. Bruce Croft,Alistair Moffat, and Justin Zobel.
2005.Similarity measures for tracking informationflow.
Proc.
of CIKM.
Pages 517-524.Frederick Mosteller, David Lee Wallace.
1964.Inference and disputed authorship: TheFederalist.
Addison-Wesley.Arjun Mukherjee, Bing Liu, and Natalie Glance.2012.
Spotting Fake Reviewer Groups in Con-sumer Reviews.
Proc.
of WWW, Pages 191-200.Arvind Narayanan, Hristo Paskov, Neil ZhenqiangGong, et al2012.
On the feasibility of internet-scale author identification.
Proceedings of the2012 IEEE Symposium on Security and Privacy.Pages 300-314Jasmine Novak, Prabhakar Raghavan, AndrewTomkins.
2004.
Anti-aliasing on the web.
Proc.of WWW, Pages 30-39Myle Ott, Yejin Choi, Claire Cardie, Jeffrey T.Hancock.
2011.
Finding Deceptive OpinionSpam by Any Stretch of the Imagination.
Proc.of ACL.Myle Ott, Claire Cardie, Jeffrey T. Hancock.
2012.Estimating the prevalence of deception in onlinereview communities.
Proc.
of WWW.Fuchun Peng, Dale Schuurmans, Shaojun Wang,and Vlado Keselj.
2003.
Language independentauthorship attribution using character levellanguage models.
Proc.
of EACL, Pages 267-274.Conrad Sanderson and Simon Guenter.
2006.Short text authorship attribution via sequencekernels, markov chains and author unmasking:an investigation.
Proc.
of EMNLP, Pages 482-491.Yanir Seroussi, Fabian Bohnert, Ingrid Zukerman.2012.
Authorship Attribution with Author-aware Topic Models.
Proc.
of ACL, 2:264-269.Thamar Solorio, Sangita Pillay, Sindhu Raghavan,Manuel Montes y G?omez.
2011.
ModalitySpecific Meta Features for AuthorshipAttribution in Web Forum Posts.
Proc.
ofIJCNLP, Pages 156-164.Efstathios Stamatatos.
2009.
A Survey of ModernAuthorship Attribution Methods.
Journal of theAmerican Society for Information Science andTechnology, 60(3):538-556, Wiley.Efstathios Stamatatos, George Kokkinakis, andNikos Fakotakis.
2000.
Automatic textcategorization in terms of genre and author.Comput.
Linguist.
26:471-495.?zlem Uzuner and Boris Katz.
2005.
Acomparative study of language models for bookand author recognition.
Proc.
of IJCNLP, Pages969-980.Vladimir N. Vapnik.
1998.
Statistical LearningTheory.
Wiley-Interscience, NY.O.
de Vel, A. Anderson, M. Corney and G.Mohay.
2001.
Mining Email Content for AuthorIdentification Forensics.
Sigmod Record, 30:55-64.Kyung-Hyan Yoo and Ulrike Gretzel.
2009.Comparison of Deceptive and Truthful TravelReviews.
Information and CommunicationTechnologies in Tourism, Pages 37-47.Georgy Udnv Yule.
1944.
The statistical study ofliterary vocabulary.
Cambridge UniversityPress.Guan Wang, Sihong Xie, Bing Liu, Philip S. Yu.2011.
Review Graph based Online StoreReview Spammer Detection.
Proc.
of ICDM.Ying Zhao and Justin Zobel.
2005.
Effective andscalable authorship attribution using functionwords.
Proceeding of Information RetrivalTechnology, Pages 174-189.Rong Zheng, Jiexun Li, Hsinchun Chen, and ZanHuang.
2006.
A framework for authorship iden-tification of online messages: Writing style fea-tures and classification techniques.
Journal ofthe American Society of Information Scienceand Technology 57:378-393.1135
