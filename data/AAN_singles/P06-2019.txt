Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 144?151,Sydney, July 2006. c?2006 Association for Computational LinguisticsConstraint-based Sentence CompressionAn Integer Programming ApproachJames Clarke and Mirella LapataSchool of Informatics, University of Edinburgh2 Bucclecuch Place, Edinburgh EH8 9LW, UKjclarke@ed.ac.uk, mlap@inf.ed.ac.ukAbstractThe ability to compress sentences whilepreserving their grammaticality and mostof their meaning has recently receivedmuch attention.
Our work views sentencecompression as an optimisation problem.We develop an integer programming for-mulation and infer globally optimal com-pressions in the face of linguistically moti-vated constraints.
We show that such a for-mulation allows for relatively simple andknowledge-lean compression models thatdo not require parallel corpora or large-scale resources.
The proposed approachyields results comparable and in somecases superior to state-of-the-art.1 IntroductionA mechanism for automatically compressing sen-tences while preserving their grammaticality andmost important information would greatly bene-fit a wide range of applications.
Examples includetext summarisation (Jing 2000), subtitle genera-tion from spoken transcripts (Vandeghinste andPan 2004) and information retrieval (Olivers andDolan 1999).
Sentence compression is a complexparaphrasing task with information loss involv-ing substitution, deletion, insertion, and reorderingoperations.
Recent years have witnessed increasedinterest on a simpler instantiation of the compres-sion problem, namely word deletion (Knight andMarcu 2002; Riezler et al 2003; Turner and Char-niak 2005).
More formally, given an input sen-tence of words W = w1,w2, .
.
.
,wn, a compressionis formed by removing any subset of these words.Sentence compression has received both gener-ative and discriminative formulations in the liter-ature.
Generative approaches (Knight and Marcu2002; Turner and Charniak 2005) are instantia-tions of the noisy-channel model: given a long sen-tence l, the aim is to find the corresponding shortsentence s which maximises the conditional prob-ability P(s|l).
In a discriminative setting (Knightand Marcu 2002; Riezler et al 2003; McDonald2006), sentences are represented by a rich fea-ture space (typically induced from parse trees) andthe goal is to learn rewrite rules indicating whichwords should be deleted in a given context.
Bothmodelling paradigms assume access to a trainingcorpus consisting of original sentences and theircompressions.Unsupervised approaches to the compressionproblem are few and far between (see Hori and Fu-rui 2004 and Turner and Charniak 2005 for excep-tions).
This is surprising considering that parallelcorpora of original-compressed sentences are notnaturally available in the way multilingual corporaare.
The scarcity of such data is demonstrated bythe fact that most work to date has focused on asingle parallel corpus, namely the Ziff-Davis cor-pus (Knight and Marcu 2002).
And some effortinto developing appropriate training data would benecessary when porting existing algorithms to newlanguages or domains.In this paper we present an unsupervised modelof sentence compression that does not rely on aparallel corpus ?
all that is required is a corpusof uncompressed sentences and a parser.
Given along sentence, our task is to form a compressionby preserving the words that maximise a scoringfunction.
In our case, the scoring function is ann-gram language model, ?with a few strings at-tached?.
While straightforward to estimate, a lan-guage model is a fairly primitive scoring function:it has no notion of the overall sentence structure,grammaticality or underlying meaning.
We thuscouple our language model with a small numberof structural and semantic constraints capturingglobal properties of the compression process.We encode the language model and linguisticconstraints as linear inequalities and use IntegerProgramming (IP) to infer compressions that areconsistent with both.
The IP formulation allows usto capture global sentence properties and can beeasily manipulated to provide compressions tai-lored for specific applications.
For example, we144could prevent overly long or overly short compres-sions or generally avoid compressions that lacka main verb or consist of repetitions of the sameword.In the following section we provide an overviewof previous approaches to sentence compression.In Section 3 we motivate the treatment of sentencecompression as an optimisation problem and for-mulate our language model and constraints in theIP framework.
Section 4 discusses our experimen-tal set-up and Section 5 presents our results.
Dis-cussion of future work concludes the paper.2 Previous WorkJing (2000) was perhaps the first to tackle the sen-tence compression problem.
Her approach usesmultiple knowledge sources to determine whichphrases in a sentence to remove.
Central to hersystem is a grammar checking module that spec-ifies which sentential constituents are grammati-cally obligatory and should therefore be presentin the compression.
This is achieved using sim-ple rules and a large-scale lexicon.
Other knowl-edge sources include WordNet and corpus evi-dence gathered from a parallel corpus of original-compressed sentence pairs.
A phrase is removedonly if it is not grammatically obligatory, not thefocus of the local context and has a reasonabledeletion probability (estimated from the parallelcorpus).In contrast to Jing (2000), the bulk of the re-search on sentence compression relies exclusivelyon corpus data for modelling the compressionprocess without recourse to extensive knowledgesources (e.g., WordNet).
Approaches based on thenoisy-channel model (Knight and Marcu 2002;Turner and Charniak 2005) consist of a sourcemodel P(s) (whose role is to guarantee that thegenerated compression is grammatical), a chan-nel model P(l|s) (capturing the probability thatthe long sentence l is an expansion of the com-pressed sentence s), and a decoder (which searchesfor the compression s that maximises P(s)P(l|s)).The channel model is typically estimated usinga parallel corpus, although Turner and Charniak(2005) also present semi-supervised and unsu-pervised variants of the channel model that esti-mate P(l|s) without parallel data.Discriminative formulations of the compres-sion task include decision-tree learning (Knightand Marcu 2002), maximum entropy (Riezleret al 2003), support vector machines (Nguyenet al 2004), and large-margin learning (McDonald2006).
We describe here the decision-tree modelin more detail since we will use it as a basis forcomparison when evaluating our own models (seeSection 4).
According to this model, compressionis performed through a tree rewriting process in-spired by the shift-reduce parsing paradigm.
A se-quence of shift-reduce-drop actions are performedon a long parse tree, l, to create a smaller tree, s.The compression process begins with an inputlist generated from the leaves of the original sen-tence?s parse tree and an empty stack.
?Shift?
oper-ations move leaves from the input list to the stackwhile ?drop?
operations delete from the input list.Reduce operations are used to build trees from theleaves on the stack.
A decision-tree is trained on aset of automatically generated learning cases froma parallel corpus.
Each learning case has a targetaction associated with it and is decomposed into aset of indicative features.
The decision-tree learnswhich action to perform given this set of features.The final model is applied in a deterministic fash-ion in which the features for the current state areextracted and the decision-tree is queried.
This isrepeated until the input list is empty and the finalcompression is recovered by traversing the leavesof resulting tree on the stack.While most compression models operate overconstituents, Hori and Furui (2004) propose amodel which generates compressions throughword deletion.
The model does not utilise paralleldata or syntactic information in any form.
Given aprespecified compression rate, it searches for thecompression with the highest score according to afunction measuring the importance of each wordand the linguistic likelihood of the resulting com-pressions (language model probability).
The scoreis maximised through a dynamic programming al-gorithm.Although sentence compression has not beenexplicitly formulated as an optimisation problem,previous approaches have treated it in these terms.The decoding process in the noisy-channel modelsearches for the best compression given the sourceand channel models.
However, the compressionfound is usually sub-optimal as heuristics are usedto reduce the search space or is only locally op-timal due to the search method employed.
Thedecoding process used in Turner and Charniak?s(2005) model first searches for the best combina-tion of rules to apply.
As they traverse their listof compression rules they remove sentences out-side the 100 best compressions (according to theirchannel model).
This list is eventually truncatedto 25 compressions.In other models (Hori and Furui 2004; McDon-ald 2006) the compression score is maximised145using dynamic programming.
The latter guaran-tees we will find the global optimum provided theprinciple of optimality holds.
This principle statesthat given the current state, the optimal decisionfor each of the remaining stages does not dependon previously reached stages or previously madedecisions (Winston and Venkataramanan 2003).However, we know this to be false in the case ofsentence compression.
For example, if we haveincluded modifiers to the left of a head noun inthe compression then it makes sense that we mustinclude the head also.
With a dynamic program-ming approach we cannot easily guarantee suchconstraints hold.3 Problem FormulationOur work models sentence compression explicitlyas an optimisation problem.
There are 2n possiblecompressions for each sentence and while manyof these will be unreasonable (Knight and Marcu2002), it is unlikely that only one compressionwill be satisfactory.
Ideally, we require a func-tion that captures the operations (or rules) that canbe performed on a sentence to create a compres-sion while at the same time factoring how desir-able each operation makes the resulting compres-sion.
We can then perform a search over all possi-ble compressions and select the best one, as deter-mined by how desirable it is.Our formulation consists of two basic compo-nents: a language model (scoring function) and asmall number of constraints ensuring that the re-sulting compressions are structurally and semanti-cally valid.
Our task is to find a globally optimalcompression in the presence of these constraints.We solve this inference problem using Integer Pro-gramming without resorting to heuristics or ap-proximations during the decoding process.
Integerprogramming has been recently applied to severalclassification tasks, including relation extraction(Roth and Yih 2004), semantic role labelling (Pun-yakanok et al 2004), and the generation of routedirections (Marciniak and Strube 2005).Before describing our model in detail, we in-troduce some of the concepts and terms used inLinear Programming and Integer Programming(see Winston and Venkataramanan 2003 for an in-troduction).
Linear Programming (LP) is a toolfor solving optimisation problems in which theaim is to maximise (or minimise) a given functionwith respect to a set of constraints.
The functionto be maximised (or minimised) is referred to asthe objective function.
Both the objective functionand constraints must be linear.
A number of deci-sion variables are under our control which exertinfluence on the objective function.
Specifically,they have to be optimised in order to maximise(or minimise) the objective function.
Finally, a setof constraints restrict the values that the decisionvariables can take.
Integer Programming is an ex-tension of linear programming where all decisionvariables must take integer values.3.1 Language ModelAssume we have a sentence W = w1,w2, .
.
.
,wnfor which we wish to generate a compression.We introduce a decision variable for each wordin the original sentence and constrain it to be bi-nary; a value of 0 represents a word being dropped,whereas a value of 1 includes the word in the com-pression.
Let:yi ={ 1 if wi is in the compression0 otherwise ?i?
[1 .
.
.n]If we were using a unigram language model,our objective function would maximise the overallsum of the decision variables (i.e., words) multi-plied by their unigram probabilities (all probabili-ties throughout this paper are log-transformed):maxz = n?i=1yi ?P(wi)Thus if a word is selected, its corresponding yi isgiven a value of 1, and its probability P(wi) ac-cording to the language model will be counted inour total score, z.A unigram language model will probably gener-ate many ungrammatical compressions.
We there-fore use a more context-aware model in our objec-tive function, namely a trigram model.
Formulat-ing a trigram model in terms of an integer programbecomes a more involved task since we now mustmake decisions based on word sequences ratherthan isolated words.
We first create some extra de-cision variables:pi ={1 if wi starts the compression0 otherwise ?i ?
[1 .
.
.n]qi j =??
?1 if sequence wi,w j endsthe compression ?i ?
[1 .
.
.n?1]0 otherwise ?
j ?
[i+1 .
.
.n]xi jk =??
?1 if sequence wi,w j,wk ?i ?
[1 .
.
.n?2]is in the compression ?
j ?
[i+1 .
.
.n?1]0 otherwise ?k ?
[ j +1 .
.
.n]Our objective function is given in Equation (1).This is the sum of all possible trigrams that canoccur in all compressions of the original sentencewhere w0 represents the ?start?
token and wi is theith word in sentence W .
Equation (2) constrains146the decision variables to be binary.maxz = n?i=1pi ?P(wi|start)+n?2?i=1n?1?j=i+1n?k= j+1xi jk ?P(wk|wi,w j)+n?1?i=0n?j=i+1qi j ?P(end|wi,w j) (1)subject to:yi, pi,qi j,xi jk = 0 or 1 (2)The objective function in (1) allows any combi-nation of trigrams to be selected.
This means thatinvalid trigram sequences (e.g., two or more tri-grams containing the symbol ?end?)
could appearin the output compression.
We avoid this situationby introducing sequential constraints (on the de-cision variables yi,xi jk, pi, and qi j) that restrict theset of allowable trigram combinations.Constraint 1 Exactly one word can begin asentence.n?i=1pi = 1 (3)Constraint 2 If a word is included in the sen-tence it must either start the sentence or be pre-ceded by two other words or one other word andthe ?start?
token w0.yk ?
pk ?k?2?i=0k?1?j=1xi jk = 0 (4)?k : k ?
[1 .
.
.n]Constraint 3 If a word is included in the sen-tence it must either be preceded by one word andfollowed by another or it must be preceded by oneword and end the sentence.y j ?j?1?i=0n?k= j+1xi jk ?j?1?i=0qi j = 0 (5)?
j : j ?
[1 .
.
.n]Constraint 4 If a word is in the sentence itmust be followed by two words or followed by oneword and then the end of the sentence or it must bepreceded by one word and end the sentence.yi ?n?1?j=i+1n?k= j+1xi jk ?n?j=i+1qi j ?i?1?h=0qhi = 0 (6)?i : i ?
[1 .
.
.n]Constraint 5 Exactly one word pair can endthe sentence.n?1?i=0n?j=i+1qi j = 1 (7)Example compressions using the trigram modeljust described are given in Table 1.
The model inO: He became a power player in Greek Politics in1974, when he founded the socialist Pasok Party.LM: He became a player in the Pasok.Mod: He became a player in the Pasok Party.Sen: He became a player in politics.Sig: He became a player in politics when he foundedthe Pasok Party.O: Finally, AppleShare Printer Server, formerly aseparate package, is now bundled with Apple-Share File Server.LM: Finally, AppleShare, a separate, AppleShare.Mod: Finally, AppleShare Server, is bundled.Sen: Finally, AppleShare Server, is bundled withServer.Sig: AppleShare Printer Server package is now bun-dled with AppleShare File Server.Table 1: Compression examples (O: original sen-tence, LM: compression with the trigram model,Mod: compression with LM and modifier con-straints, Sen: compression with LM, Mod andsentential constraints, Sig: compression with LM,Mod, Sen, and significance score)its current state does a reasonable job of modellinglocal word dependencies, but is unable to capturesyntactic dependencies that could potentially al-low more meaningful compressions.
For example,it does not know that Pasok Party is the objectof founded or that Appleshare modifies PrinterServer.3.2 Linguistic ConstraintsIn this section we propose a set of global con-straints that extend the basic language model pre-sented in Equations (1)?(7).
Our aim is to bringsome syntactic knowledge into the compressionmodel and to preserve the meaning of the originalsentence as much as possible.
Our constraints arelinguistically and semantically motivated in a sim-ilar fashion to the grammar checking componentof Jing (2000).
Importantly, we do not require anyadditional knowledge sources (such as a lexicon)beyond the parse and grammatical relations of theoriginal sentence.
This is provided in our experi-ments by the Robust Accurate Statistical Parsing(RASP) toolkit (Briscoe and Carroll 2002).
How-ever, there is nothing inherent in our formulationthat restricts us to RASP; any other parser withsimilar output could serve our purposes.Modifier Constraints Modifier constraintsensure that relationships between head words andtheir modifiers remain grammatical in the com-pression:yi ?
y j ?
0 (8)?i, j : w j ?
wi?s ncmodsyi ?
y j ?
0 (9)?i, j : w j ?
wi?s detmods147Equation (8) guarantees that if we include a non-clausal modifier (ncmod) in the compression thenthe head of the modifier must also be included; thisis repeated for determiners (detmod) in (9).We also want to ensure that the meaning of theoriginal sentence is preserved in the compression,particularly in the face of negation.
Equation (10)implements this by forcing not in the compressionwhen the head is included.
A similar constraintis added for possessive modifiers (e.g., his, our),as shown in Equation (11).
Genitives (e.g., John?sgift) are treated separately, mainly because theyare encoded as different relations in the parser (seeEquation (12)).yi ?
y j = 0 (10)?i, j : w j ?
wi?s ncmods?w j = notyi ?
y j = 0 (11)?i, j : w j ?
wi?s possessive detmodsyi ?
y j = 0 (12)?i, j : wi ?
possessive ncmods?w j = possessiveCompression examples with the addition of themodifier constraints are shown in Table 1.
Al-though the compressions are grammatical (see theinclusion of Party due to the modifier Pasok andServer due to AppleShare), they are not entirelymeaning preserving.Sentential Constraints We also define a fewintuitive constraints that take the overall sentencestructure into account.
The first constraint (Equa-tion (13)) ensures that if a verb is present in thecompression then so are its arguments, and if anyof the arguments are included in the compressionthen the verb must also be included.
We thus forcethe program to make the same decision on theverb, its subject, and object.yi ?
y j = 0 (13)?i, j : w j ?
subject/object of verb wiOur second constraint forces the compression tocontain at least one verb provided the original sen-tence contains one as well:?i?verbsyi ?
1 (14)Other sentential constraints include Equa-tions (15) and (16) which apply to prepositionalphrases, wh-phrases and complements.
These con-straints force the introducing term (i.e., the prepo-sition, complement or wh-word) to be included inthe compression if any word from within the syn-tactic constituent is also included.
The reverse isalso true, i.e., if the introducing term is included atleast one other word from the syntactic constituentshould also be included.yi ?
y j ?
0 (15)?i, j : w j ?
PP/COMP/WH-P?wi starts PP/COMP/WH-P?i?PP/COMP/WH-Pyi ?
y j ?
0 (16)?
j : w j starts PP/COMP/WH-PWe also wish to handle coordination.
If two headwords are conjoined in the original sentence, thenif they are included in the compression the coordi-nating conjunction must also be included:(1?
yi)+ y j ?
1 (17)(1?
yi)+ yk ?
1 (18)yi +(1?
y j)+(1?
yk) ?
1 (19)?i, j,k : w j ?wk conjoined by wiTable 1 illustrates the compression output whensentential constraints are added to the model.
Wesee that politics is forced into the compression dueto the presence of in; furthermore, since bundledis in the compression, its object with Server is in-cluded too.Compression-related Constraints Finally,we impose some hard constraints on the com-pression output.
First, Equation (20) disallowsanything within brackets in the original sentencefrom being included in the compression.
Thisis a somewhat superficial attempt at excludingparenthetical and potentially unimportant materialfrom the compression.
Second, Equation (21)forces personal pronouns to be included in thecompression.
The constraint is important forgenerating coherent document as opposed tosentence compressions.yi = 0 (20)?i : wi ?
bracketsyi = 1 (21)?i : wi ?
personal pronounsIt is also possible to influence the length of thecompressed sentence.
For example, Equation (22)forces the compression to contain at least b tokens.Alternatively, we could force the compression tobe exactly b tokens (by substituting ?
with =in (22)) or to be less than b tokens (by replacing ?with ?
).1n?i=1yi ?
b (22)3.3 Significance ScoreWhile the constraint-based language model pro-duces more grammatical output than a regular lan-1Compression rate can be also limited to a range by in-cluding two inequality constraints.148guage model, the sentences are typically not greatcompressions.
The language model has no notionof which content words to include in the compres-sion and thus prefers words it has seen before.
Butwords or constituents will be of different relativeimportance in different documents or even sen-tences.Inspired by Hori and Furui (2004), we add toour objective function (see Equation (1)) a signif-icance score designed to highlight important con-tent words.
Specifically, we modify Hori and Fu-rui?s significance score to give more weight to con-tent words that appear in the deepest level of em-bedding in the syntactic tree.
The latter usuallycontains the gist of the original sentence:I(wi) =lN?
fi log FaFi (23)The significance score above is computed using alarge corpus where wi is a topic word (i.e., a nounor verb), fi and Fi are the frequency of wi in thedocument and corpus respectively, and Fa is thesum of all topic words in the corpus.
l is the num-ber of clause constituents above wi, and N is thedeepest level of embedding.
The modified objec-tive function is given below:maxz = n?i=1yi ?
I(wi)+n?i=1pi ?P(wi|start)+n?2?i=1n?1?j=i+1n?k= j+1xi jk ?P(wk|wi,w j)+n?1?i=0n?j=i+1qi j ?P(end|wi,w j) (24)A weighting factor could be also added to the ob-jective function, to counterbalance the importanceof the language model and the significance score.4 Evaluation Set-upWe evaluated the approach presented in the pre-vious sections against Knight and Marcu?s (2002)decision-tree model.
This model is a good basis forcomparison as it operates on parse trees and there-fore is aware of syntactic structure (as our modelsare) but requires a large parallel corpus for trainingwhereas our models do not; and it yields compara-ble performance to the noisy-channel model.2 Thedecision-tree model was compared against twovariants of our IP model.
Both variants employedthe constraints described in Section 3.2 but dif-fered in that one variant included the significance2Turner and Charniak (2005) argue that the noisy-channelmodel is not an appropriate compression model since it usesa source model trained on uncompressed sentences and as aresult tends to consider compressed sentences less likely thanuncompressed ones.score in its objective function (see (24)), whereasthe other one did not (see (1)).
In both cases thesequential constraints from Section 3.1 were ap-plied to ensure that the language model was well-formed.
We give details below on the corpora weused and explain how the different model parame-ters were estimated.
We also discuss how evalua-tion was carried out using human judgements.Corpora We evaluate our systems on two dif-ferent corpora.
The first is the compression corpusof Knight and Marcu (2002) derived automaticallyfrom document-abstract pairs of the Ziff-Daviscorpus.
This corpus has been used in most pre-vious compression work.
We also created a com-pression corpus from the HUB-4 1996 EnglishBroadcast News corpus (provided by the LDC).We asked annotators to produce compressions for50 broadcast news stories (1,370 sentences).3The Ziff-Davis corpus is partitioned into train-ing (1,035 sentences) and test set (32 sentences).We held out 50 sentences from the training for de-velopment purposes.
We also split the BroadcastNews corpus into a training and test set (1,237/133sentences).
Forty sentences were randomly se-lected for evaluation purposes, 20 from the testportion of the Ziff-Davis corpus and 20 from theBroadcast News corpus test set.Parameter Estimation The decision-treemodel was trained, using the same feature setas Knight and Marcu (2002) on the Ziff-Daviscorpus and used to obtain compressions for bothtest corpora.4 For our IP models, we used alanguage model trained on 25 million tokens fromthe North American News corpus using the CMU-Cambridge Language Modeling Toolkit (Clarksonand Rosenfeld 1997) with a vocabulary size of50,000 tokens and Good-Turing discounting.The significance score used in our second modelwas calculated using 25 million tokens from theBroadcast News Corpus (for the spoken data) and25 million tokens from the American News TextCorpus (for the written data).
Finally, the modelthat includes the significance score was optimisedagainst a loss function similar to McDonald(2006) to bring the language model and the scoreinto harmony.
We used Powell?s method (Presset al 1992) and 50 sentences (randomly selectedfrom the training set).3The corpus is available from http://homepages.inf.ed.ac.uk/s0460084/data/.4We found that the decision-tree was unable to producemeaningful compressions when trained on the BroadcastNews corpus (in most cases it recreated the original sen-tence).
Thus we used the decision model trained on Ziff-Davis to generate Broadcast News compressions.149We also set a minimum compression length (us-ing the constraint in Equation (22)) in both ourmodels to avoid overly short compressions.
Thelength was set at 40% of the original sentencelength or five tokens, whichever was larger.
Sen-tences under five tokens were not compressed.In our modeling framework, we generate andsolve an IP for every sentence we wish to com-press.
We employed lp solve for this purpose, anefficient Mixed Integer Programming solver.5 Sen-tences typically take less than a few seconds tocompress on a 2 GHz Pentium IV machine.Human Evaluation As mentioned earlier, theoutput of our models is evaluated on 40 exam-ples.
Although the size of our test set is compa-rable to previous studies (which are typically as-sessed on 32 sentences from the Ziff-Davis cor-pus), the sample is too small to conduct signif-icance testing.
To counteract this, human judge-ments are often collected on compression out-put; however the evaluations are limited to smallsubject pools (often four judges; Knight andMarcu 2002; Turner and Charniak 2005; McDon-ald 2006) which makes difficult to apply inferen-tial statistics on the data.
We overcome this prob-lem by conducting our evaluation using a largersample of subjects.Specifically, we elicited human judgementsfrom 56 unpaid volunteers, all self reported na-tive English speakers.
The elicitation study wasconducted over the Internet.
Participants were pre-sented with a set of instructions that explained thesentence compression task with examples.
Theywere asked to judge 160 compressions in to-tal.
These included the output of the three au-tomatic systems on the 40 test sentences pairedwith their gold standard compressions.
Partici-pants were asked to read the original sentence andthen reveal its compression by pressing a button.They were told that all compressions were gen-erated automatically.
A Latin square design en-sured that subjects did not see two different com-pressions of the same sentence.
The order of thesentences was randomised.
Participants rated eachcompression on a five point scale based on the in-formation retained and its grammaticality.
Exam-ples of our experimental items are given in Table 2.5 ResultsOur results are summarised in Table 3 which de-tails the compression rates6 and average human5The software is available from http://www.geocities.com/lpsolve/.6We follow previous work (see references) in using theterm ?compression rate?
to refer to the percentage of wordsO: Apparently Fergie very much wants to have a ca-reer in television.G: Fergie wants a career in television.D: A career in television.LM: Fergie wants to have a career.Sig: Fergie wants to have a career in television.O: The SCAMP module, designed and built byUnisys and based on an Intel process, contains theentire 48-bit A-series processor.G: The SCAMP module contains the entire 48-bit A-series processor.D: The SCAMP module designed Unisys and basedon an Intel process.LM: The SCAMP module, contains the 48-bit A-seriesprocessor.Sig: The SCAMP module, designed and built byUnisys and based on process, contains the A-series processor.Table 2: Compression examples (O: original sen-tence, G: Gold standard, D: Decision-tree, LM: IPlanguage model, Sig: IP language model with sig-nificance score)Model CompR RatingDecision-tree 56.1% 2.22?
?LangModel 49.0% 2.23?
?LangModel+Significance 73.6% 2.83?Gold Standard 62.3% 3.68?Table 3: Compression results; compression rate(CompR) and average human judgements (Rat-ing); ?
: sig.
diff.
from gold standard; ?
: sig.
diff.from LangModel+Significanceratings (Rating) for the three systems and the goldstandard.
As can be seen, the IP language model(LangModel) is most aggressive in terms of com-pression rate as it reduces the original sentenceson average by half (49%).
Recall that we enforce aminimum compression rate of 40% (see (22)).
Thefact that the resulting compressions are longer, in-dicates that our constraints instill some linguisticknowledge into the language model, thus enablingit to prefer longer sentences over extremely shortones.
The decision-tree model compresses slightlyless than our IP language model at 56.1% but stillbelow the gold standard rate.
We see a large com-pression rate increase from 49% to 73.6% whenwe introduce the significance score into the objec-tive function.
This is around 10% higher than thegold standard compression rate.We now turn to the results of our elicitationstudy.
We performed an Analysis of Variance(ANOVA) to examine the effect of different systemcompressions.
Statistical tests were carried out onthe mean of the ratings shown in Table 3.
We ob-serve a reliable effect of compression type by sub-retained in the compression.150jects (F1(3,165) = 132.74, p < 0.01) and items(F2(3,117) = 18.94, p < 0.01).
Post-hoc Tukeytests revealed that gold standard compressions areperceived as significantly better than those gener-ated by all automatic systems (?
< 0.05).
There isno significant difference between the IP languagemodel and decision-tree systems.
However, the IPmodel with the significance score delivers a sig-nificant increase in performance over the languagemodel and the decision tree (?
< 0.05).These results indicate that reasonable compres-sions can be obtained with very little supervision.Our constraint-based language model does notmake use of a parallel corpus, whereas our secondvariant uses only 50 parallel sentences for tuningthe weights of the objective function.
The modelsdescribed in this paper could be easily adapted toother domains or languages provided that syntac-tic analysis tools are to some extent available.6 Conclusions and Future WorkIn this paper we have presented a novel methodfor automatic sentence compression.
A key aspectof our approach is the use of integer program-ming for inferring globally optimal compressionsin the presence of linguistically motivated con-straints.
We have shown that such a formulationallows for a relatively simple and knowledge-leancompression model that does not require parallelcorpora or access to large-scale knowledge bases.Our results demonstrate that the IP model yieldsperformance comparable to state-of-the-art with-out any supervision.
We also observe significantperformance gains when a small amount of train-ing data is employed (50 parallel sentences).
Be-yond the systems discussed in this paper, the ap-proach holds promise for other models using de-coding algorithms for searching the space of pos-sible compressions.
The search process could beframed as an integer program in a similar fashionto our work here.We obtain our best results using a model whoseobjective function includes a significance score.The significance score relies mainly on syntacticand lexical information for determining whethera word is important or not.
An appealing futuredirection is the incorporation of discourse-basedconstraints into our models.
The latter would high-light topical words at the document-level insteadof considering each sentence in isolation.
An-other important issue concerns the portability ofthe models presented here to other languages anddomains.
We plan to apply our method to lan-guages with more flexible word order than English(e.g., German) and more challenging spoken do-mains (e.g., meeting data) where parsing technol-ogy may be less reliable.AcknowledgementsThanks to Jean Carletta, Amit Dubey, Frank Keller, SteveRenals, and Sebastian Riedel for helpful comments and sug-gestions.
Lapata acknowledges the support of EPSRC (grantGR/T04540/01).ReferencesBriscoe, E. J. and J. Carroll.
2002.
Robust accurate statisti-cal annotation of general text.
In Proceedings of the 3rdLREC.
Las Palmas, Gran Canaria, pages 1499?1504.Clarkson, Philip and Ronald Rosenfeld.
1997.
Statistical lan-guage modeling using the CMU?cambridge toolkit.
InProceedings of Eurospeech.
Rhodes, Greece, pages 2707?2710.Hori, Chiori and Sadaoki Furui.
2004.
Speech summariza-tion: an approach through word extraction and a methodfor evaluation.
IEICE Transactions on Information andSystems E87-D(1):15?25.Jing, Hongyan.
2000.
Sentence reduction for automatic textsummarization.
In Proceedings of the 6th ANLP.
Seattle,WA, pages 310?315.Knight, Kevin and Daniel Marcu.
2002.
Summarization be-yond sentence extraction: a probabilistic approach to sen-tence compression.
Artificial Intelligence 139(1):91?107.Marciniak, Tomasz and Michael Strube.
2005.
Beyond thepipeline: Discrete optimization in NLP.
In Proceedings ofthe 9th CoNLL.
Ann Arbor, MI, pages 136?143.McDonald, Ryan.
2006.
Discriminative sentence compres-sion with soft syntactic constraints.
In Proceedings of the11th EACL.
Trento, Italy, pages 297?304.Nguyen, Minh Le, Akira Shimazu, Susumu Horiguchi,Tu Bao Ho, and Masaru Fukushi.
2004.
Probabilistic sen-tence reduction using support vector machines.
In Pro-ceedings of the 20th COLING.
Geneva, Switzerland, pages743?749.Olivers, S. H. and W. B. Dolan.
1999.
Less is more; eliminat-ing index terms from subordinate clauses.
In Proceedingsof the 37th ACL.
College Park, MD, pages 349?356.Press, William H., Saul A. Teukolsky, William T. Vetterling,and Brian P. Flannery.
1992.
Numerical Recipes in C: TheArt of Scientific Computing.
Cambridge University Press.Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav Zimak.2004.
Semantic role labeling via integer linear program-ming inference.
In Proceedings of the 20th COLING.Geneva, Switzerland, pages 1346?1352.Riezler, Stefan, Tracy H. King, Richard Crouch, and AnnieZaenen.
2003.
Statistical sentence condensation usingambiguity packing and stochastic disambiguation meth-ods for lexical-functional grammar.
In Proceedings ofthe HLT/NAACL.
Edmonton, Canada, pages 118?125.Roth, Dan and Wen-tau Yih.
2004.
A linear programmingformulation for global inference in natural language tasks.In Proceedings of the 8th CoNLL.
Boston, MA, pages 1?8.Turner, Jenine and Eugene Charniak.
2005.
Supervised andunsupervised learning for sentence compression.
In Pro-ceedings of the 43rd ACL.
Ann Arbor, MI, pages 290?297.Vandeghinste, Vincent and Yi Pan.
2004.
Sentence compres-sion for automated subtitling: A hybrid approach.
In Pro-ceedings of the ACL Workshop on Text Summarization.Barcelona, Spain, pages 89?95.Winston, Wayne L. and Munirpallam Venkataramanan.2003.
Introduction to Mathematical Programming.Brooks/Cole.151
