Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1256?1265,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsAnnotating and Predicting Non-Restrictive Noun Phrase ModificationsGabriel Stanovsky Ido DaganComputer Science Department, Bar-Ilan Universitygabriel.satanovsky@gmail.comdagan@cs.biu.ac.ilAbstractThe distinction between restrictive andnon-restrictive modification in nounphrases is a well studied subject inlinguistics.
Automatically identifyingnon-restrictive modifiers can provide NLPapplications with shorter, more salientarguments, which were found beneficialby several recent works.
While previouswork showed that restrictiveness can beannotated with high agreement, no largescale corpus was created, hindering thedevelopment of suitable classificationalgorithms.
In this work we devise a novelcrowdsourcing annotation methodology,and an accompanying large scale corpus.Then, we present a robust automatedsystem which identifies non-restrictivemodifiers, notably improving over priormethods.1 IntroductionLinguistic literature provides a large body of re-search distinguishing between two types of mod-ifiers within noun phrases: (1) Restrictive modi-fiers, which constitute an integral part of the entityreferenced by the NP, e.g., the underlined modifierin ?She wore the necklace that her mother gaveher?, versus (2) Non-restrictive modifiers, whichprovide an additional or parenthetical informationon an already definite entity, e.g., ?The speakerthanked president Obama who just came into theroom?
(Huddleston et al, 2002; Fabb, 1990; Um-bach, 2006).The distinction between the two types is seman-tic in nature and relies heavily on the context ofthe NP.
Evidently, many syntactic constructionscan appear in both restrictive and non-restrictiveuses.
While the previous examples were of rela-tive clauses, Figure 1 demonstrates this distinctionin various other syntactic constructions.Identifying and removing non-restrictive mod-ifiers yields shorter NP arguments, which provedbeneficial in many NLP tasks.
In the context ofabstractive summarization (Ganesan et al, 2010)or sentence compression (Knight and Marcu,2002), non-restrictive modifiers can be removedto shorten sentences, while restrictive modificationshould be preserved.Further, recent work in information extractionshowed that shorter arguments can be beneficialfor downstream tasks.
Angeli et al (2015) builtan Open-IE system which focuses on shorter ar-gument spans, and demonstrated its usefulness ina state-of-the-art Knowledge Base Population sys-tem.
Stanovsky et al (2015) compared the per-formance of several off-the-shelf analyzers in dif-ferent semantic tasks.
Most relevant to this workis the comparison between Open-IE and Seman-tic Role Labeling (Carreras and M`arquez, 2005).Specifically, they suggest that SRL?s longer argu-ments introduce noise which hurts performancefor downstream tasks.Finally, in question answering, omitting non-restrictive modification can assist in providingmore concise answers, or in matching betweenmultiple answer occurrences.Despite these benefits, there is currently no con-sistent large scale annotation of restrictiveness,which hinders the development of automatic toolsfor its classification.
In prior art in this field, Dor-nescu et al (2014) used trained annotators to markrestrictiveness in a large corpus.
Although theyreached good agreement levels in restrictivenessannotation, their corpus suffered from inconsisten-cies, since it conflated restrictiveness annotationwith inconsistent modifier span annotation.The contributions of this work are twofold.
Pri-marily, we propose a novel crowdsroucing anno-1256tation methodology which decouples the binary(restrictive / non-restrictive) distinction from themodifier span annotation (Section 3).
Followingthis methodology, in Section 4 we present a largescale annotated corpus, which will allow furtherresearch into the automatic identification of non-restrictive modification.Additionally, we developed a strong automaticclassifier, which learns from our new corpus (Sec-tion 5).
This classifier uses new linguistically mo-tivated features which are robust enough to per-form well over automatically predicted parse trees.The corpus and the automatic classifier are bothmade publicly available.1While there is still much room for improve-ment, especially in some of the harder, morecontext-dependent, cases (most notably, preposi-tional and adjectival modifiers), our system pro-vides an applicable means for identifying non-restrictive modification in a realistic NLP setting.2 BackgroundIn this section we cover relevant literature fromseveral domains.
In Section 2.1 we discuss the es-tablished linguistic distinction between restrictiveand non-restrictive modification.
Following, inSection 2.2 we discuss previous NLP work on an-notating and identifying this distinction.
Finally,in Section 2.3 we briefly describe the recent QA-SRL annotation paradigm (He et al, 2015), whichwe utilize in Section 3 as part of our annotationscheme.2.1 Non-Restrictive ModificationThroughout the paper we follow Huddleston etal.
?s (2002) well-known distinction between twotypes of NP modifiers: (1) Restrictive modifiers,for which the content of the modifier is an integralpart of the meaning of the containing NP, and, incontrast, (2) Non-restrictive modifiers, that presenta separate, parenthetical unit of information aboutthe NP.While some syntactic modifiers (such as deter-miners or genitives) are always restrictive, othersare known to appear in both restrictive as well asnon-restrictive uses, depending on semantics andcontext (Huddleston et al, 2002; Fabb, 1990; Um-bach, 2006).
Among these are relative clauses, ad-jectival, prepositional, non-finite, and verbal mod-1http://www.cs.biu.ac.il/?nlp/resources/downloads(RC1) The necklace that her mother gave her+is in the safe.
(RC2) The governor disagreed with the U.S am-bassador to China who seemed nervous?.
(NF1) People living near the site+will have tobe evacuated.
(NF2) sheriff Arthur Lester, standing against thewall?, looked tired.
(PP1) The kid from New York+won the lottery.
(PP2) The assassination of Franz Ferdinand fromAustria?started WWI.
(AD1) The good+boys won.
(AD2) The water level rose a good?12 inches.Figure 1: Restrictive (marked in red and a plussign) and non-restrictive (marked in blue and a mi-nus sign) examples in different syntactic construc-tions, see elaboration in Section 2.
Examples in-dex: RC - Relative clause, NF - Non-finite clauses(Huddleston et al [p. 1265]), PP - Prepositionalmodifiers, AD - Adjectival modifiers (Huddlestonet al [p. 528]).ifiers.
See Figure 1 for examples of different syn-tactic constructions appearing in both restrictive aswell as non-restrictive contexts.For example, for relative clause, Huddlestonet al [p. 1058] identifies both restrictive aswell as non-restrictive uses (for which they usethe terms integrated and supplementary, respec-tively).
In the sentence marked (RC1), the high-lighted relative clause is restrictive, distinguishingthe necklace being referred to from other neck-laces, while in sentence (RC2), the relative clausedoes not pick an entity from a larger set, but in-stead presents separate information about an al-ready specified definite entity.2.2 Non-Restrictive Modification in NLPSyntactic and semantic annotations generallyavoid the distinction between restrictive and non-restrictive modification (referred here as ?restric-tiveness?
annotation).The syntactic annotation of the Penn TreeBank(Marcus et al, 1993) and its common conversionto dependency trees (e.g., (de Marneffe and Man-ning, 2008)) do not differentiate the cases dis-cussed above, providing the same syntactic struc-ture for the semantically different instances.
SeeFigure 2 for an example.Furthermore, prominent semantic annotations,1257such as PropBank (Palmer et al, 2005), AMR(Banarescu et al, 2013), CCG (Hockenmaier andSteedman, 2007), or FrameNet (Baker et al,1998), also avoid this distinction.
For exam-ple, PropBank does not differentiate between suchmodifiers, treating both types of modification asan integral part of an argument NP.Two recent works have focused on automat-ically identifying non-restrictive modifications.Honnibal et al (2010) added simple automatedrestrictiveness annotations to NP-modifiers in theCCGbank (Hockenmaier and Steedman, 2007).Following a writing style and grammar rule, amodifier was judged as non-restrictive if and onlyif it was preceded by a comma.2This annotationwas not intrinsically evaluated, as it was carriedas part of an extrinsic evaluation of a statisticalparser.Having similar goals to ours, Dornescu et al(2014) sets the prior art at annotating and predict-ing non-restrictive modification.
In the annotationphase, each of their trained annotators was askedto (1) Mark spans of words in the sentence asforming an NP modifier, and (2) Mark each spanthey annotated in (1) as either restrictive or non-restrictive, and specify its type from a predefinedlist (e.g., relative clause, adjectival modifier, etc.
).Their inter-annotator agreement on the first task(modifier span) was low, reaching pairwise F1score of only 54.9%, possibly due to problemsin the annotation procedure, as acknowledged bythe authors.
The second part of the annotationachieved better agreement levels, reaching kappaof 0.78 (substantial agreement) for type annotationand 0.51 (moderate agreement) for restrictivenessannotation.3Following the creation of the annotated dataset,they developed rule based and machine learningclassifiers.
All of their classifiers performed onlyat about 47% F1, at least partly due to the incon-sistencies in span annotation discussed above.To conclude this survey, although an effort wasmade by Dorenscu et al (2014), there is currentlyno available consistent corpus annotated with non-restrictive modifiers.2Notice that this is indeed the case in some of the non-restrictive examples in Figure 1.3Note that the agreement for the first task is reported inF1 while the second task is reported in Cohen?s kappa.the boy who entered the roomdetnsubjrcmoddetdobjpresident Obama who entered the roomnnnsubjrcmoddetdobjFigure 2: Restrictive (top) and non-restrictive(bottom) NP modifications receive the same rep-resentation in dependency trees.
See Section 2.2.2.3 QA-SRLTraditional Semantic Role Labeling (SRL) (Car-reras and M`arquez, 2005) is typically perceived asanswering argument role questions, such as who,what, to whom, when, or where, regarding a tar-get predicate.
For instance, PropBank?s ARG0 forthe predicate say answers the question ?who saidsomething?
?.QA-SRL (He et al, 2015) suggests that answer-ing explicit role questions is an intuitive meansto solicit predicate-argument structures from non-expert annotators.
Annotators are presented with asentence in which a target predicate4was marked,and are requested to annotate argument role ques-tions, phrased using a restricted grammar, and cor-responding answers.For example, given the sentence ?PresidentObama who flew to Russia called the vice presi-dent?
and the target predicate called, an annotatorcan intuitively provide the following QA pairs: (1)Who called?
President Obama and (2) Whom didsomeone call?
the vice president.In order to assess the validity of their annotationscheme, He et al annotated a large sample of thePropBank corpus (1241 sentences) with QA-SRL,and showed high agreement with PropBank overthis sample.
In the following sections we makeuse of these explicit role questions for annotatingnon-restrictive modifiers.3 Annotation MethodologyAs mentioned in the Introduction, the first goalof this work is to assemble a large and consis-tent corpus, annotated with non-restrictive modifi-cations.
In this section, we present a crowdsourc-ing methodology which allows us to generate suchcorpus in a cost-effective manner (Section 3.2).
Asa preliminary step, we conducted a smaller scale4Currently consisting of automatically annotated verbs.1258expert annotation (Section 3.1), which will serveas a gold standard with which to test the crowsd-sourced annotations.3.1 Expert AnnotationTwo researchers, with linguistics and NLP educa-tion, were presented with a sample of 219 mod-ifiers of NPs in 100 sentences,5and were askedto annotate each modifier as either restrictive ornon-restrictive, according to the linguistic defini-tion presented in Section 2.
Prior to annotating theexpert dataset, the annotators discussed the pro-cess and resolved conflicts on a development setof 20 modifiers.The annotators agreement was found to be high,reaching agreement on 93.5% of the instances, and?
of 84.2% .
An analysis of the few disagreementsfound that the deviations between the annotatorsstem from semantic ambiguities, where two legit-imate readings of the sentence led to disagreeingannotations.
For example, in ?sympathetic fanshave sent Ms. Shere copies of her recipes clippedfrom magazines over the years?, one annotatorread the underlined modifier clause as restrictive,identifying particular recipes, while the second an-notator read the modifier as non-restrictive, addingsupplementary information on the sent recipes.Finally, we compose the expert annotationdataset from the 207 modifiers agreed upon byboth annotators.
In the next section we usethis dataset to evaluate the quality of our crowd-sourced annotations.3.2 Crowdsourcing Annotation ProcessIn our scheme, each annotation instance assigns abinary label (restrictive or non-restrictive) to a 4-tuple (s, v, p,m) ?
where m is a modifier of thenoun phrase p, which is an argument of a verbalpredicate v, in a sentence s. We incorporate v inour scheme in order to provide non-trained anno-tators with an argument role question (discussedin 2.3), as elaborated below.6Consider, for example, the sentence s ?
?thespeaker thanked [President Obama who just en-tered the room]?.
We want to annotate the re-strictiveness value of the relative clause m (under-lined), which modifies the matrix noun phrase p5These were taken at random from the development par-tition of the corpus described in Section 4.6Our annotation currently covers the most common caseof NPs which serve as arguments of verbal predicates.
(bracketed), which is in turn an argument of a gov-erning predicate v (in bold).Our annotation procedure does not require theannotator to be familiar with the formal linguisticdefinition of restrictiveness.
Instead, we use bi-nary question-answering (true / false questions) asan intuitive formulation of non-restrictive modifi-cation.
We present annotators with the argumentrole question pertaining to the argument NP, andask whether this NP without the modifier gives thesame answer to the argument role question as theoriginal NP did.In our example, an annotator is presented withthe argument role question ?whom did someonethank??
(which is answered by p), and is askedto decide whether the reduced NP, ?PresidentObama?, provides the same answer to the ques-tion as the full NP does.
If the answer is positive(as in this case), we consider the modifier to benon-restrictive, otherwise we consider it to be re-strictive.As an example for the restrictive case, consider?she wore [the necklace that her mother gaveher]?, and the respective argument role-question?what did someone wear??.
In this case, as op-posed to the previous example, the reduced NP(?the necklace?)
does not refer to the same entityas the original NP, since we lose the specific iden-tity of the necklace which was worn.The intuition for this process arises from thelinguistic definition for modifier restrictiveness.Namely, a restrictive modifier is defined as an in-tegral part of the NP, and a non-restrictive modifieras providing supplementary or additional informa-tion about it.
Therefore, in the restrictive case,omitting the modifier would necessarily changethe meaning of the answer, while in the non-restrictive case, omitting it would not change theentity referenced by the full NP, and would there-fore provide the same answer to the argument rolequestion.4 CorpusIn this section we describe the creation of a consis-tent human-annotated restrictiveness corpus, usingthe annotation process described in the previoussection.
We show this corpus to be of high qualityby comparing it with the independent expert anno-tation.
In Section 5 we use this corpus to train andtest several automatic classifiers.1259Modifier Type Identified By # Non-RestrictiveAgreement?
%Adjectival pos = JJ 684 41.36% 74.70 87.36Prepositional pos = IN / TMP / LOC 693 36.22% 61.65 85.10Appositive rel = APPO / PRN 342 73.68% 60.29 80.00Non-Finite rel = TO 279 68.82% 71.04 86.48Verbal pos = VB and not relative clause 150 69.33% 100 100Relative clause pos = VB and child pos = WP 43 79.07% 100 100Total - 2191 51.12% 73.79 87.00Table 1: Corpus statistics by modifier types, which were identified by part of speech (pos) and depen-dency label (rel) (Section 4.1).
The number of instances (#) and non-restrictiveness percentage refer tothe full crowdsourced annotation.
Agreement (Cohen?s ?
and percent of matching instances) is reportedfor the expert-annotated data (Section 4.2), between the expert and crowdsourced annotations.4.1 Data CollectionWe use the dataset which He et al (2015) an-notated with Question-Answer pairs (discussed inSection 2.3), and keep their train / dev / test splitinto 744 / 249 / 248 sentences, respectively.
Thisconveniently allows us to link between argumentNPs and their corresponding argument role ques-tion needed for our annotation process, as de-scribed in previous section.This dataset is composed of 1241 sentencesfrom the CoNLL 2009 English dataset (Haji?c etal., 2009), which consists of newswire text anno-tated by the Penn TreeBank (Marcus et al, 1993),PropBank (Palmer et al, 2005), and NomBank(Meyers et al, 2004), and converted into depen-dency grammar by (Johansson and Nugues, 2008).As mentioned in Section 3.2, each of our anno-tation instances is composed of a sentence s, a ver-bal predicate v, a noun phrase p, and a modifierm.We extract each such possible tuple from the set ofsentences in the following automatic manner:1.
Identify a verb v in the gold dependency tree.2.
Follow its outgoing dependency arcs to anoun phrase argument p (a dependent of vwith a nominal part of speech).3.
Find m, a modifying clause of p which mightbe non-restrictive, according to the rules de-scribed in Table 1, under the ?Identified By?column.
This filters out modifiers whichare always restrictive, such as determinersor genitives, following (Huddleston et al,2002), as discussed in Section 2.
Notice thatthis automatic modifier detection decouplesthe span annotation from the restrictivenessannotation, which was a source for inconsis-tencies in Dornescu et als annotation (Sec-tion 2.2).This automatic process yields a dataset of 2191modifiers of 1930 NPs in 1241 sentences.
We notethat our collection process ensures that the cor-pus correlates with the syntactic dependency an-notation of the CoNLL 2009 shared task, and cantherefore be seen as an augmentation of its modi-fier labels to include restrictiveness annotations.In order to find the corresponding argument rolequestion, we follow the process carried by He etal.
; An argument NP is matched to an annotatedQuestion-Answer pair if the NP head is within theannotated answer span.
Following this matchingprocess yields a match for 1840 of the NPs.For the remaining 90 NPs we manually com-pose an argument role question by looking at thegoverning predicate and its argument NP.
For ex-ample, given the sentence ?
[The son of an im-migrant stonemason of Slovenian descent] wasraised in a small borough outside Ebensburg?, thepredicate raised and the bracketed NP argument,we produce the argument role question ?Who wasraised?
?.The corpus category distribution is depicted inTable 1, under column labeled ?#?.
In later sec-tions we report agreement and performance acrossthese categories to produce finer grained analyses.4.2 Crowdsourcing AnnotationWe use Amazon Mechanical Turk7to annotate the2191 modifiers for restrictiveness, according to the7https://www.mturk.com1260process defined in Section 3.2.
Each modifier wasgiven to 5 annotators, and the final tag was as-signed by majority vote.
We used the developmentset to refine the guidelines, task presentation, andthe number of annotators.Each annotator was paid 5c for the annotation ofan NP, which in average provided 1.16 modifiers.This sets the average price for obtaining a singlemodifier annotation at 5 ?51.16= 21.5c.The agreement with the 217 NP modifiers anno-tated by the experts (Section 3.1) and percentageof positive (non-restrictive) examples per categorycan be found in Table 1, in the columns labeled?agreement?.
The labels are generally balanced,with 51.12% non-restrictive modifiers in the entiredataset (varying between 36.22% for prepositionalmodifiers and 79.07% for relative clauses).Overall, the crowdsourced annotation reachedgood agreement levels with our expert annota-tion, achieving 73.79 ?
score (substantial agree-ment).
The lowest agreement levels were foundon prepositional and appositive modifiers (61.65%and 60.29%).8Indeed, as discussed in Section2, these are often subtle decisions which relyheavily on context.
For example, the followinginstances were disagreed upon between our ex-pert annotation and the crowdsourced annotation:In ?
[Charles LaBella , the assistant U.S. attor-ney prosecuting the Marcos case], did n?t returnphone calls seeking comment?
(an appositive ex-ample), the experts annotated the underlined mod-ifier as non-restrictive, while the crowdsource an-notation marked it as restrictive.
Inversely, in ?Theamendment prompted [an ironic protest] fromMr.
Thurmond?, the experts annotated the adjecti-val modifier as restrictive, while the crowdsourceannotation tagged it as non-restrictive.5 Predicting Non-RestrictiveModificationIn this section we present an automatic systemwhich: (1) Identifies NP modifiers in a depen-dency parser?s output (as shown in Table 1, col-umn ?Identified By?)
and (2) Uses a CRF model toclassify each modifier as either restrictive or non-restrictive, based on the features listed in Table 2,8While linguistic literature generally regards appositivesas non-restrictive, some of the appositions marked in the de-pendency conversion are in fact misclassified coordinations,which explains why some of them were marked as restrictive.and elaborated below.95.1 BaselinesWe begin by replicating the algorithms in the twoprior works discussed in Section 2.2.
This allowsus to test their performance consistently againstour new human annotated dataset.Replicating (Honnibal et al, 2010) They anno-tated a modifier as restrictive if and only if it waspreceded with a comma.
We re-implement thisbaseline and classify all of the modifiers in the testset according to this simple property.Replicating (Dornescu et al, 2014) Their bestperforming ML-based algorithm10uses the super-vised CRFsuite classifier (Okazaki, 2007) over?standard features used in chunking, such as wordform, lemma and part of speech tags?.
Replicat-ing their baseline, we extract the list of featuresdetailed in Table 2 (in the row labeled ?chunkingfeatures?
).5.2 Our ClassifierIn addition to Dornescu et al?s generic chunkingframework, we also extract features which wereidentified in the linguistic literature as indicativefor non-restrictive modifications.
These featuresare then used in the CRFsuite classifier (the sameCRF classifier used by Donescu et al) to make thebinary decision.
The following paragraphs elabo-rate on the motivation for each of the features.Enclosing commas We extend Honnibal?s etal.
?s classification method as a binary featurewhich marks whether the clause is both precededand terminated with a comma.
This followsfrom a well-known writing style and grammar rulewhich indicates that non-restrictive clausal modi-fiers should be enclosed with a comma.Governing relative In the linguistic literature,it was posited that the word introducing a clausalmodifier (termed relative) is an indication for therestrictiveness of the subordinate clause.
For ex-ample, Huddleston.
et al (2002) [p. 1059]analyzes the word ?that?
as generally introduc-ing a restrictive modifier, while a wh-pronoun is9We use our new crowdsourced corpus to train our modelas well as the baseline model.10They also implement a rule-based method, namedDAPR, which, when combined with the described ML ap-proach surpasses their ML algorithm by ?1.5% increase inF1.
We could not find a publicly available implementation ofthis method.1261more likely to introduce non-restrictive modifica-tion.
We therefore extract features of the wordwhich governs the relative, such as the surfaceform, its lemma, POS tag, and more.
The full listis shown under ?Governing relative?
in Table 2.Named entities As illustrated throughout thepaper, modifiers of named entities tend to be non-restrictive.
We run the Stanford Named EntityRecognizer (NER) (Finkel et al, 2005) and intro-duce a feature indicating the type of named entity(PERSON, ORG or LOC), where applicable.Lexical word embeddings We include the pre-trained word embeddings of the modifier?s headword, calculated by (Mikolov et al, 2013).
Thesedistributional features help the classifier associatebetween similar words (for example, if ?good?
isnon-restrictive in some contexts, it is likely that?fine?
is also non-restrictive within similar con-texts).Modifier type We add the automatically identi-fied modifier type as a feature, to associate certainfeatures as indicative for certain types of modifiers(e.g., enclosing commas might be good indicatorsfor relative clause, while word embeddings can bespecifically helpful for adjectival modifiers).6 EvaluationWe use the QA-SRL test section (containing 412NP modifiers) to evaluate each of the systems de-scribed in Section 5 on gold and predicted trees,both provided in the CoNLL 2009 dataset (the pre-dicted dependency relations were obtained usingMaltParser (Nivre et al, 2007)).
The gold settingallows us to test the performance of the systemswithout accumulating parser errors.
In addition, itallows us to partition and analyze our dataset ac-cording to the gold modifier type.
The predictedsetting, on the other hand, allows us to evaluateour classifier in a real-world application scenario,given automatic parsing output.6.1 Gold TreesThe results for each of the systems across our cate-gories on the gold trees are shown in Table 3.
Notethat we regard non-restrictive modification as pos-itive examples, and restrictive modification as neg-ative examples.
This is in line with the applicativegoal of reducing argument span by removing non-restrictive modifiers, discussed in the Introduction.Switching the labels does not significantly changeSystem Feature Type DescriptionHonnibal et al Preceding comma w[-1] == ,Chunking features feats[head-1](Dornescu et al) feats[head]feats[head+1]This paper Enclosing commastrue iff theclause is precededand terminatedwith commasfeats[parent-1]Governing relative feats[parent]feats[parent+1]feats[pobj-1]Prepositions feats[pobj]feats[pobj+1]NERPERSON,ORGANIZATION,LOCATIONLexical wordembeddingsMikolov et als300-dimensionalcontinuous wordembeddingsModifier typeone of thetypes describedin Table 1Table 2: Features used for classification in eachof the systems as described in Section 5. head -head of the modifier in the dependency tree.
par-ent - parent of head in the dependency tree.
pobj- object of the preposition, in case of prepositionalhead.
feats[i] refers to extracting the followingfeatures from the word i: POS tag, lemma, is title,is all lower case, is all upper case, is beginning /end of sentence.the numbers, since the corpus is relatively wellbalanced between the two labels (as can be seen inTable 1).
Following are several observations basedon an error analysis of these results.Prepositional and adjectival modifiers areharder to predict All systems had more diffi-culties in classifying both of these categories.
Thisreiterates the relatively lower agreement for thesecategories between the crowdsource and expertannotation, discussed in Section 4.2.For clausal modifiers, preceding commas aregood in precision but poor for recall As canbe seen in Honnibal et al?s columns, a precedingcomma is a good indication for a non-restrictiveclausal modifier (all categories excluding adjecti-val or verbal modifiers), but classifying solely byits existence misses many of the non-restrictive in-stances.1262Modifier Type # Precision Recall F1Honnibal Dornescu Our Honnibal Dornescu Our Honnibal Dornescu OurPrepositional 135 .83 .67 .69 .1 .16 .41 .18 .26 .51Adjectival 111 .33 .38 .59 .06 .06 .21 .11 .11 .31Appositive 78 .77 .81 .82 .34 .93 .98 .47 .87 .89Non-Finite 55 .77 .63 .64 .29 .97 .97 .42 .76 .77Verbal 20 0 .75 .75 0 1 1 0 .86 .86Relative clause 13 1 .85 .85 .27 1 1 .43 .92 .92Total 412 .72 .72 .73 .19 .58 .68 .3 .64 .72Table 3: Test set performance of the 3 different systems described in Sections 5 and 6 on gold trees fromthe CoNLL 2009 dataset, across the different categories defined in Section 4.Features P R F1All .73 .68 .72Baseline- comma .72 .68 .70- chunking .72 .66 .69New- governing relative .74 .61 .67- prepositions .73 .67 .70- word embeddings .72 .69 .71- NER .71 .68 .70- mod type .74 .66 .70Table 4: Feature ablation tests on gold trees.
Eachrow specifies a different feature set ?
?All?
speci-fies the entire feature set from Table 2, while eachsubsequent line removes one type of features.
(Dornescu et al, 2014) performs better on ourdataset Their method achieves much better re-sults on our dataset (compare 64% overall F1 onour dataset with their reported 45.29% F1 on theirdataset).
This speaks both for their method as avalid signal for restrictiveness annotation, as wellas for the improved consistency of our dataset.Our system improves recall Overall, our sys-tem significantly outperforms both baselines bymore than 8% gain in F1 score.
Specifically, thenumbers show clearly that we improve recall in thefrequent categories of prepositional and adjectivalmodifiers.
Furthermore, the results of an ablationtest on our features (shown in Table 4) show thatchunking and governing relative features providethe highest individual impact.6.2 Predicted TreesTo test our classifier in a realistic setting we evalu-ate its performance on predicted dependency trees.To obtain the candidate modifiers, we use the sameextractor presented in previous sections, appliedon the predicted trees in the test section of theCoNLL 2009 dataset.
We then apply the modelsSystem P R F1Candidate Extraction .91 .93 .92Honnibal .71 .18 .29Dornescu .68 .53 .59Our .69 .63 .66Table 5: Restrictiveness results (bottom threelines) on predicted trees.
The top line (CandidateExtraction) measures the percent of correct modi-fiers identified in the predicted trees (shared acrossall of the classifiers).
See Section 6.2.trained on the gold train set of the same corpus.For evaluation, we use the gold labels and com-pute (1) precision ?
the percent of predicted non-restrictive modifiers which match a gold non-restrictive modifier, and (2) recall ?
the percent ofgold non-restrictive modifiers which match a pre-dicted non-restrictive modifier.
Note that this met-ric is strict, conflating both parser errors with ourclassifier?s errors.
The results are shown in Table5.The first line in the table measures the perfor-mance of the modifier extractor module on the pre-dicted trees.
A predicted modifier is consideredcorrect if it agrees with a gold modifier on both itssyntactic head as well as its span.
The modifierextractor module is shared across all classifiers, asdiscussed in Section 5, and its performance on thepredicted trees imposes an upper bound on all theclassifiers.Both our and Dornescu?s classifiers drop 5-6points in F1, keeping the differences observedon the gold trees, while Honnibal et al?s simplecomma-based classifier is less sensitive to parsererrors, dropping only one point in F1.This small drop stems from our classifierslargely relying only on the modifier head and itsspan for feature computation, generally ignoring1263parsing errors within the modifier subtree.7 Conclusions and Future WorkWe presented an end-to-end framework for restric-tiveness annotation, including a novel QA-SRLbased crowdsourcing methodology and a first con-sistent human-annotated corpus.
Furthermore, wepresented a linguistically motivated classifier, sur-passing the previous baseline by 8% gain in F1.Future work can use our annotated corpus to de-velop classifiers that deal better with prepositionaland adjectival modifiers, which require deeper se-mantic analysis.AcknowledgmentsWe would like to thank Michael Elhadad and YoavGoldberg for fruitful discussions, and the anony-mous reviewers for their helpful comments.This work was supported in part by grants fromthe MAGNET program of the Israeli Office of theChief Scientist (OCS), the Israel Science Founda-tion grant 880/12, and the German Research Foun-dation through the German-Israeli Project Cooper-ation (DIP, grant DA 1600/1-1).ReferencesGabor Angeli, Melvin Johnson Premkumar, andChristopher D. Manning.
2015.
Leveraging lin-guistic structure for open domain information ex-traction.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguistics(ACL 2015).Collin F Baker, Charles J Fillmore, and John B Lowe.1998.
The berkeley framenet project.
In Proceed-ings of ACL, pages 86?90.
Association for Compu-tational Linguistics.Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract meaning representationfor sembanking.Xavier Carreras and Llu?
?s M`arquez.
2005.
Introduc-tion to the conll-2005 shared task: Semantic role la-beling.
In Proceedings of CONLL, pages 152?164.Marie-Catherine de Marneffe and Christopher D Man-ning.
2008.
The stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, pages 1?8.Iustin Dornescu, Richard Evans, and ConstantinOrasan.
2014.
Relative clause extraction for syn-tactic simplification.
COLING 2014, page 1.Nigel Fabb.
1990.
The difference between english re-strictive and nonrestrictive relative clauses.
Journalof linguistics, 26(01):57?77.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: a graph-based approach to abstrac-tive summarization of highly redundant opinions.
InProceedings of the 23rd international conference oncomputational linguistics, pages 340?348.
Associa-tion for Computational Linguistics.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, et al 2009.
The conll-2009shared task: Syntactic and semantic dependenciesin multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?18.
Associa-tion for Computational Linguistics.Luheng He, Mike Lewis, and Luke Zettlemoyer.
2015.Question-answer driven semantic role labeling: Us-ing natural language to annotate natural language.In the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).Julia Hockenmaier and Mark Steedman.
2007.
Ccg-bank: A corpus of ccg derivations and dependencystructures extracted from the penn treebank.
InComputational Linguistics.Matthew Honnibal, James R Curran, and Johan Bos.2010.
Rebanking ccgbank for improved np inter-pretation.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 207?215.
Association for ComputationalLinguistics.Rodney Huddleston, Geoffrey K Pullum, et al 2002.The cambridge grammar of english.
Language.Cambridge: Cambridge University Press.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic-semantic analysis withpropbank and nombank.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning, pages 183?187.
Association forComputational Linguistics.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139(1):91?107.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational linguistics, 19(2):313?330.1264Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
The nombank project:An interim report.
In HLT-NAACL 2004 workshop:Frontiers in corpus annotation, pages 24?31.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(02):95?135.Naoaki Okazaki.
2007.
Crfsuite: a fast implementa-tion of conditional random fields (crfs).Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational linguistics,31(1):71?106.Gabriel Stanovsky, Ido Dagan, and Mausam.
2015.Open IE as an intermediate structure for semantictasks.
In Proceedings of the 53rd Annual Meeting ofthe Association for Computational Linguistics (ACL2015).Carla Umbach.
2006.
Non-restrictive modification andbackgrounding.
In Proceedings of the Ninth Sympo-sium on Logic and Language, pages 152?159.1265
