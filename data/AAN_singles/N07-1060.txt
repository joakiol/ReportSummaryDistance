Proceedings of NAACL HLT 2007, pages 476?483,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsAutomatic and human scoring of word definition responsesKevyn Collins-Thompson and Jamie CallanLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA, U.S.A. 15213-8213{kct|callan}@cs.cmu.eduAbstractAssessing learning progress is a criticalstep in language learning applications andexperiments.
In word learning, for exam-ple, one important type of assessment isa definition production test, in which sub-jects are asked to produce a short defini-tion of the word being learned.
In currentpractice, each free response is manuallyscored according to how well its mean-ing matches the target definition.
Manualscoring is not only time-consuming, butalso limited in its flexibility and ability todetect partial learning effects.This study describes an effective auto-matic method for scoring free responsesto definition production tests.
The algo-rithm compares the text of the free re-sponse to the text of a reference definitionusing a statistical model of text semanticsimilarity that uses Markov chains on agraph of individual word relations.
Themodel can take advantage of both corpus-and knowledge-based resources.
Evalu-ated on a new corpus of human-judgedfree responses, our method achieved sig-nificant improvements over random andcosine baselines in both rank correlationand label error.1 IntroductionHuman language technologies are playing an in-creasingly important role in the science and prac-tice of language learning.
For example, intelligentComputer Assisted Language Learning (CALL) sys-tems are being developed that can automatically tai-lor lessons and questions to the needs of individualstudents (Heilman et al, 2006).
One critical taskthat language tutors, word learning experiments, andrelated applications have in common is assessing thelearning progress of the student or experiment sub-ject during the course of the session.When the task is learning new vocabulary, a vari-ety of tests have been developed to measure wordlearning progress.
Some tests, such as multiple-choice selection of a correct synonym or cloze com-pletion, are relatively passive.
In production tests,on the other hand, students are asked to write or saya short phrase or sentence that uses the word beinglearned, called the target word, in a specified way.In one important type of production test, called adefinition production test, the subject is asked to de-scribe the meaning of the target word, as they under-stand it at that point in the session.
The use of suchtests has typically required a teacher or researcherto manually score each response by judging its sim-ilarity in meaning to the reference definition of thetarget word.
The resulting scores can then be usedto analyze how a person?s learning of the word re-sponded to different stimuli, such as seeing the wordused in context.
A sample target word and its ref-erence definition, along with examples of human-judged responses, are given in Sections 3.3 and 4.1.However, manual scoring of the definition re-sponses has several drawbacks.
First, it is time-consuming and must be done by trained experts.Moreover, if the researcher wanted to test a new hy-476pothesis by examining the responses with respect toa different but related definition, the entire set of re-sponses would have to be manually re-scored againstthe new target.
Second, manual scoring can oftenbe limited in its ability to detect when partial learn-ing has taken place.
This is due to the basic trade-off between the sophistication of the graded scoringscale, and the ease and consistency with which hu-man judges can use the scale.
For example, it maybe that the subject did not learn the complete mean-ing of a particular target word, but did learn that thistarget word had negative connotations.
The usualbinary or ternary score would provide no or littleindication of such effects.
Finally, because manualscoring almost always must be done off-line after theend of the session, it presents an obstacle to our goalof creating learning systems that can adapt quickly,within a single learning session.This study describes an effective automatedmethod for assessing word learning by scoring freeresponses to definition production tests.
The methodis flexible: it can be used to analyze a response withrespect to whatever reference target(s) the teacher orresearcher chooses.
Such a test represents a pow-erful new tool for language learning research.
It isalso a compelling application of human languagetechnologies research on semantic similarity, andwe review related work for that area in Section 2.Our probabilistic model for computing text seman-tic similarity, described in Section 3, can use bothcorpus-based and knowledge-based resources.
InSection 4 we describe a new dataset of human def-inition judgments and use it to measure the effec-tiveness of the model against other measures of textsimilarity.
Finally, in Section 5 we discuss furtherdirections and applications of our work.2 Related WorkThe problem of judging a subject response against atarget definition is a type of text similarity problem.Moreover, it is a text semantic similarity task, sincewe require more than measuring direct word overlapbetween the two text fragments.
For example, if thedefinition of the target word ameliorate is to improvesomething and the subject response is make it better,the response clearly indicates that the subject knowsthe meaning of the word, and thus should receive ahigh score, even though the response and the targetdefinition have no words in common.Because most responses are short (1 ?
10 words)our task falls somewhere between word-word simi-larity and passage similarity.
There is a broad fieldof existing work in estimating the semantic similar-ity of individual words.
This field may be roughlydivided into two groups.
First, there are corpus-based measures, which use statistics or models de-rived from a large training collection.
These requirelittle or no human effort to construct, but are limitedin the richness of the features they can reliably repre-sent.
Second, there are knowledge-based measures,which rely on specialized resources such as dictio-naries, thesauri, experimental data, WordNet, and soon.
Knowledge-based measures tend to be comple-mentary to a corpus-based approach and emphasizeprecision in favor of recall.
This is discussed further,along with a good general summary of text semanticsimilarity work, by (Mihalcea et al, 2006).Because of the fundamental nature of the se-mantic similarity problem, there are close connec-tions with other areas of human language tech-nologies such as information retrieval (Salton andLesk, 1971), text alignment in machine transla-tion (Jayaraman and Lavie, 2005), text summariza-tion (Mani and Maybury, 1999), and textual co-herence (Foltz et al, 1998).
Educational applica-tions include automated scoring of essays, surveyedin (Valenti et al, 2003), and assessment of short-answer free-response items (Burstein et al, 1999).As we describe in Section 3, we use a graph tomodel relations between words to perform a kindof semantic smoothing on the language models ofthe subject response and target definition beforecomparing them.
Several types of relation, suchas synonymy and co-occurrence, may be combinedto model the interactions between terms.
(Caoet al, 2005) also formulated a term dependencymodel combining multiple term relations in a lan-guage modeling framework, applied to informationretrieval.
Our graph-based approach may be viewedas a probabilistic variation on the spreading activa-tion concept, originally proposed for word-word se-mantic similarity by (Quillian, 1967).Finally, (Mihalcea et al, 2006) describe a text se-mantic similarity measure that combines word-wordsimilarities between the passages being compared.477Due to limitations in the knowledge-based similaritymeasures used, semantic similarity is only estimatedbetween words with the same part-of-speech.
Ourgraph-based approach can relate words of differenttypes and does not have this limitation.
(Mihalceaet al, 2006) also evaluate their method in terms ofparaphrase recognition using binary judgments.
Weview our task as somewhat different than paraphraserecognition.
First, our task is not symmetric: we donot expect the target definition to be a paraphraseof the subject?s free response.
Second, because weseek sensitive measures of learning, we want to dis-tinguish a range of semantic differences beyond abinary yes/no decision.3 Statistical Text Similarity ModelWe start by describing relations between pairs ofterms using a general probability distribution.
Thesepairs can then combine into a graph, which we canapply to define a semantic distance between terms.3.1 Relations between individual wordsOne way to model word-to-word relationships is us-ing a mixture of links, where each link defines a par-ticular type of relationship.
In a graph, this may berepresented by a pair of nodes being joined by mul-tiple weighted edges, with each edge correspond-ing to a different link type.
Our link-based modelis partially based on one defined by (Toutanova etal., 2004) for prepositional attachment.
We allowdirected edges because some relationships such ashypernyms may be asymmetric.
The following areexamples of different types of links.1.
Stemming: Two words are based on commonmorphology.
Example: stem and stemming.We used Porter stemming (Porter, 1980).2.
Synonyms and near-synonyms: Two wordsshare practically all aspects of meaning.Example: quaff and drink.
Our synonymscame from WordNet (Miller, 1995).3.
Co-occurrence.
Both words tend to appear to-gether in the same contexts.Example: politics and election.4.
Hyper- and hyponyms: Relations such as ?Xis a kind of Y ?, as obtained from Wordnet orother thesaurus-like resources.Example: airplane and transportation.5.
Free association: A relation defined by the factthat a person is likely to give one word as a free-association response to the other.Example: disaster and fear.
Our data was ob-tained from the Univ.
of South Florida associa-tion database (Nelson et al, 1998).We denote link functions using ?1, .
.
.
, ?m tosummarize different types of interactions betweenwords.
Each ?m(wi, wj) represents a specific typeof lexical or semantic relation or constraint betweenwi and wj .
For each link ?m, we also define aweight ?m that gives the strength of the relationshipbetween wi and wj for that link.Our goal is to predict the likelihood of a targetdefinition D given a test response R consisting ofterms {w0 .
.
.
wk} drawn from a common vocabu-lary V .
We are thus interested in the conditional dis-tribution p(D | R).
We start by defining a simplemodel that can combine the link functions in a gen-eral purpose way to produce the conditional distribu-tion p(wi|wj) given arbitrary terms wi and wj .
Weuse a log-linear model of the general formp(wi|wj) =1ZexpL?m=0?m(i)?m(wi, wj) (1)In the next sections we show how to combine theestimate of individual pairs p(wi|wj) into a largergraph of term relations, which will enable us to cal-culate the desired p(D | R).3.2 Combining term relations using graphsGraphs provide one rich model for representing mul-tiple word relationships.
They can be directed orundirected, and typically use nodes of words, withword labels at the vertices, and edges denoting wordrelationships.
In this model, the dependency be-tween two words represents a single inference stepin which the label of the destination word is inferredfrom the source word.
Multiple inference steps maythen be chained together to perform longer-range in-ference about word relations.
In this way, we can in-fer the similarity of two terms without requiring di-rect evidence for the relations between that specificpair.
Using the link functions defined in Section 3.1,478we imagine a generative process where an author Acreates a short text of N words as follows.Step 0: Choose an initial word w0 with probabil-ity P (w0|A).
(If we have already generated Nwords, stop.
)Step i: Given we have chosen wi?1, then with prob-ability 1??
output the word wi?1 and reset theprocess to step 0.
Otherwise, with probability?, sample a new word wi according to the dis-tribution:P (wi|wi?1) =1ZexpL?m=0?m(i)?m(wi, wi?1)(2)where Z is the normalization quantity.This conditional probability may be interpretedas a mixture model in which a particular link type?m(.)
is chosen with probability ?m(i) at timestepi.
Note that the mixture is allowed to change at eachtimestep.
For simplicity, we limit the number ofsuch changes by grouping the timesteps of the walkinto three stages: early, middle and final.
The func-tion ?
(i) defines how timestep i maps to stage s,where s ?
{0, 1, 2}, and we now refer to ?m(s) in-stead of ?m(i).Suppose we have a definition D consisting ofterms {di}.
For each link type ?m(.)
we define atransition matrix C(D,m) based on the definitionD.
The reason D influences the transition matrixis that some link types, such as proximity and co-occurrence, are context-specific.
Each stage s hasan overall transition matrix C(D, s) as the mixtureof the individual C(D,m), as follows.C(D, s) =M?m=1?m(s)C(D,m) (3)Combining the stages over k steps into a singletransition matrix, which we denote Ck, we haveCk =k?i=0C(D,?
(i)) (4)We denote the (i, j) entry of a matrix Ak by Aki,j .Then for a particular term di, the probability that achain reaches di after k steps, starting at word w isPk(di|w) = (1 ?
?
)?kCkw,di (5)where we identify w and di with their correspondingindices into the vocabulary V .
The overall probabil-ity p(di|w) of generating a definition term di givena word w is thereforeP (di|w) =?
?k=0Pk(di|w) = (1??)(?
?k=0?kCk)w,di(6)The walk continuation probability ?
can beviewed as a penalty for long chains of inference.
Inpractice, to perform the random walk steps we re-place the infinite sum of Eq.
6 with a small numberof steps (up to 5) on a sparse representation of theadjacency graph.
We obtained effective link weights?m(i) empirically using held-out data.
For simplic-ity we assume that the same ?
is used across all linktypes, but further improvement may be possible byextending the model to use link-specific decays ?m.Fine-tuning these parameter estimation methods is asubject of future work.3.3 Using the model for definition scoringIn our study the reference definition for the targetword consisted of the target word, a rare synonym,a more frequent synonym, and a short glossary-likedefinition phrase.
For example, the reference defini-tion for abscond wasabscond; absquatulate; escape; to leave quicklyand secretly and hide oneself, often to avoid arrestor prosecution.In general, we define the score of a response Rwith respect to a definition D as the probabilitythat the definition is generated by the response, orp(D|R).
Equivalently, we can score by log p(D|R)since the log function is monotonic.
So making thesimplifying assumption that the terms di ?
D areexchangable (the bag-of-words assumption), andtaking logarithms, we have:log p(D|R) = log?di?Dp(di|R)=?di?Dlog[(1 ?
?
)(m?k=0?kCk)R,di ](7)Suppose that the response to be scored is run fromthe cops.
In practical terms, Eq.
7 means that for our479example, we ?light up?
the nodes in the graph cor-responding to run, from, the and cops by assigningsome initial probability, and the graph is then ?run?using the transition matrix C according to Eq.
7.
Inthis study, the initial node probabilities are set to val-ues proportional to the idf values of the correspond-ing term, so that P (di) = idf(di)P idf(di) .
After m steps,the probabilities at the nodes for each term in thereference definition R are read off, and their log-arithms summed.
Similar to an AND calculation,we calculate a product of sums over the graph, sothat responses reflecting multiple aspects of the tar-get definition are rewarded more highly than a verystrong prediction for only a single definition term.4 EvaluationWe first describe our corpus of gold standard humanjudgments.
We then explain the different text sim-ilarity methods and baselines we computed on thecorpus responses.
Finally, we give an analysis anddiscussion of the results.4.1 CorpusWe obtained a set of 734 responses to definition pro-duction tests from a word learning experiment at theUniversity of Pittsburgh (Bolger et al, 2006).
Intotal, 72 target words, selected by the same group,were used in the experiment.
In this experiment,subjects were asked to learn the meaning of targetwords after seeing them used in a series of contextsentences.
We set aside 70 responses for training,leaving 664 responses in the final test dataset.Each response instance was coded using the scaleshown in Table 1, and a sample set of subject re-sponses and scores is shown in Table 2.
The targetword was treated as having several key aspects ofmeaning.
The coders were instructed to judge a re-sponse according to how well it covered the variousaspects of the target definition.
If the response cov-ered all aspects of the target definition, but also in-cluded extra irrelevant information, this was treatedas a partial match at the discretion of the coders.We obtained three codings of the final dataset.The first two codings were obtained using an in-dependent group, the QDAP Center at the Univer-sity of Pittsburgh.
Initially, five human coders, withvarying degrees of general coding experience, wereScore Meaning0 Completely wrong1 Some partial aspect is correct2 One major aspect, or more than oneminor aspect, is correct3 Covers all aspects correctlyTable 1: Scale for human definition judgements.Response HumanScoredepart secretly 3quietly make away, escape 3to flee, run away 2flee 2to get away with 1to steal or take 0Table 2: Examples of human scores of responses forthe target word abscond.trained by the authors using one set of 10 exampleinstances and two training sessions of 30 instanceseach.
Between the two training sessions, one of theauthors met with the coders to discuss the ratingsand refine the rating guidelines.
After training, theauthors selected the two coders who had the bestinter-coder agreement on the 60 training instances.These two coders then labeled the final test set of664 instances.
Our third coding was obtained froman initial coding created by an expert in the Univer-sity of Pittsburgh Psychology department and thenadjusted by one of the authors to resolve a smallnumber of internal inconsistencies, such as when thesame response to the same target had been given adifferent score.Inter-coder agreement was measured using lin-ear weighted kappa, a standard technique for or-dinal scales.
Weighted kappa scores for all threecoder pairs are shown in Table 3.
Overall, agree-ment ranged from moderate (0.64) to good (0.72).4.2 Baseline MethodsWe computed three baselines as reference points forlower and upper performance bounds.Random.
The response items were assigned la-bels randomly.480Coder pair WeightedKappa1, 2 0.682, 3 0.641, 3 0.72Table 3: Weighted kappa inter-rater reliability forthree human coders on our definition responsedataset (664 items).Method Spearman RankCorrelationRandom 0.3661Cosine 0.4731LSA 0.4868Markov 0.6111LSA + Markov 0.6365Human 0.8744Table 4: Ability of methods to match human rankingof responses, as measured by Spearman rank corre-lation (corrected for ties).Human choice of label.
We include a methodthat, given an item and a human label from one of thecoders, simply returns a label of the same item froma different coder, with results repeated and averagedover all coders.
This gives an indication of an upperbound based on human performance.Cosine similarity using tf.idf weighting.
Cosinesimilarity is a widely-used text similarity methodfor tasks where the passages being compared of-ten have significant direct word overlap.
We repre-sent response items and reference definitions as vec-tors of terms using tf.idf weighting, a standard tech-nique from information retrieval (Salton and Buck-ley, 1997) that combines term frequency (tf) withterm specificity (idf).
A good summary of argumentsfor using idf can be found in (Robertson, 2004).
Tocompute idf, we used frequencies from a standard100-million-word corpus of written and spoken En-glish 1.
We included a minimal semantic similar-ity component by applying Porter stemming (Porter,1980) on terms.1The British National Corpus (Burnage and Dunlop, 1992),using American spelling conversion.4.3 MethodsIn addition to the baseline methods, we also ran thefollowing three algorithms over the responses.Markov chains (?Markov?).
This is the methoddescribed in Section 3.
A maximum of 5 randomwalk steps were used, with a walk continuationprobability of 0.8.
Each walk step used a mixture ofsynonym, stem, co-occurrence, and free-associationlinks.
The link weights were trained on a small setof held-out data.Latent Semantic Analysis (LSA).
LSA (Lan-dauer et al, 1998) is a corpus-based unsupervisedtechnique that uses dimensionality reduction to clus-ter terms according to multi-order co-occurrence re-lations.
In these experiments, we obtained LSA-based similarity scores between responses and targetdefinitions using the software running on the Univer-sity of Colorado LSA Web site (LSA site, 2006).
Weused the pairwise text passage comparison facility,using the maximum 300 latent factors and a generalEnglish corpus (Grade 1 ?
first-year college).Although LSA and the Markov chain approachare based on different principles, we chose to ap-ply LSA to this new response-scoring task and cor-pus because LSA has been widely used as a text se-mantic similarity measure for other tasks and showngood performance (Foltz et al, 1998).LSA+Markov.
To test the effectiveness of com-bining two different ?
and possibly complemen-tary ?
approaches to response scoring, we createda normalized, weighted linear combination of theLSA and Markov scores, with the model combina-tion weight being derived from cross-validation on aheld-out dataset.4.4 ResultsWe measured the effectiveness of each scoringmethod from two perspectives: ranking quality, andlabel accuracy.First, we measured how well each scoring methodwas able to rank response items by similarity to thetarget definition.
To do this, we calculated the Spear-man Rank Correlation (corrected for ties) betweenthe ranking based on the scoring method and theranking based on the human-assigned scores, aver-aged over all sets of target word responses.Table 4 summarizes the ranking results.
For481Method Label error (RMS)Top 1 Top 3Random 1.4954 1.6643Cosine 0.8194 1.0540LSA 0.8009 0.9965Markov 0.7222 0.7968LSA + Markov 1.1111 1.0650Human 0.1944 0.4167Table 5: Root mean squared error (RMSE) of la-bel(s) for top-ranked item, and top-three items forall 77 words in the dataset.overall quality of ranking, the Markov method hadsignificantly better performance than the other au-tomated methods (p < 2.38e?5).
LSA gave asmall, but not significant, improvement in overallrank quality over the cosine baseline.
2 The sim-ple combination of LSA and Markov resulted in aslightly higher but statistically insignificant differ-ence (p < 0.253).Second, we examined the ability of each methodto find the most accurate responses ?
that is, the re-sponses with the highest human label on average ?for a given target word.
To do this, we calculated theRoot Mean Squared Error (RMSE) of the label as-signed to the top item, and the top three items.
Theresults are shown in Table 5.
For top-item detec-tion, our Markov model had the lowest RMS error(0.7222) of the automated methods, but the differ-ences from Cosine and LSA were not statisticallysignificant, while differences for all three from Ran-dom and Human baselines were significant.
Forthe top three items, the difference between Markov(0.7968) and LSA (0.9965) was significant at thep < 0.03 level.Comparing the overall rank accuracy with top-item accuracy, the combined LSA + Markov methodwas significantly worse at finding the three best-quality responses (RMSE of 1.0650) than Markov(0.7968) or LSA (0.9965) alone.
The reasons forthis require further study.2All statistical significance results reported here used theWilcoxon Signed-Ranks test.5 DiscussionEven though definition scoring may seem morestraightforward than other automated learning as-sessment problems, human performance was stillsignificantly above the best automated methods inour study, for both ranking and label accuracy.
Thereare certain additions to our model which seem likelyto result in further improvement.One of the most important is the ability to identifyphrases or colloquial expressions.
Given the shortlength of a response, these seem critical to handleproperly.
For example, to get away with somethingis commonly understood to mean secretly guilty, nota physical action.
Yet the near-identical phrase toget away from something means something very dif-ferent when phrases and idioms are considered.Despite the gap between human and automatedperformance, the current level of accuracy of theMarkov chain approach has already led to somepromising early results in word learning research.For example, in a separate study of incrementalword learning (Frishkoff et al, 2006), we used ourmeasure to track increments in word knowledgeacross multiple trials.
Each trial consisted of a sin-gle passage that was either supportive ?
containingclues to the meaning of unfamiliar words ?
or notsupportive.
In this separate study, broad learning ef-fects identified by our measure were consistent witheffects found using manually-scored pre- and post-tests.
Our automated method also revealed a pre-viously unknown interaction between trial spacing,the proportion of supportive contexts per word, andreader skill.In future applications, we envision using our auto-mated measure to allow a form of feedback for intel-ligent language tutors, so that the system can auto-matically adapt its behavior based on the student?stest responses.
With some adjustments, the samescoring model described in this study may also beapplied to the problem of finding supportive contextsfor students.6 ConclusionsWe presented results for both automated and hu-man performance of an important task for languagelearning applications: scoring definition responses.We described a probabilistic model of text seman-482tic similarity that uses Markov chains on a graph ofterm relations to perform a kind of semantic smooth-ing.
This model incorporated both corpus-based andknowledge-based resources to compute text seman-tic similarity.
We measured the effectiveness of bothour method and LSA compared to cosine and ran-dom baselines, using a new corpus of human judg-ments on definition responses from a language learn-ing experiment.
Our method outperformed the tf.idfcosine similarity baseline in ranking quality and inability to find high-scoring definitions.
BecauseLSA and our Markov chain method are based ondifferent approaches and resources, it is difficult todraw definitive conclusions about performance dif-ferences between the two methods.Looking beyond definition scoring, we believe au-tomated methods for assessing word learning havegreat potential as a new scientific tool for languagelearning researchers, and as a key component of in-telligent tutoring systems that can adapt to students.AcknowledgementsWe thank D.J.
Bolger and C. Perfetti for the useof their definition response data, Stuart Shulmanfor his guidance of the human coding effort, andthe anonymous reviewers for their comments.
Thiswork supported by U.S. Dept.
of Education grantR305G03123.
Any opinions, findings, and conclu-sions or recommendations expressed in this materialare the authors?
and do not necessarily reflect thoseof the sponsors.ReferencesD.J.
Bolger, M. Balass, E. Landen and C.A.
Perfetti.2006.
Contextual Variation and Definitions in Learn-ing the Meanings of Words.
(In press.)G.
Burnage and D. Dunlop.
1992.
Encoding the BritishNational Corpus.
English Language Corpora: De-sign, Analysis and Exploitation.
The 13th Intl.
Conf.on Engl.
Lang.
Res.
in Computerized Corpora.
Ni-jmegen.
J. Aarts, P. de Haan, N. Oostdijk, Eds.J.
Burstein, S. Wolff, and L. Chi.
1999.
Using Lexi-cal Semantic Techniques to Classify Free-Responses.Breadth and Depth of Semantic Lexicons.
KluwerAcad.
Press, p. 1?18.G.
Cao, J-Y.
Nie, and J. Bai.
Integrating Word Relation-ships into Language Models.
SIGIR 2005, 298?305.P.W.
Foltz, W. Kintsch, and T. Landauer.
1998.
An Intro-duction to Latent Semantic Analysis.
Discourse Pro-cesses, 25(2):285?307.G.
Frishkoff, K. Collins-Thompson, J. Callan, and C.Perfetti.
2007.
The Nature of Incremental WordLearning: Context Quality, Spacing Effects, and SkillDifferences in Meaning Acquisition Across MultipleContexts.
(In preparation.)M.
Heilman, K. Collins-Thompson, J. Callan and M. Es-kanazi.
2006.
Classroom Success of an Intelligent Tu-toring System for Lexical Practice and Reading Com-prehension.
ICSLP 2006.S.
Jayaraman and A. Lavie.
Multi-Engine Ma-chine Translation Guided by Explicit Word Matching.EAMT 2005.T.K.
Landauer, P.W.
Foltz, and D. Laham.
1998.
AnIntroduction to Latent Semantic Analysis.
DiscourseProcesses, 25:259?284.LSA Web Site.
http://lsa.colorado.eduI.
Mani and M.T.
Maybury (Eds.)
1999.
Advances inAutomatic Text Summarization.
MIT Press.R.
Mihalcea, C. Corley, and C. Strapparava.
2006.Corpus-based and Knowledge-based Measures of TextSemantic Similarity.
AAAI 2006G.
Miller.
1998.
WordNet: A Lexical Database for En-glish.
Communications of the ACM, 38(11) 39?41.D.L.
Nelson, C.L.
McEvoy, and T.A.
Schreiber.1998.
The University of South Florida wordassociation, rhyme, and word fragment norms.http://www.usf.edu/FreeAssociation/M.
Porter.
1980.
An Algorithm for Suffix-stripping.
Program, 14(3) 130?137.http://www.tartarus.org/martin/PorterStemmerM.
Quillian.
1967.
Word Concepts: A Theory and Sim-ulation of some Basic Semantic Capabilities.
Behav.Sci., 12: 410?430.S.
Robertson.
2004.
Understanding Inverse DocumentFrequency: on Theoretical Arguments for IDF.
J. ofDocumentation, 60:503?520.G.
Salton and C. Buckley.
1997.
Term Weighting Ap-proaches in Automatic Text Retrieval.
Reading in In-formation Retrieval.
Morgan Kaufmann.G.
Salton and M. Lesk.
1971.
Computer Evaluationof Indexing and Text Processing.
Prentice-Hall.
143?
180.K.
Toutanova, C.D.
Manning, and A.Y.
Ng.
2004.
Learn-ing Random Walk Models for Inducing Word Depen-dency Distributions.
ICML 2004.S.
Valenti, F. Neri, and A. Cucchiarelli.
2003.
AnOverview of Current Research on Automated EssayGrading.
J. of Info.
Tech.
Ed., Vol.
2.483
