Training and Evaluation of aSpoken Language Understanding SystemDeborah A.. Dahl, Lynette Hirschman, Lewis M. Norton,Marcia C. Linebarger, David Magerman,Nghi Nguyen and Catherine N. BallUnisys Defense SystemsCenter for Advanced Information TechnologyPO Box 517Paoli, PA 19301IntroductionThis paper describes our results on a spoken languageapplication for finding directions.
The spoken languagesystem consists of the MIT SUMMIT speech recognitionsystem (\[20\]) loosely coupled to the UNISYS PUNDITlanguage understanding system (\[9\]) with SUMMIT pro-viding the top N candidates (based on acoustic score) tothe PUNDIT  system.
The direction finding capability isprovided by an expert system which is also part of theMIT VOYAGER system \[18\]).
1One major goal in this research has been to under-stand issues of training vs. coverage in porting a lan-guage understanding system to a new domain.
Specifi-cally, we wished to determine how much data it takes totrain a spoken language system to a given level of per-formance for a new domain.
We can use the answer tothis question in the process of designing data collectiontasks to decide how much data to collect.
We address arelated question, that is, how to quantify the growth ofa system as a function of training, in \[12\].To explore the relationship of training to coverage,we have developed a methodology to measure coverageof unseen material as a function of training material.Using successive batches of new material, we assessedcoverage on a batch of unseen material, then trained onthis material until we reached a certain level of coverage,then repeated the experiment on a new batch of material.The system coverage seemed to level off at about 70%coverage of unseen data after 1000 sentences of trainingdata.A second goal was to develop a methodology for auto-matically tuning a broad-coverage rammar to a newapplication domain.
This approach avoids repeatingdomain independent grammar development work overagain for each new domain.
To do this we developed amethod for deriving a minimal grammar and a minimal1This work was supported by DARPA contract N000014-89-C0171, administered by the Otl'ice of Naval Research.
We aregrateful to Victor Zue of MIT for mak ing the SUMMIT and VOY-AGER systems available to us, and for providing us with thedirection-finding data.
We also wish to thank  Mitch Marcus ofthe University of Pennsylvania for making the services of outsideevaluators available to us, and Beatrice Santorini for coordinatingthe evaluation.
Bill Scholz of the Unisys Center for Advanced I.n-formation Technology provided valuable assistance in the designand analysis of the black box evaluation task.lexicon from a corpus of training material.
The applica-tion of this technique to the DIRECTION-FINDING corpuswill be described in this paper.
We then compared thecoverage and performance of the minimal grammar andlexicon on a test set, and found that a two-fold decreasein parse time was achieved with only a small loss of cov-erage.Our second major focus was on evaluation of spe-cific algorithms, using the natural anguage system as atestbed.
In particular, we compared the performance oftwo algorithms for reference resolution processing.
Fi-nally, our third major focus has been to evaluate theoverall coverage and accuracy of the entire spoken lan-guage system.
We did this using two test corpora col-lected at MIT (\[17\]), containing a total of 1015 utter-ances.
The system was evaluated on the basis of the firstutterance of the N-best output of SUMMIT accepted byPUNDIT, or the first candidate of the N-best if no utter-ance was accepted by PUNDIT.
This paper reports resultsfor word accuracy, sentence accuracy, application accu-racy (generating an answer judged reasonable by naiveevaluators), and finally false alarm rate (incorrect, inco-herent or incomplete answers).System OverviewThe VOYAGER system has been described in detail else-where, (\[18\]) so we will only briefly describe it here.VOYAQER is a spoken language system for finding di-rections in Cambridge, Massachusetts.
For example theuser can ask questions about the locations of objects uchas restaurants, universities, and hotels and distances be-tween them.
It provides output in the form of a map dis-play as well as natural anguage.
We have used the SUM-MIT speech recognition system as well as the direction-finding expert system from VOYACER in the system weare reporting on.The architecture of the system which we report onhere has also been largely described elsewhere (\[1\]), withthe exception of the N-best processing, and so will onlybe summarized here.
There are five major componentsof the system, the speech recognition system (SUMMIT) ,the dialog manager (VFE), the PUNDIT natural anguageprocessing system, the module which formats PUNDIT 's212output for the direction finder (QTIP), and the directionfinder itself.
VFE takes SUMMIT'S N-best output (com-puted using a word-pair grammar of perplexity 60), andsends it to PUNDIT for syntactic and semantic analysis.The first candidate which PUNDIT accepts is sent to qTIP,where it is formatted and sent to the direction finder.When the direction finder's English response is returnedto VFE, it is sent to PUNDIT as well, so that the informa-tion from the direction finder's response can be processedand incorporated into the discourse context.
This fea-ture allows the user to refer to things that the directionfinder has mentioned, and in general allows the user andthe expert system to engage in a dialog.
The PUNDITnatural anguage processing system (\[9\])is mplementedin Prolog and consists of a top-down backtracking parser(\[10\]), a semantic interpreter (\[13\]), and a pragmaticscomponent (\[5\]).
The system uses a semantic-net basedknowledge representation system (\[6\]).TrainingTraining Data and CoverageIn order to measure the effect of training on coverage,we developed a standardized training technique.
We be-gan by training the system on a set of 176 developmentsentences to a level of 96% percent apphcation accuracy.We then ran a batch of 300 previously unseen sentencesthrough the system and measured accuracy.
This wasfollowed by a development stage where we trained thesystem to about 80% accuracy.
Then the system wasgiven a new set of 300 unseen sentences.
This cycle oftesting and development was repeated for approximately1000 sentences, and we observed a leveling off of theperformance of the system on unseen data at approx-imately 70%.
The growth of coverage is illustrated inFigure 1.
The cold run coverage is coverage measuredon each batch of sentences before any development hadtaken place.
Development coverage was the coverage af-ter the system was developed for that batch of sentences,and final coverage was the increased coverage that wasachieved after development on later batches of sentences.Grammar  Prun ing  Exper imentsUse of tight syntactic and semantic onstraints i an im-portant source of constraint in a spoken language un-derstanding system.
There are two approaches to con-structing a tight grammar for a given corpus of train-ing material.
One approach is to build the grammarincrementally, based on the observed training data, asin TINA (\[14\]).
This approach has the disadvantage ofconstructing a basic grammar of English over again foreach domain.
The other approach is to prune a generalgrammar of English to cover only those constructionsseen in the training data.
This approach as the advan-tage of making available a 'l ibrary' of constructions (thatis, the full grammar) which can easily be added to thesystem when additional data indicates a need for them.In both cases, the coverage of the grammar will directlyreflect the amount of training data seen.We have developed a technique for pruning our gen-eral English grammar, based on supervised training.
Wemake use of the fact that PUNDIT provides a detailedparse tree, reflecting the set of BNF definitions used inparsing the sentence.
The parse tree also contains eachword labeled by part of speech.
Given a corpus of train-ing sentences with their correct parses, a program canidentify those constructions used to obtain that parseand can extract the associated rules from the generalgrammar.
Similarly, it can identify how each word is ac-tually used in the training data and extract a minimallexical definition reflecting the word's usage in context.Using these techniques, we performed a small set ofexperiments on the effects of pruning both the grammarand the lexicon.
There are several ways to analyze howpruning affects overall system behavior.
We can lookfor reduction in perplexity; however, given the heavilycontext-dependent ature of our grammar, the effectsof pruning seemed quite small (10-15% reduction).
Wealso looked at the effect of pruning on overall system per-formance.
Given a grammar based on some 500 train-ing sentences, we observed a two-fold speed-up when thesame sentences were run using the pruned grammar andlexicon (reducing the average time to correct parse from4.2 sec to 2.1 seconds, on a Sun 3-60).
We also lookedat the relation between coverage and amount of train-ing data.
Our tests indicated that we did not lose muchcoverage by pruning the grammar (-3 % after trainingon 226 sentences and -2 % after training on 526 sen-tences).
We lost more coverage from pruning the lexicon(-33 % after training on 226 sentences, and -7 % aftertraining on 526 sentences), but this was largely to the"unknown" word problem, more than to pruning awayneeded meanings.PUNDIT Coverage of Voyager Datam ................ mI iiiiiiiiiii!iiii!A B C D EB~tchFigure 1: Coverage ofvoYAoEP~ data on 5 successive setsof 300 sentences213These experiments, though limited, indicate thatgrammar and lexicon pruning may offer significant re-ductions in processing time with only small losses incoverage.
Furthermore, although we have not yet exper-imented with the pruned grammar on spoken input, weexpect that the pruned grammar will improve the abil-ity of PUNDIT to reject ungrammatical candidates froma speech recognizer.Evaluat ionWe have explored several new approaches to evaluation,including using the natural language system as a toolfor comparing specific algorithms, and subjective blackbox evaluation of accuracy as well as standard word andsentence accuracy measurements.Reference  Reso lu t ion  Algor i thmsUsing the language understanding system to comparealgorithms gives a very tightly controlled comparison oftwo or more specific approaches to a problem, and isappropriate when the algorithms of interest are modular,as they are in this case.We investigated the performance of two reference res-olution algorithms, using PUNDIT as a testbed.
The dataconsisted of a set of 68 discourses (the set of discourseswith inter-sentential naphoric references) taken fromthe 1000 direction finding utterances which had beenprocessed by PUNDIT.
The reference resolution algo-rithms were both variations of the focusing/centering ap-proach (\[7\],\[8\], \[15\])).
We compared object and subjectpreference.
Object preference means that the direct ob-ject from the previous entence is preferred over the sub-ject as the referent of a pronoun in the current sentence.Subject preference means that the subject is preferred.Both approaches have been advocated in the literature,(\[2\],\[3\],\[11\]) but have not been tested using naturally oc-curring data.
We ran the set of 68 discourses and tabu-lated accuracy of reference resolution for each pronoun.No significant difference in accuracy was found.
Accu-racy was near 100 % in both cases.
The amount of timespent by the system performing reference resolution wasalso measured and was found not to differ significantlyin each condition.
These findings suggest that both algo-rithms perform equally well in this domain.
This reflectsthe fact that there are very few instances where both asubject and an object are competing candidates for a ref-erent in this data.
Additional details of this investigationare reported in \[4\].Sys tem Eva luat ionWe used a subjective black box measure and word andsentence accuracy to evaluate the the system as a whole.Subjective black box evaluation allows us to evaluateperformance on queries which have an unspecified num-ber of acceptable answers.
For example, we can evaluatevague queries, which require a clarification dialog to elicit214more information before they can be submitted to theapplication back end.
We can also ask questions of hu-man judges that cannot be asked of an automatic evalu-ation technique, such as whether an answer was partiallycorrect, or whether an error message was helpful or not.Although there is always a question of reliability whenhuman judges are used, when judges are provided withclear and explicit instructions, human judgements canbe quite reliable.MethodologyThe system was evaluated at the level of word, sentenceand application accuracy.
Word and sentence level accu-racy were measured using the NIST evaluation software.In order to evaluate the system at the application level,we designed a black box evaluation task, using humanevaluators to score each interchange of a dialog betweenthe system and a user.
The evaluators were five stu-dents at the University of Pennsylvania and one Unisysemployee who was not a system developer.This evaluation task was similar to one reported byMIT (\[19\]) in that the evaluators were asked to cate-gorize both the queries and the responses.
The querieswere categorized as to whether they were appropriateor inappropriate, given the capabilities of the applica-tion.
They were also categorized on a three point scaleof clarity, where 1 represents a clear and fluent query, 2a partially garbled or badly stated query, and 3 repre-sents a query that is partially or entirely uninterpretable.The responses were categorized first as to whether theywere answers or error messages.
The answers were thensubdivided into 'correct', 'partially correct', and 'incor-rect or misleading'.
The error messages were categorizedas either 'helpful', 'unhelpful', or 'incorrect or mislead-ing'.
The logs from Test Set 1, containing the originalorthographic transcription of the query plus the system'sresponse, were scored by three judges.
Due to time con-straints, the logs of Test Set 2, also containing the refer-ence utterance and the system's response, were scored byonly one judge.
The logs were presented to the judges viaan interactive program which displays a query/responseinterchange (including intermediate clarification dialog)and elicits responses for each category.RestdtsWord accuracy with PUNDIT as a filter was computed onthe basis of the first candidate accepted by the syntac-tic and semantic omponents of PUNDIT, or if no candi-date was accepted, on the first candidate.
Word accu-racy without PUNDIT was computed on the basis of thefirst candidate of the N-best (i.e., the candidate with thehighest acoustic score).Table 1 shows the results on Test Set 1 for word, sen-tence, and application accuracy.
Table 2 displays theresults for Test Set 2.
For the purposes of applicationaccuracy, 'correct response' means either a correct an-swer or a helpful error message providing a meaningfuldiagnosis of a query falling outside the system.
'FalseW/ PUNDIT as filterW/O PUNDIT as filterWord Accuracy SentenceCorrect Error Accuracy80.0 26.176.4 31.1Application AccuracyCorrect False Alarm34.3 45.7 10.720.6 22.0 9.3Table 1: Word, sentence and application accuracy with and without PUNDIT filter for Test Set 1 (11 speakers, 519query/response pairs) (PUNDIT interpreting the utterance in all cases)alarms' include partially correct responses, incorrect re-sponses, and incorrect error messages.
The natural lan-guage component in our current system plays two roles;that is, as filter for the speech recognizer and as inter-preter of recognized utterances.
For this reason, we havedistinguished these roles in the tables.
Using PUNDITas a filter means that the utterance recognized by thespoken language system is the candidate accepted byPUNDIT.
Not using PUNDIT as a filter means that theutterance recognized by the spoken language system isthe first candidate of the N-best.
Using PUNDIT to in-terpret he utterance means that the candidate selectedfrom the N-best (by whatever means) is sent to PUNDITfor interpretation and translation into function calls tothe direction finding expert system.Analys isUsing human judges for the black box evaluation requiresassessing their reliability.
Judges must be able to agreeon their classifications or this approach will not be useful.We measured the reliability of the judges in the blackbox evaluation by using an analysis of variance techniquedescribed in \[16\].
For the judgement of most interest,that is, whether the answer was correct, partially correct,or incorrect, the mean reliability averaged over speakerswas .78 with a standard eviation of .09 for the Test Set1.
Since we had only one judge for Test Set 2, we do nothave reliability measurements for that data.This reliability score can be interpreted as saying thatif a new set of judges did these tasks we would expectthe correlation between the new judgements and the oldjudgments to have this value.
The high reliability ofthe correctness judgement is not surprising, since this isa fairly objective judgement.
The other, more subjec-tive, judgements which we asked the evaluators to makewere less reliable than correctness.
For example, thereliability of the fluency judgements on the test datawas only .34.
We believe that this could be improvedthrough more explicit instructions and some additionaltraining for the evaluators, although it may be that thefluency judgement is of only marginal interest anyway.Disagreements among the judges were mediated by anexpert judge who was familiar with the evaluation task,but was not a system developer.Tables 1 and 2 show that, while PUNDIT as a filter didnot provide a large improvement in word accuracy, it didprovide a fairly large improvement in sentence accuracy,and it roughly doubled application accuracy.
Applica-tion accuracy went from 22.0 to 45.7 percent for TestSet 1 and from 28.0 to 51.6 percent for Test Set 2.
Thisimprovement results from PUNDIT 'S  ability to reject un-interpretable candidates in the set of N-best candidates,so that only meaningful candidates are considered forinterpretation.
It is also interesting to note that wordaccuracy and application accuracy do not covary com-pletely.
There are two reasons for this.
First, it is possi-ble for a candidate that is semantically equivalent to thereference answer to be accepted by the natural anguagesystem and for the answer to be judged correct.
For ex-ample, this would be the case if the reference utterancewas How do I get to the nearest bank?
but the naturallanguage system accepted How would I get to the nearestbank?.
The upshot of this situation is a correct score forapplication accuracy but a lower score for word accuracy.On the other hand, it is possible for the reference can-didate to be accepted by the natural language system,but to then be misunderstood and consequently give anincorrect response.
This means that word accuracy isgood even though application accuracy is bad.Reference Referencein N-best not in N-best OverallPundit right 76.0 91.9 83.6Pundit wrong 24.0 9.1 17.4Table 3: PUNDIT's performance on Test Set 1 (11 speak-ers, 519 query/response pairs), depending on whether ornot reference query occurred in N-best (N=40).Reference Referencein N-best not in N-best OverallPundit right 83.5 86.1 84.7Pundit wrong 16.5 13.9 16.3Table 4: PUNDIT's performance on Test Set 2 (10 speak-ers, 496 query/response pairs), depending on whether ornot reference query occurred in N-best (N=100).Over and above these summary scores, we wished toinvestigate the performance of PUNDIT  depending on thestate of affairs in the N-best.
For example, we looked atwhat PUNDIT did when the reference query occurrs inthe N-best and when it does not.
If the reference query215W/ PUNDIT as filterW/O PUNDIT as filterWord Accuracy SentenceCorrect Error Accuracy77.5 29.1 33.774.1 33.4 20.8Application AccuracyCorrect False Alarm51.6 9.328.0 2.2Table 2: Word, sentence and application accuracy with and without PUNDIT filter for Test Set 2 (10 speakers, 496query/response pairs), PUNDIT interpreting the utterance in all cases)occurs in the N-best then the right thing for the naturallanguage system to do is to find it, or find a semanticallyequivalent candidate, and give a correct answer.
On theother hand, if the reference query is not in the N-best,then the right thing to do is either to find a semanticallyequivalent candidate and give a correct answer, or toreject all candidates.
Table 3 shows how often PUNDITdid the right thing, by these criteria, for Test Set 1 withN=40, and Table 4 shows similar results for Test Set 2with N=100.We were also interested in looking at the performanceof the system as a function of the location of the refer-ence answer in the N-best.
This bears on the question ofwhat the optimal setting is for N. Intuitively, looking atmore candidates increases the probability that the ref-erence utterance or a semantic equivalent will be found,but at the same time it increases the probability of afalse alarm, with the natural anguage system finding anacceptable candidate that differs semantically from thereference utterance in crucial ways.
If we could quantifythe relationships among N, the rate of correct responses,and the false alarm rate, it would give us a technique forsetting N for optimal accuracy of the spoken languagesystem, given a particular speech recognizer and a par-ticular language understanding component.
2 In Figure2, we show the cumulative correct answers and the cu-mulative false alarms as a function of the location of thereference utterance in the direction finding data fromTest Set 1 and Test Set 2.
In order to determine theoptimal setting of N for the SUMMIT/PUNDIT system, welooked at the difference between the cumulative correctresponses and false alarms as a function of the locationof the reference utterance in the N-best.
Figure 3 showsthis difference, assuming that correct responses and falsealarms have an equal weighting.
Obviously they do nothave an equal weighting in general.
In fact, in mostapplications false alarms should probably be heavily pe-nalized.
If we weight the cost of a false alarm at threetimes the benefit of a correct response, then the optimalperformance of this system is obtained with N = 10 forTest Set 1 and N = 11 for Test Set 2.
That is, if theanswer is not found in the top 10-11 candidates, the costof false alarms exceeds the benefit of increased correctresponses.Conc lus ionsThis paper has described several approaches to trainingand evaluation of spoken language systems.
In the areaof training we described an approach to measuring theperformance of the system on previously unseen data asincreasing amounts of training data are used.
This ex-periment demonstrated that a level of 70 % coverage ofunseen data was reached after the system was trainedon 1000 sentences.
We also described how a general,broad coverage grammar and lexicon can be automati-cally pruned to fit a specific application, thus saving theeffort involved in building grammars from scratch foreach application.We also described several approaches to evaluation.We used the system as a tool to evaluate alternativealgorithms for reference resolution, and we also evalu-ated the entire system on word, sentence, and applica-tion accuracy.
Application accuracy was evaluated usinga black box technique with evaluators who were not sys-tem developers.
We found that the evaluators used inthis task were relatively reliable, and we expect that im-provements in training and instructions would improvethe reliability.
Overall application accuracy for the testdata was 51.6 %.
Separating the performance of  PUNDITfrom that of the entire spoken language system, we foundthat PUNDIT did the "right thing" 84.7 % of the time forthe test data.
Finally we demonstrated a new techniquefor determining the optimal setting of N for the N-bestoutput from the speech recognizer in a loosely coupledsystem, for a given speech recognizer, language under-standing system, and application.References\[1\] Catherine N. Ball, Deborah Dahl, Lewis M. Nor-ton, Lynette Hirschman, Carl Weir, and MarciaLinebarger.
Answers and questions: Processingmessages and queries.
In Proceedings ofthe DARPASpeech and Natural Language Workshop, Cape Cod,MA, October 1989.\[2\]2 This leaves out another  important  component  off opt imal N, ofcourse, which is t ime, since t~ larger N would normM.ly be expected \[3\]to result  in increased processing time.216Susan E. Brennan, Marilyn W. Friedman, andCarl J. Pollard.
A centering approach to pronouns.In Proceedings of the 25th Annual Meeting of theAssociation for Computational Linguistics, pages155-162, Stanford, CA, 1987.Deborah A. Dahl.
Focusing and reference resolu-tion in PUNDIT.
In Proceedings of the 5th NationalOI~ ?.oc2"O OE~0_ OTest Set 2I I I I I I0 20 40 60 80 100Location of Reference Answer in N-bestFigure 2: Percent right and percent false alarm vs. location of reference query in N-best, for Test Set 1 and Test Set2Conference on Artificial Intelligence, Philadelphia, \[9\]PA, August 1986.\[4\] Deborah A. Dahl.
Evaluation ofpragmatics process-ing in a direction finding domain.
In Proceedings ofthe FiSh Rocky Mountain Conference on ArtificialIntelligence, Las Cruces, New Mexico, 1990.\[10\]\[5\] Deborah A. Dahl and Catherine N. Ball.
Refer-ence resolution in PUNDIT.
In P. Saint-Dizier andS.
Szpakowicz, editors, Logic and logic grammarsfor language processing.
Elfis I-Iorwood Limited, inpress.\[6\] Michael Freeman, Lynette Hirschman, Donald \[11\]McKay, and Martha Palmer.
KNET: A logic-basedassociative network framework for expert systems.Technical Memo 12, SDC--A Burroughs Company,P.O.
Box 517, Paoli, PA 19301, September 1983.
\[12\]\[7\] Barbara Grosz, Aravind K. Joshi, and Scott Wein-stein.
Providing a unified account of definite nounphrases in discourse.
Proceedings of the ~lst An-nual Meeting of the Association for ComputationalLinguistics, pages 44-50, 1983.\[8\] Barbara J. Grosz, Aravind K. Joshi, and Scott We- \[13\]instein.
Towards a computational theory of dis-course interpretation, unpubhshed mss., 1986.217Lynette Hirschman, Martha Palmer, John Dowd-ing, Deborah Dahl, Marcia Linebarger, RebeccaPassonneau, Frangois-Michel Lang, Catherine Ball,and Carl Weir.
The PUNDIT natural-language pro-cessing system.
In AI Systems in Government Con-ference.
Computer Society of the IEEE, March 1989.Lynette Hirschman and Karl Puder.
Restrictiongrammar in prolog.
In Proceedings of the First In-ternational Logic Programming Conference, pages85-90.
Association pour la Diffusion et le Devel-oppement de Prolog, Marseilles, 1982.Megumi Kameyama.
Zero Anaphova: The Case ofJapanese.
PhD thesis, Stanford University, Stan-ford, CA, 1985.Lewis M. Norton, Deborah A. Dahl, Donald P.McKay, Lynette Hirschman, Marcia C. Linebarger,David Magerman, and Catherine N. Ball.
Manage-ment and evaluation of interactive dialog in the airtravel domain.
In Proceedings of the Darpa Speechand Language Workshop, Hidden Valley, PA, June1990.Martha Palmer.
Semantic Processing fov Finite Do-mains.
Cambridge University Press, Cambridge,England, 1990.OCDOi..DO(n OO 03 oOO,.IO~r~OTest Set 1* Test Set 2I i I I I I0 20 40 60 80 100Location of Reference Answer in N-bestFigure 3: Percent right - percent false alarm vs. location of reference query in N-best, for Test Set 1 and Test Set 2\[14\] Stephanie Seneff.
Tina: a probabilistic syntacticparser for speech understanding systems.
In Pro-ceedings of the First DARPA Speech and NaturalLanguage Workshop, Philadelphia, PA, February1989.\[15\] C.L.
Sidner.
Towards acomputational theory of def-inite anaphora comprehension in English discourse.PhD thesis, MIT, 1979.\[16\] B. J. Winer.
Statistical Principles in EzperimentalDesign.
McGraw-Hill Book Company, New York,1971.\[17\] Victor Zue, Nancy Daly, James Glass, David Good-ine, Hong Leung, Michael Phillips, Joseph Polifroni,Stephanie Seneff, and Michal Soclof.
The collectionand preliminary analysis of a spontaneous speechdatabase.
In Proceedings of the DARPA Speech andNatural Language Workshop, Cape Cod, MA, Oc-tober 1989.\[18\] Victor Zue, James Glass, David Goodine, HongLeung, Michael Phillips, Joseph Polifroni, andStephanie Seneff.
The VOYAGER speech understand-ing system: A progress report.
In Proceedings oftheDARPA Speech and Natural Language Workshop,Cape Cod, MA, October 1989.\[19\] Victor Zue, James Glass, David Goodine, HongLeung, Michael Phillips, Joseph Polifroni, and\[20\]218Stephanie Seneff.
Preliminary evaluation of the voy-ager spoken language system.
In Proceedings of theDARPA Speech and Natural Language Workshop,Cape Cod, MA, October 1989.Victor Zue, James Glass, Michael Phillips, andStephanie Seneff.
The MIT SUMMIT speech recog-nition system: A progress report.
In Proceedingsof the First DARPA Speech and Natural LanguageWorkshop, Philadelphia, PA, February 1989.
