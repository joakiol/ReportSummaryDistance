Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 535?545, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsModelling Sequential Text with an Adaptive Topic ModelLan Du?Department of ComputingMacquarie UniversitySydney, Australialan.du@mq.edu.auWray Buntine?Canberra Research LabNational ICT AustraliaCanberra, Australiawray.buntine@nicta.com.auHuidong Jin?CSIRO Mathematics, Informaticsand Statistics,Canberra, Australiawarren.jin@csiro.auAbstractTopic models are increasingly being used fortext analysis tasks, often times replacing ear-lier semantic techniques such as latent seman-tic analysis.
In this paper, we develop a noveladaptive topic model with the ability to adapttopics from both the previous segment and theparent document.
For this proposed model, aGibbs sampler is developed for doing poste-rior inference.
Experimental results show thatwith topic adaptation, our model significantlyimproves over existing approaches in terms ofperplexity, and is able to uncover clear se-quential structure on, for example, HermanMelville?s book ?Moby Dick?.1 IntroductionNatural language text usually consists of topicallystructured and coherent components, such as groupsof sentences that form paragraphs and groups ofparagraphs that form sections.
Topical coherence indocuments facilitates readers?
comprehension, andreflects the author?s intended structure.
Capturingthis structural topical dependency should lead to im-proved topic modelling.
It also seems reasonableto propose that text analysis tasks that involve thestructure of a document, for instance, summarisationand segmentation, should also be improved by topicmodels that better model that structure.Recently, topic models are increasingly beingused for text analysis tasks such as summarisa-?This work was partially done when Du was at College ofEngineering & Computer Science, the Australian National Uni-versity when working together with Buntine and Jin there.tion (Arora and Ravindran, 2008) and segmenta-tion (Misra et al2011; Eisenstein and Barzilay,2008), often times replacing earlier semantic tech-niques such as latent semantic analysis (Deerwesteret al1990).
Topic models can be improved by bet-ter modelling the semantic aspects of text, for in-stance integrating collocations into the model (John-son, 2010; Hardisty et al2010) or encouraging top-ics to be more semantically coherent (Newman etal., 2011) based on lexical coherence models (New-man et al2010), modelling the structural aspectsof documents, for instance modelling a documentas a set of segments (Du et al2010; Wang et al2011; Chen et al2009), or improving the under-lying statistical methods (Teh et al2006; Wallachet al2009).
Topic models, like statistical parsingmethods, are using more sophisticated latent vari-able methods in order to model different aspects ofthese problems.In this paper, we are interested in developing anew topic model which can take into account thestructural topic dependency by following the higherlevel document subject structure, but we hope to re-tain the general flavour of topic models, where com-ponents (e.g., sentences) can be a mixture of topics.Thus we need to depart from the earlier HMM stylemodels, see, e.g., (Blei and Moreno, 2001; Gruberet al2007).
Inspired by the idea that documentsusually exhibits internal structure (e.g., (Wang et al2011)), in which semantically related units are clus-tered together to form semantically structural seg-ments, we treat documents as sequences of segments(e.g., sentences, paragraphs, sections, or chapters).In this way, we can model the topic correlation be-535~?~?1 ~?2 ~?3 ~?4~?~?1 ~?2 ~?3 ~?4~?~?1 ~?2 ~?3 ~?4(H) (S)(M)~?~?1 ~?2 ~?3 ~?4(B)Figure 1: Different structural relationships for topics ofsections in a 4-part document, hierarchical (H), sequen-tial (S), both (B) or mixed (M).tween the segments in a ?bag of segments?
fashion,i.e., beyond the ?bag of words?
assumption, and re-veal how topics evolve among segments.Indeed, we were impressed by the improvementin perplexity obtained by the segmented topic model(STM) (Du et al2010), so we considered the prob-lem of whether one can add sequence informationinto a structured topic model as well.
Figure 1 illus-trates the type of structural information being con-sidered, where the vectors are some representationof the content.
STM is represented by the hierar-chical model.
A strictly sequential model wouldseem unrealistic for some documents, for instancebooks.
A topic model using the strictly sequentialmodel was developed (Du et al2012) but it report-edly performs halfway between STM and LDA.
Inthis paper, we develop an adaptive topic model togo beyond a strictly sequential model while allowsome hierarchical influence.
There are two possiblehybrids, one called ?mixed?
has distinct breaks inthe sequence, while the other called ?both?
overlaysboth sequence and hierarchy and there could be rel-ative strengths associated with the arrows.
We em-ploy the ?both?
hybrid but use the relative strengthsto adaptively allow it to approximate the ?mixed?hybrid.Research in Machine Learning and Natural Lan-guage Processing has attempted to model varioustopical dependencies.
Some work considers struc-ture within the sentence level by mixing hiddenMarkov models (HMMs) and topics on a word byword basis: the aspect HMM (Blei and Moreno,2001) and the HMM-LDA model (Griffiths et al2005) that models both short-range syntactic depen-dencies and longer semantic dependencies.
Thesemodels operate at a finer level than we are consider-ing at a segment (like paragraph or section) level.
Tomake a tool like the HMM work at higher levels, oneneeds to make stronger assumptions, for instance as-signing each sentence a single topic and then topicspecific word models can be used: the hidden topicMarkov model (Gruber et al2007) that models thetransitional topic structure; a global model based onthe generalised Mallows model (Chen et al2009),and a HMM based content model (Barzilay andLee, 2004).
Researchers have also considered time-series of topics: various kinds of dynamic topicmodels, following early work of (Blei and Lafferty,2006), represent a collection as a sequence of sub-collections in epochs.
Here, one is modelling thecollections over broad epochs, not the structure of asingle document that our model considers.This paper is organised as follows.
We firstpresent background theory in Section 2.
Then thenew model is presented in Section 3, followed byGibbs sampling theory and algorithm in Sections 4and 5 respectively.
Experiments are reported in Sec-tion 6 with a conclusion in Section 7.2 BackgroundThe basic topic model is first presented in Sec-tion 2.1, as a point of departure.
In seeking to de-velop a general sequential topic model, we hopeto go beyond a strictly sequential model and allowsome hierarchical influence.
This, however, presentstwo challenges: modelling and statistical inference.Hierarchical inference (and thus sequential infer-ence) over probability vectors can be handled us-ing the theory of hierarchical Poisson-Dirichlet pro-cesses (PDPs).
This is presented in Section 2.2.2.1 The LDA modelThe benchmark model for topic modelling is latentDirichlet alation (LDA) (Blei et al2003), a la-tent variable model of documents.
Documents areindexed by i, and words ~w are observed data.
Thelatent variables are ~?i (the topic distribution for adocument) and ~z (the topic assignments for observedwords), and the model parameter of ~?k?s (word dis-tributions).
These notation are later extended in Ta-536ble 1.
The generative model is as follows:~?k ?
DirichletW (~?)
?
k~?i ?
DirichletK (~?)
?
izi,l ?
DiscreteK (~?i) ?
i, lwi,l ?
DiscreteK(~?zi,l)?
i, l .DirichletK(?)
is a K-dimensional Dirichlet distribu-tion.
The hyper-parameter ~?
is a Dirichlet prior onword distributions (i.e., a Dirichlet smoothing on themultinomial parameter ~?k (Blei et al2003)) and theDirichlet prior ~?
on topic distributions.2.2 Hierarchical PDPsA discrete probability vector ~?
of finite dimensionK is sampled from some distribution F?
(~?0) witha parameter set, say ?
, and is also dependent ona parent probability vector ~?0 also of finite dimen-sion K. Then a sample of size N is taken ac-cording to the probability vector ~?, represented as~z ?
{1, ...,K}N .
This data is collected into counts~n = (n1, ..., nK) where nk is the number of data in~z with value k and?k nk = N .
This situation isrepresented as follows:~?
?
F?
(~?0); ~zi ?
DiscreteK(~?)
for i = 1, ..., N .Commonly in topic modelling, the Dirichlet distri-bution is used for discrete probability vectors.
Inthis case F?
(~?0) ?
DirichletK(b~?0), ?
?
(K, b)where b is the concentration parameter.
Bayesiananalysis yields a marginalised likelihood, after inte-grating out ~?, ofp(~z??
?, ~?0,Dirichlet)=Beta (~n+ b~?0)Beta (b~?0), (1)where Beta(?)
is the vector valued function normal-ising the Dirichlet distribution.
A problem here isthat p(~z|b, ~?0) is an intractable function of ~?0.Dirichlet processes and Poisson-Dirichlet pro-cesses alleviate this problem by using an auxiliaryvariable trick (Robert and Casella, 2004).
That is,we introduce an auxiliary variable over which wealso sample but do not need to record.
The auxiliaryvariable is the table count1 which is a tk for each nk1Based on the Chinese Restaurant analogy (Teh et al2006),each table has a dish, a data value, while data, the customer, isassigned to tables, and multiple tables can serve the same dish.and it represents the number of ?tables?
over whichthe nk ?customers?
are spread out.
Thus the follow-ing constraints hold:0 ?
tk ?
nk and tk = 0 iff nk = 0 .
(2)When the distribution over probability vectors fol-lows a Poisson-Dirichlet process which has two pa-rameters ?
?
(a, b) and the parent distribution ~?0,then F?
(~?0) ?
PDP(a, b, ~?0).
Here a is the dis-count parameter, b the concentration parameter and~?0 the base measure.
In this case Bayesian analysisyields an augmented marginalised likelihood (Bun-tine and Hutter, 2012), after integrating out ~?, ofp(~z,~t??
?, ~?0,PDP)=(b|a)T(b)N?kSnktk,a (?0,k)tk (3)where T =?k tk, (x|y)N =?N?1n=0 (x + ny) de-notes the Pochhammer symbol, (x)N = (x|1)N , andSNM,a is a generalized Stirling number that is readilytabulated (Buntine and Hutter, 2012).There are two fundamental things to notice aboutEquation (3).
Positively, the term in ~?0 takes theform of a multinomial likelihood, so we can prop-agate it up and perform inference on ~?0 unen-cumbered by the functional mess of Equation (1).Thus Poisson-Dirichlet processes allow one to doBayesian reasoning on hierarchies of probabilityvectors (Teh, 2006; Teh et al2006).
Negatively,however, one needs to sample the auxiliary vari-ables ~t leading to some problems: The range of tk,{0, ..., nk}, is broad.
Also, contributions from in-dividual data zi have been lost so the mixing of theMCMC can sometimes be slow.
We confirmed theseproblems on our first implementation of the Adap-tive Topic Model presented next in Section 3.A further improvement on PDP sampling isachieved in (Chen et al2011), where another aux-iliary variable is introduced, a so-called table in-dicator, that for each datum zi indicates whetherit is the ?head of its table?
(recall the nk data arespread over tk tables, each table has one and onlyone ?head?).
Let ri = 1 if zi is the ?head of itstable,?
and zero otherwise.
According to this ?ta-ble?
logic, the number of tables for nk must be thenumber of data zi that are also head of table, sotk =?Ni=1 1zi=k1ri=1.
Moreover, given this def-inition, the first constraint of Equation (2) on tk is537automatically satisfied.
Finally, with tk tables thenthere must be exactly tk heads of table, and we areindifferent about which data are heads of table, thusp(~z, ~r??
?, ~?0,PDP)= p(~z,~t??
?, ~?0,PDP)?k(nktk)?1.
(4)When using this marginalised likelihood in a Gibbssampler, the zi themselves are usually latent so alsosampled, and we develop a blocked Gibbs samplerfor (zi, ri).
Since ~r only appears indirectly throughthe table counts ~t, one does not need to store the ~r,instead just resamples an ri when needed accordingto the proportion tw/nw where zi = w.3 The proposed Adaptive Topic ModelIn this section an adaptive topic model (AdaTM) isdeveloped, a fully structured topic model, by usinga PDP to simultaneously model the hierarchical andthe sequential topic structures.
Documents are as-sumed to be broken into a sequence of segments.Topic distributions are used to mimic the subjects ofdocuments and subtopics of their segments.
The no-tations and terminologies used in the following sec-tions are given in Table 1.In AdaTM, the two topic structures are capturedby drawing topic distributions from the PDPs withtwo base distributions as follows.
The documenttopic distribution ~?i and the jth segment topic dis-Table 1: List of notation for AdaTMK number of topicsI number of documentsJi number of segments in document iLi,j number of words in document i, segment jW number of words in dictionary~?i document topic probabilities for document i~?
K-dimensional prior for each ~?i~?i,j segment topic probabilities for document i andsegment j?i,j mixture weight associating with the link be-tween ~?i.j and ~?i,j?1~?
word probability vectors as a K ?W matrix~?k word probability vector for topic k, entries in ?~?
W -dimensional prior for each ~?kwi,j,l word in document i, segment j, position lzi,j,l topic for word wi,j,lwLzIK?????1?21wLz2????JwLzJ???
?Figure 2: The adaptive topic model: ~?
is the documenttopic distribution, ~?1, ~?2, .
.
.
, ~?J are the segment topicdistributions, and ~?
is a set of the mixture weights.tribution ~?i,j are linearly combined to give a basedistribution for the (j + 1)th segment?s topic dis-tribution ~?i,j+1.
The topic distribution of the firstsegment, i.e., ~?i,1, is drawn directly with the basedistribution ~?i.
Call this generative process topicadaptation.
The graphical representation of AdaTMis shown in Figure 2, and clearly shows the combi-nation of sequence and hierarchy for the topic prob-abilities.
Note the linear combination at each node~?i,j is weighted with latent proportions ?i,j .The resultant model for AdaTM is:~?k ?
DirichletW (~?)
?
k~?i ?
DirichletK (~?)
?
i?i,j ?
Beta(?S , ?T ) ?
i, j~?i,j ?
PDP (?i,j~?i,j?1 + (1?
?i,j)~?i, a, b)zi,j,l ?
DiscreteK (~?i,j) ?
i, j, lwi,j,l ?
DiscreteK(~?zi,j,l)?
i, j, l .For notational convenience, let ~?i,0 = ~?i.
Assumethe dimensionality of the Dirichlet distribution (i.e.,the number of topics) is known and fixed, and wordprobabilities are parameterised with aK?W matrix~?
= (~?1, ..., ~?K).4 Gibbs Sampling FormulationGiven observations and model parameters, comput-ing the posterior distribution of latent variables is in-feasible for AdaTM due to the intractable computa-538Table 2: List of statistics for AdaTMMi,k,w the total number of words in document i withdictionary index w and being assigned to topickMk,w total Mi,k,w for document i, i.e.,?iMi,k,w~Mk vector of W values Mk,wni,j,k topic count in document i segment j for topic kNi,j topic total in document i segment j, i.e.,?Kk=1 ni,j,kti,j,k table count in the CPR for document i and para-graph j, for topic k that is inherited back toparagraph j ?
1 and ~?i,j?1.si,j,k table count in the CPR for document i and para-graph j, for topic k that is inherited back to thedocument and ~?i.Ti,j total table count in the CRP for document i andsegment j, equal to?Kk=1 ti,j,k.Si,j total table count in the CRP for document i andsegment j, equal to?Kk=1 si,j,k.~ti,j table count vector of ti,j,k?s for segment j.~si,j table count vector of si,j,k?s for segment j.tion of marginal probabilities.
Therefore, we have touse approximate inference techniques.
This sectionproposes a blocked Gibbs sampling algorithm basedon methods from Chen et al2011).
Table 2 listsall statistics needed in the algorithm.
Note for easierunderstanding, terminologies of the Chinese Restau-rant Process (Teh et al2006) will be used, i.e., cus-tomers, dishes and restaurants, correspond to words,topics and segments respectively.The first major complication, over the use of thehierarchical PDP and Equation (3) and the table in-dicator trick of Equation (4), is handling the lin-ear combination of ?i,j~?i,j?1 + (1 ?
?i,j)~?i usedin the PDPs.
We manage this as follows: First,Equation (3) shows that a contribution of the form(?0,k)tk results.
In our case, this becomes?k(?i,j?i,j?1,k + (1?
?i,j)?i,k)t?i,j,kwhere t?i,j,k is the corresponding introduced auxil-iary variable the table count which is involved withconstraints on ni,j,k+ti,j+1,k, from Equation (2).
Todeal with this power of a sum, we break the countst?i,j,k into two parts, those that contribute to ~?i,j?1and those that contribute to ~?i.
We call these partsti,j,k and si,j,k respectively.
The product can then beexpanded and ?i,j integrated out.
This yields:Beta (Si,j + ?S , Ti,j + ?T )?k?ti,j,ki,j?1,k?si,j,ki,k .The powers ?ti,j,ki,j?1,k and ?si,j,ki,k can then be pushedup to the next nodes in the PDP/Dirichlet hierarchy.Note the standard constraints and table indicators arealso needed here.The precise form of the table indicators needs tobe considered as well since there is a hierarchy forthem, and this is the second major complication inthe model.
As discussed in Chen et al2011), tableindicators are not required to be recorded, instead,randomly sampled in Gibbs cycles.
The table indi-cators when known can be used to reconstruct thetable counts ti,j,k and si,j,k, and are reconstructedby sampling from them.
For now, denote the tableindicators as ui,j,l for word wi,j,l.To complete a formulation suitable for Gibbssampling, we first compute the marginal distribu-tion of the observations ~w1:I,1:J (words), the topicassignments ~z1:I,1:J and the table indicators ~u1:I,1:J .The Dirichlet integral is used to integrate out thedocument topic distributions ~?1:I and the topic-by-words matrix ~?, and the joint posterior dis-tribution computed for a PDP is used to recur-sively marginalise out the segment topic distribu-tions ~?1:I,1:J .
With these variables marginalised out,we derive the following marginal distributionp(~z1:I,1:J , ~w1:I,1:J , ~u1:I,1:J??
~?,~?, a, b) = (5)I?i=1BetaK(~?+?Jij=1 ~si,j)BetaK (~?)K?k=1BetaW(~?
+ ~Mk)BetaW (~?
)I?i=1Ji?j=1Beta (Si,j + ?S , Ti,j + ?T )(b|a)Ti,j+Si,j(b)Ni,j+Ti,j+1I?i=1Ji?j=1K?k=1((ni,j,k + ti,j+1,k)(ti,j,k + si,j,k))?1Sni,j,k+ti,j+1,kti,j,k+si,j,k,a .And the following constraints apply:ti,j,k + si,j,k ?
ni,j,k + ti,j+1,k, (6)ti,j,k + si,j,k = 0 iff ni,j,k + ti,j+1,k = 0 .
(7)The first constraint falls out naturally when table in-dicators are used.
For convenience of the formulas,539set ti,Ji+1,k = 0 (there is no Ji + 1 segment) andti,1,k = 0 (the first segment only uses ~?i).Now let us consider again the table indicatorsui,j,l for word wi,j,l.
If this word is in topic k at doc-ument i and segment j, then it contributes a count toni,j,k.
It also indicates if it contributes a new table,or a count to t?i,j,k for the PDP at this node.
How-ever, as we discussed above, this then contributes toeither ti,j,k or si,j,k.
If it contributes to ti,j,k, thenit recurses up to contribute a data count to the PDPfor document i segment j ?
1.
Thus it also needs atable indicator at that node.
Consequently, the tableindicator ui,j,l for word wi,j,l must specify whetherit contributes a table to all PDP nodes reachable byit in the graph.We define ui,j,l specifically as ui,j,l = (u1, u2)such that u1 ?
[?1, 0, 1] and u2 ?
[1, ?
?
?
, j],where u2 indicates segment denoted by node ?jup to which wi,j,l contributes a table.
Given u2,u1 = ?1 denotes wi,j,l contributes a table count tosi,u2,k and ti,j?,k for u2 < j?
?
j; u1 = 0 denoteswi,j,l does not contribute a table to node u2, but con-tributes a table count to ti,j?,k for u2 < j?
?
j; andu1 = 1 denotes wi,j,l contributes a table count toeach ti,j?,k for u2 ?
j?
?
j.Now, we are ready to compute the conditionalprobabilities for jointly sampling topics and table in-dicators from the model posterior of Equation (5).5 Gibbs Sampling AlgorithmThe Gibbs sampler iterates over words, doing ablocked sample of (zi,j,l, ui,j,l).
The first task is toreconstruct ui,j,l since it is not stored.
Since the pos-terior of Equation (5) does not explicitly mentionthe ui,j,l?s, they occur indirectly through the tablecounts, and we can randomly reconstruct them bysampling them uniformly from the space of possi-bilities.
Following this, we then remove the values(zi,j,l, ui,j,l) from the full set of statistics.
Finally,we block sample new values for (zi,j,l, ui,j,l) andadd them to the statistics.
The new ui,j,l is subse-quently forgotten and the zi,j,l recorded.Reconstructing table indicator ui,j,l: We start atthe node indexed i, j.
If si,j,k+ti,j,k = 1 and ni,j,k+ti,j+1,k > 1 then no tables can be removed sincethere is only one table but several customers at thetable.
Thus ui,j,l = (u1, u2) = (0, j) and there is nosampling.
Otherwise, by symmetry arguments, wesample u1 viap(u1 = ?1, 0, 1|u2 = j, zi,j,l = k) ?
(si,j,k, ti,j,k, ni,j,k + ti,j+1,k ?
si,j,k ?
ti,j,k) ,since there are ni,j,k+ti,j+1,k data distributed acrossthe three possibilities.
If after sampling u1 = ?1,the data contributes a table count up to ~?i and soui,j,l = (u1, u2) = (?1, j).
If u1 = 0, the ui,j,l =(u1, u2) = (0, j).
Otherwise, the data contributes atable count up to the parent PDP for ~?i,j?1 and werecurse, repeating the sampling process at the parentnode.
Note, however, that the table indicator (0, j?
)for j?
< j is equivalent to the table indicator (1, j?+1) as far as statistics is concerned.Block sampling (zi,j,l, ui,j,l): The full set of pos-sibilities are, for each possible topic zi,j,l = k:?
no tables are created, so ui,j,l = (0, j),?
tables are created contributing a table count allthe way up to node j?
(?
j) but stop at j?
anddo not subsequently contribute a count to ~?i, soui,j,l = (1, j?),?
tables are created contributing a table count allthe way up to node j?
?
j but stop at j?
andalso subsequently contribute a count to ~?i, soui,j,l = (?1, j?
).These three possibilities lead to detailed but fairlystraight forward changes to the posterior of Equa-tion (5).
Thus a full blocked sampler for (zi,j,l, ui,j,l)can be constructed.Estimates: learnt values of ~?i, ~?i,j , ~?k are neededfor evaluation, perplexity calculations, etc.
Theseare estimated by taking averages after the Gibbssampler has burnt in, using the standard posteriormeans for Dirichlets and Poisson-Dirichlets.6 ExperimentsIn the experimental work, we have three objectives:(1) to explore the setting of hyper-parameters, (2) tocompare the model with the earlier sequential LDA(SeqLDA) of (Du et al2012), STM of (Du et al2010) and standard LDA, and (3) to view the resultsin detail on a number of characteristic problems.540Table 3: Datasets#docs #segs #words vocabPat-A 500 51,748 2,146,464 16,573Pat-B 397 9,123 417,631 7,663Pat-G06 500 11,938 655,694 6,844Pat-H 500 11,662 562,439 10,114Pat-F 140 3,181 166,091 4,674Prince-C 1 26 10,588 3,292Prince-P 1 192 10.588 3,292Moby Dick 1 135 88,802 16,2236.1 DatasetsFor general testing, five patent datasets are ran-domly selected from U.S. patents granted in 2009and 2010.
Patents in Pat-A are selected from in-ternational patent class (IPC) ?A?, which is about?HUMAN NECESSITIES?
; those in Pat-B are se-lected from class ?B60?
about ?VEHICLES INGENERAL?
; those in Pat-H are selected fromclass ?H?
about ?ELECTRICITY?
; those in Pat-F are selected from class ?F?
about ?MECHAN-ICAL ENGINEERING; LIGHTING; HEATING;WEAPONS; BLASTING?
; and those in Pat-G areselected from class ?G06?
about ?COMPUTING;CALCULATING; COUNTING?.
All the patents inthese five datasets are split into paragraphs that aretaken as segments, and the sequence of paragraphsin each patent is reserved in order to maintain theoriginal layout.
All the stop words, the top 10 com-mon words, the uncommon words (i.e., words in lessthan five patents) and numbers have been removed.Two books used for more detailed investigationare ?The Prince?
by Niccolo` Machiavelli and ?MobyDick?
by Herman Melville.
They are split into chap-ters and/or paragraphs which are treated as seg-ments, and only stop-words are removed.
Table 3shows in detail the statistics of these datasets afterpreprocessing.6.2 DesignPerplexity, a standard measure of dictionary-basedcompressibility, is used for comparison.
When re-porting test perplexities, the held-out perplexitymeasure (Rosen-Zvi et al2004) is used to evaluatethe generalisation capability to the unseen data.
Thisis known to be unbiased.
To compute the held-outperplexity, 20% of patents in each data set was ran-1025 50 100 150 200 250 3008009001000110012001300bPerplexityPat?BPat?FPat?GPat?H(a) fix a = 00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.980095011001250aPerplexityPat?BPat?GPat?HPat?F(b) fix b = 10Figure 3: Analysis of parameters of Poisson-Dirichletprocess.
(a) shows how perplexity changes with b; (b)shows how it changes with a.0 50 100 150 200600700800900100011001200Lamda_SPerplexityPat?BPat?FPat?GPat?HPat?A(a) fix ?T = 10 50 100 150 200600700800900100011001200Lamda_TPerplexityPat?APat?BPat?FPat?HPat?G(b) fix ?S = 1Figure 4: Analysis of the two parameters for Beta distri-bution.
(a) how perplexity changes with ?S ; (b) how itchanges with ?T .domly held out from training to be used for testing.For this, 1000 Gibbs cycles were done for burn-infollowed by 500 cycles with a lag for 100 for pa-rameter estimation.We implemented all the four models, e.g., LDA,STM, SeqTM and AdaTM in C, and ran them on adesktop with Intel Core i5 CPU (2.8GHz?4), eventhough our code is not multi-threaded.
Perplexitycalculations, data input and handling, etc., were thesame for all algorithms.
We note that the currentAdaTM implementation is an order of magnitudeslower than regular LDA per major Gibbs cycle.6.3 Hyper-parameters in AdaTMExperiments on the impact of the hyper-parameterson the patent data sets were as follows: First, fixingK = 50, the Beta parameters ?T = 1 and ?S = 1,optimise symmetric ?, and do two variations fix-a:a = 0.0, trying b = 1, 5, 10, 25, ..., 300, and fix-b:b = 10, trying a = 0.1, 0.2, ..., 0.9.
Second, fix-?T(fix-?S): fix a = 0.2 and ?T (?S) = 1, optimiseb and ?, change ?S(?T ) = 0.1, 1, 10, 50, 100, 200.Figures 3 and 4 show the corresponding plots.
Fig-ure 3(b) and Figure 4(a) show that varying the val-ues of a and ?S does not significantly change the541510 25 50 100 15063093012301530183021302430Number of TopicsPerplexityLDA_DLDA_PSeqLDASTMAdaTM(a) Pat-A0510 25 50 100 150910106012101360151016601810Number of TopicsPerplexityLDA_DLDA_PSeqLDASTMAdaTM(b) Pat-H510 25 50 100 15070085010001150130014501600Number of TopicsPerplexityLDA_DLDA_PSeqLDASTMAdaTM(c) Pat-B0510 25 50 100 150670820970112012701420Number of TopicsPerplexityLDA_DLDA_PSeqLDASTMAdaTM(d) Pat-F0510 25 50 100 15091011101310151017101910Number of TopicsPerplexityLDA_DLDA_PSeqLDASTMAdaTM(e) Pat-G510 25 50 100 150?10015406590115140160Number of TopicsPerplexity DifferencePat?APat?BPat?FPat?GPat?H(f) ShuffleFigure 5: Perplexity comparisons.perplexity.
In contrast, Figure 3(a) shows differentb values significantly change perplexity.
Therefore,we sought to optimise b.
The experiment of fixing?S = 1 and changing ?T shows a small ?T is pre-ferred.6.4 Perplexity ComparisonPerplexity comparisons were done with the defaultsettings a = 0.2, ?
= 0.1, ?
= 0.01, ?S = 1,?T = 1 and b optimised automatically using thescheme from (Du et al2012).
Figure 5 showsthe results on these five patent datasets for differ-ent numbers of topics.
LDA D is LDA run on wholepatents, and LDA P is LDA run on the paragraphswithin patents.
Table 4 gives the p-values of a one-tail paired t-test for AdaTM versus the others, wherelower p-value indicates AdaTM has statistically sig-nificant lower perplexity.
From this we can see thatAdaTM is statistically significantly better than Se-qLDA and LDA, and somewhat better than STM.In addition, we ran another set of experimentsby randomly shuffling the order of paragraphs ineach patent several times before running AdaTM.Then, we calculate the difference between perplex-ities with and without random shuffle.
Figure 5(f)shows the plot of differences in each data sets.
Thepositive difference means randomly shuffling the or-der of paragraphs indeed increases the perplexity.It can further prove that there does exist sequentialtopic structure in patents, which confirms the findingin (Du et al2012).6.5 Topic Evolution ComparisonsAll the comparison experiments reported in this sec-tion are run with 20 topics, the upper limit for easyvisualisation, and without optimising any parame-ters.
The Dirichlet Priors are fixed as ?k = 0.1and ?w = 0.01.
For AdaTM, SeqLDA, and STM,a = 0.0 and b = 100 for ?The Prince?
and b = 200for ?Moby Dick?.
These settings have proven ro-bust in experiments.
To align the topics so visual-isations match, the sequential models are initialisedusing an LDA model built at the chapter level.
More-over, all the models are run at both the chapter andthe paragraph level.
With the common initialisation,both paragraph level and chapter level models canTable 4: P-values for one-tail paired t-test on the fivepatent datasets.AdaTMPat-G Pat-A Pat-F Pat-H Pat-BLDA D .0001 .0001 .0002 .0001 .0001LDA P .0041 .0030 .0022 .0071 .0096SeqLDA .0029 .0047 .0003 .0012 .0023STM .0220 .0066 .0210 .0629 .0853542(a) Evolution of paragraph topics for LDA(b) Topic alignment of LDA versus AdaTM top-ics for chaptersFigure 6: Analysis on ?The Prince?.be aligned.To visualise topic evolution, we use a plot withone colour per topic displayed over the sequence.Figure 6(a) shows this for LDA run on paragraphsof ?The Prince?.
The proportion of 20 topics is theY-axis, spread across the unit interval.
The para-graphs run along the X-axis, so the topic evolutionis clearly displayed.
One can see there is no se-quential structure in this derived by the LDA model,and similar plots result from ?Moby Dick?
for LDA.Figure 6(b) shows the alignment of topics betweenthe initialising model (LDA+chapters) and AdaTMrun on chapters.
Each point in the matrix gives theHellinger distance between the corresponding top-ics, color coded.
The plots for the other models,chapters or paragraphs, are similar so plots like Fig-ure 6(a) for the other models can be meaningfullycompared.Figure 7 then shows the corresponding evolutionplots for AdaTM and SeqLDA on chapters and para-graphs.
The contrast of these with LDA is stark.The large improvement in perplexity for AdaTM(see Section 6.4) along with no change in lexi-cal coherence (see Section 6.2) means that the se-(a) AdaTM on chapters(b) AdaTM on paragraphs(c) SeqLDA on chapters(d) SeqLDA on paragraphsFigure 7: Topic Evolution on ?The Prince?.quential information is actually beneficial statisti-cally.
Note that SeqLDA, while exhibiting slightlystronger sequential structure than AdaTM in these543(a) LDA on chapters(b) STM on Chapters(c) AdaTM on ChaptersFigure 8: Topic Evolution on ?Moby Dick?.figures has significantly worse test perplexity, so itssequential affect is too strong and harming results.Also, note that some topics have different time se-quence profiles between AdaTM and SeqLDA.
In-deed, inspection of the top words for each showthese topics differ somewhat.
So while the LDAto AdaTM/SeqLDA topic correspondences are quitegood due to the use of LDA initialisation, the cor-respondences between AdaTM and SeqLDA havedegraded.
We see that AdaTM has nearly as goodsequential characteristics as SeqLDA.
Furthermore,segment topic distribution ?i,j of SeqLDA are grad-ually deviating from the document topic distribution?i, which is not the case for AdaTM.Results for ?Moby Dick?
on chapters are com-parable.
Figure 8 shows similar topic evolutionplots for LDA, STM and AdaTM.
In contrast, theAdaTM topic evolutions are much clearer for theless frequent topics, as shown in Figure 8(c).
Var-ious parts of this are readily interpreted from thestoryline.
Here we briefly discuss topics by theircolour: black: Captain Peleg and the business ofsigning on; yellow: inns, housing, bed; mauve:Queequeg; azure: (around chapters 60-80) detailsof whales aqua: (peaks at 8, 82, 88) pulpit, schoolsand mythology of whaling.We see that AdaTM can be used to understand thetopics with regards to the sequential structure of abook.
In contrast, the sequential nature for LDA andSTM is lost in the noise.
It can be very interesting toapply the proposed topic models to some text anal-ysis tasks, such as topic segmentation, summarisa-tion, and semantic title evaluation, which are subjectto our future work.7 ConclusionA model for adaptive sequential topic modelling hasbeen developed to improve over a simple exchange-able segments model STM (Du et al2010) and anaive sequential model SeqLDA (Du et al2012) interms of perplexity and its confirmed ability to un-cover sequential structure in the topics.
One couldextract meaningful topics from a book like HermanMelville?s ?Moby Dick?
and concurrently gain theirsequential profile.
The current Gibbs sampler isslower than regular LDA, so future work is to speedup the algorithm.AcknowledgmentsThe authors would like to thank all the anonymous re-viewers for their valuable comments.
Lan Du wassupported under the Australian Research Council?sDiscovery Projects funding scheme (project numbersDP110102506 and DP110102593).
Dr. Huidong Jinwas partly supported by CSIRO Mathematics, Informat-ics and Statistics for this work.
NICTA is funded by theAustralian Government as represented by the Departmentof Broadband, Communications and the Digital Econ-omy and the Australian Research Council through theICT Center of Excellence program.544ReferencesR.
Arora and B. Ravindran.
2008.
Latent Dirichlet alcation and singular value decomposition based multi-document summarization.
In ICDM ?08: Proc.
of2008 Eighth IEEE Inter.
Conf.
on Data Mining, pages713?718.R.
Barzilay and L. Lee.
2004.
Catching the drift: Prob-abilistic content models, with applications to genera-tion and summarization.
In HLT-NAACL 2004: MainProceedings, pages 113?120.
Association for Compu-tational Linguistics.D.M.
Blei and J.D.
Lafferty.
2006.
Dynamic topic mod-els.
In ICML ?06: Proc.
of 23rd international confer-ence on Machine learning, pages 113?120.D.M.
Blei and P.J.
Moreno.
2001.
Topic segmenta-tion with an aspect hidden Markov model.
In Proc.of 24th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 343?348.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
LatentDirichlet alation.
Journal of Machine Learning Re-search, 3:993?1022.W.
Buntine and M. Hutter.
2012.
A Bayesian viewof the Poisson-Dirichlet process.
Technical ReportarXiv:1007.0296v2, ArXiv, Cornell, February.H.
Chen, S.R.K.
Branavan, R. Barzilay, and D.R.
Karger.2009.
Global models of document structure using la-tent permutations.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conf.
of theNorth American Chapter of the Association for Com-putational Linguistics, pages 371?379, Stroudsburg,PA, USA.
Association for Computational Linguistics.C.
Chen, L. Du, and W. Buntine.
2011.
Sampling for thePoisson-Dirichlet process.
In European Conf.
on Ma-chine Learning and Principles and Practice of Knowl-edge Discovery in Database, pages 296?311.S.C.
Deerwester, S.T.
Dumais, T.K.
Landauer, G.W.
Fur-nas, and R.A. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Society ofInformation Science, 41(6):391?407.L.
Du, W. Buntine, and H. Jin.
2010.
A segmented topicmodel based on the two-parameter Poisson-Dirichletprocess.
Machine Learning, 81:5?19.L.
Du, W. Buntine, H. Jin, and C. Chen.
2012.
Sequentiallatent dirichlet alation.
Knowledge and InformationSystems, 31(3):475?503.J.
Eisenstein and R. Barzilay.
2008.
Bayesian unsuper-vised topic segmentation.
In Proc.
of Conf.
on Empir-ical Methods in Natural Language Processing, pages334?343.
Association for Computational Linguistics.T.L.
Griffiths, M. Steyvers, D.M.
Blei, and J.B. Tenen-baum.
2005.
Integrating topics and syntax.
In Ad-vances in Neural Information Processing Systems 17,pages 537?544.A.
Gruber, Y. Weiss, and M. Rosen-Zvi.
2007.
Hiddentopic markov models.
Journal of Machine LearningResearch - Proceedings Track, 2:163?170.E.A.
Hardisty, J. Boyd-Graber, and P. Resnik.
2010.Modeling perspective using adaptor grammars.
InProc.
of the 2010 Conf.
on Empirical Methods in Nat-ural Language Processing, pages 284?292, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.M.
Johnson.
2010.
PCFGs, topic models, adaptor gram-mars and learning topical collocations and the struc-ture of proper names.
In Proc.
of 48th Annual Meetingof the ACL, pages 1148?1157, Uppsala, Sweden, July.Association for Computational Linguistics.H.
Misra, F. Yvon, O. Capp, and J. Jose.
2011.
Text seg-mentation: A topic modeling perspective.
InformationProcessing & Management, 47(4):528?544.D.
Newman, J.H.
Lau, K. Grieser, and T. Baldwin.
2010.Automatic evaluation of topic coherence.
In NorthAmerican Chapter of the Association for Computa-tional Linguistics - Human Language Technologies,pages 100?108.D.
Newman, E.V.
Bonilla, and W. Buntine.
2011.
Im-proving topic coherence with regularized topic mod-els.
In J. Shawe-Taylor, R.S.
Zemel, P. Bartlett,F.C.N.
Pereira, and K.Q.
Weinberger, editors, Ad-vances in Neural Information Processing Systems 24,pages 496?504.C.P.
Robert and G. Casella.
2004.
Monte Carlo statisti-cal methods.
Springer.
second edition.M.
Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.2004.
The author-topic model for authors and docu-ments.
In Proc.
of 20th conference on Uncertainty inArtificial Intelligence, pages 487?494.Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006.Hierarchical Dirichlet processes.
Journal of the Amer-ican Statistical Association, 101:1566?1581.Y.
W. Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proc.
of21st Inter.
Conf.
on Computational Linguistics and the44th annual meeting of the Association for Computa-tional Linguistics, pages 985?992.H.
Wallach, D. Mimno, and A. McCallum.
2009.
Re-thinking LDA: Why priors matter.
In Advances inNeural Information Processing Systems 19.H.
Wang, D. Zhang, and C. Zhai.
2011.
Structural topicmodel for latent topical structure analysis.
In Proc.
of49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies -Volume 1, pages 1526?1535, Stroudsburg, PA, USA.Association for Computational Linguistics.545
