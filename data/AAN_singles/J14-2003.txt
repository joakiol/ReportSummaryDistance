Authorship Attribution with Topic ModelsYanir Seroussi?Monash UniversityIngrid Zukerman?
?Monash UniversityFabian Bohnert?Monash UniversityAuthorship attribution deals with identifying the authors of anonymous texts.
Traditionally,research in this field has focused on formal texts, such as essays and novels, but recentlymore attention has been given to texts generated by on-line users, such as e-mails and blogs.Authorship attribution of such on-line texts is a more challenging task than traditional author-ship attribution, because such texts tend to be short, and the number of candidate authors isoften larger than in traditional settings.
We address this challenge by using topic models toobtain author representations.
In addition to exploring novel ways of applying two popular topicmodels to this task, we test our new model that projects authors and documents to two disjointtopic spaces.
Utilizing our model in authorship attribution yields state-of-the-art performance onseveral data sets, containing either formal texts written by a few authors or informal texts gener-ated by tens to thousands of on-line users.
We also present experimental results that demonstratethe applicability of topical author representations to two other problems: inferring the sentimentpolarity of texts, and predicting the ratings that users would give to items such as movies.1.
IntroductionAuthorship attribution has attracted much attention due to its many applications in,for example, computer forensics, criminal law, military intelligence, and humanitiesresearch (Juola 2006; Stamatatos 2009; Argamon and Juola 2011).
The traditional prob-lem, which is the focus of this article, is to attribute anonymous test texts to one ofa set of known candidate authors, whose training texts are supplied in advance (i.e.,supervised classification).
Whereas most of the early work on authorship attributionfocused on formal texts with only a few candidate authors, researchers have recently?
Faculty of Information Technology, Monash University, Clayton, Victoria 3800, Australia.E-mail: yanir.seroussi@monash.edu.??
Faculty of Information Technology, Monash University, Clayton, Victoria 3800, Australia.E-mail: ingrid.zukerman@monash.edu.?
Faculty of Information Technology, Monash University, Clayton, Victoria 3800, Australia.E-mail: fabian.bohnert@monash.edu.Submission received: 30 December 2012; revised submission received: 9 May 2013; accepted forpublication: 23 June 2013.doi:10.1162/COLI a 00173?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 2turned their attention to scenarios involving informal texts and tens to thousands ofauthors (Koppel, Schler, and Argamon 2011; Luyckx and Daelemans 2011).
In parallel,topic models have gained popularity as a means of discovering themes in such largetext corpora (Blei 2012).
This article explores authorship attribution with topic models,extending the work presented by Seroussi and colleagues (Seroussi, Zukerman, andBohnert 2011; Seroussi, Bohnert, and Zukerman 2012) by reporting additional experi-mental results and applications of topic-based author representations that go beyondtraditional authorship attribution.Topic models work by defining a probabilistic representation of the latent structureof corpora through latent factors called topics, which are commonly associated withdistributions over words (Blei 2012).
For example, in the popular Latent DirichletAllocation (LDA) topic model, each document is associated with a distribution overtopics, and each word in the document is generated according to its topic?s distributionover words (Blei, Ng, and Jordan 2003).
The word distributions often correspond toa human-interpretable notion of topics, but this is not guaranteed, as interpretabilitydepends on the corpus used for training the model.
Indeed, when we ran LDA on a dataset of movie reviews and message board posts, we found that some word distributionscorrespond to authorship style as reflected by authors?
vocabulary, with netspeakwords such as ?wanna,?
?alot,?
and ?haha?
assigned to one topic, and words such as?compelling?
and ?beautifully?
assigned to a different topic.
This finding motivatedour use of LDA for authorship attribution (Seroussi, Zukerman, and Bohnert 2011).One limitation of LDA is that it does not model authors explicitly.
This led us touse Rosen-Zvi et al.
?s (2004) Author-Topic (AT) model to obtain improved authorshipattribution results (Seroussi, Bohnert, and Zukerman 2012).
However, AT is also limitedin that it does not model documents.
We addressed this limitation through the DisjointAuthor-Document Topic (DADT) model?a topic model that draws on the strengths ofLDA and AT, while addressing their limitations by integrating them into a single model.Our DADT model extends the model introduced by Seroussi, Bohnert, and Zukerman(2012), which could only be trained on single-authored texts.
In this article, we providea detailed account of the extended model.
In addition, we offer experimental results forfive data sets, extending the results by Seroussi, Bohnert, and Zukerman (2012), whichwere restricted to two data sets of informal texts with many authors.
Our experimentsshow that DADT yields state-of-the-art performance on these data sets, which containeither formal texts written by a few authors or informal texts where the number ofcandidate authors ranges from 62 to about 20,000.Although our evaluation is focused on single-authored texts, AT and DADT canalso be used to model authors based on multi-authored texts, such as research papers.
Todemonstrate the potential utility of this capability of the models, we present the resultsof a preliminary study, where we use AT and DADT to identify anonymous reviewersbased on publicly available information (reviewer lists and the reviewers?
publications,which are often multi-authored).
Our results indicate that reviewers may be identifiedwith moderate accuracy, at least in small conference tracks and workshops.
We hopethat these results will help fuel discussions on the issue of reviewer anonymity.Our finding that topic models yield good authorship attribution performanceindicates that they capture aspects of authorship style, which is known to be indicativeof author characteristics such as demographic information and personality traits(Argamon et al.
2009).
This is in addition to the well-established result that topic modelscan be used to represent authors?
interests (Rosen-Zvi et al.
2004).
An implication ofthese results is that topic models may be used to obtain text-based representations ofusers in scenarios where user-generated texts are available.
We demonstrate this by270Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Modelsshowing how topic models can be utilized to improve the performance of methods wedeveloped to address the popular tasks of polarity inference and rating prediction.This article is structured as follows.
Section 2 surveys related work.
Section 3discusses LDA, AT, and DADT and the author representations they yield.
Section 4introduces authorship attribution methods, which are evaluated in Section 5.
Section 6presents applications of our topic-based approach, and Section 7 concludes the article.2.
Related WorkAuthorship attribution has a long history that predates modern computing.
For exam-ple, Mendenhall (1887) suggested that word length can be used to distinguish works bydifferent authors.
Modern interest in authorship attribution is commonly traced backto Mosteller and Wallace?s (1964) study on applying Bayesian statistical analysis offunction word frequencies to uncover the authors of the Federalist Papers (Juola 2006;Koppel, Schler, and Argamon 2009; Stamatatos 2009).
The interest in authorship attri-bution is due to its many applications in areas such as computer forensics, criminal law,military intelligence, and humanities research.
In recent years, authorship attributionresearch has been fuelled by advances in natural language processing, text mining,machine learning, information retrieval, and statistical analysis.
This has motivatedthe organization of workshops and competitions to facilitate the development andcomparison of authorship attribution methods (Juola 2004; Argamon and Juola 2011).In this article, we focus on the closed-set attribution task, where training texts bythe candidate authors are supplied in advance, and for each test text, the goal is toattribute the text to the correct author out of the candidate authors (Argamon and Juola2011).
Related tasks include open-set attribution, where some test texts may not havebeen written by any of the candidate authors, and verification, where texts by only onecandidate author are supplied in advance, and the task is to verify whether test textswere written by the candidate author (Koppel and Schler 2004; Sanderson and Guenter2006; Koppel, Schler, and Argamon 2011).Regardless of the task, a challenge currently faced by researchers in the field isaddressing scenarios with many candidate authors and varying amounts of data perauthor (Argamon and Juola 2011; Koppel, Schler, and Argamon 2011; Luyckx andDaelemans 2011).
This challenge is illustrated by the corpus chosen for the PAN?11competition (Argamon and Juola 2011), which contains short e-mails by tens of authors.Other examples are Koppel, Schler, and Argamon?s (2011) work on a corpus of blogposts by thousands of authors, and Luyckx and Daelemans?s (2011) study of the effectof the number of authors and training set size on authorship attribution performanceon data sets of student essays.
Our approach to authorship attribution addresses thischallenge by using topic models, which are known to successfully deal with varyingamounts of text (Blei 2012).We know of only one previous case where topic models were used for authorshipattribution of single-authored texts: Rajkumar et al.
(2009) reported preliminary resultson using LDA topic distributions as feature vectors for support vector machines (SVM),but they did not compare the results obtained with LDA-based SVM to those obtainedwith SVM trained on tokens only (we present the results of such a comparison inSection 5).
We know of two related studies that followed the publication of ourinitial LDA-based results (Seroussi, Zukerman, and Bohnert 2011): Wong, Dras andJohnson?s (2011) work on native language identification with LDA, and Pearl andSteyvers?s (2012) study of authorship verification where some of the features are topicdistributions.
Although Wong, Dras and Johnson reported only limited success (perhaps271Computational Linguistics Volume 40, Number 2because an author?s native language may manifest itself in only a few words, or maybedue to data-set-specific issues), Pearl and Steyvers found that topical representationshelped them achieve state-of-the-art verification accuracy.
Pearl and Steyvers?sfindings further strengthen our hypothesis that topic models yield meaningful authorrepresentations.
We take this observation one step further by defining our DADTmodel, and applying it to several authorship attribution scenarios, where it yieldsbetter performance than LDA-based approaches and methods based on the AT model(Section 5).A line of research that has garnered much interest in recent years is the definition ofgeneric topic models that incorporate metadata labels (Blei 2012).
These models canbe divided into two types: upstream models, which use the labels to constrain thetopics, and downstream models, which generate the labels from the topics (Mimnoand McCallum 2008).
Generic models have the appealing advantage of obviating theneed to define a new model for each new task (e.g., they may be used to obtainauthor representations by defining a metadata label for each author).
However, thisadvantage may come at the price of increased computational complexity or poorerperformance than that of task-specific models (Mimno and McCallum 2008).
As thefocus of our work is on modeling authors, we experimented only with LDA and withthe task-specific topic models discussed in Section 3 (AT and DADT, which modelauthors explicitly).
The applicability of generic models to authorship attribution is anopen question that would be interesting to investigate in the future.
Nonetheless, mostof the generic models surveyed here have properties that make them unsuitable forour purposes.Examples of generic upstream models include DiscLDA (Lacoste-Julien, Sha, andJordan 2008), Labeled LDA (Ramage et al.
2009), and DMR (Mimno and McCallum2008).
The former two dedicate at least one topic to each metadata label, making themtoo computationally expensive to use on data sets with thousands of authors, suchas the Blog and IMDb1M data sets (Section 5.1).
In contrast to DiscLDA and LabeledLDA, DMR uses less topics by sharing them between labels.
Mimno and McCallum(2008) showed that DMR outperformed AT on authorship attribution of multi-authoreddocuments.
Despite this, we decided to use AT, because we found in preliminary exper-iments that AT performs better than DMR on authorship attribution of single-authoredtexts.
Such texts are the main focus of this article.
Nonetheless, it is worth noting thatMimno and McCallum?s experiments were performed on a data set of research paperswhere stopwords were filtered out.
We do not discard stopwords in most experiments,because they are known to be indicators of authorship (Koppel, Schler, and Argamon2009).1A representative example of a generic downstream model is sLDA (Blei andMcAuliffe 2007), which generates labels from each document?s topic assignments viaa generalized linear model.
This model was extended by Zhu, Ahmed, and Xing (2009),who introduced MedLDA, where training is done in a way that maximizes the marginbetween labels, which is ?arguably more suitable?
for inferring document labels.
Zhu1 Following Salton (1981), we use the term stopwords to denote the most common words?we use hisEnglish stopword list from the SMART information retrieval system (Salton 1971), which is availablefrom www.lextek.com/manuals/onix/stopwords2.html.
Stopword lists typically include a set ofnon-content words, which is a superset of the function words in a given language.
Although Koppel,Schler, and Argamon (2009) found that good performance can be obtained by relying only on functionwords, they also showed that the data-driven approach of relying on the most common words in acorpus yields superior performance in most cases.
Either way, discarding stopwords is likely to yieldpoor results, as we show in Section 5.3.272Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Modelsand Xing (2010) further extended that work by introducing sCTRF, which combinessLDA with conditional random fields to accommodate arbitrary types of features.
Zhuand Xing applied these models to the polarity inference task, and found that supportvector regression outperformed sLDA and performed comparably to MedLDA (thesethree models used only unigrams), whereas sCTRF yielded the best performance byincorporating additional feature types (e.g., part-of-speech tags and a lexicon of positiveand negative words).
Based on these results, we decided to leave experiments withdownstream models for future work, as it seems unlikely that we would obtain goodresults on the authorship attribution task without considering other feature types inaddition to token unigrams (which is beyond the scope of this article).3.
Topic Models and Author RepresentationsThis section introduces notation, provides a discussion of the meaning of the parametersused by the topic models, and describes the three topic models considered in thisarticle (LDA, AT, and DADT), focusing on the author representations that they yield.3.1 Notation and PreliminariesWe denote matrices and stacked vectors in uppercase boldface italics (e.g., M), andvectors in lowercase boldface italics (e.g., v).
The element at the i-th row and j-th columnof a matrix M is denoted mij, and vector elements are denoted in lowercase italics witha subscript index (e.g., vi).
Sets are denoted with calligraphic font (e.g., S).
In addition,Dir and Cat denote the Dirichlet and categorical distributions, respectively.The values of the parameters that are given as input to the models are either deter-mined by the corpus (Section 3.1.1) or configured when using the models (Sections 3.1.2and 3.1.3).
Table 1 shows the models?
corpus-dependent and configurable parameterswith their lengths and meanings (scalars have length 1).
Corpus-dependent parametersare at the top, configurable document-related parameters are in the middle (for LDATable 1Corpus-dependent and configurable parameters.Symbol Length MeaningA 1 Number of authorsD 1 Number of documentsV 1 Vocabulary sizeNd 1 Number of words in document dT(D) 1 Number of document topics?
(D) T(D) Document topic prior?
(D) V Word in document topic prior?
(D) 1 Document words in document priorT(A) 1 Number of author topics?
(A) T(A) Author topic prior?
(A) V Word in author topic prior?
A Author in corpus prior?
(A) 1 Author words in document prior273Computational Linguistics Volume 40, Number 2and DADT), and configurable author-related parameters are at the bottom (for ATand DADT).3.1.1 Corpus-Dependent Parameters.
The following parameters depend on the corpus,and are thus considered to be observed:A: Number of authors.
We use a ?
{1, .
.
.
, A} to denote an author identifier.D: Number of documents.
We use d ?
{1, .
.
.
, D} to denote a document identifier.V: Vocabulary size.
We use v ?
{1, .
.
.
, V} to denote a unique word identifier.Nd: Number of words in document d. We use i ?
{1, .
.
.
, Nd} to denote a word indexin document d.A: Document authors.
This is a D-dimensional vector of vectors, where the d-thelement ad contains the authors of the d-th document.
In cases where the corpuscontains only single-authored texts, we use the scalar ad to denote the author ofthe d-th document, since ad is always of unit length.W : Document words.
This is a D-dimensional vector of vectors, where the d-thelement wd contains the words of the d-th document.
The vector wd is of length Nd,and wdi ?
{1, .
.
.
, V} is the i-th word in the d-th document.3.1.2 Number of Topics.
We make a distinction between document topics and author topics.In both cases, ?topics?
describe distributions over all the words in the vocabulary.
Thedifference is that document topics are word distributions that arise from documents,while author topics are word distributions that characterize the authors.
LDA uses onlydocument topics, whereas AT uses only author topics.
DADT, our hybrid model, usesboth document topics and author topics.All three models take the number of topics as a configurable parameter, denotedby T(D) for the number of document topics and by T(A) for the number of authortopics.
Although the models have other configurable parameters (introduced subse-quently), we found that the number of topics has the largest impact on model perfor-mance because it controls the overall model complexity.
For example, setting T(D) = 1in LDA means that all the words in all the documents are drawn from the sametopic (i.e., a single distribution for all the words), whereas setting T(D) = 200 gives LDAmuch more freedom to adapt to the corpus, as each word can be drawn from one of200 distributions.It is worth noting that techniques for determining the optimal number of topicshave been suggested.
For example, Teh et al.
(2006) used hierarchical Dirichlet processesto learn the number of topics while inferring the LDA model.
We did not experimentwith such techniques as they tend to complicate model inference, and we found thatusing a constant number of topics yields good performance.
Nonetheless, we note thatutilizing such techniques may be a worthwhile future research direction, especially todetermine the balance between document topics and author topics for DADT.3.1.3 Distribution Priors.
The following parameters are the priors of the Dirichlet and betadistributions used by the models.
In contrast to the number of topics, which controlsmodel complexity, the priors allow users of the models to specify their prior knowledgeand beliefs about the data.
In addition, the number of topics imposes a rigid constrainton the inferred model, whereas the effect of the priors on the model is expected to dimin-ish as the amount of observed data increases (Equations (3), (7), and (11)).
Indeed, wefound in our experiments that varying prior values had a small effect on performancecompared to varying the number of topics (Figure 8b, Section 5.3.4).274Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic ModelsThe priors are defined as follows (all vector elements and scalars are positive):?
(D): Document topic prior ?
a vector of length T(D).?
(D): Prior for words in document topics ?
a vector of length V.?
(A): Author topic prior ?
a vector of length T(A).?
(A): Prior for words in author topics ?
a vector of length V.?
(D): Document words in document prior.?
(A): Author words in document prior.?
: Author in corpus prior ?
a vector of length A.The support of a K-dimensional Dirichlet distribution Dir(?)
is the set of K-dimensional vectors with elements in the range [0, 1] whose sum is 1 (the Dirichletdistribution is a multivariate generalization of the beta distribution).
Hence, each drawfrom the Dirichlet distribution can be seen as defining the parameters of a categoricaldistribution.
This is illustrated by Figure 1, which shows the Dirichlet distributiondensity in the three-dimensional case for three different prior vectors ?
(the densityis triangular because the drawn vector elements have to sum to 1?each corner ofthe triangle corresponds to a dimension of the distribution, denoted 1, 2, and 3 inFigure 1).
When the prior vector is symmetric (i.e., all its elements have the samevalue), the density is also symmetric (Figures 1a and 1b).
Symmetric priors with elementvalues that are greater than 1 yield densities that are concentrated in the middle of thetriangle, meaning that categorical vectors with relatively uniform values are likely tobe drawn (Figure 1a).
On the other hand, symmetric priors with element values thatare less than 1 yield sparse densities with high values in the corners of the triangle,meaning that the categorical vectors are likely to have one element whose value isgreater than the other elements (Figure 1b).
Finally, when the prior is asymmetric,vectors that give higher probabilities to the elements with higher prior values are likelyto be drawn (Figure 1c).The document and author topic priors (?
(D) and ?
(A), respectively) encode ourbeliefs about the document and author topic distributions, respectively.
They are oftenset to be symmetric, because we have no reason to favor one topic over the other beforewe have seen the data (Steyvers and Griffiths 2007).
Wallach, Mimno, and McCallum(2009) argue that using asymmetric priors in LDA is beneficial, and suggest a methodthat learns such priors as part of model inference (by placing another prior on the ?
(D)prior).
We implemented Wallach, Mimno and McCallum?s method for all the modelswe considered, but found that it did not improve authorship attribution accuracy inpreliminary experiments.
Thus, in all our experiments we set the elements of ?
(D)and ?
(A) to min{0.1, 5/T(D)} and min{0.1, 5/T(A)}, respectively, yielding relativelysparse topic distributions, since we expect each document and author to be sufficiently12 312 312 3HighLow(a) ?
= (2, 2, 2) (b) ?
= (0.5, 0.5, 0.5) (c) ?
= (0.9, 0.5, 0.5)Figure 1Three-dimensional Dirichlet probability density, given three prior vectors.275Computational Linguistics Volume 40, Number 2represented by only a few topics.
This choice follows the recommendations fromLingPipe?s documentation (alias-i.com/lingpipe), which are based on empiricalevidence from several corpora.The priors for words in document and author topics (?
(D) and ?
(A), respectively)encode our beliefs about the word distributions.
As for the topic distribution priors,symmetric priors are often used, with a default value of 0.01 for all the vector ele-ments (yielding sparse word distributions, as indicated earlier), meaning that each topicis expected to assign high probabilities to only a few top words (Steyvers and Griffiths2007).
In contrast to the topic distribution priors, Wallach, Mimno, and McCallum (2009)found in their experiments on LDA that using an asymmetric ?
(D) was of no benefit.This is because using an asymmetric ?
(D) means that we encode a prior preferencefor a certain word to appear in all topics (e.g., a word represented by corner 1 inFigure 1c).
For the same reason, using a symmetric ?
(A) is a sensible choice for AT.
Incontrast to LDA and AT, our DADT model distinguishes between document words andauthor words, and thus uses both ?
(D) and ?
(A) as priors.
This allows us to encode ourprior knowledge that stopword use is indicative of authorship.
Thus, for DADT we set?
(D)v = 0.01?
e and ?
(A)v = 0.01 + e for all v, where v is a stopword (e can be set to zeroto obtain symmetric priors).DADT?s ?
(D) and ?
(A) priors encode our prior belief about the balance betweendocument words and author words in a given document.
Document words (drawnfrom document topics) are expected to be representative of the documents in thecorpus, whereas author words (drawn from author topics) characterize the authorsin the corpus.
For example, if we asked two different authors to write a report aboutLDA, both reports are likely to contain content words like Dirichlet, topic, and prior,but the frequencies of non-content words (i.e., function words and other indicators ofauthorship style) are likely to vary across the reports.
In this case, the content wordsare expected to be allocated to document topics, and the non-content words whoseusage varies across authors would be allocated to author topics.
In cases where theauthors write about different issues, DADT may allocate some content words to authortopics (i.e., the meaning of DADT?s topics is expected to be corpus-specific).
Accordingto DADT?s definition (Section 3.4.1), which uses the beta distribution, the prior expectedvalue of the portion of each document that is composed of author words is?(A)?
(A) + ?
(D) (1)with a variance of?(A)?(D)(?
(A) + ?
(D))2 (?
(A) + ?
(D) + 1) (2)In our experiments, we chose values for ?
(D) and ?
(A) by deciding on the expected valueand variance, and solving these equations for ?
(D) and ?
(A).Finally, DADT?s ?
prior determines the prior belief about an author having written adocument (without looking at the actual words in the document).
This prior is only usedon documents with unobserved authors (i.e., when attributing authors to anonymoustexts).
Because we have no reason to favor one author over the other, we use a uniformprior, setting ?a = 1 for each author a.276Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Modelswdi DNdzdi?
D?d(D)(D)?
T?t (D)(D) (D)Figure 2Latent Dirichlet allocation (LDA).3.2 LDA3.2.1 Model Definition.
LDA was originally defined by Blei, Ng, and Jordan (2003).
Herewe describe Griffiths and Steyvers?s (2004) extended version.
The idea behind LDA isthat each document in a corpus is described by a distribution over topics, and each wordin the document is drawn from its topic?s word distribution.
Figure 2 presents LDA inplate notation, where observed variables are in shaded circles, unobserved variables arein unshaded circles, and each box represents repeated sampling, with the number ofrepetitions at the bottom-right corner.
Formally, the generative process is: (1) for eachtopic t, draw a word distribution?
(D)t ?
Dir(?
(D)); (2) for each document d, draw a topicdistribution ?
(D)d ?
Dir(?
(D)); and (3) for each word index i in each document d, draw atopic zdi ?
Cat (?
(D)d ), and the word wdi ?
Cat (?
(D)zdi ).3.2.2 Model Inference.
Topic models are commonly inferred using either collapsed Gibbssampling (Griffiths and Steyvers 2004; Rosen-Zvi et al.
2004) or methods based on vari-ational inference (Blei, Ng, and Jordan 2003).
We use collapsed Gibbs sampling to inferall models due to its efficiency and ease of implementation.
This involves repeatedlysampling from the conditional distribution of the latent parameters, which is obtainedanalytically by marginalizing over the topic and word distributions, and using the prop-erties of conjugate priors.
This conditional distribution is given in Equation (3) (Griffithsand Steyvers 2004; Steyvers and Griffiths 2007):p(zdi = t|W ,Z?di;?
(D),?
(D))??
(D)t + c(DT)dt?T(D)t?=1(?(D)t?
+ c(DT)dt?)?
(D)wdi + c(DTV)twdi?Vv=1(?
(D)v + c(DTV)tv) (3)where W is the corpus; Z?di contains all the topic assignments, excluding the assign-ment for the i-th word of the d-th document; c(DT)dt is the count of topic t in document d;and c(DTV)twdi is the count of word wdi in document topic t. Here, these counts excludethe di-th topic assignment (i.e., zdi).Commonly, several Gibbs sampling chains are run, and several samples are retainedfrom each chain after a burn-in period, which allows the chain to reach its stationarydistribution.
For each sample, the topic distributions and the word distributions areestimated using their expected values, given the topic assignments Z.
These expectedvalues are given in Equations (4) and (5):E[?
(D)dt |Z] =?
(D)t + c(DT)dt?T(D)t?=1(?(D)t?
+ c(DT)dt?)
(4)277Computational Linguistics Volume 40, Number 2E[?
(D)tv |Z] =?
(D)v + c(DTV)tv?Vv?=1(?(D)v?
+ c(DTV)tv?)
(5)where in this case the counts are over the full topic and author assignments.
Thetwo equations take a similar form due to the fact that the Dirichlet distribution isthe conjugate prior of the categorical distribution (Griffiths and Steyvers 2004).
Notethat these values cannot be averaged across samples due to the exchangeability of thetopics (Steyvers and Griffiths 2007) (e.g., topic 1 in one sample is not necessarily thesame as topic 1 in another sample).The examined authorship attribution problem follows a supervised classificationsetup, where training texts with known candidate authors are given in advance.
Testtexts are classified one by one, and the goal is to attribute each test text to one of thecandidate authors.
As the word distributions of the LDA model inferred in the trainingphase are unlikely to change much due to the addition of a single test document, in theclassification phase we consider each topic?s word distribution to be observed, settingit to its expected value according to Equation (5).
This yields the following samplingequation for a given test text w?
(w?
is a word vector of length N?
):p(z?i = t|w?, z??i;?
(D),?
(D))??
(D)t + c?(DT)t?T(D)t?=1(?(D)t?
+ c?(DT)t?)?
(D)tw?i(6)where z?i is the topic assignment for the i-th word in w?, z?
?i contains all of w?
?s topicassignments except for the i-th assignment, and c?
(DT)t is the count of words assigned totopic t, excluding the i-th assignment.As done in the training phase, we set the test text?s topic distribution ??
(D) to itsexpected value according to Equation (4), where c(DT)dt is replaced with c?
(DT)t (which nowcontains the counts over the full vector of topic assignments z?).
Note that because weassume that the ?
(D)t values are observed in the classification phase, the topics are notexchangeable.
This means that we can average the E[??
(D)t |z?]
values across test samplesobtained from the same sampling chain.3.2.3 Author Representations.
LDA does not directly model authors, but it can still be usedto obtain valuable information about them.
The output of LDA consists of distributionsover topics ?
(D)d for each document d. As the number of topics T(D) is commonly muchsmaller than the size of the vocabulary V, these topical representations form a lower-dimensional representation of the corpus.
The LDA-based author representation weconsider in this article is LDA-M (LDA with multiple documents per author), whereeach author a is represented as the set of distributions over topics of their documents,namely, the set {?
(D)d |ad = a}, where ad is the author of document d. An alternativeapproach is LDA-S (LDA with a single document per author), where each author?sdocuments are concatenated into a single document in a preprocessing step, LDA is runon the concatenated documents, and each author is represented by a single distributionover topics (the distribution of the concatenated document).An advantage of LDA-S over LDA-M is that LDA-S yields a much more compactauthor representation than LDA-M, especially for authors who wrote many documents.However, this compactness may come at the price of accuracy, as markers that maybe present only in a few short documents by one author may lose their prominence278Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Modelsif these documents are concatenated with longer documents.
It is worth noting thatconcatenating each author?s documents into one document has been named theprofile-based approach in previous authorship attribution studies, in contrast to theinstance-based approach, where each document is considered separately (Stamatatos2009).A limitation of these representations is that they apply only to corpora of single-authored documents, and there is no straightforward way of extending them to considermulti-authored documents.
This limitation is addressed by AT, which we present inthe next section.
Note that when analyzing single-authored documents, the authorrepresentations yielded by AT are equivalent to LDA-S?s representations.
Therefore, wedo not report results obtained with LDA-S.
Nonetheless, practitioners may find it easierto use LDA-S than AT due to the relative prevalence of LDA implementations (in fact,our initial modeling approach was LDA-S for exactly this reason).3.3 AT3.3.1 Model Definition.
AT was introduced by Rosen-Zvi et al.
(2004) to model authorinterests in corpora of multi-authored texts (e.g., research papers).
The main ideabehind AT is that each document is generated from the topic distributions of itsobserved authors, rather than from a document-specific topic distribution.
Figure 3presents AT in plate notation.
Formally, the generative process is: (1) for each topic t,draw a word distribution ?
(A)t ?
Dir(?
(A)); (2) for each author a, draw a topicdistribution ?
(A)a ?
Dir(?
(A)); and (3) for each word index i in each document d, drawan author xdi uniformly from the document?s set of authors ad, a topic zdi ?
Cat(?
(A)xdi),and the word wdi ?
Cat(?
(A)zdi).3.3.2 Model Inference.
As for LDA, we use collapsed Gibbs sampling to infer AT.
Thisinvolves repeatedly sampling from Equation (7) (Rosen-Zvi et al.
2004, 2010):p(xdi = a,zdi = t???
?A,W ,X?di,Z?di;?
(A),?
(A))??
(A)t + c(AT)at?T(A)t?=1(?(A)t?
+ c(AT)at?)?
(A)wdi + c(ATV)twdi?Vv=1(?
(A)v + c(ATV)tv) (7)where X?di and Z?di are all the author and topic assignments, respectively, excludingthe assignment for the i-th word of the d-th document; c(AT)at is the count of topic tassignments to author a; and c(ATV)twdi is the count of word wdi in author topic t. Here,all the counts exclude the di-th assignments (i.e., xdi and zdi).
We sample xdi and zdiwdi DNdzdixdiDadAT?t (A)(A)?a(A)?(A)?
(A)Figure 3The Author-Topic (AT) model.279Computational Linguistics Volume 40, Number 2jointly because this yields faster convergence than separate sampling (Rosen-Zvi et al.2010).Similarly to LDA, we estimate the topic and word distributions using their expectedvalues given the author assignments X and the topic assignments Z:E[?
(A)at |X,Z] =?
(A)t + c(AT)at?T(A)t?=1(?(A)t?
+ c(AT)at?)
(8)E[?
(A)tv |Z] =?
(A)v + c(ATV)tv?Vv?=1(?(A)v?
+ c(ATV)tv?)
(9)where in this case the counts are over the full author and topic assignments.In the classification phase, we do not know the author a?
of the test text w?
(weassume that test texts are single-authored).
If we did, no sampling would be required toobtain a?
?s topic distribution because it is already inferred in the training phase (Equa-tion (8)).
Hence, we assume that a?
is a ?new,?
previously unknown author, and utilizeGibbs sampling to infer this author?s topic distribution ??
(A) by repeatedly samplingfrom Equation (10) (as for LDA, the word distributions are assumed to be observed andset to their expected values according to Equation (9)):p(z?i = t|w?, z??i;?
(A),?
(A))??
(A)t + c?(AT)t?T(A)t?=1(?(A)t?
+ c?(AT)t?)?
(A)tw?i(10)where z?i is the topic assignment for the i-th word in w?, z?
?i contains all of w?
?s topicassignments except for the i-th assignment, and c?
(AT)t is the count of topic t assignmentsto author a?
(excluding the i-th assignment).
Similarly to LDA, we then set ??
(A) to itsexpected value according to Equation (8), where c(AT)at is replaced with c?
(AT)t over the fullassignment vector z?.3.3.3 Author Representations.
AT naturally yields author representations in the form ofdistributions over topics.
That is, each author a is represented as a distribution overtopics ?
(A)a .
However, AT is limited because all the documents by the same authorsare generated in an identical manner (Section 3.3.1).
To address this limitation, Rosen-Zvi et al.
(2010) introduced ?fictitious?
authors, adding a unique ?author?
to eachdocument.
This allows AT to adapt itself to each document without changing themodel specification.
Therefore, we consider the two following variants: (1) AT: ?Pure?AT, without fictitious authors; and (2) AT-FA: AT, when run with the additional pre-processing step of adding a fictitious author to each document.3.4 DADT3.4.1 Model Definition.
Our DADT model can be seen as a combination of LDA andAT, which is meant to address the weaknesses of both models while retaining theirstrengths.
The main idea behind DADT is that words are generated from two disjointsets of topics: document topics and author topics.
Words generated from documenttopics follow the same generation process as in LDA, whereas words generated from280Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Modelsauthor topics are generated in an AT-like fashion.
This approach has the potential ben-efit of separating ?document?
words from ?author?
words.
That is, words whose usevaries across documents are expected to be found in document topics, whereas wordswhose use varies between authors are expected to be assigned to author topics.
Figure 4presents the graphical representation of the model, where the document-dependentparameters appear on the left-hand side, and the author-dependent parameters appearon the right-hand side.
Formally, the generative process is as follows (we mark eachstep as coming from either LDA or AT, or as new in DADT).Corpus level:L. For each document topic t, draw a word distribution ?
(D)t ?
Dir(?
(D)).A.
For each author topic t, draw a word distribution ?
(A)t ?
Dir(?
(A)).A.
For each author a, draw an author topic distribution ?
(A)a ?
Dir(?
(A)).D.
Draw an author distribution ?
?
Dir(?
).Document level.
For each document d:L. Draw d?s document topic distribution ?
(D)d ?
Dir(?
(D)).D.
Draw d?s author set ad by repeatedly sampling without replacement from Cat (?).D.
Draw d?s author/document topic ratio pid ?
Beta(?
(A), ?
(D)).Word level.
For each word index i ?
{1, .
.
.
, Nd}:D. Draw the author/document topic indicator ydi ?
Bernoulli(pid).L.
If ydi = 0, use document topics: draw a topic zdi ?
Cat(?
(D)d), and theword wdi ?
Cat(?(D)zdi).A.
If ydi = 1, use author topics: Draw an author xdi uniformly from ad,a topic zdi ?
Cat(?
(A)xdi), and the word wdi ?
Cat(?
(A)zdi).It is worth noting that drawing the document?s author set can also be modeled assampling from Wallenius?s noncentral hypergeometric distribution (Fog 2008) with aweight vector ?
and a parameter vector whose elements are all equal to 1.
In this article,we consider only situations where ad is observed when the model is inferred.
Whenhandling documents with unknown authors in our authorship attribution experiments,we assume that all anonymous texts are single-authored.
(D) (A)wdi?
T DNdzdi?t?
D?dydiADad ???d?
(D)(D)(D)(D) (D)??(A)?
(A)T?t (A)(A)?a(A)xdiFigure 4The Disjoint Author-Document Topic (DADT) model.281Computational Linguistics Volume 40, Number 23.4.2 Model Inference.
We infer DADT using collapsed Gibbs sampling, as done for LDAand AT.
This involves repeatedly sampling from the following conditional distributionof the latent parameters:p(xdi = a,ydi = y, zdi = t???
?A,W ,X?di,Y?di,Z?di;?
(D),?
(D), ?(D),?
(A),?
(A), ?(A))?
(11)???????????(?
(D) + c(DD)d)?
(D)t +c(DT)dt?T(D)t?=1(?(D)t?+c(DT)dt?)?(D)wdi+c(DTV)twdi?Vv=1(?
(D)v +c(DTV)tv) if y = 0(?
(A) + c(DA)d)?
(A)t +c(AT)at?T(A)t?=1(?(A)t?+c(AT)at?)?(A)wdi+c(ATV)twdi?Vv=1(?
(A)v +c(ATV)tv) if y = 1where Y?di contains the topic indicators, excluding the di-th value; and c(DD)d and c(DA)dare the counts of words assigned to document or author topics in document d, respec-tively.
The other variables are defined as for LDA and AT.
Here, all the counts excludethe di-th assignments (i.e., xdi, ydi, and zdi).The building blocks of our DADT model are clearly visible in Equation (11).
LDA?sEquation (3) is contained in the y = 0 case, where the word is drawn from documenttopics, and AT?s Equation (7) is contained in the y = 1 case, where the word is drawnfrom author topics.
However, Equation (11) also demonstrates the main differencebetween DADT and its building blocks, as DADT considers both documents and authorsduring the inference process by assigning each word to either a document topic or anauthor topic, where document topics and author topics come from disjoint sets.As for LDA and AT, we ran several sampling chains in our experiments, retainingsamples from each chain after a burn-in period (a sample consists of X, Y, and Z).For each sample, the topic and word distributions are estimated using their expectedvalues given the latent variable assignments.
The expected values for the topic andword distributions are the same as for LDA and AT, and the expected values for theauthor/document ratio and the corpus author distribution are:E[pid|Y] =?
(A) + c(DA)d?
(D) + ?
(A) + Nd(12)E[?a|W] =?a + c(AD)a?Aa?=1(?a?
+ c(AD)a?)
(13)where the counts are now over the full assignments X, Y, and Z.
As in LDA and AT,these equations were straightforward to obtain, because the Dirichlet distribution is theconjugate prior of the categorical distribution and the beta distribution is the conjugateprior of the Bernoulli distribution.
It is worth noting that because we assume that thedocuments?
authors are observed during model inference, the expected value of eachelement of the corpus distribution over authors ?a does not vary across samples, as itonly depends on the prior ?a and on author a?s count of documents in the corpus c(AD)a .In the classification phase, we do not know the author a?
of the test text w?.
As forAT, we assume that a?
is a previously unknown author.
We set the word distributions totheir expected values from the training phase, and infer a?
?s author topic distribution ??
(A)282Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Modelstogether with the test text?s document topic distribution ??
(D) and author/documenttopic ratio p?i by repeatedly sampling fromp(y?i = y, z?i = t|w?, y?
?i, z??i;?
(D),?
(A),?
(D),?
(A), ?
(D), ?(A))?
(14)???????????(?
(D) + c?
(DD)) ?
(D)t +c?(DT)t?T(D)t?=1(?(D)t?+c?(DT)t?)?
(D)tw?iif y = 0(?
(A) + c?
(DA)) ?
(A)t +c?(AT)t?T(A)t?=1(?(A)t?+c?(AT)t?)?
(A)tw?iif y = 1where y?i is the topic indicator for the i-th word, y?
?i contains all of w?
?s topic indicatorsexcept for the i-th indicator, and c?
(DD) and c?
(DA) are the counts of words assignedto document and author topics, respectively, excluding the i-th assignment (the othervariables are defined as in Equations (6) and (10)).
The expected values of ??
(D) and ??
(A)are the same as for LDA and AT, respectively.
The expected value of p?i is obtained byreplacing c(DA)d and Nd with c?
(DA) and N?
in Equation (12) (where c?
(DA) now contains thecounts over the full vector of indicators y?
).3.4.3 Author Representations and Comparison to LDA and AT.
DADT can be seen as a gen-eralization of LDA and AT?setting DADT?s number of author topics T(A) to zero yieldsa model that is equivalent to LDA, and setting the number of document topics T(D) tozero yields a model that is equivalent to AT.
An advantage of DADT over LDA andAT is that both documents and authors are accounted for in the model?s definition, andare represented via distributions over document and author topics, respectively.
Hence,preprocessing steps such as concatenating each author?s documents or adding fictitiousauthors?as done in LDA-S and AT-FA to obtain author and document representations,respectively?are unnecessary.Of the LDA and AT variants presented in Sections 3.2.3 and 3.3.3, DADT mightseem most similar to AT-FA.
However, there are several key differences between DADTand AT-FA.First, in DADT, author topics are disjoint from document topics, with different priors foreach topic set.
Thus, the number of author topics T(A) can be different from the numberof document topics T(D), which enables us to vary the number of author and documenttopics according to the number of authors and documents in the corpus.
For example,in the judgment data set (Section 5.1.1), which includes only a few authors that wrotemany long documents, we expect that small values of T(A) compared to T(D) wouldsuffice to get good author representations.
By contrast, modeling the 19,320 authors ofthe Blog data set (Section 5.1.5) is expected to require many more author topics.
On suchlarge data sets, using more than a few hundred topics may become too computationallyexpensive, because adding topics increases model complexity and thus adds to theruntime of the inference algorithm.
Hence, being able to specify the balance betweendocument and author topics in such cases is beneficial (Section 5.4).Second, DADT places different priors on the word distributions for author topics anddocument topics (?
(A) and ?
(D), respectively).
We know from previous work that stop-words are strong indicators of authorship (Koppel, Schler, and Argamon 2009).
Ourmodel allows us to encode this prior knowledge by giving elements that correspondto stopwords in ?
(A) higher weights than such elements in ?
(D).
We found that thisproperty of DADT has practical benefits, as it improved the accuracy of DADT-basedauthorship attribution methods in our experiments (Section 5).283Computational Linguistics Volume 40, Number 2Third, DADT learns the ratio between document words and author words on a per-document basis, and makes it possible to specify a prior belief of what this ratioshould be.
We show that this has practical benefits in our authorship attributionexperiments (Section 5): Specifying a prior belief that on average about 80% of eachdocument is composed of author words can yield better results than using AT?s fic-titious author approach that evenly splits each document into author and documentwords.Fourth, DADT defines the process that generates authors.
This allows us to consider thenumber of texts by each author when performing authorship attribution.
In addition,this enables the use of DADT in a semi-supervised setup by training on documentswith unknown authors?an extension that is left for future work.4.
Authorship Attribution MethodsThis section introduces the authorship attribution methods considered in this article.
InSection 4.1, we discuss our baseline method (SVM trained on tokens), and Sections 4.2,4.3, 4.4, and 4.5 introduce methods based on LDA, AT, AT-FA, and DADT, respectively.These methods are summarized in Table 2.We consider two approaches to using topic models in authorship attribution:dimensionality reduction and probabilistic.Under the dimensionality reduction approach, the original documents are con-verted to topic distributions, and the topic distributions are used as input to a classifier.Generally, this approach makes it possible to use classifiers that are too computationallyexpensive to use with a large feature set, e.g., Webb, Boughton and Wang?s (2005) AODEclassifier, whose time complexity is quadratic in the number of features.
We use thereduced document representations as input to SVM, and compare their performancewith the performance obtained with SVM trained directly on tokens (denoted TokenSVM).
This allows us to roughly gauge how much information is lost by converting textsfrom token representations to topic representations.
However, this approach ignores theprobabilistic nature of the underlying topic model, and thus does not fully test the utilityof the author representations yielded by the model?these are better tested by the nextapproach.Table 2Summary of authorship attribution methods.Method DescriptionToken SVM Baseline: SVM trained on token frequenciesLDA-SVM SVM trained on LDA document topic distributionsAT-SVM SVM trained on AT author topic distributionsAT-P Probabilistic attribution with ATAT-FA-SVM SVM trained on AT-FA author topic distributions (real and fictitious)AT-FA-P1 Probabilistic attribution with AT-FA (classification without fictitious authors)AT-FA-P2 Probabilistic attribution with AT-FA (classification with fictitious authors)DADT-SVM SVM trained on DADT document and author topic distributionsDADT-P Probabilistic attribution with DADT284Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic ModelsIn contrast to dimensionality reduction methods, probabilistic methods utilize theunderlying model?s definitions directly to estimate the probability that a given authorwrote a given test text.
These methods require the model to be aware of authors, whichmeans that LDA cannot be used in this case.
We expect this approach to outperformthe dimensionality reduction approach because the probabilistic approach considers thestructure of the topic model.An alternative approach that we considered uses a distance measure (e.g., Hellingerdistance) to find the author whose topic distributions are closest to the distributionsinferred from the test text.
We do not describe distance-based methods in this articlebecause we found that they yield poor results in most cases (Seroussi 2012), probablybecause they do not fully consider the underlying structure of the topic model.4.1 Baseline: Token SVMOur baseline method is SVM trained on token frequency features (i.e., token countsdivided by the total number of tokens in the document).
This method is known to yieldstate-of-the-art authorship attribution performance on this feature set; that is, whencomparing methods without any further feature engineering, Token SVM is expectedto yield good performance with minimal tuning (Koppel, Schler, and Argamon 2009).We use the one-versus-all setup to handle non-binary authorship attribution scenarios.This setup scales linearly in the number of authors and was shown to be at least aseffective as other multi-class SVM approaches in many cases (Rifkin and Klautau 2004).It is worth noting that unlike the topic models, the Token SVM baseline is trainedwith the goal of maximizing the authorship attribution accuracy, which may give TokenSVM an advantage over topic-based methods.
Further, as a discriminative classificationapproach, SVM may yield better performance than probabilistic topic-based methods,which are generative classifiers (Ng and Jordan 2001).
However, as demonstrated byNg and Jordan?s comparison of discriminative and generative classifiers, this betterperformance may only be obtained in the presence of ?enough?
training data (just howmuch data is ?enough?
depends on the data set).4.2 Methods Based on LDA4.2.1 Dimensionality Reduction: LDA-SVM.
Using LDA for dimensionality reduction isrelatively straightforward?all it entails is converting the training and test texts totopic distributions as described in Section 3.2.2, and using these topic distributions asclassifier features.
Because we use SVM, it is possible to directly compare the resultsobtained with the LDA-SVM method to the baseline results obtained by running SVMtrained directly on token frequencies.This LDA-SVM approach was utilized by Blei, Ng, and Jordan (2003) to demonstratethe dimensionality reduction capabilities of LDA on the task of classifying articlesaccording to a set of predefined categories.
To the best of our knowledge, only Rajkumaret al.
(2009) have previously applied LDA-SVM to authorship attribution?they pub-lished preliminary results obtained by running LDA-SVM, but did not compare theirresults to a Token SVM baseline.
In Section 5, we present the results of more extensiveexperiments on the applicability of this approach to authorship attribution.4.3 Methods Based on AT4.3.1 Dimensionality Reduction: AT-SVM.
We cannot use AT to obtain document topicdistributions, because AT only infers author topic distributions (Section 3.3).
Hence,we train the SVM component on the author topic representations (each document is285Computational Linguistics Volume 40, Number 2converted to its author topic distribution).
For each test text, we assume that it waswritten by a previously unknown author, infer this author?s topic distribution ??
(A)(Sec-tion 3.3.2), and classify this distribution.
This may be seen as very radical dimensionalityreduction, because each author?s entire set of training documents is reduced to a singleauthor topic distribution.4.3.2 Probabilistic: AT-P. For each author a, AT-P calculates the probability of the test textwords given the AT model inferred from the training texts, under the assumption thatthe test text was written by a.
It returns the author for whom this probability is thehighest:arg maxa?{1,...,A}p(w?|a?
= a,?
(A),?
(A))?
arg maxa?{1,...,A}N??i=1T(A)?t=1?
(A)at ?
(A)tw?i(15)This method does not require any topic inference in the classification phase, because theauthor topic distributions?
(A) and topic word distributions ?
(A) are already inferred attraining time.
It is worth noting that we use the log of this probability for reasons ofnumerical stability.As mentioned at the beginning of this section, we expect AT-P to outperformAT-SVM because AT-P relies directly on the probabilistic structure of the AT model.In addition, AT-P has the advantage of not requiring any topic inference in the classifi-cation phase.We also performed preliminary experiments with a method that: (1) assumes thatthe test text was co-written by all the candidate authors, (2) infers the word-to-authorassignments for the test text, and (3) returns the author that was attributed the mostwords.
However, we found that this method performs poorly in comparison with otherAT-based approaches in three-way authorship attribution.
In addition, this method wastoo computationally expensive to run in cases with many authors, as it requires iteratingthrough all the authors for every test text word in each sampling iteration.4.4 Methods Based on AT-FAAT-FA is the same model as AT, but it is run with the preprocessing step of addingan additional fictitious author to each training document.
Hence, different constraintsapply to AT-FA in the classification phase.
This is because in this phase, we cannotconserve AT-FA?s assumption that all the texts are written by a real author togetherwith a fictitious author, since we do not know who wrote the test text.
Hence, if wewere to assume that the real author is a previously unknown author, as done for AT,we would have no way of telling the previously unknown author from the fictitiousauthor, because they are both unique to the test text.
We consider two possible ways ofaddressing this:1.
Assume that the test text was written only by a real, previously unknown,author (without a fictitious author), and infer this author?s topic distribu-tion ??
(A) (as in AT).2.
For each training author a, assume that the test text was written by a together witha fictitious author fa and infer the fictitious author?s topic distribution ??(A)fa.
Thisresults in a set of fictitious author topic distributions, each matching a trainingauthor.286Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic ModelsAlthough the second alternative may appear more attractive because it does not violatethe fictitious author assumption of AT-FA, we cannot use it with the dimensionalityreduction method (AT-FA-SVM, as described in the following section), as this methodrequires inferring the topic distribution of the previously unknown author ??
(A).4.4.1 Dimensionality Reduction: AT-FA-SVM.
AT-FA yields a topic distribution for eachtraining document (i.e., the topic distribution of the fictitious author associated withthe document), and a topic distribution for each real author (all the distributions areover the same topic set).
We convert each training document to the concatenation ofthese two distributions, and use this concatenation as input to the SVM component.
Inthe classification phase, we assume that the test text was written by a single previouslyunknown author, and represent the test text as the concatenation of the inferred topicdistribution ??
(A) to itself.It is worth noting that our DADT model offers a more elegant solution than con-catenating the same distribution to itself, because DADT differentiates between authortopics and document topics?a distinction that AT-FA attempts to capture throughfictitious authors.
Hence, we expect the DADT-SVM approach, which we define inSection 4.5, to perform better than AT-FA-SVM.
Nonetheless, we also experiment withAT-FA-SVM for the sake of completeness.4.4.2 Probabilistic: AT-FA-P. For the probabilistic approach, we consider two variants,matching the two alternatives outlined earlier.1.
AT-FA-P1.
This variant is identical in the classification phase to AT-P?it returnsthe author that maximizes the probability of the test text?s words according toEquation (15), assuming that the test text was not co-written by a fictitious author.2.
AT-FA-P2.
This variant performs the following steps for each author a: (1) assumethat the test text was written by a and a fictitious author fa; (2) infer the topicdistribution of the fictitious author ??
(A)fa ; (3) calculate the probability of the testtext words under the assumption that it was written by a and fa, and given theinferred ??
(A)fa ; and (4) return the author for whom the probability of the test textwords is maximized:arg maxa?
{1,...,A}p(w?|a?, ??
(A)fa ,?(A),?
(A))?
arg maxa?{1,...,A}N??i=1T(A)?t=1(?
(A)at ?
(A)tw?i+ ??
(A)fat ?
(A)tw?i)(16)where a?
= {a, fa} is the test text?s set of authors.The problem with this approach is that it is too computationally expensive touse on data sets with many candidate authors, as it requires running a separateinference procedure for each author.
Nonetheless, in cases where AT-FA-P2 can berun, we expect it to perform better than AT-FA-P1 because it does not violate thefictitious author assumption of AT-FA.4.5 Methods Based on DADT4.5.1 Dimensionality Reduction: DADT-SVM.
DADT yields a document topic distribu-tion ?
(D)d for each document d, and an author topic distribution ?
(A)a for each author a.Similarly to AT-FA-SVM, we convert each training document to the concatenation ofthese two distributions, and use this concatenation as input to the SVM component.287Computational Linguistics Volume 40, Number 2In contrast to AT-FA, DADT?s document topic distributions are defined over atopic set that is disjoint from the author topic set.
This makes it possible to assumethat the test text was written by a previously unknown author, and obtain the testtext?s document distribution ??
(D) together with the previously unknown author?s topicdistribution ??
(A) (following the procedure described in Section 3.4.2).
As in the trainingphase, test texts are represented as the concatenation of these two distributions.We expect DADT-SVM to outperform AT-FA-SVM, because we are able to main-tain the assumptions of DADT in the classification phase, which we cannot do inAT-FA-SVM.
Further, DADT-SVM should perform better than AT-SVM, because DADT-SVM accounts for differences between individual documents, whereas AT-SVM repre-sents each author using a single training instance.
Hypothesizing about the expectedperformance of DADT-SVM in comparison to LDA-SVM is harder: We expect per-formance to be corpus-dependent to a certain degree?in data sets where differencesbetween individual documents are important, LDA-SVM may have an advantage, as allthe words are allocated to document topics.
On the other hand, in data sets where thedifferences between authors are more important, DADT-SVM may outperform LDA-SVM because it represents the authors explicitly.4.5.2 Probabilistic: DADT-P.
This method assumes that the test text was written by apreviously unknown author, infers the test text?s document topic distribution ??
(D) andthe author/document topic ratio p?i, and returns the most probable author according tothe following equation:arg maxa?{1,...,A}p(a?
= a|w?, p?i, ??
(D), ?
(A)a ,?
(D),?
(A),?a)?
(17)arg maxa?{1,...,A}?aN??i=1??p?iT(A)?t=1?
(A)at ?
(A)tw?i+ (1?
p?i)T(D)?t=1??
(D)t ?(D)tw?i?
?It is worth noting that in preliminary experiments, we found that an alternativeapproach that avoids sampling p?i and ??
(D) by setting p?i = 1 yields poor performance,probably because it ?forces?
all the words to be author words, including words that arevery likely to be document words.
In addition, we found that following an approachwhere p?i and ??
(D) are sampled separately for each author (similarly to AT-FA-P2) yieldscomparable performance to sampling only once by following the previously-unknownauthor assumption.
However, the former approach is too computationally expensiveto run on data sets with many candidate authors.
Hence, we present only the resultsobtained with the approach that performs sampling only once.5.
EvaluationThis section presents the results of our evaluation.
We first describe the data sets weused (Section 5.1) and our experimental setup (Section 5.2), followed by the results ofour experiments on the Judgment and PAN?11 data sets (Section 5.3).
Then, we presentthe results of a more restricted set of experiments on the larger IMDb62, IMDb1M, andBlog data sets (Section 5.4) and summarize our key findings (Section 5.5).288Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models5.1 Data SetsWe experimented with five data sets: Judgment, PAN?11, IMDb62, IMDb1M, andBlog.
Judgment, IMDb62, and IMDb1M were collected and introduced by us, andare freely available for research use (Judgment can be downloaded from www.csse.monash.edu.au/research/umnl/data, and IMDb62 and IMDb1M are available uponrequest).
The two other data sets were introduced by other researchers, are publiclyavailable, and were used to facilitate comparison between our methods and previouswork.
Table 3 presents some data set statistics.5.1.1 Judgment.
The Judgment data set contains judgments by three judges who servedon the Australian High Court from 1913 to 1975: Dixon, McTiernan, and Rich (ab-breviated to D, M, and R, respectively, in Table 3).
We created this data set to verifyrumors that Dixon ghost-wrote some of the judgments attributed to McTiernan andRich (Seroussi, Smyth, and Zukerman 2011).
This data set is an example of a traditionalauthorship attribution data set, as it contains only three authors who wrote relativelylong texts in a formal language.
In this article, we only use judgments with undisputedauthorship, which were written in periods when only one of the three judges servedon the High Court (Dixon?s 1929?1964 judgments, McTiernan?s 1965?1975 judgments,and Rich?s 1913?1928 judgments).
We removed numbers from the texts to ensure thatdates cannot be used to discriminate between judges.
We also removed quotes to ensurethat the classifiers take into account only the actual authors?
language use (removal wasdone automatically by matching regular expressions for numbers and text in quotationmarks).
Because all three judges dealt with various topics, it is likely that successfulmethods would have to consider each author?s style, rather than rely solely on contentfeatures in the texts.As Table 3 shows, the Judgment data set contains the smallest number of authors ofthe data sets we considered, but these authors wrote more texts than the average authorin PAN?11, IMDb1M, and Blog.
Judgments are also substantially longer than the texts inall the other data sets, which should make authorship attribution on the Judgment dataset relatively easy.Table 3Data set statistics.Judgment PAN?11 IMDb62 IMDb1M BlogAuthors 3 72 62 22,116 19,320Texts 1,342 Trn: 9,335 79,550 271,625 678,161Vld: 1,296Tst: 1,300Texts per D: 902 Trn: 129.7 (139.3) 1283.1 12.3 35.1author M: 253 Vld: 19.9 (19.0) (685.8) (92.1) (105.0)mean (stddev) R: 187 Tst: 20.3 (18.9)Tokens per D: 2,858.6 (2,456.9) Trn: 60.8 (109.4) 281.8 124.2 248.4text M: 1,310.7 (1,248.4) Vld: 65.3 (98.9) (234.8) (166.6) (510.8)mean (stddev) R: 783.0 (878.5) Tst: 71.0 (115.1)289Computational Linguistics Volume 40, Number 25.1.2 PAN?11.
The PAN?11 data sets were introduced as part of the PAN 2011 compe-tition (available from pan.webis.de) (Argamon and Juola 2011).
These data sets wereextracted from the Enron e-mail corpus (www.cs.cmu.edu/~enron), and were designedto emulate closed-class and open-class authorship attribution and authorship verifica-tion scenarios (Section 2).
These data sets represent authorship attribution scenariosthat may arise in computer forensics, such as the case noted by Chaski (2005), where anemployee who was terminated for sending a racist e-mail claimed that any person withaccess to his computer could have sent the e-mail.In our experiments, we used the largest PAN?11 data set, with e-mails by 72 authors.Unlike the other data sets we used, this data set is split into training, validation, andtesting subsets (abbreviated to Trn, Vld, and Tst, respectively, in Table 3).
We focusedon the closed-class problem, using the validation and testing sets that contain texts onlyby training authors.
The only change we made to the original data set was droppingtwo training and two validation texts that were automatically generated, which weredetected by length and content.
This had a negligible effect on method accuracy, butmade the statistics in Table 3 more representative of the data (e.g., the mean count oftokens per text is 65.3 in the validation set without the two automatically generatedtexts, compared with 338.3 in the full validation set).Using this data set allows us to test our methods on short and informal texts withmore authors than in traditional authorship attribution.
As Table 3 shows, the PAN?11data set contains the shortest texts of the data sets we considered.
This fact, togetherwith the training/validation/testing structure of the data set, make it possible to runmany experiments on this data set before moving on to larger data sets.5.1.3 IMDb62.
IMDb62 contains 62,000 movie reviews and 17,550 message board postsby 62 prolific users of the Internet Movie database (IMDb, www.imdb.com).
We intro-duced this data set (Seroussi, Zukerman, and Bohnert 2010) to test our author-awarepolarity inference approach (Section 6.2).
Each user wrote 1,000 reviews (sampled fromtheir full set of reviews), and a variable number of message board posts, which aremostly movie-related, but may also be about television, music, and other topics.
Thisdata set allows us to test our approach in a setting where all the texts have similarthemes, and the number of authors is relatively small, but is already much larger thanthe number of authors considered in traditional authorship attribution settings.
Unlikethe other data sets of informal texts, IMDb62 consists only of prolific authors, allowingus to test our approach in a scenario where training data is plentiful.5.1.4 IMDb1M.
Although the IMDb62 data set is useful for testing our methods on small-to medium-scale problems, it cannot be seen as an adequate representation of large-scale problems.
This is especially relevant to the task of rating prediction, in whichtypical data sets contain thousands of users (Section 6.3).
Hence, we created IMDb1Mby randomly generating one million valid IMDb user IDs and downloading the reviewsand message board posts written by these users (Seroussi, Bohnert, and Zukerman2011).
Unfortunately, most of the randomly generated IDs led to users who submit-ted neither reviews nor posts?we found that about 5% of the entire user populationsubmitted posts, and less than 3% wrote reviews.
After filtering out users who have notsubmitted any rated reviews, we were left with 22,116 users.
These users, who make upthe IMDb1M data set, submitted 204,809 posts and 66,816 rated reviews.IMDb1M can be seen as complementary to the IMDb62 data set, as IMDb62 allowsus to test scenarios in which the user population is made up of prolific users, whereas290Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic ModelsIMDb1M contains a more varied sample of the population.
However, because we didnot impose a minimum threshold on the number of reviews or posts, the IMDb1Mpopulation is very challenging as it includes many users with few texts (e.g., about56% of the users in IMDb1M wrote only one text).
It is worth noting that three usersappear in both IMDb62 and IMDb1M.
In IMDb62 these three users authored 3,000reviews and 268 posts in total (about 4.8% of the total number of reviews and 1.5% of theposts), and in IMDb1M they authored 5,695 reviews and 358 posts (about 8.5% of the re-views and 0.2% of the posts).
The difference in the number of reviews is due to thesampling we performed when we created IMDb62, and the difference in the numberof posts is due to the time difference between the creation of the two data sets.5.1.5 Blog.
The Blog data set is the largest data set we consider, containing 678,161blog posts by 19,320 authors (available from u.cs.biu.ac.il/~koppel).
It was createdby Schler et al.
(2006) to learn about the relation between language use and demographiccharacteristics, such as age and gender.
We use this data set to test how our authorshipattribution methods scale to handle thousands of authors.
As blog posts can be aboutany topic, this data set is less restricted than the Judgment, PAN?11, and IMDb data sets.Further, the large number of authors ensures that every topic is likely to interest at leastseveral authors, meaning that methods that rely only on content are unlikely to performas well as methods that also take author style into account.5.2 Experimental SetupWe used different experimental setups, depending on the data set.
PAN?11 experimentsfollowed the setup of the PAN?11 competition (Argamon and Juola 2011): We trained allthe methods on the given training data set, tuned the parameters according to resultsobtained for the given validation data set, and ran the tuned methods on the giventesting data set.
For all the other data sets we utilized ten-fold cross validation.
In allcases, we report the overall classification accuracy, that is, the percentage of test textscorrectly attributed to their author.
Statistically significant differences are reported whenp < 0.05 according to McNemar?s test (when reporting results in a table, the best resultfor each column is in boldface, and several boldface results mean that the differencesbetween them are not statistically significant).In our experiments, we used the L2-regularized linear SVM implementation ofLIBLINEAR (Fan et al.
2008), which is well suited for large-scale text classification.
Weexperimented with cost parameter values from the set {.
.
.
, 10?1, 100, 101, .
.
.
}, until noaccuracy improvement was obtained (starting from 100 = 1 and going in both direc-tions).
We report the results obtained with the value that yielded the highest accuracy,which gives an optimistic estimate for the performance of the Token SVM baseline.We used collapsed Gibbs sampling to train all the topic models, running four chainswith a burn-in of 1,000 iterations.
In the Judgment, PAN?11, and IMDb62 experiments,we retained eight samples per chain with a spacing of 100 iterations.
In the IMDb1Mand Blog experiments, we retained one sample per chain due to runtime constraints.Because we cannot average topic distribution estimates obtained from training samplesdue to topic exchangeability (Steyvers and Griffiths 2007), we averaged the probabilitiescalculated from the retained samples.
In the dimensionality reduction experiments, weused the topic distributions from a single training sample to ensure that the number offeatures is substantially reduced (an alternative approach would be to use the concate-nation of all the samples, but this may result in a large number of features, and em-ploying this alternative approach did not improve results in preliminary experiments).291Computational Linguistics Volume 40, Number 2For test text sampling, we used a burn-in of 10 iterations and averaged the parameterestimates over the next 10 iterations in a similar manner to the procedure used by Rosen-Zvi et al.
(2010).
We found that these settings yield stable results across different randomseed values.To enable a fair comparison between the topic-based methods and the Token SVMbaseline, all methods were trained on the same token representations of the texts.
Inmost experiments, we did not apply any filters and simply used all the tokens as theyappear in the text.
In some cases, as indicated throughout this section, we either retainedonly stopwords or discarded the stopwords in a preprocessing step that was appliedbefore running the methods.
This allowed us to obtain rough estimates of the effectof considering only style words, considering only content words, and considering bothstyle and content.
However, we note that this is only a crude way of separating style andcontent, because some stopwords may contain content clues, whereas some words thatdo not appear in the stopword list may be seen as indicators of personal style, regardlessof content.We found that the number of topics has a large impact on performance, andthe effect of other configurable parameters is smaller (Section 3.1).
Hence, we usedsymmetric topic priors, setting all the elements of ?
(D) and ?
(A) to min{0.1, 5/T(D)}and min{0.1, 5/T(A)}, respectively.
For all models, we set ?w = 0.01 for each word w asthe base measure for the prior of words in topics.
Because DADT allows us to encode ourprior knowledge that stopword use is indicative of authorship, we set ?
(D)w = 0.01?
eand ?
(A)w = 0.01 + e for all w, where w is a stopword.
Unless otherwise specified, weset e = 0.009, based on tuning experiments on Judgment and PAN?11 (Section 5.3).
Sim-ilarly, we set ?
(D) = 1.222 and ?
(A) = 4.889 for DADT, based on the same experiments.In addition, we set ?a = 1 for each author a, yielding smoothed estimates for the corpusdistribution of authors ?.5.3 Experiments on Small Data SetsIn this section, we present the results of our experiments on the Judgment data set,which contains judgments by three judges, and on the PAN?11 data set, which con-tains e-mails by 72 authors.
Authorship attribution on the PAN?11 data set is morechallenging than on the Judgment data set, because PAN?11 texts are shorter thanjudgments, and some of the PAN?11 authors wrote only a few e-mails.
We first presentthe results obtained with LDA, followed by the results obtained with AT (with andwithout fictitious authors), and with our DADT-based methods, which yielded the bestperformance.
We end this section with experiments that explore the effect of applyingstopword filters to the corpus in a preprocessing step.
These experiments demonstratethat our DADT-based approach models authorship indicators other than content words.As discussed in Section 5.2, we ran ten-fold cross validation on the Judgmentdata set.
On PAN?11, we tuned the methods on the validation subset and report theresults obtained on the testing subset with the settings that yielded the best validationresults (i.e., each method was run multiple times on the validation subset and only onceon the testing subset).
We present some tuning results together with testing results toillustrate the behavior of the various methods.
It is worth noting that for most methods,PAN?11 testing results are better than the best validation results.
This may be becauseon average testing texts are about 10% longer than validation texts (Section 5.1.2).5.3.1 LDA.
Figure 5 presents the results of the LDA experiment, with Judgment results inFigure 5a, and PAN?11 validation and testing results in Figures 5b and 5c, respectively.292Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models01020304050607080901000  50  100  150  200  250  300  350  400Accuracy [%]Number of TopicsToken SVM Majority LDA-SVM  01020304050607080901000  50  100  150  200  250  300  350  400Accuracy [%]Number of TopicsToken SVM Majority LDA-SVM(a) Judgment data set (b) PAN?11 validationMethod T(D) AccuracyMajority ?
7.15%Token SVM ?
53.31%LDA-SVM 200 31.92%(c) PAN?11 testingFigure 5LDA results (data sets: Judgment and PAN?11).On the Judgment data set, the best performance obtained by training an SVM classi-fier on LDA topic distributions (LDA-SVM with 100 topics) was somewhat worse thanthat obtained by training directly on tokens (Token SVM), but was still much better thana majority baseline (the differences between LDA-SVM and both the Token SVM andmajority baselines are statistically significant in all cases).
This indicates that althoughsome authorship indicators are lost when using LDA for dimensionality reduction,many are retained despite the fact that LDA?s document representations are much morecompact than the raw token representations.The ranking of methods on PAN?11 is similar to the ranking on the Judgment dataset, though on Judgment the difference between LDA-SVM and Token SVM is muchsmaller.
The reason for this difference may be that LDA does not consider authors inthe model-building stage.
Although this had a relatively small effect on performancein the three-way judgment attribution scenarios, it appears that accounting for authorsis important in scenarios with many authors.
As the rest of this article deals with suchscenarios, we decided not to use LDA for modeling authors in subsequent sections.5.3.2 AT.
Figure 6 presents the results of the AT experiment, with Judgment results inFigure 6a and PAN?11 validation and testing results in Figures 6b and 6c, respectively.In contrast to LDA-SVM, AT-SVM was very sensitive to the number of topics.
Thisis probably because AT-SVM?s dimensionality reduction is more radical than LDA-SVM?s: In AT-SVM, each document is reduced to the same distribution over authortopics because AT does not model individual documents (Section 4.3).
Notably, AT-SVM?s performance was very poor when 200 and 400 topics were used, possibly becausethe more fine-grained topic distributions yielded by using more topics resulted insparser author representations (where some topics were allocated only a few words),which may have caused the SVM component to overfit.
This trend is more pronouncedin the Judgment results than in the PAN?11 results: On Judgment, AT-SVM with 200 and293Computational Linguistics Volume 40, Number 2400 topics yielded poorer results than the majority baseline, probably because the effectof sparsity is larger when considering three authors than when modeling 72 authors.The probabilistic AT-P method significantly outperformed AT-SVM and the ma-jority baseline on both data sets.
Although AT-P performed comparably to TokenSVM on the PAN?11 data set, it was significantly outperformed by Token SVM on theJudgment data set.
Nonetheless, these results indicate that AT captures many of theindicators required for authorship attribution.
This is despite the fact that AT was notdesigned with authorship attribution in mind.
Hence, it represents each author witha single distribution over topics while ignoring differences and similarities betweendocuments (which may be important for the authorship attribution task).
This stands incontrast to the Token SVM baseline, which attempts to build a document-based modelthat is optimized for the classification goal of authorship attribution (Section 4.1).5.3.3 AT-FA.
Figure 7 presents the results of the AT-FA experiment, with Judgmentresults in Figure 7a and PAN?11 validation and testing results in Figures 7b and 7c,respectively.On both data sets, the highest accuracy yielded by AT-FA-SVM and AT-FA-P1 wassignificantly lower than that obtained by the corresponding methods in the AT casewithout fictitious authors (AT-SVM and AT-P, respectively).
This may seem surprising,because the only difference between AT and AT-FA is the addition of a fictitious authorfor each document, which was shown to improve AT?s ability to predict unseen por-tions of documents (Rosen-Zvi et al.
2010).
However, the reason for AT-FA-SVM andAT-FA-P1?s poor performance may be that they do not conserve the underlying as-sumption of fictitious authors in the classification stage, i.e., they do not assume thatthe test text was written by a fictitious author together with a previously unseen author(Section 4.4).
This is probably also the reason why the probabilistic AT-FA-P2 signifi-01020304050607080901000  50  100  150  200  250  300  350  400Accuracy [%]Number of TopicsToken SVMMajority AT-SVMAT-P  01020304050607080901000  50  100  150  200  250  300  350  400Accuracy [%]Number of TopicsToken SVMMajority AT-SVMAT-P(a) Judgment data set (b) PAN?11 validationMethod T(A) AccuracyMajority ?
7.15%Token SVM ?
53.31%AT-SVM 50 39.23%AT-P 100 53.08%(c) PAN?11 testingFigure 6AT results (data sets: Judgment and PAN?11).294Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models01020304050607080901000  50  100  150  200  250  300  350  400Accuracy [%]Number of TopicsToken SVMMajority AT-FA-SVMAT-FA-P1 AT-FA-P2  01020304050607080901000  50  100  150  200  250  300  350  400Accuracy [%]Number of TopicsToken SVMMajority AT-FA-SVMAT-FA-P1(a) Judgment data set (b) PAN?11 validationMethod T(A) AccuracyMajority ?
7.15%Token SVM ?
53.31%AT-FA-SVM 50 23.15%AT-FA-P1 400 29.69%PAN?11 testingFigure 7AT-FA results (data set: Judgment and PAN?11).cantly outperformed AT-FA-P1 by a large margin on the Judgment data set?AT-FA-P2conserves the fictitious author assumption, whereas AT-FA-P1 ignores it (we did not runthe AT-FA-P2 method on PAN?11 because it requires running a separate sampling chainfor each candidate author and test text, which makes it too computationally expensiveto run in cases with many candidate authors and test texts).When comparing AT-FA-P2 to the baselines (on Judgment), we see that it wassignificantly outperformed by Token SVM for all topic numbers, but yielded signifi-cantly better performance than the majority baseline.
Despite the fact that AT-FA-P2was outperformed by Token SVM, the margin was not large when enough topics wereused (AT-FA-P2 yielded its best accuracy of 89.60% with 100 topics, in comparison withToken SVM?s accuracy of 91.15%).
This indicates that representing both documents andauthors in the topic model may have advantages in terms of authorship attribution.
Thisfurther motivates the use of our DADT model, which considers documents and authorswithout requiring the preprocessing step of adding fictitious authors.5.3.4 DADT.
Figure 8a presents the results of the DADT experiment on the Judg-ment data set, obtained with 10 author topics, 90 document topics, and prior settingsof ?
(D) = 1.222, ?
(A) = 4.889, and e = 0.009 (other parameter settings are discussedsubsequently).
These results are compared to the baselines (majority and Token SVM),and to the best topic-based result obtained on this data set thus far (by AT-FA-P2 with100 topics).
As we can see, the best DADT-based result was obtained with the proba-bilistic DADT-P method, which significantly outperformed all the other methods.
Thisdemonstrates the effectiveness of our DADT model in capturing author characteristicsthat are relevant to authorship attribution.Notably, DADT-SVM yielded significantly poorer results than DADT-P. DADT-SVM?s relatively weak performance may be because its use of document topics295Computational Linguistics Volume 40, Number 2Method AccuracyMajority 67.21%Token SVM 91.15%AT-FA-P2 89.60%DADT-SVM 85.49%DADT-P 93.64%T(D) T(A) ?
(D) ?
(A) e Accuracy90 10 1 1 0 93.81%90 10 1.222 4.889 0 93.49%90 10 1.222 4.889 0.009 93.64%50 50 1.222 4.889 0.009 92.88%10 90 1.222 4.889 0.009 88.62%(a) Tuned DADT methods (b) DADT-P tuningFigure 8DADT results (data set: Judgment).introduces noise that causes the SVM component to underperform, as DADT?sdocument topics are not expected to be indicative of authorship.The separation of document words from author words that is obtained by usingDADT on the Judgment data set is illustrated by Figure 9, which presents three docu-ment topics and three author topics in word-cloud form.
The top 50 tokens from eachtopic are shown, where the size and shade of each token indicates its frequency in thetopic.
This anecdotal sample of topics reflects the general trend that we noticed in thisdata set, where document topics represent different types of cases, and the top tokensin author topics do not carry content information and are dominated by stopwords(a) Document topics (b) Author topicsFigure 9DADT topic examples.296Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Modelsand punctuation (LDA and AT topics were similar to DADT?s author topics due to theprevalence of stopwords and the lack of document?author separation in these models).This trend is in line with what we expected, because all three judges handled casesof different types, and thus content words are unlikely to carry enough information toadequately represent the judges.
As discussed in Section 3.1.3, this separation of contentand style is corpus-dependent and is expected to occur only in cases where content isindependent of author identity.
Indeed, we did not observe such a clear separation inour experiments on other data sets.Our choice of DADT settings reflects the following insights:r We used 100 topics overall based on the results of the other topic-based methods,which showed that good results are obtained with this number of overall topics.We chose the 90/10 document/author topic split because in the case of the Judg-ment data set, DADT models only three authors who wrote many documents.r Setting ?
(D) = 1.222 and ?
(A) = 4.889 encodes our prior belief that the portion ofeach document that is composed of author words is 80% on average, with 15%standard deviation (obtained as described in Section 3.1.3).r Setting e = 0.009 encodes our prior belief that stopword choice is more likely tobe influenced by the identity of the author than by the content of the documents(Section 5.2).Somewhat surprisingly, these settings did not have a large effect on the performance ofthe methods in most cases.
This is demonstrated by the results presented in Figure 8b,which were obtained by varying the values of these parameters and running the DADT-P method.
As Figure 8b shows, the results obtained with a setting of ?
(D) = ?
(A) = 1,which can be seen as encoding no strong prior belief about the document/author wordbalance in each document (it is equivalent to setting a uniform prior on this balance),were comparable to the results obtained with ?
(D) = 1.222 and ?
(A) = 4.889.
Likewise,changing e from 0 to 0.009 only had a minor effect on the results.
The only setting thatmade a relatively large difference is the document/author topic split: Changing it from90/10 to 10/90 yielded poorer results.
However, the 50/50 split yielded close results tothe 90/10 split, which shows that in this case, the document/author topic split settingis only sensitive to relatively large variations.It is likely that performing an exhaustive grid search for the optimal parametersettings for each method would allow us to obtain somewhat improved results.
How-ever, such a search would be computationally expensive, as the model needs to beretrained and tested for each fold, parameter set, and method.
Therefore, we decidedto present the results obtained with the non-optimized settings, which are sufficient todemonstrate the merits of our DADT approach, as DADT-P outperformed all the othermethods discussed so far.On PAN?11, we ran the DADT experiments with 100 topics overall, as this numberof topics yielded the best topic-based results of the models and methods whose resultswe presented thus far (AT-P with 100 topics yielded the best results of the methodsbased on LDA, AT, and AT-FA).
Figure 10b shows the results of tuning DADT?s settingsand running DADT-P on the PAN?11 validation set.
The PAN?11 tuning experimentshows a clearer picture in terms of accuracy differences between different parametersettings than the Judgment experiments.
Specifically, when we used uninformed uni-form priors on the document/author word split (?
(D) = ?
(A) = 1), and the same word-in-topic priors for both document and author words (e = 0), the obtained accuracy wascomparable to AT-P?s accuracy.
On the other hand, setting ?
(D) = 1.222 and ?
(A) = 4.889,which encodes our prior belief that on average 80% (with a standard deviation of 15%)297Computational Linguistics Volume 40, Number 2of each document is composed of author words, significantly improved performance.Setting e = 0.009 to encode our prior knowledge that stopwords are indicators of au-thorship yielded an additional improvement.
Finally, the last two results in Figure 10bdemonstrate the importance of having enough topics to model the authors: Accuracydropped by about 4 percentage points when we used 50 author topics and 50 documenttopics, and by about 24 percentage points when we used only 10 author topics and 90document topics, rather than 90 author topics and 10 document topics.
This leads us toconjecture that it would be beneficial to pursue a future extension that learns the topicbalance automatically, e.g., in a similar manner to Teh et al.
?s (2006) method of inferringthe number of topics in LDA.Figure 10a presents the PAN?11 results obtained with the DADT-based methods,using the best setting from Figure 10b: 10 document topics, 90 author topics, ?
(D) =1.222, ?
(A) = 4.889, and e = 0.009.
As Figure 10a shows, DADT-P, which obtained thebest performance of all the methods tested in this section, is the only method thatoutperformed Token SVM.
This implies that our DADT model is the most suitable ofthe models we considered for capturing patterns in the data that are important forauthorship attribution, at least in scenarios that are similar to the PAN?11 case.DADT-P?s testing result is comparable to the third-best accuracy (out of 17) obtainedin the PAN?11 competition (Argamon and Juola 2011) (competitors were ranked accord-ing to macro-averaged and micro-averaged precision, recall, and F1; the micro-averagedmeasures are all equivalent to the accuracy measure in this case, because each of the testtexts is assigned to a single candidate author).
However, to the best of our knowledge,DADT-P obtained the best accuracy for a fully supervised method that uses only uni-gram features.
Specifically, Kourtis and Stamatatos (2011), who obtained the highest ac-curacy (65.8%), assumed that all the test texts are given to the classifier at the same timeand used this additional information with a semi-supervised method, whereas Kernet al.
(2011) and Tanguy et al.
(2011), who obtained the second-best (64.2%) and third-best (59.4%) accuracies, respectively, used various feature types (e.g., features obtainedfrom parse trees).
In addition, preprocessing differences make it hard to compare themethods on a level playing field.
Nonetheless, we note that extending DADT to enablesemi-supervised classification and additional feature types are promising directions forfuture work.5.3.5 Testing the Effect of Stopwords.
The results reported up to this point were all obtainedby running the methods on document representations that include all the tokens.
Asdiscussed in Section 5.2, discarding or retaining stopwords provides a crude way ofseparating style from content.
We ran a set of experiments where we either discardedMethod AccuracyMajority 7.15%Token SVM 53.31%AT-P 53.08%DADT-SVM 39.69%DADT-P 59.38%T(D) T(A) ?
(D) ?
(A) e Accuracy10 90 1 1 0 48.53%10 90 1.222 4.889 0 53.40%10 90 1.222 4.889 0.009 54.86%50 50 1.222 4.889 0.009 50.31%90 10 1.222 4.889 0.009 30.48%(a) Tuned DADT methods (testing subset) (b) DADT-P tuning (validation subset)Figure 10DADT results (data set: PAN?11).298Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic ModelsTable 4Stopword experiment results (data sets: Judgment and PAN?11).Method Judgment PAN?11 TestingAll Discard Retain only All Discard Retain onlywords stopwords stopwords words stopwords stopwordsMajority 67.21% 67.21% 67.21% 7.15% 7.15% 7.15%Token SVM 91.15% 86.18% 92.76% 53.31% 46.46% 28.38%DADT-P 93.64% 89.28% 90.85% 59.38% 54.69% 18.54%stopwords in a preprocessing step or retained only stopwords, and then ran the TokenSVM baseline and the DADT-P method, which obtained the best performance when allthe tokens were used (DADT was run with the same settings used to obtain the tunedresults from the previous section).The results of this experiment are presented in Table 4.
As the results show, dis-carding stopwords caused the Token SVM baseline to yield poorer performance thanwhen all the tokens were used, but retaining only stopwords significantly improvedToken SVM?s performance on Judgment and yielded a substantial drop in performanceon PAN?11.
Interestingly, this was not the case with DADT-P, where either discarding orretaining stopwords caused a statistically significant drop in performance in compari-son with using all the tokens.
The reason why DADT-P?s performance dropped whenonly stopwords were used may be that DADT was designed under the assumption thatall the tokens in the corpus are retained.
However, we are encouraged by the fact thatDADT-P?s performance drop on Judgment was not very large when only stopwordswere retained, as it indicates that DADT captures stylistic elements in the authors?
texts.Another encouraging result is that DADT-P yielded significantly better perfor-mance than Token SVM when using feature sets that included all the tokens or all thetokens without stopwords.
DADT-P appears to harness the extra information from non-stopword tokens more effectively than Token SVM, despite the fact that such tokenstend to occur less frequently in the texts than stopwords.
Further, the vocabulary size ofthese two feature sets is larger than that of the stopword-only feature set, which suggeststhat DADT-P is more resilient to noise than Token SVM.It is worth noting that some content-independent information is lost when onlystopwords are retained.
For example, the phrase ?in my opinion?
appears in texts by allthree authors in the Judgment data set, but is used more frequently by McTiernan (itoccurs in about 82% of his judgments) than by Dixon (69%) or Rich (58%).
As thefrequency of this phrase is apparently dependent on author style and independent ofthe specific content of a given judgment, it is probably safe to assume that it wouldbe beneficial to retain the word ?opinion?
(this is also evidenced by the dominance ofthis word in the third author topic in Figure 9).
However, this word does not appearin our stopword list.
This problem is more pronounced in the PAN?11 data set, whereit appears that other words beyond stopwords are also indicative of authorship.
Forinstance, Tanguy et al.
(2011) used the openings and closings of the e-mails in the dataset as separately weighted features.
Openings can start with words such as ?hello,?
?hi,??hey,?
and ?dear,?
but only the first two words appear in our stopword list, meaningthat even when only stopwords are retained some stylistic features are lost.
These exam-ples highlight the difficulties in extracting words that are truly content-independent?aproblem that would be especially relevant when trying to adapt an authorship classifier299Computational Linguistics Volume 40, Number 2Table 5Large-scale experiment results (data sets: IMDb62, IMDb1M, and Blog).Method IMDb62 IMDb1M Blog (prolific) Blog (full)Majority 7.37% 3.00% 1.28% 0.62%Token SVM 92.52% 43.85% 32.96% 24.13%AT-P 89.62% 40.82% 37.59% 23.03%DADT-P 91.79% 44.23% 43.65% 28.62%trained on texts from one domain to texts from a completely different domain (thisproblem is beyond the scope of this study).
A possible solution is to obtain corpus-specific stopwords?for example, by extracting a list of frequent words?but this givesrise to new problems, such as determining a frequency threshold.
We decided not topursue such a solution because the PAN?11 results show that improved performanceis not guaranteed when only stopwords are retained, even when Token SVM is used.Hence, in the remainder of this article we use all the words, that is, we neither discardstopwords nor retain only stopwords.5.4 Experiments on Large Data SetsIn this section, we report the results of our experiments on the IMDb62, IMDb1M, andBlog data sets.
Both IMDb data sets contain movie reviews and message board posts,with IMDb62 consisting of texts by 62 prolific authors (with at least 1,000 texts each),and IMDb1M consisting of texts by 22,116 authors, who are mostly non-prolific.
TheBlog data set contains blog posts by 19,320 authors, and is the largest of the data setswe considered in terms of token count?it contains about 168 million tokens, whereasIMDb62 and IMDb1M contain about 22 and 34 million tokens, respectively.
In additionto running experiments on the full Blog data set, we considered a subset that contains allthe texts by the 1,000 most prolific authors (this subset contains about 69 million tokensoverall in 332,797 posts?about 49% of the posts in the full Blog data set).Due to resource constraints, we performed a more restricted set of experimentson IMDb62, IMDb1M, and Blog than on the Judgment and PAN?11 data sets (whichcontain about 3 and 0.74 million tokens, respectively).
We ran only the Token SVMbaseline, AT-P, and DADT-P, as these methods yielded the best performance in thePAN?11 experiments.
We set the overall number of topics of AT and DADT to 200 topicsfor IMDb62, and 400 topics for IMDb1M and Blog.
We set DADT?s document/authortopic split to 50/150 for IMDb62 and 50/350 for IMDb1M and Blog, and used the priorsetting that yielded the best PAN?11 results (?
(D) = 1.222, ?
(A) = 4.889, and e = 0.009).As in the PAN?11 experiments, we determined the overall number of topics based onAT-P?s performance with 25, 50, 100, 200, and 400 topics.
The document/author topicsplits we tested were 10/190, 50/150, and 100/100 for IMDb62, and 10/390, 50/350,and 100/300 for IMDb1M and Blog.Table 5 shows the results of this set of experiments.
As in our previous experiments,DADT-P consistently outperformed AT-P, which indicates that using disjoint sets ofdocument and author topics yields author representations that are more suitable forauthorship attribution than using only author topics.
In contrast to the previous exper-iments, Token SVM outperformed DADT-P in one case: the IMDb62 data set.
This maybe because discriminative methods (such as Token SVM) tend to outperform generative300Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Modelsmethods (such as DADT-P) in scenarios where training data is abundant (Ng and Jordan2001), which is the case with IMDb62?it contains at least 900 texts per author in eachtraining fold.A notable result is that although all the methods yielded relatively low accuracieson the full Blog data set, the topic-based methods experienced a larger drop in accuracythan Token SVM when transitioning from the prolific author subset to the full dataset.
This may be because topic-based methods use a single model, making them moresensitive to the number of authors than Token SVM?s one-versus-all setup that usesone model per author (this sensitivity may also explain why DADT-P outperformedToken SVM by a relatively small margin on IMDb1M).
This result suggests a directionfor future work in the form of an ensemble of Token SVM and DADT-P.
The potential ofthis direction is demonstrated by the fact that a perfect oracle, which chooses the correctanswer between Token SVM and DADT-P when they disagree, yields an accuracy of37.36% on the full Blog data set.5.5 Summary of Key FindingsIn summary, we found that the DADT-based probabilistic approach (DADT-P) yieldedstrong performance on the five data sets we considered, outperforming the TokenSVM baseline in four out of the five cases.
We showed that DADT-P is more suitablefor authorship attribution than methods based on LDA and AT (with or without fic-titious authors), and than using DADT for dimensionality reduction.
Although ourresults demonstrate that separating document words from author words is a goodapproach to authorship attribution, relying only on unigrams is a limitation (whichis shared by LDA, AT, and DADT).
We discuss ways of addressing this limitation inSection 7.DADT?s improved performance in comparison with methods based on LDA and ATcomes at a price of more parameters to tune.
However, the most important parameter isthe number of topics?we found that the prior values that yielded good results on thesmall data sets also obtained good performance on the large data sets without furthertuning.
We offered a simple recipe to determine the number of topics for DADT-P: Firstrun AT-P to find the overall number of topics (which is equivalent to running DADT-P without document topics), and then tune the document/author topic balance.
Asmentioned in Section 3.1.2, this procedure can be obviated by automatically learningthe topic balance, which is left for future work.6.
ApplicationsThis section presents three applications of topic-based author representations: identi-fying anonymous reviewers (Section 6.1), author-aware polarity inference (Section 6.2),and text-aware rating prediction (Section 6.3).6.1 Reviewer IdentificationAT and DADT can potentially be used to identify anonymous reviewers based on pub-licly available data?the reviewer list (which is commonly available), and the reviewers?published papers.
The main question in this case is whether authorship markers learnedfrom (often multi-authored) texts in one domain (the papers) can be used to classifysingle-authored texts from a related domain (the reviews).301Computational Linguistics Volume 40, Number 2To start answering this question, we considered a small conference track, whichattracted 18 submissions that were each reviewed by two reviewers.
We collected thebodies of 10 papers (without references, author names, acknowledgments, etc.)
by eachof the 18 reviewers that were listed in the proceedings, which resulted in a training cor-pus of 171 documents with 196 authors overall (some of the reviewers have co-authoredpapers with other reviewers).
We omitted authors with only one paper, because theirpresence is equivalent to having fictitious authors, which may hurt performance(Section 5.3).
This resulted in a total of 77 authors.
Our test data set consisted of 19reviews by the 9 reviewers who gave us permission to use their reviews.We trained AT and DADT on the paper corpus under the setup described inSection 5.2, and used AT-P and DADT-P to classify the reviews.
The best accuracy,8/19, was obtained by DADT-P with 10 document topics and 90 author topics.
Theaccuracy of AT-P (with 100 topics) was slightly worse, at 7/19.
In addition, the correctreviewer appeared in the top-five list of probable authors for 15/19 of the reviews withDADT-P and 11/19 with AT-P (the list of probable authors included all 18 reviewers?we considered all the reviewers as candidates because this did not require using anyprivate information and it made our experimental setup more realistic).
We obtainedbetter results by eliminating non-reviewers from the training corpus (thus training onthe 171 documents with 18 authors overall).
DADT-P required only 25 document topicsand 25 author topics in this case, and its accuracy rose to 10/19 (AT-P again performedworse with an accuracy of 7/19).
In 16/19 of the cases the correct reviewer appeared inDADT-P?s top-five list, compared to 12/19 with AT-P.These results were obtained on a very small data set.
Still, they indicate that re-viewer identification is feasible (note that it is unlikely that DADT-P?s performanceis only due to content words, as interest areas are often shared between reviewers).To verify this, a fully fledged study should be done on a corpus of reviews froma large conference, with a training corpus that includes each author?s full body ofpublications (perhaps dropping very old publications, which we did not do).
As faras we know, such a study is yet to be performed.
The closest work we know of is byNanavati et al.
(2011), who considered the question of whether ?insiders,?
who servedas program committee members and thus had access to non-anonymous reviews, canuse these reviews as training data to identify reviewers.
Although they found that theycould identify reviewers with high accuracy, the main limitation of their approach isthat it relies on private data.Nonetheless, we believe that reviewer anonymity needs to be addressed.
Oneapproach is to use tools that obfuscate author identity, as developed by, for example,Kacmarcik and Gamon (2006) and Brennan and Greenstadt (2009).
However, as this maylead to an ?arms race?
between such tools and authorship analysis methods, perhapsthe best approach is to forgo anonymity completely, as advocated by some researchersand editors (Groves 2010).
This is an open question with no simple answers, but wehope that our results will help motivate the search for solutions.6.2 Author-Aware Polarity InferenceSentiment analysis deals with inferring people?s sentiments and opinions fromtexts (Pang and Lee 2008; Liu and Zhang 2012).
One of the main tasks in this field ispolarity inference, where the goal is to infer the degree of positive or negative sentimentof texts (Pang and Lee 2008).
Even though the way polarity is expressed often dependson the author, most of the work in this field ignores authors.
We addressed this gap(Seroussi, Zukerman, and Bohnert 2010; Seroussi 2012) by introducing a framework302Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Modelsthat considers authors when performing polarity inference, by combining the outputs ofauthor-specific inference models in a manner that makes it possible to consider authorsimilarity.
We showed that our framework outperforms two state-of-the-art baselinesintroduced by Pang and Lee (2005): one that ignores authorship information, andanother that considers only the model learned for the author of the text whose polaritywe want to infer.
These results support our hypothesis that the way sentiment is ex-pressed is often author-dependent, and shows that our approach successfully harnessesthis dependency to improve polarity inference performance.Topic-based representations of authors suggest a way of measuring similarity be-tween authors based on their texts, which can be used by our polarity inference frame-work.
Such measures are expected to capture authors?
interests and aspects of theirauthorship style, which is indicative of demographic attributes and personality traits.We hypothesize that compact representation of authors using topic distributions wouldhelp handle the inherent noisiness of large data sets of user-generated texts withoutlosing much information, as it did on the authorship attribution task.To test this hypothesis, we experimented with a simple variant of our polarityinference framework, which infers the polarity rating of a sentiment-bearing text qwritten by author a according to a weighted average2?a?
?Na waa?
r?a?q?a?
?Na waa?
(18)where Na is the set of neighbors of author a, waa?
is a non-negative similarity weightassigned to each neighbor a?, and r?a?q is the polarity inferred by the inferrer of a?for q (each inferrer is a support vector regression model trained on the labeled textsby a?).
The neighborhood Na is obtained for each author a by learning a threshold onthe number of similar authors to consider.
This is done by performing five-fold crossvalidation on a?s set of labeled texts to find the threshold that minimizes the root meansquared error (RMSE) out of a set of candidate thresholds.We compare the results obtained with baselines of equal weights (i.e., an un-weighted average) and token frequency similarity with those obtained with similaritymeasures based on the AT and DADT topic models.
The token frequency similaritymeasure is the cosine similarity of the frequency vectors of all the tokens in the authors?vocabularies.
The AT and DADT similarity measures are calculated as one minus theHellinger distance between the author topic distributions.We ran this experiment on the IMDb62 data set.
To test our approach in a varietyof scenarios, we utilized the GivenX protocol, where each target author has exactly Xtraining samples (Breese, Heckerman, and Kadie 1998).
Specifically, we performed ten-fold cross validation over authors, where we partitioned the authors into ten folds anditerated over the folds, using nine folds as the training folds and the remaining foldas the test fold.
The model was trained on all the labeled texts (IMDb62 reviews, eachwith a polarity rating assigned by its author) by the authors in the training folds, andexactly X labeled texts by each target author in the test fold.
The model was then testedon the remaining samples by each target author.
This process was repeated five times2 This is not the strongest variant of those explored by Seroussi, Zukerman, and Bohnert (2010) andSeroussi (2012), where we found that normalizing the inferences from the neighborhood and consideringa model trained on author a?s texts improves performance.
Using a simple weighted average allows us tocompare similarity measures independently of these enhancements.303Computational Linguistics Volume 40, Number 2with different random seeds, and the RMSE was averaged across folds (here and inthe next section we use a paired two-tailed t-test to measure statistical significance, aspolarity inference and rating prediction are regression problems).
Note that the GivenXprotocol cannot be used to reliably compare the performance of the same methodacross different X values (e.g., testing how the performance of a method varies fromGiven1 to Given100), because the test samples vary across X values.
Rather, we use thisprotocol to compare different methods under the same conditions, e.g., by comparingthe performance of using DADT to that of employing equal weights under the Given10scenario.Figure 11 presents the results of this experiment.
As the figure shows, the AT andDADT similarity measures outperformed the baselines and performed comparably toeach other (the differences between either AT or DADT and the baselines are statisti-cally significant in all cases except for Given5 and Given10 with the token frequencymeasure, whereas the differences between DADT and AT are not statistically significantin all cases).
It is worth noting that we did not tune AT?s and DADT?s parameters.Instead, we used the settings that yielded the best authorship attribution performanceon the IMDb62 data set (Section 5.4).
It appears that in this case DADT?s approach ofde-noising the author representations by modeling authors and documents over twodisjoint sets of topics is of little benefit in comparison with AT?s approach of using onlyauthor topics.
This may appear to stand in contrast to the results of our authorshipattribution experiments (Section 5), but it could be because the similarity measures donot require the models?
full discriminatory power, which is where DADT?s strengthslie (Section 3.4.3).
Nonetheless, we are encouraged by the fact that using either AT orDADT yielded better results than both the equal weights and token frequency baselinesin most cases.
This is despite the fact that these models operate in a space of lowerdimensionality than the token frequency measure, which demonstrates the strength oftopic-based approaches for author representation.1.781.81.821.841.861.881.91.921.941.960  20  40  60  80  100RMSE300  500  700  900Labelled Texts per Target AuthorEqual weightsToken frequenciesATDADTFigure 11Author-aware polarity inference with topic models (data set: IMDb62).304Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Models6.3 Text-Aware Rating PredictionRecommender systems help users deal with information overload by finding andrecommending items of personal interest (Resnick and Varian 1997).
Rating predictionis a core component of many recommender systems (Herlocker et al.
1999).
Recently,rating prediction algorithms that are based on matrix factorization have becomeincreasingly popular, due to their high accuracy and scalability (Koren, Bell, andVolinsky 2009).
However, such algorithms often deliver inaccurate rating predictionsfor users with few ratings (this is known as the new user problem).
We introducedan extension to the matrix factorization algorithm that considers user attributeswhen generating rating predictions (Seroussi, Bohnert, and Zukerman 2011; Seroussi2012).
We showed that using either demographic attributes or text-based attributesextracted with the LDA-S model, which is equivalent to AT (Section 3.2.3), outperformsstate-of-the-art baselines that consider only ratings, thereby enabling more accurategeneration of personalized rating predictions for new users.
In the case of AT, thesepredictions are generated without requiring users to explicitly supply any informationabout themselves and their preferences.Our framework predicts the rating user u would give to item i by switching betweenour attribute-based model and Koren, Bell, and Volinsky?s (2009) ratings-only modelaccording to an empirically set threshold n on the size of user u?s known rating setRu:r?ui ={?
+ b(I)i +?Tt=1 p(t|u)(b(A)t + z>?t y?i)|Ru| < n?
+ b(U)u + b(I)i + x>?uy?i otherwise(19)where ?
is the global rating mean; b(U)u , b(I)i , and b(A)t are the user, item, and attributebiases, respectively; and x?u, y?i, and z?t denote the u-th, i-th, and t-th columns of theuser, item, and attribute factor matrices X, Y, and Z, respectively.
The probability of auser u having one of the T attributes t is denoted by p(t|u), which in the case of ATand DADT is the user?s probability of using the author topic t, i.e., ?
(A)ut (each topicmodel is inferred from the texts written by the user).
We infer the biases and factormatrices using gradient descent in two stages (all the available ratings are used in bothstages): (1) infer the ratings-only part of the model; and (2) infer the attribute-basedpart of the model, assuming that the item biases and factor matrix are given (Seroussi,Bohnert, and Zukerman 2011).We ran Given0 and Given1 experiments on the IMDb1M data set, where the trainingset consisted of message board posts and rated reviews, and calculated the RMSE on thetest ratings (the reviews associated with these ratings were hidden from the models).The baseline methods were non-personalized prediction (?
+ b(I)i , which is roughlyequivalent to item i?s rating mean), and the personalized, ratings-only model, whichcould only be used in the Given1 case.
We set the number of author topics to 75 forGiven0 and 125 for Given1, as this yielded the best results for AT (out of 5, 10, 25, 50,75, 100, 125, and 150).
In DADT?s case, we used additional five document topics, whichyielded the best results (out of 1, 5, 10, and 25).
As the attribute-based model is sensitiveonly to the number of author topics, this enabled us to perform a fair comparisonbetween the two models.The results of this experiment are presented in Table 6.
Both AT and DADT out-performed the baselines, which supports our hypothesis that considering user textsby using topic-based author representations can yield personalized and accurate rating305Computational Linguistics Volume 40, Number 2Table 6Text-aware rating prediction with AT and DADT (data set: IMDb1M).Method Given0 Given1Non-personalized 2.733 2.691Personalized (only ratings) ?
2.734Personalized (AT) 2.719 2.668Personalized (DADT) 2.719 2.678predictions, potentially leading to improved recommendations.
The reason DADT didnot outperform AT may be that DADT tends to yield user representations that helpdiscriminate between texts by individual users (as shown in our authorship attributionexperiments), but such representations are not as useful when utilized as attributes, be-cause the attribute-based model requires a representation that captures commonalitiesbetween users.7.
Conclusion and Future WorkIn this article, we extended and added detail to the work of Seroussi, Zukerman, andBohnert (2011) and Seroussi, Bohnert, and Zukerman (2012) by reporting additionalexperimental results and applications of topic-based author representations thatgo beyond traditional authorship attribution.
We provided experimental results forauthorship attribution methods that are based on three topic models (LDA, AT, andDADT) for several scenarios where the number of authors varies from three to about20,000.
Specifically, we showed that in most cases, a probabilistic approach that isbased on our DADT model (DADT-P) yields the best results, outperforming methodsbased on LDA and AT, as well as a Token SVM baseline.
This indicates that ourtopic-based approach successfully captures indicators of authors?
style (which isindicative of author characteristics such as demographic attributes and personalitytraits) as reflected by their texts.
We harnessed this property when we used AT andDADT to uncover the authors of anonymous reviews where the training texts are multi-authored, improve performance when measuring similarity between authors based ontheir texts in our polarity inference framework, and obtain compact representationsof users for our rating prediction framework.The work presented in this article can be extended in many ways.
One directionwould be to address the limitation of relying only on unigrams as features by consider-ing word order.
This can possibly be pursued by adding author-awareness to Griffithset al.
?s (2004) HMM-LDA model, which considers word order by combining LDA with aHidden Markov Model.
Author awareness can also be introduced into the models sug-gested by Wallach (2006) and Wang, McCallum, and Wei (2007), who made each worddependent on both its topic and on the previous word (at a considerable computationalcost).
A more general alternative would be to enable the use of various feature types, forexample, by incorporating conditional random fields into DADT in a manner similar toZhu and Xing?s (2010) model.
This direction can also be pursued by using DADT-P inan ensemble with SVMs that can be trained on feature types other than token unigrams,which may also have the added value of combining the strengths of DADT with thoseof the SVM approach (Section 5.4).
Testing these approaches with character n-gramswould be of particular interest, as they often deliver strong performance, sometimesoutperforming token unigrams (Koppel, Schler, and Argamon 2009).306Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic ModelsOur DADT-P method can be extended to handle situations where the test texts mayhave not been written by any of the candidate authors (i.e., open-set attribution and ver-ification, described in Section 2).
A fairly straightforward approach consists of settinga threshold on the probability assigned to the selected author based on performanceon held-out data?if the probability of the selected author is below the threshold, then?unknown author?
is returned.
This approach was successfully used by Tanguy et al.
(2011) in conjunction with a maximum entropy classifier.Another potential extension would be to automatically infer the optimal numberof author and document topics.
This is likely to yield improved results, because thenumber of topics had the largest impact on performance among the parameters con-sidered in our experiments (Section 5.5).
In addition, our models can be extended toaddress semi-supervised authorship attribution, and may potentially be applied toany scenario where user-generated texts are available, going beyond the applicationspresented in Section 6.AcknowledgmentsThis research was supported in part by grantLP0883416 from the Australian ResearchCouncil.
The authors thank Russell Smythfor the collaboration on initial results on theJudgment data set, Mark Carman for fruitfuldiscussions on topic modeling, and theanonymous reviewers for their insightfulcomments.ReferencesArgamon, Shlomo and Patrick Juola.2011.
Overview of the internationalauthorship identification competition atPAN-2011.
In CLEF 2011: Proceedingsof the 2011 Conference on Multilingual andMultimodal Information Access Evaluation(Lab and Workshop Notebook Papers),Amsterdam.Argamon, Shlomo, Moshe Koppel, James W.Pennebaker, and Jonathan Schler.
2009.Automatically profiling the author of ananonymous text.
Communications of theACM, 52(2):119?123.Blei, David M. 2012.
Probabilistic topicmodels.
Communications of the ACM,55(4):77?84.Blei, David M. and Jon D. McAuliffe.
2007.Supervised topic models.
In NIPS 2007:Proceedings of the 21st Annual Conferenceon Neural Information Processing Systems,pages 121?128, Vancouver.Blei, David M., Andrew Y. Ng, and Michael I.Jordan.
2003.
Latent Dirichlet allocation.Journal of Machine Learning Research,3:993?1022.Breese, John S., David Heckerman, andCarl Kadie.
1998.
Empirical analysis ofpredictive algorithms for collaborativefiltering.
In UAI 1998: Proceedingsof the 14th Conference on Uncertaintyin Artificial Intelligence, pages 43?52,Madison, WI.Brennan, Michael and Rachel Greenstadt.2009.
Practical attacks against authorshiprecognition techniques.
In IAAI 2009:Proceedings of the 21st Conference onInnovative Applications of ArtificialIntelligence, pages 60?65, Pasadena, CA.Chaski, Carole E. 2005. Who?s at thekeyboard?
Authorship attributionin digital evidence investigations.International Journal of Digital Evidence, 4(1).Fan, Rong-En, Kai-Wei Chang, Cho-JuiHsieh, Xiang-Rui Wang, and Chih-Jen Lin.2008.
LIBLINEAR: A library for largelinear classification.
Journal of MachineLearning Research, 9:1871?1874.Fog, Agner.
2008.
Calculation methods forWallenius?
noncentral hypergeometricdistribution.
Communications in Statistics,Simulation and Computation, 37(2):258?273.Griffiths, Thomas L. and Mark Steyvers.2004.
Finding scientific topics.
Proceedingsof the National Academy of Sciences,101(Suppl.
1):5228?5235.Griffiths, Thomas L., Mark Steyvers,David M. Blei, and Joshua B. Tenenbaum.2004.
Integrating topics and syntax.
InNIPS 2004: Proceedings of the 18th AnnualConference on Neural Information ProcessingSystems, pages 537?544, Vancouver.Groves, Trish.
2010.
Is open peer review thefairest system?
Yes.
BMJ, 341:c6424.Herlocker, Jonathan L., Joseph A. Konstan,Al Borchers, and John Riedl.
1999.
Analgorithmic framework for performingcollaborative filtering.
In SIGIR 1999:Proceedings of the 22nd InternationalACM SIGIR Conference on Research and307Computational Linguistics Volume 40, Number 2Development in Information Retrieval,pages 230?237, Berkeley, CA.Juola, Patrick.
2004.
Ad-hoc authorshipattribution competition.
In ALLC-ACH2004: Proceedings of the 2004 JointInternational Conference of the Association forLiterary and Linguistic Computing and theAssociation for Computers and theHumanities, pages 175?176, Go?teborg.Juola, Patrick.
2006.
Authorship attribution.Foundations and Trends in InformationRetrieval, 1(3):233?334.Kacmarcik, Gary and Michael Gamon.2006.
Obfuscating document stylometryto preserve author anonymity.
InCOLING-ACL 2006: Proceedings of the 21stInternational Conference on ComputationalLinguistics and 44th Annual Meeting of theAssociation for Computational Linguistics(Main Conference Poster Sessions),pages 444?451, Sydney.Kern, Roman, Christin Seifert, MarioZechner, and Michael Granitzer.
2011.Vote/veto meta-classifier for authorshipidentification.
In CLEF 2011: Proceedings ofthe 2011 Conference on Multilingual andMultimodal Information Access Evaluation(Lab and Workshop Notebook Papers),Amsterdam.Koppel, Moshe and Jonathan Schler.
2004.Authorship verification as a one-classclassification problem.
In ICML 2004:Proceedings of the 21st InternationalConference on Machine Learning,pages 62?68, Banff.Koppel, Moshe, Jonathan Schler, andShlomo Argamon.
2009.
Computationalmethods in authorship attribution.Journal of the American Society forInformation Science and Technology,60(1):9?26.Koppel, Moshe, Jonathan Schler, andShlomo Argamon.
2011.
Authorshipattribution in the wild.
LanguageResources and Evaluation, 45(1):83?94.Koren, Yehuda, Robert Bell, and ChrisVolinsky.
2009.
Matrix factorizationtechniques for recommender systems.IEEE Computer, 42(8):30?37.Kourtis, Ioannis and Efstathios Stamatatos.2011.
Author identification usingsemi-supervised learning.
In CLEF 2011:Proceedings of the 2011 Conference onMultilingual and Multimodal InformationAccess Evaluation (Lab and WorkshopNotebook Papers), Amsterdam.Lacoste-Julien, Simon, Fei Sha, andMichael I. Jordan.
2008.
DiscLDA:Discriminative learning for dimensionalityreduction and classification.
In NIPS 2008:Proceedings of the 22nd Annual Conferenceon Neural Information Processing Systems,pages 897?904, Vancouver.Liu, Bing and Lei Zhang.
2012.
A survey ofopinion mining and sentiment analysis.
InCharu C. Aggarwal and ChengXiang Zhai,editors, Mining Text Data.
Springer US,pages 415?463.Luyckx, Kim and Walter Daelemans.
2011.The effect of author set size and data sizein authorship attribution.
Literary andLinguistic Computing, 26(1):35?55.Mendenhall, Thomas C. 1887.
Thecharacteristic curves of composition.Science, 9(214S):237?246.Mimno, David and Andrew McCallum.
2008.Topic models conditioned on arbitraryfeatures with Dirichlet-multinomialregression.
In UAI 2008: Proceedingsof the 24th Conference on Uncertainty inArtificial Intelligence, pages 411?418,Helsinki.Mosteller, Frederick and David L. Wallace.1964.
Inference and Disputed Authorship:The Federalist.
Addison-Wesley.Nanavati, Mihir, Nathan Taylor, WilliamAiello, and Andrew Warfield.
2011.Herbert West?deanonymizer.In HotSec?11: Proceedings of the 6thUSENIX Workshop on Hot Topics in Security,San Francisco, CA.Ng, Andrew Y. and Michael I. Jordan.
2001.On discriminative vs. generativeclassifiers: A comparison of logisticregression and naive Bayes.
In NIPS 2001:Proceedings of the 15th Annual Conference onNeural Information Processing Systems,pages 841?848, Vancouver.Pang, Bo and Lillian Lee.
2005.
Seeing stars:Exploiting class relationships for sentimentcategorization with respect to ratingscales.
In ACL 2005: Proceedings of the 43rdAnnual Meeting of the Association forComputational Linguistics, pages 115?124,Ann Arbor, MI.Pang, Bo and Lillian Lee.
2008.
Opinionmining and sentiment analysis.Foundations and Trends in InformationRetrieval, 2(1?2):1?135.Pearl, Lisa and Mark Steyvers.
2012.Detecting authorship deception: Asupervised machine learning approachusing author writeprints.
Literary andLinguistic Computing, 27(2):183?196.Rajkumar, Arun, Saradha Ravi,Venkatasubramanian Suresh,M.
Narasimha Murthy, and C. E. VeniMadhavan.
2009.
Stopwords and308Seroussi, Zukerman, and Bohnert Authorship Attribution with Topic Modelsstylometry: A latent Dirichlet allocationapproach.
In Proceedings of the NIPS2009 Workshop on Applications for TopicModels: Text and Beyond (Poster Session),Whistler.Ramage, Daniel, David Hall, RameshNallapati, and Christopher D. Manning.2009.
Labeled LDA: A supervisedtopic model for credit attribution inmulti-labeled corpora.
In EMNLP 2009:Proceedings of the 2009 Conference onEmpirical Methods in Natural LanguageProcessing, pages 248?256, Singapore.Resnick, Paul and Hal R. Varian.
1997.Recommender systems.
Communicationsof the ACM, 40(3):56?58.Rifkin, Ryan and Aldebaro Klautau.
2004.In defense of one-vs-all classification.Journal of Machine Learning Research,5(Jan):101?141.Rosen-Zvi, Michal, ChaitanyaChemudugunta, Thomas Griffiths,Padhraic Smyth, and Mark Steyvers.2010.
Learning author-topic models fromtext corpora.
ACM Transactions onInformation Systems, 28(1):1?38.Rosen-Zvi, Michal, Thomas Griffiths,Mark Steyvers, and Padhraic Smyth.2004.
The author-topic model for authorsand documents.
In UAI 2004: Proceedingsof the 20th Conference on Uncertainty inArtificial Intelligence, pages 487?494, Banff.Salton, Gerard.
1971.
The SMART RetrievalSystem?Experiments in AutomaticDocument Processing.
Prentice-Hall, Inc.,Upper Saddle River, NJ.Salton, Gerard.
1981.
A blueprint forautomatic indexing.
SIGIR Forum,16(2):22?38.Sanderson, Conrad and Simon Guenter.2006.
Short text authorship attribution viasequence kernels, Markov chains andauthor unmasking: An investigation.In EMNLP 2006: Proceedings of the 2006Conference on Empirical Methods in NaturalLanguage Processing, pages 482?491,Sydney.Schler, Jonathan, Moshe Koppel, ShlomoArgamon, and James W. Pennebaker.2006.
Effects of age and gender onblogging.
In Proceedings of AAAI SpringSymposium on Computational Approachesfor Analyzing Weblogs, pages 199?205,Stanford, CA.Seroussi, Yanir.
2012.
Text Mining and RatingPrediction with Topical User Models.
Ph.D.thesis, Faculty of Information Technology,Monash University, Clayton, Victoria,Australia.Seroussi, Yanir, Fabian Bohnert, and IngridZukerman.
2011.
Personalized ratingprediction for new users using latentfactor models.
In HT 2011: Proceedings ofthe 22nd International ACM Conference onHypertext and Hypermedia, pages 47?56,Eindhoven.Seroussi, Yanir, Fabian Bohnert, andIngrid Zukerman.
2012.
Authorshipattribution with author-aware topicmodels.
In ACL 2012: Proceedings of the50th Annual Meeting of the Association forComputational Linguistics (Volume 2:Short Papers), pages 264?269,Jeju Island.Seroussi, Yanir, Russell Smyth, andIngrid Zukerman.
2011.
Ghosts fromthe High Court?s past: Evidence fromcomputational linguistics for Dixonghosting for McTiernan and Rich.University of New South Wales LawJournal, 34(3):984?1005.Seroussi, Yanir, Ingrid Zukerman, andFabian Bohnert.
2010.
Collaborativeinference of sentiments from texts.In UMAP 2010: Proceedings of the18th International Conference on UserModeling, Adaptation and Personalization,pages 195?206, Waikoloa, HI.Seroussi, Yanir, Ingrid Zukerman, and FabianBohnert.
2011.
Authorship attribution withlatent Dirichlet allocation.
In CoNLL 2011:Proceedings of the 15th InternationalConference on Computational NaturalLanguage Learning, pages 181?189,Portland, OR.Stamatatos, Efstathios.
2009.
A survey ofmodern authorship attribution methods.Journal of the American Society forInformation Science and Technology,60(3):538?556.Steyvers, Mark and Tom Griffiths.
2007.Probabilistic topic models.
In Thomas K.Landauer, Danielle S. McNamara,Simon Dennis, and Walter Kintsch,editors, Handbook of Latent SemanticAnalysis.
Lawrence Erlbaum Associates,pages 427?448.Tanguy, Ludovic, Assaf Urieli, BasilioCalderone, Nabil Hathout, andFranck Sajous.
2011.
A multitude oflinguistically-rich features for authorshipattribution.
In CLEF 2011: Proceedingsof the 2011 Conference on Multilingual andMultimodal Information Access Evaluation(Lab and Workshop Notebook Papers),Amsterdam.Teh, Yee Whye, Michael I. Jordan,Matthew J. Beal, and David M. Blei.
2006.309Computational Linguistics Volume 40, Number 2Hierarchical Dirichlet processes.
Journalof the American Statistical Association,101(476):1566?1581.Wallach, Hanna M. 2006.
Topic modeling:Beyond bag-of-words.
In ICML 2006:Proceedings of the 23rd InternationalConference on Machine Learning,pages 977?984, Pittsburgh, PA.Wallach, Hanna M., David Mimno,and Andrew McCallum.
2009.Rethinking LDA: Why priors matter.In NIPS 2009: Proceedings of the23rd Annual Conference on NeuralInformation Processing Systems,pages 1,973?1,981, Vancouver.Wang, Xuerui, Andrew McCallum, andXing Wei.
2007.
Topical N-grams: Phraseand topic discovery, with an applicationto information retrieval.
In ICDM 2007:Proceedings of the 7th IEEE InternationalConference on Data Mining, pages 697?702,Omaha, NE.Webb, Geoffrey I., Janice R. Boughton, andZhihai Wang.
2005.
Not so naive Bayes:Aggregating one-dependence estimators.Machine Learning, 58(1):5?24.Wong, Sze-Meng Jojo, Mark Dras,and Mark Johnson.
2011.
Topic modelingfor native language identification.In ALTA 2011: Proceedings of theAustralasian Language TechnologyAssociation Workshop, pages 115?124,Canberra.Zhu, Jun, Amr Ahmed, and Eric P. Xing.2009.
MedLDA: Maximum marginsupervised topic models for regression andclassification.
In ICML 2009: Proceedings ofthe 26th International Conference on MachineLearning, pages 1,257?1,264, Montreal.Zhu, Jun and Eric P. Xing.
2010.
Conditionaltopic random fields.
In ICML 2010:Proceedings of the 27th InternationalConference on Machine Learning,pages 1,239?1,246, Haifa.310
