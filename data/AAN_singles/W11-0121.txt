Elaborating a Knowledge Base for Deep Lexical SemanticsNiloofar Montazeri and Jerry R. HobbsInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, CaliforniaAbstractWe describe the methodology for constructing axioms defining event-related words, anchoredin core theories of change of state and causality.
We first derive from WordNet senses a smallerset of abstract, general ?supersenses?.
We encode axioms for these, and we test them on textualentailment pairs.
We look at two specific examples in detail to illustrate both the power of themethod and the holes in the knowledge base that it exposes.
Then we address the problem of holesmore systematically, asking, for example, what kinds of ?pairwise interactions?
are possible for coretheory predicates like change and cause.11 IntroductionFrom the sentenceRussia is blocking oil from entering Ukraine.we would like to be able to concludeOil can not be delivered to Ukraine.But doing this requires fairly complex inference, because the words ?block?, ?enter?, ?can?, ?not?
and?deliver?
carve up the world in different ways.
Our approach is to define words such as these by meansof axioms that link with underlying core theories2 explicating such very basic concepts as change ofstate and causality.
Given the logical form of sentences like these two, we apply these axioms to expressthe meaning of the sentences in more fundamental predicates, and do a certain amount of defeasiblereasoning in the core theories to determine that the second follows from the first.More generally, we are engaged in an enterprise we call ?deep lexical semantics?
(Hobbs, 2008), inwhich we develop various core theories of fundamental commonsense phenomena and define Englishword senses by means of axioms using predicates explicated in these theories.
Among the core theoriesare cognition, microsociology, and the structure of events.
The last of these is the focus of this paper.
Weuse textual entailment pairs like the above to test out subsets of related axioms.
This process enforces a1This research was supported in part by the Defense Advanced Research Projects Agency (DARPA) Machine ReadingProgram under Air Force Research Laboratory (AFRL) prime contract no.
FA8750-09-C-0172, and in part by the Office ofNaval Research under contract no.
N00014-09-1-1029.. Any opinions, findings, and conclusion or recommendations expressedin this material are those of the author(s) and do not necessarily reflect the view of the DARPA, AFRL, ONR, or the USgovernment.2http://www.isi.edu/ hobbs/csk.html.195uniformity in the way axioms are constructed, and also exposes missing inferences in the core theories.The latter is a major issue in this paper.In Section 2 we describe three aspects of the framework we are working in?the logical form weuse, abductive interpretation and defeasibility, and the core theories of change of state and causality.
InSection 3 we describe the methodology we use for constructing axioms, deriving from WordNet sensesa smaller set of abstract, general ?supersenses?, encoding axioms for these, and testing them on textualentailment pairs.
In Section 4 we look at two specific examples to illustrate both the power of the methodand the holes in the knowledge base that it exposes.
In Section 5 we address the problem of holes moresystematically, specifically asking, for example, what kinds of ?pairwise interactions?
are possible forcore theory predicates like change and cause.2 FrameworkWe use a logical notation in which states and events (eventualities) are reified.
Specifically, if the ex-pression (p x) says that p is true of x, then (p?
e x) says that e is the eventuality of p being trueof x. Eventuality e may exist in the real world (Rexist), in which case (p x) holds, or it may onlyexist in some modal context, in which case that is expressed simply as another property of the possibleindividual e. (In this paper we use a subset of Common Logic3 for the syntax of our notation.
)The logical form of a sentence is a flat conjunction of existentially quantified postive literals, withabout one literal per morpheme.
(For example, logical words like ?not?
and ?or?
are treated as expressingpredications about possible eventualities.)
We have developed software4 to translate Penn TreeBank-styletrees (as well as other syntactic formalisms) into this notation.
The underlying core theories are expressedas axioms in this notation (Hobbs, 1985).The interpretation of a text is taken to be the lowest-cost abductive proof of the logical form ofthe text, given the knowledge base.
That is, to interpret a text we prove the logical form, allowing forassumptions at cost, and pick the lowest-cost proof.
Factors involved in computing costs include, besidesthe number of assumptions, the salience of axioms, the plausibility of axioms expressing defeasibleknowledge, and consiliance or the degree to which the pervasive implicit redundancy of natural languagetexts is exploited.
We have demonstrated that many interpretation problems are solved as a by-productof finding the lowest-cost proof.
This method has been implemented in an abductive theorem-provercalled Mini-Tacitus5 that has been used in a number of applications (Hobbs et al, 1993; Mulkar et al,2007), and is used in the textual entailment problems described here.
We are also working toward aprobabilistic semantics for the cost of proofs (Blythe et al, 2011).
Abductive interpretation accounts forscript-like understanding of text?a script predicate provides the most economical interpretation (Hobbset al, 1993)?but also enables interpretation of novel texts.Most commonsense knowledge is defeasible, i.e., it can be defeated.
This is represented in ourframework by having a unique ?et cetera?
proposition in the antecedent of Horn clauses that cannot beproved but can be assumed at a cost corresponding to the likeliehood that the conclusion is true.
Forexample, the axiom(forall (x) (if (and (bird x)(etc-i x))(fly x)))would say that if x is a bird and other unspecified conditions hold, (etc-i), then x flies.
No otheraxioms enable proving (etc-i x), but it can be assumed, and hence participate in the lowest cost3http://common-logic.org/.4http://www.rutumulkar.com/download/NL-Pipeline/NL-Pipeline.php.5http://rutumulkar.com/download/TACITUS/tacitus.php.196proof.
The index i is unique to this axiom.
In this paper rather than invent new indices for each axiom, wewill use the abbreviation (etc) to indicate the defeasibility of the rule.
(This approach to defeasibilityis similar to circumscription (McCarthy, 1980).
)We have articulated a number of core theories6.
The two most relevant to this paper are the theoryof change of state and the theory of causality.
The predication (change?
e e1 e2) says that e isa change of state whose initial state is e1 and whose final state is e2.
The chief properties of changeare that there is some entity whose state is undergoing change, that change is defeasibly transitive,that e1 and e2 cannot be the same unless there has been an intermediate state that is different, and thatchange is consistent with the before relation from our core theory of time.
Since many lexical itemsfocus only on the initial or the final state of a change, we introduce for convenience the predications(changeFrom?
e e1) and (changeTo?
e e2), defined in terms of change.The chief distinction in our core theory of causality is between the notions of causalComplex andcause.
A causal complex includes all the states and events that have to happen or hold in order for theeffect to happen.
A cause is that contextually relevant element of the causal complex that is somehowcentral to the effect, whether because it is an action the agent performs, because it is not normally true,or for some other reason.
Most of our knowledge about causality is expressed in terms of the predicatecause, rather than in terms of causal complexes, because we rarely if ever know the complete causalcomplex.
Typically planning, explanation, and the interpretation of texts (though not diagnosis) involvesreasoning about cause.
Among the principal properties of cause are that it is defeasibly transitive,that events defeasibly have causes, and that cause is consistent with before.We also have a core theory of time, and the times of states and events can be represented as temporalproperties of the reified eventualities.
The theory of time has an essential function in axioms for wordsexplicitly referencing time, such as ?schedule?
and ?delay?.
But for most of the words we are explicatingin this effort, we base our approach to the dynamic aspects of the world on the cognitively more basictheory of change of state.
For example, the word ?enter?
is axiomatized as a change of state from beingoutside to being inside, and the fact that being outside comes before being inside follows from the axiomrelating the predicates change and before.We find that reifying states and events as eventualities and treating them as first-class individuals ispreferable to employing the event calculus (Gruninger and Menzel, 2010; Mueller, 2006) which makes asharp distinction between the two, because language makes no distinction in where they can appear andwe can give them a uniform treatment.3 MethodologyOur methodology consists of three steps.1.
Analyzing the structure of a word?s WordNet senses.2.
Writing axioms for the most general senses3.
Testing the axioms on textual entailment pairs.Our focus in this paper is on words involving the concepts of change of state and causality, or eventwords, such as ?block?, ?delay?, ?deliver?, ?destroy?, ?enter?, ?escape?, ?give?, ?hit?, ?manage?, and?provide?.
For each word, we analyze the structure of its WordNet senses.
Typically, there will be pairsthat differ only in, for example, constraints on their arguments or in that one is inchoative and the other6http://www.isi.edu/ hobbs/csk.html.197causative.
This analysis generally leads to a radial structure indicating how one sense leads by incre-ments, logically and perhaps chronologically, to another word sense (Lakoff, 1987).
The analysis alsoleads us to posit ?supersenses?
that cover two or more WordNet senses.
(Frequently, these supersensescorrespond to senses in FrameNet (Baker et al, 2003) or VerbNet (Kipper et al, 2006), which tend to becoarser grained; sometimes the desired senses are in WordNet itself.
)For example, for the verb ?enter?, three WordNet senses involve a change into a state:V2: become a participantV4: play a part inV9: set out on an enterpriseCall this supersense S1.
Two other senses add a causal role to this:V5: make a record ofV8: put or introduce into somethingTwo more senses specialize supersense S1 by restricting the target state to be in a physical location:V1: come or go intoV6: come on stageOne other sense specializes S1 by restricting the target state to be membership in a group.V3: register formally as a participant or memberKnowing this radial structure of the senses helps enforce uniformity in the construction of the axioms.
Ifthe senses are close, their axioms should be almost the same.We are currently only constructing axioms for the most general or abstract senses or supersenses.In this way, although we are missing some of the implications of the more specialized senses, we arecapturing the most basic topological structure in the meanings of the words.
Moreover, the specializedsenses usually tap into some specialized domain that needs to be axiomatized before the axioms for thesesenses can be written.In constructing the axioms in the event domain, we are very much informed by the long tradition ofwork on lexical decomposition in linguistics (e.g., Gruber, 1965; Jackendoff, 1972).
Our work differsfrom this in that our decompositions are done as logical inferences and not as tree transformations as inthe earliest linguistic work, they are not obligatory but only inferences that may or may not be part of thelowest-cost abductive proof, and the ?primitives?
into which we decompose the words are explicated intheories that enable reasoning about the concepts.Figure 1 shows the radial structure of the senses for the word ?enter?, together with the axioms thatcharacterize each sense.
A link between two word senses means an incremental change in the axiom forone gives the axiom for the other.
For example, the axiom for enter-S2 says that if x1 enters x2 inx3, then x1 causes a change to the eventuality i1 in which x2 is in x3; and the expanded axiom forenter-S1.1 states that if x1 enters x2, then there is a change to a state e1 in which x1 is in x2.
Soenter-S2 and enter-S1.1 are closely related and thus linked together.Abstraction is a special incremental change where one sense S1.1 specializes another sense S1 ei-ther by adding more predicates to or specializing some of the predicates in S1?s axiom.
We representabstractions via arrows pointing from the subsenses to the supersenses.
In Figure 1, enter-S1.1 andenter-S1.2 both specialize enter-S1.
The predicate enter-S1.1 adds an extra predicate de-scribing e1 as an in eventuality and enter-S1.2 specializes e1 to membership in x2, where x2 is agroup.198Figure 1: Senses of and axioms for the verb ?enter?The supersenses capture the basic topology of the senses they subsume.
The extra information thatthe subsenses convey are typically the types and properties of the arguments, such as being a place or aprocess, or qualities of the causing event, such as being sudden or forceful.For each set of inferentially related words we construct textual entailment pairs, where the hypothesis(H) intuitively follows from text (T), and use these for testing and evaluation.
The person writing theaxioms does not know what the pairs are, and the person constructing the pairs does not know what theaxioms look like.The ideal test then is whether given a knowledge base K consisting of all the axioms, H cannot beproven from K alone, but H can be proven from the union of K and the best intepretation of T. This isoften too stringent a condition, since H may contain irrelevant material that doesn?t follow from T, so analternative is to determine whether the lowest cost abductive proof of H given K plus T is substantiallylower than the lowest cost abductive proof of H given K alone, where ?substantially lower?
is defined bya threshold that can be trained (Ovchinnikova et al, 2011).4 Two ExamplesHere we work through two examples to illustrate how textual entailment problems are handled in ourframework.
In these examples, given a text T and a hypothesis H, we ask if H can be proven from T,perhaps with a small number of low-cost assumptions.Because the examples we deal with involve a great deal of embedding, we need to use the primedpredicates, keeping the eventuality arguments explicit.We also assume in these examples that lexical disambiguation has been done correctly.
With morecontext, lexical disambiguation should fall out of the best interpretation, but it is unreasonable to ex-pect that in these short examples.
In practice we run the examples both with disambiguated and withnondisambiguated predicates.In these examples we do not show the costs, although they are used by our system.The first example is the pairT: Russia is blocking oil from entering Ukraine.H: Oil cannot be delivered to Ukraine.199The relevant part of the logical form of the text is(and (block-V3?
b1 x1 e1)(enter-S2?
e1 o1 u1))That is, there is a blocking event b1 in which Russia x1 blocks eventuality e1 from occurring, and e1 isthe eventuality of oil o1 entering Ukraine u1.
The -V3 on block indicates that it is the third WordNetsense of the verb ?block?
and the -S2 suffix on enter indicates that it is the second supersense of?enter?.The relevant part of the logical form of the hypothesis is(and (not?
n2 c2) (can-S1?
c2 x2 d2) (deliver-S2?
d2 x2 o2 u2))That is, n2 is the eventuality that c2 is not the case, where c2 is some x2?s being able to do d2, whered2 is x2?s delivering oil o2 to Ukraine u2.
Note that we don?t know yet that the oil and Ukraine in thetwo sentences are coreferential.The axiom relating the third verb sense of ?block?
to the underlying core theories isAX4: (forall (c1 x1 e1)(if (block-V3?
c1 x1 e1)(exist (n1 p1)(and (cause?
c1 x1 n1)(not?
n1 p1)(possible?
p1 e1)))))This rule says that for x1 to block some eventuality e1 is for x1 to cause e1 not to be possible.
(In thisexample, for expositional simplicity, we have allowed the eventuality c1 of blocking be the same as theeventuality of causing, where properly they should be closely related but not identical.
)The other axioms needed in this example areAX1: (forall (c1 e1)(if (and (possible?
c1 e1)(etc))(exist (x1)(can-S1?
c1 x1 e1))))AX2: (forall (d1 x1 c1 r1 x2 x3)(if (and (cause?
d1 x1 c1)(changeTo?
c1 r1)(rel?
r1 x2 x3)(deliver-S2?
d1 x1 x2 x3))))AX3: (forall (c1 x1 x2)(if (enter-S2?
c1 x1 x2)(exist (i1)(and changeTo?
c1 i1)(in?
i1 x1 x2))))AX1 says that defeasibly, if an eventuality e1 is possible, then someone can do it.
AX2 says that if x1causes a change to a situation r1 in which x2 in in some relation to x3, then in a very general sense(S2), x1 has delivered x2 to x3.
AX3 says that if c1 is the eventuality of x1 entering x2, then c1 isthe change into a state i1 in which x1 is in x2.Starting with the logical form of H as the initial interpretation and applying axioms AX1 and AX2,we get interpretation H1:H1: (and (not?
n2 c2) (possible?
c2 d2) (cause?
d2 x2 c1)(changeTo?
c1 r1)(rel?
r1 o2 u2))At this point we are stuck in our effort to back-chain to T. An axiom is missing, namely, one that saysthat ?in?
is a relation between two entities.AX5: (forall (r x1 x2) (if (in?
r1 x1 x2)(rel?
r1 x1 x2)))Using AX5, we can back-chain from H1 and derive interpretation H2:H2: (and (not?
n2 c2)(possible?
c2 d2)(cause?
d2 x2 c1)(changeTo?
c1 r1)(in?
r1 o2 u2))We can then further back-chain with AX3 to interpretation H3:200H3: (and (not?
n2 c2)(possible?
c2 d2)(cause?
d2 x2 c1)(enter-S2?
c1 o2 u2))Again, we need a missing axiom, AX6, to get closer to the logical form of T:AX6: (forall (p e1)(if (and (possible?
p,e1)(etc))(exist (c x1) (and (possible?
p c)(cause?
c x1 e1)))))That is, if something is possible, it is possible for something to cause it.
Using this axiom, we can deriveH4: (and (not?
n2 c2)(possible?
c2 c1)(enter-S2?
c1 o2 u2))The final missing axiom, AX7, says that if x1 causes eventuality c2 not to occur, then c2 doesn?t occur.AX7: (forall (n x1 n1 c2)(if (and (cause?
n x1 n1)(not?
n1 c2))( not?
n c2)))Using this we derive interpretation H5.H5: (and (cause?
n2 x3 n)(not?
n c2)(possible?
c2 c1)(enter-S2?
c1 o2 u2))We can now apply the rule for ?block?, identifying b1 and n2, x1 and x3, e1 and c1, o1 and o2, andu1 and u2, yielding H6 and establishing the entailment relation between H and T.H6: (and (block-V3?
n2 x3 c1)(enter-S2?
c1 o2 u2))Our second example is the text-hypothesis pairT: The plane managed to escape the attack.H: The plane was not captured.The relevant parts of the logical forms of T and H are as follows:T: (and (manage-V1?
m1 p1 e1)(escape-S1?
e1 p1 a1))H: (and (not?
n2 c2)(capture-S1?
c2 x2 p2))The axioms relating these words to the core theories are as follows:AX1: (forall (cp c x2 n chf a y1 x3 y0 x2)(if (and (changeTo?
cp c)(cause?
c x2 n)(not?
n chf)(changeFrom?
chf a)(at?
a y1 x3)(arg?
y0 x2))(capture?
cp y0 y1)))AX2: (forall (es x0 x1)(if (escape?
es x0 x1)(exist (ch a)(and (cause?
es x0 ch)(changeFrom?
ch a)(at?
a x0 x1)))))AX3: (forall (m y0 e1)(if (manage?
m y0 e1) (Rexist (m e1))))201The first says that a change to a situation in which x2 is causing y1 not to change location is a capturingby some y0 of y1.
The second says that escaping implies causing a change from being at a location.The third says that if you manage to do e1, then e1 occurs.Using these axioms, we would like to establish the entailment relation from T to H. However, inorder for this reasoning to go through, we need several more axioms?saying that if an eventuality doesnot hold, there has been no change to that eventuality, and nothing has caused it to occur; that doublenegation cancels out; and that if something is caused, it occurs.It may seem at first blush that any new text-hypothesis pair will reveal new axioms that must beencoded, and that therefore it is hopeless ever to achieve completeness in the theories.
But a closerexamination reveals that the missing axioms all involve relations among the most fundamental predicates,like cause, change, not, and possible.
These are axioms that should be a part of the core theoriesof change and causality.
They are not a random collection of facts, any one of which may turn out tobe necessary for any given example.
Rather we can investigate the possibilities systematically.
Thatinvestigation is what we describe in the following section.5 Relations among Fundamental PredicatesFor completeness in the core theories, we need to look at pairs of fundamental predicates and ask what re-lations hold between them, what their composition yields, and for each such axiom whether it is defeasi-ble or indefeasible.
The predicates we consider are possible, Rexist, not, cause, changeFrom,and changeTo.The first type of axiom formulates the relationship between two predicates.
For example, the rulerelating cause and Rexist is(forall (x e) (if (cause x e)(Rexist e)))That is, if something is caused, then it actually occurs.
Other rules of this type are as follows:(forall (x e) (if (Rexist e)(possible e)))(forall (e) (if (and (Rexist e)(etc))(exist (x)(cause x e))))(forall (e2)(if (changeTo e2)(exist (e1)(and (changeFrom e1)(not?
e1 e2)))))(forall (e1)(if (changeFrom e1)(exist (e2)(and (changeTo e2)(not?
e2 e1)))))(forall (e) (if (changeTo e)(Rexist e)))(forall (e) (if (changeFrom e)(not e)))(forall (e) (if (and (Rexist e)(etc))(changeTo e)))That is, if something occurs, it is possible and, defeasibly, something causes it.
If there is a change tosome state obtaining, then there is a change from its not obtaining, and vice versa.
If there is a changeto something, then it obtains, and if there is a change from something, then it no longer obtains.
If somestate obtains, then defeasibly there was a change from something else to that state obtaining.The second type of axiom involves the composition of predicates, and gives us rules of the form202(forall (e1 e2 x) (if (and (p?
e1 e2)(q?
e2 x)) (r?
e1 x)))That is, when p is applied to q, what relation r do we get?Figure 2 shows the axioms encoding these compositions.
The rows correspond to the (p?
e1e2)?s and the columns correspond to the (q?
e2 x)?s, and the cell contains the consequents (r?
e1x).
If the rule is defeasible, the cell indicates that by adding (etc) to the antecedent.
The consequentsin italics are derivable from other rules.Figure 2: Axioms expressing compositions of fundamental predicatesFor example, in the possible-possible cell, the rule says that if it is possible that somethingis possible, then it is possible.
To take a more complex example, the changeFrom-cause cell saysthat if there is a change from some entity causing (or maintaining) a state, then defeasibly there will be achange from that state.
So if a glass is released, it will fall.We have also looked at axioms whose pattern is the converse of those in Figure 2.
For example, ifsomething does not hold, then it was not caused.
Many of the axioms used in the examples are of thissort.6 ConclusionIf we are ever to have sophisticated natural language understanding, our systems will have to be able todraw inferences like the ones illustrated here, and therefore they will need axioms of this complexity orsomething equivalent.
Because of their complexity, we cannot expect to be able to acquire the axiomsautomatically by statistical methods.
But that does not mean the situation is bleak.
We have shown inthis paper that there is a systematic methodology for developing axioms characterizing the meanings ofwords in a way that enforces uniformity and for elaborating the core theories these axioms are anchoredin.
Doing this for several thousand of the most common words in English would produce a huge gain inthe inferential power of our systems, as illustrated by the textual entailment examples in this paper, andwould be an enterprise no greater in scope than the manual construction of other widely used resourcessuch as WordNet and FrameNet.203References[1] Baker, C., Fillmore, C., Cronin, B.: The Structure of the Framenet Database, International Journal of Lexi-cography, Volume 16.3: (2003) 281-296.
[2] J. Blythe, J. Hobbs, P. Domingos, R. Kate, and R. Mooney, 2011.
?Implementing Weighted Abduction inMarkov Logic?, Proceedings of the 9th International Conference on Computational Semantics, Oxford,United Kingdom.
[3] Gruber, Jeffrey C., 1965.
Studies in Lexical Relations, unpublished Ph.D. dissertation, Massachusetts Instituteof Technology, Cambridge, Massachusetts.
[4] Gruninger, Michael, and Christopher Menzel, 2010.
?The Process Specification Language (PSL) Theory andApplications?, AI Magazine, Vol 24, No 3.
[5] Hobbs, Jerry R. 1985.
?Ontological Promiscuity.?
Proceedings, 23rd Annual Meeting of the Association forComputational Linguistics, pp.
61-69.
Chicago, Illinois, July 1985.
[6] Hobbs, Jerry R., 2008.
?Deep Lexical Semantics?, Proceedings, 9th International Conference on IntelligentText Processing and Computational Linguistics (CICLing-2008), Haifa, Israel, February 2008.
[7] Hobbs, Jerry R., Mark Stickel, Douglas Appelt, and Paul Martin, 1993.
?Interpretation as Abduction?, Arti-ficial Intelligence, Vol.
63, Nos.
1-2, pp.
69-142.
[8] Jackendoff, Ray S. 1972.
Semantic interpretation in generative grammar.
Cambridge, MA: The MIT Press.
[9] Kipper, Karin, Anna Korhonen, Neville Ryant, and Martha Palmer (2006) Extensive Classifications of En-glish verbs.
Proceedings, 12th EURALEX International Congress.
Turin, Italy.
September, 2006.
[10] Lakoff, George, 1987.
Women, Fire, and Dangerous Things: What Categories Reveal About the Mind, Uni-versity of Chicago Press, Chicago.
[11] McCarthy, John, 1980.
?Circumscription: A Form of Nonmonotonic Reasoning?, Artificial Intelligence, 13:27-39.
[12] Mueller, Erik T., 2006.
Commonsense Reasoning, Morgan Kaufmann Publishers, Inc., San Mateo, California.
[13] R. Mulkar, J.R. Hobbs, and E. Hovy, 2007.
?Learning from Reading Syntactically Complex Biology Texts?,Proceedings of the AAAI Spring Symposium Commonsense?07.
Stanford University, CA, 2007.
[14] E. Ovchinnikova, N. Montazeri, T. Alexandrov, J. Hobbs, M. McCord, and R. Mulkar-Mehta, 2011.
?Abduc-tive Reasoning with a Large Knowledge Base for Discourse Processing?, Proceedings of the 9th InternationalConference on Computational Semantics, Oxford, United Kingdom.204
