Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 76?85,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsTowards Probabilistic Acceptors and Transducers for Feature StructuresDaniel QuernheimInstitute for Natural Language ProcessingUniversita?t Stuttgart, GermanyPfaffenwaldring 5b, 70569 Stuttgartdaniel@ims.uni-stuttgart.deKevin KnightUniversity of Southern CaliforniaInformation Sciences InstituteMarina del Rey, California 90292knight@isi.eduAbstractWeighted finite-state acceptors and transduc-ers (Pereira and Riley, 1997) are a criticaltechnology for NLP and speech systems.
Theyflexibly capture many kinds of stateful left-to-right substitution, simple transducers can becomposed into more complex ones, and theyare EM- trainable.
They are unable to han-dle long-range syntactic movement, but treeacceptors and transducers address this weak-ness (Knight and Graehl, 2005).
Tree au-tomata have been profitably used in syntax-based MT systems.
Still, strings and trees areboth weak at representing linguistic structureinvolving semantics and reference (?who didwhat to whom?).
Feature structures providean attractive, well-studied, standard format(Shieber, 1986; Rounds and Kasper, 1986),which we can view computationally as di-rected acyclic graphs.
In this paper, we de-velop probabilistic acceptors and transducersfor feature structures, demonstrate them onlinguistic problems, and lay down a founda-tion for semantics-based MT.1 IntroductionWeighted finite-state acceptors and transducers(Pereira and Riley, 1997) provide a clean andpractical knowledge representation for string-basedspeech and language problems.
Complex problemscan be broken down into cascades of simple trans-ducers, and generic algorithms (best path, composi-tion, EM, etc) can be re-used across problems.String automata only have limited memory andcannot handle complex transformations needed inmachine translation (MT).
Weighted tree acceptorsand transducers (Ge?cseg and Steinby, 1984; Knightand Graehl, 2005) have proven valuable in these sce-narios.
For example, systems that transduce sourcestrings into target syntactic trees performed well inrecent MT evaluations (NIST, 2009).To build the next generation of language systems,we would like to represent and transform deeper lin-guistic structures, e.g., ones that explicitly capturesemantic ?who does what to whom?
relationships,with syntactic sugar stripped away.
Feature struc-tures are a well-studied formalism for capturing nat-ural language semantics; Shieber (1986) and Knight(1989) provide overviews.
A feature structure is de-fined as a collection of unordered features, each ofwhich has a value.
The value may be an atomic sym-bol, or it may itself be another feature structure.
Fur-thermore, structures may be re-entrant, which meansthat two feature paths may point to the same value.Figure 1 shows a feature structure that capturesthe meaning of a sample sentence.
This seman-tic structure provides much more information thana typical parse, including semantic roles on bothnouns and verbs.
Note how ?Pascale?
plays fourdifferent semantic roles, even though it appears onlyonce overtly in the string.
The feature structure alsomakes clear which roles are unfilled (such as theagent of the charging), by omitting them.
For com-putational purposes, feature structures are often rep-resented as rooted, directed acyclic graphs with edgeand leaf labels.While feature structures are widely used in hand-built grammars, there has been no compelling pro-posal for weighted acceptors and transducers for76???????????????????????????
?INSTANCE chargeTHEME 1[INSTANCE personNAME ?Pascale?]PRED??????????????????
?INSTANCE andOP1?????
?INSTANCE resistAGENT 1THEME[INSTANCE arrestTHEME 1]??????OP2???
?INSTANCE intoxicateTHEME 1LOCATION[INSTANCE public]??????????????????????????????????????????????????
?chargeandresistarrestintoxicateperson Pascalepublicinstance predthemeinstance op1op2instance themeagentinstancethemeinstancethemelocationinstance nameinstanceCHARGEANDRESIST INTOXICATEARRESTPERSON PUBLICPASCALECHARGE 7?
charge(theme, pred)AND 7?
and(op1, op2)RESIST 7?
resist(agent, theme)ARREST 7?
arrest(theme)INTOXICATE 7?
intoxicate(theme, location)PUBLIC 7?
public()PERSON 7?
person(name)PASCALE 7?
?Pascale?Figure 1: A feature structure representing the semantics of ?Pascale was charged with resisting arrest and public intox-ication,?
the corresponding dag, and the simplified dag with argument mapping.
Dag edges always point downward.FSAfstring?nl-XTOPs?1translate ?RTGetree?FSArank ?FSAestringfstring?
parse + understand ?
esem?
rank ?
esem?
generate ?
etree?
rank ?
estringFigure 2: Pipelines for syntax-based and for semantics-based MT.
Devices: FSA = finite string automaton;ln-XTOPs = linear non-deleting extended top-down tree-to-string transducer; RTG = regular tree grammar.77string automata tree automata graph automatak-best .
.
.
paths through a WFSA(Viterbi, 1967; Eppstein, 1998).
.
.
trees in a weighted forest(Jime?nez and Marzal, 2000;Huang and Chiang, 2005)?EM training Forward-backward EM (Baumet al, 1970; Eisner, 2003)Tree transducer EM training(Graehl et al, 2008)?Determinization .
.
.
of weighted string acceptors(Mohri, 1997).
.
.
of weighted tree acceptors(Borchardt and Vogler, 2003;May and Knight, 2006a)?Transducer com-positionWFST composition (Pereira andRiley, 1997)Many transducers not closed un-der composition (Maletti et al,2009)?General tools AT&T FSM (Mohri et al,2000), Carmel (Graehl, 1997),OpenFST (Riley et al, 2009)Tiburon (May and Knight,2006b)?Table 1: General-purpose algorithms for strings, trees and feature structures.them.
Such automata would be of great use.
Forexample, a weighted graph acceptor could form thebasis of a semantic language model, and a weightedgraph-to-tree transducer could form the basis of anatural language understanding (NLU) or genera-tion (NLG) system, depending on which directionit is employed.
Putting NLU and NLG together,we can also envision semantics-based MT systems(Figure 2).
A similar approach has been takenby Graham et al (2009) who incorporate LFG f-structures, which are deep syntax feature structures,into their (automatically acquired) transfer rules.Feature structure graph acceptors and transducerscould themselves be learned from semantically-annotated data, and their weights trained by EM.However, there is some distance to be traveled.Table 1 gives a snapshot of some efficient, genericalgorithms for string automata (mainly developed inthe last century), plus algorithms for tree automata(mainly developed in the last ten years).
These algo-rithms have been packaged in general-purpose soft-ware toolkits like AT&T FSM (Mohri et al, 2000),OpenFST (Riley et al, 2009), and Tiburon (Mayand Knight, 2006b).
A research program for graphsshould hold similar value.Formal graph manipulation has, fortunately, re-ceived prior attention.
A unification grammarcan specify semantic mappings for strings (Moore,1989), effectively capturing an infinite set ofstring/graph pairs.
But unification grammars seemtoo powerful to admit the efficient algorithms wedesire in Table 1, and weighted versions are notpopular.
Hyperedge replacement grammars (Dreweset al, 1997; Courcelle and Engelfriet, 1995)are another natural candidate for graph acceptors,and a synchronous hyperedge replacement gram-mar might serve as a graph transducer.
Finally,Kamimura and Slutzki (1981, 1982) propose graphacceptor and graph-to-tree transducer formalismsfor rooted directed acyclic graphs.
Their model hasbeen extended to multi-rooted dags (Bossut et al,1988; Bossut and Warin, 1992; Bossut et al, 1995)and arbitrary hypergraphs (Bozapalidis and Kalam-pakas, 2006; Bozapalidis and Kalampakas, 2008);however, these extensions seem too powerful forNLP.
Hence, we use the model of Kamimura andSlutzki (1981, 1982) as a starting point for our def-inition, then we give a natural language example,followed by an initial set of generic algorithms forgraph automata.2 PreliminariesIn this section, we will define directed acyclic graphswhich are our model for semantic structures.Let us just define some basic notions: We willwrite R for the real numbers.
An alphabet is justa finite set of symbols.Intuitively, a rooted ordered directed acyclicgraph, or dag for short, can be seen as a tree thatallows sharing of subtrees.
However, it is not nec-essarily a maximally shared tree that has no isomor-phic subtrees (consider the examples in Figure 3).78(3a)WANTBELIEVEBOY GIRL (3b)WANTBELIEVEBOY BOY GIRLFigure 3: Maximally shared tree (a) and not maximallyshared tree (b; note the two BOY nodes) can be distinctdags.
The dag in (a) means ?The boy wants to believe thegirl,?
while the dag in (b) means ?The boy wants someother boy to believe the girl.?
(4a)>BELIEVEBOY GIRL (4b)>BOY GIRLFigure 4: Subdag of dag (3a) and subdag of dag (3b) inFigure 3.More formally, we define a directed graph over analphabet ?
as a triple G = (V,E, `) of a finite set ofnodes V , a finite set of edgesE ?
V ?V connectingtwo nodes each and a labeling function ` : V ?
?.We say that (v, w) is an outgoing edge of v and anincoming edge of w, and we say that w is a child ofv and v is a parent of w. A directed graph is a dag ifit is?
acyclic: V is totally ordered such that there isno (v, w) ?
E with v > w;?
ordered: for each V , there is a total order bothon the incoming edges and the outgoing edges;?
and rooted: min(V ) is transitively connectedby to all other nodes.This is a simplified account of the dags presented inSection 1.
Instead of edge-labels, we will assumethat this information is encoded explicitely in thenode-labels for the INSTANCE feature and implic-itly in the node-labels and the order of the outgoingedges for the remaining features.
Figure 1 shows afeature structure and its corresponding dag.
Nodeswith differently-labeled outgoing edges can thus bedifferentiated.
Since the number of ingoing edges isnot fixed, a node can have arbitrary many parents.For instance, the PERSON node in Figure 1 has fourparents.
We call the number of incoming edges of agiven node its head rank, and the number of outgo-ing edges its tail rank.(left)WANT?
(right)WANTBOY GIRLFigure 5: (left) Remainder of (3a) after removing (4a).
(right) Dag resulting from replacing (4a) by (4b) in (3a).We also need incomplete dags in order to com-pose larger dags from smaller ones.
An incompletedag is a dag in which some edges does not nec-essarily have to be connected to two nodes; theycan be ?dangling?
from one node.
We representthis by adding special nodes > and ?
to the dag.If an incomplete dag has m edges (>, v) and nedges (v,?
), we call it an (m,n)-dag.
An (m,n)-dag G can be composed with an (n, o)-dag G?
byidentifying the n downward-dangling edges of Gwith the n upward-dangling edges of G?
in the rightorder; the result G ?
G?
is a (m, o)-dag.
Fur-thermore, two dags H and H ?
of type (m,n) and(m?, n?)
can be composed horizontally by puttingtheir upward-dangling edges next to each other andtheir downward-dangling edges next to each other,resulting in a new (m + m?, n + n?)
dag H ?
H ?.If G1, .
.
.
, G` can be composed (vertically and hor-izontally) in such a way that we obtain G, then Giare called subdags of G.An (m,n)-subdag H of a dag G can be replacedby an (m,n)-subdag H ?, resulting in the dag G?,written G[H ?
H ?]
= G?.
An example is depictedin Figure 5, showing how a dag is split into two sub-dags, of which one is replaced by another incom-plete dag.
Our account of dag replacement is a sim-plified version of general hypergraph replacementthat has been formulated by Engelfriet and Vereijken(1997) and axiomatized by Bozapalidis and Kalam-pakas (2004).Trees are dags where every node has at most oneincoming edge.
Tree substitution is then just a spe-cial case of dag composition.
We will write the set ofdags over an alphabet ?
as D?
and the set of treesover ?
as T?, and T?
(V ) is the set of trees withleaves labeled with variables from the set V .3 Dag acceptors and transducersThe purpose of dag acceptors and dag transducersis to compactly represent (i) a possibly-infinite setof dags, (ii) a possibly-infinite set of (dag, tree)79pairs, and (iii) a possibly-infinite set of (graph, tree,weight) triples.Dag acceptors and dag transducers are a gener-alization of tree acceptors and transducers (Comonet al, 2007).
Our model is a variant of the dagacceptors defined by Kamimura and Slutzki (1981)and the dag-to-tree transducers by Kamimura andSlutzki (1982).
The original definition imposedstricter constraints on the class of dags.
Theirdevices operated on graphs called derivation dags(short: d-dags) which are always planar.
In particu-lar, the authors required all the parents and childrenof a given node to be adjacent, which was due to thefact that they were interested in derivation graphsof unrestricted phrase-structure grammar.
(Whilethe derivation structures of context-free grammar aretrees, the derivation structures of type-0 grammarsare d-dags.)
We dropped this constraint since itwould render the class of dags unsuitable for linguis-tic purposes.
Also, we do not require planarity.Kamimura and Slutzki (1981, 1982) defined threedevices: (i) the bottom-up dag acceptor, (ii) the top-down dag acceptor (both accepting d-dags) and (iii)the bottom-up dag-to-tree transducer (transformingd-dags into trees).
We demonstrate the applicationof a slightly extended version of (ii) to unrestricteddags (semantic dags) and describe a top-down dag-to-tree transducer model, which they did not investi-gate.
Furthermore, we add weights to the models.A (weighted) finite dag acceptor is a structureM = (Q, q0,?, R,w) where Q is a finite set ofstates and q0 the start state, ?
is an alphabet ofnode labels, and R is a set of rules of the formr : ?
?
?, where r is the (unique) rule identifierand (i) ?
?
Qm(?)
and ?
?
r(Qn) for m,n ?
Nand some ?
?
?
(an explicit rule of type (m,n)) or(ii) ?
?
Qm and ?
?
r(Q) (an implicit rule of type(m, 1)).
The function w : R ?
R assigns a weightto each rule.Intuitively, explicit rules consume input, whileimplicit rules are used for state changes and joiningedges only.
The devices introduced by Kamimuraand Slutzki (1981) only had explicit rules.We define the derivation relation of M by rewrit-ing of configurations.
A configuration of M is a dagover ?
?R ?Q with the restriction that every state-labeled node has head and tail rank 1.
Let c be aconfiguration of M and r : ?
?
?
an explicit rule(q)WANT ?
1(r, q) ?0.3?
(1)(q)BELIEVE ?
2(r, q) ?0.2?
(2)(r)BOY ?
3 ?0.3?
(3)(r)GIRL ?
4 ?0.3?
(4)(r)?
?
5 ?0.1?
(5)(q)?
?
6 ?0.1?
(6)(q)?
7(r) ?0.4?
(7)Figure 7: Ruleset of the dag acceptor in Example 1.of type (m,n).
Then c =?r c?
if ?
matches a sub-dag of c, and c?
= c[??
?
].Now let c be a configuration of M and r : ??
?an implicit rule of type (m, 1).
If a configurationc?
can be obtained by replacing m nodes labeled ?such that all tails lead to the same node and are in theright order, by the single state-node ?, then we sayc =?r c?.
Example derivation steps are shown inFigure 6 (see Example 1).
We denote the transitiveand reflexive closure of =?
by =?
?.A dag G is accepted by M if there is a deriva-tion q0(G) =??
G?, where G?
is a dag over ?
(R).Note that the derivation steps of a given derivationare partially ordered; many derivations can share thesame partial order.
In order to avoid spurious deriva-tions, recall that the nodes of G are ordered, and as-sume that nodes are rewritten according to this or-der: the resulting derivation is called a canonicalderivation.
The set of all canonical derivations fora given graph G is D(G).
The set of all dags ac-cepted byM is the dag language L(M).
The weightw(d) of a derivation dag (represented by its canon-ical derivation) d = G =?r1 G1 =?r2 .
.
.
=?rnGn is?ni=1w(rn), and the weight of a dag G is?d?D(G)w(d).
The weighted language L(N) is afunction that maps every dag to its weight in N .Example 1.
Let?
= {GIRL, BOY, BELIEVE,WANT, ?
}and consider the top-down dag acceptor M =({q, r}, q,?, R,w) which has a ruleset containingthe explicit and implicit (1, 1) rules given in Fig-ure 7.
The weights defined by w have been writtendirectly after the rules in angle brackets.
This ac-80qWANTBELIEVEBOY GIRL=?11r qBELIEVEBOY GIRL=?21r 2r qBOY GIRL=?8128 qr GIRLBOY=?3128 q3 GIRL=?7128 73 rGIRL=?4128 73 4Figure 6: Derivation of a dag using the dag acceptor of Example 1.
The weight of the derivation is w(1) ?w(2) ?w(8) ?w(3) ?
w(7) ?
w(4) = 0.3 ?
0.2 ?
0.2 ?
0.3 ?
0.4 ?
0.3 = 0.000432.ceptor can accept dags that involve boys and girlsbelieving and wanting.
One of them is given in Fig-ure 3b.
To obtain dags that are not trees, let us addthe following implicit (2, 1) and (3, 1) rules:(r, r)?
8(r) ?0.2?
(8)(r, r, r)?
9(r) ?0.1?
(9)A non-treelike dag is given in Figure 3a, while itsderivation is given in Figure 6.
Note that the effectof rule (8) could be simulated by rule (9).Let us now define dag-to-tree transducers.
Con-trarily to Kamimura and Slutzki (1982), who definedonly the bottom-up case and were skeptical of an el-egant top-down formulation, we only consider top-down devices.A (weighted) top-down dag-to-tree transducer isa machine T = (Q, q0,?,?, R,w) which is definedin the same way as a finite dag acceptor, except forthe additional output alphabet ?
and the rules?
right-hand side.
A dag-to-tree transducer explicit rulehas the form r : ?
?
?
where ?
?
Qm(?)
and?
?
(T?
(Q(Xn)))m for m,n ?
N. Intuitively, thismeans that the left-hand side still consists of a sym-bol and m ?incoming states?, while the right-handside now are m trees over ?
with states and n vari-ables used to process the n child subdags.
Implicit(m, 1) rules are defined in the same way, having moutput trees over one variable.
The dag-to-tree trans-ducer T defines a relation L(T ) ?
D?
?
T?
?
R.A derivation step of T is defined analogously tothe acceptor case by replacement of ?
by ?.
How-ever, copying rules (those that use a variable morethan once in a right-hand side) and deleting rules(those that do not use a rule at all) are problematic inthe dag case.
In the tree world, every tree can be bro-ken up into a root symbol and independent subtrees.This is not true in the dag world, where there is shar-ing between subdags.
Therefore, if an edge reach-ing a given symbol ?
is not followed at all (deletingrule), the transducer is going to choke if not everyedge entering ?
is ignored.
In the case of copyingrules, the part of the input dag that has not yet beenprocessed must be copied, and the configuration issplit into two sub-configurations which must bothbe derived in parallel.
We will therefore restrict our-selves to linear (non-copying) non-deleting rules inthis paper.4 NLP exampleRecall the example dag acceptor from Example 1.This acceptor generates an sentences about boys andgirls wanting and believing.
Figure 3 shows somesample graphs from this language.Next, we build a transducer that relates thesegraphs to corresponding English.
This is quite chal-lenging, as BOY may be referred to in many ways(?the boy?, ?he?, ?him?, ?himself?, ?his?, or zero),and of course, there are many syntactic devices forrepresenting semantic role clusters.
Because of am-biguity, the mapping between graphs and English ismany-to-many.
Figure 8 is a fragment of our trans-ducer, and Figure 9 shows a sample derivation.Passives are useful for realizing graphs withempty roles (?the girl is wanted?
or ?the girl wantsto be believed?).
Note that we can remove syntactic0 (zero) elements with a standard tree-to-tree trans-ducer, should we desire.
(qs)WANT(x, y)?
S(qnomg(x), is wanted, qzero(y))(qinfg)BELIEVE(x, y)?
INF(qzero(y), to be believed, qzerog(y))(qzero)?
?
081(qs)WANT(x, y)?
S(qnomb(x),wants, qinfb(y)) (10)(qinfb)BELIEVE(x, y)?
INF(qaccg(x), to believe, qaccb(y)) (11)(qaccg)GIRL ?
NP(the girl) (12)(qnomb, qaccb)BOY ?
NP(the boy), NP(him) (13)Figure 8: Transducer rules mapping semantic graphs to syntactic trees.qWANTBELIEVEBOY GIRL=?10Sqnomb wants qinfbBELIEVEBOY GIRL=?11Sqnomb wants INFqaccg to believe qaccbBOY GIRL=?12,13SINFNP NP NPthe boy wants the girl to believe himFigure 9: Derivation from graph to tree ?the boy wants the girl to believe him?.Events can be realized with nouns as well as verbs(?his desire for her, to believe, him?
):(qnp)WANT(x, y)?
NP(qpossb(x), ?s desire, qinfb(y))We note that transducer rules can be applied in ei-ther direction, semantics-to-English or English-to-semantics.
Though this microworld is small, it cer-tainly presents interesting challenges for any graphtransduction framework.
For example, given ?theboy?s desire is to be believed by the girl,?
the trans-ducer?s graph must make BOY the theme of BE-LIEVE.5 Generic dag acceptor and transduceralgorithmsIn this section we give algorithms for standard tasks.5.1 Membership checkingMembership checking is the task of determining, fora given finite dag acceptor M and an input dag G,whether G ?
L(M), or in the weighted case, com-pute the weight of G. Recall that the set of nodesof G is ordered.
We can therefore walk through Gaccording to this order and process each node on itsown.
A very simple algorithm can be given in theframework of ?parsing as deduction?
(Shieber et al,1995):Items: configurations, i.e.
dags over ?
?Q ?RAxiom: G, a dag over ?Goal: dag over RInference rule: if an item has only ancestors fromQ, apply a matching rule from R to obtain anew itemThis algorithm is correct and complete and can beimplemented in time O(2|G|) since there are expo-nentially many configurations.
Moreover, the set ofderivation dags is the result of this parser, and a fi-nite dag acceptor representing the derivation dagscan be constructed on the fly.
It can be easily ex-tended to check membership of (dag, tree) pairs in adag-to-tree transducer and to generate all the treesthat are obtained from a given dag (?forward ap-plication?).
In order to compute weights, the tech-niques by Goodman (1999) can be used.5.2 1-best and k-best generationThe k-best algorithm finds the highest-weighted kderivations (not dags) in a given (weighted) dag ac-ceptor.
If no weights are available, other measurescan be used (e.g.
the number of derivation steps orsymbol frequencies).
We can implement the k-bestalgorithm (of which 1-best is a special case) by gen-erating graphs and putting incomplete graphs on apriority queue sorted by weight.
If rule weights areprobabilities between 0 and 1, monotonicity ensuresthat the k-best graphs are found, as the weights ofincomplete hypotheses never increase.82q?=?1WANTr q?=?8WANT> qrr?=?2WANTBELIEVEr q?=?3WANTBELIEVEqBOY ?=?7WANTBELIEVErBOY ?=?4WANTBELIEVEBOY GIRLFigure 10: Example derivation in ?generation mode?.Dags are generated by taking the basic incompletedags (rule dags) defined by each rule and concate-nating them using the dangling edges.
Every dan-gling edge of the rule dag can be identified with adangling edge of the current hypothesis (if the orien-tation matches) or be left unconnected for later con-nection.
In that way, all children and parents for agiven node are eventually created.
Strictly speaking,the resulting structures are not dags anymore as theycan contain multiple > and ?
symbols.
A samplegeneration is shown in Figure 10.
Note how the or-der of rules applied is different from the example inFigure 6.Using the dag acceptor as a generating device inthis way is unproblematic, but poses two challenges.First, we have to avoid cyclicity, which is easily con-firmed by keeping nodes topologically sorted.Second, to avoid spurious ambiguity (wherederivations describe the same derivation dag, butonly differ by the order of rule application), spe-cial care is needed.
A simple solution is to sort theedges in each incomplete dag to obtain a canonical(?leftmost?)
derivation.
We start with the start state(which has head rank 0).
This is the first incompletedag that is pushed on the dag queue.
Then we repeat-edly pop an incomplete dag G from the dag queue.The first unused edge e of G is then attached to anew node v by identifying e with one of v?s edgesif the states are compatible.
Remaining edges of thenew node (incoming or outgoing) can be identifiedwith other unused edges of G or left for later attach-ment.
The resulting dags are pushed onto the queue.Whenever a dag has no unused edges, it is com-plete and the corresponding derivation can be re-turned.
The generation process stops when k com-plete derivations have been produced.
This k-bestalgorithm can also be used to generate tree outputfor a dag-to-tree transducer, and by restricting theshape of the output tree, for ?backward application?
(given a tree, which dags map to it?
).6 Future workThe work presented in this paper is being imple-mented in a toolkit that will be made publicly avail-able.
Of course, there is a lot of room for improve-ment, both from the theoretical and the practicalviewpoint.
This is a brief list of items for future re-search:?
Complexity analysis of the algorithms.?
Closure properties of dag acceptors and dag-to-tree transducers as well as composition withtree transducers.?
Investigate a reasonable probabilistic modeland training procedures.?
Extended left-hand sides to condition on alarger semantic context, just like extended top-down tree transducers (Maletti et al, 2009).?
Handling flat, unordered, sparse sets of rela-tions that are typical of feature structures.
Cur-rently, rules are very specific to the number ofchildren and parents.
A first step in this direc-tion is given by implicit rules that can handle apotentially arbitrary number of parents.?
Hand-annotated resources such as (dag, tree)pairs, similar to treebanks for syntactic repre-sentations.AcknowledgementsThis research was supported in part by ARO grantW911NF-10-1-0533.
The first author was supportedby the German Research Foundation (DFG) grantMA 4959/1?1.83ReferencesL.
E. Baum, T. Petrie, G. Soules, and N. Weiss.
1970.A maximization technique occurring in the statisticalanalysis of probabilistic functions of Markov chains.Ann.
Math.
Statist., 41(1):164171.Bjo?rn Borchardt and Heiko Vogler.
2003.
Determiniza-tion of finite state weighted tree automata.
J. Autom.Lang.
Comb., 8(3):417?463.Francis Bossut and Bruno Warin.
1992.
Automata andpattern matching in planar directed acyclic graphs.In Imre Simon, editor, Proc.
LATIN, volume 583 ofLNCS, pages 76?86.
Springer.Francis Bossut, Max Dauchet, and Bruno Warin.
1988.Automata and rational expressions on planar graphs.In Michal Chytil, Ladislav Janiga, and Va?clav Koubek,editors, Proc.
MFCS, volume 324 of LNCS, pages190?200.
Springer.Francis Bossut, Max Dauchet, and Bruno Warin.
1995.A kleene theorem for a class of planar acyclic graphs.Inf.
Comput., 117(2):251?265.Symeon Bozapalidis and Antonios Kalampakas.
2004.An axiomatization of graphs.
Acta Inf., 41(1):19?61.Symeon Bozapalidis and Antonios Kalampakas.
2006.Recognizability of graph and pattern languages.
ActaInf., 42(8-9):553?581.Symeon Bozapalidis and Antonios Kalampakas.
2008.Graph automata.
Theor.
Comput.
Sci., 393(1-3):147?165.H.
Comon, M. Dauchet, R. Gilleron, C. Lo?ding,F.
Jacquemard, D. Lugiez, S. Tison, and M. Tom-masi.
2007.
Tree automata techniques and appli-cations.
Available on: http://www.grappa.univ-lille3.fr/tata.
release October, 12th2007.Bruno Courcelle and Joost Engelfriet.
1995.
A logicalcharacterization of the sets of hypergraphs defined byhyperedge replacement grammars.
Math.
Syst.
The-ory, 28(6):515?552.Frank Drewes, Hans-Jo?rg Kreowski, and Annegret Ha-bel.
1997.
Hyperedge replacement, graph grammars.In Grzegorz Rozenberg, editor, Handbook of GraphGrammars, pages 95?162.
World Scientific.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proc.
ACL, pages205?208.
ACL.Joost Engelfriet and Jan Joris Vereijken.
1997.
Context-free graph grammars and concatenation of graphs.Acta Inf., 34(10):773?803.David Eppstein.
1998.
Finding the k shortest paths.SIAM J.
Comput., 28(2):652?673.Ferenc Ge?cseg and Magnus Steinby.
1984.
Tree Au-tomata.
Akade?miai Kiado?, Budapest, Hungary.Joshua Goodman.
1999.
Semiring parsing.
Comput.Linguist., 25:573?605.Jonathan Graehl, Kevin Knight, and Jonathan May.2008.
Training tree transducers.
Comput.
Linguist.,34(3):391?427.Jonathan Graehl.
1997.
Carmel finite-state toolkit.http://www.isi.edu/licensed-sw/carmel.Yvette Graham, Josef van Genabith, and Anton Bryl.2009.
F-structure transfer-based statistical machinetranslation.
In Proc.
LFG.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
IWPT.V?
?ctor M. Jime?nez and Andre?s Marzal.
2000.
Computa-tion of the n best parse trees for weighted and stochas-tic context-free grammars.
In Proc.
SSPR/SPR, vol-ume 1876 of LNCS, pages 183?192.
Springer.Tsutomu Kamimura and Giora Slutzki.
1981.
Paral-lel and two-way automata on directed ordered acyclicgraphs.
Inf.
Control, 49(1):10?51.Tsutomu Kamimura and Giora Slutzki.
1982.
Transduc-tions of dags and trees.
Math.
Syst.
Theory, 15(3):225?249.Kevin Knight and Jonathan Graehl.
2005.
An overviewof probabilistic tree transducers for natural languageprocessing.
In Proc.
CICLing, volume 3406 of LNCS,pages 1?24.
Springer.Kevin Knight.
1989.
Unification: A multidisciplinarysurvey.
ACM Comput.
Surv., 21(1):93?124.Andreas Maletti, Jonathan Graehl, Mark Hopkins, andKevin Knight.
2009.
The power of extended top-downtree transducers.
SIAM J.
Comput., 39(2):410?430.Jonathan May and Kevin Knight.
2006a.
A better n-bestlist: Practical determinization of weighted finite treeautomata.
In Proc.
HLT-NAACL.
ACL.Jonathan May and Kevin Knight.
2006b.
Tiburon: Aweighted tree automata toolkit.
In Oscar H. Ibarra andHsu-Chun Yen, editors, Proc.
CIAA, volume 4094 ofLNCS, pages 102?113.
Springer.Mehryar Mohri, Fernando C. N. Pereira, and MichaelRiley.
2000.
The design principles of a weightedfinite-state transducer library.
Theor.
Comput.
Sci.,231(1):17?32.Mehryar Mohri.
1997.
Finite-state transducers in lan-guage and speech processing.
Computational Linguis-tics, 23(2):269?311.Robert C. Moore.
1989.
Unification-based semantic in-terpretation.
In Proc.
ACL, pages 33?41.
ACL.NIST.
2009.
NIST Open Machine Translation 2009Evaluation (MT09).
http://www.itl.nist.gov/iad/mig/tests/mt/2009/.84Fernando Pereira and Michael Riley.
1997.
Speechrecognition by composition of weighted finite au-tomata.
In Finite-State Language Processing, pages431?453.
MIT Press.Michael Riley, Cyril Allauzen, and Martin Jansche.2009.
OpenFST: An open-source, weighted finite-state transducer library and its applications to speechand language.
In Ciprian Chelba, Paul B. Kantor, andBrian Roark, editors, Proc.
HLT-NAACL (Tutorial Ab-stracts), pages 9?10.
ACL.William C. Rounds and Robert T. Kasper.
1986.
A com-plete logical calculus for record structures representinglinguistic information.
In Proc.
LICS, pages 38?43.IEEE Computer Society.Stuart M. Shieber, Yves Schabes, and Fernando C. N.Pereira.
1995.
Principles and implementation of de-ductive parsing.
J. Log.
Program., 24(1&2):3?36.Stuart M. Shieber.
1986.
An Introduction to Unification-Based Approaches to Grammar, volume 4 of CSLILecture Notes.
CSLI Publications, Stanford, CA.Andrew Viterbi.
1967.
Error bounds for convolutionalcodes and an asymptotically optimum decoding al-gorithm.
IEEE Transactions on Information Theory,13(2):260?269.85
