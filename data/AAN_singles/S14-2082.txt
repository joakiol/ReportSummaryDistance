Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 471?476,Dublin, Ireland, August 23-24, 2014.Priberam: A Turbo Semantic Parser with Second Order FeaturesAndr?e F. T.
Martins?
?Mariana S. C.
Almeida??
?Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal?Instituto de Telecomunicac?
?oes, Instituto Superior T?ecnico, 1049-001 Lisboa, Portugal{atm,mla}@priberam.ptAbstractThis paper presents our contribution tothe SemEval-2014 shared task on Broad-Coverage Semantic Dependency Parsing.We employ a feature-rich linear model, in-cluding scores for first and second-orderdependencies (arcs, siblings, grandparentsand co-parents).
Decoding is performed ina global manner by solving a linear relax-ation with alternating directions dual de-composition (AD3).
Our system achievedthe top score in the open challenge, and thesecond highest score in the closed track.1 IntroductionThe last decade saw a considerable progress in sta-tistical modeling for dependency syntactic pars-ing (K?ubler et al., 2009).
Models that incorporaterich global features are typically more accurate,even if pruning is necessary or decoding needs tobe approximate (McDonald et al., 2006; Koo andCollins, 2010; Bohnet and Nivre, 2012; Martins etal., 2009, 2013).
This paper applies the same ratio-nale to semantic dependency parsing, in whichthe output variable is a semantic graph, ratherthan a syntactic tree.
We extend a recently pro-posed dependency parser, TurboParser (Martins etal., 2010, 2013), to be able to perform semanticparsing using any of the three formalisms consid-ered in this shared task (DM, PAS, and PCEDT).The result is TurboSemanticParser, which we re-lease as open-source software.1We describe here a second order model for se-mantic parsing (?2).
We follow prior work in se-mantic role labeling (Toutanova et al., 2005; Jo-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1http://labs.priberam.com/Resources/TurboSemanticParserFigure 1: Example of a semantic graph in the DMformalism (sentence #22006003).
We treat topnodes as a special semantic role TOP whose predi-cate is a dummy root symbol.hansson and Nugues, 2008; Das et al., 2012; Flani-gan et al., 2014), by adding constraints and model-ing interactions among arguments within the sameframe; however, we go beyond such sibling in-teractions to consider more complex grandpar-ent and co-parent structures, effectively correlat-ing different predicates.
We formulate parsing asa global optimization problem and solve a relax-ation through AD3, a fast dual decomposition al-gorithm in which several simple local subprob-lems are solved iteratively (?3).
Through a richset of features (?4), we arrive at top accuracies atparsing speeds around 1,000 tokens per second, asdescribed in the experimental section (?5).2 A Second Order Model for ParsingFigure 1 depicts a sentence and its semantic graph.We cast semantic parsing as a structured predic-tion problem.
Let x be a sentence and Y(x) theset of possible dependency graphs.
We assumeeach candidate graph y ?
Y(x) can be repre-sented as a set of substructures (called parts) inan underlying set S (e.g., predicates, arcs, pairsof adjacent arcs).
We design a score function fwhich decomposes as a sum over these substruc-tures, f(x, y) :=?s?Sfs(x, ys).
We parametrizethis function using a weight vector w, and writeeach atomic function as fs(x, ys) := w?
?s(x, ys),where ?s(x, ys) is a vector of local features.
Thedecoding problem consists in obtaining the best-471Algorithm 1 Decoding in an Arc-Factored Model1: input: Predicate scores ?P(p), arc scores ?A(p ?
a),labeled arc scores ?LA(pr?
a).2: Initialize semantic graph G?
?3: for p = 0 to L do4: Initialize ?
?
?P(p), frame A(p)?
?5: for a = 1 to L do6: Set r??
argmaxr?LA(pr?
a)7: if ?A(p?
a) + ?LA(pr??
a) > 0 then8: A(p)?
A(p) ?
{?p, a, r??
}9: ?
?
?
+ ?A(p?
a) + ?LA(pr??
a)10: end if11: end for12: if ?
> 0 then set G?
G ?
{?p,A(p)?
}13: end for14: output: semantic graph G.scored semantic graph y?
given a sentence x:y?
= arg maxy?Y(x)f(x, y).
(1)Our choice of parts is given in Figure 2.
The sec-ond order parts are inspired by prior work in syn-tactic parsing, modeling interactions for pairs of(unlabeled) dependency arcs, such as grandpar-ents (Carreras, 2007) and siblings (Smith and Eis-ner, 2008; Martins et al., 2009).
The main noveltyis co-parent parts, which, to the best of our knowl-edge, were never considered before, as they onlymake sense when multiple parents are allowed.If all parts were basic, decoding could be doneindependently for each predicate p, as illustratedin Algorithm 1.
The total runtime, for a sentencewith L words, is O(L2|R|), where R is the setof semantic roles.
Adding consecutive siblingsstill permits independent decoding for each pred-icate, but dynamic programming is necessary todecode the best argument frame, increasing theruntime to O(L3|R|).
The addition of consec-utive co-parents, grandparents, and arbitrary sib-lings and co-parents breaks this independency andsets a demand for approximate decoding.
Evenwithout second-order parts, the inclusion of hardconstraints (such as requiring some roles to beunique, see ?3) also makes the problem harder.2Rather than looking for a model in which exactdecoding is tractable, which could be even morestringent for parsing semantic graphs than for de-pendency trees, we embrace approximate decod-ing strategies.
Namely, our approach is based on2Albeit the dynamic program could still incorporate con-straints for unique roles (by appending a bit-string to the stateto mark semantic roles that have been filled), runtime be-comes exponential in the number of unique roles, only beingfeasible when this number is small.Figure 2: Parts considered in this paper.
Thetop row illustrate the basic parts, representing theevent that a word is a predicate, or the existence ofan arc between a predicate and an argument, even-tually labeled with a semantic role.
Our second-order model looks at some pairs of arcs: arcs bear-ing a grandparent relationship, arguments of thesame predicate, predicates sharing the same argu-ment, and consecutive versions of these two.dual decomposition, a class of optimization tech-niques that tackle the dual of combinatorial prob-lems in a modular and extensible manner (Ko-modakis et al., 2007; Rush et al., 2010).
We em-ploy alternating directions dual decomposition(AD3; Martins et al., 2011).
Like the subgradi-ent algorithm of Rush et al.
(2010), AD3splitsthe original problem into local subproblems, andseeks an agreement on the overlapping variables.The difference is that the AD3subproblems havean additional quadratic term to accelerate con-sensus, achieving a faster convergence rate bothin theory and in practice (Martins et al., 2012,2013).
For several factors (such as logic factorsrepresenting AND, OR and XOR constraints, bud-get constraints, and binary pairwise factors), thesequadratic subproblems can be solved efficiently.For dense or structured factors, the quadratic sub-problems can be solved as a sequence of localViterbi decoding steps, via an active set method(Martins, 2014); this local decoding operation isthe same that needs to be performed in the subgra-dient algorithm.
We describe these subproblemsin detail in the next section.3 Solving the SubproblemsPredicate and Arc-Factored Parts.
We captureall the basic parts with a single component.
Asstated in ?2, local decoding in this component hasa runtime of O(L2|R|), by using Algorithm 1.Unique Roles.
We assume some roles areunique, i.e., they can occur at most once for the472same predicate.3To cope with unique roles, weadd hard constraints of the kind?aI(pr?
a ?
y) ?
1, ?p,?r ?
Runiq, (2)where Runiqis the set of unique roles.
This set isobtained from the training data by looking at theroles that never occur multiple times in the goldargument frames.4The constraint above corre-sponds to a ATMOSTONE factor, which is built-inin AD3and can be decoded in linear time (ren-dering the runtime O(L2|Runiq|) when aggregat-ing all such factors).
These have also been usedby Das et al.
(2012) in frame-semantic parsing.Grandparents, Arbitrary Siblings and Co-parents.
The second-order parts in the middlerow of Figure 2 all involve the simultaneous inclu-sion of a pair of arcs, without further dependencyon the remaining arcs.
We handle each of theseparts using a simple pairwise factor (called PAIRin the AD3toolkit).
The total runtime to locallydecode these factors is O(L3).Predicate Automata.
To handle consecutivesiblings, we adapt the simple head automatonmodel (Alshawi, 1996; Smith and Eisner, 2008;Koo et al., 2010) to semantic parsing.
We in-troduce one automaton for each predicate p andattachment direction (left or right).
We describeright-side predicate automata; their left-side coun-terparts are analogous.
Let ?a0, a1, .
.
.
, ak+1?
bethe sequence of right modifiers of p, with a0=START and ak+1= END.
Then, we have the fol-lowing component capturing consecutive siblings:fCSIBp,?(p?
a1, .
.
.
, p?
ak) =?k+1j=1?CSIB(p, aj?1, aj).
(3)Maximizing fCSIBp,?via dynamic programming hasa cost of O(L2), yielding O(L3) total runtime.Argument Automata.
For consecutive co-parents, we introduce another automaton which isanalogous to the predicate automaton, but wherearrows are reversed.
Let ?p0, p1, .
.
.
, pk+1?
bethe sequence of right predicates that take a asargument (the left-side case is analagous), withp0= START and pk+1= END.
We define:fCCPa,?(a?
p1, .
.
.
, a?
pk) =?k+1j=1?CCP(a, pj?1, pj).
(4)3Such roles have been called ?deterministic?
by Flaniganet al.
(2014).4For PAS, all 43 roles were found unique; for DM, thisnumber is 40 out of 52, and for PCEDT only 3 out of 69.The total runtime is also O(L3).4 FeaturesWe define binary features for each part representedin Figure 2.
Most of the features are taken fromTurboParser (Martins et al., 2013), while othersare inspired by the semantic parser of Johanssonand Nugues (2008).
Those features marked with?require information from the dependency syntacticparser, and are only used in the open track.5Predicate Features.
Our predicate features are:?
PREDWORD, PREDLEMMA, PREDPOS.
Lexi-cal form, lemma, and POS tag of the predicate.?
PREDREL.
?Syntactic dependency relation be-tween the predicate and its head.?
PREDHEADWORD/POS.
?Form and POS tagof the predicate syntactic head, conjoined withthe predicate word and POS tag.?
PREDMODWORD/POS/REL.
?Form, POS tag,and dependency relation of the predicate syn-tactic dependents, conjoined with the predicateword and POS tag.Arc Features.
All features above, plus the fol-lowing (conjoined with arc direction and label):?
ARGWORD, ARGLEMMA, ARGPOS.
The lex-ical form, lemma, and POS tag of the argument.?
ARGREL.
?Syntactic dependency relation be-tween the argument and its head.?
LEFTWORD/POS,?RIGHTWORD/POS.
?Form/POS tag of the leftmost/rightmost de-pendent of the argument, conjoined with thepredicate word and POS tag.?
LEFTSIBWORD/POS,?RIGHTSIBWORD/POS.
?Form/POS tag of the left/right sibling of theargument, conjoined with the predicate tag.?
PREDCONTEXTWORD, PREDCONTEXTPOS,PREDCONTEXTLEMMA.
Word, POS, andlemma on the left and right context of the pred-icate (context size is 2).?
PREDCONTEXTPOSBIGRAM/TRIGRAM.
Bi-gram and trigram of POS tags on the left andright side of the predicate.?
PREDVOICE.
?Predicate voice: active, passive,or none.
Determined from the syntactic depen-dency tree as in Johansson and Nugues (2008).5For the open track, the only external information used byour system were the provided automatic dependency trees.473?
PREDWORDARGWORD, PREDWORDARG-POS, PREDPOSARGWORD, PREDPOSARG-POS.
Predicate word/tag conjoined withargument word/tag.?
PREDARGPOSCONTEXT.
Several featuresconjoining the POS of words surrounding thepredicate and argument (similar to the contex-tual features in McDonald et al.
(2005)).?
EXACTARCLENGTH, BINNEDARCLENGTH.Exact and binned arc length (distance betweenpredicate and argument), conjoined with thepredicate and argument POS tags.?
POSINBETWEEN, WORDINBETWEEN.
POSand forms between the predicate and argument,conjoined with their own POS tags and forms.?
RELPATH,?POSPATH.
?Path in the syntacticdependency tree between the predicate and theargument.
The path is formed either by depen-dency relations or by POS tags.Second Order Features.
These involve a pred-icate, an argument, and a ?companion word?
(which can be a second argument, in the case ofsiblings, a second predicate, for co-parents, or theargument of another argument, for grandparents).In all cases, features are of the following kind:?
POSTRIPLET.
POS tags of the predicate, theargument, and the companion word.?
UNILEXICAL.
One word form (for the predi-cate/argument/companion) and two POS tags.?
BILEXICAL.
One POS tag (for the predi-cate/argument/companion) and two word forms.?
PAIRWISE.
Backed-off pair features for thecompanion word form/POS tag and the wordform/POS of the predicate/argument.5 Experimental ResultsAll models were trained by running 10 epochs ofmax-loss MIRA with C = 0.01 (Crammer et al.,2006).
The cost function takes into account mis-matches between predicted and gold dependen-cies, with a cost cPon labeled arcs incorrectlypredicted (false positives) and a cost cRon goldlabeled arcs that were missed (false negatives).These values were set through cross-validation inthe dev set, yielding cP= 0.4 and cR= 0.6 in allruns, except for the DM and PCEDT datasets in theclosed track, for which cP= 0.3 and cR= 0.7.To speed up decoding, we discard arcs whoseposterior probability is below 10?4, according to aprobabilistic unlabeled first-order pruner.
Table 1shows a significant reduction of the search spacewith a very small drop in recall.Table 2 shows our final results in the test set,for a model trained in the train and developmentpartitions.
Our system achieved the best score inthe open track (an LF score of 86.27%, averagedover DM, PAS, and PCEDT), and the second best inthe closed track, after the Peking team.
Overall,we observe that the precision and recall in PCEDTare far below the other two formalisms, but thisdifference is much smaller when looking at unla-beled scores.
Comparing the results in the closedand open tracks, we observe a consistent improve-ment in the three formalisms of around 1% in F1from using syntactic information.
While this con-firms previous findings that syntactic features areimportant in semantic role labeling (Toutanova etal., 2005; Johansson and Nugues, 2008), these im-provements are less striking than expected.
Weconjecture this is due to the fact that our model inthe closed track already incorporates a variety ofcontextual features which are nearly as informa-tive as those extracted from the dependency trees.Finally, to assess the importance of the secondorder features, Table 3 reports experiments in thedev-set that progressively add several groups offeatures, along with runtimes.
We can see thatsiblings, co-parents, and grandparents all providevaluable information that improves the final scores(with the exception of the PCEDT labeled scores,where the difference is negligible).
This comesat only a small cost in terms of runtime, which isaround 1,000 tokens per second for the full mod-els.UR # UA/tok LR # LA/tokDM 99.33 3.5 (13.4%) 99.22 34.4 (2.5%)PAS 99.53 3.3 (12.5%) 99.49 20.8 (1.9%)PCEDT 99.03 2.1 (8.2%) 98.77 54.5 (3.0%)Table 1: Pruner statistics in the dev-set, for theopen track.
Shown are oracle recall scores, consid-ering both unlabeled (UR) and labeled arcs (LR);and the averaged number of unlabeled and la-beled arcs per token that remained after the prun-ing stage (# UA/tok and # LA/tok).
In brackets,we show the fraction of unlabeled/labeled arcs thatsurvived the pruning.474UP UR UF LP LR LFDM, closed 90.14 88.65 89.39 88.82 87.35 88.08PAS, closed 93.18 91.12 92.14 91.95 89.92 90.93PCEDT, closed 90.21 85.51 87.80 78.80 74.70 76.70average, closed ?
?
89.77 ?
?
85.24DM, open 91.41 89.26 90.32 90.23 88.11 89.16PAS, open 93.62 92.01 92.81 92.56 90.97 91.76PCEDT, open 91.58 86.61 89.03 80.14 75.79 77.90average, open ?
?
90.72 ?
?
86.27Table 2: Submitted results for the closed and opentracks.
For comparison, the best-performing sys-tem in the closed track (Peking) obtained averagedUF and LF scores of 91.03% and 85.91%, respec-tively.UF LF Tok/secDM, arc-factored 89.90 88.96 1,681DM, arc-factored, pruned 89.85 88.90 2,642+siblings 90.34 89.34 1,838+co-parents 90.80 89.76 1,073+grandparent (full) 90.95 89.90 955PAS, arc-factored 92.34 91.40 1,927PAS, arc-factored, pruned 92.35 91.40 2,914+siblings 92.45 91.45 2,106+co-parents 92.71 91.71 1,104+grandparent (full) 92.87 91.87 1,043PCEDT, arc-factored 87.90 79.90 1,558PCEDT, arc-factored, pruned 87.74 79.83 2,906+siblings 88.46 79.98 2,066+co-parents 90.17 79.90 1,531+grandparent (full) 90.18 80.03 1,371Table 3: Results in the dev-set for the open track,progressively adding several groups of features,until the full model is obtained.
We report un-labeled/labeled F1and parsing speeds in tokensper second.
Our speeds include the time necessaryfor pruning, evaluating features, and decoding, asmeasured on a Intel Core i7 processor @3.4 GHz.6 ConclusionsWe have described a system for broad-coveragesemantic dependency parsing.
Our system, whichis inspired by prior work in syntactic parsing, im-plements a linear model with second-order fea-tures, being able to model interactions betweensiblings, grandparents and co-parents.
We haveshown empirically that second-order features havean impact in the final scores.
Approximate de-coding was performed via alternating directionsdual decomposition (AD3), yielding fast runtimesof around 1,000 tokens per second.AcknowledgementsWe would like to thank the reviewers fortheir helpful comments.
This work was par-tially supported by the EU/FEDER programme,QREN/POR Lisboa (Portugal), under the Intelligoproject (contract 2012/24803) and by a FCT grantPTDC/EEI-SII/2312/2012.ReferencesHiyan Alshawi.
1996.
Head automata and bilingualtiling: Translation with minimal representations.
InProc.
of Annual Meeting of the Association for Com-putational Linguistics, pages 167?176.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Proc.of the Empirical Methods in Natural Language Pro-cessing, pages 1455?1465.Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In InternationalConference on Natural Language Learning, pages957?961.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
On-line Passive-Aggressive Algorithms.
Journal of Ma-chine Learning Research, 7:551?585.Dipanjan Das, Andr?e F. T. Martins, and Noah A. Smith.2012.
An Exact Dual Decomposition Algorithmfor Shallow Semantic Parsing with Constraints.
InProc.
of First Joint Conference on Lexical and Com-putational Semantics (*SEM), pages 209?217.Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,Chris Dyer, and Noah A. Smith.
2014.
A discrim-inative graph-based parser for the abstract mean-ing representation.
In Proc.
of the Annual Meet-ing of the Association for Computational Linguis-tics, pages 1426?1436.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysis withPropBank and NomBank.
International Conferenceon Natural Language Learning, pages 183?187.Nikos Komodakis, Nikos Paragios, and Georgios Tzir-itas.
2007.
MRF optimization via dual decompo-sition: Message-passing revisited.
In Proc.
of In-ternational Conference on Computer Vision, pages1?8.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proc.
of Annual Meet-ing of the Association for Computational Linguis-tics, pages 1?11.Terry Koo, Alexander M. Rush, Michael Collins,Tommi Jaakkola, and David Sontag.
2010.
Dualdecomposition for parsing with non-projective headautomata.
In Proc.
of Empirical Methods for Natu-ral Language Processing, pages 1288?1298.475Sandra K?ubler, Ryan McDonald, and Joakim Nivre.2009.
Dependency parsing.
Morgan & ClaypoolPublishers.Andr?e F. T. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise Integer Linear Programming Formu-lations for Dependency Parsing.
In Proc.
of AnnualMeeting of the Association for Computational Lin-guistics, pages 342?350.Andr?e F. T. Martins, Noah A. Smith, Eric P. Xing,Pedro M. Q. Aguiar, and M?ario A. T. Figueiredo.2010.
Turbo Parsers: Dependency Parsing by Ap-proximate Variational Inference.
In Proc.
of Em-pirical Methods for Natural Language Processing,pages 34?44.Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.Aguiar, and M?ario A. T. Figueiredo.
2011.
Dual De-composition with Many Overlapping Components.In Proc.
of Empirical Methods for Natural LanguageProcessing, pages 238?249.Andr?e F. T. Martins, M?ario A. T. Figueiredo, PedroM.
Q. Aguiar, Noah A. Smith, and Eric P. Xing.2012.
Alternating directions dual decomposition.Arxiv preprint arXiv:1212.6550.Andr?e F. T. Martins, Miguel B. Almeida, and Noah A.Smith.
2013.
Turning on the turbo: Fast third-ordernon-projective turbo parsers.
In Proc.
of the AnnualMeeting of the Association for Computational Lin-guistics, pages 617?622.Andr?e F. T. Martins.
2014.
AD3: A Fast Decoderfor Structured Prediction.
In S. Nowozin, P. Gehler,J.
Jancsary, and C. Lampert, editors, AdvancedStructured Prediction.
MIT Press, Cambridge, MA,USA.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proc.
of Annual Meeting of theAssociation for Computational Linguistics, pages91?98.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proc.
of InternationalConference on Natural Language Learning, pages216?220.Alexander M. Rush, David Sontag, Michael Collins,and Tommi Jaakkola.
2010.
On dual decomposi-tion and linear programming relaxations for naturallanguage processing.
In Proc.
of Empirical Methodsfor Natural Language Processing.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proc.
of EmpiricalMethods for Natural Language Processing, pages145?156.Kristina Toutanova, Aria Haghighi, and ChristopherManning.
2005.
Joint learning improves semanticrole labeling.
In ACL, pages 589?596.476
