Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 68?77,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Constituent-Based Approach to Argument Labelingwith Joint Inference in Discourse ParsingFang Kong1?Hwee Tou Ng2Guodong Zhou11School of Computer Science and Technology, Soochow University, China2Department of Computer Science, National University of Singaporekongfang@suda.edu.cn nght@comp.nus.edu.sg gdzhou@suda.edu.cnAbstractDiscourse parsing is a challenging taskand plays a critical role in discourse anal-ysis.
In this paper, we focus on label-ing full argument spans of discourse con-nectives in the Penn Discourse Treebank(PDTB).
Previous studies cast this taskas a linear tagging or subtree extractionproblem.
In this paper, we propose anovel constituent-based approach to argu-ment labeling, which integrates the ad-vantages of both linear tagging and sub-tree extraction.
In particular, the pro-posed approach unifies intra- and inter-sentence cases by treating the immediate-ly preceding sentence as a special con-stituent.
Besides, a joint inference mech-anism is introduced to incorporate glob-al information across arguments into ourconstituent-based approach via integer lin-ear programming.
Evaluation on PDT-B shows significant performance improve-ments of our constituent-based approachover the best state-of-the-art system.
It al-so shows the effectiveness of our joint in-ference mechanism in modeling global in-formation across arguments.1 IntroductionDiscourse parsing determines the internal struc-ture of a text and identifies the discourse rela-tions between its text units.
It has attracted in-creasing attention in recent years due to its impor-tance in text understanding, especially since therelease of the Penn Discourse Treebank (PDTB)corpus (Prasad et al., 2008), which adds a layer ofdiscourse annotations on top of the Penn Treebank?The research reported in this paper was carried out whileFang Kong was a research fellow at the National Universityof Singapore.
(PTB) corpus (Marcus et al., 1993).
As the largestavailable discourse corpus, the PDTB corpus hasbecome the defacto benchmark in recent studieson discourse parsing.Compared to connective identification and dis-course relation classification in discourse parsing,the task of labeling full argument spans of dis-course connectives is much harder and thus morechallenging.
For connective identification, Lin etal.
(2014) achieved the performance of 95.76%and 93.62% in F-measure using gold-standard andautomatic parse trees, respectively.
For discourserelation classification, Lin et al.
(2014) achievedthe performance of 86.77% in F-measure on clas-sifying discourse relations into 16 level 2 types.However, for argument labeling, Lin et al.
(2014)only achieved the performance of 53.85% in F-measure using gold-standard parse trees and con-nectives, much lower than the inter-annotation a-greement of 90.20% (Miltsakaki et al., 2004).In this paper, we focus on argument labeling inthe PDTB corpus.
In particular, we propose a nov-el constituent-based approach to argument label-ing which views constituents as candidate argu-ments.
Besides, our approach unifies intra- andinter-sentence cases by treating the immediatelypreceding sentence as a special constituent.
Final-ly, a joint inference mechanism is introduced toincorporate global information across argumentsvia integer linear programming.
Evaluation on thePDTB corpus shows the effectiveness of our ap-proach.The rest of this paper is organized as follows.Section 2 briefly introduces the PDTB corpus.Related work on argument labeling is reviewedin Section 3.
In Section 4, we describe ourconstituent-based approach to argument labeling.In Section 5, we present our joint inference mech-anism via integer linear programming (ILP).
Sec-tion 6 gives the experimental results and analysis.Finally, we conclude in Section 7.682 Penn Discourse TreebankAs the first large-scale annotated corpus that fol-lows the lexically grounded, predicate-argumentapproach in D-LTAG (Lexicalized Tree Adjoin-ing Grammar for Discourse) (Webber, 2004), thePDTB regards a connective as the predicate of adiscourse relation which takes exactly two text s-pans as its arguments.
In particular, the text spanthat the connective is syntactically attached to iscalled Arg2, and the other is called Arg1.Although discourse relations can be either ex-plicitly or implicitly expressed in PDTB, this pa-per focuses only on explicit discourse relationsthat are explicitly signaled by discourse connec-tives.
Example (1) shows an explicit discourse re-lation from the article wsj 2314 with connectiveso underlined, Arg1 span italicized, and Arg2 s-pan bolded.
(1) But its competitors have much broader busi-ness interests and so are better cushionedagainst price swings .Note that a connective and its arguments can ap-pear in any relative order, and an argument can bearbitrarily far away from its corresponding con-nective.
Although the position of Arg2 is fixedonce the connective is located, Arg1 can occur inthe same sentence as the connective (SS), in a sen-tence preceding that of the connective (PS), or ina sentence following that of the connective (FS),with proportions of 60.9%, 39.1%, and less than0.1% respectively for explicit relations in the PDT-B corpus (Prasad et al., 2008).
Besides, out ofall PS cases where Arg1 occurs in some preced-ing sentence, 79.9% of them are the exact imme-diately preceding sentence.
As such, in this paper,we only consider the current sentence containingthe connective and its immediately preceding sen-tence as the text span where Arg1 occurs, similarto what was done in (Lin et al., 2014).3 Related WorkFor argument labeling in discourse parsing on thePDTB corpus, the related work can be classifiedinto two categories: locating parts of arguments,and labeling full argument spans.As a representative on locating parts of argu-ments, Wellner and Pustejovsky (2007) proposedseveral machine learning approaches to identifythe head words of the two arguments for discourseconnectives.
Following this work, Elwell andBaldridge (2008) combined general and connec-tive specific rankers to improve the performanceof labeling the head words of the two arguments.Prasad et al.
(2010) proposed a set of heuristics tolocate the position of the Arg1 sentences for inter-sentence cases.
The limitation of locating parts ofarguments, such as the positions and head word-s, is that it is only a partial solution to argumentlabeling in discourse parsing.In comparison, labeling full argument spans canprovide a complete solution to argument labelingin discourse parsing and has thus attracted increas-ing attention recently, adopting either a subtreeextraction approach (Dinesh et al.
(2005), Lin etal.
(2014)) or a linear tagging approach (Ghosh etal.
(2011)).As a representative subtree extraction approach,Dinesh et al.
(2005) proposed an automatic treesubtraction algorithm to locate argument spans forintra-sentential subordinating connectives.
How-ever, only dealing with intra-sentential subordinat-ing connectives is not sufficient since they con-stitute only 40.93% of all cases.
Instead, Lin etal.
(2014) proposed a two-step approach.
First, anargument position identifier was employed to lo-cate the position of Arg1.
For the PS case, it di-rectly selects the immediately preceding sentenceas Arg1.
For other cases, an argument node iden-tifier was employed to locate the Arg1- and Arg2-nodes.
Next, a tree subtraction algorithm was usedto extract the arguments.
However, as pointed outin Dinesh et al.
(2005), it is not necessarily thecase that a connective, Arg1, or Arg2 is dominatedby a single node in the parse tree (that is, it can bedominated by a set of nodes).
Figure 1 shows thegold-standard parse tree corresponding to Exam-ple (1).
It shows that Arg1 includes three nodes:[CCBut], [NPits competitors], [V Phave muchbroader business interests], and Arg2 includes t-wo nodes: [CCand], [V Pare better cushioned a-gainst price swings].
Therefore, such an argumen-t node identifier has inherent shortcomings in la-beling arguments.
Besides, the errors propagat-ed from the upstream argument position classifiermay adversely affect the performance of the down-stream argument node identifier.As a representative linear tagging approach,Ghosh et al.
(2011) cast argument labeling as a lin-ear tagging task using conditional random fields.Ghosh et al.
(2012) further improved the perfor-69SVPVPVPPPNPNNSswingsNNpriceINagainstVBNcushionedADVPRBRbetterVBPareRBsoCCandVPNPNNSinterestsNNbusinessADJPJJRbroaderRBmuchVBPhaveNPNNScompetitorsPRPitsCCButFigure 1: The gold-standard parse tree corresponding to Example (1)mance with integration of the n-best results.While the subtree extraction approach locatesargument spans based on the nodes of a parse treeand is thus capable of using rich syntactic informa-tion, the linear tagging approach works on the to-kens in a sentence and is thus capable of capturinglocal sequential dependency between tokens.
Inthis paper, we take advantage of both subtree ex-traction and linear tagging approaches by propos-ing a novel constituent-based approach.
Further-more, intra- and inter-sentence cases are unifiedby treating the immediately preceding sentence asa special constituent.
Finally, a joint inferencemechanism is proposed to add global informationacross arguments.4 A Constituent-Based Approach toArgument LabelingOur constituent-based approach works by firstcasting the constituents extracted from a parse treeas argument candidates, then determining the roleof every constituent as part of Arg1, Arg2, orNULL, and finally, merging all the constituentsfor Arg1 and Arg2 to obtain the Arg1 and Arg2text spans respectively.
Obviously, the key tothe success of our constituent-based approach isconstituent-based argument classification, whichdetermines the role of every constituent argumentcandidate.As stated above, the PDTB views a connectiveas the predicate of a discourse relation.
Similarto semantic role labeling (SRL), for a given con-nective, the majority of the constituents in a parsetree may not be its arguments (Xue and Palmer,2004).
This indicates that negative instances (con-stituents marked NULL) may overwhelm positiveinstances.
To address this problem, we use asimple algorithm to prune out these constituentswhich are clearly not arguments to the connectivein question.4.1 PruningThe pruning algorithm works recursively in pre-processing, starting from the target connective n-ode, i.e.
the lowest node dominating the connec-tive.
First, all the siblings of the connective nodeare collected as candidates, then we move on tothe parent of the connective node and collect it-s siblings, and so on until we reach the root ofthe parse tree.
In addition, if the target connec-tive node does not cover the connective exactly,the children of the target connective node are alsocollected.For the example shown in Figure 1, we can lo-cate the target connective node [RBso] and returnfive constituents ?
[V Phave much broader busi-ness interests], [CCand], [V Pare better cushionedagainst price swings], [CCBut], and [NPits com-petitors] ?
as argument candidates.It is not surprising that the pruning algorithmworks better on gold parse trees than automaticparse trees.
Using gold parse trees, our pruning al-gorithm can recall 89.56% and 92.98% (489 out of546 Arg1s, 808 out of 869 Arg2s in the test data)of the Arg1 and Arg2 spans respectively and pruneout 81.96% (16284 out of 19869) of the nodes inthe parse trees.
In comparison, when automaticparse trees (based on the Charniak parser (Char-niak, 2000)) are used, our pruning algorithm canrecall 80.59% and 89.87% of the Arg1 and Arg2spans respectively and prune out 81.70% (1619070Feature Description ExampleCON-Str The string of the given connective (case-sensitive) soCON-LStr The lowercase string of the given connective soCON-CatThe syntactic category of the given connective: sub-ordinating, coordinating, or discourse adverbialSubordinatingCON-iLSib Number of left siblings of the connective 2CON-iRSib Number of right siblings of the connective 1NT-CtxThe context of the constituent.
We use POS combi-nation of the constituent, its parent, left sibling andright sibling to represent the context.
When there isno parent or siblings, it is marked NULL.VP-VP-NULL-CCCON-NT-PathThe path from the parent node of the connective tothe node of the constituentRB ?
V P ?
V PCON-NT-PositionThe position of the constituent relative to the connec-tive: left, right, or previousleftCON-NT-Path-iLsibThe path from the parent node of the connective tothe node of the constituent and whether the numberof left siblings of the connective is greater than oneRB ?
V P ?
V P :>1Table 1: Features employed in argument classification.out of 19816) of the nodes in the parse trees.4.2 Argument ClassificationIn this paper, a multi-category classifier is em-ployed to determine the role of an argument can-didate (i.e., Arg1, Arg2, or NULL).
Table 1 liststhe features employed in argument classification,which reflect the properties of the connective andthe candidate constituent, and the relationship be-tween them.
The third column of Table 1 showsthe features corresponding to Figure 1, consider-ing [RBso] as the given connective and [V Phavemuch broader business interests] as the constituentin question.Similar to Lin et al.
(2014), we obtained the syn-tactic category of the connectives from the list pro-vided in Knott (1996).
However, different fromLin et al.
(2014), only the siblings of the root pathnodes (i.e., the nodes occurring in the path of theconnective to root) are collected as the candidateconstituents in the pruning stage, and the value ofthe relative position can be left or right, indicat-ing that the constituent is located on the left- orright-hand of the root path respectively.
Besides,we view the root of the previous sentence as a spe-cial candidate constituent.
For example, the valueof the feature CON-NT-Position is previous whenthe current constituent is the root of the previoussentence.
Finally, we use the part-of-speech (POS)combination of the constituent itself, its parent n-ode, left sibling node and right sibling node to rep-resent the context of the candidate constituent.
In-tuitively, this information can help determine therole of the constituent.For the example shown in Figure 1, we first em-ploy the pruning algorithm to get the candidateconstituents, and then employ our argument clas-sifier to determine the role for every candidate.For example, if the five candidates are labeled asArg1, Arg2, Arg2, Arg1, and Arg1, respectively,we merge all the Arg1 constituents to obtain theArg1 text span (i.e., But its competitors have muchbroader business interests).
Similarly, we mergethe two Arg2 constituents to obtain the Arg2 text s-pan (i.e., and are better cushioned against priceswings).5 Joint Inference via Integer LinearProgrammingIn the above approach, decisions are always madefor each candidate independently, ignoring globalinformation across candidates in the final output.For example, although an argument span can besplit into multiple discontinuous segments (e.g.,the Arg2 span of Example (1) contains two dis-continuous segments, and, are better cushionedagainst price swings), the number of discontinu-ous segments is always limited.
Statistics on thePDTB corpus shows that the number of discontin-71uous segments for both Arg1 and Arg2 is generally(>= 99%) at most 2.
For Example (1), from leftto right, we can obtain the list of constituent can-didates: [CCBut], [NPits competitors], [V Phavemuch broader business interests], [CCand], [V Pare better cushioned against price swings].
If ourargument classifier wrongly determines the rolesas Arg1, Arg2, Arg1, Arg2, and Arg1 respectively,we can find that the achieved Arg1 span containsthree discontinuous segments.
Such errors may becorrected from a global perspective.In this paper, a joint inference mechanism is in-troduced to incorporate various kinds of knowl-edge to resolve the inconsistencies in argumen-t classification to ensure global legitimate predic-tions.
In particular, the joint inference mechanismis formalized as a constrained optimization prob-lem, represented as an integer linear programming(ILP) task.
It takes as input the argument classi-fiers?
confidence scores for each constituent can-didate along with a list of constraints, and outputsthe optimal solution that maximizes the objectivefunction incorporating the confidence scores, sub-ject to the constraints that encode various kinds ofknowledge.In this section, we meet the requirement of ILPwith focus on the definition of variables, the objec-tive function, and the problem-specific constraints,along with ILP-based joint inference integratingmultiple systems.5.1 Definition of VariablesGiven an input sentence, the task of argumen-t labeling is to determine what labels should beassigned to which constituents corresponding towhich connective.
It is therefore natural that en-coding the output space of argument labeling re-quires various kinds of information about the con-nectives, the argument candidates correspondingto a connective, and their argument labels.Given an input sentence s, we define followingvariables:(1) P : the set of connectives in a sentence.
(2) p ?
P : a connective in P .
(3) C(p): the set of argument candidates corre-sponding to connective p.
(i.e., the parse treenodes obtained in the pruning stage).
(4) c ?
C(p): an argument candidate.
(5) L: the set of argument labels {Arg1, Arg2,NULL }.
(6) l ?
L: an argument label in L.In addition, we define the integer variables asfollows:Zlc,p?
{0, 1} (1)If Zlc,p= 1, the argument candidate c, whichcorresponds to connective p, should be assignedthe label l. Otherwise, the argument candidate c isnot assigned this label.5.2 The Objective FunctionThe objective of joint inference is to find the bestarguments for all the connectives in one sentence.For every connective, the pruning algorithm is firstemployed to determine the set of correspondingargument candidates.
Then, the argument classifi-er is used to assign a label to every candidate.
Foran individual labeling Zlc,p, we measure the qualitybased on the confidence scores, fl,c,p, returned bythe argument classifier.
Thus, the objective func-tion can be defined asmax?l,c,pfl,c,pZlc,p(2)5.3 ConstraintsAs the key to the success of ILP-based joint infer-ence, the following constraints are employed:Constraint 1: The arguments correspondingto a connective cannot overlap with the connec-tive.
Let c1, c2..., ckbe the argument candidatesthat correspond to the same connective and over-lap with the connective in a sentence.1Then thisconstraint ensures that none of them will be as-signed as Arg1 or Arg2.k?i=1ZNULLci,p= k (3)Constraint 2: There are no overlapping or em-bedding arguments.
Let c1, c2..., ckbe the argu-ment candidates that correspond to the same con-nective and cover the same word in a sentence.21Only when the target connective node does not cover theconnective exactly and our pruning algorithm collects all thechildren of the target connective node as part of constituentcandidates, such overlap can be introduced.2This constraint only works in system combination ofSection 5.4, where additional phantom candidates may intro-duce such overlap.72Then this constraint ensures that at most one ofthe constituents can be assigned as Arg1 or Arg2.That is, at least k ?
1 constituents should be as-signed the special label NULL.k?i=1ZNULLci,p?
k ?
1 (4)Constraint 3: For a connective, there is at leastone constituent candidate assigned as Arg2.?cZArg2c,p?
1 (5)Constraint 4: Since we view the previous com-plete sentence as a special Arg1 constituent candi-date, denoted as m, there is at least one candidateassigned as Arg1 for every connective.
?cZArg1c,p+ ZArg1m,p?
1 (6)Constraint 5: The number of discontinuousconstituents assigned as Arg1 or Arg2 should be atmost 2.
That is, if argument candidates c1, c2..., ckcorresponding to the same connective are discon-tinuous, this constraint ensures that at most twoof the constituents can be assigned the same labelArg1 or Arg2.k?i=1ZArg1ci,p?
2, andk?i=1ZArg2ci,p?
2 (7)5.4 System CombinationPrevious work shows that the performance of ar-gument labeling heavily depends on the quality ofthe syntactic parser.
It is natural that combiningdifferent argument labeling systems on differen-t parse trees can potentially improve the overallperformance of argument labeling.To explore this potential, we build two argu-ment labeling systems ?
one using the Berke-ley parser (Petrov et al., 2006) and the other theCharniak parser (Charniak, 2000).
Previous s-tudies show that these two syntactic parsers tendto produce different parse trees for the same sen-tence (Zhang et al., 2009).
For example, our pre-liminary experiment shows that applying the prun-ing algorithm on the output of the Charniak parserproduces a list of candidates with recall of 80.59%and 89.87% for Arg1 and Arg2 respectively, whileachieving recall of 78.6% and 91.1% for Arg1 andArg2 respectively on the output of the BerkeleyFigure 2: An example on unifying different candi-dates.parser.
It also shows that combining these two can-didate lists significantly improves recall to 85.7%and 93.0% for Arg1 and Arg2, respectively.In subsection 5.2, we only consider the con-fidence scores returned by an argument classifier.Here, we proceed to combine the probabilities pro-duced by two argument classifiers.
There are tworemaining problems to resolve:?
How do we unify the two candidate lists?In principle, constituents spanning the samesequence of words should be viewed as thesame candidate.
That is, for different can-didates, we can unify them by adding phan-tom candidates.
This is similar to the ap-proach proposed by Punyakanok et al.
(2008)for the semantic role labeling task.
For exam-ple, Figure 2 shows the candidate lists gen-erated by our pruning algorithm based on t-wo different parse trees given the segment?its competitors have much broader businessinterests?.
Dashed lines are used for phan-tom candidates and solid lines for true can-didates.
Here, system A produces one can-didate a1, with two phantom candidates a2and a3 added.
Analogously, phantom can-didate b3 is added to the candidate list out-put by System B.
In this way, we can get theunified candidate list: ?its competitors havemuch broader business interests?, ?its com-petitors?, ?have much broader business inter-ests?.?
How do we compute the confidence score forevery decision?
For every candidate in theunified list, we first determine whether it isa true candidate based on the specific parsetree.
Then, for a true candidate, we extrac-t the features from the corresponding parse73tree.
On this basis, we can determine theconfidence score using our argument classi-fier.
For a phantom candidate, we set thesame prior distribution as the confidence s-core.
In particular, the probability of the?NULL?
class is set to 0.55, following (Pun-yakanok et al., 2008), and the probabilities ofArg1 and Arg2 are set to their occurrence fre-quencies in the training data.
For the exampleshown in Figure 2, since System A return-s ?its competitors have much broader busi-ness interests?
as a true candidate, we can ob-tain its confidence score using our argumen-t classifier.
For the two phantom candidates?
?its competitors?
and ?have much broaderbusiness interests?
?
we use the prior dis-tributions directly.
This applies to the candi-dates for System B.
Finally, we simply aver-age the estimated probabilities to determinethe final probability estimate for every candi-date in the unified list.6 ExperimentsIn this section, we systematically evaluate ourconstituent-based approach with a joint inferencemechanism to argument labeling on the PDTBcorpus.6.1 Experimental settingsAll our classifiers are trained using the OpenNLPmaximum entropy package3with the default pa-rameters (i.e.
without smoothing and with 100iterations).
As the PDTB corpus is aligned withthe PTB corpus, the gold parse trees and sentenceboundaries are obtained from PTB.
Under the au-tomatic setting, the NIST sentence segmenter4andthe Charniak parser5are used to segment and parsethe sentences, respectively.
lp solve6is used forour joint inference.This paper focuses on automatically labelingthe full argument spans of discourse connec-tives.
For a fair comparison with start-of-the-art systems, we use the NUS PDTB-style end-to-end discourse parser7to perform other sub-tasks of discourse parsing except argument label-ing, which includes connective identification, non-3http://maxent.sourceforge.net/4http://duc.nist.gov/duc2004/software/duc2003.breakSent.tar.gz5ftp://ftp.cs.brown.edu/pub/nlparser/6http://lpsolve.sourceforge.net/7http://wing.comp.nus.edu.sg/ linzihen/parser/explicit discourse relation identification and clas-sification.Finally, we evaluate our system on two aspects:(1) the dependence on the parse trees (GS/Auto,using gold standard or automatic parse trees andsentence boundaries); and (2) the impact of errorspropagated from previous components (noEP/EP,using gold annotation or automatic results fromprevious components).
In combination, we havefour different settings: GS+noEP, GS+EP, Au-to+noEP and Auto+EP.
Same as Lin et al.
(2014),we report exact match results under these four set-tings.
Here, exact match means two spans matchidentically, except beginning or ending punctua-tion symbols.6.2 Experimental resultsWe first evaluate the effectiveness of ourconstituent-based approach by comparing our sys-tem with the state-of-the-art systems, ignoringthe joint inference mechanism.
Then, the con-tribution of the joint inference mechanism to ourconstituent-based approach, and finally the contri-bution of our argument labeling system to the end-to-end discourse parser are presented.Effectiveness of our constituent-based ap-proachBy comparing with two state-of-the-art argu-ment labeling approaches, we determine the effec-tiveness of our constituent-based approach.Comparison with the linear tagging approachAs a representative linear tagging approach,Ghosh et al.
(2011; 2012; 2012) only reported theexact match results for Arg1 and Arg2 using theevaluation script for chunking evaluation8underGS+noEP setting with Section 02?22 of the PDTBcorpus for training, Section 23?24 for testing, andSection 00?01 for development.
It is also worthmentioning that an argument span can containmultiple discontinuous segments (i.e., chunks), sochunking evaluation only shows the exact matchof every argument segment but not the exact matchof every argument span.
In order to fairly compareour system with theirs, we evaluate our system us-ing both the exact metric and the chunking eval-uation.
Table 2 compares the results of our sys-tem without joint inference and the results report-ed by Ghosh et al.
(2012) on the same data split.We can find that our system performs much bet-8http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt74ter than Ghosh?s on both Arg1 and Arg2, even onmuch stricter metrics.Systems Arg1 Arg2ours using exact match 65.68 84.50ours using chunking evaluation 67.48 88.08reported by Ghosh et al.
(2012) 59.39 79.48Table 2: Performance (F1) comparison of our ar-gument labeling approach with the linear taggingapproach as adopted in Ghosh et al.
(2012)Comparison with the subtree extracting ap-proachFor a fair comparison, we also conduct ourexperiments on the same data split of Lin etal.
(2014) with Section 02 to 21 for training, Sec-tion 22 for development, and Section 23 for test-ing.
Table 3 compares our labeling system withoutjoint inference with Lin et al.
(2014), a representa-tive subtree extracting approach.
From the results,we find that the performance of our argument la-beling system significantly improves under all set-tings.
This is because Lin et al.
(2014) consideredall the internal nodes of the parse trees, whereasthe pruning algorithm in our approach can effec-tively filter out those unlikely constituents whendetermining Arg1 and Arg2.Setting Arg1 Arg2Arg1&2oursGS+noEP 62.84 84.07 55.69GS+EP 61.46 81.30 54.31Auto+EP 56.04 76.53 48.89Lin?sGS+noEP 59.15 82.23 53.85GS+EP 57.64 79.80 52.29Auto+EP 47.68 70.27 40.37Table 3: Performance (F1) comparison of our ar-gument labeling approach with the subtree extrac-tion approach as adopted in Lin et al.
(2014)As justified above, by integrating the advan-tages of both linear tagging and subtree extraction,our constituent-based approach can capture bothrich syntactic information from parse trees andlocal sequential dependency between tokens.
Theresults show that our constituent-based approachindeed significantly improves the performanceof argument labeling, compared to both lineartagging and subtree extracting approaches.Contribution of Joint InferenceSame as Lin et al.
(2014), we conduct our ex-periments using Section 02 to 21 for training, Sec-tion 22 for development, and Section 23 for test-ing.
Table 4 lists the performance of our argumen-t labeling system without and with ILP inferenceunder four different settings, while Table 5 reportsthe contribution of system combination.
It showsthe following:?
On the performance comparison of Arg1 andArg2, the performance on Arg2 is much bet-ter than that on Arg1 with the performancegap up to 8% under different settings.
This isdue to the fact that the relationship betweenArg2 and the connective is much closer.
Thisresult is also consistent with previous studieson argument labeling.?
On the impact of error propagation from con-nective identification, the errors propagatedfrom connective identification reduce the per-formance of argument labeling by less than2% in both Arg1 and Arg2 F-measure underdifferent settings.?
On the impact of parse trees, using automat-ic parse trees reduces the performance of ar-gument labeling by about 5.5% in both Arg1and Arg2 F-measure under different settings.In comparison with the impact of error prop-agation, parse trees have much more impacton argument labeling.?
On the impact of joint inference, it improvesthe performance of argument labeling, espe-cially on automatic parse trees by about 2%.9?
On the impact of system combination, theperformance is improved by about 1.5%.Setting Arg1 Arg2Arg1&2withoutJointInferenceGS+noEP 62.84 84.07 55.69GS+EP 61.46 81.30 54.31Auto+noEP 57.75 79.85 50.27Auto+EP 56.04 76.53 48.89withJointInferenceGS+noEP 65.76 83.86 58.18GS+EP 63.96 81.19 56.37Auto+noEP 60.24 79.74 52.55Auto+EP 58.10 76.53 50.73Table 4: Performance (F1) of our argument label-ing approach.Contribution to the end-to-end discourse pars-er9Unless otherwise specified, all the improvements in thispaper are significant with p < 0.001.75Systems Setting Arg1 Arg2Arg1&2CharniaknoEP 60.24 79.74 52.55EP 58.10 76.53 50.73BerkeleynoEP 60.78 80.07 52.98EP 58.80 77.21 51.43CombinednoEP 61.97 80.61 54.50EP 59.72 77.55 52.52Table 5: Contribution of System Combination inJoint Inference.Lastly, we focus on the contribution of our ar-gument labeling approach to the overall perfor-mance of the end-to-end discourse parser.
Thisis done by replacing the argument labeling mod-el of the NUS PDTB-style end-to-end discourseparser with our argument labeling model.
Table 6shows the results using gold parse trees and auto-matic parse trees, respectively.10From the results,we find that using gold parse trees, our argumentlabeling approach significantly improves the per-formance of the end-to-end system by about 1.8%in F-measure, while using automatic parse trees,the improvement significantly enlarges to 6.7% inF-measure.Setting New d-parser Lin et al.
?s (2014)GS 34.80 33.00Auto 27.39 20.64Table 6: Performance (F1) of the end-to-end dis-course parser.7 ConclusionIn this paper, we focus on the problem of auto-matically labeling the full argument spans of dis-course connectives.
In particular, we propose aconstituent-based approach to integrate the advan-tages of both subtree extraction and linear taggingapproaches.
Moreover, our proposed approach in-tegrates inter- and intra-sentence argument label-ing by viewing the immediately preceding sen-tence as a special constituent.
Finally, a join-t inference mechanism is introduced to incorpo-rate global information across arguments into our10Further analysis found that the error propagated fromsentence segmentation can reduce the performance of theend-to-end discourse parser.
Retraining the NIST sentencesegmenter using Section 02 to 21 of the PDTB corpus, theoriginal NUS PDTB-style end-to-end discourse parser canachieve the performance of 25.25% in F-measure, while thenew version (i.e.
replace the argument labeling model withour argument labeling model) can achieve the performanceof 30.06% in F-measure.constituent-based approach via integer linear pro-gramming.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.This research is also partially supported by Keyproject 61333018 and 61331011 under the Nation-al Natural Science Foundation of China.ReferencesEugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the First Meetingof the North American Chapter of the Associationfor Computational Linguistics, pages 132?139.Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, RashmiPrasad, Aravind Joshi, and Bonnie Webber.
2005.Attribution and the (non-)alignment of syntactic anddiscourse arguments of connectives.
In Proceedingsof the Workshop on Frontiers in Corpus AnnotationII: Pie in the Sky, pages 29?36.Robert Elwell and Jason Baldridge.
2008.
Discourseconnective argument identification with connectivespecific rankers.
In Second IEEE International Con-ference on Semantic Computing, pages 198?205.Sucheta Ghosh, Richard Johansson, Giuseppe Riccar-di, and Sara Tonelli.
2011.
Shallow discourse pars-ing with conditional random fields.
In Proceedingsof the 5th International Joint Conference on NaturalLanguage Processing, pages 1071?1079.Sucheta Ghosh, Giuseppe Riccardi, and Richard Jo-hansson.
2012.
Global features for shallow dis-course parsing.
In Proceedings of the 13th AnnualMeeting of the Special Interest Group on Discourseand Dialogue, pages 150?159.Sucheta Ghosh.
2012.
End-to-End Discourse Parsingusing Cascaded Structured Prediction.
Ph.D. thesis,University of Trento.Alistair Knott.
1996.
A Data-Driven Methodology forMotivating a Set of Coherence Relations.
Ph.D. the-sis, University of Edinburgh.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
APDTB-styled end-to-end discourse parser.
NaturalLanguage Engineering, 20(2):151?184.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.76Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, andBonnie Webber.
2004.
The Penn Discourse Tree-bank.
In Proceedings of the Fourth InternationalConference on Language Resources and Evaluation,pages 2237?2240.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 433?440.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse TreeBank 2.0.In Proceedings of the LREC 2008 Conference, pages2961?2968.Rashmi Prasad, Aravind Joshi, and Bonnie Webber.2010.
Exploiting scope for shallow discourse pars-ing.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The important of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2):257?287.Bonnie Webber.
2004.
D-LTAG: extending lexicalizedTAG to discourse.
Cognitive Science, 28(5):751?779.Ben Wellner and James Pustejovsky.
2007.
Automat-ically identifying the arguments of discourse con-nectives.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 92?101.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proceedingsof 2004 Conference on Empirical Methods in Natu-ral Language Processing, pages 88?94.Hui Zhang, Min Zhang, Chew Lim Tan, and HaizhouLi.
2009.
K-best combination of syntactic parsers.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1552?1560.77
