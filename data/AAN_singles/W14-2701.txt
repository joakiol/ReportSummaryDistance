Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 1?6,Baltimore, Maryland USA, 27 June 2014.c?2014 Association for Computational LinguisticsDetecting sociostructural beliefs about group status differences in onlinediscussionsBrian Riordan Heather WadeAptima, Inc.3100 Presidential DriveFairborn, OH 45324{briordan, hwade}@aptima.comAfzal UpalDefence R&D Canada Toronto1133 Sheppard Ave WToronto, ON, M3K 2C9Afzal.Upal@drdc-rddc.gc.caAbstractDetection of fine-grained opinions andbeliefs holds promise for improved so-cial media analysis for social science re-search, business intelligence, and govern-ment decision-makers.
While commercialapplications focus on mapping landscapesof opinions towards brands and products,our goal is to map ?sociostructural?
land-scapes of perceptions of social groups.
Inthis work, we focus on the detection ofviews of social group status differences.We report an analysis of methods for de-tecting views of the legitimacy of incomeinequality in the U.S. from online dis-cussions, and demonstrate detection ratescompetitive with results from similar taskssuch as debate stance classification.1 IntroductionSocial media and the internet continue to be a vastresource for exploring and analyzing public opin-ion.
While there has been a longstanding focuson detecting sentiment for commercial applica-tions (Liu, 2012), in recent years there has been in-creased interest in detecting opinions and perspec-tives in politics and social science more generally(Grimmer & Stewart, 2013).
Examples includeanalyzing people?s perceptions of particular politi-cal issues by classifying debate stances (Hasan andNg, 2013) and detecting the expression of ideol-ogy (Sim et al., 2013).
Research has increasinglyturned from detecting opinions and beliefs in gen-eral (Prabhakaran et al., 2010) to discerning par-ticular types of opinions or beliefs for specific ap-plications.The goal of our work is to detect indicators ofpeople?s views of social conditions and intergroupperceptions in social media.
Working within theframework of Social Identity Theory (Tajfel andTurner, 1979; Tajfel and Turner, 1986; Turner,1999), we explore detection of the linguistic cor-relates of sociostructural beliefs.
Sociostructuralbeliefs are abstract theoretical constructs in SocialIdentity Theory that underpin individual and so-cial identity formation and individual actions thataffect the relations between social groups.For this study, we focus on class-based socialgroups and the views of individuals on the issueof income inequality.
We seek to detect people?sviews of the legitimacy of the socio-economicstructure that has resulted in increasing income in-equality, particularly in the U.S. Our approach fo-cuses on comments on news articles related to theissue of income inequality.
We develop a series ofsupervised classifiers for detecting the expressionof views on the legitimacy of income inequality.We show promising results comparable to detec-tion rates for other studies of social and politicalperspectives.2 BackgroundSocial Identity Theory attempts to account for howsubjectively perceived social structure can leadpeople to define themselves in terms of a sharedsocial identity and thereby produce forms of in-tergroup behavior.
Social identity ?
how peopleperceive their relations to the multiple groups towhich they belong ?
is argued to be a crucial partof a person?s self-concept.
People invoke part oftheir social identities whenever they think of them-selves as belonging to one gender, ethnicity, socialclass, religion, etc.
Group membership and socialidentity play a role in shaping interpersonal inter-actions.Social Identity Theory (as well as social catego-rization theory) holds that people are sensitive togroup status differences and are motivated to viewtheir own social groups positively.
These two fac-tors are key drivers of individuals?
social identitymanagement strategies.
For example, membership1in a relatively low-status group may engender per-ceptions of deprivation, which in turn may result inindividuals taking actions to increase their group?sstatus or diminish the status of other groups (Tajfeland Turner, 1979; Tajfel and Turner, 1986).
Ac-cording to Social Identity Theory, a group mem-ber?s expectations of rewards of group member-ship are importantly affected by sociostructuralbeliefs about the nature of group status differ-ences.
Group status differences are thought to beshaped by three types of these beliefs:?
Legitimacy: the degree to which people be-lieve that group status differences are valid.?
Stability: people?s sense of how likely the sta-tus hierarchy is to last into the future.?
Permeability: the perception of how easy it isfor outsiders to enter or leave the group.Based on these sociostructural beliefs and percep-tions of the relative deprivation of one?s group,people are motivated to take actions to maintainand enhance their group?s image.3 Detecting sociostructural beliefsA central challenge for extracting sociostructuralbeliefs is determining where they are likely tooccur in natural discourse on the internet.
So-ciostructural beliefs relate to group status differ-ences ?
for example, in terms of wealth, power, orprestige.
Hence, the most likely context for so-ciostructural belief expressions is discussions ofissues that relate to such social differences.While debate-focused websites (e.g., createde-bate.com, debate.org) hold potential as a datasource, we found that in practice such websiteshad few discussions of issues that might relate tosociostructural beliefs and, furthermore, the num-ber of posts for each topic was generally small.In contrast, we found that highly relevant data canbe harvested from comments on news or opinionarticles from large newspapers or popular mediawebsites.
Articles and op-eds commonly generatehundreds of responses.
We considered a varietyof topics related to social differences in ethnicity,gender, religion, etc., but found the most data onthe topic of income inequality in the U.S. We col-lected comments across several news articles andop-ed pieces that focused on income inequality.In the context of income inequality, the socialgroups are hard to rigorously define, but in com-ments it was common to observe a dichotomy be-tween ?rich?
and ?poor,?
or ?the 1 percent?
and?everyone else?.
We observed comments on eachof the three types of sociostructural beliefs ?
legit-imacy, stability, permeability ?
but by far the mostcommon topic of discussion was the legitimacy ofa large income gap.
Therefore, we focused on de-tecting expressions of legitimacy and leave the ex-traction of expressions of stability and permeabil-ity to future work.In past survey research related to sociostructuralbeliefs (Kessler and Mummendey, 2002; Mum-mendey et al., 1999), participants were askedto respond to explicit statements reflecting so-ciostructural beliefs ?
for example, It is [justi-fied|right|legitimate|accurate|fair] that [proposi-tion].
However, we found no instances of suchexplicit expressions in our data.
Nevertheless,beliefs about legitimacy are implicit in many in-stances.
For example, consider this comment:Now we are all victims and we shouldbe given our fair share instead of earn-ing our fair share.
All the wealth shouldbe redistributed.
The wealthy are vil-ianized.
The ones who have been ableto rely on their vision, innovation, selfmotivation, sacrifice and wits are be-ing called out by the envious.Like it ornot, the one-percenters are the ones whohave advanced humanity to the higheststandard ofliving - ever.Although there is no explicit articulation of abelief that it is legitimate for income inequality toexist across social groups, for human annotators,it is not difficult to infer that this author likelybelieves that this is the case.
Our goal is to un-cover cases like this where sociostructural beliefsare strongly implicit.We formulated the problem as staged text clas-sification (cf.
Lamb et al.
(2013)):1.
Finding comments that implicity express thesociostructural belief in the legitimacy or il-legitimacy of income inequality (+/-E);2.
Making a binary classification of the author?ssociostructural belief (income inequality islegitimate or not) (+/-L).4 Data CollectionWe scraped more than 10,000 comments from ar-ticles from major internet media outlets related to2the income inequality issue in the U.S., includingCNN, The New York Times, Daily Finance, andmarketwatch.com (The Wall Street Journal).
Forexample, we collected comments from the CNNop-ed ?Is income inequality ?morally wrong??
?1,which had attracted several thousand comments atthe time of data collection (and continues to re-ceive more).An initial set was randomly selected for annota-tion for +/-E and +/-L by one of the authors.
An-other author independently annotated a subset ofthese comments (N=100) and agreement was as-sessed.
While the agreement was low for the +/-E label (?
= .282), for comments that the anno-tators agreed were +E, the inter-annotator agree-ment was high (?
= .916).
After the annotatorsdiscussed and resolved differences in the +/-E an-notation guidelines, the first annotator continuedthe annotation process to compile a final dataset.Table 1 gives a summary of the final corpus.+ - TotalExpression related to le-gitimacy (E)400 1,088 1,488Support for legitimacy (L) 174 226 400Table 1: Dataset annotation statistics.5 Features5.1 N-gramsAs with similar tasks such as debate stance classi-fication and sentiment tagging, token-level differ-ences should provide a strong baseline for discrim-inating between the classes of belief expression(+/-E) and the belief in legitimacy (+/-L).
There-fore, we explored a variety of combinations of n-gram features, including surface tokens, lemmas,and parts of speech.5.2 Word classesBeyond n-gram features, we expected that co-herent sets of tokens would pattern together forimplicit beliefs about legitimacy of status differ-ences.
One of the authors coded a total of 24classes for the income inequality setting based onannotating a subset of about 100 comments.
Ex-amples are shown in Table 2.
The classes reflectedboth semantic similarity and, for some, polarity ofthe sociostructural belief.1http://www.cnn.com/2013/07/25/opinion/sutter-income-inequality-moral-obama/Word class Example wordsincome inequality gap, widening, inequalitylack of income in-equalityequal chance, never fair,free societythe non-rich (+) the 99%, have-notsthe non-rich (+/-) the poor, middle-classthe non-rich (-) lazy, dumbchange (+) fix, make changeschange (-) redistribution, imposegreed greed, exploithardship can?t afford, cost of livingrich ?
epithets shameful, evil, no empathypoor ?
epithets soviet, communist, envyrich individuals Buffet, Gates, Bloombergsociety safety net, playing fieldbusiness companies, profitmoney wealth, income level, salarythe rich (+) wealthy, those with meansthe rich (+/-) upper middle classthe rich (-) extreme rich, the 1%deserve deserve, earnwork / effort work harder, effortsuccess success, fortune, move upgovernment regulation, bloatedtaxes taxes, taxpayer, pay most oflifestyle save, budget, responsibilityTable 2: Example word classes.5.3 Quotation-related featuresExcerpts from other posters?
comments and quo-tations of famous individuals are common in ourdataset.
For example:?Everyone in America has an equalchance an equal opportunity to suc-ceed.?
Dont know if Id go THAT far.The author quotes a previous post?s words in or-der to explicitly disagree with a statement.
In thiscase, n-gram features might indicate that the com-ment should be labeled +L (since comments dis-cussing an ?equal opportunity to succeed?
typi-cally expressed this belief).
However, the sec-ond sentence expresses a negation of the ideas inthe quoted text.
This issue is common in dia-logic social media settings, particularly when de-bating political or social issues, and poses a chal-lenge to surface-oriented classifiers (Malouf andMullen, 2008).
To address this issue, n-gram fea-tures were computed specifically for text inside3quotes (?quote features?)
and text outside quotes(?nonquote features?).
In the quote above, thewords Everyone in America has an equal chance...would contribute to the quote n-grams.6 ExperimentsFor classification, we experimented with NaiveBayes and MaxEnt (via MALLET2) and SVMs(via LIBSVM3).
Our baseline was a majority classpredictor.
We began by comparing the results ofseveral different n-gram sets, including n-gramsfrom surface text or lemmatization, binary labelsor count features, combinations of unigrams, bi-grams, trigrams, and 4-grams, and the inclusionor exclusion of stopwords.
We found that the n-grams set of binary labels for unigrams, bigrams,trigrams, and 4-grams after lemmatization had thehighest performance.
The inclusion of stopwordsgenerally afforded better performance; hence wedo not remove stopwords.We explored the hypothesis that this result wasdue to the inclusion of negation operators amongstopwords.
Negation may be useful to retain inn-grams to distinguish expressions such as didn?tearn from earned.
We removed negation operatorsfrom the stopword list.
However, other than Max-Ent, performance was worse4.
What stylometricfeatures that stopwords capture to distinguish au-thors?
beliefs in this task is left for future work.Classifier +/-E +/-LMLE 73.1 56.5MaxEnt 79.9 66.0Naive Bayes 75.9 68.3SVM 80.1 66.3Table 3: Comparison of classifiers by accuracy onthe +/-E and +/-L task with a feature set of: uni-gram, bigram, trigram, and 4-gram lemma labels,stopwords included.
MLE = majority class.The results for both the +/-E and +/-L tasks areshown in Table 3.
We report accuracy followingprevious related work.
We only report results forthe staged classifier setting (-E posts were not an-notated for +/-L).
For the +/-E task, absolute ac-curacy values were high due to the very unbal-anced dataset (cf.
Table 1).
On the +/-L task,Naive Bayes achieved the highest accuracy score.2http://mallet.cs.umass.edu/3http://www.csie.ntu.edu.tw/ cjlin/libsvm/4ME = 66.5, NB = 65.8, SVM = 63.0Our dataset consisted of a mix of short and longcomments (M = 45.4 tokens, SD = 37.5 tokens),which, interestingly, was not unfavorable to NaiveBayes (cf.
Wang and Manning (2012)).
All classi-fiers were significantly better than the baseline (bypaired samples t-tests on accuracy across folds incross-validation with p<.05) in both tasks.
On +/-E, MaxEnt and SVM were not significantly differ-ent; both performed better than Naive Bayes.
On+/-L, there were no significant differences.Tables 4 and 5 report the results after addingthe +/-L problem-specific features to the best n-gram set.
The addition of the word class featuresprovides a small improvement in accuracy acrossthe classifiers.
MaxEnt?s performance approachedsignificance compared to the others (p <.1) Theseresults confirm that, for the task of detecting so-ciostructural beliefs about legitimacy in this do-main, words tokens do tend to co-occur in topicaland polarity-based word classes.
However, it islikely that our word class feature set suffered fromlimited coverage relative to the diversity of expres-sions used in the domain.Feature set MLE ME NB SVMn-grams 56.5 66.0 68.3 66.3+ WC counts 56.5 70.8 68.8 67.0+ WC lab.
56.5 69.5 68.0 67.0+ WC counts, lab.
56.5 69.8 68.8 66.8Table 4: Classification accuracies for the +/-L taskon variants of word class (WC) feature sets forMaxEnt, NB, and SVM.
MLE = majority class.Table 5 reports the results of adding quotationfeatures.
Performance improved with the additionof these features, most notably with the additionof both quote and nonquote features.
While theseresults suggest that accounting for quotations isimportant, the inclusion of quotation-related fea-tures only differentiates between words appearingin quotations from those outside quotations, anddoes not represent any relationship between thetwo sets of features.
The appearance of terms in aquotation that are typically not found in quotationsand that are used by people expressing a particularstance is often a strong indicator that the opinionof the text surrounding the quotation is the oppo-site of that in the quotation a relationship foundby Malouf and Mullen (2008)).
Hence, more re-search that explores relations between terms in andoutside of quotations would seem worthwhile.4Finally, we experimented with combining bothword class features and quotation features, but per-formance did not improve over the results for wordclass features or quote features alone.Feature set MLE ME NB SVMn-grams 56.5 66.0 68.3 66.3+ Q count 56.5 67.0 68.3 66.8+ Q labels 56.5 66.0 68.8 66.3+ Q count & lab.
56.5 66.5 69.3 65.3+ NQ labels 56.5 66.3 69.0 65.3+ Q & NQ 56.5 67.3 70.0 66.3repl.
w/ Q & NQ 56.5 67.3 70.5 66.3Table 5: Classification accuracies for the +/-L taskon variants of quotation (Q, NQ) feature sets forMaxEnt, NB, and SVM.
MLE = majority class.7 Error analysis7.1 Focus on a specific sub-issueIn discussions on income inequality, there are?sub-issues?
that are repeatedly discussed in com-ments, including taxes, welfare, the U.S. economy,and business owners.
The difficulty of classifyingthese kinds of comments stems from the difficultyof deciding whether the comments contain an im-plicit expression of a sociostructural belief, i.e.the +/- E classification problem.
Inference basedon world knowledge may be required to chain to-gether the steps that link expressions to beliefs.7.2 Personal stories used as examplesIn discussions involving social status, we observedthat people often use personal examples to supporttheir positions.My Dad slept in a dresser drawer on thefloor with cotton stuffed under a sheet...He graduated with an engineering de-gree summa cum laude and has neverbeen un-employed for 45 years becausehe always worked harder and made him-self more valuable than his peers.
No GIBill No Pell Grants No Welfare...While a human annotator can usually infer whichview on legitimacy such a story supports, the con-tent can seem unrelated to the issue of interest.Similar behavior occurs on debate websites, wheredescriptions of personal experiences add materialirrelevant to stance, often leading to misclassifica-tion (Hasan and Ng, 2013).7.3 Importance of contextWhile we considered comments independently forour classification task, comments can refer to orreply to previous comments, such that the meaningof a comment can be obscured without the con-tent of these related comments.
To address this is-sue, techniques for incorporating other commentsin dialog threads may be fruitful (Walker et al.,2012; Hasan and Ng, 2013).8 Related WorkThe goal of detection of sociostructural beliefs inthe context of Social Identity Theory is similar towork in debate stance classification (Anand et al.,2011; Hasan and Ng, 2013; Somasundaran andWiebe, 2009; Walker et al., 2012).
For example,Hasan and Ng (2013) developed methods for clas-sifying author postings on debate websites into bi-nary classes reflecting opposing stances on polit-ical issues (e.g., gay marriage).
Our setting dif-fers in that ?sides?
of the issue are only hypoth-esized (i.e., legitimate/illegitimate) and not given,and stances are never explicitly observed.
How-ever, the behavior of posters appears to be similaracross debate sites and comments on news articles.The work here also fits into the increasing focuson content analysis for political and social scienceanalysis (Grimmer and Stewart, 2013).
Much re-cent work has focused on analysis of artifacts fromthe political arena, such as speeches, floor debates,or press releases (Gerrish and Blei, 2012; Sim etal., 2013; Thomas et al., 2006).9 DiscussionThis work explored the task of detecting latent au-thor beliefs in social media analysis.
We focusedon the specific problem of detecting and classi-fying sociostructural beliefs from Social IdentityTheory ?
beliefs about the legitimacy, stability,and permeability of social groups and their status.We collected and analyzed a dataset of social me-dia comments centering on the issue of income in-equality and sought to classify implicit author be-liefs on the legitimacy of class-based income dis-parity.
Because of the heavily implicit nature ofsociostructural belief expression, we formulatedthe detection problem as a form of text classifi-cation.
Our approach achieved classification accu-racies competitive with results from similar taskssuch as debate stance classification.5ReferencesAnand, Pranav, Walker, Marilyn, Abbott, Rob, Tree,Jean.
E. Fox, Bowmani, Robeson, and Minor,Michael.
2011.
Classifying stance in online de-bate.
In Proceedings of the 2nd workshop on com-putational approaches to subjectivity and sentimentanalysis.Gerrish, Sean M., and Blei, David M. 2012.
How TheyVote: Issue-Adjusted Models of Legislative Behav-ior.
In Advances in Neural Information ProcessingSystems.Grimmer, Justin, and Stewart, Brandon M. 2013.
Textas data: The promise and pitfalls of automatic con-tent analysis methods for political texts.
PoliticalAnalysis, 21(3), 267297.Hasan, Kazi Saidul, and Ng, Vincent.
2013.
FrameSemantics for Stance Classification.
CoNLL-2013,124.Kessler, Thomas, and Mummendey, Am?elie.
2002.Sequential or parallel processes?
A longitudi-nal field study concerning determinants of identity-management strategies.
Journal of Personality andSocial Psychology, 82(1), 75-88.Lamb, Alex, Paul, Michael J., and Dredze, Mark.2013.
Separating fact from fear: Tracking flu in-fections on Twitter.
In Proceedings of NAACL-HLT.Liu, Bing.
2012.
Sentiment analysis and opinion min-ing.
Morgan & Claypool.Malouf, Robert, and Mullen, Tony.
2008.
Takingsides: User classification for informal online polit-ical discourse.
Internet Research, 18(2), 177-190.Mummendey, Am?elie, Klink, Andreas, Mielke, Rose-marie, Wenzel, Michael, and Blanz, Mathias.
1999.Sociostructural characteristics of intergroup rela-tions and identity management strategies: resultsfrom a field study in East Germany.
European Jour-nal of Social Psychology, 29(2-3), 259285.Prabhakaran, Vinodkumar, Rambow, Owen, and Diab,Mona.
2010.
Automatic committed belief tagging.In Proceedings of COLING.Sim, Yanchuan, Acree, Brice, Gross, Justin H., andSmith, Noah A.
2013.
Measuring ideological pro-portions in political speeches.
In Proceedings ofEMNLP.Somasundaran, Swapna, and Wiebe, Janyce.
2009.Recognizing stances in online debates.
In Proceed-ings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP.Tajfel, Henri and Turner, John C. 1979.
An integrativetheory of intergroup conflict.
In W. G. Austin and S.Worchel (Eds.
), The social psychology of intergrouprelations (pp.
3347).
Monterey, CA: Brooks-Cole.Tajfel, Henri and Turner, John C. 1986.
The so-cial identity theory of intergroup behaviour.
In S.Worchel, and W. G. Austin (Eds.
), Psychology of in-tergroup relations (pp.
724).
Chicago, IL: Nelson-Hall.Thomas, Matt, Pang, Bo, and Lee, Lillian.
2006.Get out the vote: Determining support or opposi-tion from Congressional floor-debate transcripts.
InProceedings of EMNLP.Turner, John C. 1999.
Some current issues in researchon social identity and self-categorization thoeries InEllemers, N., Spears, R., Doosje, B.
Social identity(pp.
6-34).
Oxford: Blackwell.Walker, Marilyn A., Anand, Pranav, Abbott, Robert,and Grant, Ricky.
2012.
Stance classification usingdialogic properties of persuasion.
In Proceedings ofNAACL-HLT.Wang, Sida I., and Manning, Christopher D. 2012.Baselines and Bigrams: Simple, Good Sentimentand Topic Classification.
In Proceedings of ACL.6
