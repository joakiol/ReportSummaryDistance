Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 39?48,Portland, Oregon, June 2011. c?2011 Association for Computational LinguisticsTop-down recognizers for MCFGs and MGsEdward P. Stablerstabler@ucla.eduAbstractThis paper defines a normal form for MCFGsthat includes strongly equivalent representa-tions of many MG variants, and presentsan incremental priority-queue-based TD rec-ognizer for these MCFGs.
After introduc-ing MGs with overt phrasal movement, headmovement and simple adjunction are addedwithout change in the recognizer.
The MGrepresentation can be used directly, so thateven rather sophisticated analyses of properlynon-CF languages can be defined very suc-cinctly.
As with the similar stack-based CF-methods, finite memory suffices for the recog-nition of infinite languages, and a fully con-nected left context for probabilistic analysis isavailable at every point.1 IntroductionIn the years after Joshi (1985) proposed that humanlanguages are weakly and strongly ?mildly contextsensitive?
(MCS), it was discovered that many in-dependently proposed grammar formalisms defineexactly the same MCS languages.
The languagesdefined by Joshi?s tree adjoining grammars (TAGs)are exactly the same as those defined by a ver-sion of Steedman?s combinatory categorial gram-mars, and the same as those defined by head wrap-ping grammars (Vijay-Shanker and Weir, 1994).
Aslightly larger class of languages is defined by an-other variant of TAGs (set-local multicomponent),by a version of Pollard?s generalized phrase struc-ture grammars called multiple context free gram-mars (MCFGs), and by a wide range of minimalistgrammar (MG) formalizations of Chomskian syn-tax (Seki et al, 1991; Michaelis, 1998; Michaelis,2001b; Harkema, 2001a; Stabler, 2011).
Theseremarkable convergences provide evidence fromacross grammatical traditions that something likethese MCS proposals may be approximately right,and so it is natural to consider psychological modelsthat fit with these proposals.
With a range of per-formance models for a range of MCS grammars, itbecomes possible to explore how grammatical de-pendencies interact with other factors in the condi-tioning of human linguistic performance.For context free grammars (CFGs), perhaps thesimplest parsing model is top-down: beginning withthe prediction of a sentence, rules are applied to theleftmost predicted category until a terminal elementis reached, which is then checked against the in-put.
This parsing method is of interest in psycho-logical modeling not only because it uses the gram-mar in a very transparent way, but also it is becauseit is predictive in a way that may be similar to hu-man parsing.
At every point in analyzing a sentencefrom left to right, the structure that has been con-structed is fully connected: grammatical relation-ships among the elements that have been heard havebeen guessed, and there are no pieces of structurewhich have not been integrated.
Consequently, thisstructure can be interpreted by a standard composi-tional semantics and may be appropriate for ?incre-mental?
models of sentence interpretation (cf.
Had-dock, 1989; Chambers et al, 2004; Shen and Joshi,2005; Altmann and Mirkovic?, 2009; Demberg andKeller, 2009, Kato and Matsubara, 2009; Schuler,2010).
And like human parsing, when used with39backtracking or a beam search, TD memory de-mands need not continually increase with sentencelength: a fixed bound on stack depth and on back-track or beam depth suffices for infinitely many sen-tences.
Furthermore, TD parsing provides explicit,relevant ?left contexts?
for probabilistic condition-ing (Roark and Johnson, 1999; Roark, 2001; Roark,2004).
But it has not been clear until recently howto apply this method to Chomskian syntax or anyof the other MCS grammar formalisms.
There havebeen some proposals along these lines, but they haveeither been unnecessarily complex or applicable toonly a restricted to range of grammatical proposals(Chesi, 2007; Mainguy, 2010).This paper extends TD parsing to minimalist con-text free grammars (MCFGs) in a certain normalform and presents minimalist grammars (MGs) as asuccinct representation for some of those MCFGs.With this extension, the TD parsing method han-dles an infinite range of MCFGs that encompasses,strongly and weakly, an infinite range of (many vari-ants of) MGs in a very transparent and direct way.The parsing method can be defined in complete de-tail very easily, and, abstracting away from limita-tions of time and memory, it is provably sound andcomplete for all those grammars.The TD recognizer for MCFGs is presented in ?4,generalizing and adapting ideas from earlier work(Mainguy, 2010; Villemonte de la Clergerie, 2002).Instead of using a stack memory, this recognizeruses a ?priority queue,?
which just means that wecan access all the elements in memory, sorting theminto left-to-right order.
Then it is easy to observe:(?3.2) while the reference to MCFG is useful forunderstanding the recognizer, an MG representationcan be used directly without explicitly computingout its MCFG equivalent; (?5.1) the extensions forhead movement and simple adjunction allow the rec-ognizer of ?4 to apply without change; (?5.2) like itsstack-based CF counterpart, the MG recognizer re-quires only finite memory to recognize certain infi-nite subsets of languages ?
that is, memory demandsdo not always strictly increase with sentence length;and (?5.3) the TD recognizer provides, at every pointin processing the input, a fully connected left con-text for interpretation and probabilistic condition-ing, unlike LC and other familiar methods.
Sincea very wide range of grammatical proposals can beexpressed in this formalism and parsed transparentlyby this method, it is straightforward to compute fullyexplicit and syntactically sophisticated parses of thesorts of sentences used in psycholinguistic studies.2 MCFGsMCFGs are first defined by Seki et al (1991),but here it will be convenient to represent MCFGsin a Prolog-like Horn clause notation, as inKanazawa (2009).
In this notation, the familiar con-text free rule for sentences would be writtenS(x01x11) :- NP (x01),V P (x11).Reading :- as ?if?, this formula says that a stringformed by concatenating any string x01 with stringx11 is an S, if x01 is an NP, and x11 is a VP.
Wenumber the variables on the right side in such a wayas to indicate that each variable that appears on theright side of any rule appears exactly once on theright and once on the left.
Lexical rules likeNP (Mary)V P (sings),have empty ?right sides?
and no variables in this no-tation.MCFGs allow categories to have multiple stringarguments, so that, for example, a VP with a wh-phrase that is moving to another position could berepresented with two string arguments, one of whichholds the moving element.
In general, each MCFGrule for building an instance of category A from cat-egories B0 .
.
.
Bn (n ?
0) has the form,A(t1, .
.
.
, td(A)) :- B0(x01 , .
.
.
, x0d(B0)),.
.
.
,Bn(xn1 , .
.
.
, xnd(Bn)),where each ti is an term (i.e.
a sequence) over the (fi-nite nonempty) vocabulary ?
and the variables thatappear on the right; no variable on the right occursmore than once on the left (no copying); and the des-ignated ?start?
category S has ?arity?
or ?dimension?d(S) = 1.
For any such grammar, the languageL(G) is the set of strings s ?
??
such that we canderive S(s).40Here, we restrict attention to a normal form inwhich (i) each MCFG rule is nondeleting in thesense that every variable xij on the right occurs ex-actly once on the left, and (ii) each rule is either lexi-cal or nonlexical, where a lexical rule is one in whichn = 0 and d(A) = 1 and t1 ?
??{?
}, and a nonlex-ical rule is one in which n > 0 and each ti ?
V ar?.Clearly these additional restrictions do not affect theexpressive power of the grammars.2.1 Example 1Consider this MCFG for {aibjcidj | i, j > 0}, with 5non-lexical rules, 4 lexical rules, and start categoryS.
We letter the rules for later reference:a.
S(x0x1x2x3) :- AC(x0, x2), BD(x1, x3)b. AC(x0x2, x1x3) :- A(x0), C(x1), AC(x2, x3)c. AC(x0, x1) :- A(x0), C(x1)d. BD(x0x2, x1x3) :- B(x0),D(x1), BD(x2, x3)e. BD(x0, x1) :- B(x0),D(x1)f. A(a)g. B(b)h. C(c)i.
D(d)With this grammar we can show that abbcdd has cat-egory S with a derivation tree like this:S(abbcdd)AC(a, c)A(a) C(c)BD(bb, dd)B(b) D(d) BD(b, d)B(b) D(d)See, for example, Kanazawa (2009) for a more de-tailed discussion of MCFGs in this format.3 MGs as MCFGsMichaelis (1998; 2001a) shows that every MG hasa ?strongly equivalent?
MCFG, in the sense that theMG derivation trees are a relabeling of the MCFGderivation trees.
Here we present MGs as finite setsof lexical rules that define MCFGs.
MG categoriescontain finite tuples of feature sequences, where thefeatures include categories like N,V,A,P,.
.
.
, selec-tors for those categories =N,=V,=A,=P,.
.
.
, licensors+case,+wh,.
.
.
, and licensees -case,-wh,.
.
.
.
In ourMCFG representation, a category is a tuple?x, ?0, ?1, .
.
.
, ?j ?where (i) j ?
0, (ii) x = 1 if the element is lexicaland 0 otherwise, (iii) each ?i is a nonempty featuresequence, and (iv) the category has dimension j+1.An MG is then given by a specified start categoryand a finite set of lexical rules?
1, ?0 ?
(a).for some a ?
?.
The MG defines the language gen-erated by its lexicon together with MCFG rules de-termined by the lexicon, as follows.
Let ?2(Lex)be the set of feature sequences ?0 contained in thelexical rules, and let k be the number of differ-ent types of licensees f that occur in the lexicalrules.
For all 0 ?
i, j ?
k, all x, y ?
{0, 1}, all?, ?, ?i, ?i ?
suffix(?2(Lex)), and ?
6= ?, we havethese ?merge?
rules, broken as usual into the caseswhere (i) we are merging into complement positionon the right, (ii) merging into specifier position onthe left, or (iii) merging with something that is mov-ing:?
0, ?, ?1, .
.
.
, ?j ?
(s0t0, t1, .
.
.
, tj) :-?
1,=f?
?
(s0),?x, f, ?1, .
.
.
, ?j ?
(t0, .
.
.
, tj)?
0, ?, ?1, .
.
.
, ?i, ?1, .
.
.
, ?j ?
(t0s0, s1, .
.
.
, si, t1, .
.
.
, tj) :-?
0,=f?, ?1, .
.
.
, ?i, ?
(s0, .
.
.
, si),?x, f, ?1, .
.
.
, ?j ?
(t0, .
.
.
, tj)?
0, ?, ?1, .
.
.
, ?i, ?, ?1, .
.
.
, ?j ?
(s0, .
.
.
, si, t0, .
.
.
, tj) :-?x,=f?, ?1, .
.
.
, ?i, ?
(s0, .
.
.
, si),?
y, f?, ?1, .
.
.
, ?j ?
(t0, .
.
.
, tj)And we have these ?move?
rules, broken as usualinto the cases where the moving element is landing,when ?i = -f ,?
0, ?, ?1, .
.
.
, ?i?1, ?i+1, .
.
.
, ?j ?
(sis0, s1, .
.
.
, si?1, si+1, .
.
.
, sj) :-?
0,+f?, ?1, .
.
.
, ?j ?
(s0, .
.
.
, sj),and cases where the moving element must moveagain, when ?i = -f?,?
0, ?, ?1, .
.
.
, ?i?1, ?, ?i+1, .
.
.
, ?j ?
(s0, .
.
.
, si) :-?
0,+f?, ?1, .
.
.
, ?j ?
(s0, .
.
.
, si),where none of ?1, .
.
.
, ?i?1, ?i+1, .
.
.
, ?j begin with-f .
The language of the MG is the MCFL defined bythe lexicon and all instances of these 5 rule schemes(always a finite set).By varying the lexicon, MGs can define all theMCFLs (Michaelis, 2001b; Harkema, 2001b), i.e.,41the set-local multi-component tree adjoining lan-guages (MCTALs) (Weir, 1988; Seki et al, 1991).TALs are a proper subset, defined by ?well-nested2-MCFGs?
(Seki et al, 1991; Kanazawa, 2009).3.1 Example 2Consider the following lexicon containing 7 items,with the ?complementizer?
start category C,?1,=D =D V?
(likes) ?1,D?
(Mary)?1,=C =D V?
(knows) ?1,D?
(John)?1,=V C?(?)
?1,D -wh?
(who)?1,=V +wh C?(?
)Using the definition given just above, this deter-mines an MG.
This is a derivation tree for one ofthe infinitely many expressions of category C:?0,C?
(Mary knows who John likes)?1,=V C?(?)
?0,V?
(Mary knows who John likes)?0,=D V?
(knows who John likes)?1,=C =D V?
(knows) ?0,C?
(who John likes)?0,+wh C,-wh?
(John likes,who)?1,=V +wh C?(?)
?0,V,-wh?
(John likes,who)?0,=D V,-wh?
(likes,who)?1,=D =D V?
(likes) ?1,D -wh?(who)?1,D?(John)?1,D?
(Mary)If we relabel this tree so that each instance of mergeis labeled Merge or ?, and each instance of move islabeled Move or ?, the result is the correspondingMG derivation tree, usually depicted like this:??
::=V C ?
?knows::=C =D V ???
:=V +wh C ?
?likes::=D =D V who::D -whJohn::DMary::DIn fact, the latter tree fully specifies the MCFGderivation above, because, in every MG derivation,for every internal node, the categories of the childrendetermine which rule applies.
This is easily verifiedby checking the 5 schemes for non-lexical rules onthe previous page; the left side of each rule is a func-tion of the right.
Consequently the MCFG categoriesat the internal nodes can be regarded as specifyingthe states of a deterministic finite state bottom-uptree recognizer for the MG derivation trees (Kobeleet al, 2007; Graf, 2011; Kobele, 2011).3.2 MCFGs need not be computedWe did not explicitly present the nonlexical MCFGrules used in the previous section ?3.1, since they aredetermined by the lexical rules.
The first rule usedat the root of the derivation tree is, for example, aninstance of the first rule scheme in ?3, namely:?
0, C ?
(s0t0) :- ?
1,=V C ?
(s0), ?
0, V ?
(t0).Generating these non-lexical MCFG rules from theMG lexicon is straightforward, and has been im-plemented in (freely available) software by Guillau-min (2004).
But the definition given in ?3 requiresthat all feature sequences in all rules be suffixesof lexical feature sequences, and notice that in anyderivation tree, like the one shown in ?3.1, for exam-ple, feature sequences increase along the left branchfrom any node to the leaf which is its ?head.?
Alongany such path, the feature sequences increase onefeature at a time until they reach the lexical leaf.
Soin effect, if we are building the derivation top-down,each step adds or ?unchecks?
features in lexical se-quences one at a time, and obviously the options fordoing this can be seen without compiling out all theMCFG nonlexical rules.4 The top-down recognizerFor any sequence s of elements of S, let |s|=thelength of s and nth(i, s) = a iff a ?
S, and forsome u, v ?
S?, s = uav and |u| = i. Adapt-ing basic ideas from earlier work (Mainguy, 2010;Villemonte de la Clergerie, 2002) for TD recogni-tion, we will instantiate variables not with strings butwith indices i ?
N?
to represent linear order of con-stituents, to obtain indexed atoms A(i1, .
.
.
, id(A)).Consider any nonlexical rule ?
:- ?
and any in-dexed atom ?
where?=A(t1, .
.
.
, td(A))?=A(i1, .
.
.
, id(A))?=B0(x01 , .
.
.
, x0d(B0)), .
.
.
, Bn(xn1 , .
.
.
, xnd(Bn)).For each variable xij in ?, defineindex?,?
(xij ) ={ik if tk = xijikp if |tk| > 1, xij = nth(p, tk).42Let index?,?(?)
be the result of replacing each vari-able xij in ?
by index?,?
(xij ).
Finally, let trim(?
)map ?
to itself except in the case when when everyindex in ?
begins with the same integer n, in whichcase that initial n is deleted from every index.Define a total order on the indices N?
as follows.For any ?, ?
?
N?,?
< ?
iff??????
= ?
6= ?, or?
= i?
?, ?
= j?
?, i < j, or?
= i?
?, ?
= i?
?, ??
< ?
?.For any atom ?, let ?(?)
be the least index in ?.
So,for example, ?
(AB(31, 240)) = 240.
And for anyindexed atoms ?, ?, let ?
< ?
iff ?(?)
< ?(?).
Weuse this order it to sort categories into left-to-rightorder in the ?expand?
rule below.We now define TD recognition in a deductive for-mat.
The state of the recognition sequence is givenby a (remaining input,priority queue) pair, where thequeue represents the memory of predicted elements,sorted according to < so that they can be processedfrom left to right.
We have 1 initial axiom, whichpredicts that input s will have start category S, whereS initially has index ?
:(s, S(?
))The main work is done by the expand rule, whichpops atom ?
off the queue, leaving sequence ?
un-derneath.
Then, for any rule ?
:- ?
with ?
of thesame category as ?, we compute index?,?(?
), ap-pend the result and ?, then sort and trim:(s, ??
)(s, sort(trim(index?,?(?)?)))?
:- ?
(We could use ordered insertion instead of sorting,and we could trim the indices much more aggres-sively, but we stick to simple formulations here.)
Fi-nally, we have a scan rule, which scans input a if wehave predicted an A and our grammar tells us thatA(a).
For all a ?
(?
?
?
), s ?
?
?, n ?
N?
:(as, A(n) ?)(s,?
)A(a)A string s is accepted if we can use these rules to getfrom the start axiom to (?,?).
This represents the factthat we have consumed the whole input and there areno outstanding predictions in memory.4.1 Example 1, continued.Here is the sequence of recognizer states that acceptsabbcdd, using the grammar presented in ?2.1:initial axiom:init.
(abbcdd, S(?
))expand with rule a:1.
(abbcdd, AC(0,2),BD(1,3))expand with rule c (note sort):2.
(abbcdd, A(0),BD(1,3),C(2))scan with rule f:3.
(bbcdd, BD(1,3),C(2))expand with rule d:4.
(bbcdd, B(10),BD(11,31),C(2),D(30))scan with rule g:5.
(bcdd, BD(11,31),C(2),D(30))expand with rule e:6.
(bcdd, B(11),C(2),D(30),D(31))scan with rule g:7.
(cdd, C(2),D(30),D(31))scan with rule h (note trim removes 3):8.
(dd, D(0),D(1))scan with rule i:9.
(d, D(1))scan with rule i:10.
(?, ?
)The number of recognizer steps is always exactlythe number of nodes in the corresponding derivationtree; compare this accepting sequence to the deriva-tion tree shown in ?2.1, for example.5 Properties and extensions5.1 Adding adjunction, head movementFrey and Ga?rtner (2002) propose that adjunction beadded to MGs by (i) allowing another kind of select-ing feature ?f , which selects but does not ?checkand delete?
the feature f of a phrase that it modi-fies, where (ii) the head of the result is the selected,?modified?
phrase that it combines with, and (iii)the selecting ?modifier?
cannot have any constituentsmoving out of it.
We can implement these ideas byadding a rule scheme like the following (comparethe first rule scheme in ?3):?
0, f?, ?1, .
.
.
, ?j ?
(t0s0, t1, .
.
.
, tj) :-?
y, f?, ?1, .
.
.
, ?j ?
(t0, .
.
.
, tj),?x,?f ?
(s0).Note this rule ?attaches?
the modifier on the right.We could also allow left modifiers, but in the exam-ples below will only use this one.43Some analyses of simple tensed sentences say thattense affixes ?hop?
onto the verb after the verb hascombined with its object.
Affix hopping and headmovement are more challenging that adjunction, butprevious approaches can be adapted to the presentperspective by making two changes: (i) we keep thehead separate from other material in its phrase untilthat phrase is merged with another phrase, so nowevery non-lexical category A has d(A) ?
3 and (ii)we add diacritics to the selection features to indi-cate whether hopping or head movement should ap-ply in the merge step.
To indicate that a head Aselects category f we give A the feature =f , butto indicate the the head of A should hop onto thehead of the selected constituent, we give A the fea-ture f=>.
Essentially this representation of MGswith head movement and affix hopping as MCFGs isimmediate from the formalization in Stabler (2001)and the automated translation by Guillaumin (2004).The examples in this paper below will use only affixhopping which is defined by the following modifiedversion of the first rule in ?3:?
0, ?, ?1, .
.
.
, ?j ?
(?, ?, tsthshtc, t1, .
.
.
, tj) :-?
1, f=>?
?
(sh),?x, f, ?1, .
.
.
, ?j ?
(ts, th, tc, t1, .
.
.
, tj)The first atom on the right side of this rule, the ?se-lector?, is a lexical head with string sh.
The sec-ond atom on the right of the rule has string com-ponents ts, th, tc (these are the specifier, head, andcomplement strings) together with j ?
0 moving el-ements t1, .
.
.
, tj .
In the result on the left, we seethat the lexical selector sh is ?hopped?
to the right ofthe selected head th, where it is sandwiched betweenthe other concatenated parts of the selected phrase,leaving ?
in the head position.
Since the usual startcategory C now has 3 components, like every otherhead, we begin with a special category S that servesonly to concatenate the 3 components of the matrixcomplementizer phrase, by providing the recognizerwith this additional initializing rule:S(ssshsc) :- ?x,C ?
(ss, sh, sc).The nature of adjunction is not quite clear, andthere is even less consensus about whether headmovement or affix hopping or both are needed ingrammars of human languages, but these illustratehow easily the MCFG approach to MGs can be ex-tended.
Like many of the other MG variants, theseextensions do not change the class of languages thatcan be defined (Stabler, 2011), and the recognizerdefined in ?4 can handle them without change.With head movement and adjunction we can,for example, provide a roughly traditional analy-sis of the famous example sentence from King andJust (1991) shown in Figure 1.
Note again thatthe derivation tree in that figure has lexical items atthe leaves, and these completely determine the non-lexical rules and the structure of the derivation.
Var-ious representations of the ?derived trees?, like theX-bar tree shown in this figure, are easily computedfrom the derivation tree (Kobele et al, 2007).
AndFigure 2 shows the recognizer steps accepting thatsentence.
Plotting queue size versus recognizer step,and simply overlaying the King and Just self-pacedreading times to see if they are roughly similar, wesee that, at least in sentences like these, readers gomore slowly when the queue gets large:012345670  5  10  15  20  25  30TD queue sizeKing and Just reading timesRecent work has challenged the claim that readingtimes are a function of the number of predictions inmemory, (e.g., Nakatani and Gibson, 2008, p.81) butpreliminary studies suggest that other performancemeasures may correlate (Bachrach, 2008; Brennanet al, 2010; VanWagenen et al, 2011).
Exploringthese possibilities is beyond the scope this paper.The present point is that any analysis expressiblein the MG formalism can be parsed transparentlywith this approach, assessing its memory demands;partially parallel beam search models for ambigu-ity, used in natural language engineering, can alsobe straightforwardly assessed.44??
::=T C ?
?-ed::V=> +ep T ?
?admit::=D =D V ?the::=N D error::N?the::=N D -ep ?reporter::N ?
?that::=T +wh ?N ?
?-ed::V=> +ep T ?
?attack::=D =D V ?
::D -wh?the::=N D -ep senator::NCPC?C TPDP(2)D?Dthe NPNPNreporterCPDP(1)?C?CthatTPDP(0)D?DtheNPN?NsenatorT?T VPDPt(0)V?VVattackT-edDPt(1)T?T VPDPt(2)V?VVadmitT-edDPD?DtheNPN?NerrorFigure 1: 28 node derivation tree and corresponding X-bar tree for King and Just (1991) example5.2 Infinite languages with finite memoryAlthough memory use is not the main concern ofthis paper, it is worth noting that, as in stack-basedCF models, memory demands do not necessarily in-crease without bound as sentence length increases.So for example, we can extend the naive grammarof Figure 2 to accept this is the man that kiss -edthe maid that milk -ed the cow that toss -ed the dogthat worry -ed the cat that chase -ed the rat, a sen-tence with 6 clauses, and use no more memory atany time than is needed for the 2 clause King andJust example.
Dynamic, chart-based parsing meth-ods usually require more memory without bound assentence length grows, even when there is little orno indeterminacy.5.3 ConnectednessMore directly relevant to incremental models is thefact that the portions of the derivation traversed atany point in TD recognition are all connected to eachother, their syntactic relations are established.
As wesee in all our examples, the TD recognizer is alwaystraversing the derivation tree on paths connected tothe root; while the indexing and sorting ensures thatthe leaves are scanned in the order of their appear-ance in the derived X-bar tree.
Left corner traver-sals do not have this property.
Consider a sentencelike the reporter poured the egg in the bowl over theflour.
In a syntax in the spirit of the one we see inFigure 1, for example, in the bowl could be right ad-joined to the direct object, and over the flour rightadjoined to VP.
Let VP1 be the parent of over theflour, and VP2 its sister.
With LC, VP1 will be pre-dicted right after the subject is completed.
But theverb is the left corner of VP2, and VP2 will not beattached to VP1 ?
and so the subject and verb willnot be connected ?
until VP1 is completed.
This de-lay in the LC attachment of the subject to the verbcan be extended by adding additional right modifiersto the direct object or the verb phrase, but the evi-dence suggests that listeners make such connectionsimmediately upon hearing the words, as the TD rec-ognizer does.6 Future workStandard methods for handling indeterminacy intop-down CF parsers, when there are multiple waysto expand a derivation top down, are easily adaptedto the MCFG and MG parsers proposed here.
Withbacktracking search, left recursion can cause non-45termination, but a probabilistic beam search can dobetter.
For ?
= (i,?)
any recognizer state, letstep(?)
be the (possibly empty) sequence of all thenext states that are licensed by the rules in ?3 (al-ways finitely many).
A probabilistic beam searchuses the rules,?
(s, S(?))?
init??prune(sortC(step(?)?
))search,popping a recognizer state ?
off the top of the queue?
?, appending step(?)
and ?, then sorting andpruning the result.
The sort in the search stepsis done according to the probability of each parserstate in context C , where the context may include ahistory of previous recognizer steps ?
i.e.
of eachderivation up to this point ?
but also possibly ex-trasentential information of any sort.
The pruningrule acts to remove highly improbable analyses, andsuccess is achieved if a step puts (?, ?)
on top ofthe queue.
Roark shows that this ability to condi-tion on material not in parser memory ?
indeed onanything in the left context ?
can allow better esti-mates of parse probability.
On small experimentalgrammars, we are finding that TD beam search per-formance can be better than our chart parsers usingthe same grammar.
Further feasibility studies are in1 init.
(trttsa-a-te, S(?
))1 init.
(trttsa-a-te, ?0,C?
(0,1,2))2 1.
(trttsa-a-te, ?1,=TC?(1),?0,T?
(20,21,22))1 2.
(trttsa-a-te, ?0,T?
(0,1,2))1 3.
(trttsa-a-te, ?0,+epT,-ep?
(01,1,2,00))2 4.
(trttsa-a-te, ?0,V,-ep?(20,21,23,00),?1,V=>+epT?
(22))3 5.
(trttsa-a-te, ?0,D-ep?(000,001,002),?0,=DV?(20,21,23),?1,V=>+epT?
(22))4 6.
(trttsa-a-te, ?1,=ND-ep?(001),?0,N?(0020,0021,0022),?0,=DV?(20,21,23),?1,V=>+epT?
(22))3 7.
(rttsa-a-te, ?0,N?(0020,0021,0022),?0,=DV?(20,21,23),?1,V=>+epT?
(22))4 8.
(rttsa-a-te, ?1,N?(0021),?0,?N?(00220,00221,00222),?0,=DV?(20,21,23),?1,V=>+epT?
(22))3 9.
(ttsa-a-te, ?0,?N?(00220,00221,00222),?0,=DV?(20,21,23),?1,V=>+epT?
(22))3 10.
(ttsa-a-te, ?0,+wh?N ,-wh?(002201,00221,00222,002200),?0,=DV?(20,21,23),?1,V=>+epT?
(22))4 11.
(ttsa-a-te, ?0,T,-wh?(002220,002221,002222,002200),?1,=T+wh?N?(00221),?0,=DV?(20,21,23),?1,V=>+epT?
(22))4 12.
(ttsa-a-te, ?0,+epT,-ep,-wh?(0022201,002221,002222,0022200,002200),?1,=T+wh?N?(00221),?0,=DV?(20,21,23),?1,V=>+epT?
(22))5 13.
(ttsa-a-te, ?0,V,-ep,-wh?(0022220,0022221,0022223,0022200,002200),?1,=T+wh?N?(00221),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?
(22))6 14.
(ttsa-a-te, ?0,=DV,-wh?(0022220,0022221,0022223,002200),?1,=T+wh?N?(00221),?0,D-ep?(00222000,00222001,00222002),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?
(22))7 15.
(ttsa-a-te, ?1,D-wh?(002200),?1,=T+wh?N?(00221),?0,D-ep?(00222000,00222001,00222002),?1,=D=DV?(0022221),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?
(22))6 16.
(ttsa-a-te, ?1,=T+wh?N?(00221),?0,D-ep?(00222000,00222001,00222002),?1,=D=DV?(0022221),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?
(22))5 17.
(tsa-a-te, ?0,D-ep?(00222000,00222001,00222002),?1,=D=DV?(0022221),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?
(22))6 18.
(tsa-a-te, ?1,=ND-ep?(00222001),?1,N?(00222002),?1,=D=DV?(0022221),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?
(22))5 19.
(sa-a-te, ?1,N?(00222002),?1,=D=DV?(0022221),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?
(22))4 20.
(a-a-te, ?1,=D=DV?(0022221),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?
(22))3 21.
(-a-te, ?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?
(22))2 22.
(a-te, ?0,=DV?(20,21,23),?1,V=>+epT?
(22))3 23.
(a-te, ?1,=D=DV?(1),?1,V=>+epT?(2),?0,D?
(30,31,32))2 24.
(-te, ?1,V=>+epT?(2),?0,D?
(30,31,32))1 25.
(te, ?0,D?
(30,31,32))2 26.
(te, ?1,=ND?(1),?1,N?
(2))1 27.
(e, ?1,N?
(2))0 28.
(?, ?
)Figure 2: 28 step TD recognition of derivation in Figure 1, abbreviating input words by their initial characters.
Theleft column indicates queue size, plotted in ?5.1.46progress.The recognizer presented here simplifies Main-guy?s (2010) top-down MG recognizer by generaliz-ing it handle an MCFG normal form, so that a widerange of MG extensions are immediately accommo-dated.
This is made easy when we adopt Kanazawa?sHorn clause formulation of MCFGs where the orderof variables on the left side of the rules so visibly in-dicates the surface order of string components.
Withthe Horn clause notation, the indexing can be string-based and general rather than tree-based and tied toparticular assumptions about how the MGs work.Transparently generalizing the operations of CF TDrecognizers, the indexing and operations here arealso slightly simpler than ?thread automata?
(Ville-monte de la Clergerie, 2002).
Compare also theindexing, sometimes more or less similar, in chart-based recognizers of MCF and closely related sys-tems (Burden and Ljunglo?f, 2005; Harkema, 2001c;Boullier, 1998; Kallmeyer, 2010).Mainguy shows that when the probability of aderivation is the product of the rule probabilities, asusual, and when those rule probabilities are given bya consistent probability assignment, a beam searchwithout pruning will always find a derivation if thereis one.
When there is no derivation, though, anunpruned search can fail to terminate; a pruningrule can guarantee termination in such cases.
Thoseresults extend to the MCFG recognizers proposedhere.
Various applications have found it better touse a beam search with top-down recognition of left-or right-corner transforms of CF grammars (Roark,2001; Roark, 2004; Schuler, 2010; Wu et al, 2010);those transforms can (but need not always) disruptgrammatical connectedness as noted in ?5.3.
Workin progress explores the possibilities for such strate-gies in incremental MCFG parsing.
It would alsobe interesting to generalize Hale?s (2011) ?rationalparser?
to these grammars.AcknowledgmentsThanks to Thomas Mainguy, Sarah VanWagenenand ?Eric Villemonte de la Clergerie for helpful dis-cussions of this material.ReferencesGerry T. M. Altmann and Jelena Mirkovic?.
2009.
Incre-mentality and prediction in human sentence process-ing.
Cognitive Science, 33:583?809.Asaf Bachrach.
2008.
Imaging Neural Correlates of Syn-tactic Complexity in a Naturalistic Context.
Ph.D. the-sis, Massachusetts Institute of Technology.Pierre Boullier.
1998.
Proposal for a natural lan-guage processing syntactic backbone.
Technical Re-port 3242, Projet Atoll, INRIA, Rocquencourt.Jonathan Brennan, Yuval Nir, Uri Hasson, RafaelMalach, David J. Heeger, and Liinay Pylkka?nen.2010.
Syntactic structure building in the anterior tem-poral lobe during natural story listening.
Forthcoming.Ha?kan Burden and Peter Ljunglo?f.
2005.
Parsing linearcontext-free rewriting systems.
In Ninth InternationalWorkshop on Parsing Technologies, IWPT?05.Craig G. Chambers, Michael K. Tanenhaus, Kathleen M.Eberhard, Hana Filip, and Greg N. Carlson.
2004.Actions and affordances in syntactic ambiguity resolu-tion.
Journal of Experimental Psychology: Learning,Memory and Cognition, 30(3):687?696.Cristiano Chesi.
2007.
An introduction to phase-basedminimalist grammars: Why move is top-down fromleft-to-right.
Technical report, Centro Interdepartmen-tale di Studi Cognitivi sul Linguaggio.Vera Demberg and Frank Keller.
2009.
A computationalmodel of prediction in human parsing: Unifying local-ity and surprisal effects.
In Proceedings of the 29thmeeting of the Cognitive Science Society (CogSci-09),Amsterdam.Werner Frey and Hans-Martin Ga?rtner.
2002.
On thetreatment of scrambling and adjunction in minimal-ist grammars.
In Proceedings, Formal Grammar?02,Trento.Thomas Graf.
2011.
Closure properties of minimalistderivation tree languages.
In Logical Aspects of Com-putational Linguistics, LACL?11, Forthcoming.Matthieu Guillaumin.
2004.
Conversions be-tween mildly sensitive grammars.
UCLA and?Ecole Normale Supe?rieure.
http://www.linguistics.ucla.edu/people/stabler/epssw.htm.Nicholas J. Haddock.
1989.
Computational models ofincremental semantic interpretation.
Language andCognitive Processes, 4((3/4)):337?368.John T. Hale.
2011.
What a rational parser would do.Cognitive Science, 35(3):399?443.Henk Harkema.
2001a.
A characterization of minimalistlanguages.
In Proceedings, Logical Aspects of Com-putational Linguistics, LACL?01, Port-aux-Rocs, LeCroisic, France.Henk Harkema.
2001b.
A characterization of minimalistlanguages.
In Philippe de Groote, Glyn Morrill, and47Christian Retore?, editors, Logical Aspects of Compu-tational Linguistics, Lecture Notes in Artificial Intelli-gence, No.
2099, pages 193?211, NY.
Springer.Henk Harkema.
2001c.
Parsing Minimalist Languages.Ph.D.
thesis, University of California, Los Angeles.Aravind Joshi.
1985.
How much context-sensitivity isnecessary for characterizing structural descriptions.
InD.
Dowty, L. Karttunen, and A. Zwicky, editors, Natu-ral Language Processing: Theoretical, Computationaland Psychological Perspectives, pages 206?250.
Cam-bridge University Press, NY.Laura Kallmeyer.
2010.
Parsing beyond context-freegrammars.
Springer, NY.Makoto Kanazawa.
2009.
A pumping lemma for well-nested multiple context free grammars.
In 13th In-ternational Conference on Developments in LanguageTheory, DLT 2009.Yoshihide Kato and Shigeki Matsubara.
2009.
In-cremental parsing with adjoining operation.
IE-ICE Transactions on Information and Systems,E92.D(12):2306?2312.Jonathan King and Marcel Adam Just.
1991.
Individualdifferences in syntactic processing: the role of workingmemory.
Journal of Memory and Language, 30:580?602.Gregory M. Kobele, Christian Retore?, and Sylvain Sal-vati.
2007.
An automata-theoretic approach to min-imalism.
In J. Rogers and S. Kepser, editors, ModelTheoretic Syntax at 10, ESSLLI?07.GregoryM.
Kobele.
2011.
Minimalist tree languages areclosed under intersection with recognizable tree lan-guages.
In Logical Aspects of Computational Linguis-tics, LACL?11, Forthcoming.Thomas Mainguy.
2010.
A probabilistictop-down parser for minimalist grammars.http://arxiv.org/abs/1010.1826v1.Jens Michaelis.
1998.
Derivational minimalism is mildlycontext-sensitive.
In Proceedings, Logical Aspects ofComputational Linguistics, LACL?98, pages 179?198,NY.
Springer.Jens Michaelis.
2001a.
On Formal Properties of Mini-malist Grammars.
Ph.D. thesis, Universita?t Potsdam.Linguistics in Potsdam 13, Universita?tsbibliothek,Potsdam, Germany.Jens Michaelis.
2001b.
Transforming linear contextfree rewriting systems into minimalist grammars.
InP.
de Groote, G. Morrill, and C.
Retore?, editors, Logi-cal Aspects of Computational Linguistics, LNCS 2099,pages 228?244, NY.
Springer.Kentaro Nakatani and Edward Gibson.
2008.
Distin-guishing theories of syntactic expectation cost in sen-tence comprehension: Evidence from Japanese.
Lin-guistics, 46(1):63?86.Brian Roark and Mark Johnson.
1999.
Efficient proba-bilistic top-down and left-corner parsing.
In Proceed-ings of the 37th Annual Meeting of the Association forComputational Linguistics, pages 421?428.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.Brian Roark.
2004.
Robust garden path parsing.
NaturalLanguage Engineering, 10(1):1?24.William Schuler.
2010.
Incremental parsing in boundedmemory.
In Proceedings of the 10th InternationalWorkshop on Tree Adjoining Grammars and RelatedFrameworks, TAG+10.Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, andTadao Kasami.
1991.
On multiple context-free gram-mars.
Theoretical Computer Science, 88:191?229.Libin Shen and Aravind Joshi.
2005.
Incremental LTAGparsing.
In Proceedings, Human Language Technol-ogy Conference and Conference on Empirical Meth-ods in Human Language Processing.Edward P. Stabler.
2001.
Recognizing head movement.In Philippe de Groote, Glyn Morrill, and Christian Re-tore?, editors, Logical Aspects of Computational Lin-guistics, Lecture Notes in Artificial Intelligence, No.2099, pages 254?260.
Springer, NY.Edward P. Stabler.
2011.
Computational perspectives onminimalism.
In Cedric Boeckx, editor, Oxford Hand-book of Linguistic Minimalism, pages 617?641.
Ox-ford University Press, Oxford.Sarah VanWagenen, Jonathan Brennan, and Edward P.Stabler.
2011.
Evaluating parsing strategies in sen-tence processing.
In Proceedings of the CUNY Sen-tence Processing Conference.K.
Vijay-Shanker and David Weir.
1994.
The equiva-lence of four extensions of context free grammar for-malisms.
Mathematical Systems Theory, 27:511?545.
?Eric Villemonte de la Clergerie.
2002.
Parsing MCS lan-guages with thread automata.
In Proceedings of the6th International Workshop on Tree Adjoining Gram-mars and Related Frameworks, TAG+6.David Weir.
1988.
Characterizing Mildly Context-Sensitive Grammar Formalisms.
Ph.D. thesis, Univer-sity of Pennsylvania, Philadelphia.Stephen Wu, Asaf Bachrach, Carlos Cardenas, andWilliam Schuler.
2010.
Complexity metrics in an in-cremental right-corner parser.
In Proceedings of the48th Annual Meeting of the Association for ComputerLinguistics, pages 1189?1198.48
