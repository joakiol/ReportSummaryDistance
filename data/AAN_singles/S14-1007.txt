Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 57?62,Dublin, Ireland, August 23-24 2014.Identifying semantic relations in a specialized corpus throughdistributional analysis of a cooccurrence tensorGabriel Bernier-ColborneOLST (Universit?e de Montr?eal)C.P.
6128, succ.
Centre-VilleMontr?eal (Qu?ebec) Canada H3C 3J7gabriel.bernier-colborne@umontreal.caAbstractWe describe a method of encoding cooc-currence information in a three-way tensorfrom which HAL-style word space mod-els can be derived.
We use these models toidentify semantic relations in a specializedcorpus.
Results suggest that the tensor-based methods we propose are more ro-bust than the basic HAL model in somerespects.1 IntroductionWord space models such as LSA (Landauer andDumais, 1997) and HAL (Lund et al., 1995) havebeen shown to identify semantic relations fromcorpus data quite effectively.
However, the per-formance of such models depends on the parame-ters used to construct the word space.
In the caseof HAL, parameters such as the size of the con-text window can have a significant impact on theability of the model to identify semantic relationsand on the types of relations (e.g.
paradigmatic orsyntagmatic) captured.In this paper, we describe a method of encodingcooccurrence information which employs a three-way tensor instead of a matrix.
Because the ten-sor explicitly encodes the distance between a tar-get word and the context words that co-occur withit, it allows us to extract matrices corresponding toHAL models with different context windows with-out repeatedly processing the whole corpus, but italso allows us to experiment with different kindsof word spaces.
We describe one method wherebyfeatures are selected in different slices of the ten-sor corresponding to different distances betweenthe target and context words, and another whichuses SVD for dimensionality reduction.
ModelsThis work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/are evaluated and compared on reference data ex-tracted from a specialized dictionary of the envi-ronment domain, as our target application is theidentification of lexico-semantic relations in spe-cialized corpora.
Preliminary results suggest thetensor-based methods are more robust than the ba-sic HAL model in some respects.2 Related WorkThe tensor encoding method we describe is basedon the Hyperspace Analogue to Language, orHAL, model (Lund et al., 1995; Lund andBurgess, 1996), which has been shown to be par-ticularly effective at modeling paradigmatic rela-tions such as synonymy.
In the HAL model, wordorder is taken into account insofar as the word vec-tors it produces contain information about both thecooccurrents that precede a word and those thatfollow it.
In recent years, there have been severalproposals that aim to add word order informationto models that rely mainly on word context infor-mation (Jones and Mewhort, 2007; Sahlgren et al.,2008), including models based on multi-way ten-sors.
Symonds et al.
(2011) proposed an efficienttensor encoding method which builds on unstruc-tured word space models (i.e.
models based onsimple cooccurrence rather than syntactic struc-ture) by adding order information.
The method wedescribe differs in that it explicitly encodes the dis-tance between a target word and its cooccurrents.Multi-way tensors have been used to constructdifferent kinds of word space models in recentyears.
Turney (2007) used a word-word-patterntensor to model semantic similarity, Van de Cruys(2009) used a tensor containing corpus-derivedsubject-verb-object triples to model selectionalpreferences, and Baroni and Lenci (2010) pro-posed a general, tensor-based framework for struc-tured word space models.
The tensor encodingmethod we describe differs in that it is based onan unstructured word space model, HAL.573 HALThe HAL model employs a sliding context win-dow to compute a word-word cooccurrence ma-trix, which we will note A, in which value aijisbased on the number of times context word wjap-pears within the context window of target word wi.Thus, words that share cooccurrents will be closerin word space.
If equal weight is given to all con-text words in the window, regardless of distance,we call the context window rectangular.
In theoriginal HAL model, the values added to A areinversely proportional to the distance between thetarget word and context word in a given context.In this case, the context window is triangular.In the HAL model, the cooccurrence matrix iscomputed by considering only the context wordsthat occur before the target word.
Once the ma-trix has been computed, row vector ai:containscooccurrence information about words precedingwi, and column vector a:icontains informationabout those that follow it.
The row vector andcolumn vector of each target word are concate-nated, such that the resulting word vectors con-tain information about both left-cooccurrents andright-cooccurrents.
We call this type of contextwindow directional, following (Sahlgren, 2006),as opposed to a symetric context window, in whichcooccurrence counts in the left and right contextsare summed.
In our experiment, we only use onetype of context window (directional and rectangu-lar), but models corresponding to different typesof context windows can be derived from the cooc-currence tensor we describe in section 4.Once the values in A have been computed, theycan be weighted using schemes such as TF-ITF(Lavelli et al., 2004) and Positive Pointwise Mu-tual Information (PPMI), which we use here asit has been shown to be particularly effective byBullinaria and Levy (2007).
Finally, a distance orsimilarity measure is used to compare word vec-tors.
Lund and Burgess (1996) use Minkowskidistances.
We will use the cosine similarity, asdid Sch?utze (1992) in a model similar to HAL andwhich directly influenced its development.4 The Cooccurrence TensorIn the following description of the cooccurrencetensor, we follow the notational guidelines of(Kolda, 2006), as in (Turney, 2007; Baroni andLenci, 2010).
Let W be the vocabulary1, whichwe index by i to refer to a target word and by j forcontext words.
Furthermore, let P , indexed by k,be a set of positions, relative to a target word wi,in which a context word wjcan co-occur with wi.In other words, this is the signed distance betweenwjand wi, in number of words.
For instance, inthe sentence ?a dog bit the mailman?, we wouldsay that ?dog?
co-occurs with ?bit?
in position?1.If we only consider the words directly adjacent toa target word, then P = {?1,+1}.
If the tensorencoding method is used to generate HAL-stylecooccurrence matrices corresponding to differentcontext windows, then P would include all posi-tions in the largest window under consideration.In a cooccurrence matrix A, aijcontains thefrequency at which word wjco-occurs with wordwiin a fixed context window.
Rather than comput-ing matrices using fixed-size context windows, wecan construct a cooccurrence tensor X, a labeledthree-way tensor in which values xijkindicate thefrequency at which word wjco-occurs with wordwiin position pk.
Table 1 illustrates a cooccur-rence tensor for the sentence ?dogs bite mailmen?using a context window of 1 (P = {?1,+1}), inthe form of a nested table.In tensor X, xi:kdenotes the row vector of wiat position pk, x:jkdenotes the column vector ofword wjat position pkand xij:denotes the tubevector indicating the frequency at which wjco-occurs with wiin each of the positions in P .HAL-style cooccurrence matrices correspond-ing to different context windows can be extractedfrom the tensor by summing and concatenatingvarious slices of the tensor.
A frontal slice X::krepresents a I ?
J cooccurrence matrix for po-sition pk.
A cooccurrence matrix correspondingto a symetric context window of size n can beextracted by summing the slices X::kfor pk?
{?n,?n + 1, .
.
.
, n}.
For a directional window,we first sum the slices for pk?
{?n, .
.
.
,?1},then sum the slices for pk?
{1, .
.
.
, n}, then con-catenate the 2 resulting matrices horizontally.Thus, summing and concatenating slices allowsus to extract HAL-style cooccurrence matrices.
Adifferent kind of model can also be obtained byconcatenating slices of the tensor.
For instance, ifwe concatenate X::kfor pk?
{?2,?1,+1,+2}horizontally, we obtain a matrix containing a vec-1We assume that the target and context words are the sameset, but this need not be the case.58j=1:dog j=2:bite j=3:mailmank=1:?1 k=2:+1 k=1:?1 k=2:+1 k=1:?1 k=2:+1i=1:dog 0 0 0 1 0 0i=2:bite 1 0 0 0 0 1i=3:mailman 0 0 1 0 0 0Table 1: A 3?
3?
2 cooccurrence tensor.tor of length 4J (instead of the 2J-length vectorsof the HAL model) for each target word, whichencodes cooccurrence information about 4 specificpositions relative to that word.
We will refer to thismethod as the tensor slicing method.
Note that ifP = {?1, 1} the resulting matrix is identical to aHAL model with context size 1As the size of the resulting vectors is KJ , thismethod can result in very high-dimensional wordvectors.
In the original HAL model, Lund etal.
(1995) reduced the dimensionality of the vec-tors through feature selection, by keeping only thefeatures that have the highest variance.
Sch?utze(1992), on the other hand, used truncated SVD forthis purpose.
Both techniques can be used with thetensor slicing method.
In our experiment, SVDwas applied to the matrices obtained by concate-nating tensor slices horizontally2.
As for featureselection, a fixed number of features (those withthe highest variance) were selected from each sliceof the tensor, and these reduced slices were thenconcatenated.It must be acknowledged that this tensor encod-ing method is not efficient in terms of memory.However, this was not a major issue in our exper-imental setting, as the size of the vocabulary wassmall (5K words), and we limited the number ofpositions in P to 10.
Also, a sparse tensor wasused to reduce memory consumption.5 Experiment5.1 Corpus and PreprocessingIn this experiment, we used the PANACEA En-vironment English monolingual corpus, which is2We also tried concatenating slices vertically (thus ob-taining a matrix where rows correspond to <target word,position> tuples and columns correspond to context words)before applying SVD, then concatenating all row vectors cor-responding to the same target word, but we will not reportthe results here for lack of space.
Concatenating slices hor-izontally performed better and seems more intuitive, and thesize of the resulting vectors is not dependent on the numberof positions in P .freely distributed by ELDA for research purposes3(Catalog Reference ELRA-W0063).
This corpuscontains 28071 documents (?50 million tokens)dealing with different aspects of the environmentdomain, harvested from web sites using a focusedcrawler.
The corpus was converted from XML toraw text, various string normalization operationswere then applied, and the corpus was lemmatizedusing TreeTagger (Schmid, 1994).
The vocabu-lary (W ) was selected based on word frequency:we used the 5000 most frequent words in the cor-pus, excluding stop words and strings containingnon-alphabetic characters.
During computation ofthe cooccurrence tensor, OOV words were ignored(rather than deleted), and the context window wasallowed to span sentence boundaries.5.2 Evaluation DataModels were evaluated using reference data ex-tracted from DiCoEnviro4, a specialized dictio-nary of the environment.
This dictionary de-scribes the meaning and behaviour of terms ofthe environment domain as well as the lexico-semantic relations between these terms.
Of thevarious relations encoded in the dictionary, wefocused on a subset of three paradigmatic rela-tions: near-synonyms (terms that have similarmeanings), antonyms (opposite meanings), andhyponyms (kinds of).
446 pairs containing a head-word and a related term were extracted from thedictionary.
We then filtered out the pairs that con-tained at least one OOV term, and were left with374 pairs containing two paradigmatically-related,single-word terms.
About two thirds (246) of theseexamples were used for parameter selection, andthe rest were set aside for a final comparison ofthe highest-scoring models.3http://catalog.elra.info/product_info.php?products_id=11844http://olst.ling.umontreal.ca/cgi-bin/dicoenviro/search_enviro.cgi(under construction).595.3 Automatic EvaluationEach model was automatically evaluated on thereference data as follows.
For each <headword,related term> pair in the training set, we computedthe cosine similarity between the headword andall other words in the vocabulary, then observedthe rank of the related term in the sorted list ofneighbours.
The score used to compare modelsis recall at k (R@k), which is the percentage ofcases where the related term is among the k near-est neighbours of the headword.
It should be notedthat a score of 100% is not always possible in thissetting (depending on the value of k), as someheadwords have more than 1 related term in thereference data.
Nonetheless, since most (?70%)have 1 or 2 related terms, R@k for some smallvalue of k (we use k = 10) should be a good indica-tor of accuracy.
A measure that explicitly accountsfor the fact that different terms have different num-bers of related terms (e.g.
R-precision) would be agood alternative.5.4 Models TestedWe compared HAL and the tensor slicingmethod using either feature selection or SVD5,as explained in section 4.
We will refer toeach of these models as HALSEL, TNSRSEL,HALSVDand TNSRSVD.
Context sizes rangedfrom 1 to 5 words.
For feature selection,the number of features could take values in{1000, 2000, .
.
.
, 10000}, 10000 being the max-imum number of features in a HAL model us-ing a vocabulary of 5000 words.
In the caseof TNSRSEL, to determine the number of fea-tures selected per slice, we took each value in{1000, 2000, .
.
.
, 10000}, divided it by K (thenumber of positions in P ), and rounded down.This way, once the slices are concatenated, thetotal number of features is equal to (or slightlyless than) that of one of the HALSELmod-els, allowing for a straightforward comparison.When SVD was used instead of feature selection,the number of components could take values in{100, 200, ..., 1000}.
In all cases, word vectorswere weighted using PPMI and normalized6.5We used the SVD implementation (ARPACK solver)provided in the scikit-learn toolkit (Pedregosa et al., 2011).6For HALSELand TNSRSEL, we apply PPMI weightingafter feature selection.
In the case of TNSRSEL, we wantedto avoid weighting each slice of the tensor separately.
Wedecided to apply weighting after feature selection in the caseof HALSELas well in order to enable a more straightforwardcomparison.
We should also note that, in our experimentsabsorb extreme precipitationemit severe rainfallsequester intense snowfallconvert harsh temperatureproduce catastrophic rainaccumulate unusual evaporationstore seasonal runoffradiate mild moistureconsume cold snowremove dramatic weatherreflect increase depositionTable 2: 10 nearest neighbours of 3 environmentalterms using the HALSELmodel.6 ResultsTable 2 illustrates the kinds of relations identifiedby the basic HALSELmodel.
It shows the 10 near-est neighbours of the verb absorb, the adjectiveextreme and the noun precipitation.
If we com-pare these results with the paradigmatic relationsencoded in DiCoEnviro, we see that, in the caseof absorb, 3 of its neighbours are encoded in thedictionary, and all 3 are antonyms or terms havingopposite meanings: emit, radiate, and reflect.
Asfor extreme, the top 2 neighbours are both encodedin the dictionary as near-synonyms.
Finally, rainand snow are both encoded as kinds of precipita-tion.
Most of the other neighbours shown here arealso paradigmatically related to the query terms.Thus, HAL seems quite capable of identifying thethree types of paradigmatic relations we hoped toidentify.Table 3 shows the best R@10 achieved by eachmodel on the training set, which was used to tunethe context size and number of features or compo-nents, and their scores on the test set, which wasonly used to compare the best models.
In the caseof HALSEL, the best model has a context windowsize of 1 and uses 9K out of 10K available features.As for TNSRSEL, the best model had a context sizeof 2 (P = {?2,?1,+1,+2}) and 10000 features(2500 per slice).
It performed only slightly betteron the training set, however it beat the HAL modelwith a wider margin on the test set.using HAL, PPMI weighting performed better when appliedafter feature selection, especially for low numbers of features.60200 400 600 800 1000Nb components3035404550556065R@10(%)(a)200 400 600 800 1000Nb components3035404550556065(b)context size12345Figure 1: HAL vs. tensor slicing method using SVD for dimensionality reduction.
R@10 is plottedagainst number of components.
Models are identical when context size is 1.
(a) HALSVD(b) TNSRSVDModel Train TestHALSEL60.57 57.03TNSRSEL60.98 60.94HALSVD59.76 56.25TNSRSVD60.57 60.16Table 3: R@10 (%) of best models.The best HALSVDmodel used a 1-word windowand 1000 components, whereas the best TNSRSVDmodel had a context size of 2 and 800 components.Again, the tensor-based model slightly edged outthe HAL model on the training set, but performedconsiderably better on the test set.Further analysis of the results indeed suggeststhat the tensor slicing method is more robust insome respects than the basic HAL model.
Fig-ure 1 compares the performance of HALSVDandTNSRSVDon the training set, taking into accountcontext size and number of components.
It showsthat the HAL model is quite sensitive to contextsize, narrower context performing better in thistask.
The tensor-based method reduces this gap inperformance between context sizes, the gain beinggreater for larger context sizes.
Furthermore, us-ing the tensor-based method with a slightly widercontext (2) raises R@10 for most values of thenumber of components.
Results obtained withHALSELand TNSRSELfollow the same trend, thetensor-based method being more robust with re-spect to context size.
For lack of space, we onlyshow the plot comparing HALSVDand TNSRSVD.7 Concluding RemarksThe work presented in this paper is still in its ex-ploratory phase.
The tensor slicing method wedescribed has only been evaluated on one corpusand one set of reference data.
Experiments wouldneed to be carried out on common word spaceevaluation tasks in order to compare its perfor-mance to that of HAL and other word space mod-els.
However, our results suggest that the tensor-based methods are more robust than the basic HALmodel to a certain extent, and can improve accu-racy.
This could prove especially useful in settingswhere no reference data are available for parame-ter tuning.Various possibilities offered by the cooccur-rence tensor remain to be explored, such asweighting the number of features selected perslice using some function of the distance betweenwords, extracting matrices from the tensor by ap-plying various functions to the tube vectors corre-sponding to each word pair, and applying weight-ing functions that have been generalized to higher-order tensors (Van de Cruys, 2011) or tensor de-composition methods such as those described in(Turney, 2007).AcknowledgementsWe would like to thank the anonymous reviewersfor their helpful and thorough comments.
Fundingwas provided by the Social Sciences and Humani-ties Research Council of Canada.61ReferencesMarco Baroni and Alessandro Lenci.
2010.
Dis-tributional memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):673?721.John A Bullinaria and Joseph P Levy.
2007.
Extractingsemantic representations from word co-occurrencestatistics: A computational study.
Behavior Re-search Methods, 39(3):510?526.Michael N Jones and Douglas JK Mewhort.
2007.Representing word meaning and order informationin a composite holographic lexicon.
PsychologicalReview, 114(1):1?37.Tamara Gibson Kolda.
2006.
Multilinear operatorsfor higher-order decompositions.
Technical ReportSAND2006-2081, Sandia National Laboratories.Thomas K Landauer and Susan T Dumais.
1997.
Asolution to Plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological Review,104(2):211.Alberto Lavelli, Fabrizio Sebastiani, and RobertoZanoli.
2004.
Distributional term representations:An experimental comparison.
In Proceedings ofthe 13th ACM International Conference on Informa-tion and Knowledge Management, pages 615?624.ACM.Kevin Lund and Curt Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments, & Computers, 28(2):203?208.Kevin Lund, Curt Burgess, and Ruth Ann Atchley.1995.
Semantic and associative priming in high-dimensional semantic space.
In Proceedings of the17th Annual Conference of the Cognitive ScienceSociety, volume 17, pages 660?665.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Magnus Sahlgren, Anders Holst, and Pentti Kanerva.2008.
Permutations as a means to encode orderin word space.
In Proceedings of the 30th AnnualConference of the Cognitive Science Society, pages1300?1305.
Cognitive Science Society.Magnus Sahlgren.
2006.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, StockholmUniversity.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing.Hinrich Sch?utze.
1992.
Dimensions of meaning.
InProceedings of the 1992 ACM/IEEE Conference onSupercomputing (Supercomputing?92), pages 787?796.
IEEE Computer Society Press.Michael Symonds, Peter D Bruza, Laurianne Sitbon,and Ian Turner.
2011.
Modelling word meaning us-ing efficient tensor representations.
In Proceedingsof 25th Pacific Asia Conference on Language, Infor-mation and Computation.Peter Turney.
2007.
Empirical evaluation of fourtensor decomposition algorithms.
Technical ReportERB-1152, National Research Council of Canada,Ottawa.Tim Van de Cruys.
2009.
A non-negative tensor fac-torization model for selectional preference induc-tion.
In Proceedings of the Workshop on Geomet-rical Models of Natural Language Semantics, pages83?90.
ACL.Tim Van de Cruys.
2011.
Two multivariate general-izations of pointwise mutual information.
In Pro-ceedings of the Workshop on Distributional Seman-tics and Compositionality, pages 16?20.
ACL.62
