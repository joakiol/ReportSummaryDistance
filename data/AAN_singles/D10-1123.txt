Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1266?1276,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsIdentifying Functional Relations in Web TextThomas Lin, Mausam, Oren EtzioniTuring CenterUniversity of WashingtonSeattle, WA 98195, USA{tlin,mausam,etzioni}@cs.washington.eduAbstractDetermining whether a textual phrase denotesa functional relation (i.e., a relation that mapseach domain element to a unique range el-ement) is useful for numerous NLP taskssuch as synonym resolution and contradic-tion detection.
Previous work on this prob-lem has relied on either counting methods orlexico-syntactic patterns.
However, determin-ing whether a relation is functional, by ana-lyzing mentions of the relation in a corpus,is challenging due to ambiguity, synonymy,anaphora, and other linguistic phenomena.We present the LEIBNIZ system that over-comes these challenges by exploiting the syn-ergy between the Web corpus and freely-available knowledge resources such as Free-base.
It first computes multiple typed function-ality scores, representing functionality of therelation phrase when its arguments are con-strained to specific types.
It then aggregatesthese scores to predict the global functionalityfor the phrase.
LEIBNIZ outperforms previ-ous work, increasing area under the precision-recall curve from 0.61 to 0.88.
We utilizeLEIBNIZ to generate the first public reposi-tory of automatically-identified functional re-lations.1 IntroductionThe paradigm of Open Information Extraction (IE)(Banko et al, 2007; Banko and Etzioni, 2008) hasscaled extraction technology to the massive set ofrelations expressed in Web text.
However, additionalwork is needed to better understand these relations,and to place them in richer semantic structures.
Astep in that direction is identifying the properties ofthese relations, e.g., symmetry, transitivity and ourfocus in this paper ?
functionality.
We refer to thisproblem as functionality identification.A binary relation is functional if, for a given arg1,there is exactly one unique value for arg2.
Exam-ples of functional relations are father, death date,birth city, etc.
We define a relation phrase to befunctional if all semantic relations commonly ex-pressed by that phrase are functional.
For exam-ple, we say that the phrase ?was born in?
denotesa functional relation, because the different seman-tic relations expressed by the phrase (e.g., birth city,birth year, etc.)
are all functional.Knowing that a relation is functional is helpfulfor numerous NLP inference tasks.
Previous workhas used functionality for the tasks of contradictiondetection (Ritter et al, 2008), quantifier scope dis-ambiguation (Srinivasan and Yates, 2009), and syn-onym resolution (Yates and Etzioni, 2009).
It couldalso aid in other tasks such as ontology generationand information extraction.
For example, considertwo sentences from a contradiction detection task:(1) ?George Washington was born in Virginia.?
and(2) ?George Washington was born in Texas.
?As Ritter et al (2008) points out, we can only de-termine that the two sentences are contradictory ifwe know that the semantic relation referred to bythe phrase ?was born in?
is functional, and that bothVirginia and Texas are distinct states.Automatic functionality identification is essentialwhen dealing with a large number of relations as inOpen IE, or in complex domains where expert help1266DistributionalDifferenceAssertionsWebCorpus IEFunctionality prediction for Web relationsCombination PolicyFreebaseCleanListstypelistsinstances per relationfunctionality scores (typed)Figure 1: Our system, LEIBNIZ, uses the Web and Free-base to determine functionality of Web relations.is scarce or expensive (e.g., biomedical texts).
Thispaper tackles automatic functionality identificationusing Web text.
While functionality identificationhas been utilized as a module in various NLP sys-tems, this is the first paper to focus exclusively onfunctionality identification as a bona fide NLP infer-ence task.It is natural to identify functions based on triplesextracted from text instead of analyzing sentencesdirectly.
Thus, as our input, we utilize tuples ex-tracted by TEXTRUNNER (Banko and Etzioni, 2008)when run over a corpus of 500 million webpages.TEXTRUNNER maps sentences to tuples of the form<arg1, relation phrase, arg2> and enables ourLEIBNIZ system to focus on the problem of decid-ing whether the relation phrase is a function.The naive approach, which classifies a relationphrase as non-functional if several arg1s have multi-ple arg2s in our extraction set, fails due to severalreasons: synonymy ?
a unique entity may be re-ferred by multiple strings, polysemy of both entitiesand relations ?
a unique string may refer to multipleentities/relations, metaphorical usage, extraction er-rors and more.
These phenomena conspire to makethe functionality determination task inherently sta-tistical and surprisingly challenging.In addition, a functional relation phrase may ap-pear non-functional until we consider the types of itsarguments.
In our ?was born in?
example, <GeorgeWashington, was born in, 1732> does not contradict<George Washington, was born in, Virginia> eventhough we see two distinct arg2s for the same arg1.To solve functionality identification, we need to con-sider typed relations where the relations analyzedare constrained to have specific argument types.We develop several approaches to overcome thesechallenges.
Our first scheme employs approximateargument merging to overcome the synonymy andanaphora problems.
Our second approach, DIS-TRDIFF, takes a statistical view of the problemand learns a separator for the typical count dis-tributions of functional versus non-functional rela-tions.
Finally, our third and most successful scheme,CLEANLISTS, identifies and processes a cleanersubset of the data by intersecting the corpus with en-tities in a secondary knowledge-base (in our case,Freebase (Metaweb Technologies, 2009)).
Utiliz-ing pre-defined types, CLEANLISTS first identifiestyped functionality for suitable types for that rela-tion phrase, and then combines them to output a finalfunctionality label.
LEIBNIZ, a hybrid of CLEAN-LISTS and DISTRDIFF, returns state-of-the-art re-sults for our task.Our work makes the following contributions:1.
We identify several linguistic phenomena thatmake the problem of corpus-based functional-ity identification surprisingly difficult.2.
We designed and implemented three noveltechniques for identifying functionality basedon instance-based counting, distributional dif-ferences, and use of external knowledge bases.3.
Our best method, LEIBNIZ, outperforms theexisting approaches by wide margins, increas-ing area under the precision-recall curve from0.61 to 0.88.
It is also capable of distinguishingfunctionality of typed relation phrases, whenthe arguments are restricted to specific types.4.
Utilizing LEIBNIZ, we created the first publicrepository of functional relations.12 Related WorkThere is a recent surge in large knowledge basesconstructed by human collaboration such as Free-base (Metaweb Technologies, 2009) and VerbNet(Kipper-Schuler, 2005).
VerbNet annotates itsverbs with several properties but not functionality.Freebase does annotate some relations with an ?isunique?
property, which is similar to functionality,but the number of relations in Freebase is still much1available at http://www.cs.washington.edu/research/leibniz1267George Washington was born in :VirginiaWestmoreland CountyAmericaa towna plantation1732Februarythe British colony of VirginiaRudy Giuliani visited:FloridaBoca Raton Synagoguethe Florida EvergladesSouth CarolinaMichiganRepublican headquartersa famous cheesesteak restaurantPhiladelphiaColonial Beach, VirginiaFigure 2: Sample arg2 values for a non-functional relation (visited) vs. a functional relation (was born in) illustratethe challenge in discriminating functionality from Web text.smaller than the hundreds of thousands of relationsexisting on the Web, necessitating automatic ap-proaches to functionality identification.Discovering functional dependencies has beenrecognized as an important database analysis tech-nique (Huhtala et al, 1999; Yao and Hamilton,2008), but the database community does not addressany of the linguistic phenomena which make thisa challenging problem in NLP.
Three groups of re-searchers have studied functionality identification inthe context of natural language.AuContraire (Ritter et al, 2008) is a contradic-tion detection system that also learns relation func-tionality.
Their approach combines a probabilis-tic model based on (Downey et al, 2005) with es-timates on whether each arg1 is ambiguous.
Theestimates are used to weight each arg1?s contri-bution to an overall functionality score for eachrelation.
Both argument-ambiguity and relation-functionality are jointly estimated using an EM-likemethod.
While elegant, AuContraire requires sub-stantial hand-engineered knowledge, which limitsthe scalability of their approach.Lexico-syntactic patterns: Srinivasan and Yates(2009) disambiguate a quantifier?s scope by firstmaking judgments about relation functionality.
Forfunctionality, they look for numeric phrases follow-ing the relation.
For example, the presence of the nu-meric term ?four?
in the sentence ?the fire destroyedfour shops?
suggests that destroyed is not functional,since the same arg1 can destroy multiple things.The key problem with this approach is that it oftenassigns different functionality labels for the presenttense and past tense phrases of the same semantic re-lation.
For example, it will consider ?lived in?
to benon-functional, but ?lives in?
to be functional, sincewe rarely say ?someone lives in many cities?.
Sinceboth these phrases refer to the same semantic rela-tion this approach has low precision.
Moreover, itperforms poorly for relation phrases that naturallyexpect numbers as the target argument (e.g., ?has anatomic number of?
).While these lexico-syntactic patterns do not per-form as well for our task, they are well-suited foridentifying whether a verb phrase can take multipleobjects or not.
This can be understood as a function-ality property of the verb phrase within a sentence,as opposed to functionality of the semantic relationthe phrase represents.WIE: In a preliminary study, Popescu (2007) ap-plies an instance based counting approach, but herrelations require manually annotated type restric-tions, which makes the approach less scalable.Finally, functionality is just one property of rela-tions that can be learned from text.
A number ofother studies (Guarino and Welty, 2004; Volker etal., 2005; Culotta et al, 2006) have examined detect-ing other relation properties from text and applyingthem to tasks such as ontology cleaning.3 Challenges for Functionality IdentificationA functional binary relation r is formally defined asone such that ?x, y1, y2 : r(x, y1)?r(x, y2)?
y1 =y2.
We define a relation string to be functional if allsemantic relations commonly expressed by the rela-tion string are individually functional.
Thus, underour definition, ?was born in?
and ?died in?
are func-tional, even though they can take different arg2s forthe same arg1, e.g., year, city, state, country, etc.The definition of a functional relation suggests anaive instance-based counting algorithm for identi-fying functionality.
?Look for the number of arg2sfor each arg1.
If all (or most) arg1s have exactly onearg2, label the relation phrase functional, else, non-functional.?
Unfortunately, this naive algorithm failsfor our task exposing several linguistic phenomena1268that make our problem hard (see Figure 2):Synonymy: Various arg2s for the same arg1 mayrefer to the same entity.
This makes many func-tional relations seem non-functional.
For instance,<George Washington, was born in, Virginia> and<George Washington, was born in, the Britishcolony of Virginia> are not in conflict.
Otherexamples of synonyms include ?Windy City?
and?Chicago?
; ?3rd March?
and ?03/03?, etc.Anaphora: An entity can be referred to by usingseveral phrases.
For instance,<George Washington,was born in, a town> does not conflict with his be-ing born in ?Colonial Beach, Virginia?, since ?town?is an anaphora for his city of birth.
Other examplesinclude ?The US President?
for ?George W. Bush?,and ?the superpower?
to refer to ?United States?.
Theeffect is similar to that of synonyms ?
many relationsincorrectly appear non-functional.Argument Ambiguity: <George Washington, wasborn in, ?Kortrijk, Belgium?> in addition to his be-ing born in ?Virginia?
suggests that ?was born in?is non-functional.
However, the real cause is that?George Washington?
is ambiguous and refers to dif-ferent people.
This ambiguity gets more pronouncedif the person is referred to just by their first (or lastname), e.g., ?Clinton?
is commonly used to refer toboth Hillary and Bill Clinton.Relation Phrase Ambiguity: A relation phrase canhave several senses.
For instance ?weighs 80 kilos?is a different weighs than ?weighs his options?.Type Restrictions: A closely related problemis type-variations in the argument.
E.g., <GeorgeWashington, was born in, America> vs. <GeorgeWashington, born in, Virginia> both use the samesense of ?was born in?
but refer to different semanticrelations ?
one that takes a country in arg2, and theother that takes a state.
Moreover, different argu-ment types may result in different functionality la-bels.
For example, ?published in?
is functional if thearg2 is a year, but non-functional if it is a language,since a book could be published in many languages.We refer to this finer notion of functionality as typedfunctionality.Data Sparsity: There is limited data for more ob-scure relations instances and non-functional relationphrases appear functional due to lack of evidence.Textually Functional Relations: Last but not least,some relations that are not functional may appearfunctional in text.
An example is ?collects?.
We col-lect many things, but rarely mention it in text.
Usu-ally, someone?s collection is mentioned in text onlywhen it makes the news.
We name such relationstextually functional.
Even though we could buildtechniques to reduce the impact of other phenomena,no instance based counting scheme could overcomethe challenge posed by textually functional relations.Finally, we note that our functionality predictoroperates over tuples generated by an Open IE sys-tem.
The extractors are not perfect and their errorscan also complicate our analysis.4 AlgorithmsTo overcome these challenges, we design three al-gorithms.
Our first algorithm, IBC, applies severalrules to determine whether two arg2s are equal.
Oursecond algorithm, DISTRDIFF, takes a statistical ap-proach, and tries to learn a discriminator betweentypical count distributions for functional and non-functional relations.
Our final approach, CLEAN-LISTS, applies counting over a cleaner subset of thecorpus, which is generated based on entities presentin a secondary KB such as Freebase.From this section onwards, we gloss over the dis-tinction between a semantic relation and a relationphrase, since our algorithms do not have access torelations and operate only at the phrase level.
Weuse ?relation?
to refer to the phrases.4.1 Instance Based Counting (IBC)For each relation, IBC computes a global function-ality score by aggregating local functionality scoresfor each arg1.
The local functionality for each arg1computes the fraction of arg2 pairs that refer to thesame entity.
To operationalize this computation weneed to identify which arg2s co-refer.
Moreover, wealso need to pick an aggregation strategy to combinelocal functionality scores.Data Cleaning: Common nouns in arg1s are of-ten anaphoras for other entities.
For example, <thecompany, was headquartered in, ...> refers to dif-ferent companies in different extractions.
To combatthis, IBC restricts arg1s to proper nouns.
Secondly,to counter extraction errors and data bias, it retains1269George Washington was born in :ColonialBeachColonialBeach,VirginiaWestmoreland County, VirginiaFebruary February 1732 1732Figure 3: IBC judges that Colonial Beach and Westmore-land County, Virginia refer to the same entity.an extraction only once per unique sentence.
Thisreduces the disproportionately large frequencies ofsome assertions that are generated from a single ar-ticle published at multiple websites.
Similarly, it al-lows an extraction only once per website url.
More-over, it filters out any arg1 that does not appear atleast 10 times with that relation.Equality Checking: This key component judgesif two arg2s refer to the same entity.
It first em-ploys weak typing by disallowing equality checksacross common nouns, proper nouns, dates andnumbers.
This mitigates the relation ambiguityproblem, since we never compare ?born in(1732)?and ?born in(Virginia)?.
Within the same category itjudges two arg2s to co-refer if they share a contentword.
It also performs a connected component anal-ysis (Hopcraft and Tarjan, 1973) to take a transitiveclosure of arg2s judged equal (see Figure 3).For example, for the relation ?was named after?and arg1=?Bluetooth?
our corpus has three arg2s:?Harald Bluetooth?, ?Harald Bluetooth, the King ofDenmark?
and ?the King of Denmark?.
Our equal-ity method judges all three as referring to the sameentity.
Note that this is a heuristic approach, whichcould make mistakes.
But for an error, there needsto be extractions with the same arg1, relation andsimilar arg2s.
Such cases exist, but are not com-mon.
Our equality checking mitigates the problemsof anaphora, synonymy as well as some typing.Aggregation: We try several methods to aggre-gate local functionality scores for each arg1 into aglobal score for the relation.
These include, a simpleaverage, a weighted average weighted by frequencyof each arg1, a weighted average weighted by logof frequency of each arg1, and a Bayesian approachthat estimates the probability that a relation is func-tional using statistics over a small development set.Overall, the log-weighting works the best: it assignsa higher score for popular arguments, but not so highthat it drowns out all the other evidence.4.2 DISTRDIFFOur second algorithm, DISTRDIFF, takes a purelystatistical, discriminative view of the problem.
Itrecognizes that, due to aforementioned reasons,whether a relation is functional or not, there arebound to be several arg1s that look locally functionaland several that look locally non-functional.
Thedifference is in the number of such arg1s ?
a func-tional relation will have more of the former type.DISTRDIFF studies the count distributions for asmall development set of functional relations (andsimilarly for non-functional) and attempts to builda separator between the two.
As an illustration,Figure 4(a) plots the arg2 counts for various arg1sfor a functional relation (?is headquartered in?
).Each curve represents a unique arg1.
For an arg1,the x-axis represents the rank (based on frequency)of arg2s and y-axis represents the normalized fre-quency of the arg2.
For example, if an arg1 is foundwith just one arg2, then x=1 will match with y=1(the first point has all the mass) and x=2 will matchwith y=0.
If, on the other hand, an arg1 is foundwith five arg2s, say, appearing ten times each, thenthe first five x-points will map to 0.2 and the sixthpoint will map to 0.We illustrate the same plot for a non-functionalrelation (?visited?)
in Figure 4(b).
It is evident fromthe two figures that, as one would expect, curves formost arg1s die early in case of a functional relation,whereas the lower ranked arg2s are more denselypopulated in case of a non-functional relation.We aggregate this information using slope of thebest-fit line for each arg1 curve.
For functional re-lations, the best-fit lines have steep slopes, whereasfor non-functional the lines are flatter.
We bucket theslopes in integer bins and count the fraction of arg1sappearing in each bin.
This lets us aggregate theinformation into a single slope-distribution for eachrelation.
Bold lines in Figure 4(c) illustrate the aver-age slope-distributions, averaged over ten sample re-lations of each kind ?
dashed for non-functional andsolid for functional.
Most non-functional relationshave a much higher probability of arg1s with lowmagnitude slopes, whereas functional relations are12700%10%20%30%40%50%60%70%80%90%100%1 2 3 4 5 6 7 8 9 10Argument2 MassResult Positionis headquartered in (functional)0%10%20%30%40%50%60%70%80%90%100%1 2 3 4 5 6 7 8 9 10Argument2 MassResult Positionvisited (not functional)0%10%20%30%40%50%60%70%80%90%100%0 1 2 3 4 10MassFloor(|Slope|)Average Slope Distributionsfunctional averagenonfunctional averagewas born on (sample func)visited (sample nonfunc)Figure 4: DISTRDIFF: Arg2 count distributions fall more sharply for (a) a sample functional relation, than (b) asample non-functional relation.
(c) The distance of aggregated slope-distributions from average slope-distributionscan be used to predict the functionality.the opposite.
Notice that the aggregated curve for?visited?
in the figure is closer to the average curvefor non-functional than to functional and vice-versafor ?was born on?.We plot the aggregated slope-distributions foreach relation and use the distance from average dis-tributions as a means to predict the functionality.
Weuse KL divergence (Kullback and Leibler, 1951) tocompute the distance between two distributions.
Wescore a relation?s functionality in three ways using:(1) KLFUNC, its distance from average functionalslope-distribution Favg, (2) KLDIFF, its distancefrom average functional minus its distance from av-erage non-functional Navg, and (3) average of thesetwo scores.
For a relation with slope distribution R,the scores are computed as:KLFUNC =?iR(i)lnR(i)Favg(i)KLDIFF = KLFUNC - (?iR(i)lnR(i)Navg(i))Section 5.2 compares the three scoring functions.A purely statistical approach is resilient to noisydata, and does not need to explicitly account for thevarious issues we detailed earlier.
A disadvantageis that it cannot handle relation ambiguity and typerestrictions.
Moreover, we may need to relearn theseparator if applying DISTRDIFF to a corpus withvery different count distributions.4.3 CLEANLISTSOur third algorithm, CLEANLISTS, is based on theintuition that for identifying functionality we neednot reason over all the data in our corpus; instead,a small but cleaner subset of the data may workbest.
This clean subset should ideally be free of syn-onyms, ambiguities and anaphora, and be typed.Several knowledge-bases such as Wordnet,Wikipedia, and Freebase (Fellbaum, 1998;Wikipedia, 2004; Metaweb Technologies, 2009),are readily and freely available and they all provideclean typed lists of entities.
In our experimentsCLEANLISTS employs Freebase as a source ofclean lists, but we could use any of these or otherdomain-specific ontologies such as SNOMED(Price and Spackman, 2000) as well.CLEANLISTS takes the intersection of Freebaseentities with our corpus to generate a clean subset forfunctionality analysis.
Freebase currently has over12 million entities in over 1,000 typed lists.
Thus,this intersection retains significant portions of theuseful data, and gets rid of most of anaphora andsynonymy issues.
Moreover, by matching againsttyped lists, many relation ambiguities are separatedas well, since ambiguous relations often take dif-ferent types in the arguments (e.g., ?ran(Distance)?vs.
?ran(Company)?).
To mitigate the effect of argu-ment ambiguity, we additionally get rid of instancesin which arg1s match multiple names in the Freebaselist of names.As an example, consider the ?was born in?
rela-tion.
CLEANLISTS will remove instances with only?Clinton?
in arg1, since it matches multiple peoplein Freebase.
It will treat the different types, e.g.,cities, states, countries, months separately and ana-lyze the functionality for each of these individually.1271By intersecting the relation data with argument listsfor these types, we will be left with a smaller, butmuch cleaner, subset of relation data, one for eachtype.
CLEANLISTS analyzes each subset using sim-ple, instance based counting and computes a typedfunctionality score for each type.
Thus, it first com-putes typed functionality for each relation.There are two subtleties in applying this algo-rithm.
First, we need to identify the set of types toconsider for each relation.
Our algorithm currentlypicks the types that occur most in each relation?sobserved data.
In the future, we could also use aselectional preferences system (Ritter et al, 2010;Kozareva and Hovy, 2010).
Note that we removeFreebase types such as Written Work from consid-eration for containing many entities whose primarysenses are not that type.
For example, both ?Al Gore?and ?William Clinton?
are also names of books, butreferences in text to these are rarely a reference tothe written work sense.Secondly, an argument could belong to multipleFreebase lists.
For example, ?California?
is both acity and a state.
We apply a simple heuristic: if astring appears in multiple lists under consideration,we assign it to the smallest of the lists (the list ofcities is much larger than states).
This simple heuris-tic usually assigns an argument to its intended type.On a development set, the error rate of this heuristicis<4%, though it varies a bit depending on the typesinvolved.CLEANLISTS determines the overall functional-ity of a relation string by aggregating the scores foreach type.
It outputs functional if a majority of typedsenses for the relation are functional.
For example,CLEANLISTS judges ?was born in?
to be functional,since all relevant type restrictions are individuallytyped functional ?
everyone is born in exactly onecountry, city, state, month, etc.CLEANLISTS has a much higher precision due tothe intersection with clean lists, though at some costof recall.
The reason for lower recall is that the ap-proach has a bias towards types that are easy to enu-merate.
It does not have different distances (e.g., 50kms, 20 miles, etc.)
in its lists.
Moreover, argumentsthat do not correspond to a noun cannot be handled.For example, in the sentence, ?He weighed eatinga cheeseburger against eating a salad?, the arg2 of?weighed?
can?t be matched to a Freebase list.
Toincrease the recall we back off to DISTRDIFF in thecases when CLEANLISTS is unable to make a pre-diction.
This combination gives the best balance ofprecision and recall for our task.
We name our finalsystem LEIBNIZ.One current limitation is that using only thosearg2s that exactly match clean lists leaves out somegood data (e.g., a tuple with an arg2 of ?Univ ofWash?
will not match against a list of universitiesthat spells it as ?University of Washington?).
Be-cause we have access to entity types, using typedequality checkers (Prager et al, 2007) with the cleanlists would allow us to recapture much of this usefuldata.
Moreover, the knowledge of functions couldapply to building new type nanotheories and reduceconsiderable manual effort.
We wish to study this inthe future.5 EvaluationIn our evaluation, we wish to answer three ques-tions: (1) How do our three approaches, InstanceBased Counting (IBC), DISTRDIFF, and CLEAN-LISTS, compare on the functionality identificationtask?
(2) How does our final system, LEIBNIZ,compare against the existing state of the art tech-niques?
(3) How well is LEIBNIZ able to identifytyped functionality for different types in the samerelation phrase?5.1 DatasetFor our experiments we test on the set of 887 re-lations used by Ritter et al (2008) in their exper-iments.
We use the Open IE corpus generated byrunning TEXTRUNNER on 500 million high qualityWebpages (Banko and Etzioni, 2008) as the sourceof instance data for these relations.
Extractor andcorpus differences lead to some relations not occur-ring (or not occurring with sufficient frequency toproperly analyze, i.e.,?
5 arg1 with?
10 evidence),leaving a dataset of 629 relations on which to test.Two human experts tagged these relations forfunctionality.
Tagging the functionality of relationphrases can be a bit subjective, as it requires theexperts to imagine the various senses of a phraseand judge functionality over all those senses.
Theinter-annotator agreement between the experts was95.5%.
We limit ourselves to the subset of the data127200.10.20.30.40.50.60.70.80.910 0.2 0.4 0.6 0.8 1PrecisionRecallDistr.
Diff.
Scoring FunctionsKLfuncKLdiffAverage00.10.20.30.40.50.60.70.80.910 0.2 0.4 0.6 0.8 1PrecisionRecallInternal Methods ComparisonIBCDistr.
Diff.CleanListsFigure 5: (a) The best scoring method for DISTRDIFF averages KLFUNC and KLDIFF.
(b) CLEANLISTS performssignificantly better than DISTRDIFF, which performs significantly better than IBC.on which the two experts agreed (a subset of 601relation phrases).5.2 Internal ComparisonsFirst, we compare the three scoring functions forDISTRDIFF.
We vary the score thresholds to gener-ate the different points on the precision-recall curvesfor each of the three.
Figure 5(a) plots these curves.It is evident that the hybrid scoring function, i.e.,one which is an average of KLFUNC (distance fromaverage functional) and KLDIFF (distance from av-erage functional minus distance from average non-functional) performs the best.
We use this scoring inthe further experiments involving DISTRDIFF.Next, we compare our three algorithms onthe dataset.
Figure 5(b) reports the results.CLEANLISTS outperforms DISTRDIFF by vast mar-gins, covering a 33.5% additional area under theprecision-recall curve.
Overall, CLEANLISTS findsthe very high precision points, because of its use ofclean data.
However, it is unable to make 23.1% ofthe predictions, primarily because the intersectionbetween the corpus and Freebase entities results invery little data for those relations.
DISTRDIFF per-forms better than IBC, due to its statistical nature,but the issues described in Section 3 plague boththese systems much more than CLEANLISTS.To increase the recall LEIBNIZ uses a combina-tion of DISTRDIFF and CLEANLISTS, in which thealgorithm backs off to DISTRDIFF if CLEANLISTSis unable to output a prediction.5.3 External ComparisonsWe next compare LEIBNIZ against the existing stateof the art approaches.
Our competitors are AuCon-traire and NumericTerms (Ritter et al, 2008; Srini-vasan and Yates, 2009).
Because we use the Au-Contraire dataset, we report the results from theirbest performing system.
We reimplement a versionof NumericTerms using their list of numeric quanti-fiers and extraction patterns that best correspond toour relation format.
We run our implementation ofNumericTerms on a dataset of 100 million Englishsentences from a crawl of high quality Webpages togenerate the functionality labels.Figure 6(a) reports the results of this experiment.We find that LEIBNIZ outperforms AuContraire byvast margins covering an additional 44% area in theprecision-recall curve.
AuContraire?s AUC is 0.61whereas LEIBNIZ covers 0.88.
A Bootstrap Per-centile Test (Keller et al, 2005) on F1 score foundthe improvement of our techniques over AuCon-traire to be statistically significant at ?
= 0.05.
Nu-mericTerms does not perform well, because it makesdecisions based only on the local evidence in a sen-tence, and does not integrate the knowledge fromdifferent occurrences of the same relation.
It returnsmany false positives, such as ?lives in?, which ap-pear functional to the lexico-syntactic pattern, butare clearly non-functional, e.g., one could live inmany places over a lifetime.An example of a LEIBNIZ error is the ?repre-sented?
relation.
LEIBNIZ classifies this as func-tional, because it finds several strongly functionalsenses (e.g., when a person represents a country),127300.10.20.30.40.50.60.70.80.910 0.2 0.4 0.6 0.8 1PrecisionRecallTyped FunctionalityAuContraireNumericTermsLeibniz00.10.20.30.40.50.60.70.80.910 0.2 0.4 0.6 0.8 1PrecisionRecallExternal ComparisonAuContraireNumericTermsLeibnizFigure 6: (a) LEIBNIZ, which is a hybrid of CLEANLISTS and DISTRDIFF, achieves 0.88 AUC and outperforms the0.61 AUC from AuContraire (Ritter et al, 2008) and the 0.05 AUC from NumericTerms (Srinivasan and Yates, 2009).
(b) LEIBNIZ is able to tease apart different senses of polysemous relations much better than other systems.but the human experts might have had some non-functional senses in mind while labeling.5.4 Typed FunctionalityNext, we conduct a study of the typed functional-ity task.
We test on ten common polysemous re-lations, each having both a functional and a non-functional sense.
An example is the ?was pub-lished in?
relation.
If arg2 is a year it is func-tional, e.g.
<Harry Potter 5, was published in,2003>.
However, ?was published in(Language)?is not functional, e.g.
<Harry Potter 5, was pub-lished in, [French / Spanish / English]>.
Simi-larly, ?will become(Company)?
is functional becausewhen a company is renamed, it transitions awayfrom the old name exactly once, e.g.
<Cingular,will become, AT&T Wireless>.
However, ?will be-come(government title)?
is not functional, becausepeople can hold different offices in their life, e.g.,<Obama, will become, [Senator / President]>.In this experiment, a simple baseline of predict-ing the same label for the two types of each rela-tion achieves a precision of 0.5.
Figure 6(b) presentsthe results of this study.
AuContraire achieves a flat0.5, since it cannot distinguish between types.
Nu-mericTerms can be modified to distinguish betweenbasic types ?
check the word just after the numericterm to see whether it matches the type name.
Forexample, the modified NumericTerms will searchthe Web for instances of ?was published in [nu-meric term] years?
vs. ?was published in [numericterm] languages?.
This scheme works better whenthe type name is simple (e.g., languages) rather thancomplex (e.g., government titles).LEIBNIZ performs the best and is able to teaseapart the functionality of various types very well.When LEIBNIZ did not work, it was generally be-cause of textual functionality, which is a larger issuefor typed functionality than general functionality.
Ofcourse, these results are merely suggestive ?
we per-form a larger-scale experiment and generate a repos-itory of typed functions next.6 A Repository of Functional RelationsWe now report on a repository of typed functionalrelations generated automatically by applying LEIB-NIZ to a large collection of relation phrases.
Insteadof starting with the most frequent relations fromTEXTRUNNER, we use OCCAM?s relations (Faderet al, 2010) because they are more specific.
For in-stance, where TEXTRUNNER outputs an underspec-ified tuple, <Gold, has, an atomic number of 79>,OCCAM extracts <Gold, has an atomic number of,79>.
OCCAM enables LEIBNIZ to identify far morefunctional relations than TEXTRUNNER.6.1 Addressing Evidence SparsityScaling up to a large collection of typed relationsrequires us to consider the size of our data sets.
Forexample, consider which relation is more likely to befunctional?a relation with 10 instances all of whichindicate functionality versus a relation with 100 in-stances where 95 behave functionally.To address this problem, we adapt the likelihoodratio approach from Schoenmackers et al (2010).1274For a typed relation with n instances, f of which in-dicate functionality, the G-test (Dunning, 1993), G= 2*(f*ln(f/k)+(n-f)*ln((n-f)/(n-k))), provides a mea-sure for the likelihood that the relation is not func-tional.
Here k denotes the evidence indicating func-tionality for the case where the relation is not func-tional.
Setting k = n*0.25 worked well for us.
ThisG-score replaces our previous metric for scoringfunctional relations.6.2 Evaluation of the RepositoryIn CLEANLISTS a factor that affects the quality ofthe results is the exact set of lists that is used.
Ifthe lists are not clean, results get noisy.
For exam-ple, Freebase?s list of films contains 73,000 entries,many of which (e.g., ?Egg?)
are not films in their pri-mary senses.
Even with heuristics such as assigningterms to their smallest lists and disqualifying dictio-nary words that occur from large type lists, there isstill significant noise left.Using LEIBNIZ with a set of 35 clean lists onOCCAM?s extraction corpus, we generated a repos-itory of 5,520 typed functional relations.
To eval-uate this resource a human expert tagged a randomsubset of the top 1,000 relations.
Of these relations22% were either ill-formed or had non-sensical typeconstraints.
From the well-formed typed relationsthe precision was estimated to be 0.8.
About halfthe errors were due to textual functionality and therest were LEIBNIZ errors.
Some examples of goodfunctions found include isTheSequelTo(videogame)and areTheBirthstoneFor(month).
An example ofa textually functional relation found is wasThe-FounderOf(company).This is the first public repository of automatically-identified functional relations.
Scaling up our dataset forced us to confront new sources of noise in-cluding extractor errors, errors due to mismatchedtypes, and errors due to sparse evidence.
Still, ourinitial results are encouraging and we hope that ourresource will be valuable as a baseline for futurework.7 ConclusionsFunctionality identification is an important subtaskfor Web-scale information extraction and other ma-chine reading tasks.
We study the problem of pre-dicting the functionality of a relation phrase auto-matically from Web text.
We presented three algo-rithms for this task: (1) instance-based counting, (2)DISTRDIFF, which takes a statistical approach anddiscriminatively classifies the relations using aver-age arg-distributions, and (3) CLEANLISTS, whichperforms instance based counting on a subset ofclean data generated by intersection of the corpuswith a knowledge-base like Freebase.Our best approach, LEIBNIZ, is a hybrid ofDISTRDIFF and CLEANLISTS, and outperformsthe existing state-of-the-art approaches by covering44% more area under the precision-recall curve.
Wealso observe that an important sub-component ofidentifying a functional relation phrase is identifyingtyped functionality, i.e., functionality when the ar-guments of the relation phrase are type-constrained.Because CLEANLISTS is able to use typed lists, itcan successfully identify typed functionality.We run our techniques on a large set of relations tooutput a first repository of typed functional relations.We release this list for further use by the researchcommunity.2Future Work: Functionality is one of the sev-eral properties a relation can possess.
Others in-clude selectional preferences, transitivity (Schoen-mackers et al, 2008), mutual exclusion, symme-try, etc.
These properties are very useful in increas-ing our understanding about these Open IE relationstrings.
We believe that the general principles devel-oped in this work, for example, connecting the OpenIE knowledge with an existing knowledge resource,will come in very handy in identifying these otherproperties.AcknowledgementsWe would like to thank Alan Ritter, Alex Yates andAnthony Fader for access to their data sets.
Wewould like to thank Stephen Soderland, Yoav Artzi,and the anonymous reviewers for helpful commentson previous drafts.
This research was supportedin part by NSF grant IIS-0803481, ONR grantN00014-08-1-0431, DARPA contract FA8750-09-C-0179, a NDSEG Fellowship, and carried out atthe University of Washington?s Turing Center.2available at http://www.cs.washington.edu/research/leibniz1275ReferencesM.
Banko and O. Etzioni.
2008.
The tradeoffs betweenopen and traditional relation extraction.
In Proceed-ings of the 46th Annual Meeting of the Association forComputational Linguistics (ACL).M.
Banko, M. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfrom the web.
In Proceedings of the 20th InternationalJoint Conference on Artificial Intelligence (IJCAI).A.
Culotta, A. McCallum, and J. Betz.
2006.
Integratingprobabilistic extraction models and data mining to dis-cover relations and patterns in text.
In Proceedings ofthe HLT-NAACL.D.
Downey, O. Etzioni, and S. Soderland.
2005.
A prob-abilistic model of redundancy in information extrac-tion.
In Proceedings of the 19th International JointConference on Artificial Intelligence (IJCAI).T.
Dunning.
1993.
Accurate methods for the statistics ofsurprise and coincidence.
In Computational Linguis-tics, volume 19.A.
Fader, O. Etzioni, and S. Soderland.
2010.
Identi-fying well-specified relations for open information ex-traction.
(In preparation).C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lex-ical Database.
MIT Press, Cambridge, MA.N.
Guarino and C. Welty.
2004.
An overview of On-toClean.
In Handbook of Ontologies in InformationSystems, pages 151?172.J.
Hopcraft and R. Tarjan.
1973.
Efficient algorithmsfor graph manipulation.
Communications of the ACM,16:372?378.Y.
Huhtala, J. Ka?rkka?inen, P. Porkka, and H. Toivonen.1999.
TANE: An efficient algorithm for discover-ing functional and approximate dependencies.
In TheComputer Journal.M.
Keller, S. Bengio, and S.Y.
Wong.
2005.
Bench-marking non-parametric statistical tests.
In Advancesin Neural Information Processing Systems (NIPS) 18.K.
Kipper-Schuler.
2005.
Verbnet: A broad-coverage,comprehensive verb lexicon.
In Ph.D. thesis.
Univer-sity of Pennsylvania.Z.
Kozareva and E. Hovy.
2010.
Learning argumentsand supertypes of semantic relations using recursivepatterns.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics (ACL).S.
Kullback and R.A. Leibler.
1951.
On informationand sufficiency.
Annals of Mathematical Statistics,22(1):79?86.Metaweb Technologies.
2009.
Freebase data dumps.
Inhttp://download.freebase.com/datadumps/.A-M. Popescu.
2007.
Information extraction from un-structured web text.
In Ph.D. thesis.
University ofWashington.J.
Prager, S. Luger, and J. Chu-Carroll.
2007.
Type nan-otheories: a framework for term comparison.
In Pro-ceedings of the 16th ACM Conference on Informationand Knowledge Management (CIKM).C.
Price and K. Spackman.
2000.
SNOMED clincalterms.
In British Journal of Healthcare Computing &Information Management, volume 17.A.
Ritter, D. Downey, S. Soderland, and O. Etzioni.2008.
It?s a contradiction - no, it?s not: A case studyusing functional relations.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).A.
Ritter, Mausam, and O. Etzioni.
2010.
A latent dirich-let alocation method for selectional preferences.
InProceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics (ACL).S.
Schoenmackers, O. Etzioni, and D. Weld.
2008.
Scal-ing textual inference to the web.
In Proceedings ofthe 2008 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).S.
Schoenmackers, J. Davis, O. Etzioni, and D. Weld.2010.
Learning first-order horn clauses from web text.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing (EMNLP).P.
Srinivasan and A. Yates.
2009.
Quantifier scopedisambiguation using extracted pragmatic knowledge:Preliminary results.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP).J.
Volker, D. Vrandecic, and Y.
Sure.
2005.
Auto-matic evaluation of ontologies (AEON).
In Proceed-ings of the 4th International Semantic Web Conference(ISWC).Wikipedia.
2004.
Wikipedia: The free encyclopedia.
Inhttp://www.wikipedia.org.
Wikimedia Foundation.H.
Yao and H. Hamilton.
2008.
Mining functional de-pendencies from data.
In Data Mining and KnowledgeDiscovery.A.
Yates and O. Etzioni.
2009.
Unsupervised methodsfor determining object and relation synonyms on theweb.
In Journal of Artificial Intelligence Research,volume 34, pages 255?296.1276
