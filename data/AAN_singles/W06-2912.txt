Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 85?92, New York City, June 2006. c?2006 Association for Computational LinguisticsUnsupervised Parsing with U-DOPRens BodSchool of Computer ScienceUniversity of St AndrewsNorth Haugh, St AndrewsKY16 9SX Scotland, UKrb@dcs.st-and.ac.ukAbstractWe propose a generalization of the super-vised DOP model to unsupervised learning.This new model, which we call U-DOP,initially assigns all possible unlabeled binarytrees to a set of sentences and next uses allsubtrees from (a large subset of) these binarytrees to compute the most probable parsetrees.
We show how U-DOP can beimplemented by a PCFG-reduction tech-nique and report competitive results onEnglish (WSJ), German (NEGRA) andChinese (CTB) data.
To the best of ourknowledge, this is the first paper whichaccurately bootstraps structure for WallStreet Journal sentences up to 40 wordsobtaining roughly the same accuracy as abinarized supervised PCFG.
We show thatprevious approaches to unsupervised parsinghave shortcomings in that they eitherconstrain the lexical or the structural context,or both.1   IntroductionHow can we learn syntactic structure from unlabeleddata in an unsupervised way?
The importance ofunsupervised parsing is nowadays widely acknow-ledged.
While supervised parsers suffer fromshortage of hand-annotated data, unsupervisedparsers operate with unlabeled raw data, of whichunlimited quantities are available.
During the lastfew years there has been considerable progress inunsupervised parsing.
To give a brief overview: vanZaanen (2000) achieved 39.2% unlabeled f-score onATIS word strings by a sentence-aligning techniquecalled ABL.
Clark (2001) reports 42.0% unlabeledf-score on the same data using distributionalclustering, and Klein and Manning (2002) obtain51.2% unlabeled f-score on ATIS part-of-speechstrings using a constituent-context model calledCCM.
Moreover, on Penn Wall Street Journal p-o-s-strings ?
10 (WSJ10), Klein and Manning (2002)report 71.1% unlabeled f-score.
And the hybridapproach of Klein and Manning (2004), whichcombines a constituency and a dependency model,leads to a further increase of 77.6% f-score.Although there has thus been steadyprogress in unsupervised parsing, all theseapproaches have shortcomings in that they eitherconstrain the lexical or the structural context that istaken into account, or both.
For example, the CCMmodel by Klein and Manning (2005) is said todescribe "all contiguous subsequences of asentence" (Klein and Manning 2005: 1410).
Whilethis is a very rich lexical model, it is still limited inthat it neglects dependencies that are non-contiguoussuch as between more  and than  in "BA carriedmore people than cargo".
Moreover, by using an"all-substrings" approach, CCM risks to under-represent structural context.
Similar shortcomingscan be found in other unsupervised models.In this paper we will try to directly modelstructural as well as lexical context withoutconstraining any dependencies beforehand.
Anapproach that may seem apt in this respect is an all-subtrees approach (e.g Bod 2003; Goodman 2003;Collins and Duffy 2002).
Subtrees can model bothcontiguous and non-contiguous lexical dependencies(see section 2) and they also model constituents in ahierarchical context.
Moreover, we can view the all-subtrees approach as a generalization of Klein andManning's all-substrings approach and van Zaanen'sABL model.In the current paper, we will use the all-subtrees approach as proposed in Data-Oriented85Parsing or DOP (Bod 1998).
We will generalize thesupervised version of DOP to unsupervised parsing.The key idea of our approach is to initially assign allpossible unlabeled binary trees to a set of givensentences, and to next use counts of all subtreesfrom (a large random subset of) these binary trees tocompute the most probable parse trees.
To the bestof our knowledge, such a model has never beentried out.
We will refer to this unsupervised DOPmodel as U-DOP , while the supervised DOP model(which uses hand-annotated trees) will be referred toas S-DOP.
Moreover, we will continue to refer tothe general approach simply as DOP .U-DOP is not just an engineering approachto unsupervised learning but can also be motivatedfrom a cognitive perspective (Bod 2006): if we don'thave a clue which trees should be assigned tosentences in the initial stages of language acquisit-ion, we can just as well assume that initially all treesare possible.
Only those (sub)trees that partake incomputing the most probable parse trees for newsentences are actually "learned".
We have argued inBod (2006) that such an integration of unsupervisedand supervised methods results in an integratedmodel for language learning and language use.In the following we will first explain howU-DOP works, and how it can be approximated bya PCFG-reduction technique.
Next, in section 3 wediscuss a number of experiments with U-DOP andcompare it to previous models on English (WSJ),German (NEGRA) and Chinese (CTB) data.
To thebest of our knowledge, this is the first paper whichbootstraps structure for WSJ sentences up to 40words obtaining roughly the same accuracy as abinarized supervised PCFG.
This is remarkablesince unsupervised models are clearly at adisavantage compared to supervised models whichcan literally reuse manually annotated data.2   Unsupervised data-oriented parsingAt a general level, U-DOP consists of the followingthree steps:1.
Assign all possible binary trees to a set ofsentences2.
Convert the binary trees into a PCFG-reductionof DOP3.
Compute the most probable parse tree for eachsentenceNote that in unsupervised parsing we do not need tosplit the data into a training and a test set.
In thispaper, we will present results both on entire corporaand on 90-10 splits of such corpora so as to makeour results comparable to a supervised  PCFG usingthe treebank grammars of the same data ("S-PCFG" ).In the following we will first describe eachof the three steps given above where we initiallyfocus on inducing trees for p-o-s strings for theWSJ10 (we will deal with other corpora and themuch larger WSJ40 in section 3).
As shown byKlein and Manning (2002, 2004), the extension toinducing trees for words instead of p-o-s tags israther straightforward since there exist severalunsupervised part-of-speech taggers with highaccuracy, which can be combined with unsupervisedparsing (see e.g.
Sch?tze 1996; Clark 2000).Step 1: Assign all binary trees to p-o-s stringsfrom the WSJ10The WSJ10 contains 7422 sentences ?
10 wordsafter removing empty elements and punctuation.
Weassigned all possible binary trees to thecorresponding part-of-speech sequences of thesesentences, where each root node is labeled S andeach internal node is labeled X .
As an example,consider the p-o-s string NNS VBD JJ NNS, whichmay correspond for instance to the sentenceInvestors suffered heavy losses .
This string has atotal of five binary trees shown in figure 1 -- wherefor readability we add words as well.NNS VBD JJ NNSInvestors suffered heavy lossesXXSNNS VBD JJ NNSInvestors suffered heavy lossesXXSNNS VBD JJ NNSInvestors suffered heavy lossesXXSNNS VBD JJ NNSInvestors suffered heavy lossesXXSNNS VBD JJ NNSInvestors suffered heavy lossesXXSFigure 1.
All binary trees for NNS VBD JJ NNS(Investors suffered heavy losses)86The total number of binary trees for a sentence oflength n  is given by the Catalan number Cn?1,where Cn = (2n)!/((n+1)!n!).
Thus while a sentenceof 4 words has 5 binary trees, a sentence of 8 wordshas already 429 binary trees, and a sentence of 10words has 4862 binary trees.
Of course, we canrepresent the set of binary trees of a string inpolynomial time and space by means of a chart,resulting in a chart-like parse forest if we alsoinclude pointers.
But if we want to extract rules orsubtrees from these binary trees -- as in DOP -- weneed to unpack the parse forest.
And since the totalnumber of binary trees that can be assigned to theWSJ10 is almost 12 million, it is doubtful whetherwe can apply the unrestricted U-DOP model to sucha corpus.However, for longer sentences the binarytrees are highly redundant.
In these larger trees, thereare many rules like X ?
XX  which bear littleinformation.
To make parsing with U-DOP possiblewe therefore applied a simple heuristic which takesrandom samples from the binary trees for sentences?
7 words before they are fed to the DOP parser.These samples were taken from the distribution ofall binary trees by randomly choosing nodes andtheir expansions from the chart-like parse forests ofthe sentences (which effectively favors trees withmore frequent subtrees).
For sentences of 7 wordswe randomly sample 60% of the trees, and forsentences of 8, 9 and 10 words we samplerespectively 30%, 15% and 7.5% of the trees.
In thisway, the set of remaining binary trees contains 8.23* 105 trees, which we will refer to as the binarytree-set.
Although it can happen that the correct treeis deleted for some sentence in the binary tree-set,there is enough redundancy in the tree-set such thateither the correct binary tree can be generated byother subtrees or that a remaining tree onlyminimally differs from the correct tree.
Of course,we may expect better results if all binary trees arekept, but this involves enormous computationalresources which will be postponed to futureresearch.Step 2: Convert the trees into a PCFG-reduction of DOPThe underlying idea of U-DOP is to take all subtreesfrom the binary tree-set to compute the mostprobable tree for each sentence.
Subtrees from thetrees in figure 1 include for example the subtrees infigure 2 (where we again added words forreadability).
Note that U-DOP takes into accountboth contiguous and non-contiguous substrings.NNS VBDInvestors sufferedXXSVBDsufferedXXNNS NNSInvestors lossesXXSJJ NNSheavy lossesXXSJJ NNSheavy lossesXNNS VBDInvestors sufferedXVBD JJsuffered heavyXFigure 2.
Some subtrees from the binary trees  forNNS VBD JJ NNS given in figure 1As in the supervised DOP approach (Bod 1998), U-DOP parses a sentence by combining corpus-subtrees from the binary tree-set by means of aleftmost node substitution operation, indicated as ?.The probability of a parse tree is computed bysumming up the probabilities of all derivationsproducing it, while the probability of a derivation iscomputed by multiplying the (smoothed) relativefrequencies of its subtrees.
That is, the probability ofa subtree t  is taken as the number of occurrences of tin the binary tree-set, | t |, divided by the totalnumber of occurrences of all subtrees t' with thesame root label as t. Let r(t) return the root label of t:P(t)  =   | t |?
t': r( t')=r( t)   | t' |The subtree probabilities are smoothed by applyingsimple Good-Turing to the subtree distribution (seeBod 1998: 85-87).
The probability of a derivationt1?...
?tn  is computed by the product of theprobabilities of its subtrees t i:P(t1?...
?tn)  =  ?i P(ti)Since there may be distinct derivations that generatethe same parse tree, the probability of a parse tree T87is the sum of the probabilities of its distinctderivations.
Let ti d  be the i-th subtree in thederivation d that produces tree T , then the probabilityof T is given byP(T)  =  ?d?i P(tid)As we will explain under step 3, the most probableparse tree of a sentence is estimated by Viterbi n-best summing up the probabilities of derivations thatgenerate the same tree.It may be evident that had we only thesentence Investors suffered heavy losses in ourcorpus, there would be no difference in probabilitybetween the five parse trees in figure 1, and U-DOPwould not be able to distinguish between thedifferent trees.
However, if we have a differentsentence where JJ NNS (heavy losses)  appears in adifferent context, e.g.
in Heavy losses werereported , its covering subtree gets a relatively higherfrequency and the parse tree where heavy lossesoccurs as a constituent gets a higher total probabilitythan alternative parse trees.
Of course, it is left to theexperimental evaluation whether non-constituents("distituents") such as VBD JJ will be ruled out byU-DOP (section 3).An important feature of (U-)DOP is that itconsiders counts of subtrees of a wide range ofsizes: everything from counts of single-level rules toentire trees.
A disadvantage of the approach is thatan extremely large number of subtrees (andderivations) must be taken into account.
Fortunately,there exists a rather compact PCFG-reduction ofDOP which can also be used for U-DOP(Goodman 2003).
Here we will only give a shortsummary of this PCFG-reduction.
(Collins andDuffy 2002 show how a tree kernel can be used foran all-subtrees representation, which we will notdiscuss here.
)Goodman's reduction method first assignsevery node in every tree a unique number which iscalled its address.
The notation A@k denotes thenode at address k where A is the nonterminallabeling that node.
A new nonterminal is created foreach node in the training data.
This nonterminal iscalled Ak.
Let a j represent the number of subtreesheaded by the node A@j .
Let a represent the numberof subtrees headed by nodes with nonterminal A,that is a = ?ja j. Goodman then gives a small PCFGwith the following property: for every subtree in thetraining corpus headed by A, the grammar willgenerate an isomorphic subderivation withprobability 1/a.
For a node A@j(B@k, C@l) , thefollowing eight PCFG rules in figure 3 aregenerated, where the number in parenthesesfollowing a rule is its probability.Aj ?
BC        (1/aj)    A ?
BC         (1/a)Aj ?
BkC      (bk/aj)    A ?
BkC       (bk/a)Aj ?
BCl       (c l/aj)    A ?
BC l         (cl/a)Aj ?
BkCl     (bkc l/aj)    A ?
BkCl       (bkcl/a)Figure 3.
PCFG-reduction of DOPIn this PCFG reduction, bk represents the number ofsubtrees headed by the node B@k, and cl refers tothe number of subtrees headed by the node C@l.Goodman shows by simple induction that hisconstruction produces PCFG derivationsisomorphic to (U-)DOP derivations with equalprobability (Goodman 2003: 130-133).
This meansthat summing up over derivations of a tree in DOPyields the same probability as summing over all theisomorphic derivations in the PCFG.1The PCFG-reduction for U-DOP is slightlysimpler than in figure 3 since the only labels are Sand X , and the part-of-speech tags.
For the tree-setof 8.23 * 105 binary trees generated under step 1,Goodman's reduction method results in a totalnumber of 14.8 * 106 distinct PCFG rules.
While itis still feasible to parse with a rule-set of this size, itis evident that our approach can deal with longersentences only if we further reduce the size of ourbinary tree-set.It should be kept in mind that while theprobabilities of all parse trees generated by DOPsum up to 1, these probabilities do not converge tothe "true" probabilities if the corpus grows toinfinity (Johnson 2002).
In fact, in Bod et al (2003)we showed that the most probable parse tree asdefined above has a tendency to be constructed bythe shortest derivation (consisting of the fewest andthus largest subtrees).
A large subtree is overruledonly if the combined relative frequencies of smallersubtrees yields a larger score.
We refer to Zollmannand Sima'an (2005) for a recently proposedestimator that is statistically consistent (though it isnot yet known how this estimator performs on theWSJ) and to Zuidema (2006) for a theoreticalcomparison of existing estimators for DOP.1As in Bod (2003) and Goodman (2003: 136), weadditionally use a correction factor to redress DOP'sbias discussed in Johnson (2002).88Step 3: Compute the most probable parse treefor each WSJ10 stringWhile Goodman's reduction method allows forefficiently computing the most probable derivationfor each sentence (i.e.
the Viterbi parse), it does notallow for an efficient computation of (U-)DOP'smost probable parse tree since there may beexponentially many derivations for each tree whoseprobabilities have to be summed up.
In fact, theproblem of computing the most probable tree inDOP is known to be NP hard (Sima'an 1996).
Yet,the PCFG reduction in figure 4 can be used toestimate  DOP's most probable parse tree by aViterbi n-best search in combination with a CKYparser which computes the n  most likely derivationsand next sums up the probabilities of the derivationsproducing the same tree.
(We can considerablyimprove efficiency by using k-best hypergraphparsing as recently proposed by Huang and Chiang2005, but this will be left to future research).In this paper, we estimate the most probableparse tree from the 100 most probable derivations(at least for the relatively small WSJ10).
Althoughsuch a heuristic does not guarantee that the mostprobable parse is actually found, it is shown in Bod(2000) to perform at least as well as the estimationof the most probable parse with Monte Carlotechniques.
However, in computing the 100 mostprobable derivations by means of Viterbi it isprohibitive to keep track of all subderivations at eachedge in the chart.
We therefore use a pruningtechnique which deletes any item with a probabilityless than 10?5 times of that of the best item fromthe chart.To make our parse results comparable tothose of Klein and Manning (2002, 2004, 2005), wewill use exactly the same evaluation metrics forunlabeled precision (UP) and unlabeled recall (UR),defined in Klein (2005: 21-22).
Klein's definitionsslightly differ from the standard PARSEVALmetrics: multiplicity of brackets is ignored, bracketsof span one are ignored and the bracket labels areignored.
The two metrics of UP and UR arecombined by the unlabled f-score F1 which isdefined as the harmonic mean of UP and UR: F1 =2*UP*UR/(UP+UR).
It should be kept in mind thatthese evaluation metrics were clearly inspired by theevaluation of supervised  parsing which aims atmimicking given  tree annotations as closely aspossible.
Unsupervised parsing is different in thisrespect and it is questionable whether an evaluationon a pre-annotated corpus such as the WSJ is themost appropriate one.
For a subtle discussion onthis issue, see Clark (2001) or Klein (2005).3   Experiments3.1 Comparing U-DOP to previous workUsing the method described above, our parsingexperiment with all p-o-s strings from the WSJ10results in an f-score of 78.5%.
We next tested U-DOP on two additional domains from Chinese andGerman which were also used in Klein andManning (2002, 2004): the Chinese treebank (Xueet al 2002) and the NEGRA corpus (Skut et al1997).
The CTB10 is the subset of p-o-s stringsfrom the Penn Chinese treebank containing 10words or less after removal of punctuation (2437strings).
The NEGRA10 is the subset of p-o-sstrings of the same length from the NEGRA corpususing the supplied converson into Penn treebankformat (2175 strings).
Table 1 shows the results ofU-DOP in terms of UP, UR and F1 compared tothe results of the CCM model by Klein andManning (2002), the DMV dependency learningmodel by Klein and Manning (2004) together withtheir combined model DMV+CCM.Model English German Chinese(WSJ10) (NEGRA10) (CTB10)UP UR F1 UP UR F1 UP UR F1CCM 64.2 81.6 71.9 48.1 85.5 61.6 34.6 64.3 45.0DMV 46.6 59.2 52.1 38.4 69.5 49.5 35.9 66.7 46.7DMV+CCM 69.3 88.0 77.6 49.6 89.7 63.9 33.3 62.0 43.3U-DOP 70.8 88.2 78.5 51.2 90.5 65.4 36.3 64.9 46.6Table 1.
Results of U-DOP compared to previousmodels on the same dataTable 1 indicates that our model scores slightlybetter than Klein and Manning's combinedDMV+CCM model, although the differences aresmall (note that for Chinese the single DMV modelscores better than the combined model and slightlybetter than U-DOP).
But where Klein andManning's combined model is based on both aconstituency and a dependency model, U-DOP is,like CCM, only based on a notion of constituency.Compared to CCM alone, the all-subtrees approachemployed by U-DOP shows a clear improvement(except perhaps for Chinese).
It thus seems to payoff to use all subtrees rather than just all(contiguous) substrings in bootstrapping89constituency.
It would be interesting to investigatean extension of U-DOP towards dependencyparsing, which we will leave for future research.
It isalso noteworthy that U-DOP does not employ aseparate class for non-constituents, so-calleddistituents, while CCM does.
Thus good results canbe obtained without keeping track of distituents butby simply assigning all binary trees to the stringsand letting the DOP model decide which substringsare most likely to form constituents.To give an idea of the constituents learnedby U-DOP for the WSJ10, table 2 shows the 10most frequently constituents in the trees induced byU-DOP together with the 10 actually mostfrequently occurring constituents in the WSJ10 andthe 10 most frequently occurring part-of-speechsequences (bigrams) in the WSJ10.Rank Most frequent Most Frequent Most frequentU-DOP constituents WSJ10 constituents WSJ10 substrings1 DT NN DT NN NNP NNP2 NNP NNP NNP NNP DT NN3 DT JJ NN CD CD JJ NN4 IN DT NN JJ NNS IN DT5 CD CD DT JJ NN NN IN6 DT NNS DT NNS DT JJ7 JJ NNS JJ NN JJ NNS8 JJ NN CD NN NN NN9 VBN IN IN NN CD CD10 VBD NNS IN DT NN NN VBZTable 2.
Most frequently learned constituents byU-DOP together with most frequently occurringconstituents and p-o-s sequences (for WSJ10)Note that there are no distituents among U-DOP's10 most frequently learned constituents, whilst thethird column shows that distituents such as IN DTor DT JJ occur very frequently as substrings in theWSJ10.
This may be explained by the fact that (theconstituent) DT NN occurs more frequently as asubstring in the WSJ10 than (the distituent) IN DT,and therefore U-DOP's probability model will favora covering subtree for IN DT NN which consists ofa division into IN X and DT NN rather than into INDT and X NN, other things being equal.
The samekind reasoning can be made for a subtree for DT JJNN where the constituent JJ NN occurs morefrequently as a substring than the distituent DT JJ.Of course the situation is somewhat more complexin DOP's sum-of-products model, but our argumentmay illustrate why distituents like IN DT or DT JJare not proposed among the most frequentconstituents by U-DOP while larger constituentslike IN DT NN and DT JJ NN are in fact proposed.3.2 Testing U-DOP on held-out sets and longersentences (up to 40 words)We were also interested in U-DOP's performanceon a held-out test set such that we could compare themodel with a supervised PCFG treebank grammartrained and tested on the same data (S-PCFG).
Westarted by testing U-DOP on 10 different 90%/10%splits of the WSJ10, where 90% was used forinducing the trees, and 10% to parse new sentencesby subtrees from the binary trees from the trainingset (or actually a PCFG-reduction thereof).
Thesupervised PCFG was right-binarized as in Kleinand Manning (2005).
The following table shows theresults.Model UP UR F1U-DOP 70.6 88.1 78.3S-PCFG 84.0 79.8 81.8Table 3.
Average f-scores of U-DOP compared to asupervised PCFG (S-PCFG) on 10 different 90-10splits of the WSJ10Comparing table 1 with table 3, we see that on 10held-out WSJ10 test sets U-DOP performs with anaverage f-score of 78.3% (SD=2.1%) only slightlyworse than when using the entire WSJ10 corpus(78.5%).
Next, note that U-DOP's results come nearto the average performance of a binarized supervisedPCFG which achieves 81.8% unlabeled f-score(SD=1.8%).
U-DOP's unlabeled recall is evenhigher than that of the supervised PCFG.
Moreover,according to paired t-testing, the differences in f-scores were not  statistically significant.
(If thePCFG was not post-binarized, its average f-scorewas 89.0%.
)As a final test case for this paper, we wereinterested in evaluating U-DOP on WSJ sentences ?40 words, i.e.
the WSJ40, which is with almost50,000 sentences a much more challenging test casethan the relatively small WSJ10.
The main problemfor U-DOP is the astronomically large number ofpossible binary trees for longer sentences, whichtherefore need to be even more heavily pruned thanbefore.We used a similar sampling heuristic as insection 2.
We started by taking 100% of the trees forsentences ?
7 words.
Next, for longer sentences wereduced this percentage with the relative increase ofthe Catalan number.
This effectively means that werandomly selected the same number of trees foreach sentence ?
8 words, which is 132 (i.e.
the90number of possible binary trees for a 7-wordsentence).
As mentioned in section 2, our samplingapproach favors more frequent trees, and trees withmore frequent subtrees.
The binary tree-set obtainedin this way for the WSJ40 consists of 5.11 * 106different trees.
This resulted in a total of 88+ milliondistinct PCFG rules according to the reductiontechnique in section 2.
As this is the largest PCFGwe have ever attempted to parse with, it wasprohibitive to estimate the most probable parse treefrom 100 most probable derivations using Viterbi n-best.
Instead, we used a beam of only 15 mostprobable derivations, and selected the most probableparse from these.
(The number 15 is admittedly adhoc, and was inspired by the performance of the so-called SL-DOP model in Bod 2002, 2003).
Thefollowing table shows the results of U-DOP on theWSJ40 using 10 different 90-10 splits, compared toa  supervised binarized PCFG (S-PCFG) and asupervised binarized DOP model (S-DOP) on thesame data.Model F1U-DOP 64.2S-PCFG 64.7S-DOP 81.9Table 4.
Performance of U-DOP on WSJ40using10 different 90-10 splits, compared to abinarized  S-PCFG and a binarized S-DOP model.Table 4 shows that U-DOP obtains about the sameresults as a binarized supervised PCFG on WSJsentences ?
40 words.
Moreover, the differencesbetween U-DOP and S-PCFG were not statisticallysignificant.
This result is important as it shows thatit is possible to parse the rather challinging WSJ in acompletely unsupervised  way obtaining roughly thesame accuracy as a supervised PCFG.
This seemsto be in contrast with the CCM model which quicklydegrades if sentence length is increased (see Klein2005).
As Klein (2005: 97) notes, CCM's strengthis finding common short constituent chunks.
U-DOP on the other hand has a preference for large(even largest possible) constituent chunks.
Klein(2005: 97) reports that the combination of CCM andDMV seems to be more stable with increasingsentence length.
It would be extremely interesting tosee how DMV+CCM performs on the WSJ40.It should be kept in mind that simpletreebank PCFGs do not constitute state-of-the-artsupervised parsers.
Table 4 indicates that U-DOP'sperformance remains still far behind that of S-DOP(and indeed of other state-of-the-art supervisedparsers such as Bod 2003 or Charniak and Johnson2005).
Moreover, if S-DOP is not post-binarized, itsaverage f-score on the WSJ40 is 90.1% -- and thereare some hybrid DOP models that obtain evenhigher scores (see Bod 2003).
Our long-term goal isto try to outperform S-DOP by U-DOP.
Animportant advantage of U-DOP is of course that itonly needs unannotated data of which unlimitedquanitities are available.
Thus it would be interestingto test how U-DOP performs if trained on e.g.
100times more data.
Yet, as long as we compute our f-scores on hand-annotated data like Penn's WSJ, theS-DOP model is clearly at an advantage.
Wetherefore plan to compare U-DOP and S-DOP (andother supervised parsers) in a concrete applicationsuch as phrase-based machine translation or as alanguage model for speech recognition.4   ConclusionsWe have shown that the general DOP approach canbe generalized to unsupervised learning, effectivelyleading to a single model for both supervised andunsupervised parsing.
Our new model, U-DOP,uses all subtrees from (in principle) all binary treesof a set of sentences to compute the most probableparse trees for (new) sentences.
Although heavypruning of trees is necessary to make our approachfeasible in practice, we obtained competitive resultson English, German and Chinese data.
Our parsingresults are similar to the performance of a binarizedsupervised PCFG on the WSJ ?
40 sentences.
Thistriggers the provocative question as to whether it ispossible to beat supervised parsing by unsupervisedparsing.
To cope with the problem of evaluation, wepropose to test U-DOP in specific applicationsrather than on hand-annotated data.ReferencesBod, R. 1998.
Beyond Grammar: An Experience-Based Theory of Language, Stanford: CSLIPublications (Lecture notes number 88),distributed by Cambridge University Press.Bod, R. 2000.
An improved parser for data-orientedlexical-functional analysis.
ProceedingsACL'2000 , Hong Kong.Bod, R. 2002.
A unified model of structuralorganization in language and music.
Journal of91Artificial Intelligence Research 17(2002), 289-308.Bod, R., R. Scha and K. Sima'an (eds.)
2003.
Data-Oriented Parsing .
CSLI Publications/Universityof Chicago Press.Bod, R. 2003.
An efficient implementation of a newDOP model.
Proceedings EACL'2003,Budapest.Bod, R. 2006.
Exemplar-based syntax: How to getproductivity from examples?
The LinguisticReview 23(3), Special Isssue on Exemplar-Based Models in Linguistics.Charniak, E. and M. Johnson 2005.
Coarse-to-finen-best parsing and Max-Ent discriminativereranking.
Proceedings ACL'2005 , Ann-Arbor.Clark, A.
2000.
Inducing syntactic categories bycontext distribution clustering.
ProceedingsCONLL'2000.Clark, A.
2001.
Unsupervised induction ofstochastic context-free grammars usingdistributional clustering.
Proceed ingsCONLL'2001 .Collins, M. and N. Duffy 2002.
New rankingalgorithms for parsing and tagging: kernels overdiscrete structures, and the voted perceptron.Proceedings ACL'2002 , Philadelphia.Goodman, J.
2003.
Efficient algorithms for theDOP model.
In R. Bod, R. Scha and K.
Sima'an(eds.).
Data-Oriented Parsing , The Universityof Chicago Press.Huang, L. and Chiang D. 2005.
Better k-bestparsing.
Proceedings IWPT'2005, Vancouver.Johnson, M. 2002.
The DOP estimation method isbiased and inconsistent.
ComputationalLinguistics  28, 71-76.Klein, D. 2005.
The Unsupervised Learning ofNatural Language Structure.
PhD thesis,Stanford University.Klein, D. and C. Manning 2002.
A generalconstituent-context model for improvedgrammar induction.
Proceedings ACL'2002 ,Philadelphia.Klein, D. and C. Manning 2004.
Corpus-basedinduction of syntactic structure: models ofdependency and constituency.
ProceedingsACL'2004 , Barcelona.Klein, D. and C. Manning 2005.
Natural languagegrammar induction with a generative constituent-context model.
Pattern Recognition  38, 1407-1419.Sch?tze, H. 1995.
Distributional part-of-speechtagging.
Proceedings ACL'1995, Dublin.Sima'an, K. 1996.
Computational complexity ofprobabilistic disambiguation by means of treegrammars.
Proceedings COLING'1996,Copenhagen.Skut, W., B. Krenn, T. Brants and H. Uszkoreit1997.
An annotation scheme for free word orderlanguages.
Proceedings ANLP'97.Xue, N., F. Chiou and M. Palmer 2002.
Building alarge-scale annotated Chinese corpus.Proceedings COLING 2002 , Taipei.van Zaanen, M. 2000.
ABL: Alignment-BasedLearning.
Proceedings COLING'2000 ,Saarbr?cken.Zollmann, A. and K. Sima'an 2005.
A consistentand efficient estimator for data-oriented parsing.Journal of Automata, Languages andCombinatorics, in press.Zuidema, W. 2006.
Theoretical evaluation ofestimation methods for data-oriented parsing.Proceedings EACL'2006, Trento.92
