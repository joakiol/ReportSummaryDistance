Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1105?1112Manchester, August 2008Automatic Generation of Parallel TreebanksVentsislav ZhechevNCLT, School of ComputingDublin City UniversityDublin, Irelandvzhechev@computing.dcu.ieAndy WayNCLT, School of ComputingDublin City UniversityDublin, Irelandaway@computing.dcu.ieAbstractThe need for syntactically annotated datafor use in natural language processing hasincreased dramatically in recent years.
Thisis true especially for parallel treebanks, ofwhich very few exist.
The ones that existare mainly hand-crafted and too small forreliable use in data-oriented applications.In this paper we introduce a novel platformfor fast and robust automatic generation ofparallel treebanks.
The software we havedeveloped based on this platform has beenshown to handle large data sets.
We alsopresent evaluation results demonstratingthe quality of the derived treebanks anddiscuss some possible modifications andimprovements that can lead to even betterresults.
We expect the presented platformto help boost research in the field of data-oriented machine translation and lead toadvancements in other fields where paral-lel treebanks can be employed.1 IntroductionIn recent years much effort has been made to makeuse of syntactic information in statistical machinetranslation (MT) systems (Hearne and Way, 2006,Nesson et al, 2006).
This has led to increased in-terest in the development of parallel treebanks asthe source for such syntactic data.
They consistof a parallel corpus, both sides of which havebeen parsed and aligned at the sub-tree level.So far parallel treebanks have been createdmanually or semi-automatically.
This has provento be a laborious and time-consuming task that isprone to errors and inconsistencies (Samuelssonand Volk, 2007).
Because of this, only a few paral-lel treebanks exist and none are of sufficient size forproductive use in any statistical MT application.In this paper we present a novel platform forthe automatic generation of parallel treebanksfrom parallel corpora and discuss several meth-ods for the evaluation of the results.
We discussalgorithms both for cases in which monolingualparsers exist for both languages and for cases inwhich such parsers are not available.
The paralleltreebanks created with the methods described inthis paper can be used by different statistical MTapplications and for translation studies.We start in section 2 by introducing the tech-niques for automatic generation of parallel tree-banks.
The evaluation methods and results areintroduced in section 3 and in section 4 we givesuggestions for possible improvements to thegeneration technology and to the evaluation algo-rithms.
Finally, in section 5 we present existingparallel treebanks and conclude in section 6.2 Automatic Generation ofParallel TreebanksIn this section we introduce a method for the auto-matic generation of parallel treebanks from paral-lel corpora.
The only tool that is required besidesthe software presented in this paper is a wordalignment tool.
Such tools exist and some are freelyavailable (eg.
GIZA++ (Och and Ney, 2003)).
Ifmonolingual phrase-structure parsers1  or at leastPOS taggers exist for both languages, their use forpre-processing the data is highly recommended.In all cases, a word alignment tool is used tofirst obtain word-alignment probabilities for the?
2008.
Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.1 Henceforth, we will use ?parser?
to mean ?monolingual phrase-structure parser?, unless stated otherwise.1105parallel corpus in question for both language di-rections.
We will start with the description of thecase in which parsers are available for both lan-guages, as this is the core of the system.
Theparsers are used to parse both sides of the paral-lel corpus.
The resulting parsed data and word-alignment probability tables are then used as theinput to a sub-tree alignment algorithm that in-troduces links between nodes in correspondingtrees according to their translational equivalencescores.
The output of the sub-tree aligner is thedesired parallel treebank.If there is no parser available for at least oneof the languages, the parallel corpus ?
togetherwith the word-alignment tables ?
is fed directlyto a modified version of the sub-tree aligner.
Inthis modification of the alignment algorithm, allpossible binary phrase-structure trees are hy-pothesised for each sentence in a sentence pair.Afterwards ?
during the induction of alignments?
only those tree nodes are left intact that takepart in the alignments or are necessary for theproduction of connected trees.
Thus, the output isagain a parallel treebank with unambiguousphrase-structure trees for each language side.In the present version of our software, if aparser or a POS tagger exists only for one of thelanguages in the parallel corpus you want towork with, they cannot be made use of.
With thetree-to-tree and string-to-string modules in place,it is a minor task to add a tree-to-string andstring-to-tree modules that will allow for themaximum utilisation of any available resources.We plan to start the development and evaluationof these new modules shortly.We will now look at the currently availablealignment algorithms in greater detail, startingwith the tree-to-tree alignment and then movingon to the string-to-string case.2.1 Tree-to-Tree AlignmentFirst, the tree-to-tree aligner has to follow certainprinciples to fit in the framework described above:?
Independence with respect to language pair,constituent-labelling scheme and POS tagset.
Any language-dependence would re-quire human input to adjust the aligner to anew language pair.?
Preservation of the original tree structures.We regard these structures as accurate en-codings of the languages, and any change tothem might distort the encoded information.?
Dependence on a minimal number of externalresources, so that the aligner can be used evenfor languages with few available resources.?
The word-level alignments should be guidedby links higher up the trees, where morecontext information is available.These principles guarantee the usability of thealgorithm for any language pair in many differentcontexts.
Additionally, there are a few well-formedness criteria that have to be followed toenforce feasible alignments:?
A node in a tree may only be linked once.?
Descendants / ancestors of a source linkednode may only be linked to descendants /ancestors of its target linked counterpart.Links produced according to these criteria en-code enough information to allow the inferenceof complex translational patterns from a paralleltreebank, including some idiosyncratic transla-tional divergences, as discussed in (Hearne et al,2007).
In what follows, a hypothesised alignmentis regarded as incompatible with the existingalignments if it violates any of these criteria.The sub-tree aligner operates on a per sentence-pair basis and each sentence-pair is processed intwo stages.
First, for each possible hypotheticallink between two nodes, a translational equiva-lence score is calculated.
Only the links forwhich a nonzero score is calculated are stored forfurther processing.
Unary productions from theoriginal trees, if available, are collapsed to singlenodes, preserving all labels.
Thus the aligner willconsider a single node ?
instead of severalnodes ?
for the same lexical span.
This does notreduce the power of the aligner, as the transla-tional equivalence scores are based on the sur-face strings and not on the tree structures.During the second stage, the optimal combina-tion of links is selected from among the availablenonzero links.
The selection can be performedusing either a greedy search, or a full search forthe best combination.Translational EquivalenceGiven a tree pair S, T and a hypothesis s, t, wefirst compute the strings in (1), where si?six andtj?tjy denote the terminal sequences dominatedby s and t respectively, and S1?Sm and T1?Tndenote the terminal sequences dominated by S andT.
Here, inside are the strings that represent thespans of the nodes being linked and outside are thestrings that lay outside the spans of those nodes.1106(1)inside outsidesl = si?six sl = S1?si  1six + 1?Smtl = t j?t jy tl = T1?t j  1t jy + 1?Tn(2)  s, t =  sl tl  tl sl  sl tl  tl sl(3)  x y = P xi y jjyyixThe score for the given hypothesis s, t  iscomputed using (2) and (3).
According to theformula in (3), the word-alignment probabilitiesare used to get an average vote by the source to-kens for each target token.
Then the product ofthe votes for the target words gives the alignmentprobability for the two strings.
The final transla-tional equivalence score is the product of thealignment probabilities for the inside and outsidestrings in both language directions as in (2).Greedy-Search AlgorithmThe greedy-search algorithm is very simple.
Theset of nonzero-scoring links is processed itera-tively by linking the highest-scoring hypothesisat each iteration and discarding all hypothesesthat are incompatible with it until the set is empty.Problems arise when there happen to be severalhypotheses that share the same highest score.
Thereare two distinct cases that can be observed here:these top-scoring hypotheses may or may not repre-sent incompatible links.
If all such hypotheses arecompatible, they are all linked at the same time andall remaining unprocessed hypotheses that areincompatible with any of those links are discarded.In case even one among the top-scoring hypothe-ses is incompatible with the others, these hypothe-ses are skipped and processed at a later stage.The sub-tree aligner can be built to use one oftwo possible skipping strategies, which we willcall skip1  and skip2.
According to the skip1 strat-egy, hypotheses are simply skipped until a scoreis reached, for which only one hypothesis exists.This hypothesis is then linked and the selectionalgorithm continues as usual.The skip2 strategy is more complex, in that wealso keep track of which nodes take part in theskipped hypotheses.
Then, when a candidate forlinking is found, it is only linked if it does notinclude any of these nodes.
The motivation be-hind this strategy is that a situation may occur inwhich a low-scoring hypothesis for a given con-stituent is selected in the same iteration ashigher-scoring hypotheses for the same constitu-ent were skipped, thereby preventing one of thecompeting higher-scoring hypotheses from beingselected and resulting in an undesired link.Regardless of whether skip1 or skip2 is used,sometimes a situation occurs in which the onlyhypotheses remaining unprocessed are equallylikely candidates for linking according to the se-lection strategy.
In such ambiguous cases ourdecision is not to link anything, rather than makea decision that might be wrong.During initial testing of the aligner we foundthat often lexical links would get higher scoresthan the non-lexical links,2  which sometimes re-sulted in poor lexical links preventing the selec-tion of bona fide non-lexical ones.
To addressthis issue, an extension to the selection algorithmwas developed, which we call span1.
When en-abled, this extension results in the set of nonzerohypotheses being split in two subsets: one con-taining all hypotheses for lexical links, and onecontaining the hypotheses for non-lexical links.Links are then first selected from the second sub-set, and only when it is exhausted does the selec-tion continue with the lexical one.
This divisiondoes not affect the discarding of incompatiblelinks after linking; incompatible links are dis-carded in whichever set they are found.Full-Search AlgorithmThis is a backtracking recursive algorithm thatenumerates all possible combinations of non-crossing links.
All maximal combinations3  foundduring the search are stored for further process-ing.
After the search is complete, the probabilitymass of each combination is calculated by sum-ming the translational equivalence scores for allthe links in the combination.
The maximal com-bination of non-crossing links that has the high-est probability mass is selected as the best align-ment for the sentence pair.Often, there are several distinct maximal combi-nations that share the highest probability mass; forlonger sentences this number can rise to severalhundred.
The disambiguation strategy that we cur-rently employ is to take the largest common subsetof all maximal combinations.
Another strategywould be to output all possible combinations andmark them as relating to the same sentence pair,thus leaving the disambiguation to the applica-tion that uses the resulting parallel treebank.2 lexical are such links, for which at least one of the linked nodes spans over only one word.
All other links are non-lexical.3 A maximal combination of non-crossing links is a combination of links for which any newly added link would beincompatible with at least one of the links already in the combination.11072.2 String-to-String AlignmentThe string-to-string aligner can accept as its in-put plain or POS-tagged data.
For a pair of sen-tences, all possible binary trees are first con-structed for each sentence.
All nodes in thesetrees have the same label (X) and are then used asavailable link targets.
In the case of POS-taggeddata, the pre-terminal nodes receive the POS tagsas labels.
Here it is obvious that the number oflinks will be much higher than for the sub-treealignment case, so the string-to-string alignerwill operate much more slowly.After all link-hypothesis scores have been cal-culated, the string-to-string aligner continueswith the selection of links in the same manner asthe sub-tree aligner, with one extension; after alink has been selected ?
besides all incompati-ble links ?
all binary trees that do not includethe linked nodes are discarded with any nonzerohypotheses attached to them.
In this way, onlythose binary trees that are compatible with theselected links remain after the linking process.In an additional step for the string-to-stringaligner, all non-linked nodes (except for the rootnodes) are discarded, thus allowing for the construc-tion of unambiguous n-ary trees for the source andtarget sentences.
If necessary, non-linked nodes areleft intact to provide supporting structure in the trees.3 Evaluation and ResultsThe quality of a parallel treebank depends directlyon the quality of the sub-tree alignments that itcontains.
Because of this, we use the evaluationresults mainly as a metric for the improvementsin the sub-tree aligner during development.
Ofcourse, the evaluation presented in this sectionalso presents an insight into the usability of theparallel treebanks produced using our method.For the evaluation of the aligner, a battery of in-trinsic and extrinsic tests was developed.
As a refer-ence for the tests, a hand-crafted parallel treebankwas used (HomeCentre (Hearne and Way, 2006)).This treebank consists of 810 English?French sen-tence pairs.
As discussed in section 5, we are notaware of an existing parallel treebank besides theHomeCentre that can be used directly for crossevaluation and comparison to versions automati-cally generated using the sub-tree aligner.The word-alignment probabilities required byour system were obtained by running the Mosesdecoder4 (Koehn et al, 2007) on the plain sentencesfrom the HomeCentre in both language directions.We will first describe the intrinsic testing andthen go into the details of the extrinsic evaluation.3.1 Intrinsic EvaluationThe intrinsic evaluation is performed by compar-ing the links induced by the automatic aligner tothe manually annotated links in the HomeCentretreebank.
This evaluation can only be performedfor the result of the tree-to-tree alignment, as thestring-to-string alignment produces differenttrees.
The metrics used for the comparison areprecision and recall for all alignments and lexicaland non-lexical alignments alone.
The results ofthe evaluation are shown in Table 1.5all links lexical links non-lexical linksConfigura-tionsskip1skip2skip1_span1skip2_span1preci-sionrecall preci-sionrecall preci-sionrecall61,29% 77,46% 51,06% 79,99% 80,75% 75,69%61,54% 77,50% 51,29% 80,03% 80,75% 75,70%61,56% 78,44% 51,53% 80,51% 78,67% 77,22%61,79% 78,49% 51,76% 80,60% 78,73% 77,22%Table 1.
Intrinsic evaluation resultsLooking first to the all links column, it is imme-diately apparent that recall is significantly higherthan precision for all configurations.
In fact, allaligner variations consistently induce on averagetwo more links than exist in the manual version.Considering the lexical links and non-lexical linkscolumns, apparently the bulk of the automaticallyinduced links that do not occur in the manual an-notation are at the lexical level, as attested by thelow precision at the lexical level and balancedprecision and recall at the non-lexical level.If the manual alignments in the HomeCentreare regarded as a gold standard, it would seemthat fewer lexical links should be produced,while the quality of the non-lexical links needsimprovement.
We will try to judge whether thisis really the case using the extrinsic evaluationtechniques described below.3.2 Extrinsic EvaluationFor extrinsic evaluation, we trained and tested aDOT system (Hearne and Way, 2006) using themanually aligned HomeCentre treebank andevaluated the output translations to acquire base-line scores.
We then trained the system on theautomatically generated treebank and repeated4 We found that using the Moses word-alignment probabilities yielded better results than those output directly by GIZA++.5 Throughout the paper we use boldface to highlight the best results and italics for the worst.1108the same tests, such that the only differenceacross runs are the alignments.For testing, we used the six English?Frenchtraining / test splits for the HomeCentre used in(Hearne and Way, 2006).
Each test set contains 80test sentences and each training set contains 730tree pairs.
We evaluated the translation outputusing three automatic evaluation metrics: BLEU(Papineni et al, 2002), NIST (Doddington, 2002)and METEOR (Banerjee and Lavie, 2005).
Weaveraged the results over the six splits.
We alsomeasured test-data coverage of the translation sys-tem, i.e.
the percentage of test sentences for whichfull trees were generated during translation.We performed this evaluation using both thetree-to-tree algorithm and the string-to-stringalgorithm, employing greedy-search selection.For the latter case we extracted POS-tagged sen-tences from the HomeCentre and used them asinput for the aligner.
The results for the tree-to-tree case are presented in Table 2 and for thestring-to-string case in Table 3.Configurations BLEU NIST METEOR Coveragemanualskip1skip2skip1_span1skip2_span10,5222 6,8931 71,8531% 68,5417%0,5236 6,8412 72,2485% 72,0833%0,5233 6,8617 72,2847% 71,8750%0,5296 6,8570 72,9833% 72,0833%0,5334 6,9210 72,9736% 71,8750%Table 2.
Tree-to-tree extrinsic evaluationLet us first look at the results from the tree-to-tree aligner.
Overall, the scores obtained whenusing the manual alignments are very competitivewith those derived using the manually aligneddata.
In fact, NIST is the only metric for which theperformance is below the baseline.
An importantobservation is that the coverage of the translationsystem is up to 3.5% higher when using the auto-matic alignments.
Another observation is that skip2leads to better performance on the NIST metricover skip1, but the results from the other metrics arenot so conclusive.
The use of span1 leads to bettertranslation scores.
Ths results seem to point atthe skip1_span1 and skip2_span1 configurationsas the best-suited for further development.Unexpectedly, the results of the extrinsicevaluation do not strictly follow the trends foundin the intrinsic evaluation.
Further analysis of thedata revealed that direct comparison of the man-ual and automatic alignments is not appropriate,especially regarding the lexical alignments.
Themanual alignments were produced with the aimof maximising precision, but the coverage-basedautomatic alignments lead to higher translationscores.
This is the result of having many fewermanual word-alignments than automatic ones, asthe low precision scores in the intrinsic evalua-tion show.
From this we conclude that the im-provement of the automatic aligner should not beaimed at better matching the manual alignments,but rather at improving the quality of the transla-tions produced using the automatic alignments.Configurations BLEU NIST METEOR Coveragemanualskip1skip2skip1_span1skip2_span10,5222 6,8931 71,8531% 68,5417%0,4939 6,6321 72,5192% 92,5000%0,4886 6,5777 72,8241% 92,2917%0,4661 6,3090 73,1017% 92,2917%0,4683 6,3353 73,2828% 92,2917%Table 3.
String-to-string extrinsic evaluationIf we now look at the evaluation of the string-to-string aligner, we see quite peculiar results.There is more than 20% increase in coveragecompared to the tree-to-tree aligner, but the onlyother metric that sees improvement ?
albeitmodest ?
is METEOR.
It is also the only metricthat follows the trends observed in the tree-to-tree evaluation results.
Not only are the resultsfor the BLEU and NIST metrics lower, but theyalso seem to follow reversed trends.
It is unclearwhat the reason for such an outcome is, and fur-ther investigation ?
including on other data sets?
is needed.
Still, as far as the METEOR metricis concerned, the use of the string-to-string algo-rithm for the generation of parallel treebanksseems to be warranted.The results obtained from the intrinsic and ex-trinsic evaluations show that the methods de-scribed in this paper produce high quality paralleltreebanks.
Using the automatically generated tree-banks, a DOT system produces results with simi-lar translation quality and better coverage com-pared to its performance using manually aligneddata.
This makes our methods a good alternativeto the manual construction of parallel treebanks.3.3 Using the Full-Search Algorithmas an Evaluation MetricThe full-search selection algorithm is combinato-rial in nature and for sentence pairs with morethan 100 nonzero link hypotheses its time re-quirements become prohibitive.
Still, this algo-rithm can be used in its current form for devel-opment purposes.It is reasonable to ask whether the greedy-searchalgorithm produces the best set of alignments fora given sentence pair.
It could be that it picks alocal maximum differing greatly from the absolutemaximal set of alignments, thus producing eitherlow quality links or a small number of links.1109The full-search selection algorithm can be usedto test the performance of the greedy search, as it bydefinition produces the best available set of align-ments.
We decided to use the rate of coincidencebetween the alignments induced using both selec-tion algorithms as a metric for the quality of thelinks derived using the greedy search: the higher thenumber of cases in which the greedy-search algo-rithm matches the result of the full-search algo-rithm, the better the quality of the greedy search.We ran this coincidence evaluation for all fourconfigurations of the aligner.
The results are pre-sented in Table 4.
It should be noted that 30 sen-tence pairs from the HomeCentre could not behandled by the full-search algorithm within areasonable timeframe and were skipped.all links lexical links non-lexical linksConfigura-tionsskip1skip2skip1_span1skip2_span1preci-sionrecall preci-sionrecall preci-sionrecall98,71% 99,18% 98,36% 99,14% 99,57% 99,21%99,23% 99,21% 99,06% 99,17% 99,57% 99,23%95.78% 97,00% 95.92% 96,33% 95.19% 99,21%96,27% 97,09% 96,58% 96,44% 95,25% 99,21%Table 4.
Evaluation against full-search resultsThe outcome of this test seems to be unex-pected and a little disconcerting in view of theresults obtained from the extrinsic evaluation.
Itdoes not seem reasonable that the configurationsincluding span1 should obtain scores that arerelatively much worse than the scores for theother configurations, when we saw them performbetter at the extrinsic evaluation tests.The reason for this discrepancy might not beobvious, but it is fairly simple and lies in the na-ture of the span1 extension.
As discussed in sec-tion 2.1, span1 introduces a separation in the in-duction of lexical and non-lexical links.
The full-search algorithm, however, derives the maximallink set from a common pool of all nonzeroalignment hypotheses.
This suggests that an ex-tension to the full-search algorithm similar tospan1 should be developed to allow for theevaluation of configurations using this feature.Nevertheless, this evaluation shows some veryimportant results.
Besides the fact that configura-tions using skip2 perform slightly better thanthose using skip1, we see that the greedy searchcomes very close to the best maximal link set.Our tests show that in over 95% of the cases thegreedy search finds the best maximal link setavailable for the particular sentence pair.The results are very encouraging and showthat the fast greedy-search algorithm producesthe desired results and there is no need to use theprohibitively slow full-search algorithm, exceptfor comparison purposes.4 A Review of Possible EnhancementsHere, we discuss possible avenues for the im-provement of the quality of the parallel treebanksproduced using the methods presented in this paper.As already stated in section 3, the quality of aparallel treebank is to be judged by the quality ofthe induced sub-tree alignments.
Thus, all effortshould be directed at producing better alignments.There are two possible ways to address this: oneoption is to work on improving the alignmentalgorithm, and the other option is to improve thescoring mechanism used by the aligner.Improvements to the alignment algorithm canbe evaluated against the full-search selection al-gorithm.
The evaluation results from section 3.3suggest, however, that the margin for improve-ment here is very small.
Thus, we do not expect anyimprovements here to bring serious boosts in over-all performance.
Nevertheless, we plan to investi-gate one possible modification to the greedy search.It can be argued that each newly induced link ina sentence pair should affect the decisions regard-ing which links to select further in the alignmentprocess for this sentence pair.
This can be simulatedto a certain extent by the introduction of a simplere-scoring module to the aligner.
Each time a newlink has been selected, this module will be used torecalculate the scores of the remaining links, con-sidering the restrictions on the possible word-levelalignments introduced by this link, e.g.
that wordswithin the spans of the nodes being linked cannotbe aligned to words outside those spans.The effects of changes to the scoring mecha-nism used can only be evaluated using extrinsicmethods, as such changes also influence the op-eration of the full-search selection.
On this front,we plan to investigate a maximum-entropy-basedscoring mechanism.
We expect such a mecha-nism to better encode mathematically the de-pendence of the translational equivalence scoreson the word-alignment probabilities.Besides the improvements to the sub-treealigner, we plan to extend the whole generationframework with two additional modules: forstring-to-tree and tree-to-string alignment.
Thiswould allow for better utilisation of all availableresources for the derivation of a parallel treebankfrom a parallel corpus.We also plan to perform large-scale extrinsicevaluation experiments.
Though the evaluation re-sults presented in section 3 are very promising, they1110were performed on a very small set of data.
(JohnTinsley (p.c.)
reports successfully deriving a paral-lel treebank with over 700 000 sentence-pairs usingour software.)
Further experiments on larger datasets ?
from different languages, as well as fromdifferent domains ?
should help better understandthe real qualities of the methods presented here.5 Existing Parallel TreebanksIn this section we look at several attempts at thecreation of parallel treebanks besides the Home-Centre treebank presented earlier.Closest to the material presented in this papercomes the parallel treebank presented in (Sam-uelsson and Volk, 2006).
This manually createdtreebank aligns three languages ?
German, Eng-lish and Swedish ?
consisting of over 1000 sen-tences from each language.
The main differencecompared to our method is that they allow many-to-many lexical alignments and one-to-many non-lexical alignments.
The authors also allow unaryproductions in the trees, which, as stated in section2.1, does not provide any additional useful infor-mation.
Another difference is that they deepen theoriginal German and Swedish trees before align-ment, rather than preserve their original form.A further attempt to align phrase-structuretrees is presented in (Uibo et al, 2005).
Theauthors develop a rule-based method for aligningEstonian and German sentences.
The paralleltreebank consist of over 500 sentences, but in theversion presented only NPs are aligned.In (Han et al, 2002) the authors claim to have builta Korean?English parallel treebank with over 5000phrase-structure tree pairs, but at the time of writingwe were unable to find details about this treebank.Although the Prague Czech?English Depend-ency Treebank (PCEDT (mejrek et al, 2004)) canbe used as a parallel treebank, it is not such per se.The authors do not use phrase-structure trees.
In-stead, tectogrammatical dependency structures areused (Hajiov?, 2000).
Either a word alignmenttool like GIZA++ or a probabilistic electronic dic-tionary (supplied with the treebank) can be used toautomatically align the dependency structures.
Thepresented version contains over 21000 sentencepairs that can be aligned.
Because of its nature, thistreebank can only be used by MT systems that em-ploy tectogrammatical dependency structures.We are also aware of the existence of the LinES(Ahrenberg, 2007), CroCo (Hansen-Schirra et al,2006) and FuSe (Cyrus, 2006) parallel corpora.Although it seems possible to use them as paralleltreebanks, they have been designed to serve asresources for the study of translational phenomenaand it does not appear that they can be used effec-tively for other natural language processing tasks.An attempt to develop an automatic tree-to-tree aligner is described in (Groves et al, 2004).The authors present a promising rule-based sys-tem.
Further testing, however, has shown that therules are only applicable to a particular treebankand language pair.
This means that the set ofrules has to be adjusted for each particular case.Thus, the methods presented in this paper arethe only available ones that can be used to pro-duce a sufficiently large parallel treebank appro-priate for use by state-of-the-art statistical MTapplications (eg.
DOT (Hearne and Way, 2006)).66 ConclusionsWe have presented a novel platform for the fastand robust automatic generation of parallel tree-banks.
The algorithms described are completelylanguage-pair-independent and require a minimalnumber of resources; besides a parallel corpus, aword alignment tool is the only extra softwarerequired.
If available, POS taggers or monolin-gual phrase-structure parsers can be used to pre-process the data.
Certain extensions to the cur-rent software are planned that will assure the op-timal use of any available resources.A series of evaluations have shown promisingresults.
The quality of the automatically generatedparallel treebanks is very high, even improving ona manually created treebank on certain metrics.We plan to carry out extensive large-scale testingon a range of language pairs, which we expect tocorroborate the results reported in this paper.
Theplanned improvements to the algorithms discussedin section 4 are expected to further increase thequality of the generated parallel treebanks.Currently existing treebanks are small and re-quire extensive human resources to be created andextended, which has limited their use for data-oriented tasks.
The platform presented in this pa-per provides a means to circumvent these prob-lems by allowing for the fast automatic genera-tion of very large parallel treebanks with very littlehuman effort, thus overcoming this hurdle forresearch in tree-based machine translation.6 An alternative methodology is described in (Lavie et al, to appear), but this work was not available at the time of writing.1111AcknowledgementsWe would like to thank Khalil Sima'an, MaryHearne and John Tinsley for many insightful dis-cussions.
This work was generously supported byScience Foundation Ireland (grant no.
05/RF/CMS064) and the Irish Centre for High-EndComputing (http://www.ichec.ie).ReferencesAhrenberg, Lars.
2007.
LinES: An English-SwedishParallel Treebank.
In Proceedings of the 16thNordic Conference of Computational Linguistics(NODALIDA ?07), pp.
270?274.
Tartu, Estonia.Banerjee, Satanjeev and Alon Lavie.
2005.METEOR: An Automatic Metric for MTEvaluation with Improved Correlation withHuman Judgements.
In Proceedings of theWorkshop on Intrinsic and Extrinsic Evalua-tion Measures for MT and/or Summarizationat the 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL ?05),pp.
65?72.
Ann Arbor, MI.mejrek, Martin, Jan Cu?n, Ji?
Havelka, JanHaji and Vladislav Kubo	.
2004.
PragueCzech-English Dependency Treebank: Syntac-tically Annotated Resources for MachineTranslation.
In Proceedings of the 4th Interna-tional Conference on Language Resources andEvaluation (LREC?04).
Lisbon, Portugal.Cyrus, Lea.
2006.
Building a resource for studyingtranslation shifts.
In Proceedings of the 5th Con-ference of Language Resources and Evaluation(LREC ?06), pp.1240?1245.
Genoa, Italy.Doddington, George.
2002.
Automatic Evaluationof Machine Translation Quality Using N-GramCo-Occurrence Statistics.
In Proceedings of theARPA Workshop on Human Language Technol-ogy, pp.
128?132.
San-Diego, CA.Groves, Declan, Mary Hearne and Andy Way.2004.
Robust Sub-Sentential Alignment ofPhrase-Structure Trees.
In Proceedings of the20th International Conference on Computa-tional Linguistics (CoLing?04), pp.
1072?1078.Geneva, Switzerland: COLING.Hajiov?, Eva.
2000.
Dependency-Based Underlying-Structure Tagging of a Very Large Czech Corpus.TAL (Special Issue Grammaires de D?pendance /Dependency Grammars), 41 (1): 47?66.Han, Chung-hye, Na-Rare Han, Eon-Suk Ko andMartha Palmer.
2002.
Development andEvaluation of a Korean Treebank and its Appli-cation to NLP.
In Proceedings of the 3rd Inter-national Conference on Language Resourcesand Evaluation (LREC  ?02), pp.
1635?1642.Las Palmas, Canary Islands, Spain.Hansen-Schirra, Silvia, Stella Neumann and Mi-haela Vela.
2006.
Multi-dimensional Annota-tion and Alignment in an English-GermanTranslation Corpus.
In Proceedings of theworkshop on Multi-dimensional Markup inNatural Language Processing (NLPXML ?06),pp.
35?42.
Trento, Italy.Hearne, Mary and Andy Way.
2006.
Disambigua-tion Strategies for Data-Oriented Translation.In Proceedings of the 11th Conference of theEuropean Association for Machine Translation(EAMT?06), pp.
59?68.
Oslo, Norway.Hearne, Mary, John Tinsley, Ventsislav Zhechev andAndy Way.
2007.
Capturing Translational Diver-gences with a Statistical Tree-to-Tree Aligner.
InProceedings of the 11th International Conferenceon Theoretical and Methodological Issues in Ma-chine Translation (TMI ?07), pp.
85?94.
Sk?vde,Sweden: Sk?vde University Studies in Informatics.Koehn, Philipp, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Ber-toldi, Brooke Cowan, Wade Shen, Christine Mo-ran, Richard Zens, Chris Dyer, Ondej Bojar, Al-exandra Constantin and Evan Herbst.
2007.Moses: Open Source Toolkit for Statistical Ma-chine Translation.
In Proceedings of the Demoand Poster Sessions of the 45th Annual Meeting ofthe Association for Computational Linguistics(ACL ?07), pp.
177?180.
Prague, Czech Republic.Lavie, Alon, Alok Parlikar and Vamshi Ambati.
toappear.
Syntax-driven Learning of Sub-sententialTranslation Equivalents and Translation Rulesfrom Parsed Parallel Corpora.
In Proceedings ofthe 2nd Workshop on Syntax and Structure in Sta-tistical Translation (SSST?08).
Columbus, OH.Nesson, Rebecca, Stuart M. Shieber and AlexanderRush.
2006.
Induction of Probabilistic Synchro-nous Tree-Insertion Grammars for MachineTranslation.
In Proceedings of the 7th Conferenceof the Association for Machine Translation in theAmericas (AMTA?06), pp.
128?137.
Boston, MA.Och, Franz Josef and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29 (1): 19?51.Papineni, Kishore, Salim Roukos, Todd Wardand Wei-Jing Zhu.
2002.
BLEU: A Method forAutomatic Evaluation of Machine Translation.In Proceedings of the 40th Annual Meeting ofthe Association of Computational Linguistics(ACL ?02), pp.
311?318.
Philadelphia, PA.Samuelsson, Yvonne and Martin Volk.
2006.Phrase Alignment in Parallel Treebanks.
InProceedings of the 5th Workshop on Treebanksand Linguistic Theories (TLT ?06), pp.
91?102.Prague, Czech Republic.Samuelsson, Yvonne and Martin Volk.
2007.Alignment Tools for Parallel Treebanks.
InProceedings of the GLDV Fr?hjahrstaggung.T?bingen, Germany.Uibo, Heli, Krista Liin and Martin Volk.
2005.
Phrasealignment of Estonian-German parallel treebanks.Paper presented at Workshop ?Exploiting parallelcorpora in up to 20 languages?, Arona, Italy.1112
