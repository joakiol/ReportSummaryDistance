A Chinese LPCFG Parser with Hybrid Character InformationWenzhi Xu, Chaobo Sun and Caixia YuanSchool of Computer,Beijing University of Posts and Telecommunications,Beijing, 100876 China{ earl808, sunchaobo}@gmail.comyuancx@bupt.edu.cnAbstractWe present a new probabilistic modelbased on the lexical PCFG model, whichcan easily utilize the Chinese character in-formation to solve the lexical informationsparseness in lexical PCFG model.
Wediscuss in particular some important fea-tures that can improve the parsing perfor-mance, and describe the strategy of mod-ifying original label structure to reducethe label ambiguities.
Final experimentdemonstrates that the character informa-tion and label modification improve theparsing performance.1 IntroductionParsing is an important and fundamental task innatural language processing.
The challenge ofChinese parser has been the focus of attention inrecent years, and many different kinds of Chi-nese parsing models are investigated.
(Bikel,2000) adopts Head-Driven model to parse Chi-nese.
(Levy, 2003) analyzes the difficulties ofChinese parsing through comparing the differ-ences between Chinese and English.
(Wang,2006) utilizes shift-reduce approach, dramaticallyimproved the decoding speed of parsing.
All theseresearch adopted the same models which are alsoused in English parser ?
the models based on thewords.However, there is a big difference between En-glish and Chinese: the expressing unit in Englishis word, while character is the smallest unit inChinese.
Due to difficulties of word segmenta-tion, especially for different segmenting criteria,many researchers explored parsing Chinese basedon characters.
The parser of (Luo, 2003) receivedsentence as input and conducted word segmenta-tion and syntactic parsing at the same time, butthey did not utilize the character information ingenerating subtree; (Zhao, 2009)?s dependencyparsing tree totally abandoned the word concept,so the dependency relations are the relations be-tween characters.We combine both word and character informa-tion to gain better performance of parsing.
Al-though the criteria of segmentation are difficult tobe unified, different criteria conflict only withinthe phrases which have little influence on thestructure between phrases.
So we still use wordas our basic unit of parsing.
Although wordhas been proved to be effective in head-drivenparser (Collins,1999), the data of word depen-dence is very sparse.
While it is worthy to notethat words with similar concept always share thesame characters in Chinese.
For instance, ???
[(scientist)?, ?{??
[(historian)?, etc., sharethe same character ?
[(expert)?, since they belongto the same concept ?expert in a certain field?.
Sothe problem of word sparseness can be solved bycombining the character information to some ex-tent.Throughout this paper, we use TCT Treebank(Zhou, 2004) as experimental data.
TCT mainlyconsists of binary trees, with a few of multi-branch and single-branch trees.
Thus, we firsttransfer all trees to binary trees.
Then we useLexical-PCFG model to exploit the word andcharacter information, and Maximum EntropyModel to calculate the probability of induced treesas (Charinak, 2000).
Finally we use CKY-baseddecoder.In the following section, we will introduce howto utilize character information in our parsingmodel and the other features in detail.
Section 3gives experiment results and analysis, which showimprovement of our parsing approach.
Section 4presents the conclusion and future work.2 Lexical PCFG model2.1 Model IntroductionStarting from the Lexical-PCFG model (Model 2in Collins, 1999), we propose a new generativeprocess which can conveniently exploit the char-acter information and other features.Assume P is the label of parent, H is the headchild of the rule, and L1, ..., Ln and R1, ..., Rn isthe left and right modifiers of H .
Then the rule ofLexical-PCFG (LPCFG) can be written as:P (hw, ht) ?
(1)Ln(lwn, ltn)...L1(lw1, lt1)H(hw, ht)R1(rw1, rt1)...Rn(rwn, rtn),where (hw, ht) represents the head word andhead tag of head child, (lw1, lt1), ..., (lwn, ltn)and (rw1, rt1), ..., (rwn, rtn) are the head wordsand head tags of left and right modifiers, and par-ent node P ?s head word and tag are the same asthat of H .As mentioned above, our trees are all binarytrees.
In this case, the LPCFG can be written as:P (hw, ht)?
H(hw, ht)R(rw, rt), (2)P (hw, ht)?
L(lw, lt)H(hw, ht).
(3)Formula 2 and 3 represent that the head child isthe left or right child respectively.
The probabilityof the rule is conditioned on the head words andtags of head and its modified children, which isspecified as:Pr(P,L,H|hw, ht, lw, lt), (4)andPr(P,R,H|hw, ht, rw, rt).
(5)To calculate these probabilities, we rewriteEquation 4 and 5 by three factors in 6 and 7 us-ing the chain rule.Pr(P,R,H|hw, ht, lw, lt) = (6)Prd(P ?DIR|hw, ht, lw, lt) ?
Prh(H|P, hw, ht)?Prm(L?DIR|P,H, hw, ht),P r(P,R,H|hw, ht, rw, rt) = (7)Prd(P ?DIR|hw, ht, rw, rt) ?
Prh(H|P, hw, ht)?Prm(R?DIR|P,H, hw, ht).in which, DIR=LEFT/RIGHT.
DIR in P-DIRis used to discriminate different positions of headchild, DIR in L-DIR and R-DIR are used to repre-sent different positions of modifiers.The calculation processes of Equation 6 and 7can be interpreted by following generative pro-cess.
Firstly, the head words and tags of chil-dren generate the parent and the head position (thefirst probability in Equation 6 and 7).
We definethis probability as the word dependency probabil-ity Prd: if two words (or characters in words)always appear together in the training data, thisprobability will be large (< 1); if two words (orcharacters in words) do not have any dependencein the training data, this probability will be ap-proximately equal to 1/|Y |, where |Y | is the pre-dicted number of the Prd.
The second probabilitygenerates the head child label (defined as the headchild probability Prh), we hold that the head wordand tag of modifier do not provide information todetermine the head child label, so we omit them.The third one produces the modifier label, whichis defined as the modifier probability Prm, andevaluates the dependency relation between modi-fier and the head child.
We also omit the influenceof the head word and tag of modifier.For example, assume there is a tree as shownin Figure 1.
For the rule ?vp ?
v np?, headchild of parent vp and np are the left child v andright child n respectively, so ?|?(organize)?
and?;[(expert)?
are the head word of vp and np.Thus the LPCFG rule is ?vp(|?,v)?
v(|?,v)Figure 1: Tree representation of LPCFG rule.np(;[,n)?.
The probability can be written as:Prd(vp-LEFT | |?,v,;[,n) ?
Prh(v|vp,|?,v) ?
Prm(np-RIGHT | vp, v,|?,v).2.2 Probability Model and Feature SetWe use Maximum Entropy (ME) Model to com-pute probabilities of candidate trees.
ME modelestimate parameters that would maximize the en-tropy over distributions, meanwhile satisfy certainconstraints.
These constraints will force the modelto reflect characteristic of training data.
With thefeature function, Maximum Entropy can exploitkinds of features flexibly, some of which are veryimportant to improve the performance of tasks athand.
ME model has been applied successfullyin many tasks, such as parser (Charniak, 2000;Luo, 2003), POS tagging (Ratnaparkhi,1996), etc.In our experiment, we use Maxent toolkit de-velopped by Zhang (Zhang, 2004), which usesthe LBFGS algorithm for parameter estimation.Details of the model and toolkit can be seen in(Berger, 1996; Zhang, 2004).Our features consist of four parts: basic fea-tures, character features, context features andoverlapping features of character and context.
Ba-sic features are traditional LPCFG features, in-cluding head word, head tag and the label.
Weextract the first and last characters of a word as theCharacter features, of course for a single characterword the first and last character are the same.
Con-text features are defined as the previous and fol-lowing POS tags of the current subtree, and thesefeatures utilize the information outside of the sub-tree very well without increasing the complex-ity of parsing decoder.
Overlapping features arethe combinations of character features and contextfeatures.Take Chinese sentence ??
?/n d/p ??
?/n|?/vk'/b;[/n|?/v?/v (Com-mittee is composed of experts organized by theMinistry of Agriculture)?
for example, the corre-sponding rule is ?vp(|?,v) ?
v(|?,v) np(;[,n)?, the feature template of the example sen-tence is shown in Table 1.When applying the character information, it isworthwhile to note that character is always com-bined with the POS tag of the word since thesense of single character varies as word?s POS tagchanges.
For example, the sense of ?O(love)?in verb ?Oo(care and protect)?
and noun ?O?(love)?
is different.
Of course the sense dis-cussed here is reflected in the dependency ofwords: ?Oo(care and protect)?
can be followedby some nouns which are objects, while ?O?(love)?
can not.For the multi-branch tree, (Collins,1999) calcu-lates the probability of the left or right modifierwith a feature which represent whether there aremodifiers between current modifier and the headchild (distance feature).
But in the situation ofbinary tree, it is obvious that current modifier isunlikely to follow other modifiers.
Since the rep-resentation of binary tree conforms to the X-bartheory of Chomsky, we can modify the head childlabel to get this non-local information in binarytree.
For instance, a multi-branche tree rule ?vp1?
pp d vp2?
corresponding to these two binarytree rules: ?vp3 ?
pp vp4?
and ?vp4 ?
d vp5?
(the index numbers of the vb here stand for dif-ferent vp).
So when we calculate the probabilityof pp with the multi-branch situations, d lies be-tween pp and vp2.
While in binary tree situation,we cannot catch this information between pp andvp4.
However, we can modify vp4 to vp-LEFT,which means there is a modifier at the left childof vp4, then we get the similar effect in (Collins,1999).
We call this as the head position labeling.2.3 Label splitting and Head PositionModifying(Klein, 2003) improves the performance of parservia splitting the POS tag in corpus.
We split thenon-terminal label using the same approaches (as-suming the POS tag is terminal label).
The needof label splitting is that the corpus does not suffi-ciently consider different situations and treat themTable 1: Feature templates and symbol explanation.Prd (vp-LEFT) Prh (v) Prm (np-RIGHT)basic lw rw |?
;[ p vp p h vp vfeature lw lt rw rt |?v;[n p hw vp|?
p h hw vp v|?p hw ht vp|?v p h hw ht vp v|?vp ht vp v p h ht vp v vchar.
lw frc rt |?
;n p fhc ht vp|v p h fhc ht vp v|vfeature other combinations , p lhc ht vp?v p h lhc ht vp v?vflc lt frc rt |v;nother combinations ,context lw rw pt1 at1 |?
;[n v p pt1 vp n p h pt1 vp v nfeature lw lt rw rt pt1 at1 |?v;[n n v p pt1 pt2 vp n p p h pt1 pt2 vp v n pp at1 vp v p h at1 vp v vp at1 at2 vp v v p h at1 at2 vp v v vp pt1 at1 vp n v p h pt1 at1 vp v n vp hw pt1 at1 vp|?v n p h hw pt1 at1 vp v|?v noverlap lw frc rt pt1 at1 |?
;n n v p fhc ht pt1 at1 vp|v n v p h fhc ht pt1 at1 vp v|v n vfeature other combinations ... p lhc ht pt1 at1 vp?v n v p h lhc ht pt1 at1 vp v?v n vflc lt frc rt pt1 at1 |v;n n vother combinations ...Symbol Explanationfrc lrc the first and last characters of the head word of the right childpt at previous and following POS tag of current subtree, number indicates the positionflc llc the first and last characters of the head word of the left childfhc lhc the first and last characters of the head wordas the same label which results in ambiguity.
Fur-thermore, in our experiment, the corpus that weadopt is binary tree.
Though the rule set in binarytree is closed, it brings stronger independent as-sumption (Jonson,1998).
Thus splitting the labelcan make the node label represent more informa-tion from descendants.
Just like the intuition ofhead position labeling, this is also one method toutilize the non-local information.
We mainly con-sider these modifying as follows.First of all, we split the label vp.
There arethree kinds of verb phrases: the first one is thatthere is modifier ahead (such as advp); the secondphrase consists of an object; while the third onehas the form of two verbs or verb plus an auxil-iary word.
The formal two situations can not fol-low any object any more (some double-object verbphrase may be continued to contain object, buttheir POS label is different with common verb),the vp in last situation can be followed by object(there maybe actually no object).
If we do not dis-criminate these situations, it will be easy to resultin dividing the object into two objects during pars-ing test, just as shown in Figure 2.
However, ifwe modify vp in the third situation into vb, thenthis difference can be discriminated well.
We takea simple statistics as an example to illustrate thesense.
Assume our object is np, rule ?vp ?
vpnp?
appears for 5,284 times in corpus before mod-ifying, while it present only 166 times after mod-ifying.Figure 2: Parsing Result Example: (a) is a correcttree, (b) is a wrong one, while the probability maybe not small enough, (c) is also wrong, but theprobability is very small due to the symbol vb.Secondly, we also split the np tag.
We noticethat a noun phrase, which consists of non-noun(phrase) modifier (such as ADJP, PP) and a noun(phrase), is always the final noun phrase but rarelypart of another noun phrase.
So we transform thenp, which has the non-noun (phrase) modifier, tonm.
From the statistics of corpus, we find rule ?np?
np n?
occurs for 4,502 times, while ?np?
nmn?
only appears 826 times.Finally, we change the head position of prepo-sition phrase.
The head position of prepositionphrases in corpus mostly is the phrase behind thepreposition, but we found the grammar of prepo-sition phrase is much related to the preposition.Take the preposition ??(by)?
and ??(to)?
as ex-ample, these two prepositions occur for 755 and1,300 times respectively.
In our corpus, 98.7% ofpreposition phrases with?(by)?
are the modifiersof verb phrases, while only 57.2% of phrases with??(to)?
appear as the modifiers of verb phrases,and the remaining 42.8% are the modifiers of nounphrases.3 Experiment Result and AnalysisOur experiments are conducted on the TCT cor-pus, which is used as the standard data of theCIPS-SIGHAN Parser 2010 bakeoff.
We omit thesentences with length 1 during training and test-ing.
Performance on the test corpus is evaluatedwith the standard measures from (SIGHAN RE-PORT, 2010).We submit two results for the parsing bakeoff:one is single model we described in Section 2, an-other is reranking model, which is an attempt toapply a perceptron algorithm to rerank the 50-bestresult produced by the ME model.1 Table 2 showsthe result of our parser compared with the top onein this bakeoff.
Since the parser we built is strictlydependent on the POS tags, the precision of POStagging has a harsh effect on the overall parsingperformance.The performance of the rerank model is lightlylower than that of the single model.
The mostlikely reason is that the features we count onare far from enough, and the informative featuresproved to be useful in (Charniak and Johnson,2003) are not yet included in our discriminativeranker.
Besides, the rank model we used is asimple perceptron learner, more delicated model,such as ME model used in (Charniak and Johnson,1More details can be found in (Charniak and Johnson,2003; Huang, 2008).
The features we used include Paren-tRule, RightBranch, Rule, Heads, WProj described in (Char-niak and Johnson, 2003).Table 3: Results of different features with no limitsentence length.feature set LR LP F CB 0CB 2CBbasic 80.19 79.61 79.90 1.20 56.10 83.49+ch 81.91 81.38.
81.65 1.10 58.34 84.95+cont 85.53 85.34 85.44 0.83 65.62 88.86+ch + cont 86.17 85.94 86.06 0.80 66.61 89.62+ch + cont + ol 86.34 86.13 86.24 0.79 66.65 89.81+ch + cont + ol + cwd 86.47 86.26 86.37 0.78 66.73 89.87+ch + cont + ol + cwd + cm 87.03 86.77 86.90 0.75 67.06 90.36+ch + cont + ol + cwd + cm + hpl 87.20 86.94 87.07 0.74 67.43 90.40ch=character feature, cont=context featureol=overlap feature, cwd=coordinate word dependencecm=corpus modifying, hpl= head position label2003), might improve the result.In order to make clear how different featureseffect the parser performance, we conducted ex-periments on the TCT data provided by CIPS-ParEval-2009 for Chinese parser bakeoff 2, sincethe sentences in CIPSParEval-2009 are given withhead words and gold-standard POS tags.
The re-sults of our parser are given in Table 3.
From Ta-ble 3 we can see that character features bring theimprovement of F score for 1.75 compared withthe basic features (line 2 vs line 3), and for 0.8after adding the context features (line 4 vs line6).
These results show that character features canimprove the model with basic features very well.After applying the context features, character fea-tures can still bring improvement, which statesthat character features can solve the ambiguitiesthat can not be solved by the context features.One likely reason why character information ishelpful is that the character can partly representthe meaning of word and can partly resolve thesparseness problem of word dependence as beenobserved in the work of (Kang, 2005).
Kang cal-culated the statistics for 50,000 double characterswords and divided the methods of constructingword into 8 types according to the relations ofmeaning between word and characters:(1) A+B=A=B (2) A+B=A(3) A+B=B (4) A+B=C(5) A+B=A+B (6) A+B=A+B+D(7) A+B=A+D (8) A+B=D+BA and B stand for the meaning of the two char-acters which are used to construct the word.
Cis a totally new meaning and D represents an ad-2http://www.ncmmsc.org/CIPS-ParsEval-2009/index.asp, the first workshop on Chinese SyntacticParsing Evaluation, November, 2009.Table 2: Results of different features with no limit sentence length.
?B+C?-P ?B+C?-R ?B+C?-F1 ?B+C+H?-P ?B+C+H?-R ?B+C+H?-F1 POS-PTop one 85.42 85.35 85.39 83.69 83.63 83.66 93.96Single 74.86 76.05 75.45 71.06 72.20 71.63 87.00Rerank 74.48 75.64 75.05 70.72 71.81 71.26 87.00Table 4: The relation between the meaning ofwords and characters.type 1 2 3 4 5 6 7 8word number 4035 1031 297 4201 14455 23562 2780 1886rate(%) 7.71 1.97 0.57 8.02 27.60 44.99 5.31 3.60ditional meaning.
The expression after the first?=?
is the meaning of the word, and the symbol?+?
indicates the melding of meaning.
For exam-ple, A+B=A+D indicates that the word retains themeaning of character A, and adds new meaningD.
The distribution of each type in the dataset isshown in Table 4.
From Table 4 we can see thattype 4, i.e., there are no relation between charac-ters and word, occupies only 8.02%.
This dataproves that the word inherits the meaning from thecharacters which are used to construct the word.However, the relations are really complicated.
Forexample, some words only inherit the meaning offormal characters and others of the last characters.This might be the reason why character informa-tion does not have very obvious effect as expected.In our parsing model, context features are re-ally helpful to the parsing accuracy.
Different withthe decision method in (Rantnaparkhi, 1999) and(Wang, 2006), and reranking in (Collins, 2000)which all can utilize the context of current sub-tree very well (not only the POS tag), the CYKdecoding algorithm restricts our context features.However, we can conveniently exploit the POStags around the current subtree without increas-ing the complexity of decoding and thus improvethe performance.Commonly, each subtree has only one headword.
However, we notice that the two headwords of two coordinate children are equivalent,as illustrated in Figure 3.
We assume that the par-ent node of these two children is A and the twohead word are all the head words of A. WhenA is the child of the parent node B, all the headwords in A can be dependent with the other headFigure 3: Example of dependence between coor-dinate words.words of another child C. When A is still thehead child of B, the head words of B are also thesame as A.
Then we can extract more word depen-dence data.
For example, A have two head words???(construct)?
and ??
(complete)?, and ???(rule)?
is the head word of C, then we considerthat ???(construct)?
and ??(complete)?
areall dependent with ???(rule)?.
Meanwhile, A isalso the head child of B, and the head words of Bare also ???(construct)?
and ??
(complete)?.During the decoding, we choose the most proba-ble dependence as the dependence probability ofB.
From the result, we can see that this strategyyields 0.17 improvements in the F score.Label splitting can also improve the perfor-mance.
However, modifying the labels needmuch linguistic knowledge and manual work.
(Petrov, 2006) proposed an automated splittingand merging method.
As an attempt, we testedthe effectiveness of it in our parser empirically.When tested on the TCT data provided by CIPS-ParsEval-2009 for Chinese parser, bakeoff the la-bel spitting improve the F1 measure from 0.864 to0.869.4 Conclusion and Future WorkThis paper presents a new lexical PCFG model,which can synthetically exploit the word and char-acter information.
The results of experimentprove the effectiveness of character information.Also our model can utilize the context featuresand some non-local features which can dramati-cally improve the performance.In future work, we need improve the decod-ing algorithm to exploit more complex features.As the parser we build is greatly dependent onthe preprocessing result of word segmentation,POS tagging and head labeling, a critical direc-tion of future work is to do word-segmentation,POS tagging, head detection and parsing in a uni-fied framework.
Besides, as for the K-best rerank-ing, we should take into account more informativefeatures and more powerful reranking model.AcknowledgmentThis research has been partially supported bythe National Science Foundation of China (NO.NSFC90920006).
We also thank Xiaojie Wang,Huixing Jiang, Jiashen Sun and Bichuan Zhangfor useful discussion of this work.ReferencesA.L.
Beger, S. A.
D Pietra, and V.J.D Pietra.
1996.A maximum entropy approach to natural languageprocessing.
Computational Linguistics, 39?71.D.M.
Bikel and D. Chiang.
2000.
Two statistical pars-ing models applied to the Chinese Treebank.
In Pro-ceedings of the Second Chinese Language Process-ing Workshop, 1?6.E.
Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st NAACL, Seattle,WA, 132?139.E.
Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proceddingsof the Fourteenth National Conference on ArtificialIntelligence, Menlo Park, CA.
598?603.X.
Chen, C.N.
Huang, M. Li, and C.Y.
Kit.
2009.
Bet-ter Parser Combination.
In CIPS ParsEval, Beijing,China.
81?90.M.
Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. Dissertation,University of Pennsylvania.M.
Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proceedings of ICML 2000,175?182.S.Y.
Kang, X.X.
Xu, and M.S.
Sun.
2005.
TheResearch on the Modern Chinese Semantic Word-Formation.
Journal of Chinese Language and Com-puting, 103?112.D.
Klein and C. Manning.
2003.
Accurate unlexical-ized parsing.
In Proceedings of ACL 2003, 423?430.L.
Huang.
2008.
Forest Reranking: DiscriminativeParsing with Non-Local Features.
In Proceedingsof ACL 2008, 586?594.D.
Klein and C.D.
Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of ACL 2003, 423?430.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.In Proceedings of ACL 2005, 173?180.R.
Levy and C.D.
Manning.
2003.
Is it harder to parseChinese, or the Chinese Treebank?
In Proceedingsof ACL 2003, 439?446.X.Q.
Luo.
2003.
A maximum entropy Chinesecharacter-based parser.
In Proceedings of EMNLP2003, 192?199.M.
Johnson.
1998.
PCFG models of linguistic treerepresentations.
Computational Linguistics, 613?632.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable treeannotation.
In Proceedings of ACL 2006, 433?440.A.
Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of EMNLP 1996,133?142.A.
Ratnaparkhi.
1999.
Learning to parse natural lan-guage with maximum entropy models.
MachineLearning, 503?512.M.W.
Wang, K. Sagae, and T. Mitamura.
2006.
AFast, Accurate Deterministic Parser for Chinese.
InProceedings of ACL 2006, 425?432.L.
Zhang.
2004.
Reference Manual.
Maximum En-tropy Modeling Toolkit for Python and C++.H.
Zhao.
2009.
Character-Level Dependencies in Chi-nese: Usefulness and Learning.
In Proceedings of12th ECACL 2009, 879?887.SIGHAN REPORT.
2010.
SIGHAN REPORT ONTASK2.
In Proceedings of CIPS-SIGHAN 2010,Beijing, China.Q.
Zhou.
2004.
Annotation Scheme for Chinese Tree-bank.
Journal of Chinese Information Processing,1?8.
