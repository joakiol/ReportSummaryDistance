Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 137?140,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsMining Wikipedia Revision Histories for Improving Sentence CompressionElif Yamangil Rani NelkenSchool of Engineering and Applied SciencesHarvard UniversityCambridge, MA 02138, USA{elif,nelken}@eecs.harvard.eduAbstractA well-recognized limitation of research onsupervised sentence compression is the dearthof available training data.
We propose a newand bountiful resource for such training data,which we obtain by mining the revision his-tory of Wikipedia for sentence compressionsand expansions.
Using only a fraction of theavailable Wikipedia data, we have collecteda training corpus of over 380,000 sentencepairs, two orders of magnitude larger than thestandardly used Ziff-Davis corpus.
Using thisnewfound data, we propose a novel lexical-ized noisy channel model for sentence com-pression, achieving improved results in gram-maticality and compression rate criteria with aslight decrease in importance.1 IntroductionWith the increasing success of machine translation(MT) in recent years, several researchers have sug-gested transferring similar methods for monolingualtext rewriting tasks.
In particular, Knight and Marcu(2000) (KM) applied a channel model to the task ofsentence compression ?
dropping words from an in-dividual sentence while retaining its important in-formation, and without sacrificing its grammatical-ity.
Compressed sentences can be useful either ontheir own, e.g., for subtitles, or as part of a largersummarization or MT system.
A well-recognizedproblem of this approach, however, is data spar-sity.
While bilingual parallel corpora are abundantlyavailable, monolingual parallel corpora, and espe-cially collections of sentence compressions are van-ishingly rare.
Indeed, most work on sentence com-pression has used the Ziff-Davis corpus (Knight andMarcu, 2000), which consists of a mere 1067 sen-tence pairs.
While data sparsity is a common prob-lem of many NLP tasks, it is much more severe forsentence compression, leading Turner and Charniak(2005) to question the applicability of the channelmodel for this task altogether.Our contribution in this paper is twofold.
First,we solve the data sparsity issue by showing thatabundant sentence compressions can be extractedfrom Wikipedia?s revision history.
Second, we usethis data to validate the channel model approachfor text compression, and improve upon it by cre-ating a novel fully lexicalized compression model.Our model improves grammaticality and compres-sion rate with only a slight decrease in importance.2 Data: Wikipedia revision histories as asource of sentence compressionsMany researchers are increasingly turning toWikipedia as a large-scale data source for trainingNLP systems.
The vast majority of this work usesonly the most recent version of the articles.
In fact,Wikipedia conveniently provides not only the lat-est version, but the entire revision history of eachof its articles, as dramatically visualized by Vie?gaset al (2004).
Through Wikipedia?s collaborativeediting process, articles are iteratively amended andrefined by multiple Web users.
Users can usuallychange any aspect of the document?s structure andcontent, but for our purposes here, we focus only onsentence-level edits that add or drop words.We have downloaded the July snapshot of the137English Wikipedia, consisting of 1.4 million arti-cles, and mined a subset of them for such compres-sions/expansions.
We make the simplifying assump-tion that all such edits also retain the core mean-ing of the sentence, and are therefore valid trainingdata for our purposes.
This assumption is of coursepatently na?
?ve, as there are many cases in which suchrevisions reverse sentence meaning, add or drop es-sential information, are part of a flame war, etc.Classifying these edits is an interesting task whichwe relegate to future work.1From about one-third of the snapshot, we ex-tracted over 380,000 sentence pairs, which is 2 or-ders of magnitude more than the Ziff-Davis corpus.2Wikipedia currently has 2.3 million articles and isconstantly expanding.
We can therefore expect anincrease of another order of magnitude.
We thus canafford to be extremely selective of the sentence pairswe use.
To handle a dataset of such size (hundreds ofGBs), we split it into smaller chunks, and distributeall the processing.More technically, for each article, we first extractall revisions, and split each revision into a list of itssentences.
We run an edit-distance comparison be-tween each such pair, treating each sentence as anatomic ?letter?.
We look for all replacements of onesentence by another and check whether one sentenceis a compression of the other.3 We then run Collins?parser (1997), using just the sentence pairs whereparsing succeeds with a negative log likelihood be-low 200.3 Noisy channel modelWe follow KM in modeling the problem using a gen-erative noisy channel model, but use the new-foundtraining data to lexicalize the model.
Sentences starttheir life in short form, s, are ranked by a sourcelanguage model, p(s), and then probabilistically ex-panded to form the long sentence, p(l|s).
Duringdecoding, given a long sentence, we seek the mostlikely short sentence that could have generated it.1For instance, compressions are more likely to signal op-tional information than expansions; the lexical items added arelikely to be indicative of the type of edit, etc.2The sentence pair corpus is available by contacting theauthors.3We ignore word reorderings or replacements that are be-yond word addition or deletion.Using Bayes?
rule, this is equivalent to seeking theshort sentence s that maximizes p(s) ?
p(l|s).3.1 Lexicalized channel modelKM?s original model was purely syntax-based.Daume et al (2002) used a lexicalized PCFG torerank the compressions, showing that the additionof lexical information helps eliminate improbablecompressions.
Here, we propose to enhance lexical-ization by including lexical information within thechannel model, allowing us to better model whichcompressions are likely and which are not.
A min-imal example pair illustrating the utility of lexical-ization is the following.
(1) Hillary barely won the primaries.
(2) Hillary almost won the primaries.The validity of dropping the adverbial here clearlydepends on the lexical value of the adverb.
It is moreacceptable to drop the adverb in Sentence 1, sincedropping it in Sentence 2 reverses the meaning.
Welearn probabilities of the form:p( S[won]NP[Hillary] ADVP[almost] VP[won]| S[won]NP[Hillary] VP[won])Our model has the power of making compression de-cisions based on lexical dependencies between thecompressed and retained parts of the parse tree.Note that Daume et al?s reranking model cannotachieve this type of distinction, since it is based onreranking the compressed version, at which point theadverb is no longer available.Since we are interested not only in learning howto compress, but also when to compress, we also in-clude in this procedure unchanged CFG rule pairsthat are attested in the corpus.
Thus, different waysof expanding a CFG rule compete with each other aswell as the possibility of not doing any expansion.3.2 SmoothingIn order to smooth our estimates we use Witten-Belldiscounting (1991) with 6 levels of back-off.
Thismethod enables us to tune the confidence parameterassociated with an estimate inversely proportionallywith the diversity of the context of the estimate.
Thedifferent levels are illustrated in Table 1.
Level 1,138the most specific level, is fully lexicalized.
Transi-tioning to levels 2 to 4, we lose the lexical informa-tion about the subtrees that are not dropped, the headchild bearing subtree, and the dropped subtrees, re-spectively.
At level 4, we end up with the non-lexicalized estimates that are equivalent to KM?smodel.
In subsequent back off levels, we abstractaway from the CFG rules.
In particular, level 5 es-timates the probability of dropping subtrees in thecontext of a certain parent and head child, and level6 estimates the probability of the same outcome inthe coarser context of a parent only.3.3 Source modelIn addition to the lexicalized channel model, we alsouse a lexicalized probabilistic syntax-based sourcemodel, which we train from the parser?s output onthe short sentences of each pair.3.4 DecodingWe implemented the forest-based statistical sen-tence generation method of Langkilde (2000).
KMtailored this method to sentence compression, com-pactly encoding all compressions of a sentence ina forest structure.
The forest ranking algorithmwhich extracts compressed parse trees, optimizedthe model scores as well as an additional bigramscore.
Since our model is lexicalized, the bigramscores become less relevant, which was confirmedby experimentation during development.
Thereforein our implementation we exclude the bigram scoresand other related aspects of the algorithm such aspruning of bigram-suboptimal phrases.4 EvaluationWe evaluated our system using the same method asKM, using the same 32 sentences taken from theZiff-Davis corpus.
We solicited judgments of im-portance (the value of the retained information), andgrammaticality for our compression, the KM results,and human compressions from 8 judges, on a scaleof 1 (worst) to 5 (best).
Mean and standard deviationare shown in Table 2.
Our model improves gram-maticality and compression rate criteria with only aslight decrease in importance.
Here are some illus-trative examples, with the deleted material shown inbrackets:(3) The chemical etching process [used for glareprotection] is effective and will help if youroffice has the fluorescent-light overkill [that?s typical in offices].
(4) Prices range from $5,000 [for a microvax2000] to $179,000 [for the vax 8000 orhigher series].We suspect that the decrease in importance stemsfrom our indiscriminative usage of compressionsand expansions to train our system.
We hypothesizethat in Wikipedia, expansions often add more usefulinformation, as opposed to compressions which aremore likely to drop superfluous or erroneous infor-mation.4 Further work is required to classify sen-tence modifications.Since one of our model?s back-off levels simulatesKM?s model, we plan to perform an additional com-parative evaluation of both models trained on thesame data.5 Discussion and future workTurner and Charniak (2005) question the viabilityof a noisy channel model for the sentence compres-sion task.
Briefly put, in the typically sparse datasetting, there is no way to distinguish between theprobability of a sentence as a short sentence and itsprobability as a regular sentence of English.
Fur-thermore, the channel model is likely to prefer toleave sentences intact, since that is the most preva-lent pattern in the training data.
Thus, they argue,the channel model is not really compressing, and itis only by virtue of the length penalty that anythinggets shortened at all.
Our hope here is that by usinga far richer source of short sentences, as well as ahuge source of compressions, we can overcome thisproblem.
The noisy channel model posits a virtualcompetition on each word of coming either from thesource model (in which case it is retained in the com-pression) or from the channel model (in which caseit is dropped).
By having access to a large data setfor the first time, we hope to be able to learn whichparts of the sentence are more likely to come from4For instance, here is an expansion seen in the data, wherethe added information (italicized) is important: ?In 1952 and1953 he was stationed in Sendai, Japan during the Korean Warand was shot.?
It would be undesirable to drop this addedphrase.139Back-off level expanded short1 S[won]?
NP[Hillary] ADVP[almost] VP[won] S[won]?
NP[Hillary] VP[won]2 S[won]?
NP ADVP[almost] VP[won] S[won]?
NP VP[won]3 S?
NP ADVP[almost] VP S?
NP VP4 S?
NP ADVP VP S?
NP VP5 parent = S, head-child = VP, child = ADVP parent = S, head-child = VP6 parent = S, child = ADVP parent = STable 1: Back off levelsKM Our model HumansCompression 72.91% 67.38% 53.33%Grammaticality 4.02?1.03 4.31?0.78 4.78?0.17Importance 3.86?1.09 3.65?1.07 3.90?0.58Table 2: Evaluation resultswhich of the two parts of the model.
Further work isrequired in order to clarify this point.Naturally, discriminative models such as McDon-ald (2006) are also likely to improve by using theadded data.
We leave the exploration of this topicfor future work.Finally, we believe that the Wikipedia revisionhistory offers a wonderful resource for many addi-tional NLP tasks, which we have begun exploring.AcknowledgmentsThis work was partially supported by a Google re-search award, ?Mining Wikipedia?s Revision His-tory?.
We thank Stuart Shieber for his comments onan early draft of this paper, Kevin Knight and DanielMarcu for sharing the Ziff-Davis dataset with us, andthe volunteers for rating sentences.
Yamangil thanksMichael Collins for his feedback on the project idea.ReferencesMichael Collins.
1997.
Three generative, lexicalizedmodels for statistical parsing.
In Philip R. Cohen andWolfgangWahlster, editors, Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computa-tional Linguistics and Eighth Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics, pages 16?23, Somerset, New Jersey.
As-sociation for Computational Linguistics.H.
Daume, Kevin Knight, I Langkilde-Geary, DanielMarcu, and K Yamada.
2002.
The importance of lexi-calized syntax models for natural language generationtasks.
Proceedings of the Second International Confer-ence on Natural Language Generation.
Arden House,NJ, July 1-3.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In Proceedings of the Seventeenth National Con-ference on Artificial Intelligence and Twelfth Confer-ence on Innovative Applications of Artificial Intelli-gence, pages 703?710.
AAAI Press / The MIT Press.Irene Langkilde.
2000.
Forest-based statistical sentencegeneration.
In Proceedings of the first conference onNorth American chapter of the Association for Com-putational Linguistics, pages 170?177, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Ryan T. McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceedingsof EACL 2006, 11st Conference of the EuropeanChap-ter of the Association for Computational Linguistics,April 3-7, 2006, Trento, Italy, pages 297?304.Jenine Turner and Eugene Charniak.
2005.
Supervisedand unsupervised learning for sentence compression.In ACL ?05: Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, pages290?297,Morristown, NJ, USA.
Association for Com-putational Linguistics.Fernanda B.
Vie?gas, Martin Wattenberg, and KushalDave.
2004.
Studying cooperation and conflict be-tween authors with istory flow visualizations.
In Eliza-beth Dykstra-Erickson andManfred Tscheligi, editors,CHI, pages 575?582.
ACM.I.Witten and T. Bell.
1991.
The zero-frequencyproblem:Estimating the probabilities of novel events in adaptivetext compression.
IEEE Transactions on InformationTheory, 37(4).140
