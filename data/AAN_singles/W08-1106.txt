Extractive vs. NLG-based Abstractive Summarization of EvaluativeText: The Effect of Corpus ControversialityGiuseppe Carenini and Jackie Chi Kit Cheung1Department of Computer ScienceUniversity of British ColumbiaVancouver, B.C.
V6T 1Z4, Canada{carenini,cckitpw}@cs.ubc.caAbstractExtractive  summarization  is  the  strategy  ofconcatenating  extracts  taken  from  a  corpusinto a summary, while abstractive summariza-tion  involves  paraphrasing  the  corpus  usingnovel sentences.
We define a novel  measureof  corpus  controversiality  of  opinions  con-tained in evaluative text, and report the resultsof  a  user  study  comparing  extractive  andNLG-based abstractive summarization at dif-ferent levels of controversiality.
While the ab-stractive summarizer performs better overall,the results suggest  that the margin by whichabstraction  outperforms  extraction  is  greaterwhen  controversiality  is  high,  providing  acontext  in  which  the  need  for  generation-based methods is especially great.1 IntroductionThere are two main approaches to the task of sum-marization?extraction and abstraction (Hahn andMani, 2000).
Extraction involves concatenating ex-tracts  taken  from  the  corpus  into  a  summary,whereas abstraction involves generating novel sen-tences from information extracted from the corpus.It has been observed that in the context of multi-document summarization of news articles, extrac-tion may be inappropriate because it may producesummaries which are overly verbose or biased to-wards some sources (Barzilay et al, 1999).
How-ever, there has been little work identifying specificfactors which might affect the performance of eachstrategy  in  summarizing  evaluative  documentscontaining opinions and preferences, such as cus-tomer reviews or blogs.
This work aims to addressthis gap by exploring one dimension along whichthe effectiveness of the two paradigms could vary;namely,  the controversiality of  the opinions  con-tained in the corpus.In this paper, we make the following contribu-tions.
Firstly,  we define a measure  of  controver-siality of opinions in the corpus based on informa-tion entropy.
Secondly, we run a user study to testthe  hypothesis  that  a  controversial  corpus  hasgreater  need  of  abstractive  methods  and  conse-quently of NLG techniques.
Intuitively, extractingsentences from multiple users whose opinions arediverse and wide-ranging may not reflect the over-all  opinion,  whereas it  may be adequate content-wise if opinions are roughly the same across users.As a secondary contribution, we propose a methodfor structuring text when summarizing controver-sial corpora.
This method is used in our study forgenerating abstractive summaries.The results  of  the user  study support  our  hy-pothesis  that  a  NLG summarizer  outperforms  anextractive summarizer by more  when the contro-versiality is high.2 Related WorkThere  has  been  little  work  comparing  extractiveand abstractive multi-document summarization.
Aprevious  study  on  summarizing  evaluative  text(Carenini et.
al, 2006) showed that extraction andabstraction performed about equally well,  thoughfor different reasons.
The study, however, did not1Authors are listed in alphabetical order.33look at the effect of the controversiality of the cor-pus on the relative performance of the two strate-gies.To the best of our knowledge, the task of mea-suring the controversiality of opinions in a corpushas  not  been  studied  before.
Some  well  knownmeasures  are  related to  this  task,  including vari-ance, information entropy,  and measures of inter-rater reliability.
(e.g.
Fleiss' Kappa (Fleiss, 1971),Krippendorff's Alpha (Krippendorff, 1980)).
How-ever, these existing measures do not satisfy certainproperties that a sound measure of controversialityshould possess, prompting us to develop our ownbased on information entropy.Summary evaluation is a challenging open re-search  area.
Existing  methods  include  solicitinghuman judgements, task-based approaches, and au-tomatic approaches.Task-based evaluation measures  the  effective-ness of a summarizer for its intended purpose.
(e.g.
(McKeown et al, 2005)) This approach, however,is less applicable in this work because we are inter-ested in evaluating specific properties of the sum-mary such as the grammaticality and the content,which may be difficult to evaluate with an overalltask-based  approach.
Furthermore,  the  design  ofthe task may intrinsically favour abstractive or ex-tractive  summarization.
As  an  extreme  example,asking for a list of specific comments from userswould clearly favour extractive summarization.Another method for summary evaluation is thePyramid method (Nenkova and Passonneau, 2004),which takes into account the fact that human sum-maries with different content can be equally infor-mative.
Multiple human summaries are taken to bemodels, and chunks of meaning known as Summa-ry Content  Units  (SCU)  are  manually  identified.Peer summaries are evaluated based on how manySCUs they share with the model  summaries,  andthe  number  of  model  summaries  in  which  theseSCUs are found.
Although this  method has beentested in DUC 2006 and DUC 2005 (Passonneau etal., 2006), (Passonneau et al, 2005) in the domainof news articles, it has not been tested for evalua-tive text.
A pilot study that we conducted on a setof customer reviews on a product using the Pyra-mid method revealed several problems specific tothe  evaluative  domain.
For  example,  summarieswhich  misrepresented  the  polarity of  the  evalua-tions for a certain feature were not penalized, andhuman summaries sometimes produced contradic-tory statements about the distribution of the opin-ions.
In one case, one model summary claimed thata feature is positively rated, while another claimedthe opposite, whereas the machine summary indi-cated that this feature drew mixed reviews.
Clear-ly, only one of these positions should be regardedas correct.
Further work is needed to resolve theseproblems.There are also automatic methods for summaryevaluation,  such  as  ROUGE  (Lin,  2004),  whichgives  a  score  based  on  the  similarity  in  the  se-quences of words between a human-written modelsummary  and  the  machine  summary.
WhileROUGE scores have been shown to often correlatequite well with human judgements (Nenkova et al,2007), they do not provide insights into the specif-ic strengths and weaknesses of the summary.The method of summarization evaluation usedin this work is to ask users to complete a question-naire about summaries that they are presented with.The questionnaire consists of questions asking forLikert  ratings  and  is  adapted  from the  question-naire in (Carenini et al, 2006).3 Representative SystemsIn our user study, we compare an abstractive andan extractive multi-document summarizer that areboth developed specifically for the evaluative do-main.
These summarizers have been found to pro-duce quantitatively similar results, and both signif-icantly outperform a baseline summarizer, which isthe MEAD summarization framework with all op-tions set to the default (Radev et al, 2000).Both summarizers  rely on information  extrac-tion from the corpus.
First, sentences with opinionsneed to be identified, along with the features of theentity that are evaluated, the strength, and polarity(positive  or  negative)  of  the  evaluation.
For  in-stance, in a corpus of customer reviews, the sen-tence ?Excellent picture quality - on par with myPioneer, Panasonic, and JVC players.?
contains anopinion on the feature  picture quality of  a DVDplayer, and is a very positive evaluation (+3 on ascale from -3 to +3).
We rely on methods from pre-vious  work  for  these  tasks  (Hu  and  Liu,  2004).Once these features, called Crude Features (CFs),are extracted, they are mapped onto a taxonomy ofUser Defined Features (UDFs), so named becausethey can be defined by the user.
This mapping pro-vides a better conceptual organization of the CFs34by  grouping  together  semantically  similar  CFs,such as jpeg picture and jpeg slide show under theUDF JPEG.
For the purposes of our study, featureextraction,  polarity/strength identification and themapping from CFs to UDFs are not done automati-cally as in (Hu and Liu, 2004) and (Carenini et al2005).
Instead, ?gold standard?
annotations by hu-mans are used in order to focus on the effect of thesummarization strategy.3.1 Abstractive Summarizer: SEAThe abstractive summarizer is the Summarizer ofEvaluative Arguments (SEA), adapted from GEA,a system for generating evaluative text tailored tothe user's preferences (Carenini and Moore, 2006).In  SEA,  units  of  content  are  organized  byUDFs.
The importance of each UDF is based onthe  number  and  strength  of  evaluations  of  CFsmapped to this UDF, as well as the importance ofits children UDFs.
Content selection consists of re-peating the following two steps  until  the desirednumber of UDFs have been selected: (i)  greedilyselecting the most important UDF (ii) recalculatingthe measure of importance scores for the remainingUDFs.The content structuring, microplanning, and re-alization  stages  of  SEA are  adapted  from GEA.Each selected UDF is realized in the final summaryby one clause, generated from a template patternbased  on  the  number  and  distribution  ofpolarity/strength evaluations of the UDF.
For ex-ample, the UDF video output with an average po-larity/strength of near -3 might be realized as ?sev-eral customers found the video output to be terri-ble.
?While experimenting with the SEA summariz-er,  we  noticed  that  the  document  structuring  ofSEA summaries, which is adapted from GEA andis based on guidelines from argumentation theory(Carenini  and Moore,  2000),  sometimes  soundedunnatural.
We  found  that  controversially  ratedUDF features (roughly balanced positive and nega-tive evaluations) were treated as contrasts to thosewhich were uncontroversially rated (either mostlypositive, or mostly negative evaluations).
In SEA,contrast relations between features are realized bycue phrases signalling contrast such as ?however?and ?although?.
These cue phrases appear to signala contrast that is too strong for the relation betweencontroversial and uncontroversial features.
An ex-ample of a SEA summary suffering from this prob-lem can be found in Figure 1.To solve this problem, we devised an alterna-tive content structure for controversial corpora, inwhich  all  controversial  features  appear  first,  fol-lowed by all  positively and negatively evaluatedfeatures.3.2 Extractive Summarizer: MEAD*The  extractive  approach  is  represented  byMEAD*, which is adapted from the open sourcesummarization  framework  MEAD (Radev  et  al.,2000).After  information  extraction,  MEAD*  ordersCFs  by the  number  of  sentences  evaluating  thatCF, and selects a sentence from each CF until theword limit has been reached.
The sentence that isselected for each CF is  the one with the highestsum of  polarity/strength evaluations  for  any fea-ture, so sentences that mention more CFs tend tobe  selected.
The  selected  sentences  are  then  or-dered according to the UDF hierarchy by a depth-first traversal through the UDF tree so that moreabstract  features  tend  to  precede  more  specificones.MEAD* does not have a special mechanism todeal with controversial features.
It is not clear howoverall controversiality of a feature can be effec-tively expressed with extraction, as each sentenceconveys a specific and unique opinion.
One couldinclude two sentences of opposite polarity for eachcontroversial  feature.
However,  in  several  casesthat we considered, this produced extremely inco-herent text that did not seem to convey the gist ofthe overall controversiality of the feature.Customers had mixed opinions about the Apex AD2600.Although several customers found the video output to bepoor and some customers disliked the user interface, cus-tomers had mixed opinions about the range of compatibledisc formats.
However, users did agree on some things.Some users found the extra features to be very good eventhough customers had mixed opinions about the supplieduniversal remote control.Figure 1: SEA summary of a controversial corpus witha document structuring problem.
Controversial and un-controversial features are interwoven.
See Figure 3 foran example of a summary structured with our alterna-tive strategy.353.3 Links to the CorpusIn common with the previous study on which thisis  based,  both  the  SEA and MEAD* summariescontain ?clickable footnotes?
which are links backinto an original user review, with a relevant sen-tence highlighted.
These footnotes serve to providedetails  for  the  abstractive  SEA summarizer,  andcontext for the sentences chosen by the extractiveMEAD*  summarizer.
They  also  aid  the  partici-pants of the user study in checking the contents ofthe summary.
The sample sentences for SEA areselected by a method similar to the MEAD* sen-tence selection algorithm.
One of the questions inthe questionnaire provided to users targets the ef-fectiveness of the footnotes as an aid to the sum-mary.4 Measuring ControversialityThe opinion sentences in the corpus are annotatedwith  the  CF  that  they  evaluate  as  well  as  thestrength, from 1 to 3, and polarity, positive or neg-ative, of the evaluation.
It is natural then, to base ameasure  of  controversiality on these  annotations.To  measure  the  controversiality  of  a  corpus,  wefirst  measure  the  controversiality  of  each  of  thefeatures in the corpus.
We list two properties that ameasure of feature controversiality should satisfy.Strength-sensitivity:  The  measure  should  besensitive to the strength of the evaluations.
e.g.
Po-larity/strength  (P/S)  evaluations  of  -2  and  +2should be less controversial than -3 and +3Polarity-sensitivity: The measure should be sen-sitive the polarity of the evaluations.
e.g.
P/S eval-uations of -1 and +1 should be more controversialthan +1 and +3.The rationale for this property is that positiveand negative evaluations are fundamentally differ-ent, and this distinction is more important than thedifference in intensity.
Thus,  though a numericalscale would suggest that -1 and +1 are as distant as+1  and  +3,  a  suitable  controversiality  measureshould not treat them so.In addition, the overall measure of corpus con-troversiality should also satisfy the following twofeatures.CF-weighting: CFs should be weighted by thenumber of evaluations they contain when calculat-ing the overall value of controversiality for the cor-pus.CF-independence: The controversiality of indi-vidual CFs should not affect each other.An alternative is to calculate controversiality byUDFs  instead  of  CFs.
However,  not  all  CFsmapped to the same UDF represent the same con-cept.
For example, the CFs picture clarity and col-or signal are both mapped to the UDF video out-put.4.1 Existing Measures of VariabilitySince the problem of measuring the variability of adistribution has been well studied, we first exam-ined existing metrics including variance, entropy,kappa, weighted kappa, Krippendorff?s alpha, andinformation  entropy.
Each  of  these,  however,  isproblematic in their canonical form, leading us todevise a new metric based on information entropywhich satisfies the above properties.
Existing met-rics will now be examined in turn.Variance:  Variance  does  not  satisfy  polarity-sensitivity, as the statistic only takes into accountthe difference of each data point to the mean, andthe sign of the data point plays no role.Information Entropy: The canonical form of in-formation entropy does not satisfy strength or po-larity  sensitivity,  because  the  measure  considersthe discrete values of the distribution to be an un-ordered set.Measures of Inter-rater Reliability: Many mea-sures exist  to assess inter-rater agreement or dis-agreement,  which  is  the  task  of  measuring  howsimilarly two or more judges rate one or more sub-jects beyond chance (dis)agreement.
Various ver-sions  of  Kappa  and  Krippendorff's  Alpha  (Krip-pendorff, 1980), which have shown to be equiva-lent in their most generalized forms (Passonneau,1997), can be modified to satisfy all the propertieslisted above.
However, there are important differ-ences between the tasks of  measuring controver-siality and measuring inter-rater reliability.
Kappaand Krippendorff's Alpha correct for chance agree-ment  between raters,  which is  appropriate  in  thecontext  of  inter-rater  reliability  calculations,  be-cause judges are asked to give their  opinions onitems that are given to them.
In contrast,  expres-sions  of  opinion  are  volunteered  by  users,  andusers  self-select  the  features  they  comment  on.Thus,  it  is  reasonable  to  assume  that  they neverrandomly  select  an  evaluation  for  a  feature,  andchance agreement does not exist.364.2 Entropy-based ControversialityWe define here  our novel  measure  of  controver-siality, which is based on information entropy be-cause  it  can  be  more  easily  adapted  to  measurecontroversiality.
As has been stated, entropy in itsoriginal form over the evaluations of a CF is notsensitive to strength or polarity.
To correct this, wefirst  aggregate  the  positive  and  negative  evalua-tions for each CF separately, and then calculate theentropy based on the resultant Bernoulli distribu-tion.Let ps(cfj) be the set of polarity/strength evalua-tions  for  cfj.
Let  the  importance  of  a  feature,imp(cfj), be the sum of the absolute values of thepolarity/strength evaluations for cfj.imp?cf j ?= ?ps k?
ps?
cf j?
?psk?Define:imp_ pos ?cf j?= ?psk ?
ps?
cf j??
psk?0?psk?imp_ neg ?cf j ?= ?psk?
ps ?cf j?
?psk?0?psk?Now, calculate the entropy of the Bernoulli dis-tribution  corresponding  to  the  importance  of  thetwo polarities  to  satisfy polarity-sensitivity.
Thatis, Bernoulli with parameter?
j=imp_ pos ?cf j?
/imp?cf j ?H ??
j?=??
j log2?
j??1??
j ?
log2 ?1??
j?Next, we scale this score by the importance ofthe evaluations divided by the maximum possibleimportance for this number of evaluations to satis-fy strength-sensitivity.
Since our scale is from -3 to+3, the maximum possible importance for a featureis three times the number of evaluations.max_imp?cf j ?=3?
?ps ?cf j ?
?Then the controversiality of a feature is:contro ?cf j ?= imp?cf j?
?H ??
j?max_imp?cf j ?The case corresponding to the highest possiblefeature  controversiality,  then,  would  be  the  bi-modal case with equal numbers of evaluations onthe extreme positive and negative bins (Figure 2).Note, however, that controversiality is not simplybimodality.
A unimodal normal-like distribution ofevaluations  centred on zero,  for  example,  shouldintuitively  be  somewhat  controversial,  becausethere are equal numbers  of  positive and negativeevaluations.
Our entropy-based feature controver-siality measure is able to take this into account.To calculate the controversiality of the corpus,a weighted average is taken over the CF controver-siality scores, with the weight being equal to oneless  than the  number  of  evaluations  for  that  CF.We subtract one to eliminate any CF where onlyone evaluation is made, as that CF has an entropyscore of one by default  before scaling by impor-tance.
This  procedure  satisfies  properties  CF-weighting and CF-independence.w ?cf j?=?ps ?cf j ??
?1contro ?corpus?=?
w?cf j ?
?contro ?cf j??
w?cf j?Although the annotations in this  corpus rangefrom -3 to +3, it would be easy to rescale opinionannotations of different corpora to apply this met-ric.
Note that empirically,  this measure correlateshighly with Kappa and Krippendorff's Alpha.5 User StudyOur main hypothesis that extractive summarizationis outperformed even more in the case of contro-versial corpora was tested by a user study, whichcompared the results of MEAD* and the modifiedSEA.
First, ten subsets of 30 user reviews were se-lected from the corpus of 101 reviews of the ApexAD2600  DVD  player  from  amazon.com  bystochastic  local  search.
Five of  these  subsets  arecontroversial, with controversiality scores between0.83 and 0.88, and five of these are uncontrover-sial, with controversiality scores of 0.
A set of thir-Figure 2: Sample feature controversiality scores forthree different distributions of polarity/strength evalua-tions.37ty user reviews per subcorpus was needed to createa summary of sufficient length, which in our casewas about 80 words in length.Twenty university students were recruited andpresented with two summaries of the same subcor-pus,  one  generated  from  SEA  and  one  fromMEAD*.
We generated ten subcorpora in total, soeach subcorpus was assigned to two participants.One of these participants was shown the SEA sum-mary first, and the other was shown the MEAD*summary first, to eliminate the order of presenta-tion as a source of variation.The participants were asked to take on the roleof an employee of Apex, and told that they wouldhave to write a summary for the quality assurancedepartment  of  the  company about  the  product  inquestion.
The purpose of this was to prime them tolook for information that should be included in asummary  of  this  corpus.
They were  given  thirtyminutes to read the reviews, and take notes.They were then presented with a questionnaireon the summaries,  consisting of ten Likert ratingquestions.
Five of these questions targeted the lin-guistic quality of the summary, based on linguisticwell-formedness questions used at DUC 2005, onetargeted the ?clickable footnotes?
linking to samplesentences  in  the  summary  (see  section  3.3),  andthree evaluated the contents of the summary.
Thethree questions targeted Recall,  Precision, and thegeneral Accuracy of the summary contents respec-tively.
The tenth question asked for a general over-all quality judgement of the summary.After  familiarizing  themselves  with  the  ques-tionnaire, the participants were presented with thetwo summaries in sequence, and asked to fill outthe questionnaire while reading the summary.
Theywere  allowed to  return to  the  original  set  of  re-views during this time.
Lastly, they were given anadditional questionnaire which asked them to com-pare  the  two  summaries  that  they  were  shown.Questions  in  the  questionnaire  not  found  in(Carenini et al, 2006) are attached in Appendix A.6 Results6.1 Quantitative ResultsWe convert the Likert responses from a scale fromStrongly Disagree to Strong Agree to a scale from1 to 5, with 1 corresponding to Strongly Disagree,and 5 to Strongly Agree.
We group the ten ques-tions into four categories: linguistic (questions 1 to5),  content  (questions 6 to 8),  footnote (question9),  and  overall  (question  10).
See  Table  1 for  abreakdown of the  responses for  each question ateach controversiality level.For our analysis, we adopt a two-step approachthat has been applied in Computational Linguistics(Di Eugenio et al, 2002) as well as in HCI (Hinck-ley et al, 1997).First, we perform a two-way Analysis of Vari-ance (ANOVA) test using the average response ofthe questions in each category.
The two factors arecontroversiality of the corpus (high or low) as in-dependent  samples,  and the summarizer  (SEA orMEAD*)  as  repeated  measures.
We  repeat  thisprocedure  for  the  average  of  the  ten  questions,termed  Macro below.
The p-values of these testsare summarized in Table 2.The results  of  the ANOVA tests  indicate thatSEA significantly outperforms MEAD* in terms oflinguistic and overall quality, as well as for all thequestions combined.
It does not significantly out-perform MEAD* by content, or in the amount thatthe  included  sample  sentences  linked  to  by  thefootnotes aid the summary.
No significant differ-ences are found in the performance of the summa-SEACustomers had mixed opinions about the Apex AD2600 1,2possibly because users were divided on the range of compatibledisc formats 3,4 and there was disagreement among the usersabout the video output 5,6.
However, users did agree on somethings.
Some purchasers found the extra features 7 to be verygood and some customers really liked the surround sound sup-port 8 and thought the user interface 9 was poor.MEAD*When we tried to hook up the first one , it was broken - themotor would not eject discs or close the door .
1 The buildquality feels solid , it does n't shake or whine while playingdiscs , and the picture and sound is top notch ( both dts anddd5.1 sound good ) .
2 The progressive scan option can beturned off easily by a button on the remote control which isone of the simplest and easiest remote controls i have everseen or used .
3 It plays original dvds and cds and playsmp3s and jpegs .
4Figure 3: Sample SEA and MEAD* summaries for a controversial corpus.
The numbers within the summaries arefootnotes linking the summary to an original user review from the corpus.38rizers  over  the  two levels  of  controversiality  forany of the question sets .While  the  average  differences  in  scores  be-tween  the  SEA  and  MEAD*  summarizers  aregreater in the controversial case for the linguistic,content, and macro averages as well as the ques-tion on the overall quality, the p-values for interac-tion between the two factors in the two-way ANO-VA test are not significant.For the second step of the analysis,  we use aone-tailed  sign  test  (Siegel  and  Castellan,  1988)over the difference in performance of the summa-rizers at the two levels of controversiality for thequestions in the questionnaire.
We encode + in thecase  where  the  difference  between  SEA  andMEAD* is greater for a question in the controver-sial setting, ?
if the difference is smaller, and wediscard a question if the difference is the same (e.g.the Footnote question).
Since the Overall questionis likely correlated with the responses of the otherquestions, we did not include it in the test.
Afterdiscarding the Footnote question, the p-value overthe  remaining  eight  questions  is  0.0352,  whichlends support to our hypothesis that the abstractionis better by more when the corpus is controversial.We  also  analyze  the  users'  summary  prefer-ences at the two levels of controversiality.
A strongpreference  for  SEA  is  encoded  as  a  5,  while  astrong preference for MEAD* is encoded as a 1,with 3 being neutral.
Using a two-tailed unpairedtwo-sample t-test, we do not find a significant dif-ference  in  the  participants'  summary  preferences(p=0.6237).
However, participants sometimes pre-ferred summaries for reasons other than linguisticor  content  quality,  or  may  base  their  judgementonly on one aspect of the summary.
For instance,one  participant  rated  SEA  at  least  as  well  asMEAD* in all questions except Footnote, yet pre-ferred MEAD* to SEA overall  precisely becauseMEAD* was felt  to have made better use of thefootnotes than SEA.6.2 Qualitative ResultsThe  qualitative  comments  that  participants  wereasked to provide along with the Likert scores con-firmed the observations that led us to formulate theinitial hypothesis.In  the  controversial  subcorpora,  participantsgenerally  agreed  that  the  abstractive  nature  ofSEA's generated text was an advantage.
For exam-ple, one participant lauded SEA for attempting to?synthesize the reviews?
and said that it  ?did re-flect the mixed nature of the reviews, and coveredsome common complaints.?
The participant, how-ever, said that SEA ?was somewhat misleading inthat it understated the extent to which reviews werenegative.
In particular, agreement was reported onQuestionSetControver-sialitySummarizer Controversialityx SummarizerLinguistic 0.7226 <0.0001 0.2639Content 0.9215 0.1906 0.2277Footnote 0.2457 0.7805 1Overall 0.6301 0.0115 0.2000Macro 0.7127 0.0003 0.1655Table 2: Two-way ANOVA p-values.Table 1: Breakdown of average Likert question responses for each summary at the two levels of controversiali-ty as well as the difference between SEA and MEAD*.Controversial UncontroversialSEA MEAD* (SEA ?
MEAD*)SEA MEAD* (SEA ?
MEAD*)Question Mean St. Dev.
Mean St. Dev.
Mean St. Dev.
Mean St. Dev.
Mean St. Dev.
Mean St. Dev.Grammaticality 4.5 0.53 3.4 1.26 1.1 0.99 4.2 0.92 2.78 1.3 1.56 1.51Non-redundancy 4.2 0.92 4 1.07 0.25 1.58 3.7 0.95 3.8 1.14 -0.1 1.45Referential clarity 4.5 0.53 3.44 1.33 1 1.22 4.2 1.03 3.5 1.18 0.7 1.34Focus 4.11 1.27 2.1 0.88 2.22 0.83 3.9 1.1 2.6 1.35 1.3 1.57Structure and Coherence 4.1 0.99 1.9 0.99 2.2 1.14 3.8 1.4 2.3 1.06 1.5 1.9Linguistic 4.29 0.87 2.91 1.35 1.39 1.34 3.96 1.07 3 1.29 0.98 1.63Recall 2.8 1.32 1.8 1.23 1 1.33 2.5 1.27 2.5 1.43 0 1.89Precision 3.9 1.1 2.7 1.64 1.2 1.23 3.5 1.27 3.3 0.95 0.2 1.93Accuracy 3.4 0.97 3.3 1.57 0.1 1.2 3.1 1.52 3.2 1.03 -0.1 2.28Content 3.37 1.19 2.6 1.57 0.77 1.3 3.03 1.38 3 1.17 0.03 1.97Footnote 4 1.05 3.9 0.88 0.1 1.66 3.6 1.07 3.5 1.35 0.1 1.6Overall 3.8 0.79 2.4 1.17 1.4 1.07 3.2 1.23 2.7 0.82 0.5 1.84Macro ?
Footnote 3.92 1.06 2.75 1.41 1.17 1.32 3.57 1.26 2.97 1.2 0.61 1.81Macro 3.93 1.05 2.87 1.4 1.06 1.39 3.57 1.24 3.02 1.22 0.56 1.7939some  features  where  none existed,  and problemswith reliability were not mentioned.
?Participants disagreed on the information cover-age of the MEAD* summary.
One participant saidthat MEAD* includes ?almost all the informationabout the Apex 2600 DVD player?, while anothersaid that it ?does not reflect all information fromthe customer reviews.
?In the  uncontroversial  subcorpora,  more  userscriticized SEA for its inaccuracy in content selec-tion.
One participant felt that SEA ?made general-izations that were not precise or accurate.?
Partici-pants  had  specific  comments  about  the  featuresthat SEA mentioned that they did not consider im-portant.
For  example,  one  comment  was  that?Compatibility with CDs was not a general prob-lem,  nor were issues with the remote  control,  orvideo output (when it worked).?
MEAD* was criti-cized for being ?overly specific?, but users praisedMEAD* for being ?not at all redundant?, and saidthat it ?included information I felt was important.
?7 Conclusion and Future WorkWe have explored the controversiality of opinionsin a corpus of evaluative text as an aspect whichmay determine how well abstractive and extractivesummarization  strategies  perform.
We  have  pre-sented a novel measure of controversiality, and re-ported on the results of a user study which suggestthat abstraction by NLG outperforms extraction bya larger amount in more controversial corpora.
Wehave also presented a document structuring strate-gy for summarization of controversial corpora.Our  work  has  implications  in  practical  deci-sions on summarization strategy choice; an extrac-tive approach, which may be easier to implementbecause of its lack of requirement for natural lan-guage generation, may suffice if the controversiali-ty of opinions in a corpus is sufficiently low.A future approach to summarization of evalua-tive text might combine extraction and abstractionin  order  to  combine  the  different  strengths  thateach bring to the summary.
The controversiality ofthe corpus might be one factor determining the mixof abstraction and extraction in the summary.
Thefootnotes linking to sample sentences in the corpusin SEA are already one form of this combined ap-proach.
Further  work  is  needed  to  integrate  thistext into the summary itself, possibly in a modifiedform.As a final note to our user study, further studiesshould be done with different corpora and summa-rization systems to increase the external validity ofour results.AcknowledgementsWe would like to thank Raymond T. Ng, GabrielMurray  and  Lucas  Rizoli  for  their  helpful  com-ments.
This work was partially funded by the Nat-ural Sciences and Engineering Research Council ofCanada.ReferencesRegina Barzilay,  Kathleen R. McKeown, and MichaelElhadad.
1999.
Information fusion in the context ofmulti-document summarization.
In  Proc.
37th ACL,pages 550?557.Giuseppe Carenini and Johanna D. Moore.
2006.
Gener-ating and evaluating evaluative arguments.
ArtificialIntelligence, 170(11):925-952.Giuseppe  Carenini,  Raymond  Ng  and  Adam  Pauls.2006.
Multi-document  summarization  of  evaluativetext.
In Proc.
11th EACL 2006, pages 305-312.Giuseppe  Carenini,  Raymond  T.  Ng  and  Ed  Zwart.2005.
Extracting Knowledge from Evaluative Text.In Proc.
3rd International Conference on KnowledgeCapture, pages 11-18.Giuseppe  Carenini  and  Johanna  D.  Moore.
2000.
Astrategy for generating evaluative arguments.
In FirstINLG, pages 47-54, Mitzpe Ramon, Israel.Barbara Di Eugenio, Michael Glass, and Michael J. Tro-lio.
2002.
The DIAG experiments: Natural languagegeneration  for  intelligent  tutoring  systems.
In  INL-G02, The 2nd INLG, pages 120-127.Joseph L. Fleiss.
1971.
Measuring nominal scale agree-ment  among  many  raters.
Psychological  Bulletin.76:378-382.U.
Hahn and I. Mani.
2000.
The challenges of automaticsummarization.
IEEE Computer, 33(11):29-36.Ken  Hinckley,  Randy  Pausch,  Dennis  Proffitt,  JamesPatten, and Neal Kassell.
1997.
Cooperative bimanu-al action.
In Proc.
CHI Conference.Minqing Hu and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In  Proc.
10th ACM SIGKDDconference, pages 168-177.Klaus Krippendorff.
1980.
Content Analysis: An Intro-duction to Its Methodology.
Sage Publications, Bev-erly Hills, CA.Chin-Yew Lin.
2004.
ROUGE: A Package for Automat-ic Evaluation of Summaries.
In  Proc.
of  Workshopon Text Summarization Branches Out, Post-Confer-ence Workshop of ACL 2004, Barcelona, Spain.402005.
Linguistic quality questions from the 2005 docu-ment  understanding  conference.http://duc.nist.gov/duc2005/quality-questions.txtKathleen McKeown, Rebecca Passonneau, David Elson,Ani Nenkova, and Julia Hirschberg.
2005.
Do sum-maries help?
A task-based evaluation of multi-docu-ment summarization.
In Proc.
SIGIR 2005.Ani  Nenkova,  Rebecca  J.  Passonneau,  and K.  McKe-own.
2007.
The pyramid method: incorporating hu-man  content  selection  variation  in  summarizationevaluation.
ACM Transactions on Speech and Lan-guage Processing, 4(2).Ani Nenkova and Rebecca J. Passonneau.
2004.
Evalu-ating content selection in summarization: The pyra-mid method.
In Proc.
NAACL/HLT.Rebecca  J.  Passonneau,  Kathleen  McKeown,  SergeySigleman, and Adam Goodkind.
2006.
Applying thepyramid method in the 2006 Document Understand-ing Conference.
In Proc.
DUC'06.Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-own, and Sergey Sigleman.
2005.
Applying the pyra-mid method in DUC 2005.
In Proc.
DUC'05.Rebecca J. Passonneau.
1997.
Applying Reliability Met-rics  to  Co-Reference  Annotation.
Department  ofComputer  Science,  Columbia  University,  TRCUCS-017-97.Dragomir  Radev,  Hongyan  Jing,  and  MalgorzataBudzikowska.
2000.
Centroid-based  summarizationof  multiple  documents:  sentence  extraction,  utility-based  evaluation,  and  user  studies.
In  Proc.ANLP/NAACL Workshop on Automatic Summariza-tion.S.
Siegel and N. J. Castellan, Jr. 1988.
Nonparametricstatistics for the behaviorial sciences.
McGraw Hill.Appendix A.
Additional QuestionsFootnotes: a) Did you use the footnotes when re-viewing the summary?b) Answer this question only if you answered?Yes?
to the previous question.
The clickable foot-notes were a helpful addition to the summary.Summary Comparison Questions:1) List any Pros and Cons you can think of foreach of the summaries.
Point form is okay.2) Overall, which summary did you prefer?3) Why did you  prefer  this  summary?
(If  thereason overlaps with some points from question 1,put a star next to those points in the chart.
)4) Do you have any other comments about thereviews or summaries, the tasks, or the experimentin general?
If so, please write them below.41
