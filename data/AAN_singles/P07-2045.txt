Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177?180,Prague, June 2007. c?2007 Association for Computational LinguisticsMoses: Open Source Toolkit for Statistical Machine TranslationPhilipp KoehnHieu HoangAlexandra BirchChris Callison-BurchUniversity of Edin-burgh1Marcello FedericoNicola BertoldiITC-irst2Brooke CowanWade ShenChristine MoranMIT3Richard ZensRWTH Aachen4Chris DyerUniversity of Maryland5Ond?ej BojarCharles University6Alexandra ConstantinWilliams College7Evan HerbstCornell81 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk.2{federico, bertoldi}@itc.it.
3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu.
4zens@i6.informatik.rwth-aachen.de.
5 redpony@umd.edu.
6 bojar@ufal.ms.mff.cuni.cz.
707aec_2@williams.edu.
8 evh4@cornell.eduAbstractWe describe an open-source toolkit for sta-tistical machine translation whose novelcontributions are (a) support for linguisti-cally motivated factors, (b) confusion net-work decoding, and (c) efficient data for-mats for translation models and languagemodels.
In addition to the SMT decoder,the toolkit also includes a wide variety oftools for training, tuning and applying thesystem to many translation tasks.1 MotivationPhrase-based statistical machine translation(Koehn et al 2003) has emerged as the dominantparadigm in machine translation research.
How-ever, until now, most work in this field has beencarried out on proprietary and in-house researchsystems.
This lack of openness has created a highbarrier to entry for researchers as many of thecomponents required have had to be duplicated.This has also hindered effective comparisons of thedifferent elements of the systems.By providing a free and complete toolkit, wehope that this will stimulate the development of thefield.
For this system to be adopted by the commu-nity, it must demonstrate performance that is com-parable to the best available systems.
Moses hasshown that it achieves results comparable to themost competitive and widely used statistical ma-chine translation systems in translation quality andrun-time (Shen et al 2006).
It features all the ca-pabilities of the closed sourced Pharaoh decoder(Koehn 2004).Apart from providing an open-source toolkitfor SMT, a further motivation for Moses is to ex-tend phrase-based translation with factors and con-fusion network decoding.The current phrase-based approach to statisti-cal machine translation is limited to the mapping ofsmall text chunks without any explicit use of lin-guistic information, be it morphological, syntactic,or semantic.
These additional sources of informa-tion have been shown to be valuable when inte-grated into pre-processing or post-processing steps.Moses also integrates confusion network de-coding, which allows the translation of ambiguousinput.
This enables, for instance, the tighter inte-gration of speech recognition and machine transla-tion.
Instead of passing along the one-best outputof the recognizer, a network of different wordchoices may be examined by the machine transla-tion system.Efficient data structures in Moses for thememory-intensive translation model and languagemodel allow the exploitation of much larger dataresources with limited hardware.1772 ToolkitThe toolkit is a complete out-of-the-box trans-lation system for academic research.
It consists ofall the components needed to preprocess data, trainthe language models and the translation models.
Italso contains tools for tuning these models usingminimum error rate training (Och 2003) and evalu-ating the resulting translations using the BLEUscore (Papineni et al 2002).Moses uses standard external tools for some ofthe tasks to avoid duplication, such as GIZA++(Och and Ney 2003) for word alignments andSRILM for language modeling.
Also, since thesetasks are often CPU intensive, the toolkit has beendesigned to work with Sun Grid Engine parallelenvironment to increase throughput.In order to unify the experimental stages, autility has been developed to run repeatable ex-periments.
This uses the tools contained in Mosesand requires minimal changes to set up and cus-tomize.The toolkit has been hosted and developed un-der sourceforge.net since inception.
Moses has anactive research community and has reached over1000 downloads as of 1st March 2007.The main online presence is athttp://www.statmt.org/moses/where many sources of information about theproject can be found.
Moses was the subject of thisyear?s Johns Hopkins University Workshop onMachine Translation (Koehn et al 2006).The decoder is the core component of Moses.To minimize the learning curve for many research-ers, the decoder was developed as a drop-in re-placement for Pharaoh, the popular phrase-baseddecoder.In order for the toolkit to be adopted by thecommunity, and to make it easy for others to con-tribute to the project, we kept to the followingprinciples when developing the decoder:?
Accessibility?
Easy to Maintain?
Flexibility?
Easy for distributed team development?
PortabilityIt was developed in C++ for efficiency and fol-lowed modular, object-oriented design.3 Factored Translation ModelNon-factored SMT typically deals only withthe surface form of words and has one phrase table,as shown in Figure 1.i am buying you a green catusing phrase dictionary:iam buyingyouagreencatjeach?tevousunvertchata uneje vous ach?te un chat vertTranslate:In factored translation models, the surfaceforms may be augmented with different factors,such as POS tags or lemma.
This creates a factoredrepresentation of each word, Figure 2.1 1 1 / sing /je vous achet un chatPRO PRO VB ART NNje vous acheter un chatst st st present masc masc?
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
?1 1 / 1 sing singi buy you a catPRO VB PRO ART NNi tobuy you a catst st present st?
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
??
?Mapping of source phrases to target phrasesmay be decomposed into several steps.
Decompo-sition of the decoding process into various stepsmeans that different factors can be modeled sepa-rately.
Modeling factors in isolation allows forflexibility in their application.
It can also increaseaccuracy and reduce sparsity by minimizing thenumber dependencies for each step.For example, we can decompose translatingfrom surface forms to surface forms and lemma, asshown in Figure 3.Figure 2.
Factored translationFigure 1.
Non-factored translation178Figure 3.
Example of graph of decoding stepsBy allowing the graph to be user definable, wecan experiment to find the optimum configurationfor a given language pair and available data.The factors on the source sentence are consid-ered fixed, therefore, there is no decoding stepwhich create source factors from other source fac-tors.
However, Moses can have ambiguous input inthe form of confusion networks.
This input typehas been used successfully for speech to texttranslation (Shen et al 2006).Every factor on the target language can have itsown language model.
Since many factors, likelemmas and POS tags, are less sparse than surfaceforms, it is possible to create a higher order lan-guage models for these factors.
This may encour-age more syntactically correct output.
In Figure 3we apply two language models, indicated by theshaded arrows, one over the words and anotherover the lemmas.
Moses is also able to integratefactored language models, such as those describedin (Bilmes and Kirchhoff 2003) and (Axelrod2006).4 Confusion Network DecodingMachine translation input currently takes theform of simple sequences of words.
However,there are increasing demands to integrate machinetranslation technology into larger informationprocessing systems with upstream NLP/speechprocessing tools (such as named entity recognizers,speech recognizers, morphological analyzers, etc.
).These upstream processes tend to generate multiple,erroneous hypotheses with varying confidence.Current MT systems are designed to process onlyone input hypothesis, making them vulnerable toerrors in the input.In experiments with confusion networks, wehave focused so far on the speech translation case,where the input is generated by a speech recog-nizer.
Namely, our goal is to improve performanceof spoken language translation by better integratingspeech recognition and machine translation models.Translation from speech input is considered moredifficult than translation from text for several rea-sons.
Spoken language has many styles and genres,such as, formal read speech, unplanned speeches,interviews, spontaneous conversations; it producesless controlled language, presenting more relaxedsyntax and spontaneous speech phenomena.
Fi-nally, translation of spoken language is prone tospeech recognition errors, which can possibly cor-rupt the syntax and the meaning of the input.There is also empirical evidence that bettertranslations can be obtained from transcriptions ofthe speech recognizer which resulted in lowerscores.
This suggests that improvements can beachieved by applying machine translation on alarge set of transcription hypotheses generated bythe speech recognizers and by combining scores ofacoustic models, language models, and translationmodels.Recently, approaches have been proposed forimproving translation quality through the process-ing of multiple input hypotheses.
We have imple-mented in Moses confusion network decoding asdiscussed in (Bertoldi and Federico 2005), and de-veloped a simpler translation model and a moreefficient implementation of the search algorithm.Remarkably, the confusion network decoder re-sulted in an extension of the standard text decoder.5 Efficient Data Structures for Transla-tion Model and Language ModelsWith the availability of ever-increasingamounts of training data, it has become a challengefor machine translation systems to cope with theresulting strain on computational resources.
Insteadof simply buying larger machines with, say, 12 GBof main memory, the implementation of more effi-cient data structures in Moses makes it possible toexploit larger data resources with limited hardwareinfrastructure.A phrase translation table easily takes up giga-bytes of disk space, but for the translation of a sin-gle sentence only a tiny fraction of this table isneeded.
Moses implements an efficient representa-tion of the phrase translation table.
Its key proper-ties are a prefix tree structure for source words andon demand loading, i.e.
only the fraction of thephrase table that is needed to translate a sentence isloaded into the working memory of the decoder.179For the Chinese-English NIST  task, the mem-ory requirement of the phrase table is reduced from1.7 gigabytes to less than 20 mega bytes, with noloss in translation quality and speed (Zens and Ney2007).The other large data resource for statistical ma-chine translation is the language model.
Almostunlimited text resources can be collected from theInternet and used as training data for languagemodeling.
This results in language models that aretoo large to easily fit into memory.The Moses system implements a data structurefor language models that is more efficient than thecanonical SRILM (Stolcke 2002) implementationused in most systems.
The language model on diskis also converted into this binary format, resultingin a minimal loading time during start-up of thedecoder.An even more compact representation of thelanguage model is the result of the quantization ofthe word prediction and back-off probabilities ofthe language model.
Instead of representing theseprobabilities with 4 byte or 8 byte floats, they aresorted into bins, resulting in (typically) 256 binswhich can be referenced with a single 1 byte index.This quantized language model, albeit being lessaccurate, has only minimal impact on translationperformance (Federico and Bertoldi 2006).6 Conclusion and Future WorkThis paper has presented a suite of open-sourcetools which we believe will be of value to the MTresearch community.We have also described a new SMT decoderwhich can incorporate some linguistic features in aconsistent and flexible framework.
This new direc-tion in research opens up many possibilities andissues that require further research and experimen-tation.
Initial results show the potential benefit offactors for statistical machine translation, (Koehnet al 2006) and (Koehn and Hoang 2007).ReferencesAxelrod, Amittai.
"Factored Language Model for Sta-tistical Machine Translation."
MRes Thesis.Edinburgh University, 2006.Bertoldi, Nicola, and Marcello Federico.
"A New De-coder for Spoken Language Translation Basedon Confusion Networks."
Automatic SpeechRecognition and Understanding Workshop(ASRU), 2005.Bilmes, Jeff A, and Katrin Kirchhoff.
"Factored Lan-guage Models and Generalized Parallel Back-off."
HLT/NACCL, 2003.Koehn, Philipp.
"Pharaoh: A Beam Search Decoder forPhrase-Based Statistical Machine TranslationModels."
AMTA, 2004.Koehn, Philipp, Marcello Federico, Wade Shen, NicolaBertoldi, Ondrej Bojar, Chris Callison-Burch,Brooke Cowan, Chris Dyer, Hieu Hoang,Richard Zens, Alexandra Constantin, ChristineCorbett Moran, and Evan Herbst.
"OpenSource Toolkit for Statistical Machine Transla-tion".
Report of the 2006 Summer Workshop atJohns Hopkins University, 2006.Koehn, Philipp, and Hieu Hoang.
"Factored TranslationModels."
EMNLP, 2007.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
"Statistical Phrase-Based Translation.
"HLT/NAACL, 2003.Och, Franz Josef.
"Minimum Error Rate Training forStatistical Machine Translation."
ACL, 2003.Och, Franz Josef, and Hermann Ney.
"A SystematicComparison of Various Statistical AlignmentModels."
Computational Linguistics 29.1(2003): 19-51.Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
"BLEU: A Method for AutomaticEvaluation of Machine Translation."
ACL,2002.Shen, Wade, Richard Zens, Nicola Bertoldi, andMarcello Federico.
"The JHU Workshop 2006Iwslt System."
International Workshop on Spo-ken Language Translation, 2006.Stolcke, Andreas.
"SRILM an Extensible LanguageModeling Toolkit."
Intl.
Conf.
on Spoken Lan-guage Processing, 2002.Zens, Richard, and Hermann Ney.
"Efficient Phrase-Table Representation for Machine Translationwith Applications to Online MT and SpeechRecognition."
HLT/NAACL, 2007.180
