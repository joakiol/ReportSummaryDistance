Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73?80,Dublin, Ireland, August 23-24, 2014.SemEval-2014 Task 9: Sentiment Analysis in TwitterSara RosenthalColumbia Universitysara@cs.columbia.eduPreslav NakovQatar Computing Research Institutepnakov@qf.org.qaAlan RitterCarnegie Mellon Universityrittera@cs.cmu.eduVeselin StoyanovJohns Hopkins Universityves@cs.jhu.eduAbstractWe describe the Sentiment Analysis inTwitter task, ran as part of SemEval-2014.It is a continuation of the last year?s taskthat ran successfully as part of SemEval-2013.
As in 2013, this was the most popu-lar SemEval task; a total of 46 teams con-tributed 27 submissions for subtask A (21teams) and 50 submissions for subtask B(44 teams).
This year, we introduced threenew test sets: (i) regular tweets, (ii) sarcas-tic tweets, and (iii) LiveJournal sentences.We further tested on (iv) 2013 tweets, and(v) 2013 SMS messages.
The highest F1-score on (i) was achieved by NRC-Canadaat 86.63 for subtask A and by TeamX at70.96 for subtask B.1 IntroductionIn the past decade, new forms of communica-tion have emerged and have become ubiquitousthrough social media.
Microblogs (e.g., Twitter),Weblogs (e.g., LiveJournal) and cell phone mes-sages (SMS) are often used to share opinions andsentiments about the surrounding world, and theavailability of social content generated on sitessuch as Twitter creates new opportunities to au-tomatically study public opinion.Working with these informal text genrespresents new challenges for natural language pro-cessing beyond those encountered when work-ing with more traditional text genres such asnewswire.
The language in social media is veryinformal, with creative spelling and punctuation,misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RTfor re-tweet and #hashtags1.This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1Hashtags are a type of tagging for Twitter messages.Moreover, tweets and SMS messages are short:a sentence or a headline rather than a document.How to handle such challenges so as to automat-ically mine and understand people?s opinions andsentiments has only recently been the subject ofresearch (Jansen et al., 2009; Barbosa and Feng,2010; Bifet et al., 2011; Davidov et al., 2010;O?Connor et al., 2010; Pak and Paroubek, 2010;Tumasjan et al., 2010; Kouloumpis et al., 2011).Several corpora with detailed opinion and sen-timent annotation have been made freely avail-able, e.g., the MPQA newswire corpus (Wiebe etal., 2005), the movie reviews corpus (Pang et al.,2002), or the restaurant and laptop reviews cor-pora that are part of this year?s SemEval Task 4(Pontiki et al., 2014).
These corpora have provedvery valuable as resources for learning about thelanguage of sentiment in general, but they do notfocus on tweets.
While some Twitter sentimentdatasets were created prior to SemEval-2013, theywere either small and proprietary, such as the i-sieve corpus (Kouloumpis et al., 2011) or focusedsolely on message-level sentiment.Thus, the primary goal of our SemEval task isto promote research that will lead to better un-derstanding of how sentiment is conveyed in So-cial Media.
Toward that goal, we created the Se-mEval Tweet corpus as part of our inaugural Sen-timent Analysis in Twitter Task, SemEval-2013Task 2 (Nakov et al., 2013).
It contains tweetsand SMS messages with sentiment expressions an-notated with contextual phrase-level and message-level polarity.
This year, we extended the corpusby adding new tweets and LiveJournal sentences.Another interesting phenomenon that has beenstudied in Twitter is the use of the #sarcasm hash-tag to indicate that a tweet should not be taken lit-erally (Gonz?alez-Ib?a?nez et al., 2011; Liebrecht etal., 2013).
In fact, sarcasm indicates that the mes-sage polarity should be flipped.
With this in mind,this year, we also evaluate on sarcastic tweets.73In the remainder of this paper, we first describethe task, the dataset creation process and the eval-uation methodology.
We then summarize the char-acteristics of the approaches taken by the partici-pating systems, and we discuss their scores.2 Task DescriptionAs SemEval-2013 Task 2, we included two sub-tasks: an expression-level subtask and a message-level subtask.
Participants could choose to partici-pate in either or both.
Below we provide short de-scriptions of the objectives of these two subtasks.Subtask A: Contextual Polarity DisambiguationGiven a message containing a marked in-stance of a word or a phrase, determinewhether that instance is positive, negative orneutral in that context.
The instance bound-aries were provided: this was a classificationtask, not an entity recognition task.Subtask B: Message Polarity ClassificationGiven a message, decide whether it is ofpositive, negative, or neutral sentiment.For messages conveying both positive andnegative sentiment, the stronger one is to bechosen.Each participating team was allowed to submitresults for two different systems per subtask: oneconstrained, and one unconstrained.
A constrainedsystem could only use the provided data for train-ing, but it could also use other resources such aslexicons obtained elsewhere.
An unconstrainedsystem could use any additional data as part ofthe training process; this could be done in a super-vised, semi-supervised, or unsupervised fashion.Note that constrained/unconstrained refers tothe data used to train a classifier.
For example,if other data (excluding the test data) was used todevelop a sentiment lexicon, and the lexicon wasused to generate features, the system would stillbe constrained.
However, if other data (excludingthe test data) was used to develop a sentiment lexi-con, and this lexicon was used to automatically la-bel additional Tweet/SMS messages and then usedwith the original data to train the classifier, thensuch a system would be considered unconstrained.3 DatasetsIn this section, we describe the process of collect-ing and annotating the 2014 testing tweets, includ-ing the sarcastic ones, and LiveJournal sentences.Corpus Positive Negative Objective/ NeutralTwitter2013-train 5,895 3,131 471Twitter2013-dev 648 430 57Twitter2013-test 2,734 1,541 160SMS2013-test 1,071 1,104 159Twitter2014-test 1,807 578 88Twitter2014-sarcasm 82 37 5LiveJournal2014-test 660 511 144Table 1: Dataset statistics for Subtask A.3.1 Datasets UsedFor training and development, we released theTwitter train/dev/test datasets from SemEval-2013task 2, as well as the SMS test set, which uses mes-sages from the NUS SMS corpus (Chen and Kan,2013), which we annotated for sentiment in 2013.We further added a new 2014 Twitter test set,as well as a small set of tweets that containedthe #sarcasm hashtag to determine how sarcasmaffects the tweet polarity.
Finally, we includedsentences from LiveJournal in order to determinehow systems trained on Twitter perform on othersources.
The statistics for each dataset and foreach subtask are shown in Tables 1 and 2.Corpus Positive Negative Objective/ NeutralTwitter2013-train 3,662 1,466 4,600Twitter2013-dev 575 340 739Twitter2013-test 1,572 601 1,640SMS2013-test 492 394 1,207Twitter2014-test 982 202 669Twitter2014-sarcasm 33 40 13LiveJournal2014-test 427 304 411Table 2: Dataset statistics for Subtask B.3.2 AnnotationWe annotated the new tweets as in 2013: by iden-tifying tweets from popular topics that containsentiment-bearing words by using SentiWordNet(Baccianella et al., 2010) as a filter.
We altered theannotation task for the sarcastic tweets, displayingthem to the Mechanical Turk annotators withoutthe #sarcasm hashtag; the Turkers had to deter-mine whether the tweet is sarcastic on their own.Moreover, we asked Turkers to indicate the degreeof sarcasm as (a) definitely sarcastic, (b) probablysarcastic, and (c) not sarcastic.As in 2013, we combined the annotations usingintersection, where a word had to appear in 2/3of the annotations to be accepted.
An annotatedexample from each source is shown in Table 3.74Source Example PolarityTwitter Why would you [still]- wear shorts when it?s this cold?!
I [love]+ how Britain see?s abit of sun and they?re [like ?OOOH]+ LET?S STRIP!
?positiveSMS [Sorry]- I think tonight [cannot]- and I [not feeling well]- after my rest.
negativeLiveJournal [Cool]+ posts , dude ; very [colorful]+ , and [artsy]+ .
positiveTwitter Sarcasm [Thanks]+ manager for putting me on the schedule for Sunday negativeTable 3: Example of polarity for each source of messages.
The target phrases are marked in [.
.
.
], andare followed by their polarity; the sentence-level polarity is shown in the last column.3.3 Tweets DeliveryWe did not deliver the annotated tweets to the par-ticipants directly; instead, we released annotationindexes, a list of corresponding Twitter IDs, anda download script that extracts the correspond-ing tweets via the Twitter API.2We provided thetweets in this manner in order to ensure that Twit-ter?s terms of service are not violated.
Unfor-tunately, due to this restriction, the task partici-pants had access to different number of trainingtweets depending on when they did the download-ing.
This varied between a minimum of 5,215tweets and the full set of 10,882 tweets.
On av-erage the teams were able to collect close to 9,000tweets; for teams that did not participate in 2013,this was about 8,500.
The difference in trainingdata size did not seem to have had a major impact.In fact, the top two teams in subtask B (coooollland TeamX) trained on less than 8,500 tweets.4 ScoringThe participating systems were required to per-form a three-way classification for both subtasks.A particular marked phrase (for subtask A) or anentire message (for subtask B) was to be classi-fied as positive, negative or objective/neutral.
Wescored the systems by computing a score for pre-dicting positive/negative phrases/messages.
Forinstance, to compute positive precision, ppos, wefind the number of phrases/messages that a sys-tem correctly predicted to be positive, and we di-vide that number by the total number it predictedto be positive.
To compute positive recall, rpos,we find the number of phrases/messages correctlypredicted to be positive and we divide that numberby the total number of positives in the gold stan-dard.
We then calculate F1-score for the positiveclass as follows Fpos=2(ppos+rpos)ppos?rpos.
We carryout a similar computation for Fneg, for the nega-tive phrases/messages.
The overall score is thenF = (Fpos+ Fneg)/2.2https://dev.twitter.comWe used the two test sets from 2013 and thethree from 2014, which we combined into one testset and we shuffled to make it hard to guess whichset a sentence came from.
This guaranteed thatparticipants would submit predictions for all fivetest sets.
It also allowed us to test how well sys-tems trained on standard tweets generalize to sar-castic tweets and to LiveJournal sentences, with-out the participants putting extra efforts into this.The participants were also not informed about thesource the extra test sets come from.We provided the participants with a scorer thatoutputs the overall score F and a confusion matrixfor each of the five test sets.5 Participants and ResultsThe results are shown in Tables 4 and 5, and theteam affiliations are shown in Table 6.
Tables 4and 5 contain results on the two progress test sets(tweets and SMS messages), which are the officialtest sets from the 2013 edition of the task, and onthe three new official 2014 testsets (tweets, tweetswith sarcasm, and LiveJournal).
The tables fur-ther show macro- and micro-averaged results overthe 2014 datasets.
There is an index for each re-sult showing the relative rank of that result withinthe respective column.
The participating systemsare ranked by their score on the Twitter-2014 test-set, which is the official ranking for the task; allremaining rankings are secondary.As we mentioned above, the participants werenot told that the 2013 test sets would be includedin the big 2014 test set, so that they do not over-tune their systems on them.
However, the 2013test sets were made available for development, butit was explicitly forbidden to use them for training.Still, some participants did not notice this restric-tion, which resulted in their unusually high scoreson Twitter2013-test; we did our best to identifyall such cases, and we asked the authors to submitcorrected runs.
The tables mark such resubmis-sions accordingly.75Most of the submissions were constrained, withjust a few unconstrained: 7 out of 27 for subtaskA, and 8 out of 50 for subtask B.
In any case, thebest systems were constrained.
Some teams par-ticipated with both a constrained and an uncon-strained system, but the unconstrained system wasnot always better than the constrained one: some-times it was worse, sometimes it performed thesame.
Thus, we decided to produce a single rank-ing, including both constrained and unconstrainedsystems, where we mark the latter accordingly.5.1 Subtask ATable 4 shows the results for subtask A, which at-tracted 27 submissions from 21 teams.
There wereseven unconstrained submissions: five teams sub-mitted both a constrained and an unconstrainedrun, and two teams submitted an unconstrainedrun only.
The best systems were constrained.
Allparticipating systems outperformed the majorityclass baseline by a sizable margin.5.2 Subtask BThe results for subtask B are shown in Table 5.The subtask attracted 50 submissions from 44teams.
There were eight unconstrained submis-sions: six teams submitted both a constrained andan unconstrained run, and two teams submitted anunconstrained run only.
As for subtask A, the bestsystems were constrained.
Again, all participatingsystems outperformed the majority class baseline;however, some systems were very close to it.6 DiscussionOverall, we observed similar trends as inSemEval-2013 Task 2.
Almost all systems usedsupervised learning.
Most systems were con-strained, including the best ones in all categories.As in 2013, we observed several cases of a teamsubmitting a constrained and an unconstrained runand the constrained run performing better.It is unclear why unconstrained systems did notoutperform constrained ones.
It could be becauseparticipants did not use enough external data orbecause the data they used was too different fromTwitter or from our annotation method.
Or it couldbe due to our definition of unconstrained, whichlabels as unconstrained systems that use additionaltweets directly, but considers unconstrained thosethat use additional tweets to build sentiment lexi-cons and then use these lexicons.As in 2013, the most popular classifiers wereSVM, MaxEnt, and Naive Bayes.
Moreover, twosubmissions used deep learning, coooolll (HarbinInstitute of Technology) and ThinkPositive (IBMResearch, Brazil), which were ranked second andtenth on subtask B, respectively.The features used were quite varied, includ-ing word-based (e.g., word and character n-grams, word shapes, and lemmata), syntactic, andTwitter-specific such as emoticons and abbrevia-tions.
The participants still relied heavily on lex-icons of opinion words, the most popular onesbeing the same as in 2013: MPQA, SentiWord-Net and Bing Liu?s opinion lexicon.
Popular thisyear was also the NRC lexicon (Mohammad etal., 2013), created by the best-performing team in2013, which is top-performing this year as well.Preprocessing of tweets was still a popular tech-nique.
In addition to standard NLP steps suchas tokenization, stemming, lemmatization, stop-word removal and POS tagging, most teams ap-plied some kind of Twitter-specific processingsuch as substitution/removal of URLs, substitu-tion of emoticons, word normalization, abbrevi-ation lookup, and punctuation removal.
Finally,several of the teams used Twitter-tuned NLP toolssuch as part of speech and named entity taggers(Gimpel et al., 2011; Ritter et al., 2011).The similarity of preprocessing techniques,NLP tools, classifiers and features used in 2013and this year is probably partially due to manyteams participating in both years.
As Table 6shows, 18 out of the 46 teams are returning teams.Comparing the results on the progress Twit-ter test in 2013 and 2014, we can see that NRC-Canada, the 2013 winner for subtask A, havenow improved their F1 score from 88.93 to 90.14,which is the 2014 best score.
The best score on theProgress SMS in 2014 of 89.31 belongs to ECNU;this is a big jump compared to their 2013 score of76.69, but it is less compared to the 2013 best of88.37 achieved by GU-MLT-LT. For subtask B, onthe Twitter progress testset, the 2013 winner NRC-Canada improves their 2013 result from 69.02 to70.75, which is the second best in 2014; the win-ner in 2014, TeamX, achieves 72.12.
On the SMSprogress test, the 2013 winner NRC-Canada im-proves its F1 score from 68.46 to 70.28.
Overall,we see consistent improvements on the progresstestset for both subtasks: 0-1 and 2-3 points abso-lute for subtasks A and B, respectively.76Uncon- 2013: Progress 2014: Official 2014: Average# System strain.?
Tweet SMS Tweet Tweet Live- Macro Microsarcasm Journal1 NRC-Canada 90.14188.03486.63177.13585.49283.08285.6112 SentiKLUE 90.11285.16884.83279.32385.61183.25185.1523 CMUQ-Hybrid?88.94487.98584.40376.99684.21381.87384.0534 CMU-Qatar?89.85388.08383.45478.07483.89581.80483.5645 ECNU X 87.29689.26282.93573.71881.69779.44781.8566 ECNU 87.28789.31182.67673.71981.67879.35881.7577 Think Positive X 88.06587.65682.05776.74780.901279.90681.1598 Kea?84.831084.141081.22865.941781.161176.111380.70109 Lt 3 86.28885.26781.02970.761380.441377.411180.331310 senti.ue 84.051178.721680.541082.75181.90681.73581.47811 LyS 85.69981.441279.921171.671083.95478.511082.21512 UKPDIPF 80.451579.051479.671265.631881.42975.571480.331113 UKPDIPF X 80.451679.051579.671365.631981.421075.571580.331214 TJP 81.131484.41979.301471.201278.271576.261278.391515 SAP-RI 80.321780.261377.261570.641477.681875.191777.321616 senti.ue?X 83.801282.931177.071680.02279.701478.93978.831417 SAIL 78.471874.462076.891765.562070.622271.022172.572118 columbia nlp81.501374.551976.541861.762278.191672.161977.111819 IIT-Patna 76.542075.991876.431971.431177.991775.281677.261720 Citius X 76.591969.312175.212068.401575.822073.141875.381921 Citius 74.712161.442573.032165.182171.642169.952271.902222 IITPatna 70.912377.041772.252266.321676.031971.532074.452023 SU-sentilab 74.342262.582468.262353.312569.532363.702468.592324 Univ.
Warwick?62.252660.122667.282458.082464.892563.422565.482525 Univ.
Warwick?X 64.912563.012367.172560.592367.462465.072367.142426 DAEDALUS 67.422463.922260.982645.272761.012655.752660.502627 DAEDALUS X 61.952755.972758.112749.192658.652755.322758.1727Majority baseline 38.1 31.5 42.2 39.8 33.4Table 4: Results for subtask A.
The?indicates system resubmissions (because they initially trained onTwitter2013-test), and theindicates a system that includes a task co-organizer as a team member.
Thesystems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasetsare indicated with a subscript.
The last two columns show macro- and micro-averaged results across thethree 2014 test datasets.Finally, note that for both subtasks, the best sys-tems on the Twitter-2014 dataset are those that per-formed best on the 2013 progress Twitter dataset:NRC-Canada for subtask A, and TeamX (Fuji Xe-rox Co., Ltd.) for subtask B.It is interesting to note that the best resultsfor Twitter2014-test are lower than those forTwitter2013-test for both subtask A (86.63 vs.90.14) and subtask B (70.96 vs 72.12).
This isso despite the baselines for Twitter2014-test be-ing higher than those for Twitter2013-test: 42.2 vs.38.1 for subtask A, and 34.6 vs. 29.2 for subtaskB.
Most likely, having access to Twitter2013-testat development time, teams have overfitted on it.
Itcould be also the case that some of the sentimentdictionaries that were built in 2013 have becomesomewhat outdated by 2014.Finally, note that while some teams such asNRC-Canada performed well across all test sets,other such as TeamX, which used a weightingscheme tuned specifically for class imbalances intweets, were only strong on Twitter datasets.7 ConclusionWe have described the data, the experimentalsetup and the results for SemEval-2014 Task 9.As in 2013, our task was the most popular one atSemEval-2014, attracting 46 participating teams:21 in subtask A (27 submissions) and 44 in sub-task B (50 submissions).We introduced three new test sets for 2014: anin-domain Twitter dataset, an out-of-domain Live-Journal test set, and a dataset of tweets contain-ing sarcastic content.
While the performance onthe LiveJournal test set was mostly comparableto the in-domain Twitter test set, for most teamsthere was a sharp drop in performance for sarcas-tic tweets, highlighting better handling of sarcas-tic language as one important direction for futurework in Twitter sentiment analysis.We plan to run the task again in 2015 with theinclusion of a new sub-evaluation on detecting sar-casm with the goal of stimulating research in thisarea; we further plan to add one more test domain.77Uncon- 2013: Progress 2014: Official 2014: Average# System strain.?
Tweet SMS Tweet Tweet Live- Macro Microsarcasm Journal1 TeamX 72.12157.362670.96156.50369.441565.63369.9952 coooolll 70.40367.68270.14246.662472.90563.231270.5123 RTRGO 69.10567.51369.95347.092372.20663.081370.1534 NRC-Canada 70.75270.28169.85458.16174.84167.62171.3715 TUGAS 65.641362.771169.00552.871269.791363.89668.8486 CISUC KIS?67.56865.90667.95655.49574.46265.97270.0247 SAIL 66.801156.982867.77757.26269.341764.79468.06108 SWISS-CHOCOLATE 64.811866.43567.54849.461673.25463.421069.1569 Synalp-Empathic 63.652362.541267.43951.061571.75963.411168.57910 Think Positive X 68.15763.20967.041047.852166.962460.621866.471511 SentiKLUE 69.06667.40467.021143.363073.99361.461468.94712 JOINT FORCES X 66.611262.201366.791245.402670.021260.741767.391213 AMI ERIC 70.09460.292066.551348.192065.322660.022165.582014 AUEB 63.922164.32866.381456.16470.751164.43567.711115 CMU-Qatar?65.111762.951065.531540.523865.632557.232764.872416 Lt 3 65.561464.78765.471647.762268.562060.601966.121717 columbia nlp64.601959.842165.421740.024068.791958.082565.961918 LyS 66.921060.451964.921842.403369.791459.042266.101819 NILC USP 65.391561.351663.941942.063469.021858.342465.212120 senti.ue 67.34959.342363.812055.31671.391063.50766.381621 UKPDIPF 60.652960.561763.772154.59771.92763.43866.531322 UKPDIPF X 60.653060.561863.772254.59871.92863.43966.531423 SU-FMI?60.962861.671563.622348.341968.242160.072064.912324 ECNU 62.312759.752263.172451.431469.441661.351565.172225 ECNU X 63.722256.732963.042549.331764.083158.822363.042726 Rapanakis 58.523254.023563.012644.692759.713755.803161.283227 Citius X 63.252458.282462.942746.132564.542957.872663.062628 CMUQ-Hybrid?63.222561.751462.712840.953765.142756.273063.002829 Citius 62.532657.692561.922941.003662.403355.113361.513130 KUNLPLab 58.123355.893161.723044.602863.773256.702962.002931 senti.ue?X 65.211656.163061.473154.09968.082261.211663.712532 UPV-ELiRF 63.972055.363359.333237.464264.113053.633760.493333 USP Biocom 58.053453.573659.213343.562967.802356.862861.963034 DAEDALUS X 58.943154.963457.643435.264460.993551.303958.263535 IIT-Patna 52.584051.963757.253541.333560.393652.993857.973636 DejaVu 57.433655.573257.023642.463264.692854.723459.463437 GPLSI 57.493546.634256.063753.901057.324155.763256.473738 BUAP 56.853744.274455.763851.521353.944453.743654.973939 SAP-RI 50.184449.004155.473948.641857.864053.993556.173840 UMCC DLSI Sem 51.964150.013855.404042.763153.124550.434054.204241 IBM EG 54.513846.624352.264134.144659.243848.554354.344142 Alberta 53.853949.054052.064240.403952.384648.284451.854443 lsis lif 46.384638.564752.024334.644561.093449.254154.904044 SU-sentilab 50.174549.603949.524431.494755.114245.374751.094545 SINAI 50.594257.342749.504531.154958.333946.334652.264346 IITPatna 50.324340.564648.224636.734354.684346.544550.294647 Univ.
Warwick 39.174829.504945.564739.774139.604941.644843.194848 UMCC DLSI Graph 43.244736.664845.494853.151147.814748.824246.564749 Univ.
Warwick X 34.235024.635045.114931.404829.345035.284938.884950 DAEDALUS 36.574940.864533.035028.965040.834834.275035.8150Majority baseline 29.2 19.0 34.6 27.7 27.2Table 5: Results for subtask B.
The?indicates system resubmissions (because they initially trained onTwitter2013-test), and theindicates a system that includes a task co-organizer as a team member.
Thesystems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasetsare indicated with a subscript.
The last two columns show macro- and micro-averaged results across thethree 2014 test datasets.In the 2015 edition of the task, we might alsoremove the constrained/unconstrained distinction.Finally, as there are multiple opinions about atopic in Twitter, we would like to focus on detect-ing the sentiment trend towards a topic.AcknowledgementsWe would like to thank Kathleen McKeown andSmaranda Muresan for funding the 2014 Twittertest sets.
We also thank the anonymous reviewers.78Subtasks Team Affiliation 2013?B Alberta University of AlbertaB AMI ERIC AMI Software R&D and Universit?e de Lyon (ERIC LYON 2) yesB AUEB Athens University of Economics and Business yesB BUAP Benem?erita Universidad Aut?onoma de PueblaB CISUC KIS University of CoimbraA, B Citius University of Santiago de CompostelaA, B CMU-Qatar Carnegie Mellon University, QatarA, B CMUQ-Hybrid Carnegie Mellon University, Qatar (different from the above)A, B columbia nlp Columbia University yesB cooolll Harbin Institute of TechnologyA, B DAEDALUS DaedalusB DejaVu Indian Institute of Technology, KanpurA, B ECNU East China Normal University yesB GPLSI University of AlicanteB IBM EG IBM EgyptA, B IITPatna Indian Institute of Technology, PatnaA, B IIT-Patna Indian Institute of Technology, Patna (different from the above)B JOINT FORCES Zurich University of Applied SciencesA Kea York University, Toronto yesB KUNLPLab Koc?
UniversityB lsis lif Aix-Marseille University yesA, B Lt 3 Ghent UniversityA, B LyS Universidade da Coru?naB NILC USP University of S?ao Paulo yesA, B NRC-Canada National Research Council Canada yesB Rapanakis Stamatis RapanakisB RTRGO Retresco GmbH and University of Gothenburg yesA, B SAIL Signal Analysis and Interpretation Laboratory yesA, B SAP-RI SAP Research and InnovationA, B senti.ue Universidade de?Evora yesA, B SentiKLUE Friedrich-Alexander-Universit?at Erlangen-N?urnberg yesB SINAI University of Ja?en yesB SU-FMI Sofia UniversityA, B SU-sentilab Sabanci University yesB SWISS-CHOCOLATE ETH ZurichB Synalp-Empathic University of LorraineB TeamX Fuji Xerox Co., Ltd.A, B Think Positive IBM Research, BrazilA TJP University of Northumbria at Newcastle Upon Tyne yesB TUGAS Instituto de Engenharia de Sistemas e Computadores, yesInvestigac?
?ao e Desenvolvimento em LisboaA, B UKPDIPF Ubiquitous Knowledge Processing LabB UMCC DLSI Graph Universidad de Matanzas and Univarsidad de Alicante yesB UMCC DLSI Sem Universidad de Matanzas and Univarsidad de Alicante (different from above) yesA, B Univ.
Warwick University of WarwickB UPV-ELiRF Universitat Polit`ecnica de Val`enciaB USP Biocom University of S?ao Paulo and Federal University of S?ao CarlosTable 6: Participating teams, their affiliations, subtasks they have taken part in, and an indication aboutwhether the team participated in SemEval-2013 Task 2.79ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Se-bastiani.
2010.
SentiWordNet 3.0: An enhancedlexical resource for sentiment analysis and opinionmining.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation,LREC ?10, Valletta, Malta.Luciano Barbosa and Junlan Feng.
2010.
Robust sen-timent detection on Twitter from biased and noisydata.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,COLING ?10, pages 36?44, Beijing, China.Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,and Ricard Gavald`a.
2011.
Detecting sentimentchange in Twitter streaming data.
Journal of Ma-chine Learning Research, Proceedings Track, 17:5?11.Tao Chen and Min-Yen Kan. 2013.
Creating alive, public short message service corpus: the NUSSMS corpus.
Language Resources and Evaluation,47(2):299?335.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Semi-supervised recognition of sarcasm in Twitterand Amazon.
In Proceedings of the Fourteenth Con-ference on Computational Natural Language Learn-ing, CoNLL ?10, pages 107?116, Uppsala, Sweden.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor Twitter: Annotation, features, and experiments.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, ACL-HLT ?11, pages 42?47, Portland, Oregon, USA.Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, andNina Wacholder.
2011.
Identifying sarcasm in Twit-ter: a closer look.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies - Short Pa-pers, ACL-HLT ?11, pages 581?586, Portland, Ore-gon, USA.Bernard Jansen, Mimi Zhang, Kate Sobel, and AbdurChowdury.
2009.
Twitter power: Tweets as elec-tronic word of mouth.
J.
Am.
Soc.
Inf.
Sci.
Technol.,60(11):2169?2188.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: Thegood the bad and the OMG!
In Proceedings ofthe Fifth International Conference on Weblogs andSocial Media, ICWSM ?11, Barcelona, Catalonia,Spain.Christine Liebrecht, Florian Kunneman, and AntalVan den Bosch.
2013.
The perfect solution for de-tecting sarcasm in tweets #not.
In Proceedings ofthe 4th Workshop on Computational Approaches toSubjectivity, Sentiment and Social Media Analysis,pages 29?37, Atlanta, Georgia, USA.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceed-ings of the Seventh international workshop on Se-mantic Evaluation Exercises, SemEval-2013, pages321?327, Atlanta, Georgia, USA.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
SemEval-2013 task 2: Sentiment analysis inTwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation, SemEval ?13, pages 312?320,Atlanta, Georgia, USA.Brendan O?Connor, Ramnath Balasubramanyan, BryanRoutledge, and Noah Smith.
2010.
From tweetsto polls: Linking text sentiment to public opiniontime series.
In Proceedings of the Fourth Inter-national Conference on Weblogs and Social Media,ICWSM ?10, Washington, DC, USA.Alexander Pak and Patrick Paroubek.
2010.
Twit-ter based system: Using Twitter for disambiguatingsentiment ambiguous adjectives.
In Proceedings ofthe 5th International Workshop on Semantic Evalu-ation, SemEval ?10, pages 436?439, Uppsala, Swe-den.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment classification usingmachine learning techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing - Volume 10, EMNLP ?02, pages79?86.Maria Pontiki, Harris Papageorgiou, Dimitrios Gala-nis, Ion Androutsopoulos, John Pavlopoulos, andSuresh Manandhar.
2014.
SemEval-2014 task 4:Aspect based sentiment analysis.
In Proceedings ofthe 8th International Workshop on Semantic Evalu-ation, SemEval ?14, Dublin, Ireland.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An ex-perimental study.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, EMNLP ?11, pages 1524?1534, Edinburgh,Scotland, UK.Andranik Tumasjan, Timm Sprenger, Philipp Sandner,and Isabell Welpe.
2010.
Predicting elections withTwitter: What 140 characters reveal about politi-cal sentiment.
In Proceedings of the Fourth Inter-national Conference on Weblogs and Social Media,ICWSM ?10, Washington, DC, USA.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2-3):165?210.80
