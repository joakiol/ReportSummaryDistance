Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 710?719,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsKernel Based Discourse Relation Recognition with TemporalOrdering InformationWenTing Wang1                   Jian Su1                   Chew Lim Tan21Institute for Infocomm Research1 Fusionopolis Way, #21-01 ConnexisSingapore 138632{wwang,sujian}@i2r.a-star.edu.sg2Department of Computer ScienceUniversity of SingaporeSingapore 117417tacl@comp.nus.edu.sgAbstractSyntactic knowledge is important for dis-course relation recognition.
Yet only heu-ristically selected flat paths and 2-levelproduction rules have been used to incor-porate such information so far.
In thispaper we propose using tree kernel basedapproach to automatically mine the syn-tactic information from the parse trees fordiscourse analysis, applying kernel func-tion to the tree structures directly.
Thesestructural syntactic features, togetherwith other normal flat features are incor-porated into our composite kernel to cap-ture diverse knowledge for simultaneousdiscourse identification and classificationfor both explicit and implicit relations.The experiment shows tree kernel ap-proach is able to give statistical signifi-cant improvements over flat syntacticpath feature.
We also illustrate that treekernel approach covers more structure in-formation than the production rules,which allows tree kernel to further incor-porate information from a higher dimen-sion space for possible better discrimina-tion.
Besides, we further propose to leve-rage on temporal ordering information toconstrain the interpretation of discourserelation, which also demonstrate statistic-al significant improvements for discourserelation recognition on PDTB 2.0 forboth explicit and implicit as well.1 IntroductionDiscourse relations capture the internal structureand logical relationship of coherent text, includ-ing Temporal, Causal and Contrastive relationsetc.
The ability of recognizing such relations be-tween text units including identifying and classi-fying provides important information to othernatural language processing systems, such aslanguage generation, document summarization,and question answering.
For example, Causalrelation can be used to answer more sophisti-cated, non-factoid ?Why?
questions.Lee et al (2006) demonstrates that modelingdiscourse structure requires prior linguistic anal-ysis on syntax.
This shows the importance ofsyntactic knowledge to discourse analysis.
How-ever, most of previous work only deploys lexicaland semantic features (Marcu and Echihabi,2002; Pettibone and PonBarry, 2003; Saito et al,2006; Ben and James, 2007; Lin et al, 2009; Pit-ler et al, 2009) with only two exceptions (Benand James, 2007; Lin et al, 2009).
Nevertheless,Ben and James (2007) only uses flat syntacticpath connecting connective and arguments in theparse tree.
The hierarchical structured informa-tion in the trees is not well preserved in their flatsyntactic path features.
Besides, such a syntacticfeature selected and defined according to linguis-tic intuition has its limitation, as it remains un-clear what kinds of syntactic heuristics are effec-tive for discourse analysis.The more recent work from Lin et al (2009)uses 2-level production rules to represent parsetree information.
Yet it doesn?t cover all the oth-er sub-trees structural information which can bealso useful for the recognition.In this paper we propose using tree kernelbased method to automatically mine the syntactic710information from the parse trees for discourseanalysis, applying kernel function to the parsetree structures directly.
These structural syntacticfeatures, together with other flat features are thenincorporated into our composite kernel to capturediverse knowledge for simultaneous discourseidentification and classification.
The experimentshows that tree kernel is able to effectively in-corporate syntactic structural information andproduce statistical significant improvements overflat syntactic path feature for the recognition ofboth explicit and implicit relation in Penn Dis-course Treebank (PDTB; Prasad et al, 2008).We also illustrate that tree kernel approach cov-ers more structure information than the produc-tion rules, which allows tree kernel to furtherwork on a higher dimensional space for possiblebetter discrimination.Besides, inspired by the linguistic study ontense and discourse anaphor (Webber, 1988), wefurther propose to incorporate temporal orderinginformation to constrain the interpretation of dis-course relation, which also demonstrates statis-tical significant improvements for discourse rela-tion recognition on PDTB v2.0 for both explicitand implicit relations.The organization of the rest of the paper is asfollows.
We briefly introduce PDTB in Section2.
Section 3 gives the related work on tree kernelapproach in NLP and its difference with produc-tion rules, and also linguistic study on tense anddiscourse anaphor.
Section 4 introduces theframe work for discourse recognition, as well asthe baseline feature space and the SVM classifi-er.
We present our kernel-based method in Sec-tion 5, and the usage of temporal ordering featurein Section 6.
Section 7 shows the experimentsand discussions.
We conclude our works in Sec-tion 8.2 Penn Discourse Tree BankThe Penn Discourse Treebank (PDTB) is thelargest available annotated corpora of discourserelations (Prasad et al, 2008) over 2,312 WallStreet Journal articles.
The PDTB models dis-course relation in the predicate-argument view,where a discourse connective (e.g., but) is treatedas a predicate taking two text spans as its argu-ments.
The argument that the discourse connec-tive syntactically bounds to is called Arg2, andthe other argument is called Arg1.The PDTB provides annotations for both ex-plicit and implicit discourse relations.
An explicitrelation is triggered by an explicit connective.Example (1) shows an explicit Contrast relationsignaled by the discourse connective ?but?.(1).
Arg1.
Yesterday, the retailing and finan-cial services giant reported a 16% drop inthird-quarter earnings to $257.5 million,or 75 cents a share, from a restated $305million, or 80 cents a share, a year earlier.Arg2.
But the news was even worse forSears's core U.S. retailing operation, thelargest in the nation.In the PDTB, local implicit relations are alsoannotated.
The annotators insert a connectiveexpression that best conveys the inferred implicitrelation between adjacent sentences within thesame paragraph.
In Example (2), the annotatorsselect ?because?
as the most appropriate connec-tive to express the inferred Causal relation be-tween the sentences.
There is one special labelAltLex pre-defined for cases where the insertionof an Implicit connective to express an inferredrelation led to a redundancy in the expression ofthe relation.
In Example (3), the Causal relationderived between sentences is alternatively lexi-calized by some non-connective expressionshown in square brackets, so no implicit connec-tive is inserted.
In our experiments, we treat Alt-Lex Relations the same way as normal Implicitrelations.(2).
Arg1.
Some have raised their cash posi-tions to record levels.Arg2.
Implicit = Because High cash po-sitions help buffer a fund when the marketfalls.(3).
Arg1.
Ms. Bartlett?s previous work,which earned her an international reputa-tion in the non-horticultural art world, of-ten took gardens as its nominal subject.Arg2.
[Mayhap this metaphorical con-nection made] the BPC Fine Arts Com-mittee think she had a literal green thumb.The PDTB also captures two non-implicit cas-es: (a) Entity relation where the relation betweenadjacent sentences is based on entity coherence(Knott et al, 2001) as in Example (4); and (b) Norelation where no discourse or entity-based cohe-rence relation can be inferred between adjacentsentences.711(4).
But for South Garden, the grid was to bea 3-D network of masonry or hedge wallswith real plants inside them.In a Letter to the BPCA, kelly/varnellcalled this ?arbitrary and amateurish.
?Each Explicit, Implicit and AltLex relation isannotated with a sense.
The senses in PDTB arearranged in a three-level hierarchy.
The top levelhas four tags representing four major semanticclasses: Temporal, Contingency, Comparisonand Expansion.
For each class, a second level oftypes is defined to further refine the semantic ofthe class levels.
For example, Contingency hastwo types Cause and Condition.
A third level ofsubtype specifies the semantic contribution ofeach argument.
In our experiments, we use onlythe top level of the sense annotations.3 Related WorkTree Kernel based Approach in NLP.
Whilethe feature based approach may not be able tofully utilize the syntactic information in a parsetree, an alternative to the feature-based methods,tree kernel methods (Haussler, 1999) have beenproposed to implicitly explore features in a highdimensional space by employing a kernel func-tion to calculate the similarity between two ob-jects directly.
In particular, the kernel methodscould be very effective at reducing the burden offeature engineering for structured objects in NLPresearch (Culotta and Sorensen, 2004).
This isbecause a kernel can measure the similarity be-tween two discrete structured objects by directlyusing the original representation of the objectsinstead of explicitly enumerating their features.Indeed, using kernel methods to mine structur-al knowledge has shown success in some NLPapplications like parsing (Collins and Duffy,2001; Moschitti, 2004) and relation extraction(Zelenko et al, 2003; Zhang et al, 2006).
How-ever, to our knowledge, the application of such atechnique to discourse relation recognition stillremains unexplored.Lin et al (2009) has explored the 2-level pro-duction rules for discourse analysis.
However,Figure 1 shows that only 2-level sub-tree struc-tures (e.g.
??
- ?? )
are covered in productionrules.
Other sub-trees beyond 2-level (e.g.
??
- ??
)are only captured in the tree kernel, which allowstree kernel to further leverage on informationfrom higher dimension space for possible betterdiscrimination.
Especially, when there areenough training data, this is similar to the studyon language modeling that N-gram beyond uni-gram and bigram further improves the perfor-mance in large corpus.Tense and Temporal Ordering Information.Linguistic studies (Webber, 1988) show that atensed clause ??
provides two pieces of semanticinformation: (a) a description of an event (or sit-uation) ??
; and (b) a particular configuration ofthe point of event (??
), the point of reference(??)
and the point of speech (??).
Both the cha-racteristics of ??
and the configuration of ?
?, ?
?and ??
are critical to interpret the relationship ofevent ??
with other events in the discourse mod-el.
Our observation on temporal ordering infor-mation is in line with the above, which is alsoincorporated in our discourse analyzer.4 The Recognition FrameworkIn the learning framework, a training or testinginstance is formed by a non-overlappingclause(s)/sentence(s) pair.
Specifically, since im-plicit relations in PDTB are defined to be local,only clauses from adjacent sentences are pairedfor implicit cases.
During training, for each dis-course relation encountered, a positive instanceis created by pairing the two arguments.
Also aFigure 1.
Different sub-tree sets for ?1 used by2-level production rules and convolution treekernel approaches.
??
-??
and ?1  itself are cov-ered by tree kernel, while only ??
-??
are coveredby production rules.DecompositionCEGFHABD(?1) AB C(??)
DF E(??)CD(??)
EG(??)FH(??)DEGFH(??)
(??)
ACDBDEGFHC(?? )
C (??
)DF E(??)
ACDBF E712set of negative instances is formed by paringeach argument with neighboring non-argumentclauses or sentences.
Based on the training in-stances, a binary classifier is generated for eachtype using a particular learning algorithm.
Dur-ing resolution, (a) clauses within same sentenceand sentences within three-sentence spans arepaired to form an explicit testing instance; and(b) neighboring sentences within three-sentencespans are paired to form an implicit testing in-stance.
The instance is presented to each explicitor implicit relation classifier which then returns aclass label with a confidence value indicating thelikelihood that the candidate pair holds a particu-lar discourse relation.
The relation with the high-est confidence value will be assigned to the pair.4.1 Base FeaturesIn our system, the base features adopted includelexical pair, distance and attribution etc.
as listedin Table 1.
All these base features have beenproved effective for discourse analysis in pre-vious work.4.2 Support Vector MachineIn theory, any discriminative learning algorithmis applicable to learn the classifier for discourseanalysis.
In our study, we use Support VectorMachine (Vapnik, 1995) to allow the use of ker-nels to incorporate the structure feature.Suppose the training set ?
consists of labeledvectors { ??
,??
}, where ??
is the feature vectorof a training instance and ??
is its class label.
Theclassifier learned by SVM is:?
?
= ???
?????
?
??
+ ?
?=1where ??
is the learned parameter for a featurevector ??
, and ?
is another parameter which canbe derived from ??
.
A testing instance ?
is clas-sified as positive if ?
?
> 01.One advantage of SVM is that we can use treekernel approach to capture syntactic parse treeinformation in a particular high-dimension space.In the next section, we will discuss how to usekernel to incorporate the more complex structurefeature.5 Incorporating Structural SyntacticInformationA parse tree that covers both discourse argu-ments could provide us much syntactic informa-tion related to the pair.
Both the syntactic flatpath connecting connective and arguments andthe 2-level production rules in the parse tree usedin previous study can be directly described by thetree structure.
Other syntactic knowledge thatmay be helpful for discourse resolution couldalso be implicitly represented in the tree.
There-fore, by comparing the common sub-structuresbetween two trees we can find out to which leveltwo trees contain similar syntactic information,which can be done using a convolution tree ker-nel.The value returned from the tree kernel re-flects the similarity between two instances insyntax.
Such syntactic similarity can be furthercombined with other flat linguistic features tocompute the overall similarity between two in-stances through a composite kernel.
And thus anSVM classifier can be learned and then used forrecognition.5.1 Structural Syntactic FeatureParsing is a sentence level processing.
However,in many cases two discourse arguments do notoccur in the same sentence.
To present their syn-tactic properties and relations in a single treestructure, we construct a syntax tree for each pa-ragraph by attaching the parsing trees of all itssentences to an upper paragraph node.
In thispaper, we only consider discourse relations with-in 3 sentences, which only occur within each pa-1 In our task, the result of ?
?
is used as the confidencevalue of the candidate argument pair ?
to hold a particulardiscourse relation.FeatureNamesDescription(F1)  cue phrase(F2) neighboring punctuation(F3)  position of connective ifpresents(F4) extents of arguments(F5)  relative order of  arguments(F6)  distance between  arguments(F7)  grammatical role of  arguments(F8)  lexical pairs(F9) attributionTable 1.
Base Feature Set713ragraph, thus paragraph parse trees are sufficient.Our 3-sentence spans cover 95% discourse rela-tion cases in PDTB v2.0.Having obtained the parse tree of a paragraph,we shall consider how to select the appropriateportion of the tree as the structured feature for agiven instance.
As each instance is related to twoarguments, the structured feature at least shouldbe able to cover both of these two arguments.Generally, the more substructure of the tree isincluded, the more syntactic information wouldbe provided, but at the same time the more noisyinformation would likely be introduced.
In ourstudy, we examine three structured features thatcontain different substructures of the paragraphparse tree:Min-Expansion This feature records the mi-nimal structure covering both argumentsand connective word in the parse tree.
Itonly includes the nodes occurring in theshortest path connecting Arg1, Arg2 andconnective, via the nearest commonlycommanding node.
For example, consi-dering Example (5), Figure 2 illustratesthe representation of the structured featurefor this relation instance.
Note that thetwo clauses underlined with dashed linesare attributions which are not part of therelation.(5).
Arg1.
Suppression of the book, JudgeOakes observed, would operate as a priorrestraint and thus involve the FirstAmendment.Arg2.
Moreover, and here Judge Oakeswent to the heart of the question, ?Respon-sible biographers and historians constantlyuse primary sources, letters, diaries andmemoranda.
?Simple-Expansion Min-Expansion could, tosome degree, describe the syntactic rela-tionships between the connective and ar-guments.
However, the syntactic proper-ties of the argument pair might not becaptured, because the tree structure sur-rounding the argument is not taken intoconsideration.
To incorporate such infor-mation, Simple-Expansion not only con-tains all the nodes in Min-Expansion, butalso includes the first-level children ofthese nodes2.
Figure 3 illustrates such afeature for Example (5).
We can see thatthe nodes ?PRN?
in both sentences are in-cluded in the feature.Full-Expansion This feature focuses on thetree structure between two arguments.
Itnot only includes all the nodes in Simple-Expansion, but also the nodes (beneaththe nearest commanding parent) that cov-er the words between the two arguments.Such a feature keeps the most informationrelated to the argument pair.
Figure 42 We will not expand the nodes denoting the sentences otherthan where the arguments occur.Figure 2.
Min-Expansion tree built from gol-den standard parse tree for the explicit dis-course relation in Example (5).
Note that todistinguish from other words, we explicitlymark up in the structured feature the argumentsand connective, by appending a string tag?Arg1?, ?Arg2?
and ?Connective?
respective-ly.Figure 3.
Simple-Expansion tree for the expli-cit discourse relation in Example (5).714shows the structure for feature Full-Expansion of Example (5).
As illustrated,different from in Simple-Expansion, eachsub-tree of ?PRN?
in each sentence is ful-ly expanded and all its children nodes areincluded in Full-Expansion.5.2 Convolution Parse Tree KernelGiven the parse tree defined above, we use thesame convolution tree kernel as described in(Collins and Duffy, 2002) and (Moschitti, 2004).In general, we can represent a parse tree ?
by avector of integer counts of each sub-tree type(regardless of its ancestors):?
?
= (#??
????????
??
????
1,?
, # ??????????
??????
?,?
, # ??
????????
??????
?
).This results in a very high dimensionalitysince the number of different sub-trees is expo-nential in its size.
Thus, it is computational in-feasible to directly use the feature vector ?(?
).To solve the computational issue, a tree kernelfunction is introduced to calculate the dot prod-uct between the above high dimensional vectorsefficiently.Given two tree segments ?1  and ?2 , the treekernel function is defined:?
?1 ,?2 = < ?
?1 ,?
?2 >=  ?
?1  ?
,?
?2 [?
]?=    ??
?1 ?
??(?2)??2??2?1?
?1where  ?1and ?2 are the sets of all nodes in trees?1and ?2, respectively; and ??(?)
is the indicatorfunction that is 1 iff a subtree of type ?
occurswith root at node ?
or zero otherwise.
(Collinsand Duffy, 2002) shows that ?
(?1 ,?2) is an in-stance of convolution kernels over tree struc-tures, and can be computed in ?
( ?1 ,  ?2 ) bythe following recursive definitions:?
?1 ,?2 =  ??
?1 ?
??(?2)?
(1) ?
?1 ,?2 = 0  if ?1  and ?2  do not have thesame syntactic tag or their children are different;(2) else if both ?1 and  ?2 are pre-terminals (i.e.POS tags), ?
?1 ,?2 = 1 ?
?
;(3)  else, ?
?1 ,?2 =?
(1 + ?(??(??
(?1)?=1 ?1 , ?
), ??
(?2 , ?
))),where ??
(?1) is the number of the children of?1 , ??
(?, ?)
is the ???
child of node ?
and ?
(0 < ?
< 1) is the decay factor in order to makethe kernel value less variable with respect to thesub-tree sizes.
In addition, the recursive rule (3)holds because given two nodes with the samechildren, one can construct common sub-treesusing these children and common sub-trees offurther offspring.The parse tree kernel counts the number ofcommon sub-trees as the syntactic similaritymeasure between two instances.
The time com-plexity for computing this kernel is ?
( ?1 ?
?2 ).5.3 Composite Tree KernelBesides the above convolution parse tree kernel?
????
?1 , ?2 = ?
(?1 ,?2) defined to capture thesyntactic information between two instances ?1and ?2, we also use another kernel ?
????
to cap-ture other flat features, such as base features (de-scribed in Table 1) and temporal ordering infor-mation (described in Section 6).
In our study, thecomposite kernel is defined in the followingway:?
1 ?1 , ?2 = ?
?
?
????
?1 , ?2 +1 ?
?
?
?
????
?1 , ?2 .Here, ?
(?,?)
can be normalized by ?
?, ?
=?
?, ?
?
?, ?
?
?
?, ?
and ?
is the coeffi-cient.6 Using Temporal Ordering Informa-tionIn our discourse analyzer, we also add in tem-poral information to be used as features to pre-dict discourse relations.
This is because both ourobservations and some linguistic studies (Web-ber, 1988) show that temporal ordering informa-tion including tense, aspectual and event ordersbetween two arguments may constrain the dis-course relation type.
For example, the connectiveFigure 4.
Full-Expansion tree for the explicitdiscourse relation in Example (5).715word is the same in both Example (6) and (7),but the tense shift from progressive form inclause 6.a to simple past form in clause 6.b, indi-cating that the twisting occurred during the stateof running the marathon, usually signals a tem-poral discourse relation; while in Example (7),both clauses are in past tense and it is marked asa Causal relation.(6).
a.
Yesterday Holly was running a mara-thonb.
when she twisted her ankle.(7).
a.
Use of dispersants was approvedb.
when a test on the third day showedsome positive results.Inspired by the linguistic model from Webber(1988) as described in Section 3, we explore thetemporal order of events in two adjacent sen-tences for discourse relation interpretation.
Hereevent is represented by the head of verb, and thetemporal order refers to the logical occurrence(i.e.
before/at/after) between events.
For in-stance, the event ordering in Example (8) can beinterpreted as:?????
??????
???????
?????(????)
.8.  a.  John went to the hospital.b.
He had broken his ankle on a patch ofice.We notice that the feasible temporal order ofevents differs for different discourse relations.For example, in causal relations, cause eventusually happens before effect event, i.e.?????
?????
???????
?????(??????
).So it is possible to infer a causal relation inExample (8) if and only if 8.b is taken to be thecause event and 8.a is taken to be the effectevent.
That is, 8.b is taken as happening prior tohis going into hospital.In our experiments, we use the TARSQI3  sys-tem to identify event, analyze tense and aspectualinformation, and label the temporal order ofevents.
Then the tense and temporal orderinginformation is extracted as features for discourserelation recognition.3 http://www.isi.edu/tarsqi/7 Experiments and ResultsIn this section we provide the results of a set ofexperiments focused on the task of simultaneousdiscourse identification and classification.7.1 Experimental SettingsWe experiment on PDTB v2.0 corpus.
Besidesfour top-level discourse relations, we also con-sider Entity and No relations described in Section2.
We directly use the golden standard parsetrees in Penn TreeBank.
We employ an SVMcoreference resolver trained and tested on ACE2005 with 79.5% Precision, 66.7% Recall and72.5% F1 to label coreference mentions of thesame named entity in an article.
For learning, weuse the binary SVMLight developed by (Joa-chims, 1998) and Tree Kernel Toolkits devel-oped by (Moschitti, 2004).
All classifiers aretrained with default learning parameters.The performance is evaluated using Accuracywhich is calculated as follow:????????
=???????????
?+ ??????????????
?Sections 2-22 are used for training and Sec-tions 23-24 for testing.
In this paper, we onlyconsider any non-overlapping clauses/sentencespair in 3-sentence spans.
For training, there were14812, 12843 and 4410 instances for Explicit,Implicit and Entity+No relations respectively;while for testing, the number was 1489, 1167 and380.7.2 System with Structural KernelTable 2 lists the performance of simultaneousidentification and classification on level-1 dis-course senses.
In the first row, only base featuresdescribed in Section 4 are used.
In the secondrow, we test Ben and James (2007)?s algorithmwhich uses heuristically defined syntactic pathsand acts as a good baseline to compare with ourlearned-based approach using the structured in-formation.
The last three rows of Table 2 reportsthe results combining base features with threesyntactic structured features (i.e.
Min-Expansion,Simple-Expansion and Full-Expansion) de-scribed in Section 5.We can see that all our tree kernels outperformthe manually constructed flat path feature in allthree groups including Explicit only, Implicitonly and All relations, with the accuracy increas-ing by 1.8%, 6.7% and 3.1% respectively.
Espe-cially, it shows that structural syntactic informa-tion is more helpful for Implicit cases which isgenerally much harder than Explicit cases.
We716conduct chi square statistical significance test onAll relations between flat path approach andSimple-Expansion approach, which shows theperformance improvements are statistical signifi-cant (?
< 0.05) through incorporating tree ker-nel.
This proves that structural syntactic informa-tion has good predication power for discourseanalysis in both explicit and implicit relations.We also observe that among the three syntacticstructured features, Min-Expansion and Simple-Expansion achieve similar performances whichare better than the result for Full-Expansion.
Thismay be due to that most significant informationis with the arguments and the shortest path con-necting connectives and arguments.
However,Full-Expansion that includes more informationin other branches may introduce too many detailswhich are rather tangential to discourse recogni-tion.
Our subsequent reports will focus on Sim-ple-Expansion, unless otherwise specified.As described in Section 5, to compute thestructural information, parse trees for differentsentences are connected to form a large tree for aparagraph.
It would be interesting to find howthe structured information works for discourserelations whose arguments reside in differentsentences.
For this purpose, we test the accuracyfor discourse relations with the two argumentsoccurring in the same sentence, one-sentenceapart, and two-sentence apart.
Table 3 comparesthe learning systems with/without the structuredfeature present.
From the table, for all three cas-es, the accuracies drop with the increase of thedistances between the two arguments.
However,adding the structured information would bringconsistent improvement against the baselinesregardless of the number of sentence distance.This observation suggests that the structured syn-tactic information is more helpful for inter-sentential discourse analysis.We also concern about how the structured in-formation works for identification and classifica-tion respectively.
Table 4 lists the results for thetwo sub-tasks.
As shown, with the structured in-formation incorporated, the system (Base + TreeKernel) can boost the performance of the twobaselines (Base Features in the first row andBase+ Manually selected paths in the second row), forboth identification and classification respective-ly.
We also observe that the structural syntacticinformation is more helpful for classification taskwhich is generally harder than identification.This is in line with the intuition that classifica-tion is generally a much harder task.
We find thatdue to the weak modeling of Entity relations,many Entity relations which are non-discourserelation instances are mis-identified as implicitExpansion relations.
Nevertheless, it clearly di-rects our future work.7.3 System with Temporal Ordering Infor-mationTo examine the effectiveness of our temporalordering information, we perform experimentsFeaturesAccuracyExplicit Implicit AllBase Features 67.1 29 48.6Base + Manuallyselected flat pathfeatures70.3 32 52.6Base + Tree kernel(Min-Expansion)71.9 38.6 55.6Base + Tree kernel(Simple-Expansion)72.1 38.7 55.7Base + Tree kernel(Full-Expansion)71.8 38.4 55.4Sentence Dis-tance0(959)1(1746)2(331)Base Features 52 49.2 35.5Base + Manuallyselected flat pathfeatures56.7 52 43.8Base + TreeKernel58.3 55.6 49.7Tasks Identifica-tionClassifica-tionBase Features 58.6 50.5Base + Manuallyselected flat pathfeatures59.7 52.6Base + TreeKernel63.3 59.3Table 3.
Results of the syntactic structured kernelfor discourse relations recognition with argu-ments in different sentences apart.Table 4.
Results of the syntactic structured ker-nel for simultaneous discourse identification andclassification subtasks.Table 2.
Results of the syntactic structured ker-nels on level-1 discourse relation recognition.717on simultaneous identification and classificationof level-1 discourse relations to compare withusing only base feature set as baseline.
The re-sults are shown in Table 5.
We observe that theuse of temporal ordering information increasesthe accuracy by 3%, 3.6% and 3.2% for Explicit,Implicit and All groups respectively.
We conductchi square statistical significant test on All rela-tions, which shows the performance improve-ment is statistical significant (?
< 0.05).
It indi-cates that temporal ordering information canconstrain the discourse relation types inferredwithin a clause(s)/sentence(s) pair for both expli-cit and implicit relations.We observe that although temporal orderinginformation is useful in both explicit and implicitrelation recognition, the contributions of the spe-cific information are quite different for the twocases.
In our experiments, we use tense and as-pectual information for explicit relations, whileevent ordering information is used for implicitrelations.
The reason is explicit connective itselfprovides a strong hint for explicit relation, sotense and aspectual analysis which yields a relia-ble result can provide additional constraints, thuscan help explicit relation recognition.
However,event ordering which would inevitably involvemore noises will adversely affect the explicit re-lation recognition performance.
On the otherhand, for implicit relations with no explicit con-nective words, tense and aspectual informationalone is not enough for discourse analysis.
Eventordering can provide more necessary informationto further constrain the inferred relations.7.4 Overall ResultsWe also evaluate our model which combinesbase features, tree kernel and tense/temporal or-dering information together on Explicit, Implicitand All Relations respectively.
The overall re-sults are shown in Table 6.8 Conclusions and Future WorksThe purpose of this paper is to explore how tomake use of the structural syntactic knowledge todo discourse relation recognition.
In previouswork, syntactic information from parse trees isrepresented as a set of heuristically selected flatpaths or 2-level production rules.
However, thefeatures defined this way may not necessarilycapture all useful syntactic information providedby the parse trees for discourse analysis.
In thepaper, we propose a kernel-based method to in-corporate the structural information embedded inparse trees.
Specifically, we directly utilize thesyntactic parse tree as a structure feature, andthen apply kernels to such a feature, togetherwith other normal features.
The experimentalresults on PDTB v2.0 show that our kernel-basedapproach is able to give statistical significantimprovement over flat syntactic path method.
Inaddition, we also propose to incorporate tempor-al ordering information to constrain the interpre-tation of discourse relations, which also demon-strate statistical significant improvements fordiscourse relation recognition, both explicit andimplicit.In future, we plan to model Entity relationswhich constitute 24% of Implicit+Entity+No re-lation cases, thus to improve the accuracy of re-lation detection.ReferenceBen W. and James P. 2007.
Automatically Identifyingthe Arguments of Discourse Connectives.
In Pro-ceedings of the 2007 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages92-101.Culotta A. and Sorensen J.
2004.
Dependency TreeKernel for Relation Extraction.
In Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics (ACL 2004), pages 423-429.Collins M. and Duffy N. 2001.
New Ranking Algo-rithms for Parsing and Tagging: Kernels over Dis-FeaturesAccuracyExplicit Implicit AllBase Features 67.1 29 48.6Base + Tem-poral OrderingInformation70.1 32.6 51.8Relations AccuracyExplicit 74.2Implicit 40.0All 57.3Table 5.
Results of tense and temporal orderinformation on level-1 discourse relations.Table 6.
Overall results for combined model(Base  + Tree Kernel + Tense/Temporal).718crete Structures and the Voted Perceptron.
In Pro-ceedings of the 40th Annual Meeting of the Associ-ation for Computational Linguistics (ACL 2002),pages 263-270.Collins M. and Duffy N. 2002.
Convolution Kernelsfor Natural Language.
NIPS-2001.Haussler D. 1999.
Convolution Kernels on DiscreteStructures.
Technical Report UCS-CRL-99-10,University of California, Santa Cruz.Joachims T.  1999.
Making Large-scale SVM Learn-ing Practical.
In Advances in Kernel Methods ?Support Vector Learning.
MIT Press.Knott, A., Oberlander, J., O?Donnel, M., and Mellish,C.
2001.
Beyond elaboration: the interaction of re-lations and focus in coherent text.
In T. Sanders, J.Schilperoord, and W. Spooren, editors, Text Re-presentation: Linguistic and Psycholinguistics As-pects, pages 181-196.
Benjamins, Amsterdam.Lee A., Prasad R., Joshi A., Dinesh N. and WebberB.
2006.
Complexity of dependencies in discourse:are dependencies in discourse more complex thanin syntax?
In Proceedings of the 5th InternationalWorkshop on Treebanks and Linguistic Theories.Prague, Czech Republic, December.Lin Z., Kan M. and Ng H. 2009.
Recognizing ImplicitDiscourse Relations in the Penn Discourse Tree-bank.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP 2009), Singapore, August.Marcu D. and Echihabi A.
2002.
An UnsupervisedApproach to Recognizing Discourse Relations.
InProceedings of the 40th Annual Meeting of ACL,pages 368-375.Moschitti A.
2004.
A Study on Convolution Kernelsfor Shallow Semantic Parsing.
In Proceedings ofthe 42th Annual Meeting of the Association forComputational Linguistics (ACL 2004), pages 335-342.Pettibone J. and Pon-Barry H. 2003.
A Maximum En-tropy Approach to Recognizing Discourse Rela-tions in Spoken Language.
Working Paper.
TheStanford Natural Language Processing Group, June6.Pitler E., Louis A. and Nenkova A.
2009.
AutomaticSense Predication for Implicit Discourse Relationsin Text.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the Association forComputational Linguistics and the 4th InternationalJoint Conference on Natural Language Processingof the Asian Federation of Natural LanguageProcessing (ACL-IJCNLP 2009).Prasad R., Dinesh N., Lee A., Miltsakaki E., RobaldoL., Joshi A. and Webber B.
2008.
The Penn Dis-course TreeBank 2.0.
In Proceedings of the 6th In-ternational Conference on Language Resources andEvaluation (LREC 2008).Saito M., Yamamoto K. and Sekine S. 2006.
Usingphrasal patterns to identify discourse relations.
InProceedings of the Human Language TechnologyConference of the North American Chapter of theAssociation for Computational Linguistics (HLT-NAACL 2006), pages 133?136, New York, USA.Vapnik V.  1995.
The Nature of Statistical LearningTheory.
Springer-Verlag, New York.Webber Bonnie.
1988.
Tense as Discourse Anaphor.Computational Linguistics, 14:61?73.Zelenko D., Aone C. and Richardella A.
2003.
Ker-nel Methods for Relation Extraction.
Journal ofMachine Learning Research, 3(6):1083-1106.Zhang M., Zhang J. and Su J.
Exploring SyntacticFeatures for Relation Extraction using a Convolu-tion Tree Kernel.
In Proceedings of the HumanLanguage Technology conference - North Ameri-can chapter of the Association for ComputationalLinguistics annual meeting (HLT-NAACL 2006),New York, USA.719
