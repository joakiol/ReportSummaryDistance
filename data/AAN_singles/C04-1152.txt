Efficient Unsupervised Recursive Word Segmentation Using MinimumDescription LengthShlomo ARGAMONa Navot AKIVAb Amihood AMIRb Oren KAPAHbaIllinois Institute of Technology, Dept.
of Computer Science, Chicago, IL 60616, USAargamon@iit.edubBar-Ilan University, Dept.
of Computer Science, Ramat Gan 52900, ISRAEL{navot,amir,kapaho}@cs.biu.ac.ilAbstractAutomatic word segmentation is a basic re-quirement for unsupervised learning in morpho-logical analysis.
In this paper, we formulate anovel recursive method for minimum descrip-tion length (MDL) word segmentation, whosebasic operation is resegmenting the corpus ona prefix (equivalently, a suffix).
We derive alocal expression for the change in descriptionlength under resegmentation, i.e., one which de-pends only on properties of the specific prefix(not on the rest of the corpus).
Such a formula-tion permits use of a new and efficient algorithmfor greedy morphological segmentation of thecorpus in a recursive manner.
In particular, ourmethod does not restrict words to be segmentedonly once, into a stem+affix form, as do manyextant techniques.
Early results for English andTurkish corpora are promising.1 IntroductionAlthough computational morphological analyzershave existed for many years for a number of lan-guages, there are still many languages for which nosuch analyzer exists, but for which there is an abun-dance of electronically-available text.
Developinga morphological analyzer for a new language byhand can be costly and time-consuming, requiringa great deal of effort by highly-specialized experts.Supervised learning methods, on the other hand, re-quire annotated data, which is often scarce or non-existent, and is also costly to develop.
For thisreason, there is increasing interest in unsupervisedlearning of morphology, in which unannotated textis analysed to find morphological structures.
Evenapproximate unsupervised morphological analysiscan be useful, as an aid to human annotators.This paper addresses a key task for unsuper-vised morphological analysis: word segmentation,segmenting words into their most basic meaning-ful constituents (substrings), called morphs (ortho-graphic realizations of morphemes).
We adopt theminimum description length (MDL) approach toword segmentation, which has been shown to be ef-fective in recent work (notably (Goldsmith, 2001)and (Brent et al, 1995)).
The minimum descrip-tion length principle (Barron et al, 1998) is aninformation-theoretic criterion to prefer that modelfor observed data which gives a minimal length cod-ing of the observed data set (given the model) to-gether with the model itself.1.1 Our approachOur approach in this paper is to better clarify theuse of MDL for morphological segmentation by en-abling direct use of a variety of MDL coding criteriain a general and efficient search algorithm.
Issues ofcomputational efficiency have been a bottleneck inwork on unsupervised morphological analysis, lead-ing to various approximations and heuristics beingused.
Our key contribution is to show how a lo-cal formulation of description length (DL) for wordsegmentation enables an efficient algorithm (basedon pattern-matching methods) for greedy morpho-logical segmentation of the corpus.
We thus providea search framework method which avoids some re-strictions needed in previous work for efficiency.
Inparticular, our method segments words in the cor-pus recursively, enabling multiple morphs to be ex-tracted from a single word, rather than just allow-ing a single stem+affix pair for a given word, as inmany previous approaches.
For example, we mightfind the segmentation inter+nation+al+ist,whereas a single-boundary method would segmentthe word on just one of those boundaries.This paper describes the first step in a larger re-search program; it?s purpose is to show how to mostefficiently recursively segment the words in a cor-pus based on an MDL criterion, rather than exhibit afull morphological analysis system.
The proceduredeveloped here is a component of a larger plannedsystem, which will use semantic and structural in-formation to correct word segmentation errors andwill cluster morphological relations into productiveparadigms.1.2 Related workSeveral systems for unsupervised learning of mor-phology have been developed over the last decade orso.
De?jean (1998), extending ideas in Harris (1955),describes a system for finding the most frequent af-fixes in a language and identifying possible mor-pheme boundaries by frequency bounds on the num-ber of possible characters following a given char-acter sequence.
Brent et al (1995) give an in-formation theoretic method for discovering mean-ingful affixes, which was later extended to enablea novel search algorithm based on a probabilisticword-generation model (Snover et al, 2002).
Gold-smith (2001) gives a comprehensive heuristic al-gorithm for unsupervised morphological analysis,which uses an MDL criterion to segment wordsand find morphological paradigms (called signa-tures).
Similarly, Creutz and Lagus (2002) use anMDL formulation for word segmentation.
All ofthese approaches assume a stem+affix morpholog-ical paradigm.Further, the above approaches only consider in-formation in words?
character sequences for im-prove morphological segmentation, and do not con-sider syntactic or semantic context.
Schone and Ju-rafsky (2000) extend this by using latent semanticanalysis (Dumais et al, 1988) to require that a pro-posed stem+affix split is sufficiently semanticallysimilar to the stem before the split is accepted.
Aconceptually similar approach is taken by Baroni etal.
(2002) who combine use of edit distance to mea-sure orthographic similarity and mutual informationto measure semantic similarity, to determine mor-phologically related word pairs.2 Overview of the ApproachIn this section we provide an overview of our ap-proach to greedy construction of a set of morphs(a dictionary), using a minimal description length(MDL) criterion (Barron et al, 1998) (we presentthree alternative MDL-type criteria below, of vary-ing levels of sophistication).
The idea is to initializea dictionary of morphs to the set of all word types inthe corpus, and incrementally refine it by resegment-ing affixes (either prefixes or suffixes) from the cor-pus.
Resegmenting on a prefix p (depicted in Fig-ure 1) means adding the prefix as a new morph, andremoving it from all words where it occurs as a pre-fix.
Some of the morphs thus created may alreadyexist in the corpus (e.g., ?cognition?
in Fig.
1).
Wedenote the set of morphs starting with p as Vp, andthe set of continuations that follow p by Sp (i.e.,Vp = pSp).
The number of occurrences of a morphm in the corpus (as currently segmented) is denotedDictionary before Dictionary afterrelic reretire licrecognition tirerelive cognitiontire livecognition farmfarmFigure 1: Illustration of resegmenting on the prefix re-.Note that Vre ={relic, retire, recognition, relive}, andSre ={lic, tire, cognition, live}.by C(m), and the number of tokens in the corpuswith prefix p is denoted B(p) = ?vk?Vp C(vk).The algorithm examines all prefixes of currentmorphs in the dictionary as resegmentation candi-dates.
The candidate p?
that would give the greatestdecrease in description length upon resegmentationis chosen, and the corpus is then resegmented on p?.This is repeated until no candidate can decrease de-scription length.Key to this process is efficient resegmentation ofthe corpus, which entails incremental update of thedescription length change that each prefix p willgive upon resegmentation, denoted ?CODEp (thechange in the coding cost CODE(M,Data) for thecorpus plus the model M ).
This is achieved in twoways.
First, we develop (Sec.
3) expressions for?CODEp which depend only on simple propertiesof p, Vp, and Sp, and their occurrences in the corpus.This locality property obviates the need to exam-ine most of the corpus to determine ?CODEp.
Sec-ond, we use a novel word/suffix indexing data struc-ture which permits efficient resegmentation and up-date of the statistics on which ?CODEp depends(Sec.
4).
Initial experimental results for the differentmodels using our algorithm are given in Section 5.3 Local Description Length ModelsAs we show below, the key to efficiency is derivinglocal expressions for the change in coding lengththat will be caused by resegmentation on a partic-ular prefix p. That is, this coding length change,?CODEp, should depend only on direct propertiesof p, those morphs Vp = {vk = psk} for which it isa prefix, and those strings Sp = {sk|psk ?
Vp} (p?scontinuations).
This enables us to efficiently main-tain the necessary data about the corpus and to up-date it on resegmentation, avoiding costly scanningof the entire corpus on each iteration.We now describe three description length mod-els for word segmentation.
First, we introduce localdescription length via two simple models, and thengive a derivation of a local expression for descrip-tion length change for a more realistic descriptionlength measure.3.1 Model 1: Dictionary countPerhaps the simplest possible model is to find a seg-mentation which minimizes the number of morphsin the dictionary CODE1(M,Data) = |M |.
Al-though the global minimum will almost always bethe trivial solution where each morph is an individ-ual letter, this trivial solution may be avoided byenforcing a minimal morph length (of 2, say).
Fur-thermore, when implemented via a greedy prefix (orsuffix) resegmenting algorithm, this measure givessurprisingly good results, as we show below.Locality in this model is easily shown, as?CODE1p(M) = 1 + |Sp ?M | ?
|Vp|= 1 ?
|Sp ?M |since p is added to M as are all its continuationsnot currently in M , while each morph vk ?
Vp is re-moved (being resegmented as the 2-morph sequencepsk).3.2 Model 1a: Adjusted countWe also found a heuristic modification of Model1 to work well, based on the intuition that an af-fix with more continuations that are current morphswill be better, while to a lesser extent more contin-uations that are not current morphs indicates lowerquality.
This gives the local heuristic formula:?CODE1ap (M) = 1 + |Sp ?M | ?
?|Sp ?M |where ?
is a tunable parameter determining the rel-ative weights of the two factors.3.3 Model 2: MDLA more theoretically motivated model seeks to min-imize the combined coding cost of the corpus andthe dictionary (Barron et al, 1998):CODE2(Data|M) + CODE2(M)where we assume a minimal length code for the cor-pus based on the morphs in the dictionary1 .The coding cost of the dictionary M is:CODE2(M) = CODE2(M)= b?m?M len(m)1As is well known, MDL model estimation is equivalent toMAP estimation for appropriately chosen prior and conditionaldata distribution (Barron et al, 1998).where b is the number of bits needed to represent acharacter and len(m) is the length of m in charac-ters.The coding cost CODE(Data|M) of the corpusgiven the dictionary is simply the total number ofbits to encode the data using M ?s code:CODE2(Data|M)= CODE(M(Data) = M1...N )= ?
?Ni=1 log P (mi)= ?
?|M |j=1C(mj) log P (mj)= ?
?|M |j=1C(mj)(logC(mj) ?
logN)where M(Data) is the corpus segmented accordingto M , N is the number of morph tokens in the seg-mented corpus, mi is the ith morph token in thatsegmentation, P (m) is the probability of morphm in the corpus estimated as P (m) = C(m)/N ,C(m) is the number of times morph m appears inthe corpus, |M | is the total number of morph typesin M , and mj is the jth morph type in the M .Now suppose we wish to add a new morph to Mby resegmenting on a prefix p from all morphs shar-ing that prefix, as above.
First, consider the totalchange in cost for the dictionary.
Note that the ad-dition of the new morph p will cause an increase ofblen(p) bits to the total dictionary size.
At the sametime, each new morph s ?
Sp ?
M will add itscoding cost blen(s), while each preexisting morphs?
?
Sp?M will not change the dictionary length atall.
Finally, each vk is removed from the dictionary,giving a change of ?blen(vk).
The total change incoding cost for the dictionary by resegmenting on pis thus:?CODE2p(M) = b (len(p)+?sk?
(Sp?M) len(sk)?
?k len(vk))Now consider the change in coding cost for thecorpus after resegmentation.
First, consider eachpreexisting morph type m 6?
Vp, with the samecount after resegmentation (since it does not con-tain p).
The coding cost of each occurrence of m,however, will change, since the total number of to-kens in the corpus will change.
Thus the total costchange for such an m is:?CODE2p(Data|m 6?
Vp)= C(m)(log P (m) ?
log P?
(m))= C(m)(logC(m) ?
logN ?
logC(m) + log N?
)= C(m)(log N?
?
logN)= C(m)(log(N + B(p)) ?
logN)The total corpus cost change for unchanged morphsdepends only on N and B(p):?CODE2p(Data|M ?
Vp)=?m?M?Vp C(m)(log(N + B(p)) ?
logN)= (?m?M?Vp C(m))(log(N + B(p)) ?
logN)= (N ?
?vk C(vk))(log(N + B(p)) ?
logN)= (N ?B(p))(log(N + B(p)) ?
logN)Now, consider explicitly each morph vk ?
Vpwhich will be split after resegmentation.
First,remove the code for each occurrence of vk fromthe corpus coding: C(vk) log P (vk).
Next, add acode for each occurrence of the new morph cre-ated by the prefix: ?C(vk) log P?
(p), where P?
(p) =B(p)/(N + B(p)) is the probability of morph pin the resegmented corpus.
Finally, code the con-tinuations sk: ?C(vk) log P?
(sk) (where P?
(sk) =C?(sk)N?
=C(vk)+C(sk)N?
is the probability of the ?new?morph sk).
Putting this together, we have the cor-pus coding cost change for Vp (noting that B(p) =?vk C(vk)):?CODE2p(Data|Vp)=?vk C(vk)[ logP (vk) ?
log P?
(p) ?
log P?
(sk) ]=?vk C(vk) (logC(vk) ?
logN+ log N?
?
logB(p)+ log N?
?
log C?
(sk))= ?vk C(vk)(logC(vk) ?
log C?
(sk))+B(p)(2 log N?
?
logN)?B(p) logB(p)Thus the cost change for resegmenting on p is:?CODE2p(M,Data)= ?CODE2p(M) + ?CODE2p(Data|M)= ?CODE2p(M) + ?CODE2p(Data|M ?
Vp)+?CODE2p(Data|Vp)= b[len(p) +?sk?
(Sp?M) len(sk) ?
?vk len(vk)]+ (N ?B(p)) (log(N + B(p)) ?
logN)+ ?vk C(vk)(logC(vk) ?
log C?
(sk))+B(p)(2 log N?
?
logN)?B(p) logB(p)Note that all terms are local to the prefix p, its in-cluding morphs Vp and its continuations Sp.
Thiswill enable an efficient incremental algorithm forgreedy segmentation of all words in the corpus, asdescribed in the next section.4 Efficient Greedy Prefix SearchThe straightforward greedy algorithm schema forfinding an approximately minimal cost dictio-nary is to repeatedly find the best prefix p?
=argminp ?CODEp(M,Data) and resegment thecorpus on p?, until no p?
exists with negative?CODE.
However, the expense of passing over theentire corpus repeatedly would be prohibitive.
Dueto lack of space, we sketch here our method forcaching corpus statistics in a pair of tries, in sucha way that ?CODEp can be easily computed for anyprefix p, and such that the data structures can be ef-ficiently updated when resegmenting on a prefix p.(A heap is also used for efficiently finding the bestprefix.
)The main data structures consist of two tries.
Thefirst, which we term the main suffix trie (MST), is asuffix trie (Gusfield, 1997) for all the words in thecorpus.
Each node in the MST represents either theprefix of a current morph (initially, a word in thecorpus), or the prefix of a potential morph (in caseits preceding prefix gets segmented).
Each suchnode is labeled with various statistics of its prefix p(denoted by the path to it from the root) and its suf-fixes, such as its prefix length len(p), its count B(p),the number of its continuations |Sp|, and the col-lective length of its continuations?sk?Sp len(sk),as well as the current value of ?CODEp(M,Data)(computed from these statistics).
Also, each noderepresenting the end of an actual word in the corpusis marked as such.The second trie, the reversed prefix trie (RPT),contains all the words in the corpus in reverse.Hence each node in the RPT corresponds to the suf-fix of a word in the corpus.
We maintain a list ofpointers at each node in the RPT to each node in theMST which has an identical suffix.
This allows ef-ficient access to all prefixes of a given string.
Also,those nodes corresponding to a complete word inthe corpus are marked.Initial construction of the data structures can bedone in time linear in the size of the corpus, us-ing straightforward extensions of known suffix trieconstruction techniques (Gusfield, 1997).
Findingthe best prefix p?
can be done efficiently by stor-ing pointers to all the prefixes in a heap, keyed by?CODEp.
To then remove all words prefixed by p?and add all its continuations as new morphs (as wellas p?
itself), proceed as follows, for each continua-tion sk:1.
If sk is marked in RPT, then it is a completeword, and only its count needs to be updated.2.
Otherwise(a) Mark sk?s node in MST as a completeword, and update its statistics(b) Add sRk to RPT and mark the correspond-ing nodes in MST as accepting stems.3.
Update the heap for the changed prefixes.Prefixesre- *ter-un- im-in- com-de- trans-con- sub-dis- *se-pre- en-ex- *pa-pro- *pe-over- *mi-Suffixes-?s *-at-ing -ate-ed -ive-es -able-ly -ment-er -or?-ers -en-ion ?-ors?-ions ?-ings-al *-isFigure 2: The first 20 English prefix and suffix morphsextracted from Reuters-21578 corpus using Model 1.Meaningless morphs are marked by ?*?
; nonminimalmeaningful morphs by ??
?.Prefixes Suffixes?
= 1 ?
= 2 ?
= 1 ?
= 2over-non-under-mis-food-stock-feed-view-work-export-book-warn-borrow-depres-market-high-narrow-turn-trail-steel-un-over-non-*der-dis-mis-out-inter-trans-re-super-fore-up-down-tele-stock-im-air-euro-mid--?s-ly-ness-ship?-ships?-ization-ize?-ized?-isation?-izing?-izes?-holders?-izations?-isations-water?-ised-ise?-ising?-ises?-iser-?s-ly-ness-ment?-ments?-ized-ize?-ization?-izing?-isation?-ised-ise?-ising?-ises-ship-men?-ened?-ening?-izes*-mentalFigure 3: The first 20 English prefix and suffix morphsextracted using Model 1a, as above.The complexity for resegmenting on p isO(len(p) +?sk?Splen(sk) + NSUF(Sp) log(|M |))where NSUF(Sp) is the number of different morphsin the previous dictionary that have a suffix in Sp(which need to be updated in the heap).5 Experimental ResultsIn this section we give initial results for the abovealgorithm in English and Turkish, showing howmeaningful morphs are extracted using differentgreedy MDL criteria.
Recall that the models andalgorithm described in this paper are intended asparts of a more comprehensive morphological anal-ysis system, as we describe below in future work.5.1 EnglishFor evaluation in English, we used the standardReuters-21578 corpus of news articles (comprising1.7M word tokens and 32,811 unique words).
Foreach of the 3 models described above, we extractedmorphs either by resegmenting on prefixes or onsuffixes (looking at the words reversed).
When seg-menting according to Models 1 and 2, a minimumprefix length of 2 was enforced, to improve morphquality (though not for suffixes, since in Englishthere are some one-letter suffixes such as -s).First, consider morphs found by Model 1 (Fig.
2).The prefix morphs found are surprisingly good forthis simple model, with only one wrong in the first15 extracted.
That erroneous morph is ter-, whichis part of inter-, however in- was extractedfirst; this kind of error could be ameliorated bya merging postprocessing step.
The suffixes aresimilarly good, although oddly the system did notfind -s, which caused it to find several compos-ite morphs, such as -ers and -ions, which canget resegmented into their components (-er+s and-ion+s) later.Model 1a also performs extremely well, for dif-ferent values of ?
(we show just ?
= 1 and ?
= 2in Fig.
3, for lack of space).
Note that the morphsfound by this model differ qualitatively from thosefound by Model 1, in that we get longer morphsmore related to agglutination than to regular inflec-tion patterns.
This suggests that multiple statisticalmodels should be used together to extract differentfacets of a language?s morphological composition.Finally, morphs from the more complex Model2 are given in Fig.
4.
As in Model 1a, Model2 gives more agglutinative morphs than inflectivemorphs, and has a greater tendency to segment com-plex morphs (such as -ification-), which pre-sumably will later be resegmented into their com-ponent parts (e.g., -if+ic+at+ion).
This mayenable construction of hierarchical models of mor-phological composition in the future.5.2 TurkishIn addition to English, we tested the method?s abil-ity to extract meaningful morphs on a small corpusof Turkish texts from the Turkish Natural LanguageProcessing Initiative (Oflazer, 2001), which consistsof one foreign ministry press release, texts of twotreaties, and three journal articles on translation.The corpus comprises 20,284 individual words, ofwhich 5961 are unique.
Turkish is a highly aggluti-native language, hence a prime candidate for recur-sive morphological segmentation.
Results for Mod-els 1 and 2 are shown in Tables 5?8.
MeaningfulPrefixesnon- rein-bio- over-?disi- *ine-diss- ?interc-video- fluor-financier- wood-quadr- key-*kl- *kar-weather- vin-*jas- ?kings-Suffixes-?s -ville-town -field?-ification ?-ians?-alize ?-alising?-ically ?-ological-tech -wood?-ioning ?-etic?-sively -point?-nating -tally?-tational *-utingFigure 4: The first 20 English prefix and suffix morphsextracted using Model 2, as above, with b = 8.p Meaningbahs- talk (about)terk- leavingredd- refuse, rejectedzikr- mention (someone)bey- Mr., sirakt- agreehaps- (im)prisonbirbirlerin- one to anothers?efin- your chieftedbirler- precautionsbirin- somebodyhu?ku?mlerin- your opinionsu?lkesin- his countryelimiz- our handdu?zenlemelerin- your arrangementsyerin- your placekendin- yourselfdevletler- governmentsbic?imin- your styleistedig?im- (thing) that I wantFigure 5: Turkish morphs segmented as prefixes usingModel 1.morphs were found using all models, with Model 2finding longer morphs, as in English.
We do notesome issues with boundary letters for Model 2 pre-fixes, however.6 ConclusionsWe have given a firmer foundation for the use ofminimal description length (MDL) criteria for mor-phological analysis by giving a novel local formula-tion of the change in description length (DL) uponresegmentation of the corpus on a prefix (or suffix),p Meaning-nin of-n?n of-n?
your-na to your-ler plural form-leri plural form-nda at your-ni your-lerin your (things)-ki thatp Meaning-si of-ndan from-lar?
plural form-lar plural form-s?
of-lar?n your (pl.)
(things)-lerine to your (pl.)
(things)-ya to-lara to (pl.
)-dir isFigure 6: Turkish morphs segmented as suffixes usingModel 1.p Meaninghizmet(l)- serviceneden(l)- reasonmadd- materialbirbir- one anotherbelg- documentizlenim- observationnitelik- specificationen- widthdil- languagebilg(i)- knowledgep Meaningbahs- mentionzih(in)- memoryverg(i)- taxperson(el)- employeebiri- one ofverme- givingvere(n)- giverbelirsi(z)- unknownbildirim- announcementzikr- mentionFigure 7: Turkish morphs segmented as prefixes usingModel 2.
Turkish letters in parentheses are not in thesegmented morphs, though a better segmentation wouldhave included them.which enables an efficient algorithm for greedy con-struction of a morph dictionary using an MDL cri-terion.
The algorithm we have devised is generic, inthat it may easily be applied to any local descriptionlength model.
Early results of our method, as evalu-ated by examination of the morphs it extracts, showhigh accuracy in finding meaningful morphs basedsolely on orthographic considerations; in fact, wefind that Model 1, which depends only on the num-ber of morphs in the dictionary (and not on frequen-cies in the corpus at all) gives surprisingly good re-sults, though Model 2 may generally be preferable(more experiments on varied and larger corpora stillremain to be run).We see two immediate directions for future work.The first comprises direct improvements to the tech-niques presented here.
Rather than segmenting pre-fixes and suffixes separately, the data structures andalgorithms should be extended to segment both pre-fixes and suffixes in the current morph list, depend-ing on which gives the best overall DL improve-ment.
Related is the need to enable approximatematching of ?boundary?
characters due to ortho-graphic shifts such as -y to -i-, as well as incorpo-rating other orthographic filters on possible morphs(such as requiring prefixes to contain a vowel).
An-other algorithmic extension will be to develop anp Meaning-isine toward (someone)-nlerinin of their (things)-taki which at-isini from, towards-yeti to-iyorsa if (pres.
)-ili with-likte at (the place of)-?in of-imizden from ourp Meaning-ilerine to (pl.
)-lemektedir it does*-tik-ilemez cannot do-lerimizi our things-mun from my-mlar (plural)-tmak to-unca while-lu withFigure 8: Turkish morphs segmented as suffixes usingModel 2; tables as in Figure 5.efficient beam-search algorithm (avoiding copyingthe entire data structure), which may improve accu-racy over the current greedy search method.
In ad-dition, we will investigate the use of more sophisti-cated DL models, including, for example, semanticsimilarity between candidate affixes and stems, us-ing the probability of occurrence of individual char-acters for coding, or using n-gram probabilities forcoding the corpus as a sequence of morphs (insteadof the unigram coding model used here and previ-ously).The second direction involves integrating the cur-rent algorithm into a larger system for more compre-hensive morphological analysis.
As noted above,due to the greedy nature of the search, a recombi-nation step may be needed to ?glue?
morphs thatgot incorrectly separated (such as un- and -der-).More fundamentally, we intend to use the algorithmpresented here (with the above extensions) as a sub-routine in a paradigm construction system along thelines of Goldsmith (2001).
It seems likely that effi-cient and accurate MDL segmentation as we presenthere will enable more effective search through thespace of possible morphological signatures.AcknowledgementsThanks to Moshe Fresko and Kagan Agun for helpwith the Turkish translations, as well as the anony-mous reviewers for their comments.ReferencesM.
Baroni, J. Matiasek, and H. Trost.
2002.
Unsu-pervised discovery of morphologically related wordsbased on orthographic and semantic similarity.
InProceedings of the Workshop on Morphologicaland Phonological Learning of ACL/SIGPHON-2002,pages 48?57.Andrew Barron, Jorma Rissanen, and Bin Yu.
1998.
Theminimum description length principle in coding andmodeling.
IEEE Transactions on Information Theory,44(6):2743?2760, October.Michael R. Brent, Sreerama K. Murthy, and AndrewLundberg.
1995.
Discovering morphemic suffixes: Acase study in minimum description length induction.In Proceedings of the Fifth International Workshop onArtificial Intelligence and Statistics, Ft. Lauderdale,FL.Mathias Creutz and Krista Lagus.
2002.
Unsuperviseddiscovery of morphemes.
In Proceedings of the Work-shop on Morphological and Phonological Learning ofACL-02, pages 21?30, Philadelphia.Herve?
De?jean.
1998.
Morphemes as necessary conceptfor structures discovery from untagged corpora.
InWorkshop on Paradigms and Grounding in NaturalLanguage Learning, pages 295?299, Adelaide.S.
T. Dumais, G. W. Furnas, T. K. Landauer, S. Deer-wester, and R. Harshman.
1988.
Using latent seman-tic analysis to improve access to textual information.In Proceedings of the SIGCHI conference on Humanfactors in computing systems, pages 281?285.
ACMPress.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 27:153?198.Dan Gusfield.
1997.
Algorithms on Strings, Trees, andSequences - Computer Science and Computational Bi-ology.
Cambridge University Press.Zellig Harris.
1955.
From phoneme to morpheme.
Lan-guage, 31:190?222.Kemal Oflazer.
2001.
English Turkish alignedparallel corpora.
Turkish Natural LanguageProcessing Initiative, Bilkent University.http://www.nlp.cs.bilkent.edu.tr/Turklang/corpus/par-corpus/.Patrick Schone and Daniel Jurafsky.
2000.
Knowledgefree induction of morphology using latent semanticanalysis.
In Proceedings of CoNLL-2000 and LLL-2000, pages 67?72, Lisbon.Matthew Snover, Gaja Jarosz, and Michael Brent.
2002.Unsupervised learning of morphology using a noveldirected search algorithm: Taking the first step.
InACL-2002 Workshop on Morphological and Phono-logical Learning.
