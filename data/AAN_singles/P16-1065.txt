Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 686?696,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Discriminative Topic Model using Document Network StructureWeiwei YangComputer ScienceUniversity of MarylandCollege Park, MDwwyang@cs.umd.eduJordan Boyd-GraberComputer ScienceUniversity of ColoradoBoulder, COJordan.Boyd.Graber@colorado.eduPhilip ResnikLinguistics and UMIACSUniversity of MarylandCollege Park, MDresnik@umd.eduAbstractDocument collections often have links be-tween documents?citations, hyperlinks,or revisions?and which links are added isoften based on topical similarity.
To modelthese intuitions, we introduce a new topicmodel for documents situated within a net-work structure, integrating latent blocksof documents with a max-margin learningcriterion for link prediction using topic-and word-level features.
Experiments ona scientific paper dataset and collectionof webpages show that, by more robustlyexploiting the rich link structure within adocument network, our model improveslink prediction, topic quality, and blockdistributions.1 IntroductionDocuments often appear within a network struc-ture: social media mentions, retweets, and fol-lower relationships; Web pages by hyperlinks; sci-entific papers by citations.
Network structure in-teracts with the topics in the text, in that docu-ments linked in a network are more likely to havesimilar topic distributions.
For instance, a cita-tion link between two papers suggests that theyare about a similar field, and a mentioning linkbetween two social media users often indicatescommon interests.
Conversely, documents?
sim-ilar topic distributions can suggest links betweenthem.
For example, topic model (Blei et al, 2003,LDA) and block detection papers (Holland et al,1983) are relevant to our research, so we cite them.Similarly, if a social media user A finds anotheruser B with shared interests, then A is more likelyto follow B.Our approach is part of a natural progressionof network modeling in which models integratemore information in more sophisticated ways.Some past methods only consider the network it-self (Kim and Leskovec, 2012; Liben-Nowell andKleinberg, 2007), which loses the rich informationin text.
In other cases, methods take both links andtext into account (Chaturvedi et al, 2012), but theyare modeled separately, not jointly, limiting themodel?s ability to capture interactions between thetwo.
The relational topic model (Chang and Blei,2010, RTM) goes further, jointly modeling topicsand links, but it considers only pairwise documentrelationships, failing to capture network structureat the level of groups or blocks of documents.We propose a new joint model that makes fulleruse of the rich link structure within a documentnetwork.
Specifically, our model embeds theweighted stochastic block model (Aicher et al,2014, WSBM) to identify blocks in which docu-ments are densely connected.
WSBM basically cat-egorizes each item in a network probabilisticallyas belonging to one of L blocks, by reviewingits connections with each block.
Our model canbe viewed as a principled probabilistic extensionof Yang et al (2015), who identify blocks in a doc-ument network deterministically as strongly con-nected components (SCC).
Like them, we assign adistinct Dirichlet prior to each block to capture itstopical commonalities.
Jointly, a linear regressionmodel with a discriminative, max-margin objec-tive function (Zhu et al, 2012; Zhu et al, 2014) istrained to reconstruct the links, taking into accountthe features of documents?
topic and word distri-butions (Nguyen et al, 2013), block assignments,and inter-block link rates.We validate our approach on a scientific pa-per abstract dataset and collection of webpages,with citation links and hyperlinks respectively, topredict links among previously unseen documentsand from those new documents to training docu-ments.
Embedding the WSBM in a network/topic686??
?
LL?ab y A DD?DFigure 1: Weighted Stochastic Block Modelmodel leads to substantial improvements in linkprediction over previous models; it also improvesblock detection and topic interpretability.
The keyadvantage in embedding WSBM is its flexibilityand robustness in the face of noisy links.
Our re-sults also lend additional support for using max-margin learning for a ?downstream?
supervisedtopic model (McAuliffe and Blei, 2008), and thatpredictions from lexical as well as topic featuresimproves performance (Nguyen et al, 2013).The rest of this paper is organized as follows.Section 2 introduces two previous link-modelingmethods, WSBM and RTM.
Section 3 presents ourmethods to incorporate block priors in topic mod-eling and include various features in link predic-tion, as well as the aggregated discriminative topicmodel whose posterior inference is introduced inSection 4.
In Section 5 we show how our modelcan improve link prediction and (often) improvetopic coherence.2 Dealing with Links2.1 Weighted Stochastic Block ModelWSBM (Aicher et al, 2014) is a generalizedstochastic block model (Holland et al, 1983;Wang and Wong, 1987, SBM) and predicts non-negative integer-weight links, instead of binary-weight links.
A block is a collection of doc-uments which are densely connected with eachother but sparsely connected with documents inother blocks.
WSBM assumes that a document be-longs to exactly one block.
A link connecting twodocuments in blocks l and l?has a weight gen-erated from a Poisson distribution with parame-ters ?l,l?which has a Gamma prior with param-eters a and b, as Figure 1 shows.The whole generative process is:1.
For each pair of blocks (l, l?)
?
{1, .
.
.
, L}2(a) Draw inter-block link rate ?l,l??
Gamma(a, b)2.
Draw block distribution ?
?
Dir(?)3.
For each document d ?
{1, .
.
.
, D}(a) Draw block assignment yd?
Mult(?
)Figure 2: SCC can be distracted by spurious linksconnecting two groups, while WSBM maintains thedistinction.D E KM'dN dNdT 'dT ', ddBdz dw 'dz 'dwKQFigure 3: A Two-document Segment of RTM4.
For each link (d, d?)
?
{1, .
.
.
, D}2(a) Draw link weight Ad,d??
Poisson(?yd,yd?
)WSBM is a probabilistic block detection algo-rithm and more robust than some deterministic al-gorithms like SCC, which is vulnerable to noisylinks.
For instance, we would intuitively say Fig-ure 2 has two blocks?as denoted by coloring?whether or not the dashed link exists.
If the dashedlink does not exist, both WSBM and SCC can iden-tify two blocks.
However, if the dashed link doesexist, SCC will return only one big block that con-tains all nodes, while WSBM still keeps the nodesin two reasonable blocks.2.2 Relational Topic ModelRTM (Chang and Blei, 2010) is a downstreammodel that generates documents and links simul-taneously (Figure 3).
Its generative process is:1.
For each topic k ?
{1, .
.
.
,K}(a) Draw word distribution ?k ?
Dir(?
)(b) Draw topic regression parameter ?k?
N (0, ?2)2.
For each document d ?
{1, .
.
.
, D}(a) Draw topic distribution ?d ?
Dir(?
)(b) For each token td,nin document di.
Draw topic assignment zd,n?
Mult(?d)ii.
Draw word wd,n?
Mult(?zd,n)3.
For each explicit link (d, d?
)(a) Draw link weight Bd,d??
?(?
| zd,zd?
,?
)In the inference process, the updating of topicassignments is guided by links so that linked doc-uments are more likely to have similar topic distri-butions.
Meanwhile, the linear regression (whose687DE'D KMLS dNDTzwyFigure 4: Graphical Model of BP-LDAoutput is fed into link probability function ?)
isupdated to maximize the network likelihood usingcurrent topic assignments.3 Discriminative Topic Model with BlockPrior and Various FeaturesOur model is able to identify blocks from the net-work with an embedded WSBM, extract topic pat-terns of each block as prior knowledge, and use allthis information to reconstruct the links.3.1 LDA with Block Priors (BP-LDA)As argued in the introduction, linked documentsare likely to have similar topic distributions, whichcan be generalized to documents in the sameblock.
Inspired by this intuition and the blockassignment we obtain in the previous section, wewant to extract some prior knowledge from theseblocks.
Thus we propose an LDA with blockpriors, hence BP-LDA, as shown in Figure 4, whichhas the following generative process:1.
For each topic k ?
{1, .
.
.
,K}(a) Draw word distribution ?k ?
Dir(?)2.
For each block l ?
{1, .
.
.
, L}(a) Draw topic distribution pil ?
Dir(??)3.
For each document d ?
{1, .
.
.
, D}(a) Draw topic distribution ?d ?
Dir(?piyd)(b) For each token td,nin document di.
Draw topic assignment zd,n?
Mult(?d)ii.
Draw word wd,n?
Mult(?zd,n)Unlike conventional LDA, which uses an un-informative topic prior, BP-LDA puts a Dirich-let prior pi on each block to capture the block?stopic distribution and use it as an informative priorwhen drawing each document?s topic distribution.In other words, a document?s topic distribution?i.e., what the document is about?is not just in-formed by the words present in the document butthe broader context of its network neighborhood.D E KM'dN dNdT 'dT QLL?Wdw 'dwdz 'dz Kdy 'dy?
U', ddBTopical Feature Lexical Feature Block FeatureFigure 5: A two-document segment of VF-RTM.Various features are denoted by grayscale.
Bd,d?isobserved, but we keep it in white background toavoid confusion.3.2 RTM with Various Features (VF-RTM)Building on Chang and Blei (2010), we want togenerate the links between documents based onvarious features, hence VF-RTM.
In addition totopic distributions, VF-RTM also includes docu-ments?
word distributions (Nguyen et al, 2013)and the link rate of two documents?
assignedblocks, with the intent that these additional fea-tures improve link generation.
VF-RTM involvesthe relationship between a pair of documents, soit is difficult to show the whole model; thereforeFigure 5 illustrates with a two-document segment.The generative process is:1.
For each pair of blocks (l, l?)
?
{1, .
.
.
, L}2(a) Draw block regression parameter ?l,l??
N (0, ?2)2.
For each topic k ?
{1, .
.
.
,K}(a) Draw word distribution ?k ?
Dir(?
)(b) Draw topic regression parameter ?k?
N (0, ?2)3.
For each word v ?
{1, .
.
.
, V }(a) Draw lexical regression parameter ?v?
N (0, ?2)4.
For each document d ?
{1, .
.
.
, D}(a) Draw topic distribution ?d ?
Dir(?
)(b) For each token td,nin document di.
Draw topic assignment zd,n?
Mult(?d)ii.
Draw word wd,n?
Mult(?zd,n)5.
For each explicit link (d, d?
)(a) Draw link weightBd,d??
?(?
| yd, yd?,?,zd,zd?
,wd,wd?
,?, ?
,?
)Links are generated by a link probability func-tion ?
which takes the regression value Rd,d?ofdocuments d and d?as an argument.
Assumingdocuments d and d?belong to blocks l and l?re-spectively, Rd,d?isRd,d?= ?T(zd?
zd?)
+ ?T(wd?wd?)
+ ?l,l?
?l,l?, (1)688where zdis a K-length vector with each el-ement zd,k=1Nd?n1 (zd,n= k); wdis aV -length vector with each element wd,v=1Nd?n1 (wd,n= v); ?
denotes the Hadamard(element-wise) product;1?, ?
, and ?
are theweight vectors and matrix for topic-based, lexical-based and rate-based predictions, respectively.A common choice of ?
is a sigmoid (Changand Blei, 2010).
However, we instead use hingeloss so that VF-RTM can use the max-margin prin-ciple, making more effective use of side informa-tion when inferring topic assignments (Zhu et al,2012).
Using hinge loss, the probability that doc-uments d and d?are linked isPr (Bd,d?)
= exp (?2 max(0, ?d,d?))
, (2)where ?d,d?= 1?Bd,d?Rd,d?.
Positive and negativelink weights are denoted by 1 and -1, respectively,in contrast to sigmoid loss.3.3 Aggregated ModelFinally, we put all the pieces together and proposeLBH-RTM: RTM with lexical weights (L), blockpriors (B), and hinge loss (H).
Its graphical modelis given in Figure 6.1.
For each pair of blocks (l, l?)
?
{1, .
.
.
, L}2(a) Draw inter-block link rate ?l,l??
Gamma(a, b)(b) Draw block regression parameter ?l,l??
N (0, ?2)2.
Draw block distribution ?
?
Dir(?)3.
For each block l ?
{1, .
.
.
, L}(a) Draw topic distribution pil ?
Dir(??)4.
For each topic k ?
{1, .
.
.
,K}(a) Draw word distribution ?k ?
Dir(?
)(b) Draw topic regression parameter ?k?
N (0, ?2)5.
For each word v ?
{1, .
.
.
, V }(a) Draw lexical regression parameter ?v?
N (0, ?2)6.
For each document d ?
{1, .
.
.
, D}(a) Draw block assignment yd?
Mult(?
)(b) Draw topic distribution ?d ?
Dir(?piyd)(c) For each token td,nin document di.
Draw topic assignment zd,n?
Mult(?d)ii.
Draw word wd,n?
Mult(?zd,n)7.
For each link (d, d?)
?
{1, .
.
.
, D}2(a) Draw link weight Ad,d??
Poisson(?yd,yd?)8.
For each explicit link (d, d?
)(a) Draw link weightBd,d??
?(?
| yd, yd?,?,zd,zd?
,wd,wd?
,?, ?
,?
)A and B are assumed independent in themodel, but they can be derived from the same setof links in practice.1As Chang and Blei (2010) point out, the Hadamard prod-uct is able to capture similarity between hidden topic repre-sentations of two documents.Algorithm 1 Sampling Process1: Set ?
= 1 and initialize topic assignments2: form = 1 to M do3: Optimize ?, ?
, and ?
using L-BFGS4: for d = 1 to D do5: Draw block assignment yd6: for each token n do7: Draw a topic assignment zd,n8: end for9: for each explicit link (d, d?)
do10: Draw ??1d,d?
(and then ?d,d?
)11: end for12: end for13: end forLink set A is primarily used to find blocks, soit treats all links deterministically.
In other words,the links observed in the input are considered ex-plicit positive links, while the unobserved links areconsidered explicit negative links, in contrast tothe implicit links inB.In terms of link setB, while it adopts all explicitpositive links from the input, it does not deny theexistence of unobserved links, or implicit negativelinks.
Thus B consists of only explicit positivelinks.
However, to avoid overfitting, we samplesome implicit links and add them to B as explicitnegative links.4 Posterior InferencePosterior inference (Algorithm 1) consists of thesampling of topic and block assignments and theoptimization of weight vectors and matrix.2Weadd an auxiliary variable ?
for hinge loss (see Sec-tion 4.2), so the updating of ?
is not necessarywhen using sigmoid loss.The sampling procedure is an iterative processafter initialization (Line 1).
In each iteration,we first optimize the weight vectors and matrix(Line 3) before updating documents?
block assign-ments (Line 5) and topic assignments (Line 7).When using hinge loss, the auxiliary variable?
forevery explicit link needs to be updated (Line 10).4.1 Sampling Block AssignmentsBlock assignment sampling is done by Gibbs sam-pling, using the block assignments and links in A2More details about sampling procedures and equations inthis section (including the sampling and optimization equa-tions using sigmoid loss) are available in the supplementarymaterial.689?D E'D?
KMLS ?
LL?
'dNdNdT 'dT abdy 'dy ',ddA ',ddBdz dw'dz 'dw KWUQFigure 6: The graphical model of LBH-RTM for two documents, in which a weighted stochastic blockmodel is embedded (?, ?, y, a, b, ?, and A).
Each document?s topic distribution has an informativeprior pi.
The model predicts links between documents (B) based on topics (z), words (w), and inter-block link rates (?
), using a max-margin objective.excluding document d and its related links.3Theprobability that d is assigned to block l isPr(yd= l |A?d,y?d, a, b, ?)
?
(N?dl+ ?)??l?
(S?de(l, l?)
+ b)S?dw(l,l?
)+a(S?de(l, l?)
+ b+ Se(d, l?))S?dw(l,l?)+a+Sw(d,l?)Sw(d,l?
)?1?i=0(S?dw(l, l?)
+ a+ i), (3)where Nlis the number of documents assigned toblock l;?ddenotes that the count excludes doc-ument d; Sw(d, l) and Sw(l, l?)
are the sums oflink weights from document d to block l and fromblock l to block l?, respectively:Sw(d, l) =?d?:yd?=lAd,d?
(4)Sw(l, l?)
=?d:yd=lSw(d, l?).
(5)Se(d, l) is the number of possible links from doc-ument d to l (i.e., assuming document d connectsto every document in block l), which equals Nl.The number of possible links from block l to l?is Se(l, l?)
(i.e., assuming every document inblock l connects to every document in block l?
):Se(l, l?)
={Nl?Nl?l 6= l?12Nl(Nl?
1) l = l?.
(6)If we rearrange the terms of Equation 3 and putthe terms which have Sw(d, l?)
together, the value3These equations deal with undirected edges, but they canbe adapted for directed edges.
See supplementary material.of Sw(d, l?)
increases (i.e., document d is moredensely connected with documents in block l?
), theprobability of assigning d to block l decreases ex-ponentially.
Thus if d is more densely connectedwith block l and sparsely connected with otherblocks, it is more likely to be assigned to block l.4.2 Sampling Topic AssignmentsFollowing Polson and Scott (2011), by introducingan auxiliary variable ?d,d?, the conditional prob-ability of assigning td,n, the n-th token in docu-ment d, to topic k isPr(zd,n= k |z?d,n,w?d,n, wd,n= v, yd= l)?
(N?d,nd,k+ ?pi?d,nl,k)N?d,nk,v+ ?N?d,nk,?+ V ??d?exp(?
(?d,d?+ ?d,d?)22?d,d?
), (7)whereNd,kis the number of tokens in document dthat are assigned to topic k;Nk,vdenotes the countof word v assigned to topic k; Marginal countsare denoted by ?
;?d,ndenotes that the count ex-cludes td,n; d?denotes all documents that haveexplicit links with document d. The block topicprior pi?d,nl,kis estimated based on the maximalpath assumption (Cowans, 2006; Wallach, 2008):pi?d,nl,k=?d?
:yd?=lN?d,nd?,k+ ???d?:yd?=lN?d,nd?,?+K??.
(8)the link prediction argument ?d,d?is?d,d?= 1?Bd,d?(?kNd,?Nd?,kNd?,?+R?d,nd,d?).
(9)690whereR?d,nd,d?=K?k=1?kN?d,nd,kNd,?Nd?,kNd?,?+V?v=1?vNd,vNd,?Nd?,vNd?,?+ ?yd,yd??yd,yd?.
(10)Looking at the first term of Equation 7, theprobability of assigning td,nto topic k dependsnot only on its own topic distribution, but also thetopic distribution of the block it belongs to.
Thelinks also matter: Equation 9 gives us the intuitionthat a topic which could increase the likelihood oflinks is more likely to be selected, which formsan interaction between topics and the link graph?the links are guiding the topic sampling while up-dating topic assignments is maximizing the likeli-hood of the link graph.4.3 Parameter OptimizationWhile topic assignments are updated iteratively,the weight vectors and matrix ?, ?
, and ?
areoptimized in each global iteration over the wholecorpus using L-BFGS (Liu and Nocedal, 1989).
Ittakes the likelihood of generatingB using ?, ?
, ?,and current topic and block assignments as the ob-jective function, and optimizes it using the par-tial derivatives with respect to every weight vec-tor/matrix element.The log likelihood ofB using hinge loss isL(B) ???d,d?R2d,d??
2(1 + ?d,d?)Bd,d?Rd,d?2?d,d??K?k=1?2k2?2?V?v=1?2v2?2?L?l=1L?l?=1?2l,l?2?2.
(11)We also need to update the auxiliary vari-able ?d,d?.
Since the likelihood of ?d,d?fol-lows a generalized inverse Gaussian distributionGIG(?d,d?
;12, 1, ?2d,d?
), we sample its recipro-cal ?
?1d,d?from an inverse Gaussian distribution asPr(?
?1d,d?|z,w,?, ?
,?
)= IG(??1d,d?
;1|?d,d?|, 1).
(12)5 Experimental ResultsWe evaluate using the two datasets.
The first one isCORA dataset (McCallum et al, 2000).
After re-moving stopwords and words that appear in fewerthan ten documents, as well as documents with noModelPLRCORA WEBKBRTM (Chang and Blei, 2010) 419.33 141.65LCH-RTM (Yang et al, 2015) 459.55 150.32BS-RTM 391.88 127.25LBS-RTM 383.25 125.41LBH-RTM 360.38 111.79Table 1: Predictive Link Rank Resultswords or links, our vocabulary has 1,240 uniquewords.
The corpus has 2,362 computer science pa-per abstracts with 4,231 citation links.The second dataset is WEBKB.
It is already pre-processed and has 1,703 unique words in vocabu-lary.
The corpus has 877 web pages with 1,608hyperlinks.We treat all links as undirected.
Both datasetsare split into 5 folds, each further split into a devel-opment and test set with approximately the samesize when used for evaluation.5.1 Link Prediction ResultsIn this section, we evaluate LBH-RTM and its varia-tions on link prediction tasks using predictive linkrank (PLR).
A document?s PLR is the average rankof the documents to which it has explicit positivelinks, among all documents, so lower PLR is better.Following the experiment setup in Chang andBlei (2010), we train the models on the train-ing set and predict citation links within held-outdocuments as well as from held-out documentsto training documents.
We tune two importantparameters??
and negative edge ratio (the ratioof the number of sampled negative links to thenumber of explicit positive links)?on the devel-opment set and apply the trained model which per-forms the best on the development set to the testset.4The cross validation results are given in Ta-ble 1, where models are differently equipped withlexical weights (L), WSBM prior (B), SCC prior (C),hinge loss (H), and sigmoid loss (S).5Link pre-diction generally improves with incremental appli-cation of prior knowledge and more sophisticatedlearning techniques.The embedded WSBM brings around 6.5% and10.2% improvement over RTM in PLR on the4We also tune the number of blocks for embedded WSBMand set it to 35 (CORA) and 20 (WEBKB).
The block topicpriors are not applied on unseen documents, since we don?thave available links.5The values of RTM are different from the result reportedby Chang and Blei (2010), because we re-preprocessed theCORA dataset and used different parameters.691CORA and WEBKB datasets, respectively.
Thisindicates that the blocks identified by WSBM arereasonable and consistent with reality.
The lexi-cal weights also help link prediction (LBS-RTM),though less for BS-RTM.
This is understandablesince word distributions are much sparser and donot make as significant a contribution as topic dis-tributions.
Finally, hinge loss improves PLR sub-stantially (LBH-RTM), about 14.1% and 21.1% im-provement over RTM on the CORA and WEBKBdatasets respectively, demonstrating the effective-ness of max-margin learning.The only difference between LCH-RTM andLBH-RTM is the block detection algorithm.
How-ever, their link prediction performance is polesapart?LCH-RTM even fails to outperform RTM.This implies that the quality of blocks identifiedby SCC is not as good as WSBM, which we alsoillustrate in Section 5.4.5.2 Illustrative ExampleWe illustrate our model?s behavior qualitativelyby looking at two abstracts, Koplon and Sontag(1997) and Albertini and Sontag (1992) from theCORA dataset, designated K and A for short.Paper K studies the application of Fourier-typeactivation functions in fully recurrent neural net-works.
Paper A shows that if two neural networkshave equal behaviors as ?black boxes?, they musthave the same number of neurons and the sameweights (except sign reversals).From the titles and abstracts, we can easily findthat both of them are about neural networks (NN).They both contain words like neural, neuron, net-work, recurrent, activation, and nonlinear, whichcorresponds to the topic with words neural, net-work, train, learn, function, recurrent, etc.
Thereis a citation between K and A.
The ranking of thislink improves as the model gets more sophisti-cated (Table 2), except LCH-RTM, which is con-sistent with our PLR results.In Figure 7, we also show the proportions oftopics that dominate the two documents accord-ing to the various models.
There are multiple top-ics dominating K and A according to RTM (Fig-ure 7(a)).
As the model gets more sophisticated,the NN topic proportion gets higher.
Finally, onlythe NN topic dominates the two documents whenLBH-RTM is applied (Figure 7(e)).LCH-RTM gives the highest proportion to theNN topic (Figure 7(b)).
However, the NN topicModel Rank of the LinkRTM 1,265LCH-RTM 1,385BS-RTM 635LBS-RTM 132LBH-RTM 106Table 2: PLR of the citation link between exampledocuments K and A (described in Section 5.2)ModelFET LLRCORA WEBKB CORA WEBKBRTM 0.1330 0.1312 3.001 6.055LCH-RTM 0.1418 0.1678 3.071 6.577BS-RTM 0.1415 0.1950 3.033 6.418LBS-RTM 0.1342 0.1963 2.984 6.212LBH-RTM 0.1453 0.2628 3.105 6.669Table 3: Average Association Scores of Topicssplits into two topics and the proportions are notassigned to the same topic, which greatly bringsdown the link prediction performance.
The split-ting of the NN topic also happens in other mod-els (Figure 7(a) and 7(d)), but they assign propor-tions to the same topic(s).
Further comparing withLBH-RTM, the blocks detected by SCC are not im-proving the modeling of topics and links?somedocuments that should be in two different blocksare assigned to the same one, as we will show inSection 5.4.5.3 Topic Quality ResultsWe use an automatic coherence detectionmethod (Lau et al, 2014) to evaluate topic quality.Specifically, for each topic, we pick out the top nwords and compute the average association scoreof each pair of words, based on the held-outdocuments in development and test sets.We choose n = 25 and use Fisher?s exacttest (Upton, 1992, FET) and log likelihood ra-tio (Moore, 2004, LLR) as the association mea-sures (Table 3).
The main advantage of these mea-sures is that they are robust even when the refer-ence corpus is not large.Coherence improves with WSBM and max-margin learning, but drops a little when addinglexical weights except the FET score on the WE-BKB dataset, because lexical weights are intendedto improve link prediction performance, not topicquality.
Topic quality of LBH-RTM is also betterthan that of LCH-RTM, suggesting that WSBM ben-efits topic quality more than SCC.6920.0 0.2 0.4 0.6 0.8 1.0NN-1NN-2Sequential ModelVisionBelief NetworkKnowledge BaseParallel ComputingA K(a) RTM Topic Proportions0.0 0.2 0.4 0.6 0.8 1.0NN-1NN-2Sequential ModelAlgorithm BoundA K(b) LCH-RTM Topic Proportions0.0 0.2 0.4 0.6 0.8 1.0NNSystem BehaviorResearch GrantOptimization-1Optimization-2A K(c) BS-RTM Topic Proportions0.0 0.2 0.4 0.6 0.8 1.0NN-1NN-2Random ProcessOptimizationEvolutionary Comput.A K(d) LBS-RTM Topic Proportions0.0 0.2 0.4 0.6 0.8 1.0NNBayesian NetworkLinear FunctionMCMCA K(e) LBH-RTM Topic ProportionsFigure 7: Topic proportions given by various models on our two illustrative documents (K and A, de-scribed in described in Section 5.2).
As the model gets more sophisticated, the NN topic proportion getshigher and finally dominates the two documents when LBH-RTM is applied.
Though LCH-RTM gives thehighest proportion to the NN topic, it splits the NN topic into two and does not assign the proportions tothe same one.Block 1 2#Nodes 42 84#Links in the Block 55 142#Links across Blocks 2Table 4: Statistics of Blocks 1 (learning theory)and 2 (Bayes nets), which are merged in SCC.5.4 Block AnalysisIn this section, we illustrate the effectiveness ofthe embedded WSBM over SCC.6As we haveargued, WSBM is able to separate two internallydensely-connected blocks even if there are fewlinks connecting them, while SCC tends to mergethem in this case.
As an example, we focuson two blocks in the CORA dataset identified byWSBM, designated Blocks 1 and 2.
Some statis-tics are given in Table 4.
The two blocks arevery sparsely connected, but comparatively quitedensely connected inside either block.
The twoblocks?
topic distributions also reveal their differ-ences: abstracts in Block 1 mainly focus on learn-ing theory (learn, algorithm, bound, result, etc.
)and MCMC (markov, chain, distribution, converge,etc.).
Abstracts in Block 2, however, have higher6We omit the comparison of WSBM with other models, be-cause this has been done by Aicher et al (2014).
In addition,WSBM is a probabilistic method while SCC is deterministic.They are not comparable quantitatively, so we compare themqualitatively.weights on Bayesian networks (network, model,learn, bayesian, etc.)
and Bayesian estimation (es-timate, bayesian, parameter, analysis, etc.
), whichdiffers from Block 1?s emphasis.
Because of thetwo inter-block links, SCC merges the two blocksinto one, which makes the block topic distributionunclear and misleads the sampler.
WSBM, on theother hand, keeps the two blocks separate, whichgenerates a high-quality prior for the sampler.6 Related WorkTopic models are widely used in information re-trieval (Wei and Croft, 2006), word sense dis-ambiguation (Boyd-Graber et al, 2007), dialoguesegmentation (Purver et al, 2006), and collabora-tive filtering (Marlin, 2003).Topic models can be extended in either up-stream or downstream way.
Upstream modelsgenerate topics conditioned on supervisory in-formation (Daum?e III, 2009; Mimno and Mc-Callum, 2012; Li and Perona, 2005).
Down-stream models, on the contrary, generates topicsand supervisory data simultaneously, which turnsunsupervised topic models to (semi-)supervisedones.
Supervisory data, like labels of documentsand links between documents, can be generatedfrom either a maximum likelihood estimation ap-proach (McAuliffe and Blei, 2008; Chang and693Blei, 2010; Boyd-Graber and Resnik, 2010) or amaximum entropy discrimination approach (Zhuet al, 2012; Yang et al, 2015).In block detection literature, stochastic blockmodel (Holland et al, 1983; Wang and Wong,1987, SBM) is one of the most basic generativemodels dealing with binary-weighted edges.
SBMassumes that each node belongs to only one blockand each link exists with a probability that de-pends on the block assignments of its connect-ing nodes.
It has been generalized for degree-correction (Karrer and Newman, 2011), bipartitestructure (Larremore et al, 2014), and categorialvalues (Guimer`a and Sales-Pardo, 2013), as wellas nonnegative integer-weight network (Aicher etal., 2014, WSBM).Our model combines both topic model andblock detection in a unified framework.
It takestext, links, and the interaction between text andlinks into account simultaneously, contrast to themethods that only consider graph structure (Kimand Leskovec, 2012; Liben-Nowell and Kleinberg,2007) or separate text and links (Chaturvedi et al,2012).7 Conclusions and Future WorkWe introduce LBH-RTM, a discriminative topicmodel that jointly models topics and documentlinks, detecting blocks in the document net-work probabilistically by embedding the weightedstochastic block model, rather via connected-components as in previous models.
A separateDirichlet prior for each block captures its topicpreferences, serving as an informed prior wheninferring documents?
topic distributions.
Max-margin learning learns to predict links from docu-ments?
topic and word distributions and block as-signments.Our model better captures the connections andcontent of paper abstracts, as measured by predic-tive link rank and topic quality.
LBH-RTM yieldstopics with better coherence, though not all tech-niques contribute to the improvement.
We sup-port our quantitative results with qualitative anal-ysis looking at a pair of example documents andat a pair of blocks, highlighting the robustness ofembedded WSBM over blocks defined as SCC.As next steps, we plan to explore model varia-tions to support a wider range of use cases.
Forexample, although we have presented a version ofthe model defined using undirected binary weightedges in the experiment, it would be straightfor-ward to adapt to model both directed/undirectedand binary/nonnegative real weight edges.
We arealso interested in modeling changing topics andvocabularies (Blei and Lafferty, 2006; Zhai andBoyd-Graber, 2013).
In the spirit of treating linksprobabilistically, we plan to explore applicationof the model in suggesting links that do not ex-ist but should, for example in discovering missedcitations, marking social dynamics (Nguyen et al,2014), and identifying topically related content inmultilingual networks of documents (Hu et al,2014).AcknowledgmentThis research has been supported in part, undersubcontract to Raytheon BBN Technologies, byDARPA award HR0011-15-C-0113.
Boyd-Graberis also supported by NSF grants IIS/1320538,IIS/1409287, and NCSE/1422492.
Any opinions,findings, conclusions, or recommendations ex-pressed here are those of the authors and do notnecessarily reflect the view of the sponsors.ReferencesChristopher Aicher, Abigail Z. Jacobs, and AaronClauset.
2014.
Learning latent block structure inweighted networks.
Journal of Complex Networks.Francesca Albertini and Eduardo D. Sontag.
1992.
Forneural networks, function determines form.
In Pro-ceedings of IEEE Conference on Decision and Con-trol.David M. Blei and John D. Lafferty.
2006.
Dynamictopic models.
In Proceedings of the InternationalConference of Machine Learning.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research.Jordan Boyd-Graber and Philip Resnik.
2010.
Holis-tic sentiment analysis across languages: Multilin-gual supervised latent Dirichlet alocation.
In Pro-ceedings of Empirical Methods in Natural LanguageProcessing.Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambigua-tion.
In Proceedings of Empirical Methods in Natu-ral Language Processing.Jonathan Chang and David M. Blei.
2010.
Hierarchi-cal relational models for document networks.
TheAnnals of Applied Statistics.694Snigdha Chaturvedi, Hal Daum?e III, Taesun Moon, andShashank Srivastava.
2012.
A topical graph kernelfor link prediction in labeled graphs.
In Proceedingsof the International Conference of Machine Learn-ing.Philip J. Cowans.
2006.
Probabilistic Document Mod-elling.
Ph.D. thesis, University of Cambridge.Hal Daum?e III.
2009.
Markov random topic fields.In Proceedings of the Association for ComputationalLinguistics.Roger Guimer`a and Marta Sales-Pardo.
2013.
A net-work inference method for large-scale unsupervisedidentification of novel drug-drug interactions.
PLoSComputational Biology.Paul W. Holland, Kathryn Blackmond Laskey, andSamuel Leinhardt.
1983.
Stochastic blockmodels:First steps.
Social Networks.Yuening Hu, Ke Zhai, Vlad Eidelman, and JordanBoyd-Graber.
2014.
Polylingual tree-based topicmodels for translation domain adaptation.
In Pro-ceedings of the Association for Computational Lin-guistics.Brian Karrer and Mark EJ Newman.
2011.
Stochasticblockmodels and community structure in networks.Physical Review E.Myunghwan Kim and Jure Leskovec.
2012.
La-tent multi-group membership graph model.
In Pro-ceedings of the International Conference of MachineLearning.Ren?ee Koplon and Eduardo D. Sontag.
1997.
UsingFourier-neural recurrent networks to fit sequentialinput/output data.
Neurocomputing.Daniel B. Larremore, Aaron Clauset, and Abigail Z. Ja-cobs.
2014.
Efficiently inferring community struc-ture in bipartite networks.
Physical Review E.Jey Han Lau, David Newman, and Timothy Baldwin.2014.
Machine reading tea leaves: Automaticallyevaluating topic coherence and topic model quality.In Proceedings of the Association for ComputationalLinguistics.Fei-Fei Li and Pietro Perona.
2005.
A Bayesian hier-archical model for learning natural scene categories.In Computer Vision and Pattern Recognition.David Liben-Nowell and Jon Kleinberg.
2007.
Thelink-prediction problem for social networks.
Jour-nal of the American Society for Information Scienceand Technology.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming.Benjamin Marlin.
2003.
Modeling user rating profilesfor collaborative filtering.
In Proceedings of Ad-vances in Neural Information Processing Systems.Jon D. McAuliffe and David M. Blei.
2008.
Super-vised topic models.
In Proceedings of Advances inNeural Information Processing Systems.Andrew Kachites McCallum, Kamal Nigam, JasonRennie, and Kristie Seymore.
2000.
Automating theconstruction of Internet portals with machine learn-ing.
Information Retrieval.David Mimno and Andrew McCallum.
2012.
Topicmodels conditioned on arbitrary features withDirichlet-multinomial regression.
In Proceedings ofUncertainty in Artificial Intelligence.Robert Moore.
2004.
On log-likelihood-ratios and thesignificance of rare events.
In Proceedings of Em-pirical Methods in Natural Language Processing.Viet-An Nguyen, Jordan Boyd-Graber, and PhilipResnik.
2013.
Lexical and hierarchical topic regres-sion.
In Proceedings of Advances in Neural Infor-mation Processing Systems.Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,Deborah Cai, Jennifer Midberry, and Yuanxin Wang.2014.
Modeling topic control to detect influencein conversations using nonparametric topic models.Machine Learning.Nicholas G. Polson and Steven L. Scott.
2011.Data augmentation for support vector machines.Bayesian Analysis.Matthew Purver, Thomas L. Griffiths, Konrad P.K?ording, and Joshua B. Tenenbaum.
2006.
Unsu-pervised topic modelling for multi-party spoken dis-course.
In Proceedings of the Association for Com-putational Linguistics.Graham JG Upton.
1992.
Fisher?s exact test.
Journalof the Royal Statistical Society.Hanna M. Wallach.
2008.
Structured Topic Models forLanguage.
Ph.D. thesis, University of Cambridge.Yuchung J. Wang and George Y. Wong.
1987.
Stochas-tic blockmodels for directed graphs.
Journal of theAmerican Statistical Association.Xing Wei and W. Bruce Croft.
2006.
LDA-based doc-ument models for ad-hoc retrieval.
In Proceedingsof the ACM SIGIR Conference on Research and De-velopment in Information Retrieval.Weiwei Yang, Jordan Boyd-Graber, and Philip Resnik.2015.
Birds of a feather linked together: A discrim-inative topic model using link-based priors.
In Pro-ceedings of Empirical Methods in Natural LanguageProcessing.Ke Zhai and Jordan Boyd-Graber.
2013.
Online latentDirichlet alocation with infinite vocabulary.
In Pro-ceedings of the International Conference of MachineLearning.695Jun Zhu, Amr Ahmed, and Eric P. Xing.
2012.MedLDA: Maximum margin supervised topic mod-els.
Journal of Machine Learning Research.Jun Zhu, Ning Chen, Hugh Perkins, and Bo Zhang.2014.
Gibbs max-margin topic models with dataaugmentation.
Journal of Machine Learning Re-search.696
