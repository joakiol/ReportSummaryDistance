Learning Subjective Nouns using Extraction Pattern Bootstrapping?Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112riloff@cs.utah.eduJanyce WiebeDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260wiebe@cs.pitt.eduTheresa WilsonIntelligent Systems ProgramUniversity of PittsburghPittsburgh, PA 15260twilson@cs.pitt.eduAbstractWe explore the idea of creating a subjectiv-ity classifier that uses lists of subjective nounslearned by bootstrapping algorithms.
The goalof our research is to develop a system thatcan distinguish subjective sentences from ob-jective sentences.
First, we use two bootstrap-ping algorithms that exploit extraction patternsto learn sets of subjective nouns.
Then wetrain a Naive Bayes classifier using the subjec-tive nouns, discourse features, and subjectivityclues identified in prior research.
The boot-strapping algorithms learned over 1000 subjec-tive nouns, and the subjectivity classifier per-formed well, achieving 77% recall with 81%precision.1 IntroductionMany natural language processing applications couldbenefit from being able to distinguish between factualand subjective information.
Subjective remarks comein a variety of forms, including opinions, rants, allega-tions, accusations, suspicions, and speculation.
Ideally,information extraction systems should be able to distin-guish between factual information (which should be ex-tracted) and non-factual information (which should bediscarded or labeled as uncertain).
Question answeringsystems should distinguish between factual and specula-tive answers.
Multi-perspective question answering aimsto present multiple answers to the user based upon specu-lation or opinions derived from different sources.
Multi-?
This work was supported in part by the National Sci-ence Foundation under grants IIS-0208798 and IRI-9704240.The data preparation was performed in support of the North-east Regional Reseach Center (NRRC) which is sponsored bythe Advanced Research and Development Activity (ARDA), aU.S.
Government entity which sponsors and promotes researchof import to the Intelligence Community which includes but isnot limited to the CIA, DIA, NSA, NIMA, and NRO.document summarization systems need to summarize dif-ferent opinions and perspectives.
Spam filtering systemsmust recognize rants and emotional tirades, among otherthings.
In general, nearly any system that seeks to iden-tify information could benefit from being able to separatefactual and subjective information.Subjective language has been previously studied infields such as linguistics, literary theory, psychology, andcontent analysis.
Some manually-developed knowledgeresources exist, but there is no comprehensive dictionaryof subjective language.Meta-Bootstrapping (Riloff and Jones, 1999) andBasilisk (Thelen and Riloff, 2002) are bootstrapping al-gorithms that use automatically generated extraction pat-terns to identify words belonging to a semantic cate-gory.
We hypothesized that extraction patterns couldalso identify subjective words.
For example, the pat-tern ?expressed <direct object>?
often extracts subjec-tive nouns, such as ?concern?, ?hope?, and ?support?.Furthermore, these bootstrapping algorithms require onlya handful of seed words and unannotated texts for train-ing; no annotated data is needed at all.In this paper, we use the Meta-Bootstrapping andBasilisk algorithms to learn lists of subjective nouns froma large collection of unannotated texts.
Then we traina subjectivity classifier on a small set of annotated data,using the subjective nouns as features along with someother previously identified subjectivity features.
Our ex-perimental results show that the subjectivity classifierperforms well (77% recall with 81% precision) and thatthe learned nouns improve upon previous state-of-the-artsubjectivity results (Wiebe et al, 1999).2 Subjectivity Data2.1 The Annotation SchemeIn 2002, an annotation scheme was developedfor a U.S. government-sponsored project with ateam of 10 researchers (the annotation instruc-tions and project reports are available on the Webat http://www.cs.pitt.edu/?wiebe/pubs/ardasummer02/).Edmonton, May-June 2003held at HLT-NAACL 2003 , pp.
25-32Proceeings of the Seventh CoNLL conferenceThe scheme was inspired by work in linguistics andliterary theory on subjectivity, which focuses on howopinions, emotions, etc.
are expressed linguistically incontext (Banfield, 1982).
The scheme is more detailedand comprehensive than previous ones.
We mention onlythose aspects of the annotation scheme relevant to thispaper.The goal of the annotation scheme is to identify andcharacterize expressions of private states in a sentence.Private state is a general covering term for opinions, eval-uations, emotions, and speculations (Quirk et al, 1985).For example, in sentence (1) the writer is expressing anegative evaluation.
(1) ?The time has come, gentlemen, for Sharon, the as-sassin, to realize that injustice cannot last long.
?Sentence (2) reflects the private state of Western coun-tries.
Mugabe?s use of ?overwhelmingly?
also reflects aprivate state, his positive reaction to and characterizationof his victory.
(2) ?Western countries were left frustrated and impotentafter Robert Mugabe formally declared that he had over-whelmingly won Zimbabwe?s presidential election.
?Annotators are also asked to judge the strength of eachprivate state.
A private state can have low, medium, highor extreme strength.2.2 Corpus and Agreement ResultsOur data consists of English-language versions of foreignnews documents from FBIS, the U.S. Foreign BroadcastInformation Service.
The data is from a variety of publi-cations and countries.
The annotated corpus used to trainand test our subjectivity classifiers (the experiment cor-pus) consists of 109 documents with a total of 2197 sen-tences.
We used a separate, annotated tuning corpus of33 documents with a total of 698 sentences to establishsome experimental parameters.1Each document was annotated by one or both of twoannotators, A and T. To allow us to measure interanno-tator agreement, the annotators independently annotatedthe same 12 documents with a total of 178 sentences.
Webegan with a strict measure of agreement at the sentencelevel by first considering whether the annotator markedany private-state expression, of any strength, anywherein the sentence.
If so, the sentence should be subjective.Otherwise, it is objective.
Table 1 shows the contingencytable.
The percentage agreement is 88%, and the ?
valueis 0.71.1The annotated data will be available to U.S. governmentcontractors this summer.
We are working to resolve copyrightissues to make it available to the wider research community.Tagger TSubj ObjTagger A Subj nyy = 112 nyn = 16Obj nny = 6 nnn = 44Table 1: Agreement for sentence-level annotationsTagger TSubj ObjTagger A Subj nyy = 106 nyn = 9Obj nny = 0 nnn = 44Table 2: Agreement for sentence-level annotations, low-strength cases removedOne would expect that there are clear cases of objec-tive sentences, clear cases of subjective sentences, andborderline sentences in between.
The agreement studysupports this.
In terms of our annotations, we definea sentence as borderline if it has at least one private-state expression identified by at least one annotator, andall strength ratings of private-state expressions are low.Table 2 shows the agreement results when such border-line sentences are removed (19 sentences, or 11% of theagreement test corpus).
The percentage agreement in-creases to 94% and the ?
value increases to 0.87.As expected, the majority of disagreement cases in-volve low-strength subjectivity.
The annotators consis-tently agree about which are the clear cases of subjectivesentences.
This leads us to define the gold-standard thatwe use in our experiments.
A sentence is subjective if itcontains at least one private-state expression of mediumor higher strength.
The second class, which we call ob-jective, consists of everything else.
Thus, sentences withonly mild traces of subjectivity are tossed into the objec-tive category, making the system?s goal to find the clearlysubjective sentences.3 Using Extraction Patterns to LearnSubjective NounsIn the last few years, two bootstrapping algorithms havebeen developed to create semantic dictionaries by ex-ploiting extraction patterns: Meta-Bootstrapping (Riloffand Jones, 1999) and Basilisk (Thelen and Riloff, 2002).Extraction patterns were originally developed for infor-mation extraction tasks (Cardie, 1997).
They representlexico-syntactic expressions that typically rely on shal-low parsing and syntactic role assignment.
For example,the pattern ?<subject> was hired?
would apply to sen-tences that contain the verb ?hired?
in the passive voice.The subject would be extracted as the hiree.Meta-Bootstrapping and Basilisk were designed tolearn words that belong to a semantic category (e.g.,?truck?
is a VEHICLE and ?seashore?
is a LOCATION).Both algorithms begin with unannotated texts and seedwords that represent a semantic category.
A bootstrap-ping process looks for words that appear in the same ex-traction patterns as the seeds and hypothesizes that thosewords belong to the same semantic class.
The principlebehind this approach is that words of the same semanticclass appear in similar pattern contexts.
For example, thephrases ?lived in?
and ?traveled to?
will co-occur withmany noun phrases that represent LOCATIONS.In our research, we want to automatically identifywords that are subjective.
Subjective terms have manydifferent semantic meanings, but we believe that the samecontextual principle applies to subjectivity.
In this sec-tion, we briefly overview these bootstrapping algorithmsand explain how we used them to generate lists of subjec-tive nouns.3.1 Meta-BootstrappingThe Meta-Bootstrapping (?MetaBoot?)
process (Riloffand Jones, 1999) begins with a small set of seed wordsthat represent a targeted semantic category (e.g., 10words that represent LOCATIONS) and an unannotatedcorpus.
First, MetaBoot automatically creates a set of ex-traction patterns for the corpus by applying and instanti-ating syntactic templates.
This process literally producesthousands of extraction patterns that, collectively, will ex-tract every noun phrase in the corpus.
Next, MetaBootcomputes a score for each pattern based upon the num-ber of seed words among its extractions.
The best pat-tern is saved and all of its extracted noun phrases areautomatically labeled as the targeted semantic category.2MetaBoot then re-scores the extraction patterns, using theoriginal seed words as well as the newly labeled words,and the process repeats.
This procedure is called mutualbootstrapping.A second level of bootstrapping (the ?meta-?
boot-strapping part) makes the algorithm more robust.
Whenthe mutual bootstrapping process is finished, all nounsthat were put into the semantic dictionary are re-evaluated.
Each noun is assigned a score based on howmany different patterns extracted it.
Only the five bestnouns are allowed to remain in the dictionary.
The otherentries are discarded, and the mutual bootstrapping pro-cess starts over again using the revised semantic dictio-nary.3.2 BasiliskBasilisk (Thelen and Riloff, 2002) is a more recent boot-strapping algorithm that also utilizes extraction patternsto create a semantic dictionary.
Similarly, Basilisk be-gins with an unannotated text corpus and a small set of2Our implementation of Meta-Bootstrapping learns individ-ual nouns (vs. noun phrases) and discards capitalized words.seed words for a semantic category.
The bootstrappingprocess involves three steps.
(1) Basilisk automaticallygenerates a set of extraction patterns for the corpus andscores each pattern based upon the number of seed wordsamong its extractions.
This step is identical to the firststep of Meta-Bootstrapping.
Basilisk then puts the bestpatterns into a Pattern Pool.
(2) All nouns3 extracted by apattern in the Pattern Pool are put into a Candidate WordPool.
Basilisk scores each noun based upon the set ofpatterns that extracted it and their collective associationwith the seed words.
(3) The top 10 nouns are labeled asthe targeted semantic class and are added to the dictio-nary.
The bootstrapping process then repeats, using theoriginal seeds and the newly labeled words.The main difference between Basilisk and Meta-Bootstrapping is that Basilisk scores each noun basedon collective information gathered from all patterns thatextracted it.
In contrast, Meta-Bootstrapping identifiesa single best pattern and assumes that everything it ex-tracted belongs to the same semantic class.
The secondlevel of bootstrapping smoothes over some of the prob-lems caused by this assumption.
In comparative experi-ments (Thelen and Riloff, 2002), Basilisk outperformedMeta-Bootstrapping.
But since our goal of learning sub-jective nouns is different from the original intent of thealgorithms, we tried them both.
We also suspected theymight learn different words, in which case using both al-gorithms could be worthwhile.3.3 Experimental ResultsThe Meta-Bootstrapping and Basilisk algorithms needseed words and an unannotated text corpus as input.Since we did not need annotated texts, we created a muchlarger training corpus, the bootstrapping corpus, by gath-ering 950 new texts from the FBIS source mentionedin Section 2.2.
To find candidate seed words, we auto-matically identified 850 nouns that were positively corre-lated with subjective sentences in another data set.
How-ever, it is crucial that the seed words occur frequentlyin our FBIS texts or the bootstrapping process will notget off the ground.
So we searched for each of the 850nouns in the bootstrapping corpus, sorted them by fre-quency, and manually selected 20 high-frequency wordsthat we judged to be strongly subjective.
Table 3 showsthe 20 seed words used for both Meta-Bootstrapping andBasilisk.We ran each bootstrapping algorithm for 400 itera-tions, generating 5 words per iteration.
Basilisk gener-ated 2000 nouns and Meta-Bootstrapping generated 1996nouns.4 Table 4 shows some examples of extraction pat-3Technically, each head noun of an extracted noun phrase.4Meta-Bootstrapping will sometimes produce fewer than 5words per iteration if it has low confidence in its judgements.cowardice embarrassment hatred outragecrap fool hell slanderdelight gloom hypocrisy sighdisdain grievance love twitdismay happiness nonsense virtueTable 3: Subjective Seed WordsExtraction Patterns Examples of Extracted Nounsexpressed <dobj> condolences, hope, grief,views, worries, recognitionindicative of <np> compromise, desire, thinkinginject <dobj> vitality, hatredreaffirmed <dobj> resolve, position, commitmentvoiced <dobj> outrage, support, skepticism,disagreement, opposition,concerns, gratitude, indignationshow of <np> support, strength, goodwill,solidarity, feeling<subject> was shared anxiety, view, niceties, feelingTable 4: Extraction Pattern Examplesterns that were discovered to be associated with subjec-tive nouns.Meta-Bootstrapping and Basilisk are semi-automaticlexicon generation tools because, although the bootstrap-ping process is 100% automatic, the resulting lexiconsneed to be reviewed by a human.5 So we manually re-viewed the 3996 words proposed by the algorithms.
Thisprocess is very fast; it takes only a few seconds to classifyeach word.
The entire review process took approximately3-4 hours.
One author did this labeling; this person didnot look at or run tests on the experiment corpus.Strong Subjective Weak Subjectivetyranny scum aberration plaguesmokescreen bully allusion riskapologist devil apprehensions dramabarbarian liar beneficiary trickbelligerence pariah resistant promisecondemnation venom credence intriguesanctimonious diatribe distortion unityexaggeration mockery eyebrows failuresrepudiation anguish inclination toleranceinsinuation fallacies liability persistentantagonism evil assault trustatrocities genius benefit successdenunciation goodwill blood spiritexploitation injustice controversy slumphumiliation innuendo likelihood sincerityill-treatment revenge peaceful eternitysympathy rogue pressure rejectionTable 5: Examples of Learned Subjective Nouns5This is because NLP systems expect dictionaries to havehigh integrity.
Even if the algorithms could achieve 90% ac-curacy, a dictionary in which 1 of every 10 words is definedincorrectly would probably not be desirable.B M B ?
M B ?
MStrongSubj 372 192 110 454WeakSubj 453 330 185 598Total 825 522 295 1052Table 6: Subjective Word Lexicons after Manual Review(B=Basilisk, M=MetaBootstrapping)0.20.30.40.50.60.70.80.910 200 400 600 800 1000 1200 1400 1600 1800 2000%of WordsSubjectiveNumber of Words Generated?Basilisk?
?MetaBoot?Figure 1: Accuracy during BootstrappingWe classified the words as StrongSubjective, WeakSub-jective, or Objective.
Objective terms are not subjective atall (e.g., ?chair?
or ?city?).
StrongSubjective terms havestrong, unambiguously subjective connotations, such as?bully?
or ?barbarian?.
WeakSubjective was used forthree situations: (1) words that have weak subjective con-notations, such as ?aberration?
which implies somethingout of the ordinary but does not evoke a strong sense ofjudgement, (2) words that have multiple senses or uses,where one is subjective but the other is not.
For example,the word ?plague?
can refer to a disease (objective) or anonslaught of something negative (subjective), (3) wordsthat are objective by themselves but appear in idiomaticexpressions that are subjective.
For example, the word?eyebrows?
was labeled WeakSubjective because the ex-pression ?raised eyebrows?
probably occurs more oftenin our corpus than literal references to ?eyebrows?.
Ta-ble 5 shows examples of learned words that were classi-fied as StrongSubjective or WeakSubjective.Once the words had been manually classified, we couldgo back and measure the effectiveness of the algorithms.The graph in Figure 1 tracks their accuracy as the boot-strapping progressed.
The X-axis shows the number ofwords generated so far.
The Y-axis shows the percent-age of those words that were manually classified as sub-jective.
As is typical of bootstrapping algorithms, ac-curacy was high during the initial iterations but taperedoff as the bootstrapping continued.
After 20 words,both algorithms were 95% accurate.
After 100 wordsBasilisk was 75% accurate and MetaBoot was 81% accu-rate.
After 1000 words, accuracy dropped to about 28%for MetaBoot, but Basilisk was still performing reason-ably well at 53%.
Although 53% accuracy is not high fora fully automatic process, Basilisk depends on a humanto review the words so 53% accuracy means that the hu-man is accepting every other word, on average.
Thus, thereviewer?s time was still being spent productively evenafter 1000 words had been hypothesized.Table 6 shows the size of the final lexicons createdby the bootstrapping algorithms.
The first two columnsshow the number of subjective terms learned by Basiliskand Meta-Bootstrapping.
Basilisk was more prolific, gen-erating 825 subjective terms compared to 522 for Meta-Bootstrapping.
The third column shows the intersectionbetween their word lists.
There was substantial overlap,but both algorithms produced many words that the otherdid not.
The last column shows the results of mergingtheir lists.
In total, the bootstrapping algorithms produced1052 subjective nouns.4 Creating Subjectivity ClassifiersTo evaluate the subjective nouns, we trained a NaiveBayes classifier using the nouns as features.
We also in-corporated previously established subjectivity clues, andadded some new discourse features.
In this section, wedescribe all the feature sets and present performance re-sults for subjectivity classifiers trained on different com-binations of these features.
The threshold values and fea-ture representations used in this section are the ones thatproduced the best results on our separate tuning corpus.4.1 Subjective Noun FeaturesWe defined four features to represent the sets of subjec-tive nouns produced by the bootstrapping algorithms.BA-Strong: the set of StrongSubjective nouns generatedby BasiliskBA-Weak: the set of WeakSubjective nouns generatedby BasiliskMB-Strong: the set of StrongSubjective nouns generatedby Meta-BootstrappingMB-Weak: the set of WeakSubjective nouns generatedby Meta-BootstrappingFor each set, we created a three-valued feature based onthe presence of 0, 1, or ?
2 words from that set.
We usedthe nouns as feature sets, rather than define a separatefeature for each word, so the classifier could generalizeover the set to minimize sparse data problems.
We willrefer to these as the SubjNoun features.4.2 Previously Established FeaturesWiebe, Bruce, & O?Hara (1999) developed a machinelearning system to classify subjective sentences.
We ex-perimented with the features that they used, both to com-pare their results to ours and to see if we could benefitfrom their features.
We will refer to these as the WBOfeatures.WBO includes a set of stems positively correlated withthe subjective training examples (subjStems) and a setof stems positively correlated with the objective trainingexamples (objStems).
We defined a three-valued featurefor the presence of 0, 1, or ?
2 members of subjStemsin a sentence, and likewise for objStems.
For our exper-iments, subjStems includes stems that appear ?
7 timesin the training set, and for which the precision is 1.25times the baseline word precision for that training set.objStems contains the stems that appear ?
7 times andfor which at least 50% of their occurrences in the trainingset are in objective sentences.
WBO also includes a bi-nary feature for each of the following: the presence in thesentence of a pronoun, an adjective, a cardinal number, amodal other than will, and an adverb other than not.We also added manually-developed features found byother researchers.
We created 14 feature sets represent-ing some classes from (Levin, 1993; Ballmer and Bren-nenstuhl, 1981), some Framenet lemmas with frame ele-ment experiencer (Baker et al, 1998), adjectives manu-ally annotated for polarity (Hatzivassiloglou and McKe-own, 1997), and some subjectivity clues listed in (Wiebe,1990).
We represented each set as a three-valued featurebased on the presence of 0, 1, or ?
2 members of the set.We will refer to these as the manual features.4.3 Discourse FeaturesWe created discourse features to capture the density ofclues in the text surrounding a sentence.
First, we com-puted the average number of subjective clues and objec-tive clues per sentence, normalized by sentence length.The subjective clues, subjClues, are all sets for which3-valued features were defined above (except objStems).The objective clues consist only of objStems.
For sen-tence S, let ClueRatesubj(S) = |subjClues in S||S| andClueRateobj(S) = |objStems in S||S| .
Then we defineAvgClueRatesubj to be the average of ClueRate(S)over all sentences S and similarly for AvgClueRateobj.Next, we characterize the number of subjective andobjective clues in the previous and next sentences as:higher-than-expected (high), lower-than-expected (low),or expected (medium).
The value for ClueRatesubj(S)is high if ClueRatesubj(S) ?
AvgClueRatesubj ?
1.3;low if ClueRatesubj(S) ?
AvgClueRatesubj/1.3; oth-erwise it is medium.
The values for ClueRateobj(S) aredefined similarly.Using these definitions we created four features:ClueRatesubj for the previous and following sen-tences, and ClueRateobj for the previous and follow-ing sentences.
We also defined a feature for sentencelength.
Let AvgSentLen be the average sentence length.SentLen(S) is high if length(S) ?
AvgSentLen?1.3;low if length(S) ?
AvgSentLen/1.3; and medium oth-erwise.4.4 Classification ResultsWe conducted experiments to evaluate the performanceof the feature sets, both individually and in various com-binations.
Unless otherwise noted, all experiments in-volved training a Naive Bayes classifier using a particu-lar set of features.
We evaluated each classifier using 25-fold cross validation on the experiment corpus and usedpaired t-tests to measure significance at the 95% confi-dence level.
As our evaluation metrics, we computed ac-curacy (Acc) as the percentage of the system?s classifica-tions that match the gold-standard, and precision (Prec)and recall (Rec) with respect to subjective sentences.Acc Prec Rec(1) Bag-Of-Words 73.3 81.7 70.9(2) WBO 72.1 76.0 77.4(3) Most-Frequent 59.0 59.0 100.0Table 7: Baselines for ComparisonTable 7 shows three baseline experiments.
Row (3)represents the common baseline of assigning every sen-tence to the most frequent class.
The Most-Frequentbaseline achieves 59% accuracy because 59% of the sen-tences in the gold-standard are subjective.
Row (2) isa Naive Bayes classifier that uses the WBO features,which performed well in prior research on sentence-levelsubjectivity classification (Wiebe et al, 1999).
Row (1)shows a Naive Bayes classifier that uses unigram bag-of-words features, with one binary feature for the absenceor presence in the sentence of each word that appearedduring training.
Pang et al (2002) reported that a similarexperiment produced their best results on a related clas-sification task.
The difference in accuracy between Rows(1) and (2) is not statistically significant (Bag-of-Word?shigher precision is balanced by WBO?s higher recall).Next, we trained a Naive Bayes classifier using onlythe SubjNoun features.
This classifier achieved goodprecision (77%) but only moderate recall (64%).
Uponfurther inspection, we discovered that the subjectivenouns are good subjectivity indicators when they appear,but not every subjective sentence contains one of them.And, relatively few sentences contain more than one,making it difficult to recognize contextual effects (i.e.,multiple clues in a region).
We concluded that the ap-propriate way to benefit from the subjective nouns is touse them in tandem with other subjectivity clues.Acc Prec Rec(1) 76.1 81.3 77.4 WBO+SubjNoun+manual+discourse(2) 74.3 78.6 77.8 WBO+SubjNoun(3) 72.1 76.0 77.4 WBOTable 8: Results with New FeaturesTable 8 shows the results of Naive Bayes classifierstrained with different combinations of features.
The ac-curacy differences between all pairs of experiments inTable 8 are statistically significant.
Row (3) uses onlythe WBO features (also shown in Table 7 as a baseline).Row (2) uses the WBO features as well as the SubjNounfeatures.
There is a synergy between these feature sets:using both types of features achieves better performancethan either one alone.
The difference is mainly precision,presumably because the classifier found more and bettercombinations of features.
In Row (1), we also added themanual and discourse features.
The discourse featuresexplicitly identify contexts in which multiple clues arefound.
This classifier produced even better performance,achieving 81.3% precision with 77.4% recall.
The 76.1%accuracy result is significantly higher than the accuracyresults for all of the other classifiers (in both Table 8 andTable 7).Finally, higher precision classification can be obtainedby simply classifying a sentence as subjective if it con-tains any of the StrongSubjective nouns.
On our data, thismethod produces 87% precision with 26% recall.
Thisapproach could support applications for which precisionis paramount.5 Related WorkSeveral types of research have involved document-levelsubjectivity classification.
Some work identifies inflam-matory texts (e.g., (Spertus, 1997)) or classifies reviewsas positive or negative ((Turney, 2002; Pang et al, 2002)).Tong?s system (Tong, 2001) generates sentiment time-lines, tracking online discussions and creating graphs ofpositive and negative opinion messages over time.
Re-search in genre classification may include recognition ofsubjective genres such as editorials (e.g., (Karlgren andCutting, 1994; Kessler et al, 1997; Wiebe et al, 2001)).In contrast, our work classifies individual sentences, asdoes the research in (Wiebe et al, 1999).
Sentence-levelsubjectivity classification is useful because most docu-ments contain a mix of subjective and objective sen-tences.
For example, newspaper articles are typicallythought to be relatively objective, but (Wiebe et al, 2001)reported that, in their corpus, 44% of sentences (in arti-cles that are not editorials or reviews) were subjective.Some previous work has focused explicitly on learn-ing subjective words and phrases.
(Hatzivassiloglou andMcKeown, 1997) describes a method for identifying thesemantic orientation of words, for example that beauti-ful expresses positive sentiments.
Researchers have fo-cused on learning adjectives or adjectival phrases (Tur-ney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe,2000) and verbs (Wiebe et al, 2001), but no previouswork has focused on learning nouns.
A unique aspectof our work is the use of bootstrapping methods that ex-ploit extraction patterns.
(Turney, 2002) used patternsrepresenting part-of-speech sequences, (Hatzivassiloglouand McKeown, 1997) recognized adjectival phrases, and(Wiebe et al, 2001) learned N-grams.
The extractionpatterns used in our research are linguistically richer pat-terns, requiring shallow parsing and syntactic role assign-ment.In recent years several techniques have been developedfor semantic lexicon creation (e.g., (Hearst, 1992; Riloffand Shepherd, 1997; Roark and Charniak, 1998; Cara-ballo, 1999)).
Semantic word learning is different fromsubjective word learning, but we have shown that Meta-Bootstrapping and Basilisk could be successfully appliedto subjectivity learning.
Perhaps some of these othermethods could also be used to learn subjective words.6 ConclusionsThis research produced interesting insights as well as per-formance results.
First, we demonstrated that weaklysupervised bootstrapping techniques can learn subjec-tive terms from unannotated texts.
Subjective featureslearned from unannotated documents can augment or en-hance features learned from annotated training data us-ing more traditional supervised learning techniques.
Sec-ond, Basilisk and Meta-Bootstrapping proved to be use-ful for a different task than they were originally intended.By seeding the algorithms with subjective words, the ex-traction patterns identified expressions that are associatedwith subjective nouns.
This suggests that the bootstrap-ping algorithms should be able to learn not only generalsemantic categories, but any category for which wordsappear in similar linguistic phrases.
Third, our best sub-jectivity classifier used a wide variety of features.
Sub-jectivity is a complex linguistic phenomenon and our evi-dence suggests that reliable subjectivity classification re-quires a broad array of features.ReferencesC.
Baker, C. Fillmore, and J. Lowe.
1998.
The berkeleyframenet project.
In Proceedings of the COLING-ACL.T.
Ballmer and W. Brennenstuhl.
1981.
Speech Act Clas-sification: A Study in the Lexical Analysis of EnglishSpeech Activity Verbs.
Springer-Verlag.A.
Banfield.
1982.
Unspeakable Sentences.
Routledgeand Kegan Paul, Boston.S.
Caraballo.
1999.
Automatic Acquisition of aHypernym-Labeled Noun Hierarchy from Text.
InProceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 120?126.C.
Cardie.
1997.
Empirical Methods in Information Ex-traction.
AI Magazine, 18(4):65?79.V.
Hatzivassiloglou and K. McKeown.
1997.
Predictingthe semantic orientation of adjectives.
In ACL-EACL1997.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proc.
of the 14th Interna-tional Conference on Computational Linguistics.J.
Karlgren and D. Cutting.
1994.
Recognizing text gen-res with simple metrics using discriminant analysis.
InCOLING-94.B.
Kessler, G. Nunberg, and H. Schutze.
1997.
Auto-matic detection of text genre.
In Proc.
ACL-EACL-97.Beth Levin.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press, Chicago.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
sentiment classification using machine learningtechniques.
In Proceedings of the 2002 Conference onEmpirical Methods in Natural Language Processing.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985.A Comprehensive Grammar of the English Language.Longman, New York.E.
Riloff and R. Jones.
1999.
Learning Dictionaries forInformation Extraction by Multi-Level Bootstrapping.In Proceedings of the 16th National Conference on Ar-tificial Intelligence.E.
Riloff and J. Shepherd.
1997.
A Corpus-Based Ap-proach for Building Semantic Lexicons.
In Proceed-ings of the Second Conference on Empirical Methodsin Natural Language Processing, pages 117?124.B.
Roark and E. Charniak.
1998.
Noun-phraseCo-occurrence Statistics for Semi-automatic Seman-tic Lexicon Construction.
In Proceedings of the 36thAnnual Meeting of the Association for ComputationalLinguistics, pages 1110?1116.E.
Spertus.
1997.
Smokey: Automatic recognition ofhostile messages.
In Proc.
IAAI.M.
Thelen and E. Riloff.
2002.
A Bootstrapping Methodfor Learning Semantic Lexicons Using Extraction Pattern Contexts.
In Proceedings of the 2002 Conferenceon Empirical Methods in Natural Language Process-ing.R.
Tong.
2001.
An operational system for detecting andtracking opinions in on-line discussion.
In SIGIR 2001Workshop on Operational Text Classification.P.
Turney.
2002.
Thumbs Up or Thumbs Down?
Seman-tic Orientation Applied to Unsupervised Classificationof Reviews.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics.J.
Wiebe, R. Bruce, and T. O?Hara.
1999.
Developmentand use of a gold standard data set for subjectivity clas-sifications.
In Proc.
37th Annual Meeting of the Assoc.for Computational Linguistics (ACL-99).J.
Wiebe, T. Wilson, and M. Bell.
2001.
Identifying col-locations for recognizing opinions.
In Proc.
ACL-01Workshop on Collocation: Computational Extraction,Analysis, and Exploitation, July.J.
Wiebe.
1990.
Recognizing Subjective Sentences: AComputational Investigation of Narrative Text.
Ph.D.thesis, State University of New York at Buffalo.J.
Wiebe.
2000.
Learning subjective adjectives from cor-pora.
In 17th National Conference on Artificial Intelli-gence.
