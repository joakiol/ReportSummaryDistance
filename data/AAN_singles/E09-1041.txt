Proceedings of the 12th Conference of the European Chapter of the ACL, pages 354?362,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsLearning-Based Named Entity Recognition for Morphologically-Rich,Resource-Scarce LanguagesKazi Saidul Hasan and Md.
Altaf ur Rahman and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{saidul,altaf,vince}@hlt.utdallas.eduAbstractNamed entity recognition for morpholog-ically rich, case-insensitive languages, in-cluding the majority of semitic languages,Iranian languages, and Indian languages,is inherently more difficult than its Englishcounterpart.
Worse still, progress on ma-chine learning approaches to named entityrecognition for many of these languagesis currently hampered by the scarcity ofannotated data and the lack of an accu-rate part-of-speech tagger.
While it ispossible to rely on manually-constructedgazetteers to combat data scarcity, thisgazetteer-centric approach has the poten-tial weakness of creating irreproducibleresults, since these name lists are notpublicly available in general.
Motivatedin part by this concern, we present alearning-based named entity recognizerthat does not rely on manually-constructedgazetteers, using Bengali as our represen-tative resource-scarce, morphologically-rich language.
Our recognizer achievesa relative improvement of 7.5% in F-measure over a baseline recognizer.
Im-provements arise from (1) using in-duced affixes, (2) extracting informationfrom online lexical databases, and (3)jointly modeling part-of-speech taggingand named entity recognition.1 IntroductionWhile research in natural language processing hasgained a lot of momentum in the past severaldecades, much of this research effort has been fo-cusing on only a handful of politically-importantlanguages such as English, Chinese, and Arabic.On the other hand, being the fifth most spoken lan-guage1 with more than 200 million native speakersresiding mostly in Bangladesh and the Indian stateof West Bengal, Bengali has far less electronicresources than the aforementioned languages.
Infact, a major obstacle to the automatic processingof Bengali is the scarcity of annotated corpora.One potential solution to the problem of datascarcity is to hand-annotate a small amount ofdata with the desired linguistic information andthen develop bootstrapping algorithms for com-bining this small amount of labeled data witha large amount of unlabeled data.
In fact, co-training (Blum and Mitchell, 1998) has been suc-cessfully applied to English named entity recog-nition (NER) (Collins & Singer [henceforth C&S](1999)).
In C&S?s approach, consecutive wordstagged as proper nouns are first identified as poten-tial NEs, and each such NE is then labeled by com-bining the outputs of two co-trained classifiers.Unfortunately, there are practical difficulties in ap-plying this technique to Bengali NER.
First, oneof C&S?s co-trained classifiers uses features basedon capitalization, but Bengali is case-insensitive.Second, C&S identify potential NEs based onproper nouns, but unlike English, (1) proper nounidentification for Bengali is non-trivial, due to thelack of capitalization; and (2) there does not ex-ist an accurate Bengali part-of-speech (POS) tag-ger for providing such information, owing to thescarcity of annotated data for training the tagger.In other words, Bengali NER is complicated notonly by the scarcity of annotated data, but also bythe lack of an accurate POS tagger.
One couldimagine building a Bengali POS tagger using un-1See http://en.wikipedia.org/wiki/Bengali language.354supervised induction techniques that have beensuccessfully developed for English (e.g., Schu?tze(1995), Clark (2003)), including the recently-proposed prototype-driven approach (Haghighiand Klein, 2006) and Bayesian approach (Gold-water and Griffiths, 2007).
The majority of theseapproaches operate by clustering distributionallysimilar words, but they are unlikely to work wellfor Bengali for two reasons.
First, Bengali is arelatively free word order language, and hencethe distributional information collected for Ben-gali words may not be as reliable as that for En-glish words.
Second, many closed-class wordsthat typically appear in the distributional repre-sentation of an English word (e.g., prepositionsand particles such as ?in?
and ?to?)
are realizedas inflections in Bengali, and the absence of theseinformative words implies that the context vectormay no longer capture sufficient information foraccurately clustering the Bengali words.In view of the above problems, many learning-based Bengali NE recognizers have relied heavilyon manually-constructed name lists for identify-ing persons, organizations, and locations.
Thereare at least two weaknesses associated with thisgazetteer-centric approach.
First, these name listsare typically not publicly available, making it dif-ficult to reproduce the results of these NE recog-nizers.
Second, it is not clear how comprehen-sive these lists are.
Relying on comprehensive liststhat comprise a large portion of the names in thetest set essentially reduces the NER problem to adictionary-lookup problem, which is arguably notvery interesting from a research perspective.In addition, many existing learning-based Ben-gali NE recognizers have several common weak-nesses.
First, they use as features pseudo-affixes,which are created by extracting the first n and thelast n characters of a word (where 1 ?
n ?
4)(e.g., Dandapat et al (2007)).
While affixes en-code essential grammatical information in Ben-gali due to its morphological richness, this extrac-tion method is arguably too ad-hoc and does notcover many useful affixes.
Second, they typicallyadopt a pipelined NER architecture, performingPOS tagging prior to NER and encoding the result-ing not-so-accurate POS information as a feature.In other words, errors in POS tagging are propa-gated to the NE recognizer via the POS feature,thus limiting its performance.Motivated in part by these weaknesses, we in-vestigate how to improve a learning-based NE rec-ognizer that does not rely on manually-constructedgazetteers.
Specifically, we investigate two learn-ing architectures for our NER system.
The firstone is the aforementioned pipelined architecturein which the NE recognizer uses as features theoutput of a POS tagger that is trained indepen-dently of the recognizer.
Unlike existing BengaliPOS and NE taggers, however, we examine twonew knowledge sources for training these taggers:(1) affixes induced from an unannotated corpusand (2) semantic class information extracted fromWikipedia.
In the second architecture, we jointlylearn the POS tagging and the NER tasks, allow-ing features for one task to be accessible to theother task during learning.
The goal is to exam-ine whether any benefits can be obtained via jointmodeling, which could address the error propaga-tion problem with the pipelined architecture.While we focus on Bengali NER in this pa-per, none of the proposed techniques are language-specific.
In fact, we believe that these techniquesare of relevance and interest to the EACL com-munity because they can be equally applicable tothe numerous resource-scarce European and Mid-dle Eastern languages that share similar linguis-tic and extra-linguistic properties as Bengali.
Forinstance, the majority of semitic languages andIranian languages are, like Bengali, morpholog-ically productive; and many East European lan-guages such as Czech and Polish resemble Bengaliin terms of not only their morphological richness,but also their relatively free word order.The rest of the paper is organized as follows.In Section 2, we briefly describe the related work.Sections 3 and 4 show how we induce affixes froman unannotated corpus and extract semantic classinformation from Wikipedia.
In Sections 5 and6, we train and evaluate a POS tagger and an NErecognizer independently, augmenting the featureset typically used for these two tasks with our newknowledge sources.
Finally, we describe and eval-uate our joint model in Section 7.2 Related WorkCucerzan and Yarowsky (1999) exploit morpho-logical and contextual patterns to propose alanguage-independent solution to NER.
They useaffixes based on the paradigm that named enti-ties corresponding to a particular class have sim-ilar morphological structure.
Their bootstrapping355approach is tested on Romanian, English, Greek,Turkish, and Hindi.
The recall for Hindi is thelowest (27.84%) among the five languages, sug-gesting that the lack of case information can sig-nificantly complicate the NER task.To investigate the role of gazetteers in NER,Mikheev et al (1999) combine grammar rules withmaximum entropy models and vary the gazetteersize.
Experimental results show that (1) the F-scores for NE classes like person and organiza-tion are still high without gazetteers, ranging from85% to 92%; and (2) a small list of country namescan improve the low F-score for locations substan-tially.
It is worth noting that their recognizer re-quires that the input data contain POS tags andsimple semantic tags, whereas ours automaticallyacquires such linguistic information.
In addition,their approach uses part of the dataset to extend thegazetteer.
Therefore, the resulting gazetteer list isspecific to a particular domain; on the other hand,our approach does not generate a domain-specificlist, since it makes use of Wikipedia articles.Kozareva (2006) generates gazetteer lists forperson and location names from unlabeled datausing common patterns and a graph explorationalgorithm.
The location pattern is essentiallya preposition followed by capitalized contextwords.
However, this approach is inadequate for amorphologically-rich language like Bengali, sinceprepositions are often realized as inflections.3 Affix InductionSince Bengali is morphologically productive, a lotof grammatical information about Bengali wordsis expressed via affixes.
Hence, these affixes couldserve as useful features for training POS and NEtaggers.
In this section, we show how to induceaffixes from an unannotated corpus.We rely on a simple idea proposed by Keshavaand Pitler (2006) for inducing affixes.
Assume that(1) V is a vocabulary (i.e., a set of distinct words)extracted from a large, unannotated corpus, (2) ?and ?
are two character sequences, and (3) ??
isthe concatenation of ?
and ?.
If ??
and ?
arefound in V , we extract ?
as a suffix.
Similarly, if??
and ?
are found in V , we extract ?
as a prefix.In principle, we can use all of the induced af-fixes as features for training a POS tagger and anNE recognizer.
However, we choose to use onlythose features that survive our feature selectionprocess (to be described below), for the follow-ing reasons.
First, the number of induced affixesis large, and using only a subset of them as fea-tures could make the training process more effi-cient.
Second, the above affix induction method isarguably overly simplistic and hence many of theinduced affixes could be spurious.Our feature selection process is fairly simple:we (1) score each affix by multiplying its fre-quency (i.e., the number of distinct words in V towhich each affix attaches) and its length2, and (2)select only those whose score is above a certainthreshold.
In our experiments, we set this thresh-old to 50, and generate our vocabulary of 140Kwords from five years of articles taken from theBengali newspaper Prothom Alo.
This enables usto induce 979 prefixes and 975 suffixes.4 Semantic Class Induction fromWikipediaWikipedia has recently been used as a knowl-edge source for various language processing tasks,including taxonomy construction (Ponzetto andStrube, 2007a), coreference resolution (Ponzettoand Strube, 2007b), and English NER (e.g.,Bunescu and Pas?ca (2006), Cucerzan (2007),Kazama and Torisawa (2007), Watanabe et al(2007)).
Unlike previous work on using Wikipediafor NER, our goal here is to (1) generate a listof phrases and tokens that are potentially namedentities from the 16914 articles in the BengaliWikipedia3 and (2) heuristically annotate each ofthem with one of four classes, namely, PER (per-son), ORG (organization), LOC (location), or OTH-ERS (i.e., anything other than PER, ORG and LOC).4.1 Generating an Annotated List of PhrasesWe employ the steps below to generate our anno-tated list.Generating and annotating the titles Recallthat each Wikipedia article has been optionally as-signed to one or more categories by its creatorand/or editors.
We use these categories to help an-notate the title of an article.
Specifically, if an ar-ticle has a category whose name starts with ?Bornon?
or ?Death on,?
we label the corresponding ti-tle with PER.
Similarly, if it has a category whosename starts with ?Cities of?
or ?Countries of,?
we2The dependence on frequency and length is motivated bythe observation that less frequent and shorter affixes are morelikely to be erroneous (see Goldsmith (2001)).3See http://bn.wikipedia.org.
In our experiments, we usedthe Bengali Wikipedia dump obtained on October 22, 2007.356NE Class KeywordsPER ?born,?
?died,?
?one,?
?famous?LOC ?city,?
?area,?
?population,?
?located,?
?part of?ORG ?establish,?
?situate,?
?publish?Table 1: Keywords for each named entity classlabel the title as LOC.
If an article does not be-long to one of the four categories above, we labelits title with the help of a small set of seed key-words shown in Table 1.
Specifically, for each ofthe three NE classes shown on the left of Table1, we compute a weighted sum of its keywords:a keyword that appears in the first paragraph hasa weight of 3, a keyword that appears elsewherein the article has a weight of 1, and a keywordthat does not appear in the article has a weight of0.
The rationale behind using different weights issimple: the first paragraph is typically a brief ex-position of the title, so it should in principle con-tain words that correlate more closely with the ti-tle than words appearing in the rest of the article.We then label the title with the class that has thelargest weighted sum.
Note, however, that we ig-nore any article that contains fewer than two key-words, since we do not have reliable evidence forlabeling its title as one of the NE classes.
We putall these annotated titles into a title list.Getting more location names To get more loca-tion names, we search for the character sequences?birth place:?
and ?death place:?
in each article,extracting the phrase following any of these se-quences and label it as LOC.
We put all such la-beled locations into the title list.Generating and annotating the tokens in the ti-tles Next, we extract the word tokens from eachtitle in the title list and label each token with anNE class.
The reason for doing this is to improvegeneralization: if ?Dhaka University?
is labeled asORG in the title list, then it is desirable to also labelthe token ?University?
as ORG, because this couldhelp identify an unseen phrase that contains theterm ?University?
as an organization.
Our tokenlabeling method is fairly simple.
First, we gener-ate the tokens from each title in the title list, as-signing to each token the same NE label as thatof the title from which it is generated.
For in-stance, from the title ?Anna Frank,?
?Anna?
willbe labeled as PER; and from ?Anna University,?
?Anna?
will be labeled as LOC.
To resolve suchambiguities (i.e., assigning different labels to thesame token), we keep a count of how many times?Anna?
is labeled with each NE class, and set itsfinal label to be the most frequent NE class.
Weput all these annotated tokens into a token list.
Ifthe title list and the token list have an element incommon, we remove the element from the tokenlist, since we have a higher confidence in the la-bels of the titles.Merging the lists Finally, we append the tokenlist to the title list.
The resulting title list contains4885 PERs, 15176 LOCs, and 188 ORGs.4.2 Applying the Annotated List to a TextWe can now use the title list to annotate a text.Specifically, we process each word w in the text ina left-to-right manner, using the following steps:1.
Check whether w has been labeled.
If so, weskip this word and process the next one.2.
Check whether w appears in the SamsadBengali-English Dictionary4.
If so, we as-sume that w is more likely to be used as anon-named entity, thus leaving the word un-labeled and processing the next word instead.3.
Find the longest unlabeled word sequence5that begins with w and appears in the titlelist.
If no such sequence exists, we leave wunlabeled and process the next word.
Oth-erwise, we label it with the NE tag givenby the title list.
To exemplify, consider atext that starts with the sentence ?Smith Col-lege is in Massachusetts.?
When processing?Smith,?
?Smith College?
is the longest se-quence that starts with ?Smith?
and appearsin the title list (as an ORG).
As a result, welabel all occurrences of ?Smith College?
inthe text as an ORG.
(Note that without usingthe longest match heuristic, ?Smith?
wouldlikely be mislabeled as PER.)
In addition, wetake the last word of the ORG (which in thiscase is ?College?)
and annotate each of its oc-currence in the rest of the text as ORG.6These automatic annotations will then be usedto derive a set of WIKI features for training ourPOS tagger and NE recognizer.
Hence, unlikeexisting Bengali NE recognizers, our ?gazetteers?are induced rather than manually created.4See http://dsal.uchicago.edu/dictionaries/biswasbengali/.5This is a sequence in which each word is unlabeled.6However, if we have a PER match (e.g., ?Anna Frank?
)or a LOC match (e.g., ?Las Vegas?
), we take each word in thematched phrase and label each of its occurrence in the rest ofthe text with the same NE tag.357Current word wiPrevious word wi?12nd previous word wi?2Next word wi+12nd next word wi+2Current pseudo-affixes pfi (prefix), sfi (suffix)Current induced affixes pii (prefix), sii (suffix)Previous induced affixes pii?1 (prefix), sii?1 (suffix)Induced affix bigrams pii?1pii (prefix), sii?1sii (suffix)Current Wiki tag wikiiPrevious Wiki tag wikii?1Wiki bigram wikii?1wikiiWord bigrams wi?2wi?1, wi?1wi, wiwi+1,wi+1wi+2Word trigrams wi?2wi?1wiCurrent number qiTable 2: Feature templates for the POS taggingexperiments5 Part-of-Speech TaggingIn this section, we will show how we train andevaluate our POS tagger.
As mentioned before, wehypothesize that introducing our two knowledgesources into the feature set for the tagger couldimprove its performance: using the induced affixescould improve the extraction of grammatical infor-mation from the words, and using the Wikipedia-induced list, which in principle should comprisemostly of names, could help improve the identifi-cation of proper nouns.Corpus Our corpus is composed of 77942 wordsand is annotated with one of 26 POS tags in thetagset defined by IIIT Hyderabad7.
Using this cor-pus, we perform 5-fold cross-validation (CV) ex-periments in our evaluation.
It is worth noting thatthis dataset has a high unknown word rate of 15%(averaged over the five folds), which is due to thesmall size of the dataset.
While this rate is compa-rable to another Bengali POS dataset described inDandapat et al (2007), it is much higher than the2.6% unknown word rate in the test set for Ratna-parkhi?s (1996) English POS tagging experiments.Creating training instances Following previ-ous work on POS tagging, we create one train-ing instance for each word in the training set.
Theclass value of an instance is the POS tag of the cor-responding word.
Each instance is represented bya set of linguistic features, as described next.7A detailed description of these POS tags can be found inhttp://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf,and are omitted here due to space limitations.
This tagsetand the Penn Treebank tagset differ in that (1) nouns do nothave a number feature; (2) verbs do not have a tense feature;and (3) adjectives and adverbs are not subcategorized.Features Our feature set consists of (1) base-line features motivated by those used in Danda-pat et al?s (2007) Bengali POS tagger and Singhet al?s (2006) Hindi POS tagger, as well as (2)features derived from our induced affixes and theWikipedia-induced list.
More specifically, thebaseline feature set has (1) word unigrams, bi-grams and trigrams; (2) pseudo-affix features thatare created by taking the first three characters andthe last three characters of the current word; and(3) a binary feature that determines whether thecurrent word is a number.
As far as our new fea-tures are concerned, we create one induced prefixfeature and one induced suffix feature from boththe current word and the previous word, as wellas two bigrams involving induced prefixes and in-duced suffixes.
We also create three WIKI features,including the Wikipedia-induced NE tag of thecurrent word and that of the previous word, as wellas the combination of these two tags.
Note thatthe Wikipedia-induced tag of a word can be ob-tained by annotating the test sentence under con-sideration using the list generated from the Ben-gali Wikipedia (see Section 4).
To make the de-scription of these features more concrete, we showthe feature templates in Table 2.Learning algorithm We used CRF++8, a C++implementation of conditional random fields (Laf-ferty et al, 2001), as our learning algorithm fortraining a POS tagging model.Evaluating the model To evaluate the resultingPOS tagger, we generate test instances in the sameway as the training instances.
5-fold CV results ofthe POS tagger are shown in Table 3.
Each rowconsists of three numbers: the overall accuracy,as well as the accuracies on the seen and the un-seen words.
Row 1 shows the accuracy when thebaseline feature set is used; row 2 shows the ac-curacy when the baseline feature set is augmentedwith our two induced affix features; and the lastrow shows the results when both the induced af-fix and the WIKI features are incorporated intothe baseline feature set.
Perhaps not surprisingly,(1) adding more features improves performance,and (2) accuracies on the seen words are substan-tially better than those on the unseen words.
Infact, adding the induced affixes to the baseline fea-ture set yields a 7.8% reduction in relative errorin overall accuracy.
We also applied a two-tailedpaired t-test (p < 0.01), first to the overall accura-8Available from http://crfpp.sourceforge.net358Experiment Overall Seen UnseenBaseline 89.83 92.96 72.08Baseline+Induced Affixes 90.57 93.39 74.64Baseline+Induced Affixes+Wiki 90.80 93.50 75.58Table 3: 5-fold cross-validation accuracies forPOS taggingPredicted Tag Correct Tag % of ErrorNN NNP 22.7NN JJ 9.6JJ NN 7.4NNP NN 5.0NN VM 4.9Table 4: Most frequent errors for POS taggingcies in rows 1 and 2, and then to the overall accu-racies in rows 2 and 3.
Both pairs of numbers arestatistically significantly different from each other,meaning that incorporating the two induced affixfeatures and then the WIKI features both yields sig-nificant improvements.Error analysis To better understand the results,we examined the errors made by the tagger.
Themost frequent errors are shown in Table 4.
Fromthe table, we see that the largest source of errorsarises from mislabeling proper nouns as commonnouns.
This should be expected, as proper nounidentification is difficult due to the lack of capital-ization information.
Unfortunately, failure to iden-tify proper nouns could severely limit the recall ofan NE recognizer.
Also, adjectives and commonnouns are difficult to distinguish, since these twosyntactic categories are morphologically and dis-tributionally similar to each other.
Finally, manyerrors appear to involve mislabeling a word as acommon noun.
The reason is that there is a largerpercentage of common nouns (almost 30%) in thetraining set than other POS tags, thus causing themodel to prefer tagging a word as a common noun.6 Named Entity RecognitionIn this section, we show how to train and evaluateour NE recognizer.
The recognizer adopts a tradi-tional architecture, assuming that POS tagging isperformed prior to NER.
In other words, the NErecognizer will use the POS acquired in Section 5as one of its features.
As in Section 5, we will fo-cus on examining how our knowledge sources (theinduced affixes and the WIKI features) impact theperformance of our recognizer.Corpus The corpus we used for NER evaluationis the same as the one described in the previousPOS of current word tiPOS of previous word ti?1POS of 2nd previous word ti?2POS of next word ti+1POS of 2nd next word ti+2POS bigrams ti?2ti?1, ti?1ti, titi+1, ti+1ti+2First word fwiTable 5: Additional feature templates for the NERexperimentssection.
Specifically, in addition to POS infor-mation, each sentence in the corpus is annotatedwith NE information.
We focus on recognizing thethree major NE types in this paper, namely persons(PER), organizations (ORG), and locations (LOC).There are 1721 PERs, 104 ORGs, and 686 LOCs inthe corpus.
As far as evaluation is concerned, weconduct 5-fold CV experiments, dividing the cor-pus into the same five folds as in POS tagging.Creating training instances We view NErecognition as a sequence labeling problem.
Inother words, we combine NE identification andclassification into one step, labeling each word ina test text with its NE tag.
Any word that does notbelong to one of our three NE tags will be labeledas OTHERS.
We adopt the IOB convention, pre-ceding an NE tag with a B if the word is the firstword of an NE and an I otherwise.
Now, to trainthe NE recognizer, we create one training instancefrom each word in a training text.
The class valueof an instance is the NE tag of the correspondingword, or OTHERS if the word is not part of an NE.Each instance is represented by a set of linguisticfeatures, as described next.Features Our feature set consists of (1) base-line features motivated by those used in Ekbalet al?s (2008) Bengali NE recognizer, as well as(2) features derived from our induced affixes andthe Wikipedia-induced list.
More specifically, thebaseline feature set has (1) word unigrams; (2)pseudo-affix features that are created by taking thefirst three characters and the last three charactersof the current word; (3) a binary feature that deter-mines whether the current word is the first word ofa sentence; and (4) a set of POS-related features,including the POS of the current word and its sur-rounding words, as well as POS bigrams formedfrom the current and surrounding words.
Our in-duced affixes and WIKI features are incorporatedinto the baseline NE feature set in the same man-ner as in POS tagging.
In essence, the feature tem-359Experiment R P FBaseline 60.97 74.46 67.05Person 66.18 74.06 69.90Organization 29.81 44.93 35.84Location 52.62 80.40 63.61Baseline+Induced Affixes 60.45 73.30 66.26Person 65.70 72.61 69.02Organization 31.73 46.48 37.71Location 51.46 80.05 62.64Baseline+Induced Affixes+Wiki 63.24 75.19 68.70Person 66.47 75.16 70.55Organization 30.77 43.84 36.16Location 60.06 79.69 68.50Table 6: 5-fold cross-validation results for NERplates employed by the NE recognizer are the top12 templates in Table 2 and those in Table 5.Learning algorithm We again use CRF++ asour sequence learner for acquiring the recognizer.Evaluating the model To evaluate the resultingNE tagger, we generate test instances in the sameway as the training instances.
To score the outputof the recognizer, we use the CoNLL-2000 scor-ing program9, which reports performance in termsof recall (R), precision (P), and F-measure (F).
AllNE results shown in Table 6 are averages of the5-fold CV experiments.
The first block of the Ta-ble 6 shows the overall results when the baselinefeature set is used; in addition, we also show re-sults for each of the three NE tags.
As we can see,the baseline achieves an F-measure of 67.05.
Thesecond block shows the results when the baselinefeature set is augmented with our two induced af-fix features.
Somewhat unexpectedly, F-measuredrops by 0.8% in comparison to the baseline.
Ad-ditional experiments are needed to determine thereason.
Finally, when the WIKI features are in-corporated into the augmented feature set, the sys-tem achieves an F-measure of 68.70 (see the thirdblock), representing a statistically significant in-crease of 1.6% in F-measure over the baseline.As we can see, improvements stem primarily fromdramatic gains in recall for locations.Discussions Several points deserve mentioning.First, the model performs poorly on the ORGs, ow-ing to the small number of organization namesin the corpus.
Worse still, the recall drops afteradding the WIKI features.
We examined the listof induced ORG names and found that it is fairlynoisy.
This can be attributed in part to the diffi-culty in forming a set of seed words that can ex-tract ORGs with high precision (e.g., the ORG seed?situate?
extracted many LOCs).
Second, using the9http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txtWIKI features does not help recalling the PERs.
Acloser examination of the corpus reveals the rea-son: many sentences describe fictitious characters,whereas Wikipedia would be most useful for arti-cles that describe famous people.
Overall, whilethe WIKI features provide our recognizer with asmall, but significant, improvement, the useful-ness of the Bengali Wikipedia is currently lim-ited by its small size.
Nevertheless, we believe theBengali Wikipedia will become a useful resourcefor language processing as its size increases.7 A Joint Model for POS Tagging andNERThe NE recognizer described thus far has adopteda pipelined architecture, and hence its perfor-mance could be limited by the errors of the POStagger.
In fact, as discussed before, the majorsource of errors made by our POS tagger concernsthe confusion between proper nouns and commonnouns, and this type of error, when propagatedto the NE recognizer, could severely limit its re-call.
Also, there is strong empirical support forthis argument: the NE recognizers, when given ac-cess to the correct POS tags, have F-scores rang-ing from 76-79%, which are 10% higher on aver-age than those with POS tags that were automat-ically computed.
Consequently, we hypothesizethat modeling POS tagging and NER jointly wouldyield better performance than learning the twotasks separately.
In fact, many approaches havebeen developed to jointly model POS tagging andnoun phrase chunking, including transformation-based learning (Ngai and Florian, 2001), factorialHMMs (Duh, 2005), and dynamic CRFs (Suttonet al, 2007).
Some of these approaches are fairlysophisticated and also require intensive computa-tions during inference.
For instance, when jointlymodeling POS tagging and chunking, Sutton et al(2007) reduce the number of POS tags from 45to 5 when training a factorial dynamic CRF on asmall dataset (with only 209 sentences) in order toreduce training and inference time.In contrast, we propose a relatively simplemodel for jointly learning Bengali POS taggingand NER, by exploiting the limited dependenciesbetween the two tasks.
Specifically, we make theobservation that most of the Bengali words that arepart of an NE are also proper nouns.
In fact, basedon statistics collected from our evaluation corpus(see Sections 5 and 6), this observation is correct360Experiment R P FBaseline 54.76 81.70 65.57Baseline+Induced Affixes 56.79 88.96 69.32Baseline+Induced Affixes+Wiki 61.73 86.35 71.99Table 7: 5-fold cross-validation joint modeling re-sults for NER97.3% of the time.
Note, however, that this ob-servation does not hold for English, since manyprepositions and determiners are part of an NE.On the other hand, this observation largely holdsfor Bengali because prepositions and determinersare typically realized as noun suffixes.This limited dependency between the POS tagsand the NE tags allows us to develop a simplemodel for jointly learning the two tasks.
Morespecifically, we will use CRF++ to learn the jointmodel.
Training and test instances are generatedas described in the previous two subsections (i.e.,one instance per word).
The feature set will con-sist of the union of the features that were used totrain the POS tagger and the NE tagger indepen-dently, minus the POS-related features that wereused in the NE tagger.
The class value of an in-stance is computed as follows.
If a word is not aproper noun, its class is simply its POS tag.
Oth-erwise, its class is its NE tag, which can be PER,ORG, LOC, or OTHERS.
In other words, our jointmodel exploits the observation that we made ear-lier in the section by assuming that only propernouns can be part of a named entity.
This allowsus to train a joint model without substantially in-creasing the number of classes.We again evaluate our joint model using 5-foldCV experiments.
The NE results of the model areshown in Table 7.
The rows here can be interpretedin the same manner as those in Table 6.
Compar-ing these three experiments with their counterpartsin Table 6, we can see that, except for the base-line, jointly modeling offers a significant improve-ment of 3.3% in overall F-measure.10 In particu-lar, the joint model benefits significantly from our10The POS tagging results are not shown due to space lim-itations.
Overall, the POS accuracies drop insignificantly asa result of joint modeling, for the following reason.
Recallfrom Section 5 that the major source of POS tagging errorsarises from the mislabeling of many proper nouns as com-mon nouns, due primarily to the large number of commonnouns in the corpus.
The joint model aggravates this prob-lem by subcategorizing the proper nouns into different NEclasses, causing the tagger to have an even stronger bias to-wards labeling a proper noun as a common noun than before.Nevertheless, as seen from the results in Tables 6 and 7, sucha bias has yielded an increase in NER precision.two knowledge sources, achieving an F-measureof 71.99% when both of them are incorporated.Finally, to better understand the value of the in-duced affix features in the joint model as well asthe pipelined model described in Section 6, weconducted an ablation experiment, in which we in-corporated only the WIKI features into the base-line feature set.
With pipelined modeling, the F-measure for NER is 68.87%, which is similar tothe case where both induced affixes and the WIKIfeatures are used.
With joint modeling, however,the F-measure for NER is 70.87%, which is 1%lower than the best joint modeling score.
Theseresults provide suggestive evidence that the in-duced affix features play a significant role in theimproved performance of the joint model.8 ConclusionsWe have explored two types of linguistic fea-tures, namely the induced affix features and theWikipedia-related features, to improve a BengaliPOS tagger and NE recognizer.
Our experimen-tal results have demonstrated that (1) both types offeatures significantly improve a baseline POS tag-ger and (2) the Wikipedia-related features signif-icantly improve a baseline NE recognizer.
More-over, by exploiting the limited dependencies be-tween Bengali POS tags and NE tags, we pro-posed a new model for jointly learning the twotasks, which not only avoids the error-propagationproblem present in the pipelined system architec-ture, but also yields statistically significant im-provements over the NE recognizer that is trainedindependently of the POS tagger.
When applied incombination, our three extensions contributed to arelative improvement of 7.5% in F-measure overthe baseline NE recognizer.
Most importantly, webelieve that these extensions are of relevance andinterest to the EACL community because manyEuropean and Middle Eastern languages resembleBengali in terms of not only their morphologicalrichness but also their scarcity of annotated cor-pora.
We plan to empirically verify our belief infuture work.AcknowledgmentsWe thank the three anonymous reviewers for theirinvaluable comments on the paper.
We also thankCRBLP, BRAC University, Bangladesh, for pro-viding us with Bengali resources.
This work wassupported in part by NSF Grant IIS-0812261.361ReferencesAvrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Pro-ceedings of COLT, pages 92?100.Razvan Bunescu and Marius Pas?ca.
2006.
Using en-cyclopedic knowledge for named entity disambigua-tion.
In Proceedings of EACL, pages 9?16.Alexander Clark.
2003.
Combining distributional andmorphological information for part-of-speech induc-tion.
In Proceedings of EACL, pages 59?66.Michael Collins and Yoram Singer.
1999.
Unsuper-vised models for named entity classification.
In Pro-ceedings of EMNLP/VLC, pages 100?110.Silviu Cucerzan and David Yarowsky.
1999.
Lan-guage independent named entity recognition com-bining morphological and contextual evidence.
InProceedings of EMNLP/VLC, pages 90?99.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on Wikipedia data.
In Proceed-ings of EMNLP-CoNLL, pages 708?716.Sandipan Dandapat, Sudeshna Sarkar, and AnupamBasu.
2007.
Automatic part-of-speech tagging forBengali: An approach for morphologically rich lan-guages in a poor resource scenario.
In Proceedingsof the ACL Companion Volume, pages 221?224.Kevin Duh.
2005.
Jointly labeling multiple sequences:A factorial HMM approach.
In Proceedings of theACL Student Research Workshop, pages 19?24.Asif Ekbal, Rejwanul Haque, and Sivaji Bandyopad-hyay.
2008.
Named entity recognition in Bengali:A conditional random field approach.
In Proceed-ings of IJCNLP, pages 589?594.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 27(2):153?198.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully Bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of the ACL, pages744?751.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofHLT-NAACL, pages 320?327.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Ex-ploiting Wikipedia as external knowledge for namedentity recognition.
In Proceedings of EMNLP-CoNLL, pages 698?707.Samarth Keshava and Emily Pitler.
2006.
A simpler,intuitive approach to morpheme induction.
In PAS-CAL Challenge Workshop on Unsupervised Segmen-tation of Words into Morphemes.Zornitsa Kozareva.
2006.
Bootstrapping named entityrecognition with automatically generated gazetteerlists.
In Proceedings of the EACL Student ResearchWorkshop, pages 15?22.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of ICML, pages 282?289.Andrei Mikheev, Marc Moens, and Claire Grover.1999.
Named entity recognition without gazetteers.In Proceedings of EACL, pages 1?8.Grace Ngai and Radu Florian.
2001.
Transformationbased learning in the fast lane.
In Proceedings ofNAACL, pages 40?47.Simone Paolo Ponzetto and Michael Strube.
2007a.Deriving a large scale taxonomy from wikipedia.
InProceedings of AAAI, pages 1440?1445.Simone Paolo Ponzetto and Michael Strube.
2007b.Knowledge derived from Wikipedia for computingsemantic relatedness.
Journal of Artificial Intelli-gence Research, 30:181?212.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof EMNLP, pages 133?142.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of EACL, pages 141?148.Smriti Singh, Kuhoo Gupta, Manish Shrivastava, andPushpak Bhattacharyya.
2006.
Morphological rich-ness offsets resource demand ?
Experiences in con-structing a POS tagger for Hindi.
In Proceedingsof the COLING/ACL 2006 Main Conference PosterSessions, pages 779?786.Charles Sutton, Andrew McCallum, and KhashayarRohanimanesh.
2007.
Dynamic conditional randomfields: Factorized probabilistic models for labelingand segmenting sequence data.
Journal of MachineLearning Research, 8:693?723.Yotaro Watanabe, Masayuki Asahara, and Yuji Mat-sumoto.
2007.
A graph-based approach to namedentity categorization in Wikipedia using conditionalrandom fields.
In Proceedings of EMNLP-CoNLL,pages 649?657.362
