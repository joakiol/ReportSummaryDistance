Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1466?1477,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Fast Unified Model for Parsing and Sentence UnderstandingSamuel R. Bowman1,2,3,?sbowman@stanford.eduJon Gauthier2,3,4,?jgauthie@stanford.eduAbhinav Rastogi3,5arastogi@stanford.eduRaghav Gupta2,3,6rgupta93@stanford.eduChristopher D. Manning1,2,3,6manning@stanford.eduChristopher Potts1,6cgpotts@stanford.edu1Stanford Linguistics2Stanford NLP Group3Stanford AI Lab4Stanford Symbolic Systems5Stanford Electrical Engineering6Stanford Computer ScienceAbstractTree-structured neural networks exploitvaluable syntactic parse information asthey interpret the meanings of sentences.However, they suffer from two key techni-cal problems that make them slow and un-wieldy for large-scale NLP tasks: they usu-ally operate on parsed sentences and theydo not directly support batched computa-tion.
We address these issues by introduc-ing the Stack-augmented Parser-InterpreterNeural Network (SPINN), which combinesparsing and interpretation within a singletree-sequence hybrid model by integratingtree-structured sentence interpretation intothe linear sequential structure of a shift-reduce parser.
Ourmodel supports batchedcomputation for a speedup of up to 25?over other tree-structured models, and itsintegrated parser can operate on unparseddata with little loss in accuracy.
We evalu-ate it on the Stanford NLI entailment taskand show that it significantly outperformsother sentence-encoding models.1 IntroductionA wide range of current models in NLP are builtaround a neural network component that producesvector representations of sentence meaning (e.g.,Sutskever et al, 2014; Tai et al, 2015).
This com-ponent, the sentence encoder, is generally formu-lated as a learned parametric function from a se-quence of word vectors to a sentence vector, andthis function can take a range of different forms.Common sentence encoders include sequence-based recurrent neural networkmodels (RNNs, seeFigure 1a)with Long Short-TermMemory (LSTM,?The first two authors contributed equally.the old cat ate...the cat sat down...(a) A conventional sequence-based RNN for two sentences....the old cat ateatethe old catold catcatoldthe...the cat sat downsat downdownsatthe catcatthe(b) A conventional TreeRNN for two sentences.Figure 1: An illustration of two standard designsfor sentence encoders.
The TreeRNN, unlike thesequence-based RNN, requires a substantially dif-ferent connection structure for each sentence, mak-ing batched computation impractical.Hochreiter and Schmidhuber, 1997), which ac-cumulate information over the sentence sequen-tially; convolutional neural networks (Kalchbren-ner et al, 2014; Zhang et al, 2015), which accu-mulate information using filters over short local se-quences of words or characters; and tree-structuredrecursive neural networks (TreeRNNs, Goller andK?chler, 1996; Socher et al, 2011a, see Figure 1b),which propagate information up a binary parse tree.Of these, the TreeRNN appears to be the prin-cipled choice, since meaning in natural languagesentences is known to be constructed recursivelyaccording to a tree structure (Dowty, 2007, i.a.
).TreeRNNs have shown promise (Tai et al, 2015;Li et al, 2015; Bowman et al, 2015b), but have1466bufferdownsatstackcatthecompositiontrackingtransitionreducedownsatthe catcompositiontrackingtransitionshiftdownsatthe cattracking(a) The SPINNmodel unrolled for two transitions during the processing of the sentence the cat sat down.
?Tracking?, ?transition?,and ?composition?
are neural network layers.
Gray arrows indicate connections which are blocked by a gating function.bufferstackt = 0downsatcattheshiftt = 1downsatcattheshiftt = 2downsatcatthereducet = 3downsatthe catshiftt = 4downsatthe catshiftt = 5downsatthe catreducet = 6sat downthe catreducet = 7 = T(the cat) (sat down)output to modelfor semantic task(b) The fully unrolled SPINN for the cat sat down, with neural network layers omitted for clarity.Figure 2: Two views of the Stack-augmented Parser-Interpreter Neural Network (SPINN).largely been overlooked in favor of sequence-based RNNs because of their incompatibility withbatched computation and their reliance on externalparsers.
Batched computation?performing syn-chronized computation across many examples atonce?yields order-of-magnitude improvements inmodel run time, and is crucial in enabling neuralnetworks to be trained efficiently on large datasets.Because TreeRNNs use a different model structurefor each sentence, as in Figure 1, efficient batchingis impossible in standard implementations.
Partlyto address efficiency problems, standard TreeRNNmodels commonly only operate on sentences thathave already been processed by a syntactic parser,which slows and complicates the use of these mod-els at test time for most applications.This paper introduces a new model to addressboth these issues: the Stack-augmented Parser-Interpreter Neural Network, or SPINN, shown inFigure 2.
SPINN executes the computations of atree-structured model in a linearized sequence, andcan incorporate a neural network parser that pro-duces the required parse structure on the fly.
Thisdesign improves upon the TreeRNN architecturein three ways: At test time, it can simultaneouslyparse and interpret unparsed sentences, removingthe dependence on an external parser at nearly noadditional computational cost.
Secondly, it sup-ports batched computation for both parsed and un-parsed sentences, yielding dramatic speedups overstandard TreeRNNs.
Finally, it supports a noveltree-sequence hybrid architecture for handling lo-cal linear context in sentence interpretation.
Thismodel is a basically plausible model of human sen-tence processing and yields substantial accuracygains over pure sequence- or tree-based models.We evaluate SPINNon the StanfordNatural Lan-guage Inference entailment task (SNLI, Bowmanet al, 2015a), and find that it significantly out-performs other sentence-encoding-based models,even with a relatively simple and underpoweredimplementation of the built-in parser.
We also findthat SPINN yields speed increases of up to 25?over a standard TreeRNN implementation.2 Related workThere is a fairly long history of work on buildingneural network-based parsers that use the core op-erations and data structures from transition-basedparsing, of which shift-reduce parsing is a variant(Henderson, 2004; Emami and Jelinek, 2005; Titovand Henderson, 2010; Chen and Manning, 2014;Buys and Blunsom, 2015; Dyer et al, 2015; Kiper-wasser and Goldberg, 2016).
In addition, there hasbeen recent work proposing models designed pri-marily for generative language modeling tasks thatuse this architecture as well (Zhang et al, 2016;Dyer et al, 2016).
To our knowledge, SPINN isthe first model to use this architecture for the pur-pose of sentence interpretation, rather than parsing1467or generation.Socher et al (2011a,b) present versions of theTreeRNN model which are capable of operatingover unparsed inputs.
However, these methods re-quire an expensive search process at test time.
Ourmodel presents a much faster alternative approach.3 Our model: SPINN3.1 Background: Shift-reduce parsingSPINN is inspired by shift-reduce parsing (Ahoand Ullman, 1972), which builds a tree structureover a sequence (e.g., a natural language sentence)by a single left-to-right scan over its tokens.
Theformalism is widely used in natural language pars-ing (e.g., Shieber, 1983; Nivre, 2003).A shift-reduce parser accepts a sequence ofinput tokens x = (x0, .
.
.
, xN?1) and consumestransitions a = (a0, .
.
.
, aT?1), where each at ?
{shift, reduce} specifies one step of the parsingprocess.
In general a parsermay also generate thesetransitions on the fly as it reads the tokens.
It pro-ceeds left-to-right through a transition sequence,combining the input tokens x incrementally intoa tree structure.
For any binary-branching treestructure over N words, this requires T = 2N ?
1transitions through a total of T + 1 states.The parser uses two auxiliary data structures:a stack S of partially completed subtrees and abuffer B of tokens yet to be parsed.
The parseris initialized with the stack empty and the buffercontaining the tokens x of the sentence in order.Let ?S, B?
= ?
?, x?
denote this starting state.
It nextproceeds through the transition sequence, whereeach transition at selects one of the two followingoperations.
Below, the | symbol denotes the cons(concatenation) operator.
We arbitrarily choose toalways cons on the left in the notation below.shift: ?S, x | B?
?
?x | S, B?.
This operationpops an element from the buffer and pushes iton to the top of the stack.reduce: ?x | y | S, B?
?
?
(x, y) | S, B?.
Thisoperation pops the top two elements from thestack, merges them, and pushes the result backon to the stack.3.2 Composition and representationSPINN is based on a shift-reduce parser, but it isdesigned to produce a vector representation of asentence as its output, rather than a tree as in stan-dard shift-reduce parsing.
It modifies the shift-reduce formalism by using fixed length vectors torepresent each entry in the stack and the buffer.Correspondingly, its reduce operation combinestwo vector representations from the stack into an-other vector using a neural network function.The composition function When a reduce op-eration is performed, the vector representations oftwo tree nodes are popped off of the stack and fedinto a composition function, which is a neural net-work function that produces a representation for anew tree node that is the parent of the two poppednodes.
This new node is pushed on to the stack.The TreeLSTM composition function (Tai et al,2015) generalizes the LSTM neural network layerto tree- rather than sequence-based inputs, and itshares with the LSTM the idea of representing in-termediate states as a pair of an active state repre-sentation~h and a memory representation ~c.
Ourversion is formulated as:????????????
?~i~f l~fr~o~g?????????????=?????????????????tanh?????????????*..,Wcomp????????~h1s~h2s~e???????
?+~bcomp+//-(1)~c = ~f l  ~c2s +~fr  ~c1s +~i  ~g(2)~h = ~o  ~c(3)where?
is the sigmoid activation function,  is theelementwise product, the pairs ?~h1s, ~c1s ?
and ?~h2s, ~c2s ?are the two input tree nodes popped off the stack,and ~e is an optional vector-valued input argumentwhich is either empty or comes from an externalsource like the tracking LSTM (see Section 3.3).The result of this function, the pair ?~h, ~c?, is placedback on the stack.
Each vector-valued variablelisted is of dimension D except ~e, of the indepen-dent dimension Dtracking.The stack and buffer The stack and the bufferare arrays of N elements each (for sentences of upto N words), with the two D-dimensional vectors~h and ~c in each element.Word representations We use word represen-tations based on the 300D vectors provided withGloVe (Pennington et al, 2014).
We do not updatethese representations during training.
Instead, weuse a learned linear transformation to map each in-put word vector ~xGloVeinto a vector pair ?~h, ~c?
thatis stored in the buffer:(4)[~h~c]= Wwd~xGloVe+~bwd14683.3 The tracking LSTMIn addition to the stack, buffer, and compositionfunction, our full model includes an additionalcomponent: the tracking LSTM.
This is a simplesequence-based LSTM RNN that operates in tan-dem with the model, taking inputs from the bufferand stack at each step.
It is meant to maintain alow-resolution summary of the portion of the sen-tence that has been processed so far, which is usedfor two purposes: it supplies feature representa-tions to the transition classifier, which allows themodel to stand alone as a parser, and it additionallysupplies a secondary input ~e to the compositionfunction?see (1)?allowing context informationto enter the construction of sentence meaning andforming what is effectively a tree-sequence hybridmodel.The tracking LSTM?s inputs (yellow in Figure 2)are the top element of the buffer~h1b (which wouldbe moved in a shift operation) and the top twoelements of the stack~h1s and~h2s (which would becomposed in a reduce operation).Why a tree-sequence hybrid?
Lexical ambigu-ity is ubiquitous in natural language.
Most wordshave multiple senses or meanings, and it is gener-ally necessary to use the context in which a wordoccurs to determine which of its senses or mean-ings is meant in a given sentence.
Even thoughTreeRNNs are more effective at composing mean-ings in principle, this ambiguity can give simplersequence-based sentence-encoding models an ad-vantage: when a sequence-based model first pro-cesses a word, it has direct access to a state vec-tor that summarizes the left context of that word,which acts as a cue for disambiguation.
In con-trast, when a standard tree-structured model firstprocesses a word, it only has access to the con-stituent that the word is merging with, which isoften just a single additional word.
Feeding a con-text representation from the tracking LSTM intothe composition function is a simple and efficientway tomitigate this disadvantage of tree-structuredmodels.
Using left linear context to disambiguateis also a plausible model of human interpretation.It would be straightforward to augment SPINNto support the use of some amount of right-sidecontext as well, but this would add complexity tothe model that we think is largely unnecessary:humans are very effective at understanding the be-ginnings of sentences before having seen or heardthe ends, suggesting that it is possible to get bywithout the unavailable right-side context.3.4 Parsing: Predicting transitionsFor SPINN to operate on unparsed inputs, it needsto produce its own transition sequence a ratherthan relying on an external parser to supply it aspart of the input.
To do this, the model predictsat at each step using a simple two-way softmaxclassifier whose input is the state of the trackingLSTM:(5) ~pa= softmax(Wtrans~htracking+~btrans)The above model is nearly the simplest viable im-plementation of a transition decision function.
Incontrast, the decision functions in state-of-the-arttransition-based parsers tend to use significantlyricher feature sets as inputs, including features con-taining information about several upcoming wordson the buffer.
The value~htrackingis a function ofonly the very top of the buffer and the top two stackelements at each timestep.At test time, the model uses whichever transi-tion (i.e., shift or reduce) is assigned a higher(unnormalized) probability.
The prediction func-tion is trained to mimic the decisions of an externalparser.
These decisions are used as inputs to themodel during training.
For SNLI, we use the bi-nary Stanford PCFGParser parses that are includedwith the corpus.
We did not find scheduled sam-pling (Bengio et al, 2015)?having the model useits own transition decisions sometimes at trainingtime?to help.3.5 Implementation issuesRepresenting the stack efficiently A na?ve im-plementation of SPINN needs to handle a sizeO(N ) stack at each timestep, any element of whichmay be involved in later computations.
A na?vebackpropagation implementation would then re-quire storing each of the O(N ) stacks for a back-ward pass, leading to a per-example space require-ment of O(NTD) floats.
This requirement is pro-hibitively large for significant batch sizes or sen-tence lengths N .
Such a na?ve implementationwould also require copying a largely unchangedstack at each timestep, since each shift or reduceoperation writes only one new representation to thetop of the stack.We propose a space-efficient stack representa-tion inspired by the zipper technique (Huet, 1997)that we call thin stack.
For each input sentence, we1469Algorithm 1 The thin stack algorithm1: function Step(bufferTop, a, t, S, Q)2: if a = shift then3: S[t] := bufferTop4: else if a = reduce then5: right := S[Q.pop()]6: left := S[Q.pop()]7: S[t] := Compose(left, right)8: Q.push(t)represent the stack with a single T ?
D matrix S.Each row S[t] (for 0 < t ?
T) represents the top ofthe actual stack at timestep t. At each timestep wecan shift a new element onto the stack, or reducethe top two elements of the stack into a single ele-ment.
To shift an element from the buffer to the topof the stack at timestep t, we simply write it intothe location S[t].
In order to perform the reduceoperation, we need to retrieve the top two elementsof the actual stack.
We maintain a queue Q ofpointers into S which contains the row indices ofS which are still present in the actual stack.
Thetop two elements of the stack can be found by us-ing the final two pointers in the queue Q. Theseretrieved elements are used to perform the reduceoperation, which modifies Q to mark that somerows of S have now been replaced in the actualstack.
Algorithm 1 describes the full mechanics ofa stack feedforward in this compressed representa-tion.
It operates on the single T ?
D matrix S anda backpointer queue Q.
Table 1 shows an examplerun.This stack representation requires substantiallyless space.
It stores each element involved in thefeedforward computation exactly once, meaningthat this representation can still support efficientbackpropagation.
Furthermore, all of the updatesto S and Q can be performed batched and in-placeon a GPU, yielding substantial speed gains overboth a more na?ve SPINN implementation and astandard TreeRNN implementation.
We describespeed results in Section 3.7.Preparing the data At training time, SPINN re-quires both a transition sequence a and a tokensequence x as its inputs for each sentence.
Thetoken sequence is simply the words in the sentencein order.
a can be obtained from any constituencyparse for the sentence by first converting that parseinto an unlabeled binary parse, then linearizing it(with the usual in-order traversal), then taking eacht S[t] Qt at0 shift1 Spot 1 shift2 sat 1 2 shift3 down 1 2 3 reduce4 (sat down) 1 4 reduce5 (Spot (sat down)) 5Table 1: The thin-stack algorithm operating onthe input sequence x = (Spot, sat, down) and thetransition sequence shown in the rightmost column.S[t] shows the top of the stack at each step t. Thelast two elements of Q (underlined) specify whichrows t would be involved in a reduce operation atthe next step.word token as a shift transition and each ?)?
as areduce transition, as here:Unlabeled binary parse: ( ( the cat ) ( sat down ) )x: the, cat, sat, downa: shift, shift, reduce, shift, shift, reduce, reduceHandling variable sentence lengths For anysentence model to be trained with batched com-putation, it is necessary to pad or crop sentences toa fixed length.
We fix this length at N = 25 words,longer than about 98% of sentences in SNLI.
Tran-sition sequences a are cropped at the left or paddedat the left with shifts.
Token sequences x are thencropped or padded with empty tokens at the leftto match the number of shifts added or removedfrom a, and can then be padded with empty tokensat the right to meet the desired length N .3.6 TreeRNN-equivalenceWithout the addition of the tracking LSTM, SPINN(in particular the SPINN-PI-NT variant, for parsedinput, no tracking) is precisely equivalent to a con-ventional tree-structured neural network model inthe function that it computes, and therefore it alsohas the same learning dynamics.
In both, the rep-resentation of each sentence consists of the repre-sentations of the words combined recursively usinga TreeRNN composition function (in our case, theTreeLSTM function).
SPINN, however, is dramat-ically faster, and supports both integrated parsingand a novel approach to context through the track-ing LSTM.147032 64 128 256 512 1024 20480510152025Batch sizeFeedforwardtime(sec)Thin-stack GPUCPU (Irsoy and Cardie, 2014)RNNFigure 3: Feedforward speed comparison.3.7 Inference speedIn this section, we compare the test-time speed ofour SPINN-PI-NT with an equivalent TreeRNNimplemented in the conventional fashion and witha standard RNN sequence model.
While thefull models evaluated below are implemented andtrained using Theano (Theano Development Team,2016), which is reasonably efficient but not perfectfor our model, we wish to compare well-optimizedimplementations of all three models.
To do this,we reimplement the feedforward1 of SPINN-PI-NTand an LSTM RNN baseline in C++/CUDA, andcompare that implementation with a CPU-basedC++/Eigen TreeRNN implementation from Irsoyand Cardie (2014), which we modified to performexactly the same computations as SPINN-PI-NT.2TreeRNNs like this can only operate on a singleexample at a time and are thus poorly suited forGPU computation.Each model is restricted to run on sentences of30 tokens or fewer.
We fix the model dimension Dand the word embedding dimension at 300.
We runthe CPU performance test on a 2.20 GHz 16-coreIntel Xeon E5-2660 processor with hyperthreadingenabled.
We test our thin-stack implementationand the RNN model on an NVIDIA Titan X GPU.Figure 3 compares the sentence encoding speedof the three models on random input data.
We ob-serve a substantial difference in runtime betweenthe CPU and thin-stack implementations that in-creases with batch size.
With a large but practical1We chose to reimplement and evaluate only the feedfor-ward/inference pass, as inference speed is the relevant perfor-mance metric for most practical applications.2The original code for Irsoy & Cardie?s model is avail-able at https://github.com/oir/deep-recursive.
Ouroptimized C++/CUDA models and the Theano source codefor the full SPINN are available at https://github.com/stanfordnlp/spinn.batch size of 512, the largest on which we testedthe TreeRNN, our model is about 25?
faster thanthe standard CPU implementation, and about 4?slower than the RNN baseline.Though this experiment only covers SPINN-PI-NT, the results should be similar for the fullSPINN model: most of the computation involvedin running SPINN is involved in populating thebuffer, applying the composition function, andmanipulating the buffer and the stack, with thelow-dimensional tracking and parsing componentsadding only a small additional load.4 NLI ExperimentsWe evaluate SPINN on the task of natural lan-guage inference (NLI, a.k.a.
recognizing textualentailment, or RTE; Dagan et al, 2006).
NLI is asentence pair classification task, in which a modelreads two sentences (a premise and a hypothesis),and outputs a judgment of entailment, contradic-tion, or neutral, reflecting the relationship betweenthe meanings of the two sentences.
Below is an ex-ample sentence pair and judgment from the SNLIcorpus which we use in our experiments:Premise: Girl in a red coat, blue head wrap and jeans ismaking a snow angel.Hypothesis: A girl outside plays in the snow.Label: entailmentSNLI is a corpus of 570k human-labeled pairs ofscene descriptions like this one.
We use the stan-dard train?test split and ignore unlabeled exam-ples, which leaves about 549k examples for train-ing, 9,842 for development, and 9,824 for testing.SNLI labels are roughly balanced, with the mostfrequent label, entailment, making up 34.2% of thetest set.Although NLI is framed as a simple three-wayclassification task, it is nonetheless an effectiveway of evaluating the ability of a model to ex-tract broadly informative representations of sen-tence meaning.
In order for a model to performreliably well on NLI, it must be able to representand reason with the core phenomena of natural lan-guage semantics, including quantification, corefer-ence, scope, and several types of ambiguity.4.1 Applying SPINN to SNLICreating a sentence-pair classifier To classifyan SNLI sentence pair, we run two copies of SPINNwith shared parameters: one on the premise sen-tence and another on the hypothesis sentence.
Wethen use their outputs (the~h states at the top of each1471Param.
Range Strategy RNN SP.-PI-NT SP.-PI SP.Initial LR 2 ?
10?4?2 ?
10?2log 5 ?
10?33 ?
10?47 ?
10?32 ?
10?3L2 regularization ?
8 ?
10?7?3 ?
10?5log 4 ?
10?63 ?
10?62 ?
10?53 ?
10?5Transition cost ?
0.5?4.0 lin ?
?
?
3.9Embedding transformation dropout 80?95% lin ?
83% 92% 86%Classifier MLP dropout 80?95% lin 94% 94% 93% 94%Tracking LSTM size Dtracking24?128 log ?
?
61 79Classifier MLP layers 1?3 lin 2 2 2 1Table 2: Hyperparameter ranges and values.
Range shows the hyperparameter ranges explored duringrandom search.
Strategy indicateswhether sampling from the rangewas uniform, or log-uniform.
Dropoutparameters are expressed as keep rates rather than drop rates.stack at time t = T) to construct a feature vector~xclassifierfor the pair.
This feature vector consistsof the concatenation of these two sentence vec-tors, their difference, and their elementwise prod-uct (following Mou et al, 2016):(6) ~xclassifier=???????????~hpremise~hhypothesis~hpremise?~hhypothesis~hpremise~hhypothesis??????????
?This feature vector is then passed to a series of1024D ReLU neural network layers (i.e., an MLP;the number of layers is tuned as a hyperparameter),then passed into a linear transformation, and thenfinally passed to a softmax layer, which yields adistribution over the three labels.The objective function Our objective combinesa cross-entropy objective Lsfor the SNLI classifi-cation task, cross-entropy objectivesLtpandLthforthe parsing decision for each of the two sentencesat each step t, and an L2 regularization term on thetrained parameters.
The terms are weighted usingthe tuned hyperparameters ?
and ?
:(7)Lm=Ls+ ?T?1?t=0(Ltp+ Lth) + ???
?22Initialization, optimization, and tuning Weinitialize the model parameters using the nonpara-metric strategy of He et al (2015), with the excep-tion of the softmax classifier parameters, whichwe initialize using random uniform samples from[?0.005, 0.005].We use minibatch SGD with the RMSProp op-timizer (Tieleman and Hinton, 2012) and a tunedstarting learning rate that decays by a factor of 0.75every 10k steps.
We apply both dropout (Srivas-tava et al, 2014) and batch normalization (Ioffeand Szegedy, 2015) to the output of the word em-bedding projection layer and to the feature vectorsthat serve as the inputs and outputs to the MLP thatprecedes the final entailment classifier.We train each model for 250k steps in each run,using a batch size of 32.
We track each model?sperformance on the development set during train-ing and save parameters when this performancereaches a new peak.
We use early stopping, eval-uating on the test set using the parameters thatperform best on the development set.We use random search to tune the hyperparame-ters of each model, setting the ranges for search foreach hyperparameter heuristically (and validatingthe reasonableness of the ranges on the develop-ment set), and then launching eight copies of eachexperiment each with newly sampled hyperparam-eters from those ranges.
Table 2 shows the hyper-parameters used in the best run of each model.4.2 Models evaluatedWe evaluate four models.
The four all use thesentence-pair classifier architecture described inSection 4.1, and differ only in the function com-puting the sentence encodings.
First, a single-layer LSTM RNN (similar to that of Bowmanet al, 2015a) serves as a baseline encoder.
Next,the minimal SPINN-PI-NT model (equivalent to aTreeLSTM) introduces the SPINN model design.SPINN-PI adds the tracking LSTM to that design.Finally, the full SPINN adds the integrated parser.We compare our models against several base-lines, including the strongest published non-neuralnetwork-based result from Bowman et al (2015a)and previous neural network models built aroundseveral types of sentence encoders.1472Model Params.
Trans.
acc.
(%) Train acc.
(%) Test acc.
(%)Previous non-NN resultsLexicalized classifier (Bowman et al, 2015a) ?
?
99.7 78.2Previous sentence encoder-based NN results100D LSTM encoders (Bowman et al, 2015a) 221k ?
84.8 77.61024D pretrained GRU encoders (Vendrov et al, 2016) 15m ?
98.8 81.4300D Tree-based CNN encoders (Mou et al, 2016) 3.5m ?
83.4 82.1Our results300D LSTM RNN encoders 3.0m ?
83.9 80.6300D SPINN-PI-NT (parsed input, no tracking) encoders 3.4m ?
84.4 80.9300D SPINN-PI (parsed input) encoders 3.7m ?
89.2 83.2300D SPINN (unparsed input) encoders 2.7m 92.4 87.2 82.6Table 3: Results on SNLI 3-way inference classification.
Params.
is the approximate number of trainedparameters (excluding word embeddings for all models).
Trans.
acc.
is the model?s accuracy in predictingparsing transitions at test time.
Train and test are SNLI classification accuracy.4.3 ResultsTable 3 shows our results on SNLI.
For the fullSPINN, we also report a measure of agreement be-tween this model?s parses and the parses includedwith SNLI, calculated as classification accuracyover transitions averaged across timesteps.We find that the bare SPINN-PI-NT model per-forms little better than the RNN baseline, but thatSPINN-PIwith the added tracking LSTMperformswell.
The success of SPINN-PI, which is the hy-brid tree-sequence model, suggests that the tree-and sequence-based encoding methods are at leastpartially complementary, with the sequence modelpresumably providing useful local word disam-biguation.
The full SPINN model with its rela-tively weak internal parser performs slightly lesswell, but nonetheless robustly exceeds the perfor-mance of the RNN baseline.Both SPINN-PI and the full SPINN significantlyoutperform all previous sentence-encoding mod-els.
Most notably, these models outperform thetree-based CNN of Mou et al (2016), which alsouses tree-structured composition for local featureextraction, but uses simpler pooling techniquesto build sentence features in the interest of effi-ciency.
Our results show that a model that usestree-structured composition fully (SPINN) outper-forms one which uses it only partially (tree-basedCNN), which in turn outperforms one which doesnot use it at all (RNN).The full SPINN performed moderately well atreproducing the Stanford Parser?s parses of theSNLI data at a transition-by-transition level, with92.4% accuracy at test time.3 However, its transi-3Note that this is scoring the model against automatiction prediction errors are fairly evenly distributedacross sentences, and most sentences were as-signed partially invalid transition sequences thateither left a few words out of the final representa-tion or incorporated a few padding tokens into thefinal representation.4.4 DiscussionThe use of tree structure improves the performanceof sentence-encodingmodels for SNLI.We suspectthat this improvement is largely due to the more ef-ficient learning of accurate generalizations overall,and not to any particular few phenomena.
How-ever, some patterns are identifiable in the results.While all four models under study have troublewith negation, the tree-structured SPINN modelsdo quite substantially better on these pairs.
Thisis likely due to the fact that parse trees make thescope of any instance of negation (the portion ofthe sentence?s content that is negated) relativelyeasy to identify and separate from the rest of thesentence.
For test set sentence pairs like the onebelow where negation (not or n?t) does not appearin the premise but does appear in the hypothesis,the RNN shows 67% accuracy, while all three tree-structured models exceed 73%.
Only the RNN gotthe below example wrong:Premise: The rhythmic gymnast completes her floor exer-cise at the competition.Hypothesis: The gymnast cannot finish her exercise.Label: contradictionNote that the presence of negation in the hypothesisis correlated with a label of contradiction in SNLI,but not as strongly as one might intuit?only 45%of these examples in the test set are labeled asparses, not a human-judged gold standard.1473contradictions.In addition, it seems that tree-structured mod-els, and especially the tree-sequence hybrid mod-els, are more effective than RNNs at extracting in-formative representations of long sentences.
TheRNN model falls off in test accuracy more quicklywith increasing sentence length than SPINN-PI-NT, which in turn falls of substantially faster thanthe two hybrid models, repeating a pattern seenmore dramatically on artificial data in Bowmanet al (2015b).
On pairs with premises of 20 ormore words, the RNN?s 76.7% accuracy, whileSPINN-PI reaches 80.2%.
All three SPINN mod-els labeled the following example correctly, whilethe RNN did not:Premise: A man wearing glasses and a ragged costumeis playing a Jaguar electric guitar and singing with theaccompaniment of a drummer.Hypothesis: A man with glasses and a disheveled outfit isplaying a guitar and singing along with a drummer.Label: entailmentWe suspect that the hybrid nature of the fullSPINN model is also responsible for its surpris-ing ability to perform better than an RNN baselineeven when its internal parser is relatively ineffec-tive at producing correct full-sentence parses.
Itmay act somewhat like the tree-based CNN, onlywith access to larger trees: using tree structure tobuild up local phrase meanings, and then using thetracking LSTM, at least in part, to combine thosemeanings.Finally, as is likely inevitable for models evalu-ated on SNLI, all four models under study did sev-eral percent worse on test examples whose groundtruth label is neutral than on examples of theother two classes.
Entailment?neutral and neu-tral?contradiction confusions appear to be muchharder to avoid than entailment?contradiction con-fusions, where relatively superficial cues might bemore readily useful.5 Conclusions and future workWe introduce amodel architecture (SPINN-PI-NT)that is equivalent to a TreeLSTM, but an order ofmagnitude faster at test time.
We expand that archi-tecture into a tree-sequence hybrid model (SPINN-PI), and show that this yields significant gains onthe SNLI entailment task.
Finally, we show thatit is possible to exploit the strengths of this modelwithout the need for an external parser by inte-grating a fast parser into the model (as in the fullSPINN), and that the lack of external parse infor-mation yields little loss in accuracy.Because this paper aims to introduce a generalpurpose model for sentence encoding, we do notpursue the use of soft attention (Bahdanau et al,2015; Rockt?schel et al, 2016), despite its demon-strated effectiveness on the SNLI task.4 However,we expect that it should be possible to produc-tively combine our model with soft attention toreach state-of-the-art performance.Our tracking LSTM uses only simple, quick-to-compute features drawn from the head of the bufferand the head of the stack.
It is plausible that giv-ing the tracking LSTM access to more informationfrom the buffer and stack at each step would allowit to better represent the context at each tree node,yielding both better parsing and better sentenceencoding.
One promising way to pursue this goalwould be to encode the full contents of the stackand buffer at each time step following the methodused by Dyer et al (2015).For a more ambitious goal, we expect thatit should be possible to implement a variant ofSPINN on top of a modified stack data structurewith differentiable push and pop operations (asin Grefenstette et al, 2015; Joulin and Mikolov,2015).
This would make it possible for the modelto learn to parse using guidance from the se-mantic representation objective, which currently isblocked from influencing the key parsing param-eters by our use of hard shift/reduce decisions.This change would allow the model to learn to pro-duce parses that are, in aggregate, better suited tosupporting semantic interpretation than those sup-plied in the training data.AcknowledgmentsWe acknowledge financial support from a GoogleFacultyResearchAward, the StanfordData ScienceInitiative, and the National Science Foundation un-der grant nos.
BCS 1456077 and IIS 1514268.Some of the Tesla K40s used for this researchwere donated by the NVIDIA Corporation.
Wealso thank Kelvin Guu, Noah Goodman, and manyothers in the Stanford NLP group for helpful com-ments.4Attention-based models like Rockt?schel et al (2016),Wang and Jiang (2016), and the unpublished Cheng et al(2016) have shown accuracies as high as 86.3% on SNLI, butare more narrowly engineered to suit the task and do not yieldsentence encodings.1474ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
Thetheory of parsing, translation, and compiling.Prentice-Hall, Inc.Dzmitry Bahdanau, Kyunghyun Cho, and YoshuaBengio.
2015.
Neural machine translation byjointly learning to align and translate.
In Pro-ceedings of the International Conference onLearning Representations (ICLR).
San Diego,CA.Samy Bengio, Oriol Vinyals, Navdeep Jaitly, andNoam Shazeer.
2015.
Scheduled sampling forsequence prediction with recurrent neural net-works.
In Advances in Neural Information Pro-cessing Systems (NIPS) 29.
Montr?al, Qu?bec,pages 1171?1179.Samuel R. Bowman, Gabor Angeli, ChristopherPotts, and Christopher D. Manning.
2015a.
Alarge annotated corpus for learning natural lan-guage inference.
In Proceedings of the 2015Conference on Empirical Methods in NaturalLanguage Processing.
Association for Compu-tational Linguistics, Lisbon, Portugal, pages632?642.Samuel R. Bowman, Christopher D. Manning,and Christopher Potts.
2015b.
Tree-structuredcomposition in neural networks without tree-structured architectures.
In Proceedings of the2015 NIPS Workshop on Cognitive Compu-tation: Integrating Neural and Symbolic Ap-proaches.
Montr?al, Qu?bec, pages 50?55.Jan Buys and Phil Blunsom.
2015.
Generative in-cremental dependency parsing with neural net-works.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Lin-guistics and the 7th International Joint Con-ference on Natural Language Processing (Vol-ume 2: Short Papers).
Association for Compu-tational Linguistics, Beijing, China, pages 863?869.Danqi Chen and Christopher Manning.
2014.
Afast and accurate dependency parser using neu-ral networks.
InProceedings of the 2014Confer-ence onEmpiricalMethods inNatural LanguageProcessing (EMNLP).
Association for Compu-tational Linguistics, Doha, Qatar, pages 740?750.Jianpeng Cheng, Li Dong, and Mirella Lapata.2016.
Long short-term memory-networks formachine reading.
arXiv:1601.06733.Ido Dagan, Oren Glickman, and BernardoMagnini.
2006.
The PASCAL recognising tex-tual entailment challenge.
In Machine learningchallenges.
Evaluating predictive uncertainty,visual object classification, and recognising tec-tual entailment, Springer, pages 177?190.David Dowty.
2007.
Compositionality as an em-pirical problem.
In Proceedings of the BrownUniversity Conference on Direct Composition-ality.
Oxford Univ.
Press.ChrisDyer,Miguel Ballesteros,WangLing, AustinMatthews, andNoahA.
Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
In Proceedings of the 53rd An-nual Meeting of the Association for Compu-tational Linguistics and the 7th InternationalJoint Conference on Natural Language Pro-cessing (Volume 1: Long Papers).
Associationfor Computational Linguistics, Beijing, China,pages 334?343.Chris Dyer, Adhiguna Kuncoro, Miguel Balles-teros, and Noah A. Smith.
2016.
Recurrent neu-ral network grammars.
In Proceedings of the2016Conference of the North AmericanChapterof the Association for Computational Linguis-tics: Human Language Technologies.
Associa-tion for Computational Linguistics, San Diego,California, pages 199?209.Ahmad Emami and Frederick Jelinek.
2005.
Aneural syntactic language model.
Machinelearning 60(1?3):195?227.Christoph Goller and Andreas K?chler.
1996.Learning task-dependent distributed represen-tations by backpropagation through structure.In Proceedings of the IEEE International Con-ference on Neural Networks.
Washington, DC,pages 347?352.Edward Grefenstette, Karl Moritz Hermann,Mustafa Suleyman, and Phil Blunsom.
2015.Learning to transduce with unbounded mem-ory.
In Advances in Neural Information Pro-cessing Systems (NIPS) 29.
Montr?al, Qu?bec,pages 1828?1836.Kaiming He, Xiangyu Zhang, Shaoqing Ren, andJian Sun.
2015.
Delving deep into rectifiers:Surpassing human-level performance on Image-Net classification.
InProceedings of the Interna-1475tional Conference on Computer Vision (ICCV).Santiago, Chile, pages 1026?1034.James Henderson.
2004.
Discriminative trainingof a neural network statistical parser.
In Pro-ceedings of the 42nd Meeting of the Associationfor Computational Linguistics (ACL?04), MainVolume.
Barcelona, Spain, pages 95?102.Sepp Hochreiter and J?rgen Schmidhuber.
1997.Long short-term memory.
Neural computation9(8):1735?1780.G?rard Huet.
1997.
The zipper.
Journal of func-tional programming 7(5):549?554.Sergey Ioffe and Christian Szegedy.
2015.
Batchnormalization: Accelerating deep network train-ing by reducing internal covariate shift.
In Pro-ceedings of the International Conference onMa-chine Learning (ICML).
Lille, France.Ozan Irsoy and Claire Cardie.
2014.
Deep recur-sive neural networks for compositionality in lan-guage.
In Z.Ghahramani,M.Welling, C.Cortes,N.
D. Lawrence, and K. Q. Weinberger, editors,Advances in Neural Information Processing Sys-tems (NIPS) 27, Curran Associates, Inc., pages2096?2104.Armand Joulin and Tomas Mikolov.
2015.
Infer-ring algorithmic patterns with stack-augmentedrecurrent nets.
In Advances in Neural Informa-tion Processing Systems (NIPS) 29.
Montr?al,Qu?bec.Nal Kalchbrenner, Edward Grefenstette, and PhilBlunsom.
2014.
A convolutional neural net-work for modelling sentences.
InProceedings ofthe 52nd Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Pa-pers).
Association for Computational Linguis-tics, Baltimore, Maryland, pages 655?665.Eliyahu Kiperwasser and Yoav Goldberg.
2016.Easy-first dependency parsing with hierarchicaltree LSTMs.
arXiv:1603.00375.Jiwei Li, Thang Luong, Dan Jurafsky, and EduardHovy.
2015.
When are tree structures neces-sary for deep learning of representations?
InProceedings of the 2015 Conference on Empir-ical Methods in Natural Language Processing.Association for Computational Linguistics, Lis-bon, Portugal, pages 2304?2314.Lili Mou, Men Rui, Ge Li, Yan Xu, Lu Zhang, RuiYan, and Zhi Jin.
2016.
Natural language in-ference by tree-based convolution and heuristicmatching.
In Proceedings of the ACL.
Berlin,Germany.Joakim Nivre.
2003.
An efficient algorithm forprojective dependency parsing.
In Proceedingsof the 8th International Workshop on ParsingTechnologies (IWPT).
pages 149?160.Jeffrey Pennington, Richard Socher, and Christo-pher Manning.
2014.
Glove: Global vectorsfor word representation.
In Proceedings of the2014 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP).
Associa-tion for Computational Linguistics, Doha, Qatar,pages 1532?1543.Tim Rockt?schel, Edward Grefenstette, KarlMoritz Hermann, Tom??
Ko?isk`y, and PhilBlunsom.
2016.
Reasoning about entailmentwith neural attention.
In Proceedings of the In-ternational Conference on Learning Represen-tations (ICLR).
San Juan, Puerto Rico.Stuart M. Shieber.
1983.
Sentence disambiguationby a shift-reduce parsing technique.
In Proceed-ings of the 21st Annual Meeting of the Asso-ciation for Computational Linguistics.
Associa-tion for Computational Linguistics, Cambridge,Massachusetts, USA, pages 113?118.Richard Socher, Cliff Chiung-Yu Lin, Andrew Ng,and Chris Manning.
2011a.
Parsing naturalscenes and natural language with recursive neu-ral networks.
In LiseGetoor andTobias Scheffer,editors, Proceedings of the 28th InternationalConference on Machine Learning (ICML-11).ACM, New York, NY, USA, ICML ?11, pages129?136.Richard Socher, Jeffrey Pennington, Eric H.Huang, AndrewY.Ng, and Christopher D.Man-ning.
2011b.
Semi-supervised recursive autoen-coders for predicting sentiment distributions.
InProceedings of the 2011 Conference on Empir-ical Methods in Natural Language Processing.Association for Computational Linguistics, Ed-inburgh, Scotland, UK., pages 151?161.Nitish Srivastava, Geoffrey Hinton, AlexKrizhevsky, Ilya Sutskever, and RuslanSalakhutdinov.
2014.
Dropout: A simple way toprevent neural networks from overfitting.
Jour-nal of Machine Learning Research 15:1929?1958.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.2014.
Sequence to sequence learning with neu-1476ral networks.
In Advances in Neural InformationProcessing Systems (NIPS) 28.Kai Sheng Tai, Richard Socher, and Christopher D.Manning.
2015.
Improved semantic representa-tions from tree-structured long short-termmem-ory networks.
InProceedings of the 53rd AnnualMeeting of the Association for ComputationalLinguistics and the 7th International Joint Con-ference on Natural Language Processing (Vol-ume 1: Long Papers).
Association for Computa-tional Linguistics, Beijing, China, pages 1556?1566.Theano Development Team.
2016.
Theano: APython framework for fast computation of math-ematical expressions.
arXiv:1605.02688.Tijmen Tieleman and Geoffrey Hinton.
2012.
Lec-ture 6.5 ?
RMSProp: Divide the gradient by arunning average of its recent magnitude.
InNeu-ral Networks for Machine Learning, Coursera.Ivan Titov and James Henderson.
2010.
A latentvariable model for generative dependency pars-ing.
In Harry Bunt, Paola Merlo, and JoakimNivre, editors, Trends in Parsing Technology,Springer, Netherlands, pages 35?55.Ivan Vendrov, Ryan Kiros, Sanja Fidler, andRaquel Urtasun.
2016.
Order-embeddings ofimages and language.
In Proceedings of the In-ternational Conference on Learning Represen-tations (ICLR).
San Juan, Puerto Rico.Shuohang Wang and Jing Jiang.
2016.
Learningnatural language inference with LSTM.
In Pro-ceedings of the 2016 Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics: Human Language Tech-nologies.
Association for Computational Lin-guistics, San Diego, California, pages 1442?1451.Xiang Zhang, Junbo Zhao, and Yann LeCun.
2015.Character-level convolutional networks for textclassification.
In Advances in Neural Informa-tion Processing Systems (NIPS) 29.
Montr?al,Qu?bec, pages 649?657.Xingxing Zhang, Liang Lu, and Mirella Lapata.2016.
Top-down tree long short-term memorynetworks.
In Proceedings of the 2016 Confer-ence of the North American Chapter of the Asso-ciation for Computational Linguistics: HumanLanguage Technologies.
Association for Com-putational Linguistics, San Diego, California,pages 310?320.1477
