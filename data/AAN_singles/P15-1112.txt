Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1159?1168,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Re-ranking Model for Dependency Parserwith Recursive Convolutional Neural NetworkChenxi Zhu, Xipeng Qiu?, Xinchi Chen, Xuanjing HuangShanghai Key Laboratory of Intelligent Information Processing, Fudan UniversitySchool of Computer Science, Fudan University825 Zhangheng Road, Shanghai, China{czhu13,xpqiu,xinchichen13,xjhuang}@fudan.edu.cnAbstractIn this work, we address the prob-lem to model all the nodes (words orphrases) in a dependency tree with thedense representations.
We propose arecursive convolutional neural network(RCNN) architecture to capture syntac-tic and compositional-semantic represen-tations of phrases and words in a depen-dency tree.
Different with the original re-cursive neural network, we introduce theconvolution and pooling layers, which canmodel a variety of compositions by thefeature maps and choose the most infor-mative compositions by the pooling lay-ers.
Based on RCNN, we use a discrimina-tive model to re-rank a k-best list of can-didate dependency parsing trees.
The ex-periments show that RCNN is very effec-tive to improve the state-of-the-art depen-dency parsing on both English and Chi-nese datasets.1 IntroductionFeature-based discriminative supervised modelshave achieved much progress in dependency pars-ing (Nivre, 2004; Yamada and Matsumoto, 2003;McDonald et al, 2005), which typically use mil-lions of discrete binary features generated from alimited size training data.
However, the ability ofthese models is restricted by the design of features.The number of features could be so large that theresult models are too complicated for practical useand prone to overfit on training corpus due to datasparseness.Recently, many methods are proposed to learnvarious distributed representations on both syn-tax and semantics levels.
These distributed repre-sentations have been extensively applied on many?Corresponding author.a,Det red,JJ bike,NNConvolutionPoolinga red bike,NNa bike,NN red bike,NNFigure 1: Illustration of a RCNN unit.natural language processing (NLP) tasks, such assyntax (Turian et al, 2010; Mikolov et al, 2010;Collobert et al, 2011; Chen and Manning, 2014)and semantics (Huang et al, 2012; Mikolov et al,2013).
Distributed representations are to representwords (or phrase) by the dense, low-dimensionaland real-valued vectors, which help address thecurse of dimensionality and have better general-ization than discrete representations.For dependency parsing, Chen et al (2014)and Bansal et al (2014) used the dense vectors(embeddings) to represent words or features andfound these representations are complementaryto the traditional discrete feature representation.However, these two methods only focus on thedense representations (embeddings) of words orfeatures.
These embeddings are pre-trained andkeep unchanged in the training phase of parsingmodel, which cannot be optimized for the specifictasks.Besides, it is also important to represent the(unseen) phrases with dense vector in dependencyparsing.
Since the dependency tree is also in re-cursive structure, it is intuitive to use the recur-sive neural network (RNN), which is used for con-stituent parsing (Socher et al, 2013a).
However,recursive neural network can only process the bi-nary combination and is not suitable for depen-dency parsing, since a parent node may have twoor more child nodes in dependency tree.In this work, we address the problem to rep-1159resent all level nodes (words or phrases) withdense representations in a dependency tree.
Wepropose a recursive convolutional neural net-work (RCNN) architecture to capture syntac-tic and compositional-semantic representations ofphrases and words.
RCNN is a general architec-ture and can deal with k-ary parsing tree, there-fore it is very suitable for dependency parsing.
Foreach node in a given dependency tree, we first usea RCNN unit to model the interactions between itand each of its children and choose the most infor-mative features by a pooling layer.
Thus, we canapply the RCNN unit recursively to get the vectorrepresentation of the whole dependency tree.
Theoutput of each RCNN unit is used as the input ofthe RCNN unit of its parent node, until it outputs asingle fixed-length vector at root node.
Figure 1 il-lustrates an example how a RCNN unit representsthe phrases ?a red bike?
as continuous vectors.The contributions of this paper can be summa-rized as follows.?
RCNN is a general architecture to model thedistributed representations of a phrase or sen-tence with its dependency tree.
AlthoughRCNN is just used for the re-ranking of thedependency parser in this paper, it can beregarded as semantic modelling of text se-quences and handle the input sequences ofvarying length into a fixed-length vector.
Theparameters in RCNN can be learned jointlywith some other NLP tasks, such as text clas-sification.?
Each RCNN unit can model the complicatedinteractions of the head word and its children.Combined with a specific task, RCNN cancapture the most useful semantic and struc-ture information by the convolution and pool-ing layers.?
When applied to the re-ranking model forparsing, RCNN improve the accuracy of baseparser to make accurate parsing decisions.The experiments on two benchmark datasetsshow that RCNN outperforms the state-of-the-art models.2 Recursive Neural NetworkIn this section, we briefly describe the recur-sive neural network architecture of (Socher et al,2013a).a,Det red,JJ bike,NNred bike,NPa red bike,NPFigure 2: Illustration of a RNN unit.The idea of recursive neural networks (RNN)for natural language processing (NLP) is to train adeep learning model that can be applied to phrasesand sentences, which have a grammatical structure(Pollack, 1990; Socher et al, 2013c).
RNN can bealso regarded as a general structure to model sen-tence.
At every node in the tree, the contexts at theleft and right children of the node are combinedby a classical layer.
The weights of the layer areshared across all nodes in the tree.
The layer com-puted at the top node gives a representation for thewhole sentence.Following the binary tree structure, RNN canassign a fixed-length vector to each word at theleaves of the tree, and combine word and phrasepairs recursively to create intermediate node vec-tors of the same length, eventually having one fi-nal vector representing the whole sentence.
Multi-ple recursive combination functions have been ex-plored, from linear transformation matrices to ten-sor products (Socher et al, 2013c).
Figure 2 illus-trates the architecture of RNN.The binary tree can be represented in the formof branching triplets (p?
c1c2).
Each such tripletdenotes that a parent node p has two children andeach ckcan be either a word or a non-terminalnode in the tree.Given a labeled binary parse tree,((p2?
ap1), (p1?
bc)), the node represen-tations are computed byp1= f(W[bc]),p2= f(W[ap1]), (1)where (p1,p2,a,b, c) are the vector representa-tion of (p1, p2, a, b, c) respectively, which are de-noted by lowercase bold font letters; W is a matrixof parameters of the RNN.Based on RNN, Socher et al (2013a) intro-duced a compositional vector grammar, whichuses the syntactically untied weights W to learnthe syntactic-semantic, compositional vector rep-resentations.
In order to compute the score of1160how plausible of a syntactic constituent a parentis, RNN uses a single-unit linear layer for all pi:s(pi) = v ?
pi, (2)where v is a vector of parameters that need to betrained.
This score will be used to find the high-est scoring tree.
For more details on how standardRNN can be used for parsing, see (Socher et al,2011).Costa et al (2003) applied recursive neural net-works to re-rank possible phrase attachments in anincremental constituency parser.
Their work is thefirst to show that RNNs can capture enough in-formation to make the correct parsing decisions.Menchetti et al (2005) used RNNs to re-rank dif-ferent constituency parses.
For their results on fullsentence parsing, they re-ranked candidate treescreated by the Collins parser (Collins, 2003).3 Recursive Convolutional NeuralNetworkThe dependency grammar is a widely used syntac-tic structure, which directly reflects relationshipsamong the words in a sentence.
In a dependencytree, all nodes are terminal (words) and each nodemay have more than two children.
Therefore, thestandard RNN architecture is not suitable for de-pendency grammar since it is based on the binarytree.
In this section, we propose a more generalarchitecture, called recursive convolutional neu-ral network (RCNN), which borrows the idea ofconvolutional neural network (CNN) and can dealwith to k-ary tree.3.1 RCNN UnitFor ease of exposition, we first describe the ba-sic unit of RCNN.
A RCNN unit is to model ahead word and its children.
Different from theconstituent tree, the dependency tree does not havenon-terminal nodes.
Each node consists of a wordand its POS tags.
Each node should have a differ-ent interaction with its head node.Word Embeddings Given a word dictionaryW ,each word w ?
W is represented as a real-valuedvector (word embedding) w ?
Rmwhere m is thedimensionality of the vector space.
The word em-beddings are then stacked into a embedding ma-trix M ?
Rm|W|.
For a word w ?
W , its cor-responding word embedding Embed(w) ?
Rmisretrieved by the lookup table layer.
The matrix Mxh =xKx2x1whd(h,ci)ConvolutionMax pooling?????????tanh??
?Phrase Representations of ChildrenWord EmbeddingDistance EmbeddingFigure 3: Architecture of a RCNN unit.is initialized with pre-training embeddings and up-dated by back-propagation.Distance Embeddings Besides word embed-dings, we also use distributed vector to representthe relative distance of a head word h and one ofits children c. For example, as shown in Figure 1,the relative distances of ?bike?
to ?a?
and ?red?
are-2 and -1, respectively.
The relative distances alsoare mapped to a vector of dimension md(a hy-perparameter); this vector is randomly initialized.Distance embedding is a usual way to encode thedistance information in neural model, which hasbeen proven effectively in several tasks.
Our ex-perimental results also show that the distance em-bedding gives more benefits than the traditionalrepresentation.
The relative distance can encodethe structure information of a subtree.Convolution The word and distance embed-dings are subsequently fed into the convolutioncomponent to model the interactions between twolinked nodes.Different with standard RNN, there are no non-terminal nodes in dependency tree.
Each node hin dependency tree has two associated distributedrepresentations:1. word embedding wh?
Rm, which is denotedas its own information according to its wordform;11612. phrase representation xh?
Rm, which is de-noted as the joint representation of the wholesubtree rooted at h. In particular, when h isleaf node, xh= wh.Given a subtree rooted at h in dependency tree,we define ci, 0 < i ?
L as the i-th child node ofh, where L represents the number of children.For each pair (h, ci), we use a convolutionalhidden layer to compute their combination repre-sentation zi.zi= tanh(W(h,ci)pi), 0 < i ?
K, (3)where W(h,ci)?
Rm?nis the linear compositionmatrix, which depends on the POS tags of h andci; pi?
Rnis the concatenated representation ofh and the i-th child, which consists of the headword embeddings wh, the child phrase represen-tation xciand the distance embeddings dh,ciof hand ci,pi= xh?
xci?
d(h,ci), (4)where ?
represents the concatenation operation.The distances dh,ciis the relative distance of hand ciin a given sentence.
Then, the relative dis-tances also are mapped to m-dimensional vectors.Different from constituent tree, the combinationshould consider the order or position of each childin dependency tree.In our model, we do not use the POS tags em-beddings directly.
Since the composition matrixvaries on the different pair of POS tags of h andci, it can capture the different syntactic combina-tions.
For example, the combination of adjectiveand noun should be different with that of verb andnoun.After the composition operations, we use tanhas the non-linear activation function to get a hid-den representation z.Max Pooling After convolution, we get Z(h)=[z1, z2, ?
?
?
, zK], where K is dynamic and de-pends on the number of children of h. To trans-form Z to a fixed length and determine the mostuseful semantic and structure information, we per-form a max pooling operation to Z on rows.x(h)j= maxiZ(h)j,i, 0 < j ?
m. (5)Thus, we obtain the vector representation xh?Rmof the whole subtree rooted at node h.Figure 3 shows the architecture of our proposedRCNN unit.rooteatsashimiI withchopsticksRCNN Unitw ( eat)w ( I ) w ( sashimi) x( with )x( eat)RCNN Unitx( root)w ( root)RCNN Unitw ( with )w ( chopstic ks )Figure 4: Example of a RCNN unitGiven a whole dependency tree, we can applythe RCNN unit recursively to get the vector rep-resentation of the whole sentence.
The output ofeach RCNN unit is used as the input of the RCNNunit of its parent node.Thus, RCNN can be used to model the dis-tributed representations of a phrase or sentencewith its dependency tree and applied to many NLPtasks.
The parameters in RCNN can be learnedjointly with the specific NLP tasks.
Each RCNNunit can model the complicated interactions of thehead word and its children.
Combined with a spe-cific task, RCNN can select the useful semanticand structure information by the convolution andmax pooling layers.Figure 4 shows an example of RCNN to modelthe sentence ?I eat sashimi with chopsitcks?.4 ParsingIn order to measure the plausibility of a subtreerooted at h in dependency tree, we use a single-unit linear layer neural network to compute thescore of its RCNN unit.For constituent parsing, the representation of anon-terminal node only depends on its two chil-dren.
The combination is relative simple and itscorrectness can be measured with the final repre-sentation of the non-terminal node (Socher et al,2013a).However for dependency parsing, all combina-tions of the head h and its children ci(0 < i ?
K)are important to measure the correctness of thesubtree.
Therefore, our score function s(h) iscomputed on all of hidden layers zi(0 < i ?
K):s(h) =K?i=1v(h,ci)?
zi, (6)where v(h,ci)?
Rm?1is the score vector, which1162also depends on the POS tags of h and ci.Given a sentence x and its dependency tree y,the goodness of a complete tree is measured bysumming the scores of all the RCNN units.s(x, y,?)
=?h?ys(h), (7)where h ?
y is the node in tree y; ?
={?W,?v,?w,?d} including the combinationmatrix set ?W, the score vector set ?v, the wordembeddings ?wand distance embeddings ?d.Finally, we can predict dependency tree y?
withhighest score for sentence x.y?
= arg maxy?gen(x)s(x, y,?
), (8)where gen(x) is defined as the set of all possibletrees for sentence x.
When applied in re-ranking,gen(x) is the set of the k-best outputs of a baseparser.5 TrainingFor a given training instance (xi, yi), we use themax-margin criterion to train our model.
We firstpredict the dependency tree y?iwith the highestscore for each xiand define a structured marginloss ?
(yi, y?i) between the predicted tree y?iandthe given correct tree yi.
?
(yi, y?i) is measuredby counting the number of nodes yiwith an incor-rect span (or label) in the proposed tree (Goodman,1998).?
(yi, y?i) =?d?y?i?1{d /?
yi} (9)where ?
is a discount parameter and d representsthe nodes in trees.Given a set of training dependency parses D,the final training objective is to minimize the lossfunction J(?
), plus a l2-regulation term:J(?)
=1|D|?(xi,yi)?Dri(?)
+?2??
?22, (10)whereri(?)
= maxy?i?Y (xi)( 0, st(xi, y?i,?
)+ ?
(yi, y?i)?
st(xi, yi,?)
) .
(11)By minimizing this object, the score of the cor-rect tree yiis increased and the score of the highestscoring incorrect tree y?iis decreased.We use a generalization of gradient descentcalled subgradient method (Ratliff et al, 2007)which computes a gradient-like direction.
Thesubgradient of equation is:?J??=1|D|?
(xi,yi)?D(?st(xi, y?i,?)???
?st(xi, yi,?)??)
+ ??.
(12)To minimize the objective, we use the diagonalvariant of AdaGrad (Duchi et al, 2011).
The pa-rameter update for the i-th parameter ?t,iat timestep t is as follows:?t,i= ?t?1,i???
?t?=1g2?,igt,i, (13)where ?
is the initial learning rate and g??
R|?i|is the subgradient at time step ?
for parameter ?i.6 Re-rankersRe-ranking k-best lists was introduced by Collinsand Koo (2005) and Charniak and Johnson (2005).They used discriminative methods to re-rank theconstituent parsing.
In the dependency parsing,Sangati et al (2009) used a third-order generativemodel for re-ranking k-best lists of base parser.Hayashi et al (2013) used a discriminative for-est re-ranking algorithm for dependency parsing.These re-ranking models achieved a substantialraise on the parsing performances.Given T (x), the set of k-best trees of a sentencex from a base parser, we use the popular mixturere-ranking strategy (Hayashi et al, 2013; Le andMikolov, 2014), which is a combination of the ourmodel and the base parser.y?i= arg maxy?T (xi)?st(xi, y,?)
+ (1?
?
)sb(xi, y)(14)where ?
?
[0, 1] is a hyperparameter; st(xi, y,?
)and sb(xi, y) are the scores given by RCNN andthe base parser respectively.To apply RCNN into re-ranking model, we firstget the k-best outputs of all sentences in trainset with a base parser.
Thus, we can train theRCNN in a discriminative way and optimize there-ranking strategy for a particular base parser.Note that the role of RCNN is not fully valuedwhen applied in re-ranking model since that thegen(x) in Eq.
(8) is just the k-best outputs of a base1163parser, not the set of all possible trees for sentencex.
The parameters of RCNN could overfit to k-best outputs of training set.7 Experiments7.1 DatasetsTo empirically demonstrate the effectiveness ofour approach, we use two datasets in different lan-guages (English and Chinese) in our experimen-tal evaluation and compare our model against theother state-of-the-art methods using the unlabeledattachment score (UAS) metric ignoring punctua-tion.English For English dataset, we follow the stan-dard splits of Penn Treebank (PTB), usingsections 2-21 for training, section 22 as de-velopment set and section 23 as test set.
Wetag the development and test sets using an au-tomatic POS tagger (at 97.2% accuracy), andtag the training set using four-way jackknif-ing similar to (Collins and Koo, 2005).Chinese For Chinese dataset, we follow the samesplit of the Penn Chinese Treeban (CTB5)as described in (Zhang and Clark, 2008) anduse sections 001-815, 1001-1136 as trainingset, sections 886-931, 1148- 1151 as devel-opment set, and sections 816-885, 1137-1147as test set.
Dependencies are converted by us-ing the Penn2Malt tool with the head-findingrules of (Zhang and Clark, 2008).
And fol-lowing (Zhang and Clark, 2008) (Zhang andNivre, 2011), we use gold segmentation andPOS tags for the input.We use the linear-time incremental parser(Huang and Sagae, 2010) as our base parser andcalculate the 64-best parses at the top cell of thechart.
Note that we optimize the training settingsfor base parser and the results are slightly im-proved on (Huang and Sagae, 2010).
Then we usemax-margin criterion to train RCNN.
Finally, weuse the mixture strategy to re-rank the top 64-bestparses.For initialization of parameters, we trainword2vec embeddings (Mikolov et al, 2013) onWikipedia corpus for English and Chinese respec-tively.
For the combination matrices and scorevectors, we use the random initialization within(0.01, 0.01).
The parameters which achieve thebest unlabeled attachment score on the develop-ment set will be chosen for the final evaluation.7.2 English DatasetWe first evaluate the performances of the RCNNand re-ranker (Eq.
(14)) on the development set.Figure 5 shows UASs of different models withvarying k. The base parser achieves 92.45%.When k = 64, the oracle best of base parserachieves 97.34%, while the oracle worst achieves73.30% (-19.15%) .
RCNN achieves the maxi-mum improvement of 93.00%(+0.55%) when k =6.
When k > 6, the performance of RCNN de-clines with the increase of k but is still higherthan baseline (92.45%).
The reason behind thisis that RCNN could require more negative sam-ples to avoid overfitting when k is large.
Since thenegative samples are limited in the k-best outputsof a base parser, the learnt parameters could easilyoverfits to the training set.The mixture re-ranker achieves the maximumimprovement of 93.50%(+1.05%) when k = 64.In mixture re-ranker, ?
is optimised by searchingwith the step-size 0.005.Therefore, we use the mixture re-ranker in thefollowing experiments since it can take the advan-tages of both the RCNN and base models.Figure 6 shows the accuracies on the top tenPOS tags of the modifier words with the largestimprovements.
We can see that our re-rankercan improve the accuracies of CC and IN, andtherefore may indirectly result in rising the thewell-known coordinating conjunction and PP-attachment problems.The final experimental results on test set areshown in Table 1.
The hyperparameters of ourmodel are set as in Table 2.
Our re-ranker achievesthe maximum improvement of 93.83%(+1.48%)on test set.
Our system performs slightly betterthan many state-of-the-art systems such as Zhangand Clark (2008) and Huang and Sagae (2010).It outperforms Hayashi et al (2013) and Le andZuidema (2014), which also use the mixture re-ranking strategy.Since the result of ranker is conditioned to k-best results of base parser, we also do an experi-ment to avoid this limitation by adding the oracleto k-best candidates.
With including oracle, there-ranker can achieve 94.16% on UAS, which isshown in the last line (?our re-ranker (with ora-cle)?)
of Table 1.11641 2 3 4 5 678 9 10 32 64929394959697kUAS(%)Oracle BestRCNNRe-ranker1 2 3 4 5 678 9 10 32 647580859095kUAS(%)Oracle BestRCNNRe-rankerOracle Worst(a) without the oracle worst result (b) with the oracle worst resultFigure 5: UAS with varying k on the development set.
Oracle best: always choosing the best result in thek-best of base parser; Oracle worst: always choosing the worst result in the k-best of base parser; RCNN:choosing the most probable candidate according to the score of RCNN; Re-ranker: a combination of theRCNN and base parser.WRB JJR WDT VBG VBP JJS RB CC MD IN7580859095UAS(%)Base Paser Re-rankerFigure 6: Accuracies on the top ten POS tags ofthe modifier words with the largest improvementson the development set.7.3 Chinese DatasetWe also make experiments on the Penn ChineseTreebank (CTB5).
The hyperparameters is thesame as the previous experiment on English exceptthat ?
is optimised by searching with the step-size0.005.The final experimental results on the test setare shown in Table 3.
Our re-ranker achieves theperformance of 85.71%(+0.25%) on the test set,which also outperforms the previous state-of-the-art methods.
With adding oracle, the re-ranker canachieve 87.43% on UAS, which is shown in thelast line (?our re-ranker (with oracle)?)
of Table 3.UASTraditional MethodsZhang and Clark (2008) 91.4Huang and Sagae (2010) 92.1Distributed RepresentationsStenetorp (2013) 86.25Chen et al (2014) 93.74Chen and Manning (2014) 92.0Re-rankersHayashi et al (2013) 93.12Le and Zuidema (2014) 93.12Our baseline 92.35Our re-ranker 93.83(+1.48)Our re-ranker (with oracle) 94.16Table 1: Accuracy on English test set.
Our base-line is the result of base parser; our re-ranker usesthe mixture strategy on the 64-best outputs of baseparser; our re-ranker(with oracle) is to add the or-acle to k-best outputs of base parser.Compared with the re-ranking model of Hayashi etal.
(2013), that use a large number of handcraftedfeatures, our model can achieve a competitive per-formance with the minimal feature engineering.7.4 DiscussionsThe performance of the re-ranking model is af-fected by the base parser.
The small divergence ofthe dependency trees in the output list also resultsto overfitting in training phase.
Although our re-1165Word embedding size m = 25Distance embedding size md= 25Initial learning rate ?
= 0.1Margin loss discount ?
= 2.0Regularization ?
= 10?4k-best k = 64Table 2: Hyperparameters of our modelUASTraditional MethodsZhang and Clark (2008) 84.33Huang and Sagae (2010) 85.20Distributed RepresentationsChen et al (2014) 82.94Chen and Manning (2014) 83.9Re-rankersHayashi et al (2013) 85.9Our baseline 85.46Our re-ranker 85.71(+0.25)Our re-ranker (with oracle) 87.43Table 3: Accuracy on Chinese test set.ranker outperforms the state-of-the-art methods, itcan also benefit from improving the quality of thecandidate results.
It was also reported in other re-ranking works that a larger k (eg.
k > 64) resultsthe worse performance.
We think the reason is thatthe oracle best increases when k is larger, but theoracle worst decrease with larger degree.
The er-ror types increase greatly.
The re-ranking modelrequires more negative samples to avoid overfit-ting.
When k is larger, the number of negativesamples also needs to multiply increase for train-ing.
However, we just can obtain at most k neg-ative samples from the k-best outputs of the baseparser.The experiments also show that the our modelcan achieves significant improvements by addingthe oracles into the output lists of the base parser.This indicates that our model can be boosted bya better set of the candidate results, which can beimplemented by combining the RCNN in the de-coding algorithm.8 Related WorkThere have been several works to use neural net-works and distributed representation for depen-dency parsing.
?a, Det red, JJ bike , NNa red bike , NNFigure 7: Example of a DT-RNN unitStenetorp (2013) attempted to build recursiveneural networks for transition-based dependencyparsing, however the empirical performance of hismodel is still unsatisfactory.
Chen and Manning(2014) improved the transition-based dependencyparsing by representing all words, POS tags andarc labels as dense vectors, and modeled their in-teractions with neural network to make predictionsof actions.
Their methods aim to transition-basedparsing and can not model the sentence in seman-tic vector space for other NLP tasks.Socher et al (2013b) proposed a composi-tional vectors computed by dependency tree RNN(DT-RNN) to map sentences and images into acommon embedding space.
However, there aretwo major differences as follows.
1) They firstsummed up all child nodes into a dense vector vcand then composed subtree representation from vcand vector parent node.
In contrast, our modelfirst combine the parent and each child and thenchoose the most informative features with a pool-ing layer.
2) We represent the relative positionof each child and its parent with distributed rep-resentation (position embeddings), which is veryuseful for convolutional layer.
Figure 7 shows anexample of DTRNN to illustrates how RCNN rep-resents phrases as continuous vectors.Specific to the re-ranking model, Le andZuidema (2014) proposed a generative re-rankingmodel with Inside-Outside Recursive Neural Net-work (IORNN), which can process trees bothbottom-up and top-down.
However, IORNNworks in generative way and just estimates theprobability of a given tree, so IORNN cannot fullyutilize the incorrect trees in k-best candidate re-sults.
Besides, IORNN treats dependency tree as asequence, which can be regarded as a generaliza-tion of simple recurrent neural network (SRNN)(Elman, 1990).
Unlike IORNN, our proposedRCNN is a discriminative model and can opti-mize the re-ranking strategy for a particular base1166parser.
Another difference is that RCNN computesthe score of tree in a recursive way, which is morenatural for the hierarchical structure of natural lan-guage.
Besides, the RCNN can not only be usedfor the re-ranking, but also be regarded as generalmodel to represent sentence with its dependencytree.9 ConclusionIn this work, we address the problem to rep-resent all level nodes (words or phrases) withdense representations in a dependency tree.
Wepropose a recursive convolutional neural net-work (RCNN) architecture to capture the syntac-tic and compositional-semantic representations ofphrases and words.
RCNN is a general architec-ture and can deal with k-ary parsing tree, there-fore RCNN is very suitable for many NLP tasksto minimize the effort in feature engineering witha external dependency parser.
Although RCNNis just used for the re-ranking of the dependencyparser in this paper, it can be regarded as seman-tic modelling of text sequences and handle the in-put sequences of varying length into a fixed-lengthvector.
The parameters in RCNN can be learnedjointly with some other NLP tasks, such as textclassification.For the future research, we will develop an inte-grated parser to combine RCNN with a decodingalgorithm.
We believe that the integrated parsercan achieve better performance without the limi-tation of base parser.
Moreover, we also wish toinvestigate the ability of our model for other NLPtasks.AcknowledgmentsWe would like to thank the anonymous review-ers for their valuable comments.
This workwas partially funded by the National Natural Sci-ence Foundation of China (61472088, 61473092),the National High Technology Research and De-velopment Program of China (2015AA015408),Shanghai Science and Technology DevelopmentFunds (14ZR1403200), Shanghai Leading Aca-demic Discipline Project (B114).ReferencesMohit Bansal, Kevin Gimpel, and Karen Livescu.2014.
Tailoring continuous word representations fordependency parsing.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics (ACL?05), pages 173?180, Ann Arbor, Michi-gan, June.
Association for Computational Linguis-tics.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 740?750.Wenliang Chen, Yue Zhang, and Min Zhang.
2014.Feature embedding for dependency parsing.
In Pro-ceedings of COLING 2014, the 25th InternationalConference on Computational Linguistics: Techni-cal Papers, pages 816?826, Dublin, Ireland, August.Dublin City University and Association for Compu-tational Linguistics.Michael Collins and Terry Koo.
2005.
Discriminativereranking for natural language parsing.
Computa-tional Linguistics, 31(1):25?70.Michael Collins.
2003.
Head-driven statistical mod-els for natural language parsing.
Computational lin-guistics, 29(4):589?637.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,and Giovanni Soda.
2003.
Towards incrementalparsing of natural language using recursive neuralnetworks.
Applied Intelligence, 19(1-2):9?25.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Jeffrey L Elman.
1990.
Finding structure in time.Cognitive science, 14(2):179?211.Joshua Goodman.
1998.
Parsing inside-out.
arXivpreprint cmp-lg/9805007.Katsuhiko Hayashi, Shuhei Kondo, and Yuji Mat-sumoto.
2013.
Efficient stacked dependency pars-ing by forest reranking.
TACL, 1:139?150.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1077?1086.
Association for Computational Linguistics.1167Eric Huang, Richard Socher, Christopher Manning,and Andrew Ng.
2012.
Improving word represen-tations via global context and multiple word proto-types.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 873?882, Jeju Island,Korea, July.
Association for Computational Linguis-tics.Quoc V. Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
In Pro-ceedings of ICML.Phong Le and Willem Zuidema.
2014.
The inside-outside recursive neural network model for depen-dency parsing.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 729?739, Doha, Qatar,October.
Association for Computational Linguistics.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, ACL ?05, pages 91?98.Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, andMassimiliano Pontil.
2005.
Wide coverage naturallanguage processing using kernel methods and neu-ral networks for structured data.
Pattern Recogni-tion Letters, 26(12):1896?1906.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems.Joakim Nivre.
2004.
Incrementality in deterministicdependency parsing.
In Proceedings of the Work-shop on Incremental Parsing: Bringing Engineeringand Cognition Together, pages 50?57.
Associationfor Computational Linguistics.Jordan B Pollack.
1990.
Recursive distributed repre-sentations.
Artificial Intelligence, 46(1):77?105.Nathan D Ratliff, J Andrew Bagnell, and Martin AZinkevich.
2007.
(online) subgradient methodsfor structured prediction.
In Eleventh InternationalConference on Artificial Intelligence and Statistics(AIStats).Federico Sangati, Willem Zuidema, and Rens Bod.2009.
A generative re-ranking model for depen-dency parsing.
In Proceedings of the 11th Interna-tional Conference on Parsing Technologies, pages238?241.
Association for Computational Linguis-tics.Richard Socher, Cliff C Lin, Chris Manning, and An-drew Y Ng.
2011.
Parsing natural scenes and nat-ural language with recursive neural networks.
InProceedings of the 28th International Conference onMachine Learning (ICML-11), pages 129?136.Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013a.
Parsing with compo-sitional vector grammars.
In In Proceedings of theACL conference.
Citeseer.Richard Socher, Q Le, C Manning, and A Ng.
2013b.Grounded compositional semantics for finding anddescribing images with sentences.
In NIPS DeepLearning Workshop.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts.
2013c.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In EMNLP.Pontus Stenetorp.
2013.
Transition-based dependencyparsing using recursive neural networks.
In NIPSWorkshop on Deep Learning.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In ACL.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
InProceedings of the International Workshop on Pars-ing Technologies (IWPT), volume 3.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based and transition-based dependency parsing us-ing beam-search.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 562?571.
Association for Computa-tional Linguistics.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers-Volume 2, pages188?193.
Association for Computational Linguis-tics.1168
