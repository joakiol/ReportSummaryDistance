Proceedings of NAACL HLT 2007, pages 204?211,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsData-Driven Graph Construction for Semi-Supervised Graph-BasedLearning in NLPAndrei AlexandrescuDept.
of Computer Science and EngineeringUniversity of WashingtonSeattle, WA, 98195andrei@cs.washington.eduKatrin KirchhoffDept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195katrin@ee.washington.eduAbstractGraph-based semi-supervised learning hasrecently emerged as a promising approachto data-sparse learning problems in natu-ral language processing.
All graph-basedalgorithms rely on a graph that jointly rep-resents labeled and unlabeled data points.The problem of how to best construct thisgraph remains largely unsolved.
In thispaper we introduce a data-driven methodthat optimizes the representation of theinitial feature space for graph construc-tion by means of a supervised classifier.We apply this technique in the frame-work of label propagation and evaluateit on two different classification tasks, amulti-class lexicon acquisition task and aword sense disambiguation task.
Signifi-cant improvements are demonstrated overboth label propagation using conventionalgraph construction and state-of-the-art su-pervised classifiers.1 IntroductionNatural Language Processing (NLP) applicationsbenefit from the availability of large amounts of an-notated data.
However, such data is often scarce,particularly for non-mainstream languages.
Semi-supervised learning addresses this problem by com-bining large amounts of unlabeled data with a smallset of labeled data in order to learn a classifica-tion function.
One class of semi-supervised learn-ing algorithms that has recently attracted increasedinterest is graph-based learning.
Graph-based tech-niques represent labeled and unlabeled data pointsas nodes in a graph with weighted edges encodingthe similarity of pairs of samples.
Various tech-niques are then available for transferring class la-bels from the labeled to the unlabeled data points.These approaches have shown good performance incases where the data is characterized by an underly-ing manifold structure and samples are judged to besimilar by local similarity measures.
However, thequestion of how to best construct the graph formingthe basis of the learning procedure is still an under-investigated research problem.
NLP learning taskspresent additional problems since they often rely ondiscrete or heterogeneous feature spaces for whichstandard similarity measures (such as Euclidean orcosine distance) are suboptimal.We propose a two-pass data-driven technique forgraph construction in the framework of label propa-gation (Zhu, 2005).
First, we use a supervised clas-sifier trained on the labeled subset to transform theinitial feature space (consisting of e.g.
lexical, con-textual, or syntactic features) into a continuous rep-resentation in the form of soft label predictions.
Thisrepresentation is then used as a basis for measur-ing similarity among samples that determines thestructure of the graph used for the second, semi-supervised learning step.
It is important to note that,rather than simply cascading the supervised and thesemi-supervised learner, we optimize the combina-tion with respect to the properties required of thegraph.
We present several techniques for such op-timization, including regularization of the first-passclassifier, biasing by class priors, and linear combi-204nation of classifier predictions with known features.The proposed approach is evaluated on a lexiconlearning task using the Wall Street Journal (WSJ)corpus, and on the SENSEVAL-3 word sense dis-ambiguation task.
In both cases our technique sig-nificantly outperforms our baseline systems (labelpropagation using standard graph construction anddiscriminatively trained supervised classifiers).2 BackgroundSeveral graph-based learning techniques have re-cently been developed and applied to NLP prob-lems: minimum cuts (Pang and Lee, 2004), randomwalks (Mihalcea, 2005; Otterbacher et al, 2005),graph matching (Haghighi et al, 2005), and labelpropagation (Niu et al, 2005).
Here we focus onlabel propagation as a learning technique.2.1 Label propagationThe basic label propagation (LP) algorithm (Zhu andGhahramani, 2002; Zhu, 2005) has as inputs:?
a labeled set {(x1, y1), (x2, y2), .
.
.
, (xn, yn)},where xi are samples (feature vectors) and yi ?
{1, 2, .
.
.
, C} are their corresponding labels;?
an unlabeled set {xn+1, .
.
.
, xN};?
a distance measure d(i, j) i, j ?
{1, .
.
.
N} de-fined on the feature space.The goal is to infer the labels {yn+1, .
.
.
, yN} forthe unlabeled set.
The algorithm represents all Ndata points as vertices in an undirected graph withweighted edges.
Initially, only the known data ver-tices are labeled.
The edge linking vertices i and jhas weight:wij = exp(?d(i, j)2?2)(1)where ?
is a hyperparameter that needs to be empir-ically chosen or learned separately.
wij indicates thelabel affinity of vertices: the larger wij is, the morelikely it is that i and j have the same label.
The LPalgorithm constructs a row-normalized N ?N tran-sition probability matrix P as follows:Pij = P (i?
j) =wij?Nk=1 wik(2)The algorithm probabilistically pushes labels fromthe labeled nodes to the unlabeled nodes.
To do so, itdefines the n?C hard labels matrix Y and the N?Csoft labels matrix f , whose first n rows are identicalto Y .
The hard labels matrix Y is invariant throughthe algorithm and is initialized with probability 1 forthe known label and 0 for all other labels:Yic = ?
(yi, C) (3)where ?
is Kronecker?s delta function.
The algo-rithm iterates as follows:1. f ?
?
P ?
f2.
f ?
[rows 1 to n] ?
Y3.
If f ?
?= f , stop4.
f ?
f ?5.
Repeat from step 1In each iteration, step 2 fixes the known labels,which might otherwise be overriden by propagatedlabels.
The resulting labels for each feature xi,where i ?
{n + 1, .
.
.
, N}, are:li = arg maxj=1,...,Cfij (4)It is important that the distance measure is locallyaccurate, i.e.
nodes connected by an edge with ahigh weight should have the same label.
The globaldistance is less relevant since label information willbe propagated from labeled points through the entirespace.
This is why LP works well with a local dis-tance measure that might be unsuitable as a globaldistance measure.Applications of LP include handwriting recogni-tion (Zhu and Ghahramani, 2002), image classifi-cation (Balcan et al, 2005) and retrieval (Qin etal., 2005), and protein classification (Weston et al,2003).
In NLP, label propagation has been used forword sense disambiguation (Niu et al, 2005), doc-ument classification (Zhu, 2005), sentiment analy-sis (Goldberg and Zhu, 2006), and relation extrac-tion (Chen et al, 2006).2.2 Graph constructionOne of the main problems in LP, as well as othergraph-based learning techniques, is how to best con-struct the graph.
Currently, graph construction ?ismore of an art than science?
(Zhu, 2005).
Typically,edge weights are derived from a simple Euclideanor cosine distance measure, regardless of the natureof the underlying features.
Edges are then estab-lished either by connecting all nodes, by applyinga single global threshold to the edge weights, or byconnecting each node to its k nearest neighbors ac-cording to the edge weights.
This procedure is oftensuboptimal: Euclidean distance relies on a model ofnormally distributed i.i.d.
random variables; cosine205distance likewise assumes that the different featurevector dimensions are uncorrelated.
However, manyapplications, particularly in NLP, rely on featurespaces with correlated dimensions.
Moreover, fea-tures may have different ranges and different types(e.g.
continuous, binary, multi-valued), which en-tails the need for normalization, binning, or scaling.Finally, common distance measures do not take ad-vantage of domain knowledge that might be avail-able.Some attempts have been made at improving thestandard method of graph construction.
For in-stance, in a face identification task (Balcan et al,2005), domain knowledge was used to identify threedifferent edge sets based on time, color and facefeatures, associating a different hyperparameter witheach.
The resulting graph was then created by super-posing edge sets.
Zhu (Zhu, 2005, Ch.
7) describesgraph construction using separate ?
hyperparame-ters for each feature dimension, and presents a data-driven way (evidence maximization) for learning thevalues of the parameters.3 Data-driven graph constructionUnlike previous work, we propose to optimize thefeature representation used for graph constructionby learning it with a first-pass supervised classi-fier.
Under this approach, similarity of samples isdefined as similarity of the output values producedby a classifier applied to the original feature repre-sentation of the samples.
This idea bears similar-ity to classifier cascading (Alpaydin and Kaynak,1998), where classifiers are trained around a rule-exceptions paradigm; however, in our case, the clas-sifiers work together, the first acting as a jointly op-timized feature mapping function for the second.1.
Train a first-pass supervised classifier that out-puts soft label predictions Zi for all sam-ples i ?
{1, .
.
.
N}, e.g.
a posterior prob-ability distribution over target labels: Zi =?pi1, pi2, .
.
.
, piC?;2.
Apply postprocessing to Zi if needed.3.
Use vectors Zi and an appropriately chosen dis-tance measure to construct a graph for LP.4.
Perform label propagation over the constructedgraph to find the labeling of the test samples.The advantages of this procedure are:?
Uniform range and type of features: The out-put from a first-pass classifier can produce well-defined features, e.g.
posterior probability distribu-tions.
This eliminates the problem of input featuresof different ranges and types (e.g.
binary vs. multi-valued, continuous vs. categorical attributes) whichare often used in combination.?
Feature postprocessing: The transformation offeatures into a different space also opens up pos-sibilities for postprocessing (e.g.
probability distri-bution warping) depending on the requirements ofthe second-pass learner.
In addition, different dis-tance functions (e.g.
those defined on probabilityspaces) can be used, which avoids violating assump-tions made by metrics such as Euclidean and cosinedistance.?
Optimizing class separation: The learned repre-sentation of labeled training samples might revealbetter clusters in the data than the original represen-tation: a discriminatively-trained first pass classifierwill attempt to maximize the separation of samplesbelonging to different classes.
Moreover, the first-pass classifier may learn a feature transformationthat suppresses noise in the original input space.Difficulties with the proposed approach might arisewhen the first-pass classifier yields confident butwrong predictions, especially for outlier samples inthe original space.
For this reason, the first-passclassifier and the graph-based learner should notsimply be concatenated without modification, butthe first classifier should be optimized with respectto the requirements of the second.
In our case, thechoice of first-pass classifier and joint optimizationtechniques are determined by the particular learningtask and are detailed below.4 Tasks4.1 Lexicon acquisition taskOur first task is a part-of-speech (POS) lexicon ac-quisition task, i.e.
the labels to be predicted are thesets of POS tags associated with each word in a lex-icon.
Note that this is not a tagging task: we are notattempting to identify the correct POS of each wordin running text.
Rather, for each word in the vocab-ulary, we attempt to infer the set of possible POStags.
Our choice of this task is motivated by ourlong-term goal of applying this technique to lexiconacquisition for resource-poor languages: POS lexi-206cons are one of the most basic language resources,which enable subsequent training of taggers, chun-kers, etc.
We assume that a small set of words can bereliably annotated, and that POS-sets for the remain-ing words can be inferred by semi-supervised learn-ing.
Rather than choosing a genuinely resource-poorlanguage for this task, we use the English Wall StreetJournal (WSJ) corpus and artificially limit the sizeof the labeled set.
This is because the WSJ corpus iswidely obtainable and allows easy replication of ourexperiments.We use sections 0-18 of the Wall Street Journalcorpus (N = 44, 492).
Words have between 1 and4 POS tags, with an average of 1.1 per word.
Thenumber of POS tags is 36, and we treat every POScombination as a unique class, resulting in C = 158distinct labels.
We use three different randomly se-lected training sets of various sizes: 5000, 10000,and 15000 words, representing about 11%, 22%, and34% of the entire data set respectively; the rest of thedata was used for testing.
In order to avoid experi-mental bias, we run all experiments on five differ-ent randomly chosen labeled subsets and report av-erages and standard deviations.
Due to the randomsampling of the data it is possible that some labelsnever occur in the training set or only occur once.We train our classifiers only on those labels that oc-cur at least twice, which results in 60-63 classes.
La-bels not present in the training set will therefore notbe hypothesized and are guaranteed to be errors.
Wedelete samples with unknown labels from our unla-beled set since their percentage is less than 0.5% onaverage.We use the following features to represent sam-ples:?
Integer: the three-letter suffix of the word;?
Integer: The four-letter suffix of the word;?
Integer ?
4: The indices of the four most fre-quent words that immediately precede the wordin the WSJ text;?
Boolean: word contains capital letters;?
Boolean: word consists only of capital letters;?
Boolean: word contains digits;?
Boolean: word contains a hyphen;?
Boolean: word contains other special charac-ters (e.g.
?&?
).We have also experimented with shorter suffixes andwith prefixes but those features tended to degradeperformance.4.2 SENSEVAL-3 word sense disambiguationtaskThe second task is word sense disambiguation usingthe SENSEVAL-3 corpus (Mihalcea et al, 2004), toenable a comparison of our method with previouslypublished results.
The goal is to disambiguate thedifferent senses of each of 57 words given the sen-tences within which they occur.
There are 7860 sam-ples for training and 3944 for testing.
In line withexisting work (Lee and Ng, 2002; Niu et al, 2005),we use the following features:?
Integer ?
7: seven features consisting of thePOS of the previous three words, the POS ofthe next three words, and the POS of the worditself.
We used the MXPOST tagger (Ratna-parkhi, 1996) for POS annotation.?
Integer?
?variable length?
: a bag of all wordsin the surrounding context.?
Integer ?
15: Local collocations Cij (i, j arethe bounds of the collocation window)?wordcombinations from the context of the word todisambiguate.
In addition to the 11 collocationsused in similar work (Lee and Ng, 2002), wealso used C?3,1, C?3,2, C?2,3, C?1,3.Note that syntactic features, which have been used insome previous studies on this dataset (Mohammadand Pedersen, 2004), were not included.
We apply asimple feature selection method: a feature X is se-lected if the conditional entropy H(Y |X) is abovea fixed threshold (1 bit) in the training set, and if Xalso occurs in the test set (note that no label infor-mation from the test data is used for this purpose).5 ExperimentsFor both tasks we compare the performance of a su-pervised classifier, label propagation using the stan-dard input features and either Euclidean or cosinedistance, and LP using the output from a first-passsupervised classifier.5.1 Lexicon acquisition task5.1.1 First-pass classifierFor this task, the first-pass classifier is a multi-layer perceptron (MLP) with the topology shownin Fig.
1.
The input features are mapped to con-207x 2x 4x 1x 3P(y | x)MihoWih WhoAFigure 1: Architecture of first-pass supervised classifier (MLP)for lexicon acquisition.tinuous values by a discrete-to-continuous mappinglayer M , which is itself learned during the MLPtraining process.
This layer connects to the hiddenlayer h, which in turn is connected to the outputlayer o.
The entire network is trained via backprop-agation.
The training criterion maximizes the regu-larized log-likelihood of the training data:L = 1nn?t=1log P (yt|xt, ?)
+ R(?)
(5)The use of an additional continuous mapping layeris similar to the use of hidden continuous word rep-resentations in neural language modeling (Bengio etal., 2000) and yields better results than a standard3-layer MLP topology.Problems caused by data scarcity arise when someof the input features of the unlabeled words havenever been seen in the training set, resulting in un-trained, randomly-initialized values for those fea-ture vector components.
We address this problemby creating an approximation layer A that finds theknown input feature vector x?
that is most similarto x (by measuring the cosine similarity betweenthe vectors).
Then xk is replaced with x?k, resultingin vector x?
= ?x1, .
.
.
, xk?1, x?k, xk+1, .
.
.
, xf ?
thathas no unseen features and is closest to the originalvector.5.1.2 LP SetupWe use a dense graph approach.
The WSJ sethas a total of 44,492 words, therefore the P ma-trix that the algorithm requires would have 44, 492?44, 492 ?= 2?
109 elements.
Due to the matrix size,we avoid the analytical solution of the LP problem,which requires inverting the P matrix, and choosethe iterative approach described above (Sec.
2.1) in-stead.
Convergence is stopped when the maximumrelative difference between each cell of f and thecorresponding cell of f ?
is less than 1%.Also for data size reasons, we apply LP in chunks.While the training set stays in memory, the testdata is loaded in fixed-size chunks, labeled, and dis-carded.
This approach has yielded similar resultsfor various chunk sizes, suggesting that chunking isa good approximation of whole-set label propaga-tion.1 LP in chunks is also amenable to paralleliza-tion: Our system labels different chunks in parallel.We trained the ?
hyperparameter by three-foldcross-validation on the training data, using a geo-metric progression with limits 0.1 and 10 and ratio2.
We set fixed upper limits of edges between anunlabeled node and its labeled neighbors to 15, andbetween an unlabeled node and its unlabeled neigh-bors to 5.
The approach of setting different limitsamong different kinds of nodes is also used in re-lated work (Goldberg and Zhu, 2006).For graph construction we tested: (a) the originaldiscrete input representation with cosine distance;(b) the classifier output features (probability distri-butions) with the Jeffries-Matusita distance.5.2 Combination optimizationThe static parameters of the MLP (learning rate, reg-ularization rate, and number of hidden units) wereoptimized for the LP step by 5-fold cross-validationon the training data.
This process is important be-cause overspecialization is detrimental to the com-bined system: an overspecialized first-pass classi-fier may output very confident but wrong predic-tions for unseen patterns, thus placing such samplesat large distances from all correctly labeled sam-ples.
A strongly regularized neural network, by con-trast, will output smoother probability distributionsfor unseen patterns.
Such outputs also result in asmoother graph, which in turn helps the LP process.Thus, we found that a network with only 12 hiddenunits and relatively high R(?)
in Eq.
5 (10% of theweight value) performed best in combination withLP (at an insignificant cost in accuracy when used1In fact, experiments have shown that performance tends todegrade for larger chunk sizes, suggesting that whole-set LPmight be affected by ?artifact?
clusters that are not related tothe labels.208as an isolated classifier).5.2.1 ResultsWe first conducted an experiment to measure thesmoothness of the underlying graph, S(G), in thetwo LP experiments according to the following for-mula: S(G) =?yi 6=yj ,(i>n?j>n)wij (6)where yi is the label of sample i.
(Lower values arebetter as they reflect less affinity between nodes ofdifferent labels.)
The value of S(G) was in all casessignificantly better on graphs constructed with ourproposed technique than on graphs constructed inthe standard way (see Table 1).
Table 1 also showsthe performance comparison between LP over thediscrete representation and cosine distance (?LP?
),the neural network itself (?NN?
), and LP over thecontinuous representation (?NN+LP?
), on all dif-ferent subsets and for different training sizes.
Forscarce labeled data (5000 samples) the neural net-work, which uses a strictly supervised training pro-cedure, is at a clear disadvantage.
However, for alarger training set the network is able to performmore accurately than the LP learner that uses thediscrete features directly.
The third, combined tech-nique outperforms the first two significantly.2 Thedifferences are more pronounced for smaller train-ing set sizes.
Interestingly, the LP is able to extractinformation from largely erroneous (noisy) distribu-tions learned by the neural network.5.3 Word Sense DisambiguationWe compare the performance of an SVM classifier,an LP learner using the same input features as theSVM, and an LP learner using the SVM outputs asinput features.
To analyze the influence of train-ing set size on accuracy, we randomly sample sub-sets of the training data (25%, 50%, and 75%) anduse the remaining training data plus the test dataas unlabeled data, similarly to the procedure fol-lowed in related work (Niu et al, 2005).
The re-sults are averaged over five different random sam-plings.
The samplings were chosen such that therewas at least one sample for each label in the trainingset.
SENSEVAL-3 sports multi-labeled samples and2Significance was tested using a difference of proportionssignificance test; the significance level is 0.01 or smaller in allcases.samples with the ?unknown?
label.
We eliminate allsamples labeled as unknown and retain only the firstlabel for the multi-labeled instances.5.3.1 SVM setupThe use of SVM vs. MLP in this case was justi-fied by the very small training data set.
An MLP hasmany parameters and needs a considerable amountof data for effective training, so for this task withonly on the order of 102 training samples per classi-fier, an SVM was deemed more appropriate.
We usethe SVMlight package to build a set of binary clas-sifiers in a one-versus-all formulation of the multi-class classification problem.
The features input toeach SVM consist of the discrete features describedabove (Sec.
4.2) after feature selection.
After train-ing SVMs for each target label against the union ofall others, we evaluate the SVM approach against thetest set by using the winner-takes-all strategy: thepredicted label corresponds to the SVM that outputsthe largest value.5.3.2 LP setupAgain we set up two LP systems: one using theoriginal feature space (after feature selection, whichbenefited all of the tested systems) and one using theSVM outputs.
Both use a cosine distance measure.The ?
parameter (see Eq.
1) is optimized through3-fold cross-validation on the training set.5.4 Combination optimizationUnlike MLPs, SVMs do not compute a smooth out-put distribution but base the classification decisionon the sign of the output values.
In order to smoothoutput values with a view towards graph construc-tion we applied the following techniques:1.
Combining SVM predictions and perfect fea-ture vectors: After training, the SVM actu-ally outputs wrong label predictions for a smallnumber (?
5%) of training samples.
These out-puts could simply be replaced with the perfectSVM predictions (1 for the true class, -1 else-where) since the labels are known.
However,the second-pass learner might actually bene-fit from the information contained in the mis-classifications.
We therefore linearly combinethe SVM predictions with the ?perfect?
feature209Initial labels Model S(G) avg.
Accuracy (%)Set 1 Set 2 Set 3 Set 4 Set 5 Average5000 NN ?
50.70 59.22 63.77 60.09 54.58 57.67 ?
4.55LP 451.54 58.37 59.91 60.88 62.01 59.47 60.13 ?
1.24NN+LP 409.79 58.03 63.91 66.62 65.93 57.76 62.45 ?
3.8310000 NN ?
65.86 60.19 67.52 65.68 65.64 64.98 ?
2.49LP 381.16 58.27 60.04 60.85 61.99 62.06 60.64 ?
1.40NN+LP 315.53 69.36 64.73 69.50 70.26 67.71 68.31 ?
1.9715000 NN ?
69.85 66.42 70.88 70.71 72.18 70.01 ?
1.94LP 299.10 58.51 61.00 60.94 63.53 60.98 60.99 ?
1.59NN+LP 235.83 70.59 69.45 69.99 71.20 73.45 70.94 ?
1.39Table 1: Accuracy results of neural classification (NN), LP with discrete features (LP), and combined (NN+LP), over 5 randomsamplings of 5000, 10000, and 15000 labeled words in the WSJ lexicon acquisition task.
S(G) is the smoothness of the graphvectors v that contain 1 at the correct label po-sition and -1 elsewhere:s?i = ?si + (1?
?
)vi (7)where si, s?i are the i?th input and output featurevectors and ?
a parameter fixed at 0.5.2.
Biasing uninformative distributions: For sometraining samples, although the predicted classlabel was correct, the outputs of the SVM wererelatively close to one another, i.e.
the decisionwas borderline.
We decided to bias these SVMoutputs in the right direction by using the sameformula as in equation 7.3.
Weighting by class priors: For each trainingsample, a corresponding sample with the per-fect output features was added, thus doublingthe total number of labeled nodes in the graph.These synthesized nodes are akin to the ?don-gle?
nodes (Goldberg and Zhu, 2006).
The dif-ference is that, while dongle nodes are onlylinked to one node, our artificial nodes aretreated like any other node and as such can con-nect to several other nodes.
The role of the arti-ficial nodes is to serve as authorities during theLP process and to emphasize class priors.5.4.1 ResultsAs before, we measured the smoothness of thegraphs in the two label propagation setups and foundthat in all cases the smoothness of the graph pro-duced with our method was better when comparedto the graphs produced using the standard approach,as shown in Table 3, which also shows accuracy re-sults for the SVM (?SVM?
label), LP over the stan-dard graph (?LP?
), and label propagation over SVMoutputs (?SVM+LP?).
The latter system consistentlyperforms best in all cases, although the most markedgains occur in the upper range of labeled samplespercentage.
The gain of the best data-driven LP overthe knowledge-based LP is significant in the 100%and 75% cases.# System Acc.
(%)1 htsa3 (Grozea, 2004) 72.92 IRST-kernels (Strapparava et al, 2004) 72.63 nusels (Lee et al, 2004) 72.44 SENSEVAL-3 contest baseline 55.25 Niu et al (Niu et al, 2005) LP/J-S 70.36 Niu et al LP/cosine 68.47 Niu et al SVM 69.7Table 2: Accuracy results of other published systems onSENSEVAL-3.
1-3 use syntactic features; 5-7 are directly com-parably to our system.For comparison purposes, Table 2 shows resultsof other published systems against the SENSEVALcorpus.
The ?htsa3?, ?IRST-kernels?, and ?nusels?systems were the winners of the SENSEVAL-3 con-test and used extra input features (syntactic rela-tions).
The Niu et al work (Niu et al, 2005) ismost comparable to ours.
We attribute the slightlyhigher performance of our SVM due to our featureselection process.
The LP/cosine system is a systemsimilar to our LP system using the discrete features,and the LP/Jensen-Shannon system is also similarbut uses a distance measure derived from Jensen-Shannon divergence.6 ConclusionsWe have presented a data-driven graph constructiontechnique for label propagation that utilizes a first-210Initial labels Model S(G) avg.
Accuracy (%)Set 1 Set 2 Set 3 Set 4 Set 5 Average25% SVM ?
62.94 62.53 62.69 63.52 62.99 62.93 ?
0.34LP 44.71 63.27 61.84 63.26 62.96 63.30 62.93 ?
0.56SVM+LP 39.67 63.39 63.20 63.95 63.68 63.91 63.63 ?
0.2950% SVM ?
67.90 66.75 67.57 67.44 66.79 67.29 ?
0.45LP 33.17 67.84 66.57 67.35 66.52 66.35 66.93 ?
0.57SVM+LP 24.19 67.95 67.54 67.93 68.21 68.11 67.95 ?
0.2375% SVM ?
69.54 70.19 68.75 69.80 68.73 69.40 ?
0.58LP 29.93 68.87 68.65 68.58 68.42 67.19 68.34 ?
0.59SVM+LP 16.19 69.98 70.05 69.69 70.38 68.94 69.81 ?
0.49100% SVM ?
70.74LP 21.72 69.69SVM+LP 13.17 71.72Table 3: Accuracy results of support vector machine (SVM), label propagation over discrete features (LP), and label propagationover SVM outputs (SVM+LP), each trained with 25%, 50%, 75% (5 random samplings each), and 100% of the train set.
Theimprovements of SVM+LP are significant over LP in the 75% and 100% cases.
S(G) is the graph smoothnesspass supervised classifier.
The outputs from thisclassifier (especially when optimized for the second-pass learner) were shown to serve as a better repre-sentation for graph-based semi-supervised learning.Classification results on two learning tasks showedsignificantly better performance compared to LP us-ing standard graph construction and the supervisedclassifier alone.Acknowledgments This work was funded byNSF under grant no.
IIS-0326276.
Any opinions,findings and conclusions, or recommendations ex-pressed herein are those of the authors and do notnecessarily reflect the views of this agency.ReferencesE.
Alpaydin and C. Kaynak.
1998.
Cascading classifiers.
Ky-bernetika, 34:369?374.Balcan et al 2005.
Person identification in webcam images.
InICML Workshop on Learning with Partially Classified Train-ing Data.Y.
Bengio, R. Ducharme, and P. Vincent.
2000.
A neural prob-abilistic language model.
In NIPS.J.
Chen, D. Ji, C.L.
Tan, and Z. Niu.
2006.
Relation ExtractionUsing Label Propagation Based Semi-supervised Learning.In Proceedings of ACL, pages 129?136.A.
Goldberg and J. Zhu.
2006.
Seeing stars when there aren?tmany stars: Graph-based semi-supervised learning for sen-timent categorization.
In HLT-NAACL Workshop on Graph-based Algorithms for Natural Language Processing.C.
Grozea.
2004.
Finding optimal parameter settings for highperformance word sense disambiguation.
Proceedings ofSenseval-3 Workshop.A.
Haghighi, A. Ng, and C.D.
Manning.
2005.
Robust textualinference via graph matching.
Proceedings of EMNLP.Y.K.
Lee and H.T.
Ng.
2002.
An empirical evaluation of knowl-edge sources and learning algorithms for word sense disam-biguation.
In Proceedings of EMNLP, pages 41?48.Y.K.
Lee, H.T.
Ng, and T.K.
Chia.
2004.
Supervised WordSense Disambiguation with Support Vector Machines andMultiple Knowledge Sources.
SENSEVAL-3.R.
Mihalcea, T. Chklovski, and A. Killgariff.
2004.
TheSenseval-3 English Lexical Sample Task.
In Proceedingsof ACL/SIGLEX Senseval-3.R.
Mihalcea.
2005.
Unsupervised large-vocabulary word sensedisambiguation with graph-based algorithms for sequencedata labeling.
In Proceedings of HLT/EMNLP, pages 411?418.S.
Mohammad and T. Pedersen.
2004.
Complementarity ofLexical and Simple Syntactic Features: The SyntaLex Ap-proach to Senseval-3.
Proceedings of the SENSEVAL-3.Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan.
2005.
Wordsense disambiguation using label propagation based semi-supervised learning.
In ACL ?05.J.
Otterbacher, G. Erkan, and D.R.
Radev.
2005.
Using Ran-dom Walks for Question-focused Sentence Retrieval.
Pro-ceedings of HLT/EMNLP, pages 915?922.B.
Pang and L. Lee.
2004.
A sentimental education: Sen-timent analysis using subjectivity summarization based onminimum cuts.
In Proceedings of ACL, pages 271?278.T.
Qin, T.-Y.
Liu, X.-D. Zhang, W.-Y.
Ma, and H.-J.
Zhang.2005.
Subspace clustering and label propagation for activefeedback in image retrieval.
In MMM, pages 172?179.A.
Ratnaparkhi.
1996.
A maximum entropy model for part-of-speech tagging.
In Proceedings of EMNLP, pages 133?142.C.
Strapparava, A. Gliozzo, and C. Giuliano.
2004.
Patternabstraction and term similarity for word sense disambigua-tion: IRST at SENSEVAL-3.
Proc.
of SENSEVAL-3, pages229?234.J.
Weston, C. Leslie, D. Zhou, A. Elisseeff, and W. Noble.2003.
Semi-supervised protein classification using clusterkernels.X.
Zhu and Z. Ghahramani.
2002.
Learning from labeledand unlabeled data with label propagation.
Technical report,CMU-CALD-02.Xiaojin Zhu.
2005.
Semi-Supervised Learning with Graphs.Ph.D.
thesis, Carnegie Mellon University.
CMU-LTI-05-192.211
