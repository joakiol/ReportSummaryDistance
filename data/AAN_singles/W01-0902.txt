Empirical Methods for Evaluating Dialog SystemsTim PaekMicrosoft ResearchOne Microsoft WayRedmond, WA 98052 USAtimpaek@microsoft.comAbstractWe examine what purpose adialog metric serves and thenpropose empirical methods forevaluating systems that meet thatpurpose.
The methods include aprotocol for conducting a wizard-of-oz experiment and a basic setof descriptive statistics forsubstantiating performance claimsusing the data collected from theexperiment as an ideal benchmarkor ?gold standard?
for makingcomparative judgments.
Themethods also provide a practicalmeans of optimizing the systemthrough component analysis andcost valuation.1 IntroductionIn evaluating the performance of dialog systems,designers face a number of complicated issues.On the one hand, dialog systems are ultimatelycreated for the user, so usability factors such assatisfaction or likelihood of future use should bethe final criteria.
On the other hand, becauseusability factors are subjective, they can beerratic and highly dependent on features of theuser interface (Kamm et al, 1999).
So, designershave turned to ?objective?
metrics such asdialog success rate or completion time.Unfortunately, due to the interactive nature ofdialog, these metrics do not always correspondto the most effective user experience (Lamel etal., 2000).
Furthermore, several different metricsmay contradict one another (Kamm et al, 1999),leaving designers with the tricky task ofuntangling the interactions or correlationsbetween metrics.Instead of focusing on developing newmetrics that circumvents the problems above, wemaintain that designers need to make better useof the ones that already exist.
Toward that end,we first examine what purpose a dialog metricserves and then propose empirical methods forevaluating systems that meet that purpose.
Themethods include a protocol for conducting awizard-of-oz experiment and a basic set ofdescriptive statistics for substantiatingperformance claims using the data collectedfrom the experiment as an ideal benchmark or?gold standard?
for making comparativejudgments.
The methods also provide a practicalmeans of optimizing the system throughcomponent analysis and cost valuation.2 PurposePerformance can be measured in myriad ways.Indeed, for evaluating dialog systems, the oneproblem designers do not encounter is lack ofchoice.
Dialog metrics come in a diverseassortment of styles.
They can be subjective orobjective, deriving from questionnaires or logfiles.
They can vary in scale, from the utterancelevel to the overall dialog (Glass et al, 2000).They can treat the system as a ?black box,?describing only its external behavior (Eckert etal., 1998), or as a ?glass box,?
detailing itsinternal processing.
If one metric fails to suffice,dialog metrics can be combined.
For example,the PARADISE framework allows designers topredict user satisfaction from a linearcombination of objective metrics such as meanrecognition score and task completion (Kamm etal., 1999; Litman & Pan, 1999; Walker et al,1997).Why so many metrics?
The answer has to dowith more than just the absence of agreed uponstandards in the research community,notwithstanding significant efforts in thatdirection (Gibbon et al, 1997).
Part of thereason deals with what purpose a dialog metricserves.
Designers often have multiple andsometimes inconsistent needs.
Four of the mosttypical needs are:?
Provide an accurate estimation of howwell a system meets the goals of thedomain task.?
Allow for comparative judgments of onesystem against another, and if possible,across different domain tasks.?
Identify factors or components in thesystem that can be improved.?
Discover tradeoffs or correlations betweenfactors.The above list of course is not intended to beexhaustive.
The point of creating the list is tohighlight the kinds of obstacles designers arelikely to face in trying to satisfy just thesetypical needs.
Consider the first need.Providing an accurate estimation of how wella system meets the goals of the domain taskdepends on how well the designers havedelineated all the possible goals of interaction.Unfortunately, users often have finer goals thanthose anticipated by designers, even for domaintasks that seem well defined, such as airlineticket reservation.
For example, a user may beleisurely hunting for a vacation and not careabout destination or time of travel, or the usermay be frantically looking for an emergencyticket and not care about price.
The?appropriate?
dialog metric should reflect thiskind of subtlety.
While ?time to completion?
ismore appropriate for emergency tickets,?concept efficiency rate?
is more appropriate forthe savvy vacationer.
As psychologists havelong recognized, when people engage inconversation, they make sure that they mutuallyunderstand the goals, roles, and behaviors thatcan be expected (Clark, 1996; Clark & Brennan,1991; Clark & Schaefer, 1989; Paek & Horvitz,1999, 2000).
They evaluate the ?performance?of the dialog based on their mutualunderstanding and expectations.Not only do different users have differentgoals, they sometimes have multiple goals, ormore often, their goals change dynamically inresponse to system behavior such ascommunication failures (Danieli & Gerbino,1995; Paek & Horvitz, 1999).
Because goalsengender expectations that then influenceevaluation at different points of time, usabilityratings are notoriously hard to interpret,especially if the system is not equipped to inferand keep track of user goals (Horvitz & Paek,1999; Paek & Horvitz, 2000).The second typical need for a dialog metric ?allowing for comparative judgments, introducesyet further obstacles.
In addition tounanticipated, dynamically changing user goals,different systems employ different dialogstrategies operating under different architecturalconstraints, rendering the search for dialogmetrics that generalize across systems a lofty ifnot unattainable pursuit.
While the PARADISEframework facilitates some comparison ofdialog systems in different domain tasks,generalization is limited because differentarchitectural constraints obviate certain factorsin the statistical model (Kamm et al, 1997).
Forexample, although the ability to ?barge-in?
turnsout to be a significant predictor of usability,many systems do not support this.
Taskcompletion based on the kappa statistic appearsto be a good candidate for a common measure,but only if every dialog system represented thedomain task as an Attribute-Value Matrix(AVM).
Unfortunately, that requirementexcludes systems that use Bayesian networks orother non-symbolic representations.
This hasprompted some researchers to argue that a?common inventory of concepts?
is necessary tohave standard metrics for evaluation acrosssystems and domain tasks (Kamm et al, 1997;Glass et al, 2000).
As we discuss in the nextsection, the argument is actually backwards; wecan use the metrics we already have to define acommon inventory of concepts.
Furthermore,with the proper set of descriptive statistics, wecan exploit these metrics to address the third andfourth typical needs of designers, that ofidentifying contributing factors, along with theirtradeoffs, and optimizing them.This is not to say that comparative judgmentsare impossible; rather, it takes some amount ofcareful work to make them meaningful.
Whenresearch papers describe evaluation studies ofthe performance of dialog systems, it isimperative that they provide a baselinecomparison from which to benchmark theirsystems.
Even when readers understand thescale of the metrics being reported, without abaseline, the numbers convey very little aboutthe quality of experience users can expect of thesystem.
For example, suppose a paper reportsthat a dialog system received an averageusability score of 9.5/10, a high conceptefficiency rate of 90%, and a low word error rateof 5%.
The numbers sound terrific, but theycould have resulted from low user expectationsresulting from a simplistic interface.
Practicallyspeaking, to make sense of the numbers, readerseither have to experience interacting with thesystem themselves, or have a baselinecomparison for the domain task.
This is trueeven if the paper reports a statistical model forpredicting one or more of the dialog metricsfrom the others, which may reveal tradeoffs butnot how well the system performs relative to thebaseline.To sum up, in considering the purpose adialog metric serves, we examined four typicalneeds and discussed the kinds of obstaclesdesigners are likely to face in finding a dialogmetric that satisfies those needs.
The obstaclesthemselves present distinct challenges: first,keeping track of user goals and performanceexpectations based on the goals, and second,establishing a baseline from which to benchmarksystems and make comparative judgments.Assuming that designers equip their system tohandle the first challenge, we now proposeempirical methods that allow them to handle thesecond.
These methods do not require newdialog metrics, but instead take advantage ofexisting ones through experimental design and abasic set of descriptive statistics.
They alsoprovide a practical means of optimizing thesystem.3 Empirical methodsIf designers want to make comparativejudgments about the performance of a dialogsystem relative to another system so that readersunacquainted with either system can understandthe reported metrics, they need a baseline.Fortunately, in evaluating dialog betweenhumans and computers, the ?gold standard?
isoftentimes known; namely, human conversation.The most intuitive and effective way tosubstantiate performance claims is to compare adialog system on a particular domain task withhow human beings perform on the same task.Because human performance constitutes an idealbenchmark, readers can make sense of thereported metrics by assessing how close thesystem approaches the gold standard.Furthermore, with a benchmark, designers canFigure 1.
Wizard-of-Oz study for the purpose ofestablishing a baseline comparison.optimize their system through componentanalysis and cost valuation.In this section, we outline an experimentalprotocol for obtaining human performance datathat can serve as a gold standard.
We thenhighlight a basic set of descriptive statistics forsubstantiating performance claims, as well as foroptimization.3.1 Experimental protocolCollecting human performance data forestablishing a gold standard requires conductinga carefully controlled wizard-of-oz (WOZ)experiment.
The general idea is that userscommunicate with a human ?wizard?
under theillusion that they are interacting with acomputational system.
For spoken dialogsystems, maintaining the illusion usuallyinvolves utilizing a synthetic voice to outputwizard responses, often through voice distortionor a text-to-speech (TTS) generator.The typical use of a WOZ study is to recordand analyze user input and wizard output.
Thisallows designers to know what to expect andwhat they should try to support.
User input isespecially critical for speech recognitionsystems that rely on the collected data foracoustic training and language modeling.
Initerative WOZ studies, previously collected datais used to adjust the system so that as theperformance of the system improves, the studiesemploy less of the wizard and more of thesystem (Glass et al, 2000).
In the process,design constraints in the interface may berevealed, in which case, further studies areUsersWizardDialog SystemorExperimentally Controlled CurtainControlled InputControlled Outputconducted until acceptable tradeoffs are found(Bernsen et al, 1998).In contrast to the typical use, a WOZ studyfor establishing a gold standard prohibitsmodifications to the interface or experimental?curtain.?
As shown in Figure 1, all input andoutput through the interface must be carefullycontrolled.
If designers want to use previouslycollected performance data as a gold standard,they need to verify that all input and output haveremained constant.
The protocol for establishinga gold standard is straightforward:?
Select a dialog metric to serve as anobjective function for evaluation andoptimization.?
Vary the component or feature that bestmatches the desired performance claim forthe dialog metric.?
Hold all other input and output throughthe interface constant so that the onlyunknown variable is who does the internalprocessing.?
Repeat using different wizards, makingsure that each wizard follows strictguidelines for interacting with subjects.To motivate the above protocol, considerhow a WOZ study might be used to evaluatespoken dialog systems.
As almost everydesigner has found, the ?Achilles?
heel?
ofspoken interaction is the fragility of the speechrecognizer.
System performance depends highlyon the quality of the recognition.
Suppose adesigner is interested in bolstering therobustness of a dialog system by exploitingdifferent types of repair strategies.
Using taskcompletion rate as an objective function, thedesigner varies the repair strategies utilized bythe system.
To make claims about the robustnessof particular types of repair strategies, thedesigner must keep all other input and outputconstant.
In particular, the protocol demands thatthe wizard in the experiment must receiveutterances through the same speech recognizeras the dialog system.
The performance of thewizard on the same quality of input as the dialogsystem constitutes the gold standard.
Thedesigner may also wish to keep the set of repairstrategies constant while varying the use ordisuse of the speech recognizer to estimate howmuch the recognizer alone degrades taskcompletion rate.A deep intuition underlies the experimentalcontrol of the speech recognizer.
As researchershave observed, people with impaired hearing ornon-native language skills still manage tocommunicate effectively despite noisy oruncertain input.
Unfortunately, the same cannotbe said of computers with analogousdeficiencies.
People overcome their deficienciesby collaboratively working out the mutual beliefthat their utterances have been understoodsufficiently for current purposes, a processreferred to as ?grounding?
(Clark, 1996).
Repairstrategies based on grounding indeed showpromise for improving the robustness of spokendialog systems (Paek & Horvitz, 1999; Paek &Horvitz, 2000).3.1.1 PrecautionsIn following the above protocol, we point outa few precautions.
First, WOZ studies forestablishing a gold standard work best withdialog systems that are highly modular.
Themore modular the architecture of the dialogsystem, the easier it will be to test componentsby replacing a particular module of interest withthe wizard.
Without modularity, it will be harderto guarantee that all other inputs and outputshave remained constant because componentboundaries are blurred.
Ironically, after a certainpoint, a high degree of modularity may in factpreclude the experimental protocol; componentsmay be so specialized and quickly accessed by asystem that it may not be feasible to replace thatcomponent with a human.A second precaution deals with the conceptof a gold standard.
What allows the performanceof the wizard to be used as a gold standard is notthe wizard, but rather the fact that theperformance constitutes an upper bound.
If anupper bound of performance has already beenidentified, then that is the gold standard.
Forexample, graphical user interfaces (GUI) ortouch-tone systems may represent a better goldstandard for task completion rate if users finishtheir interactions with such systems ore oftenthan with human operators.
With spoken dialogsystems, the question of when the use of speechinteraction is truly compelling is often ignored.If a dialog designer runs the experimentalprotocol and observes that even human wizardscannot perform the domain task very well, thatsuggests that perhaps a gold standard may befound elsewhere.Figure 2.
Comparison of two dialog systemswith respect to the gold standard.3.2 Descriptive statisticsAfter collecting data using the experimentalprotocol, designers can make comparativejudgments about the performance of their systemrelative to other systems with a basic set ofdescriptive statistics.
The statistics build on theinitial step of fitting a statistical model on thedata fro both wizards and the dialog system.
Wediscuss precautions later.
Plotting the fittedcurves on the same graph sheds light on howbest to substantiate any performance claims.
Thegraph displays the performance of the dialogsystem along a particular dimension of interestwith the wizard data constituting a gold standardfor comparison.
Consider how this kind of?benchmark graph?
could benefit the evaluationof spoken dialog systems.Referring to previous example, suppose adesigner is interested in evaluating therobustness of two dialog systems utilizing twosets of repair strategies.
The designer varieswhich set is implemented, while holdingconstant the use of the speech recognizer.
Ingeneral, as speech recognition errors increase,task completion rate, or dialog success rate,decreases.
Not surprisingly, several researchershave found an approximately linear relationshipin plotting task completion rate as a function ofword error rate (Lamel et al, 2000; Rudnicky,2000).
Keeping this in mind, Figure 2 displays abenchmark graph for two dialog systems A andB, utilizing different repair strategies.
The fittedcurve for A is characteristically linear, while thecurve for B is polynomial.
Because wizards arepresumably more capable of anticipating andrecovering from speech recognition errors, theirFigure 3.
Distance in performance of the twosystems from the gold standard.performance data comprise the gold standard.As such, the fitted curve for the gold standard inFigure 2 stays close to the upper right handcorner of the graph in a monotonicallydecreasing fashion; that is, task completion rateremains relatively high as word error rateincreases and then gracefully degrades beforethe error rate reaches its highest level.Looking at the benchmark graph, readersimmediately get a handle on substantiatingperformance claims about robustness.
Forexample, by noticing that task completion ratefor the gold standard rapidly drops from around65% at the 80% mark to about 15% by 100%,readers know that at 80% word error rate, evenwizards, with human level intelligence, cannotrecover from failures with better than 65% taskcompletion rate.
In short, the task is not trivial.This means that if A and B report low numbersfor task completion rate beyond the 80% markfor word error rate, they may be still performingrelatively well compared to the gold standard.Numbers themselves are deceptive, unless theyare put side by side with a benchmark.Of course, a designer might not have accessto data all along the word error rate continuumas in Figure 2.
If this presents a problem, it maybe more appropriate to measure task completionrate as a function of concept error rate.
Thechoice, as stated in the experimental protocol,depends on the performance claim a designer isinterested in making.
In spoken dialog, however,where speech recognition errors abound, anotherparticularly useful benchmark graph is to plotword or concept error rate against userfrustration.
This experiment reveals any inherentBenchmark Graph01020304050607080901000 10 20 30 40 50 60 70 80 90 100Word Error Rate (%)TaskCompletionRate(%)System A System B Gold StandardGold ImpurityGraph051015202530354045500 10 20 30 40 50 60 70 80 90 100Word Error Rate (%)TaskCompletionRateDifference(%)|A - G| |B - G|System B MassSystem A MassTaskCompletionRateDifference(%)bias users may have towards speaking with acomputer in the first place.In making comparative judgments, designerscan also benefit from plotting the absolutedifference in performance from the goldstandard as a function of the same independentvariable as the benchmark graph.
Figure 3displays the difference in task completion rate,or ?gold impurity,?
for systems A and B as afunction of word error rate.
The closer a systemis to the gold standard, the smaller the ?mass?
ofthe gold impurity on the graph.
Anomalies areeasier to see, as they noticeably show up asbumps or peaks.
If a dialog system reports lownumbers but evinces little gold impurity, readercan be assured that the system is as good as itcan possibly be.Any crosses in performance can be revealingas well.
For example, in Figure 3, although Bperforms worse at lower word error rates than A,after about the 35% mark, B stays closer to thegold standard.
Hence, the designer in this casecould not categorically prefer one system to theother.
In fact, assuming that the only differencebetween A and B is the choice of repairstrategies, the designer should prefer A to B ifthe average word error rate for the speechrecognizer is below 35%, and B to A, if theaverage error rate is about 40%.
Of course, othercost considerations come into play, as wedescribe later.The final point to make about comparingdialog systems to a gold standard is that readersare able to substantiate performance claimsacross different domain tasks.
They need only tolook at how close each system approaches theirrespective gold standard in a benchmark graph,or how much mass each system puts out in agold impurity graph.
They can even do thiswithout having the luxury of experiencing anyof the compared systems.3.2.1 ComplexityWithout a gold standard, making comparativejudgments of dialog systems across differentdomain tasks poses a problem for two reasons:task complexity and interaction complexity.Tutoring physics is a generally more complexdomain task than retrieving email.
On the otherhand, task complexity alone does not explainwhat makes one dialog more complex thananother; interaction complexity also plays asignificant role.
Tutoring physics can be lesschallenging than retrieving email if the systemaccepts few inputs, essentially constraining usersto follow a predefined script.
Any dialog systemthat engages in ?mixed initiative?
will be morecomplex than one that utilizes ?system-initiated?prompts because users have more actions at theirdisposal at any point in time.The way to evaluate complexity in abenchmark graph is to measure the distance ofthe gold standard to the absolute upper bound ofperformance.
If wizards with human levelintelligence cannot themselves performreasonably close to the absolute upper bound,then either the task is very complex, or theinteraction afforded by the dialog interface is toorestrictive for wizards, or perhaps both.
Becausecomplexity is measured only in connection withthe gold standard ceteris paribus, ?benchmarkcomplexity?
can be computed as:?=?
?=nxxgUnBC0)(where U is the upper bound value of aperformance metric, n is the upper bound valuefor an independent variable X, and g(x) is thegold standard along that variable.Designers can use benchmark complexity tocompare systems across different domain tasksif they are not too concerned aboutdiscriminating between task complexity andinteraction complexity.
Otherwise, they can treatbenchmark complexity as an objective functionand vary the interaction complexity of the dialoginterface to scrutinize the effect of taskcomplexity on wizard performance, or viceversa.
In short, they need to conduct anotherexperimental study.3.2.2 PrecautionsBefore substantiating performance claims with abenchmark graph, designers must exerciseprudence in model fitting.
One precaution is tobeware of insufficient data.
Without collectingenough data, designers cannot be certain thatdifferences in the performance of a dialogsystem from the gold standard cannot beexplained simply by the variance in the fittedmodels.
To determine when there is enough datato generate reliable models, designers canconduct WOZ studies in an iterative fashion.First, collect some data and fit a statisticalmodel.
Second, plot the least squares distance,or ?
?iii xfy 2))(( , where f(x) is the fittedmodel, against the iteration.
Keep collectingmore data until the plot seems to asymptoticallyconverge.
Designers may need to report R2s forthe curves in their benchmark graphs to informreaders of the reliability of their models.Another precaution is to use differentwizards, making sure that each wizard followsstrict guidelines for interacting with subjects.The experimental protocol included thisprecaution because designers need to considerwhether a consistent gold standard is evenpossible with a given dialog interface.
Indeed,difference between wizards may uncover seriousdesign flaws in the interface.
Furthermore, usingdifferent wizards compels designers to collectmore data for the gold standard.As a final precaution, designers need towatch out for violations of model assumptionsregarding residual errors.
These are typicallywell covered in most statistics textbooks.
Forexample, because task completion rate as aperformance metric has an upper bound of100%, it is unlikely that residual errors will beequally spread out along the word error ratecontinuum.
In regression analysis, this is called?heteroscedasticity.?
Another common violationoccurs with the non-normality of the residualerrors.
Designers would do well to takeadvantage of corrective measures for both.3.2.3 Component analysisA gold standard naturally lends itself tooptimization.
With a gold standard, designerscan identify which components are contributingthe most to a performance metric by examiningthe gold impurity graph of the system with andwithout a particular component.
This kind of testis similar to how dissociations are discovered inneuroscience through ?lesion?
experiments.Carrying out stepwise comparisons of thecomponents, designers can check for tradeoffs,and even use all or part of the gold impurity asan optimization metric.
For example, suppose adesigner endeavors to improve a dialog systemfrom its current average task completion rate of70% to 80%.
In Figure 2, suppose Bincorporates a component that A does not.Looking at the corresponding word error rates inthe gold impurity graph for both systems, theFigure 4.
The cost a designer is willing to incurfor improvements to task completion rate.mass under the curve for B is slightly greaterthan that for A.
The designer can optimize theperformance of the system by selectingcomponents that minimize that mass, in whichcase, the component in B would be excluded.Because components often interact with eachother in terms of their statistical effect on theperformance metric, designers may wish to carryout a multi-dimensional analysis to weed outthose components with weak main andinteraction effects.3.2.4 Cost valuationAnother optimization use of a gold standard is tominimize the amount of ?gold?
expended indeveloping a dialog system.
Gold here includesmore than just dollars, but time and effort aswell.
Designers can determine where to investtheir research focus by calculating ?averagemarginal cost.?
To do this, they must first elicita cost function that conveys what they arewilling to pay, in terms of utility, to achievevarious levels of performance in a dialog metric(Bell et al, 1988).
Figure 4 displays what cost adesigner might be willing to incur for variousrates of task completion.
The average marginalcost can be computed by weighting goldimpurity by the cost function.
In other words,average marginal cost can be computed as:?=?
?=baxxgxfxcAMC )()()(Cost for Improvement01020304050607080901000 10 20 30 40 50 60 70 80 90 100Task Completion Rate (%)UtilityCostwhere f(x) is the performance of the system on aparticular dialog metric X, g(x) is the goldstandard on that metric, and c(x) is elicited costfunction.Following the previous example, if thedesigner endeavors to improve a system that iscurrently operating at an average taskcompletion rate of 70% to 80%, then the averagemarginal cost for that gain is simply the areaunder the cost function for that intervalmultiplied by the gold impurity for that interval.In deciding between systems or components,designers can exploit average marginal cost todrive down their expenditure.4 DiscussionInstead of focusing on developing new dialogmetrics that allow for comparative judgmentsacross different systems and domain tasks, weproposed empirical methods that accomplish thesame purpose while taking advantage of dialogmetrics that already exist.
In particular, weoutlined an experimental protocol forconducting a WOZ study to collect humanperformance data that can serve as a goldstandard.
We then described how to substantiateperformance claims using both a benchmarkgraph and a gold impurity graph.
Finally, weexplained how to optimize a dialog system usingcomponent analysis and value optimization.Without a doubt, the greatest drawback to theempirical methods proposed is the tremendouscost of conducting WOZ studies, both in termsof time and money.
In special circumstances,such as the Communicator Project, whereparticipants all work within the same domaintask, DARPA itself might finance WOZ studiesfor evaluation on behalf of the participants.
Non-participants may resort to average marginal costto optimize their own expenditure.ReferencesBell, D. E., Raiffa, H., & Tversky, A.
(Eds.).
(1988).Decision making: Descriptive, normative, andprescriptive interactions.
New York: CambridgeUniversity Press.Bersen, N. O., Dybkjaer, H. & Dybkjaer, L. (1998).Designing interactive speech systems: From firstideas to user testing.
Springer-Verlag.Clark, H.H.
(1996).
Using language.
CambridgeUniversity Press.Clark, H.H.
& Brennan, S.A. (1991).
Grounding incommunication.
In Perspectives on SociallyShared Cognition, APA Books, 127-149.Clark, H.H.
& Schaefer, E.F. (1989).
Contributing todiscourse.
Cognitive Science, 13, 259-294.Danieli, M. & Gerbino, E. (1995).
Metrics forevaluating dialogue strategies in a spokenlanguage system.
In Proc.
AAAI SpringSymposium on Empirical Methods in DiscourseInterpretation and Generation, 34-39.Eckert, W., Levin, E. & Pieraccini, R. (1998).Automatic evaluation of spoken dialogue systems.In TWLT13: Formal semantics and pragmatics ofdialogue, 99-110.Gibbon, D., Moore, R. & Winski, R.
(Eds.)
(1998).Handbook of standards and resources for spokenlanguage systems.
Spoken Language SystemAssessment, 3, Walter de Gruyter, Berlin.Glass, J., Polifroni, J., Seneff, S. & Zue, V. (2000).Data collection and performance evaluation ofspoken dialogue systems: The MIT experience.
InProc.
of ICSLP.Horvitz, E. & Paek, T. (1999).
A computationalarchitecture for conversation.
In Proc.
of 7th UserModeling, Springer Wien, 201-210.Kamm, C., Walker, M.A.
& Litman, D. (1999).Evaluating spoken language systems.
In Proc.
ofAVIOS.Lamel, L., Rosset S. & Gauvain, J.L.
(2000).Considerations in the design and evaluation ofspoken language dialog systems.
In Proc.
ofICSLP.Litman, D. & Pan, S. (1999).
Empirically evaluatingan adaptable spoken dialogue system.
In Proc.
of7th User Modeling, Springer Wien, 55-64.Paek, T. & Horvitz, E. (2000).
Conversation asaction under uncertainty.
In Proc.
of 16th UAI,Morgan Kaufmann, 455-464.Paek, T. & Horvitz, E. (1999).
Uncertainty, utility,and misunderstanding: A decision-theoreticperspective on grounding in conversationalsystems.
In Proc.
of AAAI Fall Symposium onPsychological Models of Communication, 85-92.Rudnicky, A.
(2000).
Understanding systemperformance in dialog systems.
MSR Invited Talk.Walker, M.A., Litman, D., Kamm, C. & Abella, A.(1997).
PARADISE: A framework for evaluatingspoken dialogue agents.
In Proceedings of the 35thACL.
