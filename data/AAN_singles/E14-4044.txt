Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226?230,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsCrowdsourcing Annotation of Non-Local Semantic RolesParvin Sadat FeizabadiInstitut f?ur ComputerlinguistikHeidelberg University69120 Heidelberg, Germanyfeizabadi@cl.uni-heidelberg.deSebastian Pad?oInstitut f?ur Maschinelle SprachverarbeitungStuttgart University70569 Stuttgart, Germanypado@ims.uni-stuttgart.deAbstractThis paper reports on a study of crowd-sourcing the annotation of non-local (orimplicit) frame-semantic roles, i.e., rolesthat are realized in the previous discoursecontext.
We describe two annotation se-tups (marking and gap filling) and find thatgap filling works considerably better, attain-ing an acceptable quality relatively cheaply.The produced data is available for researchpurposes.1 IntroductionIn the last years, crowdsourcing, e.g., using Ama-zon?s Mechanical Turk platform, has been used tocollect data for a range of NLP tasks, e.g., MT eval-uation (Callison-Burch, 2009), sentiment analysis(Mellebeek et al., 2010), and student answer rat-ing (Heilman and Smith, 2010).
Frame-semanticrole annotation (FSRA) is a task that requires morelinguistic expertise than most data collection tasksrealized with crowdsourcing; nevertheless it is alsoa crucial prerequisite for high-performance frame-semantic role labeling (SRL) systems (Das et al.,2014).
Thus, there are some studies that have in-vestigated FSRA as a crowdsourcing task.
It can beseparated into two parts: First, choosing the frameevoked by a given predicate in a sentence; second,assigning the semantic roles associated with thechosen frame.
Hong and Baker (2011) have re-cently addressed the first step, experimenting withvarious ways of presenting the task.
Fossati etal.
(2013) have considered both steps and opera-tionalized them separately and jointly, finding thebest results when a single annotation task is pre-sented to turkers (due to the interdependence of thetwo steps) and when the semantic role descriptionare simplified.
Both studies conclude that crowd-sourcing can produce usable results for FSRA butrequires careful design.
Our study extends theseprevious studies to the phenomenon of implicit(non-locally realized) semantic roles where anno-tators are presented with a target sentence in para-graph context, and have to decide for every rolewhether it is realized in the target sentence, else-where in the paragraph, or not at all.
Our resultsshows that implicit roles can be annotated as wellas locally realized roles in a crowdsourcing setup,again provided that good design choices are taken.2 Implicit Semantic RolesImplicit or non-locally realized semantic roles oc-cur when arguments of a predicate are understoodalthough not expressed in its direct syntactic neigh-borhood.
FrameNet (Fillmore et al., 2003) dis-tinguishes between indefinite non-instantiations(INIs), which are interpreted generically; definitenon-instantiations (DNIs), which can often be iden-tified with expressions from the previous context;and constructional non-instantiations (CNI), e.g.,passives.
For instance, in the following example,the GOAL of the predicate ?reached?
is realizedlocally, the SOURCE is a non-locally realized DNI,and the PATH is an INI and not realized at all.
(1) Phileas Fogg, having shut the door of[SOURCEhis house] at half-past eleven, andhaving put his right foot before his leftfive hundred and seventy-five times, andhis left foot before his right five hundredand seventy-six times, reached [GOALtheReform Club].Implicit roles play an important role in discoursecomprehension and coherence (Burchardt et al.,2005) and have found increasing attention over the226last years.
The development was kickstarted by thecreation of a corpus of non-local frame-semanticroles for the SemEval 2010 Task 10 (Ruppenhoferet al., 2010), which still serves as a de facto stan-dard.
A number of systems perform SRL for non-local roles (Chen et al., 2010; Silberer and Frank,2012; Laparra and Rigau, 2013), but the obtainedresults are still far from satisfactory, with the bestreported F-Score at 0.19.
The main reason is datasparsity: Due to the small size of the dataset (just438 sentences), every predicate occurs only a smallnumber of times.
Crowdsourcing can be an attrac-tive strategy to acquire more annotations.3 Experimental Setup3.1 DomainOur emphasis is on evaluating the annotation ofimplicit roles.
We reduce complexity by limitingthe number of frames and roles like earlier studies(Hong and Baker, 2011; Fossati et al., 2013).
Wefocus on verbs from the MOTION and POSITIONframes, which realize a common set of locationroles (PLACE OF EVENT, SOURCE, GOAL, PATH).This makes the task more uniform and allows us toskip frame annotation.
Information about spatialrelations, provided by such verbs, can be usefulfor many NLP tasks which reason about spatialinformation, e.g.
systems generating textual de-scriptions from visual data, robot navigation tasks,and geographical information systems or GIS (Ko-rdjamshidi et al., 2012).3.2 CorpusWe chose the novel ?Around the World in EightyDays?
by Jules Verne, annotating the ten most fre-quent predicates meeting the conditions describedabove for annotation (reach, arrive, descend, rush,follow, approach, send, cross, escape, pass).
Apost-hoc analysis later showed that each instance ofthese predicates has on average 0.67 implicit rolesidentifiable in previous context, which underlinesthe relevance of annotating such cases.
Metaphori-cal uses were discarded before annotation, whichleft an average 38.4 instances for each predicate.4 Annotation and AgreementWe decided to present target sentences with threesentences of previous context, as a compromise be-tween reading overhead and coverage of non-localroles: For nominalizations, the three previous sen-tences cover over 85% of all non-local roles (Ger-Source Goal Path PlaceExact Match 0.35 0.44 0.48 0.24Overlap 0.35 0.46 0.52 0.27Table 1: Raw agreement among annotators in the?marking?
taskber and Chai, 2012).
An example and the detaileddescription of the task were provided to the an-notators through external links.
We experimentedwith two alternatives: annotation as a marking taskand as a gap filling task (explained below).
EachHIT was annotated by five turkers who were askedto annotate both local and non-local roles, sinceidentification of local roles is necessary for reliabletagging of non-local roles.4.1 Marking TaskOur rationale was to make the task as comprehen-sible as possible for non-experts.
In each HIT, thetarget predicate in its context was shown in bold-face and the annotators were asked to answer fourquestions about ?the event in bold?
: (a) where doesthe event take place?
; (b) what is its starting point?
;(c) what is its end point?
; (d) which path is used?For every question, turkers were asked to eithermark a text span (shown in a non-editable fieldbelow the question) or click a button labeled ?notfound in the text?.
The goals of this setup were (a)to minimize annotation effort, and (b) to make thetask as layman-compatible as possible, followingFossati et al.
?s (2013) observation that linguisticdefinitions can harm results.After annotating some instances, we computedraw inter-annotator agreement (IAA).
Table 1shows IAA among turkers in two conditions (aver-age pairwise Exact Match and word-based Overlap)overall annotations for the first 49 instances.1Theoverall IAA is 37.9% (Exact Match) and 40.1%(Overlap).
We found these results to be too low tocontinue this approach.
The low results for Overlapindicate that the problems cannot be due mainly todifferences in the marked spans.
Indeed, an analy-sis showed that the main reason was that annotatorswere often confused by the presence of multiplepredicates in the paragraph.
Consequently, manyanswers marked roles pertaining not to the boldedtarget predicate but to other predicates, such as (2).
(2) Leaving Bombay, it passes through Sal-1Kappa is not applicable since we have a large number ofdisjoint annotators.227Source Goal Path PlaceExact Match 0.46 0.46 0.56 0.30Overlap 0.50 0.54 0.58 0.38Table 2: Raw agreement among annotators in the?gap filling?
taskcette, crossing to the continent oppositeTannah, goes over the chain of the West-ern Ghauts, [.
.
. ]
and, descending south-eastward by Burdivan and the French townof Chandernagor, has its terminus at Cal-cutta.Annotators would be expected to annotate thecontinent opposite Tannah as the goal of crossing,but some annotated Calcutta, the final destinationof the chain of motion events described.4.2 Gap Filling TaskSeeing that the marking task did not constrain theinterpretation of the turkers sufficiently, we movedto a second setup, gap filling, with the aim of fo-cussing the turkers?
attention to a single predicaterather than the complete set of predicates presentin the text shown.
In this task, the annotators wereasked to complete the sentence by filling in theblanks in two sentences:1.
[Agent] [Event+ed] from .
.
.
to .
.
.through .
.
.
path.2.
The whole event took place in/at .
.
.The first sentence corresponds to annotations ofthe SOURCE, GOAL, and PATH roles; the secondone of the PLACE role.
The rationale is that thepresence of the predicate in the sentence focusesthe turkers?
attention on the predicate?s actual roles.Annotators could leave gaps empty (in the case ofunrealized roles), and we asked them to remain asclose to the original material as possible, that is,avoid paraphrases.
Perfect copying is not alwayspossible, due to grammatical constraints.Table 2 shows the IAA for this design.
We seethat even though the gap filling introduced a newsource of variability (namely, the need for annota-tors to copy text), the IAA improves considerably,by up to 11% in Exact Match and 15% in Over-lap.
The new overall IAAs are 44.7% (+6.8%) and50.2% (+10.1%), respectively.
Overall, the num-bers are still fairly low.
However, note that theseIAA numbers among turkers are a lower bound forthe agreement between a ?canonical?
version ofthe turkers?
annotation (see Section 5) and an idealgold standard.
Additionally, a data analysis showedthat in the gap filling setup, many of the disagree-ments are more well-behaved: unsurprisingly, theyare often cases where annotators disagree on the ex-act range of the string to fill into the gap.
Considerthe following example:(3) Skillful detectives have been sent to all theprincipal ports of America and the Conti-nent, and he?ll be a clever fellow if he slipsthrough their fingers.
?Arguably, experts would annotate all the prin-cipal ports of America and the Continent as theGOAL role of sent.
Turkers however annotated dif-ferent spans, including all the principal ports ofAmerica, ports, as well as the ?correct?
span.
Thelowest IAA is found for the place role.
While it ispossible that our setup which required turkers toconsider a second sentence to annotate place con-tributes to the overall difficulty, our data analysisindicates that the main problem is the more vaguenature of PLACE compared to the other roles whichmade it more difficult for annotators to tag consis-tently.
Consider Example (1): the PLACE couldbe, among other things, the City, London, England,etc.
The large number of locations in the novel is acompounding factor.
We found that for some pred-icates (e.g.
arrive, reach), many turkers attemptedto resolve the ambiguity by (erroneously) annotat-ing the same text as both GOAL and PLACE, whichruns counter to the FrameNet guidelines.5 CanonicalizationWe still need to compute a ?canonical?
annotationthat combines the five turker?s annotations.
First,we need to decide whether a role should be realizedor left unrealized (i.e., INI, CNI, or DNI but not inthe presented context).
Second, we need to decideon a span for realized roles.
Canonicalization incrowdsourcing often assumes a majority principle,accepting the analysis proposed by most turkers.We found it necessary to be more flexible.
Regard-ing realization, a manual analysis of a few instancesshowed that cases of two turker annotations withnon-empty overlap could be accepted as non-localroles.
That is, turkers frequently miss non-localroles, but if two out of five annotate an overlappingspan with the same role, this is reasonable evidence.Regarding the role?s span, we used the consensus228Source Goal Path PlaceExact Match 0.72 0.67 0.82 0.50Overlap 0.72 0.69 0.82 0.54Table 3: Raw agreement between canonical crowd-sourcing annotation and expert annotation by roleLocal Non-Local UnrealizedExact Match 0.66 0.66 0.69Overlap 0.69 0.70 0.69Table 4: Raw agreement between canonical anno-tation and expert annotation by realization statusspan if it existed, and the maximal (union) span oth-erwise, given that some turkers filled the gaps justwith head words and not complete constituents.
Totest the quality of the canonical annotation, one ofthe authors had previously annotated 100 randominstances that were also presented to the turkers.We consider the result to be an expert annotationapproximating a gold standard and use it to judgethe quality of the canonical turker annotations.
Theresults are shown in Table 3.The overall raw agreement numbers are 67.80%(Exact Match) and 69.34% (Overlap).
As we hadhoped, the agreement between the canonical crowd-sourcing annotation and the expert annotation isagain substantially higher than the IAA among turk-ers.
Again, we see the highest numbers for path(the most specific role) and the lowest numbers forplace (the least specific role).To assess whether the number obtained in table3 are sensitive to realization status (explicit, im-plicit or unrealized), we broke down the agreementnumbers by realization status.
Somewhat to our(positive) surprise, the results in Table 4 indicatethat non-locally realized roles are annotated ablutas reliably as locally realized ones.
Except for theill-defined PLACE role, our reliability is compara-ble to Fossati et al.
(2013).
Given the more difficultnature of the task (annotators are given more con-text and have to make a more difficult decision),we consider this a promising result.6 Final Dataset and CostThe final dataset consists of 384 predicate in-stances.2With four roles per predicate, a totalof 1536 roles could have been realized.
We found2It can be downloaded for research purposesfrom http://www.cl.uni-heidelberg.de/?feizabadi/res.mhtmlthat more than half (60%) of the roles remainedunrealized even in context.
23% of the roles wererealized locally, and 17% non-locally.
The distri-bution over locally realized, non-locally realized,and unrealized roles varies considerably among thefour roles that we consider.
GOAL has the high-est percentage of realized roles overall (unrealizedonly for 34% of all predicate instances), and at thesame time the highest ratio of locally realized roles(48% locally realized, 18% non-locally).
This cor-responds well to FrameNet?s predictions about ourchosen predicates which realize the Goal role gen-erally as the direct object (reach) or an obligatoryprepositional phrase (arrive).
In contrast, SOURCEis realized only for 36% of all instances, and thenpredominantly non-locally (24% non-local vs. 12%local).
This shows once more that a substantial partof predicate-argument structure must be recoveredfrom previous discourse context.On average, each HIT page was annotated in 1minute and 48 seconds, which means 27 secondsper each role and a total of 60 hours for the wholeannotation.
We paid 0.15 USD for each HIT.
Sincethe number of roles in all HITs was fixed to four(source, goal, path and place), each role cost 0.04USD, which corresponds to about USD 0.19 forevery canonical role annotation.
This is about twicethe amount paid by Fossati et al.
and reflects theincreased effort inherent in a task that involvesdiscourse context.7 ConclusionThis paper presented a study on crowdsourcing theannotation of non-local semantic roles in discoursecontext, comparing a marking and a gap fillingsetup.
We found that gap filling is the more reliablechoice since the repetition of the predicate helpsfocusing the turkers?
attention on the roles at handrather than understanding of the global text.
Thus,the semantic role-based crowdsourcing approach ofFossati et al.
(2013) appears to be generalizable tothe area of non-locally realized roles, provided thatthe task is defined suitably.
Our results also supportFossati et al.
?s observation that reliable annotationscan be obtained without providing definitions ofsemantic roles.
However, we also find large differ-ences among semantic roles.
Some (like PATH) canbe annotated reliably and should be usable to trainor improve SRL systems.
Others (like PLACE) aredefined so vaguely that it is unclear how usabletheir annotations are.229ReferencesAljoscha Burchardt, Anette Frank, and Manfred Pinkal.2005.
Building text meaning representations fromcontextually related frames ?
a case study.
In Pro-ceedings of the International Workshop on Compu-tational Semantics, pages 66?77, Tilburg, Nether-lands.Chris Callison-Burch.
2009.
Fast, cheap, and cre-ative: evaluating translation quality using Amazon?sMechanical Turk.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 286?295, Singapore.Desai Chen, Nathan Schneider, Dipanjan Das, andNoah A. Smith.
2010.
Semafor: Frame argumentresolution with log-linear models.
In Proceedingsof the 5th International Workshop on Semantic Eval-uation, pages 264?267, Uppsala, Sweden.Dipanjan Das, Desai Chen, Andr?e F. T. Martins,Nathan Schneider, and Noah A. Smith.
2014.Frame-semantic parsing.
Computational Linguis-tics.
To appear.Charles J Fillmore, Christopher R Johnson, and MiriamR L Petruck.
2003.
Background to FrameNet.
Inter-national Journal of Lexicography, 16(3):235?250.Marco Fossati, Claudio Giuliano, Sara Tonelli, andFondazione Bruno Kessler.
2013.
OutsourcingFrameNet to the Crowd.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, pages 742?747, Sofia, Bulgaria.Matthew Gerber and Joyce Y Chai.
2012.
Semanticrole labeling of implicit arguments for nominal pred-icates.
Computational Linguistics, 38(4):755?798.Michael Heilman and Noah A Smith.
2010.
Rat-ing computer-generated questions with MechanicalTurk.
In Proceedings of the NAACL HLT 2010 Work-shop on Creating Speech and Language Data withAmazon?s Mechanical Turk, pages 35?40, Los An-geles, CA.Jisup Hong and Collin F. Baker.
2011.
How good isthe crowd at ?real?
WSD?
In Proceedings of the5th Linguistic Annotation Workshop, pages 30?37,Portland, Oregon, USA.Parisa Kordjamshidi, Steven Bethard, and Marie-Francine Moens.
2012.
Semeval-2012 task 3: Spa-tial role labeling.
In Proceedings of the First JointConference on Lexical and Computational Seman-tics, pages 365?373, Montr?eal, Canada.Egoitz Laparra and German Rigau.
2013.
Sources ofevidence for implicit argument resolution.
In Pro-ceedings of the 10th International Conference onComputational Semantics (IWCS 2013) ?
Long Pa-pers, pages 155?166, Potsdam, Germany.Bart Mellebeek, Francesc Benavent, Jens Grivolla,Joan Codina, Marta R Costa-Jussa, and RafaelBanchs.
2010.
Opinion mining of spanish cus-tomer comments with non-expert annotations on me-chanical turk.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, pages 114?121, Los Angeles, CA.Josef Ruppenhofer, Caroline Sporleder, R. Morante,Collin Baker, and Martha Palmer.
2010.
Semeval-2010 task 10: Linking events and their participantsin discourse.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 45?50, Up-psala, Sweden.Carina Silberer and Anette Frank.
2012.
Casting im-plicit role linking as an anaphora resolution task.In Proceedings of SEM 2012: The First Joint Con-ference on Lexical and Computational Semantics,pages 1?10, Montreal, Canada.230
