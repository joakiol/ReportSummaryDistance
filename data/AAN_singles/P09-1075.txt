Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 665?673,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPA Novel Discourse Parser Based onSupport Vector Machine ClassificationDavid A. duVerleNational Institute of InformaticsTokyo, JapanPierre & Marie Curie UniversityParis, Francedave@nii.ac.jpHelmut PrendingerNational Institute of InformaticsTokyo, Japanhelmut@nii.ac.jpAbstractThis paper introduces a new algorithm toparse discourse within the framework ofRhetorical Structure Theory (RST).
Ourmethod is based on recent advances in thefield of statistical machine learning (mul-tivariate capabilities of Support VectorMachines) and a rich feature space.
RSToffers a formal framework for hierarchicaltext organization with strong applicationsin discourse analysis and text generation.We demonstrate automated annotation ofa text with RST hierarchically organisedrelations, with results comparable to thoseachieved by specially trained human anno-tators.
Using a rich set of shallow lexical,syntactic and structural features from theinput text, our parser achieves, in lineartime, 73.9% of professional annotators?human agreement F-score.
The parser is5% to 12% more accurate than currentstate-of-the-art parsers.1 IntroductionAccording to Mann and Thompson (1988), allwell-written text is supported by a hierarchicallystructured set of coherence relations which reflectthe authors intent.
The goal of discourse parsingis to extract this high-level, rhetorical structure.Dependency parsing and other forms of syn-tactic analysis provide information on the gram-matical structure of text at the sentential level.Discourse parsing, on the other hand, focuseson a higher-level view of text, allowing someflexibility in the choice of formal representationwhile providing a wide range of applicationsin both analytical and computational linguistics.Rhetorical Structure Theory (Mann and Thomp-son, 1988) provides a framework to analyze andstudy text coherence by defining and applying a setof structural relations to composing units (?spans?
)of text.
Annotation of a text within the RSTformalism will produce a tree-like structure thatnot only reflects text-coherence but also providesinput for powerful algorithmic tools for tasks suchas text regeneration (Piwek et al, 2007).RST parsing can be seen as a two-step process:1.
Segmentation of the input text into elemen-tary discourse units (?edus?).2.
Generation of the rhetorical structure treebased on ?rhetorical relations?
(or ?coherencerelations?)
as labels of the tree, with the edusconstituting its terminal nodes.Mann and Thompson (1988) empirically estab-lished 110 distinct rhetorical relations, but pointedout that this set was flexible and open-ended.In addition to rhetorical relations, RST definesthe notion of ?nucleus?, the relatively moreimportant part of the text, and ?satellite?, whichis subordinate to the nucleus.
In Fig.
1, the left-most edu constitutes the satellite (indicated byout-going arrow), and the right-hand statementconstitutes the nucleus.
Observe that the nucleusitself is a compound of nucleus and satellite.Several attempts to automate discourse parsinghave been made.
Marcu and Soricut focussedon sentence-level parsing and developed twoprobabilistic models that use syntactic and lexicalinformation (Soricut and Marcu, 2003).
Althoughtheir algorithm, called ?SPADE?, does not producefull-text parse, it demonstrates a correlationbetween syntactic and discourse information, andtheir use to identify rhetorical relations even if nosignaling cue words are present.665RTEMPORALAfter plummet-ing 1.8% at onepoint during theday,CONTRASTthe composite re-bounded a little,but finished down5.52, at 461.70.Figure 1: Example of a simple RST tree (Source:RST Discourse Treebank (Carlson et al, 2001),wsj0667).To the best of our knowledge, Reitter?s (2003b)was the only previous research based exclusivelyon feature-rich supervised learning to producetext-level RST discourse parse trees.
However,his full outline for a working parser, using chart-parsing-style techniques, was never implemented.LeThanh et al (2004) proposed a multi-stepalgorithm to segment and organize text spans intotrees for each successive level of text organization:first at sentence level, then paragraph and finallytext.
The multi-level approach taken by theiralgorithm mitigates the combinatorial explosioneffect without treating it entirely.
At the text-level,and despite the use of beam search to explore thesolution space, the algorithm needs to produce andscore a large number of trees in order to extractthe best candidate, leading, in our experience, toimpractical calculation times for large input.More recently, Baldridge and Lascarides (2005)successfully implemented a probabilistic parserthat uses headed trees to label discourse relations.Restricting the scope of their research to texts indialog form exclusively, they elected to use themore specific framework of Segmented DiscourseRepresentation Theory (Asher and Lascarides,2003) instead of RST.In this paper, we advanced the state-of-the-artin general discourse parsing, with an implementedsolution that is computationally efficient and suf-ficiently accurate for use in real-time interactiveapplications.
The rest of this paper is organizedas follows: Section 2 describes the generalarchitecture of our system along with the choiceswe made with regard to supervised learning.Section 3 explains the different characteristics ofthe input text used to train our system.
Section 4presents our results, and Section 5 concludes thepaper.2 Building a Discourse Parser2.1 Assumptions and RestrictionsIn our work, we focused exclusively on the secondstep of the discourse parsing problem, i.e., con-structing the RST tree from a sequence of edus thathave been segmented beforehand.
The motivationfor leaving aside segmenting were both practical?
previous discourse parsing efforts (Soricut andMarcu, 2003; LeThanh et al, 2004) alreadyprovide alternatives for standalone segmentingtools ?
and scientific, namely, the greater need forimprovements in labeling.
Current state-of-the-artresults in automatic segmenting are much closerto human levels than full structure labeling (F-score ratios of automatic performance over goldstandard reported in LeThanh et al (2004): 90.2%for segmentation, 70.1% for parsing).Another restriction is to use the reduced setof 18 rhetorical relations defined in Carlsonet al (2001) and previously used by Soricutand Marcu (2003).
In this set, the 75 re-lations originally used in the RST DiscourseTreebank (RST-DT) corpus (Carlson et al,2001) are partitioned into 18 classes accord-ing to rhetorical similarity (e.g.
: PROBLEM-SOLUTION, QUESTION-ANSWER, STATEMENT-RESPONSE, TOPIC-COMMENT and COMMENT-TOPIC are all grouped under one TOPIC-COMMENT relation).
In accord with previousresearch (Soricut and Marcu, 2003; Reitter,2003b; LeThanh et al, 2004), we turned all n-ary rhetorical relations into nested binary relations(a trivial graph transformation), resulting in morealgorithmically manageable binary trees.
Finally,we assumed full conformity to the ?Principle ofsequentiality?
(Marcu, 2000), which guaranteesthat only adjacent spans of text can be putin relation within an RST tree, and drasticallyreduces the size of the solution space.2.2 Support Vector MachinesAt the core of our system is a set of classifiers,trained through supervised-learning, which, giventwo consecutive spans (atomic edus or RSTsub-trees) in an input document, will score thelikelihood of a direct structural relation as wellas probabilities for such a relation?s label andnuclearity.
Using these classifiers and a straight-forward bottom-up tree-building algorithm, wecan produce a valid tree close to human cross-666validation levels (our gold standard) in linear time-complexity (see Fig.
2).SVM ClassificationTraining Corpus(RST-TB)Test CorpusSegmentation(SPADE)PennTreebankTokenized EDUsEDUsLexicalizedSyntax TreesSyntax Parsing (Charniak's nlparse)Syntax TreesLexicalizationLexicalizationLexicalizedSyntax TreesSyntax TreesAlignmentFeature Extraction AlignmentFeature ExtractionSVM TrainingSVM Models (Binary and Multiclass)Bottom-up Tree ConstructionScored RS sub-treesRhetorical Structure TreeTokenizationTokenized EDUsFigure 2: Full system workflow.In order to improve classification accuracy, it isconvenient to train two separate classifiers:?
S: A binary classifier, for structure (existenceof a connecting node between the two inputsub-trees).?
L: A multi-class classifier, for rhetoricalrelation and nuclearity labeling.Using our reduced set of 18 super-relations and considering only validnuclearity options (e.g., (ATTRIBUTION, N, S)and (ATTRIBUTION, S, N), but not(ATTRIBUTION, N, N), as ATTRIBUTION isa purely hypotactic relation group), we come upwith a set of 41 classes for our algorithm.Support Vector Machines (SVM) (Vapnik,1995) are used to model classifiers S and L. SVMrefers to a set of supervised learning algorithmsthat are based on margin maximization.
Givenour specific type of classification problem, SVMsoffer many properties of particular interest.
First,as maximum margin classifiers, they sidestepthe common issue of overfitting (Scholkopf etal., 1995), and ensure a better control overthe generalization error (limiting the impact ofusing homogeneous newspaper articles that couldcarry important biases in prose style and lexicalcontent).
Second, SVMs offer more resilience tonoisy input.
Third, depending on the parametersused (see the use of kernel functions below),training time complexity?s dependence on featurevector size is low, in some cases linear.
This makesSVM well-fitted to treat classification problemsinvolving relatively large feature spaces such asours (?
105 features).
Finally, while mostprobabilistic classifiers, such as Naive Bayes,strongly assume feature independence, SVMsachieve very good results regardless of inputcorrelations, which is a desirable property forlanguage-related tasks.SVM algorithms make use of the ?kerneltrick?
(Aizerman et al, 1964), a method for usinglinear classifiers to solve non-linear problems.Kernel methods essentially map input data toa higher-dimensional space before attempting toclassify them.
The choice of a fitting kernelfunction requires careful analysis of the data andmust weigh the effects on both performance andtraining time.
A compromise needs to be foundduring evaluation between the general efficiencyof non-linear kernels (such as polynomial orRadial Basis Function) and low time-complexityof using a linear function (see Sect.
4).Because the original SVM algorithms build bi-nary classifiers, multi-label classification requiressome adaptation.
A possible approach is toreduce the multi-classification problem through aset of binary classifiers, each trained either ona single class (?one vs.
all?)
or by pair (?onevs.
one?).
Recent research suggests keeping theclassification whole, with a reformulation of theoriginal optimization problem to accommodatemultiple labels (?C & S?)
(Crammer and Singer,2002).2.3 Input Data and Feature ExtractionBoth S and L classifiers are trained usingmanually annotated documents taken from theRST-DT corpus.
Optimal parameters (whenapplicable) for each kernel function are obtainedthrough automated grid search with n-fold cross-validation (Staelin, 2003) on the training corpus,while a separate test set is used for performanceevaluation.
In training mode, classificationinstances are built by parsing manually annotatedtrees from the RST-DT corpus paired withlexicalized syntax trees (LS Trees) for eachsentence (see Sect.
3).
Syntax trees are taken667directly from the Penn Treebank corpus (whichcovers a superset of the RST-DT corpus), then?lexicalized?
(i.e.
tagged with lexical ?heads?
oneach internal node of the syntactic tree) using aset of canonical head-projection rules (Magerman,1995; Collins, 2003).
Due to small differencesin the way they were tokenized and pre-treated,rhetorical tree and LST are rarely a perfect match:optimal alignment is found by minimizing editdistances between word sequences.2.4 Tree-building AlgorithmBy repeatedly applying the two classifiers andfollowing a naive bottom-up tree-constructionmethod, we are able to obtain a globally satisfyingRST tree for the entire text with excellent time-complexity.The algorithm starts with a list of all atomicdiscourse sub-trees (made of single edus in theirtext order) and recursively selects the best matchbetween adjacent sub-trees (using binary classifierS), labels the newly created sub-tree (using multi-label classifier L) and updates scoring for S, untilonly one sub-tree is left: the complete rhetoricalparse tree for the input text.It can be noted that, thanks to the principleof sequentiality (see Sect.
2.1), each time twosub-trees are merged into a new sub-tree, onlyconnections with adjacent spans on each side areaffected, and therefore, only two new scores needto be computed.
Since our SVM classifiers workin linear time, the overall time-complexity of ouralgorithm is O(n).3 FeaturesInstrumental to our system?s performance isthe choice of a set of salient characteristics(?features?)
to be used as input to the SVMalgorithm for training and classification.
Once thefeatures are determined, classification instancescan be formally represented as a vector of valuesinR.We use n-fold validation on S and L classifiersto assess the impact of some sets of featureson general performance and eliminate redundantfeatures.
However, we worked under the (verified)assumption that SVMs?
capacity to handle high-dimensional data and resilience to input noise limitthe negative impact of non-useful features.In the following list of features, obtainedempirically by trial-and-error, features suffixed by?S[pan]?
are sub-tree-specific features, symmetri-cally extracted from both left and right candidatespans.
Features suffixed by ?F[ull]?
are a functionof the two sub-trees considered as a pair.
Multi-label features are turned into sets of binary valuesand trees use a trivial fixed-length binary encodingthat assumes fixed depth.3.1 Textual OrganizationAs evidenced by a number of discourse-parsing ef-forts focusing on intra-sentential parsing (Marcu,2000; Soricut and Marcu, 2003), there is a strongcorrelation between different organizational levelsof textual units and sub-trees of the RST treeboth at the sentence-level and the paragraph level.Although such correspondences are not a rule(sentences and particularly paragraphs, can oftenbe found split across separate sub-trees), theyprovide valuable high-level clues, particularly inthe task of scoring span relation priority (classifierS):Ex.
: ?Belong to same sentence?F, ?Belongto same paragraph?F, ?Number of paragraphboundaries?S, ?Number of sentence bound-aries?S.
.
.As pointed out by Reitter (Reitter, 2003a), wecan hypothesize a correlation between span lengthand some relations (for example, the satellite in aCONTRAST relation will tend to be shorter thanthe nucleus).
Therefore, it seems useful to encodedifferent measures of span size and positioning,using either tokens or edus as a distance unit:Ex.
: ?Length in tokens?S, ?Length in edus?S,?Distance to beginning of sentence in tokens?S,?Size of span over sentence in edus?S, ?Distanceto end of sentence in tokens?S.
.
.In order to better adjust to length variationsbetween different types of text, some features inthe above set are duplicated using relative, ratherthan absolute, values for positioning and distance.3.2 Lexical Clues and PunctuationWhile not always present, discourse markers(connectives, cue-words or cue-phrases, etc) havebeen shown to give good indications on discoursestructure and labeling, particularly at the sentence-level (Marcu, 2000).
We use an empirical n-gram dictionary (for n ?
{1, 2, 3}) built from thetraining corpus and culled by frequency.
As anadvantage over explicit cue-words list, this method668also takes into account non-lexical signals suchas punctuation and sentence/paragraph boundaries(inserted as artificial tokens in the original textduring input formatting) which would otherwisenecessitate a separate treatment.We counted and encoded n-gram occurrenceswhile considering only the first and last n tokensof each span.
While raising the encoding sizecompared to a ?bag of words?
approach, thisgave us significantly better performance (classifieraccuracy improved by more than 5%), particularlywhen combined with main constituent features(see Sect.
3.5 below).
This is consistent with thesuggestion that most meaningful rhetorical signalsare located on the edge of the span (Schilder,2002).We validated this approach by comparingit to results obtained with an explicit listof approximately 300 discourse-signaling cue-phrases (Oberlander et al, 1999): performancewhen using the list of cue-phrases alone wassubstantially lower than n-grams.3.3 Simple Syntactic CluesIn order to complement signal detection and toachieve better generalization (smaller dependencyon lexical content), we opted to add shallowsyntactic clues by encoding part-of-speech (POS)tags for both prefix and suffix in each span.
Usingprefixes or suffixes of length higher than n = 3 didnot seem to improve performance significantly.3.4 Dominance SetsA promising concept introduced by Soricut andMarcu (2003) in their sentence-level parser is theidentification of ?dominance sets?
in the syntaxparse trees associated to each input sentence.
Forexample, it could be difficult to correctly identifythe scope of the ATTRIBUTION relation in theexample shown in Fig.
3.
By using the associatedsyntax tree and studying the sub-trees spanned byeach edu (see Fig.
4), it is possible to quickly infera logical nesting order (?dominance?)
betweenthem: 1A > 1B > 1C.
This order allows usto favor the relation between 1B and 1C over arelation between 1A and 1B, and thus helps usto make the right structural decision and pick theright-hand tree on Fig.
3.In addition to POS tags around the frontierbetween each dominance set (see colored nodesin Fig.
4), Soricut and Marcu (2003) note that inorder to achieve good results on relation labeling,[Shoney?s Inc. said]1A [it will report a write-offof $2.5 million, or seven cents a share, for itsfourth quarter]1B [ended yesterday.
]1C (wsj0667)ELABORATIONRATTRIBUTION1A 1B1CRATTRIBUTION1AELABORATION1B 1CFigure 3: Two possible RST parses for a sentence.it is necessary to also consider lexical informa-tion (obtained through head word projection ofterminal nodes to higher internal nodes).
Basedon this definition of dominance sets, we include aset of syntactic, lexical and tree-structural featuresthat aim at a good approximation of Marcu &Soricut?s rule-based analysis of dominance setswhile keeping parsing complexity low.Ex.
: ?Distance to root of the syntax tree?S,?Distance to common ancestor in the syn-tax tree?S, ?Dominating node?s lexical headin span?S, ?Common ancestor?s POS tag?F,?Common ancestor?s lexical head?F, ?Domi-nating node?s POS tag?F (diamonds in Figure4, ?Dominated node?s POS tag?F (circles inFigure 4), ?Dominated node?s sibling?s POStag?F (rectangles in Figure 4), ?Relative positionof lexical head in sentence?S.
.
.3.5 Strong Compositionality CriterionWe make use of Marcu?s ?Strong CompositionalityCriterion?
(Marcu, 1996) through a very simpleand limited set of features, replicating shallow lex-ical and syntactic features (previously described inSections 3.2 and 3.3) on a single representativeedu (dubbed main constituent) for each span.Main constituents are selected recursively usingnuclearity information.
We purposely keepthe number of features extracted from mainconstituents comparatively low (therefore limitingthe extra dimensionality cost), as we believe ouruse of rhetorical sub-structures ultimately encodesa variation of Marcu?s compositionality criterion(see Sect.
3.6).3.6 Rhetorical Sub-structureA large majority of the features considered so farfocus exclusively on sentence-level information.6691A.1B.1C.SNP-SBJNPNNPShoneyPOS'sNNPInc.VPVBDsaidSBARSNP-SBJPRPitVPMDwillVPVBreportNPNPDTaNNwrite-offPPINofNPNPQP$$CD2.5CDmillion,,CCorNPNPCDsevenNNScentsNP-ADVDTaNNshare,,PPINforNPNPPRP$itsJJfourthNNquarterVPVBNendedNP-TMPNNyesterday..(said)(will)(quarter)(ended)(quarter)(said)(said)(will)(it)Figure 4: Using dominance sets to prioritize structural relations.Circled nodes define dominance sets and studying the frontiers between circles and diamonds gives us a dominance orderbetween each of the three sub-trees considered: 1A > 1B > 1C.
Head words obtained through partial lexicalization have beenadded between parenthesis.In order to efficiently label higher-level relations,we need more structural features that can guidegood classification decision on large spans.
Hencethe idea of encoding each span?s rhetorical subtreeinto the feature vector seems natural.Beside the role of nuclearity in the sub-structureimplied by Marcu?s compositionality criterion (seeSect.
3.5), we expect to see certain correlationsbetween the relation being classified and relationpatterns in either sub-tree, based on theoreticalconsiderations and practical observations.
Theoriginal RST theory suggests the use of ?schemas?as higher-order patterns of relations motivated bylinguistic theories and verified through empiricalanalysis of annotated trees (Mann and Thompson,1988).
In addition, some level of correlationbetween relations at different levels of the treecan be informally observed throughout the corpus.This is trivially the case for n-ary relationssuch as LIST which have been binarized in ourrepresentation, i.e., the presence of several LISTrelations in rightmost nodes of a subtree greatlyincreases the probability that the parent relationmight be a LIST itself.4 Evaluation4.1 General ConsiderationsIn looking to evaluate the performance of oursystem, we had to work with a number ofconstraints and difficulties tied to variations in themethodologies used across past works, as wellas a lack of consensus with regard to a commonevaluation corpus.
In order to accommodate thesedivergences while providing figures to evaluateboth relative and absolute performance of ouralgorithm, we used three different test sets.Absolute performance is measured on the officialtest subset of the RST-DT corpus.
A similarlyavailable subset of doubly-annotated documentsfrom the RST-DT is used to compare resultswith human agreement on the same task.
Lastly,performance against past algorithms is evaluatedwith another subset of the RST-DT, such as usedby LeThanh et al (2004) in their own evaluation.4.2 Raw SVM ClassificationAlthough our final goal is to achieve goodperformance on the entire tree-building task, auseful intermediate evaluation of our system canbe conducted by measuring raw performance ofSVM classifiers.
Binary classifier S is trainedon 52,683 instances (split approximately 1/3,2/3 between positive and negative examples),extracted from 350 documents, and tested on8,558 instances extracted from 50 documents.
Thefeature space dimension is 136,987.
Classifier Lis trained on 17,742 instances (labeled across 41classes) and tested on 2,887 instances, of samedimension as for S.Classifier: Binary (S) Multi-label (L) ReitterKernel Linear Polyn.
RBF Linear RBF RBFSoftware liblinear svmlight svmlight svmmulticlass libsvm svmlightMulti-label - C&S 1 vs. 1 1 vs. allTraining time 21.4s 5m53s 12m 15m 23m 216mAccuracy 82.2 85.0 82.9 65.8 66.8 61.0Table 1: SVM Classifier performance.
Regarding?Multi-label?, see Sect.
2.2.The noticeably good performance of linear670kernel methods in the results presented in Table 1compared to more complex polynomial and RBFkernels, would indicate that our data separatesfairly well linearly: a commonly observed effectof high-dimensional input (Chen et al, 2007) suchas ours (> 100,000 features).A baseline for absolute comparison on themulti-label classification task is given by Reit-ter (2003a) on a similar classifier, which assumesperfect segmentation of the input, as ours does.Reitter?s accuracy results of 61% match a smallerset of training instances (7976 instances from240 documents compared to 17,742 instances inour case) but with considerably less classes (16rhetorical relation labels with no nuclearity, asopposed to our 41 nuclearized relation classes).Based on these differences, this sub-component ofour system, with an accuracy of 66.8%, seems toperform well.Taking into account matters of performance andruntime complexity, we selected a linear kernel forS and an optimally parameterized RBF kernel forL, using modified versions of the liblinear andlibsvm software packages.
All further evaluationsnoted here were conducted with these.4.3 Full System PerformanceA measure of our full system?s performance isrealized by comparing structure and labeling ofthe RST tree produced by our algorithm to thatobtained through manual annotation (our goldstandard).
Standard performance indicators forsuch a task are precision, recall and F-score asmeasured by the PARSEVAL metrics (Black et al,1991), with the specific adaptations to the case ofRST trees made by Marcu (2000, page 143-144).Our first evaluation (see Table 2) was conductedusing the standard test subset of 41 files providedby the RST-DT corpus.
In order to moreaccurately compare our results to the gold standard(defined as manual agreement between humanannotators), we also evaluated performance usingthe 52 doubly-annotated files present in the RST-DT as test set (see Table 3).
In each case, theremaining 340?350 files are used for training.For each corpus evaluation, the system isrun twice: once using perfectly-segmented in-put (taken from the RST-DT), and once usingthe output of the SPADE segmenter (Soricut andMarcu, 2003).
The first measure gives us a goodidea of our system?s optimal performance (givenoptimal input), while the other gives us a morereal-world evaluation, apt for comparison withother systems.In each case, parse trees are evaluated using thefour following, increasingly complex, matchingcriteria: blank tree structure (?S?
), tree structurewith nuclearity (?N?
), tree structure with rhetoricalrelations (?R?)
and our final goal: fully labeledstructure with both nuclearity and rhetoricalrelation labels (?F?).Segment.
Manual SPADES N R F S N R FPrecision 83.0 68.4 55.3 54.8 69.5 56.1 44.9 44.4Recall 83.0 68.4 55.3 54.8 69.2 55.8 44.7 44.2F-Score 83.0 68.4 55.3 54.8 69.3 56.0 44.8 44.3Table 2: Discourse-parser evaluation dependingon segmentation using standard test subsetSystem performance Human agreementSegment.
Manual SPADE -S N R F S N R F S N R FPrecision 84.1 70.6 55.6 55.1 70.6 58.1 46.0 45.6 88.0 77.5 66.0 65.2Recall 84.1 70.6 55.6 55.1 71.2 58.6 46.4 46.0 88.1 77.6 66.1 65.3F-Score 84.1 70.6 55.6 55.1 70.9 58.3 46.2 45.8 88.1 77.5 66.0 65.3Table 3: Comparing to human-agreement de-pending on segmentation using doubly-annotatedsubsetNote: When using perfect segmentation, preci-sion and recall are identical since both trees havesame number of constituents.4.4 Comparison with other AlgorithmsTo the best of our knowledge, only two fullyfunctional text-level discourse parsing algorithmsfor general text have published their results:Marcu?s decision-tree-based parser (Marcu, 2000)and the multi-level rule-based system built byLeThanh et al (2004).
For each one, evaluationwas conducted on a different corpus, usingunavailable documents for Marcu?s and a selectionof 21 documents from the RST-DT (distinctfrom RST-DT?s test set) for LeThanh?s.
Wetherefore retrained and evaluated our classifier,using LeThanh?s set of 21 documents as testingsubset (and the rest for training) and comparedperformance (see Table 4).
In order to achievethe most uniform conditions possible, we useLeThanh?s results on 14 classes (Marcu?s use 15,ours 18) and select SPADE segmentation figuresfor both our system and Marcu?s (LeThanh?s671system uses its own segmenter and does notprovide figures for perfectly segmented input).Structure Nuclearity RelationsAlgorithm M lT dV M lT dV M lT dVPrecision 65.8 54.5 72.4 54.0 47.8 57.8 34.3 40.5 47.8Recall 34.0 52.9 73.3 21.6 46.4 58.5 13.0 39.3 48.4F-score 44.8 53.7 72.8 30.9 47.1 58.1 18.8 39.9 48.1Table 4: Side-by-side text-level algorithms com-parison: Marcu (M), LeThanh et al (lT) and ours(dV)Some discrepancies between reported humanagreement F-scores suggest that, despite ourbest efforts, evaluation metrics used by eachauthor might differ.
Another explanation may liein discrepancies between training/testing subsetsused.
In order to take into account possiblyvarying levels of difficulties between corpora, wetherefore divided each F-score by the value forhuman agreement, such as measured by eachauthor (see Table 5).
This ratio should give us afairer measure of success for the algorithm takinginto account how well it succeeds in reaching near-human level.Structure Nuclearity RelationsAlgorithm M lT dV M lT dV M lT dVF?scorealgoF?scorehuman 56.0 73.9 83.0 42.9 71.8 75.6 25.7 70.1 73.9Table 5: Performance scaled by human agreementscores: Marcu (M), LeThahn et al (lT) and ours(dV)Table 5 shows 83%, 75.6% and 73.9% of humanagreement F-scores in structure, nuclearity andrelation parsing, respectively.
Qualified by the(practical) problems of establishing comparisonconditions with scientific rigor, the scores indicatethat our system outperforms the previous state-of-the-art (LeThanh?s 73.9%, 71.8% and 70.1%).As suggested by previous research (Soricut andMarcu, 2003), these scores could likely befurther improved with the use of better-performingsegmenting algorithms.
It can however be notedthat our system seems considerably less sensitiveto imperfect segmenting than previous efforts.
Forinstance, when switching from manual segmen-tation to automatic, our performance decreasesby 12.3% and 12.9% (respectively for structureand relation F-scores) compared to 46% and 67%for Marcu?s system (LeThanh?s performance onperfect input is unknown).5 Conclusions and Future WorkIn this paper, we have shown that it is possibleto build an accurate automatic text-level discourseparser based on supervised machine-learningalgorithms, using a feature-driven approach anda manually annotated corpus.
Importantly, oursystem achieves its accuracy in linear complexityof the input size with excellent runtime per-formance.
The entire test subset in the RST-DT corpus could be fully annotated in a matterof minutes.
This opens the way to manynovel applications in real-time natural languageprocessing and generation, such as the RST-basedtransformation of monological text into dialoguesacted by virtual agents in real-time (Hernault et al,2008).Future directions for this work notably includea better tree-building algorithm, with improvedexploration of the solution space.
Borrowingtechniques from generic global optimization meta-algorithms such as simulated annealing (Kirk-patrick et al, 1983) should allow us to betterdeal with issues of local optimality while retainingacceptable time-complexity.A complete online discourse parser, incorpo-rating the parsing tool presented above com-bined with a new segmenting method has sincebeen made freely available at http://nlp.prendingerlab.net/hilda/.AcknowledgementsThis project was jointly funded by PrendingerLab (NII, Tokyo) and the National Institutefor Informatics (Tokyo), as part of a MOU(Memorandum of Understanding) program withPierre & Marie Curie University (Paris).672ReferencesM.A.
Aizerman, E.M. Braverman, and L.I.
Rozonoer.1964.
Theoretical foundations of the potentialfunction method in pattern recognition learning.Automation and Remote Control, 25(6):821?837.N.
Asher and A. Lascarides.
2003.
Logics ofconversation.
Cambridge University Press.J.
Baldridge and A. Lascarides.
2005.
Probabilistichead-driven parsing for discourse structure.
In Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning, volume 96, page 103.E.
Black, S. Abney, S. Flickenger, C. Gdaniec,C.
Grishman, P. Harrison, D. Hindle, R. Ingria,F.
Jelinek, J. Klavans, M. Liberman, et al 1991.Procedure for quantitatively comparing the syntacticcoverage of English grammars.
Proceedings of theworkshop on Speech and Natural Language, pages306?311.L.
Carlson, D. Marcu, and M.E.
Okurowski.
2001.Building a discourse-tagged corpus in the frame-work of Rhetorical Structure Theory.
Proceedingsof the Second SIGdial Workshop on Discourse andDialogue-Volume 16, pages 1?10.D.
Chen, Q.
He, and X. Wang.
2007.
Onlinear separability of data sets in feature space.Neurocomputing, 70(13-15):2441?2448.M.
Collins.
2003.
Head-Driven Statistical Modelsfor Natural Language Parsing.
ComputationalLinguistics, 29(4):589?637.K.
Crammer and Y.
Singer.
2002.
On the algorithmicimplementation of multiclass kernel-based vectormachines.
The Journal of Machine LearningResearch, 2:265?292.H.
Hernault, P. Piwek, H. Prendinger, and M. Ishizuka.2008.
Generating dialogues for virtual agents usingnested textual coherence relations.
Proceedingsof the 8th International Conference on IntelligentVirtual Agents (IVA?08), LNAI, 5208:139?145, Sept.S.
Kirkpatrick, CD Gelatt, and MP Vecchi.
1983.Optimization by Simulated Annealing.
Science,220(4598):671?680.H.
LeThanh, G. Abeysinghe, and C. Huyck.
2004.Generating discourse structures for written texts.Proceedings of the 20th international conference onComputational Linguistics.D.M.
Magerman.
1995.
Statistical decision-treemodels for parsing.
Proceedings of the 33rdannual meeting on Association for ComputationalLinguistics, pages 276?283.W.C.
Mann and S.A. Thompson.
1988.
Rhetoricalstructure theory: Toward a functional theory of textorganization.
Text, 8(3):243?281.D.
Marcu.
1996.
Building Up Rhetorical StructureTrees.
Proceedings of the National Conference onArtificial Intelligence, pages 1069?1074.D.
Marcu.
2000.
The theory and practice of discourseparsing and summarization.
MIT Press.J.
Oberlander, J.D.
Moore, J. Oberlander, A. Knott,and J. Moore.
1999.
Cue phrases in discourse:further evidence for the core: contributor distinction.Proceedings of the 1999 Levels of Representation inDiscourse Workshop (LORID?99), pages 87?93.P.
Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.2007.
Generating dialogues between virtual agentsautomatically from text.
Proceedings of the7th International Conference on Intelligent VirtualAgents (IVA ?07), LNCS, 4722:161.D.
Reitter.
2003a.
Rhetorical Analysis with Rich-Feature Support Vector Models.
UnpublishedMaster?s thesis, University of Potsdam, Potsdam,Germany.D.
Reitter.
2003b.
Simple Signals for ComplexRhetorics: On Rhetorical Analysis with Rich-Feature Support Vector Models.
Language, 18(52).F.
Schilder.
2002.
Robust discourse parsing viadiscourse markers, topicality and position.
NaturalLanguage Engineering, 8(2-3):235?255.B.
Scholkopf, C. Burges, and V. Vapnik.
1995.
Ex-tracting Support Data for a Given Task.
KnowledgeDiscovery and Data Mining, pages 252?257.R.
Soricut and D. Marcu.
2003.
Sentencelevel discourse parsing using syntactic and lexicalinformation.
Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, 1:149?156.C.
Staelin.
2003.
Parameter selection for supportvector machines.
Hewlett-Packard Company, Tech.Rep.
HPL-2002-354R1.V.N.
Vapnik.
1995.
The nature of statistical learningtheory.
Springer-Verlag New York, Inc., New York,NY, USA.673
