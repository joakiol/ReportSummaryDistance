ADAPT IVE  LANGUAGE MODEL ING US ING MIN IMUMDISCRIMINANT EST IMATION*S. De l la  P ie t ra ,  V. De l la  P ie t ra ,  R. L. Mercer ,  S. RoukosCont inuous  Speech  Recogn i t ion  Group,Thomas  J.  Watson  Research  CenterP.
O.
Box  704, York town Heights ,  NY  10598ABSTRACTWe present an algorithm to adapt a n-gramlanguage model to a document as it is dictated.The observed partial document is used to esti-mate a unigram distribution for the words thatalready occurred.
Then, we find the closest n-gram distribution to the static n.gram distribu-tion (using the discrimination i formation dis-tance measure) and that satisfies the marginalconstraints derived from the document.
Theresulting minimum discrimination i formationmodel results in a perplexity of 208 instead of290 for the static trigram model on a documentof 321 words.1 INTRODUCTIONStatistical n-gram language models are useful for speechrecognition and language translation systems becausethey provide an a-priori probability of a word sequence;these language models improve the accuracy of recog-nition or translation by a significant amount.
In thecase of trigram (n = 3) and blgram (n = 2) languagemodels, the probability of the next word conditionedon the previous words is estimated from a large corpusof text.
The resulting static language models (SLM)have fixed probabilities that are independent of thedocument being predicted.To improve the language model (LM), one can adaptthe probabilities of the language model to match thecurrent document more closely.
The partially dictated(in the case of speech recognition) document providessignificant clues about what words are more llke\]y tobe used next.
One expects many words to be bursty.For example, if in the early part of a document heword lax has been used then the probability that itwilt be used again in the same document is significantly*PAPER SUBMITTED TO THE ICASSP92PROCEEDINGS.higher than if it had not occurred yet.
In addition, ifthe words in the early part of a document suggest hatthe current document is from a particular subdomain,then we may expect other related words to occur ata higher rate than the static model may suggest.
Forexample, the words "inspection, fire, and insurance"suggest an insurance report domain, and therefore in-creased probabilities to words such as "stairwell andelectricar' .Assume that from a partial document, denoted byh, we have an estimate of the unigram distributionpa(w I h) that a word w may be used in the remain-ing part of the document.
We will denote pa(w I h)by d(w), keeping in mind that this dynamic unigramdistribution is continuously updated as the documentis dictated and is estimated for a subset of R words ofthe vocabulary.
(Typically R is on the order of a doc-ument size of about a few hundred words as comparedto a vocabulary size of 20,000 words.)
In general, thedynamic unigram distribution will be different fromthe static marginal unigram distribution, denoted byp~(w).
In this paper, we propose a method for adapt-\]ng the language model so that its marginal unigramdistribution matches the desired dynamic unigram dis-tribution d(w).The proposed approach consists of finding the modelthat requires the least pertubation of the static modeland satisfies the set of constraints that have been de-rived from the partially observed ocument.
By leastpertubation we mean that the new model is closest tothe static model, p~, using the non-symmetric Kullback-Liebler distortion measure (also known as dlscrimina-tion information, relative entropy, etc.).
The minimumdiscrimination information (MDI) p* distribution min-imizes:iover all p that satisfy a set of R linear constrailtts.In this paper, we consider marginal constraints of the103formp(i)  = d,iECrwhere we are summing over all events i in the set C~that correspond to the r-th constraint and d, is the de-sired value (for r = 1, 2, ..., R).
In our case, the events icorrespond to bigrams, (nq, w2), and the desired valuefor the r-th constraint, d,, is the marginal unigramprobability, d(w,), for a word w,.The idea of using a window of the previous N words,called a cache, to estimate dynamic frequencies for aword was proposed in \[5\] for the case of a tri-part-of-speech model and in \[6\] for a bigram model.
In \[4\] atrigram language was estimated from the cache and in-terpolated with the static trlgram model to yield about20% lower perplexity and from 5% to 25% lower recog-nition error rate on documents ranging in length from100 to 800 words.3 ALTERNATING MIN IMIZAT IONStarting with an initial estimate of the factors, thefollowing iterative algorithm is guaranteed to convergeto the optimum distribution.
At each iteration j,  picka constraint rj and adjust the corresponding factor sothat the constraint is satisfied.
In the case of marginalconstraints, the update is:new = fold dr,frj ~rj p j - l (Cr j  )where pJ-~(C,j) is the marginal of the previous estl-mate and d~j is the desired marginal.
This iteratlve al-gorithm cycles through the constraints repeatadly un-til convergence hence the name alternating (thru theconstraints) minimization.
It was proposed by Darrochand Ratcliff in \[3\].
A proof of convergence for linearconstraints i given in \[2\] .2 MIN IMUM DISCRIMINAT IONINFORMATIONThe discrimination information can be written as:D(p, ps) = -- EPlogps + EP logp  (1)= I tp , (p) -  H(p) k 0 (2)where Rps(p ) is the bit rate in transmitting source pwith model Ps and H(p) is the entropy of source p. TheMDI distribution p* satisfies the following Pythagoreaninequality:D(p,p,) > D(p,p*) + D(p*,ps)for all distributions p in the set PR of distributions thatsatisfy the R constraints.
So if we have an accurateestimate of the constraints then using the MDI distri-bution will result in a lower error by at least D(p*, p~).The'.
MDI distribution is the Maximum Entropy(ME) distribution if the static model is the uniformdistribution.Using Lagrange multipliers and differentiating withrespect o pi the probability of the i-th event, we findthat the optimum must have the formp~ = p,ifi~f~2...flnwhere the factors fir are 1 if event i is not in the con-straint set C, or some other value .f, if event i belongsto constraint set C,.
So the MDI distribution is speci-fied by the 17 factors .f,, r = 1, 2, ..., R, that correspondto the R constraints, in addition to the original staticmodel.1044 ME CACHEWe have applied the above approach to adapting a bi-gram model; we call the resulting model the ME cache.Using a cache window of the previous N words, we es-timate the desired unigram probability of al l / : /wordsthat have occurred in the cache by:d(w) = .
\de(w)where Ac is an adjustment factor taken to be the prob.-ability that the next word is already in the cache and.fc is the observed frequency of a word in the cache.Since any event (wl, wu) participates in 2 constraintsone for the left marginal d(wl) and the other for theright marginal d(w~) there are 2//-,t-1 constraint, a leftand right marginal for each word in the cache and theoverall normalization, the ME bigram cache model isgiven by:We require the left and right marginals to be equal toget a st~ttionary model.
(Since all events participate inthe normalization that factor is absorbed in the othertwo.
)The iterations fall into two groups: those in whicha left marginal is adjusted and those in which a rightmarginal is adjusted.
In each of these iterations, weadjust two factors simultaneously: one for the desiredunigram probability d(w) and the other so that theresulting ME model is a normalized istribution.
Theupdate for left marginals ispJ(wl, W2) : pJ-l(wl, W2)aj.~jwhere aj and sj are adjustments given by:1 - d(wj)8j = 1 -- p j - - l (w j ,  .)d(w?
)aj = s jp j _ l (w j ,  .
)where pJ-l(wj, .)
denotes the left marginal of the (j -1)-th estimate of the ME distribution and wj is theword that corresponds to the selected constraint at thej - th iteration.
Similar equations can be derived for theupdates for the right marginals.
The process is startedwith p0(w,, =Note that the marginM pJ(w,.)
can be computedby using R additions and multiplications.
The algo-rithm requires order//2 operation to cycle thru all con-straints once.
R is typically few hundred compared tothe vocabulary size V which is 20,000 in our case.
Wehave found that about 3 to 5 iterations are sufficientto achieve convergence.5 EXPERIMENTAL  RESULTSUsing a cache window size of about 700 words, weestimated a desired unigram distribution and a cor-responding ME bigram distribution with an MDI ofabout 2.2 bits (or 1.1 bits/word).
Since the unigramdistribution may not be exact, we do not expect to re-duce our perplexity on the next sentence by a factorlarger than 2.1 = 2 El.
The actuM reduction was afactor of 1.5 = 2 o.62 on the next 93 words of the docu-ment.
For a smMler cache size the discrepancy betweenthe MDI and actual perplexity reduction is larger.To evaluate the ME cache model we compared it tothe trigram cache model and the static trigram model.In all models we use linear interpolation between thedynamic and static components as:p(w3lwl, w2) = Acp~(w31w~, wu)-I-(1-A~)p,(w31,vl, w2)where A~ = 0.2.
The static and cache trigram prob-abilities use the usual interpolation between unigram,bigram, and trigram frequencies \[t\].
The cache trlgramprobability p~ is given by:pc( w3\]zol, w2 ) = ,\lfcl ( w3 )-t- )~2/c2( w31w2 )T A3f c3( w3lwlwhere fci are  frequencies estimated from the cache win-dow.
The interpolating weights are A1 = 0.4, ),2 = 0.5,~nd A3 = 0.1.
For the ME cache we replace the dy-namic unigram frequency f~l(w3) by the ME condi-tional bigram probability pme(W3lW2) given by:"o~, (w3 )p,( w31w2 )PmdW3lw~) = E~ o~,(w)p,(wlw2 ), w2)105Note that the sum in the denominator is order R sincethe factors are unity for the words that are not in thecache.In 'Fable 1, we compare the static, the ME cache,and the trigram cache models on three documents.Both cache models improve on the static.
The MEand trigram cache are fairly close as would be expectedsince they both have the same dynamic unigram dis-tribution.
The second experiment illustrates how theyare different.Document Words Static ME \]?igramCache CacheT1 321 290 208 218T3 426 434 291 300E1 814 294 175 182Table 1.
Perplexity on three documents.We compared the ME cache and the trigram cacheon 2 non-senslcal sentences made up from words thathave occurred in the first sentence of a document.
The2 sentences are:?
SI: the letter fire to to to?
S2: building building building buildingTable 2 shows the perplexity of each sentence at 2points in the document history: one after the first sen-tence (of length 33 words) is in the cache and the sec-ond after 10 sentences (203 words) are in the cache.We can see that the trigram cache can make some rarebigrams (wl, w~) more likely if both wx and w2 havealready occurred due to a term of the form d(wt)d(w2)whereas the ME cache still has the factor p,(wl, w2)which will tend to keep a rare bigram somewhat \]essprobM)\]e. This is particular\]y pronounced for $2, wherewe expect d(building) to be quite accurate after 10 sen-tences, the ME cache penalizes the unlikely bigram bya factor of about 13 over the trigram cache.SentenceSlSlS2S2CacheSize3320333203Trigram MECache Cache213 268417 672245 665212 2963Table 2.
Trigram and ME cache perplexity.6 CONCLUSIONThe MDI approach to adapting a language model canresult in significant perplexity reduction without aleak-age in the bigram probability model.
We expect hisfact to be important in adapting to a new domainwhere the unigram distribution d(w) can be estimatedfrom possibly tens of documents.
We are currentlypursuing such experiments.REFERENCES[11 Bahl, L., Jelinek, F., and Mercer, R.,A Statisti-cal Approach to Continuous Speech Recognition,IEEE Trans.
on PAMI, 1983.
[2] Csiszar, I., and bongo, G., In]ormation Geometryand Alternating Minimization Procedures, Statis-tics and Decisions, Supplement Issue 1:205-237,1984.
[3] Darroch, J.N., Ratcliff, D. Generalized lterativeScaling for Log-Linear Models, The Annals ofMathematical Statistics, Vol.
43, pp.
1470-1480,1972.
[4] Jelinek, F., Merialdo, B., Roukos, S., and Strauss,M., A Dynamic Language Model for Speech Recog-nition, Proceedings of Speech and Natural Lan-guage DARPA Workshop, pp.
293-295, Feb.
1991.
[5] Kuhn, R., Speech Recognition and the Frequencyof Recently Used Words: a Modified MarkovModel for Natural Language, Proceedings ofCOL-ING Budapest, Vol.
1, pp.
348-350, 1988.
Vol.
1July 1988[6] Kupiec, J., Probabilistic Models of Short andLong Distance Word Dependencies in RunningText, Proceedings of Speech and Natural Lan-guage DARPA Workshop, pp.
290-295, Feb. 1989.106
