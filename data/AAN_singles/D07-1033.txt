Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
315?324, Prague, June 2007. c?2007 Association for Computational LinguisticsA New Perceptron Algorithm forSequence Labeling with Non-local FeaturesJun?ichi Kazama and Kentaro TorisawaJapan Advanced Institute of Science and Technology (JAIST)Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan{kazama, torisawa}@jaist.ac.jpAbstractWe cannot use non-local features with cur-rent major methods of sequence labelingsuch as CRFs due to concerns about com-plexity.
We propose a new perceptron algo-rithm that can use non-local features.
Ouralgorithm allows the use of all types ofnon-local features whose values are deter-mined from the sequence and the labels.
Theweights of local and non-local features arelearned together in the training process withguaranteed convergence.
We present experi-mental results from the CoNLL 2003 namedentity recognition (NER) task to demon-strate the performance of the proposed algo-rithm.1 IntroductionMany NLP tasks such as POS tagging and namedentity recognition have recently been solved as se-quence labeling.
Discriminative methods such asConditional Random Fields (CRFs) (Lafferty et al,2001), Semi-Markov Random Fields (Sarawagi andCohen, 2004), and perceptrons (Collins, 2002a)have been popular approaches for sequence label-ing because of their excellent performance, which ismainly due to their ability to incorporate many kindsof overlapping and non-independent features.However, the common limitation of these meth-ods is that the features are limited to ?local?
fea-tures, which only depend on a very small numberof labels (usually two: the previous and the current).Although this limitation makes training and infer-ence tractable, it also excludes the use of possiblyuseful ?non-local?
features that are accessible afterall labels are determined.
For example, non-localfeatures such as ?same phrases in a document do nothave different entity classes?
were shown to be use-ful in named entity recognition (Sutton and McCal-lum, 2004; Bunescu and Mooney, 2004; Finkel etal., 2005; Krishnan and Manning, 2006).We propose a new perceptron algorithm in this pa-per that can use non-local features along with lo-cal features.
Although several methods have al-ready been proposed to incorporate non-local fea-tures (Sutton and McCallum, 2004; Bunescu andMooney, 2004; Finkel et al, 2005; Roth and Yih,2005; Krishnan and Manning, 2006; Nakagawa andMatsumoto, 2006), these present a problem thatthe types of non-local features are somewhat con-strained.
For example, Finkel et al (2005) enabledthe use of non-local features by using Gibbs sam-pling.
However, it is unclear how to apply theirmethod of determining the parameters of a non-localmodel to other types of non-local features, whichthey did not used.
Roth and Yih (2005) enabledthe use of hard constraints on labels by using inte-ger linear programming.
However, this is equivalentto only allowing non-local features whose weightsare fixed to negative infinity.
Krishnan and Manning(2006) divided the model into two CRFs, where thesecond model uses the output of the first as a kind ofnon-local information.
However, it is not possibleto use non-local features that depend on the labelsof the very candidate to be scored.
Nakagawa andMatsumoto (2006) used a Bolzmann distribution tomodel the correlation of the POS of words havingthe same lexical form in a document.
However, theirmethod can only be applied when there are conve-nient links such as the same lexical form.Since non-local features have not yet been exten-sively investigated, it is possible for us to find newuseful non-local features.
Therefore, our objectivein this study was to establish a framework, where all315types of non-local features are allowed.With non-local features, we cannot use efficientprocedures such as forward-backward proceduresand the Viterbi algorithm that are required in train-ing CRFs (Lafferty et al, 2001) and perceptrons(Collins, 2002a).
Recently, several methods (Collinsand Roark, 2004; Daume?
III and Marcu, 2005; Mc-Donald and Pereira, 2006) have been proposed withsimilar motivation to ours.
These methods allevi-ate this problem by using some approximation inperceptron-type learning.In this paper, we follow this line of research andtry to solve the problem by extending Collins?
per-ceptron algorithm (Collins, 2002a).
We exploitedthe not-so-familiar fact that we can design a per-ceptron algorithm with guaranteed convergence ifwe can find at least one wrong labeling candidateeven if we cannot perform exact inference.
We firstran the A* search only using local features to gen-erate n-best candidates (this can be efficiently per-formed), and then we only calculated the true scorewith non-local features for these candidates to finda wrong labeling candidate.
The second key ideawas to update the weights of local features duringtraining if this was necessary to generate sufficientlygood candidates.
The proposed algorithm combinedthese ideas to achieve guaranteed convergence andeffective learning with non-local features.The remainder of the paper is organized as fol-lows.
Section 2 introduces the Collins?
perceptronalgorithm.
Although this algorithm is the startingpoint for our algorithm, its baseline performance isnot outstanding.
Therefore, we present a margin ex-tension to the Collins?
perceptron in Section 3.
Thismargin perceptron became the direct basis of our al-gorithm.
We then explain our algorithm for non-local features in Section 4.
We report the experi-mental results using the CoNLL 2003 shared taskdataset in Section 6.2 Perceptron Algorithm for SequenceLabelingCollins (2002a) proposed an extension of the per-ceptron algorithm (Rosenblatt, 1958) to sequencelabeling.
Our aim in sequence labeling is to as-sign label yi ?
Y to each word xi ?
X in asequence.
We denote sequence x1, .
.
.
, xT as xand the corresponding labels as y.
We assumeweight vector ?
?
Rd and feature mapping ?that maps each (x,y) to feature vector ?
(x,y) =(?1(x,y), ?
?
?
,?d(x,y)) ?
Rd.
The model deter-mines the labels by:y?
= argmaxy?Y|x|?
(x,y) ?
?,where ?
denotes the inner product.
The aimof the learning algorithm is to obtain an ap-propriate weight vector, ?, given training set{(x1,y?1), ?
?
?
, (xL,y?L)}.The learning algorithm, which is illustrated inCollins (2002a), proceeds as follows.
The weightvector is initialized to zero.
The algorithm passesover the training examples, and each sequence is de-coded using the current weights.
If y?
is not the cor-rect answer y?, the weights are updated according tothe following rule.
?new = ?+ ?(x,y?)?
?(x,y?
).This algorithm is proved to converge (i.e., there areno more updates) in the separable case (Collins,2002a).1 That is, if there exist weight vectorU (with||U || = 1), ?
(> 0), and R (> 0) that satisfy:?i,?y ?
Y |xi| ?(xi,yi?)
?U ?
?
(xi,y) ?U ?
?,?i,?y ?
Y |xi| ||?(xi,yi?)?
?
(xi,y)|| ?
R,the number of updates is at most R2/?2.The perceptron algorithm only requires one can-didate y?
for each sequence xi, unlike the training ofCRFs where all possible candidates need to be con-sidered.
This inherent property is the key to train-ing with non-local features.
However, note that thetractability of learning and inference relies on howefficiently y?
can be found.
In practice, we can findy?
efficiently using a Viterbi-type algorithm onlywhen the features are all local, i.e., ?s(x,y) can bewritten as the sum of (two label) local features ?s as?s(x,y) =?Ti ?s(x, yi?1, yi).
This locality con-straint is also required to make the training of CRFstractable (Lafferty et al, 2001).One problem with the perceptron algorithm de-scribed so far is that it offers no treatment for over-fitting.
Thus, Collins (2002a) also proposed an av-eraged perceptron, where the final weight vector is1Collins (2002a) also provided proof that guaranteed ?good?learning for the non-separable case.
However, we have onlyconsidered the separable case throughout the paper.316Algorithm 3.1: Perceptron with margin forsequence labeling (parameters: C)?
?
0until no more updates dofor i ?
1 to L do8>>><>>>:y?
= argmaxy?
(xi,y) ?
?y??
= 2nd-besty?
(xi,y) ?
?if y?
?= y?i then?
= ?
+ ?
(xi,y?i )?
?(xi,y?
)else if ?
(xi,y?i ) ?
?
?
?(xi,y??)
?
?
?
C then?
= ?
+ ?
(xi,y?i )?
?(xi,y??
)the average of all weight vectors during training.Howerver, we found in our experiments that the av-eraged perceptron performed poorly in our setting.We therefore tried to make the perceptron algorithmmore robust to overfitting.
We will describe our ex-tension to the perceptron algorithm in the next sec-tion.3 Margin Perceptron Algorithm forSequence LabelingWe extended a perceptron with a margin (Krauth andMe?zard, 1987) to sequence labeling in this study, asCollins (2002a) extended the perceptron algorithmto sequence labeling.In the case of sequence labeling, the margin is de-fined as:?(?)
= minximiny ?=y?i?(xi,yi?)
???
?
(xi,y) ?
?||?||Assuming that the best candidate, y?, equals the cor-rect answer, y?, the margin can be re-written as:= minxi?(xi,yi?)
???
?(xi,y??)
?
?||?|| ,where y??
= 2nd-besty?
(xi,y) ??.
Using this rela-tion, the resulting algorithm becomes Algorithm 3.1.The algorithm tries to enlarge the margin as much aspossible, as well as make the best scoring candidateequal the correct answer.Constant C in Algorithm 3.1 is a tunable param-eter, which controls the trade-off between the mar-gin and convergence time.
Based on the proofsin Collins (2002a) and Li et al (2002), we canprove that the algorithm converges within (2C +R2)/?2 updates and that ?(?)
?
?C/(2C + R2) =(?/2)(1 ?
(R2/(2C + R2))) after training.
As canbe seen, the margin approaches at least half of truemargin ?
(at the cost of infinite training time), asC ?
?.Note that if the features are all local, the second-best candidate (generally n-best candidates) can alsobe found efficiently by using an A* search that usesthe best scores calculated during a Viterbi search asthe heuristic estimation (Soong and Huang, 1991).There are other methods for improving robustnessby making margin larger for the structural outputproblem.
Such methods include ALMA (Gentile,2001) used in (Daume?
III and Marcu, 2005)2, MIRA(Crammer et al, 2006) used in (McDonald et al,2005), and Max-Margin Markov Networks (Taskaret al, 2003).
However, to the best of our knowledge,there has been no prior work that has applied a per-ceptron with a margin (Krauth and Me?zard, 1987)to structured output.3 Our method described in thissection is one of the easiest to implement, whileguaranteeing a large margin.
We found in the experi-ments that our method outperformed the Collins?
av-eraged perceptron by a large margin.4 Algorithm4.1 Definition and Basic IdeaHaving described the basic perceptron algorithms,we will know explain our algorithm that learns theweights of local and non-local features in a unifiedway.Assume that we have local features and non-local features.
We use the superscript, l, forlocal features as ?li(x,y) and g for non-localfeatures as ?gi (x,y).
Then, feature mapping iswritten as ?a(x,y) = ?l(x,y) + ?g(x,y) =(?l1(x,y), ?
?
?
,?ln(x,y),?gn+1(x,y), ?
?
?
,?gd(x,y)).Here, we define:?l(x,y) = (?l1(x,y), ?
?
?
,?ln(x,y), 0, ?
?
?
, 0)?g(x,y) = (0, ?
?
?
, 0,?gn+1(x,y), ?
?
?
,?gd(x,y))Ideally, we want to determine the labels using thewhole feature set as:y?
= argmaxy?Y|x|?a(x,y) ??.2(Daume?
III and Marcu, 2005) also presents the method us-ing the averaged perceptron (Collins, 2002a)3For re-ranking problems, Shen and Joshi (2004) proposeda perceptron algorithm that also uses margins.
The difference isthat our algorithm trains the sequence labeler itself and is muchsimpler because it only aims at labeling.317Algorithm 4.1: Candidate algorithm (parameters:n, C)?
?
0until no more updates dofor i ?
1 to L do8>>>>><>>>>>:{yn} = n-besty?l(xi,y) ?
?y?
= argmaxy?
{yn}?a(xi,y) ?
?y??
= 2nd-besty?
{yn}?a(xi,y) ?
?if y?
?= yi?& ?a(xi,y?i ) ?
?
?
?a(xi,y?)
?
?
?
C then?
= ?
+ ?a(xi,y?i )?
?a(xi,y?
)else if ?a(xi,y?i ) ??
?
?a(xi,y??)
?
?
?
C then?
= ?
+ ?a(xi,y?i )?
?a(xi,y??
)However, if there are non-local features, it is impos-sible to find the highest scoring candidate efficiently,since we cannot use the Viterbi algorithm.
Thus,we cannot use the perceptron algorithms describedin the previous sections.
The training of CRFs isalso intractable for the same reason.To deal with this problem, we first relaxed our ob-jective.
The modified objective was to find a goodmodel from those with the form:{yn} = n-besty?l(x,y) ??y?
= argmaxy?
{yn}?a(x,y) ?
?, (1)That is, we first generate n-best candidates {yn}under the local model, ?l(x,y) ?
?.
This can bedone efficiently using the A* algorithm.
We thenfind the best scoring candidate under the total model,?a(x,y) ?
?, only from these n-best candidates.
If nis moderately small, this can also be done in a prac-tical amount of time.This resembles the re-ranking approach (Collinsand Duffy, 2002; Collins, 2002b).
However, unlikethe re-ranking approach, the local model, ?l(x,y) ?
?, and the total model, ?a(x,y) ?
?, correlate sincethey share a part of the vector and are trained atthe same time in our algorithm.
The re-ranking ap-proach has the disadvantage that it is necessary touse different training corpora for the first model andfor the second, or to use cross validation type train-ing, to make the training for the second meaning-ful.
This reduces the effective size of training dataor increases training time substantially.
On the otherhand, our algorithm has no such disadvantage.However, we are no longer able to find the high-est scoring candidate under ?a(x,y) ?
?
exactlywith this approach.
We cannot thus use the percep-tron algorithms directly.
However, by examining theAlgorithm 4.2: Perceptron with local andnon-local features (parameters: n, Ca, Cl)?
?
0until no more updates dofor i ?
1 to L do8>>>>>>>>>><>>>>>>>>>>:{yn} = n-besty?l(xi,y) ?
?y?
= argmaxy?
{yn}?a(xi,y) ?
?y??
= 2nd-besty?
{yn}?a(xi,y) ?
?if y?
?= y?i& ?a(xi,y?i ) ?
?
?
?a(xi,y?)
?
?
?
Ca then?
= ?
+ ?a(xi,y?i )?
?a(xi,y?)
(A)else if ?a(xi,y?i ) ??
?
?a(xi,y??)
?
?
?
Ca then?
= ?
+ ?a(xi,y?i )?
?a(xi,y??)
(A)else(B)8><>:if y1 ?= yi?
then (y1 represents the best in {yn})?
= ?
+ ?l(xi,y?i )?
?l(xi,y1)else if ?l(xi,y?i ) ?
?
?
?l(xi,y2) ?
?
?
Cl then?
= ?
+ ?l(xi,y?i )?
?l(xi,y2)proofs in Collins (2002a), we can see that the essen-tial condition for convergence is that the weights arealways updated using some y (?= y?)
that satisfies:?
(xi,y?i ) ???
?
(xi,y) ??
?
0(?
C in the case of a perceptron with a margin).
(2)That is, y does not necessarily need to be the exactbest candidate or the exact second-best candidate.The algorithm also converges in a finite number ofiterations even with Eq.
(1) as long as Eq.
(2) issatisfied.4.2 Candidate AlgorithmThe algorithm we came up with first based on theabove idea, is Algorithm 4.1.
We first find the n-best candidates using the local model, ?l(x,y) ?
?.At this point, we can determine the value of the non-local features, ?g(x,y), to form the whole featurevector, ?a(x,y), for the n-best candidates.
Next,we re-score and sort them using the total model,?a(x,y) ?
?, to find a candidate that violates themargin condition.
We call this algorithm the ?can-didate algorithm?.
After the training has finished,?a(xi,y?i ) ?
?
?
?a(xi,y) ?
?
> C is guaran-teed for all (xi,y) where y ?
{yn},y ?= y?.At first glance, this seems sufficient condition forgood models.
However, this is not true because ify?
??
{yn}, the inference defined by Eq.
(1) is notguaranteed to find the correct answer, y?.
In fact,this algorithm does not work well with non-localfeatures as we found in the experiments.3184.3 Final AlgorithmOur idea for improving the above algorithm is thatthe local model,?l(x,y)?
?, must at least be so goodthat y?
?
{yn}.
To achieve this, we added a modi-fication term that was intended to improve the localmodel when the local model was not good enougheven when the total model was good enough.The final algorithm resulted in Algorithm 4.2.
Ascan be seen, the part marked (B) has been added.
Wecall this algorithm the ?proposed algorithm?.
Notethat the algorithm prioritizes the update of the to-tal model, (A), over that of the local model, (B), al-though the opposite is also possible.
Also note thatthe update of the local model in (B) is ?aggressive?since it updates the weights until the best candidateoutput by the local model becomes the correct an-swer and satisfies the margin condition.
A ?conser-vative?
updating, where we cease the update whenthe n-best candidates contain the correct answer, isalso possible from our idea above.
We made thesechoices since they worked better than the other al-ternatives.The tunable parameters are the local margin pa-rameter, C l, the total margin parameter, Ca, and nfor the n-best search.
We used C = C l = Ca in thisstudy to reduce the search space.We can prove that the algorithm in Algorithm 4.2also converges in a finite number of iterations.
Itconverges within (2C + R2)/?2 updates, assumingthat there exist weight vector U l (with ||U l|| = 1and U li = 0 (n+1 ?
i ?
d)), ?
(> 0), and R (> 0)that satisfy:?i,?y ?
Y |xi| ?l(xi,yi?
)?U l?
?l(xi,y)?U l ?
?,?i,?y ?
Y |xi| ||?a(xi,yi?)?
?a(xi,y)|| ?
R.In addition, we can prove that ??(?)
?
?C/(2C +R2) for the margin after convergence, where ??(?
)is defined as:minximiny?{yn},?=y?i?a(xi,yi?)
???
?a(xi,y) ?
?||?||See Appendix A for the proofs.We also incorporated the idea behind Bayes pointmachines (BPMs) (Herbrich and Graepel, 2000) toimprove the robustness of our method further.
BPMstry to cancel out overfitting caused by the order ofexamples, by training several models by shufflingthe training examples.4 However, it is very timeconsuming to run the complete training process sev-eral times.
We thus ran the training in only one passover the shuffled examples several times, and usedthe averaged output weight vectors as a new initialweight vector, because we thought that the early partof training would be more seriously affected by theorder of examples.
We call this ?BPM initializa-tion?.
55 Named Entity Recognition andNon-Local FeaturesWe evaluated the performance of the proposed algo-rithm using the named entity recognition task.
Weadopted IOB (IOB2) labeling (Ramshaw and Mar-cus, 1995), where the first word of an entity of class?C?
is labeled ?B-C?, the words in the entity are la-beled ?I-C?, and other words are labeled ?O?.We used non-local features based on Finkel et al(2005).
These features are based on observationssuch as ?same phrases in a document tend to havethe same entity class?
(phrase consistency) and ?asub-phrase of a phrase tends to have the same entityclass as the phrase?
(sub-phrase consistency).
Wealso implemented the ?majority?
version of thesefeatures as used in Krishnan and Manning (2006).In addition, we used non-local features, which arebased on the observation that ?entities tend to havethe same entity class if they are in the same con-junctive or disjunctive expression?
as in ??
?
?
in U.S.,EU, and Japan?
(conjunction consistency).
This typeof non-local feature was not used by Finkel et al(2005) or Krishnan and Manning (2006).6 Experiments6.1 Data and SettingWe used the English dataset of the CoNLL 2003named entity shared task (Tjong et al, 2003) forthe experiments.
It is a corpus of English newspa-per articles, where four entity classes, PER, LOC,ORG, and MISC are annotated.
It consists of train-ing, development, and testing sets (14,987, 3,466,4The results for the perceptron algorithms generally dependon the order of the training examples.5Note that we can prove that the perceptron algorithms con-verge even though the weight vector is not initialized as ?
= 0.319and 3,684 sentences, respectively).
Automaticallyassigned POS tags and chunk tags are also provided.The CoNLL 2003 dataset contains document bound-ary markers.
We concatenated the sentences in thesame document according to these markers.6 Thisgenerated 964 documents for the training set, 216documents for the development set, and 231 docu-ments for the testing set.
The documents generatedas above become the sequence, x, in the learningalgorithms.We first evaluated the baseline performance ofa CRF model, the Collins?
perceptron, and theCollins?
averaged perceptron, as well as the marginperceptron, with only local features.
We next eval-uated the performance of our perceptron algorithmproposed for non-local features.We used the local features summarized in Table1, which are similar to those used in other studieson named entity recognition.
We omitted featureswhose surface part listed in Table 1 occurred lessthan twice in the training corpus.We used CRF++ (ver.
0.44)7 as the basis of ourimplementation.
We implemented scaling, whichis similar to that for HMMs (see such as (Rabiner,1989)), in the forward-backward phase of CRF train-ing to deal with very long sequences due to sentenceconcatenation.8We used Gaussian regularization (Chen andRosenfeld, 2000) for CRF training to avoid overfit-ting.
The parameter of the Gaussian, ?2, was tunedusing the development set.
We also tuned the marginparameter, C, for the margin perceptron algorithm.9The convergence of CRF training was determined bychecking the log-likelihood of the model.
The con-vergence of perceptron algorithms was determinedby checking the per-word labeling error, since the6We used sentence concatenation even when only using lo-cal features, since we found it does not degrade accuracy (ratherwe observed a slight increase).7http://chasen.org/?taku/software/CRF++8We also replaced the optimization module in the originalpackage with that used in the Amis maximum entropy estima-tor (http://www-tsujii.is.s.u-tokyo.ac.jp/amis) since we encoun-tered problems with the provided module in some cases.9For the Gaussian parameter, we tested {13, 25, 50, 100,200, 400, 800} (the accuracy did not change drastically amongthese values and it seems that there is no accuracy hump evenif we use smaller values).
We tested {500, 1000, 1414, 2000,2828, 4000, 5657, 8000, 11313, 16000, 32000} for the marginparameters.Table 1: Local features used.
The value of a nodefeature is determined from the current label, y0, anda surface feature determined only from x.
The valueof an edge feature is determined by the previous la-bel, y?1, the current label, y0, and a surface feature.Used surface features are the word (w), the down-cased word (wl), the POS tag (pos), the chunk tag(chk), the prefix of the word of length n (pn), thesuffix (sn), the word form features: 2d - cp (these arebased on (Bikel et al, 1999)), and the gazetteer fea-tures: go for ORG, gp for PER, and gm for MISC.These represent the (longest) match with an entry inthe gazetteer by using IOB2 tags.Node features:{?
?, x?2, x?1, x0, x+1, x+2} ?
y0x =, w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3,s4, 2d, 4d, d&a, d&-, d&/, d&,, d&., n, ic, ac,l, cp, go, gp, gmEdge features:{?
?, x?2, x?1, x0, x+1, x+2} ?
y?1 ?
y0x =, w, wl, pos, chk, p1, p2, p3, p4, s1, s2, s3,s4, 2d, 4d, d&a, d&-, d&/, d&,, d&., n, ic, ac,l, cp, go, gp, gmBigram node features:{x?2x?1, x?1x0, x0x+1} ?
y0x = wl, pos, chk, go, gp, gmBigram edge features:{x?2x?1, x?1x0, x0x+1} ?
y?1 ?
y0x = wl, pos, chk, go, gp, gmnumber of updates was not zero even after a largenumber of iterations in practice.
We stopped train-ing when the relative change in these values becameless than a pre-defined threshold (0.0001) for at leastthree iterations.We used n = 20 (n of the n-best) for trainingsince we could not use too a large n because it wouldhave slowed down training.
However, we could ex-amine a larger n during testing, since the testing timedid not dominate the time for the experiment.
Wefound an interesting property for n in our prelimi-nary experiment.
We found that an even larger n intesting (written as n?)
achieved higher accuracy, al-though it is natural to assume that the same n thatwas used in training would also be appropriate fortesting.
We thus used n?
= 100 to evaluate perfor-mance during parameter tuning.
After finding thebest C with n?
= 100, we varied n?
to investigate its320Table 2: Summary of performance (F1).Method dev test C (or ?2)local featuresCRF 91.10 86.26 100Perceptron 89.01 84.03 -Averaged perceptron 89.32 84.08 -Margin perceptron 90.98 85.64 11313+ non-local featuresCandidate (n?
= 100) 90.71 84.90 4000Proposed (n?
= 100) 91.95 86.30 5657Table 3: Effect of n?.Method dev test CProposed (n?
= 20) 91.76 86.19 5657Proposed (n?
= 100) 91.95 86.30 5657Proposed (n?
= 400) 92.13 86.39 5657Proposed (n?
= 800) 92.09 86.39 5657Proposed (n?
= 1600) 92.13 86.46 5657Proposed (n?
= 6400) 92.19 86.38 5657effects further.6.2 ResultsTable 2 compares the results.
CRF outperformedthe perceptron by a large margin.
Although the av-eraged perceptron outperformed the perceptron, theimprovement was slight.
However, the margin per-ceptron greatly outperformed compared to the aver-aged perceptron.
Yet, CRF still had the best baselineperformance with only local features.The proposed algorithm with non-local featuresimproved the performance on the test set by 0.66points over that of the margin perceptron withoutnon-local features.
The row ?Candidate?
refers tothe candidate algorithm (Algorithm 4.1).
From theresults for the candidate algorithm, we can see thatthe modification part, (B), in Algorithm 4.2 was es-sential to make learning with non-local features ef-fective.We next examined the effect of n?.
As can beseen from Table 3, an n?
larger than that for train-ing yields higher performance.
The highest perfor-mance with the proposed algorithm was achievedwhen n?
= 6400, where the improvement due tonon-local features became 0.74 points.The performance of the related work (Finkel etal., 2005; Krishnan and Manning, 2006) is listed inTable 4.
We can see that the final performance of ouralgorithm was worse than that of the related work.We changed the experimental setting slightlyto investigate our algorithm further.
Instead ofTable 4: The performance of the related work.Method dev testFinkel et al, 2005 (Finkel et al, 2005)baseline CRF - 85.51+ non-local features - 86.86Krishnan and Manning, 2006 (Krishnan and Manning, 2006)baseline CRF - 85.29+ non-local features - 87.24Table 5: Summary of performance with POS/chunktags by TagChunk.Method dev test C (or ?2)local featuresCRF 91.39 86.30 200Perceptron 89.36 84.35 -Averaged perceptron 89.76 84.50 -Margin perceptron 91.06 86.24 32000+ non-local featuresProposed (n?
= 100) 92.23 87.04 5657Proposed (n?
= 6400) 92.54 87.17 5657the POS/chunk tags provided in the CoNLL 2003dataset, we used the tags assigned by TagChunk(Daume?
III and Marcu, 2005)10 with the intentionof using more accurate tags.
The results with thissetting are summarized in Table 5.
Performance wasbetter than that in the previous experiment for all al-gorithms.
We think this was due to the quality ofthe POS/chunk tags.
It is interesting that the ef-fect of non-local features rose to 0.93 points withn?
= 6400, even though the baseline performancewas also improved.
The resulting performance ofthe proposed algorithm with non-local features ishigher than that of Finkel et al (2005) and compara-ble with that of Krishnan and Manning (2006).
Thiscomparison, of course, is not fair because the settingwas different.
However, we think the results demon-strate a potential of our new algorithm.The effect of BPM initialization was also exam-ined.
The number of BPM runs was 10 in thisexperiment.
The performance of the proposed al-gorithm dropped from 91.95/86.30 to 91.89/86.03without BPM initialization as expected in the set-ting of the experiment of Table 2.
The perfor-mance of the margin perceptron, on the other hand,changed from 90.98/85.64 to 90.98/85.90 withoutBPM initialization.
This result was unexpected fromthe result of our preliminary experiment.
However,the performance was changed from 91.06/86.24 to10http://www.cs.utah.edu/?hal/TagChunk/321Table 6: Comparison with re-ranking approach.Method dev test Clocal featuresMargin Perceptron 91.06 86.24 32000+ non-local featuresRe-ranking 1 (n?
= 100) 91.62 86.57 4000Re-ranking 1 (n?
= 80) 91.71 86.58 4000Re-ranking 2 (n?
= 100) 92.08 86.86 16000Re-ranking 2 (n?
= 800) 92.26 86.95 16000Proposed (n?
= 100) 92.23 87.04 5657Proposed (n?
= 6400) 92.54 87.17 5657Table 7: Comparison of training time (C = 5657).Method dev test time (sec.
)local featuresMargin Perceptron 91.04 86.28 15,977+ non-local featuresRe-ranking 1 (n?
= 100) 91.48 86.53 86,742Re-ranking 2 (n?
= 100) 92.02 86.85 112,138Proposed (n?
= 100) 92.23 87.04 28,88091.17/86.08 (i.e., dropped for the evaluation set asexpected), in the setting of the experiment of Table5.
Since the effect of BPM initialization is not con-clusive only from these results, we need more exper-iments on this.6.3 Comparison with re-ranking approachFinally, we compared our algorithm with the re-ranking approach (Collins and Duffy, 2002; Collins,2002b), where we first generate the n-best candi-dates using a model with only local features (thefirst model) and then re-rank the candidates usinga model with non-local features (the second model).We implemented two re-ranking models, ?re-ranking 1?
and ?re-ranking 2?.
These models dif-fer in how to incorporate the local information in thesecond model.
?re-ranking 1?
uses the score of thefirst model as a feature in addition to the non-localfeatures as in Collins (2002b).
?re-ranking 2?
usesthe same local features as the first model11 in addi-tion to the non-local features.
The first models weretrained using the margin perceptron algorithm in Al-gorithm 3.1.
The second models were trained usingthe algorithm, which is obtained by replacing {yn}with the n-best candidates by the first model.
Thefirst model used to generate n-best candidates for thedevelopment set and the test set was trained usingthe whole training data.
However, CRFs or percep-trons generally have nearly zero error on the train-ing data, although the first model should mis-label11The weights were re-trained for the second model.to some extent to make the training of the secondmodel meaningful.
To avoid this problem, we adoptcross-validation training as used in Collins (2002b).We split the training data into 5 sets.
We then trainedfive first models using 4/5 of the data, each of whichwas used to generate n-best candidates for the re-maining 1/5 of the data.As in the previous experiments, we tuned C usingthe development set with n?
= 100 and then testedother values for n?.
Table 6 shows the results.
As canbe seen, re-ranking models were outperformed byour proposed algorithm, although they also outper-formed the margin perceptron with only local fea-tures (?re-ranking 2?
seems better than ?re-ranking1?).
Table 7 shows the training time of each algo-rithm.12 Our algorithm is much faster than the re-ranking approach that uses cross-validation training,while achieving the same or higher level of perfor-mance.7 DiscussionAs we mentioned, there are some algorithms simi-lar to ours (Collins and Roark, 2004; Daume?
III andMarcu, 2005; McDonald and Pereira, 2006; Lianget al, 2006).
The differences of our algorithm fromthese algorithms are as follows.Daume?
III and Marcu (2005) presented themethod called LaSO (Learning as Search Optimiza-tion), in which intractable exact inference is approx-imated by optimizing the behavior of the search pro-cess.
The method can access non-local featuresat each search point, if their values can be deter-mined from the search decisions already made.
Theyprovided robust training algorithms with guaranteedconvergence for this framework.
However, a differ-ence is that our method can use non-local featureswhose value depends on all labels throughout train-ing, and it is unclear whether the features whose val-ues can only be determined at the end of the search(e.g., majority features) can be learned effectivelywith such an incremental manner of LaSO.The algorithm proposed by McDonald andPereira (2006) is also similar to ours.
Their tar-get was non-projective dependency parsing, whereexact inference is intractable.
Instead of using12Training time was measured on a machine with 2.33 GHzQuadCore Intel Xeons and 8 GB of memory.
C was fixed to5657.322n-best/re-scoring approach as ours, their methodmodifies the single best projective parse, whichcan be found efficiently, to find a candidate withhigher score under non-local features.
Liang et al(2006) used n candidates of a beam search in theCollins?
perceptron algorithm for machine transla-tion.
Collins and Roark (2004) proposed an approxi-mate incremental method for parsing.
Their methodcan be used for sequence labeling as well.
Thesestudies, however, did not explain the validity of theirupdating methods in terms of convergence.To achieve robust training, Daume?
III and Marcu(2005) employed the averaged perceptron (Collins,2002a) and ALMA (Gentile, 2001).
Collins andRoark (2004) used the averaged perceptron (Collins,2002a).
McDonald and Pereira (2006) used MIRA(Crammer et al, 2006).
On the other hand, we em-ployed the margin perceptron (Krauth and Me?zard,1987), extending it to sequence labeling.
We demon-strated that this greatly improved robustness.With regard to the local update, (B), in Algo-rithm 4.2, ?early updates?
(Collins and Roark, 2004)and ?y-good?
requirement in (Daume?
III and Marcu,2005) resemble our local update in that they tried toavoid the situation where the correct answer cannotbe output.
Considering such commonality, the wayof combining the local update and the non-local up-date might be one important key for further improve-ment.It is still open whether these differences are ad-vantages or disadvantages.
However, we think ouralgorithm can be a contribution to the study for in-corporating non-local features.
The convergenceguarantee is important for the confidence in thetraining results, although it does not mean high per-formance directly.
Our algorithm could at least im-prove the accuracy of NER with non-local featuresand it was indicated that our algorithm was supe-rior to the re-ranking approach in terms of accu-racy and training cost.
However, the achieved accu-racy was not better than that of related work (Finkelet al, 2005; Krishnan and Manning, 2006) basedon CRFs.
Although this might indicate the limita-tion of perceptron-based methods, it has also beenshown that there is still room for improvement inperceptron-based algorithms as our margin percep-tron algorithm demonstrated.8 ConclusionIn this paper, we presented a new perceptron algo-rithm for learning with non-local features.
We thinkthe proposed algorithm is an important step towardsachieving our final objective.
We would like to in-vestigate various types of new non-local features us-ing the proposed algorithm in future work.Appendix A: Convergence of Algorithm 4.2Let ?k be a weight vector before the kth update and?k be a variable that takes 1 when the kth update isdone in (A) and 0 when done in (B).
The update rulecan then be written as ?k+1 = ?k + ?k(?a??
?a +(1?
?k)(?l?
?
?l).13 First, we obtain?k+1 ?U l = ?k ?U l + ?k(?a?
?U l ?
?a ?U l)+(1?
?k)(?l?
?U l ?
?l ?U l)?
?k ?U l + ?k?
+ (1?
?k)?= ?k ?U l + ?
?
?1 ?U l + k?
= k?Therefore, (k?
)2 ?
(?k+1 ?
U l)2 ?
(||?k+1||||U l||)2 = ||?k+1||2 ?
(1).
On theother hand, we also obtain||?k+1||2 ?
||?k||2 + 2?k?k(?a?
?
?a)+2(1?
?k)?k(?l?
?
?l)+{?k(?a?
?
?a) + (1?
?k)(?l?
?
?l)}2?
||?k||2 + 2C + R2?
||?1||2 + k(R2 + 2C) = k(R2 + 2C)?
(2)We used ?k(?a?
?
?a) ?
Ca, ?k(?l?
?
?l) ?C l and C l = Ca = C to derive 2C in the secondinequality.
We used ||?l??
?l|| ?
||?a??
?a|| ?
Rto derive R2.Combining (1) and (2), we obtain k ?
(R2 +2C)/?2.
Substituting this into (2) gives ||?k|| ?(R2+2C)/?.
Since y?
= y?
and?a?
????a??
??
>C after convergence, we obtain??(?)
= minxi?a?
???
?a??
?
?||?|| ?
C?/(2C + R2).13We use the shorthand ?a?
= ?a(xi,y?i ), ?a =?a(xi,y), ?l?
= ?l(xi,y?i ), and ?l = ?l(xi,y) where yrepresents the candidate used to update (y?
, y??
, y1, or y2).323ReferencesD.
M. Bikel, R. L. Schwartz, and R. M. Weischedel.1999.
An algorithm that learns what?s in a name.
Ma-chine Learning, 34(1-3):211?231.R.
Bunescu and R. J. Mooney.
2004.
Collective infor-mation extraction with relational markov networks.
InACL 2004.S.
F. Chen and R. Rosenfeld.
2000.
A survey of smooth-ing techniques for ME models.
IEEE Transactions onSpeech and Audio Processing, 8(1):37?50.M.
Collins and N. Duffy.
2002.
New ranking algorithmsfor parsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In ACL 2002, pages263?270.M.
Collins and B. Roark.
2004.
Incremental parsing withthe perceptron algorithm.
In ACL 2004.M.
Collins.
2002a.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms.
In EMNLP 2002.M.
Collins.
2002b.
Ranking algorithms for named-entityextraction: Boosting and the voted perceptron.
In ACL2002.K.
Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
Journal of Machine Learning Research.H.
Daume?
III and D. Marcu.
2005.
Learning as searchoptimization: Approximate large margin methods forstructured prediction.
In ICML 2005.J.
R. Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into information ex-traction systems by Gibbs sampling.
In ACL 2005.C.
Gentile.
2001.
A new approximate maximal marginclassification algorithm.
JMLR, 3.R.
Herbrich and T. Graepel.
2000.
Large scale Bayespoint machines.
In NIPS 2000.W.
Krauth and M. Me?zard.
1987.
Learning algorithmswith optimal stability in neural networks.
Journal ofPhysics A 20, pages 745?752.V.
Krishnan and C. D. Manning.
2006.
An effective two-stage model for exploiting non-local dependencies innamed entity recognitioin.
In ACL-COLING 2006.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML 2001,pages 282?289.Y.
Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, andJ.
Kandola.
2002.
The perceptron algorithm with un-even margins.
In ICML 2002.P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In ACL-COLING 2006.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In EACL2006.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In ACL2005.T.
Nakagawa and Y. Matsumoto.
2006.
Guessing parts-of-speech of unknown words using global information.In ACL-COLING 2006.L.
R. Rabiner.
1989.
A tutorial on hidden Markov mod-els and selected applications in speech recognition.Proceedings of the IEEE, 77(2):257?286.L.
A. Ramshaw and M. P. Marcus.
1995.
Text chunk-ing using transformation-based learning.
In third ACLWorkshop on very large corpora.F.
Rosenblatt.
1958.
The perceptron: A probabilisticmodel for information storage and organization in thebrain.
Psycological Review, pages 386?407.D.
Roth and W. Yih.
2005.
Integer linear program-ming inference for conditional random fields.
In ICML2005.S.
Sarawagi and W. W. Cohen.
2004.
Semi-Markov ran-dom fields for information extraction.
In NIPS 2004.L.
Shen and A. K. Joshi.
2004.
Flexible margin selectionfor reranking with full pairwise samples.
In IJCNLP2004.F.
K. Soong and E. Huang.
1991.
A tree-trellis basedfast search for finding the n best sentence hypothesesin continuous speech recognition.
In ICASSP-91.C.
Sutton and A. McCallum.
2004.
Collective segme-nation and labeling of distant entitites in informationextraction.
University of Massachusetts Rechnical Re-port TR 04-49.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In NIPS 2003.E.
F. Tjong, K. Sang, and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In CoNLL2003.324
