Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2280?2290, Dublin, Ireland, August 23-29 2014.A Probabilistic Co-Bootstrapping Method for Entity Set ExpansionBei Shi,    Zhengzhong Zhang Le Sun,    Xianpei HanInstitute of Software,Chinese Academy of Sciences,Beijing, ChinaState Key Laboratory of Computer Science,Institute of Software,Chinese Academy of Sciences,Beijing, China{shibei, zhenzhong, sunle, xianpei}@nfs.iscas.ac.cnAbstractEntity Set Expansion (ESE) aims at automatically acquiring instances of a specific target category.Unfortunately, traditional ESE methods usually have the expansion boundary problem and the semanticdrift problem.
To resolve the above two problems, this paper proposes a probabilistic Co-Bootstrappingmethod, which can accurately determine the expansion boundary using both the positive and thediscriminant negative instances, and resolve the semantic drift problem by effectively maintaining andrefining the expansion boundary during bootstrapping iterations.
Experimental results show that ourmethod can achieve a competitive performance.1 IntroductionEntity Set Expansion (ESE) aims at automatically acquiring instances of a specific target categoryfrom text corpus or Web.
For example, given the capital seeds {Rome, Beijing, Paris}, an ESE systemshould extract all other capitals from Web, such as Ottawa, Moscow and London.
ESE system hasbeen used in many applications, e.g., dictionary construction (Cohen and Sarawagi, 2004), word sensedisambiguation (Pantel and Lin, 2002), query refinement (Hu et al., 2009), and query suggestion (Caoet al., 2008).Due to the limited supervision provided by ESE (in most cases only 3-5 seeds are given), traditionalESE systems usually employ bootstrapping methods (Cucchiarelli and Velardi, 2001; Etzioni et al.,2005; Pasca, 2007; Riloff and Jones, 1999; Wang and Cohen, 2008).
That is, the entity set isiteratively expanded through a pattern generation step and an instance extraction step.
Figure 1(a)demonstrates a simple bootstrapping process.
?This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/RomeBeijingParisMilanTokyoShanghaiLondon* is the city ofat the embassy in ** is the capital ofat the hotel in *ChicagoBerlinPattern Generation Instance ExtractionRomeBeijingParisMilanTokyoShanghaiLondonSydneyBoston* is the city ofat the embassy in ** is the capital ofto cities such as *at the hotel in *ChicagoNokia * the official web siteNew YorkPattern Generation Instance Co-ExtractionNegative Positive(a) (b)Figure 1: A demo of Bootstrapping (a) and Co-Bootstrapping (b)2280However, the traditional bootstrapping methods have two main drawbacks:1) The expansion boundary problem.
That is, using only positive seeds (i.e., some exampleentities from the category we want to expand), it is difficult to represent which entities we want toexpand and which we don?t want.
For example, starting from positive seeds {Rome, Beijing, Paris},we can expand entities at many different levels, e.g., all capitals, all cities, or even all locations.
Andall these explanations are reasonable.2) The semantic drift problem.
That is, the expansion category may change gradually when noisyinstances/patterns are introduced during the bootstrapping iterations.
For example, in Figure 1 (a), theinstance Rome will introduce a pattern ?
* is the city of?, which will introduce many noisy cityinstances such as Milan and Chicago for the expansion of Capital.
And these noisy cities in turn willintroduce more city patterns and instances, and finally will lead to a semantic drift from Capital toCity.In recent years, some methods (Curran et al, 2007; Pennacchiotti and Pantel, 2011) have exploitedmutual exclusion constraint to resolve the semantic drift problem.
These methods expand multiplecategories simultaneously, and will determine the expansion boundary based on the mutuallyexclusive property of the pre-given categories.
For instance, the exclusive categories Fruit andCompany will be jointly expanded and the expansion boundary of {Apple, Banana, Cherry} will belimited by the expansion boundary of {Google, Microsoft, Apple Inc.}.
These methods, however, stillhave the following two drawbacks:1) These methods require that the expanded categories should be mutually exclusive.
However, inmany cases the mutually exclusive assumption does not hold.
For example, many categories hold ahyponymy relation (e.g., the categories City and Capital, because the patterns for Capital are also thepatterns for City) or a high semantic overlap (e.g., the categories Movies and Novels, because somemovies are directly based on the novels of the same title.
).2) These methods require the manually determination of the mutually exclusive categories.Unfortunately, it is often very hard for even the experts to determine the categories which can definethe expansion boundaries for each other.
For example, in order to expand the category ChemicalElement, it is difficult to predict its semantic drift towards Color caused by the ambiguous instances{Silver, Gold}.In this paper, to resolve the above problems, we propose a probabilistic Co-Bootstrapping method.The first advantage of our method is that we propose a method to better define the expansion boundaryusing both the positive and the discriminant negative seeds, which can both be automatically populatedduring the bootstrapping process.
For instance, in Figure 1(b), in order to expand Capital, theCo-Bootstrapping algorithm will populate both positive instances from the positive seeds {Rome,Beijing, Paris}, and negative instances from the negative seeds {Boston, Sydney, New York}.
In thisway the expansion boundary of Capital can be accurately determined.The second advantage of our method is that we can maintain and refine the expansion boundaryduring bootstrapping iterations, so that the semantic drift problem can be effectively resolved.Specifically, we propose an effective scoring algorithm to estimate the probability that an extractedinstance belongs to the target category.
Based on this scoring algorithm, this paper can effectivelyselect positive instances and discriminant negative instances.
Therefore the expansion boundary can bemaintained and refined through the above jointly expansion process.We have evaluated our method on the expansion of thirteen categories of entities.
The experimentalresults show that our method can achieve 6%~15% P@200 performance improvement over thebaseline methods.This paper is organized as follows.
Section 2 briefly reviews related work.
Section 3 defines theproblem and proposes a probabilistic Co-Bootstrapping approach.
Experiments are presented inSection 4.
Finally, we conclude this paper and discuss some future work in Section 5.2 Related WorkIn recent years, ESE has received considerable attentions from both research (An et al., 2003;Cafarella et al., 2005; Pantel and Ravichandran, 2004; Pantel et al., 2009; Pasca, 2007; Wang andCohen, 2008) and industry communities (e.g., Google Sets).
Till now, most ESE systems employbootstrapping methods, such as DIPRE (Brin, 1998), Snowball (Agichtein and Gravano, 2000), etc.2281The main drawbacks of the traditional bootstrapping methods are the expansion boundary problemand the semantic drift problem.
Currently, two strategies have been exploited to resolve the semanticdrift problem.
The first is the ranking based approaches (Pantel and Pennacchiotti, 2006; Talukdar etal., 2008), which select highly confident patterns and instances through a ranking algorithm, with theassumption that high-ranked instances will be more likely to be the instances of the target category.The second is the mutual exclusion constraint based methods (Curran et al., 2007; McIntosh andCurran, 2008; Pennacchiotti and Pantel, 2011; Thelen and Riloff, 2002; Yangarber et al., 2002), whichexpand multiple categories simultaneously and determine the expansion boundary based on themutually exclusive property of the pre-given categories.3 The Co-Bootstrapping Method3.1 The Framework of Probabilistic Co-BootstrappingGiven the initial positive seeds and negative seeds, the goal of our method is to extract instances of aspecific target semantic category.
For demonstration, we will describe our method through the runningexample shown in Figure 1(b).Specifically, Figure 2 shows the framework of our method.
The central tasks of ourCo-Bootstrapping method are as follows:Figure 2: The framework of probabilistic Co-Bootstrapping1) Pattern Generation and Evaluation.
This step generates and evaluates patterns using thestatistics of the positive and the negative instances.
Specifically, we propose three measures of patternquality: the Generality (GE), the Precision of Extracted Instances (PE) and the Precision of NotExtracted Instances (PNE).2) Instance Co-Extraction.
This step co-extracts the positive and the negative instances usinghighly confident patterns.
Specifically, we propose an effective scoring algorithm to estimate theprobability that an extracted instance belongs to the target category based on the statistics and thequality of the patterns which extract it.3) Seed Selection.
This step selects the high ranked positive instances and discriminant negativeinstances to refine the expansion boundary by measuring how well a new instance can be used todefine the expansion boundary.The above three steps will iterate until the number of extracted entities reaches a predefinedthreshold.
We describe these steps as follows.3.2 Pattern Generation and EvaluationIn this section, we describe the pattern generation and evaluation step.
In this paper, each pattern is a4-grams lexical context of an entity.
We use the Google Web 1T corpus?s (Brants and Franz, 2006)5-grams for both the pattern generation and the instance co-extraction in ESE.
Our method generatespatterns through two steps: 1) Generate candidate patterns by matching seeds with the 5-grams.
2)Evaluate the quality of the patterns.For the first step, we simply match each seed instance with all 5-grams, then we replace thematching instance with wildcard ?*?
to generate the pattern.Extracted Positive (ep) LondonExtracted Negative (en) Shanghai, MilanNot Extracted Positive (nep) TokyoNot Extracted Negative (nen) Chicago, NokiaTable 1: (a) shows the four classes of instances according to polarity and extraction.
(b) shows the fourclasses of the instances given ?to cities such as *?Count Positive NegativeExtracted Extracted Positive (ep) Extracted Negative (en)Not ExtractedNot Extracted and Positive(nep)Not Extracted and Negative(nen)Pattern Generation and EvaluationInitialSeedsPatternPositive InstanceDiscriminant Negative InstancePositive InstanceNegative InstanceInstance Co-ExtractionSeeds Evaluation and Selection(a) (b)2282For the second step, we propose three measures to evaluate the quality of a pattern, correspondinglythe Generality (GE), the Precision of Extracted Instances (PE), and the Precision of Not ExtractedInstances (PNE).
Specifically, given a pattern, we observed that all instances can be categorized intofour classes, according to whether they belong to the target category and whether they can be extractedby the pattern (shown in Table 1(a)).
For example, given the pattern ?to cities such as *?
in Figure1(b), the instances under its four classes are shown in Table 1 (b).The proposed three measures of the quality of a pattern can be computed as follows (In most cases,we cannot get the accurate number of ep, en, nep and nen.
So this paper uses the corresponding knowninstances in the previous iteration to approximately compute ep, en, nep and nen):1) Generality (GE).
The Generality of a pattern measures how many entities can be extracted by it.A more general pattern will cover more entities than a more specific pattern.
Specifically, the GE of apattern is computed as:That is, the proportion of the instances which can be extracted by the pattern in the previous iteration.2) Precision of Extracted Instances (PE).
The PE measures how likely an instance extracted by apattern will be positive.
That is, a pattern with higher PE will be more likely to extract positiveinstances than a lower PE pattern.
The PE is computed as:That is, the proportion of positive instances within all instances which can be extracted by thepattern in the previous iteration.3) Precision of Not Extracted Instances (PNE).
The PNE measures how likely a not extractedinstance is positive.
Instances not extracted by a high PNE pattern will be more likely to be positive.PNE is computed as:Because the number of negative instances is usually much larger than the number of positiveinstances, we normalize the number of positive and negative instances in the formula.Table 2 shows these measures of some selected patterns evaluated using the Google Web 1T corpus.We can see that the above measures can effectively evaluate the quality of patterns.
For instance,GE(?
* is the city of?
)=0.566 is larger than GE(?at the embassy in *?
)=0.340, which is consistent withour intuition that the pattern ?
* is the city of?
is more general than ?at the embassy in *?.
PE(?
* is thecapital of?
)=0.928 is larger than PE(?
* is the city of?
)=0.269, which is consistent with our intuitionthat the instances extracted by ?
* is the capital of?
are more likely Capital than by?
* is the city of?.GE PE PNEat the embassy in * 0.340 0.833 0.312* is the capital of 0.321 0.928 0.224to cities such as * 0.426 0.875 0.566at the hotel in * 0.333 0.192 0.571* is the city of 0.566 0.269 0.592* the official web site 0.218 0.230 0.607Table 2: The GE, PE and PNE of some selected patterns3.3 Instance Co-ExtractionIn this section, we describe how to co-extract positive instances and discriminant negative instances.Given the generated patterns, the central task of this step is to measure the likelihood of an instance tobe positive.
The higher the likelihood, the more likely the instance belongs to the target category.
Toresolve the task, we propose a probabilistic method which predicts the probability of an instance to bepositive, i.e., the Instance Positive Probability and we denote it as P+.
Generally, the P+ is determinedby both the statistics and the quality of patterns.
We start with the observation that:22831) If an instance is extracted by a pattern with a high PE, the instance will have a high P+.2) If an instance is not extracted by a high PNE pattern, the instance will have a high P+.3) If an instance is extracted by many patterns with high PE and not extracted by many patternswith high PNE, the instance will have a high P+, and vice versa.Based on the above observations, the computation of P+ is as follows:The Situation of One PatternFor the situation that only one pattern exists, the P+ of an instance can be simply computed as:where e denotes an extracted instance and p denotes a pattern which extracts e. This formula meansthat if the instance is extracted by a pattern, the P+ is determined by the PE of the pattern.
Forexample, in Figure 3 (a), the instance Tokyo is only extracted by the pattern ?at the embassy in *?
andthe P+ is determined by the PE of ?at the embassy in *?, i.e., P+(Tokyo)=PE(?at the embassy in *?
).The above formula also means when the instance cannot be extracted by the only pattern, the P+will be determined by the PNE of the pattern.
For example, in Figure 3 (b), the instance Tokyo is notextracted by the only pattern ?at the hotel in *?
and the P+ is only determined by the PNE of ?at thehotel in *?, that is, P+(Tokyo)=PNE(?at the hotel in *?
).Figure 3: (a) Tokyo is extracted by ?at the embassy in *?.
(b) Tokyo is not extracted by ?at the hotelin *?.
(c) London is extracted by ?at the embassy in *?
and not extracted by ?to cities such as *?.The Situation of Multiple PatternsIn this section, we describe how to compute P+ in the situation of multiple patterns.
Specifically, weassume that an instance is extracted by different patterns independently.
Therefore, given all thepattern-instance relations (i.e., whether a specific pattern extracts a specific instance), the likelihoodfor an instance e being positive is computed as:where R+ is all the patterns which extract e, and R- is all the patterns which do not extract e. I+ is theset of all positive instances.
is the probability of the event ?pattern p extractsinstance e and e is positive?.
Using Bayes rule, this probability can be computed as:where  is the probability of the event ?p extracts an instance e?, its value is GE(p);is the conditional probability that e is positive under the condition ?p extracts e?,and its value is PE(p).
Finally  is computed as:is the probability of the event ?p does not extract e and e is positive?, which canbe computed as:is the probability of p not extracting an instance e, and its value is 1-GE(p).is the conditional probability that e is positive under the condition ?p does notextract e?, and its value is PNE(p).
Then  is finally computed as:Tokyo at the embassy in * Tokyo  at the hotel in * Londonat the embassy in *to cities such as *(a) (b) (c)2284For example, in Figure 3 (c), the instance London is extracted by the pattern ?at the embassy in *?and not extracted by the pattern ?to cities such as *?.
In this situation, PosLikelihood(London)=[GE(?at the embassy in *?)
?
PE(?at the embassy in?)]
?
[(1-GE(?to cities such as *?))
?
PNE(?tocities such as *?
)].Using the same intuition and the same method, the likelihood of an instance being negative iscomputed as:where  is the probability of the event ?p extracts e and e is negative?, which iscomputed as:is the probability of the event ?p does not extract e and e is negative?, which iscomputed as:For instance, in Figure 3 (c), NegLikelihood(London) = [GE(?at the embassy in *?)
?
(1-PE(?at theembassy in?))]
?
[(1-GE(?to cities such as *?))
?
(1-PNE(?to cities such as *?
))].Finally, the Instance Positive Probability, P+, is computed as:3.4 Seed SelectionIn this section, we describe how to select positive and discriminant negative instances at each iteration.To determine whether an instance is positive, we use a threshold of P+ to determine the polarity ofinstances, which can be empirically estimated from data.
The instances which have much higher P+than the threshold will be added to the set of positive instances.
For example, London and Tokyo inFigure 1 (b) are selected as positive instances.To select discriminant negative instances, we observed that not all negative instances are the sameuseful for the expansion boundary determination.
Intuitively, the discriminant negative instances arethose negative instances which are highly overlapped with the positive instances.
For instance, due tothe lower overlap between categories Fruit and Capital, Apple is not a discriminant negative instancesince it provides little information for the expansion boundary determination.
Therefore, the instancesnear the threshold are used as the discriminant negative instances in the next iteration.
(Notice that, thecomputation of GE, PE and PNE still uses all positive and negative instances, rather than onlydiscriminant negative instances).
For example, in Figure 1(b), Shanghai, Milan and Chicago areselected as discriminate negative instances, and Nokia will be neglected.
Finally the boundary betweenCapital and City can be determined by the positive instances and the discriminant negative instances.4 Experiments4.1 Experimental SettingsCategory Description Category DescriptionCAP Place: capital name FAC Facilities: names of man-made structuresELE chemical element ORG Organization: e.g.
companies, governmentalFEM Person: female first name GPE Place: Geo-political entitiesMALE Person: male first name LOC Locations other than GPEsLAST Person: last name DAT Reference to a date or periodTTL Honorific title LANG Any named languageNORP Nationality, Religion, Political(adjectival)Table 3: Target categoriesCorpus: In our experiments, we used the Google Web 1T corpus (Brants and Franz, 2006) as ourexpansion corpus.
Specifically, we use the open source package LIT-Indexer (Ceylan and Mihalcea,2011) to support efficient wildcard querying for pattern generation and instance extraction.2285Target Expansion Categories: We conduct our experiments on thirteen categories, which are shownin Table 3.
Eleven of them are from Curran et al.
(2007).
Besides the eleven categories, to evaluatehow well ESE systems can resolve the semantic drift problem, we use two additional categories(Capital and Chemical Element) which are high likely to drift into other categories.Evaluation Criteria: Following Curran et al (2007), we use precision at top n (P@N) as theperformance metrics, i.e., the percentage of correct entities in the top n ranked entities for a givencategory.
In our experiments, we use P@10, P@20, P@50, P@100 and P@200.
Since the output is aranked list of extracted entities, we also choose the average precision (AP) as the evaluation metric.
Inour experiments, the correctness of all extracted entities is manually judged.
In our experiments, wepresent results to 3 annotators, and an instance will be considered as positive if 2 annotators label it aspositive.
We also provide annotators some supporting resources for better evaluation, e.g., the entitylist of target type collected from Wikipedia.4.2 Experimental ResultsIn this section, we analyze the effect of negative instances, categories boundaries, and seed selectionstrategies.
We compare our method with the following two baseline methods: i) Only_Pos (POS):This is an entity set expansion system which uses only positive seeds.
ii) Mutual_Exclusion (ME):This is a mutual exclusion bootstrapping based ESE method, whose expansion boundary is determinedby the exclusion of the categories.We implement our method using two different settings: i) Hum_Co-Bootstrapping (Hum_CB):This is the proposed Co-Bootstrapping method in which the initial negative seeds are manually given.Specifically, we randomly select five positive seeds from the list of the category?s instances while theinitial negative seeds are manually provided.
ii) Feedback_Co-Bootstrapping (FB_CB): This is ourproposed probabilistic Co-Bootstrapping method with two steps of selecting initial negative seeds:1) Expand the entity set using only the positive seeds for only first iteration.
Return the top teninstances.
2) Select the negative instances in the top ten results of the first iteration as negative seeds.4.2.1.
Overall PerformanceSeveral papers have shown that the experimental performance may vary with different seed choices(Kozareva and Hovy, 2010; McIntosh and Curran, 2009; Vvas et al., 2009).
Therefore, we input theESE system with five different positive seed settings for each category.
Finally we average theperformance on the five settings so that the impact of seed selection can be reduced.P@10 P@20 P@50 P@100 P@200 MAPPOS 0.84 0.74 0.55 0.41 0.34 0.42ME 0.83(0.90) 0.79(0.87) 0.68(0.78) 0.58(0.67) 0.51(0.59) -Hum_CB 0.97 0.95 0.83 0.71 0.57 0.78FB_CB 0.97 0.96 0.90 0.79 0.66 0.85Table 4: The overall experimental resultsTable 4 shows the overall experimental results.
The results in parentheses are the known results ofeleven categories (without CAP and ELE) shown in (Curran et al., 2007).
MAP of ME is missedbecause there are no available results in (Curran et al., 2007).
From Table 4, we can see that:1) Our method can achieve a significant performance improvement: Compared with thebaseline POS, our method Hum_CB and FB_CB can respectively achieve a 23% and 32%improvement on P@200; Compared with the baseline ME, our method Hum_CB and FB_CB canrespectively improve P@200 by 6% and 15%.2) By explicitly representing the expansion boundary, the expansion performance can beincreased: Compared with the baseline POS, ME can achieve a 17% improvement on P@200, and ourmethod Hum_CB can achieve a 23% improvement on P@200.3) The negative seeds can better determine the expansion boundary than mutually exclusivecategories.
Compared with ME, Hum_CB and FB_CB can respectively achieve a 6% and 15%improvement on P@200.
We believe this is because using negative instances is a more accurate andmore robust way for defining and maintaining the expansion boundary than mutually exclusivecategories.22864) The system?s feedback is useful for selecting negative instances: Compared with Hum_CB,FB_CB method can significantly improve the P@200 by 9.0%.
We believe this is because that thesystem?s feedback is a good indicator of the semantic drift direction.
In contrast, it is usually difficultfor human to determine which directions the bootstrapping will drift towards.4.2.2.
Detailed Analysis: Expansion BoundaryIn Table 5, we show the top 20 positive and negative Capital instances (FB_CB setting).
From Table 5,we can make the following observations: 1) Our method can effectively generate negative instances.In Table 5, the negative instances contain cities, states, countries and general terms, all of which havea high semantic overlap with Capital category.
2) The positive instances and negative instancesgenerated by our Co-Bootstrapping method can discriminately determine the expansion boundary.
Forinstance, the negative instances Kyoto can distinguish Capital from City; Australia and China candistinguish Capital from Country;Positive InstancesLondon,  Paris,  Moscow,  Beijing,  Madrid,  Amsterdam,  Washington,  Tokyo,  Berlin,  Rome,Vienna,  Baghdad,  Athens,  Bangkok,  Cairo,  Dublin,  Brussels,  Prague,  San,  BudapestNegative Instances(with categories)City Kyoto,  Kong,  Newcastle,  Zurich,  Lincoln,  Albany,  Lyon,  LA,  ShanghaiCountry China,  AustraliaGeneral downtown,  AprilState Hawaii,  Oklahoma,  ManhattanOther Hollywood,  DC,  Tehran,  CharlotteTable 5: Top 20 positive instances and negative instances (True positive instances are in bold)4.2.3.
Detailed Analysis: Semantic Drift ProblemPOSStockholm,  Tampa,  East,  West,  Springfield,  Newport, Cincinnati,  Dublin,  Chattanooga,  Savannah,Omaha,  Cambridge,  Memphis,  Providence,  Panama,  Miami,  Cape,  Victoria,  Milan,  BerlinMELondon,  Prague,  Newport,  Cape,  Dublin,  Savannah,  Chattanooga,  Beijing,  Memphis,  Athens,Berlin,  Miami,  Plymouth,  Victoria,  Omaha,  Tokyo,  Portland,  Troy,  Anchorage,  BangkokHum_CBLondon,  Rome,  Berlin,  Paris,  Athens,  Moscow,  Tokyo,  Beijing,  Prague,  Madrid,  Vienna,Dublin,  Budapest,  Amsterdam,  Bangkok,  Brussels,  Sydney,  Cairo,  Washington,  BarcelonaFB_CBLondon,  Paris,  Moscow,  Beijing,  Washington,  Tokyo,  Berlin,  Rome,  Vienna,  Baghdad,Athens,  Bangkok,  Cairo,  Brussels,  Prague,  San,  Budapest,  Amsterdam,  Dublin,  MadridTable 6: Top 20 instances of all methods (True positive instances are in bold)To analyze how our method can resolve the semantic drift problem, Table 6 shows the top 20 positiveCapital instances of different methods.
From Table 6, we can make the following observations: i)Different methods can resolve the semantic drift problem to different extent: ME is better than POS,with 50% instances being positive, and our method is better than ME, with 95% instances beingpositive.
ii) The Co-Bootstrapping method can effectively resolve the semantic drift problem: 25% ofPOS?s top 20 instances and 50% of ME?s top 20 instances are positive.
In contrast, 90% of Hum_CB?stop 20 instances and 95% of FB_CB?s top 20 instances are positive respectively.
It proves thatCo-Bootstrapping method can better resolve the semantic drift problem than POS and ME.4.3 Parameter OptimizationFigure 4: The MAP vs. threshold of P+Our method has only one parameter: threshold of P+, which determines the instance?s polarity.Intuitively, a larger threshold of P+ will improve the precision of the positive instances but will regardsome positive instances as negative instances mistakenly.
As shown in Figure 4, our method canachieve the best MAP performance when the value of the threshold is 0.6.00.20.40.60.810.0 0.2 0.4 0.6 0.8 1.0MAPThreshold of P+MAP22874.4 Comparison with State-of-the-Art SystemsWe also compare our method with three state-of-the-art systems: Google Sets1-- an ESE applicationprovided by Google, SEAL2 -- a state-of-the-art ESE method proposed by Wang and Cohen (2008),and WMEB -- a state-of-the-art mutual exclusion based system proposed in McIntosh and Curran(2008).
To make a fair comparison, we directly use the results before the adjustment which missP@10 and P@50 in their original paper (McIntosh and Curran, 2008) and compared the performanceof these systems on nine categories in (McIntosh and Curran, 2008).
For each system, we conduct theexperiment five times to reduce the impact of seeds selection.
The average P@10, P@50, P@100 andP@200 are shown in Figure 5.Figure 5: The results compared with three state-of-the-art systemsFrom the results shown in Figure 5, we can see that our probabilistic Co-Bootstrapping method canachieve state-of-the-art performance on all metrics: Compared with the well-known baseline GoogleSets, our method can get a 42.0% improvement on P@200; Compared with the SEAL baseline, ourmethod can get a 35.0% improvement on P@200; Compared with the WMEB method, our method canachieve a 6.2% improvement on P@100 and a 3.1% improvement on P@200.5 Conclusion and Future WorkIn this paper, we proposed a probabilistic Co-Bootstrapping method for entity set expansion.
Byintroducing negative instances to define and refine the expansion boundary, our method caneffectively resolve the expansion boundary problem and the semantic drift problem.
Experimentalresults show that our method achieves significant performance improvement over the baselines, andoutperforms three state-of-the-art ESE systems.
Currently, our method did not take into account thelong tail entity expansion, i.e., the instances which appear only a few times in the corpus, such asSaipan, Roseau and Suva for the Capital category.
For future work, we will resolve the long tailentities in our Co-Bootstrapping method by taking the sparsity of instances/patterns into consideration.6 AcknowledgementsWe would like to thank three anonymous reviewers for invaluable comments and suggestions toimprove our paper.
This work is supported by the National Natural Science Foundation of China underGrants no.
61100152 and 61272324, and the National High Technology Development 863 Program ofChina under Grants no.
2013AA01A603.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball: Extracting Relations from Large Plain-Text Collections.In: Proceedings of the fifth ACM conference on Digital libraries (DL-00), Pages 85-94.Joohui An, Seungwoo Lee, and Gary Geunbae Lee.
2003.
Automatic acquisition of named entity tagged corpusfrom world wide web.
In: Proceedings of ACL-03, Pages 165-168, Volume 2.Thorsten Brants and Alex Franz.
2006.
Web 1t-5gram version1.
http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2006T131 https://docs.google.com/spreadsheet/2 http://www.boowa.com/0.978 0.9090.8480.77300.20.40.60.81P@10 P@50 P@100 P@200Google Sets SEAL WMEB Co-Bootstrapping2288Sergey Brin.
1998.
Extracting patterns and relations from the World Wide Web.
In: Proceedings of theWorkshop at the 6th International Conference on Extending Database Technology, Pages 172-183.Michael J. Cafarella, Doug Downey, Stephen Soderland, and Oren Etzioni.
2005.
KnowItNow: Fast, ScalableInformation Extraction from the Web.
In: Proceedings of EMNLP-05, Pages 563-570.Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen Liao, Enhong Chen, and Hang Li.
2008.
Context-awarequery suggestion by mining click-through and session data.
In Proceedings of KDD-08, pages 875?883.Hakan Ceylan and Rada Mihalcea.
2011.
An Efficient Indexer for Large N-Gram Corpora.
In: Proceedings ofSystem Demonstrations of ACL-11, Pages 103-108.William W. Cohen and Sunita Sarawagi.
2004.
Exploiting dictionaries in named entity extraction: combiningsemi-Markov extraction processes and data integration methods.
In: Proceedings of KDD-04, Pages 89-98.Alessandro Cucchiarelli and Paola Velardi.
2001.
Unsupervised Named Entity Recognition Using Syntactic andSemantic Contextual Evidence.
In: Computational Linguistics, Pages 123-131, Volume 27.James R. Curran, Tara Murphy, and Bernhard Scholz.
2007.
Minimising semantic drift with Mutual ExclusionBootstrapping.
In: Proceedings of the 10th Conference of the Pacific Association for ComputationalLinguistics, Pages 172?180.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S.Weld, and Alexander Yates.
2005.
Unsupervised Named-Entity Extraction from the Web: An ExperimentalStudy.
In: Artificial Intelligence, Pages 91-134, Volume 165.Jian Hu, Gang Wang, Fred Lochovsky, Jiantao Sun, and Zheng Chen.
2009.
Understanding user?s query intentwith Wikipedia.
In Proceedings of WWW-09, Pages 471?480.Zornitsa Kozareva and Eduard Hovy.
2010.
Learning arguments and supertypes of semantic relations usingrecursive patterns.
In: Proceedings of ACL-10, Pages 1482?1491.Tara McIntosh and James R. Curran.
2008.
Weighted mutual exclusion bootstrapping for domain independentlexicon and template acquisition.
In: Proceedings of the Australasian Language Technology AssociationWorkshop, Pages 97-105.Tara McIntosh and James R. Curran.
2009.
Reducing semantic drift with bagging and distributional similarity.In: Proceedings of ACL-09, Pages 396-404.Patrick Pantel and Dekang Lin.
2002.
Discovering word senses from text.
In: Proceedings of KDD-08, Pages613-619.Patrick Pantel and Deepak Ravichandran.
2004.
Automatically Labeling Semantic Classes.
In: Proceedings ofHLT/NAACL, Pages 321-328, Volume 4.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso: Leveraging Generic Patterns for AutomaticallyHarvesting Semantic Relations.
In: Proceedings of ACL-06, Pages 113?120.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu and Vishnu Vyas.
2009.
Web-ScaleDistributional Similarity and Entity Set Expansion.
In: Proceedings of EMNLP-09, Pages 938-947.Marius Pasca.
2007.
Weakly-supervised discovery of named entities using web search queries.
In: Proceedings ofCIKM-07, Pages 683-690.Marco Pennacchiotti, Patrick Pantel.
2011.
Automatically building training examples for entity extraction.
In:Proceedings of CoNLL-11, Pages 163-171.Ellen Riloff and Rosie Jones.
1999.
Learning dictionaries for information extraction using multi-levelbootstrapping.
In: Proceedings of AAAI-99, Pages 474-479.Partha P. Talukdar, Joseph Reisinger, Marius Pasca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira.2008.
Weakly-supervised acquisition of labeled class instances using graph random walks.
In: Proceedings ofEMNLP-08, Pages 582-590.Michael Thelen and Ellen Riloff.
2002.
A bootstrapping method for learning semantic lexicons using extractionpattern contexts.
In: Proceedings of ACL-02, Pages 214-221.Richard C. Wang and William W. Cohen.
2008.
Iterative Set Expansion of Named Entities using the Web.
In:Proceedings of ICDM-08, Pages 1091-1096.2289Richard C. Wang and William W. Cohen.
2009.
Automatic Set Instance Extraction using the Web.
In:Proceedings of ACL-09, Pages 441-449.Vishnu Vvas, Patrick Pantel and Eric Crestan.
2009.
Helping editors choose better seed sets for entity setexpansion.
In: Proceedings of CIKM-09, Pages 225-234Roman Yangarber, Winston Lin and Ralph Grishman.
2002.
Unsupervised learning of generalized names.
In:Proceedings of COLING-02, Pages 1-7.2290
