Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2075?2086, Dublin, Ireland, August 23-29 2014.BEL: Bagging for Entity LinkingZhe Zuo, Gjergji Kasneci, Toni Gruetze, Felix NaumannHasso Plattner InstituteProf.-Dr.-Helmert-Stra?e 2-3, 14482 Potsdam, Germany{firstname.lastname}@hpi.uni-potsdam.deAbstractWith recent advances in the areas of knowledge engineering and information extraction, the taskof linking textual mentions of named entities to corresponding ones in a knowledge base hasreceived much attention.
The rich, structured information in state-of-the-art knowledge basescan be leveraged to facilitate this task.
Although recent approaches achieve satisfactory accuracyresults, they typically suffer from at least one of the following issues: (1) the linking qualityis highly sensitive to the amount of textual information; typically, long textual fragments areneeded to capture the context of a mention, (2) the disambiguation uncertainty is not explicitlyaddressed and often only implicitly represented by the ranking of entities to which a mentioncould be linked, (3) complex, joint reasoning negatively affects the efficiency.We propose an entity linking technique that addresses the above issues by (1) operating on atextual range of relevant terms, (2) aggregating decisions from an ensemble of simple classifiers,each of which operates on a randomly sampled subset from the above range, (3) following lo-cal reasoning by exploiting previous decisions whenever possible.
In extensive experiments onhand-labeled and benchmark datasets, our approach outperformed state-of-the-art entity linkingtechniques, both in terms of quality and efficiency.1 IntroductionNamed-entity linking (NEL) is the task of establishing a mapping from textual mentions of named entitiesto canonical representations of those entities in a knowledge base.
Often, textual mentions are ambigu-ous; that is, a mention could refer to multiple named entities, but only one of them is correct in the giventextual context.
Resolving these ambiguities is often referred to as named entity disambiguation (NED),which is a highly challenging aspect of an NEL process.
More specifically, a robust NEL algorithm hasto robustly resolve ambiguities and thus build on robust NED methods.
The NED problem, however, isoften ill-posed, as only the right context and background knowledge can help disambiguate entities.
Inmany cases, the contextual information is implicit in nature and may be latently spread across variouspassages or documents, and background knowledge may not be sufficient, which makes the disambigua-tion task challenging even for human readers.
As an example, consider the sentence: ?London spent$80,000 ($2,040,000 in current value) to build a 15,000-square-foot stone mansion (?Wolf House?)
onthe property.?
A human reader knows that in general money is spent by people, but sometimes also citycouncils can spend money, and hence, in the above sentence ?London?
may refer to a person or to thecity of London.
However, when considering the contextual information, especially the key phrase ?WolfHouse?, and the fact that this was the name of the mansion of the writer Jack London, the disambiguationof ?London?
becomes obvious.The NED problem is abundant, and the above subtleties place it right at the heart of many artificialintelligence applications, such as semantic search, machine translation, business intelligence, topic de-tection, text summarization, machine vision, and many more.
In the context of information systems,the problem has been addressed in many different flavors and settings, e.g., in the structured setting ofThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/2075record-linkage and duplicate detection, where the goal is find database records that refer to the samenamed entity (Bhattacharya and Getoor, 2007; Naumann and Herschel, 2010), in the semi-structured set-ting of cleaning XML data (Weis and Naumann, 2005) or annotating Web tables (Limaye et al., 2010), inthe context of enriching Wikipedia information boxes (Wu and Weld, 2008), for the alignment of knowl-edge bases (Aumueller et al., 2005; Lacoste-Julien et al., 2013), and most prominently, in the setting ofNatural Language Processing (Bagga and Baldwin, 1998; Mann and Yarowsky, 2003; Fleischman andHovy, 2004; Bunescu and Pasca, 2006; Cucerzan, 2007), which is also the setting of this work.In the latter setting, the proliferation of clean knowledge bases with rich semantic relations betweenWeb entities, e.g., DBpedia (Auer et al., 2007), Freebase (Bollacker et al., 2008), or YAGO (Suchaneket al., 2007), has given rise to novel, reliable NEL techniques (see Section 2) that exploit the semanticrelatedness between entities for the linking process (Shen et al., 2012; Hoffart et al., 2011b; Hoffart etal., 2012).Our disambiguation model builds on a majority-voting strategy that employs a bagging of multipleranking classifiers, thus the name BEL: Bagging for Entity Linking.
Each ranking classifier operates ona randomly sampled subset of terms surrounding the mention in focus.
These terms are sampled from aso-called textual range of relevant terms, i.e., terms that are most promising for determining the contextof the mention.
Finally, based on the sampled terms, each ranking classifier proposes a ranked list ofcandidate entities and the mention is linked to the entity that is proposed as top-ranked candidate by themajority of the classifiers.In summary, the main contributions of this work are:1.
A novel ensemble-based disambiguation approach that exploits the terms that surround a textualmention to best capture its context; a parsimonious linking model that combines the above methodwith a prior probability (similar to the one presented in (Fader et al., 2009), (Hoffart et al., 2011b),or (Lin et al., 2012)) of a candidate named entity being referred to by a given mention yields ahighly efficient linking process.2.
An analysis of the disambiguation impact of the components used in BEL on the final linkingdecision.3.
A detailed quality and efficiency comparison with the state-of-the-art methods of Cucerzan(2007), Hoffart et al.
(2011b), and Hoffart et al.
(2012) on multiple real-world and synthetic datasets;apart from being more efficient, BEL also achieves a linking quality that is comparable to or evenbetter than that of the above methods.The remainder of the paper is organized as follows: The next section discusses related work.
Section 3is devoted to our NEL approach.
The experimental evaluation is presented in Section 4, before weconclude in Section 5.2 Background and Related WorkThere is a vast array of literature on the topic of resolving ambiguous mentions of named entities.
Wefocus on relevant disambiguation strategies for the NEL problem and leave aside natural language pro-cessing (NLP) techniques on named entity recognition and part-of-speech tagging; although, admittedly,for the recognition of entity mentions, such techniques are indispensable.
In this work, we assume suchNLP techniques are given and use the Stanford NER Tagger (Finkel et al., 2005) to reliably recognizetextual mentions of named entities.
Another field that we bypass is that of record linkage or duplicatedetection, where the focus is on comparing sets of database records and identifying mappings betweenrecords referring to the same entity.
Obviously, record linkage methods operate on structured data, suchas database entries with a predefined set of attributes (commonly with a known value range), which isdifferent from our NLP setting.In traditional methods, each mention and each named entity is represented by a vector of terms occur-ring in its textual context.
Vector-based similarity measures are applied to capture the affinity betweena mention and a named entity.
The feature values can go beyond simple unigram terms and consist2076of compound terms, such as bigrams, key phrases, encyclopedic facts, or categorical descriptions.
Forexample, Pedersen et al.
(2005) employed salient bigrams to represent the context of a mention; Mannand Yarowsky (2003) included biographic facts into the vector representation of a named entity, whereasCucerzan (2007) extended the term-based feature set of a Wikipedia entity by information from otherarticles linking to it, but instead of using the whole article text, only some key phrases and immediateWikipedia categories were included.
Bunescu and Pasca (2006), after deriving an entity dictionary fromWikipedia, for a given mention, rank entities by a kernel-based similarity between the textual context ofthe mention and the Wikipedia text and categories of the candidate entity.
The mention is linked to themost similar entity.The disambiguation problem has also been formulated as a probabilistic reasoning problem.
For ex-ample, Fleischman and Hovy (2004) trained a maximum entropy model to infer the probability that twomentions represent the same entity and used a modified agglomerative clustering algorithm to clustermentions using the probabilistic similarity measure.
Similarly, Sil et al.
(2012) used a log-linear modelto represent the probability of a named entity being referred to by a mention.
For both above methods,the selection of features and efficient strategies for learning their weights are crucial, as ideally all fea-ture weights should be learned in a joint fashion, which can be computationally expensive and is oftenimpeded by the ?curse of dimensionality?.Note that many of the above techniques model the implicit relatedness between terms (and term com-pounds), where the general idea is that two terms are related if many Web pages contain both of them.Measures building on this idea were refined and extended in (Milne and Witten, 2008) and (Huang etal., 2012), especially for the relatedness between Wikipedia articles.
Such implicit relatedness can leadto a large candidate space; to effectively prune this space, entity prominence priors have been integratedin various recent disambiguation models, e.g., (Fader et al., 2009; Hoffart et al., 2011b; Lin et al., 2012).Other techniques model explicit, relationship-based similarities between entities; for example, Du etal.
(2013) employed similarity measures that captured the average pair-wise proximity between candidateentities in the knowledge graph, as well as their average pair-wise conceptual similarity by means of thelowest-common-ancestor classes.
(Hoffart et al., 2011b; Hoffart et al., 2012) exploited the hypernymy-and key-phrase-based relatedness, between the k candidate entities in the knowledge base, to jointly linkk mentions occurring in the same paragraph.
A prior probability of a candidate entity being referred toby a mention was combined with the above relatedness measures in an objective maximization function.The intuition behind the hypernymy-based relatedness was that in order for k mentions (that occur inthe same textual context) to be linked correctly to l ?
k named entities in the knowledge base, the lentities should jointly exhibit a high ?semantic?
relatedness, which in (Hoffart et al., 2011b) is referredto as coherence.
Despite this principled modeling of the NEL problem in (Hoffart et al., 2011b; Hoffartet al., 2012) and the impressive quality results reported in those works, efficiency seems to be the mainbottleneck of such collective inference models.
We argue that a Web-scale NEL process should avoidcomplex reasoning strategies wherever possible.
Concerns along these lines have been also expressedin (Lin et al., 2012), where the authors highlight the need for the application of NEL techniques at Webscale.The approach presented in this paper, BEL, avoids complex, coherence-based joint reasoning.
It alsoavoids the processing of long textual passages, where multiple mentions have to occur.
Instead, we showthat a careful light-weight, independent reasoning on the linking of mentions can lead to a linking qualitythat is comparable to and sometimes even better than the one achieved by the above methods.3 The BEL AlgorithmIn this work, the focus is not on the recognition of named entity mentions in a text but rather on theirdisambiguation once the mentions are known.
Throughout this work we assume that a reliable namedentity recognition tool is available.
BEL relies on the Stanford NER Tagger (Finkel et al., 2005) torecognize textual mentions of named entities.
Once the mentions have been recognized, BEL retrievespromising candidate entities from the knowledge base and employs a careful, majority-voting algorithmto take the best possible linking decision based on the textual context of the mentions.
The method is2077described in the following subsections.3.1 High-Level Overview of the BEL AlgorithmAlgorithm 1 gives a high-level overview of the BEL approach.
The only assumption we make is thatthe textual corpus from which the knowledge base has been derived is freely available.
For example, thetextual corpus of knowledge bases such as YAGO or DBpedia is Wikipedia, which is an open source ofinformation about the entities in the two knowledge bases.Exemplarily, in Algorithm 1, we use the YAGO knowledge base to highlight the main idea of thealgorithm.
YAGO is a clean knowledge base with structured information about a large proportion of theentities contained in Wikipedia, thus being a popular representative of many state-of-the-art knowledgebases derived from Wikipedia.Once the set of mentions has been derived from a given document (line 1), for each mention, a list ofpromising candidates is derived from Wikipedia.
The candidates are ranked by a so-called ?prominence?score, representing the probability of a Wikipedia article (i.e., the entity represented by the article) beingreferred to by the mention (lines 2, 3).
In case the list of candidates is empty, the corresponding mentionis linked to a designated entity, ENULL, meaning that the mention cannot refer to a YAGO entity (lines4, 5).
The same holds for the case that the top-ranked candidate occurs in Wikipedia but not in YAGO(lines 7 - 9).
Otherwise, the joint majority decision of multiple bagged ranking classifiers is computed(lines 11 - 13).
Only if there is a majority consensus about a candidate (i.e., the candidate is ranked astop candidate by the majority of the classifiers), the mention is linked to that candidate; otherwise, themention is linked to ENULL(lines 14 - 18).09.01.20141??????????...????????...????????...????????...???
?, ??
?
,????
?, ??
?
,?
?
?
???
?, ??
?
,??
????
?
= ??
?, ?
,?,?
??|??|??
?
= ??
?, ?
,?,?
??|??|??
?
= ??
?, ?
,?,?
??|??|candidateentities?
?
= ?
?, ?
,?,?
?
?Figure 1: Strategy for generating ranking classifiers,each of which operates on a randomly sampled sub-set Si(m) from a set R(m) of relevant terms sur-rounding m and assigns contextual similarity scoresto the candidate entities based on that subset.In addition, for efficiency reasons, BEL exploits previous disambiguation decisions whenever possible.If a mention occurs multiple times in a document and was already reliably linked b times to the samenamed entity in YAGO, the previous linking decisions (for that mention) are reused without rerunningthe disambiguation process (i.e., in the experimental evaluation of BEL, the default value of b is 2).
Thisheuristics may lead to incorrect linkings, but in empirical evaluations on real-world datasets the algorithmhas shown a robust quality behavior, while being highly efficient (see Section 4).The runtime of the algorithm is dominated by the computation of the contextual similarity scores ofeach ranking classifier.
More specifically, since the parameters needed by each classifier are precom-puted, each classifier needs only O(N logN) steps to propose a context-based ranking of the N candi-dates derived by the ?prominence?
score.
Since the classifiers operate independently from each other,2078the algorithm allows parallel computation of the contextual similarity scores.
However, in this work wehave implemented a sequential version, which for K different ranking classifiers has a complexity ofO(KN logN).3.2 Bagging of Ranking Classifiers for Majority VotingThe main idea behind BEL is to leverage different contextual representations of a mention.
Each suchrepresentation is used by one ranking classifier to rank the candidate entities by their similarity to thatrepresentation.
As shown in lines 14 and 15 of Algorithm 1, the mention is linked to the candidate thatis ranked as top entity by the majority of the classifiers.
This idea gives rise to several questions: (1)How to derive the different contextual representations of a mention?
(2) How to compute the similaritybetween a contextual representation and a candidate entity?
(3) How to prune the candidate space in sucha way that only the most promising entities are considered by each of the ranking classifiers?Obviously, the latter question is focused on efficiency; to address it, we exploit a precomputed index,constructed by exploiting the intra-Wikipedia links and the Wikipedia Redirect Pages.
For every mentionm, the index contains entities that might refer to it along with a probabilistic ?prominence?
score P (e|m)by which a candidate entity e might refer to the mention.
To compute this ?prominence?
score, we collectall terms that occur in a Wikipedia article or redirect-page and are hyperlinked to another Wikipediaarticle.
From this collection we derive relative frequencies by which a Wikipedia entity (i.e., specificarticle) was hyperlinked from a given term.
For example, Jordan is hyperlinked to the article about thecountry of Jordan 60% of the time, 20% of the time it is hyperlinked to the basketball player MichaelJordan, etc.
These relative frequencies are estimations of the probability of an entity given a mentionP (e|m).
Empirically (see also Figure 2a in the evaluation section) we have found out that when rankedby this score, the top-40 candidate entities already yield a overwhelming coverage rate of ?92.4% forthe correct entity.
For top-100 this coverage rate increases only marginally (by only ?0.5%).
Thusconsidering only the top-40 entities in the candidate lists (which in general might contain hundreds oreven thousands of entities), is not only an efficient but also an effective pruning strategy.The second question involves the semantics of the similarity score.
Suppose that Si(m) stands forthe i?th contextual representation of the mention m. Our model is probabilistic in nature and holisticin the sense that the above ?prominence?
score P (e|m) just falls off the model by following principledmathematical derivations.
We start by reasoning about the probability of a candidate entity e given themention m and its context Si(m):P (e|Si(m),m) =P (Si(m)|m, e)P (m|e)P (e)P (Si(m)|m)P (m)(1)=P (e|m)P (Si(m)|m, e)P (Si(m)|m)(2)?
P (e|m)P (Si(m)|m, e) (3)?
logP (e|m) + logP (Si(m)|m, e) (4)The last two steps in the above derivation mean that ranking the candidate entities by the similar-ity score P (e|m)P (Si(m)|m, e) or by logP (e|m) + logP (Si(m)|m, e) yields the same ranking asP (e|Si(m),m).
Note that in general Si(m) depends on the entity e and not on the mention m. Hence,we can estimate P (Si(m)|m, e) as P (Si(m)|e).
So the final similarity score is given by:Sim(e, Si(m),m) := logP (e|m) + logP (Si(m)|e) (5)We estimate P (Si(m)|e) as the probability of Si(m) being generated by a language model (Zhai andLafferty, 2004) on the terms describing e in Wikipedia.
Those terms are collected from the Wikipediaarticle of e after removing stop words.
Such a language model is described by means of frequencyparameters ?e.
We construct it by indexing the terms and their frequencies in the correspondingWikipedia articles.
Figure 1 depicts the general idea behind our approach.
For different contextualrepresentations S1(m), ..., Sn(m) of a mention m, the ranking classifier responsible for Si(m) computesSim(e, Si(m),m) for each candidate entity e and ranks the candidates by this score.
Finally m is linkedto the candidate that is ranked as the top entity by the majority of the ranking classifiers.
This majority2079voting strategy reduces the uncertainty of the linking process and leads to higher precision than a singleranking classifier, while still maintaining a high recall.The final question concerns the computation of the contextual representations Si(m) of a mention m.We derive such representations by randomly sampling terms that occur in the local vicinity of a mentionin the text.
More specifically, to generate a contextual representation Si(m) from a range of N relevantterms around a mention m, we uniformly sample N times with replacement.
We run the same procedurefor all n representations.
This sampling technique is known as bootstrapping (Breiman, 1996) and hasbeen shown to have several advantages over other sampling procedures, such as increasing the contextualdiversity and mitigating strong dependencies between features.
Indeed, in the experiments, the baggingof the ranking classifiers lead to a significant improvement of ?2.5% in terms of precision compared tothe simple case where no bagging is used (see Section 4).3.3 Recognizing Non-YAGO EntitiesFor an improved accuracy of the linking process, it is also crucial to reliably recognize true negatives,i.e., mentions that refer to entities that are not present in the underlying knowledge base.
In case of theYAGO knowledge base, we first check whether the most prominent Wikipedia entity for a given mentionis presented in YAGO; if this is not the case, the mention is classified as a non-YAGO entity.
Notethat many entities from Wikipedia are not present in YAGO, either due to recently added articles, or toarticles that represent concepts1.
Furthermore, a flexible threshold is used to recognize a non-YAGOentity.
It is calculated as the maximum similarity score among the Wikpedia entities in the candidatelist that are not present in the knowledge base, or as a default ?prominence?
score, when there is nosuch entity.
If none of the candidates has a higher score than the threshold, the corresponding classifierproposes ENULLas the best candidate.
Also, in the simple case that the retrieved list of candidatesis empty, the mention is classified as a non-YAGO entity.
Although, these strategies are relativelystraight-forward, they lead to a notable improvement in the recognition of true negatives.
Furtherinvestigation of more elaborate strategies for the reliable detection of true negatives is part of our futurework agenda.3.4 Efficiency AspectsFor a better overview of the key efficiency aspects that are leveraged by BEL, we give here a succinctsummary:?
Early pruning of the candidate space while maintaining a high coverage of promising candidates?
Local and independent reasoning strategy based on sliding windows and bootstrapping aggregationfor the disambiguation process?
Highly efficient, in-memory processing of randomly sampled subsets?
Previous disambiguation decisions are exploited whenever possible; e.g., for people, locations, orcompany names that reoccur in a similar form in a document, the disambiguation process is runonly once.As it will be shown in the next section, the above considerations lead to a highly efficient linking processthat often outperforms the evaluated state-of-the-art techniques, both in terms of quality and efficiency.4 Experimental Evaluation4.1 DatasetsThree datasets were used to evaluate the BEL approach.
As a knowledge base for evaluation, we usedYAGO2 (Hoffart et al., 2011a).1In YAGO, the concepts have been derived from WordNet.2080Table 1: Datasets overall informationCoNLL-YAGO CUCERZAN KOREarticles 76 336 50mentions (total) 1431 5343 148mentions (non-YAGO) 279 936 7word count (avg.)
173 384 12CoNLL-YAGO: This dataset contains 76 randomly picked Reuters news articles of CoNLL 2003data (Tjong Kim Sang and De Meulder, 2003).
We have manually labeled the mentions, which arerecognized by the Stanford NER Tagger (Finkel et al., 2005), to the corresponding entities in YAGO2.CUCERZAN: This dataset consists of 350 Wikipedia articles that were randomly selected by S.Cucerzan to evaluate his approach (Cucerzan, 2007).
The annotated entities in this corpus are namedentities derived from the hyperlinks of mentions in these 350 Wikipedia articles.
Since some of the arti-cles are not available anymore in the Wikipedia archive, we have recovered 336 out of the 350 articles ofthe original corpus.KORE: This small dataset was produced in the realm of AIDA (Hoffart et al., 2012).
It is a syntheticcorpus consisting of 50 very short articles, where each article contains one or more hand-crafted sen-tences about different ambiguous mentions of named entities.
This dataset is quite difficult, as the namedentities in this corpus are ambiguous with sparse context.4.2 Evaluated ApproachesWe compared BEL to three other prominent approaches (Hoffart et al., 2011b; Hoffart et al., 2012;Cucerzan, 2007), which, as reported in the corresponding papers, outperform many state-of-the-art al-gorithms in terms of disambiguation and linking quality.
Experience-wise, we can confirm that the veryrecent AIDA approaches (Hoffart et al., 2011b; Hoffart et al., 2012) have indeed raised the bar for manyentity linking methods.
In our experiments, these algorithms showed a highly reliable behavior, evenwith respect to difficult disambiguation tasks.The AIDA approach comes in different versions: In its original version (Hoffart et al., 2011b), it ex-ploits a graph-based connectivity between candidate entities of multiple mentions (i.e., graph coherence,e.g., derived from the type, subclassOf edges of the knowledge graph or from the incoming links inWikipedia articles) to determine the most promising linking of the mentions.
We refer to this version ofAIDA as AIDA-GRAPH.
In another version that has been optimized for datasets such as KORE (Hoffartet al., 2012), AIDA?s coherence model has been extended to recognize key-phrases for named entities,which are then used to determine a similarity score based on key-phrase overlap between candidateentities.
We refer to this version as AIDA-KORE.Cucerzan (2007) finds a linking of mentions to Wikipedia entities, such that the sum of vector-basedsimilarities between the candidate entities and the document (containing the mentions) as well as the sim-ilarities between pairs of candidate entities is maximized.
We refer to this method as LED (Large-scaleEntity Disambiguation).
The original work has been conducted at Microsoft and the code is proprietary.Hence, we had to re-implement the algorithm according to the descriptions in the paper.
To make surethat algorithm was correctly implemented, we evaluated it on the original dataset, and achieved resultscomparable to those presented in the original paper.
Note that, since many entities from Wikipedia arenot present in YAGO, the task of linking mentions of the CUCERZAN dataset to YAGO is different fromthe original task addressed in (Cucerzan, 2007), where mentions were linked to Wikipedia articles.4.3 Parameter Analysis for BELFor BEL, the parameters are optimized to deal with common natural-language articles on the Web (e.g.,articles from encyclopedic pages or news sites).
The same parameter settings are used on all threedatasets described above to show the performance of BEL on different types of corpora.
To achieve sucha common setting of the parameters, we trained BEL on articles sampled from the above datasets, eachof which exhibits specific textual characteristics.20814.3.1 Pruning Candidate ListsIn BEL, each mention is assigned a list of candidates.
In general, such a list could contain hundreds oreven thousands of entities.
However, the mention should be linked to at most one entity in the list.85%86%87%88%89%90%91%92%93%0 10 20 30 40 50 60 70 80 90 100Coverage RateTop-K Candidates(a)67,0%67,5%68,0%68,5%69,0%69,5%70,0%0 50 100 150 200 250 300PrecisionNumber of Random Sub-samples(b)Figure 2: Parameter analysis experiments (in %): (a) Correct Entity Coverage Rate.
(b) Performance ofbagging strategy in precision in comparison to the performance of a single classifier.We randomly picked 1000 mentions from the three datasets to analyze the impact of the candidatelist size.
The coverage rate (i.e., the relative frequency by which the correct entity is contained in thelist) in relation to the list size is shown in Figure 2a.
The lists are sorted by decreasing ?prominence?scores (see Section 3).
In this experiment, 139 mentions have no corresponding entity in YAGO, while61 correct entities are missing, which means that the maximum coverage rate that a candidate selectionstrategy can achieve is 800/861 ?
92.92%.
As the curve shows, most of the correct entities are indeedlocated within the top positions of the candidate lists.
Therefore, we prune the ranked lists by selectingthe top-40 candidates for further processing.4.3.2 Range of Relevant TermsAs mentioned earlier, the bagging of classifiers is aimed at capturing the contextual information ofa mention by randomly sampling terms surrounding it, a process that is repeated several times, oncefor every ranking classifier.
As a sampling procedure we employ bootstrapping (Breiman, 1996),which captures the diversity of contextual information derived from the original range, while mitigatingdependencies between terms.
We analyze the quality of this bagging strategy mainly based on twocriteria: (1) the size of the range of relevant terms, and (2) the bagging size (i.e., number of randomlysampled subsets).55%57%59%61%63%65%67%69%5 20 35 50 65 80 95Relevant Range Size(a) Precision84%86%88%90%92%94%96%98%5 20 35 50 65 80 95Relevant Range Size(b) Recall70%72%74%76%78%80%82%84%5 20 35 50 65 80 95Relevant Range Size(c) F1-measureFigure 3: Performance of the language model by increasing the size of the range of relevant terms.The impact of the range size on the linking quality is intricate, in the sense that a larger range containsmore noise, while a smaller one has sparser context.
In BEL, the range of relevant terms is empiricallycalibrated, by evaluating the performance of BEL on different range sizes after removing stop words andnon-English terms.
To avoid any bias from the ?prominence?
score and to focus only on the context,we set the logP (e|m)-component of the scoring function to 0.
Figure 3 shows the average performance2082based on 10-fold cross validation on all evaluation datasets.
The F1-measure achieves maximum whenthe range of relevant terms contains 55 terms, which is also in agreement with the optimal setting foundby Gooi and Allan (2004).
Thus, we define the size of the range to be 55.
When a document containsless terms, BEL takes the whole text into account.The bagging strategy of BEL encourages the contextual diversity, while it reduces the linking uncer-tainty by employing majority voting.
Here, to show the impact of our bagging strategy, we randomly pick80% articles from our datasets for different bagging sizes.
Since the bagging strategy is mainly affectedby contextual information, we turn off the component responsible for the ?prominence?
score and runBEL 10 times for each bagging size on these articles.
In Figure 2b, the black horizontal line with preci-sion 67.56% is the baseline derived from the single language model classifier.
The black dots denote theaverage precisions and the error bars show the corresponding standard deviation.
As the figure shows, byincreasing the bagging size the precision increases, while the standard deviation decreases.
Consideringthe precision, efficiency, and stability of the algorithm, we use 199 subsets as the default setting (an oddnumber of voters is more likely to avoid ties when there are two top-ranked candidates by the voters).Note that, the linking process is stricter and thus leads to a decreased recall.
However, the experimentalresult shows that the impact of the bagging strategy on the increase of precision is consistently higherthan its impact on the decrease of the F1-measure; the precision increases from 67.56% to 69.73%, whilethe F1-measure decreases from 75.82% to 75.32%.
Moreover, in our opinion, it is better to suggest thata mention is not in the knowledge base than link it to a wrong entity.BEL has been evaluated with respect to its linking quality and efficiency.
The employed evaluationmeasures and the results are presented in the following subsections.4.3.3 Evaluation MeasuresFor the quality evaluation, we have measured precision, recall, and the F1-measure of each of the aboveapproaches on the mentioned datasets.A true positive (tp) is a mention that has been correctly linked to a YAGO entity.
An incorrect linkingis defined to be a false positive (fp).
Furthermore, a true negative (tn) refers to a mention that is correctlyidentified as an entity that does not occur in YAGO (i.e., non-YAGO entity).
The remaining cases aredefined as false negatives (fn).
Precision is then defined as P = tp/(tp+fp) and recall as R = tp/(tp+fn).
The F1-measure is obtained from the harmonic mean of precision and recall as F = 2PR/(P+R).For the efficiency evaluation, we have measured the runtime (in seconds) of each approach on eachdataset.4.3.4 Evaluation of Linking QualityThe results of the quality evaluation are shown in Table 2, along with the corresponding confidenceintervals, which are calculated by repeating 30 times a random sampling of subsets containing 60% ofthe documents from each dataset.
For each dataset, the results computed on all documents are withinthe intervals that correspond to a confidence level of 99% according to the Student?s t-distribution toshow that although some of the datasets are of moderate size, the 99% confidence interval of the scorescomputed on the sampled subsets is relatively small.As it can be seen, BEL significantly outperforms all the other approaches on the CoNLL-YAGOdataset, especially on precision.
Also, for the CUCERZAN dataset, the quality of BEL is compara-ble to that of AIDA-GRAPH and AIDA-KORE, and it significantly outperforms LED.
Moreover, interms of precision, BEL performs also on this latter dataset significantly better than the other approaches(i.e., from a statistical point of view).
Together with BEL?s impressive efficiency (see Section 4.3.5),the precision-related quality is a crucial scalability aspect, since when processing a high throughput ofdocuments it is highly important that the produced linkings be rather correct.For the KORE dataset, AIDA-KORE outperforms other approaches.
However, it should be noted thatKORE is a very challenging dataset and that the AIDA-KORE approach has been specifically tailoredto such datasets.
Also note that although the AIDA-KORE algorithm shows a high linking quality inthe experiments, it is the least efficient approach, since it performs complex joint reasoning over groupsof candidate entities and mentions.
In our experiments, we had to wait more than 15 hours for the2083evaluation results of this approach for KORE dataset, since the mentions contained in this corpus arehighly ambiguous.In comparison to a greedy linking strategy, where a mention is simply linked to the most prominententity according to the ?prominence?
score, which is our baseline BEL-PROM, BEL performs muchbetter on all three datasets.
This fact highlights the importance of the contextual similarity component inthe model.Table 2: Evaluation results (in %).Method Precision Recall F1CoNLL-YAGOLED62.35 96.13 75.63(-1.92,+0.25) (-0.43,+0.27) (-1.50,+0.18)AIDA-GRAPH78.67 96.29 86.59(-0.80,+1.23) (-0.20,+0.64) (-0.41,+0.82)AIDA-KORE77.11 96.21 85.61(-0.86,+0.80) (-0.64,+0.25) (-0.67,+0.47)BEL-PROM68.37 97.40 80.30(-0.89,+1.30) (-0.25,+0.32) (-0.61,+0.97)BEL81.40 95.72 87.98(-1.33,+0.78) (-0.38,+0.25) (-0.85,+0.46)CUCERZANLED63.47 96.94 76.72(-0.40,+1.01) (-0.11,+0.24) (-0.28,+0.75)AIDA-GRAPH81.30 94.64 87.47(-0.57,+0.16) (-0.28,+0.17) (-0.40,+0.11)AIDA-KORE81.35 97.31 88.61(-0.83,+0.03) (-0.25,+0.10) (-0.57,+0.03)BEL-PROM73.92 98.83 84.58(-0.53,+0.29) (-0.11,+0.06) (-0.37,+0.20)BEL82.37 93.46 87.56(-0.31,+0.25) (-0.71,+0.27) (-0.35,+0.12)KORELED40.14 100.00 57.28(-3.30,+0.88) (-0.00,+0.00) (-3.52,+0.79)AIDA-GRAPH62.33 100.00 76.79(-1.83,+0.93) (-0.00,+0.00) (-1.43,+0.68)AIDA-KORE66.67 94.95 78.33(-2.29,+1.91) (-0.87,+1.82) (-1.60,+1.48)BEL-PROM31.29 100.00 47.67(-1.83,+1.47) (-0.00,+0.00) (-2.23,+1.61)BEL54.55 76.61 63.72(-2.40,+2.53) (-0.76,+2.20) (-1.72,+2.08)Table 3: Efficiency comparisonMethod KORE CoNLL-YAGO CUCERZANBEL 30.02s 244.34s 657.44sLED 17.70s 288.52s 552.26sAIDA-GRAPH 615.35s 1,202.66s 2,897.43sAIDA-KORE >15h >11h >25h050010001500200025003000KORE CoNLL-YAGO CUCERZANruntime(second)BEL LED AIDA-GRAPH AIDA-KORE4.3.5 Efficiency EvaluationThe lean algorithmic design of BEL enables a highly efficient linking process.
For the efficiency com-parison, we used a Pentium 3.1GH machine with 8GB of main memory.
The indexes for the languagemodels of YAGO2 entities, as well as the indexed YAGO2 knowledge base (that was used by all ap-proaches for the linking) were maintained in a PostgreSQL 9.1 database.For each dataset, Table 3 shows the runtime of each approach.
Obviously, the joint reasoning strategyof AIDA-GRAPH comes at high efficiency costs; on all datasets it has been outperformed by the otherapproaches.
While LED is slightly more efficient than BEL on the KORE and CUCERZAN datasets,as shown in Table 2, it often pays a high cost in terms of quality, and BEL also outperforms it in termsof efficiency on the CoNLL-YAGO dataset.
Note that the runtime of LED and BEL are both practicallyviable from a user?s perspective.
The AIDA-KORE approach, on the other hand, lacks practical viability,since it needs several hours to process even moderately sized datasets (e.g., approx.
11 hours for theCoNLL-YAGO dataset).
For the CUCERZAN dataset, which is the largest one, although the F1-measureof AIDA-KORE is approximately 1% higher than BEL, one needs to wait more than 25 hours to get theresult.
Instead, BEL can finish the linking process in around 11 minutes.4.3.6 DiscussionBoth, the ?prominence?
score and the contextual score derived from the proposed bagging strategy haveadvantages and limitations; they are orthogonal in nature, and their individual strengths are manifestedin different ways in the final decision of the algorithm.
The value of the ?prominence?
score has ahigh impact on the final decision, when BEL is run on articles about famous people, organizations,locations, products, events etc.
Typical examples of articles that contain such entities are news reports,2084scholarly articles containing encyclopedic knowledge, and product descriptions.
In contrast, the baggedlanguage models have a high impact on the final decision in cases where the occurring mentions arehighly ambiguous but contain valid key information surrounding the mention.
Examples for such articlescan be found in all three datasets we have used.Although in many cases, linking the mention to the most prominent candidate entity leads to the correctdecision (e.g., ?Australia?
refers most probably to the country), this strategy is not reliable for manyambiguously used mentions.
For example, in one article of CoNLL-YAGO dataset, the named entity?Australia National Cricket Team?
in YAGO, was also often referred to by ?Australia?.
Nevertheless,BEL was able to establish a linking to the correct named entity.The datasets we have annotated and a preliminary online-demo of the algorithm are available online2.5 ConclusionThe focus of this work has been on lean and light-weight classification algorithms, which as an ensembleprovide a reliable and efficient linking strategy.
The comparison of our approach, BEL, with state-of-the-art techniques on manually-labeled, benchmark datasets shows that BEL indeed fulfills the abovecriteria.
Especially on longer, real-world texts, BEL shows an unprecedented quality and efficiencybehavior.
Further research is needed to understand how such an approach can be optimized for shorttexts containing highly ambiguous mentions of named entities.
We are convinced that BEL provides arobust basis for further research in this area.ReferencesS?oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives.
2007.
DB-pedia: A nucleus for a web of open data.
The Semantic Web.
Springer.David Aumueller, Hong-Hai Do, Sabine Massmann, and Erhard Rahm.
2005.
Schema and ontology matchingwith COMA++.
In Proceedings of the International Conference on Management of Data, pages 906?908.ACM.Amit Bagga and Breck Baldwin.
1998.
Entity-based cross-document coreferencing using the vector space model.In Proceedings of the annual meeting of the Association for Computational Linguistics, pages 79?85.Indrajit Bhattacharya and Lise Getoor.
2007.
Collective entity resolution in relational data.
ACM Transactions onKnowledge Discovery from Data, 1(1):5.Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008.
Freebase: a collaborativelycreated graph database for structuring human knowledge.
In Proceedings of the International Conference onManagement of Data, pages 1247?1250.Leo Breiman.
1996.
Bagging predictors.
Machine learning, 24(2):123?140.Razvan Bunescu and Marius Pasca.
2006.
Using encyclopedic knowledge for named entity disambiguation.
InProceedings of the Conference of the European Chapter of the Association for Computational Linguistics, pages9?16.Silviu Cucerzan.
2007.
Large-scale named entity disambiguation based on Wikipedia data.
In Proceedings of theJoint Conference on Empirical Methods in Natural Language Processing and Computational Natural LanguageLearning, pages 708?716.Fang Du, Yueguo Chen, and Xiaoyong Du.
2013.
Linking entities in unstructured texts with RDF knowledgebases.
In Web Technologies and Applications, pages 240?251.
Springer.Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009.
Scaling Wikipedia-based named entity disambigua-tion to arbitrary web text.
In IJCAI Workshop: User Contributed Knowledge and Artificial Intelligence: AnEvolving Synergy.Jenny Rose Finkel, Trond Grenager, and Christopher Manning.
2005.
Incorporating non-local information intoinformation extraction systems by Gibbs sampling.
In Proceedings of the annual meeting of the Association forComputational Linguistics, pages 363?370.2http://hpi-web.de/naumann/projekte/bel.html2085Michael Fleischman and Eduard Hovy.
2004.
Multi-document person name resolution.
In Proceedings of theWorkshop on Reference Resolution and its Applications.Chung Heong Gooi and James Allan.
2004.
Cross-document coreference on a large scale corpus.
In Proceedingsof the Conference on Natural Language Learning at HLT-NAACL, pages 9?16.Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, Edwin Lewis-Kelham, Gerard de Melo, and GerhardWeikum.
2011a.
YAGO2: exploring and querying world knowledge in time, space, context, and many lan-guages.
In Proceedings of the International World Wide Web Conference, pages 229?232.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F?urstenau, Manfred Pinkal, Marc Spaniol, BilyanaTaneva, Stefan Thater, and Gerhard Weikum.
2011b.
Robust disambiguation of named entities in text.
InProceedings of the Conference on Empirical Methods in Natural Language Processing, pages 782?792.Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald, and Gerhard Weikum.
2012.
KORE:keyphrase overlap relatedness for entity disambiguation.
In Proceedings of the International Conference onInformation and Knowledge Management, pages 545?554.Lan Huang, David Milne, Eibe Frank, and Ian H. Witten.
2012.
Learning a concept-based document similaritymeasure.
Journal of the American Society for Information Science and Technology, 63(8):1593?1608.Simon Lacoste-Julien, Konstantina Palla, Alex Davies, Gjergji Kasneci, Thore Graepel, and Zoubin Ghahramani.2013.
Sigma: Simple greedy matching for aligning large knowledge bases.
In Proceedings of the 19th ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, pages 572?580.
ACM.Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti.
2010.
Annotating and searching web tables usingentities, types and relationships.
Proceedings of the VLDB Endowment, 3(1-2):1338?1347, September.Thomas Lin, Mausam, and Oren Etzioni.
2012.
Entity linking at web scale.
In Proceedings of the Joint Workshopon Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 84?88.Gideon S. Mann and David Yarowsky.
2003.
Unsupervised personal name disambiguation.
In Proceedings of theConference on Natural Language Learning at HLT-NAACL, volume 4, pages 33?40.David Milne and Ian H. Witten.
2008.
An effective, low-cost measure of semantic relatedness obtained fromwikipedia links.
In Proceeding of AAAI Workshop on Wikipedia and Artificial Intelligence: an Evolving Syn-ergy, pages 25?30.
AAAI Press.Felix Naumann and Melanie Herschel.
2010.
An Introduction to Duplicate Detection.
Morgan and ClaypoolPublishers.Ted Pedersen, Amruta Purandare, and Anagha Kulkarni.
2005.
Name discrimination by clustering similar con-texts.
In Proceedings of the International Conference on Intelligent Text Processing and Computational Lin-guistics, volume 3406 of Lecture Notes in Computer Science, pages 226?237.
Springer.Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012.
LINDEN: linking named entities with knowledgebase via semantic knowledge.
In Proceedings of the International World Wide Web Conference, pages 449?458.Avirup Sil, Ernest Cronin, Penghai Nie, Yinfei Yang, Ana-Maria Popescu, and Alexander Yates.
2012.
Linkingnamed entities to any database.
In Proceedings of the Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational Natural Language Learning, pages 116?127.Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2007.
Yago: a core of semantic knowledge.
InProceedings of the International World Wide Web Conference, pages 697?706.
ACM.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
Introduction to the CoNLL-2003 shared task: language-independent named entity recognition.
In Proceedings of the Conference on Natural Language Learning atHLT-NAACL, volume 4, pages 142?147.Melanie Weis and Felix Naumann.
2005.
DogmatiX tracks down duplicates in XML.
In Proceedings of theInternational Conference on Management of Data, pages 431?442, Baltimore, MD.Fei Wu and Daniel S. Weld.
2008.
Automatically refining the wikipedia infobox ontology.
In Proceedings of theInternational World Wide Web Conference, pages 635?644.Chengxiang Zhai and John Lafferty.
2004.
A study of smoothing methods for language models applied to infor-mation retrieval.
ACM Transactions on Information Systems, 22(2):179?214, April.2086
