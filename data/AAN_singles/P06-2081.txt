Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 627?634,Sydney, July 2006. c?2006 Association for Computational LinguisticsWhose thumb is it anyway?Classifying author personality from weblog textJon OberlanderSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWj.oberlander@ed.ac.ukScott NowsonSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWs.nowson@ed.ac.ukAbstractWe report initial results on the relativelynovel task of automatic classification ofauthor personality.
Using a corpus of per-sonal weblogs, or ?blogs?, we investigatethe accuracy that can be achieved whenclassifying authors on four important per-sonality traits.
We explore both binary andmultiple classification, using differing setsof n-gram features.
Results are promisingfor all four traits examined.1 IntroductionThere is now considerable interest in affective lan-guage processing.
Work focusses on analysingsubjective features of text or speech, such as sen-timent, opinion, emotion or point of view (Panget al, 2002; Turney, 2002; Dave et al, 2003; Liuet al, 2003; Pang and Lee, 2005; Shanahan et al,2005).
Discussing affective computing in general,Picard (1997) notes that phenomena vary in du-ration, ranging from short-lived feelings, throughemotions, to moods, and ultimately to long-lived,slowly-changing personality characteristics.Within computational linguistics, most workhas focussed on sentiment and opinion concern-ing specific entities or events, and on binary clas-sifications of these.
For instance, both Pang andLee (2002) and Turney (2002) consider the thumbsup/thumbs down decision: is a film review posi-tive or negative?
However, Pang and Lee (2005)point out that ranking items or comparing re-views will benefit from finer-grained classifica-tions, over multiple ordered classes: is a film re-view two- or three- or four-star?
And at the sametime, some work now considers longer-term af-fective states.
For example, Mishne (2005) aimsto classify the primary mood of weblog post-ings; the study encompasses both fine-grained(but non-ordered) multiple classification (frus-trated/loved/etc.)
and coarse-grained binary clas-sification (active/passive, positive/negative).This paper is about the move to finer-grainedmultiple classifications; and also about weblogs.But it is also about even more persistent affec-tive states; in particular, it focusses on classifyingauthor personality.
We would argue that ongo-ing work on sentiment analysis or opinion-miningstands to benefit from progress on personality-classification.
The reason is that people vary inpersonality, and they vary in how they appraiseevents?and hence, in how strongly they phrasetheir praise or condemnation.
Reiter and Sripada(2004) suggest that lexical choice may sometimesbe determined by a writer?s idiolect?their per-sonal language preferences.
We suggest that whileidiolect can be a matter of accident or experience,it may also reflect systematic, personality-baseddifferences.
This can help explain why, as Pangand Lee (2005) note, one person?s four star re-view is another?s two-star.
To put it more bluntly,if you?re not a very outgoing sort of person, thenyour thumbs up might be mistaken for someoneelse?s thumbs down.
But how do we distinguishsuch people?
Or, if we spot a thumbs-up review,how can we tell whose thumb it is, anyway?The paper is structured as follows.
It introducestrait theories of personality, notes work to date onpersonality classification, and raises some ques-tions.
It then outlines the weblog corpus and theexperiments, which compare classification accura-cies for four personality dimensions, seven tasks,and five feature selection policies.
We discuss theimplications of the results, and related work, andend with suggestions for next steps.6272 Background: traits and languageCattell?s pioneering work led to the isolation of16 primary personality factors, and later work onsecondary factors led to Costa and McCrae?s five-factor model, closely related to the ?Big Five?models emerging from lexical research (Costa andMcCrae, 1992).
Each factor gives a continu-ous dimension for personality scoring.
Theseare: Extraversion; Neuroticism; Openness; Agree-ableness; and Conscientiousness (Matthews et al,2003).
Work has also investigated whether scoreson these dimensions correlate with language use(Scherer, 1979; Dewaele and Furnham, 1999).Building on the earlier work of Gottschalk andGleser, Pennebaker and colleagues secured signif-icant results using the Linguistic Inquiry and WordCount text analysis program (Pennebaker et al,2001).
This primarily counts relative frequenciesof word-stems in pre-defined semantic and syn-tactic categories.
It shows, for instance, that highNeuroticism scorers use: more first person singu-lar and negative emotion words; and fewer arti-cles and positive emotion words (Pennebaker andKing, 1999).So, can a text classifier trained on such featurespredict the author personality?
We know of onlyone published study: Argamon et al (2005) fo-cussed on Extraversion and Neuroticism, dividingPennebaker and King?s (1999) population into thetop- and bottom-third scorers on a dimension, anddiscarding the middle third.
For both dimensions,using a restricted feature set, they report binaryclassification accuracy of around 58%: an 8% ab-solute improvement over their baseline.
Althoughmood is more malleable, work on it is also relevant(Mishne, 2005).
Using a more typical feature set(including n-grams of words and parts-of-speech),the best mood classification accuracy was 66%, for?confused?.
At a coarser grain, moods could beclassified with accuracies of 57% (active vs. pas-sive), and 60% (positive vs. negative).So, Argamon et al used a restricted feature setfor binary classification on two dimensions: Ex-traversion and Neuroticism.
Given this, we nowpursue three questions.
(1) Can we improve per-formance on a similar binary classification task?
(2) How accurate can classification be on the otherdimensions?
(3) How accurate can multiple?three-way or five-way?classification be?3 The weblog corpus3.1 ConstructionA corpus of personal weblog (?blog?)
text has beengathered (Nowson, 2006).
Participants were re-cruited directly via e-mail to suitable candidates,and indirectly by word-of-mouth: many partici-pants wrote about the study in their blogs.
Par-ticipants were first required to answer sociobio-graphic and personality questionnaires.
The per-sonality instrument has specifically been validatedfor online completion (Buchanan, 2001).
It wasderived from the 50-item IPIP implementation ofCosta and McCrae?s (1992) revised NEO person-ality inventory; participants rate themselves on 41-items using a 5-point Likert scale.
This providesscores for Neuroticism, Extraversion, Openness,Agreeableness and Conscientiousness.After completing this stage, participants wererequested to submit one month?s worth of priorweblog postings.
The month was pre-specified soas to reduce the effects of an individual choos-ing what they considered their ?best?
or ?preferred?month.
Raw submissions were marked-up usingXML so as to automate extraction of the desiredtext.
Text was also marked-up by post type, suchas purely personal, commentary reporting of ex-ternal matters, or direct posting of internet memessuch as quizzes.
The corpus consisted of 71 par-ticipants (47 females, 24 males; average ages 27.8and 29.4, respectively) and only the text markedas ?personal?
from each weblog, approximately410,000 words.
To eliminate undue influence ofparticularly verbose individuals, the size of eachweblog file was truncated at the mean word countplus 2 standard deviations.3.2 Personality distributionIt might be thought that bloggers are more Ex-travert than most (because they express themselvesin public); or perhaps that they are less Extravert(because they keep diaries in the first place).
Infact, plotting the Extraversion scores for the cor-pus authors gives an apparently normal distribu-tion; and the same applies for three other dimen-sions.
However, scores for Openness to experi-ence are not normally distributed.
Perhaps blog-gers are more Open than average; or perhaps thereis response bias.
Without a comparison sample ofmatched non-bloggers, one cannot say, and Open-ness is not discussed further in this paper.6284 ExperimentsWe are thus confined to classifying on four per-sonality dimensions.
However, a number of othervariables remain: different learning algorithmscan be employed; authors in the corpus can begrouped in several ways, leading to various classi-fication tasks; and more or less restricted linguisticfeature sets can be used as input to the classifier.4.1 AlgorithmsSupport Vector Machines (SVM) appear to workwell for binary sentiment classification tasks, soArgamon et al (2003) and Pang and Lee (2005)consider One-vs-All, or All-vs-All, variants onSVM, to permit multiple classifications.
Choiceof algorithm is not our focus, but it remains tobe seen whether SVM outperforms Na?
?ve Bayes(NB) for personality classification.
Thus, we willuse both on the binary Tasks 1 to 3 (defined in sec-tion 4.2.1), for each of the personality dimensions,and each of the manually-selected feature sets(Levels I to IV, defined in section 4.3).
Whicheverperforms better overall is then reported in full, andused for the multiple Tasks 4 to 7 (defined in sec-tion 4.2.2).
Both approaches are applied as imple-mented in the WEKA toolkit (Witten and Frank,1999) and use 10-fold cross validation.4.2 TasksFor any blog, we have available the scores, on con-tinuous scales, of its author on four personality di-mensions.
But for the classifier, the task can bemade more or less easy, by grouping authors oneach of the dimensions.
The simplest tasks are, ofcourse, binary: given the sequence of words froma blog, the classifier simply has to decide whetherthe author is (for instance) high or low in Agree-ableness.
Binary tasks vary in difficulty, depend-ing on whether authors scoring in the middle of adimension are left out, or not; and if they are leftout, what proportion of authors are left out.More complex tasks will also vary in difficultydepending on who is left out.
But in the casesconsidered here, middle authors are now included.For a three-way task, the classifier must decideif an author is high, medium or low; and thoseauthors known to score between these categoriesmay, or may not, be left out.
In the most challeng-ing five-way task, no-one is left out.
The point ofconsidering such tasks is to gradually approximatethe most challenging task of all: continuous rating.4.2.1 Binary classification tasksIn these task variants, the goal is to classify au-thors as either high or low scorers on a dimension:1.
The easiest approach is to keep the high andlow groups as far apart as possible: high scor-ers (H) are those whose scores fall above1 SD above the mean; low scorers (L) arethose whose scores fall below 1 SD below themean.2.
Task-1 creates distinct groups, at the price ofexcluding over 50% of the corpus from theanalysis.
To include more of the corpus, pa-rameters are relaxed: the high group (HH)includes anyone whose score is above .5 SDabove the mean; the low group (LL) is simi-larly placed below.3.
The most obvious task (but not the easiest)arises from dividing the corpus in half aboutthe mean score.
This creates high (HHH) andlow (LLL) groups, covering the entire pop-ulation.
Inevitably, some HHH scorers willactually have scores much closer to those ofLLL scorers than to other HHH scorers.These sub-groups are tabulated in Table 1, giv-ing the size of each group within each trait.
Notethat in Task-2, the standard-deviation-based divi-sions contain very nearly the top third and bottomthird of the population for each dimension.
Hence,Task-2 is closest in proportion to the division bythirds used in Argamon et al (2005).Lowest .
.
.
Highest1 L ?
H2 LL ?
HH3 LLL HHHN1 12 ?
13N2 25 ?
22N3 39 32E1 11 ?
12E2 23 ?
24E3 32 39A1 11 ?
13A2 22 ?
21A3 34 37C1 11 ?
14C2 17 ?
27C3 30 41Table 1: Binary task groups: division method andauthor numbers.
N = Neuroticism; E = Extraver-sion; A = Agreeableness; C = Conscientiousness.6294.2.2 Multiple classification tasks4.
Takes the greatest distinction between high(H) and low (L) groups from Task-1, andadds a medium group, but attempts to reducethe possibility of inter-group confusion by in-cluding only the smaller medium (m) groupomitted from Task-2.
Not all subjects aretherefore included in this analysis.
Since thethree groups to be classified are completelydistinct, this should be the easiest of the fourmultiple-class tasks.5.
Following Task-4, this uses the most distincthigh (H) and low (L) groups, but now consid-ers all remaining subjects medium (M).6.
Following Task-2, this uses the larger high(hH) and low (Ll) groups, with all those inbetween forming the medium (m) group.7.
Using the distinction between the high andlow groups of Task-5 and -6, this creates a5-way split: highest (H), relatively high (h),medium (m), relatively low (l) and lowest(L).
With the greatest number of classes, thistask is the hardest.These sub-groups are tabulated in Table 2, givingthe size of each group within each trait.Lowest .
.
.
Highest4 L ?
m ?
H5 L M H6 Ll m hH7 L l m h HN4 12 ?
24 ?
13N5 12 46 13N6 25 24 22N7 12 13 24 9 13E4 11 ?
24 ?
12E5 11 48 12E6 23 24 24E7 11 12 24 12 12A4 11 ?
28 ?
13A5 11 47 13A6 22 28 21A7 11 11 28 8 13C4 11 ?
27 ?
14C5 11 46 14C6 17 27 27C7 11 6 27 13 14Table 2: 3-way/5-way task groups: divisionmethod and author numbers.
N = Neuroticism; E= Extraversion; A = Agreeableness; C = Consci-entiousness.4.3 Feature selectionThere are many possible features that can beused for automatic text classification.
These ex-periments use essentially word-based bi- and tri-grams.
It should be noted, however, that somegeneralisations have been made: all proper nounswere identified via CLAWS tagging using theWMatrix tool (Rayson, 2003), and replaced witha single marker (NP1); punctuation was collapsedinto a single marker (<p>); and additional tagscorrespond to non-linguistic features of blogs?for instance, <SOP> and <EOP> were used themark the start and end of individual blogs posts.Word n-gram approaches provide a large featurespace with which to work.
But in the generalinterest of computational tractability, it is usefulto reduce the size of the feature set.
There aremany automatic approaches to feature selection,exploiting, for instance, information gain (Quin-lan, 1993).
However, ?manual?
methods can of-fer principled ways of both reducing the size ofthe set and avoiding overfitting.
We therefore ex-plore the effect of different levels of restriction onthe feature sets, and compare them with automaticfeature selection.
The levels of restriction are asfollows:I The least restricted feature set consists of then-grams most commonly occurring within theblog corpus.
Therefore, the feature set foreach personality dimension is to be drawnfrom the same pool.
The difference lies in thenumber of features selected: the size of the setwill match that of the next level of restriction.II The next set includes only those n-gramswhich were distinctive for the two extremesof each personality trait.
Only features witha corpus frequency ?5 are included.
This al-lows accurate log-likelihood G2 statistics tobe computed (Rayson, 2003).
Distinct collo-cations are identified via a three way compar-ison between the H and L groups in Task-1(see section 4.2.1) and a third, neutral group.This neutral group contains all those individ-uals who fell in the medium group (M) forall four traits in the study; the resulting groupwas of comparable size to the H and L groupsfor each trait.
Hence, this approach selectsfeatures using only a subset of the corpus.
N-gram software was used to identify and countcollocations within a sub-corpus (Banerjee630and Pedersen, 2003).
For each feature found,its frequency and relative frequency are calcu-lated.
This permits relative frequency ratiosand log-likelihood comparisons to be madebetween High-Low, High-Neutral and Low-Neutral.
Only features that prove distinctivefor the H or L groups with a significance ofp < .01 are included in the feature set.III The next set takes into account the possibil-ity that, for a group used in Level-II, an n-gram may be used relatively frequently, butonly because a small number of authors in agroup use it very frequently, while others inthe same group use it not at all.
To enter theLevel-III set, an n-gram meeting the Level-IIcriteria must also be used by at least 50%1 ofthe individuals within the subgroup for whichit is reported to be distinctive.IV While Level-III guards against excessive indi-vidual influence, it may abstract too far fromthe fine-grained variation within a personalitytrait.
The final manual set therefore includesonly those n-grams that meet the Level-II cri-teria with p < .001, meet the Level-III crite-ria, and also correlate significantly (p < .05)with individual personality trait scores.V Finally, it is possible to allow the n-gram fea-ture set to be selected automatically duringtraining.
The set to be selected from is thebroadest of the manually filtered sets, thosen-grams that meet the Level-II criteria.
Theapproach adopted is to use the defaults withinthe WEKA toolkit: Best First search with theCfsSubsetEval evaluator (Witten and Frank,1999).Thus, a key question is when?if ever?a ?man-ual?
feature selection policy outperforms the auto-matic selection carried out under Level-V. Levels-II and -III are of particular interest, since they con-tain features derived from a subset of the corpus.Since different sub-groups are considered for eachpersonality trait, the feature sets which meet theincreasingly stringent criteria vary in size.
Table 3contains the size of each of the four manually-determined feature sets for each of the four per-sonality traits.
Note again that the number of n-grams selected from the most frequent in the cor-1Conservatively rounded down in the case of an odd num-ber of subjects.I II III IV VN 747 747 169 22 19E 701 701 167 11 20A 823 823 237 36 34C 704 704 197 22 25Table 3: Number of n-grams per set.Low High[was that] [this year]N [NP1 <p> NP1] [to eat][<p> after] [slowly <p>][is that] [and buy][point in] [and he]E [last night <p>] [cool <p>][it the] [<p> NP1][is to] [to her][thank god] [this is not]A [have any] [<p> it is][have to] [<p> after][turn up] [not have][a few weeks] [by the way]C [case <p>] [<p> i hope][okay <p>] [how i][the game] [kind of]Table 4: Examples of significant Low and Highn-grams from the Level-IV set.pus for Level-I matches the size of the set forLevel-II.
In addition, the features automatically se-lected are task-dependent, so the Level-V sets varyin size; here, the Table shows the number of fea-tures selected for Task-2.To illustrate the types of n-grams in the featuresets, Table 4 contains four of the most significantn-grams from Level-IV for each personality class.5 ResultsFor each of the 60 binary classification tasks (1to 3), the performance of the two approaches wascompared.
Na?
?ve Bayes outperformed SupportVector Machines on 41/60, with 14 wins for SVMand 5 draws.
With limited space available, wetherefore discuss only the results for NB, and useNB for Task-4 to -7.
The results for the binarytasks are displayed in Table 5.
Those for the mul-tiple tasks are displayed in Table 6.
Baseline is themajority classification.
The most accurate perfor-mance of a feature set for each task is highlighted631Task Base Lv.I Lv.II Lv.III Lv.IV Lv.VN1 52.0 52.0 92.0 84.0 96.0 92.0N2 53.2 51.1 63.8 68.1 83.6 85.1N3 54.9 54.9 60.6 53.5 71.9 83.1E1 52.2 56.5 91.3 95.7 87.0 100.0E2 51.1 44.7 74.5 72.3 66.0 93.6E3 54.9 50.7 53.5 59.2 64.8 85.9A1 54.2 62.5 100.0 100.0 95.8 100.0A2 51.2 60.5 81.4 79.1 72.1 97.7A3 52.1 53.5 60.6 69.0 66.2 93.0C1 56.0 52.0 100.0 100.0 84.0 92.0C2 61.2 54.5 77.3 81.8 72.7 93.2C3 57.7 54.9 63.4 71.8 70.4 84.5Table 5: Na?
?ve Bayes performance on binarytasks.
Raw % accuracy for 4 personality dimen-sions, 3 tasks, and 5 feature selection policies.in bold while the second most accurate is markeditalic.6 DiscussionLet us consider the results as they bear in turn onthe three main questions posed earlier: Can we im-prove on Argamon et al?s (2005) performance onbinary classification for the Extraversion and Neu-roticism dimensions?
How accurately can we clas-sify on the four personality dimensions?
And howdoes performance on multiple classification com-pare with that on binary classification?Before addressing these questions, we note therelatively good performance of NB compared with?vanilla?
SVM on the binary classification tasks.We also note that automatic selection generallyoutperforms ?manual?
selection; however overfit-ting is very likely when examining just 71 datapoints.
Therefore, we do not discuss the Level-Vresults further.6.1 Extraversion and NeuroticismThe first main question relates to the feature setschosen, because the main issue is whether word n-grams can give reasonable results on the Extraver-sion and Neuroticism classification tasks.
Of thecurrent binary classification tasks, Task-2 is mostclosely comparable to Argamon et al?s.
Here, thebest performance for Extraversion was returnedby the ?manual?
Level-II feature set, closely fol-lowed by Level-III.
The accuracy of 74.5% repre-sents a 23.4% absolute improvement over baselineTask Base Lv.I Lv.II Lv.III Lv.IV Lv.VN4 49.0 49.0 81.6 65.3 77.6 85.7N5 64.8 60.6 76.1 67.6 67.6 94.4N6 35.2 31.0 47.9 46.5 66.2 70.4N7 33.8 31.0 49.3 38.0 42.3 47.9E4 51.1 44.7 74.5 59.6 53.2 78.7E5 67.6 60.6 83.1 67.6 54.9 90.1E6 33.8 23.9 53.5 46.5 46.5 56.3E7 33.8 44.7 39.4 29.6 38.0 40.8A4 53.8 51.9 90.4 78.8 67.3 80.8A5 66.2 59.2 83.1 84.5 74.6 80.3A6 39.4 31.0 67.6 60.6 56.3 85.9A7 39.4 33.8 69.8 60.6 50.7 47.9C4 51.9 53.8 92.3 65.4 67.3 82.7C5 64.8 62.0 74.6 69.0 62.0 83.1C6 38.0 39.4 59.2 59.2 50.7 78.9C7 38.0 36.6 62.0 45.1 45.1 49.3Table 6: Na?
?ve Bayes performance on multipletasks.
Raw % accuracy for 4 personality dimen-sions, 4 tasks, and 5 feature selection policies.
(45.8% relative improvement; we report relativeimprovement over baseline because baseline accu-racies vary between tasks).
The best performancefor Neuroticism was returned by Level-IV.
The ac-curacy of 83.6% represents a 30.4% absolute im-provement over baseline (57.1% relative improve-ment).Argamon et al?s feature set combined in-sights from computational stylometrics (Koppel etal., 2002; Argamon et al, 2003) and systemic-functional grammar.
Their focus on functionwords and appraisal-related features was intendedto provide more general and informative featuresthan the usual n-grams.
Now, it is unlikely thatweblogs are easier to categorise than the genresstudied by Argamon et al So there are instead atleast two reasons for the improvement we report.First, although we did not use systemic-functional linguistic features, we did test n-gramsselected according to more or less strict policies.So, considering the manual policies, it seems thatthe Level-IV was the best-performing set for Neu-roticism.
This might be expected, given thatLevel-IV potentially overfits, allowing features tobe derived from the full corpus.
However, inspite of this, Level-II pproved best for Extraver-sion.
Secondly, in classifying an individual as highor low on some dimension, Argamon et al had632(for some of their materials) 500 words from thatindividual, whereas we had approximately 5000words.
The availability of more words per indi-vidual is to likely to help greatly in training.
Ad-ditionally, a greater volume of text increases thechances that a long term ?property?
such as per-sonality will emerge6.2 Binary classification of all dimensionsThe second question concerns the relative easeof classifying the different dimensions.
Acrosseach of Task-1 to -3, we find that classificationaccuracies for Agreeableness and Conscientious-ness tend to be higher than those for Extraver-sion and Neuroticism.
In all but two cases, theautomatically generated feature set (V) performsbest.
Putting this to one side, of the manuallyconstructed sets, the unrestricted set (I) performsworst, often below the baseline, while Level-IV isthe best for classifying each task of Neuroticism.Overall, II and III are better than IV, although thedifference is not large.As tasks increase in difficulty?as high and lowgroups become closer together, and the left-outmiddle shrinks?performance drops.
But accu-racy is still respectable.6.3 Beyond binary classificationThe final question is about how classification ac-curacy suffers as the classification task becomesmore subtle.
As expected, we find that as we addmore categories, the tasks are harder: compare theresults in the Tables for Task-1, -5 and -7.
And,as with the binary tasks, if fewer mid-scoring in-dividuals are left out, the task is typically harder:compare results for Task-4 and 5.
It does seem thatsome personality dimensions respond to task dif-ficulty more robustly than others.
For instance, onthe hardest task, the best Extraversion classifica-tion accuracy is 10.9% absolute over the baseline(32.2% relative), while the best Agreeableness ac-curacy is 30.4% absolute over the baseline (77.2%relative).
It is notable that the feature set whichreturn the best results?bar the automatic set V?tends to be Level-II, excepting for Neuroticism onTask-6, where Level-IV considerably outperformsthe other sets.A supplementary question is how the best clas-sifiers compare with human performance on thistask.
Mishne (2005) reports that, for generalmood classification on weblogs, the accuracy ofhis automatic classifier is comparable to humanperformance.
There are also general results onhuman personality classification performance incomputer-mediated communication, which sug-gest that at least some dimensions can be ac-curately judged even when computer-mediated.Vazire and Gosling (2004) report that for personalwebsites, relative accuracy of judgment was, in de-scending order: Openness > Extraversion > Neu-roticism > Agreeableness > Conscientiousness.Similarly, Gill et al (2006) report that for personale-mail, Extraversion is more accurately judgedthan Neuroticism.
The current study does not havea set of human judgments to report.
For now, it isinteresting to note that the performance profile forthe best classifiers, on the simplest tasks, appearsto diverge from the general human profile, insteadranking on raw accuracy: Agreeableness > Con-scientiousness > Neuroticism > Extraversion.7 Conclusion and next stepsThis paper has reported the first stages of our in-vestigations into classification of author personal-ity from weblog text.
Results are quite promis-ing, and comparable across all four personalitytraits.
It seems that even a small selection of fea-tures found to exhibit an empirical relationshipwith personality traits can be used to generate rea-sonably accurate classification results.
Naturally,there are still many paths to explore.
Simple re-gression analyses are reported in Nowson (2006);however, for classification, a more thorough com-parison of different machine learning methodolo-gies is required.
A richer set of features besidesn-grams should be checked, and we should not ig-nore the potential effectiveness of unigrams in thistask (Pang et al, 2002).
A completely new testset can be gathered, so as to further guard againstoverfitting, and to explore systematically the ef-fects of the amount of training data available foreach author.
And as just discussed, comparisonwith human personality classification accuracy ispotentially very interesting.However, it does seem that we are makingprogress towards being able to deal with a real-istic task: if we spot a thumbs-up review in a we-blog, we should be able to check other text in thatweblog, and tell whose thumb it is; or more accu-rately, what kind of person?s thumb it is, anyway.And that in turn should help tell us how high thethumb is really being held.6338 AcknowledgementsWe are grateful for the helpful advice of MirellaLapata, and our three anonymous reviewers.
Thesecond author was supported by a studentshipfrom the Economic and Social Research Council.ReferencesShlomo Argamon, Marin Saric, and Sterling S. Stein.2003.
Style mining of electronic messages for mul-tiple authorship discrimination: first results.
In Pro-ceedings of SIGKDD, pages 475?480.Shlomo Argamon, Sushant Dhawle, Moshe Koppel,and James W. Pennebaker.
2005.
Lexical predic-tors of personality type.
In Proceedings of the 2005Joint Annual Meeting of the Interface and the Clas-sification Society of North America.Satanjeev Banerjee and Ted Pedersen.
2003.
The de-sign, implementation, and use of the ngram statisticspackage.
In Proceedings of the Fourth InternationalConference on Intelligent Text Processing and Com-putational Linguistics, pages 370?381, Mexico City.Tom Buchanan.
2001.
Online implementation of anIPIP five factor personality inventory [web page].http://users.wmin.ac.uk/?buchant/wwwffi/introduction.html [Accessed 25/10/05].Paul T. Costa and Robert R. McCrae, 1992.
Re-vised NEO Personality Inventory (NEO-PI-R) andNEO Five-Factor Inventory (NEO-FFI): Profes-sional Manual.
Odessa, FL: Psychological Assess-ment Resources.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the peanut gallery: opinion extractionand semantic classification of product reviews.
InProceedings of the 12th International Conference onWorld Wide Web, pages 519?528.
ACM Press.Jean-Marc Dewaele and Adrian Furnham.
1999.
Ex-traversion: The unloved variable in applied linguis-tic research.
Language Learning, 49:509?544.Alastair J. Gill, Jon Oberlander, and Elizabeth Austin.2006.
Rating e-mail personality at zero acquain-tance.
Personality and Individual Differences,40:497?507.Moshe Koppel, Shlomo Argamon, and Arat Shimoni.2002.
Automatically categorizing written texts byauthor gender.
Literary and Linguistic Computing,17(4):401?412.Hugo Liu, Henry Lieberman, and Ted Selker.
2003.A model of textual affect sensing using real-worldknowledge.
In Proceedings of the 7th InternationalConference on Intelligent User Interfaces.Gerald Matthews, Ian J. Deary, and Martha C. White-man.
2003.
Personality Traits.
Cambridge Univer-sity Press, Cambridge, 2nd edition.Gilad Mishne.
2005.
Experiments with mood classifi-cation in blog posts.
In Proceedings of ACM SIGIR2005 Workshop on Stylistic Analysis of Text for In-formation Access.Scott Nowson.
2006.
The Language of Weblogs: Astudy of genre and individual differences.
Ph.D. the-sis, University of Edinburgh.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploit-ing class relationships for sentiment categorizationwith respect to rating scales.
In Proceedings of the43rd Annual Meeting of the ACL, pages 115?124.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 79?86.James W. Pennebaker and Laura King.
1999.
Lin-guistic styles: Language use as an individual differ-ence.
Journal of Personality and Social Psychology,77:1296?1312.James W. Pennebaker, Martha E. Francis, and Roger J.Booth.
2001.
Linguistic Inquiry and Word Count2001.
Lawrence Erlbaum Associates, Mahwah, NJ.Rosalind W. Picard.
1997.
Affective Computing.
MITPress, Cambridge, Ma.J.
Ross Quinlan.
1993.
C4.5: programs for machinelearning.
Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA.Paul Rayson.
2003.
Wmatrix: A statistical method andsoftware tool for linguistic analysis through corpuscomparison.
Ph.D. thesis, Lancaster University.Ehud Reiter and Somayajulu Sripada.
2004.
Contex-tual influences on near-synonym choice.
In Pro-ceedings of the Third International Conference onNatural Language Generation.Klaus Scherer.
1979.
Personality markers in speech.In K. R. Scherer and H. Giles, editors, Social Mark-ers in Speech, pages 147?209.
Cambridge Univer-sity Press, Cambridge.James G. Shanahan, Yan Qu, and Janyce Weibe, edi-tors.
2005.
Computing Attitude and Affect in Text.Springer, Dordrecht, Netherlands.Peter D. Turney.
2002.
Thumbs up or thumbs down?semantic orientation applied to unspervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting of the ACL, pages 417?424.Simine Vazire and Sam D. Gosling.
2004. e-perceptions: Personality impressions based on per-sonal websites.
Journal of Personality and SocialPsychology, 87:123?132.Ian H. Witten and Eibe Frank.
1999.
Data Mining:Practical Machine Learning Tools and Techniqueswith Java Implementations.
Morgan Kaufmann.634
