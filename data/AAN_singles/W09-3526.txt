Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 116?119,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPCombining MDL Transliteration Training with Discriminative ModelingDmitry Zelenko4300 Fair Lakes Ct.Fairfax, VA 22033, USAdmitry zelenko@sra.comAbstractWe present a transliteration system thatintroduces minimum description lengthtraining for transliteration and combinesit with discriminative modeling.
We ap-ply the proposed approach to translitera-tion from English to 8 non-Latin scripts,with promising results.1 IntroductionRecent research in transliteration and translationshowed utility of increasing the n-gram size intransliteration models and phrase tables (Koehnet al, 2003).
Yet most learning algorithms fortraining n-gram transliteration models place re-strictions on the size of n-gram due to tractabilityand overfitting issues, and, in the case of machinetranslation, construct the phrase table after train-ing the model, in an ad-hoc manner.
In this paper,we present a minimum description length (MDL)approach (Grunwald, 2007) for learning transliter-ation models comprising n-grams of unrestrictedsize.
Given a bilingual dictionary of transliterateddata we seek to derive a transliteration model sothat the combined size of the data and the model isminimized.Use of discriminative modeling for transliter-ation and translation is another promising direc-tion allowing incorporation of arbitrary featuresin the transliteration process (Zelenko and Aone,2006; Goldwasser and Roth, 2008).
Here we pro-pose to use the transliteration model derived viaMDL training as a starting point and learn themodel weights in the discriminative manner.
Thediscriminative approach also provides a naturalway to integrate the language modeling compo-nent into the transliteration decoding process.We experimentally evaluate the proposed ap-proach on the standard datasets for the task oftransliterating from English to 8 non-Latin scripts2 MDL Training for TransliterationIn our transliteration setting, we are given a stringe written in an alphabet V1 (e.g., Latin), which isto be transliterated into a string f written in an al-phabet V2 (e.g., Chinese).
We consider a transliter-ation process that is conducted by a transliterationmodel T , which represents a function mapping apair of strings (ei, fi) into a score T (ei, fi) ?
R.For an alignment 1 A = {(ei, fi)} of e and f , wedefine the alignment score T (A) = ?i T (ei, fi).For a string e and a model T , the decoding processseeks the optimal transliteration T (e) with respectto the model T :T (e) = arg maxf{ T (A) | ?A = {(ei, fi)} }Different assumptions for transliteration mod-els lead to different estimation algorithms.
Apopular approach is to assume a joint gener-ative model for pairs (e, f), so that given analignment A = {(ei, fi)}, a probability P (e, f)is defined to be?i p(ei, fi).
The probabili-ties p(ei, fi) are estimated using the EM algo-rithm, and the corresponding transliteration modelis T (ei, fi) = log(p(ei, fi)).
We can alterna-tively model the conditional probability directly:P (f |e) = ?i p(fi|ei), where we again estimatethe conditional probabilities p(fi|ei) via the EMalgorithm, and define the transliteration model ac-cordingly: T (ei, fi) = log(p(fi|ei)).
We can alsocombine joint estimation with conditional decod-ing, observing that p(fi|ei) = p(ei,fi)?f p(ei,fi)and us-ing the conditional transliteration model after esti-mating a joint generative model.Increasing the maximum n-gram size in prob-abilistic modeling approaches, at some point, de-grades model accuracy due to overfitting.
There-fore, probabilistic approaches typically use a smalln-gram size, and perform additional modeling post1Here we consider only monotone alignments.116factum: examples include joint n-gram modelingand phrase table construction in machine transla-tion.We propose to apply the MDL principle totransliteration modeling by seeking the model thatcompresses the transliteration data so that thecombined size of the compressed data and themodel is minimized.
If T corresponds to a jointprobabilistic model P = {p(ei, fi)}, then we canuse the model to encode the data D = {(e, f)} inCD(P ) = ??
(e,f)log P (e, f)= ??
(e,f)maxA?ilog p(ei, fi)bits, where A = {(ei, fi)} is an alignment of e andf .We can encode each symbol of an alphabet Vusing log |V | bits so encoding a string s of length|s| from alphabet V takes CV (s) = log |V |(|s| +1) bits (we add an extra string termination sym-bol for separability).
Therefore, we encode eachtransliteration model inCT (P ) =?
(ei,fi)CT (ei, fi)bits, where CT (ei, fi) = CV1(ei) + CV2(fi) ?log p(ei, fi) is the number of bits used to encodeboth the pair (ei, fi) and its code according to P .Thus, we seek a probability distribution P thatminimizes C(P ) = CD(P ) + CT (P ).Let P be an initial joint probability distributionfor a transliteration model T such that a string pair(ei, fi) appeared n(ei, fi) times, and p(ei, fi) =n(ei, fi)/N , where N =?
(ei,fi) n(ei, fi).Then, encoding a pair (ei, fi) takes on aver-age C(ei, fi) = CT (ei,fi)n(ei,fi) ?
log p(ei, fi) bits -here we distribute the model size component toall occurrences of (ei, fi) in the data.
Noticethat the combined data and model size C(P ) =?
(ei,fi) n(ei, fi)C(ei, fi).
It is this quantityC(ei, fi) that we propose to use when conductingthe MDL training algorithm below.1.
Pick an initial P .
Compute C(ei, fi) =CT (ei,fi)n(ei,fi) ?
log p(ei, fi).
Set combined sizeC(P ) = ?
(ei,fi) n(ei, fi)C(ei, fi).2.
Iterate: during each iteration, for each(e, f) ?
D, find the minimum codesizealignment A = argminA?i C(ei, fi) of(e, f).
Use the alignments to re-estimate Pand re-compute C .
Exit when there is no im-provement in the combined model and datasize.Experimentally, we observed fast convergence ofthe above algorithm just after a few iterations,though we cannot present a convergence proof asyet.
We picked the initial model by computingco-occurrence counts of n-gram pairs in D, thatis, n(ei, fi) =?
(e,f) min(ne(ei), nf (fi)), wherene(ei) (nf (fi)) is the number of times the n-gramei (fi) appeared in the string e (f ).Note that a Bayesian interpretation of the pro-posed approach is not straightforward due tothe use of empirical component ?
log p(ei, fi) inmodel encoding.
Changing the model encoding touse, for example, a code for n(ei, fi) would allowfor a direct Bayesian interpretation of the proposedcode, and we plan to pursue this direction in thefuture.The output of the MDL training algorithm isthe joint probability model P that we use to de-fine the transliteration model weights as the loga-rithm of corresponding conditional probabilities:T (ei, fi) = log p(ei,fi)?f p(ei,f).
During the decod-ing process of inferring f from e via an align-ment A, we integrate the language model proba-bility p(f) via a linear combination: TGEN (e) =argmaxf{T (A) + ?
log p(f)/|f |}, where ?
isa combination parameter estimated via cross-validation.3 Discriminative TrainingWe use the MDL-trained transliteration modelT as a starting point for discriminative train-ing: we consider all n-gram pairs (ei, fi) withnonzero probabilities p(ei, fi) as features of a lin-ear discriminative model TDISCR.
We also in-tegrate the normalized language modeling prob-ability p0(f) = p(f)1|f | in the discriminativemodel as one of the features: TDISCR(e) =argmaxf{T (A) + T0p0(f)}.
We learn theweights T (ei, fi) and T0 of the discriminativemodel using the average perceptron algorithm of(Collins, 2002).
Since both the transliterationmodel and the language model are required to belearned from the same data, and the language mod-eling probability is integrated into our decodingprocess, we remove the string e from the languagemodel before processing the example (f, e) during117training; we re-incorporate the string e in the lan-guage model after the example (f, e) is processedby the averaged perceptron algorithm.
We use thediscriminatively trained model as the ?standard?system in our experiments.4 ExperimentsWe use the standard data for transliteratingfrom English into 8 non-Latin scripts: Chinese(Haizhou et al, 2004); Korean, Japanese (Kanji),and Japanese (Katakana) (CJK Institute, 2009);Hindi, Tamil, Kannada, and Russian (Kumaranand Kellner, 2007).
The data is provided as partof the Named Entities Workshop 2009 MachineTransliteration Shared Task (Li et al, 2009).For all 8 datasets, we report scores on the stan-dard tests sets provided as part of the evaluation.Details of the evaluation methodology are pre-sented in (Li et al, 2009).4.1 PreprocessingWe perform the same uniform processing of data:names are considered sequences of Unicode char-acters in their standard decomposed form (NFD).In particular, Korean Hangul characters are de-composed into Jamo syllabary.
Since the evalu-ation data are provided in the re-composed form,we re-compose output of the transliteration sys-tem.We split multi-word names (in Hindi, Tamil,and Kannada datasets) in single words and con-ducted training and evaluation on the single wordlevel.
We assume no word order change for multi-word names and ignore name pairs with differentnumbers of words.4.2 System Parameters and TuningWe apply pre-set system parameters with very lit-tle tuning.
In particular, we utilize a 5-gram lan-guage model with Good-Turing discounting.
TheMDL training algorithm requires only the cardi-nalities of the corresponding alphabets as parame-ters, and we use the following approximate vocab-ulary sizes typically rounded to the closest powerof 2 (except for Chinese and Japanese): for En-glish, Russian, Tamil, and Kannada, we set |V | =32; for Katakana and Hindi, |V | = 64; for KoreanJamo, |V | = 128; for Chinese and Japanese Kanji,|V | = 1024.We perform 10 iterations of the average per-ceptron algorithm for discriminative training.
ForInit Comp Ratio DictChinese 333 Kb 158 Kb 0.48 5780Hindi 159 Kb 72 Kb 0.45 1956Japanese 170 Kb 82 Kb 0.48 4394(Kanji)Kannada 131 Kb 62 Kb 0.48 2010Japanese 289 Kb 136 Kb 0.47 3383(Katakana)Korean 69 Kb 31 Kb 0.45 1181Russian 78 Kb 37 Kb 0.48 865Tamil 134 Kb 62 Kb 0.46 1827Table 1: MDL Data and Model Compressionshowing initial data size, final combined data andmodel size, the compression ratio, and the numberof n-gram pairs in the final model.T1(Acc) T2(Acc) T2(F) T2(MRR)Chinese 0.522 0.619 0.847 0.711Hindi 0.312 0.409 0.864 0.527Japanese 0.484 0.509 0.675 0.6(Kanji)Kannada 0.227 0.345 0.854 0.462Japanese 0.318 0.420 0.807 0.541(Katakana)Korean 0.339 0.413 0.702 0.524Russian 0.488 0.566 0.919 0.662Tamil 0.267 0.374 0.880 0.512Table 2: Experimental results for transliterationfrom English to 8 non-Latin scripts comparingperformance of generative (T1) and correspondingdiscriminative (T2) models.both alignment and decoding, we use a beamsearch decoder, with the beam size set to 100.4.3 ResultsOur first set of experiments illustrates compres-sion achieved by MDL training.
Table 1 shows foreach for the training datasets, the original size ofthe data, compressed size of the data including themodel size, the compression ratio, and the numberof n-gram pairs in the final model.We see very similar compression for all lan-guages.
The number of n-gram pairs for the finalmodel is also relatively small.
In general, MDLtraining with discriminative modeling allows us todiscover a flexible small set of features (n-grampairs) without placing any restriction on n-gramsize.
We can interpret MDL training as search-118ing implicitly for the best bound on the n-gramsize together with searching for appropriate fea-tures.
Our preliminary experiments also indicatethat performance of models produced by the MDLapproach roughly corresponds to performance ofmodels trained with the optimal bound on the sizeof n-gram features.Table 2 demonstrates that discriminative model-ing significantly improves performance of the cor-responding generative models.
In this setting, theMDL training step is effectively used for featureconstruction: its goal is to automatically hone inon a small set of features whose weights are laterlearned by discriminative methods.From a broader perspective, it is an openquestion whether seeking a compact representa-tion of sequential data leads to robust and best-performing models, especially in noisy environ-ments.
For example, state-of-the-art phrase trans-lation models eschew succinct representations,and instead employ broad redundant sets of fea-tures (Koehn et al, 2003).
On the other hand,recent research show that small translation mod-els lead to superior alignment (Bodrumlu et al,2009).
Therefore, investigation of the trade-offbetween robust redundant and succinct representa-tion present an interesting area for future research.5 Related WorkThere is plethora of work on transliteration cov-ering both generative and discriminative models:(Knight and Graehl, 1997; Al-onaizan and Knight,2002; Huang et al, 2004; Haizhou et al, 2004; Ze-lenko and Aone, 2006; Sherif and Kondrak, 2007;Goldwasser and Roth, 2008).
Application of theminimum description length principle (Grunwald,2007) in natural language processing has beenheretofore mostly limited to morphological analy-sis (Goldsmith, 2001; Argamon et al, 2004).
(Bo-drumlu et al, 2009) present a related approach onoptimizing the alignment dictionary size in ma-chine translation.6 ConclusionsWe introduced a minimum description length ap-proach for training transliteration models that al-lows to avoid overfitting without putting aprioriconstraints of the size of n-grams in transliterationmodels.
We plan to apply the same paradigm toother sequence modeling tasks such as sequenceclassification and segmentation, in both super-vised and unsupervised settings.ReferencesY.
Al-onaizan and K. Knight.
2002.
Machine translit-eration of names in arabic text.
In ACL Workshopon Comp.
Approaches to Semitic Languages, pages34?46.S.
Argamon, N. Akiva, A. Amir, and O. Kapah.
2004.Efficient unsupervised recursive word segmentationusing minimum description length.
In Proceedingsof COLING.T.
Bodrumlu, K. Knight, and S. Ravi.
2009.
A new ob-jective function for word alignment.
In ProceedingsNAACL Workshop on Integer Linear Programmingfor NLP.CJK Institute.
2009. http://www.cjk.org.M.
Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP.J.
Goldsmith.
2001.
Unsupervised learning of the mor-phology of a natural language.
Computational Lin-guistics, pages 153?198.D.
Goldwasser and D. Roth.
2008.
Translitera-tion as constrained optimization.
In Proceedings ofEMNLP.P.
Grunwald.
2007.
The Minimum Description Lengthprinciple.
MIT Press.L.
Haizhou, Z. Min, and S. Jian.
2004.
A joint source-channel model for machine transliteration.
In Pro-ceedings of ACL.F.
Huang, S. Vogel, , and A. Waibel.
2004.
Improvingnamed entity translation combining phonetic and se-mantic similarities.
In Proceedings of HLT/NAACL.K.
Knight and J. Graehl.
1997.
Machine translitera-tion.
Computational Linguistics, pages 128?135.P.
Koehn, F. Och, and D. Marcu.
2003.
Statis-tical phrase-based translation.
In Proceedings ofNLT/NAACL.A.
Kumaran and T. Kellner.
2007.
A generic frame-work for machine transliteration.
In Proceedings ofSIGIR.Haizhou Li, A. Kumaran, Min Zhang, and V. Pervou-chine.
2009.
Whitepaper of news 2009 machinetransliteration shared task.
In Proceedings of ACL-IJCNLP 2009 Named Entities Workshop (NEWS2009).T.
Sherif and G. Kondrak.
2007.
Substring-basedtransliteration.
In Proceedings of ACL.D.
Zelenko and C. Aone.
2006.
Discriminative meth-ods for transliteration.
In Proceedings of EMNLP.119
