A Process Model for RecognizingCommunicative Acts and ModelingNegotiation SubdialoguesSandra  Carberry*University of DelawareLynn  Lamber tChristopher Newport UniversityNegotiation is an important part of task-oriented expert-consultation dialogues.
This paperpresents a plan-based model for understanding cooperative negotiation subdialogues.
Our sys-tem infers both the communicative actions that people pursue when speaking and the beliefsunderlying these actions.
Beliefs, and the strength of these beliefs, are recognized from the surfaceform of utterances,from discourse acts, and from the explicit and implicit acceptance ofpreviousutterances.
Our algorithm for recognizing discourse actions combines linguistic, world, and con-textual knowledge in a unified framework.
By combining these different knowledge sources, weare able to recognize complex discourse acts such as expressing doubt, to identify the relationshipof utterances to one another, and to model negotiation subdialogues.
Since negotiation isan inte-gral part of multiagent activity, our process model addresses an important aspect of cooperativeinteraction and thus is a step toward an intelligent and robust natural language consultationsystem.1.
IntroductionIn a typical expert-consultation dialogue, one participant (hereafter referred to as theexecuting agent or EA) has a goal that he 1 wants to achieve and is working withthe other participant (referred to as the consulting agent or CA) to construct a planfor achieving this goal.
Although both the plan construction process and the con-versation are collaborative activities, this does not mean that people always believewhat they are told.
In fact, part of the collaborative activity of conversation is negoti-ation of conflicting beliefs.
This negotiation is particularly important in task-orientedexpert-consultation dialogues, since the participants must resolve any conflicting be-liefs in order to work together effectively to devise a plan that is both well-formed andaddresses the executing agent's needs.
Thus, a robust natural anguage consultationsystem must be able to handle negotiation subdialogues.Even though there is wide agreement that negotiation is an integral part of multi-agent activity, previous natural language understanding systems have been unable tohandle negotiation subdialogues such as the following:(1)(2)$1: Who is teaching CS360?$2: Dr. Smith is teaching CS360.
* Department of Computer Science, Newark, DE 19716, USADepartment of Physics, Computer Science, and Engineering, Newport News, VA 23606, USA1 For exposition purposes, we will use the masculine g nder when referring to EA and the femininegender when referring to CA.
(~) 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 1(3)(4)(5)(6)$1: But isn't CS360 an undergraduate course?$2: Yes.
CS360 is an undergraduate course.Dr.
Smith teaches both graduate and undergraduate courses.$1: Who handles the CS360 lab?For example, existing systems do not recognize when an agent is expressing doubtat a previous response as in utterance (3), when an agent is attempting to resolve aconflict suggested by the other participant as in utterances (4)-(5), or when an agentis implicitly conveying acceptance of a communicated proposition as in utterance (6).These shortcomings prevent existing natural language systems from being able tohandle dialogues in which one agent initially does not accept he proposition conveyedby the other agent and initiates a negotiation subdialogue to resolve their differencesin belief.We have developed a plan-based model of dialogue that addresses these limita-tions.
Our analysis of naturally occurring dialogue indicates that one way that peopleexpress doubt at a proposition Pdoubt is by contending that some other conflictingproposition Pi is true.
Our process model includes an algorithm for recognizing suchexpressions of doubt, as well as other complex discourse acts.
The algorithm usesa multistrength belief model and a combination of linguistic, world, and contextualknowledge.
Our implemented system can recognize implicit as well as explicit accep-tance of a communicated proposition, multiple expressions of doubt at the same propo-sition, expressions of doubt at both immediately preceding and earlier utterances, andnegotiation subdialogues embedded within other negotiation subdialogues.In the remainder of this paper, we describe our system and how this process isperformed.
Section 2 describes the kinds of expressions of doubt found in our cor-pus analysis, and Section 3 discusses the factors that must be taken into account inrecognizing the kind of expression of doubt that we have been studying.
Section 4presents our process model for recognizing complex discourse acts (such as expres-sions of doubt) and assimilating them into the dialogue context.
First it discusses whyit is necessary to capture varying degrees of belief, describes the multistrength modelof belief used in our system, and discusses how our description of actions avoidsassuming that a speaker will automatically adopt a communicated proposition.
Thenit introduces the notion of an action that requires evidence for its recognition andpresents our recognition algorithm that uses a combination of linguistic, world, andcontextual knowledge.
Section 5 steps through an extended example that illustratesour system's ability to recognize complex discourse acts and model negotiation sub-dialogues.
Section 6 discusses the evaluation of our system and our plans for futurework, and Section 7 discusses related research.
The examples in this paper are takenfrom a university advisement domain, since this is the domain in which we haveimplemented our system.2.
Motivation from Naturally Occurring DialoguesTo identify how speakers express doubt, we analyzed a corpus of naturally occurringdialogues in the domains of financial planning, university courses, real estate, pets,taxes, and travel.
The real estate, pets, and financial planning (Harry Gross Transcripts1982) dialogues were transcribed from radio talk shows, the taxes and travel (SRITranscripts 1992) dialogues were transcribed from tapes of simulated interactions,and the university courses dialogues (Columbia University Transcripts, 1985) wereCarberry and Lambert Modeling Negotiation Subdialoguestranscribed from student advisement sessions.
In the corpus we found instances inwhich a speaker expressed oubt at a proposition by contending that some otherconflicting proposition was true.
2 In addition, we extracted other examples of suchexpressions of doubt from the dialogues in novels.
These kinds of expressions ofdoubt can be realized as surface negative questions or tag questions and are oftenaccompanied by the cue word but, as in the following example taken from the HarryGross financial planning dialogues (Harry Gross Transcripts 1982) in which S2's lastutterance xpresses doubt at Sl's recommendation:$1:$2:$1:$2:I would like to see that into an individual retirement account rollover ina mutual fund group.At my age?Yes.Uh, yeah but isn't there any risk?However, our corpus analysis also provided instances where surface negative and tagquestions were used to seek verification, such as in the following excerpt from the setof financial planning dialogues:$1:$2:$1:And if you have more money left after you pay the taxes what differencedoes it make if you pay a few bucks more in taxes?I'm telling my wife but she won't listen.Well maybe she'll listen to me.
If you get 200 bucks--isn't i better tohave 200 bucks and uh have 200 left than to have nothing at all?Our recognition algorithm has only been concerned with recognizing instances inwhich a speaker expresses doubt by contending that some other proposition is true.However, in our corpus, speakers also expressed oubt in the following ways, andour future work will include extending our system to handle these: 3Drawing attention to an inconsistent feature or proposition: Thespeaker brings into focus a feature or proposition that is already part ofthe dialogue context but that is intended to discredit he propositionbeing doubted.
These utterances were often realized as an ellipticalfragment and included "And you're how old?
', "Even though it's four moreyears?
", and "At my age?
"Drawing attention to violated expectations: The speaker mentions anexpectation that is inconsistent with the doubted proposition.
Anexample of this from our corpus is "You're kidding, what happened to theseventy-eight dollar fares or those sort of things ?
"2 We found a very few instances in which the speaker asked the hearer if he was sure the conflictingproposition wasn't rue; an example from our corpus is "Are you sure he didn't name himself as attorney forthe estate?"
taken from the Harry Gross financial planning dialogues (Harry Gross Transcripts 1982).Our current system does not handle such expressions of doubt.3 Walker (1996) analyzed the Harry Gross financial planning dialogues (Harry Gross Transcripts 1982) toidentify features that distinguish acceptance from rejection.
However, she did not consider expressionsof doubt and some of her rejections would fall into our "express doubt" category.3Computational Linguistics Volume 25, Number 1?
Repetition: The speaker queries the doubted proposition; this usuallytook the form of a declarative followed by a question mark, as in "Youhave 40 thou(sand) inn a ram fund?"?
Explicit statements and questions: The speaker explicitly doubts whathas been said or asks for justification; examples from our corpus include"I'm not so sure of that."
and "Who ever said that?"?
Cue words: The speaker uses discourse markers to convey his doubt.
Inaddition to the cue word but that is often used to realize expressions ofdoubts in the other categories, cue words such as Really?
and What?were used by themselves to express doubt and cue words such as eventhough were used to convey doubt in utterances such as "Even though it'sfour more years ?"3.
Recogniz ing Expressions of DoubtWhen a listener in a collaborative interaction does not accept he proposition that thespeaker is trying to convey, anegotiation subdialogue ensues in which the participantsattempt to "square away" (Joshi 1982) their disparate beliefs.
Negotiation subdialoguesoften involve complex discourse acts that implicitly refer to some proposition that ispart of the existing dialogue context.
We have found that such complex discourseacts require evidence for their recognition.
Since the motivation for our work is therecognition of communicative acts that occur in negotiation subdialogues, this sectionexamines in detail how plausibility and evidence affect he recognition of one kind ofcomplex discourse act, an expression of doubt.For a listener to recognize a discourse action, it must be plausible that the speakerholds the requisite beliefs for performing the action.
(A belief is plausible if the avail-able evidence does not refute it.)
For example, in order for a listener to interpret anutterance as felicitously asking a question in order to obtain information, it must beplausible that the speaker does not already know the information and that the speakerbelieves that the listener may be able to provide it.
Similarly, to interpret an utteranceas expressing doubt at a proposition Pdoubt by contending Pi, it must be plausible thatthe speaker holds certain beliefs.
Consider a university setting in which each coursehas only one instructor and a speaker uses the proposition Pi, that Dr. Brown teachesArchitecture, to express doubt at the proposition Pdoubt, hat Dr. Smith is teaching Ar-chitecture, as in utterance (9) of Figure 1.
In order to intend (9) as an expression ofdoubt in a collaborative dialogue, the speaker must believe1.2.3.that the hearer has some belief in Pdoubtthat Pi is truethat if Pi is true, then Pdoubt is notThe hearer must be able to plausibly ascribe each of these beliefs to the speaker inrecognizing the expression of doubt.
First, it must be plausible that the speaker believesthat the hearer has some belief in the proposition that is being doubted, since it ispointless in a collaborative dialogue to express doubt at something about which thereis no disagreement.
4 It must also be plausible that the speaker has some belief in Pi,4 We are using "express doubt" in the sense of challenging the truth of a proposition.4Carberry and Lambert Modeling Negotiation Subdialogues(7) EA: What is Dr. Smith teaching?
(8) CA: Dr. Smith is teaching Architecture.
(9) EA: Isn't Dr. Brown teaching Architecture?Figure 1A dialogue with an expression of doubt.since otherwise the hearer could not be expected to believe that the speaker was usinga conflict between Pi and Pdoubt to question the validity of Paoubt.
Similarly, it must beplausible that the speaker believes that if Pi is true, then Pdoubt is not; otherwise thehearer could not be expected to think that the speaker believes that the truth of Piraises doubts about Pdoubt.Although being able to ascribe beliefs as plausible is necessary for recognition ofall discourse actions, some discourse actions, such as the expressions ofdoubt that weconsider in this paper, require further evidence.
This evidence is provided by linguis-tic, world, and contextual knowledge.
These knowledge sources can either provideevidence for a generic discourse act (such as an expression of doubt) or evidence thatthe conditions are satisfied for performing a specific discourse act (such as expressingdoubt that Dr. Smith is teaching Architecture).
In addition, contextual knowledge cansuggest a particular interpretation when equivalent evidence xists for several specificdiscourse acts.
These knowledge sources are discussed in the next sections.3.1 Linguistic Knowledge3.1.1 Evidence for a Generic Discourse Act.
A number of researchers (Reichman1978, 1985; Grosz and Sidner 1986; Polanyi 1986; Cohen 1987; Hirschberg and Litman1987; Litman and Allen 1987; Schiffrin 1987; Hinkelman 1989; Litman and Hirschberg1990; Knott and Dale 1994; Knott and Mellish 1996; Marcu 1997) have investigatedthe use in discourse of special words and phrases uch as but, anyway, and by the way.They found that these clue words, or discourse markers, have a number of differentfunctions, including indicating the role of an utterance in the dialogue, conveying therelationship between utterances, uggesting shifts in focus of attention, conveying thestructure of the discourse, etc.Consider again the dialogue shown in Figure 1.
If EA had followed (7)-(8) with(9a)(9)a. EA: Isn't Architecture one of our required courses?then EA's utterance would not be interpreted as expressing doubt but would insteadbe understood as merely seeking information about he Architecture course.
However,if this utterance is preceded by the clue word but, as in (9b) below,(9)b. EA: But isn't Architecture one of our required courses?then the utterance is expressing doubt, though we have difficulty ascertaining thereason for this doubt--perhaps EA believes that Dr. Smith does not teach courses thatstudents are required to take!Thus, clue words comprise one source of evidence in the recognition of discourseacts.
In particular, a clue word can provide evidence for a generic discourse act, suchas Express-Doubt, but it remains for other sources to resolve what is being doubted.Computational Linguistics Volume 25, Number 13.1.2 Evidence for a Specific Discourse Act.
Expressions of doubt do not always in-clude clue words, as illustrated by utterance (9) in Figure 1.
In the absence of a clueword, we need evidence that the speaker holds the three beliefs, listed earlier in Sec-tion 3, for performing a specific discourse act.
Evidence for the second belief (that thespeaker believes that Pi is true) is often provided by the surface form of the utterance,such as an utterance of the form "Isn't Pi ?"
--for example, "Isn't Dr. Brown teaching Ar-chitecture?"
in (9).
This surface form indicates a strong belief in the queried propositionwhile a simple yes-no question, such as "Is Dr. Brown teaching Architecture?
", does not.Therefore, if EA were to follow (7)-(8) with "Is Dr. Brown teaching Architecture?
", EAwould seem to have a misconception that more than one person can teach a courseor perhaps be seeking information in order to subsequently express doubt--but heutterance itself is not conveying doubt that Dr. Smith is teaching Architecture.
Thusthe surface form of the utterance is one source of evidence that the speaker holds therequisite beliefs for performing a specific discourse act.3.2 World KnowledgeWorld knowledge in the form of stereotypical beliefs is another source of evidencethat the speaker holds the requisite beliefs for a particular discourse act.
For example,world knowledge can provide evidence for the third speaker belief, that if Pi is true,then Pdoubt is not.
Suppose that it is stereotypically believed that prestigious fellow-ships are awarded for sabbaticals, that faculty on sabbatical do not teach, and thatfaculty only teach in their area of expertise.
Consider the dialogue shown in Figure 2.After (13), there are two propositions that have been conveyed by CA but not yetcompletely accepted by EA: the proposition that Dr. Smith is not on sabbatical andthe proposition that Dr. Smith is teaching CS360, cormnunicated by utterances (13)and (11), respectively.
A subsequent utterance might express doubt at one of thesepropositions or might forego the opportunity to doubt them, perhaps by pursuingsome discourse act unrelated to either of the propositions.
Consider the followingthree possible continuations of the dialogue:(14)a.b.C.EA: Wasn't Dr. Smith awarded a Fulbright?EA: Isn't Dr. Smith a theory person?EA: Isn't Dr. Smith an excellent teacher?While (14a) and (14b) seem to be expressing doubt, (14c) is simply seeking furtherinformation about Dr. Smith.
The reason for this difference in interpretation is thatin the case of (14a) and (14b), evidence from world knowledge suggests that EAbelieves that Pi (the proposition that EA contends i  true) implies that one of the twoopen propositions i false, whereas no such evidence xists in the case of (14c).
Inthe case of (14a), since it is stereotypically believed that prestigious fellowships areawarded for sabbaticals, EA's utterance should be interpreted as expressing doubt atthe proposition that Dr. Smith is not on sabbatical.
In the case of (14b), since Dr. Smithbeing a theory person is an alternative to Dr. Smith being a systems person and it isstereotypically believed that being a systems person is necessary for teaching CS360(a systems course), EA's utterance would instead be interpreted as expressing doubtat the proposition that Dr. Smith is teaching CS360.
Thus, world knowledge in theform of stereotypical beliefs is another source of evidence that the speaker holds therequisite beliefs for performing a particular discourse act.If EA had uttered (14c), EA's utterance would be interpreted as merely seekingnew information since there is no domain knowledge suggesting that EA believes thatDr.
Smith being an excellent teacher contributes to determining whether Dr. Smith isCarberry and Lambert Modeling Negotiation Subdialogues(10) EA: Who is teachir~g CS360 (a systems course)?
(11) CA: Dr. Smith is teaching CS360.
(12) EA: Isn't Dr. Smith on sabbatical?
(13) CA: No, Dr. Smith is not on sabbatical.Figure 2A dialogue with two open propositions.on sabbatical or to identifying the instructor of CS360.
Note that (14c) demonstrateswhy plausibility alone is insufficient for recognition.
Although there is no evidencethat EA believes that Dr. Smith being an excellent teacher implies that Dr. Smith ison sabbatical or that Dr. Smith is not teaching CS360, there is also no evidence to thecontrary, and thus it is plausible that EA believes that Dr. Smith being an excellentteacher indicates that he is on sabbatical or that he is not teaching CS360.
This is notsufficient, however, to interpret (14c) as an expression of doubt.3.3 Contextual KnowledgeAn agent can infer from a dialogue many of the beliefs of the other participant.These acquired beliefs about the other participant's beliefs form one kind of contex-tual knowledge that can be used as evidence for the beliefs listed above.
In addition,contextual knowledge determines the salience (or degree of prominence) of propo-sitions at the current point in the dialogue, and salience is a factor that constrainsthe interpretation f coherent discourse actions.
Consider the first three utterances inthe dialogue shown in Figure 2.
EA's acceptance of CA's telling of the propositionthat Dr. Smith is teaching CS360 establishes the mutual belief that CA believes thatDr.
Smith is teaching CS360 and thus provides evidence for the first belief; 5in addi-tion, the proposition that Dr. Smith is teaching CS360 becomes alient and is added tothe dialogue context.
Thus, while an utterance such as (12a)(12)a. EA: Doesn't Dr. Smith usually teach theory courses?might be used following (11) to express doubt at the statement that Dr. Smith isteaching CS360, it cannot be used following (11) to express doubt at the propositionthat Dr. Smith teaches CS410 because 1) there is no reason for EA to believe that CAhas any belief in the proposition that Dr. Smith teaches CS410, and 2) the propositionthat Dr. Smith teaches CS410 is not salient at this point in the dialogue.In addition, contextual knowledge plays two other roles in the recognition ofdiscourse acts.
First, in the case of expressions of doubt, contextual knowledge dis-tinguishes propositions that have not yet been accepted by the speaker and thus areopen for rejection.
Consider again the dialogue in Figure 2.
After (13), there are twopropositions that have not yet been accepted by EA and are thus open for rejectionby EA.
If EA were to continue with (14b), repeated below,(14)b. EA: Isn't Dr. Smith a theory person?then EA would again be expressing doubt at the proposition that Dr. Smith is teach-ing CS360 and would have implicitly conveyed acceptance of the proposition that5 Note that here EA is only accepting CA's felicitous telling of the proposition, but EA is not adoptingthe proposition asone of his own beliefs.7Computational Linguistics Volume 25, Number 1Dr.
Smith is not on sabbatical.
Thus, as the conversation continues, only one propo-sition would remain open for rejection: the proposition that Dr. Smith is teachingCS360.
This claim is supported by a combination of 1) the stack paradigm (Polanyi1986; Reichman 1978; Grosz and Sidner 1986; Litman and Allen 1987), which treatstopic structure as following a stack-like discipline; 2) focusing heuristics (McKeown1983) that suggest that if a speaker has more to say about a topic, then he should doso before moving back to a topic deeper on the stack; and 3) the notion of implicitacceptance (discussed in Section 4.6) that argues that passing up the opportunity toreject an assertion in a collaborative dialogue communicates acceptance of it.Second, contextual knowledge orders propositions according to their relative saliencein the current dialogue.
This salience can be used to arbitrate among discourse actsfor which there is equivalent evidence.
Consider again the dialogue in Figure 2 andsuppose that EA had continued with (14d).(14)d.
EA: But isn't Dr. Smith an excellent teacher?Here we have a clue word suggesting an expression of doubt, but the speaker could beexpressing doubt either that Dr. Smith is not on sabbatical or that Dr. Smith is teachingCS360.
In both cases, we lack evidence for the third speaker belief.
Contextual knowl-edge suggests that, all other things being equal, the proposition being doubted is theproposition that Dr. Smith is not on sabbatical, since it is the most salient propositionthat is open for rejection at this point in the dialogue.
Thus, contextual knowledgearbitrates when equivalent evidence is available for several specific discourse acts.3.4 SummaryIn addition to the requisite speaker beliefs being plausible and the constraints on thediscourse act being satisfied (such as the constraint that a proposition be salient at thecurrent point in the dialogue), certain discourse acts require additional evidence fortheir recognition.
Two kinds of evidence that may be used in recognizing discourse ac-tions are 1) evidence (such as a clue word) for a generic discourse act, and 2) evidencethat a speaker holds the requisite beliefs for performing a particular discourse act.Evidence for these beliefs can come from linguistic, world, or contextual knowledge.Although we have illustrated each of these knowledge sources by showing how theymight provide evidence for one of the requisite beliefs for expressing doubt, it shouldbe noted that each knowledge source might also be used as evidence for other be-liefs required for expressing doubt or for beliefs for other discourse acts.
For example,although it does not generally arise in the kind of interactive dialogues that we arestudying, world knowledge in the form of stereotypical beliefs might be used as evi-dence that a speaker believes that a hearer has some belief in the doubted propositionPdoubt"4.
The Process ModelGrosz and Sidner (1986) claim that a robust model of understanding must use multi-ple knowledge sources in order to recognize the complex relationships that utteranceshave to one another.
We have developed an algorithm that combines linguistic, world,and contextual knowledge, such as that identified in Section 3, in order to recognizecomplex discourse acts, including one kind of expression of doubt.
Linguistic knowl-edge consists of clue words and the surface form of the utterance; world knowledgeincludes aset of stereotypical beliefs that users generally hold and recipes for perform-ing discourse acts; and contextual knowledge consists of a model of the user's beliefs8Carberry and Lambert Modeling Negotiation Subdialoguesacquired from the preceding dialogue, the current structure of the discourse, the ex-isting focus of attention (that aspect of the task on which the participants' attention iscurrently centered), and the relative salience (degree of prominence) of propositionsin the discourse.The remainder of this section presents the core ideas of our algorithm.
Section 4.1shows why it is necessary to capture varying degrees of belief in a proposition andpresents the multistrength belief model used in our system.
Section 4.2 describes ourrepresentation f recipes for actions and shows how our recipe for an Inform actionrefrains from assuming that the listener will adopt the communicated proposition aspart of his own beliefs; it also presents a recipe for expressing doubt and showshow constraints on the speaker's beliefs are captured in the recipe's applicability con-ditions.
Section 4.3 gives an overview of our dialogue model.
Section 4.4 describeshow chaining is used to hypothesize a sequence of higher-level discourse acts that aspeaker may be performing; Section 4.5 introduces the notion of a discourse actionthat requires evidence for its recognition; Section 4.6 discusses how our model recog-nizes implicit acceptance of a discourse act; and Section 4.7 presents our recognitionalgorithm that uses a combination of linguistic, world, and contextual knowledge inrecognizing discourse acts.4.1 A MultiStrength Belief ModelAs argued in Section 3, if a speaker is expressing doubt at a proposition Pdoubt bycontending some other proposition Pi, then the speaker must have some belief inPi.
Evidence for this belief is often provided by the surface form of the speaker'sutterance, such as an utterance of the form "Isn't Pi?"
for example, "Isn't Dr. Brownteaching Architecture?"
But if we treat such an utterance as conveying certain beliefthat Pi is true, then we cannot handle situations in which an utterance such as this ismerely requesting verification since a speaker cannot felicitously seek verification ofa proposition that he already knows is true.
Therefore, since modeling only ignoranceand certainty is inadequate for recognizing complex discourse acts, it is necessary tomodel the strength of an agent's beliefs.4.1.1 Representing Varying Degrees of Belief.
We use a multistrength model of belief,which captures not only ignorance and certainty about the truth of a proposition butalso several degrees of belief in between.
Utterances of the form "Isn't Pi ?"
are treatedas conveying a strong (but uncertain) belief in Pi.
In this way, our system is ableto handle instances in which an utterance of the form "Isn't Pi?"
is used to requestverification (since the utterance is viewed as conveying uncertainty about Pi) as wellas instances in which it is used to express doubt (since the utterance is viewed asconveying some belief in Pi).Our multistrength belief model maintains three degrees of belief: certain belief(a belief strength of C); strong but uncertain belief, as in "Isn't Dr. Brown teachingArchitecture?"
(a belief strength of S); and weak belief, as in "I think that Dr. Cayne mightbe an education instructor" (a belief strength of W).
Three degrees of disbelief (indicatedby attaching a subscript of N, such as SN to represent strong disbelief and WN torepresent weak disbelief) are also maintained, and one degree indicating no beliefabout a proposition (a belief strength of 0).
We adopted three degrees of positiveand negative belief in our model because that was the minimum number of degreesrequired for modeling the beliefs communicated in the dialogues that we examinedand in the negotiation subdialogues that our system is intended to handle.Although an agent has some specific strength of belief in a proposition, the otheragent may not always know precisely what that strength of belief is but may be able9Computational Linguistics Volume 25, Number 1to bound it--for example, he may be able to say that the first agent has some beliefin a proposition.
Our belief model uses belief intervals to capture this, where a beliefinterval specifies the range of strengths within which an agent's beliefs are thought tofall.Allen and Perrault (1980) noted the need to represent an agent's wanting to knowthe referent of a term in a proposition, without having to specify what that referentwas.
For example, if EA asks CA "Who is teaching CS360?
', we cannot represent CA'sbelief that EA wants to know the teacher of CS360 asbelieve(CA, want(EA, believe(EA, Teaches(Dr.Smith, CS360))))since this representation says that CA believes that EA wants to believe that the teacheris Dr. Smith (but EA, in asking the question, may not be predisposed toany such beliefand may in fact reject "Dr. Smith" as the answer to the question).
Allen and Perraultaddressed this with knowref and knowif predicates, which represented an agent's know-ing the referent of a term in a proposition and knowing whether a proposition is true.Thus CA's belief that EA wants to know the teacher of CS360 in the above examplemight be represented asbelieve(CA, want(EA, knowref(EA, _fac, Teaches(_fac, CS360))))In our multistrength belief model, knowref is treated as being certain about he referentof the term in the specified proposition and knowif is treated as being certain aboutwhether a proposition is true or false.As the dialogue progresses, the belief model must capture the changing beliefs ofthe user.
When a discourse act has been successful, its goals can be used to updatethe belief model.
For example, if the user explicitly accepts the proposition conveyedby the utterance "Dr. Smith is teaching Architecture" (perhaps by saying "Yes, I'I1 acceptthat"), then the system can update its belief model to include the belief that the userhimself believes that Dr. Smith is teaching Architecture.
However, explicit acceptanceis less common than implicit acceptance; Section 4.6 discusses implicit acceptance andits recognition.Since we do not currently have a response generation component, our systemprocesses the utterances of both participants, alternating between playing the role ofCA and the role of EA.
Note that this differs from playing the role of a third-partyobserver--when the system plays the role of EA, the system has access to EA's beliefs(including EA's beliefs about he current dialogue model and EA's beliefs about CA),and when the system plays the role of CA, it has access to CA's beliefs.
However,whenever the system assumes the role of a participant and processes a new utterance,it is assumed that this participant has correctly interpreted previous utterances andhas a correct model of the preceding dialogue.4.1.2 Related Work on Modeling Belief.
Young (1987) built a model in which thebeliefs of the user are part of an explicit, missing, or stereotype module (he usedthe system's beliefs as a stereotype).
Although this system provides needed differ-entiation among beliefs that the system knows the user holds, those that the systemhas attributed to the user, and those about which the system has no knowledge, thismodel still does not contain degrees of partial belief that are essential for modelingdiscourse acts such as expressing doubt.
Ballim and Wilks (1991) developed a nestedbelief model that captures an agent's beliefs about other agents' beliefs.
Their systemcombines belief ascription based on stereotypes with belief ascription based on per-turbations of the system's own beliefs, but they do not represent how strongly abeliefis held.10Carberry and Lambert Modeling Negotiation SubdialoguesGalliers (1991, 1992) has specified a nonnumeric theory of belief revision thatrelates trength of belief to persistence of belief.
She points out that a belief model forcommunication must contain a multistrength model of beliefs that can be modifiedas the conversation proceeds.
She uses endorsements (Cohen 1985) in an assumption-based truth maintenance system (ATMS \[DeKleer 1986\]) to specify a system that ordersbeliefs according to how strongly they are held.
This ordering is used to calculate whichbeliefs should be revised when beliefs are challenged in the course of conversation.Walker (1991, 1992) has examined dialogues in which people repeat what theyalready know either in question or statement form (e.g., "I have four children."
"OK.Four children.").
Walker claims that this repetition by the second speaker is given sothat the first speaker ealizes that her utterance was understood and believed.
Thatis, cooperative listeners often provide some evidence to speakers to indicate that thelistener believes the speaker's claims.
Like Galliers, Walker has based the strengthof belief on the amount and kind of evidence available for that belief.
Cohen andLevesque (1991a) also found this kind of corroboration.
Our work has not investigatedthe belief reasoning process or how much evidence is needed for an agent to have aparticular amount of confidence in a belief.
Instead, we have been concerned withtaking into account different communicated strengths of belief and the impact hat thedifferent belief strengths have on the recognition of discourse acts.Our multistrength belief model is very simple and is only intended to meet our sys-tem's need for representing how strongly an agent holds a particular belief.
We recentlybecame aware of work by Driankov on a logic in which belief/disbelief pairs capturehow strongly a proposition is believed (Driankov 1988; Bonarini, Cappelletti, and Cor-rao 1990).
6This work appears to be the only formally defined and well-developed logicthat models strength of belief.
With the exception that Driankov's logic does not in-chide a state of weak belief, it appears to provide the representational and reasoningcapability needed by our system and we intend to investigate it for future use.4.2 Discourse RecipesIn previous work, we noted the need to differentiate among domain, problem-solving,and discourse actions (Lambert and Carberry 1991; Elzer 1995).
In task-oriented con-sultation dialogues, the participants are constructing a plan for achieving some domaingoal, such as owning a home, and the resultant plan will consist of domain actionssuch as applying for a mortgage.
In order to construct the domain plan, the partici-pants pursue problem-solving actions such as evaluating alternative domain actionsor correcting an action in the partially constructed domain plan.
Domain and problem-solving actions have been investigated by many researchers (Allen and Perrault 1980;Perrault and Allen 1980; Wilensky 1981; Litman and Allen 1987; van Beek and Cohen1986; Ramshaw 1989; Carberry 1990).Discourse actions are communicative actions that are executed by the dialogueparticipants in order to obtain or convey the information eeded to pursue the problem-solving actions necessary for constructing the domain plan.
Examples of very differentdiscourse actions include answering a question, informing, and expressing doubt.
Al-though our system models domain, problem-solving, and discourse actions, this paperis only concerned with recognizing discourse acts, particularly complex discourse actssuch as expressing doubt.Our system's knowledge about how to perform actions is contained in a library6 We would like to thank one of the anonymous reviewers and Ingrid Zukerrnan for bringing this workto our attention.11Computational Linguistics Volume 25, Number 1of discourse, problem-solving, and domain recipes (Pollack 1990).
Our representationof a recipe includes a header giving the action defined by the recipe, the recipe type,preconditions, applicability conditions, constraints, a body, effects, and a goal.
Therecipe type is primitive, specialization, or decomposition.
If the recipe type is primitive,then the body of the recipe is empty  and the header action corresponds with a primitiveaction in the domain.
In a specialization recipe, the body gives a set of alternative waysof performing the header action (Pollack 1990; Kautz 1990).
For example, one mightearn credit in a course either by taking the course for credit or getting credit by exam.In a decomposit ion recipe, the body gives a set of subactions that constitute performingthe header action.
A # preceding a subaction in the body of a decomposit ion recipeindicates that the subaction can be performed any number  of times (including zero).
7Constraints limit the allowable instantiation of variables in each component  of a recipe(Litman and Allen 1987).
For example, a variable might have a constraint requiringthat it be instantiated with a proposit ion that is salient at the current point in thediscourse.
Which instantiations of variables will satisfy the constraints is part of theshared knowledge of the participants.
8Appl icabi l i ty condit ions (Carberry 1987) are conditions that must  be satisfied inorder for a recipe to be reasonable to pursue in a given situation.
The applicabilityconditions of our discourse recipes capture attitudes (beliefs and wants) that the agentof the action must  hold in order for it to be felicitous (Searle 1970).
Applicabil ityconditions differ from preconditions in that one can plan to satisfy preconditions butit is generally anomalous to try to satisfy applicability conditions.
For example, in orderfor _agent1 to inform _agent2 of _proposition, _agent1 must  believe that _propositionis true and must  not believe that _agent2 already believes _proposition.
It would beanomalous for _agent1 to try to adopt the proposit ion as one of his beliefs solely for thesake of being able to inform someone lse of it, and similarly it would be anomalousfor _agent1 to get _agent2 to disbelieve a proposit ion so that _agent1 can subsequentlyinform him of it.
9Belief intervals are used in the applicability conditions to specify the range ofstrengths that an agent's behefs may assume.
Intervals such as \[bi:bj\] specifya strength of belief between bi and bj inclusive.
For example, Figure 3 displays therecipes for the Inform and Tell discourse acts.
The goal of the Inform action,believe (_agent2, _proposition, \[C : C\] ), is that _agent2 be certain that _propositionis true.
On  the other hand, believe(_agent2, _proposition, \[CN:S\]) means that_agent2 is not convinced that _proposition is true (i.e., _agent2 could have any be-lief ranging from being certain that _proposition is false to having a strong beliefthat it is true).
Thus the applicability condition believe (_agent l, believe (_agent2,_proposition, \[CN : S\] ), \[0 :C\] ) of the Inform act in Figure 3 means that _agent I must7 In this work, we are interested in understanding, not generating, responses.
However, a generationsystem would pursue an action preceded by # if its applicability conditions are satisfied and thesystem does not believe that the action's goal will be satisfied if the action is omitted.
The beliefreasoning techniques described in Cawsey et al (1992) can be used in modeling this.8 In this research, we have assumed that the participants have equivalent knowledge of language andmaintain equivalent discourse models, and we have not addressed the problem of recognizingmiscommunication.9 How applicability conditions on discourse acts are checked uring planning is an interesting questionthat requires further esearch.
Consider an agent who wants to determine whether a proposition istrue; the agent might accomplish t is by asking another agent about he proposition.
An applicabilitycondition on asking another agent is that the speaker wants to know the other agent's belief about heproposition.
But when did this want come into existence?
It certainly must be satisfied at the time thequestion is asked, but instead of being part of the initial state, it appears to result from the speaker'sdecision about how to obtain the desired information.12Carberry and Lambert Modeling Negotiation SubdialoguesDiscourse RecipeAction: Inform(_agentl, _agent2, _proposition){_agent1 informs _agent2 of_proposition}Recipe-Type: DecompositionAppl Cond: believe(_agentl, _proposition, \[C:C\])believe(_agentl, believe(_agent2, _proposition, \[CN:S\]), \[0:C\])Body: Tell(_agentl, _agent2, _proposition)#Address-Behevability(_agentl, _agent2, _proposition)Effects: believe(_agent2, want(_agentl, believe(_agent2, _proposition, \[C:C\])),\[c:c\])Goal: believe(_agent2, _proposition, \[C:C\])Discourse RecipeAction: Tell(_agentl, _agent2, _proposition){_agent1 tells _agent 2 of_proposition}Recipe-Type: DecompositionAppl Cond: believe(_agentl, _proposition, \[C:C\])Body: Surface-Say-Prop(_agentl, _agent2, _proposition)#Address-Understanding(_agentl, _agent2, _proposition)Effects: told-about(_agentl, _agent2, _proposition)Goal: beheve(_agent2, believe(_agentl, _proposition, \[C:C\]), \[C:C\])Figure SRecipes for Inform and Tell discourse acts.either be ignorant about _agent2's belief in _proposition or have some belief (possi-bly certain) that _agent2 is not already certain that _proposition is true, i.e., _agentldoes not believe that _agent2 is already convinced that _proposition is true.
In de-termining the belief strengths pecified in the applicability conditions, we examinedthe beliefs that an agent must hold and tried to identify the minimum and maximumstrength of belief that would make the discourse act reasonable to pursue.We have divided the effects of an action into two subclasses: 1) the results ofcorrectly performing the action, which are labeled effects, and 2) the desired effectsof the action (over which the agent may lack control), which are labeled goals.
Forexample, in the case of domain actions, the effect of applying for graduate study isthat one has applied, while the goal is that one be accepted for graduate study.
Thisdistinction between effects and goals is particularly important in the case of discourseactions, where the agent cannot be assured that an action will have its intended result.Variables in recipes are represented as lowercase strings preceded by an under-score, with the string reflecting the variable's type; for example, _course1 and _course2refer to variables of type course.In Allen's seminal model of plan recognition (Allen 1979), the bodies of opera-tors could contain either goals to be achieved or action names with parameters.
Inour system, preconditions are represented as goals to be achieved, while the bodiesof recipes specify actions.
Since the recipe for each action in our recipe library con-tains a single goal in its goal field, this suffices--the goal makes clear the purpose ofthe action in a plan.
However, in a richer domain where an action could be used toachieve several different goals, 1?
it would be necessary to specify the intended goal inthe recipe body and chain from it to the desired action in order to capture the moti-vation for performing the action.
During plan recognition, our system matches goals10 For example, one might read a book to gain knowledge orto entertain oneself.13Computational Linguistics Volume 25, Number 1against preconditions of other actions, and it matches actions against he subactionsin the bodies of the recipes for other actions.
The Effects field is not used for chainingduring plan recognition; however, as discussed in Section 4.6.2, the effects and goalsin discourse recipes are used for updating a model of the user's beliefs.4.2.1 Formulation of Discourse Recipes.
The bodies of our discourse recipes are basedon work by other esearchers (Allen and Perrault 1980; Searle 1970; Cohen and Perrault1979), dialogues in which we have participated, the naturally occurring dialogues thatwe examined, and our hypotheses about how our system might be expanded in thefuture.
For example, consider the discourse recipes for Obtain-Info-Ref, Ask-Ref, andAnswer-Ref shown in the appendix.
To obtain information about a proposition via dia-logue, _agent1 must ask another agent about he proposition (Ask-ReJ) and the secondagent must provide the requested information (Answer-ReJ); this is typical of naturallyoccurring dialogue and is captured in the body of our Obtain-Info-Ref discourse act.
Theapplicability conditions of the Obtain-Info-Ref act, such as the condition that _agent1believe that _agent2 knows the information, are based on criteria identified in Searle(1970), Cohen and Perrault (1979), and Allen and Perrault (1980).
The body of the Ask-Ref action consists of making the request i self and making the request acceptable; thisis because in our own interactions we have encountered situations in which an agentwill make a request and then justify it to the listener.
The applicability conditions ofAsk-Ref refer to _agent2's beliefs about he proposition; for example, one applicabilitycondition is that _agent1 wants to know the term that _agent2 believes will satisfy aproposition.
We contend that this captures what a speaker wants in asking a listenerabout a proposition (i.e., the speaker wants to know the listener's beliefs about theproposition), and this formulation also allows the Ask-Ref to be used as a subactionof a Test-Knowledge actin a tutoring system.
11Note that the fact that a speaker whois seeking information really wants to know correct information about a propositionwas captured in the applicability conditions of the Obtain-Info-Ref discourse act.Some of our discourse recipes, such as Obtain-Info-Ref, include subactions by boththe initiating agent and the other participant.
This captures the intention of the initi-ating agent o perform his required subactions as well as the intention that the otheragent follow through on her role in the plan for this action.
Thus, once the secondagent recognizes that the initiating agent wants to obtain information, the secondagent will also recognize that the initiating agent intends for her to play the role ofproviding that information.
While an agent can construct a plan that includes acts byanother agent, the planning agent cannot guarantee the other agent's behavior andthus such discourse plans can fail.
(See Chu-Carroll and Carberry \[1994\] for researchon dialogues in which agents do not always follow through on their intended role yetstill fulfill their collaborative r sponsibilities.
)A different approach would be to maintain such knowledge about adjacency pairs(Schegloff and Sachs 1973) and expected continuations in a transition etwork separatefrom the discourse recipes, as was done by Reithinger and Maier (1995).
The advantageof this approach is that fewer discourse recipes are needed and continuations aregeneralized.
Such a representation would enable us to remove the actions that addressacceptance from our discourse recipes but still capture the expectations for them inthe transition etwork.
However, the disadvantage of this approach is that the higher-level discourse act would no longer constrain the possible continuations.
For example,11 Searle (1970) notes that here are two kinds of questions, ones whose objective is to obtain knowledgeand ones whose objective is to test another's knowledge.14Carberry and Lambert Modeling Negotiation Subdialoguesan Evaluate-Answer discourse act is an expected follow-up to an Answer-Ref when theyare part of a higher-level Test-Knowledge discourse act but not when the Answer-Ref ispart of an Obtain-Info-Ref discourse act.
Further esearch is needed to identify the bestmechanism for capturing the requisite discourse knowledge.4.2.2 The Inform Discourse Recipe.
As noted by Grosz and Sidner (1990), the assump-tion that one participant will slavishly respond to the wishes of the other participantdoes not reflect collaborative interaction.
In Cohen and Perrault's formulation of speechact operators (Cohen and Perrault 1979), the effect of an Inform was that the hearerbelieved that the speaker believed the proposition.
He postulated a Convince act thatwould cause the hearer to believe the proposition, but this act was left undevelopedand its definition did not allow for the participants onegotiate their beliefs.
The effectof an Inform act in Allen and Perrault's ystem (Allen and Perrault 1980) is that thehearer believes the communicated proposition--this definition would seem to say thatthe hearer always accepts the information provided by the speaker.
12Although Allenand Perrault's model was only concerned with recognizing the intention to perform anInform act, using his formulation to model negotiation dialogues (where Inform actionsmay not automatically accomplish their purpose) is problematic.
In Perrault's (1990)persistence model of belief, the hearer adopts a communicated proposition unless hehas evidence to the contrary, in which case his original belief persists.
Thus, modelssuch as Allen and Perrault's cannot account for a hearer who does not accept a com-municated proposition, and Perrault's model cannot account for a hearer who changeshis beliefs about a proposition after negotiation.We want to overcome these limitations and be able to handle negotiation subdia-logues in which participants attempt to come to some agreement about heir disparatebeliefs.
Thus, the body of the recipe for our Inform act (Figure 3) contains two subac-tions: one in which the speaker tells the hearer a proposition and a second in whichthe participants address the believability of the communicated proposition and try tocome to agreement.
In addition, as discussed in the preceding section, the effects of ourdiscourse recipes are often different from the goals.
Although this does not solve theproblem of recognizing perlocutionary effects, it does allow us to capture the notionthat one can, for example, perform an Inform act without he hearer adopting the com-municated proposition.
Thus, the goal of a discourse recipe is a desired perlocutionaryeffect (an effect hat the speaker wishes the action to have, e.g., believe (hearer, P,\[C:C\] ) in the case of an Inform action), and the effects of a discourse recipe are theillocutionary effects (that is, the effect hat the speech act has when it is performed andrecognized by the hearer, e.g., believe (hearer, want (speaker, bel ieve (hearer, P,\[C : C\] ) ), \[C : C\] ) in the case of an Inform action).4.2.3 An Express-Doubt Discourse Recipe.
Figure 4 presents our discourse recipe forexpressing doubt.
Note that its applicability conditions capture the requisite beliefslisted in Section 3.
The second applicability condition in Figure 4 excludes certainbelief in _proposition2; this is because the body of the recipe is an action of conveyinguncertain belief in _proposition2 and represents instances where an expression of doubtis realized as a surface negative question or a tag question.
Note also that one of theeffects of the Express-Doubt discourse act is that the listener believes that the speakerwants to resolve the conflict between the two propositions and that the goal of the12 In personal communication, Allen has said that he effect of his Inform action was intended tocapturethe agent's goal in performing the action.
In Allen (1979) he mentions the need for a Decide-to-Believeact, but nothing further isdone with it.15Computational Linguistics Volume 25, Number 1Discourse RecipeAction: Express-Doubt(_agentl, _agent2, _proposition1, _proposition2){_agent1 expresses doubt o _agent2 about _proposition1 by contending that _proposition2is true}Recipe-Type:Appl Cond:Constraints:Body:Effects:Goal:Decompositionbelieve(_agentl, believe(_agent2, _propositionl, \[S:C\]), \[S:C\])believe(_agentl, _proposition2, \[W:S\])believe(_agentl, _proposition2 ~ ~_propositionl, \[S:C\])salient( _proposition1 )Convey-Uncertain-Belief(_agentl, _agent2, _proposition2)believe(_agent2, believe(_agentl, _proposition1, \[SN:WN\]), \[S:C\])believe(_agent2, believe(_agentl,_proposition2 ~ -~_propositionl, \[S:C\]),\[s:c\])believe(_agent2, want(_agentl, Resolve-Conflict(_agent2, _agent1,_proposition1, _proposition2)), \[S:C\])want(_agent2, Resolve-Conflict(_agent2, _agent1, _proposition1,_proposition2))Figure 4A recipe for an Express-Doubt discourse act.Express-Doubt discourse act is that the listener also wants to resolve the conflict.
Themutual desire for conflict resolution resulting from a successful Express-Doubt discourseact leads to a negotiation subdialogue (initiated by the Express-Doubt action).4.3 The Dialogue ModelWe maintain a structure called a dialogue model that captures the system's beliefsabout the existing dialogue context.
The discourse level of the dialogue model containsa tree structure called the discourse tree.
Each node of the discourse tree representsa discourse or communicative act that has been initiated by one of the dialogue par-ticipants, and the children of a node represent discourse acts that are being pursuedin order to perform the parent action.
For example, asking and answering a ques-tion (Ask-Ref and Answer-ReJ) are part of obtaining information (Obtain-Info-ReJ) inthe discourse tree in Figure 5.
The lowest uncompleted action in the discourse tree ismarked as the focus of attention; 13it represents the first expectation for subsequentutterances.
In Figure 5, the focus of attention is the Tell action.
The active path consistsof the sequence of actions along the path from the action that is the focus of attentionto the root node.
The actions on the active path provide successive xpectations aboutthe role of the next utterance in the dialogue; actions closer to the current focus ofattention are regarded as more salient than those further back on the active path.
14For example, in the discourse tree of Figure 5, the first expectation is that if EA doesnot understand CA's previous statement, hen EA will now choose to address herunderstanding of it and thereby contribute to the Tell act that is the existing focusof attention.
The next expectation is that if EA has any doubt about the propositionconveyed by CA, then EA will choose to address its believability, thereby contributingto the Inform discourse act that is the next action on the active path.
(As shown inFigure 3, Address-Understanding s a subaction in the recipe for the Tell discourse act,and Address-Believability is a subaction in the recipe for the Inform discourse act.
)13 By uncompleted, we mean not as yet known to be completed.14 The active path is a sequence ofactions; this work has not considered multithreaded discourse, a topicthat Ros6 et al (1995) have begun to investigate.16Carberry and Lambert Modeling Negotiation SubdialoguesObtain-lnfo-Ref(EA, CA, _fac, TeachesC fac, CS360)).'
- e (EA, CA, _fac, Teaches(_fac, CS360)) I I Ask R f iRef-Request(EA, CA, _fac, Teaches(_fac, CS360)) I\[ Surface-WH-Question(EA, CA, _fac, TeachesC.fac, CS360)) JEA: Who is teaching CS3607Key:* Current focus of attentionFigure 5Sample discourse tree.II Answer-Ref(CA, EA, fac, Teaches(_fac, CS360)) \[Ilnform(CA, EA, Teaches(Dr.Smith, CS360)) \[* \[Tell(CA, EA, Teaches(Dr.Smith, CS360)) II Surface-Say-Prop(CA, E , Teaches(Dr.Smith, CS360)) ICA: Dr. Smith is teaching CS360.4.4 Hypothesizing Discourse Acts by ChainingOur process model starts with the semantic representation f a new utterance and usesplan inference rules (Allen and Perrault 1980; Carberry 1988) along with constraintsatisfaction (Litman and Allen 1987) to hypothesize chains of actions A1,A2 .
.
.
.
.
Anthat the speaker might be intending to perform with the utterance.
In such a chain,action Ai contributes to the performance of its successor action Ai+l.
For example, thesemantic representation f an utterance such as "Dr. Smith is teaching Architecture" isSurface-Say-Prop(_agentl, _agent2, Teaches(Dr.Smith, Architecture))A Surface-Say-Prop is a subaction in the recipe for a Tell discourse act, which in turn isa subaction in the recipe for an Inform discourse act.
Thus chaining from the surfaceutterance produces a sequence of hypothesized discourse acts, each of which plays arole in the performance of its successor on the chain.We have expanded on Litman and Allen's (1987) notion of constraint satisfactionand Allen and Perrault's (1980) use of beliefs.
As described earlier, many of the appli-cability conditions in our discourse recipes are beliefs that the agent of the action musthold in order for the action to be felicitous.
Our recognition algorithm requires thatthe system be able to plausibly ascribe these beliefs in hypothesizing a new action; ifthe belief ascription is implausible or if the constraints of the discourse recipe are notsatisfied, the inference is rejected.4.5 Evidence ActionsAs we claimed in Section 3, actions such as the expressions of doubt in utterances(14a) and (14b) (repeated in Figure 6) require evidence for their recognition.
Let usfurther examine why this is the case.
Figure 7 illustrates a situation in which wehave several discourse acts (with different degrees of salience) that the agent mightbe expected to pursue.
The solid boxes show some of the actions that are part of the17Computational Linguistics Volume 25, Number 1(10)(11)(12)(13)EA: Who is teaching CS360 (a systems course)?CA: Dr. Smith is teaching CS360.EA: Isn't Dr. Smith on sabbatical?CA: No, Dr. Smith is not on sabbatical.(14)a.(14)b.
(14)c.EA: Wasn't Dr. Smith awarded a Fulbright?EA: Isn't Dr. Smith a theory person?EA: Isn't Dr. Smith an excellent teacher?Figure 6A sample dialogue with surface negative questions.l action-l(EA, CA, PROPA) Il action-2(EA~ CAt PROPB) I ""?....._maction-3(EA, CA, PROPC) I"~.
.'
' , ,"''..,e-action(EA, CA,_propl,  PROPD) jI surface-action(EA, CA, PROPD) rI .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
_ JFigure 7Relating an inference path to the existing dialogue context.existing dialogue context 15 and the dashed boxes show a chain of actions inferred fromthe new utterance.
As depicted in the figure, the process model can infer a chain ofactions starting with some surface speech act sur face-act ion(EA,  CA, PROPD), up tosome other action, act ion  (EA, CA, PROPD), up to some other action, e -ac t ion  (EA, CA,_propl, PROPD).
The action e -ac t ion  contains two propositions.
One of these, PROPD,is instantiated by chaining from the earlier action, act ion (EA, CA, PROPD).
However,the other proposition, _propl, cannot be instantiated by plan chaining; it must beinstantiated by unification with a proposition from the existing dialogue context.
Forexample, if e -ac t ion  is identified as contributing to act ion-3  in the existing dialoguecontext, then _propl might be instantiated as PROPC.
On the other hand, if e -ac t ionis identified as contributing to ac t ion - i ,  then _propl might be instantiated as PROPA.Chaining might suggest hree possibilities (e-act ion could contribute to act ion-3,  orto ac t ion - I ,  or to neither of them), and the relative salience of the propositions in theexisting dialogue context is not sufficient o identify this relationship.As a concrete xample, consider again the dialogue segment in Figure 6 consistingof utterances (10)-(13) followed by one of utterances (14a)-(14c).
After utterance (13),15 These are actions on the active path of the dialogue model; the actions that are deepest on the activepath are closer to the current focus of attention and are therefore regarded as more salient.18Carberry and Lambert Modeling Negotiation Subdialoguesthe actions on the active path of the dialogue model include (among others):action-l: Inform(CA, EA, Teaches(Smith,CS360))action-2: Address-Unacceptance(EA,CA, Teaches(Smith,CS360), On-Sabbatical(Smith))action-3: Inform(CA, EA, ~On-Sabbatical(Smith))If EA utters (14a), (14b), or (14c), three inference paths can be constructed: one thatlinks up to action-I, one that links up to action-3, and one that does not link up toany action on the active path.
If CA utters (14a), then CA's action should be identi-fied as contributing to action-3 above and thus the proposition being doubted shouldbe recognized as ~On-Sabbatical(Smith), even though this proposition did not appearexplicitly in EA's utterance.
However, if EA utters (14b), then EA's action should beidentified as contributing only to action-1 and the proposition being doubted there-fore should be recognized as Teaches(Smith, CS360); in this case, we are rejecting theinference path that links up to action-3 even though action-3 is more salient at thispoint in the dialogue.
On the other hand, if EA utters (14c), then EA's action should berecognized as not contributing to any of the actions on the active path and interpretedas merely seeking information about Dr. Smith.
Since chaining and salience alone areinsufficient to identify the correct interpretation, we need some additional mechanism.We define an evidence-action (abbreviated e-action) to be an action that intro-duces a new parameter that cannot be directly instantiated by chaining from the ut-terance.
We contend that such actions require evidence for their recognition.
In ourmodel, the relationship between _prop1 (the proposition whose instantiation must beinferred from the existing dialogue context) and PROPD (a proposition instantiated bychaining from the current utterance) is modeled in the applicability conditions of arecipe for e-action.
For example, Express-Doubt, whose recipe was given in Figure 4,is an example of an e-action.
The parameter _proposition1 cannot be instantiatedfrom plan chaining from the surface utterance because _proposition1 does not ap-pear in the body of the Express-Doubt recipe; therefore, Express-Doubt is an e-actionbecause it contains a parameter (_proposition1) that must be instantiated by unifica-tion with a proposition extracted from the existing dialogue context.
The relationshipthat _proposition1 has to the proposition contained in the utterance that is expressingdoubt is modeled in the last applicability condition of the Express-Doubt recipe (seeFigure 4).
This applicability condition states that the agent of the Express-Doubt ac-tion believes that _propesition2 implies that _proposition1 does not hold.
As we'veshown earlier, plan chaining and plausibility are insufficient for recognizing an Express-Doubt discourse act; evidence is required.4.5.1 Types of Evidence.
Our recognition algorithm captures the kinds of evidenceidentified in Section 3: 1) evidence provided by world knowledge, contextual knowl-edge, and the surface form of the utterance indicating that the applicability conditionsfor an e-action are satisfied, and 2) linguistic evidence from clue words suggesting ageneric discourse action.Grosz and Sidner (1986) claim that when evidence is available from one source,less evidence should be required from others.
Thus, if there is evidence indicatingthat the applicability conditions for a discourse act hold, then less linguistic evidencesuggesting the discourse act should be required.
This is the case for interpreting (9)(repeated below) as an expression of doubt.
(7) EA: What is Dr. Smith teaching?
(8) CA: Dr. Smith is teaching Architecture.19Computational Linguistics Volume 25, Number 1(9) EA: Isn't Dr?
Brown teaching Architecture?Even though there is no linguistic clue word suggesting an Express-Doubt discourseact, there is enough evidence from the surface form of the utterance and from worldand contextual knowledge to correctly interpret (9) as an expression of doubt at theproposition conveyed by (8).Let us examine this evidence in more detail.
The applicability conditions of theExpress-Doubt discourse act (see Figure 4) specify that EA must have some belief ineach of the following:a?b.C.that CA believes that Dr. Smith is teaching Architecture;that Dr. Brown is teaching Architecture; andthat Dr. Brown teaching Architecture is an indication that Dr. Smith isnot teaching Architecture.Belief (c) models how the proposition in utterance (9) (that Dr. Brown is teaching Ar-chitecture) relates to the proposition in the existing dialogue context (that Dr. Smithis teaching Architecture).
Therefore, evidence that EA holds belief (c) (that Dr. Brownteaching Architecture is an indication that Dr. Smith is not teaching Architecture) isparticularly significant since it shows how the utterance relates to the preceding dis-course.The system has evidence "for all three applicability conditions.
The system's evi-dence that EA holds belief (a) is provided by beliefs derived from the goal of the Telldiscourse act.
In utterance (8), CA initiates a Tell discourse act as part of an Informdiscourse act; thus immediately after (8), both the Inform and the Tell are part of theexisting dialogue context.
If (9) is indeed an expression of doubt, then it contributes tothe Inform act by addressing the believability of the communicated proposition.
In thiscase, EA will have implicitly conveyed that EA understood CA's previous utterance,i.e., EA will have passed up the opportunity to contribute to the Tell discourse act thatis a child of the Inform act, and will have thereby implicitly conveyed that the Tell actwas successful.
This notion of implicit acceptance is discussed further in Section 4.6.Since the goal of CA's Tell act is that EA believe that CA believes that Dr. Smith teachesArchitecture, the hypothesis that the Tell act has completed successfully (and thereforethat its goal has been achieved) provides evidence that (a) is a belief held by EA.The surface form of (9) provides evidence that EA believes (b), since it conveysan uncertain but still strong belief that Dr. Brown is teaching Architecture.
Finally, ifthe system's model of a stereotypical user indicates that users typically believe thateach course has only one instructor, then this world knowledge provides evidencethat EA believes (c).
Thus, the system has evidence for all three of the applicabilityconditions.
In addition, contextual knowledge indicates that the single constraint onthe Express-Doubt discourse act is satisfied--namely, that proposition Pdoubt be salientat this point in the dialogue.
Thus, the system would recognize (9) as an expressionof doubt.However, "Isn't Dr. Smith an excellent eacher?"
would not be recognized as anexpression of doubt because the system would have no evidence that EA believes thatbeing an excellent teacher suggests that Dr. Smith is not teaching Architecture.
Thus,world knowledge helps the system to correctly differentiate between utterances thatare expressions of doubt and those that are not.On the other hand, if there is sufficient linguistic knowledge suggesting a par-ticular discourse action, then the applicability conditions hould be attributed to the20Carberry and Lambert Modeling Negotiation Subdialoguesspeaker as long as they are plausible.
So, if the clue word but is used, then a nonac-ceptance discourse action such as expressing doubt should be easier to recognize (i.e.,should require less evidence that the applicability conditions hold) than if the clueword is not present.
Thus, if EA said "But isn't Dr. Smith an excellent teacher?
", theneven though there is no world knowledge indicating that all of the applicability con-ditions hold, the linguistic clue word is sufficient evidence to interpret his utteranceas an expression of doubt.The concept of accommodation in conversation (Lewis 1979; Thomason 1990) (re-moving obstacles to the speaker's goals) suggests that a listener might recognize asurface negative question as an expression of doubt by accommodating a belief aboutsome incompatibility between the proposition conveyed by the surface negative ques-tion and a proposition that might be doubted.
But in the extreme case, this meansthat any surface negative question could be recognized as an expression of doubt.
Wecontend that there should be evidence for such recognition.
This is similar to Pollack'smodel of plan recognition (Pollack 1990) that can account for user misconceptions; in-stead of inferring a relationship between every query and the speaker's goal, Pollackrequires that the system apply only well-motivated rules that hypothesize principledvariations of the system's own beliefs and that the system treat as incoherent anyqueries that cannot be explained via these rules.
(Pollack's example of incoherence isthe query "I want to talk to Kathy, so I need to find out how to stand on my head.")
In ourmodel we look for evidence of incompatibility, and in our implemented system thisevidence takes the form of stereotypical befiefs about the domain.
While our imple-mentation does not include other means of deducing an incompatibility, they are notprecluded by our theory but are left for future work.
Moreover, it should be notedthat if the speaker intends for the hearer to recognize the expression of doubt from theincompatibility between the doubted proposition and the proposition that the speakeris contending is true, then the speaker must believe that the belief about he incompat-ibility is a mutual belief.
Our stereotypical befiefs fall into this category.
In other cases,the clue word but causes the listener to accommodate a belief about incompatibilitythat he might not otherwise have done and thereby recognize the expression of doubt.So, in the case of complex discourse acts such as expressing doubt, the systemshould require evidence for recognizing the discourse act and should prefer to rec-ognize discourse acts for which there is multiple evidence: both linguistic clue wordssuggesting the generic discourse act and evidence suggesting that the applicabilityconditions for a particular discourse act are satisfied.
However, the system should bewilling to accept just one kind of evidence when that is all that is available.4.6 Implicit AcceptanceIn a collaborative task-oriented ialogue, the participants are working together toconstruct a plan for accomplishing a task.
If the collaboration is to be successful, theparticipants must agree on the plan being constructed and the actions being taken toconstruct it.
Thus, since a communicated proposition is presumed to be relevant othis plan construction process, the dialogue participants are obligated to communi-cate as soon as possible any discrepancies in belief about such propositions (Walkerand Whittaker 1990; Chu-Carroll and Carberry 1995b) and to enter into a negotia-tion subdialogue in which they attempt o "square away" (Joshi 1982) their disparatebeliefs.In our earlier work (Carberry 1985, 1989), we claimed that a cooperative participantmust accept a response or pursue discourse goals directed toward being able to acceptthe response.
As we noted there, this acceptance need not be explicitly communicatedto the other participant; for example, failure to initiate a negotiation subdialogue con-21Computational Linguistics Volume 25, Number 1veys implicit acceptance of the proposition communicated by an Inform action.
Thisnotion of implicit acceptance is similar to an expanded form of Perrault's default rea-soning about the effects of an inform act (Perrault 1990).
Our model captures this byrecognizing implicit acceptance when an agent foregoes the opportunity to addressacceptance of an action and moves on to pursue other discourse actions.4.6.1 Acceptance Actions.
If a statement is intended to answer a question, the listenerin a collaborative dialogue must note when he believes that the statement does notsuffice as a complete answer.
However, doing so implies that the listener believes thestatement, since it is inefficient to address aproposition's completeness a  an answer ifone does not accept he proposition.
Similarly, if a listener does not believe a commu-nicated proposition, he must convey this disagreement as soon as possible (Walker andWhittaker 1990).
But by questioning the validity of a proposition, the listener conveysthat he believes that he understood the utterance.
As Clark and Schaefer (1989) note,by passing up the opportunity to ask for a repair, a listener conveys that he has un-derstood an utterance.
Thus we hypothesize that listeners convey their acceptance (orlack of acceptance) in a multistage acceptance phase: 1) understanding, 2) believability,3) completeness.
16Acceptance can be communicated xplicitly or implicitly.
We include actions thataddress acceptance in the body of six of our discourse recipes.
These recipes wereselected because they allow us to capture acceptance of a question (the recipes forAsk-Ref and Ask-IJ), acceptance of the answer to a question (the recipes for Answer-Ref and Answer-IJ), and acceptance of a statement ( he recipes for Inform and Tell).In this research, we have been primarily concerned with one aspect of acceptance:believing the proposition communicated by an Inform action.
For example, the actionsin the body of the Inform recipe (see Figure 3) are: 1) the speaker (_agent1) tells thelistener (_agent2) the proposition that the speaker wants the listener to believe; and2) the speaker and listener address believability by discussing whatever is necessary inorder for the listener and speaker to come to an agreement about this proposition.
17This second action, and the subactions executed as part of performing it, account forsubdialogues that address the believability of the proposition conveyed in the Informaction.
Other actions related to acceptance are captured in other discourse recipes.
Forexample, the Tell action has a body containing a Surface-Say-Prop action and an Address-Understanding action; the latter enables both participants to ensure that the utterancehas been understood.
Similarly, the Answer-Ref action contains an Inform action andan Address-Answer-Acceptability action that ensures that the Inform action is sufficientto answer the question.
Further esearch is needed to model the full range of actionsthat address acceptance and to recognize utterances resulting from them.The discourse tree reflects the order of acceptance actions.
As discussed above,lack of understanding should be addressed before believability.
This is reflected in thediscourse tree that results from a statement, such as the one in Figure 5, where the Tellaction (whose recipe contains an Address-Understanding action) is a descendant of theInform action (whose recipe contains an Address-Believability action); in addition, sincethe statement in Figure 5 is intended to answer a question, the Inform act is a descen-16 Questions must also be accepted and assimilated into a dialogue.
Our model has recently beenexpanded to address the acceptance of questions (Bartelt 1996), but we are concentrating on statementsin this paper.17 Since our system does not generate responses, we do not model what the speakers need to discuss;however, if a speaker expresses doubt at a proposition by contending that a second proposition is true,then the speaker is introducing this second proposition and its relationship to the first proposition asitems that need to be discussed.22Carberry and Lambert Modeling Negotiation Subdialogues(15)(16)EA: Who is teaching CS360 (a systems course)?CA: Dr. Smith is teaching CS360.(17)a.
(17)b.EA: Isn't Dr. Smith a theory person?EA: Who handles the CS360 lab?tFigure 8Dialogues conveying different implicit acceptance.dant of the Answer-Ref action (whose recipe contains an Address-Answer-Acceptabilityaction).
Since the Tell is the current focus of attention, it must be completed beforeother actions are pursued.
Thus, if the listener believes that the telling has not beensuccessful (i.e., the listener does not fully understand the utterance), then the listenerwill pursue discourse acts that contribute to its Address-Understanding subaction.
Oncethe Tell has been successfully completed, then attention reverts back to the Inform act.The Inform must be successfully completed before other higher-level acts are pursuedfurther.
Thus if the Inform has not been successful (i.e., the listener does not accept hecommunicated proposition), then the listener will pursue discourse acts that contributeto its Address-Believability subaction.We have concentrated primarily on recognizing the acceptance and nonaccep-tance of propositions communicated by Inform actions: i.e., modeling negotiation sub-dialogues in which participants do not automatically believe everything that theyare told.
Others, Allen and Schubert (1991), Clark and Schaefer (1989), Traum andHinkelman (1992), and Traum (1994) have investigated how understanding and lackof understanding are communicated and can be recognized.4.6.2 Modeling Implicit Acceptance.
Our system models implicit acceptance in col-laborative dialogue as passing up the opportunity to express lack of acceptance.
Forexample, consider the two dialogue variations hown in Figure 8.
Figure 5 depicts thediscourse tree constructed from utterances (15) and (16) in Figure 8, with the currentfocus of attention, the Tell action, marked with an asterisk.
In attempting to assimilate(17a) into this discourse tree, the system's first expectation is that (17a) will addressthe understanding of (16) if EA does not understand it (i.e., as part of the Tell ac-tion that is the current focus of attention in Figure 5).
The next expectation is that(17a) will relate to the Inform action in Figure 5, by addressing the believability of theproposition conveyed by (16).
The system finds that the best interpretation of (17a)is that of expressing doubt at the proposition that Dr. Smith is teaching CS360, thusconfirming the secondary expectation that (17a) is addressing the believability of theproposition conveyed by (16).
This recognition of (17a) as part of the Inform action inFigure 5 indicates that EA has implicitly indicated understanding, by passing up theopportunity to address understanding in the Tell action that appears at a lower levelin the discourse tree and by moving instead to a relevant higher-level action; (17a) isthus (implicitly) conveying that the Tell action has been successful.Thus, when an utterance contributes to an ancestor of an action Ai and all ofAi's applicability conditions, except hose negated by the goal, are still satisfied, thenAi is assumed to have completed successfully; if that were not true, the dialogueparticipants would have been required to address those actions.
'8 When an action18 By requiring that all applicability conditions, except those negated by the goal, still be satisfied inorderfor the action to be viewed as successful, we eliminate situations inwhich the agent of the Inform act23Computational Linguistics Volume 25, Number 1is recognized as successful, the system updates its model of the user's beliefs withthe effects and goals of the completed action.
For example, in determining whether(17a) in Figure 8 is expressing doubt at (16) (thereby implicitly indicating that (16) hasbeen understood and that the Tell action has therefore been successful), the systemtentatively hypothesizes that the effects and goals of the Tell action hold, resultingin the tentative belief that EA believes that CA believes that Dr. Smith is teachingCS360.
If the system determines that this Express-Doubt action is the most coherentinterpretation f (17a), it attributes the hypothesized beliefs to EA.Now consider a dialogue in which utterances (15) and (16) in Figure 8 are insteadfollowed by utterance (17b).
In this case, the system finds that the best interpretationof (17b) does not contribute to any of the actions in the existing discourse tree; instead(17b) is identified as initiating an entirely new Obtain-Info-Ref action at the discourselevel, resulting in a new discourse tree.
Since EA has gone on to pursue some otherdiscourse action unrelated to any of the acts that were part of the previous discoursetree, the system recognizes not only EA's understanding of (16) but also EA's implicitacceptance of the proposition conveyed by (16).
That is, because the system interpretsEA's utterance as foregoing the opportunity to initiate a negotiation subdialogue toaddress the acceptance of the proposition communicated by the Inform action, thesystem recognizes that the Inform action has been successful and that EA has implicitlyconveyed acceptance of the proposition that Dr. Smith is teaching CS360.4.7 The Recognition AlgorithmOur recognition algorithm, outlined in Figure 9, assimilates a new utterance into theexisting dialogue context and identifies discourse acts that the speaker is pursuing.It proceeds as follows: Start with the semantic representation f the utterance andextract from it two kinds of linguistic information: 1) clue words that might suggesta generic discourse act, and 2) beliefs that are conveyed by the surface form of theutterance.
In our implemented system, possible clue words are explicitly noted in thesemantic representation f the utterance, and beliefs conveyed by the surface form ofan utterance are extracted from the applicability conditions of the recipe for the surfacespeech act.
For example, the surface form of an utterance such as "Isn't Dr. Smith onsabbatical?"
conveys that the speaker has a strong but uncertain belief in the queriedproposition; this is captured in the applicability conditions of the recipe for a Surface-Neg-YN-Question discourse act (see the appendix).Next, use plan inference rules to hypothesize sequences of actions A1,Ai2 .
.
.
.
.
Ai~ i(inference paths) such that A1 is the surface action directly associated with the speak-er's utterance and Aidi is an action on the active path in the existing dialogue context.By requiring that an inference path link up with an action that is already part of theexisting dialogue context, we are capturing the expectation that the new utterancewill contribute to an action that has already been initiated.
This corresponds to afocusing heuristic that captures expectations for new utterances in an ongoing dialogue(Carberry 1990).
For any inference path, if Ai~ is not the focus of attention in theexisting dialogue context, hen Aid~ must be an ancestor of the action that is the focus ofattention; tentatively hypothesize that each of the actions on the active path betweenthe existing focus of attention (the focus of attention immediately prior to the newutterance) and Aidl have completed successfully and use this hypothesis n reasoninghas become convinced by the other participant that he proposition he was trying to convey isreallyfalse.
In such cases, the applicability condition believe(_agentl, _proposition) ofthe Inform will nolonger be true and thus the Inform act will not be viewed as completing successfully.
We have notaddressed situations in which the participants cannot resolve their disagreements andagree to disagree.24Carberry and Lambert Modeling Negotiation SubdialoguesA1 = surface action associated with the speaker's utteranceLE = clue words extracted from semantic representation f utteranceD = dialogue modelB -- l istener's beliefsA d = action at current focus of attention in D;;Construct inference paths that link up to active path of dialogue modelS*--{Pi = A1,Ai2 .
.
.
.
.
Aiei \] on-active-path(Ai~i, D)APi is an inference path constructed from A1 };;Eliminate inference paths with unsatisfied constraints or implausible applicability conditionsFor each Pi C S DoBeginBi~---BIf A~, ?
A iThen Bi ~ BiU {beliefs that A d and all actions on the active path between Ai and Aid ihave completed successfully}If (3Aj)(3Ck) Aj C Pi A is-constraint(Ck, Aj) A -~CkThen S ~ S -  PiElse If (3Aj)(3ACk) Aj E Pi A is-app-cond(ACk,Aj) A-~plausible(ACk,Bi)Then S ~ S - PiEnd;;Determine how much evidence is available for each e-actionSo ~ O, $1 ~ O, $2 ~ 0For each Pi E S DoIf (3Aj) Aj C Pi A e-action(Aj)Then BeginIf ling-evid(Aj,LE) A app-cond-evid(Aj,Bi)Then $2 ~ S2U{Pi}Else If ling-evid(Aj,LE) V app-cond-evid(Aj,Bi)Then $1 *-- S1U{Pi}EndElse So ~-- SoU{Pi} ;;So contains inference paths with no e-actions;;Select inference paths containing actions with the most evidenceIf $2 ~ 0 Then S ~- $2 ;;S contains inference paths with multiple evidenceElse If $1 # 0 Then S ~ $1 ;;S contains inference paths with evidenceElse S ~ So ;;S contains inference paths with no e-actions;;Select inference path containing the action closest o current focus of attentionIf S#0Then P ,-- Pi \] Pi E S A Pi = A1, A 6 .
.
.
.
, AidiA-~(3Pj) Pj E S APj = A1,Aj2 .
.
.
.
.
Aid jA closer-to-curr-discourse-focus(Aja j,Aiei,D)Else BeginB ~- BU {beliefs that all actions on active path have completed successfully}S ~ {Pi = A1,A6 .
.
.
.
.
Ain \] Pi is an inference path constructed from A1A no-elts-on-active-path(Pi,D)A (VAj)(VCk)(Aj ff Pi A is-constraint(Ck, Aj) ~ Ck)A (VAj)(VACk)(Aj E Pi A is-app-cond(ACk, Aj) --~ plausible(ACk, B))P ~-- Pi \[ Pi E S A -~(3Pj) Pj E S A links-closer-to-ps-dom-focus(Pj,Pi,D)End;; Assimilate utterance into dialogue modelAdd P = A1, Ap2 .
.
.
.
, Apk to DMark Ap2 as new focus of attention in DFigure 9Pseudocode outlining our recognition algorithm.25Computational Linguistics Volume 25, Number 1about he actions on the inference path.
19 If the applicability conditions for any of theactions on an inference path are implausible or if the constraints are not satisfied, rejectthe inference path.For actions that are e-actions, determine how much evidence is available for theaction.
Reject any inference paths containing an e-action for which there is neitherlinguistic evidence suggesting the generic discourse act (such as the clue word butsuggesting an Express-Doubt action) 2?
nor evidence from the surface form of the ut-terance, world knowledge, and contextual knowledge indicating that the applicabilityconditions for the particular discourse action are satisfied.
If there is an e-action forwhich both kinds of evidence xist (both linguistic evidence for the generic discourseact and evidence that the applicability conditions are satisfied), then consider onlyinference paths containing an e-action for which there is such multiple evidence andselect he inference path A1,Ai2,.
.
.
,Ai~ for which Ai~ i is closest o the focus of atten-tion in the existing dialogue context.
Otherwise, if there is an inference path contain-ing an e-action for which one kind of evidence xists, then select he inference pathA1,Ai2 .
.
.
.
.
Ai~i for which Ai~i s closest o the focus of attention i  the existing dialoguecontext.If a satisfactory inference path containing an e-action cannot be found, then con-sider inference paths that contain o e-actions.
21If there is more than one such inferencepath, select he one that links up to an action that is closest o the focus of attention onthe discourse level.
If there is no inference path linking up to an action on the existingdiscourse level, then select he inference path that links up to an action that is closestto the focus of attention on the problem-solving and domain levels.
(Our dialoguemodel actually contains three levels: domain, problem-solving, and discourse.
Thispaper is primarily concerned with recognizing actions on the discourse level; we willbriefly discuss the domain and problem-solving levels in Section 5.1.)
This latter casecorresponds to initiating a new discourse segment, and thus a new discourse tree isconstructed atthe discourse level.Our algorithm identifies abest interpretation f the speaker's utterance.
However,since the algorithm uses heuristics, its interpretation can be incorrect and miscommu-nication can result.
Our current system does not include mechanisms for detectingand recovering from such errors.
Clark and Schaeffer (1989) discuss econd, third, andfourth turn repairs in discourse, and McRoy and Hirst (1995) provide an excellentformal model of repair in dialogue.5.
Modeling Negotiation SubdialoguesThe preceding sections have provided the key mechanisms necessary for modelingnegotiation subdialogues.
Our recipes differentiate between the effects and the goalsof a discourse act.
Thus, instead of assuming that a communicated proposition willautomatically be accepted by the listener, the effect of our Inform action is only that19 The action at the focus of attention and some of its ancestor actions may have completed successfully,which becomes evident when the participants choose not to address them further.
For example, as aresult of providing an answer to a question, the active path may include the discourse actionsAnswer-Ref, Inform, and Tell with the Tell discourse act being the current focus of attention; if the otherparticipant then performs a discourse act that is a subaction of the Answer-Ref but not of the Inform,then he has accepted the proposition conveyed by the Inform and it has completed successfully.20 Our system currently maintains a list of clue words and discourse acts that each clue word mightsuggest.21 Due to length restrictions, we have omitted a part of the algorithm that deals with focusing heuristicsthat are not needed for the kinds of utterances addressed in this paper; an example of utterancesneeding the full algorithm is given in Lambert and Carberry (1991).26Carberry and Lambert Modeling Negotiation Subdialoguesthe listener believes that the speaker wants the listener to believe the communicatedproposition, while its goal is that the listener will actually adopt the proposition asone of his own beliefs.
In addition, the body of the Inform discourse recipe containsnot only an action capturing the telling of the proposition but also an action capturingthe participants' addressing the believability of the communicated proposition.
Ouralgorithm for recognizing discourse actions and assimilating them into the dialoguemodel can recognize when an agent is expressing doubt at a communicated propositionby contending that some other proposition is true.
Our ability to recognize implicitas well as explicit acceptance of a communicated proposition enables us to identifywhen an agent has adopted a communicated proposition as part of his beliefs.This section describes our implementation a d demonstrates our system's capabil-ity with two extended negotiation subdialogues that illustrate 1) the role of linguistic,contextual, and world knowledge in resolving expressions of doubt; 2) expressions ofdoubt at both immediately preceding and earlier utterances; 3) multiple expressionsof doubt at the same proposition; 4) negotiation subdialogues embedded within othernegotiation subdialogues; and 5) explicit and implicit acceptance.
The recipes for thediscourse acts used in these examples can be found in the appendix.5.1 ImplementationOur system for recognizing complex discourse acts and handling negotiation subdia-logues has been integrated into the tripartite dialogue model presented in Lambert andCarberry (1991).
This dialogue model contains three levels of tree structures, one foreach kind of action discussed in Section 4.2 (domain, problem-solving, and discourse)with links among the actions on different levels.
At the lowest level, the discourseactions are represented; these actions may contribute to the problem-solving actions atthe middle level which, in turn, may contribute to the domain actions at the highestlevel.
Figure 10 illustrates the tripartite dialogue model for a situation in which CAhas previously answered a question about the cost of registering for CS180, and thenEA asks "When does CS180 meet?"
Note that the discourse level in Figure 10 only re-flects the current query about when CS180 meets, since previous queries have alreadyachieved their discourse goals.
Since this paper is concerned almost exclusively withthe discourse level of the dialogue model, we will not discuss the overall tripartitemodel further, except o note that the construction of a new discourse tree requires thatthe system identify its relationship to existing or new actions at the problem-solvingand domain levels (Lambert and Carberry 1991).Our system has been implemented in Common Lisp on a Sun Sparcstation andtested in a university advisement domain.
Figure 11 lists some of the beliefs includedin the system's model of a stereotypical user.
In our current implementation, only theclue word but is recognized as linguistic evidence for an Express-Doubt discourse act.In future work, we will expand the clue words taken into account by our system.5.2 An Extended ExampleFigure 12 contains an extended negotiation dialogue (portions of this dialogue havebeen given earlier).
This dialogue illustrates a number of features that our system canhandle.
Utterances (18) and (19) establish the initial context in which CA has pro-22 ~ is usually implies rather than strict implication.
The semantics of this predicate is that there may be asmall  number  of cases where the antecedent is true and the consequent is not.
This is similar to adefault rule.
For example, the On-Campus  rule might  be viewed as Vy: on-campus(y) A faculty(y) A M~on-sabbatical(y) ~ ~on-sabbatical(y).
However, as with any default rule, there could be exceptions;for example, one might  be on sabbatical but have returned to campus to give a colloquium.27Computational Linguistics Volume 25, Number 1Domain  Leve l  .............. ,,, ,.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
................... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
, ,  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,: \[ T e-Course< A, CSlS0) I\[ Register(EA, CS180) \] I * I Learn-Material(EA, CSl80,_fac) ~<~- - - - -, .
,  ....................... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
| .............................................. ....... .
.......................IIProblem-Solving Level II Build-Plan(EA, CA, Take-Course(EA,CS180)) \]I Build-Plan(EA, CA, Leam-Material(EA, CSl80,_fae)) \[ .
.
.
.
.
.
.
.
.
.
.
.
.Instantiate-Vars(EA, CA, Attend-Class(EA, _place, _time), Learn-Matefial(EA, CS 180, _fac)) \]Instantiate-Single-Var(EA, CA, _time, Attend-Class(EA, _place, _time), Learn-Material(EA, CS180, _fac)) I iAIID i scourse  LevelI i I I Obtain-Info-Ref(EA, CA, _time, Meets(CS 180, _time)) \]I I\[ Ask-Ref(EA, CA, _time, Meets(CS 180, _time)) \]i I* I Ref-Request(EA, CA, _time, Meets(CS180, _time)) II II Surface-WH-Question(EA, CA, _time, Meets(CS 180, _time)) \]........................ .......................................................................... , .
.
.
, ............. ,........................................ +EA: When does CS180 meet?Key:- ->  Enable arcSubaction arc* Current focus of attentionFigure 10A simple tripartite dialogue model.vided information in response to a question from EA.
Utterances (20), (23), and (27)illustrate the use of world knowledge in resolving expressions of doubt.
Utterance(20) is an expression of doubt that initiates a negotiation subdialogue, indicating thatEA has not accepted the proposition communicated by CA.
Utterances (21) and (22)attempt o resolve this doubt by stating that Dr. Brown is not teaching Architectureand providing support for this claim.
Utterance (23) expresses doubt at the supportinginformation; utterances (24) and (25) attempt to resolve this doubt; and utterance (26)explicitly accepts the proposition communicated by (25).
Thus utterances (23)-(26)constitute a negotiation subdialogue mbedded within the negotiation subdialogueof utterances (20)-(27).
Although utterance (26) explicitly conveys acceptance of atleast the most salient communicated proposition (the proposition that Dr. Brown wasgiving a University colloquium), there are still several propositions that have not yetbeen completely accepted by EA and are thus open for rejection.
Utterance (27) againexpresses doubt at the proposition that Dr. Smith is teaching Architecture, and implic-itly conveys acceptance of the propositions that Dr. Brown is on sabbatical and that28Carberry and Lambert Modeling Negotiation SubdialoguesSabbatical rule: Teachers on sabbatical usually do not teach.
(Vx Vy course(y) A faculty(x) A on-sabbatical(x) ~ -~teaches(x, y) )On Campus rule: Teachers on campus usually are not on sabbatical.
(Vy faculty(y) A on-campus(y) ~ -~on-sabbatical(y) )One Course rule: Teachers usually teach only one course a semester.
(Vx Vy Vz # y faculty(x) A course(y) A course(z) A teaches(x, y) ~ -~teaches(x, z) )One Professor rule: Each course usually has only one instructor.
(Vx Vy Vz # y course(x) A faculty(y) A faculty(z) A teaches(y, x) ~ -~teaches(z, x) )Expertise rule: Teachers usually do not teach courses outside their area ofexpertise.
(Vx Vy Vz faculty(x) A course(y) A area(z) A specialty(x, z) A -~in-area(y, z)-~teaches(x, y) )Figure 11A few stereotypical beliefs.
22(18) EA:(19) CA:(20) EA:(21) CA:(22)(23) EA:(24) CA:(25)(26) EA:(27)Figure 12ExtendedWhat is Dr. Smith teaching?Dr.
Smith is teaching Architecture.Isn't Dr. Brown teaching Architecture?No, Dr. Brown is not teaching Architecture.Dr.
Brown is on sabbatical.But wasn't Dr. Brown on campus yesterday?Yes, Dr. Brown was on campus yesterday.Dr.
Brown was giving a University colloquium.OK.But isn't Dr. Smith a theory person?negotiation subdialogue.Dr.
Brown is not teaching Architecture.The rest of this section works through the details of how our system processesthese utterances, recognizes the discourse acts they are pursuing, and incrementallybuilds the discourse tree of the dialogue model.
In our examples, the system willswitch between playing the role of EA and the role of CA.
However, when processingan utterance, the system will have access only to the beliefs of the participant whoseidentity it has assumed (namely, the listener), along with the correct dialogue modelat the time the utterance is made.5.2.1 Utterance (18): Establishing the Initial Context.
The system first plays the roleof CA (the listener) and must understand EA's utterance of (18).
The semantic repre-sentation of (18) is:Surface-WH-question(EA, CA, _course, Teaches(Dr. Smith, _course))The Surface-WH-Question is a subaction in the body of a recipe for a Ref-Request dis-course act; the Ref-Request is a subaction in the recipe for an Ask-Ref discourse act; and29Computational Linguistics Volume 25, Number 1the Ask-Ref is a subaction in the recipe for an Obtain-Info-Ref discourse act.
Thereforethe following chain of actions is hypothesized:0btain-Info-Ref (EA, CA, _course, Teaches(Dr.Smith, _course))TAsk-Ref(EA, CA, _course, Teaches (Dr. Smith, _course))TRef-Request(EA, CA, _course, Teaches(Dr. Smith, _course))TSurface-WH-Question(EA, CA, _course, Teaches(Dr. Smith, _course))As each of the above actions is inferred, the system checks that its constraints aresatisfied and that its applicability conditions are plausible.
Since this is the only chainof actions suggested by plan inferencing on the discourse level, the system recognizesthese discourse actions; it then infers problem-solving actions from the discourse ac-tions and, eventually, domain actions from the problem-solving actions.
As actions arerecognized, the system updates its model of EA's beliefs, wants, and knowledge fromthe actions' applicability conditions.
Figure 13 shows the initial tripartite dialoguemodel that is produced.
Since this paper is primarily concerned with the recognitionof actions on the discourse level, the remainder of our figures will only display thediscourse level and will omit the problem-solving and domain level actions.5.2.2 Utterance (19): Answering the Question.
The system is now playing the role ofEA (listener) and must understand CA's utterance of (19).
The semantic representationof (19) is:Surface-Say-Prop(CA, EA, Teaches(Dr. Smith, Arch))Chaining suggests that the surface speech act might be part of a Tell action, whichmight be part of an Inform action since the surface speech act and the Tell act are partof the body of the Tell and Inform acts, respectively.
The applicability conditions for allof these actions are plausible.The system tries to extend the inference chain from the Inform action.
An Informcan be part of the recipes for several discourse actions, including Give-Background andAnswer-Ref.
However, these actions are e-actions and, with the exception of Answer-Ref, inference of these e-actions i  rejected.
For example, Give-Background is an e-actionbecause it relates the proposition in the current utterance to some other proposition,the proposition about which background is being given.
The recipe for Give-Backgroundcontains a constraint that there be a particular relationship between the proposition inthe Inform action in its body and some other proposition conveyed by CA.
Since CAhas made no previous utterances, there is no other proposition conveyed by CA andthus this constraint cannot be satisfied.
Consequently, Give-Background is rejected.
23Afull discussion of the Give-Background action and its recipe can be found in (Lambert1993).On the other hand, Answer-Kef(CA, EA, _term, _proposition) can be inferredfrom Inform(CA, EA, Teaches (Dr. Smith, Arch) ) and the system has evidence for itsrecognition.
Answer-Re/is an e-action since the parameters _term and _propositioncannot be instantiated from the Inform action that precedes it on the inference chain.23 Although CA can provide background information prior to conveying the proposition about which thebackground is being iven, the Give-Background action in these instances will be recognized inassimilating CA's second utterance (the utterance about which the background is being iven)(Lambert and Carberry 1991).30Carberry and Lambert Modeling Negotiation SubdialoguesD.omaio .Lmtet  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,i?
r qb.19m_-_Sqlv!q g.L_evel .
.
.
.
.?
- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i -  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
,: I Build-Plan(EA, CA, Take-Course(EA,_course))rII Instantiate-Vars(EA, CA, Learn-Material(EA, _course, Dr. Smith), Take-Course(EA, _course)) I* I lnstantiate-Single-Var(EA, C _course, Learn-Material(EA, course, Dr. Smith), Take-Coursel EA,_course)) I iiIID!s_egu.rs_e_ _Level.
.
.
.
.
.
.
.
.
.
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.II Obtain-Info-Ref(EA, CA, _course, Teaches(Dr. Smith, _course)) \]$I Ask-Ref(EA, CA, course, Teaches(Dr. Smith,_course)) \]e Ref-Request(EA, CA, _course, Teaches(Dr.Smith,_course))iSurface-WH-Question(EA,CA,_c'ourse,Teaches(Dr.Smith,_course)) \].
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
oKey: EA: What is Dr. Smith teaching?- -~  EnableAreSubactlon Arc* Current focus of attentionF igure  13Tripartite dialogue model for utterance (18).As discussed in Section 4.5.1, evidence for e-actions may take one of two forms: 1)evidence from world and contextual knowledge and the surface form of the utter-ance indicating that the applicability conditions for a particular e-action are satisfied,and 2) linguistic evidence from clue words suggesting a generic discourse action.
Inthis case, there are no clue words, so any evidence must be from world and con-textual knowledge or the surface form of the utterance.
Answer-Re/(CA, EA, _term,_proposition) can be a subaction in the body of 0btain- Info-Ref  (EA, CA, _term,_proposit ion); unifying with the Obtain-In/o-Re/ action that is part of the existingdiscourse tree causes the parameters _proposit ion and _term to be instantiated asTeaches (Dr .
Smith, _course) and _course, respectively, in both the Obtain-In/o-Re/and Answer-Re/actions.World and contextual knowledge provide evidence that the applicability condi-tions of Answer-Re/are satisfied with these instantiations.
The third applicability con-dition in the Answer-Re/recipe captures the required relationship between the newparameter _proposit ion and the parameter _propanswer that appears in the Informdiscourse act.
It indicates that CA must believe that _propanswer (where _propansweris instantiated from the Inform act as Teaches (Dr. Smith, Arch)) is an instance of thequeried proposition, _proposition, with the queried term _term instantiated.
Sincethe system (playing the role of EA in this case) believes that the participants have31Computational Linguistics Volume 25, Number 1equivalent knowledge about language and how terms can be instantiated 24 and sincethe system believes that the two propositions unify, the system has evidence that thethird applicability condition is satisfied.In addition, there is evidence that the other two applicability conditions are satis-fied.
With the above instantiations, these applicability conditions become:Applicability conditions for Answer-Ref :bel ieve~A,want(EA,knowref (EA,_course,be l ieve~A,Teaches(Dr .
Smith,_course),\[c:c\] ))), \[w:c\] )believe(CA, ~knowref(EA,_course,believe(CA,Teaches(Dr. Smith,_course),\[C:C\])), \[W:C\])If utterance 19 is in fact an Answer-Ref that contributes to the Obtain-Info-Ref that is partof the existing dialogue context, then CA has recognized and is responding to the Ask-Ref that is a child of the Obtain-Info-Ref in the existing dialogue context.
Consequently,in considering the Answer-Ref interpretation, the system can hypothesize that CA hasrecognized the Ask-Ref and can tentatively attribute to CA the belief that the Ask-Refsapplicability conditions were satisfied, as shown below.Beliefs attributed to CA (by virtue of hypothesis that CA has recognized the Ask-Ref):want(EA, knowref(EA, _course, believe(CA, Teaches(Dr. Smith, _course),\ [ c :c \ ] ) )~knowref(EA, _course, bel ieve(CA, Teaches(Dr. Smith, _course),  \[C:C\]))This is equivalent to tentatively hypothesizing that CA has recognized the intentionscommunicated by utterance (18) and inferred the discourse level of the dialogue modeldepicted in Figure 13.
Thus the system's model of CA's beliefs (resulting from CA'srecognition of the Ask-ReJ) provides evidence that the applicability conditions of theAnswer-Ref discourse act are satisfied.Since this inference chain is the only one containing an e-action for which thereis evidence, the system recognizes CA's utterance as providing Architecture as theanswer to EA's question about what Dr. Smith is teaching and thereby contributingto the Obtain-Info-Ref action initiated by EA.
The updated discourse tree is shown inFigure 14, with the new focus of attention marked with an asterisk.5.2.3 Utterance (20): Initial Expression of Doubt.
The system is again playing the roleof CA (listener) and must understand EA's utterance of (20).
The semantic represen-tation of (20) is:Surface-Neg-YN-Question(EA, CA, Teaches(Dr.Brown, Arch))The surface form of (20) suggests that EA thinks that Dr. Brown is teaching Archi-tecture, but is not certain of it.
This belief is captured in the applicability condition24 This does not mean that the instantiation will result in a true proposition, only that it is a legalinstantiation fthe term.
For example, CS180 is a legal instantiation ofthe _course term in theproposition Teaches(Jones _course) although Teaches(Jones, CS180) may be false.
We have notaddressed the problem of misconceptions about class membership.32Carberry and Lambert Modeling Negotiation SubdialoguesObtain-lnfo-Ref(EA, CA, _course, Teaches(Dr. Smith, course))\]\[ Ask-Ref(EA, CA.
_course, Teaches(Dr. Smith, _course)) I I Answer-Ref(CA.
E , _course, Teaches(Dr.Smith,_course)) \[tRef-Request(EA, CA, _course, Teaches(Dr.Smith,_course)) ISurface-WH-Question(EA, C , _course, Teaches(Dr.Smith,_course)) \[(18) EA: What is Dr. Smith teaching?Key:* Current focus of attentionFigure 14Discourse tree for first two utterances in Figure 12.Inform(CA, EA, Teaches(Dr.Smith,Arch)) I?
\[Te,,(CA, EA, Teac os OrSmith Arch)) \]\[ Surface-Say-Prop(CA, E Teaches(Dr. Smith, Arch)) \](19) CA: Dr. Smith is teaching Architecture.of the recipe for a Surface-Neg-YN-Question.
Since we assume a noise-free mediumand well-formed utterances, urface speech acts always execute successfully and arecorrectly recognized.
Thus, the beliefs captured in the applicability conditions of thesurface speech act are immediately entered into the system's model of EA's beliefs.The most salient interpretation of (20), that it is addressing the understanding of (19)and thus contributing to the Tell discourse act that is the current focus of attention inthe dialogue, is rejected.
25The system can construct an inference path suggesting that the utterance con-tributes to the Inform discourse act that is the parent of the Tell act in the existingdiscourse tree.
In particular, by chaining from subactions to parent actions (actionswhose recipes contain the subaction), the system constructs an inference path contain-ing the following chain of actions:Inform(CA, EA, _propositionl)TAddress-Believability(CA, EA, _propositionl)TAddress-Unacceptance(EA, CA, _propositionl, Teaches(Dr.Brown, Arch))TExpress-Doubt(EA, CA, _propositionl, Teaches(Dr.Brown, Arch))TConvey-Uncertain-Belief (EA, CA, Teaches(Dr.Brown, Arch))TSurface-Neg-YN-Question(EA, CA, Teaches (Dr.Brown, Arch))If the last action on this inference path is unified with the Inform act in the exisUngdiscourse tree, then _propositionl in the recipe for Address-Believability will be instan-tiated as Teaches (Dr. Smith, Arch), indicating that EA  uttered (20) in order to express25 In our implemented system, it is rejected because there is no recipe for the Address-Understanding actionthat is part of the body of the recipe for the Tell discourse act, and thus it is not possible to construct aninference path from the utterance to the Address-Understanding act.
In the future, our expanded systemwill include such recipes, and the interpretation will be rejected because of lack of evidence for thee-action on the inference path or because its constraints are not satisfied.33Computational Linguistics Volume 25, Number 1doubt at the proposition that Dr. Smith is teaching Architecture and thereby contributeto addressing the believability of that proposition.
This interpretation would indicatethat EA had passed up the opportunity to contribute to the Tell discourse act that isthe focus of attention in the existing discourse tree.
Thus when the system considersthis interpretation, it hypothesizes that the Tell act has been successful and that its goalhas been achieved, and it tentatively addsbelieve(EA, believe(CA, Teaches(Dr. Smith, Arch), \[C:C\]), \[C:C\])to the belief model.As we have seen previously, Express-Doubt is an e-action since it is the action onthe inference path at which the parameter _propo si t  ion 1 is first introduced.
Therefore,although the applicability conditions for each of the actions on the above inferencepath are plausible, we need evidence for the Express-Doubt act.
There is no linguisticclue suggesting that (20) is an Express-Doubt action.
The system then checks to see ifit has evidence that the applicability conditions for the Express-Doubt action hold.
Theapplicability conditions are:believe(EA, believe(CA, Teaches (Dr. Smith, Arch), \[S:C\]), \[S:C\])believe(EA, Teaches(Dr. Brown, Arch), \[W:S\])believe(EA, Teaches(Dr. Brown, Arch) --~ ~Teaches(Dr. Smith, Arch), \[S:C\])The system's belief model provides evidence for the first applicability condition, thatEA  believes that CA believes that Dr. Smith teaches Architecture, since it has beententatively updated to include the goal of the Tell discourse act, as noted above.
Thebelief model also provides evidence for the second applicability condition, since ithas been updated to include the beliefs captured in the applicability conditions of therecipe for the surface speech act.
The system's model of a stereotypical user containsthe beliefs given in Figure 11, including the belief that there is only one professor percourse.
This stereotypical belief provides evidence for the final applicability condition(that EA  believes that Dr. Brown teaching Architecture implies that Dr. Smith is notteaching Architecture).
Since users typically believe that only one teacher is used percourse, perhaps EA does also.
If EA believes that there is only one professor per courseand that Dr. Brown is teaching Architecture, then EA would believe that Dr. Smithwould not be teaching Architecture.
So the system has evidence for all three of theapplicability conditions in the Express-Doubt recipe.
In addition, the constraint of theExpress-Doubt action is satisfied since the proposition that Dr. Smith teaches Architec-ture is a parameter of an action on the active path and thus is salient.Since there is evidence from world and contextual knowledge and the surfaceform of the utterance that the applicability conditions hold for interpreting (20) as anexpression of doubt and since there is no evidence for any other e-action, the systeminfers that this is the correct interpretation a d stops.
Thus, (20) is interpreted as anExpress-Doubt action, as shown in Figure 15.5.2.4 Utterances (21)-(22): Attempted Resolution of Conflict.
The system is now play-ing the role of EA (listener) and must assimilate CA's utterances (21)-(22).
The semanticrepresentation f (21) is:Surface-Say-Prop(CA, EA, ~Teaches(Dr.Brown, Architecture))Plan chaining indicates that the Surface-Say-Prop may be part of Tell(CA, EA,~Teaches(Dr.Brown, Architecture)) ,  which might be part of Inform(CA,EA, -~Teaches(Dr. Brown, Architecture)), which might be part of Resolve-34Carberry and Lambert Modeling Negotiation Subdialogues/aoo momo4?
?ummo= o < .~o i ul ~4 <.
.omFigure 15Discourse tree form?Ua ~ -o ~ 121mm m ~u< <a, g<i  ~utterances (18)-(20).m o~ um < g ~"~ .~2oo Z~ 6 o ~m0 ",~,Conflict(CA, EA, _propositionl, _proposition2), which might in turn be partof Address-Unacceptance(EA, CA, _proposit ionl,  _proposition2).
If this is theAddress-Unacceptance action that is part of the existing discourse tree in Figure 15, thenthe Express-Doubt and Convey-Uncertain-Belief actions in Figure 15 have completed suc-cessfully.
Thus, in considering this interpretation, the system hypothesizes that these35Computational Linguistics Volume 25, Number 1actions have been successful and tentatively updates its belief model to reflect he ef-fects and goals of these actions.
In particular, the following two beliefs (among others)are tentatively added to the system's model of CA's beliefs:believe (CA, believe (EA, Teaches (Dr. Brown, Arch)-~Teaches (Dr. Smith, Arch),IS:C\]), \[s:c\])believe(CA, believe(EA,Teaches(Dr.Brown,Arch), \[W:S\]), \[S:C\])Resolve-Conflict (see the appendix) is an e-action since it introduces two new propo-sitions (the propositions about which there is conflict) that cannot be instantiated bychaining from the Inform action in its body, and the system must be able to deter-mine what conflict the utterance is trying to resolve.
If the Address-Unacceptance c-tion is unified with the Address-Unacceptance that is part of the existing discoursetree, then the conflicting propositions, _proposit ionl and _proposition2, are in-stantiated as Teaches (Dr. Smith, Arch) and Teaches (Dr.Brown, Arch), respectively,in both Address-Unacceptance and Resolve-Conflict.
The system has evidence for theResolve-Conflict action with these instantiations.
The constraints that _proposit ionland _proposition2 be salient and that _proposition2 and _proposition3 bethe opposite of one another are satisfied.
First, Teaches (Dr .
Smith, Arch) andTeaches(Dr.Brown, Arch) are the propositions instantiating _proposit ionl and_proposition2, and they are salient since they are part of an action on the active pathof the existing discourse tree.
Second, the proposition instantiating _proposition2 isthe opposite of the proposition conveyed by CA's current utterance.
Evidence for thefirst two applicability conditions, 1) that CA believes that EA beheves that Dr. Brown'steaching Architecture implies that Dr. Smith is not teaching Architecture and 2) thatCA believes that EA has an uncertain belief in the proposition that Dr. Brown teachesArchitecture, is provided by the system's tentatively updated model of CA's beliefs.Evidence for the final applicability condition, that CA believes that Dr. Smith is teach-ing Architecture, is also provided by the system's model of CA's beliefs.
When CA'sInform action in (19) was recognized, the system updated its model of CA's beliefs toinclude the beliefs contained in the applicability conditions for the Inform act; thus thebelief model indicates that CA believes that Dr. Smith is teaching Architecture.
Sincethe system has evidence for the e-action on the inference path (and since there are noother inference paths containing e-actions), the system recognizes this chain of actionsand interprets (21) as informing EA that Dr. Brown is not teaching Architecture aspart of attempting to resolve the conflict suggested by EA.
Thus the Resolve-Conflictaction is recognized as contributing to the Address-Unacceptance ction that was begunin (20).The semantic representation f (22) is:Surface-Say-Prop(CA, EA, on-sabbatical(Dr.Brown))The Surface-Say-Prop is part of Tell(CA, EA, on-sabbatical(Dr.Brown)), which ispart of Inform(CA, EA, on-sabbatical(Dr.Brown)).
Further chaining suggests thatthe Inform action could be part of several other actions.
We will discuss two of thesepossibilities, Address-Acceptance and Explain-Claim.
In the Address-Acceptance case, CAmight be uttering (22) to support he statement that she made in (21); in the Explain-Claim case, CA might be uttering (22) to explain why the supposedly conflicting propo-sitions are not really in conflict.Let us examine the Address-Acceptance case first.
Address-Acceptance(CA, EA,_propositionl) is part of a recipe for Address-Believability(CA, EA,_proposit ionl) ,  which in turn is part of a recipe for Inform(CA, EA, _proposit ionl) .36Carberry and Lambert Modeling Negotiation SubdialoguesIf this is CA's immediately preceding Inform act, then unifying with this Inform act willcause _propositionl to be instantiated as ~Teaches (Dr.Brown, Arch) in the Inform,Address-Believability, andAddress-Acceptance actions.
Address-Acceptance is an e-action,since it is the action on the inference path at which a new proposition is first in-troduced.
If _proposition3 in the recipe for Address-Acceptance (se the appendix) isinstantiated with Teaches (Dr. Brown, Arch), then the constraints are obviously sat-isfied.
(Note that _proposition2 in the recipe for Address-Acceptance is instantiatedwith on-sabbatical  (Dr. Brown) as a result of chaining from the surface speech act tothe Inform act in the body of the Address-Acceptance action.)
The system has evidencethat the applicability conditions are satisfied with these instantiafions.
Evidence forthe first applicability condition is provided by the system's model of a stereotypicaluser, which indicates that it is generally believed that professors on sabbatical do notteach.
Evidence for the second applicability condition is provided by the system'smodel of CA's beliefs, which was updated to contain the effect of utterance (20)'sConvey-Uncertain-Belief action--namely, that CA believes that EA has some belief thatDr.
Brown is teaching Architecture.
Thus there is evidence for recognizing CA's ut-terance as addressing acceptance of the proposition that Dr. Brown is not teachingArchitecture by offering support for it.Now let us examine the Explain-Claim case.
Explain-Claim(CA, EA,_propositionl, _proposition2) is part of Resolve-Conflict(CA, EA,_propositionl, _proposition2) and unifying with the Resolve-Conflict that is alreadypart of the existing discourse tree causes _proposition1 and _proposition2 in theResolve-Conflict and Explain-Claim actions to be instantiated with Teaches (Dr. Smith,Arch) and Teaches (Dr .
Brown , Arch), respectively.
In the recipe for Explain-Claim,_proposition3 has been instantiated with on-sabbatical  (Dr. Brown) by chaining fromthe surface speech act to the Explain-Claim action.
Explain-Claim is an e-action.
How-ever, the system lacks evidence for its second applicability condition, that CA believesthat Dr. Brown being on sabbatical implies that Dr. Brown teaching Architecture andDr.
Smith teaching Architecture are not in conflict with one another.
Thus this poten-tial interpretation is rejected.
Since the inference path containing the Address-Acceptancediscourse act is the only one whose e-action has evidence supporting its recognition,the system recognizes (22) as addressing the acceptance of the proposition conveyedby (21)--namely, that Dr. Brown is not teaching Architecture.
Thus, CA's response in(21) and (22) indicates that CA is trying to resolve EA's and CA's conflicting beliefs.The structure of the discourse tree after these utterances i  shown in Figure 16, abovethe numbers (18)-(22).
265.2.5 Utterances (23)-(26): Embedded Negotiation Subdialogue.
The system is nowplaying the role of CA (listener) and must assimilate EA's utterance of (23).
The se-mantic representation f (23) is:Surface-Neg-YN-Question(EA, CA, on-campus(Dr.Brown, Yesterday))Clueword(But)The Surface-Neg-YN-Question in utterance (23) is one way to Convey-Uncertain-Belief.The linguistic lue but suggests that EA is executing a nonacceptance discourse action;this nonacceptance action might be addressing (22), (21), or (19), since the propositionsconveyed by these utterances have not yet been accepted by EA and are thus openfor rejection.
Let us consider the proposition conveyed by (22), since it is the most26 For space reasons, only the action ames are shown.37Computational Linguistics Volume 25, Number 1z ~~1 !
: v~J ~ .
.
.
.mm ~ .
jF igure 16D iscourse  tree for d ia logue  in F igure 12.Jt..fJ~c~ Iuj ~--Qm i ::i= ix, .
- - - .VJo IX /- -  I~  " J?--1?-,138Carberry and Lambert Modeling Negotiation Subdialoguessalient open proposition at this point in the dialogue and thus the most expectedcandidate.
Plan chaining suggests that the Convey-Uncertain-Belief could be part of anExpress-Doubt action, which in turn could be part of an Address-Unacceptance ction,which could be part of an Address-Believability action, which could be part of the Informaction in (22).
As in utterance (20), there is evidence that the applicability conditions forthe e-action (the Express-Doubt action) hold: for example, world knowledge indicatesthat a typical user believes that professors who are on campus are not on sabbatical,providing evidence for the third applicability condition.
Thus, there is both linguisticevidence for a generic nonacceptance discourse act and evidence from world andcontextual knowledge and the surface form of the utterance that the applicabilityconditions and constraints are satisfied for the specific action of expressing doubt atthe proposition that Dr. Brown is on sabbatical.
Since no other e-action has both kindsof evidence, (23) is interpreted as expressing doubt at the proposition conveyed by(22).The system now reverts to playing the role of EA (listener) and must assimilatethe next two utterances in which CA resolves the doubt that EA has expressed in(23), by agreeing that Dr. Brown was on campus yesterday but explaining the purposeof his visit (one that is an exception to the rule that people on sabbatical are not oncampus).
Plan inferencing for utterance (24) is identical to that of utterance (21) andwill not be described further.From the Surface-Say-Prop in (25), plan inference rules suggest hat the Surface-Say-Prop is part of a Tell action that is part of an Inform action.
As was the case forutterance (22), the Inform action can be part of several different higher-level actions,including Address-Acceptance andExplain-Claim.
Since Address-Acceptance is a subac-tion in a recipe for Address-Believability, andAddress-Believability s a subaction in arecipe for Inform, CA might be trying to offer support for the Inform act of (24),Inform(CA, EA, on-campus(Dr.Brown, Yesterday)).
However, this time the appli-cability conditions for the Address-Acceptance action are implausible.
In particular, asa result of the effect of the Convey-Uncertain-Belief action in (23), the system's modelof CA's beliefs indicates that CA believes that EA has some belief in the proposi-tion that Dr. Brown was on campus yesterday.
The second applicability conditionof the recipe for addressing the acceptance of the proposition conveyed by (24),believe(CA, believe(EA, -~on-campus(Dr.Brown, Yesterday), \[W:S\]), \[W:C\]),conflicts with this belief--i.e., Address-Acceptance is r asonable to pursue only whenan agent has some reason to believe that the listener disbelieves the proposition inquestion.
Since the second applicability condition is implausible, the inference pathcontaining the Address-Acceptance action is rejected.However, the system does have evidence for interpreting (25) as an Explain-Claim.Explain-Claim(CA, EA, _propositionl, _proposition2) is part of the recipe forResolve-Conflict(CA, EA, _propositionl, _proposition2).
If this is the Resolve-Conyqict action that is closest o the focus of attention in the existing discourse tree,then unification will cause _propositionl and _proposition2 in Resolve-Conflict andExplain-Claim to be instantiated respectively with on-sabbatical(Dr.Brown) andon-campus(Dr. Brown, Yesterday).
In the recipe for Explain-Claim, _proposition3was instantiated with Give (Dr. Brown, University-Colloquiura) during chaining fromthe surface speech act.
The system has evidence for the e-action Explain-Claim becauseit has evidence that its applicability conditions hold--namely, that CA believes that EAbelieves that Dr. Brown's being on campus implies that he is not on sabbatical from theeffect of the Express-Doubt action; that CA believes that Dr. Brown's giving a Universitycolloquium implies that being on campus is not in conflict with being on sabbatical,from the model of stereotypical beliefs; and that CA believes that EA believes that39Computational Linguistics Volume 25, Number 1Dr.
Brown was on campus yesterday, from the effect of the Convey-Uncertain-Beliefdiscourse act accomplished by (23).
Since this is the only inference path containingan e-action for which the system has evidence, utterance (25) is interpreted as con-tributing to resolving the conflict suggested in (23) by explaining the claim that thepropositions do not really conflict in this instance.The system now reverts to playing the role of CA (listener) and must assimilateEA's utterances.
In (26), EA indicates explicit acceptance of the most salient Informaction, so the system is able to determine that EA has accepted CA's response in (25).Other inform actions remain open for rejection and must still be implicitly or explicitlyaccepted.
In this dialogue, the Inform actions in (22) and (21) are implicitly accepted inutterance (27).
Althougl~ utterance (27) might cause one to hypothesize that (26) wasindicating explicit acceptance of all of the propositions conveyed by utterances (21)-(25), it is not possible to decide with certainty from a simple "ok" exactly how manyInform actions EA is accepting.
Thus our system assumes that the speaker accepts aslittle as possible, which is the most salient Inform action.Utterances (23)-(26) illustrate our model's handling of negotiation subdialoguesembedded within other negotiation subdialogues.
The subtree contained within thedashed lines in Figure 16 shows the structure of this embedded negotiation subdia-logue.5.2.6 Utterance (27): Multiple Expressions of Doubt and Implicit Acceptance.
Thesystem is still playing the role of CA (listener).
The semantic representation of EA'snext utterance isSurface-Neg-YN-Question(EA, CA, Specialty(Dr. Smith, Theory))Clueword(But)The linguistic clue but in (27) again suggests nonacceptance.
Since (25) has been ex-plicitly accepted, the propositions open for rejection are those conveyed in (22), (21),and (19).
Once again, chaining from the surface speech act can produce a chain ofactions containing an Express-Doubt action and terminating with one of the Inform ac-tions that is on the active path of the existing discourse tree.
If the Inform action isInform(Ca, EA, Teaches (Dr. Smith, Arch)), then the Express-Doubt action will be in-stantiated as Expres s-Doubt (EA, CA, Teaches (Dr. Smith, Arch), Specialty (Dr. Smith,Theory) .
The system has evidence that this action's applicability conditions are sat-isfied.
The evidence for the first two applicability conditions is similar to the evidencefor interpreting utterance (20) as expressing doubt.
World knowledge provides evi-dence for the third applicability condition.
The system's model of stereotypical userbeliefs indicates that it is typically believed that faculty only teach courses in theirfield.
Other system knowledge states that Architecture and Theory are different fields.So in this case, the system's world knowledge provides evidence that Dr. Smith's beinga theory person is an indication to the user that Dr. Smith does not teach Architec-ture.
Thus the system has two kinds of evidence for interpreting (27) as expressingdoubt at the proposition conveyed by (19): linguistic evidence for a generic Express-Doubt discourse act and evidence that the applicability conditions are satisfied forthe particular discourse act of expressing doubt at the proposition that Dr. Smith isteaching Architecture.
Since the system does not have multiple evidence for any of theother interpretations, the system recognizes (27) as again expressing doubt about theproposition conveyed by (19).
Thus, the system is able to recognize and assimilate asecond expression of doubt at the proposition conveyed in (19), even after interveningdialogue.
The discourse tree for the entire dialogue is given in Figure 16.40Carberry and Lambert Modeling Negotiation Subdialogues(28)(29)(30)(31)(32)(33) EA:(34) CA:(35)(36) EA:Figure 17EA: When does CS510 meet?CA: CS510 meets on Monday night at 7PM.EA: But isn't Dr. Jones teaching CS510?CA: No, Dr. Jones is not teaching CS510.Dr.
Hart is teaching CS510.But isn't CS510 a graduate course?Yes, CS510 is a graduate course.Dr.
Hart teaches both graduate and undergraduate courses.What courses are prerequisites for CS510?A second negotiation subdialogue.Since EA's utterance reverts back to addressing the acceptance of the propositionconveyed by (19), EA has foregone the opportunity to challenge the claims made inutterances (22) and (21).
Since the befief model indicates that the applicability condi-tions of the Inform actions are still satisfied (except hose negated by achievement ofthe goal), the system infers that EA has implicitly accepted the statements in (22) and(21), that Dr. Brown is on sabbatical and that Dr. Brown is not teaching Architecture,and the system updates its model of EA's beliefs.5.3 A Second ExampleFigure 17 contains a second negotiation dialogue.
Due to space limitations, we willonly discuss two interesting features of the dialogue and its processing by our system.Utterance (30) illustrates the use of a linguistic lue word to suggest an expression ofdoubt.
In interpreting utterance (30), the system constructs an inference path contain-ing the action:Express-Doubt(EA, CA, Meets(CS510, MonYPM), Teaches(Dr. Jones, CS510))Although the system does not have evidence that all of the applicability conditionsfor this Express-Doubt action are satisfied, the linguistic lue but does provide evidencefor the generic Express-Doubt act.
Since this is the only inference path containing ane-action for which there is evidence, the system recognizes (30) as expressing doubtat the proposition that CS510 meets on Monday at 7PM by contending that Dr. Jonesis teaching CS510.
In this case, the system lacked evidence for the third applicabilitycondition in the recipe for Express-Doubt.
But having recognized that EA is expressingdoubt, it attributes to EA the beliefs captured in the applicability conditions.
In partic-ular, the system attributes to EA the belief that Dr. Jones teaching CS510 implies thatCS510 would not meet on Monday at 7PM, though it has no idea why EA believesthat this implication holds--perhaps EA befieves that Dr. Jones has to be home to takecare of his children at night.When utterance (33) occurs, there are three propositions that have not yet beenaccepted by EA, and the system considers the possibility that EA is performing oneof three express doubt actions, namelyExpress-Doubt (EA,Express-Doubt (EA,Expre s s-Doubt (EA,CA, Meets(CS510, MonTPM), Graduate-Course(CS510))CA, ~Teaches(Dr. Jones, CS510), Graduate-Course(CS510))CA, Teaehes(Dr.Hart, CS510), Graduate-Course(CS510))41Computational Linguistics Volume 25, Number 1In all three cases, the system lacks evidence that the third applicability condition inthe Express-Doubt recipe is satisfied.
However, the linguistic clue word but providesevidence for a generic Express-Doubt action.
Since the system has equivalent evidencefor all three of the Express-Doubt acts, contextual knowledge is used to choose amongthem.
Since the proposition Teaches (Dr. Hart, CS510) is closest o the existing focusof attention in the discourse tree, it is the most salient of the three propositions thatare open for rejection.
Utterance (33) is therefore interpreted as expressing doubt at theproposition that Dr. Hart is teaching CS510 by contending that it is a graduate-levelcourse.
Thus contextual knowledge arbitrates when equivalent evidence is availablefor several specific discourse acts.6.
Evaluat ion and Future WorkWe undertook an evaluation of our prototype system both to assess whether it derivedappropriate interpretations of utterances and to identify areas for further esearch.
Weobtained eight human volunteers, ix of whom are not engaged in NLP research andtwo of whom are involved in unrelated NLP projects.
The subjects were given a setof world knowledge stereotypically believed in the domain, such as that faculty onsabbatical do not normally teach.
The subjects were presented with a set of dialoguesand asked to analyze several utterances from each dialogue.
The selected utterancesdid not include simple questions initiating the dialogue or straightforward answersto questions, since it seemed likely that the subjects would agree with the system'sinterpretation a d thus the results would be biased in favor of the system.
The selectedutterances did include surface negative questions (both with and without a clue wordbut), statements interpreted by our system as support for a previous assertion or asexplanations about why a proposition was not in conflict with a previous claim, andexamples of implicit acceptance.For each utterance selected for analysis, the subjects were given a suggested inter-pretation, and asked whether the suggested interpretation was reasonable and whetherthey could identify a better interpretation.
27 For 15 of 20 utterances, the subjects unani-mously believed that the system's interpretation was best.
It should be noted there wasunanimous agreement that utterance (42) below should be interpreted as an expressionof doubt but that utterance (39) should not.Dialogue A(37)(38)(39)EA: Who is teaching architecture?CA: Dr. Smith is teaching architecture.EA: Isn't Dr. Smith an excellent teacher?Dialogue B(40) EA: Who is teaching architecture?
(41) CA: Dr. Smith is teaching architecture.
(42) EA: But isn't Dr. Smith an excellent teacher?There were two categories of utterances where the subjects disagreed.
In the caseof surface negative questions that did not express doubt, such as utterance (39) above,27 The subjects were not told that he suggested interpretation was the one produced by our system butonly that we were trying to determine how utterances in a discourse hould be interpreted.42Carberry and Lambert Modeling Negotiation Subdialoguesthe suggested interpretation given to the subjects was that the speaker was seekinginformation about whether the queried proposition was true.
When the subjects did notinterpret he utterance as an expression of doubt (see below), five of them contendedthat a better interpretation would be that EA was seeking verification of the queriedproposition.
Since our system already recognizes from the surface negative questionthat the speaker has a strong (but uncertain) belief in the queried proposition, it iseasy to extend our system so that it can explicitly identify a Seek-Veri~cation discourseact.The other category for which there was disagreement was surface negative ques-tions where a clue word was not present and the stereotypical domain knowledge didnot provide a conflict.
In two of five instances, some subjects used their own experi-ence to identify a mutual belief that might suggest a conflict, such as the belief thatsometimes certain faculty are not allowed to teach graduate level courses.
While thisknowledge cannot be captured as a default rule, it does represent a kind of sharedexperiential knowledge that would provide weak evidence for a potential conflict.However, it should be noted that our subjects were split on how these problematiccases should be interpreted, agreeing with the system's interpretation slightly morethan half the time.
There was also another such surface negative question where onesubject viewed the system's interpretation as reasonable but argued that an expressionof doubt would be a better interpretation.
In order to derive this interpretation, thesubject posited an attribute for the speaker that was neither evident from the dialoguenor stereotypically true.
(The other subjects agreed that the system's interpretation wasbest.)
These examples bear on the issue of accommodation mentioned in Section 4.5.1,since one could argue that the subjects who interpreted the utterances as expressionsof doubt were trying to accommodate an incompatibility.
This is particularly true inthe last instance where the subject found it necessary to resort o nonshared knowledgein making the interpretation.
However, it is unclear whether a speaker would expect alistener to recognize such utterances as expressions of doubt without additional clues.As noted below, our future research will consider other forms of evidence (gesturaland intonational) in order to resolve such ambiguous utterances.After they had finished analyzing the dialogues, we asked the subjects to constructthree dialogues containing an expression of doubt and to explain why the expressionof doubt should be interpreted as such.
While these dialogues provided no contradic-tions to our approach, they did provide a couple of interesting examples, such as thefollowing dialogue, that suggest areas for future work.
(43) EA: We have basil, parsley, and oregano, but we need marjoram.
(44) CA: Isn't marjoram the same as oregano?Clearly (44) is expressing doubt at the claim conveyed by (43), but it relies on sharedworld knowledge that if a list contains X items, the X items are presumed to bedifferent.
Our system does not currently include such knowledge.Our subjects commented that intonation and facial gesture might alter their in-terpretation of the utterances in the dialogues; we are beginning research that willtake these kinds of evidence into account (Carberry, Chu-Carroll, and Lambert 1996).In addition, we will be expanding the kinds of world knowledge incorporated intoour system, and will be considering both the strength of different pieces of evidenceand how several pieces of weak evidence affect interpretation.
We would also like toextend our use of linguistic clues to include a wide variety of clue words and phrasesand to recognize the functions that these words can play.
In addition, we are devel-oping a plan-based response generation component (Chu-Carroll and Carberry 1994).43Computational Linguistics Volume 25, Number 1Initial work on this component includes a subsystem that can identify what evidenceto present o a user when conflicts arise (Chu-Carroll and Carberry 1995b, 1998) andwhat information to request when the system cannot rationally decide whether toaccept a proposition conveyed by the user (Chu-Carroll and Carberry 1995a, 1998).We will also be investigating the scale-up of our system as we extend its cover-age.
Part of the motivation for the content of the current discourse recipes was theirfuture extension to other domains, such as tutoring.
For example, as discussed in Sec-tion 4.2.1, the formulation of our Ask-Ref recipe allows it to be used as a subaction of afuture Test-Knowledge discourse act since the recipe does not presume that the speakeris ignorant about he correct value of the requested term.
This should aid in extendingthe kinds of discourse acts that can be handled.
Although transporting our systemto another domain will require encoding new domain knowledge and new domainrecipes, the recipes for discourse and problem-solving acts are domain-independentand thus will remain unchanged.
Moreover, the knowledge captured in our recipes iscommunicative knowledge shared by dialogue participants; we believe that such com-municative knowledge (such as how to express doubt) is finite although the possibleintentions (such as the intention of expressing doubt at Dr. Smith teaching CS360) areinfinite.7.
Other Related Work7.1 Grosz and Sidner's Theory of Discourse ProcessingGrosz and Sidner (1986) postulated a theory of discourse structure that included lin-guistic, intentional, and attentional components, and they argued that the dominanceand satisfaction-precedes relationships between discourse segments must be identi-fied in order to determine discourse structure.
They also noted three kinds of infor-mation that contribute to determining the purposes of discourse segments and theirrelationship to one another: linguistic markers, utterance-level intentions, and generalknowledge about actions and objects.
Subsequently Lochbaum (1994) developed analgorithm based on Grosz and Sidner's SharedPlan model (Grosz and Sidner 1990)that recognizes discourse segment purposes and discourse structure.We contend that, in order to understand utterances and respond appropriately, itis necessary not only to determine the structure of the discourse but also to identifythe communicative acts that an agent intends to perform with an utterance.
2s Forexample, if a listener does not recognize when an utterance such as "Wasn't Dr. Smithon campus yesterday?"
is expressing doubt, then the listener's response might fail toaddress the reasons for this doubt.
Our research provides a computational lgorithmthat uses multiple knowledge sources to recognize complex discourse acts, includingexpressions of doubt, and to identify their relationship to one another.
This algorithmand our strategy for recognizing implicit acceptance enable us to model negotiationsubdialogues, something that previous ystems have been unable to handle.7.2 Argument Understanding SystemsSeveral researchers have built argument understanding systems, but none has ad-dressed participants coming to an agreement ormutual belief about a particular situa-tion, either because the researchers investigated monologues only (Cohen 1987; Cohenand Young 1991), or because they assumed that dialogue participants do not change28 In a dialogue, Grosz and Sidner's discourse gment purpose isintended tocapture the purpose of asegment consisting ofa series of utterances byboth participants, not the communicative intentionsunderlying each participant's di course actions.44Carberry and Lambert Modeling Negotiation Subdialoguestheir minds (Flowers, McGuire, and Birnbaum 1982; Quilici 1991).
Cohen (1987) de-veloped an argument understanding system that used clue words and an evidenceoracle to build a discourse structure for arguments based on which utterances servedas support for other utterances.
Cohen's model, however, handles only monologues, oresponses to arguments are not modeled in her system.
Birnbaum, Flowers, Dyer, andMcGuire (Flowers and Dyer 1984; McGuire, Birnbaum, and Flowers 1981; Birnbaum,Flowers, and McGuire 1980) developed a system that finds flaws in arguments anddetermines how to respond.
Quilici (1991) created a system in which agents respondto each other's arguments based on a justification pattern that will support he agent'sposition.
Both Quilici and Birnbaum et al, however, assume that all participants inan argument will retain their opinion throughout the course of the argument, andconcentrate mainly on how to find flaws in arguments and construct responses basedon those findings; they do not address actually winning arguments.
Reichman (1981)modeled informal debates by using her idea of context spaces and expectations to de-termine who should respond and what possible topics might be addressed.
However,she does not provide a detailed computational mechanism for recognizing the role ofeach utterance in a debate.7.3 Models of Collaborative BehaviorSeveral models of discourse have recently been built which view conversation as akind of collaborative behavior in which speakers try to make themselves understoodand listeners work with speakers to help speakers attain this goal.Clark and Schaefer (1989) contend that utterances must be "grounded," or un-derstood, by both parties, but they do not address conflicts in belief, only lack ofunderstanding.
Walker (1992) has found many occasions of redundancy in collabora-tive dialogues, and explains these by claiming that people repeat hemselves in orderto ensure that each utterance has been understood.
29 Clark and Wilkes-Gibbs (1990)propose a collaborative model of dialogue in which referring is viewed as a collabo-rative process and each conversation unit is viewed as a contribution, which consistsof 1) an utterance that performs a referring action, and 2) the utterances requiredto understand the referent described in the utterance.
Heeman (1991) implementedthis model in a plan-based collaborative model of dialogue that is able to plan andrecognize referring expressions and their corrections.Other collaborative models assume that two participants are working together toachieve a common goal (Cohen and Levesque 1990a, 1991a, 1991b; Lochbaum, Grosz,and Sidner 1990; Lochbaum 1991; Grosz and Sidner 1990; Searle 1990).
Searle (1990)proposes a model in which the two agents working together have a joint intention, a"we intention," instead of individual intentions.
Cohen and Levesque (1990a, 1990b,1990c, 1991a, 1991b) have developed a formal theory in which agents are jointly com-mitted to accomplishing a goal, so both parties have individual intentions to accom-plish the goal as part of their joint commitment.
Grosz, Lochbaum, and Sidner (Groszand Sidner 1990; Lochbaum, Grosz, and Sidner 1990; Lochbaum 1991) have specified asystem in which two agents are working to accomplish some common goal by build-ing a "shared plan" in which each agent holds certain beliefs and intentions.
Thesebeliefs and intentions indicate that the agents intend to perform some joint action, andthat they believe they can perform this action.
All of these models indicate the needfor modeling collaborative dialogue, but none suggests a system that can handle the29 Another eason for repetition, she claims, is for centering (Grosz, Joshi, and Weinstein 1995), but sheconcentrates on repetitions that give evidence of understanding.45Computational Linguistics Volume 25, Number 1kind of negotiation subdialogues that people often engage in when trying to negotiatetheir conflicts in belief, even when they are both working towards the same goal.8.
ConclusionWe have presented a plan-based model for handling cooperative negotiation subdia-logues.
Our system infers both the communicative actions that people pursue whenspeaking and the beliefs underlying these actions.
Beliefs, and the strength of thesebeliefs, are recognized from the surface form of utterances and from the explicit andimplicit acceptance of previous utterances.
Our algorithm for recognizing discourseactions combines linguistic, contextual, and world knowledge in a unified framework.By combining these different knowledge sources, we are able to recognize complexdiscourse acts such as expressing doubt, to identify the relationship of utterances toone another, and to model negotiation subdialogues.
Since negotiation is an integralpart of multiagent activity, our process model addresses an important aspect of coop-erative interaction and thus is a step toward an intelligent and robust natural anguageconsultation system.AcknowledgmentsThis work was supported by the National Science Foundation under Grant No.
IRI-9122026.
The Government has certain rights in this material.
We would like to thankRachel Sacher for her help in our corpus analysis and the anonymous reviewers fortheir helpful comments on the manuscript.AppendixDiscourse RecipeAction: Address-Acceptance(_agentl, _agent2, _proposition1){_agent1 tries to make _proposition1 believable to_agent2}Recipe-Type: DecompositionAppl Cond: believe(_agentl, _proposition2 --~ ~_proposition3, \[S:C\])believe(_agentl, believe(_agent2, _proposition3, \[W:S\]), \[W:C\])Constraints: opposite(_propositionl, _proposition3)Body:  Inform(_agentl, _agent2, _proposition2)Effects: believe(_agent2, believe(_agentl, _proposition2 ~ _proposition1,\[s:c\]), \[s:c\])Goa l :  enhance-believability(_agentl, _agent2, _proposition1)Discourse RecipeAction: Address-Believability(_agentl, _agent2, _proposition1){_agent1 and _agent2 address the believability of_proposition1}Recipe-Type: DecompositionAppl Cond: believe(_agentl, _proposition1, \[C:C\])believe(_agentl, believe(_agent2, _proposition1, \[CN:S\]), \[0:C\])Body:  #Address-Acceptance(_agentl, _agent2, _proposition1)#Address-Unacceptance(_agent2, _agent1, _proposition1, _proposition2)#Convey-Acceptance-Explicitly(_agent2, _agent1, _proposition1)Effects: believability-addressed(_agentl, _agent2, _proposition1)Goa l :  same-mutual-beliefs(_agentl, _agent2, _proposition1)46Carberry and Lambert Modeling Negotiation SubdialoguesDiscourse RecipeAction: Address-Unacceptance(_agentl, _agent2, _proposition1, _proposition2){By noting a conflicting _proposition2, _agent1 initiates negotiation of his unacceptanceof_proposition1}Recipe-Type:Appl Cond:Body:Effects:Goal:Decompositionbelieve(_agentl, in-conflict(_propositionl, _proposition2), \[W:C\])Express-Doubt(_agentl, _agent2, _proposition1, _proposition2)Resolve-Conflict(_agent2, _agent1, _proposition1, _proposition2)unacceptance-addressed(_agentl, _agent2, _proposition1)conflict-resolved(_propositionl, _proposition2)Discourse RecipeAction: Answer-Ref(_agentl, _agent2, _term, _proposition){_agent1 answers _agent 2's question about he referent of_term in _proposition}Recipe-Type: DecompositionAppl Cond: believe(_agentl, want(_agent2, knowref(_agent2, _term,believe(_agentl, _proposition, \[C:C\]))), \[W:C\])believe(_agentl, ~knowref(_agent2, _ erm, believe(_agentl,_proposition, \[C:C\])), \[W:C\])believe(_agentl, instantiates(_propanswer, _term, _proposition), \[C:C\])Constraints: salient(_proposition)Preconditions: question-accepted(_agentl, _agent2, _proposition)believe(_agentl, knowref(_agentl, _term, _proposition), \[C:C\])Body: Inform(_agentl, _agent2, _propanswer)#Address-Answer-Acceptability(_agentl, _agent2, _propanswer)Effects: believe(_agent2, answered-question(_agentl, _proposition), \[W:C\])Goal: knowref(_agent2, _term, believe(_agentl, _proposition, \[C:C\]))Discourse RecipeAction: Ask-Ref(_agentl, _agent2, _term, _proposition){_agent1 tries to get _agent2 to tell him the referent of the _term in _proposition}Recipe-Type: DecompositionAppl Cond: want(_agentl, knowref(_agentl, _term,believe(_agent2, _proposition, \[C:C\])))-~knowref(_agentl, _term, believe(_agent2, _proposition, \[C:C\]))Constraints: term-in(_term, _proposition)Body: Ref-Request(_agentl, _agent2, _term, _proposition)#Make-Question-Acceptable(_agentl, _agent2, _proposition)Effects: believe(_agent2, want(_agentl,Answer-Ref(_agent2, _agent1, _term, _proposition)), \[C:C\])Goal: want(_agent2, Answer-Ref(_agent2, _agent1, _term, _proposition))Discourse RecipeAction: Convey-Uncertain-Belief(_agentl, _agent2, _proposition){_agent1 conveys an uncertain belief in _proposition}Recipe-Type: SpecializationBody: Surface-Neg-YN-Question(_agentl, _agent2, _proposition)Surface-Tag-Question(_agentl, _agent2 _proposition)Effects: believe(_agent2, believe(_agentl, _proposition, \[W:S\]), \[S:C\])Goal: believe(_agent2, believe(_agentl, _proposition, \[W:S\]), \[S:C\])47Computational Linguistics Volume 25, Number 1Discourse RecipeAction: Explain-Claim(_agentl, _agent2, _proposition1, _proposition2){_agentl explains why _proposition1 and _proposition2 are not in conflict}Recipe-Type: DecompositionApp!
Cond: believe(_agentl, believe(_agent2, _proposition2 ~ ~_propositionl,\[s:c\]), \[W:Cl)believe(_agentl, _proposition3 ~ -~in-conflict(_propositionl,_proposition2), \[S:C\])believe(_agentl, believe(_agent2, _proposition2, \[W:C\]), \[S:C\])Constraints: salient(_propositionl)salient (_proposition2)Body: Inform(_agentl, _agent2, _proposition3)Effects: claim-explained(_agentl, _agent2, _proposition1)Goal: believe(_agent2, _proposition2 ~ ~_propositionl, \[CN:CN\] )Discourse RecipeAction: Express-Doubt(_agentl, _agent2, _proposition1, _proposition2){_agent1 expresses doubt o _agent2 about _proposition1 by contending that _proposition2is true}Recipe-Type:Appl Cond:Constraints:Body:Effects:Goal:Decompositionbelieve(_agentl, believe(_agent2, _proposition1, \[S:C\]), IS:C\])believe(_agentl, _proposition2, \[W:S\])believe(_agentl, _proposition2 ~ ~_propositionl, \[S:C\])salient( _proposition1 )Convey-Uncertain-Belief(_agentl, _agent2, _proposition2)believe(_agent2, believe(_agentl, _proposition1, \[SN:WN\]), \[S:C\])believe(_agent2, believe(_agentl, _proposition2 ~ ~_propositionl,\[S:Cl), \[s:c\])believe(_agent2, want(_agentl, Resolve-Conflict(_agent2, _agent1,_proposition1, _proposition2)), \[S:C\])want(_agent2, Resolve-Conflict(_agent2, _agent1, _proposition1,_proposition2))Discourse RecipeAction: Inform(_agentl, _agent2, _proposition){_agent1 informs _agent2 of_proposition}Recipe-Type: DecompositionAppl Cond: believe(_agentl, _proposition, \[C:C\])believe(regent1, believe(_agent2, _proposition, \[CN:S\]), \[0:C\])Body: Tell(_agentl, _agent2, _proposition)#Address-Believability(_agentl, _agent2, _proposition)Effects: believe(_agent2, want(_agentl, believe(_agent2, _proposition,\[c:c\])), \[c:c\])Goal: believe(_agent2, _proposition, \[C:C\])Discourse RecipeAction: Obtain-Info-Ref(_agentl, _agent2, _term, _proposition){_agent1 learns from _agent2 the referent of_term in _proposition)}Recipe-type: DecompositionAppl Cond: believe(_agentl, knowref(_agent2, _term, _proposition), \[W:C\])-~knowref(_agentl, _term, _proposition)48Carberry and Lambert Modeling Negotiation SubdialoguesConstraints:Body:Effects:Goal:want(_agentl, knowref(_agentl, _term, _proposition))term-in(_term, _proposition)Ask-Ref(_agentl, _agent2, _term, _proposition)Answer-Ref(_agent2, _agent1, _term, _proposition)information-sought(_agentl, _agent2, _proposition)knowref(_agentl, _term, _proposition)Discourse RecipeAction: Ref-Request(_agentl, _agent2, _term, _proposition){_agent1 requests the referent of_term in _proposition}Recipe-Type:Constraint:Body:Effects:Goal:Specializationterm-in(_term, proposition)Surface-WH-Question(_agentl, _agent2, _term, _proposition)believe(_agent2, requested(_agentl, _term, _proposition), \[C:C\])believe(_agent2, want(_agentl, know-ref(_agentl, _term,believe(_agent2, _proposition, \[C:C\]))), \[C:C\])Discourse RecipeAction: Resolve-Conflict(_agentl, _agent2, _proposition1, _proposition2){_agent1 resolves the conflict of_proposition1 and _proposition2}Recipe-Type: DecompositionAppl Cond: believe(_agentl, believe(_agent2, _proposition2 ~ -~_propositionl,\[S:Cl), \[w:c\])believe(_agentl, believe(_agent2, _proposition2, \[W:S\]), \[W:C\])believe(_agentl, _proposition1, \[C:C\])Constraints: equalorneg(_proposition2, _proposition3)salient(_propositionl)salient(_proposition2)Body: Inform(_agentl, _agent2, _proposition3)#Explain-Claim(_agentl, _agent2, _proposition1, _proposition2)Effects: conflict-addressed(_agentl, _agent2, _proposition1, _proposition2)Goal: believe(_agent2, i -conflict(_propositionl, _proposition2), \[CN:WN\])Discourse RecipeAction: Surface-Neg-YN-Question(_agentl, _agent2, _proposition){_agent1 makes asurface negative request about _proposition}Recipe-Type: PrimitiveAppl Cond: believe(_agentl, _proposition, \[S:S\])Effects: asked-about(_agentl, _agent2, _proposition)Goal: asked-about(_agentl, _agent2, _proposition)Discourse RecipeAction: Surface-Say-Prop(_agentl, _agent2, _proposition){_agent1 makes asurface utterance of_proposition to _agent2}Recipe-Type: PrimitiveAppl Cond: believe(_agentl, _proposition, \[C:C\])Effects: said(_agentl, _agent2, _proposition)Goal: said(_agentl, _agent2, _proposition)Discourse RecipeAction: Surface-WH-Question(_agentl, _agent2, _term, _proposition)49Computational Linguistics Volume 25, Number 1{_agent1 makes a surface request for the _term in _proposition}Recipe-Type: PrimitiveConstraints: term-in(_term, _proposition)Effects: asked-for(_agentl, _agent2, _term, _proposition)Goal: asked-for(_agentl, _agent2, _term, _proposition)Discourse RecipeAction: Tell(_agentl, _agent2, _proposition){_agent l tells _agent2 of_proposition}Recipe-Type:Appl Cond:Body:Effects:Goal:Decompositionbelieve(_agentl, _proposition, \[C:C\])Surface-Say-Prop(_agentl, _agent2, _proposition)#Address-Understanding(_agentl, _agent2, _proposition)told-about(_agentl, _agent2, _proposition)believe(_agent2, believe(_agentl, _proposition, \[C:C\]), \[C:C\])ReferencesAllen, James.
1979.
A Plan-Based Approach toSpeech Act Recognition.
Ph.D. thesis,University of Toronto, Toronto, Ontario,Canada.Allen, James and C. Raymond Perrault.1980.
Analyzing intention in utterances.Artificial Intelligence, 15:143-178.Allen, James and Lenhart Schubert.
1991.The trains project.
Technical Report 91-1,Department ofComputer Science,University of Rochester, Rochester, NY.Ballim, Afzal and Yorick Wilks.
1991.Beliefs, stereotypes, and dynamic agentmodeling.
User Modeling and User-AdaptedInteraction, 1(1):33-66.Bartelt, Margaret.
1996.
A computerprogram that recognizes rejectedquestions computationally.
In Proceedingsof NCUR-IO, pages 989-993.Birnbaum, Lawrence, Margot Flowers, andRod McGuire.
1980.
Towards an AI modelof argumentation.
In Proceedings ofthe FirstNational Conference on Artificial Intelligence,pages 306-309.Bonarini, Andrea, Ernesto Cappelletti, andAntonio Corrao.
1990.
Network-basedmanagement of subjective judgements: Aproposal accepting cyclic dependencies.Technical Report 90-067, Dipartimento diElettronica, Politecnico di Milano, Milano,Italy.Carberry, Sandra.
1985.
A pragmatics basedapproach to understanding intersententialellipsis.
In Proceedings ofthe 23rd AnnualMeeting, pages 188-197.
Association forComputational Linguistics.Carberry, Sandra.
1987.
Pragmatic modeling:Toward a robust natural languageinterface.
Computational Intelligence,3:117-136.Carberry, Sandra.
1988.
Modeling the user'splans and goals.
Computational Linguistics,14(3):23-37.Carberry, Sandra.
1989.
A Pragmatics-BasedApproach to Ellipsis Resolution.Computational Linguistics, 15(2):75-96.Carberry, Sandra.
1990.
Plan Recognition iNatural Language Dialogue.
ACL-MIT PressSeries on Natural Language Processing.MIT Press, Cambridge, MA.Carberry, Sandra, Jennifer Chu-Carroll, andLynn Lambert.
1996.
Modeling intention:Issues for spoken language dialoguesystems.
In Proceedings ofthe InternationalSymposium on Spoken Dialogue, pages13-24.Cawsey, Alison, Julia Galiiers, Steven Reece,and Karen Sparck Jones.
1992.Automating the librarian: A fundamentalapproach using belief revision.
TechnicalReport 243, University of CambridgeComputer Laboratory, Cambridge,England.Chu-Carroll, Jennifer and Sandra Carberry.1994.
A plan-based model for responsegeneration i collaborative task-orienteddialogues.
In Proceedings ofthe TwelfthNational Conference on Artificial Intelligence,pages 799-805.Chu-Carroll, Jennifer and Sandra Carberry.1995a.
Generating information-sharingsubdialogues in expert-user consultation.In Proceedings ofthe 14th International JointConference on Artificial Intelligence, pages1,243-1,250.Chu-Carroll, Jennifer and Sandra Carberry.1995b.
Response generation icollaborative negotiation.
In Proceedings ofthe 33rd Annual Meeting, pages 136-143.50Carberry and Lambert Modeling Negotiation SubdialoguesAssociation for ComputationalLinguistics.Chu-Carroll, Jennifer and Sandra Carberry.1998.
Collaborative response generationin planning dialogues.
ComputationalLinguistics, 24(3):355-400.Clark, Herbert and Edward Schaefer.
1989.Contributing to discourse.
CognitiveScience, pages 259-294.Clark, Herbert and Deanna Wilkes-Gibbs.1990.
Referring as a collaborative process.In Philip Cohen, Jerry Morgan, andMartha Pollack, editors, Intentions inCommunication.
MIT Press, Cambridge,MA, pages 463-493.Cohen, Paul R. 1985.
Heuristic Reasoningabout Uncertainty: An Artificial IntelligenceApproach.
Pitman Publishing Company.Cohen, Philip and Hector Levesque.
1990a.Intention is choice with commitment.Artificial Intelligence, 42:213-261.Cohen, Philip and Hector Levesque.
1990b.Persistence, intention, and commitment.In Philip Cohen, Jerry Morgan, andMartha Pollack, editors, Intentions inCommunication.
MIT Press, Cambridge,MA, pages 33-70.Cohen, Philip and Hector Levesque.
1990c.Rational interaction as the basis forcommunication.
In Philip Cohen, JerryMorgan, and Martha Pollack, editors,Intentions in Communication.
MIT Press,Cambridge, MA, pages 221-256.Cohen, Philip and Hector Levesque.
1991a.Confirmations and joint action.
InProceedings ofthe International JointConference on Artificial Intelligence, pages951-957.Cohen, Philip and Hector Levesque.
1991b.Teamwork.
Technical Report 504, SRIInternational, Menlo Park, California.Cohen, Philip and C. Raymond Perrault.1979.
Elements of a plan-based theory ofspeech acts.
Cognitive Science, 3:177-212.Cohen, Robin.
1987.
Analyzing the structureof argumentative discourse.
ComputationalLinguistics, 13(1-2):11-24.Cohen, Robin and Mark Anthony Young.1991.
Determining intended evidencerelations in natural anguage arguments.Computational Intelligence, 7:110-118.Columbia University Transcripts.
1985.Transcripts derived from audiotapeconversations made at ColumbiaUniversity, New York, NY.
Provided byKathleen McKeown.DeKleer, Johan.
1986.
An assumption-basedTMS.
Arti~cial Intelligence, 28:269-301.Driankov, Dimiter.
1988.
Towards aMany-Valued Logic of Quanti~ed Belief.Ph.D.
thesis, Linkoping University,Department of Computer andInformation Science, Linkoping, Sweden.Elzer, Stephanie.
1995.
The role of userpreferences and problem-solvingknowledge in plan recognition for expertconsultation systems.
In Proceedings oftheIJCAI Workshop on the Next Generation ofPlan Recognition Systems, pages 37-41.Flowers, Margot and Michael E. Dyer.
1984.Really arguing with your computer.
InProceedings ofthe National ComputerConference, pages 653-659.Flowers, Margot, Rod McGuire, andLawrence Birnbaum.
1982.
Adversaryarguments and the logic of personalattack.
In W. Lehnert and M. Ringle,editors, Strategies for Natural LanguageProcessing.
Lawrence Erlbaum Associates,Hillsdale, NJ, pages 275-294.Galliers, Julia Rose.
1991.
Belief revision anda theory of con~nunication.
TechnicalReport 193, University of Cambridge,Cambridge, England.Galliers, Julia Rose.
1992.
Autonomousbelief revision and communication.
InP.
Gardenfors, editor, Belief Revision,Cambridge tracts in theoretical computerscience.
Cambridge University Press,Cambridge, England.Grosz, Barbara, Aravind K. Joshi, and ScottWeinstein.
1995.
Centering: A frameworkfor modeling the local coherence ofdiscourse.
Computational Linguistics,21(2):203-225.Grosz, Barbara nd Candace Sidner.
1986.Attention, intentions, and the structure ofdiscourse.
Computational Linguistics,12(3):175-204.Grosz, Barbara nd Candace Sidner.
1990.Plans for discourse.
In Philip Cohen, JerryMorgan, and Martha Pollack, editors,Intentions in Communication.
MIT Press,Cambridge, MA, pages 417-444.Harry Gross Transcripts.
1982.
Transcriptsderived from tapes of the radio talk showHarry Gross: Speaking of your money.Provided by the Dept.
of ComputerScience at the University of Pennsylvania.Heeman, Peter.
1991.
A computationalmodel of collaboration on referringexpressions.
Master's thesis, University ofToronto, September.
Also TechnicalReport CSRI-251.Hinkelman, Elizabeth.
1989.
Two constraintson speech act ambiguity.
In Proceedings ofthe 27th Annual Meeting, pages 212-219.Association for ComputationalLinguistics.Hirschberg, Julia and Diane Litman.
1987.Now let's talk about now.
In Proceedingsof the 25th Annual Meeting, pages 163-171.51Computational Linguistics Volume 25, Number 1Association for ComputationalLinguistics.Joshi, Aravind K. 1982.
Mutual beliefs inquestion-answer systems.
In N. Smith,editor, Mutual Beliefs.
Academic Press, NY,pages 181-197.Kautz, Henry.
1990.
A circumscriptivetheory of plan recognition.
In PhilipCohen, Jerry Morgan, and Martha Pollack,editors, Intentions in Communication.
MITPress, Cambridge, MA, pages 105-133.Knott, Alistair and Robert Dale.
1994.
Usinglinguistic phenomena to motivate a set ofcoherence relations.
Discourse Processes,18(1):35-62.Knott, Alistair and Chris Mellish.
1996.
Afeature-based account of the relationssignalled by sentence and clauseconnectives.
Language and Speech,39(2-3):143-183.Lambert, Lynn.
1993.
Recognizing ComplexDiscourse Acts: A Tripartite Plan-Based Modelof Dialogue.
Ph.D. thesis, University ofDelaware, June.Lambert, Lynn and Sandra Carberry.
1991.A tripartite plan-based model of dialogue.In Proceedings ofthe 29th Annual Meeting,pages 47-54.
Association forComputational Linguistics.Lambert, Lynn and Sandra Carberry.
1992.Modeling negotiation subdialogues.
InProceedings ofthe 30th Annual Meeting,pages 193-200.
Association forComputational Linguistics.Lewis, D. 1979.
Scorekeeping in a languagegame.
Journal of Philosophical Logic,8:339-359.Litman, Diane and James Allen.
1987.
Aplan recognition model for subdialoguesin conversation.
Cognitive Science,11:163-200.Litman, Diane and Julia Hirschberg.
1990.Disambiguating cue phrases in text andspeech.
In Proceedings ofthe 13thInternational Conference on ComputationalLinguistics, pages 251-256.Lochbaum, Karen.
1991.
An algorithm forplan recognition in collaborativediscourse.
In Proceedings ofthe 29th AnnualMeeting, pages 33-38.
Association forComputational Linguistics.Lochbaum, Karen.
1994.
Using CollaborativePlans to Model the Intentional Structure ofDiscourse.
Ph.D. thesis, HarvardUniversity, Cambridge, MA.
TechnicalReport: TR-25-94.Lochbaum, Karen, Barbara Grosz, andCandace Sidner.
1990.
Models of plans tosupport communication: An initial report.In Proceedings ofthe Eighth NationalConference on Artificial Intelligence, pages485-490.Marcu, Daniel.
1997.
The rhetorical parsingof natural anguage text.
In Proceedings ofthe 35th Annual Meeting, pages 96-103.Association for ComputationalLinguistics.McGuire, Rod, Lawrence Birnbaum, andMargot Flowers.
1981.
Opportunisticprocessing in arguments.
In Proceedings ofthe 1981 International Joint Conference onArtificial Intelligence, pages 58-60.McKeown, Kathleen R. 1983.
Focusconstraints on language generation.
InProceedings ofthe Third National Conferenceon Artificial Intelligence, pages 582-587.McRoy, Susan and Graeme Hirst.
1995.
Therepair of speech act misunderstandingsby abductive inference.
ComputationalLinguistics, 21(4):435-478.Perrault, Raymond.
1990.
An application ofdefault logic to speech act theory.
InPhilip Cohen, Jerry Morgan, and MarthaPollack, editors, Intentions inCommunication.
MIT Press, Cambridge,MA, pages 161-185.Perrault, Raymond and James Allen.
1980.A plan-based analysis of indirect speechacts.
American Journal of ComputationalLinguistics, 6(3-4):167-182.Polanyi, Livia.
1986.
The linguistic discoursemodel: Towards a formal theory ofdiscourse structure.
Technical Report6409, Bolt Beranek and NewmanLaboratories Inc., Cambridge, MA.Pollack, Martha.
1990.
Plans as complexmental attitudes.
In Philip Cohen, JerryMorgan, and Martha Pollack, editors,Intentions in Communication.
MIT Press,Cambridge, MA, pages 77-104.Quilici, Alexander.
1991.
The CorrectionMachine: A Computer Model of Recognizingand Producing Belief Justifications inArgumentative Dialogs.
Ph.D. thesis,Department of Computer Science,University of California at Los Angeles,Los Angeles, CA.Ramshaw, Lance A.
1989.
A Metaplanmodel for problem-solving discourse.
InProceedings ofthe Fourth Conference oftheEuropean Chapter of the Association forComputational Linguistics, pages 35-42.Reichman, Rachel.
1978.
Conversationalcoherency.
Cognitive Science, 2:283-327.Reichman, Rachel.
1981.
Modeling informaldebates.
In Proceedings ofthe 1981International Joint Conference on ArtificialIntelligence, pages 19-24.Reichman, Rachel.
1985.
Getting Computers toTalk Like You and Me.
MIT Press,Cambridge, MA.52Carberry and Lambert Modeling Negotiation SubdialoguesReithinger, Norbert and Elisabeth Maier.1995.
Utilizing statistical dialogue actprocessing in verbmobil.
In Proceedings ofthe 33rd Annual Meeting, pages 116--121.Association for ComputationalLinguistics.RosG Carolyn Penstein, Barbara Di Eugenio,Lori Levin, and Carol Van Ess-Dykema.1995.
Discourse processing of dialogueswith multiple threads.
In Proceedings ofthe33rd Annual Meeting, pages 31-38.Association for ComputationalLinguistics.Schegloff, Emanuel and Harvey Sachs.
1973.Opening up closings.
Semiotica, 8:289-327.Schiffrin, Deborah.
1987.
Discourse Markers.Cambridge University Press, Cambridge,England.Searle, John.
1970.
Speech Acts: An Essay inthe Philosophy of Language.
CambridgeUniversity Press, London, England.Searle, John.
1990.
Collective Intentions andActions.
In Philip Cohen, Jerry Morgan,and Martha Pollack, editors, Intentions inCommunication.
MIT Press, Cambridge,MA, pages 401-416.SRI Transcripts.
1992.
Transcripts derivedfrom audiotape conversations made atSRI International, Menlo Park, CA.Prepared by Jacqueline Kowtko under thedirection of Patti Price.Thomason, Richmond.
1990.Accommodation, meaning, andimplicature: Interdisciplinary foundationsfor pragmatics.
In Philip Cohen, JerryMorgan, and Martha Pollack, editors,Intentions in Communication.
MIT Press,Cambridge, MA, pages 325-363.Traum, David.
1994.
A Computational Theoryof Grounding in Natural LanguageConversation.
Ph.D. thesis, University ofRochester.Traum, David and Elizabeth Hinkelman.1992.
Conversation acts in task-orientedspoken dialogue.
ComputationalIntelligence, 8(3):575-599.van Beek, Peter and Robin Cohen.
1986.Towards user specific explanations fromexpert systems.
In Proceedings ofthe SixthCanadian Conference on Arti~'cial Intelligence,pages 194-198.Walker, Marilyn.
1991.
Redundancy incollaborative dialogue.
In Working Notesfor the AAAI Fall Symposium: DiscourseStructure in Natural Language Understandingand Generation, pages 124-129.Walker, Marilyn.
1992.
Redundancy incollaborative dialogue.
In Proceedings oftheFifteenth International Conference onComputational Linguistics, pages 345-351.Walker, Marilyn.
1996.
Inferring acceptanceand rejection in dialog by default rules ofinference.
Language and Speech,39(2-3):265-304.Walker, Marilyn and Steve Whittaker.
1990.Mixed initiative in dialogue: Aninvestigation i to discourse segmentation.In Proceedings ofthe 28th Annual Meeting,pages 70-78.
Association forComputational Linguistics.Wilensky, Robert.
1981.
Meta-Planning:Representing and using knowledge aboutplanning in problem solving and naturallanguage understanding.
Cognitive Science,5:197-233.Young, Mark Anthony.
1987.
The designand implementation f an evidence oraclefor the understanding of arguments.Technical report, University of Waterloo,Waterloo, Ontario, Canada.53
