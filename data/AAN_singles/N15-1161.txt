Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1410?1415,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsUnediting: Detecting Disfluencies Without Careful TranscriptsVictoria Zayats, Mari Ostendorf and Hannaneh HajishirziElectrical Engineering DepartmentUniversity of WashingtonSeattle, WA, USA[vzayats, ostendor, hannaneh]@u.washington.eduAbstractSpeech transcripts often only capture seman-tic content, omitting disfluencies that can beuseful for analyzing social dynamics of a dis-cussion.
This work describes steps in build-ing a model that can recover a large fraction oflocations where disfluencies were present, bytransforming carefully annotated text to matchthe standard transcription style, introducing atwo-stage model for handling different typesof disfluencies, and applying semi-supervisedlearning.
Experiments show improvement indisfluency detection on Supreme Court oral ar-guments, nearly 23% improvement in F1.1 IntroductionMany hearings, lectures, news broadcasts and otherspoken proceedings are hand-transcribed and madeavailable online for easier searching and increasedaccessability.
For speed and cost reasons, standardtranscription services aim at representing seman-tic content only; thus, filled pauses (uh, um) andmany disfluencies (repetitions and self corrections)are omitted, though not all.
Careful transcripts rep-resent all the words (and word fragments spoken), asshown below with disfluent regions underlined.Careful: It is it is a we submitWhere there used to be um um um uh the decisionStandard: It is, it is, we submitWhere there used to be the decisionThese phenomena are quite common in spon-taneous speech, even in formal settings such asSupreme Court oral arguments and congressionalhearings (Zayats et al, 2014).While disfluencies may not be important for an-alyzing the topic of a discussion, the rate and typeof disfluencies provide an indication of other factorsof interest in spoken language analysis, includingcognitive load, emotion, and social cues (Shriberg,2001).
Further, predicting locations of disfluenciesin standard transcripts would help to improve timealignments of transcripts to the audio signal, and toprovide more useful text data for training languagemodels for speech recognition.
Since careful anno-tation of transcripts with this information is costly,this paper tackles the problem of recovering the dis-fluencies from clues in the standard orthographictranscripts, or ?unediting?
the transcripts.1Here, unediting is treated as detection of thereparandum of the disfluencies.
Following the struc-tural representation of (Shriberg, 1994), as in:[ we would + which we would ][ would + [ who + who ] wouldn?t ]the task is to detect the words in the brackets preced-ing the ?+?
which marks the self-interruption point.Of course, here, some of the words in those regionsmay not be in the transcript, so location is more im-portant than extent.
In addition, some cues used (i.e.filled pauses and word fragments) are not availablein standard transcripts.Three developments are combined to address theproblem of unediting with the constraint of lim-ited hand-annotated training data in the target do-main: oral arguments from the Supreme Court of theUnited States (SCOTUS) available from the OyezProject archive (oyez.org).
First, we identify mech-anisms for transforming the careful transcripts ofthe Switchboard corpus (Godfrey et al, 1992) to be1Thanks to Mark Liberman for the term ?unediting.
?1410more similar to the Oyez transcripts.
Second, we in-troduce a multi-stage model that accounts for differ-ences in the rates of repetitions and self-correctionsin standard vs. careful transcripts.
Lastly, we ap-ply semi-supervised learning to take advantage ofthe large amount of original Oyez transcripts.
Thesystem combining all these techniques, referred tohere as UNEDITOR, leads to an improvement in F1of nearly 23% compared to a baseline of trainingfrom the original disfluency-annotated Switchboardcorpus.2 Related workThis paper builds on prior work using conditionalrandom field (CRF) models (Liu et al, 2006;Georgila, 2009; Ostendorf and Hahn, 2013; Zayatset al, 2014).
More recent work has shown a benefitfrom Markov networks (Qian and Liu, 2013; Wanget al, 2014).
Since our work is on the transcriptionstyle mismatch, this work adopts the simpler CRFapproach, but can be easily extended to other classi-fication techniques.In this work, we use only text features.
Whileprosodic features have been shown to be useful(Shriberg, 1999; Kahn et al, 2005; Liu et al,2006; Wang et al, 2014), the fact that the Oyeztranscripts do not capture all the words means thatforced time alignments are unreliable and the as-sociated prosodic features are too noisy to be use-ful.
Other studies integrate disfluency detection withparsing, e.g.
(Charniak and Johnson, 2001; Johnsonand Charniak, 2004; Lease et al, 2006; Hale et al,2006; Miller, 2009; Miller et al, 2009; Zwarts etal., 2010; Rasooli and Tetreault, 2013; Honnibal andJohnson, 2014), but parsers trained on standard tree-bank data sets are not effective on the very long andcomplex sentences in SCOTUS; parser adaptation isleft for future work.There are a few studies that have investigated dis-fluency detection using cross-domain training data(Georgila et al, 2010; Ostendorf and Hahn, 2013;Zayats et al, 2014), and many more that have usedmulti-domain data for other language processingtasks.
What is different about the task addressedhere is that both the domain (topic and speakingstyle) and the transcription protocol differ betweenthe target and source domain.
There have been someattempts to transform written text to a more conver-sational style for training language models, e.g.
Bu-lyko et al (2007) inserted pause fillers and word rep-etitions, which led to reductions in perplexity thoughnot word error rate.
The work here differs in thatthe transformation is in the reverse direction (remov-ing fillers from conversational text) and punctuationcues are emphasized.3 Transforming training dataHere we describe methods for generating trainingdata for use with standard transcripts: i) transfer-ring labels from a small amount of carefully anno-tated data to corresponding standard transcripts, andii) transforming the existing Switchboard trainingset to make it more similar to the target domain.3.1 SCOTUS corporaThe Oyez Project at Chicago-Kent is a multime-dia archive containing audio and transcripts of theSupreme Court hearings since 1955.
While OYEZtranscripts are consistent with the audio in general,they are not accurate when it comes to disfluen-cies.
We notice that most simple disfluencies suchas repetitions have been omitted by OYEZ anno-tators, while more complex ones are often presentand annotators have used the ?...?
symbol at loca-tions of filled pauses or repetitions.
Having thoseexplicit cues indicating interruption points in disflu-encies makes it possible to consider recovering theuntranscribed disfluencies.For CAREFUL SCOTUS annotation, we use thedata provided by (Zayats et al, 2014), which in-cludes seven cases with carefully transcribed au-dio and hand-annotated disfluencies, with sepa-rately marked repetitions.
We develop ANNOTATEDOYEZ transcripts, by transferring disfluency labelsfor those seven cases from CAREFUL SCOTUS to thecorresponding files in OYEZ and dropping the dele-tion markers.
As a result, those transcripts are iden-tical to the original OYEZ transcripts, but in additioncontain disfluency annotation derived from CARE-FUL SCOTUS.In order to align the CAREFUL SCOTUS andORIGINAL OYEZ transcripts, we use a dynamic pro-gramming algorithm for sequence alignment withmatching scores as given in Table 1 and a deletion1411CAREFUL SCOTUS OYEZ Scoreexact match exact match 4?+?
?...?
3punctuation punctuation 2end of sentence ?...?
2word/punct ?...?
1word other word -1word punct -1Table 1: Matching scores used in dynamic programmingtranscript alignment.cost of 1.
Some examples of CAREFUL SCOTUS,OYEZ, ALIGNED OYEZ (with deletions marked) andANNOTATED OYEZ transcripts are shown below.The full corpus is available at https://ssli.ee.washington.edu/tial/data/oyez.CAREFUL SCOTUS: [ [S It is + it is ] a + ] we submitWhere there used to be um um um uh the decisionOYEZ: It is, it is, we submitWhere there used to be the decisionALIGNED OYEZ: [ [S It is, + it is ], + ] we submitWhere there used to be the decisionANNOTATED OYEZ: [ [S It is, + it is ], + ] we submitWhere there used to be the decision3.2 Switchboard transformationThe ANNOTATED OYEZ training set is a very smalldataset, and other work has shown that Switchboard(SWBD) is useful for cross-domain training for SCO-TUS (Zayats et al, 2014).
However, prior work hasbeen with careful transcripts.
SWBD transcripts donot include ?...?
symbols, and SWBD has many morecommas and other punctuation symbols.
In order tomake best use of the SWBD data, we transform it tobe more similar to the OYEZ transcripts in two steps.First, we add ?...?
after interruption points in SWBD.Second, we remove all punctuations except ?...?
inthe middle of the sentence in both of the corpora.4 Detecting disfluenciesIn this section we describe the UNEDITOR system,which is a two-stage CRF model trained on trans-formed training data and takes advantage of a largepool of unlabeled data with a self-training technique.Baseline: CRF We use a conditional random field(CRF) model that labels each word in a sentence,following a tagging approach with separate repeti-tion and non-repetition reparandum states, as in (Os-tendorf and Hahn, 2013).
The feature set includesidentity and pattern match features widely used indisfluency detection tasks, as well as distance-basedand disfluency language model features from (Zay-ats et al, 2014).4.1 Two-stage modelUsing the same features as in the baseline, we in-troduce a two-stage CRF model motivated by ourobservation that many repetitions are omitted fromthe standard transcriptions.
Thus, while 62% of dis-fluencies in CAREFUL SCOTUS are repetitions, only22% of all disfluencies in ANNOTATED OYEZ arerepetitions.
We find that training at two separatestages helps to overcome the difference in distribu-tions of two disfluency types between source and tar-get domains, and hence results in a better model foradaptation.
In the first stage, we train a model to de-tect repetitions by only considering repetition statesin the training data.
In the second stage, we train amodel to detect non-repetitions by removing all rep-etitions from the training data.
Similarly at test time,we use the first-stage model to detect repetitions,then remove all the detected repetitions, and applythe second-stage model to detect non-repetitions.
Inevaluation, we report the disfluencies detected inboth stages.4.2 Self-trainingA benefit of OYEZ transcripts is that there is a hugeamount of unlabeled data available, which makesit natural to use semi-supervised learning.
In thiswork, we use a simple self-training approach.
Firstwe apply a CRF model trained on the labeled datato the unlabeled data.
Then we augment the trainingdata with automatically labeled sentences that havebeen detected to contain a disfluency with a confi-dence score greater than 0.5, and retrain the modelwith the new augmented training set.5 Experiments and discussionWe evaluate the different sources/transformations oftraining data, self-training and the two-stage detec-tion model on ANNOTATED OYEZ transcripts fromthree cases (?30k words).1412Training set Prec Rec F1CAREFUL SCOTUS 66.1 16.7 26.7ANNOT OYEZ 86.7 20.4 33.0ORIG SWBD 62.2 29.1 39.7CAREFUL SCOTUS + ORIG SWBD 63.7 27.8 38.7ANNOT OYEZ + TRANSF SWBD 70.9 49.0 57.9Table 2: Disfluency detection of ANNOT OYEZ withdifferent training sets.5.1 Transforming training dataFirst, we assess the utility of different trainingsources and training data transformation using thebaseline model.
Note that the two SCOTUS sets arequite small (four cases, ?64k words) compared toSwitchboard (1.3M words).
Because of the differ-ence in punctuation style between the original Oyeztranscripts and the careful transcripts of both cor-pora, all sentence-internal punctuation is removed inthe CAREFUL SCOTUS and ORIG SWBD data.Table 2 reports results on training the CRF modelwith the different sources and their combinations.As expected, detection with in-domain training dataand transformed SWBD (ANNOT OYEZ+TRANSFSWBD) outperforms training on all other datasetcombinations.
Training on ANNOT OYEZ alone sig-nificantly outperforms detection (especially preci-sion) when only trained on the carefully annotateddata because of the matching transcription style.Training with ORIG SWBD outperforms trainingwith ANNOT OYEZ alone mainly due to the avail-ability of more training data in the SWBD dataset,consistent with results in (Ostendorf and Hahn,2013).
Surprisingly, the CAREFUL SCOTUS datadid not provide any benefit when added to the ORIGSWBD.Next, we study the impact of adding ?...?
sym-bols and removing punctuation for transforming theSWBD data.
Table 3 reports results for training theCRF model with the combination of ANNOT OYEZand SWBD with different transformation steps.
Weobserve that roughly 30% of the interruption pointsin CAREFUL SCOTUS are associated with the ?...
?symbol in the OYEZ transcripts; therefore, we add?...?
symbols after 1/3 of the interruption points inthe SWBD.
As expected, disfluency detection is im-proved by transforming SWBD with adding ?...?.
Thelargest gain is obtained when we also remove punc-Training set:ANNOT OYEZ+Prec Rec F1ORIGSWBD 67.8 29.3 40.9SWBD WITH ... 63.1 46.8 53.7TRANSF SWBD 70.9 49.0 57.9Table 3: The combination of ANNOT OYEZ and SWBDwith different SWBD transformation steps.Training set Prec Rec F1CAREFUL SCOTUS 57.8 21.2 31.0ANNOT OYEZ 81.7 27.3 41.0ORIG SWBD 59.0 31.7 41.2CAREFUL SCOTUS + ORIG SWBD 64.6 33.7 44.3ANNOT OYEZ + TRANSF SWBD 71.7 52.8 60.8Table 4: Self-training performance using different initialmodels.tuation (the row TRANSF SWBD).
All further exper-iments use this setting for training the models.5.2 Self-trainingHere we study the contribution of semi-supervisedlearning when applied on the baseline model (Ta-ble 5).
For self-training, we use 1,765 OYEZ tran-scripts dated 1990 - 2011 as our unlabeled data(?17.5M words), with a confidence threshold of 0.5for augmenting the training data, as described pre-viously.
We use each one of the baseline models inTable 2 as an initial model for the self-training forcomparison to the results in Table 4.
While addinga lot of in-domain data definitely helps, the qualityof the initial model plays a major role in the overallperformance.5.3 Two-stage modelFinally, we assess the impact of the two-stage modelwith and without self-training (Table 5).
For thetwo-stage semi-supervised model, self-training wasonly used for the second stage (non-repetition de-tection).
As expected, both two-stage and self-training models improve the baseline CRF model,and the combination performs the best.
The two-stage model helps to adapt the differences in dis-tribution of repetitions and non-repetitions betweenthe two domains by factoring the different prob-lems to improve the match of the more difficult non-repetition cases.
Overall, we obtain nearly 23% im-1413Model Prec Rec F11-stage 70.9 49.0 57.91-stage semi 71.7 52.8 60.82-stage 83.3 47.6 60.6UNEDITOR: 2-stage semi 76.8 52.2 62.2Table 5: Baseline, two-stages and self-training methods,comparison: baseline self-training method is trained on...., all the rest methods are trained on ANNOT OYEZ andTRANSF SWBD.
Our method, UNEDITOR combines self-training and two-stage models.provement using the full UNEDITOR system com-paring to the model trained on the ORIG SWBDdataset.6 ConclusionIn this paper we present a framework for disfluencydetection in non-careful transcripts.
Experimentsare based on the OYEZ archive of transcriptions ofSupreme Court oral arguments.
To address the prob-lem of lack of annotated data, we first transfer dis-fluency annotations from careful transcripts of a fewcases to the less precise OYEZ transcripts.
Next,we transform Switchboard transcripts to make themmore similar to the target domain.
In addition, weintroduce a two-stage model and self-training to fur-ther improve performance.Experiments show improvement in disfluency de-tection on Supreme Court oral arguments.
Start-ing from baselines of training from carefully anno-tated in-domain data (F1=26.1) or Switchboard data(F1=39.7), we achieve a substantial improvement to(F1=62.2) with our best case system UNEDITOR,which corresponds to an improvement of nearly23% over the stronger baseline.Possible extensions of this work include ex-ploring graph-based semi-supervised approaches(e.g., (Subramanya et al, 2010)) and combining thetext-based approach with flexible ASR forced align-ment allowing optional insertion of filled pauses andwords that are common as repetitions.
In addition,the availability of the automatically annotated dis-fluencies makes it possible to study the variation inrates for different cases and speakers over an ex-tended time period.AcknowledgmentsThis work was supported in part by DARPA grantFA8750-12-2-0347.
The authors thank the anony-mous reviewers for their valuable feedback to im-prove the clarity of paper.
The authors also thankSangyun Hahn for his contribution in the two-stagemodel.
The views and conclusions contained hereinare those of the authors and should not be inter-preted as necessarily representing the official poli-cies or endorsements, either expressed or implied,of DARPA or the U.S. Government.References[Bulyko et al2007] I. Bulyko, M. Ostendorf, M. Siu,T.
Ng, A. Stolcke, and O. Cetin.
2007.
Web resourcesfor language modeling in conversational speech recog-nition.
IEEE-TSLP, 5(1).
[Charniak and Johnson2001] E. Charniak and M. John-son.
2001.
Edit detection and parsing for transcribedspeech.
In Proc.
NAACL, pages 118?126.
[Georgila et al2010] K. Georgila, N. Wang, andJ.
Gratch.
2010.
Cross-domain speech disflu-ency detection.
In Proc.
Annual SIGdial Meeting onDiscourse and Dialogue.
[Georgila2009] K. Georgila.
2009.
Using integer lin-ear programming for detecting speech disfluencies.
InProc.
NAACL-HLT.
[Godfrey et al1992] J. J. Godfrey, E. C. Holliman, andJ.
McDaniel.
1992.
Switchboard: Telephone speechcorpus for research and development.
In Proc.
ACL,volume I, pages 517?520.
[Hale et al2006] John Hale, Izhak Shafran, Lisa Yung,Bonnie Dorr, Mary Harper, Anna Krasnyanskaya,Matthew Lease, Yang Liu, Brian Roark, MatthewSnover, and Robin Stewart.
2006.
PCFGs with syn-tactic and prosodic indicators of speech repairs.
InProc.
COLING-ACL.
[Honnibal and Johnson2014] Matthew Honnibal andMark Johnson.
2014.
Joint incremental disflu-ency detection and dependency parsing.
TACL,2(1):131?142.
[Johnson and Charniak2004] M. Johnson and E. Char-niak.
2004.
A tag-based noisy channel model ofspeech repairs.
In Proc.
ACL.
[Kahn et al2005] Jeremy G Kahn, Matthew Lease, Eu-gene Charniak, Mark Johnson, and Mari Ostendorf.2005.
Effective use of prosody in parsing conversa-tional speech.
In Proc.
EMNLP-HLT, pages 233?240.
[Lease et al2006] Matthew Lease, Mark Johnson, andEugene Charniak.
2006.
Recognizing disfluencies inconversational speech.
IEEE-TASLP, 14(5):169?177.1414[Liu et al2006] Y. Liu, E. Shriberg, A. Stolcke,D.
Hillard, M. Ostendorf, and M. Harper.
2006.Enriching speech recognition with automatic de-tection of sentence boundaries and disfluencies.IEEE-TASLP, 14:1526?1540.
[Miller et al2009] Tim Miller, Luan Nguyen, andWilliam Schuler.
2009.
Parsing speech repair withoutspecialized grammar symbols.
In Proc.
ACL-IJCNLP,pages 277?280.
[Miller2009] Tim Miller.
2009.
Improved syntactic mod-els for parsing speech with repairs.
In Proc.
NAACL-HLT.
[Ostendorf and Hahn2013] Mari Ostendorf and SangyunHahn.
2013.
A sequential repetition model for im-proved disfluency detection.
In Proc.
Interspeech.
[Qian and Liu2013] Xian Qian and Yang Liu.
2013.
Dis-uency detection using multi-step stacked learning.
InProc.
NAACL-HLT.
[Rasooli and Tetreault2013] Mohammad Sadegh Rasooliand Joel R Tetreault.
2013.
Joint parsing and disflu-ency detection in linear time.
In Proc.
EMNLP, pages124?129.
[Shriberg1994] E. Shriberg.
1994.
Preliminaries to a the-ory of speech disfluencies.
Ph.D. thesis, Department ofPsychology, University of California, Berkeley, CA.
[Shriberg1999] E. Shriberg.
1999.
Phonetic conse-quences of speech disfluency.
In Proc.
ICPhS, pages619?622.
[Shriberg2001] Elizabeth Shriberg.
2001.
To errrrishuman: ecology and acoustics of speech disfluen-cies.
Journal of the International Phonetic Associa-tion, 31(01):153?169.
[Subramanya et al2010] Amarnag Subramanya, SlavPetrov, and Fernando Pereira.
2010.
Efficient graph-based semi-supervised learning of structured taggingmodels.
In Proc.
EMNLP, pages 167?176.
ACL.
[Wang et al2014] Xuancong Wang, Hwee Tou Ng, andKhe Chai Sim.
2014.
A beam-search decoder for dis-fluency detection.
In Proc.
COLING.
[Zayats et al2014] Victoria Zayats, Mari Ostendorf, andHannaneh Hajishirzi.
2014.
Multidomain disfluencyand repair detection.
In Proc.
Interspeech.
[Zwarts et al2010] Simon Zwarts, Mark Johnson, andRobert Dale.
2010.
Detecting speech repairs incre-mentally using a noisy channel approach.
In Proc.COLING, pages 1371?1378.1415
