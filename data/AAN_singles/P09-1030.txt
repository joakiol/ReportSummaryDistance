Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 262?270,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPCompiling a Massive, Multilingual Dictionary via Probabilistic InferenceMausam Stephen Soderland Oren EtzioniDaniel S. Weld Michael Skinner* Jeff BilmesUniversity of Washington, Seattle *Google, Seattle{mausam,soderlan,etzioni,weld,bilmes}@cs.washington.edu mskinner@google.comAbstractCan we automatically compose a large setof Wiktionaries and translation dictionar-ies to yield a massive, multilingual dic-tionary whose coverage is substantiallygreater than that of any of its constituentdictionaries?The composition of multiple translationdictionaries leads to a transitive inferenceproblem: if word A translates to wordB which in turn translates to word C,what is the probability that C is a trans-lation of A?
The paper introduces anovel algorithm that solves this problemfor 10,000,000 words in more than 1,000languages.
The algorithm yields PANDIC-TIONARY, a novel multilingual dictionary.PANDICTIONARY contains more than fourtimes as many translations than in thelargest Wiktionary at precision 0.90 andover 200,000,000 pairwise translations inover 200,000 language pairs at precision0.8.1 Introduction and MotivationIn the era of globalization, inter-lingual com-munication is becoming increasingly important.Although nearly 7,000 languages are in use to-day (Gordon, 2005), most language resources aremono-lingual, or bi-lingual.1 This paper investi-gates whether Wiktionaries and other translationdictionaries available over the Web can be auto-matically composed to yield a massive, multilin-gual dictionary with superior coverage at compa-rable precision.We describe the automatic construction of amassive multilingual translation dictionary, called1The English Wiktionary, a lexical resource developed byvolunteers over the Internet is one notable exception that con-tains translations of English words in about 500 languages.Figure 1: A fragment of the translation graph for two sensesof the English word ?spring?.
Edges labeled ?1?
and ?3?
arefor spring in the sense of a season, and ?2?
and ?4?
are forthe flexible coil sense.
The graph shows translation entriesfrom an English dictionary merged with ones from a Frenchdictionary.PANDICTIONARY, that could serve as a resourcefor translation systems operating over a verybroad set of language pairs.
The most immedi-ate application of PANDICTIONARY is to lexicaltranslation?the translation of individual words orsimple phrases (e.g., ?sweet potato?).
Becauselexical translation does not require aligned cor-pora as input, it is feasible for a much broaderset of languages than statistical Machine Transla-tion (SMT).
Of course, lexical translation cannotreplace SMT, but it is useful for several applica-tions including translating search-engine queries,library classifications, meta-data tags,2 and recentapplications like cross-lingual image search (Et-zioni et al, 2007), and enhancing multi-lingualWikipedias (Adar et al, 2009).
Furthermore,lexical translation is a valuable component inknowledge-based Machine Translation systems,e.g., (Bond et al, 2005; Carbonell et al, 2006).PANDICTIONARY currently contains over 200million pairwise translations in over 200,000 lan-guage pairs at precision 0.8.
It is constructed frominformation harvested from 631 online dictionar-ies and Wiktionaries.
This necessitates match-2Meta-data tags appear in community Web sites such asflickr.com and del.icio.us.262ing word senses across multiple, independently-authored dictionaries.
Because of the millions oftranslations in the dictionaries, a feasible solutionto this sense matching problem has to be scalable;because sense matches are imperfect and uncer-tain, the solution has to be probabilistic.The core contribution of this paper is a princi-pled method for probabilistic sense matching to in-fer lexical translations between two languages thatdo not share a translation dictionary.
For exam-ple, our algorithm can conclude that Basque word?udaherri?
is a translation of Maori word ?koanga?in Figure 1.
Our contributions are as follows:1.
We describe the design and construction ofPANDICTIONARY?a novel lexical resourcethat spans over 200 million pairwise transla-tions in over 200,000 language pairs at 0.8precision, a four-fold increase when com-pared to the union of its input translation dic-tionaries.2.
We introduce SenseUniformPaths, a scal-able probabilistic method, based on graphsampling, for inferring lexical translations,which finds 3.5 times more inferred transla-tions at precison 0.9 than the previous bestmethod.3.
We experimentally contrast PANDIC-TIONARY with the English Wiktionary andshow that PANDICTIONARY is from 4.5 to24 times larger depending on the desiredprecision.The remainder of this paper is organized as fol-lows.
Section 2 describes our earlier work onsense matching (Etzioni et al, 2007).
Section 3describes how the PANDICTIONARY builds on andimproves on their approach.
Section 4 reports onour experimental results.
Section 5 considers re-lated work on lexical translation.
The paper con-cludes in Section 6 with directions for future work.2 Building a Translation GraphIn previous work (Etzioni et al, 2007) we intro-duced an approach to sense matching that is basedon translation graphs (see Figure 1 for an exam-ple).
Each vertex v ?
V in the graph is an or-dered pair (w, l) where w is a word in a languagel.
Undirected edges in the graph denote transla-tions between words: an edge e ?
E between (w1,l1) and (w2, l2) represents the belief that w1 andw2 share at least one word sense.Construction: The Web hosts a large num-ber of bilingual dictionaries in different languagesand several Wiktionaries.
Bilingual dictionariestranslate words from one language to another, of-ten without distinguishing the intended sense.
Forexample, an Indonesian-English dictionary gives?light?
as a translation of the Indonesian word ?en-teng?, but does not indicate whether this means il-lumination, light weight, light color, or the actionof lighting fire.The Wiktionaries (wiktionary.org) are sense-distinguished, multilingual dictionaries created byvolunteers collaborating over the Web.
A transla-tion graph is constructed by locating these dictio-naries, parsing them into a common XML format,and adding the nodes and edges to the graph.Figure 1 shows a fragment of a translationgraph, which was constructed from two sets oftranslations for the word ?spring?
from an EnglishWiktionary, and two corresponding entries froma French Wiktionary for ?printemps?
(spring sea-son) and ?ressort?
(flexible spring).
Translations ofthe season ?spring?
have edges labeled with senseID=1, the flexible coil sense has ID=2, translationsof ?printemps?
have ID=3, and so forth.3For clarity, we show only a few of the actualvertices and edges; e.g., the figure doesn?t showthe edge (ID=1) between ?udaherri?
and ?primav-era?.Inference: In our previous system we hada simple inference procedure over translationgraphs, called TRANSGRAPH, to find translationsbeyond those provided by any source dictionary.TRANSGRAPH searched for paths in the graph be-tween two vertices and estimated the probabilitythat the path maintains the same word sense alongall edges in the path, even when the edges comefrom different dictionaries.
For example, there areseveral paths between ?udaherri?
and ?koanga?
inFigure 1, but all shift from sense ID 1 to 3.
Theprobability that the two words are translations isequivalent to the probability that IDs 1 and 3 rep-resent the same sense.TRANSGRAPH used two formulae to estimatethese probabilities.
One formula estimates theprobability that two multi-lingual dictionary en-tries represent the same word sense, based on theproportion of overlapping translations for the twoentries.
For example, most of the translations of3Sense-distinguished multi-lingual entries give rise tocliques all of which share a common sense ID.263French ?printemps?
are also translations of the sea-son sense of ?spring?.
A second formula is basedon triangles in the graph (useful for bilingual dic-tionaries): a clique of 3 nodes with an edge be-tween each pair of nodes.
In such cases, there isa high probability that all 3 nodes share a wordsense.Critique: While TRANSGRAPH was the firstto present a scalable inference method for lexicaltranslation, it suffers from several drawbacks.
Itsformulae operate only on local information: pairsof senses that are adjacent in the graph or triangles.It does not incorporate evidence from longer pathswhen an explicit triangle is not present.
Moreover,the probabilities from different paths are com-bined conservatively (either taking the max overall paths, or using ?noisy or?
on paths that arecompletely disjoint, except end points), thus lead-ing to suboptimal precision/recall.In response to this critique, the next sectionpresents an inference algorithm, called SenseUni-formPaths (SP), with substantially improved recallat equivalent precision.3 Translation Inference AlgorithmsIn essence, inference over a translation graphamounts to transitive sense matching: if word Atranslates to word B, which translates in turn toword C, what is the probability that C is a trans-lation of A?
If B is polysemous then C may notshare a sense with A.
For example, in Figure 2(a)if A is the French word ?ressort?
(the flexible-coil sense of spring) and B is the English word?spring?, then Slovenian word ?vzmet?
may or maynot be a correct translation of ?ressort?
dependingon whether the edge (B,C) denotes the flexible-coil sense of spring, the season sense, or anothersense.
Indeed, given only the knowledge of thepath A ?
B ?
C we cannot claim anything withcertainty regarding A to C.However, if A, B, and C are on a circuit thatstarts at A, passes through B and C and re-turns to A, there is a high probability that allnodes on that circuit share a common word sense,given certain restrictions that we enumerate later.Where TRANSGRAPH used evidence from circuitsof length 3, we extend this to paths of arbitrarylengths.To see how this works, let us begin with the sim-plest circuit, a triangle of three nodes as shown inFigure 2(b).
We can be quite certain that ?vzmet?shares the sense of coil with both ?spring?
and?ressort?.
Our reasoning is as follows: eventhough both ?ressort?
and ?spring?
are polysemousthey share only one sense.
For a triangle to formwe have two choices ?
(1) either ?vzmet?
meansspring coil, or (2) ?vzmet?
means both the springseason and jurisdiction, but not spring coil.
Thelatter is possible but such a coincidence is very un-likely, which is why a triangle is strong evidencefor the three words to share a sense.As an example of longer paths, our inferencealgorithms can conclude that in Figure 2(c), both?molla?
and ?vzmet?
have the sense coil, eventhough no explicit triangle is present.
To showthis, let us define a translation circuit as follows:Definition 1 A translation circuit from v?1 withsense s?
is a cycle that starts and ends at v?1 withno repeated vertices (other than v?1 at end points).Moreover, the path includes an edge between v?1and another vertex v?2 that also has sense s?.All vertices on a translation circuit are mutualtranslations with high probability, as in Figure2(c).
The edge from ?spring?
indicates that ?vzmet?means either coil or season, while the edge from?ressort?
indicates that ?molla?
means either coilor jurisdiction.
The edge from ?vzmet?
to ?molla?indicates that they share a sense, which will hap-pen if all nodes share the sense season or if either?vzmet?
has the unlikely combination of coil andjurisdiction (or ?molla?
has coil and season).We also develop a mathematical model ofsense-assignment to words that lets us formallyprove these insights.
For more details on the the-ory please refer to our extended version.
This pa-per reports on our novel algorithm and experimen-tal results.These insights suggest a basic version of our al-gorithm: ?given two vertices, v?1 and v?2 , that sharea sense (say s?)
compute all translation circuitsfrom v?1 in the sense s?
; mark all vertices in thecircuits as translations of the sense s?
?.To implement this algorithm we need to decidewhether a vertex lies on a translation circuit, whichis trickier than it seems.
Notice that knowingthat v is connected independently to v?1 and v?2doesn?t imply that there exists a translation circuitthrough v, because both paths may go through acommon node, thus violating of the definition oftranslation circuit.
For example, in Figure 2(d) theCatalan word ?ploma?
has paths to both spring andressort, but there is no translation circuit through264springEnglishressortFrenchvzmetSlovenianspringEnglishressortFrenchvzmetSlovenianspringEnglishvzmetSlovenianressortFrenchmollaItalianspringEnglishressortFrenchplomaCatalanFederGerman???
?RussianspringEnglishressortFrenchfj?derSwedishpennaItalianFederGerman(a)                         (b)                                   (c)                                (d)                     (e)seasoncoiljurisdictioncoils* s*s* s*s*?
?
??
?feathercoil?
?Figure 2: Snippets of translation graphs illustrating various inference scenarios.
The nodes in question mark represent thenodes in focus for each illustration.
For all cases we are trying to infer translations of the flexible coil sense of spring.it.
Hence, it will not be considered a transla-tion.
This example also illustrates potential errorsavoided by our algorithm ?
here, German word?Feder?
mean feather and spring coil, but ?ploma?means feather and not the coil.An exhaustive search to find translation circuitswould be too slow, so we approximate the solutionby a random walk scheme.
We start the randomwalk from v?1 (or v?2) and choose random edgeswithout repeating any vertices in the current path.At each step we check if the current node has anedge to v?2 (or v?1).
If it does, then all the ver-tices in the current path form a translation circuitand, thus, are valid translations.
We repeat thisrandom walk many times and keep marking thenodes.
In our experiments for each inference taskwe performed a total of 2,000 random walks (NRin pseudo-code) of max circuit length 7.
We chosethese parameters based on a development set of 50inference tasks.Our first experiments with this basic algorithmresulted in a much higher recall than TRANS-GRAPH, albeit, at a significantly lower precision.A closer examination of the results revealed twosources of error ?
(1) errors in source dictionarydata, and (2) correlated sense shifts in translationcircuits.
Below we add two new features to ouralgorithm to deal with each of these error sources,respectively.3.1 Errors in Source DictionariesIn practice, source dictionaries contain mistakesand errors occur in processing the dictionaries tocreate the translation graph.
Thus, existence of asingle translation circuit is only limited evidencefor a vertex as a translation.
We wish to exploitthe insight that more translation circuits constitutestronger evidence.
However, the different circuitsmay share some edges, and thus the evidence can-not be simply the number of translation circuits.We model the errors in dictionaries by assigninga probability less than 1.0 to each edge4 (pe in the4In our experiments we used a flat value of 0.6, chosen bypseudo-code).
We assume that the probability ofan edge being erroneous is independent of the restof the graph.
Thus, a translation graph with pos-sible data errors converts into a distribution overaccurate translation graphs.Under this distribution, we can use the proba-bility of existence of a translation circuit through avertex as the probability that the vertex is a trans-lation.
This value captures our insights, since alarger number of translation circuits gives a higherprobability value.We sample different graph topologies from ourgiven distribution.
Some translation circuits willexist in some of the sampled graphs, but not inothers.
This, in turn, means that a given vertex vwill only be on a circuit for a fraction of the sam-pled graphs.
We take the proportion of samples inwhich v is on a circuit to be the probability that vis in the translation set.
We refer to this algorithmas Unpruned SenseUniformPaths (uSP).3.2 Avoiding Correlated Sense-shiftsThe second source of errors are circuits that in-clude a pair of nodes sharing the same polysemy,i.e., having the same pair of senses.
A circuitmight maintain sense s?
until it reaches a node thathas both s?
and a distinct si.
The next edge maylead to a node with si, but not s?, causing an ex-traction error.
The path later shifts back to senses?
at a second node that also has s?
and si.
An ex-ample for this is illustrated in Figure 2(e), whereboth the German and Swedish words mean featherand spring coil.
Here, Italian ?penna?
means onlythe feather and not the coil.Two nodes that share the same two senses oc-cur frequently in practice.
For example, manylanguages use the same word for ?heart?
(the or-gan) and center; similarly, it is common for lan-guages to use the same word for ?silver?, the metaland the color.
These correlations stem from com-parameter tuning on a development set of 50 inference tasks.In future we can use different values for different dictionariesbased on our confidence in their accuracy.265Figure 3: The set {B, C} has a shared ambiguity - eachnode has both sense 1 (from the lower clique) and sense 2(from the upper clique).
A circuit that contains two nodesfrom the same ambiguity set with an intervening node not inthat set is likely to create translation errors.mon metaphor and the shared evolutionary rootsof some languages.We are able to avoid circuits with this type ofcorrelated sense-shift by automatically identifyingambiguity sets, sets of nodes known to share mul-tiple senses.
For instance, in Figure 2(e) ?Feder?and ?fj?der?
form an ambiguity set (shown withindashed lines), as they both mean feather and coil.Definition 2 An ambiguity set A is a set of ver-tices that all share the same two senses.
I.e.,?s1, s2, with s1 6= s2 s.t.
?v ?
A, sense(v, s1)?sense(v, s2), where sense(v, s) denotes that v hassense s.To increase the precision of our algorithm weprune the circuits that contain two nodes in thesame ambiguity set and also have one or more in-tervening nodes that are not in the ambiguity set.There is a strong likelihood that the interveningnodes will represent a translation error.Ambiguity sets can be detected from the graphtopology as follows.
Each clique in the graph rep-resents a set of vertices that share a common wordsense.
When two cliques intersect in two or morevertices, the intersecting vertices share the wordsense of both cliques.
This may either mean thatboth cliques represent the same word sense, or thatthe intersecting vertices form an ambiguity set.
Alarge overlap between two cliques makes the for-mer case more likely; a small overlap makes itmore likely that we have found an ambiguity set.Figure 3 illustrates one such computation.All nodes of the clique V1, V2, A,B,C,D sharea word sense, and all nodes of the cliqueB,C,E, F,G,H also share a word sense.
The set{B,C} has nodes that have both senses, formingan ambiguity set.
We denote the set of ambiguitysets by A in the pseudo-code.Having identified these ambiguity sets, we mod-ify our random walk scheme by keeping track ofwhether we are entering or leaving an ambiguityset.
We prune away all paths that enter the sameambiguity set twice.
We name the resulting algo-rithm SenseUniformPaths (SP), summarized at ahigh level in Algorithm 1.Comparing Inference Algorithms Our evalua-tion demonstrated that SP outperforms uSP.
Boththese algorithms have significantly higher recallthan TRANSGRAPH algorithm.
The detailed re-sults are presented in Section 4.2.
We choose SPas our inference algorithm for all further research,in particular to create PANDICTIONARY.3.3 Compiling PanDictionaryOur goal is to automatically compile PANDIC-TIONARY, a sense-distinguished lexical transla-tion resource, where each entry is a distinct wordsense.
Associated with each word sense is a list oftranslations in multiple languages.We use Wiktionary senses as the base sensesfor PANDICTIONARY.
Recall that SP requires twonodes (v?1 and v?2) for inference.
We use the Wik-tionary source word as v?1 and automatically pickthe second word from the set of Wiktionary trans-lations of that sense by choosing a word that iswell connected, and, which does not appear inother senses of v?1 (i.e., is expected to share onlyone sense with v?1).We first run SenseUniformPaths to expand theapproximately 50,000 senses in the English Wik-tionary.
We further expand any senses from theother Wiktionaries that are not yet covered byPANDICTIONARY, and add these to PANDIC-TIONARY.
This results in the creation of theworld?s largest multilingual, sense-distinguishedtranslation resource, PANDICTIONARY.
It con-tains a little over 80,000 senses.
Its constructiontakes about three weeks on a 3.4 GHz processorwith a 2 GB memory.Algorithm 1 S.P.
(G, v?1, v?2,A)1: parameters NG: no.
of graph samples, NR: no.
of ran-dom walks, pe: prob.
of sampling an edge2: createNG versions ofG by sampling each edge indepen-dently with probability pe3: for all i = 1..NG do4: for all vertices v : rp[v][i] = 05: perform NR random walks starting at v?1 (or v?2 ) andpruning any walk that enters (or exits) an ambiguityset in A twice.
All walks that connect to v?2 (or v?1 )form a translation circuit.6: for all vertices v do7: if(v is on a translation circuit) rp[v][i] = 18: return?irp[v][i]NGas the prob.
that v is a translation2664 Empirical EvaluationIn our experiments we investigate three key ques-tions: (1) which of the three algorithms (TG, uSPand SP) is superior for translation inference (Sec-tion 4.2)?
(2) how does the coverage of PANDIC-TIONARY compare with the largest existing mul-tilingual dictionary, the English Wiktionary (Sec-tion 4.3)?
(3) what is the benefit of inference overthe mere aggregation of 631 dictionaries (Section4.4)?
Additionally, we evaluate the inference algo-rithm on two other dimensions ?
variation with thedegree of polysemy of source word, and variationwith original size of the seed translation set.4.1 Experimental MethodologyIdeally, we would like to evaluate a random sam-ple of the more than 1,000 languages representedin PANDICTIONARY.5 However, a high-qualityevaluation of translation between two languagesrequires a person who is fluent in both languages.Such people are hard to find and may not evenexist for many language pairs (e.g., Basque andMaori).
Thus, our evaluation was guided by ourability to recruit volunteer evaluators.
Since weare based in an English speaking country we wereable to recruit local volunteers who are fluent ina range of languages and language families, andwho are also bilingual in English.6The experiments in Sections 4.2 and 4.3 testwhether translations in a PANDICTIONARY haveaccurate word senses.
We provided our evalua-tors with a random sample of translations into theirnative language.
For each translation we showedthe English source word and gloss of the intendedsense.
For example, a Dutch evaluator was shownthe sense ?free (not imprisoned)?
together with theDutch word ?loslopende?.
The instructions wereto mark a word as correct if it could be used to ex-press the intended sense in a sentence in their na-tive language.
For experiments in Section 4.4 wetested precision of pairwise translations, by havinginformants in several pairs of languages discusswhether the words in their respective languagescan be used for the same sense.We use the tags of correct or incorrect to com-pute the precision: the percentage of correct trans-5The distribution of words in PANDICTIONARY is highlynon-uniform ranging from 182,988 words in English to 6,154words in Luxembourgish and 189 words in Tuvalu.6The languages used was based on the availability of na-tive speakers.
This varied between the different experiments,which were conducted at different times.Figure 4: The SenseUniformPaths algorithm (SP) morethan doubles the number of correct translations at precision0.95, compared to a baseline of translations that can be foundwithout inference.lations divided by correct plus incorrect transla-tions.
We then order the translations by probabil-ity and compute the precision at various probabil-ity thresholds.4.2 Comparing Inference AlgorithmsOur first evaluation compares our SenseUniform-Paths (SP) algorithm (before and after pruning)with TRANSGRAPH on both precision and num-ber of translations.To carry out this comparison, we randomly sam-pled 1,000 senses from English Wiktionary andran the three algorithms over them.
We evalu-ated the results on 7 languages ?
Chinese, Danish,German, Hindi, Japanese, Russian, and Turkish.Each informant tagged 60 random translations in-ferred by each algorithm, which resulted in 360-400 tags per algorithm7.
The precision over thesewas taken as a surrogate for the precision acrossall the senses.We compare the number of translations for eachalgorithm at comparable precisions.
The baselineis the set of translations (for these 1000 senses)found in the source dictionaries without inference,which has a precision 0.95 (as evaluated by ourinformants).8Our results are shown in Figure 4.
At this highprecision, SP more than doubles the number ofbaseline translations, finding 5 times as many in-ferred translations (in black) as TG.Indeed, both uSP and SP massively outperformTG.
SP is consistently better than uSP, since itperforms better for polysemous words, due to itspruning based on ambiguity sets.
We conclude7Some translations were marked as ?Don?t know?.8Our informants tended to underestimate precision, oftenmarking correct translations in minor senses of a word as in-correct.2670.50.60.70.80.910.0 4.0 8.0 12.0 16.0PrecisionTranslations in MillionsPanDictionaryEnglish WiktionaryFigure 5: Precision vs. coverage curve for PANDIC-TIONARY.
It quadruples the size of the English Wiktionary atprecision 0.90, is more than 8 times larger at precision 0.85and is almost 24 times the size at precision 0.7.that SP is the best inference algorithm and employit for PANDICTIONARY construction.4.3 Comparison with English WiktionaryWe now compare the coverage of PANDIC-TIONARY with the English Wiktionary at varyinglevels of precision.
The English Wiktionary is thelargest Wiktionary with a total of 403,413 transla-tions.
It is also more reliable than some other Wik-tionaries in making word sense distinctions.
In thisstudy we use only the subset of PANDICTIONARYthat was computed starting from the English Wik-tionary senses.
Thus, this subsection under-reportsPANDICTIONARY?s coverage.To evaluate a huge resource such as PANDIC-TIONARY we recruited native speakers of 14 lan-guages ?
Arabic, Bulgarian, Danish, Dutch, Ger-man, Hebrew, Hindi, Indonesian, Japanese, Ko-rean, Spanish, Turkish, Urdu, and Vietnamese.
Werandomly sampled 200 translations per language,which resulted in about 2,500 tags.
Figure 5shows the total number of translations in PANDIC-TIONARY in senses from the English Wiktionary.At precision 0.90, PANDICTIONARY has 1.8 mil-lion translations, 4.5 times as many as the EnglishWiktionary.We also compare the coverage of PANDIC-TIONARY with that of the English Wiktionary interms of languages covered.
Table 1 reports, foreach resource, the number of languages that havea minimum number of distinct words in the re-source.
PANDICTIONARY has 1.4 times as manylanguages with at least 1,000 translations at pre-cision 0.90 and more than twice at precision 0.7.These observations reaffirm our faith in the pan-lingual nature of the resource.PANDICTIONARY?s ability to expand the listsof translations provided by the EnglishWiktionaryis most pronounced for senses with a small num-0.750.80.850.90.951 2 3,4 >4PrecisionAvg precision 0.90Avg precision 0.85Polysemy of the English source word3-4Figure 6: Variation of precision with the degree of poly-semy of the source English word.
The precision decreases aspolysemy increases, still maintaining reasonably high values.ber of translations.
For example, at precision 0.90,senses that originally had 3 to 6 translations are in-creased 5.3 times in size.
The increase is 2.2 timeswhen the original sense size is greater than 20.For closer analysis we divided the Englishsource words (v?1) into different bins based on thenumber of senses that English Wiktionary lists forthem.
Figure 6 plots the variation of precision withthis degree of polysemy.
We find that translationquality decreases as degree of polysemy increases,but this decline is gradual, which suggests that SPalgorithm is able to hold its ground well in difficultinference tasks.4.4 Comparison with All Source DictionariesWe have shown that PANDICTIONARY has muchbroader coverage than the English Wiktionary, buthow much of this increase is due to the inferencealgorithm versus the mere aggregation of hundredsof translation dictionaries in PANDICTIONARY?Since most bilingual dictionaries are not sense-distinguished, we ignore the word senses andcount the number of distinct (word1, word2) trans-lation pairs.We evaluated the precision of word-word trans-lations by a collaborative tagging scheme, withtwo native speakers of different languages, whoare both bi-lingual in English.
For each sug-gested translation they discussed the varioussenses of words in their respective languagesand tag a translation correct if they found somesense that is shared by both words.
For thisstudy we tagged 7 language pairs: Hindi-Hebrew,# languages with distinct words?
1000 ?
100 ?
1English Wiktionary 49 107 505PanDictionary (0.90) 67 146 608PanDictionary (0.85) 75 175 794PanDictionary (0.70) 107 607 1066Table 1: PANDICTIONARY covers substantially more lan-guages than the English Wiktionary.268050100150200250EW 631D PD(0.9) PD(0.85) PD(0.8)Inferred transl.
Direct transl.Translations(in millions)Figure 7: The number of distinct word-word translationpairs from PANDICTIONARY is several times higher than thenumber of translation pairs in the English Wiktionary (EW)or in all 631 source dictionaries combined (631 D).
A major-ity of PANDICTIONARY translations are inferred by combin-ing entries from multiple dictionaries.Japanese-Russian, Chinese-Turkish, Japanese-German, Chinese-Russian, Bengali-German, andHindi-Turkish.Figure 7 compares the number of word-wordtranslation pairs in the English Wiktionary (EW),in all 631 source dictionaries (631 D), and in PAN-DICTIONARY at precisions 0.90, 0.85, and 0.80.PANDICTIONARY increases the number of word-word translations by 73% over the source dictio-nary translations at precision 0.90 and increases itby 2.7 times at precision 0.85.
PANDICTIONARYalso adds value by identifying the word sense ofthe translation, which is not given in most of thesource dictionaries.5 Related WorkBecause we are considering a relatively new prob-lem (automatically building a panlingual transla-tion resource) there is little work that is directly re-lated to our own.
The closest research is our previ-ous work on TRANSGRAPH algorithm (Etzioni etal., 2007).
Our current algorithm outperforms theprevious state of the art by 3.5 times at precision0.9 (see Figure 4).
Moreover, we compile this in adictionary format, thus considerably reducing theresponse time compared to TRANSGRAPH, whichperformed inference at query time.There has been considerable research on meth-ods to acquire translation lexicons from eitherMRDs (Neff and McCord, 1990; Helmreich etal., 1993; Copestake et al, 1994) or from par-allel text (Gale and Church, 1991; Fung, 1995;Melamed, 1997; Franz et al, 2001), but this hasgenerally been limited to a small number of lan-guages.
Manually engineered dictionaries such asEuroWordNet (Vossen, 1998) are also limited toa relatively small set of languages.
There is somerecent work on compiling dictionaries from mono-lingual corpora, which may scale to several lan-guage pairs in future (Haghighi et al, 2008).Little work has been done in combining mul-tiple dictionaries in a way that maintains wordsenses across dictionaries.
Gollins and Sanderson(2001) explored using triangulation between alter-nate pivot languages in cross-lingual informationretrieval.
Their triangulation essentially mixestogether circuits for all word senses, hence, is un-able to achieve high precision.Dyvik?s ?semantic mirrors?
uses translationpaths to tease apart distinct word senses frominputs that are not sense-distinguished (Dyvik,2004).
However, its expensive processing andreliance on parallel corpora would not scale tolarge numbers of languages.
Earlier (Knight andLuk, 1994) discovered senses of Spanish words bymatching several English translations to a Word-Net synset.
This approach applies only to specifickinds of bilingual dictionaries, and also requires ataxonomy of synsets in the target language.Random walks, graph sampling and MonteCarlo simulations are popular in literature, though,to our knowledge, none have applied these to ourspecific problems (Henzinger et al, 1999; Andrieuet al, 2003; Karger, 1999).6 ConclusionsWe have described the automatic construction ofa unique multilingual translation resource, calledPANDICTIONARY, by performing probabilistic in-ference over the translation graph.
Overall, theconstruction process consists of large scale in-formation extraction over the Web (parsing dic-tionaries), combining it into a single resource (atranslation graph), and then performing automatedreasoning over the graph (SenseUniformPaths) toyield a much more extensive and useful knowl-edge base.We have shown that PANDICTIONARY hasmore coverage than any other existing bilingualor multilingual dictionary.
Even at the high preci-sion of 0.90, PANDICTIONARY more than quadru-ples the size of the English Wiktionary, the largestavailable multilingual resource today.We plan to make PANDICTIONARY availableto the research community, and also to the Wik-tionary community in an effort to bolster their ef-forts.
PANDICTIONARY entries can suggest newtranslations for volunteers to add to Wiktionaryentries, particularly if combined with an intelli-gent editing tool (e.g., (Hoffmann et al, 2009)).269AcknowledgmentsThis research was supported by a gift from theUtilika Foundation to the Turing Center at Uni-versity of Washington.
We acknowledge PaulBeame, Nilesh Dalvi, Pedro Domingos, RohitKhandekar, Daniel Lowd, Parag, Jonathan Pool,Hoifung Poon, Vibhor Rastogi, Gyanit Singh forfruitful discussions and insightful comments onthe research.
We thank the language experts whodonated their time and language expertise to eval-uate our systems.
We also thank the anynomousreviewers of the previous drafts of this paper fortheir valuable suggestions in improving the evalu-ation and presentation.ReferencesE.
Adar, M. Skinner, and D. Weld.
2009.
Informationarbitrage in multi-lingual Wikipedia.
In Procs.
ofWeb Search and Data Mining(WSDM 2009).C.
Andrieu, N. De Freitas, A. Doucet, and M. Jor-dan.
2003.
An Introduction to MCMC for MachineLearning.
Machine Learning, 50:5?43.F.
Bond, S. Oepen, M. Siegel, A. Copestake, andD D. Flickinger.
2005.
Open source machine trans-lation with DELPH-IN.
In Open-Source MachineTranslation Workshop at MT Summit X.J.
Carbonell, S. Klein, D. Miller, M. Steinbaum,T.
Grassiany, and J. Frey.
2006.
Context-based ma-chine translation.
In AMTA.A.
Copestake, T. Briscoe, P. Vossen, A. Ageno,I.
Castellon, F. Ribas, G. Rigau, H. Rodriquez, andA.
Samiotou.
1994.
Acquisition of lexical trans-lation relations from MRDs.
Machine Translation,3(3?4):183?219.H.
Dyvik.
2004.
Translation as semantic mirrors: fromparallel corpus to WordNet.
Language and Comput-ers, 49(1):311?326.O.
Etzioni, K. Reiter, S. Soderland, and M. Sammer.2007.
Lexical translation with application to imagesearch on the Web.
In Machine Translation SummitXI.M.
Franz, S. McCarly, and W. Zhu.
2001.
English-Chinese information retrieval at IBM.
In Proceed-ings of TREC 2001.P.
Fung.
1995.
A pattern matching method for findingnoun and proper noun translations from noisy paral-lel corpora.
In Proceedings of ACL-1995.W.
Gale and K.W.
Church.
1991.
A Program forAligning Sentences in Bilingual Corpora.
In Pro-ceedings of ACL-1991.T.
Gollins and M. Sanderson.
2001.
Improving crosslanguage retrieval with triangulated translation.
InSIGIR.Raymond G. Gordon, Jr., editor.
2005.
Ethnologue:Languages of the World (Fifteenth Edition).
SIL In-ternational.A.
Haghighi, P. Liang, T. Berg-Kirkpatrick, andD.
Klein.
2008.
Learning bilingual lexicons frommonolingual corpora.
In ACL.S.
Helmreich, L. Guthrie, and Y. Wilks.
1993.
Theuse of machine readable dictionaries in the Panglossproject.
In AAAI Spring Symposium on BuildingLexicons for Machine Translation.Monika R. Henzinger, Allan Heydon, Michael Mitzen-macher, and Marc Najork.
1999.
Measuring indexquality using random walks on the web.
In WWW.R.
Hoffmann, S. Amershi, K. Patel, F. Wu, J. Foga-rty, and D. S. Weld.
2009.
Amplifying commu-nity content creation with mixed-initiative informa-tion extraction.
In ACM SIGCHI (CHI2009).D.
R. Karger.
1999.
A randomized fully polynomialapproximation scheme for the all-terminal networkreliability problem.
SIAM Journal of Computation,29(2):492?514.K.
Knight and S. Luk.
1994.
Building a large-scaleknowledge base for machine translation.
In AAAI.I.D.
Melamed.
1997.
A Word-to-Word Model ofTranslational Equivalence.
In Proceedings of ACL-1997 and EACL-1997, pages 490?497.M.
Neff and M. McCord.
1990.
Acquiring lexical datafrom machine-readable dictionary resources for ma-chine translation.
In 3rd Intl Conference on Theoret-ical and Methodological Issues in Machine Transla-tion of Natural Language.P.
Vossen, editor.
1998.
EuroWordNet: A multilingualdatabase with lexical semantic networds.
KluwerAcademic Publishers.270
