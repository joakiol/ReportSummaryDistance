Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 722?731,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsHybrid text simplification using synchronous dependency grammars withhand-written and automatically harvested rulesAdvaith SiddharthanComputing ScienceUniversity of AberdeenUKadvaith@abdn.ac.ukM.
A. AngroshComputing ScienceUniversity of AberdeenUKangroshmandya@abdn.ac.ukAbstractWe present an approach to text simplifi-cation based on synchronous dependencygrammars.
The higher level of abstractionafforded by dependency representationsallows for a linguistically sound treatmentof complex constructs requiring reorder-ing and morphological change, such asconversion of passive voice to active.
Wepresent a synchronous grammar formalismin which it is easy to write rules by handand also acquire them automatically fromdependency parses of aligned English andSimple English sentences.
The grammarformalism is optimised for monolingualtranslation in that it reuses ordering infor-mation from the source sentence where ap-propriate.
We demonstrate the superiorityof our approach over a leading contempo-rary system based on quasi-synchronoustree substitution grammars, both in termsof expressivity and performance.1 IntroductionText simplification is sometimes defined as theprocess of reducing the grammatical and lexi-cal complexity of a text, while still retaining theoriginal information content and meaning.
Themain goal of simplification is to make informa-tion more accessible to the large numbers of peo-ple with reduced literacy.
The National Lit-eracy Trust (http://www.literacytrust.org.uk) esti-mates that one in six adults in the UK have poorliteracy skills.
The situation is often worse in de-veloping countries.
Alu?
?sio et al.
(2008) reportthat 68% of Brazilians between 15 and 64 yearswho have studied up to 4 years only reach the rudi-mentary level of literacy, and even among thosewho have studied for 8 years, only a quarter canbe considered fully literate.
While there is a largebody of evidence that manual text simplificationis an effective intervention (Anderson and Free-body, 1981; L?Allier, 1980; Beck et al., 1991;Anderson and Davison, 1988; Linderholm et al.,2000; Kamalski et al., 2008), there has till recentlybeen little work on automatic simplification.
Thepace of research has picked up in recent yearsthough, with many teams applying machine trans-lation approaches to perform ?monolingual trans-lation?
from English to simplified English.
Thegoals of this paper are to (1) identify the limita-tions of recently published approaches to text sim-plification with regard to their coverage of linguis-tic constructs, (2) to describe an approach basedon synchronous grammars operating on typed de-pendency representations that permits a more so-phisticated handling of many linguistic constructs,and (3) to present a hybrid system that combines asmall set of hand written grammar rules for purelysyntactic constructs with a much larger set of auto-matically acquired rules for lexicalised constructsin one synchronous formalism.We summarise work on text simplification inSection 2, before describing our method in Sec-tion 3 and presenting our results in Section 4.2 Related workThere are two largely distinct bodies of work onautomatic text simplification ?
those that use hand-crafted rules, and those that apply machine trans-lation approaches.2.1 Hand-crafted text simplification systemsThe first body of work uses hand-crafted rulesto perform syntactic simplification operations(e.g., splitting coordinated and subordinatedclauses, and disembedding apposition and relativeclauses).
Some early systems (Chandrasekar etal., 1996; Siddharthan, 2002) used flat represen-tations (chunked and part-of-speech tagged text).More commonly, text simplification systems use722hand crafted rules that apply to hierarchical rep-resentations, including constituency-based parses(Canning, 2002; Candido Jr et al., 2009; De Belderand Moens, 2010) and dependency parses (Bott etal., 2012; Siddharthan, 2010; Siddharthan, 2011).For languages without corpora of simplified texts,hand crafted systems are typically the only avail-able alternative.2.2 Text simplification as monolingualtranslationRecent years have seen the increased applicationof machine translation approaches to text simpli-fication, often referred to as ?monolingual transla-tion?, and driven by the new availability of cor-pora of simplified texts such as Simple EnglishWikipedia (SEW).Wubben et al.
(2012) and Coster and Kauchak(2011) apply Phrase Based Machine Translation(PBMT) to the task of text simplification.
PMBTcan only perform a small set of simplification op-erations, such as lexical substitution, deletion andsimple paraphrase.
They are not well suited forreordering or splitting operations.
Specifically,the syntactic simplification operations that hand-crafted systems focus on are out of scope.Zhu et al.
(2010) in contrast present an approachbased on syntax-based SMT (Yamada and Knight,2001).
Their translation model encodes proba-bilities for four specific rewrite operations on theparse trees of the input sentences: substitution, re-ordering, splitting, and deletion.
Splitting is en-coded as two probabilities: A segmentation tablestores probabilities of sentence splitting at particu-lar words (e.g., which).
A completion table storesprobabilities of the splitting word to be deletedfrom the translation, and for the governing phraseto be inserted to complete the sentence.
This al-lows the translation model to handle constructssuch as relative clauses and apposition.Dras (1999) was the first to apply synchronousgrammars to monolingual tasks.
His approach isto map between two TAG grammars using a Gen-eralised Synchronous TAG formalism, and to useInteger Programming to generate a text that sat-isfies the externally imposed constraints (such aslength or readability) using minimal paraphras-ing.
Woodsend and Lapata (2011) further de-velop this line of research.
Their model is basedon quasi-synchronous grammar (Smith and Eis-ner, 2006) and integer linear programming.
Quasi-synchronous grammars, like the Generalised Syn-chronous TAGs of Dras (1999), aims to relaxthe isomorphism constraints of synchronous gram-mars, in this case by generating a loose alignmentbetween parse trees.
The Woodsend and Lapata(2011) model is trained on two different datasets:one containing alignments between sentences inWikipedia and English Simple Wikipedia, and onecontaining alignments between edits in the revi-sion history of Simple Wikipedia.
The latter per-forms best in their study, and also achieves bet-ter scores than the Zhu et al.
(2010) system, bothwhen evaluated using BLEU, and on human eval-uations of simplicity, grammaticality and meaningpreservation.
We will directly compare our ap-proach to Woodsend and Lapata (2011), as this isthe best performing contemporary system that hasthe same linguistic scope as ours.2.3 Formalisms and linguistic coverageThe systems summarised above differ primarilyin the level of linguistic knowledge they encode.PBMT systems use the least knowledge, and assuch are ill equipped to to handle simplificationsthat require morphological changes, syntactic re-ordering or sentence splitting.Syntax based approaches use syntactic knowl-edge.
However, both Zhu et al.
(2010) and Wood-send and Lapata (2011) use the Stanford Parser(Klein and Manning, 2003) for syntactic structure,and this representation lacks morphological infor-mation.
This means that some simplification op-erations such as voice conversion are not handledwell.
For example, to simplify ?trains are liked byJohn?
to ?John likes trains?, besides deleting aux-iliaries and reordering the arguments of the verb?like?, the verb also needs to agree in number withthe new subject (?John?
), and take the tense of theauxiliary verb (?are?
).The grammar acquisition process leads to fur-ther problems.
From an aligned pair ?John, whowas tired, went to sleep.?
and ?John was tired.
Hewent to sleep.
?, systems would learn a simplifica-tion rule that introduces the pronoun ?He?.
Thegoverning syntax for this rule is the verb ?went?
;hence, ?Susan, who was tired, went to sleep.
?might later get simplified as ?Susan was tired.
Hewent to sleep.
?.Hand-crafted systems have an advantage here.Such systems would typically use rules that du-plicate the noun phrase, generating ?John was723tired.
John went to sleep.?
and ?Susan wastired.
Susan went to sleep.?
Systems such as Sid-dharthan (2011) use transformation rules that en-code morphological changes as well as deletions,re-orderings, substitutions and sentence splitting,and are well suited to handle the voice conversionexample above.
On the other hand, hand-craftedsystems are limited in scope to syntactic simplifi-cation.
While purely syntactic rules can be writtenmanually, there are too many lexico-syntactic andlexical simplifications to enumerate by hand.In this paper, we present a hybrid text simpli-fication system that combines manually writtensynchronous grammars for common syntactic sim-plifications with a much larger automatically ac-quired synchronous grammar for lexicalised con-structs.
Our framework, using dependency repre-sentations, is better suited to text simplification.We demonstrate that the higher level of abstrac-tion in dependency parses allows for linguisticallycorrect rules for complex operations such as voiceconversion, while also providing a better model ofcontext for lexical simplification.3 MethodWe describe a text simplification system that usesa synchronous grammar defined over typed depen-dencies.
We demonstrate that this has specific ad-vantages over previous work on text simplifica-tion: (1) it allows for better linguistic modellingof simplification operations that require morpho-logical changes, (2) the higher level of abstractionmakes it easy to write and read grammar rules;thus common syntactic operations (such as con-version of passive to active voice) can be handledin this framework through accurate hand-writtenrules, and (3) It is easier and more elegant to au-tomatically acquire a synchronous grammar fromdata, compared to synchronous grammars basedon constituency-parses.
In this section we de-scribe our framework and text simplification sys-tem in more detail; then, in section 4, we report anevaluation that compares our system against a hu-man simplification and the Woodsend and Lapata(2011) system.3.1 Synchronous dependency insertiongrammarsDing and Palmer (2005) introduce the notion ofa Synchronous Dependency Insertion Grammar(SDIG) as a tree substitution grammar defined ondependency trees.
They define elementary trees(ETs) to be sub-sentential dependency structurescontaining one or more lexical items.
The SDIGformalism assumes that the isomorphism of thetwo syntactic structures is at the ET level, thus al-lowing for non-isomorphic tree to tree mappingat the sentence level.
We base our approach totext simplification on SDIGs, but the formalismis adapted for the monolingual task, and the rulesare written in a formalism that is suited to writ-ing rules by hand as well as automatically acquir-ing rules from aligned sentences.
Our system fol-lows the architecture proposed in Ding and Palmer(2005), reproduced in Fig.
1.
In this paper, wewill present the ET Transfer component as a set oftransformation rules.
The rest of Section 3 will fo-cus on the linguistic knowledge we need to encodein these rules, the method for automatic acquisi-tion of rules from a corpus of aligned sentences,and the generation process.Input Sentence ??
Dependency Parse ??
Source ETs?ET Transfer?Output Sentences ??
Generation ??
Target ETsFigure 1: System Architecture3.2 Extracting synchronous grammars fromaligned sentencesTo acquire a synchronous grammar from depen-dency parses of aligned English and simple En-glish sentences, we just need to identify the dif-ferences.
For example, consider two aligned sen-tences from the aligned corpus described inWood-send and Lapata (2011):1.
(a) Also, lichen fungi can reproduce sexu-ally, producing spores.
(b) Also, lichen fungi can reproduce sexu-ally by producing spores.An automatic comparison of the dependencyparses for the two sentences (using the StanfordParser, and ignoring punctuation for ease of pre-sentation) reveals that there are two typed depen-dencies that occur only in the parse of the first sen-tence, and two that occur only in the parse of thesecond sentence (in italics):724reproducexcompproducingdobjsporesreproduceprep bysporesamodproducingFigure 2: Transduction of Elementary Trees (ETs)1.
(a) 1.
(b)advmod(reproduce, Also) advmod(reproduce, Also)nn(fungi, lichen) nn(fungi, lichen)nsubj(reproduce, fungi) nsubj(reproduce, fungi)aux(reproduce, can) aux(reproduce, can)advmod(reproduce,sexually) advmod(reproduce,sexually)xcomp(reproduce,producing) amod(spores,producing)dobj(producing, spores) prep by(reproduce, spores)Thus, to convert the first sentence into the sec-ond, we need to delete two dependencies and in-troduce two others.
The rule contains variables(?Xn), which can be forced to match certain wordsin square brackets:RULE: PRODUCING2BY PRODUCING1.
DELETE(a) xcomp(?X0[reproduce], ?X1[producing])(b) dobj(?X1[producing], ?X2[spores])2.
INSERT(a) amod(?X2, ?X1)(b) prep by(?X0, ?X2)By collecting such rules, we can producea meta-grammar that can translate dependencyparses in one language (English) into the other(simplified English).
The rule above will trans-late ?reproduce, producing spores?
to ?reproduceby producing spores?.
This rule is alternativelyshown as a transduction of elementary trees in Fig.2.
Such deletion and insertion operations are cen-tral to text simplification, but a few other opera-tions are also needed to avoid broken dependencylinks in the Target ETs (cf.
Fig.
1).Consider lexical simplification; for example,where the word ?extensive?
is replaced by ?big?,resulting in one amod relation being deleted anda new one inserted.
Now, a third list is automat-ically created when a variable (?X1) is present inthe DELETE list but not the INSERT list.
Thisis a command to move any other relations (edges)involving the node ?X1 to the newly created node?X2, and ensures correct rule application in newcontexts where there might be additional relationsinvolving the deleted word.RULE: EXTENSIVE2BIG1.
DELETE(a) amod(?X0[network], ?X1[extensive])2.
INSERT(a) amod(?X0, ?X2[big])3.
NODE OPERATION(a) MOVE: ?X1 ??
?X2We also apply a process of generalisation, sothat a single rule can be created from multipleinstances in the training data.
For example, ifthe modifier ?extensive?
has been simplified to?big?
in the context of a variety of words in the?X0 position, this can be represented succinctlyas ?
?X0[networks, avalanches, blizzard, contro-versy]?.
Note that this list provides valid lexicalcontexts for application of the rule.
If the wordis seen in sufficient contexts, we make it universalby removing the list.
An example of a generalisedrule follows:RULE: *2BIG1.
DELETE(a) amod(?X0, ?X1[extensive, large, massive, siz-able, major, powerful, unprecedented, devel-oped, giant])2.
INSERT(a) amod(?X0, ?X2[big])3.
NODE OPERATION(a) MOVE: ?X1 ??
?X2This rule states that any of the words in ?
[ex-tensive, large, massive, sizable, major, power-ful, unprecedented, developed, giant]?
can be re-placed by ?big?
in any lexical context ?X0; i.e.,these words are not ambiguous.
We acquire rulessuch as the above automatically, filtering out rulesthat involve syntactic constructs that we requiremanually-written rules for (relative clauses, appo-sition, coordination and subordination).
We haveextracted 3180 rules from SEW revision historiesand aligned SEW-EW sentence pairs.
From thesame data, Woodsend and Lapata (2011) extract1431 rules, but these include rules for deletion,as well as for purely syntactic sentence splitting.The 3180 rules we derive are only lexical simpli-fications or simple paraphrases.
We do not per-form deletion operations, and use manually writ-ten rules for sentence splitting rules725Our approach allows for the encoding of locallexico-syntactic context for lexical simplification.Only if a simplification is seen in many contextsdo we generalise the rule by relaxing the lexi-cal context.
We consider this a better solution tothat implemented in Woodsend and Lapata (2011),who have to discard lexical rules that are only seenonce, because they do not model lexical context.3.3 Manual grammars for common syntacticcasesIn addition to the automatically acquired grammaras described above, our system uses a small handcrafted grammar for common syntactic simplifica-tions.
As discussed earlier, these rules are diffi-cult to learn from corpora, as difficult morphologyand tense manipulations would have to be learntfrom specific instances seen in a corpus.
In prac-tice, it is easy enough to code these rules correctly.We have 26 hand-crafted rules for apposition, rel-ative clauses, and combinations of the two.
A fur-ther 85 rules handle subordination and coordina-tion.
These are greater in number because theyare lexicalised on the conjunction.
11 further rulescover voice conversion from passive to active.
Fi-nally, we include 14 rules to standardise quota-tions; i.e., reduce various constructs for attributionto the form ?X said: Y.?
Performing this step al-lows us to simplify constructs embedded withinquotations - another case that is not handled wellby existing systems.
One of the rules for convert-ing passive to active voice is shown below:RULE: PASSIVE2ACTIVE1.
DELETE(a) nsubjpass(?X0, ?X1)(b) auxpass(?X0, ?X2)(c) agent(?X0, ?X3)2.
INSERT(a) nsubj(?X0, ?X3)(b) dobj(?X0, ?X1)3.
NODE OPERATIONS(a) AGR-TENSE: ?X0??
?X2(b) AGR-NUMBER: ?X0??
?X3The rule specifies that the node ?X0 should in-herit the tense of ?X2 and agree in number with?X3.
This rule correctly captures the morpholog-ical changes required for the verb, something notachieved by the other systems discussed in Sec-tion 2.
The dependency representation makes suchlinguistic constraints easy to write by hand.
How-ever, we are not yet in a position to learn suchconstraints automatically.
Our argument is that asmall number of grammar rules need to be codedcarefully by hand to allow us to express the diffi-cult syntactic constructions, while we can harvestlarge grammars for local paraphrase operations in-cluding lexical substitution.3.4 Elementary tree transferIn this work we apply the simplification rules ex-haustively to the dependency parse; i.e., every rulefor which the DELETE list is matched is appliediteratively.
As an illustration, consider:The cat was chased by a dog that wasbarking.det(cat-2, The-1)nsubjpass(chased-4, cat-2)auxpass(chased-4, was-3)det(dog-7, a-6)agent(chased-4, dog-7)nsubj(barking-10, dog-7)aux(barking-10, was-9)rcmod(dog-7, barking-10)Two rules match; the first simplifies relativeclauses:RULE: RELATIVECLAUSE1.
DELETE(a) rcmod(?
?X0, ?
?X1)(b) nsubj(?
?X1, ??X0)2.
INSERT(a) nsubj(?
?X1, ?
?X0)This rule removes the embedding ?rcmod?
re-lation, when there is a subject available for theverb in the relative clause.
Then we apply the ruleto convert passive to active voice, as described inSection 3.3.
Following these two rule applications,we are left with the following list of dependencies:det(cat-2, The-1)dobj(chased-4, cat-2)det(dog-7, a-6)nsubj(chased-4, dog-7)aux(barking-10, was-9)nsubj(barking-10, dog-7)This list now represents two trees with chasedand barking as root nodes:726chaseddobj nsubjcatdetdogdetthe abarkingauxnsubjwas dogdeta3.5 Generating from typed dependencyrepresentationsGenerating from constituency-based parse trees istrivial, in that leaf nodes need to be output in theorder processed by a depth first LR search.
Thehigher level of abstraction of dependency repre-sentations makes generation more complicated, asthe dependencies abstract away from constituentordering and word morphology.
One option is touse an off the shelf generator; however, this doesnot work well in practice; e.g., Siddharthan (2011)found that misanalyses by the parser can result inunacceptable word and constituent orders in thegenerated texts.
In the system described here,we follow the generation-light approach adoptedby Siddharthan (2011).
We reuse the word or-der from the input sentence as a default, and thesynchronous grammar encodes any changes in or-dering.
For example, in Rule PASSIVE2ACTIVEabove, we include a further specification:4 Traversal Order Specifications(a) Node ?X0: [?X3, ?X0, ?X1]This states that for node ?X0, the traversal ordershould be subtree ?X3 followed by current node?X0 followed by subtree ?X1.
Using this specifi-cation would allow us to traverse the tree using theoriginal word order for nodes with no order speci-fication, and the specified order where a specifica-tion exists.
In the above instance, this would leadus to simplify ?The cat is chased by the dogs?
to?the dogs chase the cat?.
Details of the genera-tion process can be found elsewhere (Siddharthan,2011, for example), but to summarise, the gen-light approach implemented here uses four lists:1.
DELETE: List of relations to delete.2.
INSERT: List of relations to insert.3.
ORDERING: List of nodes with subtree order specified4.
NODE-OPERATIONS: List of morphological changesand deletion operations on nodes.At present the automatically harvested rules donot encode morphological changes.
They do how-ever encode reordering information, which is auto-matically detected from the relative word positionsin the original and simplified training sentences.4 EvaluationWe performed a manual evaluation of how fluentand simple the text produced by our simplifica-tion system is, and the extent to which it preservesmeaning.
We use the evaluation set previouslyused by Woodsend and Lapata (2011), Zhu et al.
(2010) and Wubben et al.
(2012).
This consistsof 100 sentences from English Wikipedia, alignedwith Simple English Wikipedia (SEW) sentences.Previous work report various automatic measures,including BLEU and readability metrics such asthe Flesch-Kincaid Grade Level Index (FKGL).None of these have been validated for the auto-matic text simplification task, however, and weprefer to conduct an evaluation with human raters.Our system (henceforth, HYBRID) is comparedto QTSG (the system by Woodsend and Lapata(2011) that learns a quasi-synchronous grammarfrom the same data as the automated componentof HYBRID), and the manual gold standard SEW.We selected the first 25 sentences from the evalu-ation set for which both QTSG and HYBRID hadperformed at least one simplification1.
Five hu-man raters2were shown sets containing the origi-nal Wikipedia sentence, followed by QTSG, HY-BRID and SEW in a randomised order.
For eachsuch set, they were asked to rate each simplifiedversion for fluency, simplicity and the extent towhich it preserved the meaning of the original, us-ing a Likert scale of 1?5, where 1 is totally un-usable output, and 5 is output that is perfectlyusable.
The results are shown in Table 1.
OurHYBRID system outperforms QTSG on all threemetrics, and is comparable to the SEW version.Raters R1?3 provide very similar ratings, whileR4?5 demonstrate a greater preference for the HY-BRID system relative to the SEW.
The HYBRIDsystem performs best on meaning preservation (in136 sentences were considered and 11 sentences were ex-cluded in this process.
QTSG did not simplify 3 sentencesand HYBRID as many as 9, as it does not perform compres-sion operations.
One sentence was left unchanged by bothsystems.2R1?R4 are Computational Linguists, while R5 is a doc-toral student in Public Health Communication.
None of themare connected with this research, and none of them have pre-viously seen the output of text simplification systems.727Rater FLUENCY SIMPLICITY MEANING PRESERVATIONQTSG HYBRID SEW QTSG HYBRID SEW QTSG HYBRID SEWR1 2.60 4.44 4.60 3.04 3.88 4.36 3.16 4.68 4.24R2 3.08 4.24 4.52 3.20 4.08 4.48 3.28 4.76 4.36R3 2.40 4.20 4.68 3.12 3.80 4.44 2.96 4.52 3.80R4 2.32 3.88 3.48 2.92 3.44 3.44 2.72 4.52 3.56R5 2.00 3.44 3.48 2.00 3.52 3.56 2.48 4.52 3.84Mean 2.48 4.04 4.15 2.85 3.74 4.05 2.92 4.60 3.96Median 2 4 4 3 4 4 3 5 4Table 1: Results of human evaluation with five raters R1?R5.
QTSG is the system by Woodsend andLapata (2011).
HYBRID is the system described in this paper, with manual and automatically acquiredrules.
SEW is the human generated simplification from Simple English Wikipedia.
All differences inmeans for Simplicity and Meaning Preservation are significant (p < 0.001; t-test).
For Fluency, HYBRIDand SEW are significantly better than QTSG (p < 0.001; t-test).large part because it is the only version that doesnot delete information through sentence compres-sion).Table 2 shows some examples of simplificationsfrom the evaluation dataset, along with their av-erage scores for fluency, simplicity and meaningpreservation.
These examples have been selectedto help interpret the results in Table 1.
QTSG fre-quently generates fragments (?Komiyama is a.?,etc.
), likely through incorrect splitting rules in thegrammar; this is penalised heavily by the raters.The HYBRID system uses manually written rulesfor sentence splitting and is more robust in this re-gard.
This is confirmed by looking at standard de-viations of ratings.
For fluency, QTSG has sd =1.41, almost twice that of HYBRID (sd = .76).A similar trend is observed for meaning preserva-tion, where QTSG has sd = 1.29, compared tosd = .68 for HYBRID.QTSG does perform very elegant compressionsin some cases; this is a strength of that system.Our system aims to preserve meaning, which itdoes rather well.
However, this is is not neces-sarily a valid objective.
Perhaps future evalua-tions should distinguish between modifying infor-mation in misleading ways (undesirable) and re-moving peripheral information (desirable).
It isclear that the latter, done well, is useful and willbe addressed in future work.An error analysis shows that the main causeof errorful output for our system is parser errors,particularly mistakes in relative clause attachmentand clause boundary identificaton.
Methods suchas those in Siddharthan (2003b) can be used to im-prove parser performance on these tasks.Finally, this work and the cited related workonly investigate sentence-level text simplification.There are various discourse level effects that alsoneed to be considered when simplifying largertexts, including sentence ordering (Barzilay etal., 2002; Siddharthan, 2003a; Barzilay and La-pata, 2008), discourse connectives (Siddharthanand Katsos, 2010) and anaphora choice (Nenkovaet al., 2005; Siddharthan et al., 2011).5 ConclusionsWe have presented a framework for text sim-plification based on synchronous grammars overtyped dependency representations.
Our HYBRIDsystem, that uses hand-written rules for commonsyntactic simplifications, and automatically har-vested rules for a much larger set of lexicalisedsimplifications is more robust than a similar sys-tem based on quasi-synchronous tree substitutiongrammars, outperforming it in terms of fluency,simplicity and meaning preservation.
By abstract-ing away from constituent ordering and morpho-logical variations, our approach allows for lin-guistically sound rules to be written for complexlexico-syntactic transformations, including pas-sive to active voice.
In the version of the systemdescribed and evaluated here, changes to morphol-ogy and constituent ordering are specified withinthe rules.
Alternately, an off the shelf surface re-aliser could be used to generate from the depen-dency representation.AcknowledgementsThis research is supported by an award made bythe EPSRC; award reference: EP/J018805/1.728ORIGINAL QTSG HYBRID SEWTakanobu Komiyama(born October 3, 1984in Chiba, Japan) isa Japanese footballplayer who currentlyplays for the J-leagueteam KawasakiFrontale.His father.
Komiyamais a.Takanobu Komiyama(born October 3, 1984in Chiba, Japan) isa Japanese footballplayer.
TakanobuKomiyama at presentplays for the J-leagueteam KawasakiFrontale.Takanobu Komiyama(born 3 October 1984)is a Japanese footballplayer.
He plays forKawasaki Frontale.F=1, S=1.4, M=1 F=4, S=3.8, M=4.8 F=4.6, S=4.4, M=4.2The occupants ofSwadlincote oftenshorten its name tosimply ?Swad?.Swadlincote watch.The occupants oftenshorten its name tosimply ?Swad?.The occupants ofSwadlincote oftenshorten its name tojust ?Swad?.People from Swadlin-cote often shorten itsname to simply Swad.F=2.6, S=2.6, M=3.2 F=4.4, S=4, M=5 F=4.6, S=4.6, M=4.8Today the nearest rail-way station is Burtonupon Trent, about fivemiles away, as the rail-way between Burtonand Leicester lost itspassenger service un-der the Beeching Planin the 1960s and nowcarries only freight.Today the nearest rail-way station is Burtonupon Trent, about fivemiles away, as the rail-way between Burtonand Leicester lost ser-vice under the Beech-ing Plan in the 1960s.It now carries freight.The closest railwaystation is now Burtonupon Trent, about fivemiles away.
This isbecause the railwaybetween Burton andLeicester now carriesonly freight.
Therailway lost its passen-ger service under theBeeching Plan in the1960s.The nearest railwaystation is Burton uponTrent which is fivemiles away.F=4.2, S=3.6, M=4.6 F=4.4, S=3.8, M=5 F=4, S=4, M=2.2Since December 2005it has also been a can-didate for joining theEuropean Union andhas applied for NATOmembership.Since December 2005,it is a candidate forjoining the EuropeanUnion.Since December 2005it has also been acandidate for joiningthe European Union.And it has applied forNATO membership.Since December 2005it has also been a can-didate for joining theEuropean Union.
Ithas applied for NATOmembership.F=4, S=4.2, M=3.6 F=4.2, S=4, M=4.8 F=4.2, S=4, M=4.8Although most Irishpolitical parties recog-nize his contributionto the foundation ofthe modern Irish state,supporters of FineGael hold his memoryin particular esteem,regarding him as theirmovement?s foundingfather, through hislink to their pre-cursor Cumann nanGaedhael.The modern Irishstate watch.
MostIrish political partiesrecognize his contri-bution to foundation.Supporters of Gaelhold his memory inparticular esteem,regarding him as theirmovement?s foundingfather, through hislink to their pre-cursor Cumann nanGaedhael.Supporters of FineGael hold his mem-ory in very esteem,regarding him as theirmovement?s foundingfather, through hislink to their precursorCumann na nGaed-hael.
But, all Irishpolitical parties recog-nize his contributionto the foundation ofthe modern Irish state.Most Irish politi-cal parties think hiscontributions wereimportant to makethe modern Irishstate.
Members andsupporters of FineGael remember himin particular as oneof the founders oftheir movement, or itspredecessor Cumannna nGaedhael.F=2.6, S=3.2, M=3.8 F=3.4, S=3.6, M=4.2 F=3.6, S=3.4, M=4.6Table 2: Examples of simplifications from the test set, along with average scores for (F)luency,(S)implicity and (M)eaning Preservation.729ReferencesSandra M Alu?
?sio, Lucia Specia, Thiago AS Pardo, Er-ick G Maziero, and Renata PM Fortes.
2008.
To-wards brazilian portuguese automatic text simplifi-cation systems.
In Proceedings of the eighth ACMsymposium on Document engineering, pages 240?248.
ACM.Richard C. Anderson and Alice Davison.
1988.
Con-ceptual and empirical bases of readibility formulas.In Alice Davison and G. M. Green, editors, Linguis-tic Complexity and Text Comprehension: Readabil-ity Issues Reconsidered.
Lawrence Erlbaum Asso-ciates, Hillsdale, NJ.Richard Anderson and Peter Freebody.
1981.
Vocab-ulary knowledge.
In John Guthrie, editor, Compre-hension and Teaching: Research Reviews, pages 77?117.
International Reading Association, Newark,DE.R.
Barzilay and M. Lapata.
2008.
Modeling LocalCoherence: An Entity-Based Approach.
Computa-tional Linguistics, 34(1):1?34.R.
Barzilay, N. Elhadad, and K. McKeown.
2002.
In-ferring Strategies for Sentence Ordering in Multi-document News Summarization.
Journal of Artifi-cial Intelligence Research, 17(3):35?55.Isabel L. Beck, Margaret G. McKeown, Gale M. Sina-tra, and Jane A. Loxterman.
1991.
Revising socialstudies text from a text-processing perspective: Ev-idence of improved comprehensibility.
Reading Re-search Quarterly, pages 251?276.Stefan Bott, Horacio Saggion, and Simon Mille.
2012.Text simplification tools for spanish.
In LREC,pages 1665?1671.Arnaldo Candido Jr, ErickMaziero, Caroline Gasperin,Thiago AS Pardo, Lucia Specia, and Sandra MAluisio.
2009.
Supporting the adaptation of textsfor poor literacy readers: a text simplification ed-itor for brazilian portuguese.
In Proceedings ofthe Fourth Workshop on Innovative Use of NLPfor Building Educational Applications, pages 34?42.Association for Computational Linguistics.Yvonne Canning.
2002.
Syntactic simplification ofText.
Ph.D. thesis, University of Sunderland, UK.Raman Chandrasekar, Christine Doran, and Banga-lore Srinivas.
1996.
Motivations and methods fortext simplification.
In Proceedings of the 16th In-ternational Conference on Computational Linguis-tics (COLING ?96), pages 1041?1044, Copenhagen,Denmark.William Coster and David Kauchak.
2011.
Learning tosimplify sentences using wikipedia.
In Proceedingsof the Workshop on Monolingual Text-To-Text Gen-eration, pages 1?9.
Association for ComputationalLinguistics.Jan De Belder and Marie-Francine Moens.
2010.Text simplification for children.
In Prroceedings ofthe SIGIR workshop on accessible search systems,pages 19?26.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 541?548.
Association for Computa-tional Linguistics.Mark Dras.
1999.
Tree adjoining grammar and thereluctant paraphrasing of text.
Ph.D. thesis, Mac-quarie University NSW 2109 Australia.J.
Kamalski, T. Sanders, and L. Lentz.
2008.
Coher-ence marking, prior knowledge, and comprehensionof informative and persuasive texts: Sorting thingsout.
Discourse Processes, 45(4):323?345.D.
Klein and C.D.
Manning.
2003.
Accurate un-lexicalized parsing.
In Proceedings of the 41stAnnual Meeting on Association for ComputationalLinguistics-Volume 1, pages 423?430.
Associationfor Computational Linguistics.J.J.
L?Allier.
1980.
An evaluation study of a computer-based lesson that adjusts reading level by monitor-ing on task reader characteristics.
Ph.D. thesis,University of Minnesota, Minneapolis, MN.T.
Linderholm, M.G.
Everson, P. van den Broek,M.Mischinski, A. Crittenden, and J. Samuels.
2000.Effects of Causal Text Revisions on More-and Less-Skilled Readers?
Comprehension of Easy and Dif-ficult Texts.
Cognition and Instruction, 18(4):525?556.Ani Nenkova, Advaith Siddharthan, and KathleenMcKeown.
2005.
Automatically learning cog-nitive status for multi-document summarization ofnewswire.
In Proceedings of HLT/EMNLP 2005,pages 241?248, Vancouver, Canada.Advaith Siddharthan and Napoleon Katsos.
2010.Reformulating discourse connectives for non-expertreaders.
In Proceedings of the 11th Annual Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics (NAACL-HLT2010), Los Angeles, CA.Advaith Siddharthan, Ani Nenkova, and KathleenMcKeown.
2011.
Information status distinctionsand referring expressions: An empirical study ofreferences to people in news summaries.
Compu-tational Linguistics, 37(4):811?842.Advaith Siddharthan.
2002.
An architecture for a textsimplification system.
In Proceedings of the Lan-guage Engineering Conference (LEC?02), pages 64?71, Hyderabad, India.Advaith Siddharthan.
2003a.
Preserving discoursestructure when simplifying text.
In Proceedings of730the European Natural Language Generation Work-shop (ENLG), 11th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL?03), pages 103?110, Budapest, Hun-gary.Advaith Siddharthan.
2003b.
Resolving pronouns ro-bustly: Plumbing the depths of shallowness.
In Pro-ceedings of the Workshop on Computational Treat-ments of Anaphora, 11th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL?03), pages 7?14, Budapest, Hun-gary.Advaith Siddharthan.
2010.
Complex lexico-syntacticreformulation of sentences using typed dependencyrepresentations.
In Proceedings of the 6th Inter-national Natural Language Generation Conference(INLG 2010), Dublin Ireland.Advaith Siddharthan.
2011.
Text simplification usingtyped dependencies: a comparison of the robustnessof different generation strategies.
In Proceedings ofthe 13th European Workshop on Natural LanguageGeneration, pages 2?11.
Association for Computa-tional Linguistics.David A Smith and Jason Eisner.
2006.
Quasi-synchronous grammars: Alignment by soft projec-tion of syntactic dependencies.
In Proceedings ofthe Workshop on Statistical Machine Translation,pages 23?30.
Association for Computational Lin-guistics.Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to simplify sentences with quasi-synchronousgrammar and integer programming.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 409?420.
Associationfor Computational Linguistics.Sander Wubben, Antal van den Bosch, and EmielKrahmer.
2012.
Sentence simplification by mono-lingual machine translation.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers-Volume 1, pages1015?1024.Association for Computational Linguis-tics.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedingsof the 39th Annual Meeting on Association for Com-putational Linguistics, pages 523?530.
Associationfor Computational Linguistics.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of the23rd international conference on computational lin-guistics, pages 1353?1361.
Association for Compu-tational Linguistics.731
