Coling 2010: Poster Volume, pages 1122?1130,Beijing, August 2010Informed ways of improving data-drivendependency parsing for GermanWolfgang SeekerUniversity of StuttgartInst.
fu?r Maschinelle Sprachverarbeitungseeker@ims.uni-stuttgart.deBernd BohnetUniversity of StuttgartInst.
fu?r Maschinelle SprachverarbeitungBernd.Bohnet@ims.uni-stuttgart.deLilja ?vrelidUniversity of PotsdamInstitut fu?r Linguistikovrelid@uni-potsdam.deJonas KuhnUniversity of StuttgartInst.
fu?r Maschinelle Sprachverarbeitungjonas@ims.uni-stuttgart.deAbstractWe investigate a series of targeted modifi-cations to a data-driven dependency parserof German and show that these can behighly effective even for a relatively wellstudied language like German if they aremade on a (linguistically and methodolog-ically) informed basis and with a parserimplementation that allows for fast androbust training and application.
Mak-ing relatively small changes to a rangeof very different system components, wewere able to increase labeled accuracy ona standard test set (from the CoNLL 2009shared task), ignoring gold standard part-of-speech tags, from 87.64% to 89.40%.The study was conducted in less than fiveweeks and as a secondary project of allfour authors.
Effective modifications in-clude the quality and combination of auto-assigned morphosyntactic features enter-ing machine learning, the internal featurehandling as well as the inclusion of globalconstraints and a combination of differentparsing strategies.1 IntroductionThe past years have seen an enormous surge of in-terest in dependency parsing, mainly in the data-driven paradigm, and with a particular emphasison covering a whole set of languages with a singleapproach.
The reasons for this interest are mani-fold; the availability of shared task data from var-ious CoNLL conferences (among others (Buch-holz and Marsi, 2006; Hajic?
et al, 2009)), com-prising collections of languages based on a sin-gle representation format, has certainly been in-strumental.
But likewise, the straightforward use-fulness of dependency representations for a num-ber of tasks plays an important role.
The rela-tive language independence of the representationsmakes dependency parsing particularly attractivefor multilingually oriented work, including ma-chine translation.As data-driven approaches to dependency pars-ing have reached a certain level of maturity, it mayappear as if further improvements of parsing per-formance have to rely on relatively advanced tun-ing procedures, such as sophisticated automaticfeature selection procedures or combinations ofdifferent parsing approaches with complementarystrengths.
It is indeed still hard to pinpoint thestructural properties of a language (or annotationscheme) that make the parsing task easier for aparticular approach, so it may seem best to leavethe decision to a higher-level procedure.This paper starts from the suspicion thatwhile sophisticated tuning procedures are cer-tainly helpful, one should not underestimate thepotential of relatively simple modifications of theexperimental set-up, such as a restructuring of as-pects of the dependency format, a targeted im-provement of the quality of automatically as-signed features, or a simplification of the featurespace for machine learning ?
the modificationsjust have to be made in an informed way.
This1122presupposes two things: (i) a thorough linguisticunderstanding of the issues at hand, and (ii) a rel-atively powerful and robust experimental machin-ery which allows for experimentation in variousdirections and which should ideally support a fastturn-around cycle.We report on a small pilot study exploring thepotential of relatively small, informed modifica-tions as a way of improving parsing accuracyeven for a language that has received considerableattention in the parsing literature, including thedependency parsing literature, namely German.Within a timeframe of five weeks and spendingonly a few hours a day on the project (between agroup of four people), we were able to reach somesurprising improvements in parsing accuracy.By way of example, we experimented withmodifications in a number of rather different sys-tem areas, which we will discuss in the courseof this paper after a brief discussion of relatedwork and the data basis in Section 2.
Based on asecond-order maximum spanning tree algorithm,we used a hash kernel to facilitate the mappingof the features onto their weights for a very largenumber of features (Section 3); we modified thedependency tree representation for prepositionalphrases, adding hierarchical structure that facili-tates the picking up of generalizations (Section 4).We take advantage of a morphological analyzerto train an improved part-of-speech tagger (Sec-tion 5), and we use knowledge about the structureof morphological paradigms and the morphology-syntax interface in the feature design for machinelearning (Section 6).
As is known from other stud-ies, the combination of different parsing strategiesis advantageous; we include a relatively simpleparser stacking procedure in our pilot study (Sec-tion 7), and finally, we apply Integer Linear Pro-gramming in a targeted way to add some globalconstraints on possible combinations of arc labelswith a single head (Section 8).
Section 9 offers abrief conclusion.2 Related Work and Data BasisWe quickly review the situation in data-driven de-pendency parsing in general and on applying it toGerman specifically.The two main approaches to data-driven de-pendency parsing are transition based dependencyparsing (Nivre, 2003; Yamada and Matsumoto,2003; Titov and Henderson, 2007) and maximumspanning tree based dependency parsing (Eis-ner, 1996; Eisner, 2000; McDonald and Pereira,2006).
Transition based parsers typically havea linear or quadratic complexity (Attardi, 2006).Nivre (2009) introduced a transition based non-projective parsing algorithm that has a worst casequadratic complexity and an expected linear pars-ing time.
Titov and Henderson (2007) combineda transition based parsing algorithm, using beamsearch, with a latent variable machine learningtechnique.Maximum spanning tree based dependencyparsers decompose a dependency structure intofactors.
The factors of the first order maximumspanning tree parsing algorithm are edges consist-ing of the head, the dependent (child) and the edgelabel.
This algorithm has a quadratic complexity.The second order parsing algorithm of McDonaldand Pereira (2006) uses a separate algorithm foredge labeling.
In addition to the first order fac-tors, this algorithm uses the edges to those chil-dren which are closest to the dependent and has acomplexity of O(n3).
The second order algorithmof Carreras (2007) uses in addition to McDonaldand Pereira (2006) the child of the dependent oc-curring in the sentence between the head and thedependent as well as the edge from the dependentsto a grandchild.
The edge labeling is an integralpart of the algorithm which requires an additionalloop over the labels.
This algorithm therefore hasa complexity of O(n4).
Johansson and Nugues(2008) reduced the required number of loops overthe edge labels by considering only the edges thatexisted in the training corpus for a distinct headand child part-of-speech tag combination.Predating the surge of interest in data-baseddependency parsing, there is a relatively longtradition of dependency parsing work on Ger-man, including for instance Menzel and Schro?der(1998) and Duchier and Debusmann (2001).
Ger-man was included in the CoNLL shared tasks in2006 (Multilingual Dependency Parsing, (Buch-holz and Marsi, 2006)) and in 2009 (Syntactic andSemantic Dependencies in Multiple Languages,(Hajic?
et al, 2009)) with data based on the TIGER1123corpus (Brants et al, 2002) in both cases.
Sincethe original TIGER treebank is in a hybrid phrase-structural/dependency format with a relatively flathierarchical structure, conversion to a pure depen-dency format involves some non-trivial steps.
The2008 ACL Workshop on Parsing German includeda specific shared task on dependency parsing ofGerman (Ku?bler, 2008), based on two sets of data:again the TIGER corpus ?
however with a differ-ent conversion routine than for the CoNLL tasks ?and the Tu?Ba-D/Z corpus (Hinrichs et al, 2004).In the 2006 CoNLL task and in the 2008 ACLWorkshop task, the task was dependency parsingwith given gold standard part-of-speech tags fromthe corpus.
This is a valid way of isolating thespecific subproblem of parsing, however it is clearthat the task does not reflect the application set-ting which includes noise from automatic part-of-speech tagging.
In the 2009 CoNLL task, bothgold standard tags and automatically assigned tagswere provided.
The auto-tagged version was cre-ated with the standard model of the TreeTagger(Schmid, 1995) (i.e., with no domain-specific tag-ger training).In our experiments, we used the data set fromthe 2009 CoNLL task, for which the broadestcomparison of recent parsing approaches exists.The highest-scoring system in the shared task wasBohnet (2009) with a labeled accuracy (LAS) of87.48%, on auto-tagged data.
The highest-scoring(in fact the only) system in the dependency pars-ing track of the 2008 ACL Workshop on parsingGerman was Hall and Nivre (2008) with an LASof 90.80% on gold-tagged data, and with a dataset that is not comparable to the CoNLL data.13 Hash KernelOur parser is based on a second order maximumspanning tree algorithm and uses MIRA (Cram-mer et al, 2006) as learning technique in combi-nation with a hash kernel.
The hash kernel hasa higher accuracy since it can use additional fea-tures found during the creation of the dependency1To get an idea of how the data sets compare, we trainedthe version of our parser described in Section 3 (i.e., with-out most of the linguistically informed improvements) onthis data, achieving labeled accuracy of 92.41%, comparedto 88.06% for the 2009 CoNLL task version.tree in addition to the features extracted from thetraining examples.
The modification to MIRA issimple: we replace the feature-index mapping thatmaps the features to indices of the weight vectorby a random function.
Usually, the feature-indexmapping in the support vector machine has twotasks: The mapping maps the features to an indexand it filters out features that never occurred in adependency tree.
In our approach, we do not filterout these features, but use them as additional fea-tures.
It turns out that this choice improves pars-ing quality.
Instead of the feature-index mappingwe use the following hash function:2h ?
|(l xor(l ?
0xffffffff00000000 >> 32))% size|The Hash Kernel for structured data uses the hashfunction h : J ?
{1...n} to index ?
where ?maps the observations X to a feature space.
Wedefine ?
(x, y) as the numeric feature representa-tion indexed by J .
The learning problem is to fitthe function F so that the errors of the predictedparse tree y are as low as possible.
The scoringfunction of the Hash Kernel is defined as:3F (x, y) = ?
?w ?
?
(x, y)For different j, the hash function h(j) might gen-erate the same value k. This means that the hashfunction maps more than one feature to the sameweight which causes weight collisions.
This pro-cedure is similar to randomization of weights (fea-tures), which aims to save space by sharing val-ues in the weight vector (Blum, 2006; Rahimiand Recht, 2008).
The Hash Kernel shares valueswhen collisions occur that can be considered asan approximation of the kernel function, becausea weight might be adapted due to more than onefeature.
The approximation works very well witha weight vector size of 115 million values.With the Hash Kernel, we were able to improveon a baseline parser that already reaches a quitehigh LAS of 87.64% which is higher than the topscore for German (87.48%) in the CoNLL Sharedtask 2009.
The Hash Kernel improved that valueby 0.42 percentage points to 88.06%.
In additionto that, we obtain a large speed up in terms of pars-ing time.
The baseline parser spends an average of426 milliseconds to parse a sentence of the test2>> n shifts n bits right, and % is the modulo operation.3?
?w is the weight vector and the size of ?
?w is n.1124set and the parser with Hash Kernel only takes126 milliseconds which is an increase in speedof 3.4 times.
We get the large speed up becausethe memory access to a large array causes manyCPU cache misses which we avoid by replacingthe feature-index mapping with a hash function.As mentioned above, the speedup influences theexperimenters?
opportunities for explorative de-velopment since it reduces the turnaround time forexperimental trials.4 Restructuring of PPsIn a first step, we applied a treebank transforma-tion to our data set in order to ease the learningfor the parser.
We concentrated on prepositionalphrases (PP) to get an idea how much this kindof transformation can actually help a parser.
PPsare notoriously flat in the TIGER Treebank anno-tation (from which our data are derived) and theydo not embed a noun phrase (NP) but rather attachall parts of the noun phrase directly at PP level.This annotation was kept in the dependency ver-sion and it can cause problems for the parser sincethere are two different ways of annotating NPs: (i)for normal NPs where all dependents of the nounare attached as daughters of the head noun and (ii)for NPs in PPs where all dependents of the nounare attached as daughters to the preposition thusbeing sisters to their head noun.
We changed theannotation of PPs by identifying the head noun inthe PP and attaching all of its siblings to it.
To findthe correct head, we used a heuristic in the style ofMagerman (1995).
The head is chosen by takingthe rightmost daughter of the preposition that hasa category label according to the heuristic and islabeled with NK (noun kernel element).Table 1 shows the parser performance on thedata after PP-restructuring.4 The explanation forthe benefit of the restructuring is of course that4Note that we are evaluating against a gold standard here(and in the rest of the paper) which has been restructured aswell.
With a different gold standard one could argue that theabsolute figures we obtain are not fully comparable with theoriginal CoNLL shared task.
However, since we are doingdependency parsing, the transformation does neither add norremove any nodes from the structure nor do we change anylabels.
The only thing that is done during the transforma-tion is the reattachment of some daughters of a PP.
This isonly a small modification, and it is certainly linguisticallywarranted.now there is only one type of NP in the whole cor-pus which eases the parser?s task to correctly learnand identify them.dev.
set test setLAS UAS LAS UAShash kernel 87.40 89.79 88.06 90.24+restructured 87.49 89.97 88.30 90.44Table 1: Parser performance on restructured dataSince restructuring parts of the corpus seemsbeneficial, there might be other structures wheremore consistent annotation could help the parser,e.
g., coordination or punctuation (like in the 2008ACL Workshop data set, cp.
Footnote 1).5 Part-of-Speech TaggingHigh quality part-of-speech (PoS) tags can greatlyimprove parsing quality.
Having a verb wronglyanalyzed as a noun and similar mistakes are verylikely to mislead the parser in its decision process.A lot of the parser?s features include PoS tags andreducing the amount of errors during PoS taggingwill therefore reduce misleading feature values aswell.
Since the quality of the automatically as-signed PoS tags in the German CoNLL ?09 datais not state-of-the-art (see Table 2 below), we de-cided to retag the data with our own tagger whichuses additional information from a symbolic mor-phological analyzer to direct a statistical classifier.For the assignment of PoS tags, we applya standard maximum entropy classification ap-proach (see Ratnaparkhi (1996)).
The classes ofthe classifier are the PoS categories defined in theStuttgart-Tu?bingen Tag Set (STTS) (Schiller et al,1999).
We use standard binarized features likethe word itself, its last three letters, whether theword is capitalized, contains a hyphen, a digit orwhether it consists of digits only.
As the only non-binary feature, word length is recorded.
Thesestandard features are augmented by a number ofbinary features that support the classification pro-cess by providing a preselection of possible PoStags.
Every word is analyzed by DMOR, a finitestate morphological analyzer, from whose outputanalyses all different PoS tags are collected andadded to the feature set.
For example, DMORassigns the PoS tags NN (common noun) andADJD (predicative adjective) to the word gegan-1125gen (gone).
From these analyses two features aregenerated, namely possible-tag:NN and possible-tag:ADJD, which are strong indicators for theclassifier that one of these classes is very likelyto be the correct one.
The main idea here is touse the morphological analyzer as a sort of lexiconthat preselects the set of possible tags beforehandand then use the classifier to do the disambigua-tion (see Jurish (2003) for a more sophisticatedsystem based on Hidden-Markov models that usesroughly the same idea).
Since the PoS tags are in-cluded in the feature set, the classifier is still ableto assign every class defined in STTS even if it isnot in the preselection.
Where the morphologicalanalyzer does not know the word in question weadd features for every PoS tag representing a pro-ductive word class in German, making the reason-able assumption that the morphology knows aboutall closed-class words and word forms.
Finally,we add word form and possible tag features forthe previous and the following word to the featureset thus simulating a trigram tagger.
We used themethod of Kazama and Tsujii (2005) which usesinequality constraints to do a very efficient featureselection5 to train the maximum entropy model.We annotated the entire corpus with versionsof our own tagger, i.e., the training, developmentand test data.
In order to achieve a realistic be-havior (including remaining tagging errors, whichthe parser may be able to react to if they are sys-tematic), it was important that each section wastagged without any knowledge of the gold stan-dard tags.
For the development and test portion,this is straightforward: we trained a model on thegold PoS of the training portion of the data andapplied it to retag these two portions.
Retaggingthe training portion was a bit trickier since wecould not use a model trained on the same data,but at the same time, we wanted to use a taggerof similarly high quality ?
i.e.
one that has seen asimilar amount of training data.
The training setwas therefore split into 20 different parts and forevery split, a tagging model was trained on theother 19 parts which then was used to retag theremaining 20th part.
Table 2 shows the qualityof our tagger evaluated on the German CoNLL5We used a width factor of 1.0.?09 data in terms of accuracy and compares itto the originally annotated PoS tags which havebeen assigned by using the TreeTagger (Schmid,1995) together with the German tagging modelprovided from the TreeTagger website.
Taggingaccuracy improves consistently by about 2 per-centage points which equates to an error reductionof 44.55 % to 49.0 %.training development testoriginal 95.69 95.51 95.46retagged 97.61 97.71 97.52error red.
44.55% 49.00% 45.37%Table 2: Tagging accuracyTable 3 shows the parser performance whentrained on the newly tagged data.
The consider-able improvements in tagging accuracy visibly af-fect parsing accuracy, raising both the labeled andthe unlabeled attachment score by 0.66 percentagepoints (LAS) and 0.51 points (UAS) for the de-velopment set and by 0.45 points (LAS) and 0.64points (UAS) for the test set.dev.
set test setLAS UAS LAS UASrestructured 87.49 89.97 88.30 90.44+retagged 88.15 90.48 88.75 91.08Table 3: Parser performance on retagged data6 Morphological InformationGerman, as opposed to English, exhibits a rela-tively rich morphology.
Predicate arguments andnominal adjuncts are marked with special casemorphology which allows for a less restrictedword order in German.
The German case systemcomprises four different case values, namely nom-inative, accusative, dative and genitive case.
Sub-jects and nominal predicates are usually markedwith nominative case, objects receive accusativeor dative case and genitive case is usually usedto mark possessors in possessive constructions.There are also some temporal and spatial nominaladjuncts which require certain case values.
Sincecase is used to mark the function of a noun phrasein a clause, providing case information to a parsermight improve its performance.The morphological information in the GermanCoNLL ?09 data contains much more informationthan case alone and previous models (baseline,1126hash kernel, retagged) have used all of it.
How-ever, since we aim to improve a syntactic parser,we would like to exclude all morphological infor-mation from the parsing process that is not obvi-ously relevant to syntax, e. g. mood or tense.
Byreducing the morphological annotations to thosethat are syntactically relevant, we hope to reducethe noise that is introduced by irrelevant informa-tion.
(One might expect that machine learning andfeature selection should ?filter out?
irrelevant fea-tures, but given the relative sparsity of unambigu-ous instances of the linguistically relevant effects,drawing the line based on just a few thousand sen-tences of positive evidence would be extremelyhard even for a linguist.
)We annotated every case-bearing word in thecorpus with its case information using DMOR.With case-bearing words, we mean nouns, propernouns, attributive adjectives, determiners and allkinds of pronouns.
Other types of morphologi-cal information was discarded.
We did not usethe manually annotated and disambiguated mor-phological information already present in the cor-pus for two reasons: the first one is the same aswith the PoS tagging.
Since it is unrealistic tohave gold-standard annotation in a real-world ap-plication which deals with unseen data, we wantthe parser to learn from and hopefully adapt toimperfectly annotated data.
The second reasonis the German-inherent form syncretism in nom-inal paradigms.
The German noun inflection sys-tem is with over ten different (productive andnon-productive) inflectional patterns quite com-plicated, and to make matters worse, there areonly five different morphological markers to dis-tinguish 16 different positions in the pronoun, de-terminer and adjective paradigms and eight differ-ent positions in the noun paradigms.
Some po-sitions in the paradigm will therefore always bemarked in the same way and we would like theparser to learn that some word forms will alwaysbe ambiguous with respect to their case value.We also conducted experiments where we an-notated number and gender values in addition tocase.
The idea behind this is that number and gen-der might help to further disambiguate case val-ues.
The downside of this is the increase in fea-ture values.
Combining case and number featuresmeans a multiplication of their values creatingeight new feature values instead of four.
Addinggender annotation raises this number to 24.
Be-side the disambiguation of case, there is also an-other reason why we might want to add num-ber and gender: Inside a German noun phrase,all parts have to agree on their case and numberfeature in order to produce a well-formed nounphrase.
Furthermore, the head noun governs thegender feature of the other parts.
Thus, all threefeatures can be relevant to the construction of asyntactic structure.6 Table 4 shows the results ofour experiments with morphological features.dev.
set test setLAS UAS LAS UASretagged 88.15 90.48 88.75 91.08no morph.
87.78 90.18 88.60 90.92+case 88.04 90.48 88.77 91.13+c+n 88.21 90.62 88.88 91.13+c+n+g 87.96 90.33 88.73 90.99Table 4: Parser performance with morph.
infor-mation (c=case, n=number, g=gender)The no morph row in Table 4 shows, thatusing no morphological information at all de-creases parser performance.
When only case val-ues are annotated, the parser performance doesnot change much in comparison to the retaggedmodel, so there is no benefit here.
Adding num-ber features on the other hand improves parsingresults significantly.
This seems to support our in-tuition that number helps in disambiguating casevalues.
However, adding gender information doesnot further increase this effect but hurts parser per-formance even more than case annotation alone.This leaves us with a puzzle here.
Annotating caseand number helps the parser, but case alone orhaving case, number and gender together affectsperformance negatively.
A possible explanationmight be that the effect of the gender informationis masked by the increased number of feature val-ues (24) which confuses the parsing algorithm.7 Parser StackingNivre and McDonald (2008) show how two dif-ferent approaches to data-driven dependency pars-6Person would be another syntactically relevant informa-tion.
However, since we are dealing with a newspaper cor-pus, first and second person features appear very rarely.1127ing, the graph-based and transition-based ap-proaches, may be combined and subsequentlylearn to complement each other to achieve im-proved parsing results for different languages.MaltParser (Nivre et al, 2006) is a language-independent system for data-driven dependencyparsing which is freely available.7 It is based on adeterministic parsing strategy in combination withtreebank-induced classifiers for predicting parsingactions.
MaltParser employs a rich feature repre-sentation in order to guide parsing.
For the train-ing of the Malt parser model that we use in thestacking experiments, we use learner and parsersettings identical to the ones optimized for Ger-man in the CoNLL-X shared task (Nivre et al,2006).
Furthermore, we employ the techniqueof pseudo-projective parsing described in Nilssonand Nivre (2005) and a split prediction strategy forpredicting parse transitions and arc labels (Nivreand Hall, 2008).8 In order to obtain automaticparses for the whole data set, we perform a 10-fold split.
For the parser stacking, we follow theapproach of Nivre and McDonald (2008), usingMaltParser as a guide for the MST parser with thehash kernel, i.e., providing the arcs and labels as-signed by MaltParser as features.
Table 5 showsthe scores we obtain by parser stacking.
Althoughour version of MaltParser does not quite have thesame performance as for instance the version ofHall and Nivre (2008), its guidance leads to asmall improvement in the overall parsing results.dev.
set test setLAS UAS LAS UASMaltParser 82.47 85.78 83.84 86.8our parser 88.21 90.62 88.88 91.13+stacking 88.42 90.77 89.28 91.40Table 5: Stacked parser performance with guid-ance by MaltParser7http://maltparser.org8The feature models make use of information about thelexical form (FORM), the predicted PoS (PPOS) and the de-pendency relation constructed thus far during parsing (DEP).In addition, we make use of the predicted values for othermorphological features (PFEATS).
We employ the arc-eageralgorithm (Nivre, 2003) in combination with SVM learners,using LIBSVM with a polynomial kernel.8 RelabelingIn the relabeling step, we pursue the idea thatsome erroneous parser decisions concerning thedistribution of certain labels might be detected andrepaired in post-processing.
In German and inmost other languages, there are syntactic restric-tions on the number of subjects and objects thata verb might select.
The parser will learn this be-havior during training.
However, since it is using astatistical model with a limited context, it can stillhappen that two or more of the same grammati-cal functions are annotated for the same verb.
Buthaving two subjects annotated for a single verbmakes this particular clause uninterpretable forsubsequently applied tasks.
Therefore, we wouldlike to detect those doubly annotated grammaticalfunctions and correct them in a controlled way.The detection algorithm is simple: Runningover the words of the output parse, we check forevery word whether it has two or more daughtersannotated with the same grammatical function andif we find one, we relabel all of its daughters.9 Forthe relabeling, we applied a dependency-versionof the function labeler described in Seeker et al(2010) which uses a maximum entropy classifierthat is restrained by a number of hard constraintsimplemented as an Integer Linear Program.
Theseconstraints model the aforementioned selectionalrestrictions on the number of certain types of ver-bal arguments.
Since these are hard constraints,the labeler is not able to annotate more than oneof those grammatical functions per verb.
If wecount the number of sentences that contain doublyannotated grammatical functions in the best pars-ing results from the previous section, we get 189for the development set and 153 for the test set.About two thirds of the doubly annotated func-tions are subjects and the biggest part of the re-maining third are accusative objects which are themost common arguments of German verbs.Table 6 shows the final results after relabelingthe output of the best performing parser config-uration from the previous section.
The improve-ments on the overall scores are quite small, which9The grammatical functions we are looking for are SB(subject), OA (accusative object), DA (dative), OG (genitiveobject), OP (prepositional object), OC (clausal object), PD(predicate) and OA2 (second accusative object).1128dev.
set test setLAS UAS LAS UASstacking 88.42 90.77 89.28 91.40+relabeling 88.48 90.77 89.40 91.40Table 6: Parse quality after relabelingis partly due to the fact that the relabeling affectsonly a small subset of all labels used in the data.Furthermore, the relabeling only takes place if adoubly annotated function is detected; and evenif the relabeling is applied we have no guaranteethat the labeler will assign the labels correctly (al-though we are guaranteed to not get double func-tions).
Table 7 shows the differences in precisionand recall for the grammatical functions betweenthe original and the relabeled test set.
As one cansee, scores stay mostly the same except for SB,OA and DA.
For OA, scores improve both in recalland precision.
For DA, we trade a small decreasein precision for a huge improvement in recall andvice versa for SB, but on a much smaller scale.Generally spoken, relabeling is a local repair strat-egy that does not have so much effect on the over-all score but can help to get some important labelscorrect even if the parser made the wrong deci-sion.
Note that the relabeler can only repair incor-rect label decisions, it cannot help with wronglyattached words.original relabeledrec prec rec precDA 64.2 83.2 74.7 79.6OA 88.9 85.8 90.7 88.2OA2 0.0 NaN 0.0 NaNOC 95.2 93.5 95.1 93.7OG 33.3 66.7 66.7 80.0OP 54.2 80.8 54.2 79.9PD 77.1 76.8 77.1 76.8SB 91.0 90.6 90.7 93.7Table 7: Improvements on grammatical functionsin the relabeled test set9 ConclusionWe presented a sequence of modifications to adata-driven dependency parser of German, depart-ing from a state-of-the-art set-up in an imple-mentation that allows for fast and robust train-ing and application.
Our pilot study tested whatcan be achieved in a few weeks if the data-driventechnique is combined with a linguistically in-formed approach, i.e., testing hypotheses of whatshould be particularly effective in a very targetedway.
Most modifications were relatively small,addressing very different dimensions in the sys-tem, such as the handling of features in the Ma-chine Learning, the quality and combination ofautomatically assigned features and the ability totake into account global constraints, as well as thecombination of different parsing strategies.
Over-all, labeled accuracy on a standard test set (fromthe CoNLL 2009 shared task), ignoring gold stan-dard part-of-speech tags, increased significantlyfrom 87.64% (baseline parser without hash ker-nel) to 89.40%.10 We take this to indicate that atargeted and informed approach like the one wetested can have surprising effects even for a lan-guage that has received relatively intense consid-eration in the parsing literature.AcknowledgementsWe would like to thank Sandra Ku?bler, YannickVersley and Yi Zhang for their support.
This workwas partially supported by research grants fromthe Deutsche Forschungsgemeinschaft as part ofSFB 632 ?Information Structure?
at the Univer-sity of Potsdam and SFB 732 ?Incremental Speci-fication in Context?
at the University of Stuttgart.ReferencesAttardi, G. 2006.
Experiments with a Multilanguage Non-Projective Dependency Parser.
In Proceedings of CoNLL,pages 166?170.Blum, A.
2006.
Random Projection, Margins, Kernels, andFeature-Selection.
In LNCS, pages 52?68.
Springer.Bohnet, B.
2009.
Efficient Parsing of Syntactic and Se-mantic Dependency Structures.
In Proceedings of CoNLL2009).Brants, Sabine, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER treebank.In Proceedings of the Workshop on Treebanks and Lin-guistic Theories, Sozopol.Buchholz, Sabine and Erwin Marsi.
2006.
CoNLL-X SharedTask on Multilingual Dependency Parsing.
In In Proc.
ofCoNLL, pages 149?164.Carreras, X.
2007.
Experiments with a Higher-order Projec-tive Dependency Parser.
In EMNLP/CoNLL.10?= 0.01, measured with a tool by Dan Bikel fromwww.cis.upenn.edu/?
dbikel/download/compare.pl1129Crammer, K., O. Dekel, S. Shalev-Shwartz, and Y. Singer.2006.
Online Passive-Aggressive Algorithms.
Journal ofMachine Learning Research, 7:551?585.Duchier, Denys and Ralph Debusmann.
2001.
Topologi-cal dependency trees: a constraint-based account of linearprecedence.
In Proceedings of ACL 2001, pages 180?187, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Eisner, J.
1996.
Three New Probabilistic Models for Depen-dency Parsing: An Exploration.
In Proceedings of Coling1996, pages 340?345, Copenhaen.Eisner, J., 2000.
Bilexical Grammars and their Cubic-timeParsing Algorithms, pages 29?62.
Kluwer AcademicPublishers.Hajic?, J., M. Ciaramita, R. Johansson, D. Kawahara,M.
Anto`nia Mart?
?, L. Ma`rquez, A. Meyers, J. Nivre,S.
Pado?, J.
S?te?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue,and Y. Zhang.
2009.
The CoNLL-2009 Shared Task:Syntactic and Semantic Dependencies in Multiple Lan-guages.
In Proceedings of the 13th CoNLL-2009, June4-5, Boulder, Colorado, USA.Hall, Johan and Joakim Nivre.
2008.
A dependency-drivenparser for German dependency and constituency represen-tations.
In Proceedings of the Workshop on Parsing Ger-man, pages 47?54, Columbus, Ohio, June.
Association forComputational Linguistics.Hinrichs, Erhard, Sandra Ku?bler, Karin Naumann, HeikeTelljohann, and Julia Trushkina.
2004.
Recent develop-ments in linguistic annotations of the tu?ba-d/z treebank.In Proceedings of the Third Workshop on Treebanks andLinguistic Theories, pages 51?62, Tu?bingen, Germany.Johansson, R. and P. Nugues.
2008.
Dependency-basedSyntactic?Semantic Analysis with PropBank and Nom-Bank.
In Proceedings of the Shared Task Session ofCoNLL-2008, Manchester, UK.Jurish, Bryan.
2003.
A hybrid approach to part-of-speechtagging.
Technical report, Berlin-BrandenburgischeAkademie der Wissenschaften.Kazama, Jun?Ichi and Jun?Ichi Tsujii.
2005.
Maximum en-tropy models with inequality constraints: A case study ontext categorization.
Machine Learning, 60(1):159?194.Ku?bler, Sandra.
2008.
The PaGe 2008 shared task on pars-ing german.
In Proceedings of the Workshop on ParsingGerman, pages 55?63, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.Magerman, David M. 1995.
Statistical decision-tree mod-els for parsing.
In Proceedings of ACL 1995, pages 276?283, Morristown, NJ, USA.
Association for Computa-tional Linguistics Morristown, NJ, USA.McDonald, R. and F. Pereira.
2006.
Online Learning of Ap-proximate Dependency Parsing Algorithms.
In In Proc.of EACL, pages 81?88.Menzel, Wolfgang and Ingo Schro?der.
1998.
Decision pro-cedures for dependency parsing using graded constraints.In Proceedings of the COLING-ACL ?98 Workshop onProcessing of Dependency-Based Grammars, pages 78?87.Nilsson, Jens and Joakim Nivre.
2005.
Pseudo-projectivedependency parsing.
In Proceedings of ACL 2005, pages99?106.Nivre, Joakim and Johan Hall.
2008.
A dependency-drivenparser for German dependency and constituency represen-tations.
In Proceedings of the ACL Workshop on ParsingGerman.Nivre, J. and R. McDonald.
2008.
Integrating Graph-Basedand Transition-Based Dependency Parsers.
In ACL-08,pages 950?958, Columbus, Ohio.Nivre, Joakim, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, andSvetoslav Marinov.
2006.
Labeled pseudo-projective de-pendency parsing with Support Vector Machines.
In Pro-ceedings of CoNLL 2006.Nivre, J.
2003.
An Efficient Algorithm for Projective De-pendency Parsing.
In 8th International Workshop onParsing Technologies, pages 149?160, Nancy, France.Nivre, J.
2009.
Non-Projective Dependency Parsing in Ex-pected Linear Time.
In Proceedings of the 47th AnnualMeeting of the ACL and the 4th IJCNLP of the AFNLP,pages 351?359, Suntec, Singapore.Rahimi, A. and B. Recht.
2008.
Random Features forLarge-Scale Kernel Machines.
In Platt, J.C., D. Koller,Y.
Singer, and S. Roweis, editors, Advances in NeuralInformation Processing Systems, volume 20.
MIT Press,Cambridge, MA.Ratnaparkhi, Adwait.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of EMNLP 1996,volume 1, pages 133?142.Schiller, Anne, Simone Teufel, and Christine Sto?ckert.
1999.Guidelines fu?r das Tagging deutscher Textcorpora mitSTTS (Kleines und gro?es Tagset).
Technical Report Au-gust, Universita?t Stuttgart.Schmid, Helmut.
1995.
Improvements in part-of-speech tag-ging with an application to German.
In Proceedings of theACL SIGDAT-Workshop, volume 11.Seeker, Wolfgang, Ines Rehbein, Jonas Kuhn, and Josef VanGenabith.
2010.
Hard Constraints for Grammatical Func-tion Labelling.
In Proceedings of ACL 2010, Uppsala.Titov, I. and J. Henderson.
2007.
A Latent Variable Modelfor Generative Dependency Parsing.
In Proceedings ofIWPT, pages 144?155.Yamada, H. and Y. Matsumoto.
2003.
Statistical Depen-dency Analysis with Support Vector Machines.
In Pro-ceedings of IWPT, pages 195?206.1130
