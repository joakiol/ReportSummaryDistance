Dependency Based Logical Form TransformationsStephen Anthony and Jon PatrickSchool of Information TechnologiesThe University of SydneySydney, Australia 2006{stephen,jonpat}@it.usyd.edu.auAbstractThis paper describes a system developed for thetransformation of English sentences into a firstorder logical form representation.
The metho d-ology is centered on the use of a dependencygrammar based parser .
We demonstrate the suit-ability of applying a dependency parser basedsolution to the given task and in turn explainsome of the limitations and challenges involvedwhen using such an approach.
The efficienciesand deficiencies of our approach are discussedas well as considerations for further enhanc e-ments.1 IntroductionIn addition to the well-known all words and lexi-cal sample tasks deployed in previous Sensevalworkshops a number of new tasks have been in-cluded in this sense evaluation.
These new tasks in-clude identification of semantic roles as in FrameNet(Gildea and Jurafsky 2002), disambiguation ofWordNet glosses (Miller 1990; Fellbaum 1998;Harabagiu, Miller et al 1999), automatic acquisitio nof subcategorisation frames (Korhonen 2002; Preissand Korhonen 2002), and Logical Form Identific a-tion (LFI) (Rus 2002; Rus and Moldovan 2002).This paper discusses a solution developed for theLFI task.
The approach used here employs a func-tional dependency parser (J?rvinen and Tapanainen1997; Tapanainen and J?rvinen 1997) and uses alimited number  of additional resources.
This contri-bution is intended to demonstrate the suitability of adependency parser to the given task and also explainsome of the limitations and challenges  involvedwhen using such an approach.1.1 MotivationPart of the initial step towards the interpretation ofa sentence as postulated by Hobbs et al (1993) in-volves the proof of the logical form of a sentence.This statement entails the transformation of a sen-tence into a logical form as a fundamental buildingblock towards sentence interpretation.Advantages specifically related to the utilis ationof logical forms in language processing include asimplified interface between syntax and semantics, anatural and easily exploitable representation of syn-tactic arguments, and the potential for formation ofconceptual predicates (Rus 2002) if predicates aredisambiguated with respect to a general ontologysuch as WordNet.1.2 Task DescriptionThe Logical Form (LF) employed in this task is aflat, scope-free first order logic representation thatembeds lexical and syntactic information.
A predi-cate is generated for every nominal, verbal, adjecti-val, and adverbial content word.
The name of thepredicate is a concatenation of the lemmatised wordform and part-of-speech category.
The sentence be-low is followed by its corresponding target logicalform representation.Some students like to study in the mornings.student:n_ (x3) like:v_ (e4, x3, e6) to (e5, e6)study:v_ (e6, x3, x9) in (e7, x9) morning:n_ (x9).Relationships between predicates are sharedthrough their arguments.
The two types of argu-ments used are events (e) and entities (x).
Using thetransformation shown above as an example, theevent predicate ?like?
is labeled as e4 and has sub jectargument x3 which corresponds to ?student?
andgrammatical object argument e6 which correspondsto the ?study?
event predicate.The remainder of the argument slots are reservedfor indirect and prepositional objects.
Determiners,plurals, negation, auxiliaries, verb tenses, and punc-tuation are excluded from the final representation.2 MethodologyThe system is built using a highly modular designand is intended to be as generic and reusable as pos-sible.
The basic data structure is a flat list-like repre-sentation with generic property slots attached to eachelement.
This structure maximises compatibilitywith the final representation and allows for greaterflexibility in the types of information that may beAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsassociated with each predicate.
Figure 1 illustratesthe major proces s ing modules available and thework flow.Figure 1: Logical form identification work flowA syntactic parse including functional dependen-cies is produced on a per sentence basis.
Definitionsof the properties associated with each token are pre-sented in Table 1.Attribute ValueWord ID Integer sentence positionHead ID Integer position of head dependencyText The original word formLemma Lemmatised word formMorpho Morphological function tags.
Partsof speech and sub-featuresSyntax Surface syntactic tagsDepend Dependency functionsMAIN Main elementSUBJ Position of syntactic subjectOBJ Syntactic object positionI-OBJ Indirect object positionCOMP Position of syntactic complementPCOMP Prepositional complement positionDET Determiner dependentATTR Attributive nominalCC Coordinating conjunctionGOAL Position of goalOC Object complementTable 1: Linguistic information stored for eachtokenThe resultant parse is transformed into a lineardata structure indexed by word position.
This is il-lustrated in Table 2 using the example sentence?Some students like to study in the mornings?.
Theoriginal token text is stored, as is the lemmatisedform.WordIDLemmaHeadIDDependTextMorphoSyntax2 some 3  det Some DET >N3  stu-dent4  subj stu-dentsN NOMPLNH4  like 1 main  like V PRES VA5  to 6  pm  to INFMARKAUX6 study 4  obj study V INF VA7  in 6  tmp  in PREP EH8  the 9  det  the DET >N9 morning7 pcomp morn-ingsN NOMPLNHTable 2: Example syntactic parseHead and dependency type are the most importantclass of information used by the system.
The de-pendency type and head of the token is often di-rectly, if not indirectly, translatable into a predicateargument.
Examples of the types of dependencyfunctions employed include subject, object, preposi-tional complement, agent, subject and object com-plements, indirect object, goal, and coordinatingconjunctions.
Determiner and negator functions arealso of interest because they are excluded from thefinal represent ation.The filter module moderates the presence or ab-sence of tokens using stop lists or pass lists or acombination of both.
Stop lists are used to specifycontent to be excluded from the token stream andpass lists specify elements that should remain.
To-kens may be filtered from the stream based on anyattribute type and value listed in Table 1.
This in-formation is provided in the filter set.
The principaltypes of information filtered in this system are de-terminers based on morpholog ical tags and auxilia-ries based on syntactic tag information.
For example?some?
and ?the?
are filtered as a consequence of amorpho property equals ?DET?
stop list rule .When the token stream has been annotated withthe necessary information and has passed throughthe filter, the tokens that remain are passed throughthe logical form processor (LFP).
The main functionof the LFP is to build an inverted index identifyingall dependent tokens.
Once grammatical dependen-cies are assigned and the inverted index is built thelogical form representation may be constructed.Each predicate is constructed from the token streamin turn based on the part-of-speech category of thetoken.
The base form of the token is concatenatedwith the part-of-speech tag.
A mapping table is usedto transform the part-of-speech information pro-FiltersLogical FormprocessorFunctionaldependencyparserNatural lan-guage sen-tencesTarget logicalformFiltersetduced by the parse into the coarser grained WordNettags.Entities are the simplest type of predicate to con-struct as they contain only a single argument, forwhich the word identifier attribute value is used.Noun tokens ?student?
and ?morning?
from the ex-ample are transformed into the predicatesstudent:n_(x3) and morning:n_(x9).
Pronouns,prepositional complements, and coordinating con-junctions are dealt with individually using their re-spective dependency function values.Adjectives are constructed using the head depend-ency value as the argument unless the dependent ismarked with a subject.
In this case the argument be-comes the head of the subject.
Adverbs are createdprimarily using the dependency function alone.Verbal predicates are constructed using SUBJ,OBJ, GOAL, OC, I-OBJ, COMP, and PCOMP de-pendencies in the specified order.
A special caseexists for verbs that have object complement de-pendencies.
In these cases attributive nominals areidentified and assigned as arguments independently.The main verb ?like?
in our example is trans-formed into the pred icate like:v_(e4, x3, e6) as aresult of subject (SUBJ) and object (OBJ) dependen-cies found in ?student?
and ?study?
respectively.Given the fact that we are dealing with the mainverb, the LFP inverts the subject and object depend-encies, inserts them into the head verb token prop-erty slot and assigns their respective word identifiervalues.
The inverted properties augment the tokenslot for ?like?
which has word identifier four in Table2.
The additional elements of the inverted index usedto build the predicate are listed in Table 3.Attribute ValueOBJ 6SUBJ 3depend mainhead 1lemma likemorpho V PRESsyntax VAtext likeTable 3: Augmented token slot for ?like?Verbal predicates which also serve as grammaticalobjects also warrant special treatment.
The token?study?
is an example of this as it serves as the objectof the head verb ?like?.
A cache is used to store thesentential head, prepositional complements, subjects,and coordinating conjunctions.
The cache is used inthis instance to assign the subject and prepositionalcomplement arguments in order to form the predi-cate study:v_(e6, x3, x9).
Notice from Table 2 wordidentifier three matches the grammatical subject to-ken ?students?
and word identifier nine matches thehead of the prepositional phrase ?
in the mornings?.Once all tokens are processed the logical formtransformation is complete and the final representa-tion is presented in the aforementioned notation.3 EvaluationArgument, predicate, and sentence level precisionand recall measures are used to evaluate perform-ance of the system as compared to a gold-standard.The system was trained on a set of 50 sentences withcorresponding logical forms.
Final testing was per-formed on a set of 300 LF-sentence pairs.3.1 Argument LevelPrecision at the argument level is defined to be thenumber of correctly identified arguments divided bythe number of all identified arguments.
Recall is de-fined to be the number of correctly identified argu-ments divided by the real number of arguments thatshould be present in the target transformation.3.2 Predicate LevelPredicates must identify all arguments correctly tobe counted as a correct predicate.
Precision is de-fined to be the number of correctly ident ified predi-cates divided by the number of all attemptedpredicates.
Recall is defined as the number of cor-rectly identified predicates divided by the real num-ber of predicates that were supposed to be identifiedin the target transformation.3.3 Sentence LevelVarious other sentence level measures are alsoused.
Sentence-argument is defined as the number ofsentences that have all arguments correctly identi-fied divided by the number of sentences attempted.Sentence-predicate is similar except conditioned onpredicates.
Sentence-argument-predicate is definedto be the number of sentences that have all argu-ments correctly identified divided by the number ofsentences which have all predicates correctly identi-fied.
Sentence-argument-predicate-sentences refersto the number of sentences that have all argumentsand all pred icates correctly identified divided by thenumber of sentences attempted.4 ResultsAs stated earlier the final evaluation was con-ducted on a set of 300 sentence-LF pairs.
Table 4lists the evaluation precision and recall results usingthe measures discussed in section 3 which have beenconverted into percentages.Evaluation Measure ScoreArgument Precision 76.4Argument Recall 65.6Predicate Precision 84.0Predicate Recall 85.0Sentence-Argument 16.0Sentence-Predicate 35.3Sentence-Argument-Predicate 38.7Sentence-Argument-Predicate-Sentences  13.7Table 4: Evaluation results as percentagesThe major source of error in terms of argumentsoriginated from the parser?s inappropriate handlingof coordinating conjunctions.
Another commonsource of error arose from poor handling of nominalgroup complexes.
With regard to predicate perform-ance, the decision to forfeit the use of the availablemulti-word item list proved costly.5 Future WorkHarabagiu et al (1999) proposed a scheme for at-taching sense tags to predicates within the frame-work of transforming WordNet glosses into a logicalform.
In this way conceptual predicates may beformed to manipulate a meaning representation inmore significant ways.
Naturally the sense inventorymust be sensitive enough to allow for a meanin gfuland representative mutation to be applied to themeaning representation.6 ConclusionsDependency grammars provide a natural and in-tuitive solution to the task of logical form identific a-tion.
We have managed to demonstrate relativelygood overall performance on the given task withminimal additional processing and a very smallamount of training data.It is argued that a dependency grammar basedparse provides a rich source of knowledge that issuitable for the transformation of English sentencesinto a logical form.
It would appear that there is to alarge extent enough information embedded withinthe parser?s output to achieve the desired outcome.
Itis however apparent that other types of informationcould further improve the solution.
These types ofinformation include named entity recognition andmulti-word phrase detection.ReferencesFellbaum, C. (1998).
WordNet : An Electronic Lexi-cal Database.
Cambridge, Massachusetts;London, MIT Press.Gildea, D. and D. Jurafsky (2002).
"Automatic La-beling of Semantic Roles."
ComputationalLinguistics 28(3): 245-288.Harabagiu, S. M., G. A. Miller, et al (1999).
Word-Net 2 - A Morphologically and SemanticallyEnhanced Resource.
SIGLEX.Hobbs, J. R., M. E. Stickel, et al (1993).
"Interpreta-tion as Abduction."
Artificial Intelligence63: 69-142.J?rvinen, T. and P. Tapanainen (1997).
DependencyParser for English.
Helsinki, University ofHelsinki, Department of General Linguis-tics.Korhonen, A.
(2002).
Subcategorization Acquisition.Ph.D.
Dissertation.
Computer Laboratory,University of Cambridge.Miller, G. (1990).
"WordNet: An Online LexicalDatabase."
International Journal of Lexi-cography 3(4).Preiss, J. and A. Korhonen (2002).
Improving Sub-categorization Acquisition with WSD .
InProceedings of the ACL Workshop on WordSense Disambiguation: Recent Successesand Future Directions.Rus, V. (2002).
Logic Form For WordNet Glosses.Ph.D.
Dissertation.
Computer Science De-partment,  School of Engineering, SouthernMethodist University.Rus, V. and D. I. Moldovan (2002).
"High PrecisionLogic Form Transformation."
InternationalJournal on Artificial Intelligence Tools11(3).Tapanainen, P. and T. J?rvinen (1997).
A Non-Projective Dependency Parser.
In Proceed-ings of the 5th Conference on Applied Natu-ral Language Processing, Association forComputational Linguistics, WashingtonD.C.
