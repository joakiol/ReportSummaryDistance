Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 890?899,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPUsing the Web for Language Independent Spellchecking andAutocorrectionCasey Whitelaw and Ben Hutchinson and Grace Y Chung and Gerard EllisGoogle Inc.Level 5, 48 Pirrama Rd, Pyrmont NSW 2009, Australia{whitelaw,benhutch,gracec,ged}@google.comAbstractWe have designed, implemented and eval-uated an end-to-end system spellcheck-ing and autocorrection system that doesnot require any manually annotated train-ing data.
The World Wide Web is usedas a large noisy corpus from which weinfer knowledge about misspellings andword usage.
This is used to build an er-ror model and an n-gram language model.A small secondary set of news texts withartificially inserted misspellings are usedto tune confidence classifiers.
Becauseno manual annotation is required, our sys-tem can easily be instantiated for new lan-guages.
When evaluated on human typeddata with real misspellings in English andGerman, our web-based systems outper-form baselines which use candidate cor-rections based on hand-curated dictionar-ies.
Our system achieves 3.8% total errorrate in English.
We show similar improve-ments in preliminary results on artificialdata for Russian and Arabic.1 IntroductionSpellchecking is the task of predicting whichwords in a document are misspelled.
These pre-dictions might be presented to a user by under-lining the misspelled words.
Correction is thetask of substituting the well-spelled hypothesesfor misspellings.
Spellchecking and autocorrec-tion are widely applicable for tasks such as word-processing and postprocessing Optical CharacterRecognition.
We have designed, implementedand evaluated an end-to-end system that performsspellchecking and autocorrection.The key novelty of our work is that the sys-tem was developed entirely without the use ofmanually annotated resources or any explicitlycompiled dictionaries of well-spelled words.
Ourmulti-stage system integrates knowledge from sta-tistical error models and language models (LMs)with a statistical machine learning classifier.
Ateach stage, data are required for training modelsand determining weights on the classifiers.
Themodels and classifiers are all automatically trainedfrom frequency counts derived from the Web andfrom news data.
System performance has beenvalidated on a set of human typed data.
We havealso shown that the system can be rapidly portedacross languages with very little manual effort.Most spelling systems today require some hand-crafted language-specific resources, such as lex-ica, lists of misspellings, or rule bases.
Sys-tems using statistical models require large anno-tated corpora of spelling errors for training.
Ourstatistical models require no annotated data.
In-stead, we rely on the Web as a large noisy corpusin the following ways.
1) We infer informationabout misspellings from term usage observed onthe Web, and use this to build an error model.
2)The most frequently observed terms are taken asa noisy list of potential candidate corrections.
3)Token n-grams are used to build an LM, whichwe use to make context-appropriate corrections.Because our error model is based on scoring sub-strings, there is no fixed lexicon of well-spelledwords to determine misspellings.
Hence, bothnovel misspelled or well-spelled words are allow-able.
Moreover, in combination with an n-gramLM component, our system can detect and correctreal-word substitutions, ie, word usage and gram-matical errors.Confidence classifiers determine the thresholdsfor spelling error detection and autocorrection,given error and LM scores.
In order to train theseclassifiers, we require some textual content withsome misspellings and corresponding well-spelledwords.
A small subset of the Web data from newspages are used because we assume they contain890relatively few misspellings.
We show that con-fidence classifiers can be adequately trained andtuned without real-world spelling errors, but ratherwith clean news data injected with artificial mis-spellings.This paper will proceed as follows.
In Section 2,we survey related prior research.
Section 3 de-scribes our approach, and how we use data at eachstage of the spelling system.
In experiments (Sec-tion 4), we first verify our system on data with ar-tificial misspellings.
Then we report performanceon data with real typing errors in English and Ger-man.
We also show preliminary results from port-ing our system to Russian and Arabic.2 Related WorkSpellchecking and correction are among the oldesttext processing problems, and many different so-lutions have been proposed (Kukich, 1992).
Mostapproaches are based upon the use of one or moremanually compiled resources.
Like most areasof natural language processing, spelling systemshave been increasingly empirical, a trend that oursystem continues.The most direct approach is to model thecauses of spelling errors directly, and encode themin an algorithm or an error model.
Damerau-Levenshtein edit distance was introduced as away to detect spelling errors (Damerau, 1964).Phonetic indexing algorithms such as Metaphone,used by GNU Aspell (Atkinson, 2009), repesentwords by their approximate ?soundslike?
pronun-ciation, and allow correction of words that ap-pear orthographically dissimilar.
Metaphone reliesupon data files containing phonetic information.Linguistic intuition about the different causes ofspelling errors can also be represented explicitly inthe spelling system (Deorowicz and Ciura, 2005).Almost every spelling system to date makes useof a lexicon: a list of terms which are treated as?well-spelled?.
Lexicons are used as a source ofcorrections, and also to filter words that shouldbe ignored by the system.
Using lexicons in-troduces the distinction between ?non-word?
and?real-word?
errors, where the misspelled word isanother word in the lexicon.
This has led tothe two sub-tasks being approached separately(Golding and Schabes, 1996).
Lexicon-based ap-proaches have trouble handling terms that do notappear in the lexicon, such as proper nouns, for-eign terms, and neologisms, which can account fora large proportion of ?non-dictionary?
terms (Ah-mad and Kondrak, 2005).A word?s context provides useful evidence asto its correctness.
Contextual information can berepresented by rules (Mangu and Brill, 1997) ormore commonly in an n-gram LM.
Mays et al(1991) used a trigram LM and a lexicon, whichwas shown to be competitive despite only allow-ing for a single correction per sentence (Wilcox-O?Hearn et al, 2008).
Cucerzan and Brill (2004)claim that an LM is much more important thanthe channel model when correcting Web searchqueries.
In place of an error-free corpus, the Webhas been successfully used to correct real-worderrors using bigram features (Lapata and Keller,2004).
This work uses pre-defined confusion sets.The largest step towards an automatically train-able spelling system was the statistical model forspelling errors (Brill and Moore, 2000).
This re-places intuition or linguistic knowledge with atraining corpus of misspelling errors, which wascompiled by hand.
This approach has also beenextended to incorporate a pronunciation model(Toutanova and Moore, 2002).There has been recent attention on using Websearch query data as a source of training data, andas a target for spelling correction (Yang Zhang andLi, 2007; Cucerzan and Brill, 2004).
While querydata is a rich source of misspelling information inthe form of query-revision pairs, it is not availablefor general use, and is not used in our approach.The dependence upon manual resources hascreated a bottleneck in the development ofspelling systems.
There have been few language-independent, multi-lingual systems, or even sys-tems for languages other than English.
Language-independent systems have been evaluated on Per-sian (Barari and QasemiZadeh, 2005) and on Ara-bic and English (Hassan et al, 2008).
To ourknowledge, there are no previous evaluations ofa language-independent system across many lan-guages, for the full spelling correction task, andindeed, there are no pre-existing standard test setsfor typed data with real errors and language con-text.3 ApproachOur spelling system follows a noisy channelmodel of spelling errors (Kernighan et al, 1990).For an observed word w and a candidate correc-tion s, we compute P (s|w) as P (w|s)?
P (s).891confidenceclassifiersWebNews datawith artificialmisspellingscorrected textinput textlanguage modelerror modelterm listscoredsuggestionsFigure 1: Spelling process, and knowledge sources used.The text processing workflow and the data usedin building the system are outlined in Figure 1 anddetailed in this section.
For each token in the in-put text, candidate suggestions are drawn from theterm list (Section 3.1), and scored using an errormodel (Section 3.2).
These candidates are eval-uated in context using an LM (Section 3.3) andre-ranked.
For each token, we use classifiers (Sec-tion 3.4) to determine our confidence in whethera word has been misspelled and if so, whether itshould be autocorrected to the best-scoring sug-gestion available.3.1 Term ListWe require a list of terms to use as candidate cor-rections.
Rather than attempt to build a lexiconof words that are well-spelled, we instead take themost frequent tokens observed on the Web.
Weused a large (> 1 billion) sample of Web pages,tokenized them, and took the most frequently oc-curring ten million tokens, with very simple filtersfor non-words (too much punctuation, too short orlong).
This term list is so large that it should con-tain most well-spelled words, but also a large num-ber of non-words or misspellings.3.2 Error ModelWe use a substring error model to estimateP (w|s).
To derive the error model, let R bea partitioning of s into adjacent substrings, andsimilarly let T be a partitioning of w, such that|T | = |R|.
The partitions are thus in one-to-onealignment, and by allowing partitions to be empty,the alignment models insertions and deletions ofsubstrings.
Brill and Moore estimate P (w|s) asfollows:P (w|s) ?
maxR, T s.t.
|T |=|R||R|?i=1P (Ti|Ri) (1)Our system restricts partitionings that have sub-strings of length at most 2.To train the error model, we require triples of(intended word, observed word, count), which aredescribed below.
We use maximum likelihood es-timates of P (Ti|Ri).3.2.1 Using the Web to Infer MisspellingsTo build the error model, we require as train-ing data a set of (intended word, observed word,count) triples, which is compiled from the WorldWide Web.
Essentially the triples are built by start-ing with the term list, and a process that auto-matically discovers, from that list, putative pairsof spelled and misspelled words, along with theircounts.We believe the Web is ideal for compiling thisset of triples because with a vast amount of user-generated content, we believe that the Web con-tains a representative sample of both well-spelledand misspelled text.
The triples are not used di-rectly for proposing corrections, and since we havea substring model, they do not need to be an ex-haustive list of spelling mistakes.The procedure for finding and updating countsfor these triples also assumes that 1) misspellingstend to be orthographically similar to the intendedword; Mays et al(1991) observed that 80% of892misspellings derived from single instances of in-sertion, deletion, or substitution; and 2) words areusually spelled as intended.For the error model, we use a large corpus (up to3.7?108pages) of crawled public Web pages.
Anautomatic language-identification system is usedto identify and filter pages for the desired lan-guage.
As we only require a small window of con-text, it would also be possible to use an n-gramcollection such as the Google Web 1T dataset.Finding Close Words.
For each term in theterm list (defined in Section 3.1), we find allother terms in the list that are ?close?
to it.
Wedefine closeness using Levenshtein-Damerau editdistance, with a conservative upper bound that in-creases with word length (one edit for words ofup to four characters, two edits for up to twelvecharacters, and three for longer words).
We com-pile the term list into a trie-based data structurewhich allows for efficient searching for all termswithin a maximum edit distance.
The computa-tion is ?embarassingly parallel?
and hence easilydistributable.
In practice, we find that this stagetakes tens to hundreds of CPU-hours.Filtering Triples.
At this stage, for eachterm we have a cluster of orthographically similarterms, which we posit are potential misspellings.The set of pairs is reflexive and symmetric, e.g.
itcontains both (recieve, receive) and (receive, re-cieve).
The pairs will also include e.g.
(deceive,receive).
On the assumption that words are spelledcorrectly more often than they are misspelled, wenext filter the set such that the first term?s fre-quency is at least 10 times that of the second term.This ratio was chosen as a conservative heuristicfilter.Using Language Context.
Finally, we use thecontexts in which a term occurs to gather direc-tional weightings for misspellings.
Consider aterm w; from our source corpus, we collect theset of contexts {ci} in which w occurs.
The defi-nition of a context is relatively arbitrary; we choseto use a single word on each side, discarding con-texts with fewer than a total of ten observed occur-rences.
For each context ci, candidate ?intended?terms arew andw?s close terms (which are at least10 times as frequent as w).
The candidate whichappears in context cithe most number of times isdeemed to be the term intended by the user in thatcontext.The resulting dataset consists of triples of theoriginal observed term, one of the ?intended?terms as determined by the above algorithm, andthe number of times this term was intended.
Fora single term, it is possible (and common) to havemultiple possible triples, due to the context-basedassignment.Inspecting the output of this training processshows some interesting patterns.
Overall, thedataset is still noisy; there are many instanceswhere an obviously misspelled word is not as-signed a correction, or only some of its instancesare.
The dataset contains around 100 milliontriples, orders of magnitude larger than any man-ually compiled list of misspellings .
The kinds oferrors captured in the dataset include stereotypi-cal spelling errors, such as acomodation, but alsoOCR-style errors.
computationaUy was detectedas a misspelling of computationally where the ?U?is an OCR error for ?ll?
; similarly, Postmodem wasdetected as a misspelling of Postmodern (an exam-ple of ?keming?
).The data also includes examples of ?real-word?errors.
For example, 13% of occurrences ofoccidental are considered misspellings of acci-dental; contrasting with 89% of occurrences ofthe non-word accidential.
There are many ex-amples of terms that would not be in a normallexicon, including neologisms (mulitplayer formultiplayer), companies and products (Playsta-ton for Playstation), proper nouns (Schwarzneggerfor Schwarzenegger) and internet domain names(mysapce.com for myspace.com).3.3 Language ModelWe estimate P (s) using n-gram LMs trained ondata from the Web, using Stupid Backoff (Brantset al, 2007).
We use both forward and back-ward context, when available.
Contrary to Brilland Moore (2000), we observe that user edits of-ten have both left and right context, when editinga document.When combining the error model scores withthe LM scores, we weight the latter by taking their?
?th power, that isP (w|s) ?
P (s)?
(2)The parameter ?
reflects the relative degrees towhich the LM and the error model should betrusted.
The parameter ?
also plays the additionalrole of correcting our error model?s misestimationof the rate at which people make errors.
For exam-ple, if errors are common then by increasing ?
we893can reduce the value of P (w|w) ?
P (w)?relativeto?s 6=wP (s|w) ?
P (s)?.We train ?
by optimizing the average inverserank of the correct word on our training corpus,where the rank is calculated over all suggestionsthat we have for each token.During initial experimentation, it was noticedthat our system predicted many spurious autocor-rections at the beginnings and ends of sentences(or in the case of sentence fragments, the end ofthe fragment).
We hypothesized that we wereweighting the LM scores too highly in such cases.We therefore conditioned ?
on how much contextwas available, obtaining values ?i,jwhere i, j rep-resent the amount of context available to the LMto the left and right of the current word.
i and j arecapped at n, the order of the LM.While conditioning ?
in this way might at firstappear ad hoc, it has a natural interpretation interms of our confidence in the LM.
When there isno context to either side of a word, the LM simplyuses unigram probabilities, and this is a less trust-worthy signal than when more context is available.To train ?i,jwe partition our data into bins cor-responding to pairs i, j and optimize each ?i,jin-dependently.Training a constant ?, a value of 5.77 was ob-tained.
The conditioned weights ?i,jincreasedwith the values of i and j, ranging from ?0,0=0.82 to ?4,4= 6.89.
This confirmed our hypoth-esis that the greater the available context the moreconfident our system should be in using the LMscores.3.4 Confidence Classifiers for Checking andCorrectionSpellchecking and autocorrection were imple-mented as a three stage process.
These em-ploy confidence classifiers whereby precision-recall tradeoffs could be tuned to desirable levelsfor both spellchecking and autocorrection.First, all suggestions s for a word w are rankedaccording to their P (s|w) scores.
Second, aspellchecking classifier is used to predict whetherw is misspelled.
Third, if w is both predicted to bemisspelled and s is non-empty, an autocorrectionclassifier is used to predict whether the top-rankedsuggestion is correct.The spellchecking classifier is implemented us-ing two embedded classifiers, one of which is usedwhen s is empty, and the other when it is non-empty.
This design was chosen because the use-ful signals for predicting whether a word is mis-spelled might be quite different when there are nosuggestions available, and because certain featuresare only applicable when there are suggestions.Our experiments will compare two classifiertypes.
Both rely on training data to determinethreshold values and training weights.A ?simple?
classifier which compares the valueof log(P (s|w)) ?
log(P (w|w)), for the originalword w and the top-ranked suggestion s, with athreshold value.
If there are no suggestions otherthan w, then the log(P (s|w)) term is ignored.A logistic regression classifier that uses fivefeature sets.
The first set is a scores featurethat combines the following scoring information(i) log(P (s|w)) ?
log(P (w|w)) for top-rankedsuggestion s. (ii) LM score difference betweenthe original word w and the top suggestion s.(iii) log(P (s|w)) ?
log(P (w|w)) for second top-ranked suggestion s. (iv) LM score difference be-tween w and second top-ranked s. The other fourfeature sets encode information about case signa-tures, number of suggestions available, the tokenlength, and the amount of left and right context.Certain categories of tokens are blacklisted, andso never predicted to be misspelled.
These arenumbers, punctuation and symbols, and single-character tokens.The training process has three stages.
(1) Thecontext score weighting is trained, as describedin Section 3.3.
(2) The spellchecking classifier istrained, and tuned on held-out development data.
(3) The autocorrection classifier is trained on theinstances with suggestions that the spellcheckingclassifier predicts to be misspelled, and it too istuned on held-out development data.In the experiments reported in this paper, wetrained classifiers so as to maximize the F1-scoreon the development data.
We note that the desiredbehaviour of the spellchecking and autocorrectionclassifiers will differ depending upon the applica-tion, and that it is a strength of our system thatthese can be tuned independently.3.4.1 Training Using Artificial DataTraining and tuning the confidence classifiers re-quire supervised data, in the form of pairs of mis-spelled and well-spelled documents.
And indeedwe posit that relatively noiseless data are neededto train robust classifiers.
Since these data are894Language SentencesTrain TestEnglish 116k 58kGerman 87k 44kArabic 8k 4kRussian 8k 4kTable 1: Artificial data set sizes.
The developmentset is approximately the same size as the trainingset.not generally available, we instead use a cleancorpus into which we artificially introduce mis-spellings.
While this data is not ideal, we showthat in practice it is sufficient, and removes theneed for manually-annotated gold-standard data.We chose data from news pages crawled fromthe Web as the original, well-spelled documents.We chose news pages as an easily identifiablesource of text which we assume is almost entirelywell-spelled.
Any source of clean text could beused.
For each language the news data were di-vided into three non-overlapping data sets: thetraining and development sets were used for train-ing and tuning the confidence classifiers, and a testset was used to report evaluation results.
The dataset sizes, for the languages used in this paper, aresummarized in Table 1.Misspelled documents were created by artifi-cially introducing misspelling errors into the well-spelled text.
For all data sets, spelling errorswere randomly inserted at an average rate of 2 perhundred characters, resulting in an average wordmisspelling rate of 9.2%.
With equal likelihood,errors were either character deletions, transposi-tions, or insertions of randomly selected charac-ters from within the same document.4 Experiments4.1 Typed Data with Real ErrorsIn the absence of user data from a real application,we attempted our initial evaluation with typed datavia a data collection process.
Typed data with realerrors produced by humans were collected.
Werecruited subjects from our coworkers, and askedthem to use an online tool customized for datacollection.
Subjects were asked to randomly se-lect a Wikipedia article, copy and paste severaltext-only paragraphs into a form, and retype thoseparagraphs into a subsequent form field.
The sub-jects were asked to pick an article about a favoritecity or town.
The subjects were asked to typeat a normal pace avoiding the use of backspaceor delete buttons.
The data were tokenized, au-tomatically segmented into sentences, and manu-ally preprocessed to remove certain gross typingerrors.
For instance, if the typist omitted entirephrases/sentences by mistake, the sentence was re-moved.
We collected data for English from 25subjects, resulting in a test set of 11.6k tokens, and495 sentences.
There were 1251 misspelled tokens(10.8% misspelling rate.
)Data were collected for German Wikipedia arti-cles.
We asked 5 coworkers who were German na-tive speakers to each select a German article abouta favorite city or town, and use the same onlinetool to input their typing.
For some typists whoused English keyboards, they typed ASCII equiva-lents to non-ASCII characters in the articles.
Thiswas accounted for in the preprocessing of the ar-ticles to prevent misalignment.
Our German testset contains 118 sentences, 2306 tokens with 288misspelled tokens (12.5% misspelling rate.
)4.2 System ConfigurationsWe compare several system configurations to in-vestigate each component?s contribution.4.2.1 Baseline Systems Using AspellSystems 1 to 4 have been implemented as base-lines.
These use GNU Aspell, an open source spellchecker (Atkinson, 2009), as a suggester compo-nent plugged into our system instead of our ownWeb-based suggester.
Thus, with Aspell, the sug-gestions and error scores proposed by the systemwould all derive from Aspell?s handcrafted customdictionary and error model.
(We report results us-ing the best combination of Aspell?s parametersthat we found.
)System 1 uses Aspell tuned with the logisticregression classifier.
System 2 adds a context-weighted LM, as per Section 3.3, and uses the?simple?
classifier described in Section 3.4.
Sys-tem 3 replaces the simple classifier with the logis-tic regression classifier.
System 4 is the same butdoes not perform blacklisting.4.2.2 Systems Using Web-based SuggestionsThe Web-based suggester proposes suggestionsand error scores from among the ten million mostfrequent terms on the Web.
It suggests the 20terms with the highest values of P (w|s) ?
f(s)using the Web-derived error model.895Systems 5 to 8 correspond with Systems 1 to4, but use the Web-based suggestions instead ofAspell.4.3 Evaluation MetricsIn our evaluation, we aimed to select metrics thatwe hypothesize would correlate well with real per-formance in a word-processing application.
Inour intended system, misspelled words are auto-corrected when confidence is high and misspelledwords are flagged when a highly confident sug-gestion is absent.
This could be cast as a simpleclassification or retrieval task (Reynaert, 2008),where traditional measures of precision, recall andF metrics are used.
However we wanted to fo-cus on metrics that reflect the quality of end-to-end behavior, that account for the combined ef-fects of flagging and automatic correction.
Es-sentially, there are three states: a word could beunchanged, flagged or corrected to a suggestedword.
Hence, we report on error rates that mea-sure the errors that a user would encounter if thespellchecking/autocorrection were deployed in aword-processor.
We have identified 5 types of er-rors that a system could produce:1.
E1: A misspelled word is wrongly corrected.2.
E2: A misspelled word is not corrected but isflagged.3.
E3: A misspelled word is not corrected orflagged.4.
E4: A well spelled word is wrongly cor-rected.5.
E5: A well spelled word is wrongly flagged.It can be argued that these errors have varyingimpact on user experience.
For instance, a wellspelled word that is wrongly corrected is morefrustrating than a misspelled word that is not cor-rected but is flagged.
However, in this paper, wetreat each error equally.E1, E2, E3and E4pertain to the correctiontask.
Hence we can define Correction Error Rate(CER):CER =E1+ E2+ E3+ E4Twhere T is the total number of tokens.
E3and E5pertain to the nature of flagging.
We define Flag-ging Error Rate (FER) and Total Error Rate (TER):FER =E3+ E5TTER =E1+ E2+ E3+ E4+ E5TFor each system, we computed a No Good Sugges-tion Rate (NGS) which represents the proportionof misspelled words for which the suggestions listdid not contain the correct word.5 Results and Discussion5.1 Experiments with Artificial ErrorsSystem TER CER FER NGS1.
Aspell, no LM, LR 17.65 6.38 12.35 18.32.
Aspell, LM, Sim 4.82 2.98 2.86 18.33.
Aspell, LM, LR 4.83 2.87 2.84 18.34.
Aspell, LM, LR 22.23 2.79 19.89 16.3(no blacklist)5.
WS, no LM, LR 9.06 7.64 6.09 10.16.
WS, LM, Sim 2.62 2.26 1.43 10.17.
WS, LM, LR 2.55 2.21 1.29 10.18.
WS, LM, LR 21.48 2.21 19.75 8.9(no blacklist)Table 2: Results for English news data on an in-dependent test set with artificial spelling errors.Numbers are given in percentages.
LM: LanguageModel, Sim: Simple, LR: Logistic Regression,WS: Web-based suggestions.
NGS: No good sug-gestion rate.Results on English news data with artificialspelling errors are displayed in Table 2.
The sys-tems which do not employ the LM scores per-form substantially poorer that the ones with LMscores.
The Aspell system yields a total error rateof 17.65% and our system with Web-based sug-gestions yields TER of 9.06%.When comparing the simple scorer with the lo-gistic regression classifier, the Aspell Systems 2and 3 generate similar performances while theconfidence classifier afforded some gains in ourWeb-based suggestions system, with total error re-duced from 2.62% to 2.55%.
The ability to tuneeach phase during development has so far provenmore useful than the specific features or classifierused.
Blacklisting is crucial as seen by our resultsfor Systems 4 and 8.
When the blacklisting mech-anism is not used, performance steeply declines.When comparing overall performance for thedata between the Aspell systems and the Web-based suggestions systems, our Web-based sug-gestions fare better across the board for the newsdata with artificial misspellings.
Performance896gains are evident for each error metric that was ex-amined.
Total error rate for our best system (Sys-tem 7) reduces the error of the best Aspell sys-tem (System 3) by 45.7% (from 4.83% to 2.62%).In addition, our no good suggestion rate is only10% compared to 18% in the Aspell system.
Evenwhere no LM scores are used, our Web-based sug-gestions system outperforms the Aspell system.The above results suggest that the Web-basedsuggestions system performs at least as well asthe Aspell system.
However, it must be high-lighted that results on the test set with artificialerrors does not guarantee similar performance onreal user data.
The artificial errors were generatedat a systematically uniform rate, and are not mod-eled after real human errors made in real word-processing applications.
We attempt to considerthe impact of real human errors on our systems inthe next section.5.2 Experiments with Human ErrorsSystem TER CER FER NGSEnglish Aspell 4.58 3.33 2.86 23.0English WS 3.80 3.41 2.24 17.2German Aspell 14.09 10.23 5.94 44.4German WS 9.80 7.89 4.55 32.3Table 3: Results for Data with Real Errors in En-glish and German.Results for our system evaluated on data withreal misspellings in English and in German areshown in Table 3.
We used the systems that per-formed best on the artificial data (System 3 for As-pell, and System 7 for Web suggestions).
The mis-spelling error rates of the test sets were 10.8% and12.5% respectively, higher than those of the arti-ficial data which were used during development.For English, the Web-based suggestions resultedin a 17% improvement (from 4.58% to 3.80%) intotal error rate, but the correction error rate wasslightly (2.4%) higher.By contrast, in German our system improved to-tal error by 30%, from 14.09% to 9.80%.
Correc-tion error rate was also much lower in our Ger-man system, comparing 7.89% with 10.23% forthe Aspell system.
The no good suggestion ratesfor the real misspelling data are also higher thanthat of the news data.
Our suggestions are lim-ited to an edit distance of 2 with the original, andit was found that in real human errors, the aver-age edit distance of misspelled words is 1.38 butfor our small data, the maximum edit distance is4 in English and 7 in German.
Nonetheless, ourno good suggestion rates (17.2% and 32.3%) aremuch lower than those of the Aspell system (23%and 44%), highlighting the advantage of not usinga hand-crafted lexicon.Our results on real typed data were slightlyworse than those for the news data.
Several fac-tors may account for this.
(1) While the news datatest set does not overlap with the classifier train-ing set, the nature of the content is similar to thetrain and dev sets in that they are all news articlesfrom a one week period.
This differs substantiallyfrom Wikipedia article topics that were generallyabout the history and sights a city.
(2) Second,the method for inserting character errors (randomgeneration) was the same for the news data setswhile the real typed test set differed from the ar-tificial errors in the training set.
Typed errors areless consistent and error rates differed across sub-jects.
More in depth study is needed to understandthe nature of real typed errors.123456100  1000  10000  100000  1e+06  1e+07  1e+08  1e+09Error rateCorpus sizeTyped TERTyped CERTyped FERArtificial TERArtificial CERArtificial FERFigure 2: Effect of corpus size used to train theerror model.5.3 Effect of Web Corpus SizeTo determine the effects of the corpus size on ourautomated training, we evaluated System 7 usingerror models trained on different corpus sizes.
Weused corpora containing 103, 104, .
.
.
, 109Webpages.
We evaluated on the data set with real er-rors.
On average, about 37% of the pages in ourcorpus were in English.
So the number of pageswe used ranged from about 370 to about 3.7?108.As shown in Figure 2, the gains are small afterabout 106documents.8975.4 Correlation across data setsWe wanted to establish that performance improve-ment on the news data with artificial errors arelikely to lead to improvement on typed data withreal errors.
The seventeen English systems re-ported in Table 3, Table 2 and Figure 2 were eachevaluated on both English test sets.
The rank cor-relation coefficient between total error rates on thetwo data sets was high (?
= 0.92; p < 5?
10?6).That is, if one system performs better than anotheron our artificial spelling errors, then the first sys-tem is very likely to also perform better on realtyping errors.5.5 Experiments with More LanguagesSystem TER CER FER NGSGerman Aspell 8.64 4.28 5.25 29.4German WS 4.62 3.35 2.27 16.5Arabic Aspell 11.67 4.66 8.51 25.3Arabic WS 4.64 3.97 2.30 15.9Russian Aspell 16.75 4.40 13.11 40.5Russian WS 3.53 2.45 1.93 15.2Table 4: Results for German, Russian, Arabicnews data.Our system can be trained on many languageswith almost no manual effort.
Results for German,Arabic and Russian news data are shown in Ta-ble 4.
Performance improvements by the Web sug-gester over Aspell are greater for these languagesthan for English.
Relative performance improve-ments in total error rates are 47% in German, 60%in Arabic and 79% in Russian.
Differences in nogood suggestion rates are also very pronouncedbetween Aspell and the Web suggester.It cannot be assumed that the Arabic and Rus-sian systems would perform as well on real data.However the correlation between data sets re-ported in Section 5.4 lead us to hypothesize thata comparison between the Web suggester and As-pell on real data would be favourable.6 ConclusionsWe have implemented a spellchecking and au-tocorrection system and evaluated it on typeddata.
The main contribution of our work is thatwhile this system incorporates several knowledgesources, an error model, LM and confidence clas-sifiers, it does not require any manually annotatedresources, and infers its linguistic knowledge en-tirely from the Web.
Our approach begins with avery large term list that is noisy, containing bothspelled and misspelled words, and derived auto-matically with no human checking for whetherwords are valid or not.We believe this is the first published systemto obviate the need for any hand labeled data.We have shown that system performance improvesfrom a system that embeds handcrafted knowl-edge, yielding a 3.8% total error rate on humantyped data that originally had a 10.8% error rate.News data with artificially inserted spellings weresufficient to train confidence classifiers to a sat-isfactory level.
This was shown for both Ger-man and English.
These innovations enable therapid development of a spellchecking and correc-tion system for any language for which tokeniz-ers exist and string edit distances make sense.
Wehave done so for Arabic and Russian.In this paper, our results were obtained withoutany optimization of the parameters used in the pro-cess of gathering data from the Web.
We wanted tominimize manual tweaking particularly if it werenecessary for every language.
Thus heuristics suchas the number of terms in the term list, the criteriafor filtering triples, and the edit distance for defin-ing close words were crude, and could easily beimproved upon.
It may be beneficial to performmore tuning in future.
Furthermore, future workwill involve evaluating the performance of the sys-tem for these language on real typed data.7 AcknowledgmentWe would like to thank the anonymous reviewersfor their useful feedback and suggestions.
We alsothank our colleagues who participated in the datacollection.ReferencesFarooq Ahmad and Grzegorz Kondrak.
2005.
Learn-ing a spelling error model from search query logs.In HLT ?05: Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 955?962,Morristown, NJ, USA.
Association for Computa-tional Linguistics.K.
Atkinson.
2009.
Gnu aspell.
In Available athttp://aspell.net.Loghman Barari and Behrang QasemiZadeh.
2005.Clonizer spell checker adaptive, language indepen-dent spell checker.
In Ashraf Aboshosha et al, ed-itor, Proc.
of the first ICGST International Confer-898ence on Artificial Intelligence and Machine Learn-ing AIML 05, volume 05, pages 65?71, Cairo, Egypt,Dec.
ICGST, ICGST.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedingsof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 858?867.Eric Brill and Robert C. Moore.
2000.
An improvederror model for noisy channel spelling correction.In ACL ?00: Proceedings of the 38th Annual Meet-ing on Association for Computational Linguistics,pages 286?293.
Association for Computational Lin-guistics.S.
Cucerzan and E. Brill.
2004.
Spelling correctionas an iterative process that exploits the collectiveknowledge of web users.
In Proceedings of EMNLP2004, pages 293?300.F.J.
Damerau.
1964.
A technique for computer detec-tion and correction of spelling errors.
Communica-tions of the ACM 7, pages 171?176.S.
Deorowicz and M.G.
Ciura.
2005.
Correctingspelling errors by modelling their causes.
Interna-tional Journal of Applied Mathematics and Com-puter Science, 15(2):275?285.Andrew R. Golding and Yves Schabes.
1996.
Com-bining trigram-based and feature-based methods forcontext-sensitive spelling correction.
In In Proceed-ings of the 34th Annual Meeting of the Associationfor Computational Linguistics, pages 71?78.Ahmed Hassan, Sara Noeman, and Hany Hassan.2008.
Language independent text correction usingfinite state automata.
In Proceedings of the 2008 In-ternational Joint Conference on Natural LanguageProcessing (IJCNLP, 2008).Mark D. Kernighan, Kenneth W. Church, andWilliam A. Gale.
1990.
A spelling correction pro-gram based on a noisy channel model.
In Proceed-ings of the 13th conference on Computational lin-guistics, pages 205?210.
Association for Computa-tional Linguistics.K.
Kukich.
1992.
Techniques for automatically cor-recting words in texts.
ACM Computing Surveys 24,pages 377?439.Mirella Lapata and Frank Keller.
2004.
The web asa baseline: Evaluating the performance of unsuper-vised web-based models for a range of nlp tasks.
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Main Proceedings, pages121?128, Boston, Massachusetts, USA, May 2 -May 7.
Association for Computational Linguistics.Lidia Mangu and Eric Brill.
1997.
Automatic ruleacquisition for spelling correction.
In Douglas H.Fisher, editor, ICML, pages 187?194.
Morgan Kauf-mann.Eric Mays, Fred J. Damerau, and Robert L. Mercer.1991.
Context based spelling correction.
Informa-tion Processing and Management, 27(5):517.M.W.C.
Reynaert.
2008.
All, and only, the errors:More complete and consistent spelling and ocr-errorcorrection evaluation.
In Proceedings of the sixthinternational language resources and evaluation.Kristina Toutanova and Robert Moore.
2002.
Pronun-ciation modeling for improved spelling correction.In Proceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics (ACL),pages 144?151.L.
Amber Wilcox-O?Hearn, Graeme Hirst, and Alexan-der Budanitsky.
2008.
Real-word spelling cor-rection with trigrams: A reconsideration of themays, damerau, and mercer model.
In Alexan-der F. Gelbukh, editor, CICLing, volume 4919 ofLecture Notes in Computer Science, pages 605?616.Springer.Wei Xiang Yang Zhang, Pilian He and Mu Li.
2007.Discriminative reranking for spelling correction.
InThe 20th Pacific Asia Conference on Language, In-formation and Computation.899
