Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1210?1215,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsFine-Grained Contextual Predictions for Hard Sentiment WordsSebastian Ebert and Hinrich Sch?utzeCenter for Information and Language ProcessingUniversity of Munich, Germanyebert@cis.lmu.de, inquiries@cislmu.orgAbstractWe put forward the hypothesis that high-accuracy sentiment analysis is only pos-sible if word senses with different polar-ity are accurately recognized.
We pro-vide evidence for this hypothesis in a casestudy for the adjective ?hard?
and proposecontextually enhanced sentiment lexiconsthat contain the information necessary forsentiment-relevant sense disambiguation.An experimental evaluation demonstratesthat senses with different polarity can bedistinguished well using a combination ofstandard and novel features.1 IntroductionThis paper deals with fine-grained sentiment anal-ysis.
We aim to make three contributions.
First,based on a detailed linguistic analysis of contextsof the word ?hard?
(Section 3), we give evidencethat highly accurate sentiment analysis is only pos-sible if senses with different polarity are accu-rately recognized.Second, based on this analysis, we propose toreturn to a lexicon-based approach to sentimentanalysis that supports identifying sense distinc-tions relevant to sentiment.
Currently availablesentiment lexicons give the polarity for each wordor each sense, but this is of limited utility if sensescannot be automatically identified in context.
Weextend the lexicon-based approach by introducingthe concept of a contextually enhanced sentimentlexicon (CESL).
The lexicon entry of a word w inCESL has three components: (i) the senses of w;(ii) a sentiment annotation of each sense; (iii) adata structure that, given a context in which w oc-curs, allows to identify the sense of w used in thatcontext.As we will see in Section 3, the CESL senseinventory ?
(i) above ?
should be optimized forsentiment analysis: closely related senses with thesame sentiment should be merged whereas subtlesemantic distinctions that give rise to different po-larities should be distinguished.The data structure in (iii) is a statistical classi-fication model in the simplest case.
We will giveone other example for (iii) below: it can also be aset of centroids of context vector representations,with a mapping of these centroids to the senses.If sentiment-relevant sense disambiguation isthe first step in sentiment analysis, then power-ful contextual features are necessary to supportmaking fine-grained distinctions.
Our third con-tribution is that we experiment with deep learn-ing as a source of such features.
We look attwo types of deep learning features: word em-beddings and neural network language model pre-dictions (Section 4).
We show that deep learn-ing features significantly improve the accuracyof context-dependent polarity classification (Sec-tion 5).2 Related workInitial work on sentiment analysis was either basedon sentiment lexicons that listed words as posi-tive or negative sentiment indicators (e.g., Turney(2002), Yu and Hatzivassiloglou (2003)), on statis-tical classification approaches that represent doc-uments as ngrams (e.g., Pang et al.
(2002)) or ona combination of both (e.g., Riloff et al.
(2003),Whitelaw et al.
(2005)).
The underlying assump-tion of lexicon-based sentiment analysis is thata word always has the same sentiment.
This isclearly wrong because words can have senses withdifferent polarity, e.g., ?hard wood?
(neutral) vs.?hard memory?
(negative).Ngram approaches are also limited becausengram representations are not a good basis forrelevant generalizations.
For example, the neu-tral adverbial sense ?intense?
of ?hard?
(?laughhard?, ?try hard?)
vs. the negative adjectival mean-1210Cobuild syntax meaning example patterns sent.
# train # test1 FIRM 1 ADJ firm, stiff hard floor neu 78 52 DIFFICULT 2, 4, 9,10, 11ADJ difficult hard question hard for,hard on,hard to Vneg 2561 1203 ADVERB 3a, 5,6, 7ADV intensely work hard neu 425 194 INTENSE 3b ADJ intense hard look be hard atitneu 24 75 HARD-MAN 8 ADJ unkind hard man neg 15 06 HARD-TRUTH 12 attributiveADJdefinitelytruehard truth neu 5 67 MUSIC ADJ hard-rock-type musichard beats neu 347 158 CONTRAST ADJ opposite ofsoft transi-tionhard edge neu 3 19 NEGATIVE-P 13, 15 phrases neg 36 210 NEUTRAL-P 14, 16 phrases neu 375 27Table 1: Sense inventory of ?hard?.ing ?difficult?
(?hard life?, ?hard memory?)
cannotbe easily distinguished based on an ngram repre-sentation.
Moreover, although ngram approachescould learn the polarity of these phrases they donot generalize to new phrases.More recent compositional approaches to senti-ment analysis can outperform lexicon and ngram-based methods (e.g., Socher et al.
(2011), Socheret al.
(2013)).
However, these approaches conflatetwo different types of contextual effects: differ-ences in sense or lexical meaning (?hard memory?vs.
?hard wood?)
on the one hand and meaningcomposition like negation on the other hand.
Fromthe point of view of linguistic theory, these are dif-ferent types of contextual effects that should notbe conflated.
Recognizing that ?hard?
occurs inthe scope of negation is of no use if the basic po-larity of the contextually evoked sense of ?hard?
(e.g., negative in ?no hard memories?
vs. neutralin ?no hard wood?)
is not recognized.Wilson et al.
(2009) present an approach to clas-sify contextual polarity building on a two-step pro-cess.
First, they classify if a sentiment word is po-lar in a phrase and if so, second, they classify itspolarity.
Our approach can be seen as an exten-sion of this approach; the main difference is thatwe will show in our analysis of ?hard?
that thepolarity of phrases depends on the senses of thewords that are used.
This is evidence that high-accuracy polarity classification depends on sensedisambiguation.There has been previous work on assigning po-larity values to senses of words taken from Word-Net (e.g., Baccianella et al.
(2010), Wiebe and Mi-halcea (2006)).
However, these approaches are notable to disambiguate the sense of a word given itscontext.Previous work on representation learning forsentiment analysis includes (Maas and Ng, 2010)and (Maas et al., 2011).
Their models learn wordembeddings that capture semantic similarities andword sentiment at the same time.
Their approachfocuses on sentiment of entire sentences or docu-ments and does not consider each sentiment wordinstance at a local level.We present experiments with one supervisedand one semisupervised approach to word sensedisambiguation (WSD) in this paper.
OtherWSD approaches, e.g., thesaurus-based WSD(Yarowsky, 1992), could also be used for CESL.3 Linguistic analysis of sentimentcontexts of ?hard?We took a random sample of 5000 contexts of?hard?
in the Amazon Product Review Data (Jin-dal and Liu, 2008).
We use 200 as a test set and setaside 200 for future use.
We analyzed the remain-ing 4600 contexts using a tool we designed for thisstudy, which provides functionality for selectingand sorting contexts, including a keyword in con-text display.
If a reliable pattern has been identi-fied (e.g., the phrase ?die hard?
), then all contextsmatching the pattern can be labeled automatically.Our goal is to identify the different uses of?hard?
that are relevant for sentiment.
The basisfor our inventory is the Cobuild (Sinclair, 1987)1211lexicon entry for ?hard?.
We use Cobuild becauseit was compiled based on an empirical analysis ofcorpus data and is therefore more likely to satisfythe requirements of NLP applications than a tradi-tional dictionary.Cobuild lists 16 senses.
One of these senses(3) is split into two to distinguish the adverbial(?to accelerate hard?)
and adjectival (?hard accel-eration?)
uses of ?hard?
in the meaning ?intense?.We conflated five senses (2, 4, 9, 10, 11) refer-ring to different types of difficulty: ?hard ques-tion?
(2), ?hard work?
(4), ?hard life?
(11) andtwo variants of ?hard on?
: ?hard on someone?
(9), ?hard on something?
(10); and four differ-ent senses (3a, 5, 6, 7) referring to different typesof intensity: ?to work hard?
(3a), ?to look hard?
(5), ?to kick hard?
(6), ?to laugh hard?
(7).
Fur-thermore, we identified a number of noncompo-sitional meanings or phrases (lists NEGATIVE-Pand NEUTRAL-P in the supplementary material1)in addition to the four listed by Cobuild (13, 14,15, 16).
In addition, new senses for ?hard?
are in-troduced for opposites of senses of ?soft?
: the op-posite of ?quiet/gentle voice/sound?
(7: MUSIC;e.g., ?hard beat?, ?not too hard of a song?)
andthe opposite of ?smooth surface/texture?
(8: CON-TRAST; e.g., ?hard line?, ?hard edge?
).Table 1 lists the 10 different uses that are the re-sult of our analysis.
For each use, we give the cor-responding Cobuild sense numbers, syntactic in-formation, meaning, an example, typical patterns,polarity, and number of occurrences in trainingand test sets.7 uses are neutral and 3 are negative.
As?hard?s?
polarity in most sentiment lexicons isnegative, but only 3 out of 7 senses are negative,?hard?
provides evidence for our hypothesis thatsenses need to be disambiguated to allow for fine-grained and accurate polarity recognition.We hired two PhD students to label each of the200 contexts in the test set with one of the 10 la-bels in Table 1 (?
= .78).
Disagreement was re-solved by a third person.We have published the labeled data set of4600+200 contexts as supplementary material.4 Deep learning featuresWe use two types of deep learning features to beable to make the fine-grained distinctions neces-1All supplementary material is available at http://www.cis.lmu.de/ebert .sary for sense disambiguation.
First, we use wordembeddings similar to other recent work (see be-low).
Second, we use a deep learning languagemodel (LM) to predict the distribution of words forthe position at which the word of interest occurs.For example, an LM will predict that words like?granite?
and ?concrete?
are likely in the context?a * countertop?
and that words like ?serious?
and?difficult?
are likely in the context ?a * problem?.This is then the basis for distinguishing contextsin which ?hard?
is neutral (in the meaning ?firm,solid?)
from contexts in which it is a sentiment in-dicator (in the meaning ?difficult?).
We will usethe term predicted context distribution or PCD torefer to the distribution predicted by the LM.We use the vectorized log-bilinear languagemodel (vLBL) (Mnih and Kavukcuoglu, 2013)because it has three appealing features.
(i) Itlearns state of the art word embeddings (Mnih andKavukcuoglu, 2013).
(ii) The model is a languagemodel and can be used to calculate PCDs.
(iii) Asa linear model, vLBL can be trained much fasterthan other models (e.g., Bengio et al.
(2003)).The vLBL trains one set of word embeddingsfor the input space (R) and one for the target space(Q).
We denote the input representation of wordw as rwand the target representation as qw.
For agiven context c = w1, .
.
.
, wnthe model predictsa target representation q?
by linearly combining thecontext word representations with position depen-dent weights:q?
(c) =n?i=1dirwiwhere di?
D is the weight vector associatedwith position i in the context and  is point-wise multiplication.
Given the model parameters?
= {R,Q,D, b} the similarity between q?
and thecorrect target word embedding is computed by thesimilarity functions?
(w, c) = q?
(c)Tqw+ bwwhere bwis a bias term.We train the model with stochastic gradientdescent on mini-batches of size 100, followingthe noise-contrastive estimation training proce-dure of Mnih and Kavukcuoglu (2013).
We useAdaGrad (Duchi et al., 2011) with the initial learn-ing rate set to ?
= 0.5.
The embeddings size is setto 100.1212ngramPCDembedacc prec rec F1developmentbl 1 .62 .62 1.00 .76fully2 + .90 .91 .94 .923 + .90 .91 .92 .924 + .87 .87 .92 .905 + + .92 .92 .94 .936 + + .91 .90 .95 .927 + + .86 .83 .96 .898 + + + .92 .93 .95 .94semi9 + .85 .87 .89 .8810 + .85 .87 .89 .8811 + .76 .73 .98 .8312 + + .85 .87 .89 .8813 + + .85 .87 .89 .8814 + + .85 .89 .87 .8815 + + + .86 .87 .90 .89testbl 16 .66 .66 1.00 .80fully 17 + + + .90 .89 .96 .92semi 18 + + + .85 .85 .91 .88Table 2: Classification results; bl: baselineDuring training we do not need to normalize thesimilarity explicitly, because the normalization isimplicitly learned by the model.
However, nor-malization is still necessary for prediction.
Thenormalized PCD for a context c of word w is com-puted using the softmax function:Pc?
(w) =exp(s?
(w, c))?w?exp(s?
(w?, c))We use a window size ofws = 7 for training themodel.
We found that the model did not captureenough contextual phenomena forws = 3 and thatresults for ws = 11 did not have better qualitythan ws = 7, but had a negative impact on thetraining time.
Using a vocabulary of the 100,000most frequent words, we train the vLBL model for4 epochs on 1.3 billion 7-grams randomly selectedfrom the English Wikipedia.5 ExperimentsThe lexicon entry of ?hard?
in CESL consists of (i)the senses, (ii) the polarity annotations (neutral ornegative) and (iii) the sense disambiguation datastructure.
Components (i) and (ii) are shown inTable 1.
In this section, we evaluate two differentoptions for (iii) on the task of sentiment classifica-tion.1 2 3 4 5 6 7 812 ?3 ?4 ?
?
?5 ?
?6 ?
?7 ?
?
* ?
?8 ?
* * ?
* ?Table 3: Significant differences of lines 1?8 in Ta-ble 2; ?
: p=0.01, *: p=0.05, ?
: p=0.1The first approach is to use a statistical classi-fication model as the sense disambiguation struc-ture.
We use liblinear (Fan et al., 2008) with stan-dard parameters for classification based on threedifferent feature types: ngrams, embeddings (em-bed) and PCDs.
Ngram features are all n-gramsfor n ?
{1, 2, 3}.
As embedding features weuse (i) the mean of the input space (R) embed-dings and (ii) the mean of the target space (Q) em-beddings of the words in the context (see Blacoeand Lapata (2012) for justification of using simplemean).
As PCD features we use the PCD predictedby vLBL for the sentiment word of interest, in ourcase ?hard?.We split the set of 4600 contexts introduced inSection 3 into a training set of 4000 and a devel-opment set of 600.Table 2 (lines 1?8) shows the classification re-sults on the development set for all feature typecombinations.
Significant differences between re-sults ?
computed using the approximate random-ization test (Pad?o, 2006) ?
are given in Table 3.The majority baseline (bl), which assigns a nega-tive label to all examples, reaches F1= .76.
Theclassifier is significantly better than the baselinefor all feature combinations with F1ranging from.89 to .94.
We obtain the best classification result(.94) when all three feature types are combined(significantly better than all other feature combi-nations except for 5).Manually labeling all occurrences of a wordis expensive.
As an alternative we investigateclustering of the contexts of the word of interest.Therefore, we represent each of the 4000 con-texts of ?hard?
in the training set as its PCD2, use2To transform vectors into a format that is more appropri-ate for the underlying Gaussian model of kmeans, we take thesquare root of each probability in the PCD vectors.1213kmeans clustering with k = 100 and then labeleach cluster.
This decreases the cost of labelingby an order of magnitude since only 100 clustershave to be labeled instead of 4000 training set con-texts.Table 2 (lines 9?15) shows results for thissemisupervised approach to classification, usingthe same classifier and the same feature types, butthe cluster-based labels instead of manual labels.For most feature combinations, F1drops com-pared to fully supervised classification.
The bestperforming model for supervised classification(ngram+PCD+embed) loses 5%.This is not a large drop considering the savingsin manual labeling effort.
All results are signifi-cantly better than the baseline.
There are no signif-icant differences between the different feature sets(lines 9?15) with the exception of embed, whichis significantly worse than the other 6 sets.The centroids of the 100 clusters can serve as analternative sense disambiguation structure for thelexicon entry of ?hard?
in CESL.3Each sense s isassociated with the centroids of the clusters whosemajority sense is s.As final experiment (lines 16?18 in Table 2),we evaluate performance for the baseline and forPCD+ngram+embed ?
the best feature set ?
on thetest set.
On the test set, baseline performance is.80 (.04 higher than .76 on line 1, Table 2); F1ofPCD+ngram+embed is .92 (.02 less than develop-ment set) for supervised classification and is .88(.01 less) for semisupervised classification (com-paring to lines 8 and 15 in Table 2).
Both results(.92 and .88) are significantly higher than the base-line (.80).6 ConclusionThe sentiment of a sentence or document is theoutput of a causal chain that involves complex lin-guistic processes like contextual modification andnegation.
Our hypothesis in this paper was thatfor high-accuracy sentiment analysis, we need tomodel the root causes of this causal chain: themeanings of individual words.
This is in contrastto other work in sentiment analysis that conflatesdifferent linguistic phenomena (word sense ambi-guity, contextual effects, negation) and attempts toaddress all of them with a single model.For sense disambiguation, the first step in thecausal chain of generating sentiment, we proposed3Included in supplementary material.CESL, a contextually enhanced sentiment lexi-con that for each word w holds the inventory ofsenses of w, polarity annotations of these sensesand a data structure for assigning contexts of wto the senses.
We introduced new features forsentiment analysis to be able to perform the fine-grained modeling of context needed for CESL.
Ina case study for the word ?hard?, we showed thathigh accuracy in sentiment disambiguation can beachieved using our approach.
In future work, wewould like to show that our findings generalizefrom the case of ?hard?
to the entire sentiment lex-icon.AcknowledgmentsThis work was supported by DFG (grant SCHU2246/10).
We thank Lucia Krisnawati and SaschaRothe for their help with annotation.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In International Conference on Language Resourcesand Evaluation, pages 2200?2204.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for seman-tic composition.
In Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 546?556.
Association for Computational Linguistics.John C. Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
Journal of MachineLearning Research, 12:2121?2159.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Nitin Jindal and Bing Liu.
2008.
Opinion spamand analysis.
In International Conference on WebSearch and Web Data Mining, pages 219?230.Andrew L. Maas and Andrew Y. Ng.
2010.
A proba-bilistic model for semantic word vectors.
In AnnualConference on Advances in Neural Information Pro-cessing Systems: Deep Learning and UnsupervisedFeature Learning Workshop.1214Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and Christopher Potts.2011.
Learning word vectors for sentiment analysis.In Annual Meeting of the Association for Computa-tional Linguistics, pages 142?150.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastiveestimation.
In Annual Conference on Advancesin Neural Information Processing Systems, pages2265?2273.Sebastian Pad?o, 2006.
User?s guide to sigf: Signifi-cance testing by approximate randomisation.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment classification usingmachine learning techniques.
In Conference on Em-pirical Methods in Natural Language Processing,pages 79?86.Ellen Riloff, Janyce Wiebe, and Theresa Ann Wilson.2003.
Learning subjective nouns using extractionpattern bootstrapping.
In Conference on NaturalLanguage Learning, volume 4, pages 25?32.John Sinclair.
1987.
Looking Up: Account of theCobuild Project in Lexical Computing.
CollinsCoBUILD.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Conference on Em-pirical Methods in Natural Language Processing,pages 151?161.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher Manning, Andrew Ng, andChristopher Potts.
2013.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In Conference on Empirical Methods in Nat-ural Language Processing, pages 1631?1642.Peter D. Turney.
2002.
Thumbs up or thumbs down?semantic orientation applied to unsupervised classi-fication of reviews.
In Annual Meeting of the Asso-ciation for Computational Linguistics, pages 417?424.Casey Whitelaw, Navendu Garg, and Shlomo Arga-mon.
2005.
Using appraisal groups for sentimentanalysis.
In International Conference on Informa-tion and Knowledge Management, pages 625?631.ACM.Janyce Wiebe and Rada Mihalcea.
2006.
Word senseand subjectivity.
In Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1065?1072.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analy-sis.
Computational Linguistics, 35(3):399?433.David Yarowsky.
1992.
Word-sense disambiguationusing statistical models of Roget?s categories trainedon large corpora.
In International Conference onComputational Linguistics, pages 454?460.Hong Yu and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separating factsfrom opinions and identifying the polarity of opinionsentences.
In Conference on Empirical Methods inNatural Language Processing, pages 129?136.1215
