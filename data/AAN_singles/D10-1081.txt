Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 832?842,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsAn Efficient Algorithm for Unsupervised Word Segmentation withBranching Entropy and MDLValentin ZhikovInterdisciplinary Graduate Schoolof Science and EngineeringTokyo Institute of Technologyzhikov@lr.pi.titech.ac.jpHiroya TakamuraPrecision and Intelligence LaboratoryTokyo Institute of Technologytakamura@pi.titech.ac.jpManabu OkumuraPrecision and Intelligence LaboratoryTokyo Institute of Technologyoku@pi.titech.ac.jpAbstractThis paper proposes a fast and simple unsuper-vised word segmentation algorithm that uti-lizes the local predictability of adjacent char-acter sequences, while searching for a least-effort representation of the data.
The modeluses branching entropy as a means of con-straining the hypothesis space, in order to ef-ficiently obtain a solution that minimizes thelength of a two-part MDL code.
An evaluationwith corpora in Japanese, Thai, English, andthe ?CHILDES?
corpus for research in lan-guage development reveals that the algorithmachieves an accuracy, comparable to that ofthe state-of-the-art methods in unsupervisedword segmentation, in a significantly reducedcomputational time.1 IntroductionAs an inherent preprocessing step to nearly all NLPtasks for writing systems without orthographicalmarking of word boundaries, such as Japanese andChinese, the importance of word segmentation haslead to the emergence of a micro-genre in NLP fo-cused exclusively on this problem.Supervised probabilistic models such as Condi-tional Random Fields (CRF) (Lafferty et al, 2001)have a wide application to the morphological anal-ysis of these languages.
However, the developmentof the annotated training corpora necessary for theirfunctioning is a labor-intensive task, which involvesmultiple stages of manual tagging.
Because of thescarcity of labeled data, the domain adaptation ofmorphological analyzers is also problematic, andsemi-supervised algorithms that address this issuehave also been proposed (e.g.
Liang, 2005; Tsuboiet al, 2008).Recent advances in unsupervised word segmen-tation have been promoted by human cognition re-search, where it is involved in the modeling of themechanisms that underlie language acquisition.
An-other motivation to study unsupervised approachesis their potential to support the domain adaptation ofmorphological analyzers through the incorporationof unannotated training data, thus reducing the de-pendency on costly manual work.
Apart from theconsiderable difficulties in discovering reliable cri-teria for word induction, the practical applicationof such approaches is impeded by their prohibitivecomputational cost.In this paper, we address the issue of achiev-ing high accuracy in a practical computational timethrough an efficient method that relies on a combina-tion of evidences: the local predictability of charac-ter patterns, and the reduction of effort achieved bya given representation of the language data.
Both ofthese criteria are assumed to play a key role in nativelanguage acquisition.
The proposed model allowsexperimentation in a more realistic setting, wherethe learner is able to apply them simultaneously.
The832method shows a high performance in terms of accu-racy and speed, can be applied to language samplesof substantial length, and generalizes well to corporain different languages.2 Related WorkThe principle of least effort (Zipf, 1949) postulatesthat the path of minimum resistance underlies allhuman behavior.
Recent research has recognizedits importance in the process of language acquisi-tion (Kit, 2003).
Compression-based word induc-tion models comply to this principle, as they reor-ganize the data into a more compact representationwhile identifying the vocabulary of a text.
The min-imum description length framework (MDL) (Ris-sanen, 1978) is an appealing means of formalizingsuch models, as it provides a robust foundation forlearning and inference, based solely on compres-sion.The major problem in MDL-based word segmen-tation is the lack of standardized search algorithmsfor the exponential hypothesis space (Goldwater,2006).
The representative MDL models comparefavorably to the current state-of-the-art models interms of accuracy.
Brent and Cartwright (1996) car-ried out an exhaustive search through the possiblesegmentations of a limited subset of the data.
Yu(2000) proposed an EM optimization routine, whichachieved a high accuracy, in spite of a lower com-pression than the gold standard segmentation.As a solution to the aforementioned issue, the pro-posed method incorporates the local predictability ofcharacter sequences into the inference process.
Nu-merous studies have shown that local distributionalcues can serve well the purpose of inducing wordboundaries.
Behavioral science has confirmed thatinfants are sensitive to the transitional probabilitiesfound in speech (Saffran et al, 1996).
The increasein uncertainty following a given word prefix is awell studied criterion for morpheme boundary pre-diction (Harris, 1955).
A good deal of research hasbeen conducted on methods through which such lo-cal statistics can be applied to the word inductionproblem (e.g.
Kempe, 1999; Huang and Powers,2003; Jin and Tanaka-Ishii, 2006).
Hutchens andAdler (1998) noticed that entropic chunking has theeffect of reducing the perplexity of a text.Most methods for unsupervised word segmenta-tion based solely on local statistics presume a cer-tain ?
albeit minimum ?
level of acquaintance withthe target language.
For instance, the model ofHuang and Powers (2003) involves some parame-ters (Markov chain order, numerous threshold val-ues) that allow its adaptation to the individuality ofwritten Chinese.
In comparison, the method pro-posed in this paper generalizes easily to a variety oflanguages and domains, and is less dependent on an-notated development data.The state-of-the-art in unsupervised word seg-mentation is represented by Bayesian models.
Gold-water et al (2006) justified the importance ofcontext as a means of avoiding undersegmentation,through a method based on hierarchical Dirichletprocesses.
Mochihashi et al (2009) proposed ex-tensions to this method, which included a nestedcharacter model and an optimized inference proce-dure.
Johnson and Goldwater (2009) have proposeda novel method based on adaptor grammars, whoseaccuracy surpasses the aforementioned methods bya large margin, when appropriate assumptions aremade regarding the structural units of a language.3 Proposed Method3.1 Word segmentation with MDLThe proposed two-part code incorporates some ex-tensions of models presented in related work, aimedat achieving a more precise estimation of the repre-sentation length.
We first introduce the general two-part code, which consists of:?
the model, embodied by a codebook, i.e., a lexi-con of unique word typesM = {w1, ..., w|M |},?
the source text D, obtained through encodingthe corpus using the lexicon.The total description length amounts to the num-ber of bits necessary for simultaneous transmissionof the codebook and the source text.
Therefore, ourobjective is to minimize the combined descriptionlength of both terms:L(D,M) = L(M) + L(D|M).The description length of the data given M is cal-culated using the Shannon-Fano code:833L(D|M) = ?|M |?j=1#wj log2 P (wj),where #wj stands for the frequency of the word wjin the text.Different strategies have been proposed in the lit-erature for the calculation of the codebook cost.
Acommon technique in segmentation and morphologyinduction models is to calculate the product of thetotal length in characters of the lexicon and an esti-mate of the per-character entropy.
In this way, boththe probabilities and lengths of words are taken intoconsideration.
The use of a constant value is an ef-fective and easily computable approach, but it is farfrom precise.
For instance, in Yu (2000) the averageentropy per character is measured against the orig-inal corpus, but this model does not capture the ef-fects of the word distributions on the observed char-acter probabilities.
For this reason, we propose adifferent method: the codebook is modeled as a sep-arate Markov chain of characters.A lexicon of characters M ?
is defined.
The de-scription length of the lexicon data D?
given M ?
isthen calculated as:L(D?|M ?)
= ?|C|?i=1#ci log2 P (ci),where #ci denotes the frequency of a character ciin the lexicon of hypothesis M .
The term L(M ?
)is constant for any choice of hypothesis, as is repre-sents the character set of a corpus.The total description length under the proposedmodel is thus calculated as:L(M) + L(D|M) = L(M ?)
+ L(D?|M ?)
+ L(D|M) =?|C|?i=1#ci log2 P (ci)?|M |?j=1#wj log2 P (wj) +O(1).A rigorous definition should include two addi-tional terms, L(?|M) and L(?
?|M ?
), which give therepresentation cost of the parameters of both mod-els.
The L(?|M) can be calculated as:L(?|M) =|M | ?
12?
log2 S,where |M | ?
1 gives the number of parameters (de-grees of freedom), and S is the size of the dataset(the total length of the text in characters).
The para-metric complexity term is calculated in the sameway for the lexicon.
For a derivation of the aboveformula, refer to e.g.
Li (1998).MDL is closely related to Bayesian inference.
De-pending on the choice of a universal code, the twoapproaches can overlap, as is the case with the two-part code discussed in this paper.
It can be shownthat the model selection in our method is equiva-lent to a MAP inference, conducted under the as-sumption that the prior probability of a model de-creases exponentially with its length (Goldwater,2006).
Thus, the task that we are trying to accom-plish is to conduct a focused search through the hy-pothesis space that will allow us to obtain an approx-imation of the MAP solution in a reasonable time.The MDL framework does not provide standardsearch algorithms for obtaining the hypotheses thatminimize the description length.
In the rest of thissection, we will describe an efficient technique suit-able for the word segmentation task.3.2 Obtaining an initial hypothesisFirst, a rough initial hypothesis is built by an algo-rithm that combines the branching entropy and MDLcriteria.Given a setX , comprising all the characters foundin a text, the entropy of branching at position k of thetext is defined as:H(Xk|xk?1, ..., xk?n) =?
?x?XP (x|xk?1, ..., xk?n) log2 P (x|xk?1, ..., xk?n),where xk represents the character found at positionk, and n is the order of the Markov model over char-acters.
For brevity, hereafter we shall denote the ob-served sequence {xk?1, ..., xk?n} as {xk?1:k?n} .The above definition is extended to combine theentropy estimates in the left-to-right and right-to-left directions, as this factor has reportedly improvedperformance figures for models based on branchingentropy (Jin and Tanaka-Ishii, 2006).
The estimatesin both directions are summed up, yielding a singlevalue per position:834H ?
(Xk;k?1|xk?1:k?n;xk:k+n?1) =?
?x?XP (x|xk?1:k?n) log2 P (x|xk?1:k?n)?
?x?XP (x|xk:k+n?1) log2 P (x|xk:k+n?1).Suffix arrays are employed during the collectionof frequency statistics.
For a character model of or-der n over a testing corpus of size t and a trainingcorpus of size m, suffix arrays allow these to beacquired in O(tn logm) time.
Faster implementa-tions reduce the complexity toO(t(n+logm)).
Forfurther discussion, see Manber and Myers (1991).During the experiments, we did not use the cachingfunctionality provided by the suffix array library, butinstead kept the statistics for the current iterativepass (n-gram order and direction) in a local table.The chunking technique we adopt is to insert aboundary when the branching entropy measured insequences of length n exceeds a certain thresholdvalue (H(X|xk?1:k?n) > ?).
Both n and ?
are fixed.Within the described framework, the increase incontext length n promotes precision and recall atfirst, but causes a performance degradation when theentropy estimates become unreliable due to the re-duced frequencies of long strings.
High thresholdvalues produce a combination of high precision andlow recall, while low values result in low precisionand high recall.Since the F-score curve obtained as decreasingvalues are assigned to the threshold is typically uni-modal as in many applications of MDL, we employa bisection search routine for the estimation of thethreshold (Algorithm 1).All positions of the dataset are sorted by their en-tropy values.
At each iteration, at most two newhypotheses are built, and their description lengthsare calculated in time linear to the data size.
Thecomputational complexity of the described routineis O(t log t), where t is the corpus length in charac-ters.The order of the Markov chain n used during theentropy calculation is the only input variable of theproposed model.
Since different values perform thebest across the various languages, the most appro-priate settings can be obtained with the help of asmall annotated corpus.
However, the MDL objec-tive also enables unsupervised optimization againstAlgorithm 1 Generates an initial hypothesis.thresholds[] := sorted H(Xk) values;threshold := median of thresholds[];step := length of thresholds[]/4;direction := ascending;minimum := +?
;while step > 0 donextThreshold := thresholds[] value one step in lastdirection;DL = calculateDL(nextThreshold);if DL < minimum thenminimum:= DL; threshold := nextThreshold;step := step/2; continue;end ifreverse direction;nextThreshold := thresholds[] value one step in lastdirection;if DL < minimum thenminimum:= DL; threshold := nextThreshold;step := step/2; continue;end ifreverse direction;step := step/2;end whileCorpus [1] [2] [3] [4]CHILDES 394655.52 367711.66 368056.10 405264.53Kyoto 1.291E+07 1.289E+07 1.398E+07 1.837E+07Table 1: Length in bits of the solutions proposed by Al-gorithm 1 with respect to the character n-gram order.a sufficiently large unlabeled dataset.
The order thatminimizes the description length of the data can bediscovered in a few iterations of Algorithm 1 withincreasing values of n, and it typically matches theoptimal value of the parameter (Table 1).Although an acceptable initial segmentation canbe built using the described approach, it is possibleto obtain higher accuracy with an extended modelthat takes into account the statistics of Markovchains from several orders during the entropy calcu-lation.
This can be done by summing up the entropyestimates, in the way introduced earlier for combin-ing the values in both directions:H ??
(Xk;k?1|xk?1:k?n;xk:k+n?1) =?nmax?n=1(?x?XP (x|xk?1:k?n) log2 P (x|xk?1:k?n)+?x?XP (x|xk:k+n?1) log2 P (x|xk:k+n?1)),835where nmax is the index of the highest order to betaken into consideration.3.3 Refining the initial hypothesisIn the second phase of the proposed method, we willrefine the initial hypothesis through the reorganiza-tion of local co-occurrences which produce redun-dant description length.
We opt for greedy optimiza-tion, as our primary interest is to further explore theimpact that description length minimization has onaccuracy.
Of course, such an approach is unlikelyto obtain global minima, but it is a feasible means ofconducting the optimization process, and guaranteesa certain increase in compression.Since a preliminary segmentation is available, itis convenient to proceed by inserting or removingboundaries in the text, thus splitting or merging thealready discovered tokens.
The ranked positions in-volved in the previous step can be reused here, asthis is a way to bias the search towards areas ofthe text where boundaries are more likely to occur.Boundary insertion should start in regions where thebranching entropy is high, and removal should firstoccur in regions where the entropy is close to zero.A drawback of this approach is that it omits loca-tions where the gains are not immediately obvious,as it cannot assess the cumulative gains arising fromthe merging or splitting of all occurrences of a cer-tain pair (Algorithm 2).A clean-up routine, which compensates for thisshortage, is also implemented (Algorithm 3).
It op-erates directly on the types found in the lexicon pro-duced by Algorithm 2, and is capable of modify-ing a large number of occurrences of a given pairin a single step.
The lexicon types are sorted bytheir contribution to the total description length ofthe corpus.
For each word type, splitting or merg-ing is attempted at every letter, beginning from thecenter.
The algorithm eliminates unlikely types withlow contribution, which represent mostly noise, andredistributes their cost among more likely ones.
Thedesign of the merging routine makes it impossible toproduce types longer than the ones already found inthe lexicon, as an exhaustive search would be pro-hibitive.The evaluation of each hypothetical change inthe segmentation requires that the description lengthof the two-part code is recalculated.
In order toAlgorithm 2 Compresses local token co-occurrences.path[][]:= positions sorted by H(Xk) values;minimum := DL of model produced at initialization;repeatfor i = max H(Xk) to min H(Xk) dopos:= path[i][k];if no boundary exists at pos thenleftToken := token to the left;rightToken := token to the right;longToken := leftToken + rightToken;calculate DL after splitting;if DL < minimum thenaccept split, update model, update DP vari-ables;end ifend ifend forfor i = min H(Xk) to max H(Xk) domerge leftToken and rightToken into longTokenif DL will decrease (analogous to splitting)end foruntil no change is evident in modelAlgorithm 3 A lexicon clean-up procedure.types[] := lexicon types sorted by cost;minimum := DL of model produced by Algorithm 2;repeatfor i = min cost to max cost dofor pos = middle to both ends of types[i] dolongType := types[i];leftType := sequence from first character topos;rightType:= sequence from pos to last charac-ter;calculate DL after splitting longType into left-Type and rightType;if DL < minimum thenaccept split, update model, update DP vari-ables;break out of inner loop;end ifend forend fortypes[] := lexicon types sorted by cost;for i = max cost to min cost dofor pos = middle to both ends of types[i] domerge leftType and rightType into longType ifDL will decrease (analogous to splitting)break out of inner loop;end forend foruntil no change is evident in model836make this optimization phase computationally fea-sible, dynamic programming is employed in Algo-rithms 2 and 3.
The approach adopted for the re-calculation of the source text term L(D|M) is ex-plained below.
The estimation of the lexicon cost isanalogous.
The term L(D|M) can be rewritten as:L(D|M) = ?|M |?j=1#wj log2#wjN=?|M |?j=1#wj log2 #wj +N log2N = T1 + T2,where #wj is the frequency of wj in the segmentedcorpus, and N =?|M |j=1 #wj is the cumulative to-ken count.
In order to calculate the new length, wekeep the values of the terms T1 and T2 obtained atthe last change of the model.
Their new values arecomputed for each hypothetical split or merge on thebasis of the last values, and the expected descriptionlength is calculated as their sum.
If the produced es-timate is lower, the model is modified and the newvalues of T1 and T2 are stored for future use.In order to maintain precise token counts, Algo-rithms 2 and 3 recognize the fact that recurring se-quences (?byebye?
etc.)
appear in the corpora, andhandle them accordingly.
Known boundaries, suchas the sentence boundaries in the CHILDES corpus,are also taken into consideration.4 Experimental SettingsWe evaluated the proposed model against fourdatasets.
The first one is the Bernstein-Ratner cor-pus for language acquisition based on transcriptsfrom the CHILDES database (Bernstein-Ratner,1987).
It comprises phonetically transcribed utter-ances of adult speech directed to 13 through 21-month-old children.
We evaluated the performanceof our learner in the cases when the few boundariesamong the individual sentences are available to it(B), and when it starts from a blank state (N).
TheKyoto University Corpus (Kurohashi and Nagao,1998) is a standard dataset for Japanese morpho-logical and dependency structure analysis, whichcomprises newspaper articles and editorials from theMainichi Shimbun.
The BEST corpus for word seg-mentation and named entity recognition in Thai lan-guage combines text from a variety of sources in-Corpus Language Size(MB)Chars(K)Tokens(K)Types(K)CHILDES-B/NEnglish 0.1 95.8 33.3 1.3Kyoto Japanese 5.02 1674.9 972.9 39.5WSJ English 5.22 5220.0 1174.2 49.1BEST-E Thai 12.64 4360.2 1163.2 26.2BEST-N Thai 18.37 6422.7 1659.4 36.3BEST-A Thai 4.59 1619.9 438.7 13.9BEST-F Thai 16.18 5568.0 1670.8 22.6Wikipedia Japanese 425.0 169069.3 / /Asahi Japanese 337.2 112401.1 / /BEST-All Thai 51.2 17424.0 4371.8 73.4Table 2: Corpora used during the evaluation.
Precise to-ken and type counts have been omitted for Wikipedia andAsahi, as no gold standard segmentations are available.cluding encyclopedias (E), newspaper articles (N),scientific articles (A), and novels (F).
The WSJ sub-set of the Penn Treebank II Corpus incorporatesselected stories from the Wall Street Journal, year1989 (Marcus et al, 1994).
Both the original text(O), and a version in which all characters were con-verted to lower case (L) were used.The datasets listed above were built by remov-ing the tags and blank spaces found in the corpora,and concatenating the remaining text.
We addedtwo more training datasets for Japanese, which wereused in a separate experiment solely for the acqui-sition of frequency statistics.
One of them wascreated from 200,000 randomly chosen Wikipediaarticles, stripped from structural elements.
Theother one contains text from the year 2005 issues ofAsahi Newspaper.
Statistics regarding all describeddatasets are presented in Table 2.One whole corpus is segmented in each experi-ment, in order to avoid the statement of an extendedmodel that would allow the separation of trainingand test data.
This setting is also necessary for thedirect comparison between the proposed model andother recent methods evaluated against the entireCHILDES corpus.We report the obtained precision, recall and F-score values calculated using boundary, token andtype counts.
Precision (P) and recall (R) are definedas:P =#correct units# output units, R =#correct units#gold standard units.Boundary, token and lexicon F-scores, denotedas B-F and T -F and L-F , are calculated as the837Model Corpus & Settings B-Prec B-Rec B-F T-Prec T-Rec T-F DL(bits)Ref.DL(bits)Time(ms)1 CHILDES, ?
= 1.2, n = [1-6] 0.8667 0.8898 0.8781 0.6808 0.6990 0.6898 344781.74 1060.22a (H?)
CHILDES, n = 2 0.7636 0.9109 0.8308 0.5352 0.6384 0.5823 367711.66 300490.52 753.12b (H??)
CHILDES, nmax = 3 0.8692 0.8865 0.8777 0.6792 0.6927 0.6859 347633.07 885.31 Kyoto, ?
= 0, n = [1-6] 0.8208 0.8208 0.8208 0.5784 0.5784 0.5784 1.325E+07 54958.82a (H?)
Kyoto, n = 2 0.8100 0.8621 0.8353 0.5934 0.6316 0.6119 1.289E+07 1.120E+07 22909.72b (H??)
Kyoto, nmax = 2 0.8024 0.9177 0.8562 0.6093 0.6969 0.6501 1.248+E07 23212.8Table 3: Comparison of the proposed method (2a, 2b) with the model of Jin and Tanaka-Ishii (2006) (1).
Executiontimes include the obtaining of frequency statistics, and are represented by averages over 10 runs.harmonic averages of the corresponding precisionand recall values (F = 2PR/(P + R)).
As arule, boundary-based evaluation produces the high-est scores among the three evaluation modes, as itonly considers the correspondence between the pro-posed and the gold standard boundaries at the indi-vidual positions of the corpora.
Token-based evalua-tion is more strict ?
it accepts a word as correct onlyif its beginning and end are identified accurately, andno additional boundaries lie in between.
Lexicon-based evaluation reflects the extent to which the vo-cabulary of the original text has been recovered.It provides another useful perspective for the erroranalysis, which in combination with token scorescan give a better idea of the relationship between theaccuracy of induction and item frequency.The system was implemented in Java, however ithandled the suffix arrays through an external C li-brary called Sary.1 All experiments were conductedon a 2 GHz Core2Duo T7200 machine with 2 GBRAM.5 Results and DiscussionThe scores we obtained using the described instan-tiations of the branching entropy criterion at the ini-tialization phase are presented in Table 3, along withthose generated by our own implementation of themethod presented in Jin and Tanaka-Ishii (2006),where the threshold parameter ?
was adjusted man-ually for optimal performance.The heuristic of Jin and Tanaka-Ishii takes advan-tage of the trend that branching entropy decreasesas the observed character sequences become longer;sudden rises can thus be regarded as an indication oflocations where a boundary is likely to exist.
Theirmethod uses a common value for thresholding the1http://sary.sourceforge.netentropy change throughout all n-gram orders, andcombines the boundaries discovered in both direc-tions in a separate step.
These properties of themethod would lead to complications if we tried toemploy it in the first phase of our method (i.e.
a stepparameter for iterative adjustment of the thresholdvalue, rules for combining the boundaries, etc.
).The proposed criterion with an automatically de-termined threshold value produced slightly worseresults than that of Jin and Tanaka-Ishii at theCHILDES corpus.
However, we found out that ourapproach achieves approximately 1% higher scorewhen the best performing threshold value is selectedfrom the candidate list.
There are two observationsthat account for the suboptimal threshold choice byour algorithm.
On one hand, the correspondencebetween description length and F-score is not abso-lutely perfect, and this may pose an obstacle to theoptimization process for relatively small languagesamples.
Another issue lies in the bisection searchroutine, which suggests approximations of the de-scription length minima.
The edge that our methodhas on the Kyoto corpus can be attributed to a betterestimation of the optimal treshold value due to thelarger amount of data.The experimental results obtained at the comple-tion of Algorithm 3 are summarized in Tables 4 and5.
Presented durations include the obtaining of fre-quency statistics.
The nmax parameter is set to thevalue which maximizes the compression during theinitial phase, in order to make the results representa-tive of the case in which no annotated developmentcorpora are accessible to the algorithm.It is evident that after the optimization carried outin the second phase, the description length is re-duced to levels significantly lower than the groundtruth.
In this aspect, the algorithm outperforms theEM-based method of Yu (2000).838Corpus & Settings B-F T-F L-F Time(ms)CHILDES-B, nmax=3 0.9092 0.7542 0.5890 2597.2CHILDES-N, nmax=3 0.9070 0.7499 0.5578 2949.3Kyoto, nmax=2 0.8855 0.7131 0.3725 70164.6BEST-E, nmax=5 0.9081 0.7793 0.3549 738055.0BEST-N, nmax=5 0.8811 0.7339 0.2807 505327.0BEST-A, nmax=5 0.9045 0.7632 0.4246 250863.0BEST-F, nmax=5 0.9343 0.8216 0.4820 305522.0WSJ-O, nmax=6 0.8405 0.6059 0.3338 658214.0WSJ-L, nmax=6 0.8515 0.6373 0.3233 582382.0Table 4: Results obtained after the termination of Algo-rithm 3.Corpus & Settings DescriptionLength (Proposed)DescriptionLength (Total)CHILDES-B, nmax=3 290592.30 300490.52CHILDES-N, nmax=3 290666.12 300490.52Kyoto, nmax=2 1.078E+07 1.120E+07BEST-E, nmax=5 1.180E+07 1.252E+07BEST-N, nmax=5 1.670E+07 1.809E+07BEST-A, nmax=5 4438600.32 4711363.62BEST-F, nmax=5 1.562E+07 1.634E+07WSJ-O, nmax=6 1.358E+07 1.460E+07WSJ-L, nmax=6 1.317E+07 1.399E+07Table 5: Description length - proposed versus referencesegmentation.We conducted experiments involving various ini-tialization strategies: scattering boundaries at ran-dom throughout the text, starting from entirely un-segmented state, or considering each symbol of thetext to be a separate token.
The results obtainedwith random initialization confirm the strong rela-tionship between compression and segmentation ac-curacy, evident in the increase of token F-score be-tween the random initialization and the terminationof the algorithm, where description length is lower(Table 6).
They also reveal the importance of thebranching entropy criterion to the generation of hy-potheses that maximize the evaluation scores andcompression, as well as the role it plays in the re-duction of computational time.T-F-Score Description TimeRandom Init Refinement Length (ms)0.0441 (0.25) 0.3833 387603.02 6660.40.0713 (0.50) 0.3721 383279.86 4975.10.0596 (0.75) 0.2777 412743.67 3753.3Table 6: Experimental results for CHILDES-N with ran-domized initialization and search path.
The numbers inbrackets represent the seed boundaries/character ratios.The greedy algorithms fail to suggest any opti-mizations that improve the compression in the ex-treme cases when the boundaries/character ratio iseither 0 or 1.
When no boundaries are given, split-ting operations produce unique types with a lowfrequency that increase the cost of both parts ofthe MDL code, and are rejected.
The algorithmruns slowly, as each evaluation operates on candi-date strings of enormous length.
Similarly, when thecorpus is broken down into single-character tokens,merging individual pairs does not produce any in-crease in compression.
This could be achieved by analgorithm that estimates the total effect from merg-ing all instances of a given pair, but such an algo-rithm would be computationally infeasible for largecorpora.Finally, we tried randomizing the search path forAlgorithm 2 after an entropy-guided initialization, toobserve a small deterioration in accuracy in the finalsegmentation (less than 1% on average).Figure 1a illustrates the effect that training datasize has on the accuracy of segmentation for the Ky-oto corpus.
The learning curves are similar through-out the different corpora.
For the CHILDES cor-pus, which has a rather limited vocabulary, tokenF-score above 70% can be achieved for datasets assmall as 5000 characters of training data, providedthat reasonable values are set for the nmax parameter(we used the values presented in Table 4 throughoutthese experiments).Figure 1b shows the evolution of token F-score bystage for all corpora.
The initialization phase seemsto have the highest contribution to the formation ofthe final segmentation, and the refinement phase ishighly dependent on the output it produces.
As aconsequence, results improve when a more adequatelanguage sample is provided during the learning oflocal dependencies at initialization.
This is evidentin the experiments with the larger unlabeled Thaiand Japanese corpora.For Japanese language with the setting for thenmax parameter that maximized compression, weobserved an almost 4% increase in the token F-scoreproduced at the end of the first phase with the Asahicorpus as training data.
Only a small (less than 1%)rise was observed in the overall performance.
Thequite larger dataset of randomly chosen Wikipediaarticles achieved no improvement.
We attributed this839Figure 1: a) corpus size / accuracy relationship (Kyoto); b) accuracy levels by phase; c) accuracy levels by phasewith various corpora for frequency statistics (Kyoto); d) accuracy levels by phase with different corpora for frequencystatistics (BEST).to the higher degree of correspondence between thedomains of the Asahi and Kyoto corpora (Figure 1c).Experiments with the BEST corpus reveal bet-ter the influence of domain-specific data on the ac-curacy of segmentation.
Performance deterioratessignificantly when out-of-domain training data isused.
In spite of its size, the assorted composite cor-pus, in which in-domain and out-of-domain trainingdata are mixed, produces worse results than the cor-pora which include only domain-specific data (Fig-ure 1d).Finally, a comparison of the proposed methodwith Bayesian n-gram models is presented in Ta-ble 7.
Through the increase of compression in therefinement phase of the algorithm, accuracy is im-proved by around 3%, and the scores approach thoseof the explicit probabilistic models of Goldwater etal.
(2009) and Mochihashi et al (2009).
The pro-posed learner surpasses the other unsupervised wordinduction models in terms of processing speed.
Itshould be noticed that a direct comparison of accu-racy is not possible with Mochihashi et al (2009),as they evaluated their system with separate datasetsfor training and testing.
Furthermore, different seg-mentation standards exist for Japanese, and there-fore the ?ground truth?
provided by the Kyoto cor-pus cannot be considered an ideal measure of accu-racy.6 Conclusions and Future WorkThis paper has presented an efficient algorithm forunsupervised word induction, which relies on acombination of evidences.
New instantiations of thebranching entropy and MDL criteria have been pro-posed and evaluated against corpora in different lan-guages.
The MDL-based optimization eliminatesthe discretion in the choice of the context lengthand threshold parameters, common in segmenta-tion models based on local statistics.
At the sametime, the branching entropy criterion enables a con-strained search through the hypothesis space, allow-ing the proposed method to demonstrate a very high840Model Corpus T-Prec T-Rec T-F L-Prec L-Rec L-F TimeNPY(3) CHILDES 0.7480 0.7520 0.7500 0.4780 0.5970 0.5310 17 minNPY(2) CHILDES 0.7480 0.7670 0.7570 0.5730 0.5660 0.5700 17 minHDP(2) CHILDES 0.7520 0.6960 0.7230 0.6350 0.5520 0.5910 -Ent-MDL CHILDES 0.7634 0.7453 0.7542 0.6844 0.5170 0.5890 2.60 secNPY(2) Kyoto - - 0.6210 - - - -NPY(3) Kyoto - - 0.6660 - - - -Ent-MDL Kyoto 0.6912 0.7365 0.7131 0.5908 0.2720 0.3725 70.16 secTable 7: Comparison of the proposed method (Ent-MDL) with the methods of Mochihashi et al, 2009 (NPY) andGoldwater et al, 2009 (HDP).performance in terms of both accuracy and speed.Possible improvements of the proposed methodinclude modeling the dependencies among neigh-boring tokens, which would allow the evaluationof the context to be reflected in the cost func-tion.
Mechanisms for stochastic optimization imple-mented in the place of the greedy algorithms couldprovide an additional flexibility of search for suchmore complex models.
As the proposed approachprovides significant performance improvements, itcould be utilized in the development of more so-phisticated novel word induction schemes, e.g.
en-semble models trained independently with differentdata.
Of course, we are also going to explore themodel?s potential in the setting of semi-supervisedmorphological analysis.ReferencesBernstein-Ratner, Nan 1987.
The phonology of parent ?child speech.
Childrens Language, 6:159?174Brent, Michael R and Timothy A. Cartwright.
1996.
Dis-tributional Regularity and Phonotactic Constraints areUseful for Segmentation.
Cognition 61: 93?125Goldwater, Sharon.
2006.
Nonparametric BayesianModels of Lexical Acquisition.
Brown University,Ph.D.
ThesisGoldwater, Sharon, Thomas L. Griffiths and Mark John-son.
2006.
Contextual dependencies in unsupervisedword segmentation.
Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th annual meeting of the Association for Com-putational Linguistics, Sydney, 673?680Goldwater, Sharon, Thomas L. Griffiths and Mark John-son.
2009.
A Bayesian framework for word segmen-tation: Exploring the effects of context.
Cognition,112:1, 21?54.Harris, Zellig.
1955.
From Phoneme to Morpheme.
Lan-guage, 31(2):190-222.Huang, Jin H. and David Powers.
2003.
Chinese WordSegmentation Based on Contextual Entropy.
Proceed-ings of 17th Pacific Asia Conference, 152?158Hutchens, Jason L. and Michael D. Alder.
1998.
Findingstructure via compression.
Proceedings of the Inter-national Conference on Computational Natural Lan-guage Learning, 79?82Jin, Zhihui and Kumiko Tanaka-Ishii.
2006.
Unsuper-vised Segmentation of Chinese Text by Use of Branch-ing Entropy.
Proceedings of the COLING/ACL onMain conference poster sessions, 428?435Johnson, Mark and Sharon Goldwater.
2009.
Improvingnonparameteric Bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanAssociation for Computational Linguistics, 317?325.Kempe, Andre.
1999.
Experiments in UnsupervisedEntropy Based Corpus Segmentation.
Proceedings ofCoNLL?99, pp.
371?385Kit, Chunyu.
2003.
How does lexical acquisition begin?A cognitive perspective.
Cognitive Science 1(1): 1?50.Kurohashi, Sadao and Makoto Nagao.
1998.
Buildinga Japanese Parsed Corpus while Improving the Pars-ing System.
Proceedings of the First InternationalConference on Language Resources and Evaluation,Granada, Spain, 719?724Lafferty, John, Andrew McCallum and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Mod-els for Segmenting and Labeling Sequence Data.
Pro-ceedings of the International Conference on MachineLearning.Li, Hang.
1998.
A Probabilistic Approach to LexicalSemantic Knowledge Acquisition and Structural Dis-ambiguation.
University of Tokyo, Ph.D. ThesisLiang, Percy.
2005.
Semi-Supervised Learning for Nat-ural Language.
Massachusets Institute of Technology,Master?s Thesis.Manber, Udi and Gene Myers.
1991.
Suffix arrays: anew method for on-line string searches.
SIAM Journalon Computing 22:935?948841Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, KarenKatz and Britta Schasberger.
1994.
The Penn Tree-bank: Annotating Predicate Argument Structure.
Hu-man Language Technology, 114?119Mochihashi, Daiichi, Takeshi Yamada and Naonori Ueda.2009.
Bayesian unsupervised word segmentation withnested Pitman-Yor language modeling.
Proceedingsof the Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conference onNatural Language Processing of the Asian Federationof Natural Language Processing, 1: 100?108Rissanen, Jorma.
1978.
Modeling by Shortest Data De-scription.
Aulomatica, 14:465?471.Saffran, Jenny R., Richard N. Aslin and Elissa L. New-port.
1996.
Statistical learning in 8-month-old infantsScience; 274:1926-1928Tsuboi, Yuta, Hisashi Kashima., Hiroki Oda, ShinsukeMori and Yuji Matsumoto.
2008.
Training Condi-tional Random Fields Using Incomplete Annotations.Proceedings of the 22nd International Conference onComputational Linguistics - Volume 1,897?904.Yu, Hua.
2000.
Unsupervised word induction usingMDL criterion.
Proceedings of tne International Sym-posium of Chinese Spoken Language Processing, Bei-jing.Zipf, George K. 1949.
Human Behavior and the Princi-ple of Least Effort.
Addison-Wesley.842
