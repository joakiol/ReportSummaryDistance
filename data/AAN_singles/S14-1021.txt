Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 171?181,Dublin, Ireland, August 23-24 2014.Dead parrots make bad pets:Exploring modifier effects in noun phrasesGerm?an Kruszewski and Marco BaroniCenter for Mind/Brain Sciences (University of Trento, Italy)(german.kruszewski|marco.baroni)@unitn.itAbstractSometimes modifiers have a strong effecton core aspects of the meaning of thenouns they are attached to: A parrot isa desirable pet, but a dead parrot is, atthe very least, a rather unusual householdcompanion.
In order to stimulate compu-tational research into the impact of mod-ification on phrase meaning, we collectedand made available a large dataset contain-ing subject ratings for a variety of nounphrases and the categories they might be-long to.
We propose to use compositionaldistributional semantics to model thesedata, experimenting with numerous distri-butional semantic spaces, phrase compo-sition methods and asymmetric similaritymeasures.
Our models capture a statis-tically significant portion of the data, al-though much work is still needed beforewe achieve a full computational account ofmodification effects.1 IntroductionNot all modifiers are created equal.
Green parrotshave all essential qualities of parrots, but dead par-rots don?t.
For example, as vocally argued by thedisgruntled costumer in Monty Python?s famousDead Parrot Sketch,1dead parrots make ratherpoor pet birds.
In modifier-head constructions(that, for the purpose of this article, we restrict toright-headed adjective-noun and noun-noun con-structions), modifiers are not simply picking a sub-set of the denotation of the head they modify, butthey are often distorting the properties of the headin a radical manner.These modifier effects on phrase meaning havebeen studied extensively by theoretical linguists,1http://en.wikipedia.org/wiki/Dead_Parrot_sketchwho have focused primarily on the extreme caseof intensional modifiers such as fake, alleged andtoy, where the phrase denotes something that isno longer (or is not necessarily) a head (a toygun is not a gun).
See McNally (2013) for a re-cent review of the linguistic literature.
Cognitivescientists have looked at modification phenomenawithin the general study of conceptual combina-tion (see Chapter 12 of Murphy (2002) for an ex-tensive review).
The cognitive tradition has fo-cused on how modification affects prototypicality:a guppy is the prototypical pet fish, but it is neithera typical pet nor a typical fish (Smith and Osher-son, 1984).
This line of research has highlightedhow strong modification effects might be the rule,rather than the exception: Wisniewski (1997) re-ports that, when subjects were asked to providethe meaning for more than 200 novel modifier-head constructions, ?70% [of the answers] in-volved the construal of a noun?s referent as some-thing other than the typical category named by thenoun [head].?
Indeed, recent research suggeststhat even the most stereotypical modifiers affectprototypicality, so that subjects are less willingto attribute to quacking ducks such obvious duckproperties as having webbed feet (Connolly et al.,2007).The impact of modification on phrase mean-ing is not only very interesting from a linguisticand cognitive perspective, but also important froma practical point of view, as it might affect ex-pected entailment patterns: If parrot entails pet,then lively parrot also entails pet.
However, as wesaw above, dead parrot doesn?t necessarily entailpet (at least not from the point of view of a dis-gruntled costumer who was just sold the corpse).Being able to track the impact that modifiers haveon heads should thus have a positive effect on im-portant tasks such as recognizing textual entail-ment, paraphrasing and anaphora resolution (An-droutsopoulos and Malakasiotis, 2010; Dagan et171al., 2009; Poesio et al., 2010).Despite their theoretical and practical import,modification effects have been largely overlookedin computational linguistics, with the notable ex-ception of Boleda et al.
(2012; 2013), who onlyfocused on the extreme case of intensional adjec-tives, studied a limited number of modifiers, anddid not attempt to capture the graded nature ofmodification (a dead parrot is not a prototypicalanimal, but a toy parrot is not an animal at all).This paper aims to stimulate computational re-search into modifier effects on phrase meaning intwo ways.
First, we introduce a new, large, pub-licly available data set of modifier-head phrasesannotated with four kinds of modification-relatedsubject ratings: whether the concept denoted bythe phrase is an instance of the concept denoted byits head (is a dead parrot still a parrot?
), to whatextent it is a member of one of the larger categoriesthe head belongs to (is it still a pet?
), and typical-ity ratings for the same questions (how typical is adead parrot as a parrot?
and as a pet?
).Second, we present a first attempt to model thecollected judgments computationally.
We choosedistributional semantics (Erk, 2012) as our frameof reference, as it produces continuous similarityscores, in line with the graded nature of the mod-ification effects we are investigating.
In partic-ular, we look at the compositional extension ofdistributional semantics (Baroni, 2013), becausewe need representations not only for words, butalso phrases, and we adopt the asymmetric simi-larity measures developed in the literature on lex-ical entailment (Kotlerman et al., 2010; Lenci andBenotto, 2012), because we are interested in anasymmetric relation (to what extent the conceptdenoted by the phrase is a good instance of the tar-get class, and not vice versa).
As far as we know,this is the first time these asymmetric measuresare applied to composed representations (Baroniet al.
(2012) experimented with entailment mea-sures applied to phrase representations directlyharvested from corpora, and not derived composi-tionally).
We are thus also providing a novel eval-uation of compositional models and asymmetricmeasures on a challenging task where they couldpotentially be very useful.22Connell and Ramscar (2001) showed good correlation ofsimilarity scores produced by the LSA distributional seman-tic model with human category typicality judgments, how-ever they did not consider phrases nor adopted an asymmetricmeasure to take directionality into account.2 The Norwegian Blue Parrot data setWe introduce Norwegian Blue Parrot (NBP),3anew, large data set to explore modification effects.Given a head noun h and a modifier adjective ornoun m, NBP contains average membership andtypicality ratings for the phrase mh both as aninstance of h and as an instance of c (a broadercategory h belongs to).
As a control, we alsopresent ratings for unmodified h as an instanceof c (we will use them below to test similaritymeasures on their ability to capture the directionof the membership relation, and to zero in on theeffect of modification vs. more general member-ship/typicality effects).
We include, and indeed fo-cus on, relations with broader categories becausethey are more prone to modification effects: In-tuitively, a dead parrot is still a parrot, but it is,at the very least, an atypical pet.
The statisticsin Table 1, discussed below, confirm our intuitionthat subjects are more likely to assign lower scoreswith respect to a broader category than to the headcategory itself (although this is, no doubt, in partby construction, since we started constructing thedataset by mining examples where mh is atypi-cal of c, not h).
We collect both membership andtypicality ratings because we expect them to havedifferent implications for sound entailment.
If xis not a member of class y, then x obviously doesnot entail y.
However, if x is an atypical y, en-tailment still holds, but some typical properties ofy might not carry over (e.g., in an anaphora reso-lution setting, we might still consider co-indexingdead parrot with animal, but not with breathingcreature, despite the fact that breathing is a highlycharacteristic property of animals).In order to make sure that NBP would contain afair number of examples affected by strong mod-ification effects, we first came up with a set of?m,h, c?
tuples where, according to our own in-tuition, m makes h fairly atypical as an instanceof c. For example, a bottle is a piece of drinkware.If we add the modifier perfume, we expect that,while subjects might still agree that a perfume bot-tle is a bottle, they should generally disagree onthe statement that a perfume bottle belongs to thedrinkware category.
We refer to tuples of thissort (e.g., ?perfume, bottle, drinkware?)
as dis-torted tuples in what follows.43Available from http://clic.cimec.unitn.it/composes/4When creating the tuples, we also used some adjectives172We then constructed a number of tuples thatshould not display a strong modification effect.
Inparticular, in order to insure that any atypical rat-ing we obtained on the distorted tuples could notbe explained away by characteristics of m or halone (rather than by their combination), for eachdistorted tuple we constructed a few more tupleswith the same h and c but a different m, thatwe did not expect to be strongly distorting (e.g.,?plastic, bottle, drinkware?).
Similarly, for eachdistorted tuple we generated a few more with thesame m, but combined with (the same or differ-ent) h and c on which the m should not exert astrong effect (?perfume, bottle, container?).
Intotal, NBP is based on 489 distorted tuples and1938 more matching tuples.We constructed NBP to insure that it wouldcontain many tuples displaying strong modifica-tion effects, and highly comparable tuples that donot feature such effects.
An alternative approachwould have been to rate phrases that were ran-domly selected from a corpus.
This would haveled to a dataset reflecting a more realistic distribu-tion of modification effects, but it would not haveguaranteed, for the same number of pairs, a fairamount of distorted tuples and comparable con-trols.
We leave the study of the natural distributionof modification strength in text to further work.To find inspiration for the tuples, we looked intovarious databases containing concepts organizedby category, namely BLESS (Baroni and Lenci,2011), ConceptNet (Speer and Havasi, 2013) andWordNet (Fellbaum, 1998).
We insured that allwords in our tuples occurred at least 200 times inthe large corpus we describe below (phrases werenot filtered by frequency, due to data sparseness).Finally, when looking for tuples matching the dis-torted ones, we made sure that the mh phrases inthe new tuples have similar Pointwise Mutual In-formation to the corresponding phrases in the dis-torted tuple (or, where the latter were not attestedin the corpus, similar m and h frequencies).
Find-ing meaningful combinations among unattested orinfrequent phrases was not an easy task and therewas not always a perfect candidate.
However, thephrases selected in this way yielded challengingitems for which there is little or no direct cor-pus evidence, so that compositional models are re-quired to account for them.that have been traditionally labeled as intensional by seman-ticists: artificial, toy, former.From each source tuple (e.g.,?plastic, bottle, drinkware?
), we generated 3instance-class combinations to be rated: mh ?
c(plastic bottle ?
drinkware), mh ?
h (plasticbottle?
bottle), h?
c (bottle?
drinkware), fora total of 5,849 pairs, that constitute the final NBPdata set (2,417 mh ?
c pairs, 2,115 mh ?
hpairs and 1,317 h?
c pairs).5For each of these pairs, we collected both mem-bership and typicality ratings through two surveyson the CrowdFlower platform.6Subjects cameexclusively from English speaking countries andno special qualifications were required from them.Membership ratings were collected by asking sub-jects whether the instance is a member of the class(formulated as a yes/no question).
In a separatestudy, we asked subjects to rate how typical the in-stance is as member of the class on a 7-point scale.For both questions, we collected 10 judgments perpair and report their averages in NBP.
For both sur-veys, we added 48 control pairs with an expectedanswer (yes/no for membership, high/low rangefor typicality), that the subjects had to provide inorder for their ratings to be included in the finalset (?gold standard?
items in crowd-sourcing par-lance).
These controls included highly prototypi-cal pairs (dog?
animal), possibly with stereotyp-ical modifiers (beautiful rose?
flower), and unre-lated pairs (biology?
dance), also possibly undermodification (popular magazine?
animal).We asked for binary rather than graded member-ship judgments because these are more in line withcommonsense intuitions about category member-ship (we might naturally speak of sparrows beingmore typical birds than penguins, but it is strangeto say that they are ?more birds?).
The standardview in the psychology of concepts (Hampton,1991) is that membership judgments are the prod-uct of a hard threshold we impose on the typicalityscale (x is not y if the typicality of x as y is belowa certain, subject-dependent threshold), althoughunder certain experimental conditions subjects canalso conceptualize membership as a graded prop-erty (Kalish, 1995).Membership and typicality ratings, especiallyin borderline cases such as those we constructed,are the output of complex cognitive processeswhere large inter-subject differences are expected,5There is a larger number of mh ?
c pairs because dif-ferent tuples can lead to the same mh?
h or h?
c combi-nations.6http://crowdflower.com/173measure mh?
c mh?
h h?
c tot.memb.
0.84 (0.2) 0.97 (0.1) 0.88 (0.2) 0.89 (0.2)typ.
5.45 (1.1) 6.29 (0.6) 5.81 (1.0) 5.84 (1.0)Table 1: NBP summary statistics: Mean averageratings and their standard deviations across pairs,itemized by instance-class type and in total.
Mem-bership values range from 0 to 1, typicality valuesfrom 1 to 7.so it doesn?t make sense to worry about ?inter-annotator agreement?
in this context.
Still, severalsanity checks indicate that, overall, our subjectsunderstood our questions as we meant them, andbehaved in a reasonably coherent manner.
First,both average membership and typicality, ratingsare significantly lower (p < 0.001) for the mh ?c pairs deriving from those tuples that we manu-ally labeled as distorted than for the non-distortedones.
Moreover, for membership, in 86% of thecases at least 8 over 10 subjects gave the same re-sponse.
For typicality, the observed average rat-ing standard deviation across pairs (1.2) is signifi-cantly below what expected by chance (p < 0.05),based on a simulated random rating distribution.Membership and typicality ratings are highly cor-related, but not identical (r = 0.76)Table 1 reports mean membership and typicalityscores in NBP.
Both ratings are negatively skewed,that is, subjects had the tendency to respond as-sertively to the membership question and to givehigh typicality scores.
This is not surprising: Be-cause of the way NBP was constructed, there areabout 4 tuples with no expected strong modifica-tion effect for each distorted tuple.
Furthermore,except for the negative control items (not enteredin NBP), our questions did not feature cases wherea negative/low response would be entirely straight-forward (of the ?is a cat a building??
kind).
Weobserve moreover that, in accordance with the in-tuition we discussed at the beginning of this sec-tion, the ratings are extremely high when the classis identical to the phrase head.
On the other hand,the mh ?
c condition displays, as expected, thelowest averages, suggesting that this will be themost interesting type to model experimentally.Table 2 presents a few example entries fromNBP.
The first block of the table illustrates caseswith the highest possible membership and typical-ity scores.
At the other extreme, the second blockcontains examples with very low membership andtypicality.
Interestingly, there are also cases, suchinstance class memb.
typ.top membership, top typicalitygourmet soup food 1.00 7.00huge tiger predator 1.00 7.00sugared soda drink 1.00 7.00live fish animal 1.00 7.00Thai rice rice 1.00 7.00silver spoon spoon 1.00 7.00low membership, low typicalityfatal shooting sport 0.20 1.40human egg food 0.40 1.50perfume bottle drinkware 0.10 1.30explosive vest commodity 0.30 1.90lemon water chemical 0.20 1.60creamy rice bean 0.20 1.30top membership, (relatively) low typicalitysick tuna tuna 1.00 3.20explosive vest vest 1.00 3.50perforated sieve tool 1.00 4.20bottled oxygen substance 1.00 4.30grilled trout creature 1.00 4.40educational toy amusement 1.00 4.50Table 2: Instance-class pairs illustrating variouscombinations of membership and typicality rat-ings in NBP.as the ones in the third block of the table, where allsubjects agreed on class membership, but the typ-icality scores are relatively low (we did not findclear cases of the opposite pattern, and indeed wewould have been surprised to find highly typicalinstances of a class not being treated as membersof the class).Some examples in Table 2 illustrate an impor-tant design choice we made in constructing NBP,namely, to ignore the issue of whether potentialmodification effects are actually due to the modi-fier and the category pertaining to different wordsenses of the head term.
One might argue, forexample, that egg has a food sense and a repro-ductive vessel sense.
The human modifier picksthe second sense, and so, obviously, human eggsare judged as bad instances of food.
While wesee the point of this objection, we think it?s im-possible to draw a clear-cut distinction betweendiscrete word senses (even in the rather extremeegg case, the eggs we eat are reproductive ves-sels from a chicken point of view!).
This hasbeen long recognized in the linguistic and cog-nitive literature (Kilgarriff, 1997; Murphy, 2002),174and even by the computational word sense disam-biguation community, that is currently addressingthe continuous nature of polysemy by shifting tothe lexical-substitution-in-context task (McCarthyand Navigli, 2009).
Context provides fundamen-tal cues to disambiguating polysemous words, andnoun modifiers typically act as important disam-biguating contexts for the nouns.
Thus, we thinkthat it is more productive for computational sys-tems to handle modifier-triggered disambiguationas a special case of the more general class of mod-ification effects, than to engage in the quixoticpursuit to determine, a priori, what?s the bound-ary between a word-sense and a ?pure?
modifi-cation effect.
Note in Table 2 that grilled troutwas unanimously rated by subjects as an instanceof the creature category, despite the fact that thecooking-related grilled modifier cues a classicshift from an animal (and thus creature) sense tofood (Copestake and Briscoe, 1995).
Exampleslike this suggest that our agnosticism is warranted.3 Methods3.1 Composition modelsWe experiment with many ways to derive a phrasevector by combining the vectors of its constituents.Mitchell and Lapata (2010) proposed a set of sim-ple models in which each component of the phrasevector is a function of the corresponding compo-nents of the constituent vectors.
Given vectors ~aand~b, the weighted additive model (wadd) returnstheir weighted sum: ~p = w1~a + w2~b.
In the dila-tion model (dil), the output vector is obtained bydecomposing one of the input vectors, say~b, intoa vector parallel to ~a and its orthogonal counter-part, and then dilating only the parallel vector by afactor ?
before re-combining.
The correspondingformula is: (~a ?~a)~b + (?
?
1)(~a ?~b)~a.
In our ex-periments, we stretch the head vector in the direc-tion of the modifier (i.e., ~a is the modifier,~b is thehead).
In the multiplicative model (mult), vectorsare combined by component-wise multiplication,such that each phrase component piis given by:pi= aibi.Guevara (2010) and Zanzotto et al.
(2010) pro-pose a full form of the additive model (fulladd),where the two constituent vectors are multipliedby weight matrices before being added, so thateach phrase component is a weighted sum of allconstituent components: ~p = W1~a+W2~b.Finally, the lexical function (lexfunc) model ofBaroni and Zamparelli (2010) and Coecke et al.
(2010) takes inspiration from formal semanticsto characterize composition as function applica-tion.
In particular, in modifier-head phrases, themodifier is treated as a linear function operatingon the head vector.
Given that linear functionscan be expressed by matrices and their applicationby matrix-by-vector multiplication, the modifier isrepresented by a matrix A to be multiplied withthe modifier vector~b, so that: ~p = A~b.We use the DISSECT toolkit7to estimate theparameters of the composition methods and de-rive phrase vectors.
In particular, DISSECT findsoptimal parameter settings by learning to approx-imate corpus-extracted phrase vector exampleswith least-squares methods (Dinu et al., 2013).We use as training examples all the modifier-headphrases that contain a modifier of interest and oc-cur at least 50 times in our source corpus (see Sec-tion 3.3 below).3.2 Asymmetric similarity measuresSeveral measures to identify word pairs that standin an instance-class relationship by comparingtheir vectors have been proposed in the recent dis-tributional semantics literature (Kotlerman et al.,2010; Lenci and Benotto, 2012; Weeds et al.,2004).8While the task of deciding if u is in class vis typically framed (also by distributional semanti-cists) in binary, yes-or-no terms, all proposed mea-sures return a continuous numerical score.9Con-sequently, we conjecture that they might be well-suited to capture the graded notions of class mem-bership and typicality we recorded in NBP.10In what follows, we use wx(f) to denote theweight (value) of feature (dimension) f in the dis-tributional vector of term x. Fxdenotes the set offeatures (dimensions) in the vector of x such thatwx(f) > t, where t is a predefined threshold todecide whether a feature is active.11Importantly,7http://clic.cimec.unitn.it/composes/toolkit/8We speak of ?instance-class relations?
in a very broadand loose sense, to encompass classic relations such as hy-ponymy but also the fuzzier notion of lexical entailment.9SVM classifiers have also been shown by Baroni et al.
(2012) to be well-suited for entailment detection, but they donot naturally return continuous scores.10Subjects had to answer a yes/no question concerningclass membership, but by averaging their response we derivecontinuous membership scores.11The obvious choice for t is 0.
However, when work-ing with the low-rank spaces described in Section 3.3 below,we set t to 0.1, since after SVD/NMF smoothing we observe175all measures assume non-negative values.Most asymmetric measures proposed in the lit-erature build upon the distributional inclusion hy-pothesis, stating that ?if u is a semantically nar-rower term than v, then a significant numberof salient distributional features of u is includedin the feature vector of v as well?
(Lenci andBenotto, 2012).
In our terminology, u is the poten-tial instance, and v is the class.
We re-implementall the measures adopted by Lenci and Benotto,namely weedsprec, cosweeds, clarkede and invcl(see their paper for the original references):weedsprec(u, v) =?f?Fu?Fvwu(f)?f?Fuwu(f)cosweeds(u, v) =?weedsprec(u, v)?
cosine(u, v)clarkede(u, v) =?f?Fu?Fvmin(wu(f), wv(f))?f?Fuwu(f)invcl(u, v) =?clarkede(u, v)?
(1?
clarkede(u, v))The cosweeds formula combines weedsprecwith the widely used symmetric cosine measure:cosine(u, v) =?f?Fu?Fvwu(f)?
wv(f)??f?Fuwu(f)2??
?f?Fvwv(f)2Finally, we experiment with the carefullycrafted balapinc measure of Kotlerman et al.
(2010):balapinc(u, v) =?lin(u, v) ?
apinc(u, v)where the lin term is computed as follows:lin(u, v) =?f?Fu?Fvwu(f) + wv(f)?f?Fuwu(f) +?f?Fvwv(f)The balapinc score is the geometric averageof a symmetric similarity measure (lin) and thestrongly asymmetric apinc measure, that takeslarge values when dimensions with high values inthe vector of the more specific term are also highin the vector of the more general term (refer toKotlerman et al.
(2010) for the apinc formula).widespread low-frequency noise.3.3 Distributional semantic spacesWe extract co-occurrence information from a cor-pus of about 2.8 billion words obtained by con-catenating ukWaC,12Wikipedia13and the BritishNational Corpus.14With DISSECT, we build co-occurrence vectors for the top 20K most frequentlemmas in the source corpus (plus any NBP termmissing from this list).
We treat the top 10Kmost frequent lemmas as context elements.
Weconsider context windows of 2 and 20 words onthe two sides of the targets.
We weight the vec-tors by non-negative Pointwise Mutual Informa-tion and Local Mutual Information (Evert, 2005).We experiment with vectors in the resulting full-rank (10K-dimensional) semantic spaces as wellas with vectors in spaces of ranks 100 and 300.Rank reduction is performed by applying the Sin-gular Value Decomposition (Golub and Van Loan,1996) or Non-negative Matrix Factorization (Leeand Seung, 2000).
It is customary to represent theoutput of these operations directly in a dense low-dimensional space.
However, the asymmetric sim-ilarity measures we use assume sparse vectors (orthe ?inclusion?
criterion would be meaningless),so we project back the outcome of SVD and NMFto sparse 10K-dimensional but low-rank spaces.
Intotal, we explore 20 distinct semantic spaces.We also collect co-occurrence vectors forthe phrases needed to estimate the composi-tion method parameters (see Section 3.1 above).We use DISSECT?s ?peripheral space?
option toproject the phrase raw count vectors into the vari-ous spaces without affecting their structure.Due to memory constraints, we restrict evalua-tion in the full-rank spaces to the wadd and multmodels.4 ExperimentsGiven the methods described above, the mainquestion we want to answer is: Which combina-tion of compositional model and asymmetric sim-ilarity measure yields a better fit for the data in theNBP dataset?We start however with a sanity check on theability of the measures to capture the direction ofthe instance-class membership relation.
Even ameasure that is good at capturing degrees of mem-bership/typicality won?t be of much practical use12http://wacky.sslmit.unibo.it13http://en.wikipedia.org14http://www.natcorp.ox.ac.uk176clarkede weedsprec balapinc cosweeds invclLow-rank spaces10 8 11 8 7Full-rank spaces2 4 4 4 2Table 3: Number of spaces (over totals of 16 low-rank and 4 full-rank spaces) in which each mea-sure was able to predict class membership direc-tion significantly above chance.if it is not able to tell us which item in a pair is theinstance and which is the class.Detecting membership direction As describedin Section 2 above, NBP also contains single-word h?
c pairs (parrot?
pet).
We extractedthe subset of those that all judges considered tobe in the category membership relation, and wechecked them manually to make sure that the di-rection was one-way only.
This resulted in a setof 639 pairs where the membership relation holdsunidirectionally.
We tested all combination of se-mantic spaces (Section 3.3) and asymmetric sim-ilarity measures (Section 3.2) on the task of as-signing a higher score to the pairs in the h ?
c(vs. c ?
h) direction (e.g., (score(parrot ?pet) > score(pet ?
parrot)).
Table 3 reports,for each measure, the number of spaces in whichthe measure was able to predict membership di-rection significantly better than chance (binomialtest, p < 0.05).
We report results on full- andlow-rank (SVD, NMF) spaces separately since, asdiscussed above, for most composition models wecan only use the latter.
We observe that all mea-sures are able to significantly detect directionalityin at least some spaces.
For all the analyses below,we exclude from further testing the space-measurecombinations that failed to pass this sanity check,since they are clearly failing to capture propertiespertaining to the instance-class relation (if a com-bination is not able to tell that it is a parrot that isa pet, and not vice versa, there is no point in ask-ing if the same combination is able to model howtypical a dead parrot is as a pet).Modeling typicality ratings of mh ?
c pairsNext, for each of the remaining spaces, we firstperformed composition as described in Section 3.1above to build the representations for the nominalphrases in the NBP dataset, and then computedasymmetric similarity scores for pairs made of aphrase and the corresponding potential class.We computed the correlations between meanhuman membership or typicality ratings and thescores produced with each combination of com-position model, similarity measure and space.The resulting performance profiles for member-ship and typicality are very highly correlated (r =.99), and we thus report only the latter.
We leaveit to further work to devise measures that are morespecifically tuned to capture membership or typi-cality.Table 4 reports the top correlation coefficientsbetween typicality judgments and scores of eachmh ?
c pair (dead parrot?
pet) across spaces,organized by measures and composition meth-ods.
The best correlation is achieved with theweedsprec measure using the mult compositionmodel in a full-rank space (precisely that of con-text window size 2 and ppmi weighting).
Recallthat mult returns the component-wise product ofthe vectors it combines.
Thus, modification un-der mult is carried out by picking only those fea-tures of the head that are also present in the mod-ifier, and enhancing them by a factor given by themodifier?s feature value.
The weedsprec measureis then given by the weighted proportion of activefeatures in mh that are also active in c. Therefore,the more the modifier shares features with the par-ent category, the higher weedsprec will be.
Thismight explain why weedsprec is a good fit for themult model in measuring degrees of category typ-icality.Looking at composition methods, there is no ev-idence that the more complex, matrix-based ful-ladd and lexfunc approaches are performing anybetter than the simple multiplicative and additivemethods.
Indeed, mult shows the most consistentoverall performance, confirming the conclusion ofBlacoe and Lapata (2012) that, at the present time,when it comes to composition, ?simpler is better?.A related point emerges from the comparison ofthe low- and full-rank results for mult and wadd.The smoothing process due to dimensionality re-duction is quite disruptive for the current asym-metric measures, that are based on feature inclu-sion.
This is a further reason to stick to simplercomposition methods, that can be applied directlyin the full-rank spaces.Regarding the measures themselves, we see thatcosweeds, that balances weedsprec with the clas-sic cosine score, is the most robust, returning good177clarkede weedsprec balapinc cosweeds invclLow-rank spacesdil 9* 15* 16* 19* 8*fulladd 17* 16* 12* 24* ?3lexfunc 17* 12* 12* 27* ?2mult 13* 19* 19* 29* 12*wadd 14* 14* 16* 27* ?2Full-rank spacesmult 9* 39* 33* 36* 15*wadd 30* 34* 31* 35* 14*Table 4: Percentage Pearson r between asymmet-ric similarity measures andmh?
c typicality rat-ings.
*p < 0.001results across all composition methods.
On theother hand, the related clarkede and invcl mea-sures turn out to be quite brittle.The highly significant correlations show that themeasures do capture to some extent the patterns ofvariance in the data.
However, when consideringpotential practical applications, even the highestreported correlation (.39) is certainly not impres-sive, indicating that there is plenty of room for fur-ther research into developing better compositionmethods and/or membership/typicality measures.Focusing on the modifier effect for mh?
cpairs The typicality judgment for dead parrot asa pet is influenced by two factors: how typical par-rots are as pets, and how much more or less typicaldead parrots are as pets, as opposed to parrots ingeneral.
A good model must be able to captureboth factors (and this is what we tested above).However, we are also interested in assessing towhat extent the models are capturing the modifi-cation effectproper, as opposed to the overall de-gree of typicality of the h concept as member ofthe c category.
To focus on the modification fac-tor, we partialed out the h?c (parrot?pet) rat-ings from the mh?c (dead parrot?pet) ratingsand from the corresponding model scores (that is,we correlated the residuals of mh?c ratings andmodel-produced scores after regressing the h?cratings on both).
The results are shown in Table5.
Correlations are lower overall, but the generalpicture from the previous analysis still holds, con-firming that the computational models are (also)capturing modifier effects.
Interestingly, wadd, diland fulladd generally undergo larger performancedrops than mult and lexfunc.
Evidently, modelslike the latter, in which the modifier selects therelevant features from the head, are better suitedto explain modification than the former, in whichclarkede weedsprec balapinc cosweeds invclLow-rank spacesdil 5 ?1 ?1 ?2 7*fulladd 10* 7* 5+ 7+ ?2lexfunc 15* 9* 10* 18* ?2mult 4+ 14* 13* 15* 9*wadd 7+ 7* 9* 12+ ?2Full-rank spacesmult 1 25* 21* 24* 5+wadd 11* 18* 13* 20* 2Table 5: Percentage Pearson r between asymmet-ric similarity measures andmh?
c typicality rat-ings where h ?
c scores have been partialed out.
*p < 0.001, +p < 0.05the modifier features are just added to those of thehead by means of a linear combination.Modeling typicality ratings of mh ?
h pairsWe repeated the first analysis for pairs of the typemh ?
h (dead parrot?
parrot).
The results,shown in Table 6, are lower than in the previousanalysis.
This is probably due to the fact that, asdiscussed in Section 2, when the very same con-cept is used as phrase head and category, judg-ments are subject to a strong ceiling effect, andnone of our measures is designed to flatten outabove a certain threshold.
Indeed, if we measurethe skewness of the typicality ratings,15we obtainthat, while for h?
c and mh?
c the skewness isof?1.9 and?1.5, respectively, formh?h it getsto ?3.9.In any case, the results confirm the brittleness ofthe clarkede and invcl measures.
The linguisticallymotivated lexfunc model emerges here as a com-petitive alternative to the simpler models.
Still, thebest results are obtained with mult and cosweeds(on the full-rank, context window size 20, ppmiweighted space).
Notably, weedsprec applied to apair of the type mh?
h, where the phrase is con-structed using the mult model, results in a constantvalue of 1, whatever the modifier and the headnoun is.
This is due to the fact that the features ofa phrase composed using mult are a subset of thefeatures of the head,16and in this case the head isthe same as the category.
Therefore, by definition,weedsprec yields a score of 1 for every pair, thevariance is null and hence the correlation is unde-15A skewness factor of 0 means that the distribution is bal-anced around the mean, while the more negative the coeffi-cient is, the more the left tail is longer and the distribution isconcentrated to the right (toward high typicality values in ourcase).16In set notation: Fu?
Fv= Fusince Fu?
Fv178clarkede weedsprec balapinc cosweeds invclLow-rank spacesdil 2 ?1 ?2 ?3 4fulladd 5+ 5+ 2 1 ?1lexfunc 14* 8* 14* 17* ?1mult 3 - 13* 15* 5+wadd 6+ 8* 7+ 6 ?3Full-rank spacesmult ?2 - 18* 19* ?2wadd 7* 13* 7* 12* ?2Table 6: Percentage Pearson r between asymmet-ric similarity measures andmh?
h typicality rat-ings.
*p < 0.001, +p < 0.05fined.
As a consequence, in this case cosweeds,which is the geometric mean between weedsprecand cosine, reduces to cosine similarity!
The lattermight be effective in capturing the degree of simi-larity between the phrase and its potential categorybut, as a symmetric measure, it cannot, alone, pro-vide a full account of category typicality effects.5 ConclusionWe introduced the challenge of quantifying theimpact of modification on the meaning of nounphrases to the computational linguistics commu-nity.
We presented a new dataset that collectsmembership and typicality ratings for modifier-head phrases with respect to the category repre-sented by the head as well as a broader category.Since accounting for modifier distortion requiressemantic representations of phrases and model-ing graded judgments, we consider this an idealtestbed for compositional distributional semantics.In the interaction between compositional mod-els and directional similarity measures, we haveobserved that simpler models yield better results.Specifically, mult and wadd are economical com-position models than can be applied on full-rankspaces, which in turn work best with our similar-ity measures.Psychologists studying modification effects inconcept combination have proposed models thatare usually quite complex, relying on hand-craftedfeature definitions and making very strong as-sumptions about the combination process (see forexample Cohen and Murphy (1984), Smith et al.(1988)).
Some of these assumptions have led otherresearchers to argue that prototypes do not com-pose at all (Connolly et al., 2007).
In contrast,the approach we borrow from distributional se-mantics, while only mildly successful for now, hasthe advantage of being very simple both in its con-struction and application, and in the assumptionsthat it makes.Also notable is that we are putting under thesame umbrella tasks that have been traditionallytackled separately.
For example, among the ef-fects present in the dataset, we can find both wordsense disambiguation (see discussion at the end ofSection 2) and what Murphy (2002) calls ?knowl-edge effects?
(e.g., a plane makes a very good ma-chine, but a paper plane doesn?t).
Moreover, theseeffects can also interact (people know that a hu-man egg is actually a single, small cell, and hencenot even cannibals would consider it satisfactoryfood).
We can thus explore the empirical ques-tion of whether all these related phenomena canbe tackled together, with a single model account-ing for all of them.In conclusion, the challenge that we intro-duced brings together concept combination andnon-subsective modification phenomena studiedin psychology and theoretical linguistics, and triesto handle them with the standard machinery ofcomputational linguistics.
This challenge hasproved quite difficult for current tools, but this isexactly what we expected in the first place.
Ourgoal, from the outset, was to create a task thatcould help us delimiting the boundaries of com-putational methods for characterizing human con-cepts, while delimiting, at the same time, the no-tion of human concepts itself.AcknowledgmentsWe acknowledge ERC 2011 Starting IndependentResearch Grant n. 283554 (COMPOSES).ReferencesIon Androutsopoulos and Prodromos Malakasiotis.2010.
A survey of paraphrasing and textual entail-ment methods.
Journal of Artificial Intelligence Re-search, 38:135?187.Marco Baroni and Alessandro Lenci.
2011.
Howwe BLESSed distributional semantic evaluation.
InProceedings of the EMNLP GEMS Workshop, pages1?10, Edinburgh, UK.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,and Chung-Chieh Shan.
2012.
Entailment above179the word level in distributional semantics.
In Pro-ceedings of EACL, pages 23?32, Avignon, France.Marco Baroni.
2013.
Composition in distributionalsemantics.
Language and Linguistics Compass,7(10):511?522.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for seman-tic composition.
In Proceedings of EMNLP, pages546?556, Jeju Island, Korea.Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,and Louise McNally.
2012.
First order vs. higherorder modification in distributional semantics.
InProceedings of EMNLP, pages 1223?1233, Jeju Is-land, Korea.Gemma Boleda, Marco Baroni, Louise McNally, andNghia The Pham.
2013.
Intensionality was onlyalleged: On adjective-noun composition in distribu-tional semantics.
In Proceedings of IWCS, pages35?46, Potsdam, Germany.Graham Chapman.
1989.
The complete MontyPython?s flying circus : all the words.
PantheonBooks, New York.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributional model of meaning.
Linguis-tic Analysis, 36:345?384.Benjamin Cohen and Gregory L Murphy.
1984.
Mod-els of concepts.
Cognitive Science, 8(1):27?58.Louise Connell and Michael Ramscar.
2001.
Usingdistributional measures to model typicality in cate-gorization.
In Proceedings of CogSci, pages 226?231, Edinburgh, UK.Andrew Connolly, Jerry Fodor, Lila Gleitman, andHenry Gleitman.
2007.
Why stereotypes don?t evenmake good defaults.
Cognition, 103(1):1?22.Ann Copestake and Ted Briscoe.
1995.
Semi-productive polysemy and sense extension.
Journalof Semantics, 12:15?67.Ido Dagan, Bill Dolan, Bernardo Magnini, and DanRoth.
2009.
Recognizing textual entailment: ratio-nale, evaluation and approaches.
Natural LanguageEngineering, 15:459?476.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013.
General estimation and evaluation of com-positional distributional semantic models.
In Pro-ceedings of ACL Workshop on Continuous VectorSpace Models and their Compositionality, pages 50?58, Sofia, Bulgaria.Katrin Erk.
2012.
Vector space models of word mean-ing and phrase meaning: A survey.
Language andLinguistics Compass, 6(10):635?653.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences.
Ph.D dissertation, Stuttgart University.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,MA.Gene Golub and Charles Van Loan.
1996.
MatrixComputations (3rd ed.).
JHU Press, Baltimore, MD.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of GEMS, pages 33?37,Uppsala, Sweden.James Hampton.
1991.
The combination of prototypeconcepts.
In Paula Schwanenflugel, editor, The psy-chology of word meanings, pages 91?116.
Erlbaum,Hillsdale, NJ.Charles Kalish.
1995.
Essentialism and graded mem-bership in animal and artifact categories.
Memoryand Cognition, 23(3):335?353.Adam Kilgarriff.
1997.
I don?t believe in word senses.Computers and the Humanities, 31:91?113.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional distribu-tional similarity for lexical inference.
Natural Lan-guage Engineering, 16(4):359?389.Daniel Lee and Sebastian Seung.
2000.
Algorithms forNon-negative Matrix Factorization.
In Proceedingsof NIPS, pages 556?562.Alessandro Lenci and Giulia Benotto.
2012.
Identi-fying hypernyms in distributional semantic spaces.In Proceedings of *SEM, pages 75?79, Montreal,Canada.Diana McCarthy and Roberto Navigli.
2009.
The En-glish lexical substitution task.
Language Resourcesand Evaluation, 43(2):139?159.Louise McNally.
2013.
Modification.
In Maria Aloniand Paul Dekker, editors, Cambridge Handbook ofSemantics.
Cambridge University Press, Cambridge,UK.
In press.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Gregory Murphy.
2002.
The Big Book of Concepts.MIT Press, Cambridge, MA.Massimo Poesio, Simone Ponzetto, and Yan-nick Versley.
2010.
Computational modelsof anaphora resolution: A survey.
http://clic.cimec.unitn.it/massimo/Publications/lilt.pdf.Edward Smith and Daniel Osherson.
1984.
Concep-tual combination with prototype concepts.
Cogni-tive Science, 8(4):337?361.Edward E Smith, Daniel N Osherson, Lance J Rips,and Margaret Keane.
1988.
Combining prototypes:A selective modification model.
Cognitive Science,12(4):485?527.180Robert Speer and Catherine Havasi.
2013.
Con-ceptNet 5: A large semantic network for relationalknowledge.
In Iryna Gurevych and Jungi Kim, edi-tors, The People?s Web Meets NLP, pages 161?176.Springer, Berlin.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of COLING, pages 1015?1021, Geneva, Switzerland.Edward Wisniewski.
1997.
When concepts combine.Psychonomic Bulletin & Review, 4(2):167?183.Fabio Zanzotto, Ioannis Korkontzelos, FrancescaFalucchi, and Suresh Manandhar.
2010.
Estimat-ing linear models for compositional distributionalsemantics.
In Proceedings of COLING, pages 1263?1271, Beijing, China.181
