MCDONNELL DOUGLAS ELECTRONIC SYSTEMS COMPANY :MUC-3 Test Results and Analysi sDavid de Hilster and Amnon MeyersAdvanced Computing Technologies LabMcDonnell Douglas Electronics Systems Compan y1801 East Saint Andrew PlaceSanta Ana, California 92705-652 0e-mail : vox@young .mdc .co mphone : (714)566-5956RESULTSINLET scored 25% recall and 35% precision after interactive correction .
Non-interactive scores were 22% and 32%, respectively .
Relative to other sites, we ranked 7th i nrecall and 9th in precision .Because we are in transition from one system (VOX) to another (INLET), our scoresreflect the performance of a very new and incomplete analyzer, as well as a small vocabularyand knowledge base.
To be able to participate in MUC3, we implemented only a skimmingcapability, rather than a full-fledged syntax-driven language analyzer .
Considering the statu sof our project, we feel INLET's performance is highly commendable .
We ourselves weresurprised by the success of skimming alone in performance on the MUC3 task, especiall yconsidering the preliminary nature of our work in this area .TEST SETTING SLike many other developers of NLP systems, we have had no reason, up to now, toparameterize the trade-off between recall and precision .
At various points in the past fe wmonths, we had tested heuristics that increased recall while reducing precision .Participation in MUC3 has led us to examine such a parameter more seriously .
Inparticular, a confidence metric, whereby the system assesses its own confidence i nunderstanding a text, seems to be a useful component of NLP system development .
We can attac hsuch confidence factors to every action that the system takes in processing text, in extractinginformation from text, and in correlating the information to produce output .As an NLP system improves and its knowledge of linguistics and the domain become mor ecomplete, confidence factors can be raised .
Additionally, an arsenal of heuristics could be mad eavailable, depending on the confidence threshold assigned to the language processing task .Assignment of confidence ratings can be assisted by empirical data .
For example, we canassign an overall system confidence in identifying the incident type by scoring performance fo rthe entire MUC3 corpus .92EXPENDITURE OF EFFOR TOnce the skimmer was completed (April 22), we spent approximately 2 man-month scustomizing INLET to the MUC3 domain and task.
A substantial portion of that time was spent i nimplementing code for filling the various slots in the MUC3 template and in developin gheuristics to improve recall and precision .
Somewhat less time was spent in building an dapplying generic and domain-specific grammar rules .
System testing was relatively painless ,because INLET typically processed 100 messages in 45 minutes, and several Sun Sparcstation swere typically available for running tests .
Unfortunately, we allowed too little time fo rvocabulary addition, so that many important words and phrases were omitted (e .g .
,"machinegun", "automatic weapons") .LIMITING FACTOR SOur main problems resulted from working with a new and incomplete system .
Too oftenwe had to devote our time to fixing bugs or making improvements in the skimmer, in th egraphic representation tools, and in the knowledge addition tools .
Starting with a smal lvocabulary and little linguistic and domain knowledge was disadvantageous .
Adding a lot o fknowledge to the system over a short period of time caused many problems to surface (e .g .
,initializing the system became a time-waster, system tables overflowed several times) .
Lack ofan internal representation for information extracted from the text was yet another limitation o ndevelopment .TRAININ GWe used the entire development corpus, including the key templates, for gatherin gdomain information .
For example, we used the key templates to get fairly complete lists o fperpetrators, targets, instruments, and so on .
Similarly, we searched the corpus for keywords ,temporal, locative, and other patterns .
Many of our domain-specific grammar rules wer ecrafted using the results of such searches .The first 100 messages of the development set served as a primary development andtesting vehicle .
TST1 messages were run occasionally in order to gauge progress on unseenmessage sets .In order to shake out bugs in the system, we processed half the development set inbatches of 100 messages several days before the testing deadline .STRENGTHS AND WEAKNESSE SIn general, skimming worked much better than expected .
Based merely on our initia lresults for MUC3, we conclude that skimming is a powerful adjunct to deeper processing of text .We feel that with several months' work, continued development of skimming techniques ,combined with knowledge base and vocabulary development, would substantially raise our MUC 3score .Skimming provides extremely fast, simple, and robust text processing .
While keywordand pattern-based methods for NLP have usually met with scorn, we feel a review of thes emethods is called for .On the other hand, we are aware of the limitations of any approach that doesn't analyz etext as deeply as possible .
In order to segment incidents with great accuracy, linguistic contex t93as well as script-level understanding of the text are required .
Many reference resolutio nproblems also require such knowledge .In the near future, we will merge our skimming capability with a bottom-up syntacti canalysis mechanism, and also incorporate script-based understanding mechanisms .The INLET customization tools have proved their worth by supporting hierarchy ,grammar rule, and vocabulary addition.
Even our qualified success would have been impossibl ewithout the effectiveness of the knowledge addition framework .HITS AND MISSESOur system is fairly good at determining the incident type, using a hierarchy of keywords and patterns.
With just a few specialized rules, the system is able to process appositive sto find perpetrators, perpetrator organizations, physical targets, and human targets .
Anextensive temporal grammar was developed, though not much correlation of multiple tempora lreferences has been implemented .
A similar situation holds for locative phrases .Simple gaps in knowledge and vocabulary caused many misses on the TST2 messages .Missing vocabulary (e .g., "killings"), missing domain rules (e .g., "explosion caused damage") ,missing generic rules (e.g., "actor participated in action on object"), and missing mechanism sof various kinds led to substantially lower performance than we would expect of a more matur eINLET system .
Missing mechanisms include lack of threat handling, lack of any inferencin gcapability, lack of spelling correction, and lack of rejection of incidents for even simple reasons(e.g., an abstract object such as the "economy" is attacked) .PORTABILIT YBecause INLET is a customization shell, portability of the specific knowledge added to th esystem is not a major concern .
In 2 man-months, we were able to achieve a 25% recall and35% precision score with a relatively immature INLET system .
When the system is completed ,we expect similar customization time to result in a better system for the particular domain an dtask .The skimmer framework and knowledge addition framework are generic, as is the coreknowledge base and vocabulary .
On top of this layer is a substantial body of domain-specifi ccode and knowledge, which would necessarily have to be replaced for a new domain and task .LESSONS LEARNEDWe have demonstrated that the INLET knowledge addition framework and skimmer ca nquickly support customization to a new domain and task .
We have found that the graphicinterface for knowledge addition has speeded up customization over a system like VOX .
Finally ,we have found that skimming is a critical adjunct to deeper NLP .Our participation in MUC3 has shown us the high value of formal testing and compariso nwith other NLP efforts .
We intend to continue using the MUC3 corpus and testing system for ou rsystem development, test, and evaluation .94
