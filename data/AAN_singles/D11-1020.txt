Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 216?226,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Novel Dependency-to-String Model for Statistical Machine TranslationJun Xie, Haitao Mi and Qun LiuKey Laboratory of Intelligent Information ProcessiongInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{junxie,htmi,liuqun}@ict.ac.cnAbstractDependency structure, as a first step towardssemantics, is believed to be helpful to improvetranslation quality.
However, previous workson dependency structure based models typi-cally resort to insertion operations to completetranslations, which make it difficult to spec-ify ordering information in translation rules.In our model of this paper, we handle thisproblem by directly specifying the orderinginformation in head-dependents rules whichrepresent the source side as head-dependentsrelations and the target side as strings.
Thehead-dependents rules require only substitu-tion operation, thus our model requires noheuristics or separate ordering models of theprevious works to control the word order oftranslations.
Large-scale experiments showthat our model performs well on long dis-tance reordering, and outperforms the state-of-the-art constituency-to-string model (+1.47BLEU on average) and hierarchical phrase-based model (+0.46 BLEU on average) on twoChinese-English NIST test sets without resortto phrases or parse forest.
For the first time,a source dependency structure based modelcatches up with and surpasses the state-of-the-art translation models.1 IntroductionDependency structure represents the grammaticalrelations that hold between the words in a sentence.It encodes semantic relations directly, and has thebest inter-lingual phrasal cohesion properties (Fox,2002).
Those attractive characteristics make it pos-sible to improve translation quality by using depen-dency structures.Some researchers pay more attention to use de-pendency structure on the target side.
(Shen et al,2008) presents a string-to-dependency model, whichrestricts the target side of each hierarchical rule to bea well-formed dependency tree fragment, and em-ploys a dependency language model to make the out-put more grammatically.
This model significantlyoutperforms the state-of-the-art hierarchical phrase-based model (Chiang, 2005).
However, those string-to-tree systems run slowly in cubic time (Huang etal., 2006).Using dependency structure on the source sideis also a promising way, as tree-based systems runmuch faster (linear time vs. cubic time, see (Huanget al, 2006)).
Conventional dependency structurebased models (Lin, 2004; Quirk et al, 2005; Dingand Palmer, 2005; Xiong et al, 2007) typicallyemploy both substitution and insertion operation tocomplete translations, which make it difficult tospecify ordering information directly in the transla-tion rules.
As a result, they have to resort to eitherheuristics (Lin, 2004; Xiong et al, 2007) or sepa-rate ordering models (Quirk et al, 2005; Ding andPalmer, 2005) to control the word order of transla-tions.In this paper, we handle this problem by di-rectly specifying the ordering information in head-dependents rules that represent the source side ashead-dependents relations and the target side asstring.
The head-dependents rules have only onesubstitution operation, thus we don?t face the prob-lems appeared in previous work and get rid of the216heuristics and ordering model.
To alleviate datasparseness problem, we generalize the lexicalizedwords in head-dependents relations with their cor-responding categories.In the following parts, we first describe the moti-vation of using head-dependents relations (Section2).
Then we formalize our grammar (Section 3),present our rule acquisition algorithm (Section 4),our model (Section 5) and decoding algorithm (Sec-tion 6).
Finally, large-scale experiments (Section 7)show that our model exhibits good performance onlong distance reordering, and outperforms the state-of-the-art tree-to-string model (+1.47 BLEU on av-erage) and hierarchical phrase-based model (+0.46BLEU on average) on two Chinese-English NISTtest sets.
For the first time, a source dependency treebased model catches up with and surpasses the state-of-the-art translation models.2 Dependency Structure andHead-Dependents Relation2.1 Dependency SturctureA dependency structure for a sentence is a directedacyclic graph with words as nodes and modificationrelations as edges.
Each edge direct from a head toa dependent.
Figure 1 (a) shows an example depen-dency structure of a Chinese sentence.2010?
FIFA?????????
?2010 FIFA [World Cup] in/at [South Africa]successfully holdEach node is annotated with the part-of-speech(POS) of the related word.For convenience, we use the lexicon dependencygrammar (Hellwig, 2006) which adopts a bracketrepresentation to express a projective dependencystructure.
The dependency structure of Figure 1 (a)can be expressed as:((2010?)
(FIFA)???)
(?(??))
(??)?
?where the lexicon in brackets represents the depen-dents, while the lexicon out the brackets is the head.To construct the dependency structure of a sen-tence, the most important thing is to establish de-pendency relations and distinguish the head from thedependent.
Here are some criteria (Zwicky, 1985;x2:?2:x1:??
?1: x3:AD3:?
?x1 was held x3 x21  s l  3 2?/P/???/NR/??/NR/?
?/AD/2010?/NT/ FIFA/NRI /??/VV//P?/??
?/NR/ ??/AD/??/VV/(a)(b)(c)??
successfullys ssf ll(d)Figure 1: Examples of dependency structure (a), head-dependents relation (b), head-dependents rule (r1 of Fig-ure 2) and head rule (d).
Where ?x1:????
and?x2:??
indicate substitution sites which can be replacedby a subtree rooted at ?????
and ???
respectively.
?x3:AD?indicates a substitution site that can be replacedby a subtree whose root has part-of-speech ?AD?.
Theunderline denotes a leaf node.Hudson, 1990) for identifying a syntactic relationbetween a head and a dependent between a head-dependent pair:1. head determines the syntactic category of C,and can often replace C;2. head determines the semantic category of C;dependent gives semantic specification.2.2 Head-Dependents RelationA head-dependents relation is composed of a headand all its dependents as shown in Figure 1(b).Since all the head-dependent pairs satisfy crite-ria 1 and 2, we can deduce that a head-dependentsrelation L holds the property that the head deter-mines the syntactic and semantic categories of L,and can often replace L. Therefore, we can recur-217sively replace the bottom level head-dependent re-lations of a dependency structure with their headsuntil the root.
This implies an representation of thegeneration of a dependency structure on the basis ofhead-dependents relation.Inspired by this, we represent the translation rulesof our dependency-to-string model on the founda-tion of head-dependents relations.3 Dependency-to-String GrammarFigure 1 (c) and (d) show two examples of the trans-lation rules used in our dependency-to-string model.The former is an example of head-dependent rulesthat represent the source side as head-dependents re-lations and act as both translation rules and reorder-ing rules.
The latter is an example of head ruleswhich are used for translating words.Formally, a dependency-to-string grammar is de-fined as a tuple ?
?, N,?, R?, where ?
is a set ofsource language terminals, N is a set of categoriesfor the terminals in ?
, ?
is a set of target languageterminals, and R is a set of translation rules.
A ruler in R is a tuple ?t, s, ?
?, where:- t is a node labeled by terminal from ?
; or ahead-dependents relation of the source depen-dency structures, with each node labeled by aterminal from ?
or a variable from a set X ={x1, x2, ...} constrained by a terminal from ?or a category from N ;- s ?
(X ??)?
is the target side string;- ?
is a one-to-one mapping from nonterminalsin t to variables in s.For example, the head-dependents rule shown inFigure 1 (c) can be formalized as:t = ((x1:???)
(x2:?)
(x3:AD)??
)s = x1 was held x3 x2?
= {x1:????
x1, x2:??
x2, x3:AD?
x3}where the underline indicates a leaf node, andxi:letters indicates a pair of variable and its con-straint.A derivation is informally defined as a sequenceof steps converting a source dependency structureinto a target language string, with each step apply-ing one translation rule.
As an example, Figure 2?/P???/NR??/NR?
?/AD2010?/NT FIFA/NRI??/VV2010?
FIFAI ???
?
??
??
???/P???/NR??/NR?
?/AD2010?/NT FIFA/NRIwas held l?/P???/NR?
?/NR2010?/NT FIFA/NRIwas held successfully l  f ll?/P?
?/NR2010 FIFA [World Cup] was held successfully  I   [ rl  ]    l   f ll?
?2010 FIFA World Cup was held successfully in  I   rl       l   f ll   i2010 FIFA World Cup was held successfully in [South Africa] I  rl    l  f ll  i  [ t  fri ]parser(a)(b)(c)(d)(e)(f)(g)r3: (2010?)
(FIFA) ???
?2010 FIFA World Cupr2: ??
?successfullyr1: (x1:???
)(x2 :?)(x3:AD)??
?x1 was held x3 x2r4: ?
(x2:NR)?in x2r5: ??
?South AfricaFigure 2: An example derivation of dependency-to-stringtranslation.
The dash lines indicate the reordering whenemploying a head-dependents rule.shows the derivation for translating a Chinese (CH)sentence into an English (EN) string.CH 2010?
FIFA?????????
?EN 2010 FIFA World Cup was held successfully inSouth Africa218The Chinese sentence (a) is first parsed into a de-pendency structure (b), which is converted into anEnglish string in five steps.
First, at the root node,we apply head-dependents rule r1 shown in Figure1(c) to translate the top level head-dependents rela-tion and result in three unfinished substructures andtarget string in (c).
The rule is particular interestingsince it captures the fact: in Chinese prepositionalphrases and adverbs typically modify verbs on theleft, whereas in English prepositional phrases andadverbs typically modify verbs on the right.
Second,we use head rule r2 translating ????
into ?success-fully?
and reach situation (d).
Third, we apply head-dependents rule r3 translating the head-dependentsrelation rooted at ?????
and yield (e).
Fourth,head-dependents rules r5 partially translate the sub-tree rooted at ???
and arrive situation in (f).
Finally,we apply head rule r5 translating the residual node????
and obtain the final translation in (g).4 Rule AcquisitionThe rule acquisition begins with a word-aligned cor-pus: a set of triples ?T, S,A?, where T is a sourcedependency structure, S is a target side sentence,and A is an alignment relation between T and S.We extract from each triple ?T, S,A?
head rules thatare consistent with the word alignments and head-dependents rules that satisfy the intuition that syn-tactically close items tend to stay close across lan-guages.
We accomplish the rule acquisition throughthree steps: tree annotation, head-dependents frag-ments identification and rule induction.4.1 Tree AnnotationGiven a triple ?T, S,A?
as shown in Figure 3, wefirst annotate each node n of T with two attributes:head span and dependency span, which are definedas follows.Definition 1.
Given a node n, its head span hsp(n)is a set of index of the target words aligned to n.For example, hsp(2010?
)={1, 5}, which corre-sponds to the target words ?2010?
and ?was?.Definition 2.
A head span hsp(n) is consistent if itsatisfies the following property:?n?
?=nhsp(n?)
?
hsp(n) = ?.
?/P{5,8}{9,10}{ , }{ , }???/NR{3,4}{2-4}??/NR{9,10}{9,10}?
?/AD{7}{7}2010?/NT{1,5}{}{ , }{}FIFA/NR{2,2}{2,2}?
?/VV{6}{2-10}20101FIFA2I World3rl held6l successfully7f ll in8i South9tCup4was5Africa10friFigure 3: An annotated dependency structure.
Each nodeis annotated with two spans, the former is head span andthe latter dependency span.
The nodes in acceptable headset are displayed in gray, and the nodes in acceptable de-pendent set are denoted by boxes.
The triangle denotesthe only acceptable head-dependents fragment.For example, hsp(??)
is consistent, whilehsp(2010?)
is not consistent since hsp(2010?)
?hsp(?)
= 5.Definition 3.
Given a head span hsp(n), its closurecloz(hsp(n)) is the smallest contiguous head spanthat is a superset of hsp(n).For example, cloz(hsp(2010?))
= {1, 2, 3, 4, 5},which corresponds to the target side word sequence?2010 FIFA World Cup was?.
For simplicity, we use{1-5} to denotes the contiguous span {1, 2, 3, 4, 5}.Definition 4.
Given a subtree T ?
rooted at n, thedependency span dsp(n) of n is defined as:dsp(n) = cloz(?n?
?T ?hsp(n?)
is consistenthsp(n?
)).If the head spans of all the nodes of T ?
is not consis-tent, dsp(n) = ?.For example, since hsp(?)
is not consistent,dsp(?)=dsp(??
)={9, 10}, which corresponds tothe target words ?South?
and ?Africa?.The tree annotation can be accomplished by a sin-gle postorder transversal of T .
The extraction ofhead rules from each node can be readily achievedwith the same criteria as (Och and Ney, 2004).
In219the following, we focus on head-dependents rulesacquisition.4.2 Head-Dependents Fragments IdentificationWe then identify the head-dependents fragments thatare suitable for rule induction from the annotated de-pendency structure.To facilitate the identification process, we first de-fine two sets of dependency structure related to headspans and dependency spans.Definition 5.
A acceptable head set ahs(T) of a de-pendency structure T is a set of nodes, each of whichhas a consistent head span.For example, the elements of the acceptable headset of the dependency structure in Figure 3 are dis-played in gray.Definition 6.
A acceptable dependent set adt(T) ofa dependency structure T is a set of nodes, each ofwhich satisfies: dep(n) ?= ?.For example, the elements of the acceptable de-pendent set of the dependency structure in Figure 3are denoted by boxes.Definition 7.
We say a head-dependents fragmentsis acceptable if it satisfies the following properties:1. the root falls into acceptable head set;2. all the sinks fall into acceptable dependent set.An acceptable head-dependents fragment holdsthe property that the head span of the root and the de-pendency spans of the sinks do not overlap with eachother, which enables us to determine the reorderingin the target side.The identification of acceptable head-dependentsfragments can be achieved by a single preordertransversal of the annotated dependency structure.For each accessed internal node n, we checkwhether the head-dependents fragment f rooted atn is acceptable.
If f is acceptable, we output anacceptable head-dependents fragment; otherwise weaccess the next node.Typically, each acceptable head-dependents frag-ment has three types of nodes: internal nodes, inter-nal nodes of the dependency structure; leaf nodes,leaf nodes of the dependency structure; head node, aspecial internal node acting as the head of the relatedhead-dependents relation.
?/P{5,8}{9,10}/{ , }{ , }???/NR{3,4}{2-4}??/AD{7}{7}?
?/VV{6}{2-10}heldl successfullys ssf ll[FIFA World Cup][ I  rl  ] South Africa][ t  fri ]Input:Output:x2:?2:x1:??
?1: ???
?x1 held successfully x21  l  s ssf ll  2(x1:???)(x2:?)(??)
???
x1  held successfully x2(a)(b)Figure 4: A lexicalized head-dependents rule (b) inducedfrom the only acceptable head-dependents fragment (a)of Figure 3.4.3 Rule InductionFrom each acceptable head-dependents fragment,we induce a set of lexicalized and unlexicalizedhead-dependents rules.4.3.1 Lexicalized RuleWe induce a lexicalized head-dependents rulefrom an acceptable head-dependents fragment bythe following procedure:1. extract the head-dependents relation and markthe internal nodes as substitution sites.
Thisforms the input of a head-dependents rule;2. place the nodes in order according to the headspan of the root and the dependency spans ofthe sinks, then replace the internal nodes withvariables and the other nodes with the targetwords covered by their head spans.
This formsthe output of a head-dependents rule.Figure 4 shows an acceptable head-dependentsfragment and a lexicalized head-dependents rule in-220duced from it.4.3.2 Unlexicalized RulesSince head-dependents relations with verbs asheads typically consist of more than four nodes, em-ploying only lexicalized head-dependents rules willresult in severe sparseness problem.
To alleviatethis problem, we generalize the lexicalized head-dependents rules and induce rules with unlexicalizednodes.As we know, the modification relation of a head-dependents relation is determined by the edges.Therefore, we can replace the lexical word of eachnode with its categories (i.e.
POS) and obtain newhead-dependents relations with unlexicalized nodesholding the same modification relation.
Here we callthe lexicalized and unlexicalized head-dependentsrelations as instances of the modification relation.For a head-dependents relation with m node, we canproduce 2m ?
1 instances with unlexicalized nodes.Each instance represents the modification relationwith a different specification.Based on this observation, from each lexical-ized head-dependent rule, we generate new head-dependents rules with unlexicalized nodes accordingto the following principles:1. change the aligned part of the target string intoa new variable when turning a head node or aleaf node into its category;2. keep the target side unchanged when turning ainternal node into its category.Restrictions: Since head-dependents relationswith verbs as heads typically consists of more thanfour nodes, enumerating all the instances will re-sult in a massive grammar with too many kinds ofrules and inflexibility in decoding.
To alleviate theseproblems, we filter the grammar with the followingprinciples:1. nodes of the same type turn into their categoriessimultaneously.2.
as for leaf nodes, only those with open classwords can be turned into their categories.In our experiments of this paper, we onlyturn those dependents with POS tag in theset of {CD,DT,OD,JJ,NN,NR,NT,AD,FW,PN}into their categories.x2:?2:x1:??
?1: ?
?heldl successfullyf ll?
?x11 x22(x1:???)(x2:?)(??)
???
x1  held successfully x2x2:?2:x1:??
?1: x3:AD:heldl x33?
?x11 x22(x1:???)(x2:?
)(x3:AD) ???
x1  held x3 x2x2:P2:x1:NR1: ?
?heldl successfullyf ll?
?x11 x22(x1:NR)(x2:P)(??)
???
x1  held successfully x2x2:P2:x1:NR1: x3:AD3:heldl x33?
?x11 x22(x1:NR)(x2:P)(x3:AD) ???
x1  held x3 x2x2:?2:x1:??
?1: ?
?x44 successfullyf llx4:VV4:x11 x22(x1:???)(x2:?)(??)
x4:VV?
x1  x4 successfully x2x2:?2:x1:??
?1: x3:AD3:x44 x33x4:VV4:x11 x22(x1:???)(x2:?
)(x3:AD) x4:VV?
x1  x4  x3 x2x2:P2:x1:NR1: ?
?x44 successfullyf llx4:VV4:x11 x22(x1:NR)(x2:P)(??)
x4:VV?
x1 x4 successfully x2x2:P2:x1:NR1: x3:AD3:x44 x33x4:VV4:x11 x22(x1:NR)(x2:P)(x3:AD) x4:VV?
x1  x4 x3 x2generalize leaf generalize leafgeneralize internalgeneralize internalgeneralize leaf generalize leafgeneralizeheadFigure 5: An illustration of rule generalization.
Where?x1:????
and ?x2:??
indicate substitution siteswhich can be replaced by a subtree rooted at ????
?and ???
respectively.
?x3:AD?indicates a substitutionsite that can be replaced by a subtree whose root has part-of-speech ?AD?.
The underline denotes a leaf node.
Thebox indicates the starting lexicalized head-dependentsrule.Figure 5 illustrates the rule generalization processunder these restrictions.4.3.3 Unaligned WordsWe handle the unaligned words of the target sideby extending the head spans of the lexicalized headand leaf nodes on both left and right directions.This procedure is similar with the method of (Ochand Ney, 2004) except that we might extend several221Algorithm 1: Algorithm for Rule AcquisitionInput: Source dependency structure T , target string S, alignment AOutput: Translation rule set R1 HSet?
ACCEPTABLE HEAD(T ,S,A)2 DSet?
ACCEPTABLE DEPENDENT(T ,S,A)3 for each node n ?
HSet do4 extract head rules5 append the extracted rules to R6 if ?n?
?
child(n) n?
?
DSet7 then8 obtain a head-dependent fragment f9 induce lexicalized and unlexicalized head-dependents rules from f10 append the induced rules to R11 end12 endspans simultaneously.
In this process, we might ob-tain m(m ?
1) head-dependents rules from a head-dependent fragment in handling unaligned words.Each of these rules is assigned with a fractionalcount 1/m.4.4 Algorithm for Rule AcquisitionThe rule acquisition is a three-step process, which issummarized in Algorithm 1.We take the extracted rule set as observed data andmake use of relative frequency estimator to obtainthe translation probabilities P (t|s) and P (s|t).5 The modelFollowing (Och and Ney, 2002), we adopt a generallog-linear model.
Let d be a derivation that converta source dependency structure T into a target stringe.
The probability of d is defined as:P (d) ?
?i?i(d)?i (1)where ?i are features defined on derivations and ?iare feature weights.
In our experiments of this paper,we used seven features as follows:- translation probabilities P (t|s) and P (s|t);- lexical translation probabilities Plex(t|s) andPlex(s|t);- rule penalty exp(?1);- language model Plm(e);- word penalty exp(|e|).6 DecodingOur decoder is based on bottom up chart parsing.It finds the best derivation d?
that convert the inputdependency structure into a target string among allpossible derivations D:d?
= argmaxd?DP (D) (2)Given a source dependency structure T , the decodertransverses T in post-order.
For each accessed in-ternal node n, it enumerates all instances of the re-lated modification relation of the head-dependentsrelation rooted at n, and checks the rule set formatched translation rules.
If there is no matchedrule, we construct a pseudo translation rule accord-ing to the word order of the head-dependents rela-tion.
For example, suppose that we can not findany translation rule about to ?(2010?)
(FIFA) ???
?, we will construct a pseudo translation rule?(x1:2010?)
(x2:FIFA) x3:???
?
x1 x2 x3?.A larger translation is generated by substituting thevariables in the target side of a translation rule withthe translations of the corresponding dependents.We make use of cube pruning (Chiang, 2007; Huangand Chiang, 2007) to find the k-best items with inte-grated language model for each node.To balance performance and speed, we prune thesearch space in several ways.
First, beam thresh-222old ?
, items with a score worse than ?
times of thebest score in the same cell will be discarded; sec-ond, beam size b, items with a score worse than thebth best item in the same cell will be discarded.
Theitem consist of the necessary information used in de-coding.
Each cell contains all the items standing forthe subtree rooted at it.
For our experiments, we set?
= 10?3 and b = 300.
Additionally, we also prunerules that have the same source side (b = 100).7 ExperimentsWe evaluated the performance of our dependency-to-string model by comparison with replications ofthe hierarchical phrase-based model and the tree-to-string models on Chinese-English translation.7.1 Data preparationOur training corpus consists of 1.5M sentencepairs from LDC data, including LDC2002E18,LDC2003E07, LDC2003E14, Hansards portion ofLDC2004T07, LDC2004T08 and LDC2005T06.We parse the source sentences with StanfordParser (Klein and Manning, 2003) into projectivedependency structures, whose nodes are annotatedby POS tags and edges by typed dependencies.
Inour implementation of this paper, we make use ofthe POS tags only.We obtain the word alignments by runningGIZA++ (Och and Ney, 2003) on the corpus inboth directions and applying ?grow-diag-and?
re-finement(Koehn et al, 2003).We apply SRI Language Modeling Toolkit (Stol-cke, 2002) to train a 4-gram language model withmodified Kneser-Ney smoothing on the Xinhua por-tion of the Gigaword corpus.We use NIST MT Evaluation test set 2002 as ourdevelopment set, NIST MT Evaluation test set 2004(MT04) and 2005 (MT05) as our test set.
The qual-ity of translations is evaluated by the case insensitiveNIST BLEU-4 metric (Papineni et al, 2002).1We make use of the standard MERT (Och, 2003)to tune the feature weights in order to maximize thesystem?s BLEU score on the development set.1ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.plSystem Rule # MT04(%) MT05(%)cons2str 30M 34.55 31.94hiero-re 148M 35.29 33.22dep2str 56M 35.82+ 33.62+Table 1: Statistics of the extracted rules on training cor-pus and the BLEU scores on the test sets.
Where ?+?means dep2str significantly better than cons2str with p <0.01.7.2 The baseline modelsWe take a replication of Hiero (Chiang, 2007) asthe hierarchical phrase-based model baseline.
Inour experiments of this paper, we set the beam sizeb = 200 and the beam threshold ?
= 0.
The maxi-mum initial phrase length is 10.We use constituency-to-string model (Liu et al,2006) as the syntax-based model baseline whichmake use of composed rules (Galley et al, 2006)without handling the unaligned words.
In our exper-iments of this paper, we set the tatTable-limit=20,tatTable-threshold=10?1, stack-limit=100, stack-threshold=10?1,hight-limit=3, and length-limit=7.7.3 ResultsWe display the results of our experiments in Table1.
Our dependency-to-string model dep2str signif-icantly outperforms its constituency structure-basedcounterpart (cons2str) with +1.27 and +1.68 BLEUon MT04 and MT05 respectively.
Moreover, with-out resort to phrases or parse forest, dep2str sur-passes the hierarchical phrase-based model (hiero-re) over +0.53 and +0.4 BLEU on MT04 and MT05respectively on the basis of a 62% smaller rule set.Furthermore, We compare some actual transla-tions generated by cons2str, hiero-re and dep2str.Figure 6 shows two translations of our test setsMT04 and MT05, which are selected because eachholds a long distance dependency commonly used inChinese.In the first example, the Chinese input holdsa complex long distance dependencies ??
??
?...
?...?
???.
This dependency cor-responds to sentence pattern ?noun+prepostionalphrase+prepositional phrase+verb?, where the for-mer prepositional phrase specifies the position andthe latter specifies the time.
Both cons2str andhiero-re are confused by this sentence and mistak-223???
?
??
???
?
??
??
??
?
??
?Afterft r briefri f talkst l withit Powellll ,, thet US State Departmentt t  rt t Barnierr i r ,,saidiMT05----Segment 163Reference: After a brief talk withPowell at the US StateDepartment, Barnier said:Cons2str: Barnier after brieftalks in US State Departmentand Powell  said:Hiero-re: After a short meetingwith Barnier on the US StateDepartment, Powell said:Dep2str: After brief talks withPowell, the US StateDepartment Barnier said,??
??
??
???
?
??
??
1373 2001 ?Chinai appreciatesr i t effortsff rt off Anti -ti - Terrorismrr ri Committee itt tot promoter t allll inicountriestriMT04----Segment 1096Reference: China appreciates theefforts of the Counter-TerrorismCommittee to promote theimplementation of the resolution1373(2001) in all states and tohelp enhance the anti-terroristcapabilities of developingcountries.Cons2str: China appreciatesAnti - Terrorist Committee forpromoting implementation ofthe resolution No.
1373(2001)and help developing countriesstrength counter-terrorismcapability building for theefforts,Hiero-re: China appreciatesAnti - Terrorism Committee topromote countries implementresolution No .
1373 ( 2001 )and help developing countriesstrengthen anti-terrorismcapacity building support foreffortsDep2str: China appreciatesefforts of Anti - TerrorismCommittee to promote allcountries in the implementationof resolution  1373 ( 2001 )  , tohelp strengthen the anti-terrorism capability building ofdeveloping countries??
?
?
?
??
???
?????
?
?
?nsubjprep prep?????
?
??
?thet implementationi l t ti off ......nsubj dobjFigure 6: Actual translations produced by the baselines and our system.
For our system, we also display the longdistance dependencies correspondence in Chinese and English.
Here we omit the edges irrelevant to the long distancedependencies.enly treat ???(Powell)?
as the subjective, thusresult in translations with different meaning fromthe source sentence.
Conversely, although ???
isfalsely translated into a comma, dep2str capturesthis complex dependency and translates it into ?Af-ter ... ,(should be at) Barnier said?, which accordswith the reordering of the reference.In the second example, the Chinese input holdsa long distance dependency ???
??
...
???
which corresponds to a simple pattern ?nounphrase+verb+noun phrase?.
However, due to themodifiers of ????
which contains two sub-sentences including 24 words, the sentence looksrather complicated.
Cons2str and hiero-re fail tocapture this long distance dependency and providemonotonic translations which do not reflect themeaning of the source sentence.
In contrast, dep2strsuccessfully captures this long distance dependencyand translates it into ?China appreciates efforts of...?, which is almost the same with the reference?China appreciates the efforts of ...?.All these results prove the effectiveness of ourdependency-to-string model in both translation andlong distance reordering.
We believe that the ad-vantage of dep2str comes from the characteristics ofdependency structures tending to bring semanticallyrelated elements together (e.g., verbs become adja-cent to all their arguments) and are better suited tolexicalized models (Quirk et al, 2005).
And the in-capability of cons2str and hiero-re in handling longdistance reordering of these sentences does not lie inthe representation of translation rules but the com-promises in rule extraction or decoding so as to bal-ance the speed or grammar size and performance.The hierarchical phrase-based model prohibits anynonterminal X from spanning a substring longerthan 10 on the source side to make the decoding al-gorithm asymptotically linear-time (Chiang, 2005).224While constituency structure-based models typicallyconstrain the number of internal nodes (Galley etal., 2006) and/or the height (Liu et al, 2006) oftranslation rules so as to balance the grammar sizeand performance.
Both strategies limit the ability ofthe models in processing long distance reordering ofsentences with long and complex modification rela-tions.8 Related WorksAs a first step towards semantics, dependency struc-tures are attractive to machine translation.
Andmany efforts have been made to incorporating thisdesirable knowledge into machine translation.
(Lin, 2004; Quirk et al, 2005; Ding and Palmer,2005; Xiong et al, 2007) make use of source depen-dency structures.
(Lin, 2004) employs linear pathsas phrases and view translation as minimal path cov-ering.
(Quirk et al, 2005) extends paths to treelets,arbitrary connected subgraphs of dependency struc-tures, and propose a model based on treelet pairs.Both models require projection of the source depen-dency structure to the target side via word alignment,and thus can not handle non-isomorphism betweenlanguages.
To alleviate this problem, (Xiong et al,2007) presents a dependency treelet string corre-spondence model which directly map a dependencystructure to a target string.
(Ding and Palmer, 2005)presents a translation model based on SynchronousDependency Insertion Grammar(SDIG), which han-dles some of the non-isomorphism but requires bothsource and target dependency structures.
Most im-portant, all these works do not specify the orderinginformation directly in translation rules, and resortto either heuristics (Lin, 2004; Xiong et al, 2007) orseparate ordering models(Quirk et al, 2005; Dingand Palmer, 2005) to control the word order oftranslations.
By comparison, our model requiresonly source dependency structure, and handles non-isomorphism and ordering problems simultaneouslyby directly specifying the ordering information inthe head-dependents rules that represent the sourceside as head-dependents relations and the target sideas strings.
(Shen et al, 2008) exploits target dependencystructures as dependency language models to ensurethe grammaticality of the target string.
(Shen et al,2008) extends the hierarchical phrase-based modeland present a string-to-dependency model, whichemploys string-to-dependency rules whose sourceside are string and the target as well-formed depen-dency structures.
In contrast, our model exploitssource dependency structures, as a tree-based sys-tem, it run much faster (linear time vs. cubic time,see (Huang et al, 2006)).9 Conclusions and future workIn this paper, we present a novel dependency-to-string model, which employs head-dependents rulesthat represent the source side as head-dependentsrelations and the target side as string.
The head-dependents rules specify the ordering informationdirectly and require only substitution operation.Thus, our model does not need heuristics or order-ing model of the previous works to control the wordorder of translations.
Large scale experiments showthat our model exhibits good performance in longdistance reordering and outperforms the state-of-the-art constituency-to-string model and hierarchi-cal phrase-based model without resort to phrases andparse forest.
For the first time, a source dependency-based model shows improvement over the state-of-the-art translation models.In our future works, we will exploit the semanticinformation encoded in the dependency structureswhich is expected to further improve the transla-tions, and replace 1-best dependency structures withdependency forests so as to alleviate the influencecaused by parse errors.AcknowledgmentsThis work was supported by National Natural Sci-ence Foundation of China, Contract 60736014,60873167, 90920004.
We are grateful to the anony-mous reviewers for their thorough reviewing andvaluable suggestions.
We appreciate Yajuan Lv,Wenbin Jiang, Hao Xiong, Yang Liu, Xinyan Xiao,Tian Xia and Yun Huang for the insightful advices inboth experiments and writing.
Special thanks goesto Qian Chen for supporting my pursuit all through.ReferencesDavid Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings of225ACL 2005, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependency in-sertion grammars.
In Proceedings of ACL 2005.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In In Proceedings of EMNLP 2002,pages 304?311.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of ACL 2006, pages 961?968, Sydney, Australia,July.
Association for Computational Linguistics.Peter Hellwig.
2006.
Parsing with dependency gram-mars.
In Dependenz und Valenz / Dependency and Va-lency, volume 2, pages 1081?1109.
Berlin, New York.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language models.In Proceedings of ACL 2007, pages 144?151, Prague,Czech Republic, June.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.A syntax-directed translator with extended domain oflocality.
In Proceedings of the Workshop on Computa-tionally Hard Problems and Joint Inference in Speechand Language Processing, pages 1?8, New York City,New York, June.
Association for Computational Lin-guistics.Richard Hudson.
1990.
English Word Grammar.
Black-ell.Dan Klein and Christopher D.Manning.
2003.
Fast exactinference with a factored model for natural languageparsing.
In In Advances in Neural Information Pro-cessing Systems 15 (NIPS, pages 3?10.
MIT Press.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof the 2003 Human Language Technology Conferenceof the North American Chapter of the Association forComputational Linguistics, Edmonton, Canada, July.Dekang Lin.
2004.
A path-based transfer model formachine translation.
In Proceedings of Coling 2004,pages 625?630, Geneva, Switzerland, Aug 23?Aug27.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of ACL 2006, pages 609?616,Sydney, Australia, July.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models for sta-tistical machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 295?302, Philadelphia, Pennsylva-nia, USA, July.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL-2003, pages 160?167, Sapporo, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofACL 2002, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal smt.
In Proceedings of ACL 2005, pages 271?279.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.A new string-to-dependency machine translation al-gorithm with a target dependency language model.In Proceedings of ACL 2008: HLT, pages 577?585,Columbus, Ohio, June.
Association for ComputationalLinguistics.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proceedings of ICSLP, volume 30,pages 901?904.Deyi Xiong, Qun Liu, and Shouxun Lin.
2007.
A depen-dency treelet string correspondence model for statisti-cal machine translation.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages40?47, Prague, Czech Republic, June.Arnold M. Zwicky.
1985.
Heads.
Journal of Linguistics,21:1?29.226
