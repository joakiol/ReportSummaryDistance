Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 314?321,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsRanking Help Message Candidates Based on Robust GrammarVerification Results and Utterance History in Spoken Dialogue SystemsKazunori Komatani Satoshi Ikeda Yuichiro FukubayashiTetsuya Ogata Hiroshi G. OkunoGraduate School of InformaticsKyoto UniversityYoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan{komatani,sikeda,fukubaya,ogata,okuno}@kuis.kyoto-u.ac.jpAbstractWe address an issue of out-of-grammar(OOG) utterances in spoken dialogue sys-tems by generating help messages fornovice users.
Help generation for OOGutterances is a challenging problem be-cause language understanding (LU) re-sults based on automatic speech recogni-tion (ASR) results for such utterances arealways erroneous as important words areoften misrecognized or missed from suchutterances.
We first develop grammar ver-ification for OOG utterances on the ba-sis of a Weighted Finite-State Transducer(WFST).
It robustly identifies a grammarrule that a user intends to utter, even whensome important words are missed from theASR result.
We then adopt a ranking algo-rithm, RankBoost, whose features includethe grammar verification results and theutterance history representing the user?sexperience.1 IntroductionStudies on spoken dialogue systems have recentlyproceeded from in-laboratory systems to ones de-ployed to the open public (Raux et al, 2006; Ko-matani et al, 2007; Nisimura et al, 2005).
Ac-cordingly, opportunities are increasing as generalcitizens use the systems.
This situation meansthat novice users directly access the systems withno instruction, which is quite different from in-laboratory experiments where some instructionscan be given.
In such cases, users often experi-ence situations where their utterances are not cor-rectly recognized.
This is because of a gap be-tween the actual system and a user?s mental model,that is, a user?s expectation of the system.
Ac-tually, a user?s utterance often cannot be inter-preted by the system because of the system?s lim-ited grammar for language understanding (LU).We call such an unacceptable utterance an ?out-of-grammar (OOG) utterance.?
When users?
ut-terances are OOG, they cannot change their ut-terances into acceptable ones unless they are in-formed what expressions are acceptable by thesystem.We aim to manage the problem of OOG utter-ances by providing help messages showing an ex-ample of acceptable language expressions when auser utterance is not acceptable.
We prepare helpmessages corresponding to each grammar rule thesystem has.
We therefore assume that appropri-ate help messages can be provided if a user?s in-tention, i.e., a grammar rule the user originallyintends to use by his utterance, is correctly esti-mated.Issues for generating such help messages in-clude:1.
Estimating a grammar rule corresponding touser intention even from OOG utterances,and2.
Complementing missing information in a sin-gle utterance.The first issue focuses on the fact that automaticspeech recognition (ASR) results, used as main in-put data, are erroneous for OOG utterances.
Es-timating a grammar rule that the user intends touse becomes accordingly difficult especially whencontent words, which correspond to database en-tries such as place names and their attributes, arenot correctly recognized.
That is, any type of ASRerror in any position should be taken into consid-eration in ASR results of OOG utterances.
On the314other hand, the second issue focuses on the factthat an ASR result for an OOG utterance does notnecessarily contain sufficient information to esti-mate the user intention.
This is because of ASRerrors or that users may omit some elements fromtheir utterances because they are in context.We develop a grammar verification methodbased on Weighted Finite-State Transducer(WFST) as a solution to the first issue.
Thegrammar verification method robustly estimateswhich a grammar rule is intended to use by auser?s utterance.
The WFST is automaticallygenerated to represent an ASR result in which anypossibility of error is taken into consideration.
Wefurthermore adopt a boosting algorithm, Rank-Boost (Freund et al, 2003), to put help messagesin order of probability to address the second issue.Because it is difficult even for human annotatorsto uniquely determine which help message shouldbe provided for each case, we adopt an algorithmthat can be used for training on several dataexamples that have a certain order of priority.We also incorporate features representing theuser?s utterance history for preventing messagerepetition.2 Related WorkVarious studies have been done on generating helpmessages in spoken dialogue systems.
Gorrell etal.
(2002) trained a decision tree to classify causesof errors for OOG utterances.
Hockey et al (2003)also classified OOG utterances into the three cate-gories of endpointing errors, unknown vocabulary,and subcategorization mistakes, by comparing twokinds of ASR results.
This was called TargetedHelp and provided a user with immediate feedbacktailored to what the user said.
Lee et al (2007) alsoaddressed error recovery by generating help mes-sages in an example-based dialog modeling frame-work.
These studies, however, determined whathelp messages should be provided mainly on thebasis of literal ASR results.
Therefore, help mes-sages would be degraded by ASR results in whicha lot of information was missing, especially forOOG utterances.
The same help messages wouldbe repeated when the same ASR results were ob-tained.An example dialogue enabled by our method,especially the part of the method described in Sec-tion 4, is shown in Figure 1.
Here, user utter-ances are transcriptions, and utterance numbersU1: Tell me your recommending sites.Underlined parts are not in-vocabulary and novalid LU result is obtained.
The estimated gram-mar is [Obtaining info on a site] although the mostappropriate help message is that corresponding to[Searching tourist sites].S1: I did not understand.
You can say ?Tell methe address of Kiyomizu Temple?
for example,if getting information on a site.The help message corresponding to [Obtaining infoon a site] is provided.U2: Tell me your recommending sites.The user repeats the same utterance probably be-cause the help message (S1) was not helpful.
Theestimated grammar is [Obtaining info on a site]again.S2: I did not understand.
You can say ?Searchshrines or museums?
for example, if searchingtourist sites.Another help message corresponding to [Searchingtourist sites] is provided after ranking candidatesby also using the user?s utterance history.
[] denotes grammar rules.Figure 1: Example dialogue enabled by ourmethodstart with ?S?
and ?U?
denote system and userutterances, respectively.
In this example, ASRresults for the user utterances (U1 and U2) donot contain sufficient information because the ut-terances are short and contain out-of-vocabularywords.
These two results are similar, and ac-cordingly, the help message after U2 provided bymethods like Targeted Help (Gorrell et al, 2002;Hockey et al, 2003) is the same as Utterance S1because they are only based on ASR results.
Ourmethod can provide different help messages as Ut-terance S2 after ranking candidates by consider-ing the utterance history and grammar verificationresults.
Because the candidates are arranged inthe order of probability, the most appropriate helpmessage can be provided in fewer attempts.This ranking method for help message candi-dates is also useful in multimodal interfaces withspeech input.
Help messages are necessary whenASR is used as its input modality, and such mes-sages were actually implemented in City Browser(Gruenstein and Seneff, 2007), for example.
Thissystem lists template-based help messages on thescreen by using ASR results and internal states ofthe system.
The order of help messages is impor-tant, especially in portable devices with a smallscreen, on which the number of help messages dis-315played at one time is limited, as Hartmann andSchreiber (2008) pointed out.
Even in cases wheresufficiently large screens are available, too manyhelp messages without any order will distract theuser?s attention and thus spoil its usability.3 Grammar Verification based on WFSTWe estimate a user?s intention even from OOG ut-terances as a grammar rule that the user intendsto use by his utterance.
We call this estimationgrammar verification.
This process is applied toASR outputs based on a statistical language model(LM) in this paper.
We use two transducers: afinite-state transducer (FST) representing the taskgrammar, and weighted FST (WFST) representingan ASR result and its confidence score.
Hereafter,we denote these two as ?grammar FST?
and ?inputWFST?
and depict examples in Figure 2.A strong point of our method is that it takesall three types of ASR error into consideration.The input WFST is designed to represent all caseswhere any word in an ASR result is an inserted orsubstituted error, or any word is deleted.
Its weightis designed to reflect confidence scores of ASR re-sults.
By composing this WFST and the gram-mar FST, we can obtain all possible sequencesand their accumulated weights when arbitrary se-quences represented by the input WFST are inputinto the grammar FST.
The optimal results havingthe maximum accumulated weight consist of theLU result and the grammar rule that is the nearestto the ASR result.
The result can be obtained evenwhen any element in it is misrecognized or absentfrom the ASR result.An LU result is a set of concepts that consistof slots and their values corresponding to databaseentries the system handles.
For example, an LUresult ?month=2, day=22?
consists of two con-cepts, such as the value of slotmonth is 2, and thevalue of slot day is 22.3.1 Design of input WFST and grammar FSTIn input WFSTs and grammar FSTs, each arc rep-resenting state transitions has a label in the form of?a:b/c?
denoting its input symbol, output symbol,and weight, in this order.
Input symbol ?
means astate transition without any input symbol, that is,an epsilon transition.
Output symbol ?
means nooutput in the state transition.
For example, a statetransition ?please:?/1.0?
is executed when an in-put symbol is ?please,?
no output symbol is gen-erated, and 1.0 is added to the accumulated weight.Weights are omitted in the grammar FST becauseno weight is given in it.An input WFST is automatically constructedfrom an ASR result.
Sequential state transitionsare assigned to each word in the ASR result, andeach of them is paralleled by filler transitions, asshown in Figure 2 where the ASR result was ?Ev-ery Monday please?
for example.
Filler transitionssuch as INS, DEL, and SUB are assigned to eachstate for representing every kind of error such asinsertion, deletion, and substitution errors.
All in-put symbols in the input WFST are ?, by which theWFST represents all possible sequences contain-ing arbitrary errors.
For example, the input WFSTin Figure 2 represents all possible sequences suchas ?Every Monday please,?
?Every Monday F,?
?FMonday F,?
and so on.
Here, every word can bereplaced by the symbol F that represents an inser-tion or substitution error.
Moreover, the error sym-bol DEL can be inserted into its output symbol se-quence at any position, which corresponds to dele-tion errors in ASR results.
Each weight per statetransition is summed up and then the optimal re-sult is determined.
The weights will be explainedin Section 3.2.A grammar FST is generated from a task gram-mar, which is written by a system developer foreach task.
It determines whether an input se-quence conforms to the task grammar.
We alsoassign filler transitions to each state for handlingeach type of error of ASR results considered inthe input WFST.
A filler transition, either of INS,DEL, or SUB, is added to each state in the FSTexcept for states within keyphrases, which are ex-plicitly indicated by a system developer.
In theexample shown in Figure 2, ?SUB $ Mondaydate-repeat=Mon please?
is output for an inputsequence ?SUB Monday please?.
Here, date-repeat=Mon denotes an LU result, and $ is a sym-bol for marking words corresponding to a concept.3.2 Weights assigned to input WFSTWe defined two kinds of weights:1.
Rewards for accepted words (wacc), and2.
Penalties for each kind of error (wsub, wdel,wins).An accumulated weight for a single utterance isdefined as the sum of these weights as shown be-316Input WFSTEvery:EveryMonday:Mondayplease:pleaseGrammar FSTinput:output/weightASR result: ?Every Monday please?!"!
"#$ :!"!
"#$ :!"!
"#$ :!"!
"#$ :!"!
"#$ :!"!
"#$ :!!!
"#$%& :!!!
"#$"%& :!!!
"#$%&' :!
"# !"!
"# !"!
"# !"$:?repeat-date:?Mon=!"!
"#$ :!"!
"#$ :!"!
"#$ :!"!
"#$ :!
"# !"
!
"# !"!
"# !"!
"# !"!
"# !
"Figure 2: Example of input WFST and grammar FSTlow.w =?Eacceptedwacc +?Eerror(wsub + wdel + wins)Here, Eaccepted denotes a set of accepted wordscorresponding to elements of each grammar rule,and Eerror denotes a set of words that are not ac-cepted and that have either error symbol.
Note thatthe weights are not given beforehand but are cal-culated and given to the input WFST in runtimeaccording to each ASR result.A weight for an accepted word easr is definedby using its confidence score CM(easr) (Lee etal., 2004) and its word length.
A word length inmora is denoted as l(?
), which is normalized bythat of the longest word in the vocabulary.wacc = CM(easr)l(easr)This weight wacc gives preference to sequencescontaining longer words with higher confidencescores.Weights for each type of error have negative val-ues because they are penalties:wsub = ?
{CM(easr)l(easr) + l(egram)}/2wdel = ?
{l(e) + l(egram)}/2wins = ?
{CM(easr)l(easr) + l(e)}/2where l(e) is the average word length in the vocab-ulary and egram is a grammar element i.e., either aword or a class.
A deletion error is a case when agrammar element does not correspond to any wordin the ASR result.
A substitution error is a casewhen an element is replaced by another word inthe ASR result.
An insertion error is a case whenno grammar element corresponds to the ASR re-sult.
Every weight is defined as an average of aword length of a grammar element and the corre-sponding one in the ASR result multiplied by itsconfidence score.
When correspondences cannotbe defined in insertion and deletion errors, l(e) isused instead.
In the case when egram is a class inthe grammar, the average word length in that classis used as l(egram).3.3 Example of calculating the weightsWe show how a weight is calculated by using theexample in Figure 3.
In this example, the user ut-terance was ?Tell me a liaison of Koetsu-ji (a tem-ple name).?
The word ?liaison?
was not in the sys-tem vocabulary.
The ASR result accordingly con-tained errors for that part; the result was ?Tell meall Sakyo-ward Koetsu-ji.
?Weights are calculated for each grammar rulethe system has.
This example shows calcula-tions for two grammar rules: [get info] accept-ing ?Tell me ?item name?
of ?temple name?,?
and[search ward] accepting ?Tell me ?facility name?of ?ward name?.?
Here, [] and ??
denote a gram-mar rule and a class in grammars.
Two alignmentresults are also shown for grammar [get info] inthis example.
Weights are calculated for any align-ment as shown here, and the alignment result withthe largest weight is selected.
In this example,weight +0.16 for the grammar [get info] was thelargest.We consequently obtained the result that gram-mar rule [get info] had the highest score for thisOOG utterance and its accumulated weight was317User utterance: ?Tell me a liaison of Koetsu-ji?.
(Underlined parts denote OOG.
)ASR result tell me all Sakyo-ward Koetsu-ji(ward) (temple)grammar [get info] tell me ?item name?
of ?temple name?WFST output tell me INS SUB DEL Koetsu-jiweights +0.09 +0.06 ?0.04 ?0.11 ?0.02 +0.18 +0.16grammar [get info] tell me ?item name ?
of ?temple name?WFST output tell me SUB SUB Koetsu-jiweights +0.09 +0.06 ?0.21 ?0.10 +0.18 +0.02grammar [search ward] tell me ?facility type?
in ?ward name?WFST output tell me INS SUB DEL SUBweights +0.09 +0.06 ?0.04 ?0.12 ?0.02 ?0.21 ?0.24Figure 3: Example of calculating weights in our grammar verification+0.16.
The result also indicated each type of er-ror as a result of the alignment: ?item name?
wassubstituted by ?Sakyo-ward?, ?of?
in the grammar[get info] was deleted, and ?all?
in the ASR resultwas inserted.4 Ranking Help Message Candidates byIntegrating Dialogue ContextWe furthermore develop a method to rank helpmessage candidates per grammar rule by integrat-ing the grammar verification result and the user?sutterance history.
This complements informationthat is often absent from utterances or misrecog-nized in ASR and prevents that the same help mes-sages are repeated.
An outline of the method isdepicted in Figure 4.4.1 Features used in RankingFeatures used in our methods are listed in Table1.
These features are calculated for each helpmessage candidate corresponding to each gram-mar rule.
Features H1 to H5 represent how reli-able a grammar verification result is.
Feature H1 isa grammar verification score, that is, the resultingaccumulated weight described in Section 3.
Fea-ture H2 is calculated by normalizing H1 by thetotal score of all grammar rules.
This representshow reliable the grammar verification result is rel-atively compared to others.
Features H3 to H5represent how partially the user utterance matcheswith the grammar rule.Features H6 and H7 correspond to a dialoguecontext.
Feature H6 reflects the case in whichusers tend to repeat similar utterances when theirutterances were not understood by the system.Feature H7 represents whether and how the userknows about the language expression of the gram-mar rule.
This feature corresponds to the knowndegree we previously proposed (Fukubayashi etTable 1: Features of each instance (help messagecandidate)H1: accumulated weight of GV (GV score)H2: GV score normalized by the total GV score of otherinstancesH3: ratio of # of accepted words in GV result to # of allwordsH4: maximum number of successively accepted wordsin GV resultH5: number of accepted slots in GV resultH6: how before the grammar rule was selected as GVresult (in # of utterances)H7: maximum GV score for the grammar rule until thenH8: whether it belongs to the ?command?
classH9: whether it belongs to the ?query?
classH10: whether it belongs to the ?request-info?
classH11-H17: products of H8 and each of H1 to H7H18-H24: products of H9 and each of H1 to H7H25-H31: products of H10 and each of H1 to H7GV: grammar verificational., 2006), and prevents a help message the useralready knows from being provided repeatedly.Features H8 to H10 represent properties ofutterances corresponding to the grammar rules,which are categorized into three classes such as?command,?
?query,?
and ?request-info.?
In thesightseeing task, the numbers of grammar rules forthe three classes were 8, 4, and 11, respectively.More specifically, utterances in either ?query?
or?request-info?
class tend to appear successivelybecause they are used when users try and com-pare several query conditions; on the other hand,utterances in ?command?
class tend to appear in-dependently of the context.
Features H11 to H31are the products of features H8, H9, and H10 andeach feature from H1 to H7.
These were defined toconsider combinations of properties of utterancesrepresented by H8, H9, and H10 and their reliabil-ity represented by H1 to H7, because RankBoost318Help candidateHelp candidateRanking(RankBoost)?=TtttxhxH )()( ?1xLL )()(111xfxfiLL )()(1 ninxfxfnxUserutteranceContextdeftqi,,,?
?ParametersTrainingdatapxqxGrammarverificationCalculating featuresSorted by H(x)Statistical LM-basedASR outputsFigure 4: Outline of our ranking method for help message candidatesdoes not consider them.4.2 Ranking AlgorithmWe adopt RankBoost (Freund et al, 2003), aboosting algorithm based on machine learning, torank help message candidates.
This algorithm canbe used for training on several data examples hav-ing a certain order of priority.
This attribute fitsfor the problem in this paper; it is difficult evenfor human annotators to determine the unique ap-propriate help message to be provided.
Target in-stances x of the algorithm are help message can-didates corresponding to grammar rules in this pa-per.RankBoost trains a score function H(x) and ar-ranges instances x in the order.
Here, H(x?)
<H(x??)
means x??
is ranked higher than x?.
Thisscore function is defined as a linear combinationof weak rankers giving partial information regard-ing the order:H(x) =T?t?tht(x)where T , ht(), and ?t denote the number of boost-ing iterations, a weak ranker, and its associatedweight, respectively.
The weak ranker ht is de-fined by comparing the value of a feature fi of aninstance x with a threshold ?.
That is,ht(x) =????
?1 if fi(x) > ?0 if fi(x) ?
?qdef if fi(x) = ?
(1)where qdef ?
{0, 1}.
Here, fi(x) denotes thevalue of the i-th feature of instance x, and ?
de-notes that no value is given in fi(x).5 Experimental Evaluation5.1 Target DataData were collected by 30 subjects in total by us-ing a multi-domain spoken dialogue system thathandles five domains such as restaurant, hotel,sightseeing, bus, and weather (Komatani et al,2008).
The data consisted of 180 dialogues and11,733 utterances.
Data from five subjects wereused to determine the number of boosting iter-ations and to improve LMs for ASR.
We usedutterances in the restaurant, hotel, and sightsee-ing domains because the remaining two, bus andweather, did not have many grammar rules.
Wethen extracted OOG utterances on the basis of thegrammar verification results to evaluate the per-formance of our method for such utterances.
Weregarded an utterance whose accumulated weightwas negative as OOG.
As a result, 1,349 OOG ut-terances by 25 subjects were used for evaluation,hereafter.
These consisted of 363 utterances in therestaurant domain, 563 in the hotel domain, and423 in the sightseeing domain.
These data werecollected under the following conditions: subjectswere given no instructions on concrete languageexpressions the system accepts.
System responseswere made only by speech, and no screen for dis-playing outputs was used.
Subjects were given sixscenarios describing tasks to be completed.We used Julius1 that is a statistical-LM-basedASR engine.
We constructed class 3-gram LMsfor ASR by using 10,000 sentences generatedfrom the task grammars and the 600 utterancescollected by the five subjects.
The vocabularysizes for the restaurant, hotel, and sightseeing do-mains were 3,456, 2,625, and 3,593, and ASR ac-curacies for them were 45.8%, 57.1%, and 43.5%,respectively.
These ASR accuracies were not veryhigh because the target utterances were all OOG.A set of possible thresholds in the weak rankersdescribed in Section 4.2 consisted of all featurevalues that appeared in the training data.
The num-bers of boosting iterations were determined on thebasis of accuracies for the data by the five sub-1http://julius.sourceforge.jp/319!
"#$"#%"#&"#'"#("#)"#*""#*+,-./ 0+,-./ !+,-./ $+,-./ %+,-./1+,-./!""#$%"&!
"#$%&'$ ()*+,$-.
(/Figure 5: Accuracy when N candidates were pro-vided in sightseeing domain (1 ?
N ?
5)jects.
The numbers were 400, 100, and 500 for therestaurant, hotel, and sightseeing domains.5.2 Evaluation CriterionWe manually gave five help messages correspond-ing to grammar rules as reference labels per ut-terance in the order of having a strong relation tothe utterance.
The numbers of candidate help mes-sages were 28, 27, and 23 for the restaurant, hoteland sightseeing domains, respectively.We evaluated our ranking method as the accu-racy where at least one of the reference labels wascontained in its top N candidates.
This corre-sponds to a probability where at least one appro-priate help message was contained in a list of Ncandidates.
The accuracy was calculated by 5-foldcross validation.
In the baseline method we set,help messages were provided only by using thegrammar verification scores.5.3 ResultsResults in the sightseeing domain are plotted inFigure 5.
We can see that our method outper-formed the baseline in the accuracies for all Nvalues.
All these differences were statistically sig-nificant (p < 0.05) by the McNemar test.
The ac-curacies were also better in the other two domainsfor all N values, and the average differences forthe three domains were 11.7 points for N=1, 9.7points for N=2, and 6.7 points for N=3.
The dif-ferences were large especially for small N values.This result indicates that we can successfully re-duce the number of help messages when providingseveral ones for users.
The improvements werederived from the features we incorporated such asthe estimated user knowledge in addition to gram-mar verification results.
The baseline method wasonly based on grammar verification results for sin-gle utterances, which contained insufficient infor-mation because OOG utterances were often mis-recognized or misunderstood.Table 2: Sum of absolute values of weight ?
foreach featureH7 H17 H19 H2 H6(H7*H8) (H2*H9)9.58 6.91 6.61 6.02 6.01We also investigated dominant features by cal-culating the sum of absolute values of final weight?
for each feature in RankBoost.
Five dominantfeatures based on the sums are shown in Table2.
These five features include a feature obtainedfrom grammar verification result (H2), a featureabout the user?s utterance history (H6), a featurerepresenting estimated user knowledge (H7), andfeatures representing properties of the utterances.The most dominant feature was H7, which ap-peared twice in this table.
This was because userutterances were not likely to be OOG utterancesagain after the user had already known an expres-sion corresponding to the grammar rule, which canbe detected when user utterances for it were cor-rectly accepted, that is, its grammar verificationscore was high.
The second dominant feature wasH2, which showed that grammar verification re-sults worked effectively.6 ConclusionWe addressed an issue of OOG utterances in spo-ken dialogue systems by generating help mes-sages.
To manage situations when a user utter-ance could not be accepted, we robustly estimateda user?s intention as a grammar rule that the userintends to use.
We furthermore integrated variousinformation as well as the grammar verificationresults for complementing missing information insingle utterances, and then ranked help messagecandidates corresponding to the grammar rules forefficiently providing them.Our future work includes the following.
Theevaluation in this paper was taken place only onthe basis of utterances collected beforehand.
Pro-viding help messages itself should be evaluated byanother experiment through dialogues.
Further-more, we assumed that language expressions ofhelp messages to show an example language ex-pression were fixed.
We also need to investigatewhat kind of expression is more helpful to noviceusers.320ReferencesYoav Freund, Raj D. Iyer, Robert E. Schapire, andYoram Singer.
2003.
An efficient boosting algo-rithm for combining preferences.
Journal of Ma-chine Learning Research, 4:933?969.Yuichiro Fukubayashi, Kazunori Komatani, TetsuyaOgata, and Hiroshi G. Okuno.
2006.
Dynamichelp generation by estimating user?s mental model inspoken dialogue systems.
In Proc.
Int?l Conf.
Spo-ken Language Processing (INTERSPEECH), pages1946?1949.Genevieve Gorrell, Ian Lewin, and Manny Rayner.2002.
Adding intelligent help to mixed-initiativespoken dialogue systems.
In Proc.
Int?l Conf.
Spo-ken Language Processing (ICSLP), pages 2065?2068.Alexander Gruenstein and Stephanie Seneff.
2007.Releasing a multimodal dialogue system into thewild: User support mechanisms.
In Proc.
8th SIG-dial Workshop on Discourse and Dialogue, pages111?119.Melanie Hartmann and Daniel Schreiber.
2008.
Proac-tively adapting interfaces to individual users for mo-bile devices.
In Adaptive Hypermedia and Adap-tive Web-Based Systems, 5th International Confer-ence (AH 2008), volume 5149 of Lecture Notes inComputer Science, pages 300?303.
Springer.Beth A. Hockey, Oliver Lemon, Ellen Campana, LauraHiatt, Gregory Aist, James Hieronymus, AlexanderGruenstein, and John Dowding.
2003.
Targetedhelp for spoken dialogue systems: intelligent feed-back improves naive users?
performance.
In Proc.10th Conf.
of the European Chapter of the ACL(EACL2003), pages 147?154.Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.Okuno.
2007.
Analyzing temporal transition of realuser?s behaviors in a spoken dialogue system.
InProc.
INTERSPEECH, pages 142?145.Kazunori Komatani, Satoshi Ikeda, Tetsuya Ogata,and Hiroshi G. Okuno.
2008.
Managing out-of-grammar utterances by topic estimation with domainextensibility in multi-domain spoken dialogue sys-tems.
Speech Communication, 50(10):863?870.Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawa-hara.
2004.
Real-time word confidence scoring us-ing local posterior probabilities on tree trellis search.In IEEE Int?l Conf.
Acoust., Speech & Signal Pro-cessing (ICASSP), volume 1, pages 793?796.Cheongjae Lee, Sangkeun Jung, Donghyeon Lee, andGary Guenbae Lee.
2007.
Example-based error re-covery strategy for spoken dialog system.
In Proc.of IEEE Automatic Speech Recognition and Under-standing Workshop (ASRU), pages 538?543.Ryuichi Nisimura, Akinobu Lee, Masashi Yamada, andKiyohiro Shikano.
2005.
Operating a public spo-ken guidance system in real environment.
In Proc.European Conf.
Speech Commun.
& Tech.
(EU-ROSPEECH), pages 845?848.Antoine Raux, Dan Bohus, Brian Langner, Alan W.Black, and Maxine Eskenazi.
2006.
Doing researchon a deployed spoken dialogue system: One year ofLet?s Go!
experience.
In Proc.
INTERSPEECH.321
