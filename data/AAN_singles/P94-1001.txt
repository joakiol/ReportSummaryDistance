Discourse Obligations in Dialogue ProcessingDavid R. Traum and James F. AllenDepartment ofComputer ScienceUniversity of RochesterRochester, NY 14627-0226traum@cs,  rochester ,  edu and j ames@cs, rochester ,  eduAbstractWe show that in modeling social interaction, particularly di-alogue, the attitude of obligation can be a useful adjunct othe popularly considered attitudes of belief, goal, and inten-tion and their mutual and shared counterparts.
In particular,we show how discourse obligations can be used to accountin a natural manner for the connection between a questionand its answer in dialogue and how obligations can be usedalong with other parts of the discourse context o extend thecoverage of a dialogue system.1 MotivationMost computational models of discourse are based pri-marily on an analysis of the intentions of the speakers(e.g., \[Cohen and Perrault, 1979; Allen and Perrault, 1980;Grosz and Sidner, 1986\]).
An agent has certain goals, andcommunication results from a planning process to achievethese goals.
The speaker will form intentions based on thegoals and then act on these intentions, producing utterances.The hearer will then reconstruct a model of the speaker'sintentions upon hearing the utterance.
This approach asmany strong points, but does not provide a very satisfac-tory account of the adherence to discourse conventions indialogue.For instance, consider one simple phenomena:  questionis typically followed by an answer, or some explicit statementof an inability or refusal to answer.
The intentional storyaccount of this goes as follows.
From the production of aquestion by Agent B, Agent A recognizes Agent B's goalto find out the answer, and she adopts a goal to tell B theanswer in order to be co-operative.
A then plans to achievethe goal, thereby generating the answer.
This provides anelegant account in the simple case, but requires a strongassumption of co-operativeness.
Agent A must adopt agentB's goals as her own.
As a result, it does not explain why Asays anything when she does not know the answer or whenshe is not predisposed toadopting B's goals.Several approaches have been suggested toaccount for thisbehavior.
\[Litman and Allen, 1987\] introduced an intentionalanalysis at the discourse level in addition to the domain level,and assumed a set of conventional multi-agent actions atthe discourse level.
Others have tried to account for thiskind of behavior using social intentional constructs uch asJoint intentions \[Cohen and Levesque, 1991 \] or Shared Plans\[Grosz and Sidner, 1990\].
While these accounts do helpexplain some discourse phenomena more satisfactorily, theystill require a strong degree of cooperativity o account fordialogue coherence, and do not provide easy explanationsof why an agent might act in cases which do not supporthigh-level mutual goals.Consider astranger approaching an agent and asking, "Doyou have the time?"
It is unlikely that there is a joint intentionor shared plan, as they have never met before.
From a purelystrategic point of view, the agent may have no interest inwhether the stranger's goals are met.
Yet, typically agentswill still respond in such situations.As another example, consider a case in which the agent'sgoals are such that it prefers that an interrogating agent notfind out the requested information.
This might block theformation of an intention to inform, but what is it that inspiresthe agent o respond at all?As these examples illustrate, an account of question an-swering must go beyond recognition of speaker intentions.Questions do more than just provide vidence of a speaker'sgoals, and something more than adoption of the goals of aninterlocutor is involved in the formulating a response to aquestion.Some researchers, e.g., \[Mann, 1988; KowtkoetaL, 1991\],assume alibrary of discourse level actions, sometimes calleddialogue games, which encode common communicative in-teractions.
To be co-operative, an agent must always be par-ticipating in one of these games.
So if a question is asked,only a fixed number of activities, namely those introducedby a question, are cooperative r sponses.
Games provide abetter explanation of coherence, but still require the agent'sto recognize ach other's intentions to perform the dialoguegame.
As a result, this work can be viewed as a special caseof the intentional view.
An interesting model is described by\[Airenti et al, 1993\], which separates out the conversationalgames from the task-related games in a way similar way to\[Litman and Allen, 1987\].
Because of this separation, theydo not have to assume co-operation on the tasks each agent isperforming, but still require recognition of intention and co-operation at the conversational level.
It is left unexplainedwhat goals motivate conversational co-operation.The problem with systems which impose cooperativity inthe form of automatic goal adoption is that this makes it im-possible to reason about cases in which one might want toviolate these rules, especially in cases where the conversa-tional co-operation might conflict with the agent's personalgoals.We are developing an alternate approach that takes a stepback from the strong plan-based approach.
By the strongplan-based account, we mean models where there is a setof personal goals which directly motivates all the behaviorof the agent.
While many of the intuitions underlying theseapproaches seems close to right, we claim it is a mistake toattempt to analyze this behavior as arising entirely from theagent's high-level goals.We believe that people have a much more complex set ofmotivations for action.
In particular, much of one's behaviorarises from a sense of obligation to behave within limits setby the society that the agent is part of.
A model based onobligations differs from an intention-based approach in thatobligations are independent of shared plans and intentionrecognition.
Rather, obligations are the result of rules bywhich an agent lives by.
Social interactions are enabledby their being a sufficient compatibility between the rulesaffecting the interacting agents.
One responds to a questionbecause this is a social behavior that is strongly encouragedas one grows up, and becomes instilled in the agent.2 Sketch of SolutionThe model we propose is that an agent's behavior is deter-mined by a number of factors, including that agent's currentgoals in the domain, and a set of obligations that are inducedby a set of social conventions.
When planning, an agent con-siders both its goals and obligations in order to determine anaction that addresses both to the extent possible.
When priorintentions and obligations conflict, an agent generally willdelay pursuit of its intentions in order to satisfy the obliga-tions, although the agent may behave otherwise at the costof violating its obligations.
At any given time, an agent mayhave many obligations and many different goals, and plan-ning involves a complex tradeoff between these differentfactors.Returning to the example about questions, when an agentis asked a question, this creates an obligation to respond.The agent does not have to adopt he goal of answering thequestion as one of her personal goals in order to explain thebehavior.
Rather it is a constraint on the actions that theagent may plan to do.
In fact, the agent might have an ex-plicit goal not to answer the question, yet still is obliged tooffer a response (e.g., consider most politicians at press con-ferences).
The planning task then is to satisfy the obligationof responding to the question, without revealing the answerif at all possible.
In cases where the agent does not knowthe answer, the obligation to respond may be discharged bysome explicit statement of her inability to give the answer.3 Obligations and Discourse ObligationsObligations represent what an agent should do, according tosome set of norms.
The notion of obligation has been studiedfor many centuries, and its formal aspects are examined usingDeontic Logic.
Our needs are fairly simple, and do notrequire an extensive survey of the complexities that arise inthat literature.
Still, the intuitions underlying that work willhelp to clarify what an obligation is.
Generally, obligation isdefined in terms of a modal operator often called permissible.An action is obligatory if it is not permissible not to do it.An action is forbidden if it is not permissible.
An informalsemantics of the operator can be given by positing a set ofrules of behavior R. An action is obligatory if its occurrencelogically follows from R, and forbidden if its non-occurrencelogically follows from R. An action that might occur or not-occur according to R is neither obligatory nor forbidden.Just because an action is obligatory with respect to a set ofrules R does not mean that the agent will perform the action.So we do not adopt the model suggested by \[Shoham andTennenholtz, 1992\] in which agents' behavior cannot vio-late the defined social laws.
If  an obligation is not satisfied,then this means that one of the rules must have been broken.We assume that agents generally plan their actions to violateas few rules as possible, and so obligated actions will usu-ally occur.
But when they directly conflict with the agent'spersonal goals, the agent may choose to violate them.
Obli-gations are quite different from and can not be reduced tointentions and goals.
In particular, an agent may be obligedto do an action that is contrary to his goals (for example,consider achild who has to apologize for hitting her youngerbrother).Obligations also cannot be reduced to simple xpectations,although obligations may act as a source of expectations.Expectations can be used to guide the action interpretationand plan-recognition processes (as proposed by \[Carberry,1990\]), but expectations do not in and of themselves providea sufficient motivation for an agent o perform the expectedaction - in many cases there is nothing wrong with doingthe unexpected or not performing an expected action.
Theinterpretation f an utterance will often be clear even withoutcoherence with prior expectations.
We need to allow forthe possibility that an agent has performed an action evenwhen this violates expectations.
If an agent actually violatesobligations as well then the agent can be held accountable.
1Specific obligations arise from a variety of sources.
In aconversational setting, an accepted offer or a promise willincur an obligation.
Also, a command or request by theother party will bring about an obligation to perform therequested action.
If the obligation is to say something thenwe call this a discourse obligation.
Our model of obligationis very simple.
We use a set of rules that encode discourseconventions.
Whenever a new conversation act is determined1 \[McRoy, 1993\] uses expectations derived from Adjacency Pairstructure \[Schegloff and Sacks, 1973\], as are many of the discourseobligations considered inthis paper.
These xpectations correspondto social norms and do impose the same notion of accountabil-ity.
However, the analysis there is oriented towards discoveringmisconceptions based on violated expectations, and the alternativepossibility of violated obligations is not considered in the utter-ance recognition process, nor allowed in the utterance productionprocess.2to have been performed, then any future action that can beinferred from the conventional rules becomes an obligation.We use a simple forward chaining technique to introduceobligations.Some obligation rules based on the performance of con-versation acts are summarized in Table 1.
When an agentperforms a promise to perform an action, or performs anacceptance of a suggestion or request by another agent toperform an action, the agent obliges itself to achieve theaction in question.
When another agent requests that someaction be performed, the request itself brings an obligation toaddress the request: that is either accept it or to reject it (andmake the decision known to the requester) - the requestee isnot permitted to ignore the request.
A question establishesan obligation to answer the question.
If an utterance has notbeen understood, or is believed to be deficient in some way,this brings about an obligation to repair the utterance.source of obligation obliged actionSI Accept or Promise A $1 achieve ASt Request A $2 address Request:accept A or reject AS I YNQ whether P $2 Answer-if PS j WHQ P(x) $2 Inform-ref xutterance not understood repair utteranceor incorrectTable I: Sample Obligation Rules3.1 Obligations and BehaviorObligations (or at least beliefs that the agent has obligations)will thus form an important part of the reasoning processof a deliberative agent, e.g., the architecture proposed by\[Bratman et al, 1988\].
In addition to considering beliefsabout the world, which will govern the possibility of per-forming actions and likelyhood of success, and desires orgoals which will govern the utility or desirability of actions,a social agent will also have to consider obligations, whichgovern the permissibility of actions.There are a large number of strategies that may be used toincorporate obligations into the deliberative process, basedon how much weight they are given compared to the agentsgoals.
\[Conte and Castelfranchi, 1993\] present several strate-gies of moving from obligations to actions, including: auto-matically performing an obligated action, adopting all obli-gations as goals, or adopting an obligated action as a goalonly when performing the action results in a state desired bythe agent.
In the latter cases, these goals still might conflictwith other goals of the agent, and so are not guaranteed tobeperformed.In general, we will want to allow action based on obli-gations to supersede performance of intended actions.
Forinstance, consider an agent with an intention to do somethingas soon as possible.
If an obligation is imposed, itwill still bepossible to perform the intended action, but a well-behavedagent might need to delay performance until the obligationis dealt with.
For example, if the intention is to performa series of inform acts, and then a listener equests repairof one, a well-behaved agent will repair that inform beforeproceeding to initiate the next intended one.4 Using Discourse Obligations in a DialogueSystemWe have built a system that explicitly uses discourse obli-gations and communicative intentions to partake in naturaldialogue.
This system plays the role of the dialogue managerin the TRAINS dialogue system which acts as an intelligentplanning assistant in a transportation domain.
While this isa domain where the assumption of co-operation is generallyvalid, the obligation model still provides for a much simpleranalysis of the discourse behavior than a strongly plan-basedaccount.
An example of a dialogue that the TRAINS systemcan engage in is shown in Figure 1.
Below we describe partsof the discourse model in more detail and then show how itis used to account for aspects of this dialogue.Utt.
#123-3=63-73-845-15-267 -1~7-389=131415-2--415-5=715-8=10161718-319Speaker: UtteranceU: We better ship a boxcar of oranges to Bathby 8 AM.S : Okay.U: So we need to get a boxcar to Comingwhere there are oranges.U: There are oranges at Corning.U: Right?S : Right.U: So we need an engine to move the boxcar.U: Right?S : Right.U: So there's an engine at Avon.U: Right?S : Right.U: So we should move the engine at Avon, en-gine El, to Dansville to pick up the boxcarthere.S : Okay.U: And move it from Dansville to Corning.U: Load up some oranges into the boxcar.U: And then move it on to Bath.S : Okay.U: How does that sound?S : That's no problem.U: Good.Figure 1: Sample dialogue 2 processed by TRAINS-93.The TRAINS System \[Allen and Schubert, 1991\] is alargeintegrated natural language conversation and plan reasoning2This is a slightly simplified version of a spoken dialogue be-tween two people.
The original is dialogue 91-6.1 from \[Grosset al, 1993\].
The utterance numbering system used here reflectsthe relation to the turn and utterance numbering used there.
'3-7'represents utterance 7 within turn 3.
'=' is used to indicate mergedutterances.
Thus '3-3=6' spans four utterances in turn 3 of theoriginal, and 9=13 replaces turns 9 through 13 in the original.3system.
We concentrate here, however, on just one part ofthat system, the discourse actor which drives the actions ofthe dialogue manager module.
Figure 2 illustrates the systemfrom the viewpoint of the dialogue manager.II UserII NL InputI NL Interpretation ModulesObservedConversation ActsDialogue \]Manager j~I Domain DirectivesI Domain Task Interaction 1Modules'~'1IINL Output j-I NL Generation 1ModuleIntendedConversation ActsDomain Observationsand Directive ResponsesFigure 2: Dialogue Manager's High-Level View of the Ar-chitecture of the TRAINS Conversation SystemThe dialogue manager is responsible for maintaining theflow of conversation a d making sure that he conversationalgoals are met.
For this system, the main goals are that anexecutable plan which meets the user's goals is constructedand agreed upon by both the system and the user and thenthat he plan is executed.The dialogue manager must keep track of the current stateof the dialogue, determine the effects of observed conversa-tion acts, generate utterances back, and send commands tothe domain plan reasoner and domain plan executor whenappropriate.
Conversational ction is represented using thetheory of Conversation Acts \[Traum and Hinkelman, 1992\]which augments raditional Core Speech Acts with levels ofacts for turn-taking, rounding \[Clark and Schaefer, 1989\],and argumentation.
Each utterance will generally containacts (or partial acts) at each of these levels.4.1 Representing Mental AttitudesAs well as representing general obligations within the tem-poral logic used to represent general knowledge, the systemalso maintains two stacks (one for each conversant) ofpend-ing discourse obligations.
Each obligation on the stack isrepresented asan obligation type paired with a content.
Thestack structure is appropriate because, in general, one mustrespond to the most recently imposed obligation first.
Asexplained in Section 4.2, the system will attend to obliga-tions before considering other parts of the discourse context.Most obligations will result in the formation of intentions tocommunicate something back to the user.
When the inten-tions are formed, the obligations are removed from the stack,although they have not yet actually been met.
If, for somereason, the system dropped the intention without satisfying itand the obligation were still current, the system would placethem back on the stack.The over-riding oal for the TRAINS domain is to con-struct and execute a plan that is shared between the twoparticipants.
This leads to other goals such as accepting pro-posals that he other agent has suggested, performing domainplan synthesis, proposing plans to the other agent which thedomain plan reasoner has constructed, or executing a com-pleted plan.4.2 The Discourse Actor AlgorithmIn designing an agent o control the behavior of the dialoguemanager, we choose areactive approach in which the systemwill not deliberate and add new intentions until after it hasperformed the actions which are already intended.
As shownabove, though, new obligations will need to be addressedbefore performing intended actions.
The agent's deliberativebehavior could thus be characterized in an abstract sense as:loopperceive world and update beliefsif system has obligationsthen address obligationselse if system has performable intentionsthen perform actionselse deliberate on goalsWhen deciding what to do next, the agent first considersobligations and decides how to update the intentional struc-ture (add new goals or intentions) based on these obligations.Obligations might also lead directly to immediate action.
Ifthere are no obligations, then the agent will consider its in-tentions and perform any actions which it can to satisfy theseintentions.
If there are no performable intentions, then thesystem will deliberate on its overall goals and perhaps adoptsome new intentions (which can then be performed on thenext iteration).For the discourse actor, special consideration must begiven to the extra constraints which participation i a conver-sation imposes.
This includes ome weak general obligations(such as acknowledging utterances by others and not inter-rupting) as well as some xtra goals coming from the domainsetting to maintain a shared view of the world and the domainplans which are to be executed.
We prioritize the sources forthe deliberations of the actor as follows:1.
Discourse Obligations from Table 12.
Weak Obligation: Don't interrupt user's turn3.
Intended Speech Acts4.
Weak Obl: Grounding (coordinate mutual beliefs)5.
Discourse Goals: Domain Plan Negotiation6.
High-level Discourse Goals4The implemented actor serializes consideration of thesesources into the algorithm in Figure 3.
(1) while conversation is not finished(2) if system has obligations(3) then address obligations(4) else if system has turn(5) then if system has intended conversation acts(6) then call generator toproduce NL utterances(7) else if some material is ungrounded(8) then address grounding situation(9) else if some proposal is not accepted(I 0) then consider proposals(11 ) else if high-level goals are unsatisfied(12) then address goals(13) else release turnor attempt to end conversation(I 4) else if no one has turn(15) then take turn(16) else if long pause(17) then take turnFigure 3: Discourse Actor AlgorithmThe updating of the conversational state due to perceivedconversation acts or actions of other modules of  the systemprogresses asynchronously with the operation of the dis-course actor.
Whenever the discourse actor is active, it willfirst decide on which task to attempt, according to the pri-orities given in Figure 3, and then work on that task.
Aftercompleting a particular task, it will then run through the loopagain, searching for the next task, although by then the con-text may have changed ue to, e.g., the observance of a newutterance from the user.
The actor is always running anddecides at each iteration whether to speak or not (accord-ing to turn-taking conventions); the system does not need towait until a user utterance is observed to invoke the actor,and need not respond to user utterances in an utterance byutterance fashion.Lines 2-3 of the algorithm in Figure 3 indicate that theactor's first priority is fulfilling obligations.
I f  there areany, then the actor will do what it thinks best to meet thoseobligations.
If there is an obligation to address a request,the actor will evaluate whether the request is reasonable,and if so, accept it, otherwise reject it, or, if it does nothave sufficient information to decide, attempt to clarify theparameters.
In any case, part of meeting the obligation willbe to form an intention to tell the user of  the decision (e.g., theacceptance, rejection, or clarification).
When this intentionis acted upon and the utterance produced, the obligationwill be discharged.
Other obligation types are to repair anuninterpretable utterance or one in which the presuppositionsare violated, or to answer a question.
In question answering,the actor will query its beliefs and will answer depending onthe result, which might be that the system does not know theanswer.In most cases, the actor will merely form the intentionto produce the appropriate utterance, waiting for a chance,according to turn-taking conventions, to actually generatethe utterance.
In certain cases, though, such as a repair, thesystem will actually try to take control of the turn and pro-duce an utterance immediately.
For motivations other thanobligations, the system adopts a fairly "relaxed" conversa-tional style; it does not try to take the turn until given it by theuser unless the user pauses long enough that the conversationstarts to lag (lines 14-17).
When the system does not havethe turn, the conversational state will still be updated, but theactor will not try to deliberate or act.When the system does have the turn, the actor first (af-ter checking obligations) examines its intended conversa-tion acts.
If there are any, it calls the generator to producean utterance 3 (lines 5-6 of the discourse actor algorithm).Whatever utterances are produced are then reinterpreted (asindicated in Figure 2) and the conversational state updatedaccordingly.
This might, of  course, end up in releasing theturn.
It might not be convenient to generate all the intendedacts in one utterance, in which case there will remain someintended acts left for future utterances to take care of (unlessthe subsequent situation merits dropping those intentions).Only intended speech acts that are part of the same argumen-tation acts as those which are uttered will be kept as intentions- others will revert back to whatever caused the intention tobe formed, although subsequent deliberation might cause theintentions to be re-adopted.If there are no intended conversation acts, the next thingthe actor considers is the grounding situation (lines 7-8).The actor will try to make it mutually believed (or grounded)whether particular speech acts have been performed.
Thiswill involve acknowledging or repairing user utterances, aswell as repairing and requesting acknowledgement of thesystem's own utterances.
Generally, groundingis consideredless urgent han acting based on communicative intentions,although some grounding acts will be performed on the basisof obligations which arise while interpreting prior utterances.If all accessible utterances are grounded, the actor thenconsiders the negotiation of domain beliefs and intentions(lines 9-10).
The actor will try to work towards a shareddomain plan, adding intentions to perform the appropriatespeech acts to work towards this goal.
This includes ac-cepting, rejecting, or requesting retraction of user proposals,requesting acceptance of or retracting system proposals, andinitiating new system proposals or counterproposals.The actor will first look for User proposals which are notshared.
If any of these are found, it will add an intention toaccept he proposal, unless the proposal is deficient in someway (e.g., it will not help towards the goal or the systemhas already come up with a better alternative).
In this lattercase, the system will reject the user's proposal and presentor argue for its own proposal.
Next, the actor will lookto see if any of its own proposals have not been accepted,requesting the user to accept them if they have been, orretracting or reformulating them if they have already beenrejected.
Finally, the actor will check its private plans for3Actually, if the only utterance is an acknowledgement, the actorwill postpone the production until it checks that here is nothing elsethat it can combine in the same utterance, such as an acceptance oranswer.5any parts of the plan which have not yet been proposed.
If itfinds any here, it will adopt an intention to make a suggestionto the user.If none of the more local conversational structure con-straints described above require attention, then the actorwill concern itself with its actual high-level goals.
For theTRAINS system, this will include making calls to the domainplan reasoner and domain executor, which will often returnmaterial to update the system's private view of the plan andinitiate its own new proposals.
It is also at this point that theactor will take control of the conversation, pursuing its ownobjectives rather than responding to those of the user.Finally, if the system has no unmet goals that it can worktowards achieving (line 13), it will hand the turn back to theuser or try to end the conversation if it believes the user'sgoals have been met as well.4.3 ExamplesThe functioning of the actor can be illustrated by its behaviorin the dialogue in Figure 1.
While the discussion here isinformal and skips some details, the dialogue is actuallyprocessed in this manner by the implemented system.
Moredetail both on the dialogue manager and its operation on thisexample can be found in \[Traum, 1994\].Utterance 1is interpreted both (literally) as the initiation 4of an inform about an obligation to perform a domain action(shipping the oranges).
This utterance is also seen as an(indirect) suggestion that this action be the goal of a shareddomain plan to achieve the performance of the action.
Inaddition, this utterance releases the turn to the system.
Fig-ure 4 shows the relevant parts of the discourse state afterinterpretation f this utterance.Discourse Obligations:Turn Holder: SystemIntended Speech Acts:Unack'd Speech Acts: \[ INFORM- 1 \ ] ,  \[ SUGGEST-4 \]Unaccepted Proposals:Discourse Goals: Get-goal Build-Plan Execute-PlanFigure 4: Discourse Context after Utterance 1After interpreting utterance 1, the system first decides toacknowledge this utterance (lines 7-8 in the actor algorithm)- moving the suggestion from an unacknowledged to unac-cepted - and then to accept he proposal (lines 9-10).
Finally,the system acts on the intentions produced by these deliber-ations (lines 4-5) and produces the combined acknowledge-ment/acceptance of utterance 2.
This acceptance makes thegoal shared and also satisfies the first of the discourse goals,that of getting the domain goal to work on.Utterances 3-3=6 and 3-7 are interpreted, but not re-sponded to yet since the user keeps the turn (in this caseby following up with subsequent utterances before the sys-tem has a chance to act).
Utterance 3-8 invokes a discourse4According to the theory of Conversation Acts \[Traum andHinkelman, 1992\], Core Speech Acts such as inform are multi-agent actions which have as their effect a mutual belief, and are notcompleted unless/until they are grounded.obligation on the system to respond to the User's assertionin 3-7 and also gives the turn to the system.
The resultingdiscourse context (after the system decides to acknowledge)is shown in Figure 5.Discourse Obligations: (CHECK-IF ( :AT .
.
. )
)Turn Holder: SystemIntended Speech Acts: (Ack \[INFORM-7\] .
.
.
.
)Unack'd Speech Acts:Unaccepted Proposals: \[ SUGGEST-10 \],  \[ SUGGEST-15 \]Discourse Goals: Bu i ld -P lan  Execute -P lanFigure 5: Discourse Context after Utterance 2The system queries its domain knowledge base and de-cides that the user is correct here (there are, indeed, orangesat Coming), and so decides to meet this obligation (lines2-3) by answering in the affirmative.
This results in formingan intention to inform, which is then realized (along withthe acknowledgement of he utterances) by the generation ofutterance 4.Similar considerations hold for the system responses 6 and8.
The reasoning leading up to utterance 14 is similar to thatleading to utterance 2.
Here the user is suggesting domainactions to help lead to the goal, and the system, when it getsthe turn, acknowledges and accepts this suggestion.Utterances 15-2--4, 15-5=7, and 15-8=10 are interpretedas requests because of the imperative surface structure.
Thediscourse obligation to address the request is incurred onlywhen the system decides to acknowledge the utterances andground them.
After the decision to acknowledge, the obliga-tions are incurred, and the system then addresses the requests,deciding to accept hem all, and adding intentions to performan accept speech act, which is then produced as 16.Utterance 17 is interpreted as a request for evaluation of theplan.
When the system decided to acknowledge, this createsa discourse obligation to address the request.
The systemconsiders this (invoking the domain plan reasoner to searchthe plan for problems or incomplete parts) and decides thatthe plan will work, and so decides to perform the requestedaction - an evaluation speech act.
This is then generated as18-3.
The discourse state after the decision to acknowledgeis shown in Figure 6.Discourse Obligations: (ADDRESS \[REQUEST-49 \] )Turn Holder: SystemIntended Speech Acts: (Ack \[REQUEST-49 \])Unack'd Speech Acts:Unaccepted Proposals:Discourse Goals: Bui ld -P lan  Execute -P lanFigure 6: Discourse Context after Utterance 17After the user's assent, the system then checks its goals,and, having already come up with a suitable plan, executesthis plan in the domain by sending the completed plan to thedomain plan executor.This example illustrates only a small fraction of the capa-bilities of the dialogue model.
In this dialogue, the systemneeded only to follow the initiative of the user.
However thisarchitecture can handle varying degrees of initiative, whileremaining responsive.
The default behavior is to allow theuser to maintain the initiative through the plan constructionphase of the dialogue.
If the user stops and asks for help, oreven just gives up the initiative rather than continuing withfurther suggestions, the system will switch from plan recog-nition to plan elaboration, and will incrementally devise aplan to satisfy the goal (although this plan would probablynot be quite the same as the plan constructed inthis dialogue).We can illustrate the system behaving more on the basisof goals than obligations with a modification of the previousexample.
Here, the user releases the turn back to the systemafter utterance 2, and the deliberation proceeds as follows:the system has no obligations, no communicative intentions,nothing is ungrounded, and there are no unaccepted pro-posals, so the system starts on its high-level goals.
Givenits goal to form a shared plan, and the fact that the currentplan (consisting of the single abstract move-commodS_tyaction) is not executable, the actor will call the domain planreasoner to elaborate the plan.
This will return a list ofaugmentations to the plan which can be safely assumed (in-cluding a move-  eng  5_ne event which generates the move-commodity, given the conditions that the oranges are in aboxcar which is attached to the engine), as well as somechoice point where one of several possibilities could be added(e.g., a choice of the particular engine or boxcar to use).Assuming that the user still has not taken the turn back,the system can now propose these new items to the user.
Thechoice could be resolved in any of several ways: the domainexecutor could be queried for a preference based on priorexperience, or the system could put the matter up to the userin the form of an alternative question, or it could make anarbitrary choice and just suggest one to the user.The user will now be expected to acknowledge and reactto these proposals.
If the system does not get an acknowl-edgement, it will request acknowledgement the next time itconsiders the grounding situation.
If the proposal is not ac-cepted or rejected, the system can request an acceptance.
Ifa proposal is rejected, the system can negotiate and offer acounterproposal or accept a counter proposal from the user.Since the domain plan reasoner \[Ferguson, 1994\] performsboth plan recognition and plan elaboration i an incrementalfashion, proposals from system and user can be integratednaturally in a mixed-initiative fashion.
The termination con-dition will be a shared executable plan which achieves thegoal, and each next action in the collaborative planning pro-cess will be based on local considerations.5 DiscussionWe have argued that obligations play an important role inaccounting for the interactions in dialog.
Obligations do notreplace the plan-based model, but augment i .
The result-ing model more readily accounts for discourse behavior inadversarial situations and other situations where it is implau-sible that the agents adopt each others goals.
The obligationsencode learned social norms, and guide each agent's behav-ior without he need for intention recognition or the use ofshared plans at the discourse level.
While such complexintention recognition may be required in some complex in-teractions, it is not needed to handle the typical interactionsof everyday discourse.
Furthermore, there is no require-ment for mutually-agreed upon rules that create obligations.Clearly, the more two agents agree on the rules, the smootherthe interaction becomes, and some rules are clearly virtuallyuniversal.
But each agent has its own set of individual rules,and we do not need to appeal to shared knowledge to accountfor local discourse behavior.We have also argued that an architecture that uses obli-gations provides a much simpler implementation than thestrong plan-based approaches.
In particular, much of localdiscourse behavior can arise in a "reactive manner" withoutthe need for complex planning.
The other side of the coin,however, is a new set of problems that arise in planning ac-tions that satisfy the multiple constraints that arise from theagent's personal goals and perceived obligations.The model presented here allows naturally for a mixed-initiative conversation and varying levels of cooperativity.Following the initiative of the other can be seen as an obli-gation driven process, while leading the conversation will begoal driven.
Representing both obligations and goals explic-itly allows the system to naturally shift from one mode to theother.
In a strongly cooperative domain, such as TRAINS,the system can subordinate working on its own goals to lo-cally working on concerns of the user, without necessarilyhaving to have any shared discourse plan.
In less coopera-tive situations, the same architecture will allow a system tostill adhere to the conversational conventions, but respond indifferent ways, perhaps rejecting proposals and refusing toanswer questions.AcknowledgementsThis material is based upon work supported by ONR/DARPAunder grant number N00014-92-J-1512.
We would like tothank the rest of the TRAINS group at the University ofRochester for providing a stimulating research environmentand a context for implementing these ideas within an inte-grated system.References\[Airenti et aL, 1993\] Gabriella Airenti, Bruno G. Bara, andMarco Colombetti, "Conversation and Behavior Games inthe Pragmatics of Dialogue," Cognitive Science, 17:197-256, 1993.\[Allen andPerrault, 1980\] James Allen and C. Perrault,"Analyzing Intention in Utterances," Artificial Intelli-gence, 15(3):143-178, 1980.\[Allen and Schubert, 1991\] James F. Allen and Lenhart K.Schubert, "The TRAINS Project," TRAINS Techni-cal Note 91-1, Computer Science Dept.
University ofRochester, 1991.\[Bratman et al, 1988\] Michael E. Bratman, David J. Israel,and Martha E. Pollack, "Plans and Resource-BoundedPractical Reasoning" Technical Report TR425R, SRIInternational, September 1988, Appears in ComputationalIntelligence, Vol.
4, No.
4, 1988.7\[Carberry, 1990\] S. Carberry, Plan Recognition i  NaturalLanguage Dialogue, The MIT Press, Cambridge, MA,1990.\[Clark and Schaefer, 1989\] Herbert H. Clark and Edward ESchaefer, "Contributing toDiscourse," Cognitive Science,13:259 - 94, 1989.\[Cohen and Levesque, 1991\] PhillipR.
Cohen and Hector J.Levesque, "Confirmations and Joint Action" In Proceed-ings IJCAI-91, pages 951-957, 1991.\[Cohen and Perrault, 1979\] Phillip R. Cohen and C. R. Per-rault, "Elements of a Plan-Based Theory of Speech Acts,"Cognitive Science, 3(3): 177-212, 1979.\[Conteand Castelfranchi, 1993\] Rosaria Conte and Cris-tiano Castelfranchi, '~qorms as mental objects.
From nor-mative beliefs to normative goals" In Working NotesAAAlSpring Symposium on Reasoning about Mental States:Formal Theories and Applications., pages 40-47, March1993.\[Ferguson, 1994\] George Ferguson, "Domain Plan Reason-ing in TRAINS-93"' Trains technical note, Computer Sci-ence Dept.
University of Rochester, forthcoming, 1994.\[Gross et al, 1993\] Derek Gross, James Allen, and DavidTraum, "I'he TRAINS 91 Dialogues" TRAINS Tech-nical Note 92-1, Computer Science Dept.
University ofRochester, July 1993.\[Grosz and Sidner, 1986\] Barbara Grosz and Candice Sid-ner, "Attention, Intention, and the Structure of Discourse"CL, 12(3): 175-204, 1986.\[Grosz and Sidner, 1990\] Barbara J. Grosz and Candace L.Sidner, "Plans for Discourse" In P. R. Cohen, J. Morgan,and M. E. Pollack, editors, Intentions in Communication.MIT Press, 1990.\[Kowtkoetal., 1991\] J. Kowtko, S. Isard, and G. Doherty,"Conversational games within dialogue."
In Proceedingsof the ESPRIT Workshop on Discourse Coherence, 1991.\[Litman and Allen, 1987\] D. J. Litman and J. E Allen, "APlan Recognition model for subdialogues in conversa-tion" Cognitive Science, 11:163-200, 1987.\[Mann, 1988\] William C. Mann, "Dialogue Games: Con-ventions of Human Interaction" Argumentation, 2:511-532, 1988.\[McRoy, 1993\] Susan McRoy, Abductive Interpretation andReinterpretation of Natural Language Utterances, PhDthesis, University of Toronto, 1993, Reproduced as TRCSRI-288 Department of Computer Science, Universityof Toronto.\[Schegloffand Sacks, 1973\] E. A. Schegloff and H. Sacks,"Opening Up Closings" Semiotica, 7:289-327, 1973.\[Shoham and Tennenholtz, 1992\] Yoav Shoham and MosheTennenholtz, "On the synthesis of useful social aws forartificial agent societies" In Proceedings AAAI-92, pages276-281, 1992.\[Traum, 1994\] David R. Traum, "The TRAINS-93 DialogueManager" Trains technical note, Computer Science Dept.University of Rochester, forthcoming, 1994.\[Traum and Hinkelman, 1992\] David R. Traum and Eliz-abeth A. Hinkelman, "Conversation Acts in Task-oriented Spoken Dialogue" Computational Intelligence,8(3):575-599, 1992, Special Issue on Non-literal an-guage.8
