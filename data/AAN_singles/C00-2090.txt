Multi-level Similar Segment Matching Algorithmfor Translation Memories and Example-Based Machine TranslationEmmanuel PLANASCyber Solutions Laboratories2-4, Hikaridai Seika-cho Soraku-gunKyoto, 619-0237 Japanplanas @ soy.kecl.ntt.co.jpOsamu FURUSECyber Solutions Laboratories2-4, Hikaridai Seika-cho Soraku-gunKyoto, 619-0237 Japanfuruse@soy.kecl.ntt.co.jpAbstractWe propose a dynamic programmingalgorithm for calculaing the similaritybetween two segmeuts of words of the samelanguage.
The similarity is considered as avector whose coordinates refer to the levelsof analysis of the segments.
This algorithmis extremely efficient for retrieving the bestexample in Translation Memory systems.The calculus being constructive, it also givesthe correspondences between the words ofthe two segments.
This allows the extensionof Translation Memory systems towardsExample-based Machiue Translation.\]Introduction\[n Translation Memory (TM) or Example-BasedlVlachine Translation (EBMT) systems, one oflhe decisive tasks is to retrieve from the database,the example that best approaches the inputsentence.
In Planas (1999) we proposed a two-step retriewd procedure, where a rapid and roughindex-based search gives a short list of examplecandidates, and a refined matching selects thebest candidates from this list.
This proceduredrastically improves the reusability rate ofselected examples to 97% at worst, for ourEnglish-Japanese TM prototype; with theclassical TM strategy, this rate would constantlydecline with the number of non matched words.It also allows a better ecall rate when searchingfor very similar examples.We describe here the Multi-level SimilarSeglnent Matching (MSSM) algorithm on whichis based the second step of the above retrievalprocedure.
This algorithm does not only give thedistance between the input and the examplesource segmeuts, but also indicates which wordswould inatch together.
It uses F different levelsof data (surface words, lemlnas, parts of speech(POS), etc.)
in a combined and uniform way.The computation of the worst case requiresF*m*(n-m+2) operations, where m and n arerespectively the lengths of the input and thecandidate (m<=n).
This leads to a linearbehavior when m and n have similar lengths,which is often the case for TM segmentsLFurthermore, because this algorithm gives theexact matching links (along with the level o1'match) between all of the words of the input andthe candidate sentence, it prepares the transferstage of an evolution of TM that we call ShallowTranslation.
This involves substituting in thecorresponding translated candidate (stored in themelnory), the translation of the substitutedwords, provided that the input and the candidateare "similar enough".1 Matching Principle1.1 The TELA StructureThe purpose of this algorithm is to match twosegments of words: input i and candidate C.These can each be any sequence of words:phrases, sentences, or paragraphs, for example.Let us consider input I of length m, not as asingle segment of surface words, but rather as agroup of F parallel layered segments r \] 0 <:t<:v)each bearing m tokens.
Such a structure is shownin Figure 1, and we call it a TELA structure z. Oneach layer f, the i-th token corresponds to one ofthe paradigms of the i-th word of input i. in ourimplementation, we use a shallow analyzer thatgives three paradigms (F=3) for each surfaceI We use this algorithm on a sorted list of ah'cadysimilar sentences, retrieved with the help of an index.2 Tile idea o1' this structure is ah'eady in Lafourcade'sLEAF (1993), and is explained in Planas (1998).621CC 1C 2C 3 PNCI C2 C3nounC4verbC 5 C6 C 7NqzFa4tyPN advstayedstayverbI ii I2I ~ gony stayedI 2 Sony stay13 PN verbFigure 1: Example of matching TELA structuresword of the segments: the surface word itself(f=l), its lemma (f=2), and its POS tag (f:3).Because we do not need a syntactic analyzer, thetime required for this analysis is not an handicap,moreover such parsers are available for lnanylanguages.
Let C be a candidate segment oflength n, for matching input I of length m(n>=m).
The basic problem involves matchingf the elements of the set (C i)f<:~:.
~ .
.. to those of(I~)f<:~: j<=,n- Only three layers are shown in thefollowing examples but other types of layers,like semantics, or even non linguisticinformation like layout features can beconsidered, as in Planas (1998).
Our algorithm iswritten for the general case (F layers).1.2 Edit Distance based SimilarityWe consider a match from C to 1 as an editdistance process.
This edition uses a sequence ofbasic edit operations between the words of thesegments, like in Wagner & Fisher (1974) whoused four basic operations: deletion, insertion,strict and equal substitution between the lettersof a word.
This approach as also been followedby Gale & Church (1993) for their alignmentalgorithm, with six operations.
Here, we onlyconsider deletions and equalities (i.e.
equalsubstitutions): F+I basic operations in totaP.One equality corresponds toeach of the F layers,and a deletion affects all layers at once.
InFigure 1, the items in bold match each other, andthe strikethrough ones have to be deleted.
Theedition of C into I involves five deletions("Nikkei", "journal", "reported", that", "really"),one equality at layer 1 ("stayed"), two at layer 23 Lepage (1998) also uses deletions and one level ofequality lbr calculating his "pseudo-distance", forgetting the similarity between two strings.Cs C9stm  Monaaystrong Mondayadj UOUllMatclfing zoneI3 I4strongadjx ues ayuoun("stay", "strong"), and four at layer 3 ("PN","verb", "adj", "noun").
At the Word level, thesimilarity between the two segments isconsidered to be the relative number of words ofthe input segment hat are matched by someword of the candidate segmeut in the matchingzone (from "NTT" to "Monday" in ourexample): 1/4 in Figure 1.
The same similaritycan be considered at different levels.
Here, thelemma similarity is 2/4, and the POS similarityis 4/4.
We consider the total similarity as avector involving all layer equalities, plusdeletions: G(C, 1) = (1/4, 2/4, 4/4, 1-1/4, 1-5/9)The fourth coordinate counts the complementaryproportion of deletions in the "matching zone" ofthe candidate C. The last coordinate counts thesame proportion, relatively to the wholecandidate.
We take the complement to 1 because,the more deletions them am, the smaller thesimilarity becomes.When different Ci candidates are possible formatching I, the greatest (~(Cio, 1), according tocommon the partial order on vectors, determinesthe best candidate Cio.1.3 Matching Strategy1.3.1 BasicsWe try to match each word C a of candidate C, toa word Ij of input I. Ci inatches lj if one of theparadigms of C~ equals one of the paradigms of Ijat the same level f, i.e.
if Cfi and I\], are equal.When a failure to match two words with theirparadigms Cfi to i~ occurs at a given level f, wetry to match the words at the next upper levelf+l: Cmi and Ir+~j.
When all of the possiblelayers of the two words have been tried withoutsuccess, we try to match the next word C m to thesame Ij.
If Ci does not match any word of I at any622level, we consider that it has to be deleted.
Allwords of I have to be matched by some word ofC: no insertion is allowed (see section 1.3.4).1'.3.2 Lazy matchWith TM tools, if some useful candidates arefound, they usually utilize words silnilar to theinput words because translation memories arcapplied within very similar documents, most oftile time between ancient and newer versions ofa same document.
When tile priority is rapidity(rather than non-ambiguity), we can considerthat a lnatch is reached as soon as a word of Cand a word of I match at a certain layer f. It isnot necessary to look at upper levels, for theyshould lnatch because of tile expected similaritybetween tile input and tile candidate.
Tileprevious example illustrates this.
As upper levelsare not tested, this allows a gain in tile numberof iterations ot' the algorithm.
Experiments (seePlanas (1999)) have confirmed this to be acorrect strategy for TM.
That's why, we considerfrom now on dealing with such a lazy match.1.3.3 Exhaustive matchIn the most general case, ambiguity problemsprevent us fl'om employing the lazy strategy, anda correct inatch requires that whenever two itemsCJ~ and I f. match at a certain level f, they shouldJmatch at upper levels.
Here is an example:221 Sony stay c-~4ed~2 2 Sony stay ended M~mday;23 PN ne+m ~?erb noun( ~ Sony ,~vyed sU~ngef T44esday\]\[ Sony stay strong TuesdayI PN ~?erb adj" nounFigure 2: Lemma ambiguityh3 C2, the lemma "stay" of surface word "stay"matches tile lemma "stay" of surl'ace word"stayed" of I, but they do not match at the POSlevel (noun and verb).
Tile algorithm should goto this level to find that there is no match.
Onceagain, however, because this algorithm has beenbuilt for TM systems, such alnbiguities hardlyoccur .1.3.4 InsertionIf some items in I are not matched by any itelnof C, the match involves an insertion.Case of Translation MemoriesIf tile candidate sentences are to be used by ahmnan translator, s/he will be able to insert themissing word at the right place.
Accordingly, amatch with insertion can be used for pure TM.Case of Shallow Translation (EBMT)in the EBMT system we are targeting, we planto use tile matching sub-string of C foradaptation to I without syntactic rules.Accordingly, we consider that we do not knowwhere to insert the non matching item: in thiscase, we force tile algorithm to stop if aninsertion is needed for matching C and I. Fromnow on, we will follow this position.1.3.5 "1)'aceWe want the output of the algorithm as a list oftriplets (Cri I f, op)~< i<-.
called a "trace", where criJ - _corresponds to P; through the "op" operation.
Wenote op="f" an cquality at level f, and op="0" adeletion.
For Example 1, the trace should be:(100) (200) (300) (400) (513)  (600) (72I) (832) (943)2 Adapting Wagner & Fischer, andSellers algorithms2.1 Algorithm PrincipleThe Wagner & Fischer (W&F) dynamicprogramming algorithm in Figure 3 gives tileedit distance between C and I:For j=0 to md\[j, 0\]-i//initiating the cohmmsFor i=l to ndlO, i\]=i//initiating the rowsFor i= 1 to nForj=l to mIf(l\[j\]=C\[i\]) {d=d\[i-l, j-1\]}//equalityElse {d=d\[i-1, j-1\]+l} //subst.d\[j,i\]=min(d\[i-1, j\]+l, d\[i, j-1\]+l, d)End ForEnd ForPrint d\[n, m\]Figure 3: The Wagner & Fisher algorithmTile distance is obtained in m*n operations, bybuilding an \[re+l, n+l\] array (see Figure 6).
Illaddition, W&F (1974) proposed a backtrackingprocedure, shown in Figure 4, that scans backthis array to give a "trace" of the match between623C and 1 (i.e.
it prints the position of the matchingwords), in (m+n) operations.
The trace is thenobtained in (mn+m+n) operations in total.
Thisalgorithm was previously used in Planas (1998)at each layer of a TELA structure to give a traceby layer.
The data fi'om the traces of thedifferent layers were combined afterwards forthe purposes of TM and EBMT.
However, thisprocedure is not optimal for at least two reasons.First, the layers are compared in an independentway, leading to a waste of time in the case ofTM, because the lazy match phenomenon is notused.
Second, the combination of the results wasprocessed after the algorithm, and this required asupplementary process.
One can imagine thatprocessing the whole data in the flow of theinstructions of the algorithm is more efficient.i= i0; j = m;while (i > 0) and (j > 0)//del// i f (d \ [ i , j \ ]=d\ [ i - l , j \ ]+ l ){ i= i -1}//ins// else if(d\[i,j\]= d\[i, j- l\]+ 1) {j =j -1}else//equality orsubstitutionprint (i, j)i= i -1 ; j= j - Iend ifend whileFigure 4: W&F backtracking algorithm2.2 Two operation based mininf izat ionIf we look back at the W&F algorithm, shown inFigure 3, the part in bold represents the codeinvolved in the calculus of the next localdistance d\[i, j\].
It testes which of the four basicedit operations (deletion, insertion, equal orstrict substitution) gives the lowest partialdistance.
Nevertheless, we have shown insection 1.3.4 that only deletions and equalitiesI i 111 112 113First press theCli 0 inf inf infC1~ First 0 0 inf infC12 press 0 1 0 infC1.~ only 0 2 1 infC14 the 0 3 2 1Cl.s red 0 4 3 2C~6 button 0 5 4 3do interest us.
We therefore reduce the test in thealgorithm to that shown in Figure 5.
Furthermore,we initiate the columns of the array with infinitevalues (huge values in practice) to show thatinitial insertions are not possible, and the rows to"0", to count the deletions relatively to iuput I.See Sellers (1980) for a due explanation.If(I\[j\]=C\[i\]) {d=d\[i-l, j-I \]}//equal: no costElse {d=inf}//big integer, in theory h~finited\[j,i\] = rain (d\[i-1, j\]+l, d)//deletion or equal ?Figure 5: Matching with deletions and equalitiesAn example of the successive scores calculatedwith this algorithm are shown in Figure 6.
Thetotal distance (equal to 1) between C and Iappears in the lowest right cell.The fact that only two operations are usederadicates the ambiguity that appears inselecting the next cell in the W&F algorithmbacktracking procedure with four operations.
Inour algorithm, either there is an equality (cost 0),or a deletion (cost 1).
The possibility of havingthe same cost 1 for insertions, deletions, or strictsubstitutions has been eliminated.2.3 In t roduc ing  one equal i ty per  levelAs mentioned previously, we need to matchitems at different layers.
We introduce here twonew points to deal with this:?
In order to keep the score for each equalitydeletion, d\[i,j\] is a vector instead of anumber: d\[i,j\]=\[scorel ..... scorer, score\].?
In this vector, score~ through scor% storethe number of equalities for each layer f,and score records the number of deletions,as in W&F (underlined in the arrays).114redinfinfinfinfinf115buttoninfinfinfinfinfD\[i-1, j- l\]1 inf2 WFigure 6: Successive scores produced by the adapted W&F algorithmD\[i-1, j\]deletionCi-gsD\[i, j\]6240CIGGc~GGC7Gltii /12C l C 2 CS/l sword lem POSSony Sony PNsaythatleportedthat0 Ii L Isword Sony stays stronglem Sony stay strongPOS PN verb adj9000 oooN 9oo,,j ooo,,j9000 100_0 300imf O00inf9000 100_1 101_0 000inl"9000 1002 101_1 000inf9000 0010 1012 000inf90OO 001_1 IH1-0 000inl_"9000 001_2 311_1 021_09000 001_0 3112 021_19000 001-1 3113 021_2verbcoatiNTT NTT PNstayed stay VerbstrongTuesdaystrongerTuesdaymorning morningAdjPNnouuI4TuesdayTuesdayPN900,,j300in__f300inf300inf300in__f300inf300in__ f12101.211Figure 7: Introducing a vector of deletion and layer equalities coresFigure 7 shows an example of diflbrent scorevectors involved in a match.
To calculate thesuccessive d\[id\], we use tile algorithm of Figure5 adapted for F levels in Figure 8.If(Ir\[j\]=cf\[i\])d~=\[d' \[i- l ,j- 1 \] .. .
.
.
dV\[i - l ,j-l 1+ 1 ,d--\[i- 1j-  l \] \]Elsed~=\[0 ..... 0,inf.IEnd \]1"dd=\[d' \[i-1 ,j\] .
.
.
.
.
df\[i - I d\] .
.
.
.
dV\[i - 1 d \], d--\[i-1 ,j \]+1\]d\[j,i\] = max (d~, d,i) //equali O, or deletionFigure 8: Adapting the algorithm to F levelsWe first try to get the maxinmm nmnber ofequalities and then tile mininmm of deletions.Each tilne we find a new match in the firstcolunm, we start a new path (see I ~ matchingwith C I, C 4 and C 7 in Figure 7).
It' one of thevectors of the last column of tile array is suchthat: SUMk=r<=v (scorer) = In ,  there is a matchingsubstring of C in which there is a matching wordfor each of the words of I: this constitutes asolution.
In our example, cell (7, 4), with score121__0 shows that there is a sub chain of thecandidate that matches tile input with 1, 2, and 1matches at the word, lemma, and POS levels and0 deletions.
Cell (8, 4) indicates a similar naatch,but with 1 deletion Cmorning").
Tile best paththen ends at cell (7,4).
Starting from this cell, wecan retrieve tile full solution using the W&Fbacktrack algorithm adapted to F levels.This approach allows us to choose as compact astring as possible.
When there are severalpossible paths, like in Figure 9, the algorithm isable to choose the best matching sub-string.
Ifwe are looking for a similarity involving first11C 2 C-71 slem POSSony PNsay verbstrong adjTuesday PNand zonjNTT PNstay Verbstrong AdjTuesday PNmorning nounCri C l0 wordC~ SonyC2 stayedC3 strongerC4 TuesdayC s andC6 NTTC7 stayedCs strongerC9 TuesdayCIo morning0 l j I2 I3word Sony stays stronglem Sony stay strongPOS PN verb adjoooo ooo#_vf 9oo~ ooo~0000 1000 300inl__" O00inf0000 1001 110-0 000in__f0000 100_2 1101 120_00000 100_Q 102 12010000 1001 110._3 12020000 001_0 1104 12030000 0011 Dll-0 12040000 0012 311-1 021_00000 001_0 3112 021_10000 001_1 3113 0212Figure 9: Selecting the best concurrent sub segment\[,1TuesdayTuesdayPN)00/~r)00in__f)00int")00in_fZ200Z20!2202220322041210121!625surface word matches, then lemmas and parts ofspeech, then cell (4,4) of score 2200 will bechosen.
This strategy can be adapted toparticular needs: it suffices to change the orderof the scores in the vectors.3 Optimizing3.1 Triangularization of the arrayIn this algorithm, for each Ij, there must be atleast one possible matching C~.
Hence, in a validpath, there are at least m matches.
As a matchbetween C~ and Ij occurs when "stepping across adiagonal", the (m-l) first diagonals (from thelower left corner of the array) can not give birthto a valid path.
Therefore, we do not calculated\[i,j\] across these small diagonals.Symmetrically, the small diagonals after the lastfull one (in the upper right corner) cannot givebirth to a valid path.
We then also eliminatethese (m-l) last diagonals.
This gives a reducedmatrix as shown in the new example in Figure10.
The computed cells are then situated in aparallelogram of dimensions (n-m+l) and m.The results is: only m(n-m+l) cells have to becomputed.
Instead of initiating the first row 0 to"inf", we initiate the cells of the diagonal justbefore the last full top diagonal (between cell(0,1) and cell (3,4)in Figure 10) to "000inf" tobe sure that no insertion is possible.3.2.
ComplexityThe worst time complexity of this algorithm isF-proportional to the number of cells in thecomputed array, which is ln*(n-m+l).
With the"lazy" strategy, all F levels are often not visited.As the number of cells computed by the W&Falgorithm is m'n, our algorithm is always morerapid.
The backtracking algorithm takes m+noperations in the W&F algorithm, as well as inour algorithm, leading to m(n-m+2)+noperations in the MSSM algorithm, andm(n+l)+n operations in the W&F algorithm.The general complexity is then sub-quadratic.When the lengths of both segments to becompared are similar (like it often happens inTMs), the complexity tends towards linearity.The two graphics in Figure 11 show twointeresting particular cases (ln=n and m runningfrom 1 to n=10), comparing W&F and ouralgorithm.
For strings of similar lengths, thelonger they are, the more the MSSM algorithmbecomes interesting.
When n is fixed, theMSSM algorithm is more interesting for extremevalues of the length of I: small and similar to n.ConclusionsThe first contribution of this algorithm is toprovide TM and EBMT systems with a preciseand quick way to compare segments of wordswith a similarity vector.
This leads to an ahnostcomplete radication of noise for the matter ofretrieving similar sentences in TM systems (97%"reusability" in our prototype).
The second is tooffer an unambiguous word to word matchingthrough the "trace".
This last point opens theway to the Shallow Translation paradigm.ca0C,C2C3C4CsC6C7Csi /FC 1 C 2 C3/I 3woM !era POS~onyreportedthatNTTstayedstrongerI'uesdaymorningSony~ayLhatNTT~tay~trong?uesdaymorningPNverbconjPNVerbAdjP nounn o u n0 11 12 13 14word Sony stays strong Tuesdaylem Sony stay strong TuesdayPOS PN verb adj PN0000 900i~0000 1000 O00'mf0000 1001 101_0 900/nf0000 1002 1011 300inf 000~0000 I)01_0 1012 300inf" 000inf.0000 9011 011_0 )00in_f 000inf"0000 0111 D210 000int"0000 3211 121_00000 1211Figure 10: Eliminating left and Hght small diagonals626140120100o 806040200Comparison W&F / MSSM1 2 3 4 5 6 7 8 9 10n=m+W&F--t1~-- MSSM-'--r~---W&F + BkI : MSSM + Bkt40120100806040200Comparison W&F / MSSM~J*7 *2 3 4 5 6 7 8 9 10m (n=l O)-~ I ' - -  W& F- I~- - -  MSSM- /~- -  W&F 4.
Bk: MSSM 4.
BkFigure 11: Comparing the Wagner & Fisher andMSSM algorithmsFor more information about the use of thisalgorithm, please refer to Planas (1999).
Thesetwo contributions bring in the main differencewith relative research 4 concentrating onsimilarity only, represented by a sole integer.The TELA structure, that allows the parallel useof different layers of analysis (linguisticparadigms, but possibly non linguisticinformation) is essential to this work because itprovides the algorithm with the supplementaryinformation classical systems lack.The fact that the shallow parser (lemmas, POS)is ambiguous or not does not affect significantlythe performance of the algorithln.
If the sameparser is used for both example and inputsegments, parallel errors compensate each other.Of course, these errors do have an influence forEBMT: the non ambiguity is then a must.A first evaluation of the MSSM speed gives 0.5to 2 milliseconds for comparing only s tworandomly chosen English or Japanese sentencesover 3 levels (word, lemmas, POS).
The4 Cranias et al (1997), Thompson & Brew (1994), orin a more specific way, Lcpage (1998)5 Without he shallow analysisimplementation has been done with a DELLOptiplex GX 1 233 Mhz, Window NT, Java 1 18.This algorithm can be improved in differentways.
For speed, we can introduce a similaritythreshold so as not to evaluate the last cells ofthe columns of the computed array as soon as thethreshold is overtaken.
For adaptability, beingable to deal with a different number of tokensaccording to each layer will allow us to dealnicely with compound words.In short, if the basis of this matching algorithmis the W&F algorithm, other algorithms can beadapted similarly to deal with multi-level data.AcknowledgementsThanks to Takayuki Adachi, Francis Bond,Timothy Balwin, and Christian Boitet for theiruseful remarks and fruitful discussions.ReferencesCranias, L., Papageorgiou, H., & Pipcridis, S. (1997)Example retrieval .fron~ a 7)zmslation Memory.Natural Language Engineering 3(4), CambridgeUniversity Press, pp.
255-277.Gale, W.A.
& Church, K.W.
(1993) A program .forAligning Sentences in Bilingttal Corpora.
Compu-lational Linguistics, ACL, Vol.
19, No.
1.Lafourcade M. (1993) LEAF, ou comment garderl'Originalitd de l'ambiguitd.
Aclualitd Scienlifiquc -Troisi~mes Journdes Scientifiques Traduclique-TA-TAO, Montrdal, Canada, AUPELF-UREF, Vol.
1/1,pp.
165-185.Lepage Y.
(1998) Solving amtlogies on words: cmalgorithm.
Coling-ACL'98, Vol.
I, pp.
728-734.Sellers, P.H.
(1980) The theory and computation ofevolutionmy distances: pauenl recognition.
Jour-nal o1' Algorithms, Vol.
127, pp.
359-373.Thompson Henry S. & Brew Chris (1996) AutomaticEvaluation of Computer Generated text: FinalReport on the TextEval Project.
HumanCommunication Research Center, University ofEdinburg.Wagner, A. R. & Fischer M. (1974) 7he String-to-String Con'ection Problem.
Journal of the ACM,Vol.
21, #1, pp.
168-173.Planas, E. (1998) TELA: Structures and Algorithmsfor Memory-Based Machine 7)'anslation.
Ph.D.thesis, University Joseph Fourier, Grenoble.Planas, E. & Furuse O.
(1999) Fom~alizingTranslation Memories.
Machine TranslationSt, remit VII, Singapore, pp.
331-339627
