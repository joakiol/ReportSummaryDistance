Discourse-level argumentation in scientific articles:human and automatic annotationS imone Teufe l  and  Marc  MoensHCRC Language Techno logy GroupDiv is ion of In fo rmat icsUn ivers i ty  of  Ed inburghS.
Teufel@ed.
ac.
uk, M. Moens@ed.
ac.
ukAbst rac tIn this paper we present a rhetorically de-fined annotation scheme which is part ofour corpus-based method for the summari-sation of scientific articles.
The annotationscheme consists of seven non-hierarchicallabels which model prototypical academicargumentation and expected intentional'moves'.
In a large-scale xperiments withthree expert coders, we found the schemestable and reproducible.
We have built aresource consisting of 80 papers annotatedby the scheme, and we show that this kindof resource can be used to train a systemto automate the annotation work.1 IntroductionWork on summarisation has suffered from a lack ofappropriately annotated corpora that can be usedfor building, training and evaluating summarisationsystems.
Typically, corpus work in this area hastaken as its starting point texts target summaries:abstracts written by the researchers, upplied bythe original authors or provided by professional ab-stractors.
Training a summarisation system then in-volves learning the properties of sentences in thoseabstracts and using this knowledge to extract simi-lax abstract-worthy sentences from unseen texts.
Inthis scenario, system performance or developmentprogress can be evaluated by taking texts in a testsample and comparing the sentences extracted fromthese texts with the sentences in the target abstract.But this approach as a number of shortcomings.First, sentence xtraction on its own is a very gen-eral methodology, which can produce extracts thatare incoherent or under-informative especially whenused for high-compression summarisation (i.e.
reduc-ing a document to a small percentage of its orig-inal size).
It is difficult to overcome this prob-lem, because once sentences have been extractedfrom the source text, the context that is neededfor their interpretation is not available anymore andcannot be used to produce more coherent abstracts(Spgrck Jones, 1998).Our proposed solution to this problem is to ex-tract sentences but also to classify them into one ofa small number of possible argumentative roles, re-flecting whether the sentence xpresses a main goalof the source text, a shortcoming in someone lse'swork, etc.
The summarisation system can then usethis information to generate template-like abstracts:Main goal of the text:.
.
.
; Builds on work by:.
.
.
;Contrasts with:... ; etc.Second, the question of what constitutes a use-ful gold standard has not yet been solved satisfac-torily.
Researchers developing corpus resources forsummarisation work have often defined their owngold standard, relying on their own intuitions (see,e.g.
Luhn, 1958; Edmundson, 1969) or have usedabstracts upplied by authors or by professional ab-stractors as their gold standard (e.g.
Kupiec et al,1995; Mani and Bloedorn, 1998).
Neither approachis very satisfactory.
Relying only on your own intu-itions inevitably creates a biased resource; indeed,Rath et al (1961) report low agreement betweenhuman judges carrying out this kind of task.
Onthe other hand, using abstracts as targets is notnecessarily a good gold standard for comparison ofthe systems' results, although abstracts are the onlykind of gold standard that comes for free with thepapers.
Even if the abstracts are written by pro-fessional abstractors, there are considerable differ-ences in length, structure, and information content.This is due to differences in the common abstractpresentation style in different disciplines and to theprojected use of the abstracts (cf.
Liddy, 1991).
Inthe case of our corpus, an additional problem wasthe fact that the abstracts are written by the au-thors themselves and thus susceptible to differences84in individual writing style.For the task of summarisation and relevance deci-sion between similar papers, however, it is essentialthat the information contained in the gold standardis comparable between papers.
In our approach, thevehicle for comparability of information is similarityin argumentative roles of the associated sentences.We argue that it is more difficult to find the kind ofinformation that preserves similarity of argumenta-tive roles, and that it is not guaranteed that it willoccur in the abstract.
: .
.
.
.A related problem concerns fair evaluation Ofthe extraction methodology.
The evaluation of ex-tracted material necessarily consists of a comparisonof sentences, whereas one would really want to com-pare the informational content of the extracted sen-tences and the target abstract.
Thus it will often bethe case that a system extracts a sentence which inthat form does not appear in the supplied abstract(resulting in a low performance score) but which isnevertheless an abstract-worthy sentence.
The mis-match often arises simply because a similar idea isexpressed in the supplied abstract in a very differ-ent form.
But comparison of content is difficult toperform: it would require sentences to be mappedinto some underlying meaning representations andthen comparing these to the representations of thesentences in the gold standard.
As this is techni-cally not feasible, system performance is typicallyperformed against a fixed gold standard (e.g.
theaforementioned abstracts), which is ultimately un-desirable.Our proposed solution to this problem is to builda corpus which details not only what the abstract-worthy sentences are but also what their argumen-tative role is.
This corpus can then be used as aresource to build a system to similarly classify sen-tences in unseen texts, and to evaluate that system.This paper reports on the development of a set ofsuch argumentative roles that we have been using inour work.In particular, we employ human intuition to an-notate argumentatively defined information.
Weask our annotators to classify every sentence in thesource text in terms of its argumentative role (e.g.that it expresses the main goal of the source text, oridentifies open problems in earlier work, etc).
Underthis scenario, system evaluation is no longer a com-parison of extracted sentences against a supplied ab-stract, or against a single sentence that was chosenas expressing (e.g.)
the main goal of the source text.Instead, every sentence in the source text which ex-presses the main goal will have been identified, andthe system's performance is evaluated against thatclassification.Of course, having someone annotate text in thisway may still lead to a biased or careless annotation.We therefore needed an annotation scheme which issimple enough to be usable in a stable and intu-itive way for several annotators.
This paper alsoreports on how we tested the stability of the anno-tation scheme we developed.
A second design crite-rion for our annotation scheme was that we wantedthe roles to be annotated automatically.
This paperreports on preliminary results which show that theannotation process can indeed be automated.To summarise, we have argued that discoursestructure information will improve summarisation.Other researchers (Ono et al, 1994; Marcu, 1997)have argued similarly, although most previous workon discourse-based summarisation follows a differentdiscourse model, namely Rhetorical Structure The-ory (Mann and Thompson, 1987).
In contrast toRST, we stress the importance of rhetorical moveswhich are global to the argumentation of the paper,as opposed to more local RST-type relations.
Ourcategories are not hierarchical, and they are muchless fine-grained than RST-relations.
As mentionedabove, we wanted them to a) provide context in-formation for flexible summarisation, b) provide ahigher degree of comparability between papers, andc) provide a fairer evaluation of superficially differ-ent sentences.In the rest of this paper, we will first describe howwe chose the categories (section 2).
Second, we hadto construct training and evaluation material suchthat we could be sure that the proposed categorisa-tion yielded a reliable resource of annotated text totrain a system against, a gold standard.
The humanannotation experiments are reported in section 3.Finally, in section 4, we describe some of the auto-mated annotation work which we have started re-cently and which uses a corpus annotated accordingto our scheme as its training material.2 The  annotat ion  schemeThe domain in which we work is that of scientifc re-search articles, in particular computational linguis-tics articles.
We settled on this domain for a num-ber of reasons.
One reason is that it is a domainwe are familiar with, which helps for intermediateevaluation of the annotation work.
The other rea-son is that computational linguistics is also a ratherheterogeneous domain: the papers in our collectioncover a wide range of subject matters, such as logicprogramming, statistical language modelling, theo-reticai semantics and computational psycholinguis-tics.
This makes it a challenging test bed for our85BASICSCHEMEBACKGROUND Sentences describing some (generally accepted) backgroundknowledgeOTHER Sentences describing aspects of some specific other research in aneutral way (excluding contrastive or BASIS statements)OWN Sentences describing any aspect of the own work presented inthis paper - except what is covered by AIM or TEXTUAL, e.g.details of solution (methodology), limitations, and further work.AIM Sentences best portraying the particular (main) research goal ofthe articleTEXTUAL Explicit statements about the textual section structure of thepaperCONTRAST Sentences contrasting own work to other work; sentences point-ing out weaknesses in other research; sentences stating that theresearch task of the current paper has never been done before;direct comparisonsBASIS Statements hat the own work uses some other work as its basisor starting point, or gets support from this other workFigure 1: Overview of the a~notation schemeFULLSCHEMEscheme which we hope to be applicable in a range ofdisciplines.Despite its heterogeneity, our collection of papersdoes exhibit predictable rhetorical patterns of sci-entific argumentation.
To analyse these patternswe used STales' (1990) CARS (Creating a Researchspace) model as our starting point.The annotation scheme we designed is sum-marised in Figure 1.
The seven categories describeargumentative roles with respect o the overall com-municative act of the paper.
They are to be read asmutually exclusive labels, one of which is attributedto each sentence in a text.
There are two kinds ofcategories in this scheme: basic categories and non-basic categories.
Basic categories are defined by at-tribution of intellectual ownership; they distinguishbetween:?
statements which are presented as generally ac-cepted (BACKGROUND);?
statements which are attributed to other, spe-cific pieces of research outside the given pa-per, including the authors' own previous work(OTHER);?
statements which describe the authors' own newcontributions (OWN).The four additional (non-basic) categories aremore directly based on STales' theory.
The mostimportant of these is AIM, as this move on itsown is already a good characterisation of the en-tire paper, and thus very useful for the generationof abstracts.
The other categories are TEXTUAL,which provides information about section structurethat might prove helpful for subsequent search steps.There are two moves having to do with the author'sattitude towards previous research, namely BASISand CONTRAST.
We expect his kind of informationto be useful for the creation of typed links for biblio-metric search tools and for the automatic determi-nation of rival approaches in the field and intellec-tual ancestry of methodologies (cf.
Garfield's (1979)classification of the function of citation within re-searchers' papers).The structure in Figure 2, for example, displaysa common rhetorical pattern of scientific argumen-tation which we found in many introductions.
ABACKGROUND segment, in which the history and theimportance of the task is discussed, is followed by alonger sequence of OTHER sentences, in which spe-cific prior work is described in a neutral way.
Thisdiscussion usually terminates in a criticism of theprior work, thus giving a motivation for the ownwork presented in the paper.
The next sentence typ-ically states the specific goal or contribution of thepaper, often in a formulaic way (Myers, 1992).Such regularities, where the segments are contigu-ous, non-overlapping and non-hierarchical, can be86BACKGROUNDOTHERi Recently, new methods of.. I<REFE.RENCE><REFERENCE>fCOAIMFigure 2: Typical rhetorical pattern in a researchpaper introductionexpressed well with our category labels.
Whereasnon-basic ategories are typically short segments ofone or two sentences, the basic categories form muchlarger segments ofsentences with the same rhetoricalrole.3 Human Annotat ion3.1 Annotating full textsTo ensure that our coding scheme leads to less bi-ased annotation than some of the other resourcesavailable for building summarisation systems, and toensure that other researchers besides ourselves canuse it to replicate our results on different ypes oftexts, we wanted to examine two properties of ourscheme: stability and reproducibility (Krippendorff,1980).
Stability is the extent o which an annota-tor will produce the same classifications at differenttimes.
Reproducibility is the extent o which differ-ent annotators will produce the same classification.We use the Kappa coefficient (Siegel and Castellan,1988) to measure stability and reproducibility.
Therationale for using Kappa is explained in (Carletta,1996).The studies used to evaluate stability and repro-ducibility we describe in more detail in (Teufel etal., To Appear).
In brief, 48 papers were annotatedby three extensively trained annotators.
The train-ing period was four weeks consisting of 5 hours ofannotation per week.
There were written instruc-tions (guidelines) of 17 pages.
Skim-reading and87annotation of an average length (3800 word) pa-per typically took 20-30 minutes.
The studies howthat the training material is reliable.
In particu-lar, the basic annotation scheme is stable (K=.82,.81, .76; N=1220; k=2 for all three annotators) andreproducible (K=.71, N=4261, k=3), where k de-notes the number of annotators, N the number ofsentences annotated, and K gives the Kappa value.The full annotation scheme is stable (K=.83, .79,.81; N=1248; k -2  for all three annotators) and re-producible (K=.78, N=4031, k=3).
Overall, repro-ducibility and stability for trained annotators doesnot quite reach the levels found for, for instance,the best dialogue act coding schemes, which typi-cally reach Kappa values of around K=.80 (Carlettaet al, 1997; Jurafsky et al, 1997).
Our annotationrequires more subjective judgements and is possiblymore cognitively complex.
Our reproducibility andstability results are in the range which Krippendorff(1980) describes as giving marginally significant re-sults for reasonable size data sets when correlatingtwo coded variables which would show a clear cor-relation if there were perfect agreement.
As our re-quirements are less stringent han Krippendorff's,we find the level of agreement which we achievedacceptable.OWNOTHERBACKGROUNDCONTRASTAIMBASISTEXTUAL69.4%15.8%5.7%4.4%2.4%1.4%0.9%Figure 3: Distribution of categories0.80.70.60.5K 0.40.3O.20.10 CONTRAST AIM BASIS TEXTUALFigure 4: Reproducibility diagnostics: non-basiccategoriesFigure 3, which gives the overall distribution ofcategories, hows that OWN is by far the most fre-quent category.
Figure 4 reports how well the fournon-basic ategories could be distinguished from allother categories, measured by Krippendorff's diag-nostics for category distinctions (i.e.
collapsing allother distinctions).
When compared to the over-all reproducibility of .71, we notice that the anno-tators were good at distinguishing AIM and TEX-TUAL, and less good at determining BASIS and CON-TRAST.
This might have to do with the location ofthose types of sentences in the paper: AIM and TEX-TUAL are usually found at the beginning or end ofthe introduction section, whereas CONTRAST, andeven more so BASIS, are usually interspersed withinlonger stretches of OWN.
As a result, these cate-gories are more exposed to lapses of attention duringannotation.The fact that the annotators are good at deter-mining AIM sentences i  an important result: as AIMsentences constitute the best characterisation f theresearch paper for the summarisation task at a veryhigh compression to 1.8% of the original text length,we are particularly interested in having them anno-tated consistently in our training material.
This re-sult is clearly in contrast o studies which concludethat humans are not very reliable at this kind of task(Rath et al, 1961).
We attribute this difference to adifference in our instructions.
Whereas the subjectsin Rath et al's experiment were asked to look forthe most relevant sentences, our annotators had tolook for specific argumentative roles which seems tohave eased the task.
In addition, our guidelines givevery specific instructions for ambiguous cases.These reproducibility values are important be-cause they can act as a good evaluation measure asit factors random agreement out, unlike percentageagreement.
It also provides a realistic upper boundon performance: if the machine is treated as anothercoder, and if reproducibity does not decrease thenthe machine has reached the theoretically best re-sult, considering the cognitive difficulty of the task.3.2 Annotating parts of textsAnnotating texts with our scheme is time-consuming, so we wanted to determine if there was amore efficient way of obtaining hand-coded trainingmaterial, namely by annotating only parts of thesource texts.
For example, the abstract, introduc-tions and conclusions of source texts are often like"condensed" versions of the contents of the entire pa-per and might be good areas to restrict annotationto.
Alternatively, it might be a good idea to restrictannotation to the first 20% or the last 10% of anygiven text.
Yet another possibility for restricting therange of sentences to be annotated is based on the'alignment' idea introduced in (Kupiec et al, 1995):a simple surface measure determines sentences in thedocument that are maximally similar to sentences inthe abstract.Obviously, any of these strategies of area restric-tion would give us fewer gold standard sentences perpaper, so we would have to make sure that we stillhad enough candidate sentences for all seven cate-gories.
On the other hand, because these areas couldwell be the most clearly written and informationallyrich sections, it might be the case that the qual-ity of the resulting gold standard is higher.
In thiscase we would expect he reliability of the coding inthese areas to be higher in comparison to the reli-ability achieved overall, which in turn would resultin higher accuracy when this task is done automat-ically.
'I0.80.6K0.402.0Figure 5: Reproducibility by annotated area100%50% -Figure 6: Label distribution by annotated areaWe did extensive xperiments on this.
Figure 5shows reliability values for each of the annotatedportions of text, and Figure 6 shows the composi-88tion in terms of our labels for each of the annotatedportions of text.
The implications for corpus prepa-ration for abstract generation experiments can besummarised as follows.
If one wants to avoid manu-ally annotating entire papers but still make all argu-mentative distinctions, one can restrict he annota-tion to sentences appearing in the introduction sec-tion, even though annotators will find them slightlyharder to classify (K=.69), or to all alignable ab-stract sentences, even if there are not many alignableabstract sentences detectable overall (around 50% ofthe sentences in the abstract), or to conclusion sen-tences, even if the coverage of argumentative cate-gories is very restricted in the conclusions (mostlyAIM and OWN sentences).We also examined a fall-back option of just anno-tating the first 10% or last 5% of a paper (as not allpapers in our collection have an explicitly markedintroduction and conclusion section), but the relia-bility results of this were far less good (K=.66 andK=.63, respectively).4 Automat ic  annotat ionAll the annotation work is obviously in aid of de-velopment work, in particular for the training of asystem.
We will provide a brief description of train-ing results o as to show the practical viability of theproposed corpus preparation method.4.1 DataOur training material is a collection of 80 con-ference papers and their summaries, taken fromthe Computation and Language E-Print Archive(ht tp : / /xxx .
lan l .
gov/cmp-lg/).
The trainingmaterial contains 330,000 word tokens.The data is automatically preprocessed into xmlformat, and the following structural information ismarked up: title, summary, headings, paragraphstructure and sentences, citations in running text,and reference list at the end of the paper.
If oneof the paper's authors also appears on the authorlist of a cited paper, then that citation is markedas self citation.
Tables, equations, figures, captions,cross references are removed and replaced by placeholders.
Sentence boundaries are automatically de-tected, and the text is POS-tagged according to theUPenn tagset.Annotation of rhetorical roles for all 80 papers(around 12,000 sentences) was provided by one ofour human judges during the annotation study men-tioned above.4.2 The  method(Kupiec et al, 1995) use supervised learning to au-tomatically adjust feature weights.
Each documentsentence receives cores for each of the features, re-suiting in an estimate for the sentence's probabilityto also occur in the summary.
This probability iscalculated for each feature value as a combinationof the probability of the feature-value pair occurringin a sentence which is in the summary (successfulcase) and the probability that the feature-value pairoccurs unconditionally.We extend Kupiec et al's estimation of the proba-bility that a sentence is contained in the abstract, tothe probability that it has rhetorical role R (cf.
Fig-ure 7).P(seRIF'" '"Fk)~ 1-i~= p?Fj )whereP(s e RIF1,..P(s ~.
R):P(Fjl s e R):P():k:D:., Fk): Probability that sentence sin the source text has rhetoricalrole R, given its feature values;relative frequency of role R (con-stant);probability of feature-value pairoccurring in a sentence which is inrhetorical class R;probability that the feature-valuepair occurs unconditionally;number of feature-value pairs;j-th feature-value pair.Figure 7: Naive Bayesian classifierEvaluation of the method relies on cross-validation: the model is trained on a training setof documents, leaving one document out at a time(the test document).
The model is then used to as-sign each sentence a probability for each categoryR, and the category with the highest probability ischosen as answer for the sentence.4.3 FeaturesThe features we use in training (see Figure 8) aredifferent from Kupiecet al's because we do not es-timate overall importance in one step, but insteadguess argumentative status first and determine im-portance later.Many of our features can be read off directly fromthe way the corpus is encoded: our preprocessorsdetermine sentence-boundaries and parse the refer-ence list at the end.
This gives us a good handleon structural and locational features, as well as onfeatures related to citations.89Type of featureExplicit structureRelative locationCitationsSyntactic featuresSemantic featuresContent FeaturesNameStruct-1Struct-2Struct-3Cit-1Cit-2Syn-1Syn-2Syn-3Syn-4Sere-1Sem-2Sem-3Cont-1Cont-2Feature  descr ipt ionType of Headline of current sectionRelative position of sentence withinparagraphRelative position of sentence withinsectionPaper is segmented into 10 equally-sized segmentsDoes the sentence contain a citation orthe name of an author contained in thereference list?Does the sentence contain a self cita-tion?Tense (associated with first finite verbin sentence)Modal AuxiliariesNegationAction type of first verb in sentenceType of AgentType of formulaic expression occurringin sentenceDoes the sentence contain keywords asdetermined by the tf/idf measure?Does the sentence contain words alsooccurring in the title or headlines?Feature values8 prototypical headlines or 'non-p.rototypical'initial, medial, finalfirst, second or last third1-10Full Citation, Author Name or""NoneYes or NoPresent, Past, Present Perfect,'Past Perfect, Future or Nothing_Present or NotActive or PassivePresent or Not20 different Action Types(cf.
Figure 9) or NothingAuthors or Others or Nothing18 different ypes of FormulaicExpressions (cf.
Figure 9) orNothingYes or NoYes or NoFigure 8: Features for supervised learningmmmmThe syntactic features rely on determining thefirst finite verb in the sentence, which is done sym-bolically using POS-information.
Heuristics are usedto determine the tense and possible negation.The semantic features rely on template matching.In the feature Sem-1, a hand-crafted lexicon is usedto classify the verb into one of 20 Action Classes(cf.
Figure 9, left half), if it is one of the 388 verbscontained in the lexicon.
The feature Sem-2 encodeswhether the agent of the action is most likely to re-fer to the authors, or to other agents, e.g.
otherresearchers (177 templates).
Heuristic rules deter-mine that the agent is the subject in an active sen-tence, or the head of the by-phrase (if present) in apassive sentence.
Sere-3 encodes various other for-mulaic expressions (indicator phrases (Paice, 1981),meta-comments (Zukerman, 1991)) in order to ex-ploit explicit rhetoric phrases the authors might haveused, cf.
Figure 9, right half (414 templates).The content features use the tf/ idf method andtitle and header information for finding contentfulwords or phrases.
In contrast o all other featuresthey do not attempt o model the form or meta-discourse contained in the sentences but insteadmodel their domain (object-level) contents.4.4 Resu l tsWhen the Naive Bayesian Model is added to thepool of coders, the reproducibility drops from K=.71to K=.55.
This reproducibility value is equivalentto the value achieved by 6 human annotators withno prior training, as found in an earlier experiment(Teufel et al, To Appear).
Compared to one of theannotators, Kappa is K=.37, which corresponds topercentage accuracy of 71.2%.
This number cannotbe directly compared to experiments like Kupiec etal.
's because in their experiment a compression ofaround 3% was achieved whereas we classify eachsentence into one of the categories.Further analysis of our results shows the systemperforms well on the frequent category OWN, cf.
theconfusion matrix in Fig.
reftab:confusion.
Indeed,as Figure 3 shows, OWN is so frequent hat choos-ing OWN all the time gives us a seemingly hard-to-beat baseline with a high percentage agreementof 69% (Baseline 1).
However, the Kappa statistic,which controls for expected random agreement, re-veals just how bad that baseline really is: Kappais K=-.12 (machine vs. one annotator).
Randomchoice of categories according to the distribution ofcategories (Baseline 2) is a better baseline; Kappa90Action Types Formulaic Expression TypesAFFECTARGUMENTATIONAWARENESSBETTER,SOLUTIONCHANGECOMPARISONCONTINUATIONCONTRASTFUTURE.INTERESTINTERESTNEEDPRESENTATIONPROBLEMRESEARCHSIMILARSOLUTIONTEXTSTRUCTUREUSECOPULAPOSSESSIONwe hop._.._~e to improve these resultswe argue against an application ofwe know of no other attempts...our system outperforms that of ...we extend < CITE/> 's algorithmwe tested_ our system against...we follow X in postulating thatour approach differs from X's .
.
.we inten..d to improve our results...we are concerned with ...this approach, however, lacks...we present here a method for...th i~-~ses the problem of how to...we collected our data from...our approach resembles that of X...we solve this problem by...the paper is organized as follows...we employ X's method...our goal i...ss to...our approach has three advan-tages...GENERAL-AGENTSPECIFIC-AGENTGAP-INTRODUCTIONAIMTEXTSTRUCTUREDEIXISCONTINUATIONSIMILARITYCOMPARISONCONTRASTMETHODPREVIOUS_CONTEXTFUTUREAFFECTPROBLEMSOLUTIONPOSITIVE.ADJECTIVENEGATIVE.
ADJECT IVElinguistsaccording to < REF'~to our knowledgemain contribution of thisin section < CREF/>in this paperfollowing the argument inbears similarity towhen compared to ourhowevera novel method for XX-ingelsewhere, we haveavenue for improvementhopefullydrawbackinsightappealingunsatisfactoryFigure 9: Types of actions and formulaic expressionsHUMANAIMCONTRASTTEXTUALOWNBACKGROUNDBASISOTHERTotalAIM11511137511107242MACHINECONTRAST TEXTUAL4 I079 54 11561 6120 310 535 10213 209.i ~-, ..OWN BACKGROUND BASIS OTHER Total46 15 13 4 207280 92 40 89 59671 5 3 12 2237666 168 125 279 8435286 295 21 84 72040 4 102 55 2261120 203 173 466 20149509 782 477 989 12421Figure 10: Confusion matrix: human vs. automatic annotationfor this baseline is K=0.AIM categories can be determined with a preci-sion of 48% and a recall of 56% (cf.
Figure 11).These values are more directly comparable to Ku-piec et al's results of 44% co-selection of extractedsentences with alignable summary sentences.
Weassume that most of the sentences extracted bytheir method would have fallen into the AIM cate-gory.
The other easily determinable category for theautomatic method is TEXTUAL (p----55%; r=52%),whereas the results for the other non-basic ategoriesare relatively lower - mirroring the results for hu-mans.As far as the individual features are concerned, wefound the strongest heuristics to be location, type ofheader, citations, and the semantic lasses (indicatorphrases, agents and actions); syntactic and content-based heuristics are the weakest.
The first columnin Figure 12 gives the predictiveness of the featureAIMCONTRASTTEXTUALOwNBACKGROUNDBASISOTHERPrec is ion Recall48% 56%37% 13%55% 52%81% 91%38% 41%21% 45%47% 23%Figure 11: Precision and recall per categoryon its own, in terms of kappa between machine andone annotator.
Some of the weaker features are notpredictive nough on their own to break the domi-nance of the prior; in that case, they behave just likeBaseline 1 (K=-.12).The second column gives kappa for experimentsusing all features except the given feature, i.e.
theresults if this feature is left out of the pool of fea-91Feature Code Alone Left outStruct-1 -.12 .37Struct-2 -.12 .36Struct-3 .16 .36Struct-l-3 .18 .34I L?c I.. .171 .34Cit-1 .18 .37Cit-2 .13 .37Cit - l -2  .18 -36Syn-1 -.12 .37Syn-2 -.12 .37Syn-3 -.12 .37Syn-4 -.12 .37Syn-l-4 .
.
.
.
-.12 .37Sere-1 -.12 ".36Sere-2 .07 .35Sere-3 -.03 .36Sere-l-3 " .13 .31Cost-1 -.12 I .37Cont-2 -.12 .37Cont-lS2 -.12 .37Baseline 1 (all OWN): K=-.12Baseline 2 (random by distr.
): K=0Figure 12: Disambiguation potential of individualheuristicstures.
These numbers how that some of the weakerfeatures contribute some predictive power in combi-nation with others.While not entirely satisfactory, these results mightbe taken as an indication that we have indeed man-aged to identify the right kinds of features for argu-mentative sentence classification.
Taking the con-text into account should further increase results,as preliminary experiments with n-gram modellinghave shown.
In these experiments, we replaced theprior P(s E R) in Figure 7 with a n-gram basedprobability of that role occurring in the given con-text.5 ConclusionsIn this paper we have presented an annotationscheme for corpus based summarisation.
In tests,we have found this annotation scheme to be stableand reproducible.
On the basis of this scheme, wehave created a new kind of resource for training sum-marisation systems: a corpus annotated with labelswhich indicate the argumentative role of each sen-tence in the text.
Results of our training work showthat the annotation work can be automated.ReferencesJean Carletta, Amy Isard, Stephen Isard, Jacqueline C.Kowtko, Gwyneth Doherty-Sneddon, and Anne H.Anderson.
1997.
The reliability of a dialoguestructure coding scheme.
Computatiorml Linguistics,23(1):13-31.Jean Carletta.
1996.
Assessing agreement on classifica-tion tasks: the kappa statistic.
Computational Lin-guistics, 22(2):249-254.H.
P. Edmundson.
1969.
New methods in automaticextracting.
Journal of the Association for ComputingMachinery, 16(2):264-285.E.
Garfield.
1979.
Citation indezing: its theory and ap-plication in science, thechnology and humanities.
Wi-ley, New York.Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-asca, 1997.
Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual.
Uni-versity of Colorado, Institute of Cognitive Science.TR-97-02.Klaus Krippendorff.
1980.
Content analysis: an intro-duction to its methodology.
Sage Commtext series; 5.Sage, Beverly Hills London.Julian Kupiec, Jan O. Pedersen, and Francine Chen.1995.
A trainable document summarizer.
In Pro-ceedings of the 18th ACM-SIGIR Conference, pages68-73.Elizabeth DuRoss Liddy.
1991.
The discourse-levelstructure of empirical abstracts: an exploratory study.Information Processing and Management, 27(1):55-81.H.
P. Luhn.
1958.
The automatic reation of literatureabstracts.
IBM Journal of Research and Development,2(2):159-165.Inderjeet Mani and Eric Bloedorn.
1998.
Machine learn-ing of generic and user-focused summarization.
InProceedings of the Fifteenth National Conference onAI (AAAI-98), pages 821-826.William C. Mann and Sandra A. Thompson.
1987.Rhetorical structure theory: description and construc-tion of text structures.
In G. Kempen, editor, NaturalLangua9 e Generation: New Results in Artificial In-telligence, Psychology and Linguistics, pages 85-95,Dordrecht.
Nijhoff.Daniel Marcu.
1997.
From discourse structures to textsummaries.
In Proceedings of the ACL/EACL Work-shop on Intelligent Scalable Text Summarization.Greg Myers.
1992.
In this paper we report... - speechacts and scientific facts.
Journal of Pragmatics,17(4):295-313.Kenji Ono, Ka~uo Sumita, and Seijii Miike.
1994.
Ab-stract generation based on rhetorical structure xtrac-tion.
In Proceedings of the 15th International confer-ence on Computational Linguistics (COLING-94).92Chris D. Paice.
1981.
The automatic generation of lit-erary abstracts: an approach based on the identifi-cation of self-indicating phrases.
In Robert NormanOddy, S. E. Robertson, C. J. van Rijsbergen, andP.
W. Williams, editors, Information Retrieval Re-search, pages 172-191.
Butterworth, London.G.J Path, A. Resnick, and T. R. Savage.
1961.
Theformation of abstracts by the selection of sentences.American Documentation, 12(2):139-143.Sidney Siegel and N.J. Jr. Castellan.
1988.
Nonparamet-tic statistics for the Behavioral Sciences.
McGraw-Hill, second edition.Karen Sp~rck Jones.
1998.
Automatic summarising:factors and directions.
In AAAI Spring Symposiumon Intelligent Text Summarization.John Swales.
1990.
Genre analysis: English in academicand research settings.
Cambridge University Press.Simone Teufel, Jean Carletta, and Marc Moens.
To Ap-pear.
An annotation scheme for discourse-level argu-mentation in research articles.
In Proceedings of theNinth Conference of the European Chapter of the As-sociation for Computational Linguistics (EA CL- 99).Ingrid Zukerman.
1991.
Using meta-comments to gener-ate fluent text in a technical domain.
ComputationalIntelligence: Special Issue on Natural Language Gen-eration, 7(4):276.93
