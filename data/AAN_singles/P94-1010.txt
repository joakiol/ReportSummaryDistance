A STOCHASTIC FINITE-STATE WORD-SEGMENTATION ALGORITHMFOR CHINESERichard SproatChilin ShihWilliam GaleAT&T Bell Laboratories600 Mountain Avenue,Room {2d-451,2d-453,2c-278}Murray Hill, NJ, USA, 07974-0636{rws, cls, gale}@research, att.
comNancy ChangHarvard UniversityDivision of Applied SciencesHarvard UniversityCambridge, MA 02138nchang@das, harvard, eduAbstractWe present astochastic finite-state model for segment-ing Chinese text into dictionary entries and produc-tively derived words, and providing pronunciations forthese words; the method incorporates a class-basedmodel in its treatment of personal names.
We alsoevaluate the system's performance, taking into accountthe fact that people often do not agree on a single seg-mentation.THE PROBLEMThe initial step of any text analysis task is the tok-enization of the input into words.
For many writingsystems, using whitespace as a delimiter for wordsyields reasonable results.
However, for Chinese andother systems where whitespace is not used to delimitwords, such trivial schemes will not work.
Chinesewriting is morphosyllabic (DeFrancis, 1984), meaningthat each hanzi- 'Chinese character' - (nearly always)represents a single syllable that is (usually) also a sin-gle morpheme.
Since in Chinese, as in English, wordsmay be polysyllabic, and since hanzi are written withno intervening spaces, it is not trivial to reconstructwhich hanzi to group into words.While for some applications it may be possibleto bypass the word-segmentation problem and workstraight from hanzi, there are several reasons why thisapproach will not work in a text-to-speech (TI'S) sys-tem for Mandarin Chinese - -  the primary intendedapplication of our segmenter.
These reasons include:1.
Many hanzi are homographs whose pronunciationdepends upon word affiliation.
So, ~ is pronounceddeO ~ when it is a prenominal modification marker,but di4 in the word \[\] ~ mu4di4 'goal'; ~ is nor-mally ganl 'dry',but qian2 in a person's given name.2.
Some phonological rules depend upon correct word-segmentation, i cluding Third Tone Sandhi (Shih,1986), which changes a 3 tone into a 2 tone be-fore another 3 tone: , J~ \ ]~ xiao3 \[lao3 shu3\] 'lit-t We use pinyin transliteration with numbers epresentingtones.66tie rat', becomes xiao3 \[ lao2-shu3 \], rather thanxiao2 \[ lao2-shu3 \], because the rule first applieswithin the word lao3-shu3, blocking its phrasal ap-plication.While a minimal requirement for building a Chi-nese word-segmenter is a dictionary, a dictionary is in-sufficient since there are several classes of words thatare not generally found in dictionaries.
Among these:I. Morphologically Derived Words: PJ~l~f{l xiao3-jiang4-menO (little general-plural) ' ittle generals'.2.
Personal Names: ~,~ zhoul enl-lai2 'ZhouEnlai'.3.
Transliterated Foreign Names: ~i~: : , ,~  bu4-lang 3-shi4-wei2-ke4 'Brunswick'.We present astochastic finite-state model for seg-menting Chinese text into dictionary entries and wordsderived via the above-mentioned productive processes;as part of the treatment of personal names, we dis-cuss a class-based model which uses the Good-Turingmethod to estimate costs of previously unseen personalnames.
The segmenter handles the grouping of hanziinto words and outputs word pronunciations, with de-fault pronunciations for hanzi t cannot group; we focushere primarily on the system's ability to segment textappropriately (rather than on its pronunciation abili-ties).
We evaluate various specific aspects of the seg-mentation, and provide an evaluation of the overallsegmentation performance: this latter evaluation com-pares the performance ofthe system with that of severalhuman judges, since even people do not agree on a sin-gle correct way to segment a text,PREVIOUS WORKThere is a sizable literature on Chinese word segmenta-tion: recent reviews include (Wang et al, 1990; Wu andTseng, 1993).
Roughly, previous work can be classi-fied into purely statistical pproaches (Sproat and Shih,1990), statistical approaches which incorporate l xicalknowledge (Fan and Tsai, 1988; Lin et al, 1993), andapproaches that include lexical knowledge combinedwith heuristics (Chen and Liu, 1992).Chert and Liu's (1992) algorithm atches words ofan input sentence against a dictionary; in cases wherevarious parses are possible, aset of heuristics i appliedto disambiguate he analyses.
Various morphologicalrules are then applied to allow for morphologicallycomplex words that are not in the dictionary.
Preci-sion and recall rates of over 99% are reported, but notethat this covers only words that are in the dictionary:"the.
.
.
statistics do not count he mistakes \[that occur\]due to the existence of derived words or proper names"(Chen and Liu, 1992, page 105).
Lin et al (1993) de-scribe a sophisticated model that includes a dictionaryand a morphological nalyzer.
They also present agen-eral statistical model for detecting 'unknown words'based on hanzi and part-of-speech sequences.
How-ever, their unknown word model has the disadvantagethat it does not identify a sequence of hanzi as an un-known word of a particular category, but merely as anunknown word (of indeterminate category).
For an ap-plication like TTS, however, it is necessary toknow thata particular sequence ofhanzi is of a particular categorybecause, for example, that knowledge could affect hepronunciation.
We therefore prefer to build particularmodels for different classes of unknown words, ratherthan building a single general model.D ICT IONARY REPRESENTATIONThe lexicon of basic words and stems is represented as aweightedfinite-state randucer (WFST) (Pereira et al,1994).
Most transitions represent mappings betweenhanzi and pronunciations, and are costless.
Transitionsbetween orthographic words and their parts-of-speechare represented by e-to-category transductions and aunigram cost (negative log probability) of that wordestimated from a 20M hanzi training corpus; a portionof the WFST is given in Figure 1.
2 Besides dictionarywords, the lexicon contains all hanzi n the Big 5 Chi-nese code, with their pronunciation(s), plus entries forother characters (e.g., roman letters, numerals, specialsymbols).Given this dictionary representation, recognizing asingle Chinese word involves representing the input asa finite-state acceptor (FSA) where each arc is labeledwith a single hanzi of the input.
The left-restrictionof the dictionary WFST with the input FSA containsall and only the (single) lexical entries correspond-ing to the input.
This WFST includes the word costson arcs transducing c to category labels.
Now, input2The costs are actually for strings rather than words: wecurrently lack estimates for the words themselves.
We assignthe string cost o lexical entries with the likeliest pronuncia-tion, and a large cost to all other entries.
Thus ~j~/adv, withthe commonest pronunciafionjiangl has cost 5.98, whereas~/nc, with the rarer pronunciatJonjiang4, is assigned a highcost.
Note also that he current model is zeroeth order in thatit uses only unigram costs.
Higher order models, e.g.
bigramword models, could easily be incorporated into the presentarchitecture if desired.sentences consist of one or more entries from the dic-tionary, and we can generalize the word recognitionproblem to the word segmentation problem, by left-restricting the transitive closure of the dictionary withthe input.
The result of this left-restriction is an WFSTthat gives all and only the possible analyses of the in-put FSA into dictionary entries.
In general we do notwant all possible analyses but rather the best analysis.This is obtained by computing the least-cost path in theoutput WFST.
The final stage of segmentation involvestraversing the best path, collecting into words all se-quences of hanzi delimited by part-of-speech-labeledarcs.
Figure 2 shows an example of segmentation: thesentence \[\] 5~,~-~ "How do you say octopusin Japanese?
", consists of four words, namely \[\]ri4-wen2 'Japanese', ~ ,  zhangl-yu2 'octopus', ,~zen3-mo 'how', and -~ shuol 'say'.
In this case,\[\] ri4 is also a word (e.g.
a common abbreviation forJapan) as are 3~ wen2-zhangl 'essay', and ~, yu2'fish', so there is (at least) one alternate analysis to beconsidered.MORPHOLOGICAL  ANALYS ISThe method just described segments dictionary words,but as noted there are several classes of words thatshould be handled that are not in the dictionary.
Oneclass comprises words derived by productive morpho-logical processes, such as plural noun formation us-ing the suffix ~I menO.
The morphological anal-ysis itself can be handled using well-known tech-niques from finite-state morphology (Koskenniemi,1983; Antworth, 1990; Tzoukermann and Liberman,1990; Karttunen et al, 1992; Sproat, 1992); so, werepresent the fact that ~ attaches to nouns by allowingc-transitions from the final states of all noun entries,to the initial state of the sub-WFST representing ~I.However, for our purposes it is not sufficient o rep-resent he morphological decomposition of, say, plu-ral nouns: we also need an estimate of the cost ofthe resulting word.
For derived words that occur inour corpus we can estimate these costs as we wouldthe costs for an underived ictionary entry.
So, ~ Ijiang4-menO '(military) generals' occurs and we esti-mate its cost at 15.02.
But we also need an estimateof the probability for a non-occurring though possi-ble plural form like 15/)~I nan2-gual-menO 'pump-kins'.
Here we use the Good-Turing estimate (Baayen,1989; Church and Gale, 1991), whereby the aggre-gate probability of previously unseen members of aconstruction is estimated as NI/N, where N is thetotal number of observed tokens and N1 is the num-ber of types observed only once.
For r~l this givesprob(unseen(f~) I f~l), and to get the aggregate prob-ability of novel ~l-constructions i  a corpus we multi-ply this by prob,e~,(?
{~) to get probte~t(unseen(f~)).Finally, to estimate the probability of particular unseenword i~1/1 ~I, we use the simple bigram backoff modelprob(~)lI ~ ) - prob(~i )lI )p~ob,,~, (unsee,~(M));67: jlengl : 0.0= :.==..o.o ~~; :oo  IL~ : mini : o:o c.: 1() @' II"~.
: guo2 : 0.0(Repubilc of Chr, a)Figure 1: Partial chinese Lexicon (NC= noun; NP = proper noun)iESSAY FISHI~ :_nc ~ :wen2 \]~- :zhangl I~ :_nc ,,~,.
:yu2JAPAN\[\] " r i4  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .... .
.
.
.
....}...~"~.~;%" %.
..... *o";.2; "%./ 10.28 i e : nc : JAPANESE OCTOPUS , - HOW SAYi \[\] :ri4 ~ :wen2 g :_nc -~-:zhangl ,~!
.~/u2 E: nc \[ ~ :zen3 ~ :moO E:_adv -~:shuol g:_vbiI10.63 13.18 7.96 5.55Figure 2: Input lattice (top) and two segmentations (bottom) of the sentence 'How do you say octopus in Japanese'.
Anon-optimal analysis is shown with dotted lines in the bottom frame.68~i~ : pare91 : 0.0~ ):/klcql4 : GOE : JP, DV: SAt,:mln l :nn  I: i l l  :GO~ : Jq?
:  40.0IE :e  :GOlaJ : ml~:  GO: .
.NC : 4A1: ki lt .
."
10J I lFigure 3: An example of affixation: the plural affixcost (~r \ ] )  is computed in the obvious way.
Fig-ure 3 shows how this model is implemented as part ofthe dictionary WFST.
There is a (costless) transitionbetween the NC node and ~\].
The transition from~\] to a final state transduces c to the grammatical tag\PL with cost costte~t(unseen(~\])): cost(l~}~f~) = cost(~)~) + costt,~t(unseen(\[\])), as desired.For the seen word ~1 'generals', there is an e:nctransduction from ~ to the node preceding t~\]; thisarc has cost cost (~\ ] )  - costt,~:t(unseen(~\])), sothat the cost of the whole path is the desired cost(~t~\]).
This representation gives ~\ ]  an appropriate mor-phological decomposition, preserving information thatwould be lost by simply listing ~\[~I as an unanalyzedform.
Note that the backoffmodel assumes that there isa positive correlation between the frequency of a singu-lar noun and its plural.
An analysis of nouns that occurboth in the singular and the plural in our database re-veals that there is indeed a slight but significant positivecorrelation - -  R 2 = 0.20, p < 0.005.
This suggeststhat the backoff model is as reasonable a model as wecan use in the absence of further information about heexpected cost of a plural form.CHINESE PERSONAL NAMESFull Chinese personal names are in one respect sim-ple: they are always of the form FAMILY+GIVEN.The FAMILY name set is restricted: there are a fewhundred single-hanzi FAMILY names, and about tendouble-hanzi ones.
Given names are most commonlytwo hanzi long, occasionally one-hanzi long: thereare thus four possible name types.
The difficulty is thatGIVEN names can consist, in principle, of any hanzi orpair ofhanzi, so the possible GIVEN names are limitedonly by the total number of hanzi, though some hanziare certainly far more likely than others.
For a sequenceof hanzi that is a possible name, we wish to assign aprobability to that sequence qua name.
We use an esti-mate derived from (Chang et al, 1992).
For example,given a potential name of the form FI G1 G2, where F1is a legal FAMILY name and G1 and G2 are each hanzi,we estimate the probability of that name as the prod-uct of the probability of finding any name in text; theprobability of F1 as a FAMILY name; the probabilityof the first hanzi of a double GIVEN name being G1;the probability of the second hanzi of a double GIVENname being G2; and the probability of a name of theform SINGLE-FAMILY+DOUBLE-GIVEN.
The firstprobability is estimated from a name count in a textdatabase, whereas the last four probabilities are esti-mated from a large list of personal names) This modelis easily incorporated into the segmenter by building anWFST restricting the names to the four licit types, withcosts on the arcs for any particular name summing toan estimate of the cost of that name.
This WFST is thensummed with the WFST implementing the dictionaryand morphological rules, and the transitive closure ofthe resulting transducer is computed.3We have two such lists, one containing about 17,000 fullnames, and another containing frequencies of hanzi n thevarious name positions, derived from a million names.69There are two weaknesses in Chang et al's (1992)model, which we improve upon.
First, the model as-sumes independence b tween the first and second hanziof a double GIVEN name.
Yet, some hanzi are far moreprobable in women's names than they are in men'snames, and there is a similar list of male-oriented hanzi:mixing hanzi from these two lists is generally less likelythan would be predicted by the independence model.As a partial solution, for pairs ofhanzi that cooccur suf-ficiently often in our namelists, we use the estimatedbigram cost, rather than the independence-based cost.The second weakness is that Chang et al (1992) as-sign a uniform small cost to unseen hanzi in GIVENnames; but we know that some unseen hanzi are merelyaccidentally missing, whereas others are missing for areason - -  e.g., because they have a bad connotation.We can address this problem by first observing thatfor many hanzi, the general 'meaning' is indicated byits so-called 'semantic radical'.
Hanzi that share thesame 'radical', share an easily identifiable structuralcomponent: the plant names 2 ,  -~ and M share theGRASS radical; malady names m\[, ~,, and t~ sharethe SICKNESS radical; and ratlike animal names 1~,\[~, and 1~ share the RAT radical.
Some classes are bet-ter for names than others: in our corpora, many namesare picked from the GRASS class, very few from theSICKNESS class, and none from the RAT class.
Wecan thus better predict the probability of an unseenhanzi occurring in a name by computing a within-classGood-Turing estimate for each radical class.
Assum-ing unseen objects within each class are equiprobable,their probabilities are given by the Good-Turing theo-rem as:p~t, o~ E(N{ u)N , E(N~tS ) (1)where p~t, is the probability of one unseen hanzi inclass cls, E(N{ t') is the expected number of hanziin cls seen once, N is the total number of hanzi, andE(N~ t') is the expected number of unseen hanzi inclass cls.
The use of the Good-Turing equation pre-sumes suitable stimates of the unknown expectationsit requires.
In the denominator, the N~ u are well mea-sured by counting, and we replace the expectation bythe observation.
In the numerator, however, the countsof N{ l' are quite irregular, including several zeros (e.g.RAT, none of whose members were seen), However,there is a strong relationship between N{ t" and thenumber of hanzi in the class.
For E(N~ZS), then, wesubstitute a smooth against he number of class ele-ments.
This smooth guarantees that there are no zeroesestimated.
The final estimating equation is then:S( N~'N;,!
(2) p~U oc N *The total of all these class estimates was about 10% offfrom the Turing estimate Nt/N for the probability ofall unseen hanzi, and we renormalized the estimates sothat they would sum to Nt/N.This class-based model gives reasonable results:for six radical classes, Table 1 gives the estimated costfor an unseen hanzi n the class occurring as the secondhanzi in a double GIVEN name.
Note that the goodclasses JADE, GOLD and GRASS have lower coststhan the bad classes SICKNESS, DEATH and RAT, asdesired.TRANSL ITERAT IONS OF  FOREIGNWORDSForeign names are usually transliterated using hanziwhose sequential pronunciation mimics the source lan-guage pronunciation ofthe name.
Since foreign namescan be of any length, and since their original pronunci-ation is effectively unlimited, the identification of suchnames is tricky.
Fortunately, there are only a few hun-dred hanzi that are particularly common in translitera-tions; indeed, the commonest ones, such as ~ bal, ~Ier3, and PJ al are often clear indicators that a sequenceof hanzi containing them is foreign: even a name like~Y~f  xia4-mi3-er3 'Shamir', which is a legal Chi-nese personal name, retains a foreign flavor becauseof i~J.
As a first step towards modeling transliteratednames, we have collected all hanzi occurring more thanonce in the roughly 750 foreign names in our dictionary,and we estimate the probability of occurrence of eachhanzi n a transliteration (pT~;(hanzii)) using the max-imum likelihood estimate.
As with personal names,we also derive an estimate from text of the probabil-ity of finding a transliterated name of any kind (PTN).Finally, we model the probability of a new transliter-ated name as the product of PTN and pTg(hanzii)for each hanzii in the putative name.
4 The foreignname model is implemented asan WFST, which is thensummed with the WFST implementing the dictionary,morphological rules, and personal names; the transitiveclosure of the resulting machine is then computed.EVALUATIONIn this section we present a partial evaluation of thecurrent system in three parts.
The first is an evaluationof the system's ability to mimic humans at the task ofsegmenting text into word-sized units; the second eval-uates the proper name identification; the third measuresthe performance on morphological nalysis.
To datewe have not done a separate evaluation of foreign namerecognition.Evaluation of the Segmentation as a Whole: Pre-vious reports on Chinese segmentation have invariably4The current model is too simplistic in several respects.For instance, the common 'suffixes', -nia (e.g., Virginia) and-sia are normally transliterated as~=~ ni2-ya3 and ~\]~ n~xil-ya3, respectively.
The interdependence between \]:~ or~,  and ~r~ is not captured by our model, but this could easilybe remedied.70Table I: The cost as a novel GIVEN name (second position) for hanzi from various radical classes.JADE GOLD GRASS SICKNESS DEATH RAT14.98 15.52 15.76 16.25 16.30 16.42cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.
Theproblem with these styles of evaluation is that, as weshall demonstrate, ven human judges do not agreeperfectly on how to segment a given text.
Thus, ratherthan give a single evaluative score, we prefer to com-pare the performance ofour method with the judgmentsof several human subjects.
To this end, we picked 100sentences at random containing 4372 total hanzi froma test corpus.
We asked six native speakers - -  threefrom Taiwan (T1-T3), and three from the Mainland(M1-M3) - -  to segment the corpus.
Since we couldnot bias the subjects towards aparticular segmentationand did not presume linguistic sophistication on theirpart, the instructions were simple: subjects were tomark all places they might plausibly pause if they werereading the text aloud.
An examination ofthe subjects'bracketings confirmed that these instructions were sat-isfactory in yielding plausible word-sized units.Various segmentation approaches were then com-pared with human performance:1.
A greedy algorithm, GR: proceed through the sen-tence, taking the longest match with a dictionaryentry at each point.2.
An 'anti-greedy' algorithm, AG: instead of thelongest match, take the shortest match at each point.3.
The method being described - -  henceforth ST.Two measures that can be used to compare judgmentsare:1.
Precision.
For each pair of judges consider onejudge as the standard, computing the precision ofthe other's judgments relative to this standard.2.
Recall.
For each pair of judges, consider one judgeas the standard, computing the recall of the other'sjudgments relative to this standard.Obviously, for judges J1 and J2, taking ,/1 as stan-dard and computing the precision and recall for J2yields the same results as taking J2 as the standard,and computing for Jr, respectively, the recall and pre-cision.
We therefore used the arithmetic mean of eachinterjudge precision-recall pair as a single measure ofinterjudge similarity.
Table 2 shows these similaritymeasures.
The average agreement among the humanjudges is .76, and the average agreement between STand the humans is .75, or about 99% of the inter-humanagreement.
(GR is .73 or 96%.)
One can better visu-alize the precision-recall similarity matrix by produc-ing from that matrix a distance matrix, computing amultidimensional scaling on that distance matrix, andplotting the first two most significant dimensions.
Theresult of this is shown in Figure 4.
In addition to theautomatic methods, AG, GR and ST, just discussed,we also added to the plot the values for the currentalgorithm using only dictionary entries (i.e., no pro-ductively derived words, or names).
This is to allowfor fair comparison between the statistical method, andGR, which is also purely dictionary-based.
As canbe seen, GR and this 'pared-down' statistical methodperform quite similarly, though the statistical method isstill slightly better.
AG clearly performs much less likehumans than these methods, whereas the full statisti-cal algorithm, including morphological derivatives andnames, performs most closely to humans among theautomatic methods.
It can be also seen clearly in thisplot, two of the Taiwan speakers cluster very closelytogether, and the third Taiwan speaker is also close inthe most significant dimension (the z axis).
Two of theMainlanders also cluster close together but, interest-ingly, not particularly close to the Taiwan speakers; thethird Mainlander is much more similar to the Taiwanspeakers.Personal Name Identification: To evaluate personalname identification, we randomly selected 186 sen-tences containing 12,000 hanzi from our test corpus,and segmented the text automatically, tagging personalnames; note that for names there is always a single un-ambiguous answer, unlike the more general questionof which segmentation is correct.
The performancewas 80.99% recall and 61.83% precision.
Interest-ingly, Chang et al reported 80.67% recall and 91.87%precision on an 11,000 word corpus: seemingly, oursystem finds as many names as their system, but withfour times as many false hits.
However, we have reasonto doubt Chang et al's performance claims.
Withoutusing the same test corpus, direct comparison is ob-viously difficult; fortunately Chang et al included alist of about 60 example sentence fragments that ex-emplified various categories of performance for theirsystem.
The performance of our system on those sen-tences appeared rather better than theirs.
Now, on a setof 11 sentence fragments where they reported 100% re-call and precision for name identification, we had 80%precision and 73% recall.
However, they listed twosets, one consisting of 28 fragments and the other of 22fragments in which they had 0% precision and recall.On the first of these our system had 86% precision and64% recall; on the second it had 19% precision and33% recall.
Note that it is in precision that our over-all performance would appear to be poorer than that ofChang et al, yet based on their published examples, our71Table 2: Similarity matrix for segmentation judgmentsJudges AG GR ST M1 M2 M3 T1 T2 T3AG 0.70 0.70 0.43 0.42 0.60 0.60 0.62 0.59GR 0.99 0.62 0.64 0.79 0.82 0.81 0.72ST 0.64 0.67 0.80 0.84 0.82 0.740.77 M1M2M3T1T20.69 0.71 0.69 0.700.72 0.73 0.71 0.700.89 0.87 0.800.88 0.820.78system appears to be doing better precisionwise.
Thuswe have some confidence that our own performance isat least as good that of(Chang et al, 1992).
sEvaluation of Morphological Analysis: In Table 3we present results from small test corpora for someproductive affixes; as with names, the segmentation fmorphologically derived words is generally either ightor wrong.
The first four affixes are so-called resultativeaffixes: they denote some property of the resultant stateof an verb, as in ~,,:;~ " wang4-bu4-1iao3 (forget-not-attain) 'cannot forget'.
The last affix is the nominalplural.
Note that ~ in ~,:~: \]" is normally pronouncedas leO, but when part of a resultative it is liao3.
In thetable are the (typical) classes of words to which the affixattaches, the number found in the test corpus by themethod, the number correct (with a precision measure),and the number missed (with a recall measure).CONCLUSIONSIn this paper we have shown that good performancecan be achieved on Chinese word segmentation byus-ing probabilistic methods incorporated into a uniformstochastic finite-state model.
We believe that the ap-proach reported here compares favorably with otherreported approaches, though obviously it is impossibleto make meaningful comparisons in the absence of uni-form test databases for Chinese segmentation.
Perhapsthe single most important difference between our workand previous work is the form of the evaluation.
Aswe have observed there is often no single right answerto word segmentation in Chinese.
Therefore, claims tothe effect hat a particular algorithm gets 99% accuracyare meaningless without a clear definition of accuracy.ACKNOWLEDGEMENTSWe thank United Informatics for providing us withour corpus of Chinese text, and BDC for the 'Behav-5We were recently pointed to (Wang et al, 1992), whichwe had unfortunately missed in our previous literature search.We hope to compare our method with that of Wang et al ina future version of this paper.ior Chinese-English Electronic Dictionary'.
We fur-ther thank Dr. J.-S. Chang of Tsinghua University, forkindly providing us with the name corpora.
Finally, wethank two anonymous ACL reviewers for comments.REFERENCESEvan Antworth.
1990.
PC-KIMMO: A Two-Level Pro-cessor for Morphological Analysis.
OccasionalPublications in Academic Computing, 16.
Sum-mer Institute of Linguistics, Dallas, TX.Harald Baayen.
1989.
A Corpus-Based Approach toMorphological Productivity: Statistical Analysisand Psycholinguistic Interpretation.
Ph.D. thesis,Free University, Amsterdam.Jyun-Shen Chang, Shun-De Chen, Ying Zheng, Xian-Zhong Liu, and Shu-Jin Ke.
1992.
Large-corpus-based methods for Chinese personal name recogni-tion.
Journal of Chinese Information Processing,6(3):7-15.Keh-Jiann Chen and Shing-Huan Liu.
1992.
Wordidentification for Mandarin Chinese sentences.In Proceedings of COLING-92, pages 101-107.COLING.Kenneth Ward Church and William Gale.
1991.
Acomparison of the enhanced Good-Turing anddeleted estimation methods for estimating prob-abilities of English bigrams.
Computer Speechand Language, 5(1): 19-54.John DeFrancis.
1984.
The Chinese Language.
Uni-versity of Hawaii Press, Honolulu.C.-K.
Fan and W.-H. Tsai.
1988.
Automatic wordidentification i Chinese sentences by the relax-ation technique.
Computer Processing of Chineseand Oriental Languages, 4:33-56.Lauri Karttunen, Ronald Kaplan, and Annie Zaenen.1992.
Two-level morphology with composition.In COLING-92, pages 141-148.
COLING.Kimmo Koskenniemi.
1983.
Two-Level Morphology:a General Computational Model for Word-FormRecognition and Production.
Ph.D. thesis, Uni-versity of Helsinki, Helsinki.72o~Q)_Ec)od~qo.OooU3 o.c~oantlgreedygreedycurrent methoddict.
0nlyTaiwanMainland?
?X~Oi i !
i i i-0.3 -0.2 -0.1 0.0 0.1 0.2Dimension I (62%)Figure 4: Classical metric multidimensional scaling of distance matrix, showing the two most significant dimensions.The percentage scores on the axis labels represent the amount of data explained by the dimension i  question.Table 3: Performance on morphological nalysis.Affix Pron Base category N found N correct (prec.)
N missed (rec.
)~T c bu2-xia4 verb 20 20 (100%) 12 (63%)~-F~ bu2-xia4-qu4 verb 30 29 (97%) 1 (97%):~T bu4-1iao3 verb 72 72 (100%) 15 (83%)~$tT de2-liao3 verb 36 36 (100%) 11 (77%)r~ menO noun 141 139 (99%) 6 (96%)Ming-Yu Lin, Tung-Hui Chiang, and Keh-Yi Su.
1993.A preliminary study on unknown word problemin Chinese word segmentation.
In ROCLING 6,pages 119-141.
ROCLING.Fernando Pereira, Michael Riley, and Richard Sproat.1994.
Weighted rational transductions and theirapplication to human language processing.
InARPA Workshop on Human Language Technol-ogy, pages 249-254.
Advanced Research ProjectsAgency, March 8-11.Chilin Shih.
1986.
The Prosodic Domain of ToneSandhi in Chinese.
Ph.D. thesis, UCSD, La Jolla,CA.Richard Sproat and Chilin Shih.
1990.
A statisticalmethod for finding word boundaries in Chinesetext.
Computer P ocessing of Chinese and Orien-tal Languages, 4:336-35 i.Richard Sproat.
1992.
Morphology and Computation.MIT Press, Cambridge, MA.Evelyne Tzoukermann and Mark Liberman.
1990.
Afinite-state morphological processor for Spanish.In COLING-90, Volume 3, pages 3: 277-286.COLING.Yongheng Wang, Haiju Su, and Yan Mo.
1990.
Au-tomatic processing of chinese words.
Journal ofChinese Information Processing, 4(4): 1-11.Liang-Jyh Wang, Wei-Chuan Li, and Chao-HuangChang.
1992.
Recognizing unregistered namesfor mandarin word identification.
In Proceedingsof COLING-92, pages 1239-1243.
COLING.Zimin Wu and Gwyneth Tseng.
1993.
Chinese textsegmentation for text retrieval: Achievements andproblems.
Journal of the American Society forInformation Science, 44(9):532-542.73
