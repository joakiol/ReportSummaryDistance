Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 87?92,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsEntities' Sentiment RelevanceZvi Ben-AmiThe Hebrew UniversityJerusalem, ISRAELzvi.benami@mail.huji.ac.ilRonen FeldmanThe Hebrew UniversityJerusalem, ISRAELronen.feldman@huji.ac.ilBinyamin RosenfeldDigital TrowelNew York, USAgrurgrur@gmail.comAbstractSentiment relevance detection problems oc-cur when there is a sentiment expression in atext, and there is the question of whether ornot the expression is related to a given entityor, more generally, to a given situation.
Thepaper discusses variants of the problem, andshows that it is distinct from other somewhatsimilar problems occurring in the field of sen-timent analysis and opinion mining.
We ex-perimentally demonstrate that using the in-formation about relevancy significantly af-fects the final sentiment evaluation of the en-tities.
We then compare a set of different al-gorithms for solving the relevance detectionproblem.
The most accurate results areachieved by algorithms that use certain doc-ument-level information about the target enti-ties.
We show that this information can beaccurately extracted using supervised classi-fication methods.1 IntroductionSentiment extraction by modern sentiment analy-sis (SA) systems is usually based on searchingthe input text for sentiment-bearing words andexpressions, either general (language-wide) ordomain-specific.
In most common SA approach-es, each such expression carries a polarity value("positive" or "negative") which is possiblyweighted.
The sum of all polarity values from allexpressions found in a text becomes the senti-ment score for the whole text.People are, however, usually interested in sen-timents regarding some entity or situation, andnot in sentiments of a particular document.
Anatural way to make the SA more focused is toexplicitly bind each sentiment expression to aspecific entity, or to a small set of entities fromamong all entities mentioned in the document.The choice of which entity to bind a sentimentexpression to, can be made according to theproximity (physical, syntactical, and/or semantic)and/or salience of the entities.In this paper, we argue that all of these meth-ods can be useful in different contexts, and so thebest single algorithm should use all availableproximity information, of all kinds, together withadditional context information ?position in thedocument, section, or paragraph; proximity ofother entities; lexical contents; etc.
One of themost important context information is the type ofrelation between the target entity and the docu-ment ?
whether the entity is the main topic of thedocument, or one of several main topics, or men-tioned in passing, etc.Another layer that we'd like to add concernsthe interaction of different entity types duringSA.
In a typical situation, there is only one entitytype which is the target for SA.
In such cases,clearly distinguishing between the relevancy oftarget and non-target entities types is not essen-tial.
For example, when the general topic is aCOMPANY, and there is a sentiment expressionreferring to a PERSON or a PRODUCT, thissentiment expression is still relevant to the com-pany and can be regarded as such.
In other situa-tions, SA users may be specifically interested inan interaction between entities of different types.For example, in a medical forum setting, it maybe interesting to know the users' sentiments re-garding a given DRUG in the context of a givenDISEASE.
We will show that such situations aremodeled well enough using intersections of re-gions of relevance of the participating entitytypes, with the relevance region for each typecalculated separately.We purposefully exclude possible interactionsbetween entities of the same type, because theybehave in a different way.
The precise analysisof such interactions is a different topic from rele-87vance detection, and so it is mostly ignored inthis paper.2 Related WorkThe task of SA has drawn the attention of manyresearchers worldwide (Connor et al, 2010; Liu,2012; Loughran and Mcdonald, 2010; Pang andLee, 2004; Turney, 2002).
While most SA re-search is focused on discovering and classifyingthe expressions, some are also concerned withthe targets of the expressions and explicitly iden-tify the syntactic targets of sentiment expressions(Pang and Lee, 2004).Other related works belong to the Passage Re-trieval field, since the relevance detection prob-lem can be construed as a specific form of pas-sage retrieval problem (Liu and Croft, 2002;Tiedemann and Mur, 2008).
Different approach-es were suggested for passage retrieval (Buscaldiet al, 2010; Comas et al, 2012; Hearst, 1997;Lafferty et al, 2001; Lin et al, 2012; Liu andCroft, 2002; Lloret et al, 2012; O?Connor et al,2013; Otterbacher et al, 2009; Salton et al,1993; Wachsmuth, 2013), some are more sophis-ticated than others.The closest approach to ours is the one ofScheible and Sch?tze (2013), but in contrast tothem, we strive to discover sentiments' relevancefor all entities (of a given type) mentioned in thedocument, not necessarily topical.3 Entity RelevanceAn instance of the sentiment relevance detectionproblem for a single entity consists of a text doc-ument, a sentiment expression within the docu-ment, and a target entity.
The task is a binarydecision: 'relevant' vs. 'irrelevant'.
To solve thistask, we can use any information that can befound by analyzing the document.
Thus, we canassume that we know the parse trees of all sen-tences and the locations of all references of allentities in the document, including co-references.In addition, we make use of an extra piece ofinformation for each target entity ?
its "statuswithin the document", or "document type withrespect to the entity".
We distinguish betweenseveral types which are intuitively clearly differ-ent:?
'Target' ?
the entity is the main topic of thedocument;?
'Accidental' ?
the entity is not the main topicof the document, and is mentioned in passing;?
'RelationTarget' ?
the main topic of the doc-ument is a relation between the entity andsome other entities of the same type;?
'ListTarget' ?
the entity is one of a few equal-ly important topics, dealt with sequentially.In the datasets we use for experiments, eachentity is manually annotated with its status with-in the document, which allows us to directly ob-serve the influence of this data on the accuracyof relevance discernment.
We also show that thisdata can be automatically extracted using super-vised classification.Since this paper is primarily a study of senti-ment relevance, the actual sentiment expressionsare not always labeled in our datasets.
Instead,relevance ranges are annotated for each entity, inthe style of passage retrieval problems, with theexpectation that sentiment expressions relevantto an entity only appear in the parts of the docu-ment that are labeled as "relevant", and converse-ly, that all expressions appearing in parts labeled"irrelevant" are irrelevant.
This way of annotat-ing allows the comparing of different relevancedetection strategies independently of the mainsentiment extraction tool.All of the algorithms discussed in this paperuse the same document processing methods, thusallowing us to compare the algorithms them-selves independent of the quality and specifics ofthe underlying NLP.The multiple-entity relevance problem is dis-tinguished from the single-entity relevance prob-lem by the requirement for the sentiment expres-sion to be relevant to several entities of differenttypes.
The problem is close to Relation Extrac-tion in this sense.
The examples we are interestedin are in the medical domain and deal with threemain entity types: PERSON, DRUG, andDISEASE, where PERSON is restricted toknown physicians.
While each of the entity typescan be the target of a sentiment expression, themore interesting questions in this domain involvemultiple entities, specifically, DRUG +DISEASE ("how effective is this drug for thisdisease?
"), and PERSON + DRUG + DISEASE("what does this physician say about using thisdrug to cure this disease?
").We solve the multiple-entity relevance prob-lem by intersecting the relevance ranges of dif-ferent-type entities, thus reducing the problem tothe single-entity relevance detection.
As such,the experiments regarding the multiple-entityrelevance need only check the accuracy of thisreduction.
In the medical domain, at least, thisaccuracy appears to be adequate.884 Relevance AlgorithmsEach algorithm receives, as input, the text of thedocument, with labeled reference of the targetentity and other entities of the same type.
Thelabeled references also include all coreferentialreferences, extracted automatically by an NLPsystem.
The input text also includes labeled can-didate sentiment expressions, either manuallylabeled or automatically extracted by a rele-vance-ignoring SA system1.
The task of the algo-rithms is to label each candidate expression asrelevant or irrelevant to the target entity.
Thealgorithms are evaluated according to the accura-cy (recall, precision, and F1) of this labeling ofindividual sentiment expressions.This method produces a reasonably well-understandable quality measure (the percentageof expressions that the algorithms get right orwrong), and also allows us to compare algo-rithms focused on individual expressions andalgorithms working on text ranges.
The algo-rithms we evaluate are as follows:?
Baseline - Every expression is declared rele-vant.
This is the standard mode of operation ofdocument-level SA tools, although it is usuallyonly applied to the 'Target' entities ?
the maintopic(s) of the document.?
Physical-proximity-based - A text-range fo-cused algorithm, which labels pieces of text asrelevant or irrelevant according to their place-ment relative to the references of the target en-tity and other entities of the same type, as wellas some other contextual clues, such as para-graph boundaries.
Generally, the mentioning ofan entity starts its relevance range (and stopsthe relevance range of the previously men-tioned entity).
For the first entity reference in aparagraph, the range also extends backward tothe beginning of the sentence.
There are threeflavors of the algorithm, specifically adaptedfor different document-types-with-respect-to-the-target-entity:o 'Proximity-Accidental' - stops relevanceranges at paragraph boundaries,o 'Proximity-Targeted' - restarts relevanceranges at paragraph boundaries (every para-1In our experiments, we also use a standalone automaticFinancial SA system from Feldman et al (2010), workingin the 'ignore relevance' mode, which (1) finds and labelsall entities of the target type(s); (2) resolves all corefer-ences for the target entity type(s); (3) finds and labels allsentiment expressions, regardless of their relevance; and(4) provides dependency parses for all sentences in thecorpus.graph is assumed relevant at the start, unlessanother entity is mentioned).o 'Proximity-List' - interpolates relevanceranges over intermission paragraphs, unlessthey are explicitly irrelevant (e.g., contain-ing references of other entities of the sametype).?
Syntactic-proximity-based - An expression-focused algorithm, which labels expressions asrelevant or irrelevant according to their dis-tance to various entity references in the de-pendency parse graph.
There are two flavors ofthe algorithm: direct and reverse.
The formerconsiders an expression relevant only if it isclosest to the target entity from among all enti-ties of the same type, and the distance is suffi-ciently close.
The latter considers an expres-sion irrelevant only if it has the above-described relation to some non-target entity ofthe same type.
The rationale for the two flavorsis the distinction between 'Targeted' and 'Acci-dental' document types regarding the target en-tity.
For the 'Accidental' entities, a sentimentexpression is assumed to be relevant only if itis explicitly connected to the entity.
For 'Tar-geted' entities, an expression is irrelevant onlyif it is explicitly connected to some other entityof the same type.?
Classification-based - This algorithm consid-ers each candidate sentiment expression as aninstance of a binary classification problem, tobe solved using supervised classification.
Forevaluating this algorithm, some part of the testcorpus is used for training, and the other fortesting, with N-fold cross-validation.
The fea-tures for classification may use any infor-mation present in the input.In the current experiments, we use refer-ences of target and non-target entities, appear-ances of paragraph and document boundaries,length of syntactic connections to target andnon-target entities, when available, and explicitentity status within documents, when available.The (binary) classification features are builtfrom sequences of up to 5 occurrences of theabove-described pieces, with the pieces ap-pearing before and after the sentiment expres-sion tracked separately.
For classification, weuse a linear classifier with Large Margin train-ing (regularized perceptron, as discussed inScheible and Sch?tze, (2013)).?
Sequence-classification-based - The algo-rithm uses exactly the same features as the di-rect classification-based above, but instead ofconsidering each expression separately, it con-89siders them as a sequence, one per document.So, instead of a Large Margin binary classifier,a probabilistic sequence classifier is used(CRF, as discussed in Lafferty et al (2001)).5 ExperimentsFor the experiments, we use two manually-annotated corpora 2 , a financial corpus 3  and amedical4 corpus.
In the Financial corpus, COM-PANIEs are used as target entities and in themedical corpus, DISEASEs, DRUGs and PER-SONs are the entity types that are used as targetentities.
For the purpose of the experiments, weare interested only in single-entity sentimentsabout DRUGs, and multiple-entity sentimentsabout DRUGs + DISEASEs, and DRUGs +DISEASEs + PERSONs.The evaluation metrics in all of the experi-ments are precision, recall, and F1.
For the clas-sification-based algorithms, unless stated other-wise, we use 10-fold cross-validation.5.1 Experiment: Importance of relevanceIn the first experiment, we demonstrate the im-portance of using relevance when calculating theconsolidated sentiment score of an entity withina set of documents.
For each entity, we set the'correct' consolidated sentiment score to the av-erage of polarities of all sentiments in a corpuswhich are labeled as relevant to the entity.
Then,we compare the correct value to the two scorescalculated without considering relevance:?
'Baseline' - the average of polarities of all sen-timents in all documents where the entity ismentioned, and?
'TargetedOnly' - the average of polarities ofall sentiments in the documents where the enti-ty is labeled as target (main topic of the docu-ment).
This case models the typical state of arelevance-agnostic SA system.For this evaluation, we only compare the signof the final sentiment scores, without consideringtheir magnitudes (unless it is close to zero, in2 Fully annotating texts for semantic relevance is an arduoustask, thus the used annotated corpora are relatively small.Sample can be found at http://goo.gl/6HONHP.3 A corpus of 160 financial news documents on at least oneentity of interest, of average size ~5Kb, downloaded fromvarious financial news websites.
The dataset mentions 424different companies.4 A corpus of 160 documents, of average size ~7Kb, down-loaded following Google queries on a set of a few com-mon drugs and diseases.
The dataset mentions 722 differ-ent people, 46 diseases, and 175 drugs.which it is considered 'neutral').
The errors at thislevel indicate definite SA errors ?
miscalculatingentity's sentiment into its opposite.The results of the evaluation are as follows:The 'Baseline' scores show a large differencefrom the correct scores, with 33% and 38% ofentities having wrong final polarity in the finan-cial (COMPANY) and medical (DRUG) do-mains, respectively.
The 'TargetedOnly' scoresare somewhat closer to correct, with 12% and28% of entities with incorrect final polarities.However, the 'TargetedOnly' method naturallysuffers from a very low recall, with only 19%and 38% of entities covered in the financial andmedical domains, respectively.5.2 Experiment: Influence of entity statusIn this experiment, we compare the performanceof various algorithms while either providing orwithholding the information about the document-type-with-respect-to-the-target-entity.The performance of the physical proximity al-gorithms on the financial corpus is shown at thetop left hand side of Table 1.
The set of all in-stances of relevance detection problems in thecorpus (an instance consists of a sentiment ex-pression within a text, together with a target enti-ty) is divided into three subsets, according to thestatus of the target entity within the document.As expected, the three flavors of the physicalproximity algorithm perform much better on thecorpus subsets they are adapted to.
At the bottomleft hand side of Table 1, we similarly show theperformance of the two flavors of the syntax-proximity-based algorithm on the medical do-main (DRUG entities).
Same as above, there is alarge difference in the performance of the twoflavors of the algorithm on different subsets ofthe problem set.
Finally, at the top of Table 2, wecompare the performance of the two classifica-tion-based algorithms on the two (whole) prob-lem sets, while either keeping or withholding theentity status information from the classifier.
Thedifference in results is less pronounced here, butis still noticeable.
The reason for the smaller dif-ference, we hypothesize, is the ability of the clas-sifiers to partially infer the entity status from thevarious context clues that are used as classifica-tion features (see the experiment 5.3).5.3 Experiment: Automatic identification ofentity status using classification.In this experiment, we confirm that it is possibleto identify the entity status within documentsusing supervised classification.90Table 1.
Performance of different algorithms on three subsets of the corpus with a different status ofthe target entity within the document.Experiment Algorithm  Financial MedicalExperiment 5.2(Prec./ Rec,/F1).Classification (with entitystatus info)90/86/88 84/88/86Classification  (withoutentity status info)89/85/87 87/81/84Sequence Classification(with entity status info)96/84/90 99/84/91Sequence Classification(without entity status info)96/83/89 95/85/90Experiment 5.3(F1, (diff.
in F1from exp.
5.2))Classification 86.7  (-0.9) 83.9 (-2.0)Sequence Classification 89.7 (+0.1) 90.9 (-0.3)Experiment 5.5(F1)Baseline 37.2 28.6Physical Proximity 84.1 79.5Syntactic-Proximity 43.8 54.6Classification 87.6 85.9Sequence-Classification 91.2 89.6Table 2.
Performance of different algorithmson the different domains.The results of direct evaluation show that theaccuracies of the Medical and Financial corpora(using 10-fold X-validation) are 87.8% and82.2% respectively, and the accuracy when usingthe Medical corpus for training the Financialcorpus for testing and vice versa, are 78.2% and86.1% , respectively.The results of relevance detection using theautomatically extracted entity status values areshown at the right hand side of Table 1 and in themiddle of Table 2, which utilize the same da-tasets and algorithms as at the left hand side ofTable 1 and at the top of Table 2.
As can be seenfrom the tables, the drop in performance is small,demonstrating the success of classification-basedextraction of entity status information.5.4 Experiment: Cross-domain applicabilityIn this experiment, we test how well the classifi-ers trained on data from one domain work oninput from a different domain.The classification results using different typesof training data are shown in Table 3.Classification Sequence classificationMedical 2-fold/10-fold 84.6/85.9 85.7/89.6Train on Fin, test on Med 83.5 86.8Financial 2-fold/10-fold 86.1/87.6 90.3/91.2Train on Med, test on Fin 85.4 91.0Table 3.
Performance of classification-basedalgorithms using different training data (F1).The table confirms general independence ofthe classification performance on the domain.Comparing the 2-fold and 10-fold cross-validation results (the difference is equivalent todoubling the amount of training data), shows thatthe amount of training data is sufficient.5.5 Experiment: Overall performance ofalgorithmsIn this experiment, we simply compare the over-all accuracy of various algorithms for relevancediscernment, operating at their best parameters.The results are shown at the bottom of Table 2.Overall, classification-based algorithms performbetter than the deterministic ones, with sequence-classification performing significantly better thandirect classification.
Syntactic proximity-based isprecise, but has relatively low recall, reducing itsoverall performance.
Physical proximity-based issimplest, and produce reasonably high overallresults, although worse than the best-performingclassification-based methods.6 ConclusionThe results are mostly intuitively understood andconfirm the expectations.
We confirmed thatrelevance detection is essential for producingcorrect consolidated SA results.
We found thatthe entity status within the document is one ofthe important clues for solving the relevancedetection problem, and showed that this infor-mation can be effectively automatically extractedusing supervised classification.
We also com-pared several algorithms for relevance detection,with the results that classification-based algo-rithms generally outperform simpler ones basedon the same clues, although a very simple prox-imity-based algorithm performs reasonably wellif allowed to use the entity status information.AcknowledgmentsThis work is supported by the Israel Ministry ofScience and Technology Center of Knowledge inMachine Learning and Artificial Intelligence andthe Israel Ministry of Defense.Experiment 5.2  (Precision/Recall/F1) Experiment 5.3 ( F1, (diff.
in F1 from exp.
5.2)Accidental Targeted List Whole Accidental Targeted List WholeProximity-Accidental 84/43/57 93/76/84 92/74/82 92/72/81 60 (+2.6) 79 (-5.5) 83 (+1.1)Proximity-Targeted 31/50/38 90/84/87 55/89/68 63/83/72 38 (-0.4) 82 (-5.2) 73 (+4.3)Proximity-List 58/44/50 90/83/87 88/83/86 85/80/82 52 (+2.1) 81 (-5.9) 87 (+1.6)Proximity-Combined    89/80/84    83 (-1.2)Syntactic-Prox.-Direct 93/48/64 99/42/60   65 (+0.8) 59 (-0.2)Syntactic-Prox.-Inverse 04/72/08 70/66/68   8 (-0.2) 76 (+6.4)91ReferencesBuscaldi, D., Rosso, P., G?mez-Soriano, J., Sanchis,E., 2010.
Answering questions with an n-grambased passage retrieval engine.
J. Intell.
Inf.
Syst.34, 113?134.
doi:10.1007/s10844-009-0082-yComas, P.R., Turmo, J., M?rquez, L., 2012.
Sibyl, afactoid question-answering system for spokendocuments.
ACM Trans.
Inf.
Syst.
30, 19:1?19:40.doi:10.1145/2328967.2328972Connor, B.O., Balasubramanyan, R., Routledge, B.R.,Smith, N.A., 2010.
From Tweets to Polls?
: LinkingText Sentiment to Public Opinion Time Series, in:Proceedings of the Fourth International AAAIConference on Weblogs and Social Media.
pp.122?129.Feldman, R., Rosenfeld, B., Bar-haim, R., Fresko, M.,2010.
The Stock Sonar ?
Sentiment Analysis ofStocks Based on a Hybrid Approach, in:Proceedings of the Twenty-Third InnovativeApplications of Artificial Intelligence Conference.pp.
1642?1647.Hearst, M.A., 1997.
TextTiling: segmenting text intomulti-paragraph subtopic passages.
Comput.Linguist.
23, 33?64.Lafferty, J., McCallum, A., Pereira, F., 2001.Conditional Random Fields: Probabilistic Modelsfor Segmenting and Labeling Sequence Data., in:Proceedings of the Eighteenth InternationalConference on Machine Learning (ICML-2001).Lin, H.-T., Chi, N.-W., Hsieh, S.-H., 2012.
Aconcept-based information retrieval approach forengineering domain-specific technical documents.Adv.
Eng.
Informatics 26, 349?360.doi:http://dx.doi.org/10.1016/j.aei.2011.12.003Liu, B., 2012.
Sentiment Analysis and OpinionMining Synthesis Lectures on Human LanguageTechnologies.
Morgan & Claypool Publishers.Liu, X., Croft, W.B., 2002.
Passage retrieval based onlanguage models, in: Proceedings of the EleventhInternational Conference on Information andKnowledge Management, CIKM ?02.
ACM, NewYork, NY, USA, pp.
375?382.doi:10.1145/584792.584854Lloret, E., Balahur, A., G?mez, J., Montoyo, A.,Palomar, M., 2012.
Towards a unified frameworkfor opinion retrieval, mining and summarization.
J.Intell.
Inf.
Syst.
39, 711?747.
doi:10.1007/s10844-012-0209-4Loughran, T.I.M., Mcdonald, B., 2010.
When is aLiability not a Liability??
Textual Analysis ,Dictionaries , and 10-Ks Journal of Finance ,forthcoming.
J.
Finance 66, 35?65.O?Connor, B., Stewart, B.M., Smith, N.A., 2013.Learning to Extract International Relations fromPolitical Context, in: Proceedings of the 51stAnnual Meeting of the Association forComputational Linguistics (Volume 1: LongPapers).
Association for ComputationalLinguistics, Sofia, Bulgaria, pp.
1094?1104.Otterbacher, J., Erkan, G., Radev, D.R., 2009.
BiasedLexRank: Passage retrieval using random walkswith question-based priors.
Inf.
Process.
Manag.45, 42?54.doi:http://dx.doi.org/10.1016/j.ipm.2008.06.004Pang, B., Lee, L., 2004.
A Sentimental Education?
:Sentiment Analysis Using SubjectivitySummarization Based on Minimum Cuts.Salton, G., Allan, J., Buckley, C., 1993.
Approachesto passage retrieval in full text informationsystems, in: Proceedings of the 16th AnnualInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, SIGIR?93.
ACM, New York, NY, USA, pp.
49?58.doi:10.1145/160688.160693Scheible, C., Sch?tze, H., 2013.
Sentiment Relevance,in: Proceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics(Volume 1: Long Papers).
Association forComputational Linguistics, Sofia, Bulgaria, pp.954?963.Tiedemann, J., Mur, J., 2008.
Simple is best:experiments with different document segmentationstrategies for passage retrieval, in: Coling 2008:Proceedings of the 2nd Workshop on InformationRetrieval for Question Answering, IRQA ?08.Association for Computational Linguistics,Stroudsburg, PA, USA, pp.
17?25.Turney, P., 2002.
Thumbs Up or Thumbs Down?
?Semantic Orientation Applied to UnsupervisedClassification of Reviews, in: Proceedings of theAssociation for Computational Linguistics (ACL).pp.
417?424.Wachsmuth, H., 2013.
Information Extraction as aFiltering Task Categories and Subject Descriptors,in: To Appear in Proc.
of the 22th ACM CIKM.92
