Convolution Kernels with Feature Selectionfor Natural Language Processing TasksJun Suzuki, Hideki Isozaki and Eisaku MaedaNTT Communication Science Laboratories, NTT Corp.2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto,619-0237 Japan{jun, isozaki, maeda}@cslab.kecl.ntt.co.jpAbstractConvolution kernels, such as sequence and tree ker-nels, are advantageous for both the concept and ac-curacy of many natural language processing (NLP)tasks.
Experiments have, however, shown that theover-fitting problem often arises when these ker-nels are used in NLP tasks.
This paper discussesthis issue of convolution kernels, and then proposesa new approach based on statistical feature selec-tion that avoids this issue.
To enable the proposedmethod to be executed efficiently, it is embeddedinto an original kernel calculation process by usingsub-structure mining algorithms.
Experiments areundertaken on real NLP tasks to confirm the prob-lem with a conventional method and to compare itsperformance with that of the proposed method.1 IntroductionOver the past few years, many machine learn-ing methods have been successfully applied totasks in natural language processing (NLP).
Espe-cially, state-of-the-art performance can be achievedwith kernel methods, such as Support VectorMachine (Cortes and Vapnik, 1995).
Exam-ples include text categorization (Joachims, 1998),chunking (Kudo and Matsumoto, 2002) and pars-ing (Collins and Duffy, 2001).Another feature of this kernel methodology is thatit not only provides high accuracy but also allows usto design a kernel function suited to modeling thetask at hand.
Since natural language data take theform of sequences of words, and are generally ana-lyzed using discrete structures, such as trees (parsedtrees) and graphs (relational graphs), discrete ker-nels, such as sequence kernels (Lodhi et al, 2002),tree kernels (Collins and Duffy, 2001), and graphkernels (Suzuki et al, 2003a), have been shown tooffer excellent results.These discrete kernels are related to convolutionkernels (Haussler, 1999), which provides the con-cept of kernels over discrete structures.
Convolutionkernels allow us to treat structural features withoutexplicitly representing the feature vectors from theinput object.
That is, convolution kernels are wellsuited to NLP tasks in terms of both accuracy andconcept.Unfortunately, experiments have shown that insome cases there is a critical issue with convolutionkernels, especially in NLP tasks (Collins and Duffy,2001; Cancedda et al, 2003; Suzuki et al, 2003b).That is, the over-fitting problem arises if large ?sub-structures?
are used in the kernel calculations.
As aresult, the machine learning approach can never betrained efficiently.To solve this issue, we generally eliminate largesub-structures from the set of features used.
How-ever, the main reason for using convolution kernelsis that we aim to use structural features easily andefficiently.
If use is limited to only very small struc-tures, it negates the advantages of using convolutionkernels.This paper discusses this issue of convolutionkernels, and proposes a new method based on statis-tical feature selection.
The proposed method dealsonly with those features that are statistically signif-icant for kernel calculation, large significant sub-structures can be used without over-fitting.
More-over, the proposed method can be executed effi-ciently by embedding it in an original kernel cal-culation process by using sub-structure mining al-gorithms.In the next section, we provide a brief overviewof convolution kernels.
Section 3 discusses one is-sue of convolution kernels, the main topic of thispaper, and introduces some conventional methodsfor solving this issue.
In Section 4, we proposea new approach based on statistical feature selec-tion to offset the issue of convolution kernels us-ing an example consisting of sequence kernels.
InSection 5, we briefly discuss the application of theproposed method to other convolution kernels.
InSection 6, we compare the performance of conven-tional methods with that of the proposed method byusing real NLP tasks: question classification andsentence modality identification.
The experimentalresults described in Section 7 clarify the advantagesof the proposed method.2 Convolution KernelsConvolution kernels have been proposed as a con-cept of kernels for discrete structures, such as se-quences, trees and graphs.
This framework definesthe kernel function between input objects as the con-volution of ?sub-kernels?, i.e.
the kernels for thedecompositions (parts) of the objects.Let X and Y be discrete objects.
Conceptually,convolution kernels K(X, Y ) enumerate all sub-structures occurring in X and Y and then calculatetheir inner product, which is simply written as:K(X,Y ) = ??
(X), ?
(Y )?
=?i?i(X) ?
?i(Y ).
(1)?
represents the feature mapping from thediscrete object to the feature space; that is,?
(X) = (?1(X), .
.
.
, ?i(X), .
.
.).
With sequencekernels (Lodhi et al, 2002), input objects X and Yare sequences, and ?i(X) is a sub-sequence.
Withtree kernels (Collins and Duffy, 2001), X and Y aretrees, and ?i(X) is a sub-tree.When implemented, these kernels can be effi-ciently calculated in quadratic time by using dy-namic programming (DP).Finally, since the size of the input objects is notconstant, the kernel value is normalized using thefollowing equation.K?
(X,Y ) = K(X,Y )?K(X,X) ?
K(Y, Y )(2)The value of K?
(X, Y ) is from 0 to 1, K?
(X, Y ) = 1if and only if X = Y .2.1 Sequence KernelsTo simplify the discussion, we restrict ourselveshereafter to sequence kernels.
Other convolutionkernels are briefly addressed in Section 5.Many kinds of sequence kernels have been pro-posed for a variety of different tasks.
This paperbasically follows the framework of word sequencekernels (Cancedda et al, 2003), and so processesgapped word sequences to yield the kernel value.Let ?
be a set of finite symbols, and ?n be a setof possible (symbol) sequences whose sizes are nor less that are constructed by symbols in ?.
Themeaning of ?size?
in this paper is the number ofsymbols in the sub-structure.
Namely, in the case ofsequence, size n means length n. S and T can rep-resent any sequence.
si and tj represent the ith andjth symbols in S and T , respectively.
Therefore, aST1211 21 ?+??1?
?1111a,   b,   c,   aa,  ab,  ac,  ba,  bc,  aba,  aac,  abc,  bac,  abacabcS = abacT =p r o d .101010 0102 1 1 0 1 3?
?+ 0 ?
0 0 ?
0( a, b, c, ab, ac, bc, abc)( a, b, c, aa, ab, ac, ba, bc, aba, aac, abc, bac, abac)u35 3?
?+ +k e r n e l  v al u e?s e q u e n ce s s u b-s e q u e n ce s100Figure 1: Example of sequence kernel outputsequence S can be written as S = s1 .
.
.
si .
.
.
s|S|,where |S| represents the length of S. If sequenceu is contained in sub-sequence S[i : j] def= si .
.
.
sjof S (allowing the existence of gaps), the positionof u in S is written as i = (i1 : i|u|).
The lengthof S[i] is l(i) = i|u| ?
i1 + 1.
For example, ifu = ab and S = cacbd, then i = (2 : 4) andl(i) = 4 ?
2 + 1 = 3.By using the above notations, sequence kernelscan be defined as:KSK(S, T ) =?u??n?i|u=S[i]??
(i)?j|u=T [j]??
(j), (3)where ?
is the decay factor that handles the gappresent in a common sub-sequence u, and ?
(i) =l(i)?|u|.
In this paper, | means ?such that?.
Figure 1shows a simple example of the output of this kernel.However, in general, the number of features |?n|,which is the dimension of the feature space, be-comes very high, and it is computationally infeasi-ble to calculate Equation (3) explicitly.
The efficientrecursive calculation has been introduced in (Can-cedda et al, 2003).
To clarify the discussion, weredefine the sequence kernels with our notation.The sequence kernel can be written as follows:KSK(S, T ) =n?m=1?1?i?|S|?1?j?|T |Jm(Si, Tj).
(4)where Si and Tj represent the sub-sequences Si =s1, s2, .
.
.
, si and Tj = t1, t2, .
.
.
, tj , respectively.Let Jm(Si, Tj) be a function that returns thevalue of common sub-sequences if si = tj .Jm(Si, Tj) = J ?m?1(Si, Tj) ?
I(si, tj) (5)I(si, tj) is a function that returns a matchingvalue between si and tj .
This paper defines I(si, tj)as an indicator function that returns 1 if si = tj , oth-erwise 0.Then, J ?m(Si, Tj) and J ?
?m(Si, Tj) are introducedto calculate the common gapped sub-sequences be-tween Si and Tj .J ?m(Si, Tj) =????
?1 if m = 0,0 if j = 0 and m > 0,?J ?m(Si, Tj?1) + J ?
?m(Si, Tj?1)otherwise(6)J ?
?m(Si, Tj) =??
?0 if i = 0,?J ?
?m(Si?1, Tj) + Jm(Si?1, Tj)otherwise(7)If we calculate Equations (5) to (7) recursively,Equation (4) provides exactly the same value asEquation (3).3 Problem of Applying ConvolutionKernels to NLP tasksThis section discusses an issue that arises when ap-plying convolution kernels to NLP tasks.According to the original definition of convolu-tion kernels, all the sub-structures are enumeratedand calculated for the kernels.
The number of sub-structures in the input object usually becomes ex-ponential against input object size.
As a result, allkernel values K?
(X, Y ) are nearly 0 except the ker-nel value of the object itself, K?
(X, X), which is 1.In this situation, the machine learning process be-comes almost the same as memory-based learning.This means that we obtain a result that is very pre-cise but with very low recall.To avoid this, most conventional methods use anapproach that involves smoothing the kernel valuesor eliminating features based on the sub-structuresize.For sequence kernels, (Cancedda et al, 2003) usea feature elimination method based on the size ofsub-sequence n. This means that the kernel calcula-tion deals only with those sub-sequences whose sizeis n or less.
For tree kernels, (Collins and Duffy,2001) proposed a method that restricts the featuresbased on sub-trees depth.
These methods seem towork well on the surface, however, good results areachieved only when n is very small, i.e.
n = 2.The main reason for using convolution kernelsis that they allow us to employ structural featuressimply and efficiently.
When only small sized sub-structures are used (i.e.
n = 2), the full benefits ofconvolution kernels are missed.Moreover, these results do not mean that largersized sub-structures are not useful.
In some caseswe already know that larger sub-structures are sig-nificant features as regards solving the target prob-lem.
That is, these significant larger sub-structures,Table 1: Contingency table and notation for the chi-squared valuec c?
?rowu Ouc = y Ouc?
Ou = xu?
Ou?c Ou?c?
Ou?
?column Oc = M Oc?
Nwhich the conventional methods cannot deal withefficiently, should have a possibility of improvingthe performance furthermore.The aim of the work described in this paper isto be able to use any significant sub-structure effi-ciently, regardless of its size, to solve NLP tasks.4 Proposed Feature Selection MethodOur approach is based on statistical feature selectionin contrast to the conventional methods, which usesub-structure size.For a better understanding, consider the two-class (positive and negative) supervised classifica-tion problem.
In our approach we test the statisti-cal deviation of all the sub-structures in the trainingsamples between the appearance of positive samplesand negative samples.
This allows us to select onlythe statistically significant sub-structures when cal-culating the kernel value.Our approach, which uses a statistical metric toselect features, is quite natural.
We note, however,that kernels are calculated using the DP algorithm.Therefore, it is not clear how to calculate kernels ef-ficiently with a statistical feature selection method.First, we briefly explain a statistical metric, the chi-squared (?2) value, and provide an idea of howto select significant features.
We then describe amethod for embedding statistical feature selectioninto kernel calculation.4.1 Statistical Metric: Chi-squared ValueThere are many kinds of statistical metrics, such aschi-squared value, correlation coefficient and mu-tual information.
(Rogati and Yang, 2002) reportedthat chi-squared feature selection is the most effec-tive method for text classification.
Following thisinformation, we use ?2 values as statistical featureselection criteria.
Although we selected ?2 values,any other statistical metric can be used as long as itis based on the contingency table shown in Table 1.We briefly explain how to calculate the ?2 valueby referring to Table 1.
In the table, c and c?
rep-resent the names of classes, c for the positive classST1211 21 ?+??1?
?1( )2 u?
0.1 0.5 1.21111.5 0.9 0.8a,   b,   c,   aa,  ab,  ac,  ba,  bc,  aba,  aac,  abc,  bac,  abacabcS = abacT =p r o d .101010 0102 1 1 0 1 3?
?+ 0 ?
0 0 ?
01.0?
=t h r e s h o l d2.51 1 ?
( a, b, c, ab, ac, bc, abc)( a, b, c, aa, ab, ac, ba, bc, aba, aac, abc, bac, abac)u35 3?
?+ +2 ?+0 0 0 02 1 1 0 1 3?
?+ 0 ?
0 0 ?
0k e r n e l  v al u ek e r n e l  v al u e  u n d e r  t h e  f e at u r e  s e l e ct i o nf e at u r e  s e l e ct i o n?s e q u e n ce s s u b-s e q u e n ce s1000Figure 2: Example of statistical feature selectionand c?
for the negative class.
Ouc, Ouc?, Ou?c and Ou?c?represent the number of u that appeared in the pos-itive sample c, the number of u that appeared in thenegative sample c?, the number of u that did not ap-pear in c, and the number of u that did not appearin c?, respectively.
Let y be the number of samplesof positive class c that contain sub-sequence u, andx be the number of samples that contain u.
Let Nbe the total number of (training) samples, and M bethe number of positive samples.Since N and M are constant for (fixed) data, ?2can be written as a function of x and y,?2(x, y) = N(Ouc ?
Ou?c?
?
Ou?c ?
Ouc?
)2Ou ?
Ou?
?
Oc ?
Oc?.
(8)?2 expresses the normalized deviation of the obser-vation from the expectation.We simply represent ?2(x, y) as ?2(u).4.2 Feature Selection CriterionThe basic idea of feature selection is quite natural.First, we decide the threshold ?
of the ?2 value.
If?2(u) < ?
holds, that is, u is not statistically signif-icant, then u is eliminated from the features and thevalue of u is presumed to be 0 for the kernel value.The sequence kernel with feature selection(FSSK) can be defined as follows:KFSSK(S, T ) =????2(u)|u??n?i|u=S[i]??
(i)?j|u=T [j]??(j).
(9)The difference between Equations (3) and (9) issimply the condition of the first summation.
FSSKselects significant sub-sequence u by using the con-dition of the statistical metric ?
?
?2(u).Figure 2 shows a simple example of what FSSKcalculates for the kernel value.4.3 Efficient ?2(u) Calculation MethodIt is computationally infeasible to calculate ?2(u)for all possible u with a naive exhaustive method.In our approach, we use a sub-structure mining al-gorithm to calculate ?2(u).
The basic idea comesfrom a sequential pattern mining technique, PrefixS-pan (Pei et al, 2001), and a statistical metric prun-ing (SMP) method, Apriori SMP (Morishita andSese, 2000).
By using these techniques, all the sig-nificant sub-sequences u that satisfy ?
?
?2(u) canbe found efficiently by depth-first search and prun-ing.
Below, we briefly explain the concept involvedin finding the significant features.First, we denote uv, which is the concatenation ofsequences u and v. Then, u is a specific sequenceand uv is any sequence that is constructed by u withany suffix v. The upper bound of the ?2 value ofuv can be defined by the value of u (Morishita andSese, 2000).
?2(uv)?max(?2(yu, yu), ?2(xu ?
yu, 0))=?
?2(u)where xu and yu represent the value of x and yof u.
This inequation indicates that if ?
?2(u) is lessthan a certain threshold ?
, all sub-sequences uv canbe eliminated from the features, because no sub-sequence uv can be a feature.The PrefixSpan algorithm enumerates all the sig-nificant sub-sequences by using a depth-first searchand constructing a TRIE structure to store the sig-nificant sequences of internal results efficiently.Specifically, PrefixSpan algorithm evaluates uw,where uw represents a concatenation of a sequenceu and a symbol w, using the following three condi-tions.1.
?
?
?2(uw)2. ?
> ?2(uw), ?
> ??2(uw)3.
?
> ?2(uw), ?
?
?
?2(uw)With 1, sub-sequence uw is selected as a significantfeature.
With 2, sub-sequence uw and arbitrary sub-sequences uwv, are less than the threshold ?
.
Thenw is pruned from the TRIE, that is, all uwv where vrepresents any suffix pruned from the search space.With 3, uw is not selected as a significant featurebecause the ?2 value of uw is less than ?
, however,uwv can be a significant feature because the upper-bound ?2 value of uwv is greater than ?
, thus thesearch is continued to uwv.Figure 3 shows a simple example of PrefixSpanwith SMP that searches for the significant featuresa b c cd b c ab a ca cd a b da b c cd b cb a ca cd a b d?a b c db c1.0?
=b:c:d:+ 1-1+ 1-1-1au =w =( )2 uw?
( )2?
uw?T R I E  r e p r e s e n t at i o nx y+ 1-1+ 1-1+ 1abu =dc?w231121+ 1-1+ 1-1-1class t r ai n i n g  d at asu f f i xc:d:w =x y11 105.00.0 5.00.8 5.00.8 2 .22 .21 .90.11 .91.90.80.85.02 .2a:b:c:d:+ 1-1+ 1-1-1u = ?w =x y54422220cd1.91 .90.80.8?a b c cd b c ab a ca cd a b dsu f f i xsu f f i xa b c cd b cb a ca cd a b d5N = 2M =23145se ar ch  o r d e rp r u n e dp r u n e dFigure 3: Efficient search for statistically significantsub-sequences using the PrefixSpan algorithm withSMPby using a depth-first search with a TRIE represen-tation of the significant sequences.
The values ofeach symbol represent ?2(u) and ?
?2(u) that can becalculated from the number of xu and yu.
The TRIEstructure in the figure represents the statistically sig-nificant sub-sequences that can be shown in a pathfrom ?
to the symbol.We exploit this TRIE structure and PrefixSpanpruning method in our kernel calculation.4.4 Embedding Feature Selection in KernelCalculationThis section shows how to integrate statistical fea-ture selection in the kernel calculation.
Our pro-posed method is defined in the following equations.KFSSK(S, T ) =n?m=1?1?i?|S|?1?j?|T |Km(Si, Tj) (10)Let Km(Si, Tj) be a function that returns the sumvalue of all statistically significant common sub-sequences u if si = tj .Km(Si, Tj) =?u?
?m(Si,Tj)Ju(Si, Tj), (11)where ?m(Si, Tj) represents a set of sub-sequenceswhose size |u| is m and that satisfy the above condi-tion 1.
The ?m(Si, Tj) is defined in detail in Equa-tion (15).Then, let Ju(Si, Tj), J ?u(Si, Tj) and J ?
?u (Si, Tj)be functions that calculate the value of the commonsub-sequences between Si and Tj recursively, aswell as equations (5) to (7) for sequence kernels.
Weintroduce a special symbol ?
to represent an ?emptysequence?, and define ?w = w and |?w| = 1.Juw(Si, Tj) =??
?J ?u(Si, Tj) ?
I(w)if uw ?
?
?|uw|(Si, Tj),0 otherwise(12)where I(w) is a function that returns a matchingvalue of w. In this paper, we define I(w) is 1.?
?m(Si, Tj) has realized conditions 2 and 3; thedetails are defined in Equation (16).J ?u(Si, Tj) =????
?1 if u = ?,0 if j = 0 and u 6= ?,?J ?u(Si, Tj?1) + J ?
?u (Si, Tj?1)otherwise(13)J ?
?u (Si, Tj) =??
?0 if i = 0,?J ?
?u (Si?1, Tj) + Ju(Si?1, Tj)otherwise(14)The following five equations are introduced to se-lect a set of significant sub-sequences.
?m(Si, Tj)and ?
?m(Si, Tj) are sets of sub-sequences (features)that satisfy condition 1 and 3, respectively, whencalculating the value between Si and Tj in Equa-tions (11) and (12).
?m(Si, Tj) = {u | u ?
?
?m(Si, Tj), ?
?
?2(u)} (15)?
?m(Si, Tj) =????(??
?m?1(Si, Tj), si)if si = tj?
otherwise(16)?
(F,w) = {uw | u ?
F, ?
?
?
?2(uw)}, (17)where F represents a set of sub-sequences.
No-tice that ?m(Si, Tj) and ?
?m(Si, Tj) have only sub-sequences u that satisfy ?
?
?2(uw) or ?
??
?2(uw), respectively, if si = tj(= w); otherwisethey become empty sets.The following two equations are introduced forrecursive set operations to calculate ?m(Si, Tj) and?
?m(Si, Tj).??
?m(Si, Tj) =???????{?}
if m = 0,?
if j = 0 and m > 0,??
?m(Si, Tj?1) ?
???
?m(Si, Tj?1)otherwise(18)???
?m(Si, Tj) =????
if i = 0 ,???
?m(Si?1, Tj) ?
?
?m(Si?1, Tj)otherwise(19)In the implementation, Equations (11) to (14) canbe performed in the same way as those used to cal-culate the original sequence kernels, if the featureselection condition of Equations (15) to (19) hasbeen removed.
Then, Equations (15) to (19), whichselect significant features, are performed by the Pre-fixSpan algorithm described above and the TRIErepresentation of statistically significant features.The recursive calculation of Equations (12) to(14) and Equations (16) to (19) can be executed inthe same way and at the same time in parallel.
As aresult, statistical feature selection can be embeddedin oroginal sequence kernel calculation based on adynamic programming technique.4.5 PropertiesThe proposed method has several important advan-tages over the conventional methods.First, the feature selection criterion is based ona statistical measure, so statistically significant fea-tures are automatically selected.Second, according to Equations (10) to (18), theproposed method can be embedded in an originalkernel calculation process, which allows us to usethe same calculation procedure as the conventionalmethods.
The only difference between the originalsequence kernels and the proposed method is thatthe latter calculates a statistical metric ?2(u) by us-ing a sub-structure mining algorithm in the kernelcalculation.Third, although the kernel calculation, which uni-fies our proposed method, requires a longer train-ing time because of the feature selection, the se-lected sub-sequences have a TRIE data structure.This means a fast calculation technique proposedin (Kudo and Matsumoto, 2003) can be simply ap-plied to our method, which yields classification veryquickly.
In the classification part, the features (sub-sequences) selected in the learning part must beknown.
Therefore, we store the TRIE of selectedsub-sequences and use them during classification.5 Proposed Method Applied to OtherConvolution KernelsWe have insufficient space to discuss this subject indetail in relation to other convolution kernels.
How-ever, our proposals can be easily applied to tree ker-nels (Collins and Duffy, 2001) by using string en-coding for trees.
We enumerate nodes (labels) oftree in postorder traversal.
After that, we can em-ploy a sequential pattern mining technique to selectstatistically significant sub-trees.
This is because wecan convert to the original sub-tree form from thestring encoding representation.Table 2: Parameter values of proposed kernels andSupport Vector Machinesparameter valuesoft margin for SVM (C) 1000decay factor of gap (?)
0.5threshold of ?2 (? )
2.70553.8415As a result, we can calculate tree kernels with sta-tistical feature selection by using the original treekernel calculation with the sequential pattern min-ing technique introduced in this paper.
Moreover,we can expand our proposals to hierarchically struc-tured graph kernels (Suzuki et al, 2003a) by usinga simple extension to cover hierarchical structures.6 ExperimentsWe evaluated the performance of the proposedmethod in actual NLP tasks, namely English ques-tion classification (EQC), Japanese question classi-fication (JQC) and sentence modality identification(MI) tasks.We compared the proposed method (FSSK) witha conventional method (SK), as discussed in Sec-tion 3, and with bag-of-words (BOW) Kernel(BOW-K)(Joachims, 1998) as baseline methods.Support Vector Machine (SVM) was selected asthe kernel-based classifier for training and classifi-cation.
Table 2 shows some of the parameter valuesthat we used in the comparison.
We set thresholdsof ?
= 2.7055 (FSSK1) and ?
= 3.8415 (FSSK2)for the proposed methods; these values represent the10% and 5% level of significance in the ?2 distribu-tion with one degree of freedom, which used the ?2significant test.6.1 Question ClassificationQuestion classification is defined as a task similar totext categorization; it maps a given question into aquestion type.We evaluated the performance by using dataprovided by (Li and Roth, 2002) for Englishand (Suzuki et al, 2003b) for Japanese questionclassification and followed the experimental settingused in these papers; namely we use four typicalquestion types, LOCATION, NUMEX, ORGANI-ZATION, and TIME TOP for JQA, and ?coarse?and ?fine?
classes for EQC.
We used the one-vs-restclassifier of SVM as the multi-class classificationmethod for EQC.Figure 4 shows examples of the question classifi-cation data used here.question types input object : word sequences ([ ]: information of chunk and ?
?
: named entity)ABBREVIATION what,[B-NP] be,[B-VP] the,[B-NP] abbreviation,[I-NP] for,[B-PP] Texas,[B-NP],?B-GPE?
?,[O]DESCRIPTION what,[B-NP] be,[B-VP] Aborigines,[B-NP] ?,[O]HUMAN who,[B-NP] discover,[B-VP] America,[B-NP],?B-GPE?
?,[O]Figure 4: Examples of English question classification dataTable 3: Results of the Japanese question classification (F-measure)(a) TIME TOP (b) LOCATION (c) ORGANIZATION (d) NUMEXnFSSK1FSSK2SKBOW-K1 2 3 4 ?- .961 .958 .957 .956- .961 .956 .957 .956- .946 .910 .866 .223.902 .909 .886 .855 -1 2 3 4 ?- .795 .793 .798 .792- .788 .799 .804 .800- .791 .775 .732 .169.744 .768 .756 .747 -1 2 3 4 ?- .709 .720 .720 .723- .703 .710 .716 .720- .705 .668 .594 .035.641 690 .636 .572 -1 2 3 4 ?- .912 .915 .908 .908- .913 .916 .911 .913- .912 .885 .817 .036.842 .852 .807 .726 -6.2 Sentence Modality IdentificationFor example, sentence modality identification tech-niques are used in automatic text analysis systemsthat identify the modality of a sentence, such as?opinion?
or ?description?.The data set was created from Mainichi news arti-cles and one of three modality tags, ?opinion?, ?de-cision?
and ?description?
was applied to each sen-tence.
The data size was 1135 sentences consist-ing of 123 sentences of ?opinion?, 326 of ?decision?and 686 of ?description?.
We evaluated the resultsby using 5-fold cross validation.7 Results and DiscussionTables 3 and 4 show the results of Japanese and En-glish question classification, respectively.
Table 5shows the results of sentence modality identifica-tion.
n in each table indicates the threshold of thesub-sequence size.
n = ?
means all possible sub-sequences are used.First, SK was consistently superior to BOW-K.This indicates that the structural features were quiteefficient in performing these tasks.
In general wecan say that the use of structural features can im-prove the performance of NLP tasks that require thedetails of the contents to perform the task.Most of the results showed that SK achieves itsmaximum performance when n = 2.
The per-formance deteriorates considerably once n exceeds4.
This implies that SK with larger sub-structuresdegrade classification performance.
These resultsshow the same tendency as the previous studies dis-cussed in Section 3.
Table 6 shows the precision andrecall of SK when n = ?.
As shown in Table 6, theclassifier offered high precision but low recall.
Thisis evidence of over-fitting in learning.As shown by the above experiments, FSSK pro-Table 6: Precision and recall of SK: n = ?Precision Recall FMI:Opinion .917 .209 .339JQA:LOCATION .896 .093 .168vided consistently better performance than the con-ventional methods.
Moreover, the experiments con-firmed one important fact.
That is, in some casesmaximum performance was achieved with n =?.
This indicates that sub-sequences created us-ing very large structures can be extremely effective.Of course, a larger feature space also includes thesmaller feature spaces, ?n ?
?n+1.
If the perfor-mance is improved by using a larger n, this meansthat significant features do exist.
Thus, we can im-prove the performance of some classification prob-lems by dealing with larger substructures.
Even ifoptimum performance was not achieved with n =?, difference between the performance of smallern are quite small compared to that of SK.
This indi-cates that our method is very robust as regards sub-structure size; It therefore becomes unnecessary forus to decide sub-structure size carefully.
This in-dicates our approach, using large sub-structures, isbetter than the conventional approach of eliminatingsub-sequences based on size.8 ConclusionThis paper proposed a statistical feature selectionmethod for convolution kernels.
Our approach canselect significant features automatically based on astatistical significance test.
Our proposed methodcan be embedded in the DP based kernel calcula-tion process for convolution kernels by using sub-structure mining algorithms.Table 4: Results of English question classification (Accuracy)(a) coarse (b) finenFSSK1FSSK2SKBOW-K1 2 3 4 ?- .908 .914 .916 .912- .902 .896 .902 .906- .912 .914 .912 .892.728 .836 .864 .858 -1 2 3 4 ?- .852 .854 .852 .850- .858 .856 .854 .854- .850 .840 .830 .796.754 .792 .790 .778 -Table 5: Results of sentence modality identification (F-measure)(a) opinion (b) decision (c) descriptionnFSSK1FSSK2SKBOW-K1 2 3 4 ?- .734 .743 .746 .751- .740 .748 .750 .750- .706 .672 .577 .058.507 .531 .438 .368 -1 2 3 4 ?- .828 .858 .854 .857- .824 .855 .859 .860- .816 .834 .830 .339.652 .708 .686 .665 -1 2 3 4 ?- .896 .906 .910 .910- .894 .903 .909 .909- .902 .913 .910 .808.819 .839 .826 .793 -Experiments show that our method is superior toconventional methods.
Moreover, the results indi-cate that complex features exist and can be effective.Our method can employ them without over-fittingproblems, which yields benefits in terms of conceptand performance.ReferencesN.
Cancedda, E. Gaussier, C. Goutte, and J.-M.Renders.
2003.
Word-Sequence Kernels.
Jour-nal of Machine Learning Research, 3:1059?1082.M.
Collins and N. Duffy.
2001.
Convolution Ker-nels for Natural Language.
In Proc.
of Neural In-formation Processing Systems (NIPS?2001).C.
Cortes and V. N. Vapnik.
1995.
Support VectorNetworks.
Machine Learning, 20:273?297.D.
Haussler.
1999.
Convolution Kernels on Dis-crete Structures.
In Technical Report UCS-CRL-99-10.
UC Santa Cruz.T.
Joachims.
1998.
Text Categorization with Sup-port Vector Machines: Learning with Many Rel-evant Features.
In Proc.
of European Conferenceon Machine Learning (ECML ?98), pages 137?142.T.
Kudo and Y. Matsumoto.
2002.
Japanese Depen-dency Analysis Using Cascaded Chunking.
InProc.
of the 6th Conference on Natural LanguageLearning (CoNLL 2002), pages 63?69.T.
Kudo and Y. Matsumoto.
2003.
Fast Methods forKernel-based Text Analysis.
In Proc.
of the 41stAnnual Meeting of the Association for Computa-tional Linguistics (ACL-2003), pages 24?31.X.
Li and D. Roth.
2002.
Learning Question Clas-sifiers.
In Proc.
of the 19th International Con-ference on Computational Linguistics (COLING2002), pages 556?562.H.
Lodhi, C. Saunders, J. Shawe-Taylor, N. Cris-tianini, and C. Watkins.
2002.
Text ClassificationUsing String Kernel.
Journal of Machine Learn-ing Research, 2:419?444.S.
Morishita and J. Sese.
2000.
Traversing Item-set Lattices with Statistical Metric Pruning.
InProc.
of ACM SIGACT-SIGMOD-SIGART Symp.on Database Systems (PODS?00), pages 226?236.J.
Pei, J. Han, B. Mortazavi-Asl, and H. Pinto.2001.
PrefixSpan: Mining Sequential PatternsEfficiently by Prefix-Projected Pattern Growth.In Proc.
of the 17th International Conference onData Engineering (ICDE 2001), pages 215?224.M.
Rogati and Y. Yang.
2002.
High-performingFeature Selection for Text Classification.
InProc.
of the 2002 ACM CIKM International Con-ference on Information and Knowledge Manage-ment, pages 659?661.J.
Suzuki, T. Hirao, Y. Sasaki, and E. Maeda.2003a.
Hierarchical Directed Acyclic Graph Ker-nel: Methods for Natural Language Data.
InProc.
of the 41st Annual Meeting of the Associ-ation for Computational Linguistics (ACL-2003),pages 32?39.J.
Suzuki, Y. Sasaki, and E. Maeda.
2003b.
Kernelsfor Structured Natural Language Data.
In Proc.of the 17th Annual Conference on Neural Infor-mation Processing Systems (NIPS2003).
