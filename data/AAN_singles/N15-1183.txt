Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1597?1605,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsHierarchic syntax improves reading time predictionMarten van Schijndel William SchulerDepartment of LinguisticsThe Ohio State University{vanschm,schuler}@ling.osu.eduAbstractPrevious work has debated whether humansmake use of hierarchic syntax when process-ing language (Frank and Bod, 2011; Fos-sum and Levy, 2012).
This paper uses aneye-tracking corpus to demonstrate that hier-archic syntax significantly improves readingtime prediction over a strong n-gram baseline.This study shows that an interpolated 5-grambaseline can be made stronger by combiningn-gram statistics over entire eye-tracking re-gions rather than simply using the last n-gramin each region, but basic hierarchic syntacticmeasures are still able to achieve significantimprovements over this improved baseline.1 IntroductionIn NLP, a concern exists that models of hierarchicsyntax may be increasingly used exclusively to com-pensate for n-gram sparsity (Lease et al, 2006).
Inthe context of psycholinguistic modeling, Frank andBod (2011) find that hierarchic measures of syntac-tic processing are not as good at predicting readingtimes as sequential part-of-speech-based models ofprocessing.1Fossum and Levy (2012) follow up onthis finding and show that, when better n-gram in-formation is present in the models, measures of hi-erarchic syntactic processing cost (PCFG surprisal;Hale, 2001; Levy, 2008) are as good at predictingreading times as the sequential models presented byFrank and Bod.1Frank and Bod (2011) find that hierarchic measures signifi-cantly improve the descriptive linguistic accuracy of models butthat such measures are unable to improve upon a strong linearbaseline when predicting reading times.The present study builds on this finding by show-ing that cumulative n-gram probabilities signifi-cantly improve an n-gram baseline to better cap-ture sequential frequency statistics.
Further, thisstudy shows that measures of hierarchic structuralfrequencies (as captured by PCFG surprisal) signif-icantly improve reading time predictions over thatimproved sequential baseline.First, this work defines a stronger n-gram base-line than that used in previous studies by replacing abigram baseline computed from 101 million wordswith an interpolated 5-gram baseline computed over2.96 billion words.
Second, while previous work hasused n-grams from the end of each eye-movementregion to model reading times in that region, this pa-per finds that such models can be significantly im-proved by combining n-gram statistics over the en-tire region (Section 3).
Even when this improvedbaseline is combined with a standard n-gram base-line, this paper demonstrates that PCFG surprisal isa significant predictor of reading times (Section 4).This paper also applies region accumulation to totalsurprisal and finds that it is not significantly betterthan non-accumulated total surprisal.
In fact, cumu-lative surprisal is shown not to be a significant pre-dictor of reading times at all when a cumulative n-gram factor is included in the baseline.
Finally, thispaper compares two different models of hierarchicsyntax: the Penn Treebank (PTB) representation(Marcus et al, 1993) and the psycholinguistically-motivated Nguyen et al (2012) Generalized Cate-gorial Grammar (GCG).
Each model of syntax isshown to provide orthogonal improvements to read-ing time predictions (Section 5).1597FactorsDurationsRw4w4Rw6w5Bigram P(w4|w3) P(w6|w5)Cumu-Bigram P(w4|w3) P(w6|w5)?P(w5|w4)Table 1: Bigram factors and their predictions ofreading times in example eye-tracking regions.
wirepresents word i. Rwjwirepresents the region fromwito wj(inclusive).2 ModelingThis study fits models to reading times from theDundee corpus (Kennedy et al, 2003), which con-sists of eye-tracking data from 10 subjects who read2388 sentences of news text from the newspaper,The Independent.
Prior to using this corpus for eval-uations, the first and last fixation of each sentenceand of each line are filtered out to avoid potentiallyconfounding wrap-up effects.
Additionally, all fixa-tions after saccades (eye movements) over more than4 words are removed to avoid confounds with eye-tracker track-loss.All evaluations are done with linear mixed ef-fects models using lme4 (version 1.1-7; Bates etal., 2014).2There are two dependent reading timevariables of interest in this study: first pass dura-tions and go-past durations.
During reading, a per-son?s eye can jump over multiple words each timeit moves, this study refers to that span of words asa region.
First pass durations measure elapsed timeuntil a person?s eye leaves a given region.
Go-pastdurations measure elapsed time until a person?s eyemoves further in the text.
For example, in the fixa-tion sequence: word 4, word 6, word 3, word 7, thefirst region would be from word 4 to word 6 and thesecond region would be from word 6 to word 7.
Thefirst pass duration for the first region would consistof the time fixated on word 6 before leaving the re-gion for word 3, while the go-past duration wouldconsist of the duration from the fixation of word 6until the fixation of word 7.
Separate models are fitto each centered dependent variable.There are a number of independent variables inall evaluations in this study: sentence position (sent-2The models are fit using both the default bobyqa and thegradient nlminb algorithms to work around convergence issues.Bigram: The1red apple that the2girl ate .
.
.Cumu-Bigram: The1red apple that the2girl ateX : bigram targets X: bigram conditionsTable 2: Influences on bigram factor predictions ofreading times on girl following fixation on red.pos), word length (wlen), region length in words(rlen), whether the previous word was fixated (pre-vfix), and basic 5-gram log probability of the cur-rent word given the preceding context (5-gram).
Allindependent predictors are centered and scaled be-fore being added to each model.
The 5-gram prob-abilities are interpolated 5-grams computed over theGigaword 4.0 corpus (Graff and Cieri, 2003) usingKenLM (Heafield et al, 2013).
Gigaword 4.0 con-sists of around 2.96 billion words from around 4 mil-lion English newswire documents, which providesappropriate n-gram statistics since the Dundee cor-pus is also English news text.Each mixed effects model contains random inter-cepts for subject and word, and random by-subjectslopes for all fixed effects.
Since the following eval-uations use ablative testing to determine whether afixed effect significantly improves the fit of a modelcompared to a model without that fixed effect, allmodels in a given evaluation include random slopesfor all fixed effects used in that evaluation, even ifthe fixed effect is absent from that particular model.3 A Cumulative N-gram PredictorSince n-gram frequencies can have such a dramaticimpact on the contribution of hierarchic syntax, thisstudy tests whether n-gram factors can be improved.Models include a measure of n-gram frequencies tocapture the rarity of observed sequences.
Readersfixate longer on less predictable lexemes than onmore predictable lexemes, but the predictability ofa lexeme depends on the preceding context.
There-fore, it is common for psycholinguistic models toinclude a measure of n-gram predictability for eachfixated word conditioned on its context, but unlessprobabilities for words between fixations are also in-cluded, the probabilities used in this calculation are1598ModelFirst Pass Go-PastLog-Likelihood AIC Log-Likelihood AICBaseline ?1212399 2424868 ?1261582 2523234Base+N-gram ?1212396?2424864 ?1261577?2523226Base+Cumu-N-gram ?1212392?2424856 ?1261576?2523224Base+Both ?1212387?2424848 ?1261570?2523214Baseline random slopes: sentpos, wlen, rlen, prevfix, 5-gram, cumu-5-gramBaseline fixed effects: sentpos, wlen, rlen, prevfixTable 3: Goodness of fit of N-gram models to reading times.3Significance testing was done between eachmodel and the models in the section above it.
Significance for Base+Both applies to improvement over eachof the n-gram models.
?p < .05?p < .01not probabilities of complete word sequences andmay miss words that are parafovially previewed orsimply inferred.For example, in Table 1, the standard bigram fac-tor (top line) predicts that the reading time of theregion that ends with word 6 depends on word 5, butthe probability of word 5 given its context is neverincluded in the model, so an improbable transitionbetween words 4 and 5 would not be caught.
Thismight allow another factor to inappropriately receivecredit for an extra long fixation on word 6.
Instead,a better model would include the probabilities of ev-ery word in the sequence since that is the informa-tion that will need to be processed by the reader.
Us-ing log-probabilities, a cumulative n-gram factor canbe created simply by summing the log probabilitiesover each region (comparable to the last line of Ta-ble 1).
The cumulative n-gram predictor is able toaccount for the frequency of the entire lexical se-quence and so should provide a better reading timepredictor than the standard fixation-only n-gram pre-dictor (see Table 2 for an example).For this initial evaluation (Table 3), the baselineomits the fixed n-gram factor.
Instead, a model isconstructed without any fixed effects for n-gram.Then, the same model is fit to reading times afteradding just a fixed effect for n-gram and after addingjust a fixed effect for cumulative n-gram.
Finally,a model is fit with both the cumulative and non-cumulative n-gram factors as fixed effects.4Signifi-3Log-likelihood values are rounded to the nearest wholenumber, which is why the difference between Base andBase+Both can be larger than the cumulative difference be-tween Base and the other two models.4To ensure effects are not driven by individual subject differ-cance between the models is determined using like-lihood ratio testing.5Table 3 shows that both n-gram factors signifi-cantly improve the fit of the model and the final lineshows that each factor provides a significant orthog-onal improvement.
Both n-gram factors will there-fore be included as fixed effects and as by-subjectrandom slopes in the baselines of the remainingevaluations in this study.4 Hierarchic Syntax PredictorsThis section tests the main hypothesis of this study:that hierarchic syntactic processing is a significantcontributor to reading times.
For the purposes of thisevaluation, total PCFG surprisal (Hale, 2001; Levy,2008; Roark et al, 2009) will be used as a mea-sure of hierarchic syntactic processing.
Specifically,PCFG surprisal will be calculated using the vanSchijndel et al (2013a) incremental parser trainedon sections 02-21 of the Wall Street Journal sectionof the Penn Treebank (Marcus et al, 1993) using 5iterations of split-merge (Petrov et al, 2006) and abeam width of 5000.ences, by-subject random slopes for both predictors of interestare included in the baseline.
This practice is repeated through-out this study.5Twice the log-likelihood difference of two nested modelscan be approximated by a ?2distribution with degrees of free-dom equal to the difference in degrees of freedom of the modelsin question.
The probability of obtaining a given log-likelihooddifference D between the two models is therefore analogous toP(2 ?
D) under the corresponding ?2distribution.1599FactorsDurationsRw4w4Rw6w5surp ?log P(w4|T3) ?log P(w6|T5)cumusurp ?log P(w4|T3)?6i=5?log P(wi|Ti?1)Table 4: PCFG surprisal factors and their predictionsof reading times in example eye-tracking regions.
wirepresents word i. Tirepresents the set of trees thatcan span from w1to wi.
Rwjwirepresents the regionfrom wito wj(inclusive).4.1 SurprisalPCFG surprisal (Hale, 2001; Levy, 2008) is a mea-sure of incremental hierarchic syntactic processing.It reflects the information gained by observing agiven word in a given context.
In PCFG surprisalcalculations, context is usually taken to refer to thepreceding words in the sentence and their underly-ing syntactic structure.
The PCFG surprisal S(wi)of a word at position i may be calculated as:S(wi) =?t?Ti?1?log P(wi| t) (1)where Tirepresents the set of syntactic structuresthat can span from w1to wi.
PCFG surprisal in psy-cholinguistic models captures the influence of incre-mental hierarchic context when processing a givenword.For space considerations, in Table 4, the summa-tion over Ti?1is notationally implicit:S(wi) = ?log P(wi| Ti?1) (2)4.2 EvaluationAs in the previous section, a baseline model is fitto reading times without a fixed effect for surprisal,then surprisal is added as a fixed effect and signifi-cance of the fixed effect is determined using a like-lihood ratio test with the baseline.
The results (Ta-ble 5) show that PCFG surprisal is a significant pre-dictor of both first pass and go-past durations evenover a strong baseline including both types of n-gram factors.The preceding section showed that applying re-gion accumulation to an n-gram factor improves amodel?s fit to reading times.
Previous work sug-gests region accumulation might improve the fit ofsyntactic factors to reading times (van Schijndel andSchuler, 2013; van Schijndel et al, 2013b), but thebaselines in those studies only included unigram andbigram statistics and did not apply region accumula-tion to the n-gram models.
It does make intuitivesense that region accumulation would help improvethe fit of total PCFG surprisal for the same reason ac-cumulating n-grams helps.
For an example, see Ta-ble 4.
A non-cumulative total PCFG surprisal factor(top line) would predict that duration of region Rw6w5depends on T5(the set of trees that can span fromw1to w5), but the probability of generating the prefix ofT5is never fully calculated by this factor.
As withcumulative n-grams, cumulative PCFG surprisal ofa region can be calculated by simply summing thePCFG surprisal of each word in the region.When tested, however, the present work does notfind any improvement from region accumulation ofPCFG surprisal when stronger n-gram factors arealso included (Table 5, Row 2), suggesting that theimprovement in previous studies may have been dueto latent n-gram information captured by cumulativePCFG surprisal.
This finding is interesting becauseit suggests non-local hierarchic structure does notsignificantly influence reading times.
The next sec-tion explores this hypothesis further by testing the fitof a hierarchic syntactic formalism whose strengthlies in modeling long-distance dependencies.5 Grammar Formalism ComparisonSo far, this study has tried to allay previous concernsthat models of hierarchic syntax may just be ac-counting for the sparsity of n-gram statistics (Char-niak et al, 2006; Frank and Bod, 2011).
This sec-tion investigates whether a representation of hierar-chic syntax that preserves long-distance dependen-cies can improve reading time predictions over a hi-erarchic representation based on the Penn Treebankwhich discards long-distance dependencies.
Thisevaluation compares total PCFG surprisal as calcu-lated by the original Penn Treebank grammar to to-tal PCFG surprisal calculated by the Nguyen et al(2012) Generalized Categorial Grammar (GCG).5.1 GCGA GCG has a category set C, which consists of aset of primitive category types U , typically labeled1600ModelFirst Pass Go-PastLog-Likelihood AIC Log-Likelihood AICBaseline ?1212260 2424627 ?1261488 2523084Base+Surp ?1212253?2424617 ?1261481?2523072Base+CumuSurp ?1212259 2424627 ?1261487 2523085Base+Both ?1212253?2424619 ?1261481?2523073Baseline random slopes: sentpos, wlen, rlen, prevfix, 5-gram, cumu-5-gram, surp, cumusurpBaseline fixed effects: sentpos, wlen, rlen, prevfix, 5-gram, cumu-5-gramTable 5: Goodness of fit of hierarchic syntax models to reading times.
Significance testing was done betweeneach model and the models in the section above it.
Significance for Base+Both applies only to improvementover the CumuSurp model.
?p < .01with the part of speech of the head of a category (e.g.V, N, A, etc., for phrases or clauses headed by verbs,nouns, adjectives, etc.
), followed by one or more un-satisfied dependencies, each consisting of an opera-tor (-a and -b for adjacent argument dependenciespreceding and following a head, -c and -d for adja-cent conjunct dependencies preceding and followinga head, -g for filler-gap dependencies, -r for relativepronoun dependencies, and some others), followedby a dependent category type.
For example, the cat-egory for a transitive verb would be V-aN-bN, sinceit is headed by a verb and has unsatisfied dependen-cies to satisfied noun-headed categories precedingand following it (for the subject and direct objectnoun phrase, respectively).As in other categorial grammars, inference rulesfor local argument attachment apply functors of cat-egory c-ad or c-bd to initial or final arguments ofcategory d:d c-ad?
c (Aa)c-bd d?
c (Ab)However, the Nguyen et al (2012) GCG uses dis-tinguished inference rules for modifier attachment,which allows modifier categories to be consolidatedwith categories for modifiers in other contexts (pre-verbal, post-verbal, etc.
), and with certain predica-tive categories.
This allows derivations in the train-ing corpus involving different modifier types to alsobe consolidated, which increases the power of theextracted statistics.
Inference rules for modifier at-tachment apply initial or final modifiers of cate-gory u-ad to modificands of category c, for u ?
Uand c, d ?
C:u-ad c?
c (Ma)c u-ad?
c (Mb)The Nguyen et al (2012) GCG also uses distin-guished inference rules to introduce, propagate, andbind missing non-local arguments, similar to the gapor slash rules of Generalized Phrase Structure Gram-mar (Gazdar et al, 1985) and Head-driven PhraseStructure Grammar (Pollard and Sag, 1994).
Infer-ence rules for gap attachment hypothesize gaps asinitial arguments, final arguments, or modifiers, forc, d ?
C:c-ad?
c-gd (Ga)c-bd?
c-gd (Gb)c?
c-gd (Gc)Non-local arguments, using non-local operator andargument category ??
{-g, -h, -i, -r}?C, are thenpropagated to the consequent from all possible com-binations of antecedents.
For each rule d e ?
c ?
{Aa?b,Ma?b} :d e?
?
c?
(Ac?d,Mc?d)d?
e?
c?
(Ae?f,Me?f)d?
e?
?
c?
(Ag?h,Mg?h)In order to consolidate relative and interrogativepronouns in different pied-piping contexts into justtwo reusable categories, this grammar uses distin-guished inference rules for relative and interrogativepronouns as well as tough constructions (e.g.
this1601ModelFirst Pass Go-PastLog-Likelihood AIC Log-Likelihood AICBaseline ?1212242 2424592 ?1261474 2523055Base+PTB ?1212239?2424587 ?1261468?2523047Base+GCG ?1212239?2424589 ?1261470?2523050Base+Both ?1212235?2424583 ?1261465?2523043Baseline random slopes: sentpos, wlen, rlen, prevfix, 5-gram, cumu-5-gram, surp-GCG, surp-PTBBaseline fixed effects: sentpos, wlen, rlen, prevfix, 5-gram, cumu-5-gramTable 6: Goodness of fit of models with differing syntactic calculations to reading times.
Significance testingwas done between each model and the models in the section above it.
Base+Both first pass significanceapplies to improvement over PTB (p < .05) and to improvement over GCG (p < .01), Base+Both go-pastsignificance applies to improvement over each independent model.
?p < .05?p < .01bread is easy to cut), which introduce clauses withgap dependencies, for c, d, e ?
C, ??
{-g}?C:d-ie c-gd?
c-ie (Fa)d-re c-gd?
c-re (Fb)c-b(d?)
d?
?
c (Fc)Also, inference rules for relative pronoun attach-ment apply pronominal relative clauses of cate-gory c-rd to modificands of category e:e c-rd?
e (R)Because of its richer set of language-specific infer-ence rules, the GCG grammar annotated by Nguyenet al (2012) does not require different categories forwords like which in different pied-piping contexts:cafesNwhichN-rNwe ate inV-gNV-rNFbNRcafesNinR-aN-bNwhichN-rNR-aN-rNAbwe ateVV-g(R-aN)GcV-rNFbNR5.2 EvaluationFollowing van Schijndel et al (2013b), the GCGcalculation of PCFG surprisal comes from a GCG-reannotated version of the Penn Treebank whosegrammar rules have undergone 3 iterations of thesplit-merge algorithm (Petrov et al, 2006).
A k-bestbeam with a width of 5000 is used in order to becomparable to the PTB calculation.Significance testing is done as in the precedingevaluations: a baseline model is fit to reading times,each PCFG surprisal factor is added independentlyto the baseline, and both PCFG surprisal factors areadded concurrently to the baseline.
Each model iscompared to the next simpler models using likeli-hood ratio tests.The results (Table 6) show that GCG PCFG sur-prisal is a significant predictor of reading timeseven in the presence of the stronger n-gram base-line.
Moreover, both PTB and GCG PCFG sur-prisal significantly improve reading time predictionseven when the other PCFG surprisal measure is alsoincluded.
This suggests that each is contributingsomething the other is not.
Since the GCG gram-mar is derived from an automatically reannotatedversion of the Penn Treebank, there may be errorsin the GCG annotation which cause errors in the es-timates of underlying GCG structure.
Since the PTBgrammar is manually annotated by experts, the PTBgrammar may be receiving credit for correct struc-tural prediction in cases where GCG?s estimates areincorrect.
However, it seems likely that GCG maybe providing a better fit in cases of long-distance de-pendencies because such relations are omitted fromthe PTB grammar.A follow-up evaluation (not shown here) using theexperimental design from Section 4 but using GCGPCFG surprisal rather than PTB PCFG surprisal re-vealed that cumulative PCFG surprisal is still nota significant predictor when calculated using GCG.The failure of cumulative PCFG surprisal to improveover basic GCG PCFG surprisal could be expected1602PredictorFirst Pass Go-Pastcoef t value coef t valuesentpos ?2.47 ?3.59 ?2.82 ?3.38wlen 25.90 8.67 28.98 9.97prevfix ?30.16 ?7.81 ?37.42 ?11.49n-gram ?2.39 ?1.81 ?6.70 ?3.36cumu-n-gram ?14.69 ?7.36 ?11.68 ?5.01rlen ?5.67 ?1.31 ?12.51 ?2.59surp-GCG 4.97 2.87 5.74 2.73surp-PTB 4.20 3.23 4.85 3.29Table 7: Fixed effect predictor coefficients for Base+PTB+GCG model.since a strength of GCG is in enabling non-local de-cisions on a local basis (by propagating non-localdecisions into the category labels), so any non-localadvantage cumulative PCFG surprisal might conferis already compressed into the GCG categories.The results of this evaluation suggest that readingtimes are mostly affected by local hierarchic struc-ture, but the fact that GCG PCFG surprisal is able toprovide a significant fit even in the presence of thePTB PCFG surprisal predictor suggests that somenon-local information affects reading times.
In par-ticular, while this evaluation showed that accumu-lated syntactic context is not generally a good pre-dictor of reading times, some or all of the non-localinformation contained in the GCG categories is usedby readers and so influences reading time durationsover the local structural information reflected in thePTB PCFG surprisal measure.6 DiscussionThe finding that the hierarchic grammars orthogo-nally improve reading time predictions suggests thathierarchic structural information has a significant in-fluence on reading times.
Since both the PTB andGCG calculations of surprisal contain sequential in-formation (e.g., of part-of-speech tags), if the effectin this study was driven by purely sequential infor-mation as suggested by Frank and Bod (2011), onemight expect either the PTB or the GCG calculationsof surprisal (but not both) to be a significant predic-tor of reading times.Instead, the present set of results support recentclaims made by van Schijndel et al (2014) that non-local subcategorization decisions are made earlyduring processing and so would have a strong in-fluence on the reading time measures used in thepresent study.
Such decisions would have to beconditioned on hierarchic structural information notpresent in either PTB PCFG surprisal or the sequen-tial structure models of Frank and Bod (2011).Further, predictability has been shown to affectword duration during speech production (Jurafskyet al, 2001; Aylett and Turk, 2006), and Demberget al (2012) found that hierarchic structure signif-icantly improves over n-gram computations of pre-dictability in that domain as well.
Together, thesefindings suggest that hierarchic structure is not onlya convenient descriptive tool for linguists, but thatsuch structure is deeply rooted in the human lan-guage processor and is used during online languageprocessing.Previous work has made a distinction betweenlexical surprisal, syntactic surprisal, and total sur-prisal (Demberg and Keller, 2008; Roark et al,2009).
Given a prefix derivation of the structure ofthe context, syntactic surprisal measures the infor-mation obtained by generating the structure that willenable the attachment of a newly observed lexicalitem.
Lexical surprisal conveys the amount of infor-mation obtained by attaching the particular lexicalobservation to the new syntactic structure.
Total sur-prisal is the original formulation of surprisal and isthe composition of the other two types of surprisal(the information gained by generating a structure forthe current lexical observation and attaching the ob-servation to that structure).
Fossum and Levy (2012)show that, with a non-cumulative bigram baseline,this distinction is not significant when predicting1603reading times, so the present study simply uses to-tal surprisal.
It may be interesting in future workto see if the distinction between surprisal types be-comes more or less useful as the sequential baselineimproves.The finding that cumulative n-gram informationis useful in predicting reading times bears some re-semblance to the finding that the spillover effect ofa word is proportional to its logarithmic probabil-ity given the context (Smith and Levy, 2013).
How-ever, the spillover effect studied by Smith and Levy(2013) is one of a given fixation on the followingfixation.
The cumulative n-grams, in contrast, per-mit finer predictability of a word given the unfix-ated intervening context.
The two measures are sim-ilar in that they both permit better modeling of thepredictability of a word given its context, but thespillover measure could also be easily conceived ascontinued spillover processing from the precedingfixation, while cumulative n-grams reflect the pre-dictability of the entire region between one fixationand the next.
Further, cumulative n-grams couldconceivably also capture processing of parafovialpreview obtained during the previous fixation.
Sincethe cumulative n-gram measure improves the com-putation of predictability of a word, it could alsoprovide a better measure of the spillover effect agiven word will have.
Future work could investigatethis by using cumulative n-grams both to computethe predictability of the current word and to predictthe spillover effect from the preceding fixation.
Thepresent work suggests that doing so would provideeven better reading time predictors.7 ConclusionFirst, this work suggests that the standard account-ing for n-gram frequencies needs to change in psy-cholinguistic studies.
Currently, the standard proce-dure is to use n-gram statistics only from the end ofan eye-tracking region.
This standard calculates theinfluence of the final word in each region given thelexical context, but that context is never accountedfor in regions greater than one word in length.
In-stead, psycholinguistic models need to additionallyaccount for the probability of the context given itsown preceding context to provide a coherent modelof the probability of the observed lexical sequence.This work also shows that, even with good cumu-lative and non-cumulative estimates of the frequencyeffects generated by a given lexical sequence, mea-sures of hierarchic structure provide a significantimprovement to reading time predictions.
Further,even in the presence of both a strong n-gram base-line and a linguistically accurate measure of hierar-chic structure (PTB with 5 iterations of split-merge),a linguistically-motivated model of hierarchic struc-ture is a significant predictor of reading times.
Asdata coverage grows, some may worry that modelsof syntax will be superseded by better n-gram mod-els.
This study suggests that hierarchic syntax re-tains its value even in a world of big data.AcknowledgementsThanks to Stefan Frank for interesting discussionand helpful feedback on an earlier draft of this pa-per and to the anonymous reviewers for their com-ments.
This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant No.
DGE-1343012.Any opinion, findings, and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof the National Science Foundation.ReferencesMatthew Aylett and Alice Turk.
2006.
Language redun-dancy predicts syllabic duration and the spectral char-acteristics of vocalic syllable nuclei.
Journal of theacoustical society of America, 119(5):3048?3059.Douglas Bates, Martin Maechler, Ben Bolker, and StevenWalker, 2014. lme4: Linear mixed-effects models us-ing Eigen and S4.
R package version 1.1-7.Eugene Charniak, Mark Johnson, Micha Elsner, JosephAusterweil, David Ellis, Isaac Haxton, Catherine Hill,R.
Shrivaths, Jeremy Moore, Michael Pozar, andTheresa Vu.
2006.
Multilevel coarse-to-fine pcfgparsing.
In Proceedings of the main conference onHuman Language Technology Conference of the NorthAmerican Chapter of the Association of Computa-tional Linguistics, pages 168?175.Vera Demberg and Frank Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109(2):193?210.Vera Demberg, Asad B. Sayeed, Philip J. Gorinski, andNikolaos Engonopoulos.
2012.
Syntactic surprisal1604affects spoken word duration in conversational con-texts.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 356?367.Victoria Fossum and Roger Levy.
2012.
Sequential vs.hierarchical syntactic models of human incrementalsentence processing.
In Proceedings of CMCL 2012.Association for Computational Linguistics.Stefan Frank and Rens Bod.
2011.
Insensitivity ofthe human sentence-processing system to hierarchicalstructure.
Psychological Science.Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and IvanSag.
1985.
Generalized Phrase Structure Grammar.Harvard University Press, Cambridge, MA.David Graff and Christopher Cieri, 2003.
English Giga-word LDC2003T05.John Hale.
2001.
A probabilistic earley parser as apsycholinguistic model.
In Proceedings of the secondmeeting of the North American chapter of the Associ-ation for Computational Linguistics, pages 159?166,Pittsburgh, PA.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark,and Philipp Koehn.
2013.
Scalable modified Kneser-Ney language model estimation.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, pages 690?696, Sofia, Bulgaria,August.Daniel Jurafsky, Alan Bell, Michelle Gregory, andWilliam D. Raymond.
2001.
Probabilistic relationsbetween words: Evidence from reduction in lexicalproduction.
In Joan Bybee and Paul Hopper, editors,Frequency and the emergence of linguistic structure,pages 229?254.
John Benjamins, Amsterdam.Alan Kennedy, James Pynte, and Robin Hill.
2003.
TheDundee corpus.
In Proceedings of the 12th Europeanconference on eye movement.Matthew Lease, Eugene Charniak, Mark Johnson, andDavid McClosky.
2006.
A look at parsing and its ap-plications.
In Proceedings of AAAI.Roger Levy.
2008.
Expectation-based syntactic compre-hension.
Cognition, 106(3):1126?1177.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Luan Nguyen, Marten van Schijndel, and WilliamSchuler.
2012.
Accurate unbounded dependency re-covery using generalized categorial grammars.
InProceedings of the 24th International Conferenceon Computational Linguistics (COLING ?12), pages2125?2140, Mumbai, India.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 44thAnnual Meeting of the Association for ComputationalLinguistics (COLING/ACL?06).Carl Pollard and Ivan Sag.
1994.
Head-driven PhraseStructure Grammar.
University of Chicago Press,Chicago.Brian Roark, Asaf Bachrach, Carlos Cardenas, andChristophe Pallier.
2009.
Deriving lexical and syn-tactic expectation-based measures for psycholinguisticmodeling via incremental top-down parsing.
Proceed-ings of the 2009 Conference on Empirical Methods inNatural Langauge Processing, pages 324?333.Nathaniel J. Smith and Roger Levy.
2013.
The effectof word predictability on reading time is logarithmic.Cognition, 128:302?319.Marten van Schijndel and William Schuler.
2013.
Ananalysis of frequency- and recency-based processingcosts.
In Proceedings of NAACL-HLT 2013.
Associa-tion for Computational Linguistics.Marten van Schijndel, Andy Exley, and William Schuler.2013a.
A model of language processing as hierarchicsequential prediction.
Topics in Cognitive Science,5(3):522?540.Marten van Schijndel, Luan Nguyen, and WilliamSchuler.
2013b.
An analysis of memory-based pro-cessing costs using incremental deep syntactic depen-dency parsing.
In Proc.
of CMCL 2013.
Associationfor Computational Linguistics.Marten van Schijndel, William Schuler, and Peter WCulicover.
2014.
Frequency effects in the processingof unbounded dependencies.
In Proc.
of CogSci 2014.Cognitive Science Society.1605
