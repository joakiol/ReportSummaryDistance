Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 269?272,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPInvestigating Pitch Accent Recognition in Non-native SpeechGina-Anne LevowComputer Science DepartmentUniversity of Chicagoginalevow@gmail.comAbstractAcquisition of prosody, in addition to vo-cabulary and grammar, is essential for lan-guage learners.
However, it has receivedless attention in instruction.
To enableautomatic identification and feedback onlearners?
prosodic errors, we investigateautomatic pitch accent labeling for non-native speech.
We demonstrate that anacoustic-based context model can achieveaccuracies over 79% on binary pitch ac-cent recognition when trained on within-group data.
Furthermore, we demonstratethat good accuracies are achieved in cross-group training, where native and near-native training data result in no significantloss of accuracy on non-native test speech.These findings illustrate the potential forautomatic feedback in computer-assistedprosody learning.1 IntroductionAcquisition of prosody, in addition to vocabularyand grammar, is essential for language learners.However, intonation has been less-emphasizedboth in classroom and computer-assisted languageinstruction (Chun, 1998).
Outside of tone lan-guages, it can be difficult to characterize the fac-tors that lead to non-native prosody in learnerspeech, and it is difficult for instructors to find timefor the one-on-one interaction that is required toprovide feedback and instruction in prosody.To address these problems and enable automaticfeedback to learners in a computer-assisted lan-guage learning setting, we investigate automaticprosodic labelling of non-native speech.
Whilemany prior systems (Teixeia et al, 2000; Tep-perman and Narayanan, 2008) aim to assign ascore to the learner speech, we hope to providemore focused feedback by automatically identify-ing prosodic units, such as pitch accents in Englishor tone in Mandarin, to enable direct comparisonwith gold-standard native utterances.There has been substantial progress in auto-matic pitch accent recognition for native speech,achieving accuracies above 80% for acoustic-feature based recognition in multi-speaker cor-pora (Sridhar et al, 2007; Levow, 2008).
How-ever, there has been little study of pitch accentrecognition in non-native speech.
Given the chal-lenges posed for automatic speech recognition ofnon-native speech, we ask whether recognition ofintonational categories is practical for non-nativespeech.
To lay the foundations for computer-assisted intonation tutoring, we ask whether com-petitive accuracies can be achieved on non-nativespeech.
We further investigate whether goodrecognition accuracy can be achieved using rel-atively available labeled native or near-nativespeech, or whether it will be necessary to col-lect larger amounts of training or adaptation datamatched for speaker, language background, or lan-guage proficiency.We employ a pitch accent recognition approachthat exploits local and coarticulatory context toachieve competitive pitch accent recognition accu-racy on native speech.
Using a corpus of prosod-ically labelled native and non-native speech, weillustrate that similar acoustic contrasts hold forpitch accents in both native and non-native speech.These contrasts yield competitive accuracies onbinary pitch accent recognition using within-grouptraining data.
Furthermore, there is no significantdrop in accuracy when models trained on native ornear-native speech are employed for classificationof non-native speech.The remainder of the paper is organized as fol-lows.
We present the LeaP Corpus used for ourexperiments in Section 2.
We next describe thefeature sets employed for classification (Section 3)and contrastive acoustic analysis for these featuresin native and non-native speech (Section 4).
We269ID Descriptionc1 non-native, before prosody trainingc2 non-native, after first prosody trainingc3 non-native, after second prosody traininge1 non-native, before going abroade2 non-native, after going abroadsl ?super-learner?, near-nativena nativeTable 1: Speaker groups, with ID and descriptionin the LeaP Corpusthen describe the classifier setting and experimen-tal results in Section 5 as well as discussion.
Fi-nally, we present some conclusions and plans forfuture work.2 LeaP Corpus and the DatasetWe employ data from the LeaP Corpus (Milde andGut, 2002), collected at the University of Biele-feld as part of the ?Learning Prosody in a For-eign Language?
project.
Details of the corpus(Milde and Gut, 2002), inter-rater reliability mea-sures (Gut and Bayerl, 2004), and other researchfindings (Gut, 2009) have been reported.Here we focus on the read English segment ofthe corpus that has been labelled with modifiedEToBI tags1, to enable better comparison withprior results of prosodic labelling accuracy andalso to better model a typical language laboratorysetting where students read or repeat.
This yieldsa total of 37 recordings of just over 300 syllableseach, from 26 speakers, as in Table 1.2This setallows the evaluation of prosodic labelling acrossa range of native and non-native proficiency lev-els.
The modified version of ETobi employed bythe LeaP annotators allows transcription of 14 cat-egories of pitch accent and 14 categories of bound-ary tone.
However, in our experiments, we will fo-cus only on pitch accent recognition and will col-lapse the inventory to the relatively standard, andmore reliably annotated, four-way (high, down-stepped high, low, and unaccented) and binary (ac-cented, unaccented) label sets.1While the full corpus includes speakers from a range oflanguages, the EToBI labels were applied primarily to datafrom German speakers.2Length of recordings varies due to differences in syllab-ification and cliticization, as well as disfluencies and readingerrors.3 Acoustic-Prosodic FeaturesRecent research has highlighted the importanceof context for both tone and intonation.
Therole of context can be seen in the characteriza-tion of pitch accents such as down-stepped highand in phenomena such as downdrift across aphrase.
Further, local coarticulation with neigh-boring tones has been shown to have a signif-icant impact on the realization of prosodic ele-ments, due to articulatory constraints (Xu andSun, 2002).
The use of prosodic and coarticu-latory context has improved the effectiveness oftone and pitch accent recognition in a range of lan-guages (Mandarin (Wang and Seneff, 2000), En-glish (Sun, 2002)) and learning frameworks (deci-sion trees (Sun, 2002), HMMs (Wang and Seneff,2000), and CRFs (Levow, 2008)).Thus, in this work, we employ a rich contextualfeature set, based on that in (Levow, 2008).
Webuild on the pitch target approximation model, tak-ing the syllable as the domain of tone predictionwith a pitch height and contour target approachedexponentially over the course of the syllable, con-sistent with (Sun, 2002).
We employ an acousticmodel at the syllable level, employing pitch, in-tensity and duration measures.
The acoustic mea-sures are computed using Praat?s (Boersma, 2001)?To pitch?
and ?To intensity.?
We log-scaled andspeaker-normalized all pitch and intensity values.We compute two sets of features: one set de-scribing features local to the syllable and one setcapturing contextual information.3.1 Local featuresWe extract features to represent the pitch heightand pitch contour of the syllable.
For pitch fea-tures, we extract the following information: (a)pitch values for five evenly spaced points in thevoiced region of the syllable, (b) pitch maximum,mean, minimum, and range, and (c) pitch slope,from midpoint to end of syllable.
We also ob-tain the following non-pitch features: (a) intensitymaximum and mean and (b) syllable duration.3.2 Context ModelingTo capture local contextual influences and cues,we employ two sets of features.
The first set of fea-tures includes differences between pitch maxima,pitch means, pitches at the midpoint of the sylla-bles, pitch slopes, intensity maxima, and intensitymeans, between the current and preceding or fol-270lowing syllable.
The second set of features addsthe last pitch values from the end of the precedingsyllable and the first from the beginning of the fol-lowing syllable.
These features capture both therelative differences in pitch associated with pitchaccent as well as phenomena such as pitch peakdelay in which the actual pitch target may not bereached until the following syllable.4 Acoustic Analysis of Native andNon-native ToneTo assess the potential effectiveness of tone recog-nition for non-native speech, we analyze and com-pare native and non-native speech with respect tofeatures used for classification that have shownutility in prior work.
Pitch accents are charac-terized not only by their absolute pitch height,but also by contrast with neighboring syllables.Thus, we compare the values for pitch and deltapitch, the difference between the current and pre-ceding syllable, both with log-scaled measures forhigh-accented and unaccented syllables.
We con-trast these values within speaker group (native: na;non-native: e1, c1).
We also compare the deltapitch measures between speaker groups (na versuse1 or c1).Not only do we find significant differences fordelta pitch between accented and unaccented syl-lables for native speakers as we expect, but wefind that non-native speakers also exhibit signif-icant differences for this measure (t-test, two-tailed,p < 0.001).
Accented syllables are reli-ably higher in pitch than immediately precedingsyllables, while unaccented syllables show no con-trast.
Importantly, we further observe a significantdifference in delta pitch for high accented sylla-bles between native and non-native speech.
Na-tive speakers employ a markedly larger change inpitch to indicate accent than do non-native speak-ers, a fine-grained view consistent with findingsthat non-native speakers employ a relatively com-pressed pitch range (Gut, 2009).For one non-native group (e1), we find that al-though these speakers produce reliable contrastsin delta pitch between neighboring syllables, theoverall pitch height of high accented syllables isnot significantly different from that of unaccentedsyllables.
For native speakers and the ?c1?
non-native group, though, overall pitch height doesdiffer significantly between accented and unac-cented syllables.
This finding suggests that whileall speakers in this data set understand the locallycontrastive role of pitch accent, some non-nativespeaker groups do not have as reliable global con-trol of pitch.The presence of these reliable contrasts betweenaccented and unaccented syllables in both na-tive and non-native speech suggests that automaticpitch accent recognition in learner speech could besuccessful.5 Pitch Accent Recognition ExperimentsWe assess the effectiveness of pitch accent recog-nition on the LeaP Corpus speech.
We hope tounderstand whether pitch accent can be accuratelyrecognized in non-native speech and whether ac-curacy rates would be competitive with those onnative speech.
In addition, we aim to compare theimpact of different sources of training data.
Weassess whether non-native prosody can be recog-nized using native or near-native training speech orwhether it will be necessary to use matched train-ing data from non-natives of similar skill level orlanguage background.Thus we perform experiments on matched train-ing and test data, training and testing withingroups of speakers.
We also evaluate cross-grouptraining and testing, training on one group ofspeakers (native and near-native) and testing onanother (non-native).
We contrast all these resultswith assignment of the dominant ?unaccented?
la-bel to all instances (common class).5.1 Support Vector Machine ClassifierFor all supervised experiments reported in this pa-per, we employ a Support Vector machine (SVM)with a linear kernel.
Support Vector Machines pro-vide a fast, easily trainable classification frame-work that has proven effective in a wide range ofapplication tasks.
For example, in the binary clas-sification case, given a set of training examplespresented as feature vectors of length D, the lin-ear SVM algorithm learns a vector of weights oflength D which is a linear combination of a sub-set of the input vectors and performs classificationbased on the function f(x) = sign(wTx?
b).
Weemploy the publicly available implementation ofSVMs, LIBSVM (C-C.Cheng and Lin, 2001).5.2 ResultsWe see that, for within group training, on thebinary pitch accent recognition task, accuracies271c1 c2 c3 e1 e2 sl naWithin-group Accuracy 79.1 80.9 80.6 81 82.5 82.4 81.2Cross-group Accuracy (na) 77.2 79 81.4 80.3 82.5 83.2Cross-group Accuracy (sl) 77.3 79.9 82 80.5 82.9 81.6Common Class 56.9 59.6 56.2 70.2 64 65.5 63.6Table 2: Pitch accent recognition, within-group, cross-group with native and near-native training, andmost common class baseline: Non-native (plain), ?Super-learner?
(underline sl), Native (bold na)range from approximately 79% to 82.5%.
Theselevels are consistent with syllable-, acoustic-feature-based prosodic recognition reported in theliterature (Levow, 2008).
A summary of these re-sults appears in Table 2.
In the cross-group train-ing and testing condition, we observe some vari-ations in accuracy, for some training sets.
How-ever, crucially none of the differences betweennative-based or near-native training and within-group training reach significance for the binarypitch accent recognition task.6 ConclusionWe have demonstrated the effectiveness of pitchaccent recognition on both native and non-nativedata from the LeaP corpus, based on significantdifferences between accented and unaccented syl-lables in both native and non-native speech.
Al-though these differences are significantly larger innative speech, recognition remains robust to train-ing with native speech and testing on non-nativespeech, without significant drops in accuracy.
Thisresult argues that binary pitch accent recognitionusing native training data may be sufficiently ac-curate that to avoid collection and labeling of largeamounts of training data matched by speaker orfluency-level to support prosodic annotation andfeedback.
In future work, we plan to incorporateprosodic recognition and synthesized feedback tosupport computer-assisted prosody learning.AcknowledgmentsWe thank the creators of the LeaP Corpus as wellas C-C. Cheng and C-J.
Lin for LibSVM.
Thiswork was supported by NSF IIS: 0414919.ReferencesP.
Boersma.
2001.
Praat, a system for doing phoneticsby computer.
Glot International, 5(9?10):341?345.C-C.Cheng and C-J.
Lin.
2001.
LIBSVM:a libraryfor support vector machines.
Software available at:http://www.csie.ntu.edu.tw/ cjlin/libsvm.Dorothy M. Chun.
1998.
Signal analysis software forteaching discourse intonation.
Language Learning& Technology, 2(1):61?77.U.
Gut and P. S. Bayerl.
2004.
Measuring the relia-bility of manual annotations of speech corpora.
InProceedings of Speech Prosody 2004.U.
Gut.
2009.
Non-native speech.
A corpus-basedanalysis of phonological and phonetic properties ofL2 English and German.
Peter Lang, Frankfurt.G.-A.
Levow.
2008.
Automatic prosodic labeling withconditional random fields and rich acoustic features.In Proceedings of the IJCNLP 2008.J.-T. Milde and U.
Gut.
2002.
A prosodic corpus ofnon-native speech.
In Proceedings of the SpeechProsody 2002 Conference, pages 503?506.V.
Rangarajan Sridhar, S. Bangalore, and S. Narayanan.2007.
Exploiting acoustic and syntactic features forprosody labeling in a maximum entropy framework.In Proceedings of HLT NAACL 2007, pages 1?8.Xuejing Sun.
2002.
Pitch accent prediction using en-semble machine learning.
In Proceedings of ICSLP-2002.C.
Teixeia, H. Franco, E. Shriberg, K. Precoda, andK.
Somnez.
2000.
Prosodic features for automatictext-independent evaluation of degree of nativenessfor language learners.
In Proceedings of ICSLP2000.J.
Tepperman and S. Narayanan.
2008.
Better non-native intonation scores through prosodic theory.
InProceedings of Interspeech 2008.C.
Wang and S. Seneff.
2000.
Improved tone recogni-tion by normalizing for coarticulation and intonationeffects.
In Proceedings of 6th International Confer-ence on Spoken Language Processing.Yi Xu and X.
Sun.
2002.
Maximum speed of pitchchange and how it may relate to speech.
Journal ofthe Acoustical Society of America, 111.272
