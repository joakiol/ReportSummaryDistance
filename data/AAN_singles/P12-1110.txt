Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1045?1053,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsIncremental Joint Approach to Word Segmentation, POS Tagging, andDependency Parsing in ChineseJun Hatori1 Takuya Matsuzaki2 Yusuke Miyao2 Jun?ichi Tsujii31University of Tokyo / 7-3-1 Hongo, Bunkyo, Tokyo, Japan2National Institute of Informatics / 2-1-2 Hitotsubashi, Chiyoda, Tokyo, Japan3Microsoft Research Asia / 5 Danling Street, Haidian District, Beijing, P.R.
Chinahatori@is.s.u-tokyo.ac.jp{takuya-matsuzaki,yusuke}@nii.ac.jp jtsujii@microsoft.comAbstractWe propose the first joint model for word segmen-tation, POS tagging, and dependency parsing forChinese.
Based on an extension of the incrementaljoint model for POS tagging and dependency pars-ing (Hatori et al, 2011), we propose an efficientcharacter-based decoding method that can combinefeatures from state-of-the-art segmentation, POStagging, and dependency parsing models.
We alsodescribe our method to align comparable states inthe beam, and how we can combine features of dif-ferent characteristics in our incremental framework.In experiments using the Chinese Treebank (CTB),we show that the accuracies of the three tasks canbe improved significantly over the baseline models,particularly by 0.6% for POS tagging and 2.4% fordependency parsing.
We also perform comparisonexperiments with the partially joint models.1 IntroductionIn processing natural languages that do not includedelimiters (e.g.
spaces) between words, word seg-mentation is the crucial first step that is necessaryto perform virtually all NLP tasks.
Furthermore, theword-level information is often augmented with thePOS tags, which, along with segmentation, form thebasic foundation of statistical NLP.Because the tasks of word segmentation and POStagging have strong interactions, many studies havebeen devoted to the task of joint word segmenta-tion and POS tagging for languages such as Chi-nese (e.g.
Kruengkrai et al (2009)).
This is becausesome of the segmentation ambiguities cannot be re-solved without considering the surrounding gram-matical constructions encoded in a sequence of POStags.
The joint approach to word segmentation andPOS tagging has been reported to improve word seg-mentation and POS tagging accuracies by more than1% in Chinese (Zhang and Clark, 2008).
In addition,some researchers recently proposed a joint approachto Chinese POS tagging and dependency parsing (Liet al, 2011; Hatori et al, 2011); particularly, Ha-tori et al (2011) proposed an incremental approachto this joint task, and showed that the joint approachimproves the accuracies of these two tasks.In this context, it is natural to consider furthera question regarding the joint framework: howstrongly do the tasks of word segmentation and de-pendency parsing interact?
In the following Chinesesentences:S?
?sV  ?s ? ?scurrent peace-prize and peace operation relatedThe current peace prize and peace operations are related.S?
?s V ?s ? ?s ?Scurrent peace award peace operation related groupThe current peace is awarded to peace-operation-related groups.the only difference is the existence of the last word?S; however, whether or not this word existschanges the whole syntactic structure and segmen-tation of the sentence.
This is an example in whichword segmentation cannot be handled properly with-out considering long-range syntactic information.Syntactic information is also considered ben-eficial to improve the segmentation of out-of-vocabulary (OOV) words.
Unlike languages suchas Japanese that use a distinct character set (i.e.katakana) for foreign words, the transliterated wordsin Chinese, many of which are OOV words, fre-quently include characters that are also used as com-mon or function words.
In the current systems, theexistence of these characters causes numerous over-segmentation errors for OOV words.Based on these observations, we aim at build-ing a joint model that simultaneously processesword segmentation, POS tagging, and dependencyparsing, trying to capture global interaction among1045these three tasks.
To handle the increased computa-tional complexity, we adopt the incremental parsingframework with dynamic programming (Huang andSagae, 2010), and propose an efficient method ofcharacter-based decoding over candidate structures.Two major challenges exist in formalizing thejoint segmentation and dependency parsing task inthe character-based incremental framework.
First,we must address the problem of how to align com-parable states effectively in the beam.
Because thenumber of dependency arcs varies depending onhow words are segmented, we devise a step align-ment scheme using the number of character-basedarcs, which enables effective joint decoding for thethree tasks.Second, although the feature set is fundamen-tally a combination of those used in previous works(Zhang and Clark, 2010; Huang and Sagae, 2010), tointegrate them in a single incremental framework isnot straightforward.
Because we must perform de-cisions of three kinds (segmentation, tagging, andparsing) in an incremental framework, we must ad-just which features are to be activated when, andhow they are combined with which action labels.
Wehave also found that we must balance the learningrate between features for segmentation and taggingdecisions, and those for dependency parsing.We perform experiments using the Chinese Tree-bank (CTB) corpora, demonstrating that the accura-cies of the three tasks can be improved significantlyover the pipeline combination of the state-of-the-artjoint segmentation and POS tagging model, and thedependency parser.
We also perform comparison ex-periments with partially joint models, and investi-gate the tradeoff between the running speed and themodel performance.2 Related WorksIn Chinese, Luo (2003) proposed a joint con-stituency parser that performs segmentation, POStagging, and parsing within a single character-basedframework.
They reported that the POS tags con-tribute to segmentation accuracies by more than 1%,but the syntactic information has no substantial ef-fect on the segmentation accuracies.
In contrast,we built a joint model based on a dependency-basedframework, with a rich set of structural features.
Us-ing it, we show the first positive result in Chinesethat the segmentation accuracies can be improvedusing the syntactic information.Another line of work exists on lattice-based pars-ing for Semitic languages (Cohen and Smith, 2007;Goldberg and Tsarfaty, 2008).
These methods firstconvert an input sentence into a lattice encodingthe morphological ambiguities, and then conductjoint morphological segmentation and PCFG pars-ing.
However, the segmentation possibilities consid-ered in those studies are limited to those output byan existing morphological analyzer.
In addition, thelattice does not include word segmentation ambigu-ities crossing boundaries of space-delimited tokens.In contrast, because the Chinese language does nothave spaces between words, we fundamentally needto consider the lattice structure of the whole sen-tence.
Therefore, we place no restriction on the seg-mentation possibilities to consider, and we assess thefull potential of the joint segmentation and depen-dency parsing model.Among the many recent works on joint segmen-tation and POS tagging for Chinese, the linear-timeincremental models by Zhang and Clark (2008) andZhang and Clark (2010) largely inspired our model.Zhang and Clark (2008) proposed an incrementaljoint segmentation and POS tagging model, with aneffective feature set for Chinese.
However, it re-quires to computationally expensive multiple beamsto compare words of different lengths using beamsearch.
More recently, Zhang and Clark (2010) pro-posed an efficient character-based decoder for theirword-based model.
In their new model, a singlebeam suffices for decoding; hence, they reported thattheir model is practically ten times as fast as theiroriginal model.
To incorporate the word-level fea-tures into the character-based decoder, the featuresare decomposed into substring-level features, whichare effective for incomplete words to have compara-ble scores to complete words in the beam.
Becausewe found that even an incremental approach withbeam search is intractable if we perform the word-based decoding, we take a character-based approachto produce our joint model.The incremental framework of our model is basedon the joint POS tagging and dependency parsingmodel for Chinese (Hatori et al, 2011), which is anextension of the shift-reduce dependency parser withdynamic programming (Huang and Sagae, 2010).They specifically modified the shift action so that itassigns the POS tag when a word is shifted onto thestack.
However, because they regarded word seg-mentation as given, their model did not consider the1046interaction between segmentation and POS tagging.3 Model3.1 Incremental Joint Segmentation, POSTagging, and Dependency ParsingBased on the joint POS tagging and dependencyparsing model by Hatori et al (2011), we build ourjoint model to solve word segmentation, POS tag-ging, and dependency parsing within a single frame-work.
Particularly, we change the role of the shift ac-tion and additionally use the append action, inspiredby the character-based actions used in the joint seg-mentation and POS tagging model by Zhang andClark (2010).The list of actions used is the following:?
A: append the first character in the queue to theword on top of the stack.?
SH(t): shift the first character in the input queueas a new word onto the stack, with POS tag t.?
RL/RR: reduce the top two trees on the stack,(s0, s1), into a subtree sy0 s1 / sx0 s1, respectively.Although SH(t) is similar to the one used in Hatoriet al (2011), now it shifts the first character in thequeue as a new word, instead of shifting a word.
Fol-lowing Zhang and Clark (2010), the POS tag is as-signed to the word when its first character is shifted,and the word?tag pairs observed in the training dataand the closed-set tags (Xia, 2000) are used to pruneunlikely derivations.
Because 33 tags are defined inthe CTB tag set (Xia, 2000), our model exploits atotal of 36 actions.To train the model, we use the averaged percep-tron with the early update (Collins and Roark, 2004).In our joint model, the early update is invoked bymistakes in any of word segmentation, POS tagging,or dependency parsing.3.2 Alignment of StatesWhen dependency parsing is integrated into the taskof joint word segmentation and POS tagging, it isnot straightforward to define a scheme to align (syn-chronize) the states in the beam.
In beam search, weuse the step index that is associated with each state:the parser states in process are aligned according tothe index, and the beam search pruning is appliedto those states with the same index.
Consequently,for the beam search to function effectively, all stateswith the same index must be comparable, and allterminal states should have the same step index.We can first think of using the number of shiftedcharacters as the step index, as Zhang and Clark(2010) does.
However, because RL/RR actions canbe performed without incrementing the step index,the decoder tends to prefer states with more de-pendency arcs, resulting more likely in prematurechoice of ?reduce?
actions or oversegmentation ofwords.
Alternatively, we can consider using thenumber of actions that have been applied as the stepindex, as Hatori et al (2011) does.
However, thisresults in inconsistent numbers of actions to reachthe terminal states: some states that segment wordsinto larger chunks reach a terminal state earlier thanother states with smaller chunks.
For these reasons,we have found that both approaches yield poor mod-els that are not at all competitive with the baseline(pipeline) models1.To address this issue, we propose an indexingscheme using the number of character-based arcs.We presume that in addition to the word-to-word de-pendency arcs, each word (of length M ) implicitlyhas M ?
1 inter-character arcs, as in: AxBxC ,AxB x C , and A x B x C (each rectangle de-notes a word).
Then we can define the step index asthe sum of the number of shifted characters and thetotal number of (inter-word and intra-word) depen-dency arcs, which thereby meets all the followingconditions:(1) All subtrees spanning M consecutive charactershave the same index 2M ?
1.
(2) All terminal states have the same step index 2N(including the root arc), where N is the numberof characters in the sentence.
(3) Every action increases the index.Note that the number of shifted characters is alsonecessary to meet condition (3).
Otherwise, it allowsan unlimited number of SH(t) actions without incre-menting the step index.
Figure 1 portrays how thestates are aligned using the proposed scheme, wherea subtree is denoted as a rectangle with its partialindex shown inside it.In our framework, because an action increases thestep index by 1 (for SH(t) or RL/RR) or 2 (for A), weneed to use two beams to store new states at eachstep.
The computational complexity of the entireprocess is O(B(T + 3) ?
2N), where B is the beam1For example, in our preliminary experiment on CTB-5, thestep indexing according to the number of actions underperformsthe baseline model by 0.2?0.3% in segmentation accuracy.1047step 1 step 2step 6 step 7 step 8step 3 step 4 step 53 3 3 5555 7 75333333 331 1 1 1 1 11 11 1 11 1 1 1 1 11 1 1111 11 11 1 1 11 111 1 1 11 1 1 11 1 1 1 1 1 1 1 1 1 1 111 11 11 1 13 3Figure 1: Illustration of the alignment of steps.size, T is the number of POS tags (= 33), and Nis the number of characters in the sentence.
Theo-retically, the computational time is greater than thatwith the character-based joint segmentation and tag-ging model by Zhang and Clark (2010) by a factorof T+3T+1 ?2NN ' 2.1, when the same beam size is used.3.3 FeaturesThe feature set of our model is fundamentally a com-bination of the features used in the state-of-the-artjoint segmentation and POS tagging model (Zhangand Clark, 2010) and dependency parser (Huang andSagae, 2010), both of which are used as baselinemodels in our experiment.
However, we must care-fully adjust which features are to be activated andwhen, and how they are combined with which ac-tion labels, depending on the type of the features be-cause we intend to perform three tasks in a singleincremental framework.The list of the features used in our joint modelis presented in Table 1, where S01?S05, W01?W21, and T01?05 are taken from Zhang and Clark(2010), and P01?P28 are taken from Huang andSagae (2010).
Note that not all features are alwaysconsidered: each feature is only considered if theaction to be performed is included in the list of ac-tions in the ?When to apply?
column.
Because S01?S05 are used to represent the likelihood score ofsubstring sequences, they are only used for A andSH(t) without being combined with any action la-bel.
Because T01?T05 are used to determine thePOS tag of the word being shifted, they are only ap-plied for SH(t).
Because W01?W21 are used to de-termine whether to segment at the current positionor not, they are only used for those actions involvedin boundary determination decisions (A, SH(t), RL0,and RR0).
The action labels RL0/RR0 are used todenote the ?reduce?
actions that determine the wordboundary2, whereas RL1/RR1 denote those ?reduce?actions that are applied when the word boundary hasalready been fixed.
In addition, to capture the sharednature of boundary determination actions (SH(t),RL0/RR0), we use a generalized action label SH?
torepresent any of them when combined with W01?W21.
We also propose to use the features U01?U03,which we found are effective to adjust the character-level and substring-level scores.Regarding the parsing features P01?P28, becausewe found that P01?P17 are also useful for segmen-tation decisions, these features are applied to all ac-tions including A, with an explicit distinction of ac-tion labels RL0/RR0 from RL1/RR1.
On the otherhand, P18?P28 are only used when one of the parseractions (SH(t), RL, or RR) is applied.
Note that P07?P09 and P18?P21 (look-ahead features) require thelook-ahead information of the next word form andPOS tags, which cannot be incorporated straightfor-wardly in an incremental framework.
Although wehave found that these features can be incorporatedusing the delayed features proposed by Hatori et al(2011), we did not use them in our current modelbecause it results in the significant increase of com-putational time.3.3.1 Dictionary featuresBecause segmentation using a dictionary alonecan serve as a strong baseline in Chinese word seg-mentation (Sproat et al, 1996), the use of dictio-naries is expected to make our joint model more ro-bust and enables us to investigate the contribution ofthe syntactic dependency in a more realistic setting.Therefore, we optionally use four features D01?D04associated with external dictionaries.
These featuresdistinguish each dictionary source, reflecting the factthat different dictionaries have different characteris-tics.
These features will also be used in our reimple-mentation of the model by Zhang and Clark (2010).3.4 Adjusting the Learning Rate of FeaturesIn formulating the three tasks in the incrementalframework, we found that adjusting the update ratedepending on the type of the features (segmenta-tion/tagging vs. parsing) crucially impacts the finalperformance of the model.
To investigate this point,we define the feature vector ~?
and score ?
of the2A reduce action has an additional effect of fixing the bound-ary of the top word on the stack if the last action was A or SH(t).1048Id Feature template Label When to applyU01 q?1.e ?
q?1.t ?
A, SH(t)U02,03 q?1.e q?1.e ?
q?1.t as-is anyS01 q?1.e ?
c0 ?
AS02 q?1.t ?
c0 ?
A, SH(t)S03 q?1.t ?
q?1.b ?
c0 ?
AS04 q?1.t ?
c0 ?
C(q?1.b) ?
AS05 q?1.t ?
c0 ?
c1 ?
AD01 len(q?1.w) ?
i A,SH?
A, SH(t), RR/RL0D02 len(q?1.w) ?
q?1.t ?
i A,SH?
A, SH(t), RR/RL0D03 len(q?1.w) ?
i A,SH?
A, SH(t), RR/RL0D04 len(q?1.w) ?
q?1.t ?
i A,SH?
A, SH(t), RR/RL0(D01,02: if q?1.w ?
Di; D03,04: if q?1.w /?
Di)W01,02 q?1.w q?2.w ?
q?1.w A,SH?
A, SH(t), RR/RL0W03 q?1.w (for single-char word) A,SH?
A, SH(t), RR/RL0W04 q?1.b ?
len(q?1.w) A,SH?
A, SH(t), RR/RL0W05 q?1.e ?
len(q?1.w) A,SH?
A, SH(t), RR/RL0W06,07 q?1.e ?
c0 q?1.b ?
q?1.e A,SH?
A, SH(t), RR/RL0W08,09 q?1.w ?
c0 q?2.e ?
q?1.w A,SH?
A, SH(t), RR/RL0W10,11 q?1.b ?
c0 q?2.e ?
q?1.e A,SH?
A, SH(t), RR/RL0W12 q?2.w ?
len(q?1.w) A,SH?
A, SH(t), RR/RL0W13 len(q?2.w) ?
q?1.w A,SH?
A, SH(t), RR/RL0W14 q?1.w ?
q?1.t A,SH?
A, SH(t), RR/RL0W15 q?2.t ?
q?1.w A,SH?
A, SH(t), RR/RL0W16 q?1.t ?
q?1.w ?
q?2.e A,SH?
A, SH(t), RR/RL0W17 q?1.t ?
q?1.w ?
c0 A,SH?
A, SH(t), RR/RL0W18 q?2.e ?
q?1.w ?
c0 ?
q1.t A,SH?
A, SH(t), RR/RL0W19 q?1.t ?
q?1.e A,SH?
A, SH(t), RR/RL0W20 q?1.t ?
q?1.e ?
c A,SH?
A, SH(t), RR/RL0W21 q?1.t ?
c ?
cat(q?1.e) A,SH?
A, SH(t), RR/RL0(W20, W21: c ?
q?1.w\e)T01,02 q?1.t q?2.t ?
q?1.t SH(t) SH(t)T03,04 q?1.w c0 SH(t) SH(t)T05 c0 ?
q?1.t ?
q?1.e SH(t) SH(t)P01,02 s0.w s0.t A, SH(t), RR/RL0/1 anyP03,04 s0.w ?
s0.t s1.w A, SH(t), RR/RL0/1 anyP05,06 s1.t s1.w ?
s1.t A, SH(t), RR/RL0/1 anyP07,08 q0.w q0.t A, SH(t), RR/RL0/1 anyP09,10 q0.w ?
q0.t s0.w ?
s1.w A, SH(t), RR/RL0/1 anyP11,12 s0.t ?
s1.t s0.t ?
q0.t A, SH(t), RR/RL0/1 anyP13 s0.w ?
s0.t ?
s1.t A, SH(t), RR/RL0/1 anyP14 s0.t ?
s1.w ?
s1.t A, SH(t), RR/RL0/1 anyP15 s0.w ?
s1.w ?
s1.t A, SH(t), RR/RL0/1 anyP16 s0.w ?
s0.t ?
s1.w A, SH(t), RR/RL0/1 anyP17 s0.w ?
s0.t ?
s1.w ?
s1.t A, SH(t), RR/RL0/1 anyP18 s0.t ?
q0.t ?
q1.t as-is SH(t), RR, RLP19 s1.t ?
s0.t ?
q0.t as-is SH(t), RR, RLP20 s0.w ?
q0.t ?
q1.t as-is SH(t), RR, RLP21 s1.t ?
s0.w ?
q0.t as-is SH(t), RR, RLP22 s1.t ?
s1.rc.t ?
s0.t as-is SH(t), RR, RLP23 s1.t ?
s1.lc.t ?
s0.t as-is SH(t), RR, RLP24 s1.t ?
s1.rc.t ?
s0.w as-is SH(t), RR, RLP25 s1.t ?
s1.lc.t ?
s0.w as-is SH(t), RR, RLP26 s1.t ?
s0.t ?
s0.rc.t as-is SH(t), RR, RLP27 s1.t ?
s0.w ?
s0.lc.t as-is SH(t), RR, RLP28 s2.t ?
s1.t ?
s0.t as-is SH(t), RR, RL* q?1 and q?2 respectively denote the last-shifted word and theword shifted before q?1.
q.w and q.t respectively denote the(root) word form and POS tag of a subtree (word) q, and q.b andq.e the beginning and ending characters of q.w.
c0 and c1 arethe first and second characters in the queue.
q.w\e denotes theset of characters excluding the ending character of q.w.
len(?
)denotes the length of the word, capped at 16 if longer.
cat(?)
de-notes the category of the character, which is the set of POS tagsobserved in the training data.
Di is a dictionary, a set of words.The action label ?
means that the feature is not combined withany label; ?as-is?
denotes the use of the default action set ?A,SH(t), and RR/RL?
as is.Table 1: Feature templates for the full joint model.Training Development Test#snt #wrd #snt #wrd #oov #snt #wrd #oovCTB-5d 16k 438k 804 21k 1.2k 1.9k 50k 3.1kCTB-5j 18k 494k 352 6.8k 553 348 8.0k 278CTB-5c 15k 423k - - - - - -CTB-6 23k 641k 2.1k 60k 3.3k 2.8k 82k 4.6kCTB-7 31k 718k 10k 237k 13k 10k 245k 13kTable 2: Statistics of datasets.action a being applied to the state ?
as?
(?, a) = ~?
?
~?
(?, a) = ~?
?
{~?st(?, a) + ?p~?p(?, a)},where ~?st corresponds to the segmentation and tag-ging features (those starting with ?U?, ?S?, ?T?, or?D?
), and ~?p is the set of the parsing features (start-ing with ?P?).
Then, if we set ?p to a number smallerthan 1, perceptron updates for the parsing featureswill be kept small at the early stage of training be-cause the update is proportional to the values of thefeature vector.
However, even if ?p is initially small,the global weights for the parsing features will in-crease as needed and compensate for the small ?pas the training proceeds.
In this way, we can con-trol the contribution of syntactic dependencies at theearly stage of training.
Section 4.3 shows that thebest setting we found is ?p = 0.5: this result sug-gests that we probably should resolve remaining er-rors by preferentially using the local n-gram basedfeatures at the early stage of training.
Otherwise,the premature incorporation of the non-local syntac-tic dependencies might engender overfitting to thetraining data.4 Experiment4.1 Experimental SettingsWe use the Chinese Penn Treebank ver.
5.1, 6.0,and 7.0 (hereinafter CTB-5, CTB-6, and CTB-7)for evaluation.
These corpora are split into train-ing, development, and test sets, according to previ-ous works.
For CTB-5, we refer to the split by Duanet al (2007) as CTB-5d, and to the split by Jianget al (2008) as CTB-5j.
We also prepare a datasetfor cross validation: the dataset CTB-5c consists ofsentences from CTB-5 excluding the developmentand test sets of CTB-5d and CTB-5j.
We split CTB-5c into five sets (CTB-5c-n), and alternatively usefour of these as the training set and the rest as thetest set.
CTB-6 is split according to the official split1049described in the documentation, and CTB-7 is splitaccording to Wang et al (2011).
The statistics ofthese splits are shown in Table 2.
As external dic-tionaries, we use the HowNet Word List3, consist-ing of 91,015 words, and page names from the Chi-nese Wikipedia4 as of Oct 26, 2011, consisting of709,352 words.
These dictionaries only consist ofword forms with no frequency or POS information.We use standard measures of word-level preci-sion, recall, and F1 score, for evaluating each task.The output of dependencies cannot be correct unlessthe syntactic head and dependent of the dependencyrelation are both segmented correctly.
Following thestandard setting in dependency parsing works, weevaluate the task of dependency parsing with the un-labeled attachment scores excluding punctuations.Statistical significance is tested by McNemar?s test(?
: p < 0.05, ?
: p < 0.01).4.2 Baseline and Proposed ModelsWe use the following baseline and proposed modelsfor evaluation.?
SegTag: our reimplementation of the joint seg-mentation and POS tagging model by Zhang andClark (2010).
Table 5 shows that this reimple-mentation almost reproduces the accuracy of theirimplementation.
We used the beam of 16, whichthey reported to achieve the best accuracies.?
Dep?
: the state-of-the-art dependency parser byHuang and Sagae (2010).
We used our reimple-mentation, which is used in Hatori et al (2011).?
Dep: Dep?
without look-ahead features.?
TagDep: the joint POS tagging and dependencyparsing model (Hatori et al, 2011), where thelook-ahead features are omitted.5?
SegTag+Dep/SegTag+Dep?
: a pipeline combina-tion of SegTag and Dep or Dep?.?
SegTag+TagDep: a pipeline combination of Seg-Tag and TagDep, where only the segmentationoutput of SegTag is used as input to TagDep; theoutput tags of TagDep are used for evaluation.?
SegTagDep: the proposed full joint model.All of the models described above except Dep?
arebased on the same feature sets for segmentation and3http://www.keenage.com/html/e index.html4http://zh.wikipedia.org/wiki5We used the original implementation used in Hatori et al(2011).
In Hatori et al (2011), we confirmed that omission ofthe look-ahead features results in a 0.26% decrease in the pars-ing accuracy on CTB-5d (dev).8688909294960  10  20  30  40  50  60  70  80Seg (?_p=0.1)Seg (?_p=0.2)Seg (?_p=0.5)Seg (?_p=1.0)Tag (?_p=0.1)Tag (?_p=0.2)Tag (?_p=0.5)Tag (?_p=1.0)  6062646668707274760  10  20  30  40  50  60  70  80Dep (?_p=0.1)Dep (?_p=0.2)Dep (?_p=0.5)Dep (?_p=1.0)Figure 2: F1 scores (in %) of SegTagDep on CTB-5c-1 w.r.t.
the training epoch (x-axis) and parsingfeature weights (in legend).tagging (Zhang and Clark, 2008; Zhang and Clark,2010) and dependency parsing (Huang and Sagae,2010).
Therefore, we can investigate the contribu-tion of the joint approach through comparison withthe pipeline and joint models.4.3 Development ResultsWe have some parameters to tune: parsing featureweight ?p, beam size, and training epoch.
All theseparameters are set based on experiments on CTB-5c.For experiments on CTB-5j, CTB-6, and CTB-7, thetraining epoch is set using the development set.Figure 2 shows the F1 scores of the proposedmodel (SegTagDep) on CTB-5c-1 with respect to thetraining epoch and different parsing feature weights,where ?Seg?, ?Tag?, and ?Dep?
respectively denotethe F1 scores of word segmentation, POS tagging,and dependency parsing.
In this experiment, the ex-ternal dictionaries are not used, and the beam sizeof 32 is used.
Interestingly, if we simply set ?p to1, the accuracies seem to converge at lower levels.The ?p = 0.2 setting seems to reach almost identi-cal segmentation and tagging accuracies as the bestsetting ?p = 0.5, but the convergence occurs moreslowly.
Based on this experiment, we set ?p to 0.5throughout the experiments in this paper.Table 3 shows the performance and speed of thefull joint model (with no dictionaries) on CTB-5c-1with respect to the beam size.
Although even thebeam size of 32 results in competitive accuraciesfor word segmentation and POS tagging, the depen-dency accuracy is affected most by the increase ofthe beam size.
Based on this experiment, we set thebeam size of SegTagDep to 64 throughout the exper-1050Beam Seg Tag Dep Speed4 94.96 90.19 70.29 5.78 95.78 91.53 72.81 3.216 96.09 92.09 74.20 1.832 96.18 92.24 74.57 0.9564 96.28 92.37 74.96 0.48Table 3: F1 scores and speed (in sentences per sec.
)of SegTagDep on CTB-5c-1 w.r.t.
the beam size.iments in this paper, unless otherwise noted.4.4 Main ResultsIn this section, we present experimentally obtainedresults using the proposed and baseline models.
Ta-ble 4 shows the segmentation, POS tagging, anddependency parsing F1 scores of these models onCTB-5c.
Irrespective of the existence of the dic-tionary features, the joint model SegTagDep largelyincreases the POS tagging and dependency pars-ing accuracies (by 0.56?0.63% and 2.34?2.44%);the improvements in parsing accuracies are stillsignificant even compared with SegTag+Dep?
(thepipeline model with the look-ahead features).
How-ever, when the external dictionaries are not used(?wo/dict?
), no substantial improvements for seg-mentation accuracies were observed.
In contrast,when the dictionaries are used (?w/dict?
), the seg-mentation accuracies are now improved over thebaseline model SegTag consistently (on every trial).Although the overall improvement in segmentationis only around 0.1%, more than 1% improvement isobserved if we specifically examine OOV6 words.The difference between ?wo/dict?
and ?w/dict?
re-sults suggests that the syntactic dependencies mightwork as a noise when the segmentation model is in-sufficiently stable, but the model does improve whenit is stable, not receiving negative effects from thesyntactic dependencies.The partially joint model SegTag+TagDep isshown to perform reasonably well in dependencyparsing: with dictionaries, it achieved the 2.02% im-provement over SegTag+Dep, which is only 0.32%lower than SegTagDep.
However, whereas Seg-Tag+TagDep showed no substantial improvement intagging accuracies over SegTag (when the dictionar-ies are used), SegTagDep achieved consistent im-provements of 0.46% and 0.58% (without/with dic-6We define the OOV words as the words that have not seen inthe training data, even when the external dictionaries are used.System Seg TagKruengkrai ?09 97.87 93.67Zhang ?10 97.78 93.67Sun ?11 98.17 94.02Wang ?11 98.11 94.18SegTag 97.66 93.61SegTagDep 97.73 94.46SegTag(d) 98.18 94.08SegTagDep(d) 98.26 94.64Table 5: Final results on CTB-5j90919293949596970.05  0.1  0.2  0.5  1  2SegTag (Seg)SegTagDep (Seg)SegTag (Tag)SegTag+TagDep (Tag)SegTagDep (Tag)69707172737475760.05  0.1  0.2  0.5  1  2SegTag+Dep (Dep)SegTag+TagDep (Dep)SegTagDep (Dep)Figure 3: Performance of baseline and joint modelsw.r.t.
the average processing time (in sec.)
per sen-tence.
Each point corresponds to the beam size of4, 8, 16, 32, (64).
The beam size of 16 is used forSegTag in SegTag+Dep and SegTag+TagDep.tionaries); these differences can be attributed to thecombination of the relieved error propagation andthe incorporation of the syntactic dependencies.
Inaddition, SegTag+TagDep has OOV tagging accura-cies consistently lower than SegTag, suggesting thatthe syntactic dependency has a negative effect on thePOS tagging accuracy of OOV words7.
In contrast,this negative effect is not observed for SegTagDep:both the overall tagging accuracy and the OOV accu-racy are improved, demonstrating the effectivenessof the proposed model.Figure 3 shows the performance and processingtime comparison of various models and their com-binations.
Although SegTagDep takes a few timeslonger to achieve accuracies comparable to those ofSegTag+Dep/TagDep, it seems to present potential7This is consistent with Hatori et al (2011)?s observationthat although the joint POS tagging and dependency parsing im-proves the accuracy of syntactically influential POS tags, it hasa slight side effect of increasing the confusion between generaland proper nouns (NN vs. NR).1051Model Segmentation POS Tagging DependencyALL OOV ALL OOVwo/dictSegTag+Dep96.22 72.24 91.74 59.8272.58SegTag+Dep?
72.94 (+0.36?
)SegTag+TagDep 91.86 (+0.12?)
58.89 (-0.93?)
74.60 (+2.02?
)SegTagDep 96.19 (-0.03) 72.24 (+0.00) 92.30 (+0.56?)
61.03 (+1.21?)
74.92 (+2.34?
)w/dictSegTag+Dep96.82 78.32 92.34 65.4473.53SegTag+Dep?
73.90 (+0.37?
)SegTag+TagDep 92.35 (+0.01) 63.20 (-2.24?)
75.45 (+1.92?
)SegTagDep 96.90 (+0.08?)
79.38 (+1.06?)
92.97 (+0.63?)
67.40 (+1.96?)
75.97 (+2.44?
)Table 4: Segmentation, POS tagging, and (unlabeled attachment) dependency F1 scores averaged over fivetrials on CTB-5c.
Figures in parentheses show the differences over SegTag+Dep (?
: p < 0.01).for greater improvement, especially for tagging andparsing accuracies, when a larger beam can be used.4.5 Comparison with Other SystemsTable 5 and Table 6 show a comparison of the seg-mentation and POS tagging accuracies with otherstate-of-the-art models.
?Kruengkrai+ ?09?
is alattice-based model by Kruengkrai et al (2009).
?Zhang ?10?
is the incremental model by Zhang andClark (2010).
These two systems use no external re-sources other than the CTB corpora.
?Sun+ ?11?
is aCRF-based model (Sun, 2011) that uses a combina-tion of several models, with a dictionary of idioms.
?Wang+ ?11?
is a semi-supervised model by Wanget al (2011), which additionally uses the ChineseGigaword Corpus.Our models with dictionaries (those marked with?(d)?)
have competitive accuracies to other state-of-the-art systems, and SegTagDep(d) achieved the bestreported segmentation and POS tagging accuracies,using no additional corpora other than the dictio-naries.
Particularly, the POS tagging accuracy ismore than 0.4% higher than the previous best sys-tem thanks to the contribution of syntactic depen-dencies.
These results also suggest that the use ofreadily available dictionaries can be more effectivethan semi-supervised approaches.5 ConclusionIn this paper, we proposed the first joint modelfor word segmentation, POS tagging, and depen-dency parsing in Chinese.
The model demonstratedsubstantial improvements on the three tasks overthe pipeline combination of the state-of-the-art jointsegmentation and POS tagging model, and depen-dency parser.
Particularly, results showed that theModel CTB-6 Test CTB-7 TestSeg Tag Dep Seg Tag DepKruengkrai ?09 95.50 90.50 - 95.40 89.86 -Wang ?11 95.79 91.12 - 95.65 90.46 -SegTag+Dep 95.46 90.64 72.57 95.49 90.11 71.25SegTagDep 95.45 91.27 74.88 95.42 90.62 73.58(diff.)
-0.01 +0.63?
+2.31?
-0.07 +0.51?
+2.33?SegTag+Dep(d) 96.13 91.38 73.62 95.98 90.68 72.06SegTagDep(d) 96.18 91.95 75.76 96.07 91.28 74.58(diff.)
+0.05 +0.57?
+2.14?
+0.09?
+0.60?
+2.52?Table 6: Final results on CTB-6 and CTB-7accuracies of POS tagging and dependency pars-ing were remarkably improved by 0.6% and 2.4%,respectively corresponding to 8.3% and 10.2% er-ror reduction.
For word segmentation, althoughthe overall improvement was only around 0.1%,greater than 1% improvements was observed forOOV words.
We conducted some comparison ex-periments of the partially joint and full joint mod-els.
Compared to SegTagDep, SegTag+TagDep per-forms reasonably well in terms of dependency pars-ing accuracy, whereas the POS tagging accuraciesare more than 0.5% lower.In future work, probabilistic pruning techniquessuch as the one based on a maximum entropy modelare expected to improve the efficiency of the jointmodel further because the accuracies are apparentlystill improved if a larger beam can be used.
Moreefficient decoding would also allow the use of thelook-ahead features (Hatori et al, 2011) and richerparsing features (Zhang and Nivre, 2011).Acknowledgement We are grateful to the anony-mous reviewers for their comments and suggestions, andto Xianchao Wu, Kun Yu, Pontus Stenetorp, and Shin-suke Mori for their helpful feedback.1052ReferencesShay B. Cohen and Noah A. Smith.
2007.
Joint morpho-logical and syntactic disambiguation.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of the 42nd Annual Meeting of the Association forComputational Linguistics (ACL 2004).Xiangyu Duan, Jun Zhao, and Bo Xu.
2007.
Probabilis-tic parsing action models for multi-lingual dependencyparsing.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007.Yoav Goldberg and Reut Tsarfaty.
2008.
A single gener-ative model for joint morphological segmentation andsyntactic parsing.
In Proceedings of the 46th AnnualMeeting of the Association of Computational Linguis-tics (ACL-2008).Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2011.
Incremental joint POS taggingand dependency parsing in Chinese.
In Proceedingsof the Fifth International Joint Conference on NaturalLanguage Processing (IJCNLP-2011).Liang Huang and Kenji Sagae.
2010.
Dynamic program-ming for linear-time incremental parsing.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu.2008.
A cascaded linear model for joint Chinese wordsegmentation and part-of-speech tagging.
In Proceed-ings of the 46th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hybridmodel for joint Chinese word segmentation and POStagging.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP.Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-liang Chen, and Haizhou Haizhou.
2011.
Joint mod-els for Chinese POS tagging and dependency parsing.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing.Xiaoqiang Luo.
2003.
A maximum entropy Chinesecharacter-based parser.
In Proceedings of the 2003Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP-2003).Richard Sproat, Chilin Shih, William Gale, and NancyChang.
1996.
A stochastic finite-state word-segmentation algorithm for Chinese.
ComputationalLinguistics, 22.Weiwei Sun.
2011.
A stacked sub-word model for jointChinese word segmentation and part-of-speech tag-ging.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies.Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.2011.
Improving Chinese word segmentation and POStagging with semi-supervised methods using largeauto-analyzed data.
In Proceedings of the Fifth Inter-national Joint Conference on Natural Language Pro-cessing (IJCNLP-2011).Fei Xia.
2000.
The part-of-speech tagging guidelinesfor the Penn Chinese treebank (3.0).
Technical ReportIRCS-00-07, University of Pennsylvania Institute forResearch in Cognitive Science Technical Report, Oc-tober.Yue Zhang and Stephen Clark.
2008.
Joint word seg-mentation and POS tagging using a single perceptron.In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies.Yue Zhang and Stephen Clark.
2010.
A fast decoderfor joint word segmentation and POS-tagging usinga single discriminative model.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing (short pa-pers).1053
