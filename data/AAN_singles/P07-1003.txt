Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 17?24,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsTailoring Word Alignments to Syntactic Machine TranslationJohn DeNeroComputer Science DivisionUniversity of California, Berkeleydenero@berkeley.eduDan KleinComputer Science DivisionUniversity of California, Berkeleyklein@cs.berkeley.eduAbstractExtracting tree transducer rules for syntac-tic MT systems can be hindered by wordalignment errors that violate syntactic corre-spondences.
We propose a novel model forunsupervised word alignment which explic-itly takes into account target language con-stituent structure, while retaining the robust-ness and efficiency of the HMM alignmentmodel.
Our model?s predictions improve theyield of a tree transducer extraction system,without sacrificing alignment quality.
Wealso discuss the impact of various posterior-based methods of reconciling bidirectionalalignments.1 IntroductionSyntactic methods are an increasingly promising ap-proach to statistical machine translation, being bothalgorithmically appealing (Melamed, 2004; Wu,1997) and empirically successful (Chiang, 2005;Galley et al, 2006).
However, despite recentprogress, almost all syntactic MT systems, indeedstatistical MT systems in general, build upon crudelegacy models of word alignment.
This dependenceruns deep; for example, Galley et al (2006) requiresword alignments to project trees from the target lan-guage to the source, while Chiang (2005) requiresalignments to induce grammar rules.Word alignment models have not stood still in re-cent years.
Unsupervised methods have seen sub-stantial reductions in alignment error (Liang et al,2006) as measured by the now much-maligned AERmetric.
A host of discriminative methods have beenintroduced (Taskar et al, 2005; Moore, 2005; Ayanand Dorr, 2006).
However, few of these methodshave explicitly addressed the tension between wordalignments and the syntactic processes that employthem (Cherry and Lin, 2006; Daume?
III and Marcu,2005; Lopez and Resnik, 2005).We are particularly motivated by systems like theone described in Galley et al (2006), which con-structs translations using tree-to-string transducerrules.
These rules are extracted from a bitext anno-tated with both English (target side) parses and wordalignments.
Rules are extracted from target sideconstituents that can be projected onto contiguousspans of the source sentence via the word alignment.Constituents that project onto non-contiguous spansof the source sentence do not yield transducer rulesthemselves, and can only be incorporated into largertransducer rules.
Thus, if the word alignment of asentence pair does not respect the constituent struc-ture of the target sentence, then the minimal transla-tion units must span large tree fragments, which donot generalize well.We present and evaluate an unsupervised wordalignment model similar in character and compu-tation to the HMM model (Ney and Vogel, 1996),but which incorporates a novel, syntax-aware distor-tion component which conditions on target languageparse trees.
These trees, while automatically gener-ated and therefore imperfect, are nonetheless (1) auseful source of structural bias and (2) the same treeswhich constrain future stages of processing anyway.In our model, the trees do not rule out any align-ments, but rather softly influence the probability oftransitioning between alignment positions.
In par-ticular, transition probabilities condition upon pathsthrough the target parse tree, allowing the model toprefer distortions which respect the tree structure.17Our model generates word alignments that betterrespect the parse trees upon which they are condi-tioned, without sacrificing alignment quality.
Usingthe joint training technique of Liang et al (2006)to initialize the model parameters, we achieve anAER superior to the GIZA++ implementation ofIBM model 4 (Och and Ney, 2003) and a reduc-tion of 56.3% in aligned interior nodes, a measureof agreement between alignments and parses.
As aresult, our alignments yield more rules, which bettermatch those we would extract had we used manualalignments.2 Translation with Tree TransducersIn a tree transducer system, as in phrase-based sys-tems, the coverage and generality of the transducerinventory is strongly related to the effectiveness ofthe translation model (Galley et al, 2006).
We willdemonstrate that this coverage, in turn, is related tothe degree to which initial word alignments respectsyntactic correspondences.2.1 Rule ExtractionGalley et al (2004) proposes a method for extractingtree transducer rules from a parallel corpus.
Given asource language sentence s, a target language parsetree t of its translation, and a word-level alignment,their algorithm identifies the constituents in t whichmap onto contiguous substrings of s via the align-ment.
The root nodes of such constituents ?
denotedfrontier nodes ?
serve as the roots and leaves of treefragments that form minimal transducer rules.Frontier nodes are distinguished by their compat-ibility with the word alignment.
For a constituent cof t, we consider the set of source words sc that arealigned to c. If none of the source words in the lin-ear closure s?c (the words between the leftmost andrightmost members of sc) aligns to a target word out-side of c, then the root of c is a frontier node.
Theremaining interior nodes do not generate rules, butcan play a secondary role in a translation system.1The roots of null-aligned constituents are not fron-tier nodes, but can attach productively to multipleminimal rules.1Interior nodes can be used, for instance, in evaluatingsyntax-based language models.
They also serve to differentiatetransducer rules that have the same frontier nodes but differentinternal structure.Two transducer rules, t1 ?
s1 and t2 ?
s2,can be combined to form larger translation unitsby composing t1 and t2 at a shared frontier nodeand appropriately concatenating s1 and s2.
How-ever, no technique has yet been shown to robustlyextract smaller component rules from a large trans-ducer rule.
Thus, for the purpose of maximizing thecoverage of the extracted translation model, we pre-fer to extract many small, minimal rules and gen-erate larger rules via composition.
Maximizing thenumber of frontier nodes supports this goal, whileinducing many aligned interior nodes hinders it.2.2 Word Alignment InteractionsWe now turn to the interaction between word align-ments and the transducer extraction algorithm.
Con-sider the example sentence in figure 1A, whichdemonstrates how a particular type of alignment er-ror prevents the extraction of many useful transducerrules.
The mistaken link [la ?
the] intervenes be-tween axe?s and carrie`r, which both align within anEnglish adjective phrase, while la aligns to a distantsubspan of the English parse tree.
In this way, thealignment violates the constituent structure of theEnglish parse.While alignment errors are undesirable in gen-eral, this error is particularly problematic for asyntax-based translation system.
In a phrase-basedsystem, this link would block extraction of thephrases [axe?s sur la carrie`r ?
career oriented] and[les emplois ?
the jobs] because the error overlapswith both.
However, the intervening phrase [em-plois sont ?
jobs are] would still be extracted, atleast capturing the transfer of subject-verb agree-ment.
By contrast, the tree transducer extractionmethod fails to extract any of these fragments: thealignment error causes all non-terminal nodes inthe parse tree to be interior nodes, excluding pre-terminals and the root.
Figure 1B exposes the conse-quences: a wide array of desired rules are lost duringextraction.The degree to which a word alignment respectsthe constituent structure of a parse tree can be quan-tified by the frequency of interior nodes, which indi-cate alignment patterns that cross constituent bound-aries.
To achieve maximum coverage of the trans-lation model, we hope to infer tree-violating align-ments only when syntactic structures truly diverge.18.
(A)(B) (i)(ii)SNPVPADJPNN VBNNNSDTAUXThejobsarecareeroriented.lesemploissontax?ssurlacarri?re..LegendCorrect proposed word alignment consistent withhuman annotation.Proposed word alignment error inconsistent withhuman annotation.Word alignment constellation that renders theroot of the relevant constituent to be an interiornode.Word alignment constellation that would allow aphrase extraction in a phrase-based translationsystem, but which does not correspond to anEnglish constituent.BoldItalicFrontier node (agrees with alignment)Interior node (inconsistent with alignment)(S (NP (DT[0] NNS[1]) (VP AUX[2] (ADJV NN[3] VBN[4]) .
[5]) ?
[0] [1] [2] [3] [4] [5](S (NP (DT[0] (NNS jobs)) (VP AUX[1] (ADJV NN[2] VBN[3]) .
[4]) ?
[0] sont [1] [2] [3] [4](S (NP (DT[0] (NNS jobs)) (VP (AUX are) (ADJV NN[1] VBN[2]) .
[3]) ?
[0] emplois sont [1] [2] [3](S NP[0] VP[1] .
[2]) ?
[0] [1] [2](S (NP (DT[0] NNS[1]) VP[2] .
[3]) ?
[0] [1] [2] [3](S (NP (DT[0] (NNS jobs)) VP[2] .
[3]) ?
[0] emplois [2] [3](S (NP (DT[0] (NNS jobs)) (VP AUX[1] ADJV[2]) .
[3]) ?
[0] emplois [1] [2] [3](S (NP (DT[0] (NNS jobs)) (VP (AUX are) ADJV[1]) .
[2]) ?
[0] emplois sont [1] [2]Figure 1: In this transducer extraction example, (A) shows a proposed alignment from our test set withan alignment error that violates the constituent structure of the English sentence.
The resulting frontiernodes are printed in bold; all nodes would be frontier nodes under a correct alignment.
(B) shows a smallsample of the rules extracted under the proposed alignment, (ii), and the correct alignment, (i) and (ii).
Thesingle alignment error prevents the extraction of all rules in (i) and many more.
This alignment pattern wasobserved in our test set and corrected by our model.3 Unsupervised Word AlignmentTo allow for this preference, we present a novel con-ditional alignment model of a foreign (source) sen-tence f = {f1, ..., fJ} given an English (target) sen-tence e = {e1, ..., eI} and a target tree structure t.Like the classic IBM models (Brown et al, 1994),our model will introduce a latent alignment vectora = {a1, ..., aJ} that specifies the position of analigned target word for each source word.
Formally,our model describes p(a, f|e, t), but otherwise bor-rows heavily from the HMM alignment model ofNey and Vogel (1996).The HMM model captures the intuition that thealignment vector a will in general progress acrossthe sentence e in a pattern which is mostly local, per-haps with a few large jumps.
That is, alignments arelocally monotonic more often than not.Formally, the HMM model factors as:p(a, f|e) =J?j=1pd(aj |aj?
, j)p`(fj |eaj )where j?
is the position of the last non-null-alignedsource word before position j, p` is a lexical transfermodel, and pd is a local distortion model.
As in allsuch models, the lexical component p` is a collec-tion of unsmoothed multinomial distributions over19foreign words.The distortion model pd(aj |aj?
, j) is a distribu-tion over the signed distance aj ?
aj?
, typicallyparameterized as a multinomial, Gaussian or expo-nential distribution.
The implementation that servesas our baseline uses a multinomial distribution withseparate parameters for j = 1, j = J and sharedparameters for all 1 < j < J .
Null alignments havefixed probability at any position.
Inference over arequires only the standard forward-backward algo-rithm.3.1 Syntax-Sensitive DistortionThe broad and robust success of the HMM align-ment model underscores the utility of its assump-tions: that word-level translations can be usefullymodeled via first-degree Markov transitions and in-dependent lexical productions.
However, its distor-tion model considers only string distance, disregard-ing the constituent structure of the English sentence.To allow syntax-sensitive distortion, we considera new distortion model of the form pd(aj |aj?
, j, t).We condition on t via a generative process that tran-sitions between two English positions by traversingthe unique shortest path ?(aj?
,aj ,t) through t fromaj?
to aj .
We constrain ourselves to this shortestpath using a staged generative process.Stage 1 (POP(n?
), STOP(n?
)): Starting in the leafnode at aj?
, we choose whether to STOP orPOP from child to parent, conditioning on thetype of the parent node n?.
Upon choosingSTOP, we transition to stage 2.Stage 2 (MOVE(n?, d)): Again, conditioning on thetype of the parent n?
of the current node n, wechoose a sibling n?
based on the signed distanced = ?n?
(n) ?
?n?(n?
), where ?n?
(n) is the indexof n in the child list of n?.
Zero distance movesare disallowed.
After exactly one MOVE, wetransition to stage 3.Stage 3 (PUSH(n, ?n(n?
))): Given the current noden, we select one of its children n?, conditioningon the type of n and the position of the child?n(n?).
We continue to PUSH until reaching aleaf.This process is a first-degree Markov walkthrough the tree, conditioning on the current nodeStage 1: { Pop(VBN), Pop(ADJP), Pop(VP), Stop(S) }Stage 2: { Move(S, -1) }Stage 3: { Push(NP, 1), Push(DT, 1) }SNPVPADJPNN VBNNNSDTAUXThe jobs are career oriented ..Figure 2: An example sequence of staged tree tran-sitions implied by the unique shortest path from theword oriented (aj?
= 5) to the word the (aj = 1).and its immediate surroundings at each step.
We en-force the property that ?(aj?
,aj ,t) be unique by stag-ing the process and disallowing zero distance movesin stage 2.
Figure 2 gives an example sequence oftree transitions for a small parse tree.The parameterization of this distortion model fol-lows directly from its generative process.
Given apath ?(aj?
,aj ,t) with r = k +m+3 nodes includingthe two leaves, the nearest common ancestor, k in-tervening nodes on the ascent and m on the descent,we express it as a triple of staged tree transitions thatinclude k POPs, a STOP, a MOVE, and m PUSHes:??
{POP(n2), ..., POP(nk+1), STOP(nk+2)}{MOVE (nk+2, ?(nk+3)?
?
(nk+1))}{PUSH (nk+3, ?
(nk+4)) , ..., PUSH (nr?1, ?(nr))}?
?Next, we assign probabilities to each tree transi-tion in each stage.
In selecting these distributions,we aim to maintain the original HMM?s sensitivityto target word order:?
Selecting POP or STOP is a simple Bernoullidistribution conditioned upon a node type.?
We model both MOVE and PUSH as multino-mial distributions over the signed distance inpositions (assuming a starting position of 0 forPUSH), echoing the parameterization popularin implementations of the HMM model.This model reduces to the classic HMM distor-tion model given minimal English trees of only uni-formly labeled pre-terminals and a root node.
Theclassic 0-distance distortion would correspond to the2000.20.40.6-2 -1 0 1 2 3 4 5Likelihood HMMSyntacticThiswouldrelievethepressureonoil.SVBDT .MD VPVPNP PPDT NN IN NNFigure 3: For this example sentence, the learned dis-tortion distribution of pd(aj |aj?
, j, t) resembles itscounterpart pd(aj |aj?
, j) of the HMM model but re-flects the constituent structure of the English tree t.For instance, the short path from relieve to on givesa high transition likelihood.STOP probability of the pre-terminal label; all otherdistances would correspond to MOVE probabilitiesconditioned on the root label, and the probability oftransitioning to the terminal state would correspondto the POP probability of the root label.As in a multinomial-distortion implementation ofthe classic HMM model, we must sometimes artifi-cially normalize these distributions in the deficientcase that certain jumps extend beyond the ends ofthe local rules.
For this reason, MOVE and PUSHare actually parameterized by three values: a nodetype, a signed distance, and a range of options thatdictates a normalization adjustment.Once each tree transition generates a score, theirproduct gives the probability of the entire path, andthereby the cost of the transition between string po-sitions.
Figure 3 shows an example learned distribu-tion that reflects the structure of the given parse.With these derivation steps in place, we must ad-dress a handful of special cases to complete the gen-erative model.
We require that the Markov walkfrom leaf to leaf of the English tree must start andend at the root, using the following assumptions.1.
Given no previous alignment, we forego stages1 and 2 and begin with a series of PUSHes fromthe root of the tree to the desired leaf.2.
Given no subsequent alignments, we skipstages 2 and 3 after a series of POPs includinga pop conditioned on the root node.3.
If the first choice in stage 1 is to STOP at thecurrent leaf, then stage 2 and 3 are unneces-sary.
Hence, a choice to STOP immediately isa choice to emit another foreign word from thecurrent English word.4.
We flatten unary transitions from the tree whencomputing distortion probabilities.5.
Null alignments are treated just as in the HMMmodel, incurring a fixed cost from any position.This model can be simplified by removing all con-ditioning on node types.
However, we found thisvariant to slightly underperform the full model de-scribed above.
Intuitively, types carry informationabout cross-linguistic ordering preferences.3.2 Training ApproachBecause our model largely mirrors the genera-tive process and structure of the original HMMmodel, we apply a nearly identical training proce-dure to fit the parameters to the training data via theExpectation-Maximization algorithm.
Och and Ney(2003) gives a detailed exposition of the technique.In the E-step, we employ the forward-backwardalgorithm and current parameters to find expectedcounts for each potential pair of links in each train-ing pair.
In this familiar dynamic programming ap-proach, we must compute the distortion probabilitiesfor each pair of English positions.The minimal path between two leaves in a tree canbe computed efficiently by first finding the path fromthe root to each leaf, then comparing those paths tofind the nearest common ancestor and a path throughit ?
requiring time linear in the height of the tree.Computing distortion costs independently for eachpair of words in the sentence imposed a computa-tional overhead of roughly 50% over the originalHMM model.
The bulk of this increase arises fromthe fact that distortion probabilities in this modelmust be computed for each unique tree, in contrast21to the original HMM which has the same distortionprobabilities for all sentences of a given length.In the M-step, we re-estimate the parameters ofthe model using the expected counts collected dur-ing the E-step.
All of the component distributionsof our lexical and distortion models are multinomi-als.
Thus, upon assuming these expectations as val-ues for the hidden alignment vectors, we maximizelikelihood of the training data simply by comput-ing relative frequencies for each component multi-nomial.
For the distortion model, an expected countc(aj , aj?)
is allocated to all tree transitions along thepath ?(aj?
,aj ,t).
These allocations are summed andnormalized for each tree transition type to completere-estimation.
The method of re-estimating the lexi-cal model remains unchanged.Initialization of the lexical model affects perfor-mance dramatically.
Using the simple but effectivejoint training technique of Liang et al (2006), weinitialized the model with lexical parameters from ajointly trained implementation of IBM Model 1.3.3 Improved Posterior InferenceLiang et al (2006) shows that thresholding the pos-terior probabilities of alignments improves AER rel-ative to computing Viterbi alignments.
That is, wechoose a threshold ?
(typically ?
= 0.5), and takea = {(i, j) : p(aj = i|f, e) > ?
}.Posterior thresholding provides computationallyconvenient ways to combine multiple alignments,and bidirectional combination often corrects forerrors in individual directional alignment models.Liang et al (2006) suggests a soft intersection of amodel m with a reverse model r (foreign to English)that thresholds the product of their posteriors at eachposition:a = {(i, j) : pm(aj = i|f, e) ?
pr(ai = j|f, e) > ?}
.These intersected alignments can be quite sparse,boosting precision at the expense of recall.
Weexplore a generalized version to this approach byvarying the function c that combines pm and pr:a = {(i, j) : c(pm, pr) > ?}.
If c is the max func-tion, we recover the (hard) union of the forward andreverse posterior alignments.
If c is the min func-tion, we recover the (hard) intersection.
A novel,high performing alternative is the soft union, whichwe evaluate in the next section:c(pm, pr) =pm(aj = i|f, e) + pr(ai = j|f, e)2.Syntax-alignment compatibility can be furtherpromoted with a simple posterior decoding heuristicwe call competitive thresholding.
Given a thresholdand a matrix c of combined weights for each pos-sible link in an alignment, we include a link (i, j)only if its weight cij is above-threshold and it is con-nected to the maximum weighted link in both row iand column j.
That is, only the maximum in eachcolumn and row and a contiguous enclosing span ofabove-threshold links are included in the alignment.3.4 Related WorkThis proposed model is not the first variant of theHMM model that incorporates syntax-based distor-tion.
Lopez and Resnik (2005) considers a sim-pler tree distance distortion model.
Daume?
III andMarcu (2005) employs a syntax-aware distortionmodel for aligning summaries to documents, butcondition upon the roots of the constituents that arejumped over during a transition, instead of those thatare visited during a walk through the tree.
In the caseof syntactic machine translation, we want to condi-tion on crossing constituent boundaries, even if noconstituents are skipped in the process.4 Experimental ResultsTo understand the behavior of this model, we com-puted the standard alignment error rate (AER) per-formance metric.2 We also investigated extraction-specific metrics: the frequency of interior nodes ?
ameasure of how often the alignments violate the con-stituent structure of English parses ?
and a variant ofthe CPER metric of Ayan and Dorr (2006).We evaluated the performance of our model onboth French-English and Chinese-English manuallyaligned data sets.
For Chinese, we trained on theFBIS corpus and the LDC bilingual dictionary, thentested on 491 hand-aligned sentences from the 20022The hand-aligned test data has been annotated with bothsure alignments S and possible alignments P , with S ?
P , ac-cording to the specifications described in Och and Ney (2003).With these alignments, we compute AER for a proposed align-ment A as:?1?
|A?S|+|A?P ||A|+|S|??
100%.22French Precision Recall AERClassic HMM 93.9 93.0 6.5Syntactic HMM 95.2 91.5 6.4GIZA++ 96.0 86.1 8.6Chinese Precision Recall AERClassic HMM 81.6 78.8 19.8Syntactic HMM 82.2 76.8 20.5GIZA++?
61.9 82.6 29.7Table 1: Alignment error rates (AER) for 100k train-ing sentences.
The evaluated alignments are a softunion for French and a hard union for Chinese, bothusing competitive thresholding decoding.
?FromAyan and Dorr (2006), grow-diag-final heuristic.NIST MT evaluation set.
For French, we used theHansards data from the NAACL 2003 Shared Task.3We trained on 100k sentences for each language.4.1 Alignment Error RateWe compared our model to the original HMMmodel, identical in implementation to our syntac-tic HMM model save the distortion component.Both models were initialized using the same jointlytrained Model 1 parameters (5 iterations), thentrained independently for 5 iterations.
Both modelswere then combined with an independently trainedHMM model in the opposite direction: f ?
e.4 Ta-ble 1 summarizes the results; the two models per-form similarly.
The main benefit of our model is theeffect on rule extraction, discussed below.We also compared our French results to the pub-lic baseline GIZA++ using the script published forthe NAACL 2006 Machine Translation WorkshopShared Task.5 Similarly, we compared our Chi-nese results to the GIZA++ results in Ayan andDorr (2006).
Our models substantially outperformGIZA++, confirming results in Liang et al (2006).Table 2 shows the effect on AER of competitivethresholding and different combination functions.3Following previous work, we developed our system on the37 provided validation sentences and the first 100 sentences ofthe corpus test set.
We used the remainder as a test set.4Null emission probabilities were fixed to 1|e| , inversely pro-portional to the length of the English sentence.
The decodingthreshold was held fixed at ?
= 0.5.5Training includes 16 iterations of various IBM models anda fixed null emission probability of .01.
The output of runningGIZA++ in both directions was combined via intersection.French w/o CT with CTHard Intersection (Min) 8.4 8.4Hard Union (Max) 12.3 7.7Soft Intersection (Product) 6.9 7.1Soft Union (Average) 6.7 6.4Chinese w/o CT with CTHard Intersection (Min) 27.4 27.4Hard Union (Max) 25.0 20.5Soft Intersection (Product) 25.0 25.2Soft Union (Average) 21.1 21.6Table 2: Alignment error rates (AER) by decodingmethod for the syntactic HMM model.
The compet-itive thresholding heuristic (CT) is particularly help-ful for the hard union combination method.The most dramatic effect of competitive threshold-ing is to improve alignment quality for hard unions.It also impacts rule extraction substantially.4.2 Rule Extraction ResultsWhile its competitive AER certainly speaks to thepotential utility of our syntactic distortion model, weproposed the model for a different purpose: to mini-mize the particularly troubling alignment errors thatcross constituent boundaries and violate the struc-ture of English parse trees.
We found that while theHMM and Syntactic models have very similar AER,they make substantially different errors.To investigate the differences, we measured thedegree to which each set of alignments violated thesupplied parse trees, by counting the frequency ofinterior nodes that are not null aligned.
Figure 4summarizes the results of the experiment for French:the Syntactic distortion with competitive threshold-ing reduces tree violations substantially.
Interiornode frequency is reduced by 56% overall, withthe most dramatic improvement observed for clausalconstituents.
We observed a similar 50% reductionfor the Chinese data.Additionally, we evaluated our model with thetransducer analog to the consistent phrase error rate(CPER) metric of Ayan and Dorr (2006).
This evalu-ation computes precision, recall, and F1 of the rulesextracted under a proposed alignment, relative to therules extracted under the gold-standard sure align-ments.
Table 3 shows improvements in F1 by using23Reduction(percent)NP54.114.6VP46.310.3PP52.46.3S77.54.8SBAR58.01.9Non-Terminals53.141.1All56.3100.0CorpusFrequency0.05.010.015.020.025.030.0InteriorNode Frequency(percent) HMM Model Syntactic Model + CTCorpus frequency:Reduction (percent): 38.9 47.2 45.3 54.8 59.7 43.7 45.114.6 10.3 6.3 4.8 1.9 41.1 100Figure 4: The syntactic distortion model with com-petitive thresholding decreases the frequency of in-terior nodes for each type and the whole corpus.the syntactic HMM model and competitive thresh-olding together.
Individually, each of these changescontributes substantially to this increase.
Together,their benefits are partially, but not fully, additive.5 ConclusionIn light of the need to reconcile word alignmentswith phrase structure trees for syntactic MT, we haveproposed an HMM-like model whose distortion issensitive to such trees.
Our model substantially re-duces the number of interior nodes in the alignedcorpus and improves rule extraction while nearlyretaining the speed and alignment accuracy of theHMM model.
While it remains to be seen whetherthese improvements impact final translation accu-racy, it is reasonable to hope that, all else equal,alignments which better respect syntactic correspon-dences will be superior for syntactic MT.ReferencesNecip Fazil Ayan and Bonnie J. Dorr.
2006.
Going beyond aer:An extensive analysis of word alignments and their impacton mt.
In ACL.Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,and Robert L. Mercer.
1994.
The mathematics of statisticalmachine translation: Parameter estimation.
ComputationalLinguistics, 19:263?311.Colin Cherry and Dekang Lin.
2006.
Soft syntactic constraintsfor word alignment through discriminative training.
In ACL.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In ACL.Hal Daume?
III and Daniel Marcu.
2005.
Induction of word andphrase alignments for automatic document summarization.Computational Linguistics, 31(4):505?530, December.French Prec.
Recall F1Classic HMM Baseline 40.9 17.6 24.6Syntactic HMM + CT 33.9 22.4 27.0Relative change -17% 27% 10%Chinese Prec.
Recall F1HMM Baseline (hard) 66.1 14.5 23.7HMM Baseline (soft) 36.7 39.1 37.8Syntactic + CT (hard) 48.0 41.6 44.6Syntactic + CT (soft) 32.9 48.7 39.2Relative change?
31% 6% 18%Table 3: Relative to the classic HMM baseline, oursyntactic distortion model with competitive thresh-olding improves the tradeoff between precision andrecall of extracted transducer rules.
Both Frenchaligners were decoded using the best-performingsoft union combiner.
For Chinese, we show alignersunder both soft and hard union combiners.
?Denotesrelative change from the second line to the third line.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In HLT-NAACL.Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006.
Scal-able inference and training of context-rich syntactic transla-tion models.
In ACL.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Alignment byagreement.
In HLT-NAACL.A.
Lopez and P. Resnik.
2005.
Improved hmm alignment mod-els for languages with scarce resources.
In ACL WPT-05.I.
Dan Melamed.
2004.
Algorithms for syntax-aware statisticalmachine translation.
In Proceedings of the Conference onTheoretical and Methodological Issues in Machine Transla-tion.Robert C. Moore.
2005.
A discriminative framework for bilin-gual word alignment.
In EMNLP.Hermann Ney and Stephan Vogel.
1996.
Hmm-based wordalignment in statistical translation.
In COLING.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29:19?51.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005.
Adiscriminative matching approach to word alignment.
InEMNLP.Dekai Wu.
1997.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
ComputationalLinguistics, 23:377?404.24
