CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 17?24Manchester, August 2008A Fast Boosting-based Learner for Feature-Rich Tagging and ChunkingTomoya Iwakura Seishi OkamotoFujitsu Laboratories Ltd.1-1, Kamikodanaka 4-chome, Nakahara-ku, Kawasaki 211-8588, Japan{iwakura.tomoya,seishi}@jp.fujitsu.comAbstractCombination of features contributes to asignificant improvement in accuracy ontasks such as part-of-speech (POS) tag-ging and text chunking, compared with us-ing atomic features.
However, selectingcombination of features on learning withlarge-scale and feature-rich training datarequires long training time.
We propose afast boosting-based algorithm for learningrules represented by combination of fea-tures.
Our algorithm constructs a set ofrules by repeating the process to select sev-eral rules from a small proportion of can-didate rules.
The candidate rules are gen-erated from a subset of all the features witha technique similar to beam search.
Thenwe propose POS tagging and text chunk-ing based on our learning algorithm.
Ourtagger and chunker use candidate POS tagsor chunk tags of each word collected fromautomatically tagged data.
We evaluateour methods with English POS tagging andtext chunking.
The experimental resultsshow that the training time of our algo-rithm are about 50 times faster than Sup-port Vector Machines with polynomial ker-nel on the average while maintaining state-of-the-art accuracy and faster classificationspeed.1 IntroductionSeveral boosting-based learning algorithms havebeen applied to Natural Language Processingproblems successfully.
These include text catego-rization (Schapire and Singer, 2000), Natural Lan-guage Parsing (Collins and Koo, 2005), Englishsyntactic chunking (Kudo et al, 2005) and so on.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.Furthermore, classifiers based on boosting-based learners have shown fast classification speed(Kudo et al, 2005).However, boosting-based learning algorithmsrequire long training time.
One of the reasons isthat boosting is a method to create a final hypoth-esis by repeatedly generating a weak hypothesis ineach training iteration with a given weak learner.These weak hypotheses are combined as the fi-nal hypothesis.
Furthermore, the training speedof boosting-based algorithms becomes more of aproblem when considering combination of featuresthat contributes to improvement in accuracy.This paper proposes a fast boosting-based algo-rithm for learning rules represented by combina-tion of features.
Our learning algorithm uses thefollowing methods to learn rules from large-scaletraining samples in a short time while maintainingaccuracy; 1) Using a rule learner that learns sev-eral rules as our weak learner while ensuring a re-duction in the theoretical upper bound of the train-ing error of a boosting algorithm, 2) Repeating tolearn rules from a small proportion of candidaterules that are generated from a subset of all the fea-tures with a technique similar to beam search, 3)Changing subsets of features used by weak learnerdynamically for alleviating overfitting.We also propose feature-rich POS tagging andtext chunking based on our learning algorithm.Our POS tagger and text chunker use candidatetags of each word obtained from automaticallytagged data as features.The experimental results with English POS tag-ging and text chunking show drastically improve-ment of training speeds while maintaining compet-itive accuracy compared with previous best resultsand fast classification speeds.2 Boosting-based Learner2.1 PreliminariesWe describe the problem treated by our boosting-based learner as follows.
Let X be the set of ex-amples and Y be a set of labels {?1,+1}.
LetF = {f1, f2, ..., fM} be M types of features rep-resented by strings.
Let S be a set of training sam-17## S = {(xi, yi)}mi=1: xi?
X ,yi?
{?1}## a smoothing value ?
=1## rule number r: the initial value is 1.Initialize: For i=1,...,m: w1,i= exp(12log(W+1W?1));While (r ?
R)## Train weak-learner using (S, {wr,i}mi=1)## Get ?
types of rules: {fj}?j=1{fj}?j=1?
weak-learner(S,{wr,i}mi=1);## Update weights with confidence valueForeach f ?
{fj}?j=1c =12log(Wr,+1(f)+?Wr,?1(f)+?
)For i=1,...,m: wr+1,i= wr,iexp(?yih?f ,c?
)fr= f ; cr= c; r++;endForeachendWhileOutput: F (x) = sign(log(W+1W?1) +PRr=1h?fr,cr?
(x))Figure 1: A generalized version of our learnerples {(x1, y1), ..., (xm, ym)}, where each examplexi?
X consists of features in F , which we call afeature-set, and yi?
Y is a class label.
The goal isto induce a mappingF : X ?
Yfrom S.Let |xi| (0 < |xi| ?
M) be the number of fea-tures included in a feature-set xi, which we callthe size of xi, and xi,j?
F (1 ?
j ?
|xi| ) be afeature included in xi.1We call a feature-set ofsize k a k-feature-set.
Then we define subsets offeature-sets as follows.Definition 1 Subsets of feature-setsIf a feature-set xjcontains all the features in afeature-set xi, then we call xiis a subset of xjanddenote it asxi?
xj.Then we define weak hypothesis based on theidea of the real-valued predictions and abstaining(RVPA, for short) (Schapire and Singer, 2000).2Definition 2 Weak hypothesis for feature-setsLet f be a feature-set, called a rule, x be afeature-set, and c be a real number, called a con-fidence value, then a weak-hypothesis for feature-sets is defined ash?f ,c?
(x) ={c f ?
x0 otherwise.1Our learner can handle binary vectors as in (Morishita,2002).
When our learner treats binary vectors for M attributes{X1,...,Xm}, the learner converts each vector to the corre-sponding feature-set as xi?
{fi|Xi,j?
Xi?
Xi,j= 1}(1 ?
i ?
m, 1 ?
j ?
M ).2We use the RVPA because training with RVPA is fasterthan training with Real-valued-predictions (RVP) while main-taining competitive accuracy (Schapire and Singer, 2000).The idea of RVP is to output a confidence value for sampleswhich do not satisfy the given condition too.2.2 Boosting-based Rule LearningOur boosting-based learner selects R types of rulesfor creating a final hypothesis F on several trainingiterations.
The F is defined asF (x) = sign(PRr=1h?fr,cr?
(x)).We use a learning algorithm that generatesseveral rules from a given training samplesS = {(xi, yi)}mi=1and weights over samples{wr,1, ..., wr,m} as input of our weak learner.
wr,iis the weight of sample number i after selectingr?1 types of rules, where 0<wr,i, 1 ?
i ?
m and1 ?
r ?
R.Given such input, the weak learner selects ?types of rules {fj}?j=1(fj?
F) with gain:gain(f)def= |pWr,+1(f)?pWr,?1(f)|,where f is a feature-set, and Wr,y(f) isWr,y(f) =Pmi=1wr,i[[f ?
xi?
yi= y]],and [[pi]] is 1 if a proposition pi holds and 0 other-wise.The weak learner selects a feature-set having thehighest gain as the first rule, and the weak learnerfinally selects ?
types of feature-sets having gainin top ?
as {fj}?j=1at each boosting iteration.Then the boosting-based learner calculates theconfidence value of each f in {fj}?j=1and updatesthe weight of each sample.
The confidence valuecjfor fjis defined ascj=12log(Wr,+1(fj)Wr,?1(fj)).After the calculation of cjfor fj, the learner up-dates the weight of each sample withwr+1,i= wr,iexp(?yih?fj,cj?).
(1)Then the learner adds (fj, cj) to F as the r-th rule and its confidence value.3When wecalculate the confidence value cj+1for fj+1, weuse {wr+1,1, ..., wr+1,m}.
The learner adds (fj+1,cj+1) to F as the r+1-th rule and confidence value.After the updates of weights with {fj}?j=1, thelearner starts the next boosting iteration.
Thelearner continues training until obtaining R rules.Our boosting-based algorithm differs from theother boosting algorithms in the number of ruleslearned at each iteration.
The other boosting-basedalgorithms usually learn a rule at each iteration3Eq.
(1) is the update of the AdaBoost used in ADTreeslearning algorithm (Freund and Mason, 1999).
We usethis AdaBoost by the following two reasons.
1) The pa-per (Iwakura and Okamoto, 2007) showed that the accuracyof text chunking with the AdaBoost of ADTrees is slightlyhigher than text chunking with the AdaBoost of BoosTexterfor RVPA (Schapire and Singer, 2000), 2) We expect the Ad-aBoost of ADTrees can realize faster training because this Ad-aBoost does not normalize weights at each update comparedwith the AdaBoost of BoosTexter normalizes weights at eachiteration.18## sortByW (F ,fq): Sort features (f ?
F )## in ascending order based on weights of features## (a % b): Return the reminder of (a?
b)## |B|-buckets: B = {B[0], ..., B[|B| ?
1]}procedure distFT(S, |B|)##Calculate the weight of each featureForeach (f?F) Wr(f) =Pmi=1wr,i[[{f} ?
xi]]##Sort features based on thier weights and## store the results in FsFs ?
sortByW (F ,Wr)## Distribute features to bucketsFor i=0...M : B[(i % |B|)] = (B[(i % |B|)] ?
Fs[i])return BFigure 2: Distribute features to buckets based on weights(Schapire and Singer, 2000; Freund and Mason,1999).
Despite the difference, our boosting-basedalgorithm ensures a reduction in the theoretical up-per bound of training error of the AdaBoost.
Welist the detailed explanation in Appendix.A.Figure 1 shows an overview of our boosting-based rule learner.
To avoid to happen thatWr,+1(f) or Wr,?1(f) is very small or even zero,we use the smoothed values ?
(Schapire andSinger, 1999).
Furthermore, to reflect imbalanceclass distribution, we use the default rule (Freundand Mason, 1999), defined as12log(W+1W?1), whereWy=?mi=1[[yi= y]] for y ?
{?1}.
The initialweights are defined with the default rule.3 Fast Rule Learner3.1 Generating Candidate RulesWe use a method to generate candidate rules with-out duplication (Iwakura and Okamoto, 2007).We denote f?= f + f as the generation of k + 1-feature-set f?consisting of a feature f and a k-feature-set f .
Let ID(f) be the integer corre-sponding to f , called id, and ?
be 0-feature-set.Then we define gen generating a feature-set asgen(f , f) =(f + f if ID(f) > maxf??fID(f?)?
otherwise.We assign smaller integer to more infrequent fea-tures as id.
If there are features having the samefrequency, we assign id to each feature with lexi-cographic order of features.
Training based on thiscandidate generation showed faster training speedthan generating candidates by an arbitrary order(Iwakura and Okamoto, 2007).3.2 Training with Redistributed FeaturesWe propose a method for learning rules by repeat-ing to select a rule from a small portion of can-didate rules.
We evaluated the effectiveness offour types of methods to learn a rule from a sub-set of features on boosting-based learners with atext chunking task (Iwakura and Okamoto, 2007).The results showed that Frequency-based distribu-tion (F-dist) has shown the best accuracy.
F-dist## Fk: A set of k-feature-sets## Ro: ?
optimal rules (feature-sets)## Rk,?
: ?
k-feature-sets for generating candidates## selectNBest(R, n, S, Wr): n best rules from R## with gain on {wi,r}mi=1and training samples Sprocedure weak-learner(Fk, S, Wr)## ?
best feature-sets as rulesRo= selectNBest( Ro?
Fk, ?, S, Wr);if (?
?
k) return Ro; ## Size constraint## ?
best feature-sets in Fkfor generating candidatesRk,?= selectNBest(Fk, ?, S, Wr);?
= minf?Rogain(f); ## The gain of ?-th optimal ruleForeach ( fk?
Rk,?
)if ( u(fk) < ?)
continue; ## Upper bound of gainForeach (f ?
F ) ## Generate candidatesfk+1= gen(fk, f);if (?
?Pmi=1[[fk+1?
xi]]) Fk+1= (Fk+1?
fk+1);end Foreachend Foreachreturn weak-learner(Fk+1, S,W );Figure 3: Find optimal feature-sets with given weightsdistributes features to subsets of features, calledbuckets, based on frequencies of features.However, we guess training using a subset offeatures depends on how to distribute features tobuckets like online learning algorithms that gener-ally depend on the order of the training examples(Kazama and Torisawa, 2007).To alleviate the dependency on selected buck-ets, we propose a method that redistributes fea-tures, called Weight-based distribution (W-dist).W-dist redistributes features to buckets based onthe weight of feature defined asWr(f) =Pmi=1wr,i[[{f} ?
xi]]for each f ?
F after examining all buckets.
Fig-ure 2 describes an overview of W-dist.3.3 Weak Learner for Learning Several RulesWe propose a weak learner that learns several rulesfrom a small portion of candidate rules.Figure 3 describes an overview of the weaklearner.
At each iteration, one of the |B|-bucketsis given as an initial 1-feature-sets F1.
The weaklearner finds ?
best feature-sets as rules from can-didates consisting of F1and feature-sets generatedfrom F1.
The weak learner generates candidates k-feature-sets (1 < k) from ?
best (k-1)-feature-setsin Fk?1with gain.We also use the following pruning techniques(Morishita, 2002; Kudo et al, 2005).?
Frequency constraint: We examine candidatesseen on at least ?
different examples.?
Size constraint: We examine candidates whosesize is no greater than a size threshold ?.?
Upper bound of gain: We use the upper boundof gain defined asu(f)def= max(pWr,+1(f),pWr,?1(f)).For any feature-set f?
?F , which contains f (i.e.f ?
f?
), the gain(f?)
is bounded under u(f), since0 ?
Wr,y(f?)
?
Wr,y(f) for y ?
{?1}.
Thus, if u(f)19## S = {(xi, yi)}mi=1: xi?X , yi?
{+1}## Wr= {wr,i}mi=1: Weights of samples after learning## r types of rules.
w1,i= 1 (1 ?
i ?
m)## |B| : The size of bucket B = {B[0], ..., B[|B| ?
1]}## b, r : The current bucket and rule numberprocedure AdaBoost.SDF()B = distFT(S, |B|); ## Distributing features into B## Initialize values and weights:r = 1; b = 0; c0=12log(W+1W?1);For i = 1,...,m: w1,i= exp(c0);While (r ?
R) ## Learning R types of rules##Select ?
rules and increment bucket id bR = weak-learner(B[b], S,Wr); b++;Foreach (f ?
R) ##Update weights with each rulec =12log(Wr,+1(f)+1Wr,?1(f)+1);For i=1,..,m wr+1,i= wr,iexp(?yih?f ,c?
);fr= f ; cr= c; r++;end Foreachif (b == |B|) ## RedistributionB = distFT(S, |B|); b=0;end ifend Whilereturn F (x) = sign(c0+PRr=1h?fr,cr?
(x))Figure 4: An overview of AdaBoost.SDF?
words, words that are turned into all capitalized,prefixes and suffixes (up to 4) in a 7-word window.?
labels assigned to three words on the right.?
whether the current word has a hyphen,a number, a capital letter?
whether the current word is all capital, all small?
candidate POS tags of words in a 7-word windowFigure 5: Feature types for POS taggingis less than the gain of the current optimal rule ?
,candidates containing f are safely pruned.Figure 4 describes an overview of our algorithm,which we call AdaBoost for a weak learner learn-ing Several rules from Distributed Features (Ad-aBoost.SDF, for short).The training of AdaBoost.SDF with (?
=1, ?
= ?, 1 < |B| ) is equivalent to the approachof AdaBoost.DF (Iwakura and Okamoto, 2007).
Ifwe use (|B| = 1,?
= 1), AdaBoost.SDF examinesall features on every iteration like (Freund and Ma-son, 1999; Schapire and Singer, 2000).4 POS tagging and Text Chunking4.1 English POS TaggingWe used the Penn Wall Street Journal treebank(Marcus et al, 1994).
We split the treebank intotraining (sections 0-18), development (sections 19-21) and test (sections 22-24) as in (Collins, 2002).We used the following candidate POS tags, calledcandidate feature, in addition to commonly usedfeatures (Gim?enez and M`arquez, 2003; Toutanovaet al, 2003) shown in Figure 5.We collect candidate POS tags of each wordfrom the automatically tagged corpus provided forthe shared task of English Named Entity recog-nition in CoNLL 2003.4The corpus includes17,003,926 words with POS tags and chunk tags4http://www.cnts.ua.ac.be/conll2003 /ner/?
words and POS tags in a 5-word window.?
labels assigned to two words on the right.?
candidate chunk tags of words in a 5-word windowFigure 6: Feature types for text chunkingannotated by a POS tagger and a text chunker.Thus, the corpus includes wrong POS tags andchunk tags.We collected candidate POS tags of words thatappear more than 9 times in the corpus.
We expressthese candidates with one of the following rangesdecided by their frequency fq; 10 ?
fq < 100,100 ?
fq < 1000 and 1000 ?
fq.For example, we express ?work?
annotated asNN 2000 times like ?1000?NN?.
If ?work?
is cur-rent word, we add 1000?NN as a candidate POStag feature of the current word.
If ?work?
appearsthe next of the current word, we add 1000?NN asa candidate POS tag of the next word.4.2 Text ChunkingWe used the data prepared for CoNLL-2000 sharedtasks.5This task aims to identify 10 types ofchunks, such as, NP, VP and PP, and so on.The data consists of subsets of Penn Wall StreetJournal treebank; training (sections 15-18) and test(section 20).
We prepared the development setfrom section 21 of the treebank as in (Tsuruokaand Tsujii, 2005).6Each base phrase consists of one word or more.To identify word chunks, we use IOE2 representa-tion.
The chunks are represented by the followingtags: E-X is used for end word of a chunk of classX.
I-X is used for non-end word in an X chunk.
Ois used for word outside of any chunk.For instance, ?
[He] (NP) [reckons] (VP) [thecurrent account deficit] (NP)...?
is represented byIOE2 as follows; ?He/E-NP reckons/E-VP the/I-NP current/I-NP account/I-NP deficit/E-NP?.We used features shown in Figure 6.
We col-lected the followings as candidate chunk tags fromthe same automatically tagged corpus used in POStagging.?
Candidate tags expressed with frequency infor-mation as in POS tagging?
The ranking of each candidate decided by fre-quencies in the automatically tagged data?
Candidate tags of each wordFor example, if we collect ?work?
anno-tated as I-NP 2000 times and as E-VP 100time, we generate the following candidate fea-tures for ?work?
; 1000?I-NP, 100?E-VP<1000,rank:I-NP=1 rank:E-NP=2, candidate=I-NP andcandidate=E-VP.5http://lcg-www.uia.ac.be/conll2000/chunking/6We used http://ilk.uvt.nl/?sabine/chunklink/chunklink 2-2-2000 for conll.plfor creating development data.20Table 1: Training data for experiments: ] of S, M , ] ofcl and av. ]
of ft indicate the number samples, the distinctnumber of feature types, the number of class in each data set,and the average number of features, respectively.
POS andETC indicate POS-tagging and text chunking.
The ?-c?
in-dicates using candidate features collected from parsed unla-beled data.data ] of S M ] of cl av. ]
of ftPOS 912,344 579,052 45 22.09POS-c 912,344 579,793 45 35.39ETC 211,727 92,825 22 11.37ETC-c 211,727 93,333 22 45.49We converted the chunk representation of theautomatically tagged corpus to IOE2 and we col-lected chunk tags of each word appearing morethan nine times.4.3 Applying AdaBoost.SDFAdaBoost.SDF treats the binary classificationproblem.
To extend AdaBoost.SDF to multi-class,we used the one-vs-the-rest method.To identify proper tag sequences, we use Viterbisearch.
We map the confidence value of each clas-sifier into the range of 0 to 1 with sigmoid function7, and select a tag sequence which maximizes thesum of those log values by Viterbi search.5 Experiments5.1 Experimental SettingsWe compared AdaBoost.SDF with Support Vec-tor Machines (SVM).
SVM has shown good per-formance on POS tagging (Gim?enez and M`arquez,2003) and Text Chunking (Kudo and Matsumoto,2001).
Furthermore, SVM with polynomial kernelimplicitly expands all feature combinations with-out increasing the computational costs.
Thus, wecompared AdaBoost.SDF with SVM.8To evaluate the effectiveness of candidate fea-tures, we examined two types of experiments withcandidate features and without them.
We list thestatics of training sets in Table 1.We tested R=100,000, |B|=1,000, ?
={1,10,100}, ?={1,10,100,?
}, ?={1,2,3}, and?={1,5} for AdaBoost.SDF.
We tested the softmargin parameter C={0.1,1,10} and the kerneldegree d={1,2,3} for SVM.9We used the followings for comparison; Train-ing time is time to learn 100,000 rules.
Best train-ing time is time for generating rules to show thebest F-measure (F?=1) on development data.
Ac-curacy is F?=1on a test data with the rules at besttraining time.7s(X) = 1/(1 + exp(?
?X)), where X = F (x) is aoutput of a classifier.
We used ?=5 in this experiment.8We used TinySVM (http://chasen.org/?taku/software/TinySVM/).9We used machines with 2.66 GHz QuadCore Intel Xeonand 10 GB of memory for all the experiments.Table 2: Experimental results of POS tagging and TextChunking (TC) with candidate features.
F and time indicatethe average F?=1of test data and time (hour) to learn 100,000rules for all classes with F-dist.
These results are listed sepa-rately with respect to each ?
= {1, 5}.?
POS(?
= 1) POS (?
= 5) TC (?
= 1) TC (?
= 5)F time F time F time F time1 97.27 196.3 97.23 195.7 93.98 145.3 93.95 155.810 97.23 23.05 97.17 22.35 93.96 2.69 93.88 2.70100 96.82 2.99 96.83 2.91 93.16 0.74 93.14 0.56888990919293940  2  4  6  8  10Accuracy(F-measure)Training Time (hour)?=3 ?=?
?=1?=3 ?=?
?=10?=3 ?=?
?=100?=3 ?=1 ?=1?=3 ?=1 ?=10?=3 ?=1 ?=100?=3 ?=10 ?=1?=3 ?=10?=10?=3 ?=10 ?=100?=3 ?=100 ?=1?=3 ?=100 ?=10?=3 ?=100 ?=100Figure 7: Accuracy on development data of Text Chunk-ing (?
= 3) obtained with parsers based on F-dist.
We mea-sured accuracy obtained with rules at each training time.
Thewidest line is AdaBoost.SDF (?=1,?=?).
The others are Ad-aBoost.SDF (?=10(?),?=100(?
),?=1&?={1,10,100}).5.2 Effectiveness of Several Rule LearningTable 2 shows average accuracy and training time.We used F-dist as the distribution method.
Theseaverage accuracy obtained with rules learned byAdaBoost.SDF (?=10) on both tasks are competi-tive with the average accuracy obtained with ruleslearned by AdaBoost.SDF (?=1).
These resultshave shown that learning several rules at each iter-ation contributes significant improvement of train-ing time.
These results have also shown that thelearning several rule at each iteration methods aremore efficient than training by just using the fre-quency constraint ?.Figure 7 shows a snapshot for accuracy ob-tained with chunkers using different number ofrules.
This graph shows that chunkers basedon AdaBoost.SDF (?=10,100) and AdaBoost.SDF(?=1,?={1,10,100}) have shown better accuracy thanchunkers based on AdaBoost.SDF (?=1,?=?)
ateach training time.
These result have shown thatlearning several rules at each iteration and learningcombination of features as rules with a techniquesimilar to beam search are effective in improvingtraining time while giving a better convergence.Figure 7 also implies that taggers and chunkersbased on AdaBoost.SDF (?=100) will show betteror competitive accuracy than accuracy of the oth-ers by increasing numbers of rules to be learnedwhile maintaining faster convergence speed.21Table 3: Experimental results on POS tagging and TextChunking.
Accuracies (F?=1) on test data and training time(hour) of AdaBoost.SDF are averages of ?={1,10,100,?}
foreach ?
with F-dist and ?
= 1.
F?=1and time (hour) of SVMsare averages of C={0.1,1,10} for each kernel parameter d.POS tagging without candidate featuresAlg.
/ 1 2 3?
(d) F?=1time F?=1time F?=1time?=1 96.96 5.09 97.10 27.90 97.10 30.92?=10 96.89 0.79 97.12 4.56 97.07 4.74?=100 96.57 0.10 96.82 0.81 96.73 0.81SVMs 96.60 101.63 97.15 166.76 96.93 625.32POS tagging with candidate featuresAlg.
/ 1 2 3?
(d) F?=1time F?=1time F?=1time?=1 97.06 6.65 97.30 109.20 97.29 330.82?=10 96.98 1.27 97.29 13.26 97.23 38.27?=100 96.61 0.14 96.93 1.64 96.76 5.05SVMs 96.76 170.24 97.31 206.39 97.23 1346.04Text Chunking without candidate featuresAlg.
/ 1 2 3?
(d) F?=1time F?=1time F?=1time?=1 92.50 0.12 93.60 0.26 93.47 0.41?=10 92.34 0.02 93.50 0.05 93.39 0.07?=100 89.70 0.008 92.31 0.02 92.03 0.02SVMs 92.14 8.55 93.91 7.38 93.49 9.82Text Chunking with candidate featuresAlg.
/ 1 2 3?
(d) F?=1time F?=1time F?=1time?=1 92.89 0.25 94.19 26.10 94.04 300.77?=10 92.85 0.04 94.11 2.97 94.08 3.06?=100 91.99 0.01 93.37 0.32 93.24 0.34SVMs 92.77 12.74 94.31 9.63 94.20 49.275.3 Comparison with SVMTable 3 lists average accuracy and training timeon POS tagging and text chunking with respectto each (?, ?)
for AdaBoost.SDF and d for SVM.AdaBoost.SDF with?=10and?=100have shownmuch faster training speeds than SVM and Ad-aBoost.SDF (?=1,?=?)
that is equivalent to theAdaBoost.DF (Iwakura and Okamoto, 2007).Furthermore, the accuracy of taggers and chun-kers based on AdaBoost.SDF (?=10) have showncompetitive accuracy with those of SVM-basedand AdaBoost.DF-based taggers and chunkers.AdaBoost.SDF (?=10) showed about 6 and 54times faster training speeds than those of Ad-aBoost.DF on the average in POS tagging and textchunking.
AdaBoost.SDF (?=10) showed about147 and 9 times faster training speeds than thetraining speeds of SVM on the average of POStagging and text chunking.
On the average of theboth tasks, AdaBoost.SDF (?=10) showed about25 and 50 times faster training speed than Ad-aBoost.DF and SVM.
These results have shownthat AdaBoost.SDF with a moderate parameter ?can improve training time drastically while main-taining accuracy.These results in Table 3 have also shown thatrules represented by combination of features andthe candidate features collected from automati-cally tagged data contribute to improved accuracy.5.4 Effectiveness of RedistributionWe compared F?=1and best training time of F-dist and W-dist.
We used ?
= 2 that has shownTable 4: Results obtained with taggers and chunkers basedon F-dist and W-dist.
These results obtained with taggers andchunkers trained with ?
= {1, 10, 100,?}
and ?
= 2.
Fand time indicate average F?=1on test data and average besttraining time.POS tagging with F-dist?
?=1 ?=10 ?=100 ?=?F time F time F time F time1 97.31 30.03 97.31 64.25 97.32 142.9 97.26 89.5910 97.26 3.21 97.32 9.57 97.30 15.54 97.30 19.64100 96.86 0.62 96.95 1.32 96.95 2.13 96.96 2.43POS tagging with W-dist?
?=1 ?=10 ?=100 ?=?F time F time F time F time1 97.32 29.96 97.31 57.05 97.31 163.2 97.32 98.7110 97.24 2.66 97.30 25.70 97.28 16.20 97.29 20.49100 97.00 0.54 97.02 1.31 97.07 2.22 97.08 2.58Text Chunking with F-dist?
?=1 ?=10 ?=100 ?=?F time F time F time F time1 93.95 7.42 94.30 23.30 94.22 34.74 94.31 21.2610 93.99 0.98 94.08 2.44 94.19 3.11 94.18 3.18100 93.32 0.16 93.33 0.32 93.42 0.40 93.42 0.40Text Chunking with W-dist?
?=1 ?=10 ?=100 ?=?F time F time F time F time1 93.99 2.93 94.24 24.77 94.32 35.72 94.32 35.6110 93.98 0.71 94.30 2.82 94.29 3.60 94.30 4.05100 93.66 0.17 93.65 0.36 93.50 0.42 93.50 0.42better average accuracy than ?
= {1, 3} in bothtasks.
Table 4 lists comparison of F-dist and W-dist on POS tagging and text chunking.
Most ofaccuracy obtained with W-dist-based taggers andparsers better than accuracy obtained with F-dist-based taggers and parsers.
These results haveshown that W-dist improves accuracy without dras-tically increasing training time.
The text chunkerand the tagger trained with AdaBoost.SDF (?
= 10,?
= 10 and W-dist) has shown competitive accu-racy with that of the chunker trained with Ad-aBoost.SDF (?
= 1, ?
= ?
and F-dist) while main-taining about 7.5 times faster training speed.5.5 Tagging and Chunking SpeedsWe measured testing speeds of taggers and chun-kers based on rules or models listed in Table 5.10We examined two types of fast classification al-gorithms for polynomial kernel: Polynomial Ker-nel Inverted (PKI) and Polynomial Kernel Ex-panded (PKE).
The PKI leads to about 2 to 12times improvements, and the PKE leads to 30 to300 compared with normal classification approachof SVM (Kudo and Matsumoto, 2003).11The POS-taggers based on AdaBoost.SDF,SVM with PKI, and SVM with PKE processed4,052 words, 159 words, and 1,676 words per sec-ond, respectively.
The chunkers based on thesethree methods processed 2,732 words, 113 words,and 1,718 words per second, respectively.10We list average speeds of three times tests measured witha machine with Xeon 3.8 GHz CPU and 4 GB of memory.11We use a chunker YamCha for evaluating classificationspeeds based on PKI or PKE (http://www.chasen.org/?taku/software/yamcha/).
We list the average speeds of SVM-based tagger andchunker with PKE of a threshold parameter ?
= 0.0005 forrule selection in both task.
The accuracy obtained with mod-els converted by PKE are slightly lower than the accuracy ob-tained with their original models in our experiments.22Table 5: Comparison with previous best results: (Top :POS tagging, Bottom: Text Chunking )POS tagging F?=1Perceptron (Collins, 2002) 97.11Dep.
Networks (Toutanova et al, 2003) 97.24SVM (Gim?enez and M`arquez, 2003) 97.05ME based a bidirectional inference (Tsuruoka and Tsujii, 2005) 97.15Guided learning for bidirectional sequence classification (Shen et al, 2007) 97.33AdaBoost.SDF with candidate features (?=2,?=1,?=100, W-dist) 97.32AdaBoost.SDF with candidate features (?=2,?=10,?=10, F-dist) 97.32SVM with candidate features (C=0.1, d=2) 97.32Text Chunking F?=1Regularized Winnow + full parser output (Zhang et al, 2001) 94.17SVM-voting (Kudo and Matsumoto, 2001) 93.91ASO + unlabeled data (Ando and Zhang, 2005) 94.39CRF+Reranking(Kudo et al, 2005) 94.12ME based a bidirectional inference (Tsuruoka and Tsujii, 2005) 93.70LaSo (Approximate Large Margin Update) (Daum?e III and Marcu, 2005) 94.4HySOL (Suzuki et al, 2007) 94.36AdaBoost.SDF with candidate featuers (?=2,?=1,?=?, W-dist) 94.32AdaBoost.SDF with candidate featuers (?=2,?=10,?=10,W-dist) 94.30SVM with candidate features (C=1, d=2) 94.31One of the reasons that boosting-based classi-fiers realize faster classification speed is sparsenessof rules.
SVM learns a final hypothesis as a linearcombination of the training examples using somecoefficients.
In contrast, this boosting-based rulelearner learns a final hypothesis that is a subset ofcandidate rules (Kudo and Matsumoto, 2004).6 Related Works6.1 Comparison with Previous Best ResultsWe list previous best results on English POS tag-ging and Text chunking in Table 5.
These resultsobtained with the taggers and chunkers based onAdaBoost.SDF and SVM showed competitive F-measure with previous best results.
These showthat candidate features contribute to create state-of-the-art taggers and chunkers.These results have also shown thatAdaBoost.SDF-based taggers and chunkersshow competitive accuracy by learning combi-nation of features automatically.
Most of theseprevious works manually selected combinationof features except for SVM with polynomialkernel and (Kudo and Matsumoto, 2001) aboosting-based re-ranking (Kudo et al, 2005).6.2 Comparison with Boosting-basedLearnersLazyBoosting randomly selects a small proportionof features and selects a rule represented by a fea-ture from the selected features at each iteration(Escudero et al, 2000).Collins and Koo proposed a method only up-dates values of features co-occurring with a rulefeature on examples at each iteration (Collins andKoo, 2005).Kudo et al proposed to perform several pseudoiterations for converging fast (Kudo et al, 2005)with features in the cache that maintains the fea-tures explored in the previous iterations.AdaBoost.MHKRlearns a weak-hypothesis rep-resented by a set of rules at each boosting iteration(Sebastiani et al, 2000).AdaBoost.SDF differs from previous works inthe followings.
AdaBoost.SDF learns several rulesat each boosting iteration like AdaBoost.MHKR.However, the confidence value of each hypothe-sis in AdaBoost.MHKRdoes not always minimizethe upper bound of training error for AdaBoostbecause the value of each hypothesis consists ofthe sum of the confidence value of each rule.Compared with AdaBoost.MHKR, AdaBoost.SDFcomputes the confidence value of each rule to min-imize the upper bound of training error on givenweights of samples at each update.Furthermore, AdaBoost.SDF learns severalrules represented by combination of features fromlimited search spaces at each boosting itera-tion.
The creation of subsets of features in Ad-aBoost.SDF enables us to recreate the same classi-fier with same parameters and training data.
Recre-ation is not ensured in the random selection of sub-sets in LazyBoosting.7 ConclusionWe have proposed a fast boosting-based learner,which we call AdaBoost.SDF.
AdaBoost.SDF re-peats to learn several rules represented by combi-nation of features from a small proportion of can-didate rules.
We have also proposed methods touse candidate POS tags and chunk tags of eachword obtained from automatically tagged data asfeatures in POS tagging and text chunking.The experimental results have shown drasticallyimprovement of training speed while maintainingcompetitive accuracy compared with previous bestresults.Future work should examine our approach onseveral tasks.
Future work should also compareour algorithm with other learning algorithms.Appendix A: ConvergenceThe upper bound of the training error for AdaBoostof (Freund and Mason, 1999), which is used in Ad-aBoost.SDF, is induced by adopting THEOREM 1presented in (Schapire and Singer, 1999).
Let ZRbe?mi=1wR+1,ithat is a sum of weights updatedwith R rules.
The bound holds on the training er-ror after selecting R rules,Pmi=1[[F (xi) 6= yi]] ?
ZRis induced as follows.By unraveling the Eq.
(1), we obtainwR+1,i= exp(?yi?Rr=1h?fr,cr?(xi)).
Thus, weobtain [[F (xi) 6= yi]] ?
exp(?yiPRt=1h?fr,cr?
(xi)),since if F (xi) 6= yi, then exp(?yiPRr=1h?fr?
(xi)) ?1 .
Combining these equations gives the statedbound on training error23mXi=1[[F (xi) 6= yi]] ?mXi=1exp(?yiRXt=1h?fr,cr?
(xi))=mXi=1wR+1,i= ZR.
(2)Then we show that the upper bound of training er-ror ZRfor R rules shown in Eq.
(2) is less than orequal to the upper bound of the training error ZR?1for R-1 rules.
By unraveling the (2) and plug-ging the confidence values cR= {12log(Wr,+1(fR)Wr,?1(fR)), 0} given by the weak hypothesis into the unraveledequation, we obtain ZR?ZR?1, sinceZR=mXi=1wR+1,i=mXi=1wR,iexp(?yih?fR,cR?
)=mXi=1wR,i?Wr,+1(fR)?Wr,+1(fR) +Wr,+1(fR)exp(?cR) +Wr,?1(fR)exp(cR)= ZR?1?
(pWR,+1(fR)?pWR,?1(fR))2ReferencesAndo, Rie and Tong Zhang.
2005.
A high-performance semi-supervised learning method for text chunking.
In Proc.
of43rd Meeting of Association for Computational Linguis-tics, pages 1?9.Collins, Michael and Terry Koo.
2005.
Discriminativereranking for natural language parsing.
ComputationalLinguistics, 31(1):25?70.Collins, Michael.
2002.
Discriminative training methodsfor Hidden Markov Models: theory and experiments withperceptron algorithms.
In Proc.
of the 2002 Conferenceon Empirical Methods in Natural Language Processing,pages 1?8.Daum?e III, Hal and Daniel Marcu.
2005.
Learning assearch optimization: Approximate large margin methodsfor structured prediction.
In Proc.
of 22th InternationalConference on Machine Learning, pages 169?176.Escudero, Gerard, Llu?
?s M`arquez, and German Rigau.
2000.Boosting applied to word sense disambiguation.
In Proc.of 11th European Conference on Machine Learning, pages129?141.Freund, Yoav and Llew Mason.
1999.
The alternating de-cision tree learning algorithm,.
In Proc.
of 16th Interna-tional Conference on Machine Learning, pages 124?133.Gim?enez, Jes?us and Llu?
?s M`arquez.
2003.
Fast and accu-rate part-of-speech tagging: The SVM approach revisited.In Proc.
of International Conference Recent Advances inNatural Language Processing 2003, pages 153?163.Iwakura, Tomoya and Seishi Okamoto.
2007.
Fast trainingmethods of boosting algorithms for text analysis.
In Proc.of International Conference Recent Advances in NaturalLanguage Processing 2007, pages 274?279.Kazama, Jun?ichi and Kentaro Torisawa.
2007.
A new per-ceptron algorithm for sequence labeling with non-localfeatures.
In Proc.
of the 2007 Joint Conference on Empiri-cal Methods in Natural Language Processing and Compu-tational Natural Language Learning, pages 315?324.Kudo, Taku and Yuji Matsumoto.
2001.
Chunking with Sup-port Vector Machines.
In Proc.
of The Conference of theNorth American Chapter of the Association for Computa-tional Linguistics, pages 192?199.Kudo, Taku and Yuji Matsumoto.
2003.
Fast methods forkernel-based text analysis.
In Proc.
of 41st Meeting of As-sociation for Computational Linguistics, pages 24?31.Kudo, Taku and Yuji Matsumoto.
2004.
A boosting algo-rithm for classification of semi-structured text.
In Proc.of the 2004 Conference on Empirical Methods in NaturalLanguage Processing 2004, pages 301?308, July.Kudo, Taku, Jun Suzuki, and Hideki Isozaki.
2005.Boosting-based parse reranking with subtree features.
InProc.
of 43rd Meeting of Association for ComputationalLinguistics, pages 189?196.Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated corpusof english: The Penn Treebank.
pages 313?330.Morishita, Shinichi.
2002.
Computing optimal hypothesesefficiently for boosting.
Proc.
of 5th International Confer-ence Discovery Science, pages 471?481.Schapire, Robert E. and Yoram Singer.
1999.
Improvedboosting using confidence-rated predictions.
MachineLearning, 37(3):297?336.Schapire, Robert E. and Yoram Singer.
2000.
Boostexter:A boosting-based system for text categorization.
MachineLearning, 39(2/3):135?168.Sebastiani, Fabrizio, Alessandro Sperduti, and Nicola Val-dambrini.
2000.
An improved boosting algorithm and itsapplication to text categorization.
In Proc.
of InternationalConference on Information and Knowledge Management,pages 78?85.Shen, Libin, Giorgio Satta, and Aravind Joshi.
2007.
Guidedlearning for bidirectional sequence classification.
In Proc.of 45th Meeting of Association for Computational Linguis-tics, pages 760?767.Suzuki, Jun, Akinori Fujino, and Hideki Isozaki.
2007.
Semi-supervised structured output learning based on a hybridgenerative and discriminative approach.
In Proc.
of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Natural Lan-guage Learning, pages 791?800.Toutanova, Kristina, Dan Klein, Christopher D. Manning, andYoram Singer.
2003.
Feature-rich part-of-speech taggingwith a cyclic dependency network.
In Proc.
of the 2003Human Language Technology Conference of the NorthAmerican Chapter of the Association for ComputationalLinguistics, pages 173?180.Tsuruoka, Yoshimasa and Junichi Tsujii.
2005.
Bidirec-tional inference with the easiest-first strategy for taggingsequence data.
In Proc.
of Human Language TechnologyConference and Conference on Empirical Methods in Nat-ural Language Processing, pages 467?474.Zhang, Tong, Fred Damerau, and David Johnson.
2001.Text chunking using regularized winnow.
In Proc.
of39th Meeting of Association for Computational Linguis-tics, pages 539?546.24
