Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 261?264,New York City, June 2006. c?2006 Association for Computational LinguisticsSmartNotes: Implicit Labeling of Meeting Datathrough User Note?Taking and BrowsingSatanjeev BanerjeeLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213banerjee@cs.cmu.eduAlexander I. RudnickySchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213air@cs.cmu.eduAbstractWe have implemented SmartNotes, a sys-tem that automatically acquires labeledmeeting data as users take notes duringmeetings and browse the notes afterwards.Such data can enable meeting understand-ing components such as topic and ac-tion item detectors to automatically im-prove their performance over a sequenceof meetings.
The SmartNotes system con-sists of a laptop based note taking appli-cation, and a web based note retrieval sys-tem.
We shall demonstrate the functional-ities of this system, and will also demon-strate the labeled data obtained during typ-ical meetings and browsing sessions.1 Goals of the SmartNotes SystemMost institutions hold a large number of meetingsevery day.
Several of these meetings are important,and meeting participants need to recall the detailsof the discussions at a future date.
In a previoussurvey (Banerjee et al, 2005) of busy professors atCarnegie Mellon University we showed that meetingparticipants needed to recall details of past meetingson average about twice a month.
Performing suchretrieval is not an easy task.
It is time consuming;in our study participants took on average between15 minutes to an hour to recall the information theywere seeking.
Further, the quality of the retrievalis dependent on whether or not the participants hadaccess to the notes at the meeting.
On a scale of 0to 5, with 5 denoting complete satisfaction with re-trieval results, participants reported a satisfaction of3.4 when they did not have notes, and 4.0 when theydid.Despite the prevalence of important meetings andthe importance of notes, there is a relative paucity oftechnology to help meeting participants take noteseasily at meetings.
Some commercial applicationsallow users to take notes (e.g.
OneNote1) and evenrecord audio/video (e.g.
Quindi2), but no product at-tempts to automatically take notes.
Our long termgoal is to create a system that makes note?takingeasier by performing tasks such as automaticallyhighlighting portions of the meeting that are likelyto be important to the user, automatically detecting?note?worthy?
phrases spoken during the meeting,etc.To perform such note taking, the system needs toform an understanding of the meeting.
Our shortterm goal is to create a system that can detect thetopics of discussion, the action items being dis-cussed, and the roles of the meeting participants.Additionally, these components must adapt to spe-cific users and groups of users since different peoplewill likely take different notes at the same meeting.Thus we wish to implement the note taking systemin such a way that the user?s interactions with thesystem result in labeled meeting data that can thenbe used to adapt and improve the meeting under-standing components.Towards these goals, we have built SmartNoteswhich helps users easily record and retrieve notes.1http://office.microsoft.com/onenote2http://www.quindi.com261The system also records the user interactions to formlabeled meeting data that can later be used to auto-matically improve the meeting understanding com-ponents.
In the next section we describe the meetingunderstanding components in more detail.
Next wedescribe SmartNotes itself, and show how it is cur-rently helping users take and retrieve notes, whileacquiring labeled data to aid each of the meeting un-derstanding components.
Finally we end with a dis-cussion of what functionality we plan to demonstrateat the conference.2 Automatic Meeting UnderstandingTopic detection and segmentation: We are at-tempting to automatically detect the topics beingdiscussed at meetings.
This task consists of two sub-tasks: discovering the points in a meeting when thetopic changes, and then associating a descriptive la-bel to the segment between two topic shifts.
Our cur-rent strategy for topic shift detection (Banerjee andRudnicky, 2006a) is to perform an edge detectionusing such features as speech activity (who spokewhen and for how long), the words that each per-son spoke, etc.
For labeling, we are currently sim-ply associating the agenda item names recorded inthe notes with the segments they are most relevantto, as decided by a tf.idf matching technique.
Topicdetection is particularly useful during meeting infor-mation retrieval; (Banerjee et al, 2005) showed thatwhen users wish to retrieve information from pastmeetings, they are typically interested in a specificdiscussion topic, as opposed to an entire meeting.Action item detection: An obvious applicationof meeting understanding is the automatic discoveryand recording of action items as they are discussedduring a meeting.
Arguably one of the most impor-tant outcomes of a meeting are the action items de-cided upon, and automatically recording them couldbe a huge benefit especially to those participants thatare likely to not note them down and consequentlyforget about them later on.Meeting participant role detection: Each meet-ing participant plays a variety of roles in an insti-tution.
These roles can be based on their functionin the institution (managers, assistants, professors,students, etc), or based on their expertise (speechrecognition experts, facilities experts, etc).
Our cur-rent strategy for role detection (Banerjee and Rud-nicky, 2006b) is to train detectors on hand labeleddata.
Our next step is to perform discovery of newroles through clustering techniques.
Detecting suchroles has several benefits.
First, it allows us to buildprior expectations of a meeting between a group ofparticipants.
For example, if we know person A isa speech recognition expert and person B a speechsynthesis expert, a reasonable expectation is thatwhen they meet they are likely to talk about tech-nologies related speech processing.
Consequently,we can use this expectation to aid the action itemdetection and the topic detection in that meeting.3 SmartNotes: System DescriptionWe have implemented SmartNotes to help userstake multi?media notes during meetings, and re-trieve them later on.
SmartNotes consists of two ma-jor components: The note taking application whichmeeting participants use to take notes during themeeting, and the note retrieval application whichusers use to retrieve notes at a later point.3.1 SmartNotes Note Taking ApplicationThe note taking application is a stand?alone system,that runs on each meeting participant?s laptop, andallows him to take notes during the meeting.
In ad-dition to recording the text notes, it also records theparticipant?s speech, and video, if a video camera isconnected to the laptop.
This system is an extensionof the Carnegie Mellon Meeting Recorder (Banerjeeet al, 2004).Figure 1 shows a screen?shot of this application.It is a server?client application, and each participantlogs into a central server at the beginning of eachmeeting.
Thus, the system knows the precise iden-tity of each note taker as well as each speaker inthe meeting.
This allows us to avoid the onerousproblem of automatically detecting who is speakingat any time during the meeting.
Further, after log-ging on, each client automatically synchronizes it-self with a central NTP time server.
Thus the timestamps that each client associates with its recordingsare all synchronized, to facilitate merging and playback of audio/video during browsing (described inthe next sub?section).Once logged in, each participant?s note taking262Figure 1: Screen shot of the SmartNotes note?taking clientarea is split into two sections: a shared note takingarea, and a private note taking area.
Notes writtenin the shared area are viewable by all meeting par-ticipants.
This allows meeting participants to sharethe task of taking notes during a meeting: As long asone participant has recorded an important point dur-ing a meeting, the other participants do not need to,thus making the note taking task easier for the groupas a whole.
Private notes that a participant does notwish to share with all participants can be taken in theprivate note taking area.The interface has a mechanism to allow meetingparticipants to insert an agenda into the shared area.Once inserted, the shared area is split into as manyboxes as there are agenda items.
Participants canthen take notes during the discussion of an agendaitem in the corresponding agenda item box.
Thisis useful to the participants because it organizes thenotes as they are being taken, and, additionally, thenotes can later be retrieved agenda item by agendaitem.
Thus, the user can access all notes he has takenin different meetings regarding ?buying a printer?,without having to see the notes taken for the otheragenda items in each such meeting.In addition to being useful to the user, this act ofinserting an agenda and then taking notes within therelevant agenda item box results in generating (un-beknownst to the participant) labeled data for thetopic detection component.
Specifically, if we de-fine each agenda item as being a separate ?topic?,and make the assumption that notes are taken ap-proximately concurrent with the discussion of thecontents of the notes, then we can conclude thatthere is a shift in the topic of discussion at somepoint between the time stamp on the last note inan agenda item box, and the time stamp on the firstnote of the next agenda item box.
This informationcan then be used to improve the performance of thetopic shift detector.
The accuracy of the topic shiftdata thus acquired depends on the length of time be-tween the two time points.
Since this length is easyto calculate automatically, this information can befactored into the topic detector trainer.The interface also allows participants to enter ac-tion items through a dedicated action item form.Again the advantage of such a form to the partici-pants is that the action items (and thus the notes) arebetter organized: After the meeting, they can per-form retrieval on specific fields of the action items.For example, they can ask to retrieve all the actionitems assigned to a particular participant, or that aredue a particular day, etc.In addition to being beneficial to the participant,the action item form filling action results in gener-ating labeled data for the action item detector.Specifically, if we make the assumption that an ac-tion item form filling action is preceded by a discus-sion of the action item, then the system can couplethe contents of the form with all the speech withina window of time before the form filling action, anduse this pair as a data point to retrain its action itemdetector.3.2 SmartNotes Note Retrieval WebsiteAs notes and audio/video are recorded on each indi-vidual participant?s laptop, they also get transferredover the internet to a central meeting server.
Thistransfer occurs in the background without any in-tervention from the user, utilizes only the left?overbandwidth beyond the user?s current bandwidth us-age, and is robust to system shut?downs, crashes,etc.
This process is described in more detail in(Banerjee et al, 2004).Once the meeting is over and all the data has beentransferred to the central server, meeting participantscan use the SmartNotes multi?media notes retrievalsystem to view the notes and access the recordedaudio/video.
This is a web?based application thatuses the same login process as the stand?along note263Figure 2: Screen shot of the SmartNotes websitetaking system.
Users can view a list of meetingsthey have recorded using the SmartNotes applica-tion in the past, and then for each meeting, they canview the shared notes taken at the meeting.
Figure2 shows a screen shot of such a notes browsing ses-sion.
Additionally, participants can view their ownprivate notes taken during the meeting.In addition to viewing the notes, they can also ac-cess all recorded audio/video, indexed by the notes.That is, they can access the audio/video recordedaround the time that the note was entered.
Furtherthey can specify how many minutes before and af-ter the note they wish to access.
Since the serverhas the audio from each meeting participant?s audiochannel, the viewer of the notes can choose to listento any one person?s channel, or a combination of theaudio channels.
The merging of channels is done inreal time and is achievable because their time stampshave been synchronized during recording.In the immediate future we plan to implement asimple key?word based search on the notes recordedin all the recorded meetings (or in one specific meet-ing).
This search will return notes that match thesearch using a standard tf.idf approach.
The userwill also be provided the option of rating the qual-ity of the search retrieval on a one bit satisfied/not?satisfied scale.
If the user chooses to provide thisrating, it can be used as a feedback to improve thesearch.
Additionally, which parts of the meeting theuser chooses to access the audio/video from can beused to form a model of the parts of the meetingsmost relevant to the user.
This information can helpthe system tailor its retrieval to individual prefer-ences.4 The DemonstrationWe shall demonstrate both the SmartNotes note tak-ing client as well as the SmartNotes note?retrievalwebsite.
Specifically we will perform 2 minute longmock meetings between 2 or 3 demonstrators.
Wewill show how notes can be taken, how agendas canbe created and action items noted.
We will thenshow how the notes and the audio/video from the 2minute meeting can be accessed through the Smart-Notes note retrieval website.
We shall also show theautomatically labeled data that gets created both dur-ing the mock meeting, as well as during the brows-ing session.
Finally, if time permits, we shall showresults on how much we can improve the meetingunderstanding components?
capabilities through la-beled meeting data automatically acquired throughparticipants?
use of SmartNotes at CMU and otherinstitutions that are currently using the system.ReferencesS.
Banerjee and A. I. Rudnicky.
2006a.
A texttilingbased approach to topic boundary detection in multi?participant conversations.
Submitted for publication.S.
Banerjee and A. I. Rudnicky.
2006b.
You are whatyou say: Using meeting participants?
speech to detecttheir roles and expertise.
In Analyzing Conversationsin Text and Speech Workshop at HLT?NAACL 2006,New York City, USA, June.S.
Banerjee, J. Cohen, T. Quisel, A. Chan, Y. Pato-dia, Z. Al-Bawab, R. Zhang, P. Rybski, M. Veloso,A.
Black, R. Stern, R. Rosenfeld, and A. I. Rudnicky.2004.
Creating multi-modal, user?centric records ofmeetings with the Carnegie Mellon meeting recorderarchitecture.
In Proceedings of the ICASSP MeetingRecognition Workshop, Montreal, Canada.S.
Banerjee, C. Rose, and A. I. Rudnicky.
2005.
Thenecessity of a meeting recording and playback system,and the benefit of topic?level annotations to meetingbrowsing.
In Proceedings of the Tenth InternationalConference on Human-Computer Interaction, Rome,Italy, September.264
