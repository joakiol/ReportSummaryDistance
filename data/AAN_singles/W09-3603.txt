Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 19?26,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPAccurate Argumentative Zoning with Maximum Entropy modelsStephen Merity and Tara Murphy and James R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australia{smerity,tm,james}@it.usyd.edu.auAbstractWe present a maximum entropy classifierthat significantly improves the accuracy ofArgumentative Zoning in scientific litera-ture.
We examine the features used toachieve this result and experiment withArgumentative Zoning as a sequence tag-ging task, decoded with Viterbi using upto four previous classification decisions.The result is a 23% F-score increase on theComputational Linguistics conference pa-pers marked up by Teufel (1999).Finally, we demonstrate the performanceof our system in different scientific do-mains by applying it to a corpus of As-tronomy journal articles annotated using amodified Argumentative Zoning scheme.1 IntroductionThe task of generating automatic summarizationsof one or more texts is a central problem in Natu-ral Language Processing (NLP).
Summarization isa fundamental component for future informationretrieval and question answering systems, incor-porating both natural language understanding andnatural language generation.Comprehension-based summarization, e.g.Kintsch and Van Dijk (1978) and Brown et al(1983), is the most ambitious model of automaticsummarization, requiring a complete understand-ing of the text.
Due to the failure of rule-basedNLP and knowledge representation, other lessknowledge-intensive methods now dominate.Sentence extraction, e.g.
Brandow et al (1995)and Kupiec et al (1995), selects a small numberof abstract worthy sentences from a larger text.The resulting sentences form a collection of ex-cerpt sentences meant to capture the essence of thetext.
The next stage is information fusion (Barzi-lay et al, 1999; Knight and Marcu, 2000) whichattempts to combine the excerpts into a more cohe-sive text.
These methods can create inflexible andincoherent extracts that result in under-informativeresults (Teufel et al, 1999).Argumentative Zoning (Teufel, 1999; Teufeland Moens, 2002) attempts to solve this prob-lem by representing the structure of a text us-ing a rhetorically-based schema.
Sentences areclassified into one of a small number of non-hierarchical argumentative roles, which can thenbe used in both the sentence extraction and textgeneration/fusion phase of automatic summariza-tion.
Argumentative Zoning can enable tailoredsummarizations depending on the needs of theuser, e.g.
a layperson versus a domain expert.The first experiments in Argumentative Zon-ing used Na?
?ve Bayes (NB) classifiers (Kupiec etal., 1995; Teufel, 1999) which assume conditionalindependence of the features.
However, this as-sumption is rarely true for the kinds of rich featurerepresentations we want to use for most NLP tasks.Maximum entropy (ME) models have becomepopular in NLP because they can incorporate evi-dence from the complex, diverse and overlappingfeatures needed to represent language.
Some ex-ample applications include part-of-speech (POS)tagging (Ratnaparkhi, 1996), parsing (Johnson etal., 1999), language modelling (Rosenfeld, 1996),and text categorisation (Nigam et al, 1999).We have developed an Argumentative Zoning(zone) classifier using a ME model.
We compareour zone classifier to a reimplementation of Teufeland Moens (2002)?s NB classifier and features ontheir original Computational Linguistics corpus.Like Teufel (1999), we model zone classificationas a sequence tagging task.
Our zone classifierachieves an F-score of 96.88%, a 20% improve-ment.
We also show how Argumentative Zoningcan be applied to other domains by evaluating oursystem on a corpus of Astronomy journal articles,achieving an F-measure of 97.9%.19Category Abbr.
DescriptionBackground BKG general scientific backgroundOther OTH neutral descriptions of other researcher?s workOwn OWN neutral descriptions of the authors?
new workAim AIM statements of the particular aim of the current paperTextual TXT statements of textual organisation of the current paperContrast CTR contrastive or comparative statements about other workexplicit mention of weaknesses of other workBasis BAS statements that own work is based on other workTable 1: Teufel?s (1999) Argumentative Zones2 Argumentative ZoningTeufel (1999) introduced a new rhetorical analy-sis for scientific texts called Argumentative Zon-ing.
Each sentence of an article from the scien-tific literature is classified into one of seven basicrhetorical structures shown in Table 1.The first three: Background, Other, and Own,are part of the basic schema and represent attribu-tion of intellectual ownership.
The four additionalcategories: aim, textual, contrast, and basis, arebased upon Swales (1990)?s Creating A ResearchSpace (CARS) model, and provide pointed infor-mation about the author?s stance and the paper it-self.
Teufel assumes that each sentence only re-quires a single classification and that all sentencesclearly fit into the above structure.
The assump-tion is clearly not always correct, but is a usefulapproximation nevertheless.Due to the specific nature of these classifica-tions it is hoped that this will allow for much morerobust automatic abstraction generation.
Sum-maries of a paper could be created specifically forthe user, either focusing on the aim of the work,the work?s stance in the field (what other works itis based upon or compared with) and so on.Teufel used Argumentative Zoning to determinethe author?s use and opinion of other authors theycite in their work and also to create RhetoricalDocument Profiles (RDP), a type of summariza-tion used to provide typical information that a newreader may need in a systematic manner.For the use of Argumentative Zoning in RDPsTeufel (1999) points out that due to the redun-dancy in language that near perfect accuracy is notrequired as important pieces of information will berepeated in the paper.
Recognising these salientpoints once is enough for them to be included inthe RDP.
In further tasks, such as the analysis ofthe function of citations (Teufel et al, 2006) andautomatic summarization, higher levels of accu-racy are more critical.3 Maximum Entropy modelsMaximum entropy (ME) or log-linear models arestatistical models that can incorporate evidencefrom a diverse range of complex and potentiallyoverlapping features.
Unlike Na?
?ve Bayes (NB),the features can be conditionally dependent giventhe class, which is important since feature sets inNLP rarely satisfy this independence constraint.The ME classifier uses models of the form:p(y|x) =1Z(x)exp(n?i=1?ifi(x, y))(1)where y is the zone label, x is the context (the sen-tence) and the fi(x, y) are the features with asso-ciated weights ?i.The probability of a sequence of zone labelsy1 .
.
.
yn given a sequence of sentences is s1 .
.
.
snis approximated as follows:p(y1 .
.
.
yn|s1 .
.
.
sn) ?n?i=1p(yi|xi) (2)where xi is the context for sentence si.
In our ex-periments that treat argumentative zoning as a se-quence labelling task, the context xi incorporateshistory information ?
i.e.
the previous labellingdecisions of the classifier.
Optimal decoding ofthis sequence uses the Viterbi algorithm, which wecompare against the Oracle case of knowing thecorrect label for the previous sentence.The features are binary valued functions whichpair a zone label with various elements of the sen-tential context; for example:fj(x, y) ={1 if goal ?
x & y = AIM0 otherwise (3)goal ?
x, that is, the word goal is part of thecontext of the sentence, is a contextual predicate.The central idea in maximum entropy mod-elling is that the model chosen should satisfy all ofthe constraints imposed by the training data (in the20form of empirical feature counts from the train-ing data) whilst remaining as unbiased as possi-ble.
This is achieved by selecting the model withthe maximum entropy, i.e.
the most uniform dis-tribution, given the constraints.Our classifier uses the maximum entropy imple-mentation described in Curran and Clark (2003).Generalised Iterative Scaling (GIS) is used to esti-mate the values of the weights and we use a Gaus-sian prior over the weights (Chen and Rosenfeld,1999) which allows many rare, but informative,features to be used without overfitting.
This willbe an important property when we use sparse fea-tures like bigrams in the models below.4 Modelling Argumentative Zones4.1 Our FeaturesThe two primary sources of features for our zoneclassifier were the words in the sentences and theposition of the sentence relative to the rest of thepaper.
A number of feature types use additionalexternal resources (e.g.
semantic lists of agents orcommon rhetorical patterns) or annotations (e.g.named entities).
Where feasible we have reimple-mented the features described in Teufel (1999).
Inother cases, our features are somewhat simpler.Since the Curran and Clark (2003) classifieronly accepts binary features, any numerical fea-tures had to be bucketed into smaller sets of alter-natives to reduce sparseness, either by integer di-vision or through reducing the number by scalingto a small integer range.
The features we imple-mented are described below.Unigrams, bigrams and n-gramsA sub-sequence of n words from a given sentence.We include unigram and bigram features and re-port them individually and together (as n-grams).These features include all of the unigrams andbigrams above the feature cutoff, unlike Teufel?scont-1 features below.
Also, both the Compu-tational Linguistics and Astronomy corpora con-tain marked up citations, cross-references to ta-bles, figures, and sections and mathematical ex-pressions.
In the Computational Linguistics cor-pus self citations are distinguished from other ci-tations.
These structured elements have been nor-malised to a single token each, e.g.
__CITE__.These tokens have been retained in the unigramand bigram features.first The first four words of a sentence, added in-dividually.Sections, positions, and lengthssection A section counter which increments oneach heading to measure the distance into the doc-ument.
It does not take into consideration whetherthey are sub-headings or similar.
There are twoversions of this feature.
The first is a straightcounter (1 to n) and the second is grouped into twobuckets representing each half of the paper (break-ing at the middle section).location The position of a sentence between twoheadings (representing a section).
There are twoversions of this feature, one counts to a maxi-mum of 10 and the other represents a percentagethrough the section bucketed into 20% intervals.paragraph The position of the sentence within aparagraph.
Again there are two features ?
eitherstraight counts (with a maximum of 10) or buck-eted into thirds of a paragraph.length of sentence grouped into multiples of 3.Named entity featuresOur astronomy corpus has been manually anno-tated with domain-specific named entity informa-tion (Murphy et al, 2006).
There are 12 coarse-grained categories and 43 fine-grained categoriesincluding star, galaxy, telescope, as well as a num-ber of the usual categories including person, or-ganisation and location.
Both the coarse-grainedand fine-grained categories were used as features.4.2 Teufel (1999)?s featuresTo compare with previous work, we also im-plemented most of the features that gave Teufel(1999) the best performance.
We list all of the fea-ture types in Table 2, indicating which ones haveand have not been implemented.Teufel?s unigram features (cont-1) are filteredusing TF-IDF to select the top scoring 10 words ineach document, and then these are used to markthe top 40 sentences in each document containingthose filtered words.TLoc marks the position of the sentence overthe entire paper, using 10 unevenly sized segments(larger segments are in the middle of the paper).Struct-1 marks where a sentence appears in asection.
It divides each section into three equallysized segments; singles out the first and the lastsentence as separate segments; the second and21Name Impl?
DescriptionCont-1 yes An application of TF-IDF over the words and sentencesCont-2 partial Does the sentence contain words in the title or heading (excluding stop words)TLoc yes Position of the sentence in relation to 10 segments (A-J)Struct-1 yes Position within a sectionStruct-2 yes Relative position of sentence within a paragraphStruct-3 partial Type of headline of the current sectionTLength yes Is the sentence longer than 15 words?Syn-1 no Voice of the first finite verb in the sentenceSyn-2 no Tense of the first finite verb in the sentenceSyn-3 no Is the first finite verb modified by a modal auxiliaryCit-1 yes Does the sentence contain a citation or name of author?Formu yes Does a formulaic expression occur in the sentenceAg-1 yes Type of agentAg-2 yes Type of action (with or without negation)Table 2: Teufel (1999)?s set of featuresthird sentence as a sixth segment; and the second-last plus third-last sentence as a seventh segment.Struct-3 the type of section heading for the cur-rent section.
In our case, we have not mappedthese down to the reduced set used by Teufel.Formu uses pattern matching rules to iden-tify formulaic expressions.
Ag-1 and Ag-2 iden-tify agent and action expressions from gazetteers.Teufel (1999) provides these in the appendices.4.3 Feature CutoffFeatures that occur rarely in the training set areproblematic because the statistics extracted forthese features are not reliable.
They may still con-tribute positively to the ME model because we useGaussian smoothing (Chen and Rosenfeld, 1999)help avoid overfitting.Instead of including every possible feature, weused a cutoff to remove features that occur lessthan four times.
This primarily applies to then-gram features, especially bigrams, which werequite sparse given the small quantity of trainingdata.
Due to the speed of the ME implementationit is possible to have quite a low cut-off.4.4 History features and ViterbiIn order to take advantage of the predictabil-ity of tags given prior sequences (for example,AIM commonly following itself) we used historyfeatures and treated Argumentative Zoning as asequence labelling task.
Since each predictionnow relies on the previous decisions we used theViterbi algorithm to find the optimal sequence.Given the small number of labelling alter-natives, we experimented with several historylengths ranging from previous label to the previ-ous four labels.
To determine the impact of thisfeature in an ideal situation, we also experimentedwith using an Oracle set of history features.5 ResultsOur results are produced using ten-fold cross val-idation and are reported in terms of precision, re-call and f-score for each of the zone classes, and aweighted average over all classes.
We have inves-tigated the impact of each feature type using sub-tractive analysis, where we have also calculatedpaired t-test confidence intervals (the error valuesreported are the 95% confidence interval).The baselines for both sets were already quitehigh (at least 70%) due to the common tag ofOWN, representing the author?s own work, but ourresults show significant improvements over thisbaseline.5.1 CMP-LG CorpusThe CMP-LG corpus is a collection of 80 con-ference papers collected by Teufel (1999) fromthe Computation and Language E-Print Archive 1.The LATEX source was converted to HTML with La-tex2HTML then transformed into XML with cus-tom PERL scripts.
This text was then tokenized us-ing the TTT (Text Tokenization) System into PennTreebank format.
The result is a corpus of 12,000annotated sentences, containing 333,000 word to-kens, in XML format.We attempted to recreate Teufel?s original ex-periments by emulating the features she used withthe same type of classifier.
We usedWeka?s (Franket al, 2005) implementation of the NB classifier.Table 3 reproduces the results from Teufel andMoens (2002) alongside our reimplementation of1http://xxx.lanl.gov/cmp-lg/22original reproducedTag P R F P R FAIM 44 65 52 45.8 57.8 51.1BAS 37 40 38 23.8 37.0 28.9CTR 34 20 26 33.1 19.2 24.3BKG 40 50 45 46.9 53.6 50.1OTH 52 39 44 70.6 55.0 61.8TXT 57 66 61 66.3 47.6 55.4OWN 84 88 86 86.7 90.8 88.7Weighted 72 73 72 76.8 76.8 76.8Table 3: Teufel and Moens (2002)?s and our NBperformance on CMP-LGHistory Type Order PerformanceBaseline None 93.16Viterbi First 1.77 ?
0.49%Viterbi Second 1.97 ?
0.42%Viterbi Third 2.08 ?
0.45%Viterbi Fourth 2.1 ?
0.46%Viterbi Fifth 2.13 ?
0.46%Oracle First 3.67 ?
0.68%Oracle Second 4.06 ?
0.70%Table 4: History features on the CMP-LG corpuswith ME model of unigram/bigram features onlyFeature Classifier ViterbiNgrams -21.39?2.35% -23.23?3.24%Unigram -8.00?1.02% -7.53?1.14%Bigram -7.89?1.20% -6.87?1.44%Concept -0.06?0.24% -0.06?0.16%First -1.24?0.44% -1.14?0.39%Length -0.34?0.24% -0.40?0.25%Section -0.42?0.27% -0.27?0.33%Location 0.03?0.20% 0.04?0.07%Paragraph 0.10?0.15% 0.01?0.08%All 95.69% 96.88%Table 5: Subtractive analysis CMP-LG ME modelthe features using Weka?s NB classifier.
We havebeen able to replicate their results to a reasonableextent ?
gaining higher overall performance usingmost of their original features.
Notably, our Otherclass is significantly more accurate whilst the orig-inal Basis class did better.Our next experiment investigated the value oftreating Argumentative Zoning as a sequence la-belling task, i.e.
the impact of the Markov historyfeatures and Viterbi decoding on performance.
Forthese experiments we only used the unigram andbigram features with the maximum entropy clas-sifier.
Table 4 presents the results: the baseline isalready much higher than the NB classifier whichis a result of both the unigram/bigram features andthe ME classifier itself.The improvement using longer Markov win-dows (up to 2.13%) is also shown ?
and longerwindows are better, although there is diminishingreturns.
We chose a Markov history of the fourprevious decisions for the rest of our experiments.Table 4 also shows that knowing the previous labelperfectly (with the Oracle experiment) can make alarge difference to classification accuracy.Feature ChangeTLength -2.09?9.96%Struct-1 0.38?6.08%TLoc 0.96?7.25%Struct-3 -1.65?6.76%Cont-2 -1.10?6.39%Struct-2 1.59?7.99%Ag-1/2 -0.39?8.97%Formu 0.14?8.46%Cit-1 -1.88?5.19%Cont-1 -0.38?5.85%All 70.25%Table 6: Teufel?s Subtractive analysis CMP-LG METable 5 presents the subtractive analysis to de-termine the impact of different feature types.
Fromthis we can see that the n-grams (unigrams and bi-grams) have by far the largest impact ?
and neitherof these feature types was directly implemented byTeufel and Moens (2002).
The next most impor-tant features are the first few words (again a uni-gram type feature), length and the section number.The Markov history features also have an impactof just over 1%.Table 6 shows a different story for Teufel?s fea-tures using the maximum entropy model.
It seemsthat none of the feature types alone are making anenormous contribution and that the impact of themvaries enormously between folds (the confidenceintervals are far bigger than the differences).Finally, Table 7 gives the results of using themaximum entropy model with Markov historylength four and all of the features.
Overall, weimprove Teufel and Moens?
performance by justunder 20% on our reproduced experiments.5.2 Astronomical CorpusThe astronomical corpus was created by Mur-phy et al (2006) and consists of papers obtainedfrom arXiv (2005)?s astrophysics section (astro-ph).
The papers were converted from LATEX toUnicode by a custom script which attempted to re-tain as much of the paper?s special characters andformatting as possible.The resulting text was then processed usingMXTerminator (Reynar and Ratnaparkhi, 1997)with an additional Python script to find sentence23Category Abbr.
DescriptionBackground BKG As has been noted in prior studies , Abell|GXYC 2255|GXYC has an unusually large numberof galaxies with extended radio emission .Other OTH This is consistent with the findings of Hogg|P Fruchter|P ( 1999|DAT ) who found that GRBhosts are in general subluminous galaxies .Own OWN We scanned the data of about 1.8|DUR year|DUR ( TJDs|DUR 11000-11699|DUR ) and found30 new GRB-like events .Data DAT In Fig .
REF we present the 1.4|FRQ GHz|FRQ radio images of the cluster A2744|GXYC ,at different angular resolutions .
(subclassed from OWN)Observation OBS Smith|P et al ( 2001|DAT ) reported no detection of transient emission at sub-mm ( 850|WAVum|WAV ) wavelengths .
(subclassed from OTH)Technique TEC Reduction of the NIR images was performed with the IRAF|CODE and STSDAS|CODE pack-ages .
(subclassed from OWN)Figure 1: Examples of sentences with the given tags in the astronomical corpusTag P R FAIM 96.5 88.2 92.2BAS 86.7 89.8 88.2CTR 92.1 89.0 90.5BKG 86.0 96.3 90.9OTH 96.3 91.7 93.9TXT 98.2 93.8 95.9OWN 98.6 99.2 98.9Weighted 96.88 96.88 96.88Table 7: Final CMP-LG ME performanceFeature Classifier ViterbiNgrams -18.83?3.74% -16.03?2.99%Unigram -5.51?1.37% -5.25?2.00%Bigram -2.04?0.78% -1.79?0.87%Concept -0.18?0.29% -0.05?0.12%Entity -0.18?0.39% -0.31?0.23%First -0.02?0.29% -0.86?0.79%Length -0.06?0.16% -0.08?0.10%Paragraph -0.04?0.20% 0.07?0.19%Section -0.29?0.24% -0.40?0.57%Location -0.09?0.25% 0.06?0.15%All 98.15% 96.68%Table 8: Subtractive analysis ASTRO ME modelboundaries, and then tokenized using the PennTreebank (Marcus et al, 1993) sed script, with an-other Python script fixing common errors.
TheLATEX, which the tokenizer split off incorrectly,was then reattached.Each sentence of the corpus was then anno-tated using a modified version of the Argumen-tative Zoning schema.
While the original threezones: Background, Own, Other are used, we havereplaced the CARS labels with content labels de-scribing aspects of the work: DAT for data usedin the analysis, OBS for observations performed,and TEC for techniques applied.
Only Own andOther are subclassed with the extended schema ofData, Observation and Techniques.
Examples ofeach zone classification are shown in Figure 1.Tag P R FBKG 92.1 97.1 94.5OTH 95.0 97.1 96.1OTH-DAT 100.0 92.3 96.0OTH-OBS 91.3 93.3 92.3OTH-TEC 100.0 100.0 100.0OWN 99.9 99.3 99.6OWN-DAT 95.9 86.6 91.0OWN-OBS 98.2 89.4 93.6OWN-TEC 90.4 100.0 94.9Weighted 97.9 97.9 97.9Table 9: Final ASTRO ME model performanceTable 8 shows the impact of different featuretypes on classification accuracy for the Astron-omy corpus.
Again, the most important featuresare the n-grams (although to a slightly lesser ex-tent than for the Computational Linguistics cor-pus).
The other features make very little contri-bution at all.
Disappointingly, the (gold-standard)named entity features contribute very little addi-tional information ?
which is surprising given thatthe content categories (data and observation) aredirectly connected with some of the entity types(like telescope).In the Astronomy corpus, the Markov historyfeatures actually have a detrimental effect, whichsuggests the history is misleading.
This warrantsfurther exploration, but we suspect there may bemore changing backwards and forwards betweenargumentative zones in the Astronomy corpus.Overall, we can see that the two tasks are of a sim-ilar level of difficulty of around 96% F-score.Table 9 shows the distribution over zones andcontent labels for the Astronomy corpus.
TheBackground label is the hardest to reproduce eventhough it is not split into content sub-types.
Thesub-types are relatively rare for Other, so the re-sults should not be considered as reliable.24Tag P R FBKG NB CMP-LG 51.5% 61.1% 55.9%OTH NB CMP-LG 73.0% 64.2% 68.3%OWN NB CMP-LG 91.9% 93.1% 92.5%BKG NB ASTRO 63.1% 63.5% 63.3%OTH NB ASTRO 53.9% 39.7% 45.7%OWN NB ASTRO 88.5% 93.0% 90.7%BKG ME CMP-LG 53.6% 27.5% 36.3%OTH ME CMP-LG 63.0% 24.4% 35.2%OWN ME CMP-LG 81.7% 96.8% 88.6%BKG ME ASTRO 61.2% 29.5% 39.8%OTH ME ASTRO 50.4% 20.0% 28.6%OWN ME ASTRO 81.2% 96.7% 88.2%Table 10: Comparing CMP-LG and ASTRO directlyon the basic annotation schemeTable 10 compares the performance of ourNa?
?ve Bayes and Maximum Entropy classifierson the two corpora for just the basic annotationscheme: Background, Own and Other.
The fea-tures used are the set of Teufel features we haveimplemented (so it does not include unigram orbigram features).The results show that classifiers for both cor-pora behave in quite similar ways on the basicscheme.
Own is by far the most frequent category,and not surprisingly, it is most accurately classi-fied in both domains.
Background seems to be eas-ier to distinguish in Astronomy, but Other is moredistinct in Computational Linguistics.Further, we see no advantage to using maximumentropy models over Na?
?ve Bayes when the fea-ture set is not sophisticated/overlapping enough,and the dataset large enough, to warrant the extrapower (and cost).6 ConclusionThis paper has presented newmodels of Argumen-tative Zoning using Maximum Entropy (ME) mod-els.
We have demonstrated that using ME modelswith standard word features, such as unigrams andbigrams, significantly outperforms Na?
?ve Bayesmodels incorporating task-specific features.
Fur-ther, these task-specific features had very little ad-ditional impact on the ME model.Our ME model has raised the state-of-the-artin automatic Argumentative Zoning classificationfrom 76% to 96.88% F-score on Teufel?s Compu-tational Linguistics conference paper corpus.To test the wider applicability of ArgumentativeZoning, we have annotated a corpus of Astronomyjournal articles with a modified zone and contentscheme, and achieved a similar level of perfor-mance using our maximum entropy classifier.
Wefound that more sophisticated semantic features,e.g.
gold-standard named entities, also had littleimpact on the accuracy of our classifier.Now that we have a very accurate Argumenta-tive Zone classifier, we would like to investigatethe impact of Argumentative Zones in informationretrieval, question answering, and summarizationtasks, particularly in the astronomy domain, wherewe have additional tools such as the named entityrecognizer.In summary, using a maximum entropy classi-fier with simple unigram and bigram features re-sults in a very accurate classifier for Argumenta-tive Zones across multiple domains.AcknowledgementsWe would like to thank Sophie Liang and theanonymous reviewers for their helpful feedbackon this paper.
This work has been supported bythe Australian Research Council under Discoveryproject DP0665973.
The first author was sup-ported by the Microsoft Research Asia Scholar-ship in IT at the University of Sydney.ReferencesarXiv.
2005. arxiv.org archive.
http://arxiv.org.R.
Barzilay, K.R.
McKeown, and M. Elhadad.
1999.Information fusion in the context of multi-documentsummarization.
In Proceedings of the 37th an-nual meeting of the Association for ComputationalLinguistics on Computational Linguistics, pages550?557.
Association for Computational LinguisticsMorristown, NJ, USA.R.
Brandow, K. Mitze, and L.F. Rau.
1995.
Automaticcondensation of electronic publications by sentenceselection.
Information Processing and management,31(5):675?685.A.L.
Brown, J.D.
Day, and R.S.
Jones.
1983.
Thedevelopment of plans for summarizing texts.
ChildDevelopment, pages 968?979.Stanley Chen and Ronald Rosenfeld.
1999.
A Gaus-sian prior for smoothing maximum entropy models.Technical report, Carnegie Mellon University, Pitts-burgh, PA.James R. Curran and Stephen Clark.
2003.
Investigat-ing GIS and smoothing for maximum entropy tag-gers.
In Proceedings of the 10th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 91?98, Budapest, Hungary,12?17 April.25E.
Frank, M.A.
Hall, G. Holmes, R. Kirkby,B.
Pfahringer, I.H.
Witten, and L. Trigg.
2005.Weka-a machine learning workbench for data min-ing.
The Data Mining and Knowledge DiscoveryHandbook, pages 1305?1314.M.
Johnson, S. Geman, S. Canon, Z. Chi, and S. Rie-zler.
1999.
Estimators for stochastic ?unification-based?
grammars.
In Proceedings of the 37th Meet-ing of the ACL, pages 535?541, University of Mary-land, MD.W.
Kintsch and T.A.
Van Dijk.
1978.
Toward a modelof text comprehension and production.
Psychologi-cal review, 85(5):363?94.K.
Knight and D. Marcu.
2000.
Statistics-basedsummarization-step one: Sentence compression.
InProceedings of the National Conference on Artifi-cial Intelligence, pages 703?710.
Menlo Park, CA;Cambridge, MA; London; AAAI Press; MIT Press;1999.J.
Kupiec, J. Pedersen, and F. Chen.
1995.
A train-able document summarizer.
In Proceedings of the18th annual international ACM SIGIR conferenceon Research and development in information re-trieval, pages 68?73.
ACM New York, NY, USA.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of english: Thepenn treebank.T.
Murphy, T. McIntosh, and J.R. Curran.
2006.Named entity recognition for astronomy literature.In Proceedings of the 2006 Australasian LanguageTechnology Workshop (ALTW).K.
Nigam, J. Lafferty, and A. McCallum.
1999.
Us-ing maximum entropy for text classification.
InProceedings of the IJCAI-99 Workshop on MachineLearning for Information Filtering, pages 61?67,Stockholm, Sweden.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the EMNLPConference, pages 133?142, Philadelphia, PA.J.C.
Reynar and A. Ratnaparkhi.
1997.
A maximumentropy approach to identifying sentence bound-aries.
In Proceedings of the fifth conference on Ap-plied natural language processing, pages 16?19.R.
Rosenfeld.
1996.
A maximum entropy approach toadaptive statistical language modeling.
Computer,Speech and Language, 10:187?228.J.M.
Swales.
1990.
Genre analysis: English in aca-demic and research settings.
Cambridge UniversityPress.S.
Teufel and M. Moens.
2002.
Summarising scientificarticles?
experiments with relevance and rhetoricalstatus.
Computational Linguistics, 28(4):409?445.S.
Teufel, J. Carletta, and M. Moens.
1999.
An anno-tation scheme for discourse-level argumentation inresearch articles.
In Proceedings of EACL 1999.S.
Teufel, A. Siddharthan, and D. Tidhar.
2006.
Auto-matic classification of citation function.
In Proceed-ings of the 2006 Conference on Empirical Methodsin Natural Language Processing, pages 103?110.S.
Teufel.
1999.
Argumentative zoning: Informationextraction from scientific text.
Ph.D. thesis, Univer-sity of Edinburgh, Edinburgh, UK.26
