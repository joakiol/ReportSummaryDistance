Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 115?124,Vancouver, October 2005. c?2005 Association for Computational LinguisticsHead-Driven PCFGs with Latent-Head StatisticsDetlef PrescherInstitute for Logic, Language and ComputationUniversity of Amsterdamprescher@science.uva.nlAbstractAlthough state-of-the-art parsers for nat-ural language are lexicalized, it was re-cently shown that an accurate unlexical-ized parser for the Penn tree-bank can besimply read off a manually refined tree-bank.
While lexicalized parsers often suf-fer from sparse data, manual mark-up iscostly and largely based on individual lin-guistic intuition.
Thus, across domains,languages, and tree-bank annotations, afundamental question arises: Is it possibleto automatically induce an accurate parserfrom a tree-bank without resorting to fulllexicalization?
In this paper, we show howto induce head-driven probabilistic parserswith latent heads from a tree-bank.
Ourautomatically trained parser has a perfor-mance of 85.7% (LP/LR F1), which is al-ready better than that of early lexicalizedones.1 IntroductionState-of-the-art statistical parsers for natural lan-guage are based on probabilistic grammars acquiredfrom transformed tree-banks.
The method of trans-forming the tree-bank is of major influence on theaccuracy and coverage of the statistical parser.
Themost important tree-bank transformation in the lit-erature is lexicalization: Each node in a tree is la-beled with its head word, the most important word ofthe constituent under the node (Magerman (1995),Collins (1996), Charniak (1997), Collins (1997),Carroll and Rooth (1998), etc.).
It turns out, how-ever, that lexicalization is not unproblematic: First,there is evidence that full lexicalization does notcarry over across different tree-banks for other lan-guages, annotations or domains (Dubey and Keller,2003).
Second, full lexicalization leads to a serioussparse-data problem, which can only be solved bysophisticated smoothing and pruning techniques.Recently, Klein and Manning (2003) showed thata carefully performed linguistic mark-up of the tree-bank leads to almost the same performance results aslexicalization.
This result is attractive since unlexi-calized grammars are easy to estimate, easy to parsewith, and time- and space-efficient: Klein and Man-ning (2003) do not smooth grammar-rule probabil-ities, except unknown-word probabilities, and theydo not prune since they are able to determine themost probable parse of each full parse forest.
Bothfacts are noteworthy in the context of statistical pars-ing with a tree-bank grammar.
A drawback of theirmethod is, however, that manual linguistic mark-upis not based on abstract rules but rather on individuallinguistic intuition, which makes it difficult to repeattheir experiment and to generalize their findings tolanguages other than English.Is it possible to automatically acquire a more re-fined probabilistic grammar from a given tree-bankwithout resorting to full lexicalization?
We presenta novel method that is able to induce a parser thatis located between two extremes: a fully-lexicalizedparser on one side versus an accurate unlexicalizedparser based on a manually refined tree-bank on theother side.In short, our method is based on the same lin-guistic principles of headedness as other methods:We do believe that lexical information representsan important knowledge source.
To circumventdata sparseness resulting from full lexicalization115with words, we simply follow the suggestion ofvarious advanced linguistic theories, e.g.
Lexical-Functional Grammar (Bresnan and Kaplan, 1982),where more complex categories based on featurecombinations represent the lexical effect.
We com-plement this by a learning paradigm: lexical entriescarry latent information to be used as head informa-tion, and this head information is induced from thetree-bank.In this paper, we study two different latent-headmodels, as well as two different estimation meth-ods: The first model is built around completely hid-den heads, whereas the second one uses relativelyfine-grained combinations of Part-Of-Speech (POS)tags with hidden extra-information; The first esti-mation method selects a head-driven probabilisticcontext-free grammar (PCFG) by exploiting latent-head distributions for each node in the tree-bank,whereas the second one is more traditional, readingoff the grammar from the tree-bank annotated withthe most probable latent heads only.
In other words,both models and estimation methods differ in the de-gree of information incorporated into them as priorknowledge.
In general, it can be expected that thebetter (sharper or richer, or more accurate) the in-formation is, the better the induced grammar willbe.
Our empirical results, however, are surprising:First, estimation with latent-head distributions out-performs estimation with most-probable-head anno-tation.
Second, modeling with completely hiddenheads is almost as good as modeling with latentheads based on POS tags, and moreover, results inmuch smaller grammars.We emphasize that our task is to automatically in-duce a more refined grammar based on a few linguis-tic principles.
With automatic refinement it is harderto guarantee improved performance than with man-ual refinements (Klein and Manning, 2003) or withrefinements based on direct lexicalization (Mager-man (1995), Collins (1996), Charniak (1997), etc.
).If, however, our refinement provides improved per-formance then it has a clear advantage: it is automat-ically induced, which suggests that it is applicableacross different domains, languages and tree-bankannotations.Applying our method to the benchmark Penn tree-bank Wall-Street Journal, we obtain a refined proba-bilistic grammar that significantly improves over theoriginal tree-bank grammar and that shows perfor-mance that is on par with early work on lexicalizedprobabilistic grammars.
This is a promising resultgiven the hard task of automatic induction of im-proved probabilistic grammars.2 Head LexicalizationAs previously shown (Charniak (1997), Collins(1997), Carroll and Rooth (1998), etc.
), Context-Free Grammars (CFGs) can be transformed to lexi-calized CFGs, provided that a head-marking schemefor rules is given.
The basic idea is that the headmarking on rules is used to project lexical items upa chain of nodes.
Figure 1 displays an example.In this Section, we focus on the approaches ofCharniak (1997) and Carroll and Rooth (1998).These approaches are especially attractive for us fortwo reasons: First, both approaches make use of anexplicit linguistic grammar.
By contrast, alternativeapproaches, like Collins (1997), apply an additionaltransformation to each tree in the tree-bank, splittingeach rule into small parts, which finally results in anew grammar covering many more sentences thanthe explicit one.
Second, Charniak (1997) and Car-roll and Rooth (1998) rely on almost the same lex-icalization technique.
In fact, the significant differ-ence between them is that, in one case, a lexicalizedversion of the tree-bank grammar is learned froma corpus of trees (supervised learning), whereas, inthe other case, a lexicalized version of a manuallywritten CFG is learned from a a text corpus (un-supervised learning).
As we will see in Section 3,our approach is a blend of these approaches in thatit aims at unsupervised learning of a (latent-head-)lexicalized version of the tree-bank grammar.Starting with Charniak (1997), Figure 2 displaysan internal rule as it is used in the parse in Figure1,and its probability as defined by Charniak.
Here, His the head-child of the rule, which inherits the headh from its parent C. The children D1:d1, .
.
., Dm:dmand Dm+1:dm+1, .
.
., Dm+n:dm+n are left and rightmodifiers of H. Either n or m may be zero, andn = m = 0 for unary rules.
Because the probabil-ities occurring in Charniak?s definition are alreadyso specific that there is no real chance of obtainingthe data empirically, they are smoothed by deletedinterpolation:116S:roseNP:protsADJ:CorporateCorporateN:protsprotsVP:roseV:roserosePUNC:..Internal Rules:S:rose ??
NP:prots VP:rose PUNC:.NP:prots??
ADJ:Corporate N:protsVP:rose ??
V:roseLexical Rules:ADJ:Corporate??
CorporateN:prots ??
protsV:rose ??
rosePUNC:.
??
.Figure 1: Parse tree, and a list of the rules it contains (Charniak, 1997)C:hD1:d1 ?
?
?
Dm:dm H:h Dm+1:dm+1 ?
?
?
Dm+n:dm+npCHARNIAK97( this local tree ) = p( r | C, h, Cp ) ?
?n+mi=1 p( di | Di, C, h )(r is the unlexicalized rule,Cp is C?s parent category)Figure 2: Internal rule, and its probability (Charniak, 1997)p( r | C, h, Cp ) = ?1 ?
p?
( r | C, h, Cp )+ ?2 ?
p?
( r | C, h )+ ?3 ?
p?
( r | C, class(h) )+ ?4 ?
p?
( r | C, Cp )+ ?5 ?
p?
( r | C )p( d | D, C, h ) = ?1 ?
p?
( d | D, C, h )+ ?2 ?
p?
( d | D, C, class(h) )+ ?3 ?
p?
( d | D, C )+ ?4 ?
p?
( d | D )Here, class(h) denotes a class for the head wordh.
Charniak takes these word classes from an ex-ternal distributional clustering model, but does notdescribe this model in detail.An at a first glance different lexicalization tech-nique is described in Carroll and Rooth (1998).
Intheir approach, a grammar transformation is usedto lexicalize a manually written grammar.
The keystep for understanding their model is to imagine thatthe rule in Figure 2 is transformed to a sub-tree, theone displayed in Figure 3.
After this transformation,the sub-tree probability is simply calculated with thePCFG?s standard model; The result is also displayedin the figure.
Comparing this probability with theprobability that Charniak assigns to the rule itself,we see that the subtree probability equals the ruleprobability1.
In other words, both probability mod-els are based on the same idea for lexicalization, butthe type of the corpora they are estimated from differ(trees versus sentences).In more detail, Table 1 displays all four grammar-rule types resulting from the grammar transforma-tion of Carroll and Rooth (1998).
The underlyingentities from the original CFG are: The starting sym-bol S (also the starting symbol of the transform),the internal rule C ??
D1 .
.
.Dm H Dm+1 .
.
.Dm+n,and the lexical rule C ??
w. From these, thecontext-free transforms are generated as displayedin the table (for all possible head words h and d, andfor all non-head children D=D1, .
.
., Dm+n).
Fig-ure 4 displays an example parse on the basis of the1at least, if we ignore Charniak?s conditioning on C?s parentcategory Cp for the moment; Note that C?s parent category isavailable in the tree-bank, but may not occur in the left-handsides of the rules of a manually written CFG117C:hD1:C:h ?
?
?D1:d1Dm:C:hDm:dmH:h Dm+1:C:h ?
?
?Dm+1:dm+1Dm+n:C:hDm+n:dm+npSTANDARD-PCFG( this sub-tree )= p( D1 :C :h .
.
.
Dm :C :h H :h Dm+1 :C :h .
.
.
Dm+n :C : h | C :h ) ?
?m+ni=1 p( Di :di | Di :C :h )= p( D1 .
.
.
Dm H Dm+1 .
.
.
Dm+n | C, h ) ?
?m+ni=1 p( di | Di, C, h )= p( r | C, h ) ?
?m+ni=1 p( di | Di, C, h )(r is the unlexicalized rule)Figure 3: Transformed internal rule, and its standard-PCFG probability (Carroll and Rooth, 1998)S ??
S:h (Starting Rules)C:h ??
D1:C:h .
.
.
Dm:C:h H:h Dm+1:C:h .
.
.
Dm+n:C:h (Lexicalized Rules)D:C:h ??
D:d (Dependencies)C:w ??
w (Lexical Rules)Table 1: Context-free rule types in the transform (Carroll and Rooth, 1998)SS:roseNP:S:roseNP:protsADJ:NP:protsADJ:CorporateCorporateN:protsprotsVP:roseV:roserosePUNC:S:rosePUNC:..Starting Rule:S??
S:roseLexicalized Rules:S:rose ??
NP:S:rose VP:rose PUNC:S:roseNP:prots??
ADJ:NP:prots N:protsVP:rose ??
V:roseDependencies:NP:S:rose ??
NP:protsPUNC:S:rose ??
PUNC:.ADJ:NP:prots??
ADJ:CorporateLexical Rules:ADJ:Corporate??
CorporateN:prots ??
protsV:rose ??
rosePUNC:.
??
.Figure 4: Transformed parse tree, and a list of the rules it contains (Carroll and Rooth, 1998)118transformed grammar.
It is noteworthy that althoughCarroll and Rooth (1998) learn from a text corpusof about 50 million words, it is still necessary tosmooth the rule probabilities of the transform.
Un-like Charniak (1997), however, they do not use wordclasses in their back-off scheme.To summarize, the major problem of full-lexicalization techniques is that they lead to serioussparse-data problems.
For both models presented inthis section, a large number |T | of full word formsmakes it difficult to reliably estimate the probabilityweights of the O(|T |2) dependencies and the O(|T |)lexicalized rules.A linguistically naive approach to this problemis to use POS tags as heads to decrease the num-ber of heads.
From a computational perspective,the sparse data problem would then be completelysolved since the number |POS| of POS tags is tinycompared to the number |T | of full-word forms.Although we will demonstrate that parsing resultsbenefit already from this naive lexicalization rou-tine, we expect that (computationally and linguisti-cally) optimal head-lexicalized models are arrangedaround a number |HEADS| of head elements suchthat |POS| ?
|HEADS| << |T | .3 Latent-Head ModelsThis section defines two probability models over thetrees licensed by a head-lexicalized CFG with latenthead-information, thereby exploiting three simplelinguistic principles: (i) all rules have head mark-ers, (ii) information is projected up a chain of cat-egories marked as heads, (iii) lexical entries carrylatent head values which can be learned.
Moreover,two estimation methods for the latent-head modelsare described.Head-Lexicalized CFGs with Latent HeadsPrinciples (i) and (ii) are satisfied by all head lexical-ized models we know of, and clearly, they are alsosatisfied by the model of Carroll and Rooth (1998).Principle (iii), however, deals with latent informa-tion for lexical entries, which is beyond the capabil-ity of this model.
To see this, remember that lex-ical rules C ??
w are unambiguously transformedto C:w ??
w. Because this transformation is unam-biguous, latent information does not play a role in it.It is surprisingly simple, however, to satisfy princi-ple (iii) with slightly modified versions of Carrolland Rooth?s transformation of lexical rules.
In thefollowing, we present two of them:Lexical-Rule Transformation (Model 1): Trans-form each lexical rule C ??
w to a set of rules, hav-ing the form C:h ??
w, where h ?
{1, .
.
.
, L}, andL is a free parameter.Lexical-Rule Transformation (Model 2): Trans-form each lexical rule C ??
w to a set of rules,having the form C:h ??
w, where h ?
{C} ?
{1, .
.
.
, L}, and L is a free parameter.Both models introduce latent heads for lexical en-tries.
The difference is that Model 1 introduces com-pletely latent heads h, whereas Model 2 introducesheads h on the basis of the POS tag C of the wordw: each such head is a combination of C with an ab-stract extra-information.
Figure 5 gives an example.Because we still apply Carroll and Rooth?s gram-mar transformation scheme to the non-lexical rules,latent heads are percolated up a path of categoriesmarked as heads.Although our modifications are small, their ef-fect is remarkable.
In contrast to Carroll and Rooth(1998), where an unlexicalized tree is unambigu-ously mapped to a single transform, our models mapan unlexicalized tree to multiple transforms (for freeparameters ?
2).
Note also that although latent in-formation is freely introduced at the lexical level, itis not freely distributed over the nodes of the tree.Rather, the space of latent heads for a tree is con-strained according the linguistic principle of head-edness.
Finally, for the case L = 1, our models per-form unambiguous transformations: in Model 1 thetransformation makes no relevant changes, whereasModel 2 performs unambiguous lexicalization withPOS tags.
In the rest of the paper, we show how tolearn models with hidden, richer, and more accuratehead-information from a tree-bank, if L ?
2.Unsupervised Estimation of Head-LexicalizedCFGs with Latent HeadsIn the following, we define two methods for es-timating latent-head models.
The main difficultyhere is that the rules of a head-lexicalized CFG119SS:hVNP:S:hVNP:hNADJ:NP:hNADJ:hADJCorporateN:hNprotsVP:hVV:hVrosePUNC:S:hVPUNC:hPUNC.Starting Rule:S??
S:hVLexicalized Rules:S:hV ??
NP:S:hV VP:hV PUNC:S:hVNP:hN ??
ADJ:NP:hN N:hNVP:hV ??
V:hVDependencies:NP:S:hV ??
NP:hNPUNC:S:hV ??
PUNC:hPUNCADJ:NP:hN ??
ADJ:hADJLexical Rules:ADJ:hADJ ??
CorporateN:hN ??
protsV:hV ??
rosePUNC:hPUNC ??
.Model 1 (Completely Latent Heads):hADJ, hN, hV, and hPUNC ?
{1, .
.
.
, L}Model 2 (Latent Heads Based on POS Tags):hADJ ?
{ADJ} ?
{1, .
.
.
, L}hN ?
{N} ?
{1, .
.
.
, L}hV ?
{V} ?
{1, .
.
.
, L}hPUNC ?
{PUNC} ?
{1, .
.
.
, L}Number of Latent-Head Types ={L for Model 1|POS| ?
L for Model 2 (L is a free parameter)Figure 5: Parse tree with latent heads, and a list of the rules it contains.120Initialization: Generate a randomly initialized distribution p0 for the rules of GLEX (a head-lexicalized CFG with latent heads as previously defined).Iterations:(1) for each i = 1, 2, 3, .
.
., number of iterations do(2) set p = pi?1(3) E step: Generate a lexicalized tree-bank TLEX, by- running over all unlexicalized trees t of the original tree-bank- generating the finite set GLEX(t) of the lexicalized transforms of t- allocating the frequency c(t?)
= c(t) ?
p( t?
| t ) to the lexicalized trees t?
?
GLEX(t)[ Here, c(t) is the frequency of t in the original tree-bank ](4) M step: Read the tree-bank grammar off TLEX, by- calculating relative frequencies p?
for all rules of GLEX as occurring in TLEX(5) set pi = p?
(6) endFigure 6: Grammar induction algorithm (EM algorithm)with latent heads cannot be directly estimated fromthe tree-bank (by counting rules) since the latentheads are not annotated in the trees.
Faced with thisincomplete-data problem, we apply the Expectation-Maximization (EM) algorithm developed for thesetype of problems (Dempster et al, 1977).
For detailsof the EM algorithm, we refer to the numerous tuto-rials on EM (e.g.
Prescher (2003)).
Here, it sufficesto know that it is a sort of meta algorithm, result-ing for each incomplete-data problem in an iterativeestimation method that aims at maximum-likelihoodestimation on the data.
Disregarding the fact that weimplement a dynamic-programming version for ourexperiments (running in linear time in the size of thetrees in the tree-bank (Prescher, 2005)), the EM al-gorithm is here as displayed in Figure 6.
Beside thispure form of the EM algorithm, we also use a variantwhere the original tree-bank is annotated with mostprobable heads only.
Here is a characterization ofboth estimation methods:Estimation from latent-head distributions: Thekey steps of the EM algorithm produce a lexicalizedtree-bank TLEX, consisting of all lexicalized versionsof the original trees (E-step), and calculate the prob-abilities for the rules of GLEX on the basis of TLEX(M-step).
Clearly, all lexicalized trees in GLEX(t)differ only in the heads of their nodes.
Thus, EMestimation uses the original tree-bank, where eachnode can be thought of as annotated with a latent-head distribution.Estimation from most probable heads: By con-trast, a quite different scheme is applied in Klein andManning (2003): extensive manual annotation en-riches the tree-bank with information, but no treesare added to the tree-bank.
We borrow from thisscheme in that we take the best EM model to cal-culate the most probable head-lexicalized versionsof the trees in the original tree-bank.
After collect-ing this Viterbi-style lexicalized tree-bank, the ordi-nary tree-bank estimation yields another estimate ofGLEX.
Clearly, this estimation method uses the orig-inal tree-bank, where each node can be thought ofannotated with the most probable latent head.4 ExperimentsThis section presents empirical results across ourmodels and estimation methods.Data and ParametersTo facilitate comparison with previous work, wetrained our models on sections 2-21 of the WSJ sec-tion of the Penn tree-bank (Marcus et al, 1993).
Alltrees were modified such that: The empty top nodegot the category TOP, node labels consisted solelyof syntactic category information, empty nodes (i.e.nodes dominating the empty string) were deleted,and words in rules occurring less than 3 times inthe tree-bank were replaced by (word-suffix based)121baselineL=2L=5L=10Estimation from most probable headsModel 1(completely latent)(15 400) 73.5(17 900) 76.3(22 800) 80.7(28 100) 83.3?=9.8Model 2(POS+latent)(25 000) 78.9(32 300) 81.1(46 200) 83.3(58 900) 82.6?=4.4Estimation from head distributionsModel 1(completely latent)(15 400) 73.5(25 900) 76.9(49 200) 82.0(79 200) 84.6?=11.1Model 2(POS+latent)(25 000) 78.9(49 500) 81.6(116 300) 84.9(224 300) 85.7?=6.8Table 2: Parsing results in LP/LR F1 (the baseline is L = 1)unknown-word symbols.
No other changes weremade.On this tree-bank, we trained several head-lexicalized CFGs with latent-heads as described inSection 3, but smoothed the grammar rules usingdeleted interpolation; We also performed some pre-liminary experiments without smoothing, but afterobserving that about 3000 trees of our training cor-pus were allocated a zero-probability (resulting fromthe fact that too many grammar rules got a zero-probability), we decided to smooth all rule proba-bilities.We tried to find optimal starting parameters by re-peating the whole training process multiple times,but we observed that starting parameters affect fi-nal results only up to 0.5%.
We also tried to findoptimal iteration numbers by evaluating our modelsafter each iteration step on a held-out corpus, andobserved that the best results were obtained with 70to 130 iterations.
Within a wide range from 50 to200 iteration, however, iteration numbers affect fi-nal results only up to 0.5%Empirical ResultsWe evaluated on a parsing task performed on Sec-tion 22 of the WSJ section of the Penn tree-bank.
Forparsing, we mapped all unknown words to unknownword symbols, and applied the Viterbi algorithm asimplemented in Schmid (2004), exploiting its abil-ity to deal with highly-ambiguous grammars.
Thatis, we did not use any pruning or smoothing routinesfor parsing sentences.
We then de-transformed theresulting maximum-probability parses to the formatdescribed in the previous sub-section.
That is, wedeleted the heads, the dependencies, and the start-ing rules.
All grammars were able to exhaustivelyparse the evaluation corpus.
Table 2 displays our re-sults in terms of LP/LR F1 (Black and al., 1991).The largest number per column is printed in italics.The absolutely largest number is printed in boldface.The numbers in brackets are the number of gram-mar rules (without counting lexical rules).
The gainin LP/LR F1 per estimation method and per modelis also displayed (?).
Finally, the average trainingtime per iteration ranges from 2 to 4 hours (depend-ing on both L and the type of the model).
The aver-age parsing time is 10 seconds per sentence, whichis comparable to what is reported in Klein and Man-ning (2003).5 DiscussionFirst of all, all model instances outperform the base-line, i.e., the original grammar (F1=73.5), and thehead-lexicalized grammar with POS tags as heads(F1=78.9).
The only plausible explanation for thesesignificant improvements is that useful head classeshave been learned by our method.
Moreover, in-creasing L consistently increases F1 (except forModel 2 estimated from most probable heads; L =10 is out of the row).
We thus argue that the granu-larity of the current head classes is not fine enough;Further refinement may lead to even better latent-head statistics.Second, estimation from head distributions con-sistently outperforms estimation from most probableheads (for both models).
Although coarse-grainedmodels clearly benefit from POS information in theheads (L = 1, 2, 5), it is surprising that the bestmodels with completely latent heads are on a parwith or almost as good as the best ones using POS122LP LR F1 Exact CBModel 1 (this paper) 84.8 84.4 84.6 26.4 1.37Magerman (1995) 84.9 84.6 1.26Model 2 (this paper) 85.7 85.7 85.7 29.3 1.29Collins (1996) 86.3 85.8 1.14Matsuzaki etal.
(2005) 86.6 86.7 1.19Klein and Manning (2003) 86.9 85.7 86.3 30.9 1.10Charniak (1997) 87.4 87.5 1.00Collins (1997) 88.6 88.1 0.91Table 3: Comparison with other parsers (sentences of length ?
40)as head information.Finally, our absolutely best model (F1=85.7) com-bines POS tags with latent extra-information (L =10) and is estimated from latent-head distributions.Although it also has the largest number of gram-mar rules (about 224 300), it is still much smallerthan fully-lexicalized models.
The best model withcompletely latent heads, however, leads to almostthe same performance (F1=84.6), and has the furtheradvantage of having significantly fewer rules (onlyabout 79 200).
Moreover, it is the model whichleads to the largest gain compared to the baseline(?
= 11.1).In the rest of the section, we compare our methodto related methods.
To start with performance val-ues, Table 3 displays previous results on parsingSection 23 of the WSJ section of the Penn tree-bank.Comparison indicates that our best model is alreadybetter than the early lexicalized model of Mager-man (1995).
It is a bit worse than the unlexical-ized PCFGs of Klein and Manning (2003) and Mat-suzaki et al (2005), and of course, it is also worsethan state-of-the-art lexicalized parsers (experienceshows that evaluation results on sections 22 and 23do not differ much).Beyond performance values, we believe our for-malism and methodology have the following attrac-tive features: first, our models incorporate con-text and lexical information collected from thewhole tree-bank.
Information is bundled into ab-stract heads of higher-order information, which re-sults in a drastically reduced parameter space.
Interms of Section 2, our approach does not aim atimproving the approximation of rule probabilitiesp(r|C, h) and dependency probabilities p(d|D,C, h)by smoothing.
Rather, our approach induces headclasses for the words h and d from the tree-bankand aims at a exact calculation of rule proba-bilities p(r|C, class(h)) and dependency probabil-ities p(class(d)|D,C, class(h)).
This is in sharpcontrast to the smoothed fixed-word statistics inmost lexicalized parsing models derived from sparsedata (Magerman (1995), Collins (1996), Char-niak (1997), etc.).
Particularly, class-based depen-dency probabilities p(class(d)|D,C, class(h)) in-duced from the tree-bank are not exploited by mostof these parsers.Second, our method results in an automatic lin-guistic mark-up of tree-bank grammars.
In contrast,manual linguistic mark-up of the tree-bank like inKlein and Manning (2003) is based on individuallinguistic intuition and might be cost and time in-tensive.Third, our method can be thought of as a new lex-icalization scheme of CFG based on the notion oflatent head-information, or as a successful attemptto incorporate lexical classes into parsers, combinedwith a new word clustering method based on thecontext represented by tree structure.
It thus com-plements and extends the approach of Chiang andBikel (2002), who aim at discovering latent headmarkers in tree-banks to improve manually writtenhead-percolation rules.Finally, the method can also be viewed as an ex-tension of factorial HMMs (Ghahramani and Jordan,1995) to PCFGs: the node labels on trees are en-riched with a latent variable and the latent variablesare learned by EM.
Matsuzaki et al (2005) inde-pendently introduce a similar approach and presentempirical results that rival ours.
In contrast to us,123they do not use an explicit linguistic grammar, andthey do not attempt to constrain the space of la-tent variables by linguistic principles.
As a conse-quence, our best models are three orders of mag-nitude more space efficient than theirs (with about30 000 000 parameters).
Therefore, parsing withtheir models requires sophisticated smoothing andpruning, whereas parsing with ours does not.
More-over, we calculate the most probable latent-head-decorated parse and delete the latent heads in a post-processing step.
This is comparable to what they call?Viterbi complete tree?
parsing.
Under this regime,our parser is on a par with theirs (F1=85.5).
Thissuggests that both models have learned a compara-ble degree of information, which is surprising, be-cause we learn latent heads only, whereas they aimat learning general features.
Crucially, a final 1%improvement comes from selecting most-probableparses by bagging all complete parses with the sameincomplete skeleton beforehand; Clearly, a solu-tion to this NP-Complete problem (Sima?an, 2002)can/should be also incorporated into our parser.6 ConclusionWe introduced a method for inducing a head-drivenPCFG with latent-head statistics from a tree-bank.The automatically trained parser is time and spaceefficient and achieves a performance already betterthan early lexicalized ones.
This result suggests thatour grammar-induction method can be successfullyapplied across domains, languages, and tree-bankannotations.AcknowledgmentThis work was supported by the Netherlands Orga-nization for Scientific Research, NWO project no.612.000.312, ?Learning Stochastic Tree-Grammarsfrom Tree-banks?.
I also would like to thank YoavSeginer and Jelle Zuidema and the anonymous re-viewers.
A special thanks goes to Khalil Sima?an.ReferencesEzra Black and al.
1991.
A procedure for quantitativelycomparing the syntactic coverage of English gram-mars.
In Proc.
of DARPA-91.Joan Bresnan and Ronald M. Kaplan.
1982.
Lexicalfunctional grammar: A formal system for grammati-cal representation.
In The Mental Representation ofGrammatical Relations.
MIT Press.Glenn Carroll and Mats Rooth.
1998.
Valence inductionwith a head-lexicalized PCFG.
In Proc.
of EMNLP-3.Eugene Charniak.
1997.
Parsing with a context-freegrammar and word statistics.
In Proc.
of AAAI-97.David Chiang and D. Bikel.
2002.
Recovering latentinformation in treebanks.
In Proc.
of COLING-02.Michael Collins.
1996.
A new statistical parser based onbigram lexical dependencies.
In Proc.
of ACL-96.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proc.
of ACL-97.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
J. Royal Statist.
Soc., 39(B).Amit Dubey and Frank Keller.
2003.
Probabilistic pars-ing for German using sister-head dependencies.
InProc.
of ACL-03.Zoubin Ghahramani and Michael Jordan.
1995.
FactorialHidden Markov Models.
Technical report, MIT.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proc.
of ACL-03.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proc.
of ACL-95.Mitch Marcus, Beatrice Santorini, and MaryMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn treebank.
ComputationalLinguistics, 19(2).Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProc.
of ACL-05.Detlef Prescher.
2003.
A Tutorial on the Expectation-Maximization Algorithm Including Maximum-Likelihood Estimation and EM Training of Proba-bilistic Context-Free Grammars.
Presented at the 15thEuropean Summer School in Logic, Language andInformation (ESSLLI).Detlef Prescher.
2005.
Inducing Head-Driven PCFGswith Latent Heads: Refining a Tree-bank Grammar forParsing.
In Proc.
of the 16th European Conference onMachine Learning.Helmut Schmid.
2004.
Efficient parsing of highly am-biguous context-free grammars with bit vectors.
InProc.
of COLING-04.Khalil Sima?an.
2002.
Computational complexity ofprobabilistic disambiguation.
Grammars, 5(2).124
